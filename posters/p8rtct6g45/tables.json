[{"figure_path": "P8rTCT6g45/tables/tables_7_1.jpg", "caption": "Table 1: Pretraining LLaMA 1B with a sequence length of 256 and for 10K steps, perplexity was reported as the training average of the last 10 steps. AdamW8bit serves as the base optimizer for both.", "description": "This table presents the perplexity results for pretraining a 1B parameter LLaMA model using different methods.  The training involved a sequence length of 256 and ran for 10,000 steps.  Perplexity is reported as the average of the last 10 steps of training.  The baseline optimizer used for comparison is 8-bit AdamW.", "section": "5.1 Why do we Need Online Subspace Descent?"}, {"figure_path": "P8rTCT6g45/tables/tables_8_1.jpg", "caption": "Table 2: LLaMA 60M on C4 with sequence length 1024, with optimizers on Pt and Wt, denote as \\\"Ours {Wt optimizer} + {Pt optimizer}\\\". Adaf., and Adam refer to Adafactor and 8bit-AdamW, respectively.", "description": "This table shows the perplexity results for different combinations of optimizers used for updating the model weights (Wt) and the projection matrix (Pt) in the Online Subspace Descent method.  The experiment is conducted on the LLaMA 60M model with a sequence length of 1024 using the C4 dataset.  The table compares the performance of Online Subspace Descent against GaLore, showing that different combinations of optimizers can lead to varying performance levels. The abbreviations Adaf. and Adam refer to Adafactor and 8-bit AdamW respectively.", "section": "5.4 Can Online Subspace Descent be Applied to Different Optimizers?"}, {"figure_path": "P8rTCT6g45/tables/tables_8_2.jpg", "caption": "Table 3: Perplexity and Wall Clock Time for 7B models pretrained on C4 for 10K steps. Lower perplexity is better. Online Subspace Descent can be upto 1.3x faster than GaLore.", "description": "This table presents the results of pretraining a 7B parameter LLaMA model on the C4 dataset for 10,000 steps using both the Galore and the proposed Online Subspace Descent methods.  It compares the final perplexity achieved (lower is better, indicating better model performance) and the wall-clock time (in hours) required for training. The table highlights that Online Subspace Descent achieves a lower perplexity and a faster training time than Galore.", "section": "5.5 Can Online Subspace Descent Scale to Larger Model?"}, {"figure_path": "P8rTCT6g45/tables/tables_9_1.jpg", "caption": "Table 4: Standardized GLUE evaluation for 7B model checkpoints using eval-harness. Results are reported for various downstream tasks.", "description": "This table presents the results of downstream task evaluations on a 7B parameter Language Model.  The model was evaluated on six tasks from the GLUE benchmark: MRPC, RTE, SST-2, MNLI, QNLI, and QQP.  The table compares the performance of the proposed \"Ours\" method against the baseline Galore method, reporting the average score across all six tasks.  The scores likely represent accuracy or F1-scores, common metrics for these tasks, showcasing the relative performance improvement achieved by the proposed method on various downstream applications of the language model.", "section": "Experiment"}, {"figure_path": "P8rTCT6g45/tables/tables_14_1.jpg", "caption": "Table 5: On LLaMA 60M SS 1024, we sweep across different ranks, the trend is clear and intuitive that higher rank is preferred when it's feasible.", "description": "This table presents the results of an ablation study on the rank of the Online Subspace Descent method. It shows the perplexity achieved by the method at different ranks (32, 128, 512) and compares it to the perplexity obtained with the full rank model and GaLore method. The results demonstrate that higher ranks lead to lower perplexity, closing the gap to the full-rank baseline, but the improvement diminishes with increasing rank.", "section": "B.2 Rank Sweep"}, {"figure_path": "P8rTCT6g45/tables/tables_14_2.jpg", "caption": "Table 2: LLaMA 60M on C4 with sequence length 1024, with optimizers on Pt and Wt, denote as \"Ours {Wt optimizer} + {Pt optimizer}\". Adaf., and Adam refer to Adafactor and 8bit-AdamW, respectively.", "description": "This table presents the perplexity results of the LLaMA 60M model trained on the C4 dataset with a sequence length of 1024.  Different combinations of optimizers are used for updating the model weights (Wt) and the projection matrix (Pt).  The table compares the performance of different optimizer combinations (e.g., Lion + Lion, Adafactor + Adafactor, AdamW8bit + AdamW8bit) and their Galore counterparts.  Adaf. is an abbreviation for Adafactor, and Adam refers to 8bit-AdamW.", "section": "5.4 Can Online Subspace Descent be Applied to Different Optimizers?"}]