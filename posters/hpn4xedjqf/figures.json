[{"figure_path": "HpN4xeDJQF/figures/figures_1_1.jpg", "caption": "Figure 1: Left: The drawbacks of current collaborative agents, which train a stationary policy to manage the non-stationary dynamics of human collaborators but fail to determine the specific collaborative policies executed by humans. Right: Our approach focuses on identifying the meta-tasks underlying human decision-making and trains collaborators to match these meta-tasks in a one-to-one manner. This strategy enables effective ad-hoc collaboration with non-stationary humans.", "description": "This figure illustrates the core idea of the paper by contrasting two approaches to human-AI collaboration. The left side shows the limitations of traditional methods using a single stationary policy, highlighting the difficulty in adapting to non-stationary human behavior. The right side showcases the proposed method, which focuses on identifying meta-tasks to achieve more effective and adaptable collaboration.  The figure uses a visual metaphor of a cooking game to illustrate the concept.", "section": "1 Introduction"}, {"figure_path": "HpN4xeDJQF/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the CBPR Framework. This framework is divided into two main phases. Left: Offline Training Phase. This includes (1) constructing meta-task models using collected data and creating a meta-task library; (2) developing cooperative policies for each meta-task to compile an AI policy library; (3) establishing a performance model by evaluating each meta-task and AI policy pair. Right: Online Collaboration Phase. During a collaboration round, the process involves (a) gathering a list of historical and current human data; (b) determining the current meta-task undertaken by the human using Bayesian policy inference (refer to Equation 3-4); (c) selecting the most suitable AI policy for cooperation (as per Equation 5); and finally, (d) the AI collaborator executes actions according to the chosen policy.", "description": "The figure illustrates the CBPR (Collaborative Bayesian Policy Reuse) framework, divided into offline and online phases.  The offline phase involves building a meta-task library, training AI policies for each meta-task, and creating a performance model. The online phase shows the real-time collaboration process: (a) human action data is collected, (b) the current meta-task is inferred, (c) the optimal AI policy is selected based on the performance model and inferred meta-task, and (d) the AI takes action accordingly.", "section": "3 Collaborative Bayesian Policy Reuse"}, {"figure_path": "HpN4xeDJQF/figures/figures_6_1.jpg", "caption": "Figure 3: Comparative performance analysis against baselines when collaborators switch their rule-based policies per episode. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard deviation calculated from five random seeds.", "description": "This figure compares the performance of CBPR against three baselines (BCP, FCP, and SP) across four different Overcooked game layouts.  The agents' rule-based policies are switched every episode to simulate non-stationary human behavior.  CBPR consistently outperforms the baselines, demonstrating its ability to adapt to dynamic changes in partner strategies. The shaded areas represent the standard deviation across five different random seeds, indicating the consistency of the results.", "section": "4.1 Cooperating with Rule-Based Agents Under Dynamic Policy Switching"}, {"figure_path": "HpN4xeDJQF/figures/figures_7_1.jpg", "caption": "Figure 4: Comparative performance analysis against baselines in cooperation with partners of diverse skill levels (low, medium and high). All agents were evaluated over 50 episodes and errors bars denote 95% confidence intervals.", "description": "This figure shows the mean episodic rewards achieved by CBPR and three baseline methods (BCP, FCP, and SP) when collaborating with partners of varying skill levels (low, medium, and high) across four different Overcooked game layouts.  Error bars represent the 95% confidence intervals.  The results demonstrate CBPR's superior performance, particularly when working with lower-skilled partners. The performance difference is less pronounced in layouts where agents' movements are not hindered by each other (Asymmetric Advantage and Soup Coordination).", "section": "4.2 Cooperation with Partners of Various Collaboration Skills"}, {"figure_path": "HpN4xeDJQF/figures/figures_7_2.jpg", "caption": "Figure 4: Comparative performance analysis against baselines in cooperation with partners of diverse skill levels (low, medium and high). All agents were evaluated over 50 episodes and errors bars denote 95% confidence intervals.", "description": "This figure compares the performance of CBPR against three baselines (BCP, FCP, and SP) when collaborating with partners of varying skill levels (low, medium, and high).  The box plots show the distribution of episodic rewards across four different game layouts (Cramped Room, Coordination Ring, Asymmetric Advantages, and Soup Coordination).  Error bars represent 95% confidence intervals.  The results demonstrate CBPR's superior performance in most cases, especially when collaborating with lower-skilled partners.", "section": "4.2 Cooperation with Partners of Various Collaboration Skills"}, {"figure_path": "HpN4xeDJQF/figures/figures_8_1.jpg", "caption": "Figure 6: This case study analyzes five discontinuous frames from the Overcooked game interface to demonstrate the superiority of the CBPR algorithm. When a human player picks the cooked soup from the pot, the CBPR agent adapts by altering its initial plan to deliver the soup: it sets down the dish and places new onions in the pot, thereby showcasing its ability to adjust to human policies. In contrast, the FCP agent displays confusion when the human retrieves the soup and resumes placing onions only after the soup is served. The BCP agent rigidly adheres to its predetermined plan, continuously holding the plate without switching tasks to place onions, ignoring the fact that the soup has already been served.", "description": "This figure shows a case study comparing the performance of CBPR, FCP, and BCP agents in a collaborative cooking scenario. The figure highlights the adaptive behavior of the CBPR agent, its ability to adjust its actions based on the human partner's actions, and the contrasting rigid and less effective approaches of the FCP and BCP agents.", "section": "4.3 Cooperation with Real Humans"}, {"figure_path": "HpN4xeDJQF/figures/figures_9_1.jpg", "caption": "Figure 17: Episodic reward by using different length l of human behavior queue in other three layouts. All agents were evaluated over 50 episodes and error bars denote 95% confidence intervals.", "description": "This figure shows the ablation study on the length of the human behavior queue (l) in the CBPR algorithm. It compares the performance across three different layouts (Coordination Ring, Asymmetric Advantage, and Soup Coordination) with varying queue lengths (l=5, l=10, l=20, l=50) and skill levels (Low, Medium, High).  The error bars represent the 95% confidence intervals.  The results illustrate the effect of the length of the queue on the algorithm's ability to learn and adapt to the varying behaviors of human partners.", "section": "4.4 Ablation Study"}, {"figure_path": "HpN4xeDJQF/figures/figures_14_1.jpg", "caption": "Figure 8: The four Overcooked experiment layouts used in our study (from left to right): Cramped Room, Coordination Ring, Asymmetric Advantage, and Soup Coordination. The game mechanics involve two players collaborating to prepare and serve dishes, like soups made of onions or tomatoes. Effective teamwork is reflected in the successful delivery of multiple orders. It is noteworthy that the Marshmallow Experiment layout differs from the others in terms of cooking time and reward settings.", "description": "This figure shows the four different game layouts used in the Overcooked experiments.  Each layout presents a unique set of challenges in terms of kitchen layout and ingredient placement, requiring varying levels of teamwork and coordination between players.  The image highlights the differences in starting positions, ingredient locations, and the overall flow of the kitchen in each environment.  It also shows the reward structure and cooking time differences among the layouts.", "section": "4 Experiments"}, {"figure_path": "HpN4xeDJQF/figures/figures_17_1.jpg", "caption": "Figure 10: Training curves of BCP agents over five random seeds. The shaded area denotes the standard deviation. Noticing that the reward should not be directly compared to each other because the difficulty of the task varies with different game layouts.", "description": "This figure shows the training curves for Behavior Cloning Play (BCP) agents across different Overcooked game layouts.  The x-axis represents the number of environment steps during training, and the y-axis represents the mean episodic reward.  Each line represents a different game layout, and the shaded region shows the standard deviation across five random seeds. The caption notes that direct reward comparison across layouts is inappropriate due to varying difficulty levels.", "section": "C.2 Baselines"}, {"figure_path": "HpN4xeDJQF/figures/figures_17_2.jpg", "caption": "Figure 10: Training curves of BCP agents over five random seeds. The shaded area denotes the standard deviation. Noticing that the reward should not be directly compared to each other because the difficulty of the task varies with different game layouts.", "description": "This figure shows the training curves for Behavior Cloning Play (BCP) agents across four different Overcooked game layouts. The curves represent the average episodic reward over training time, and the shaded area represents the standard deviation across five different random seeds.  The caption notes that direct comparison of reward values between layouts isn't appropriate due to varying difficulty levels in each layout.", "section": "C.2 Baselines"}, {"figure_path": "HpN4xeDJQF/figures/figures_18_1.jpg", "caption": "Figure 10: Training curves of BCP agents over five random seeds. The shaded area denotes the standard deviation. Noticing that the reward should not be directly compared to each other because the difficulty of the task varies with different game layouts.", "description": "This figure shows the training curves for Behavior Cloning Play (BCP) agents across five different random seeds for four different Overcooked game layouts. The shaded area represents the standard deviation of the performance. The caption notes that direct reward comparison across different layouts is not appropriate because task difficulty varies.", "section": "C.2 Baselines"}, {"figure_path": "HpN4xeDJQF/figures/figures_18_2.jpg", "caption": "Figure 3: Comparative performance analysis against baselines when collaborators switch their rule-based policies per episode. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard deviation calculated from five random seeds.", "description": "This figure compares the performance of CBPR against three baselines (BCP, FCP, and SP) across four different game layouts in Overcooked.  The agents' performance is evaluated over 50 episodes, where the rule-based policies of the partners switch every episode.  The shaded areas represent the standard deviation calculated across five different random seeds.  CBPR consistently outperforms the baselines, demonstrating its adaptability to non-stationary human partners.", "section": "4.1 Cooperating with Rule-Based Agents Under Dynamic Policy Switching"}, {"figure_path": "HpN4xeDJQF/figures/figures_18_3.jpg", "caption": "Figure 3: Comparative performance analysis against baselines when collaborators switch their rule-based policies per episode. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard deviation calculated from five random seeds.", "description": "This figure compares the performance of CBPR against three baselines (BCP, FCP, and SP) across four different Overcooked game layouts.  The agents' policies switch every episode.  CBPR consistently outperforms the baselines, showcasing its superior adaptability to non-stationary human dynamics.  The shaded areas represent standard deviations, calculated from five random seeds, indicating the variability in performance.", "section": "4.1 Cooperating with Rule-Based Agents Under Dynamic Policy Switching"}, {"figure_path": "HpN4xeDJQF/figures/figures_18_4.jpg", "caption": "Figure 3: Comparative performance analysis against baselines when collaborators switch their rule-based policies per episode. All agents were evaluated over 50 continuous episodes. The shaded areas denote standard deviation calculated from five random seeds.", "description": "This figure compares the performance of CBPR against three baselines (BCP, FCP, and SP) across four different game layouts in Overcooked when collaborators switch their rule-based policies every episode.  The y-axis represents the mean episode reward, and the x-axis represents the number of episodes. The shaded areas represent the standard deviation calculated from five random seeds.  The results show that CBPR consistently outperforms the baselines, highlighting its ability to adapt to non-stationary human dynamics.", "section": "4.1 Cooperating with Rule-Based Agents Under Dynamic Policy Switching"}, {"figure_path": "HpN4xeDJQF/figures/figures_19_1.jpg", "caption": "Figure 16: Episodic reward by using different weight rho of inter-episodic belief in other three layouts. All agents were evaluated over 50 episodes and error bars denote 95% confidence intervals.", "description": "This figure shows the impact of varying the weight (rho) of the inter-episodic belief on the performance of different agents (CBPR, BCP, FCP, and SP) across three Overcooked game layouts (Coordination Ring, Asymmetric Advantage, and Soup Coordination).  The results are presented for agents with low, medium, and high skill levels. Each bar represents the mean episodic reward, and the error bars indicate the 95% confidence interval.  The results help in understanding how the balance between inter-episodic and intra-episodic belief affects collaboration.", "section": "4.4 Ablation Study"}, {"figure_path": "HpN4xeDJQF/figures/figures_19_2.jpg", "caption": "Figure 4: Comparative performance analysis against baselines in cooperation with partners of diverse skill levels (low, medium and high). All agents were evaluated over 50 episodes and errors bars denote 95% confidence intervals.", "description": "This figure compares the performance of CBPR against three baselines (BCP, FCP, and SP) when collaborating with partners of varying skill levels (low, medium, and high) across four different Overcooked game layouts.  The bar chart shows the mean episode reward for each agent and skill level, with error bars representing 95% confidence intervals. The results demonstrate CBPR's superior performance, particularly when collaborating with lower-skilled partners.  The layouts tested are Cramped Room, Coordination Ring, Asymmetric Advantage, and Soup Coordination.", "section": "4.2 Cooperation with Partners of Various Collaboration Skills"}]