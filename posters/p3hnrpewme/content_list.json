[{"type": "text", "text": "A Walsh Hadamard Derived Linear Vector Symbolic Architecture ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mohammad Mahmudul Alam1, Alexander Oberle1, Edward Raff1,2, Stella Biderman2, Tim Oates1, James Holt3 ", "page_idx": 0}, {"type": "text", "text": "1University of Maryland, Baltimore County,2Booz Allen Hamilton, 3 Laboratory for Physical Sciences m256@umbc.edu, aoberle1@umbc.edu, Raff_Edward@bah.com, biderman_stella@bah.com, oates@cs.umbc.edu, holt@lps.umd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vector Symbolic Architectures (VSAs) are one approach to developing Neurosymbolic AI, where two vectors in $\\mathbb{R}^{d}$ are \u2018bound\u2019 together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolicstyle manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamardderived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems. Code is available at https://github.com/FutureComputing4AI/ Hadamard-derived-Linear-Binding. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vector Symbolic Architectures (VSAs) are a unique approach to performing symbolic style AI. Such methods use a binding operation $B:\\mathbb{R}^{d}\\times\\bar{\\mathbb{R}^{d}}\\longrightarrow\\bar{\\mathbb{R}}^{d}$ , where $B(x,y)\\bar{=}\\;\\bar{z}$ denotes that two concepts/vectors $x$ and $y$ are connected to each other. In VSA, any arbitrary concept is assigned to vectors in $\\mathbb{R}^{d}$ (usually randomly). For example, the sentence \u201cthe fat cat and happy dog\u201d would be represented as $\\mathcal{B}(f a t,\\dot{c}a t)+\\mathcal{B}(\\mathit{h a p p y},d o g)\\stackrel{\\cdot}{=}S$ . One can then ask, \u201cwhat was happy\u201d by unbinding the vector for happy, which will return a noisy version of the vector bound to happy. The unbinding operation is denoted $B^{*}(x,y)$ , and so applying $B^{*}(S,h a p p y)\\approx d o g$ . ", "page_idx": 0}, {"type": "text", "text": "Because VSAs are applied over vectors, they offer an attractive platform for neuro-symbolic methods by having natural symbolic AI-style manipulations via differentiable operations. However, current VSA methods have largely been derived for classical AI tasks or cognitive science-inspired work. Many such VSAs have shown issues in numerical stability, computational complexity, or otherwise lower-than-desired performance in the context of a differentiable system. ", "page_idx": 0}, {"type": "text", "text": "As noted in [39], most VSAs can be viewed as a linear operation where $\\,\\!\\,B(a,b)\\:=\\:a^{\\top}G b$ and $\\beta^{*}(a,b)\\,=\\,a^{\\top}F b$ , where $G$ and $F$ are $d\\times d$ matrices. Hypothetically, these matrices could be learned via gradient descent, but would not necessarily maintain the neuro-symbolic properties of VSAs without additional constraints. Still, the framework is useful as all popular VSAs we are aware fit within this framework. By choosing $G$ and $F$ with specified structure, we can change the computational complexity from $\\bar{\\mathcal{O}}\\bar{(}d^{2})$ , down to $O(d)$ for a diagonal matrix. ", "page_idx": 0}, {"type": "text", "text": "In this work, we derive a new VSA that has multiple desirable properties for both classical VSA tasks, and in deep-learning applications. Our method will have only $O(d)$ complexity for the binding step, is numerically stable, and equals or improves upon previous VSAs on multiple recent deep learning applications. Our new VSA is derived from the Walsh Hadamard transform, and so we term our method the Hadamard-derived linear Binding (HLB) as it will avoid the ${\\mathcal{O}}(d\\log d)$ normally associated with the Hadamard transform, and has better performance than more expensive VSA alternatives. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Related work to our own will be reviewed in section 2, including our baseline VSAs and their definitions. Our new HLB will be derived in section 3, showing it theoretically desirable properties. section 4 will empirically evaluate HLB in classical VSA benchmark tasks, and in two recent deep learning tasks, showing improved performance in each scenario. We then conclude in section 5. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Smolensky [38] started the VSA approach with the Tensor Product Representation (TPR), where $d$ dimensional vectors (each representing some concept) were bound by computing an outer product. Showing distributivity $(\\mathcal{B}(\\pmb{x},\\bar{\\pmb{y}}+z)=\\bar{\\mathcal{B}}(\\pmb{x},\\pmb{y})+\\mathcal{B}(\\pmb{x},z))$ and associativity, this allowed specifying logical statements/structures [13]. However, for $\\rho$ total items to be bound together, it was impractical due to the ${\\mathcal{O}}(d^{\\rho})$ complexity. [36, 25, 24] have surveyed many of the VSAs available today, but our work will focus on three specific alternatives, as outlined in Table 1. The Vector-Derived Transformation Binding (VTB) will be a primary comparison because it is one of the most recently developed VSAs, which has shown improvements in what we will call \u201cclassic\u201d tasks, where the VSA\u2019s symbolic like properties are used to manually construct a series of binding/unbinding operations that accomplish a desired task. Note, that the VTB is unique in it is non-symmetric $({\\bar{B}}({\\pmb x},{\\pmb y})\\neq{B}({\\pmb y},{\\pmb x}))$ . Ours, and most others, are symmetric. ", "page_idx": 1}, {"type": "table", "img_path": "p3hNrpeWMe/tmp/cab01b74bbe6d0b8fe5b56a8210f3c7216141c7c65fbdc9359bd33e30fda9368.jpg", "table_caption": ["Table 1: The binding and initialization mechanisms for our new HLB with baseline methods. HLB is related to the HRR in being derived via a similar approach, but replacing the Fourier transform $\\mathcal F(\\cdot)$ with the Hadamard transform (which simplifies out). The MAP is most similar to our approach in mechanics, but the difference in derived unbinding steps leads to dramatically different performance. The VTB is the most recently developed VSA in modern use. The matrix $V_{y}$ of VTB is a blockdiagonal matrix composed from the values of the $\\textit{\\textbf{y}}$ vector, which we refer the reader to [12] for details. The TorchHD library [15] is used for implementations of prior methods. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Next is the Holographic Reduced Representation (HRR) [32], which can be defined via the Fourier transform $\\mathcal F(\\cdot)$ . One derives the inverse operation of the HRR by defining the one vector $\\vec{1}$ as the identity vector and then solving $\\mathcal{F}(\\pmb{a}^{*})_{i}\\bar{\\mathcal{F}}(\\pmb{a})_{i}\\,=\\,1$ . We will use a similar approach to deriving HLB but replacing the Fourier Transform with the Hadamard transform, making the HRR a key baseline. Last, the Multiply Add Permute (MAP) [10] is derived by taking only the diagonal of the tensor product from [38]\u2019s TPR. This results in a surprisingly simple representation of using element-wise multiplication for both binding/unbinding, making it a key baseline. The MAP binding is also notable for its continuous (MAP-C) and binary (MAP-B) forms, which will help elucidate the importance of the difference in our unbinding step compared to the initialization avoiding values near zero. HLB differs in devising for the unbinding step, and we will later show an additional corrective term that HLB employs for $\\rho$ different items bound together, that dramatically improve performance. ", "page_idx": 1}, {"type": "text", "text": "Our motivation for using the Hadamard Transform comes from its parallels to the Fourier Transform (FT) used to derive the HRR and the HRR\u2019s relatively high performance. The Hadamard matrix has a simple recursive structure, making analysis tractable, and its transpose is its own inverse, which simplifies the design of the inverse function $B^{\\ast}$ . Like the FT, WHT can be computed in log-linear time, though in our case, the derivation results in linear complexity as an added benefti. The WHT is already associative and distributive, making less work to obtain the desired properties. Finally, the WHT involves only $\\{-1,1\\}$ values, avoiding numerical instability that can occur with the HRR/FT. ", "page_idx": 1}, {"type": "text", "text": "This work shows that these motivations are well founded, as they result in a binding with comparable or improved performance in our testing. ", "page_idx": 2}, {"type": "text", "text": "Our interest in VSAs comes from their utility in both classical symbolic tasks and as useful priors in designing deep learning systems. In classic tasks VSAs are popular for designing power-efficient systems from a finite set of operations [14, 23, 17, 30]. HRRs, in particular, have shown biologically plausible models of human cognition [21, 5, 40, 6] and solving cognitive science tasks [8]. In deep learning the TPR has inspired many prior works in natural language processing [34, 16, 35]. To wit, The HRR operation has seen the most use in differentiable systems [43, 41, 42, 27, 29, 33, 1, 2, 28]. To study our method, we select two recent works that make heavy use of the neuro-symbolic capabilities of HRRs. First, an Extreme Multi-Label (XML) task that uses HRRs to represent an output space of tens to hundreds of thousands of classes $C$ in a smaller dimension $d<C$ [9], and an information privacy task that uses the HRR binding as a kind of \u201cencrypt/decrypt\u201d mechanism for heuristic security [3]. We will explain these methods in more detail in the experimental section. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we will briefly review the definition of the Hadamard matrix $H$ and its important properties that make it a strong candidate from which to derive a VSA. With these properties established, we will begin by deriving a VSA we term HLB where binding and unbinding are the same operation in the same manner as which the original HRR can be derived [32]. Any VSA must introduce noise when vectors are bound together, and we will derive the form of the noise term as $\\eta^{\\circ}$ . Unsatisfied with the magnitude of this term, we then define a projection step for the Hadamard matrix in a similar spirit to \u2018[9]\u2019s complex-unit magnitude projection to support the HRR and derive an improved operation with a new and smaller noise term $\\eta^{\\pi}$ . This will give us the HLB bind/unbind steps as noted in Table 1. ", "page_idx": 2}, {"type": "text", "text": "Hadamard $H_{d}$ is a square matrix of size $d\\!\\times d$ of orthogonal rows consisting of only $+1$ and $-1\\mathrm{s}$ given in Equation 1 where $d=2^{n}\\;\\forall\\;n\\in\\mathbb{N}:n\\geqslant0$ . Bearing in mind that Hadamard or Walsh-Hadamard Transformation (WHT) can be equivalent to discrete multi-dimensional Fourier Transform (FT) when applied to a $d$ dimensional vector [26], it has additional advantages over Discrete Fourier Transform (DFT). Unlike DFT, which operates on complex $\\mathbb{C}$ numbers and requires irrational multiplications, WHT only performs calculations on real $\\mathbb{R}$ numbers with addition and subtraction operators and does not require any irrational multiplication. ", "page_idx": 2}, {"type": "equation", "text": "$$\nH_{1}=[1]\\qquad H_{2}={\\left[\\begin{array}{l l}{1}&{1}\\\\ {1}&{-1}\\end{array}\\right]}\\qquad\\cdot\\cdot\\cdot\\qquad H_{2^{n}}={\\left[\\begin{array}{l l}{H_{2^{n-1}}}&{H_{2^{n-1}}}\\\\ {H_{2^{n-1}}}&{-H_{2^{n-1}}}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Vector symbolic architectures (VSA), for instance, Holographic Reduced Representations (HRR) employs circular convolution to represent compositional structure which is computed using Fast Fourier Transform (FFT) [32]. However, it can be numerically unstable due to irrational multiplications of complex numbers. Prior work [9] devised a projection step to mitigate the numerical instability of the FFT and it\u2019s inverse, but we instead ask if re-deriving the binding/unbinding operations may yield favorable results if we use the favorable properties of the Hadamard transform as given in Lemma 3.1. ", "page_idx": 2}, {"type": "text", "text": "Lemma 3.1 (Hadamard Properties). Let $H$ be the Hadamard matrix of size $d\\times d$ that holds the following properties for $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ . First, $H(H x)=d x$ , and second $H(x+y)=H x+H y$ . ", "page_idx": 2}, {"type": "text", "text": "The bound composition of two vectors into a single vector space is referred to as BINDING. The knowledge retrieval from a bound representation is known as UNBINDING. We define the binding function by replacing the Fourier transform in circular convolution with the Hadamard transform given in Definition 3.1. We will denote the binding function four our specific method by $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and the unbinding function by $B^{\\ast}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (Binding and Unbinding). The binding of vectors $x,y\\in\\mathbb{R}^{d}$ in Hadamard domain is defined in Equation 2 where $\\odot$ is the elementwise multiplication. The unbinding function is defined in a similar fashion, i.e., $B=B^{*}$ . In the context of binding, $B(x,y)$ combines the vectors $x$ and $y$ , whereas in the context of unbinding $B^{*}(x,y)$ refers to the retrieval of the vector associated with $y$ from $x$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{B}(x,y)=\\frac{1}{d}\\cdot H(H x\\odot H y)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Now, we will discuss the binding of $\\rho$ different representations, which will become important later in our analysis but is discussed here for adjacency to the binding definition. Composite representation in vector symbolic architectures is defined by the summation of the bound vectors. We define a parameter $\\rho\\in\\mathbb{N}:\\rho\\geqslant1$ that denotes the number of vector pairs bundled in a composite representation. Given vectors $\\dot{x}_{i},y_{i}\\in\\mathbb{R}^{d}$ and $\\forall~i\\in\\mathbb{N}:1\\leqslant i\\leqslant\\rho$ , we can define the composite representation $\\chi$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\rho=1}{\\chi}={\\cal B}(x_{1},y_{1})\\qquad\\underset{\\rho=2}{\\chi}={\\cal B}(x_{1},y_{1})+{\\cal B}(x_{2},y_{2})\\qquad\\cdot\\cdot\\cdot\\qquad\\chi_{\\rho}=\\sum_{i=1}^{\\rho}{\\cal B}(x_{i},y_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Next, we require the unbinding operation, which is defined via an inverse function via the following theorem. This will give a symbolic form of our unbinding step that retrieves the original component $\\textbf{\\em x}$ being searched for, as well as a necessary noise component $\\eta^{\\mathrm{{o}}}$ , which must exist whenever $\\rho\\geqslant2$ items are bound together without expanding the dimension $d$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Inverse Theorem). Given the identity function $H x\\cdot H x^{\\dagger}=\\mathbb{1}$ where $x^{\\dagger}$ is the inverse of $x$ in Hadamard domain, then $B^{*}(B(x_{1},y_{1})+\\cdot\\cdot\\cdot+B(x_{\\rho},y_{\\rho}),y_{i}^{\\dag})\\,=\\,\\left\\{{x_{i}}\\qquad\\qquad{i f\\quad\\rho=1}\\right.}$ where $x_{i},y_{i}\\in\\mathbb{R}^{d}$ and $\\eta_{i}^{\\mathrm{o}}$ is the noise component. ", "page_idx": 3}, {"type": "text", "text": "Proof of Theorem 3.1. We start from the identity function $H x\\cdot H x^{\\dagger}=\\mathbb{1}$ and thus $\\begin{array}{r}{H x^{\\dagger}=\\frac{\\mathbb{1}}{H x}}\\end{array}$ . Now using Equation 2 we get, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3^{*}(B(x_{1},y_{1})+\\cdots+B(x_{\\rho},y_{\\rho}),y_{i}^{\\dagger})=\\displaystyle\\frac{1}{d}\\cdot H((H x_{1}\\odot H y_{1}+\\cdots+H x_{\\rho}\\odot H y_{\\rho})\\odot\\frac{1}{H y_{i}})}\\\\ &{=\\displaystyle\\frac{1}{d}\\cdot H(H x_{i}+\\frac{1}{H y_{i}}\\odot\\sum_{j=1\\atop j\\ne i}^{\\rho}(H x_{j}\\odot H y_{j}))=x_{i}+\\frac{1}{d}\\cdot H(\\frac{1}{H y_{i}}\\odot\\sum_{j=1\\atop j\\ne i}^{\\rho}(H x_{j}\\odot H y_{j}))\\quad L e m m a\\ 3.1}\\\\ &{=\\displaystyle\\left\\{x_{i}\\qquad\\qquad\\mathrm{if}\\quad\\rho=1\\right.}\\\\ &{\\displaystyle=\\left\\{x_{i}+\\eta_{i}^{\\circ}\\quad\\quad\\mathrm{else}\\ \\rho>1}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To reduce the noise component and improve the retrieval accuracy, [9, 32] proposes a projection step to the input vectors by normalizing them by the absolute value in the Fourier domain. While such identical normalization is not useful in the Hadamard domain since it will only transform the elements to $+1$ and $-1\\mathrm{s}$ , we will define a projection step with only the Hadamard transformation without normalization given in Definition 3.2. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Projection). The projection function of $x$ is defined by $\\textstyle\\pi(x)={\\frac{1}{d}}\\cdot H x$ . ", "page_idx": 3}, {"type": "text", "text": "If we apply the Definition 3.2 to the inputs in Theorem 3.1 then we get ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{B^{*}(B(\\pi(x_{1}),\\pi(y_{1}))+\\cdots+B(\\pi(x_{\\rho}),\\pi(y_{\\rho})),\\pi(y_{i})^{\\dag})=B^{*}(\\frac{1}{d}\\cdot H(x_{1}\\odot y_{1}+\\cdots x_{\\rho}\\odot y_{\\rho}),\\frac{1}{y_{i}})}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{1}{d}\\cdot H(\\frac{1}{y_{i}}\\odot(x_{1}\\odot y_{1}+\\cdots x_{\\rho}\\odot y_{\\rho}))_{\\textstyle\\atop{\\scriptstyle.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The retrieved value would be projected onto the Hadamard domain as well and to get back the original data we apply the reverse projection. Since the inverse of the Hadamard matrix is the Hadamard matrix itself, in the reverse projection step we just apply the Hadamard transformation again which derives the output to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H(\\frac{1}{d}\\cdot H(\\frac{1}{y_{i}}\\odot(x_{1}\\odot y_{1}+\\cdots x_{\\rho}\\odot y_{\\rho})))=\\frac{1}{y_{i}}\\odot(x_{1}\\odot y_{1}+\\cdots+x_{\\rho}\\odot y_{\\rho})}\\\\ &{\\quad=\\left\\{\\!\\!\\begin{array}{l l}{x_{i}}&{\\quad\\mathrm{if}\\quad\\rho=1}\\\\ {x_{i}+\\displaystyle\\sum_{j=1,\\ j\\ne i}^{\\rho}\\frac{x_{j}y_{j}}{y_{i}}}&{\\quad\\mathrm{else}\\ \\rho>1}\\end{array}\\right.\\quad=\\left\\{\\!\\!x_{i}\\!\\quad\\begin{array}{l l}{\\!\\qquad}&{\\mathrm{if}\\quad\\rho=1}\\\\ {x_{i}+\\eta_{i}^{\\pi}}&{\\quad\\mathrm{else}\\ \\rho>1}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta_{i}^{\\pi}$ is the noise component due to the projection step. In expectation, $\\eta_{i}^{\\pi}<\\eta_{i}^{\\mathrm{o}}$ (see Appendix A). Thus, the projection step diminishes the accumulated noise. More interestingly, the retrieved output term does not contain any Hadamard matrix. Therefore, we can recast the initial binding definition by multiplying the query vector $y_{i}$ to the output of Equation 5 which makes the binding function as the sum of the element-wise product of the vector pairs and the compositional structure a linear time $\\mathcal{O}(n)$ representation. Thus, the redefinition of the binding function is $B^{\\prime}(x,y)=x\\odot y$ and $\\rho$ bundle of the vector pairs is $\\begin{array}{r}{\\chi_{\\rho}^{\\prime}=\\sum_{i=1}^{\\rho}(x_{i}\\odot y_{i})}\\end{array}$ . Consequently, the unbinding would be a simple element-wise division of the bound  r\u0159epresentation by the query vector, i.e, $\\begin{array}{r}{B^{*\\prime}(x,y)=x\\odot\\frac{1}{y}}\\end{array}$ where $x$ and $y$ are the bound and query vector, respectively. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.1 Initialization of HLB ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the binding and unbinding operations to work, vectors need to have an expected value of zero. However, since we would divide the bound vector with query during unbinding, values close to zero would destabilize the noise component and create numerical instability. Thus, we define a Mixture of Normal Distribution (MiND) with an expected value of zero but an absolute mean greater than zero given in Equation 6 where $\\boldsymbol{\\mathcal{U}}$ is the Uniform distribution. Considering half of the elements are sampled for a normal distribution of mean $-\\mu$ and the rest of the half with a mean of $\\mu$ , the resulting vector has a zero mean with an absolute mean of $\\mu$ . The properties of the vectors sampled from a MiND distribution are given in Properties 3.1. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Omega(\\mu,1/d)={\\binom{N(-\\mu,1/d)}{N(\\mu,1/d)}}\\qquad{\\mathrm{if}}\\quad\\mathcal{U}(0,1)>0.5\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Properties 3.1 (Initialization Properties). Let $x\\in\\mathbb{R}^{d}$ sampled from $\\Omega(\\mu,1/d)$ holds the following properties. $\\operatorname{E}[x]=0$ , $\\operatorname{E}[|x|]=\\mu_{*}$ , and $\\|x\\|_{2}=\\sqrt{\\mu^{2}d}$ ", "page_idx": 4}, {"type": "text", "text": "3.2 Similarity Augmentation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In VSAs, it is common to measure the similarity with an extracted embedding $\\hat{\\pmb{x}}$ with some other vector $\\textbf{\\em x}$ using the cosine similarity. For our HLB, we devise a correction term when it is known that $\\rho$ items have been bound together to extract $\\hat{\\pmb{x}}$ , i.e., $B^{*}(\\chi_{\\rho},z)=\\hat{\\pmb x}$ . Then if $\\hat{\\pmb{x}}$ is the noisy version of the true bound term $\\textbf{\\em x}$ , we want cossim $({\\hat{x}},x)=1$ , and $\\mathrm{cossim}(\\hat{\\pmb{x}},\\pmb{y})=0,\\forall\\pmb{y}\\neq\\pmb{x}$ . We achieve this by instead computing $\\mathrm{cossim}(\\hat{{\\boldsymbol{x}}},{\\boldsymbol{x}})\\cdot\\sqrt{\\rho}$ , and the derivation of this corrective term is given by Theorem 3.2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 $\\langle\\phi-\\rho$ Relationship). Given $x_{i},y_{i}\\,\\sim\\,\\Omega(\\mu,1/d)\\,\\ \\forall\\,\\,i\\,\\in\\,\\mathbb{N}\\,:\\,1\\,\\leqslant\\,i\\,\\leqslant\\,\\rho,$ , the cosine similarity $\\phi$ between the original $x_{i}$ and retrieved vector $\\hat{x_{i}}$ is approximately equal to the inverse square root of the number of vector pairs in a composite representation \u03c1 given by \u03d5 \u00ab?1\u03c1. ", "page_idx": 4}, {"type": "text", "text": "Proof of Theorem 3.2. We start with the definition of cosine similarity and insert the value of $\\hat{x_{i}}$ . The step-by-step breakdown is shown in Equation 7. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi={\\frac{\\sum_{x_{i}}^{d}x_{i}\\cdot{\\hat{x_{i}}}}{\\|x_{i}\\|_{2}\\cdot\\|{\\hat{x_{i}}}\\|_{2}}}={\\frac{{\\frac{d}{\\sum_{{i}}}}x_{i}\\cdot\\left(x_{i}+\\sum_{j=1,j\\neq i}^{\\rho}{\\frac{x_{j}y_{j}}{y_{i}}}\\right)}{\\|x_{i}\\|_{2}\\cdot\\|x_{i}+\\sum_{j=1,j\\neq i}^{\\rho}{\\frac{x_{j}y_{j}}{y_{i}}}\\|_{2}}}={\\frac{\\sum_{{\\boldsymbol{x}}_{i}}^{d}x_{i}+\\sum_{{\\boldsymbol{x}}_{i}}^{d}x_{i}\\cdot\\left(\\sum_{j=1,j\\neq i}^{\\rho}{\\frac{x_{j}y_{j}}{y_{i}}}\\right)}{\\|x_{i}\\|_{2}\\cdot\\|x_{i}+\\sum_{j=1,j\\neq i}^{\\rho}{\\frac{x_{j}y_{j}}{y_{i}}}\\|_{2}}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Employing Properties 3.1 we can derive that $\\begin{array}{r}{\\|x_{i}\\|_{2}=\\sqrt{\\sum x_{i}\\cdot x_{i}}\\,=\\sqrt{\\mu^{2}d}}\\end{array}$ and $\\|{\\frac{x_{j}\\,y_{j}}{x_{i}}}\\|\\,=\\,{\\sqrt{\\mu^{2}d}}$ . Thus, the square of the $\\|x_{i}+\\sum_{j=1,\\ j\\neq i}^{\\rho}{\\frac{x_{j}y_{j}}{y_{i}}}\\|_{2}$ can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathop{=}\\|x_{i}\\|_{2}^{2}+\\displaystyle\\sum_{j=1,\\ j\\neq i}^{\\rho}\\left\\|\\frac{x_{j}y_{j}}{y_{i}}\\right\\|_{2}^{2}\\;+\\;2\\cdot\\underbrace{\\sum_{j=1,\\ j\\neq i}^{d}\\left(\\displaystyle\\sum_{j=1,\\ j\\neq i}^{\\rho}\\frac{x_{j}y_{j}}{y_{i}}\\right)}_{\\alpha}+\\underbrace{\\sum_{j=1}^{d}\\displaystyle\\sum_{l=1}^{\\rho-1}\\sum_{l=1}^{\\rho-1}\\frac{x_{j}y_{j}}{y_{i}}\\cdot\\frac{x_{l}y_{l}}{y_{i}}}_{\\beta}}\\\\ {=\\mu^{2}d+(\\rho-1)\\cdot\\mu^{2}d+2\\alpha+2\\beta}&{{}=\\rho\\cdot\\mu^{2}d+2\\alpha+2\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, using Equation 7 and Equation 8 we can write that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{E}[\\phi]={\\frac{\\mu^{2}d+\\alpha}{{\\sqrt{\\mu^{2}d}}\\cdot{\\sqrt{\\rho\\cdot\\mu^{2}d+2\\alpha+2\\beta}}}}\\approx{\\mathrm{{}}}^{1}{\\frac{\\mu^{2}d}{{\\sqrt{\\mu^{2}d}}\\cdot{\\sqrt{\\rho\\cdot\\mu^{2}d}}}}={\\frac{\\mu^{2}d}{{\\sqrt{\\rho}}\\cdot\\mu^{2}d}}={\\frac{1}{\\sqrt{\\rho}}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The experimental result of the $\\phi-\\rho$ relationship closely follows the theoretical expectation provided in Appendix C which also indicates that the approximation is valid. Since, we know from Theorem 3.2 that similarity score $\\phi$ drops by the inverse square root of the number of vector pairs? in a composite representation $\\rho$ , in places where $\\rho$ is known or can be estimated from $\\left\\|\\chi_{\\rho}\\right\\|_{2}\\stackrel{.}{\\sim}\\mu^{2}\\sqrt{\\rho\\cdot d}$ (proof in Appendix B), it can be used to update the cosine similarity multiplying th\u203a\u203ae sc\u203a\u203aores by $\\sqrt{\\rho}$ . Equation 9 shows the updated similarity score where in a positive case $(+)$ , $\\phi$ would be close to $1/\\sqrt{\\rho}$ and in a negative case $(-),\\phi$ would be close to zero. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l r}{\\phi^{\\prime}=\\phi\\times\\sqrt{\\rho}}&{{}}&{\\phi_{(+)}^{\\prime}=\\phi_{\\rightarrow\\frac{1}{\\sqrt{\\rho}}}\\times\\sqrt{\\rho}}&{\\approx1}&{{}}&{\\phi_{(-)}^{\\prime}=\\phi_{\\rightarrow0}\\times\\sqrt{\\rho}}&{{}\\approx0}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "p3hNrpeWMe/tmp/b9c7b245f8017c7ace7ee66319b29b06a793f920c325e945a53ae62831b5f12c.jpg", "img_caption": ["Figure 1: Empirical comparison of the corrected cosine similarity scores between $\\phi_{(+)}^{\\prime}$ (on top) and $\\phi_{(-)}^{\\prime}$ (on bottom) for varying $n$ and $\\rho$ shown in heatmap. The dimension, i.e., $d=2^{n}$ is varied from 2 to 1024 $\\langle n\\in\\{1,2,\\cdots,10\\}\\rangle$ and the number of vector pairs bundled is varied from 1 to 50. This shows that we can accurately identify when a vector $\\textbf{\\em x}$ has been bound to a VSA or not when we keep track of how many pairs of terms $\\rho$ are included. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Empirical results of $\\phi^{\\prime}$ for varying $n$ and $\\rho$ are visualized and verified by a heatmap. In a composite representation $\\begin{array}{r}{\\chi_{\\rho}^{\\prime}=\\sum_{i=1}^{\\rho}(x_{i}\\odot y_{i})}\\end{array}$ , when unbinding is applied using the query $y_{i}$ , i.e., $B^{*}{}^{\\prime}(\\chi_{\\rho}^{\\prime},y_{i})=$ $\\boldsymbol{\\hat{x_{i}}}$ , a positive case is  a\u0159 similarity between $x_{i}$ and $\\boldsymbol{\\hat{x_{i}}}$ . On the contrary, similarity between $\\hat{x_{i}}$ and any $x_{j}$ where $j\\in\\{1,2,\\cdots\\,,\\rho\\}$ and $j\\neq i$ , is a negative case. Mean cosine similarity scores of 100 trials for both positive and negative cases in presented in Figure 1 where the scores for the positive cases are in the red $(\\approx1)$ shades and the scores for the negative cases are in the blue $(\\approx0)$ shades. ", "page_idx": 5}, {"type": "text", "text": "4 Empirical Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Classical VSA Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A common VSA task is, given a bundle (addition) of $\\rho$ pairs of bound vectors $\\begin{array}{r}{\\pmb{s}=\\sum_{i=1}^{\\rho}\\mathcal{B}(\\pmb{x}_{i},\\pmb{y}_{i})}\\end{array}$ , given a query $\\pmb{x}_{q}\\in\\pmb{s}$ , c can the corresponding vector $\\pmb{y}_{q}$ be correctly retrieved fro m \u0159the bundle. To test this, we perform an experiment similar to one in [37]. We first create a pool $P$ of $N\\,=\\,1000$ random vectors, then sample (with replacement) $p$ pairs of vectors for $p\\in\\{1,2,\\cdots\\,,25\\}$ . The pairs are bound together and added to create a composite representation $\\pmb{s}$ . Then, we iterate through all left pairs $\\pmb{x}_{q}$ in the composite representation and attempt to retrieve the corresponding $\\boldsymbol{y}_{q},\\forall q\\in[1,p]$ . A retrieval is considered correct if $\\mathcal{B}^{*}(\\pmb{s},\\pmb{x}_{q})^{\\top}\\pmb{y}_{q}>\\mathcal{B}^{*}(\\pmb{s},\\pmb{x}_{q})^{\\top}\\pmb{y}_{j},\\forall j\\neq q$ . The total accuracy score for the bundle is recorded, and the experiment is repeated for 50 trials. Experiments are performed to compare HRR [32], VTB [12], MAP [10], and our HLB VSAs. For each VSA, at each dimension of the vector, the area under the curve (AUC) of the accuracy vs. the no. of bound terms plot is computed, and the results are shown in Figure 2. In general, HLB has comparable performance to HRR and VTB, and performs better than MAP. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "p3hNrpeWMe/tmp/bc0f4ce561d7f51b47f9372d75b0084ed61a6595a8c5dc42cdf18e2a14a47213.jpg", "img_caption": ["Figure 2: The area under the accuracy curve due to the change of no. of bundled pairs $\\rho$ for dimensions $d$ . All the dimensions are chosen to be perfect squares due to the constraint of VTB. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "The scenario we just considered looked at bindings of only two items together, summed of many pairs of bindings. [12] proposed addition evaluations over sequential bindings that we now consider. In the random case we have an initial vector $b_{0}$ , and for $p$ rounds, we will modify it by a random vector $\\pmb{x}_{t}$ such that $\\pmb{b}_{t+1}=\\pmb{\\mathscr{B}}(\\pmb{b}_{t},\\pmb{x}_{t})$ , after which we unbind each $\\pmb{x}_{t}$ to see how well the previous $b_{t}$ is recovered. In the auto binding case, we use a single random vector $\\textbf{\\em x}$ for all $p$ rounds. ", "page_idx": 6}, {"type": "text", "text": "In this task, we are concerned with the quality of the similarity score in random/auto-binding, as we want $\\mathcal{B}^{*}(\\pmb{b}_{t+1},\\pmb{x}_{t})^{\\top}\\pmb{b}_{t}=1$ . For VSAs with approximate unbinding procedures, such as HRR, VTB, and MAP-C, the returned value will be 1 if $p=1$ but will decay as $p$ increases. HLBuses an exact unbinding procedure so that the returned value is expected to be $1\\;\\forall\\;p$ . We are also interested in the magnitude of the vectors $\\|\\boldsymbol{B}^{*}(\\boldsymbol{b}_{t+1},\\boldsymbol{x}_{t})\\|_{2}$ , where an ideal VSA has a constant magnitude that does not explode/vanish as $p$ increases. ", "page_idx": 6}, {"type": "image", "img_path": "p3hNrpeWMe/tmp/0c051e36b2ab34cd4e05ac80c6abcd9b5e4d00d7acdbe055bc8fdf12cda03b00.jpg", "img_caption": ["Figure 3: When repeatedly binding different random (left) or a single vector (right), HLB consistently returns the ideal similarity score of 1 for a present item (top row) and has a constant magnitude (bottom row), avoiding exploding/vanishing values. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3 shows that HLB maintains a stable magnitude regardless of the number of bound vectors in both cases. This property arises due to the properties of the distribution shown in Properties 3.1. As all components have an expected absolute value of 1, the product o?f all components also has an expected absolute value of 1. Thus, the norm of the binding is simply $\\sqrt{d}$ . It also shows HLB maintains the desired similarity score as $p$ increases. Combined with Figure 1 that shows the scores are near-zero when an item is not present, HLB has significant advantages in consistency for designing VSA solutions. ", "page_idx": 6}, {"type": "text", "text": "4.2 Deep Learning with Hadamard-derived Linear Binding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Two recent methods that integrate HRR with deep learning are tested to further validate our approach and briefly summarized in the two sub-sections below. In each case, we run all four VSAs and see that HLB either matches or exceeds the performance of other VSAs. In every experiment, the standard method of sampling vectors from each VSA is followed as outlined in Table 1. All the experiments are performed on a single NVIDIA TESLA PH402 GPU with 32GB memory. ", "page_idx": 7}, {"type": "text", "text": "4.2.1 Connectionist Symbolic Pseudo Secrets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "When running on low-power computing environments, it is often desirable to offload the computation to a third-party cloud environment to get the answer faster and use fewer local resources. However, this may be problematic if one does not fully trust the available cloud environments. Homomorphic encryption (HE) is the ideal means to alleviate this problem, providing cryptography for computations. HE is currently more expensive to perform than running a neural network itself [11], defeating its own utility in this scenario. Connectionist Symbolic Pseudo Secrets (CSPS) [3] provides a heuristic means of obscuring the nature of the input (content), and output (number of classes/prediction), while also reducing the total local compute required. ", "page_idx": 7}, {"type": "text", "text": "CSPS mimics a \u201cone-time-pad\u201d by taking a random VSA vector $\\pmb{s}$ as the secret and binding it to the input $\\textbf{\\em x}$ . The value $B(s,x)$ obscures the original $\\textbf{\\em x}$ , and the third-party runs the bulk of the network on their platform. A result $\\tilde{\\b{y}}$ is returned, and a small local network computes the final answer after unbinding with the secret $B^{*}(\\tilde{\\boldsymbol{y}},\\boldsymbol{s})$ . Other than changing the VSA used, we follow the same training, testing, architecture size, and validation procedure of [3]. ", "page_idx": 7}, {"type": "text", "text": "CSPS experimented with 5 datasets MNIST, SVHN, CIFAR-10 (CR10), CIFAR-100 (CR100), and Mini-ImageNet (MIN). First, we look at the accuracy of each method, which is lower due to the noise of the random vector $\\pmb{s}$ added at test time since no secret VSA is ever reused. The results are shown in Table 2, where HLB outperforms all prior methods significantly. Notably, the MAP VSA is second best despite being one of the older VSAs, indicating its similarity to HLB in using a simple binding procedure, and thus simple gradient may be an important factor in this scenario. ", "page_idx": 7}, {"type": "table", "img_path": "p3hNrpeWMe/tmp/81512295412d2b81a40fdf9ac7c5de0a88d082b24d332b7e9824f86fb53b665c.jpg", "table_caption": ["Table 2: Accuracy comparison of the proposed HLB with HRR, VTB, MAP-C, and MAP-B in CSPS. The dimensions of the inputs along with the no. of classes are listed in the Dims/Labels column. The last row shows the geometric mean of the results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "However, improved accuracy is not useful in this scenario if more information is leaked. The test in this scenario, as proposed by [3], is to calculate the Adjusted Rand Index (ARI) after attempting to cluster the inputs $\\textbf{\\em x}$ and the outputs $\\hat{\\pmb y}$ , which are available/visible to the snooping third-party. To be successful, the ARI must be near zero (indicating random label assignment) for both inputs and outputs. ", "page_idx": 7}, {"type": "text", "text": "We use K-means, Gaussian Mixture Model (GMM), Birch [45], and HDBSCAN [7] as the clustering algorithms and specify the true number of classes to each method to maximize attacker success (information they would not know). The results can be found in Table 3, where the top rows indicate the clustering of the input $B(x,s)$ , and the bottom rows the clustering of the output $\\hat{\\pmb y}$ . All the numbers are percentages $(\\%)$ , showing all methods do a good job at hiding information from the adversary (except on the MNIST dataset, which is routinely degenerate). ", "page_idx": 7}, {"type": "text", "text": "The MNIST result is a good reminder that CSPS security is heuristic, not guaranteed. Nevertheless, we see HLB has consistently close-to-zero scores for SVHN, CIFARs, and Mini-ImageNet, indicating that its improved accuracy with simultaneously improved security. This also validates the use of the VSA in deep learning architecture design and the efficacy of our approach. ", "page_idx": 7}, {"type": "table", "img_path": "p3hNrpeWMe/tmp/cc648bf20aacd3a01a7f9ad3929540977c02d23e10cf33c569b774e239c90998.jpg", "table_caption": ["Table 3: Clustering results of the main network inputs (top rows) and outputs (bottom rows) in terms of Adjusted Rand Index (ARI). Because CSPS is trying to hide information, scores near zero are better. Cell color corresponds to the cell absolute value, with blue indicating lower ARI and red indicating higher ARI. All numbers in percentages, and show HLB is better at information hiding. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2.2 Xtreme Multi-Label Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Extreme Multi-label (XML) is the scenario where, given a single input of size $d$ , $C>>d$ classes are used to predict. This is common in e-commerce applications where new products need to be tagged, and an input on the order of $d\\approx5000$ is relatively small compared to $C\\geqslant100{,}000$ or more classes. This imposes unique computational constraints due to the output space being larger than the input space and is generally only solvable because the output space is sparse \u2014 often less than 100 relevant classes will be positive for any one input. VSAs have been applied to XML by exploiting the low positive class occurrence rate to represent the problem symbolically [9]. ", "page_idx": 8}, {"type": "text", "text": "While many prior works focus on innovative strategies to cluster/make hierarchies/compress the penultimate layer[19, 20, 31, 18, 44, 22], a neuro-symbolic approach was proposed by [9]. Given $K$ total possible classes, they assigned each class a vector $c_{k}$ to be each class\u2019s representation, and the set of all classes $\\textstyle\\mathbf{a}=\\sum_{k=1}^{K}\\pmb{c}_{k}$ ", "page_idx": 8}, {"type": "text", "text": "The VSA trick used  by [9] was to define an additional \u201cpresent\u201d class $\\pmb{p}$ and a \u201cmissing\u201d class $\\mathbf{\\nabla}m$ . Then, the target output of the network $f(\\cdot)$ is itself a vector composed of two parts added together. First, $B(p,\\bar{\\Sigma_{k}}\\,c_{k})$ represents all present classes, and so the sum is over a finite smaller set. Then the absent  cl\u0159asses compute the missing representing $\\begin{array}{r}{B(m,a-\\sum_{k}c_{k})}\\end{array}$ , which again only needs to compute over the finite set of present classes, yet represents t he\u0159 set of all non-present classes by exploiting the symbolic properties of the VSA. ", "page_idx": 8}, {"type": "text", "text": "For XML classification, we have a set of $K$ classes that will be present for a given input, where $K\\approx10$ is the norm. Yet, there will be $L$ total possible classes where $L\\geqslant100,000$ is quite common. Forming a normal linear layer to produce $L$ outputs is the majority of computational work and memory use in standard XML models, and thus the target for reduction. A VSA can be used to side-step this cost, as shown by [9], by leveraging the symbolic manipulation of the outputs. First, consider the target label as a vector $s\\stackrel{\\cdot}{\\in}\\mathrm{R}^{d}$ such that $d\\ll L$ . By defining a VSA vector to represent \u201cpresent\u201d and \u201cmissing\u201d classes as $\\mathbf{p}$ and $\\mathbf{m}$ , where each class is given its own vector $c_{1,...,L}$ , we can shift the computational complexity form $\\mathcal{O}(L)$ to $\\mathcal{O}(K)$ by manipulating the \u201cmissing\u201d classes as the compliment of the present classes as shown in Equation 10. ", "page_idx": 8}, {"type": "text", "text": "Similarly, the loss to calculate the gradient can be computed based on the network\u2019s prediction $\\hat{s}$ by taking the cosine similarity between each expected class and one cosine similarity for the representation of all missing classes. The excepted response of 1 or 0 for an item being present/absent from the VSA is used to determine if we want the similarity to be 0 (1-cos) or 1 (just cos), as shown in Equation 11. ", "page_idx": 9}, {"type": "equation", "text": "$$\n:=\\overbrace{\\sum_{i\\in y_{i}=1}^{\\mathrm{Labels}\\,\\mathrm{Present}\\mathcal{O}(d K)}}^{\\mathrm{Labels\\,Present}\\mathcal{O}(d K)}+\\overbrace{\\sum_{j\\in y_{j}=-1}^{\\mathrm{Labels\\,Absent}\\mathcal{O}(d L)}}^{\\mathrm{Labels\\,Absent}\\mathcal{O}(d L)}=\\overbrace{B\\left(p,\\left(a=\\sum_{i\\in y_{i}=1}^{\\mathrm{C}}c_{i}\\right)\\right)}^{\\mathrm{Labels\\,Present}\\mathcal{O}(d K)}+\\overbrace{B\\left(m,\\left(a-\\sum_{i\\in y_{i}=1}^{\\mathrm{C}}c_{i}\\right)\\right)}^{\\mathrm{Labels\\,Absent}\\mathcal{O}(d K)}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{l o s s=\\overbrace{\\sum_{i\\in y_{i}=1}\\left(1-\\cos\\left(\\mathcal{B}^{*}(p,\\hat{s}),c_{i}\\right)\\right)}^{\\mathrm{Present\\,Classes\\}\\mathcal{O}(d\\textit{K})}+\\overbrace{\\cos\\left(\\mathcal{B}^{*}(m,\\hat{s}),\\sum_{i\\in y_{i}=1}c_{i}\\right)}^{\\mathrm{C}(m,\\hat{s})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The details and network sizes of [9] are followed, except we replace the original VSA with our four candidates. The network is trained on 8 datasets listed in Table 4 from [4] and evaluated using normalized discounted cumulative gain (nDCG) and propensity-scored (PS) based normalized discounted cumulative gain (PSnDCG) as suggested by [19]. ", "page_idx": 9}, {"type": "text", "text": "Table 4: XML classification results in dense label representation with HRR, VTB, MAP, and HLB in terms of nDCG and PSnDCG. The proposed HLB has attained the best nDCG and PSnDCG scores on all the datasets setting a new SOTA. ", "page_idx": 9}, {"type": "table", "img_path": "p3hNrpeWMe/tmp/cc9f4d8ec0f345b0172d304016adc3c8229c17df34c836005ec1b8585943f1f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "The classification result in terms of nDCG and PSnDCG in all the eight datasets is presented in Table 4 where the top four datasets are comparatively easy with maximum no. of features of 5000 and no. of labels of 4000. The bottom four datasets are comparatively hard with the no. of features and labels on the scale of $100K$ . The proposed HLB has attained the best results in all the datasets on both metrics. In contrast to the prior CSPS results, here we see that the performance differences between HRR, VTB, and MAP are more varied, with no clear \u201csecond-place\u201d performer. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, a novel linear vector symbolic architecture named HLB is presented derived from Hadamard transform. Along with an initialization condition named MiND distribution is proposed for which we proved the cosine similarity $\\phi$ is approximately equal to the inverse square root of the no. of bundled vector pairs $\\rho$ which matches with the experimental results. The proposed HLB showed superior performance in classical VSA tasks and deep learning compared to other VSAs such as HRR, VTB, and MAP. In learning tasks, HLB is applied to CSPS and XML classification tasks. In both of the tasks, HLB has achieved the best results in terms of respective metrics in all the datasets showing a diverse potential of HLB in Neuro-symbolic AI. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, and James Holt. Recasting self-attention with holographic reduced representations. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023. [2] Mohammad Mahmudul Alam, Edward Raff, and Tim Oates. Towards generalization in subitizing with neuro-symbolic loss using holographic reduced representations. Neuro-Symbolic Learning and Reasoning in the era of Large Language Models, 2023. [3] Mohammad Mahmudul Alam, Edward Raff, Tim Oates, and James Holt. Deploying convolutional networks on untrusted platforms using 2D holographic reduced representations. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 367\u2013393. PMLR, 17\u201323 Jul 2022. [4] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme classification repository: Multi-label datasets and code, 2016. [5] Peter Blouw and Chris Eliasmith. A neurally plausible encoding of word order information into a semantic vector space. 35th Annual Conference of the Cognitive Science Society, 35:1905\u20131910, 2013. [6] Peter Blouw, Eugene Solodkin, Paul Thagard, and Chris Eliasmith. Concepts as semantic pointers: A framework and computational model. Cognitive Science, 40(5):1128\u20131162, 7 2016. [7] Ricardo JGB Campello, Davoud Moulavi, and J\u00f6rg Sander. Density-based clustering based on hierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data mining, pages 160\u2013172. Springer, 2013.   \n[8] C. Eliasmith, T. C. Stewart, X. Choo, T. Bekolay, T. DeWolf, Y. Tang, and D. Rasmussen. A large-scale model of the functioning brain. Science, 338(6111):1202\u20131205, 11 2012.   \n[9] Ashwinkumar Ganesan, Hang Gao, Sunil Gandhi, Edward Raff, Tim Oates, James Holt, and Mark McLean. Learning with holographic reduced representations. Advances in neural information processing systems, 34:25606\u201325620, 2021.   \n[10] Ross W. Gayler. Multiplicative binding, representation operators & analogy (workshop poster), 1998.   \n[11] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In International conference on machine learning, pages 201\u2013210. PMLR, 2016.   \n[12] Jan Gosmann and Chris Eliasmith. Vector-derived transformation binding: An improved binding operation for deep symbol-like processing in neural networks. Neural Comput., 31(5):849\u2013869, 2019.   \n[13] Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. arXiv, 2020.   \n[14] Robert Guirado, Abbas Rahimi, Geethan Karunaratne, Eduard Alarc\u00f3n, Abu Sebastian, and Sergi Abadal. Whype: A scale-out architecture with wireless over-the-air majority for scalable in-memory hyperdimensional computing. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 13(1):137\u2013149, 2023.   \n[15] Mike Heddes, Igor Nunes, Pere Verg\u00c3\u00a9s, Denis Kleyko, Danny Abraham, Tony Givargis, Alexandru Nicolau, and Alexander Veidenbaum. Torchhd: An open source python library to support research on hyperdimensional computing and vector symbolic architectures. Journal of Machine Learning Research, 24(255):1\u201310, 2023.   \n[16] Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Wu. Tensor product generation networks for deep nlp modeling. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1263\u20131273. Association for Computational Linguistics, jun 2018.   \n[17] Mohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. Voicehd: Hyperdimensional computing for efficient speech recognition. In 2017 IEEE International Conference on Rebooting Computing (ICRC), pages 1\u20138. IEEE, nov 2017.   \n[18] Himanshu Jain, Venkatesh Balasubramanian, Bhanu Chunduri, and Manik Varma. Slice: Scalable linear extreme classifiers trained on 100 million labels for related searches. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 528\u2013536, 2019.   \n[19] Himanshu Jain, Yashoteja Prabhu, and Manik Varma. Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 935\u2013944, 2016.   \n[20] Ankit Jalan and Purushottam Kar. Accelerating extreme classification via adaptive feature agglomeration. arXiv preprint arXiv:1905.11769, 2019.   \n[21] Michael N. Jones and Douglas J.K. Mewhort. Representing word meaning and order information in a composite holographic lexicon. Psychological Review, 114(1):1\u201337, 2007.   \n[22] Armand Joulin, Moustapha Ciss\u00e9, David Grangier, Herv\u00e9 J\u00e9gou, et al. Efficient softmax approximation for gpus. In International conference on machine learning, pages 1302\u20131310. PMLR, 2017.   \n[23] Denis Kleyko, Mike Davies, Edward Paxon Frady, Pentti Kanerva, Spencer J. Kent, Bruno A. Olshausen, Evgeny Osipov, Jan M. Rabaey, Dmitri A. Rachkovskij, Abbas Rahimi, and Friedrich T. Sommer. Vector symbolic architectures as a computing framework for emerging hardware. Proceedings of the IEEE, 110(10):1538\u20131571, 2022.   \n[24] Denis Kleyko, Dmitri Rachkovskij, Evgeny Osipov, and Abbas Rahimi. A survey on hyperdimensional computing aka vector symbolic architectures, part ii: Applications, cognitive models, and challenges. ACM Comput. Surv., 55(9), jan 2023.   \n[25] Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, and Abbas Rahimi. A survey on hyperdimensional computing aka vector symbolic architectures, part i: Models and data transformations. ACM Comput. Surv., 55(6), dec 2022.   \n[26] Kunz. On the equivalence between one-dimensional discrete walsh-hadamard and multidimensional discrete fourier transforms. IEEE Transactions on Computers, 100(3):267\u2013268, 1979.   \n[27] Siyu Liao and Bo Yuan. Circconv: A structured convolution with low complexity. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):4287\u20134294, Jul. 2019.   \n[28] Mohammad Mahmudul Alam, Edward Raff, Stella R Biderman, Tim Oates, and James Holt. Holographic global convolutional networks for long-range prediction tasks in malware detection. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 4042\u20134050. PMLR, 02\u201304 May 2024.   \n[29] Nicolas Menet, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, and Abbas Rahimi. Mimonets: Multiple-input-multiple-output neural networks exploiting computation in superposition. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 39553\u201339565. Curran Associates, Inc., 2023.   \n[30] Peer Neubert, Stefan Schubert, and Peter Protzel. Learning vector symbolic architectures for reactive robot behaviours. In Proc. of Intl. Conf. on Intelligent Robots and Systems (IROS) Workshop on Machine Learning Methods for High-Level Cognitive Capabilities in Robotics, 2016.   \n[31] Alexandru Niculescu-Mizil and Ehsan Abbasnejad. Label filters for large scale multilabel classification. In Artificial intelligence and statistics, pages 1448\u20131457. PMLR, 2017.   \n[32] Tony A Plate. Holographic reduced representations. IEEE Transactions on Neural networks, 6(3):623\u2013641, 1995.   \n[33] Rebecca Saul, Mohammad Mahmudul Alam, John Hurwitz, Edward Raff, Tim Oates, and James Holt. Lempel-ziv networks. In Proceedings on \"I Can\u2019t Believe It\u2019s Not Better! - Understanding Deep Learning Through Empirical Falsification\" at NeurIPS 2022 Workshops, volume 187 of Proceedings of Machine Learning Research, pages 1\u201311. PMLR, 03 Dec 2023.   \n[34] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9355\u20139366. PMLR, 2021.   \n[35] Imanol Schlag and J\u00fcrgen Schmidhuber. Learning to reason with third order tensor products. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[36] Kenny Schlegel, Peer Neubert, and Peter Protzel. A comparison of vector symbolic architectures. arXiv, 2020.   \n[37] Kenny Schlegel, Peer Neubert, and Peter Protzel. A comparison of vector symbolic architectures. Artificial Intelligence Review, 55(6):4523\u20134555, Aug 2022.   \n[38] Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46(1):159\u2013216, 1990.   \n[39] Julia Steinberg and Haim Sompolinsky. Associative memory of structured knowledge. Scientific Reports, 12(1), December 2022.   \n[40] Terrence C. Stewart and Chris Eliasmith. Large-scale synthesis of functional spiking neural circuits. Proceedings of the IEEE, 102(5):881\u2013898, 2014.   \n[41] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018.   \n[42] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Nilesh Agrawal, and Partha Talukdar. Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, pages 3009\u20133016. AAAI Press, 2020.   \n[43] Ryosuke Yamaki, Tadahiro Taniguchi, and Daichi Mochihashi. Holographic CCG parsing. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 262\u2013276, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[44] Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and Shanfeng Zhu. Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification. Advances in neural information processing systems, 32, 2019.   \n[45] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: an efficient data clustering method for very large databases. ACM sigmod record, 25(2):103\u2013114, 1996. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Noise Decomposition ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "When a single vector pair is combined, one of the vector pairs can be exactly retrieved with the help of the other component and the inverse function, recalling the retrieved output does not contain any noise component for a single pair of vectors, i.e., $\\rho=1$ . However, when more than one vector pairs are bundled, noise starts to accumulate. In this section, we will uncover the noise components accumulated with and without the projection to the inputs and analyze their impact on expectation. We first start with the noise component without the projection step $\\eta_{i}^{\\mathrm{o}}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{i}^{\\circ}=\\frac{1}{d}\\cdot H(\\frac{1}{H y_{i}}\\odot\\sum_{j=1\\atop j\\neq i}^{\\rho}(H x_{j}\\odot H y_{j})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let, set the value of $n$ to be 1 thus, $d\\ =\\ 2^{n}\\ =\\ 2$ and the number of vector pairs $\\rho~=~2$ , i.e., $\\chi_{\\rho=2}=\\mathcal{B}(x_{1},y_{1})+\\mathcal{B}(x_{2},y_{2})$ . We want to retrieve $x_{1}$ using the query $y_{1}$ , thereby, the expression of $\\eta_{i}^{\\mathrm{o}}$ is uncovered step by step for $\\rho=2$ shown in Equation 13. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{\\mathfrak{p}=}^{0}=\\frac{1}{d}\\cdot H(\\frac{1}{H{\\eta_{\\mathfrak{p}}}_{1}}\\odot(H x_{2}\\odot H y_{2}))}\\\\ &{\\ \\ \\ \\ =\\frac{1}{d}\\cdot\\sqrt{d}\\cdot H\\left(\\begin{array}{l l}{\\frac{\\bar{v}_{1}^{(0)}+\\bar{v}_{1}^{(1)}}{\\bar{y}_{1}^{(0)}+\\bar{y}_{1}^{(1)}}\\odot}&{(x_{2}^{(0)}+x_{2}^{(1)})\\cdot(y_{2}^{(0)}+y_{2}^{(1)})}\\\\ {\\frac{\\bar{v}_{1}^{(0)}+\\bar{v}_{1}^{(1)}}{\\bar{y}_{1}^{(0)}-\\bar{y}_{1}^{(1)}}\\odot}&{(x_{2}^{(0)}-x_{2}^{(1)})\\cdot(y_{2}^{(0)}-y_{2}^{(1)})}\\end{array}\\right)}\\\\ &{\\ \\ \\ \\ =\\frac{1}{d}\\cdot d\\cdot\\left(\\begin{array}{l l}{\\frac{(x_{2}^{(0)}+x_{2}^{(1)})(y_{2}^{(0)}+y_{2}^{(1)})(y_{1}^{(0)}-\\bar{y}_{1}^{(1)})+(y_{2}^{(0)}-x_{1}^{(1)})(y_{2}^{(0)}-y_{2}^{(1)})(y_{1}^{(0)}+y_{1}^{(1)})}{(x_{2}^{(0)}+y_{2}^{(1)})(y_{1}^{(0)}+y_{1}^{(1)})(y_{1}^{(0)}-y_{1}^{(1)})}}\\\\ {\\frac{(x_{2}^{(0)}+x_{2}^{(1)})(y_{2}^{(0)}+y_{2}^{(1)})(y_{1}^{(0)}-y_{1}^{(1)})-(x_{2}^{(0)}-x_{2}^{(1)})(y_{2}^{(0)}-y_{2}^{(1)})(y_{1}^{(0)}+y_{1}^{(1)})}{(y_{1}^{(0)}+y_\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, $\\varphi_{k}\\stackrel{}{\\forall}k\\in\\mathbb{N}:1\\leqslant k\\leqslant d$ are the polynomials comprises of $(x_{2},\\,y_{2})$ , and the query vector $y_{1}$ $\\mathcal{P}$ is the vector of polynomials consisting of $\\varphi_{k}$ . From the noise expression, we can observe that the numerator is a polynomial and the denominator is the product of all the elements of the Hadamard transformation of the query vector. This is true for any value of $n$ and $\\rho$ . Thus, in general, for any query $y_{i}$ we can express $\\eta_{i}^{\\mathrm{o}}$ as shown in Equation 14. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{i}^{\\circ}=\\frac{\\overset{\\rho}{\\mathcal{P}}_{j\\neq i}(x_{j},y_{j},y_{i})}{\\prod_{k=1}^{d}(H y_{i})_{k}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The noise accumulated after applying the projection to the inputs is quite straightforward as given in Equation 15. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{i}^{\\pi}=\\frac{\\displaystyle\\sum_{j=1,\\ j\\neq i}^{\\rho}(x_{j}\\odot y_{j})}{y_{i}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Although the vectors $x_{i},y_{i}\\ \\forall\\ i\\in\\mathbb{N}:1\\leqslant i\\leqslant\\rho$ are sampled from a MiND with an expected value of 0 given in Equation 6, the sample mean of $x_{i}$ or $y_{i}$ would be $\\hat{\\mu}\\approx0$ but $\\hat{\\mu}\\neq0$ . Both the numerator of $\\eta_{i}^{\\mathrm{o}}$ and $\\eta_{i}^{\\pi}$ are the polynomials thus the expected value would be very close to 0. However, the expected value of the denominator of $\\eta_{i}^{\\mathrm{o}}$ would be $\\begin{array}{r}{\\operatorname{E}[\\prod_{k=1}^{d}(H y_{i})_{k}]\\,=\\,\\prod_{k=1}^{d}\\operatorname{E}[(H y_{i})_{k}]\\,=\\,{\\hat{\\mu}}^{d}}\\end{array}$ whereas the expected value of the denominator of $\\eta_{i}^{\\pi}$ i\u015bs $\\operatorname{E}[y_{i}]\\;=\\;{\\hat{\\mu}}$ . S in\u015bce, $\\hat{\\mu}^{d}\\,<\\,\\hat{\\mu}$ , hence, in expectation $\\eta_{i}^{\\pi}<\\eta_{i}^{\\mathrm{o}}$ . This is also verified by an empirical study where $n$ , i.e., the dimension $d=2^{n}$ is varied along with the no. of bound vector pairs $\\rho$ and the amount of absolute mean noise in retrieval is estimated. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Figure 4 shows the heatmap visualization of the noise for both $\\eta_{i}^{\\mathrm{o}}$ and $\\eta_{i}^{\\pi}$ in natural log scale. The amount of noise accumulated without any projection to the inputs is much higher compared to the noise accumulation with the projection. For varying $n$ and $\\rho$ , the maximum amount of noise accumulated when projection is applied is 7.18 and without any projection, the maximum amount of noise is 19.38. Also, most of the heatmap of $\\eta_{i}^{\\pi}$ remains in the blue region whereas as $n$ and $\\rho$ increase, the heatmap of $\\eta_{i}^{\\mathrm{o}}$ moves towards the red region. Therefore, it is evident that the projection to the inputs diminishes the amount of accumulated noise with the retrieved output. ", "page_idx": 14}, {"type": "image", "img_path": "p3hNrpeWMe/tmp/69ec38caaee0cd7dc7143e45ef8c22b17b1ec92fe993a51894971d8b4e9f6493.jpg", "img_caption": ["Figure 4: Heatmap of the empirical comparison of the noise components $\\eta_{i}^{\\mathrm{o}}$ and $\\eta_{i}^{\\pi}$ for varying $n$ and $\\rho$ shown in natural logarithm scale. The dimension, i.e., $d\\,=\\,2^{n}$ is varied from 2 to 1024 $(n\\in\\{1,2,\\cdots\\,,10\\})$ and the number of vector pairs bundled is varied from 2 to 50. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Norm Relation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem B.1 $(\\chi_{\\rho}-\\rho$ Relationship). Given $x_{i},y_{i}\\sim\\Omega(\\mu,1/d)\\in\\mathbb{R}^{d}\\;\\forall\\;i\\in\\mathbb{N}:1\\leqslant i\\leqslant\\rho,$ , the? norm of the composite representation $\\chi_{\\rho}$ is proportional to $\\sqrt{\\rho}$ and approximately equal to the $\\mu^{2}{\\sqrt{\\rho\\cdot d}}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem B.1. Given $\\chi_{\\rho}$ is the composite representation of the bound vectors, i.e., the summation of $\\rho$ no. of individual bound terms. First, let\u2019s compute the norm of the single bound term as shown in Equation 16. ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\|B(x_{i},y_{i})\\|_{2}=\\|x_{i}\\cdot y_{i}\\|_{2}}&{}\\\\ {={\\sqrt{(x_{i}^{(1)}y_{i}^{(1)})^{2}+(x_{i}^{(2)}y_{i}^{(2)})^{2}+\\dots+(x_{i}^{(d)}y_{i}^{(d)})^{2}}}}\\\\ {={\\sqrt{(\\pm\\mu^{2})^{2}+(\\pm\\mu^{2})^{2}+\\dots+(\\pm\\mu^{2})^{2}}}}&{\\left[E[x^{(1)}]\\cdot E[y^{(1)}]=\\pm\\mu\\cdot\\pm\\mu=\\pm\\mu^{2}\\right]}\\\\ {={\\sqrt{\\mu^{4}d}}}&{}\\end{array}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, let\u2019s expand and compute the square norm of the composite representation given in Equation 17. ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r l}&{\\left\\|\\chi_{\\rho}\\right\\|_{2}^{2}=\\big\\|B(x_{1},y_{1})+\\mathcal{B}(x_{2},y_{2})+\\cdot\\cdot\\cdot+\\mathcal{B}(x_{\\rho},y_{\\rho})\\big\\|_{2}^{2}}\\\\ &{\\qquad\\quad=\\|B(x_{1},y_{1})\\|_{2}^{2}+\\|B(x_{2},y_{2})\\|_{2}^{2}+\\cdot\\cdot\\cdot+\\|B(x_{\\rho},y_{\\rho})\\|_{2}^{2}\\ +\\ \\xi}\\end{array}$ where $\\xi$ is the rest of the terms of square \u203aexpansion. ${\\begin{array}{r l}&{=\\mu^{4}d+\\mu^{4}d+\\cdots+\\mu^{4}d+\\xi}\\\\ &{=\\rho\\cdot\\mu^{4}d+\\xi}\\\\ {\\left\\|\\chi_{\\rho}\\right\\|_{2}={\\sqrt{\\rho\\cdot\\mu^{4}d+\\xi}}}\\\\ &{\\approx{\\sqrt{\\rho\\cdot\\mu^{4}d}}\\quad{\\textrm{[}}\\xi{\\textrm{i s}}{\\textrm{t h e}}{\\textrm{n o i s e}}{\\textrm{t e r m}}{\\textrm{a n d}}{\\textrm{d i s c a r d e d}}{\\textrm{t o}}{\\textrm{m a k e}}{\\textrm{a n}}{\\mathrm{approximation]}}}\\\\ &{=\\mu^{2}{\\sqrt{\\rho\\cdot d}}\\quad{\\boxed{\\xi}}}\\end{array}}$ ", "page_idx": 15}, {"type": "text", "text": "Thus, given the composite representation and the mean of the MiND distribution, we can estimate the no. of bound terms bundled together by $\\rho\\approx\\left\\|\\chi_{\\rho}\\right\\|_{2}^{2}/\\mu^{4}d$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Figure 5 shows the comparison between the theoretical relationship and actual experimental results where the norm of the composite representation is computed for $\\mu=0.5$ and $\\bar{\\rho}=\\{1,2,\\cdots\\,,200\\}$ . The figure indicates that the theoretical relationship aligns with the experimental results. However, as the number of bundled pair increases, the variation in the norm increases. This is because of making the approximation by discarding $\\xi$ in Equation 17. ", "page_idx": 15}, {"type": "image", "img_path": "p3hNrpeWMe/tmp/599931427a7685c12db178428933e128dd4a2772588e31a98db591dee48fa75e.jpg", "img_caption": ["Figure 5: Comparison between the theoretical and experimental relationship of Theorem B.1. The norm of the composite representation of the bound vectors is computed for no. of bundled vectors from 1 to 200 of dimension $d=1024$ . The figure shows how the experimental value of the norm closely follows the theoretical relation between $\\left\\|\\chi_{\\rho}\\right\\|_{2}$ and $\\rho$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Cosine Relation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 3.2 shows how the cosine similarity $\\phi$ between the original $x_{i}$ and retrieved vector $\\boldsymbol{\\hat{x_{i}}}$ is approximately equal to the inverse square root of the number of vector pairs in a composite representation $\\rho$ . In this section, we will perform an empirical analysis of the theorem and compare it with the theoretical results. For $\\rho\\,=\\,\\{1,2,\\cdot\\cdot\\cdot\\,,50\\}$ , similarity score $\\phi$ is calculated for vector dimension $d=512$ . Additionally, the theoretical cosine similarity score is also calculated using the value of $\\phi$ following the theorem. Figure 6 shows the comparison between the two results where the experimental result closely follows the theoretical result. The figure also shows the standard deviation for 100 trials, indicating a minute change from the actual value. ", "page_idx": 16}, {"type": "image", "img_path": "p3hNrpeWMe/tmp/ddb685457894f74382767757afe4915f5ee47ea9505bf68428828be37332b5f2.jpg", "img_caption": ["Figure 6: Comparison between the theoretical and experimental $\\phi-\\rho$ relationship. Vectors of dimension $d=512$ are combined and retrieved with a varied number of vectors from 1 to 50. The zoom portion shows how closely experimental results match with the theoretical conclusion. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper claims to present a new Hadamard-derived linear vector symbolic architecture in the abstract and introduction which accurately reflects the contribution and scope of the paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Limitations are described in the paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All the theoretical results and proofs are provided in the Methodology section. (See section 3) ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The experimental setup is based on previous papers. All the self-used parameters are discussed in the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All the code is provided with the supplemental material. Data is publicly available and instruction is provided on how to get the data. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Experimental details and their sources are cited in the Empirical results section. (see section 4) ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See Figure 2, Figure 3 ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See section 4. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We followed the NeurIPS code of conduct. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The work has no negative societal impacts. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No high-risk data or models are used. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Original paper and code are cited in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Documentation of the code is provided. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No Crowdsourcing and Research with Human Subjects are used. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No participants are used. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]