[{"figure_path": "tBRNC6YemY/tables/tables_1_1.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents a comparison of various Large Language Models (LLMs) on three different API datasets: Torch Hub, HuggingFace, and TensorFlow Hub.  The results are categorized by LLM, retrieval method (0-shot, BM25, GPT-Index, Oracle), and API dataset.  For each combination, the overall accuracy, hallucination rate, and error rate are shown, providing a comprehensive performance comparison across different LLMs and retrieval strategies.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_7_1.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents a comprehensive evaluation of various Large Language Models (LLMs) on three different API hubs: Torch Hub, HuggingFace, and TensorFlow Hub.  The evaluation considers zero-shot performance and performance when using BM25 and GPT-Index retrievers.  Metrics include overall accuracy, hallucination rate (incorrect API calls due to model generating non-existent APIs), and error rate (incorrect API calls despite using a real API).  The table allows for a comparison of different LLMs and retrieval strategies across various API collections.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_7_2.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents the performance of various LLMs (LLaMA, GPT-3.5, GPT-4, Claude, and Gorilla) on three different API hubs (Torch Hub, HuggingFace, and TensorFlow Hub).  It compares their performance across various retrieval settings (zero-shot, BM25, GPT-Index, and Oracle). The metrics used are overall accuracy, hallucination error rate, and the error rate due to selecting the wrong API call.  The table highlights the differences in performance between various models and retrieval strategies.  Gorilla's superior performance compared to other models, especially in zero-shot scenarios, is a key finding.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_8_1.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents the performance of various LLMs (Large Language Models) on three different API hubs: Torch Hub, HuggingFace, and TensorFlow Hub.  The models are evaluated under various conditions (zero-shot, with BM25 retriever, with GPT retriever, and with an oracle retriever).  The results are presented as overall accuracy, hallucination rate (incorrect APIs generated), and error rate (correct API but with incorrect arguments).  The table allows for comparison of the accuracy and reliability of different LLMs in generating API calls across various hubs and retrieval scenarios.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_8_2.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents the performance comparison of various Large Language Models (LLMs) on three different API hubs: Torch Hub, HuggingFace, and TensorFlow Hub.  The LLMs are evaluated under different conditions, including zero-shot (no retriever), and using BM25 and GPT-Index retrievers.  The metrics used are overall accuracy, hallucination rate (hallu), and error rate (err), which represent the proportion of correctly generated API calls, hallucinated API calls, and API calls with errors, respectively.  Additionally, the table shows the accuracy on API calls with constraints (Accuracy const). The results demonstrate the effectiveness of Gorilla, a newly developed LLM, in comparison to existing LLMs.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_8_3.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents the results of evaluating various Large Language Models (LLMs) on three different API hubs: Torch Hub, HuggingFace, and TensorFlow Hub.  The evaluation metrics include overall accuracy, hallucination rate (incorrect API calls due to model generation errors), and error rate (incorrect API calls due to selecting the wrong API). The table shows the performance of each LLM in a zero-shot setting and with three different retriever settings: BM25, GPT-Index, and Oracle.  The results are useful to compare the performance of different LLMs in generating accurate API calls across various settings and hubs.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_9_1.jpg", "caption": "Table 5: Evaluating Gorilla 0-shot with GPT 3-shot incontext examples", "description": "This table compares the performance of Gorilla (zero-shot) against GPT-3.5 and GPT-4 (with three in-context examples) across three API benchmark subsets (HuggingFace, TorchHub, and TensorFlow Hub).  The metrics used are accuracy (Acc\u2191) and hallucination rate (Hall\u2193). Higher accuracy and lower hallucination are better.  The results demonstrate Gorilla's superior performance even without in-context examples.", "section": "4.4 Finetuning (vs) Prompting: Gorilla 0-shot (vs) GPT 3-shot"}, {"figure_path": "tBRNC6YemY/tables/tables_14_1.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents the performance comparison of various LLMs (LLaMA, GPT-3.5, GPT-4, Claude, and Gorilla) across three different API hubs (Torch Hub, HuggingFace, and TensorFlow Hub).  The evaluation metrics include overall accuracy, hallucination rate, and error rate. The results are shown for different retriever settings (zero-shot, BM25 retriever, GPT-Index retriever, and Oracle retriever).  The table allows for a comprehensive comparison of different models and strategies in using LLMs with APIs.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_16_1.jpg", "caption": "Table 1: Evaluating LLMs on Torch Hub, HuggingFace, and Tensorflow Hub APIs", "description": "This table presents the evaluation results of various Large Language Models (LLMs) on three different API hubs: Torch Hub, HuggingFace, and TensorFlow Hub.  It compares the performance of these LLMs in several settings: zero-shot (no retriever), and with BM25 and GPT retrievers, as well as an oracle retriever.  The metrics used are overall accuracy, hallucination rate (percentage of incorrect API calls due to hallucinations), and error rate (percentage of incorrect API calls due to other reasons). The table allows for a comparison of LLMs across different API datasets and retrieval strategies, highlighting the impact of retrieval methods on performance. ", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/tables/tables_17_1.jpg", "caption": "Table 7: Evaluating Gorilla (vs) DocPrompting Gorilla improves accuracy, while lowering the hallucination.", "description": "This table compares the performance of Gorilla and DocPrompting on a specific task.  It shows that Gorilla achieves higher accuracy while significantly reducing hallucination compared to DocPrompting.  The metrics presented are Accuracy (higher is better) and Hallucination (lower is better).  The improvements demonstrate the effectiveness of Gorilla's approach.", "section": "A.3.4 Gorilla (VS) DocPrompting"}]