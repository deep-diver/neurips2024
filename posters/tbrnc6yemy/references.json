{"references": [{"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is foundational for the field of large language models (LLMs), introducing the concept of few-shot learning, which is central to many recent LLM advancements and is directly relevant to the capabilities of Gorilla."}, {"fullname_first_author": "Chowdhery, A.", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2022-00-00", "reason": "This paper details the development of Pathways Language Model (PaLM), a large-scale language model that significantly advanced the state-of-the-art and whose architecture and training methodology inform many contemporary LLMs including the one used in Gorilla."}, {"fullname_first_author": "Touvron, H.", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-00-00", "reason": "This paper introduces LLaMA, the open-source large language model that serves as the base for Gorilla, making it a critical component of the work described."}, {"fullname_first_author": "Schick, T.", "paper_title": "Toolformer: Language models can teach themselves to use tools", "publication_date": "2023-00-00", "reason": "This work is highly relevant due to its focus on enabling LLMs to use tools, a key theme that Gorilla also addresses and expands upon, specifically in terms of scale and systematic approach."}, {"fullname_first_author": "Wang, Y.", "paper_title": "Self-instruct: Aligning language model with self generated instructions", "publication_date": "2022-00-00", "reason": "This paper introduces the self-instruct method for training LLMs, which is used in Gorilla's training process, thereby contributing directly to the methodology and performance of the model."}]}