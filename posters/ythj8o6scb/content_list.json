[{"type": "text", "text": "SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chenyang Ma Kai Lu Ta-Ying Cheng Niki Trigoni Andrew Markham University of Oxford chenyang.ma@cs.ox.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current state-of-the-art spatial reasoning-enhanced VLMs are trained to excel at spatial visual question answering (VQA). However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets. In this work, we present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner. Extensive experiments demonstrate that our spatial reasoning-imbued VLM performs well on various forms of spatial VQA and can extend to help in various downstream robotics tasks such as pick and stack and trajectory planning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Equipping vision-language models (VLMs) the capacities of spatial reasoning unlocks exciting applications, such as general-purpose reward annotation [52], robotic data generation [61], and grounding 3D object affordances [26, 38]. However, the spatial reasoning capabilities of VLMs on fine-grained spatial understanding tasks are somewhat limited. Current state-of-the-art (SOTA) spatial reasoning-enhanced VLM [12] is mostly tested on spatial visual question answering (VQA), such as determining objects\u2019 relative positions and orientations; experiments on higher-level tasks, such as scene comparisons and trajectory planning, which require more nuanced comprehension, are underexplored. ", "page_idx": 0}, {"type": "text", "text": "Many works enhance the spatial reasoning capabilities of VLMs by training/fine-tuning them on standard spatial VQA datasets [12]. As a result, VLMs primarily learn surface-level associations between image-text-data triplets. Given the scarcity and difficulty of obtaining spatially rich embodied data or high-quality human annotations for 3D-aware queries, we hypothesize that these VLMs may not generalize to questions outside their dataset distribution or adapt to more challenging tasks that require an advanced level of spatial understanding. ", "page_idx": 0}, {"type": "text", "text": "Recent studies [73, 7, 65, 69] in image space understanding show that VLMs, equipped with internetscale language knowledge, and multimodal foundation models capture complementary knowledge that can be combined to conduct new tasks spanning both modalities without additional training. Given the recent advancements in 3D foundation models [4, 41, 29], this work explores whether there exists an alternative approach to enhance VLMs with higher-level spatial-awareness by incorporating 3D priors from these models. ", "page_idx": 0}, {"type": "text", "text": "To this end, we propose SpatialPIN, a framework that utilizes progressive prompting and interactions between VLMs and 2D/3D foundation models as \u201cfree lunch\u201d to enhance spatial reasoning capabilities in a zero-shot, training-free manner. By using these foundation models to decompose, comprehend, and reconstruct an explicit 3D representation, SpatialPIN grasps the core understanding of the 3D space presented by the 2D image. This allows generalizations to various 3D-aware tasks, from VQAs to 3D trajectory planning. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We provide an extensive empirical study combining multiple off-the-shelf and handcrafted datasets, ranging from fundamental spatial questions regarding relative positions and orientations to providing fine-grained 3D information on objects\u2019 locations, sizes, inclinations, and dynamic changes, and plan for robotics tasks with full 3D trajectories. Results show that this straightforward approach significantly outperforms SOTA VLMs trained from extensive spatial VQAs (see SpatialPIN examples in Figure 1), consolidating our belief that a truly 3D-aware VLM can actually be imbued by simply injecting explicit, fundamental knowledge of the 3D scene. With the entire framework being fully modularized, each component can be easily replaced with the latest improvements within its specific domain. ", "page_idx": 1}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/59aca0f54dc2efa5dbabcdf32a51c9f2d5db2ad4792fc1f457cc42480c9e41d8.jpg", "img_caption": ["Figure 1: We present SpatialPIN, a framework to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with 3D priors in a zero-shot, training-free manner. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We investigate the problem of equipping VLMs with 3D reasoning capabilities without fine-tuning on large spatial VQA datasets. \u2022 We propose SpatialPIN, a modular plug-and-play framework that progressively enhances VLM\u2019s 3D reasoning capabilities by prompting and interacting with 3D foundational models. \u2022 We show that SpatialPIN unlocks 3D-aware applications including spatial VQA and both classic and novel robotics tasks, supported by extensive experiments. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "VLM Grounding With the recent birth of powerful LLMs and VLMs [8, 40, 2], the task of VLM grounding, or combining generative language models with real-world data to adapt to specific cases, has gained significant popularity. Several recent works focused on fine-tuning these LLMs for a wide range of downstream applications, such as interactive decision making [34], multi-task agents [62], or even tasks in interactive environments [67, 11]. A close work to ours is Socratic Model [73], a framework of combining multiple foundation models to unleash LLMs in downstream tasks. However, this work still focuses on tasks in 2D pixel space understanding of images. There remains many challenges in the 3D world to combine information for full scene understanding, which we hope to tackle in our paper. ", "page_idx": 1}, {"type": "text", "text": "VLM Spatial Understanding Many VLMs encompass the ability of image-space reasoning and understanding [13, 33, 40]. There are even efforts in incorporating these understandings into image space manipulations and editting [7, 65]. However, the current ability of VLMs to fully understand a 3D scene and the potential interactions within this scene is still rather limited. Several works build from this foundation and establish datasets to help with spatial reasoning/understanding [30, 39, 45]. Recently, SpatialVLM [12] proposed fine-tuning a VLM on 3D-VQA datasets to enhance the precision of VLMs on 3D understanding tasks. Nevertheless, using a 3D-VQA dataset only provides a partial picture to the complete 3D understanding of an image, and could lead to suboptimal performances under out-of-distribution tasks. In this work, we hope to introduce holistic 3D information from multiple 3D foundation models via prompting and interactions as a way to enhance VLMs with a comprehensive 3D understanding given RGB inputs. ", "page_idx": 1}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given an RGB image $I\\in\\mathbb{R}^{H\\times W\\times3}$ of a scene with $K$ unknown objects and a spatial task $Q$ , our goal is to inspire VLMs with spatial reasoning capabilities and solve $Q$ with fine-grained 3D understanding. ", "page_idx": 1}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/6a60780bd6a1bfb2c9227c1d275b525aa836ff63120c560de55eb0d75e60830f.jpg", "img_caption": ["Figure 2: SpatialPIN. Our plug-and-play framework is fully modularized and designed for zero-shot deployment. Each module can be easily replaced with the latest updates. Exact prompts for VLMs are in Appendix. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "To prevent the models from overftiting to the standard problems from spatial VQA datasets [12], we hope to derive a method that utilizes fundamental 3D foundation models to provide explicit scene understandings, then leverage the generalization capabilities of VLMs to tackle unforeseen tasks\u2014all within a zero-shot, training-free manner. ", "page_idx": 2}, {"type": "text", "text": "Our modular pipeline, SpatialPIN, enhances VLMs\u2019 spatial understanding of an image through progressive interactions with the scene decomposition, comprehension, and reconstruction processes with prompting. For image scene understanding (Sec. 3.1), we use VLM to describe objects by appearance and 2D location, complemented by language-guided segmentation and repainting models to obtain occlusion-free object masks. Elevating 2D understanding to coarse 3D (Sec. 3.2), we use metric depth estimation and perspective fields to estimate the 3D scene size and conduct perspective canonicalization with VLM. For fine-grained 3D understanding (Sec. 3.3), we partially reconstruct the 3D scene, with the full 3D representation of foreground objects and the background as a plane. With the reconstructed 3D scene, we summarize spatial information and prompt it to the VLM for various downstream tasks. ", "page_idx": 2}, {"type": "text", "text": "3.1 2D Image Scene Understanding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Prompting: Objects Understanding by Constraining We start with querying VLM to identify and understand objects given $I$ . We explicitly ask VLM to describe the objects by precise color, texture, and 2D spatial locations. This step is vital for two reasons: 1) enhance VLM\u2019s understanding of the objects, 2) differentiate between items of similar or identical categories and appearances. ", "page_idx": 2}, {"type": "text", "text": "As a concrete example, given the left image of Fig. 2, VLM outputs: \u201c object 0: laptop of color rose gold, texture metallic at location left-center. object 1: camera of color black, texture smooth at location center-right. ...\u201d ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2D Representations Refinement The concise descriptions of identified objects are used as input text prompts for a language-guided segmentation model, enabling the acquisition of $K$ segmentation masks $\\{\\hat{M}_{k}^{o c c}\\}_{k=1}^{K}$ , with each mask corresponding to a unique object. ", "page_idx": 2}, {"type": "text", "text": "However, an object $i\\in[1,K]$ may be occluded by other object(s), leading to an incomplete mask $M_{i}^{o c c}$ , which may be burdensome when we elevate the image to a 3D representation in the later stage. To resolve this, we create an inpainting mask, $M_{i}^{i n p}$ , for each object, in which all objects except the one itself are removed and replaced with white pixels. The inpainted masks are again fed to the language-guided segmentation model along with input text prompts such that occlusion-free object masks, $\\{M_{k}^{o f}\\}_{k=1}^{K}$ , are obtained. This two-step segmentation process for object $i$ is formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nM_{i}^{o c c}={\\bf s e g}(I,\\tau_{i}),\\quad M_{i}^{o f}={\\bf s e g}(\\mathrm{inpaint}(M_{i}^{i n p}),\\tau_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where seg denotes the language-guided segmentation and $\\tau_{i}$ denotes the description of object $i$ . In practice, to cleanly remove objects without residual fragments for inpainting, we apply dilation to and expand the white areas. Inpainted background $I_{b g}$ is acquired by removing and replacing all objects with dilated white pixels. ", "page_idx": 2}, {"type": "text", "text": "3.2 Coarse 3D Scene Understanding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Scene Size Estimation Using the estimated metric depth [29] and estimated camera intrinsic matrix by finding field of view (FOV) through perspective fields [29], we backproject to determine the dimensions of the 3D spatial scene. ", "page_idx": 3}, {"type": "text", "text": "Prompting: Perspective Canonicalization 3D information without any knowledge regarding the camera perspectives lead to ambiguities [12]. Consider a question \u201cWhat is the orientation of the bowl relative to the laptop?\u201d with the input scene in Fig. 2, but taken from a top-down perspective. VLMs may output \u201cdownward to the left\u201d, but the correct answer should be \"front-left\" because humans perceive orientation from a horizontal angle. To address this, we provide the VLM with $I$ estimated scene size, and maximum and minimum dimensions, allowing it to reason about the camera shot angle (horizontal/top-down/bottom-up). Scene size information helps differentiate shot angles by providing clues about spatial layout and object proportions. For instance, if the depth variation is small, the VLM can infer a top-down or bottom-up angle along with visual cues. ", "page_idx": 3}, {"type": "text", "text": "As a concrete example, given the left image of Fig. 2, VLM outputs: ", "page_idx": 3}, {"type": "text", "text": "\u201cVisual cues reasoning: Objects are viewed from the side, indicating the camera is positioned horizontally with a slight elevation.   \nSpatial data reasoning: The depth varies significantly from 57.50 cm to $115\\cdot00\\;\\;\\mathrm{cm}$ , indicating the camera captures the scene across different distances, supporting a horizontal perspective.   \nConclusion: horizontal.\u201d ", "page_idx": 3}, {"type": "text", "text": "3.3 Fine-Grained 3D Scene Understanding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We partially reconstruct the 3D scene with full representation of foreground objects while simplifying the inpainted background as a plane, as shown in Fig. 3(a). We summarize spatial information from the reconstructed scene and prompt it to the VLM. Please see our Appendix for implementation details about reconstruction. ", "page_idx": 3}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/d7d92de9b11398002e48ff793ec68816c43fbc4fe25314232809f06a2b368d8b.jpg", "img_caption": ["Figure 3: Our method of partial 3D scene reconstruction (a). The reconstructed scene (b) and the input image (c) show high alignment. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Scene Initialization Given the occlusion-free \uff08?)   \nobject masks, M kof }kK=1, we use single-view 3D reconstruction model [41] to acquire object 3D models, $\\{O_{k}\\}_{k=1}^{K}$ , with canonical poses determined during reconstruction. Pinhole camera is set at the origin, looking at positive depth-axis. With the estimated background plane size (Sec. 3.2), we move the background plane, $O_{b g}$ (visually identical to $I_{b g}$ ), along the depth-axis to fit precisely within the camera. ", "page_idx": 3}, {"type": "text", "text": "Scene Reconstruction To resolve the imprecision of backprojection, our goal is to position object 3D models into the reconstructed 3D scene without visual discrepancies and ensure accurate depth. Instead of using naive backprojection, for an object $i\\in[1,K]$ , we perform raycasting from object 3D center $t_{i}^{c}$ on the camera plane to object 3D center $t_{i}^{b g}$ on the background plane with metric depth $d_{i}$ . The 3D coordinate $t_{i}$ of object $i$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{i}=\\left|I_{d e p}(\\operatorname{center}(M_{i}^{o f})\\right|,\\quad t_{i}=t_{c}+\\frac{d_{i}}{\\left|t_{i}^{b g}-t_{i}^{c}\\right|}\\times(t_{i}^{b g}-t_{c}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The rotation $R_{i}$ of the 6D pose of object $i$ , $P_{i}=\\left[R_{i}\\mid t_{i}\\right]$ is explained previously. After integrating all 3D object models into the 3D scene, we refine each object\u2019s scale to accurately reflect depth variations by rendering binary masks and evaluate the length of their contour lines relative to their occlusion-free masks, through the lens of the pinhole camera, $t_{c}$ . ", "page_idx": 3}, {"type": "text", "text": "We determine the principal axes $\\textbf{\\em x}$ -axis, y-axis, and $\\mathbf{Z}$ -axis) of each object using the minimal oriented bounding box (OBB), which is essential for unlock novel applications. ", "page_idx": 3}, {"type": "text", "text": "Prompting: Objects and Spatial Context Understanding The reconstructed 3D scene from $I$ with accurate object poses and scales is denoted as $V_{0}$ . As the final step of progressive prompting, we feed VLM the fine-grained 3D information derived from $V_{0}$ , grounding on the canonicalized perspective (Sec. 3.2). For example, with the input image in Fig. 2 and a horizontal camera shot angle, depth corresponds to the positive y-axis (similarly, in a top-down/bottom-up view, depth is the negative/positive z-axis) in a right-handed coordinate system. The width and height axes can be determined accordingly, aligning each axis\u2019s orientation with human perception. ", "page_idx": 3}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/2f95778a95426bc1eb791af33c86b0ec1494339d1dd1d98ec747a603ffcd597a.jpg", "img_caption": ["Figure 4: Qualitative examples of spatial VQA. SpatialPIN outputs answers with fine-grained 3D reasoning. Zoom in for better view. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We feed VLM a paragraph describing the objects\u2019 poses, sizes, and principal axes in physical units, alongside their spatial relationships. To augment VLM\u2019s understanding, we also feed $V_{0}$ with visualized object axes (see Fig. 2B). Visualizing 3D spatial information is pivotal in improving VLMs\u2019 understanding of 3D spatial contexts derived from 2D images, validated by 3DAxiesPrompts [37]. Yet, we want to emphasize that we do not feed hardcoded information, such as objects\u2019 relative distances and inclinations, to VLMs. Instead, we aim for the summarized 3D information to enhance VLMs\u2019 general spatial understanding. ", "page_idx": 4}, {"type": "text", "text": "As a concrete example for the left image on Fig. 2:   \n\u201cObj 1 spatial context: 3D center: [7.0, 100.0, 9.0] cm; X-axis (right): [0.9529, -0.2456, 0.1779]; Y-axis (back): [-0.3528, 0.8746, 0.3327]; Z-axis (up): [-0.1761, -0.3285, 0.9279]   \nObj 1 size: 13.54 cm x 9.37 cm x 9.50 cm (WxDxH)   \nObj 1 closest per direction: left: Obj 0; right: Obj 2 ...\u201d ", "page_idx": 4}, {"type": "text", "text": "3.4 Combining External Tools for Downstream Tasks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "By partially reconstructing the 3D scene with visual alignments, our framework enables VLMs to use tools like rapidly-exploring random tree star (RRT\\*) [31] to generate accurate, collision-free paths based on task specifications (more details in Appendix). This capability unlocks novel and interesting applications when combined with task-specific prompting techniques, shown in Experiments (Sec.4). ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We conduct experiments to answer the following questions: 1) Does our framework enhance the general spatial reasoning capabilities of VLMs, and how well does it perform? 2) What novel applications does our framework unlock for VLMs, and how well do we perform in these applications? 3) How effective is each module in our framework? ", "page_idx": 4}, {"type": "text", "text": "Since we evaluate our approach on a wide range of tasks to test VLMs\u2019 higher-level spatial awareness, some tasks are novel and lack existing/open source datasets. Therefore, for all our experiments, we use a combination of 4 existing datasets and 2 hand-crafted datasets. ", "page_idx": 4}, {"type": "text", "text": "Implementations The language-guided segmentation model is Language Segment-Anything [44] and the repainting model is LaMa [55]. We use One-2- $3{-}45{+}+$ [41] for single-view 3D reconstruction, perspective fields [29] for camera intrinsic estimation, and ZoeDepth [4] for depth estimation. For partial 3D scene reconstruction, we use Blender [17] as the 3D software. All inference is run on 1 NVIDIA A10 GPU with 24GB RAM. ", "page_idx": 4}, {"type": "text", "text": "4.1 Spatial Visual Question Answering ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We experiment on the basic form of spatial VQA introduced by SpatialVLM (IaOR-VQA), and two new forms introducted by us (IaAD- & IrSD-VQA). For IaOR-VQA, please check SpatialVLM [12] for details. For IaAD- $\\&$ IrSD-VQA, please see our Appendix. ", "page_idx": 4}, {"type": "text", "text": "Intra-Image Object Relations VQA (IaOR-VQA) As the basic form of spatial VQA, it involves spatial reasoning about object relative orientations and sizes. This is divided into qualitative (e.g., \u201cis [A] in front of [B]\u201d, \u201cis [A] smaller than [B]\u201d) and quantitative (e.g., \u201chow far apart are [A] and [B]\u201d, \u201cmeasure the width of [A]\u201d) questions. ", "page_idx": 5}, {"type": "text", "text": "We follow the evaluation method of SpatialVLM [12]. Since SpatialVLM did not release their evaluation dataset, we reproduce one using RGBD images from NOCS [57] (object dataset), RT-1 [6], and BridgeData V2 [56] (robotics manipulation datasets). We sample 13, 20, and 20 distinct scenes from each. We generate QA pairs using the SpatialVLM data generation pipeline [51], followed by manual refinement. We check correctness for qualitative questions and calculate distances for quantitative questions. We annotate 300 qualitative and 200 quantitative spatial VQA pairs (SpatialVLM has 331 and 215 for each). ", "page_idx": 5}, {"type": "text", "text": "Intra-Image Angular Discrepancies VQA (IaAD-VQA) We propose a new form of Spatial VQA that needs spatial reasoning about objects\u2019 inclinations. It includes qualitative (e.g., \u201cis [A] tilted\u201d, \u201cis [A] more tilted than [B]\u201d) and quantitative questions (e.g., \u201chow many degrees is [A] tilted vertically\u201d, \u201cmeasure the angle between [A] and [B]\u201d). ", "page_idx": 5}, {"type": "text", "text": "Since this form of Spatial VQA involves out-of-plane rotations, YCBInEOAT [64] (object tracking dataset) is a suitable choice. We sample 30 scenes from it and annotate 50 questions each for qualitative and quantitative spatial VQA pairs. ", "page_idx": 5}, {"type": "text", "text": "Inter-Image Spatial Dynamics VQA (IrSD-VQA) We further propose a more challenging form of Spatial VQA. Given two images with multiple objects, the objects in the second image may move, rotate, incline, or the image may have a change in camera angle. The VLM needs to reason about these changes. Example qualitative questions include \u201cdoes [A] move, rotate, or incline\u201d, \u201cdoes [A] incline along the y-axis\u201d while quantitative questions include \u201chow far does [A] move\u201d, \u201chow many degrees does [A] rotate horizontally\u201d. ", "page_idx": 5}, {"type": "text", "text": "As it is difficult to find a dataset that meets these requirements, we craft our own. We capture 20 image pairs using an iPhone 12 Pro Max, with each image containing $1-5$ objects, and annotate 50 questions each for qualitative and quantitative spatial VQA pairs. ", "page_idx": 5}, {"type": "text", "text": "Results The results in Tables 1 and 2 on qualitative and quantitative IaOR-VQA demonstrate that providing various VLMs fine-grained 3D information enhances their spatial reasoning capacities by a large margin. Surprisingly, VLMs with math and geometry reasoning capacities (e.g., GPT-4V, GPT-4o) show substantial improvements with this information. ", "page_idx": 5}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/66b358f46e6118603ef2c39a8c7f1ca2f615c4358a9fab979cf8b0d8782705b8.jpg", "table_caption": ["Table 1: Qualitative IaOR-VQA. We exclude comparisons to PaLI [14], PaLM-E [20], and PaLM 2-E [3] as they are not open source, and include experiments with GPT-4o [1] in addition to GPT-4V [47], LLaVA-1.5 [40], and InstructBLIP [18]. We use the HF version of SpatialVLM [51]. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/3ec296f03a5da6b6e86eee0a2b05427e29f8b9814f6070f19c0b9e2dc8232cc5.jpg", "table_caption": ["Table 2: Quantitative IaOR-VQA. SpatialVLM measures the accuracy by the percentage of answers that fall within $0.5\\mathrm{x}$ to $2.0\\mathbf{x}$ of the ground truth value. We also evaluate within narrower ranges of $0.75\\mathbf{x}$ to $1.33\\mathbf{x}$ and $0.9\\mathbf{x}$ to 1.11x. \u201cOutput number\u201d means VLMs produce number in the response instead of vague descriptions. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "The results in Tables 3 and 4 demonstrate the effectiveness of our approach on both qualitative and quantitative IaOR-VQA and IrSD-VQA tasks. Notably, the performance on quantitative IaOR-VQA is suboptimal compared to quantitative IrSD-VQA, despite the latter being more challenging. We believe this is because, for quantitative IrSD-VQA, the VLM sometimes confuses the camera and world coordinate frames, comparing the object\u2019s principal axes with the world axes to reason about changes in angles. ", "page_idx": 5}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/82e661f1a8c460fc999ee9a4376092ab638c8e20b72fd2fb2e29be37239df157.jpg", "table_caption": ["Table 3: Qualitative IaAD-VQA & IrSD-VQA. Since we test SpatialPIN on one VLM backbone for our proposed spatial VQA, for fair comparison, we should use SpatialVLM backbone (PaLM 2-E [3]). However, since it is not open source, we use GPT-4o as our backbone, as it shows the most improvement with our framework. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Fig. 4 presents qualitative examples on all forms of spatial VLM. ", "page_idx": 6}, {"type": "text", "text": "4.2 Robotics Pick and Stack ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Pick and stack is a classic robotics task. Given a robot\u2019s egocentric observation of a scene with multiple objects and a task description, our pipeline uses traditional planning to solve the problem. This task demands advanced spatial reasoning, as the model must comprehend 3D locations, sizes, and physical properties of the objects (i.e., how much to grasp and how high to drop? Is the object deformable or articulated so the robotic grasper needs to grasp more firmly?). For instance, grasping and stacking a soft toy bear on a cube is significantly different from stacking a solid apple on a mug. The model reasons about grasping and stacking policies, directly outputting 3D trajectories for the robot\u2019s end effector using traditional path planning algorithm as external tool. ", "page_idx": 6}, {"type": "text", "text": "Set-Up We set up the pick-and-stack problem in the ManiSkill [22] simulator, applying real-world physics properties. Rigid and articulated objects are chosen from the YCB dataset [10] and are randomly allocated on the table within the robotic arm\u2019s reach, with observations from different perspectives. We create 50 scenes. Since robot observations from simulated scenes suffer from sim2real gap and consider that most real-world robots have depth sensors, we use ground truth camera matrix and depth. ", "page_idx": 6}, {"type": "text", "text": "We compare our method to the following baselines: 1) direct 3D information output from our framework without GPT-4o [1] reasoning about physics and object properties and 2) SpatialVLM with our RRT\\* trajectory generation module. ", "page_idx": 6}, {"type": "text", "text": "Results Table 5 shows the results, with a qualitative example demonstrated in Fig. 5. The results indicate that using precise 3D information from our framework significantly improves the success rate, and incorporating VLM reasoning further enhances performance. ", "page_idx": 6}, {"type": "text", "text": "4.3 Discovering and Planning for Robotics Tasks from a Single Image ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present a novel task that requires advanced spatial reasoning capacities of VLMs. Given a single RGB image of any scene comprising unknown environments and objects, the VLM discovers potential tasks and plans their execution with full 3D trajectories, with the motivation that it can be used for robot learning in future research. To solve this complex task and visualize the execution using our framework, we introduce: 1) a task proposal approach using VLM, 2) a novel axes-constrained 3D planning approach that enables spatial reasoning-imbued VLM to plan the object motion based on the proposed tasks by specifying waypoints. Please see Appendix for the pipeline and details. ", "page_idx": 6}, {"type": "text", "text": "Dataset We create a diverse evaluation dataset by combining self-captured photos (38) using an iPhone 12 Pro Max and scenes (13) from NOCS [57]. Our dataset covers diverse scenes (e.g., office, kitchen, bathroom), and features a rich diversity of object categories (116) and quantities (185), with each image containing $1-7$ objects and $1-3$ tasks proposed for each object (278 tasks/planned trajectories in total). The dataset\u2019s diversity is further enhanced by the variety of perspectives (e.g., frontal, top-down, side views). This deliberate choice of diverse angles, both in our own image capturing process and through the random extraction of frames from NOCS, aims to simulate a realistic and challenging array of scenes for evaluation. See Appendix for statistics and visuals. ", "page_idx": 6}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/b1997b0ceda93a95ff31b1e740f39c5163cb36339a291d810c11665e3bb4e79a.jpg", "img_caption": ["Figure 5: Qualitative examples of pick and stack (top) and task trajectory planning (bottom). SpatialPIN successfully outputs picking and stacking policies using spatial reasoning and plans 3D trajectories with geometric awareness to align with task descriptions. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/dff9d5a527bd65bdf8869cb208d46908c1f43bae40501496f35d6199c0b30f80.jpg", "table_caption": ["Table 5: Pick and stack. We classify the success rates into: 1) successfully picked, 2) successfully picked and contacted the target object but slipped/collided, and 3) successfully picked and stacked. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Qualitative Demonstration We present a qualitative example in Fig. 5. Additional examples in Appendix shows our framework\u2019s capability to produce diverse and accurate task trajectories spanning various scenes and tasks. ", "page_idx": 7}, {"type": "text", "text": "Human Evaluation: User Study We rely on human preference evaluation as one of our quantitative metrics. We ask 25 users to rate 5 translation and 5 rotation task executions in terms of task description alignment. For these complex context-dependent manipulation tasks, we instead ask users to judge 10 executions relative to human action, and to encapsulate their perception of the action in our with a single sentence. These sentence description will be used to test human understanding of our planned trajectories (please see Appendix). Note that our user study size is similar to those representative works such as ControlNet [74] and Prompt-to-Prompt [24]. Results in Table. 6. ", "page_idx": 7}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/64c9a19398135596e7c1bec5405fef5ccd34f8ee2ec877183222adfb1a6823e4.jpg", "table_caption": ["Table 6: User study. Ratings (scale 1\u22125) are averaged. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Machine Understanding We assess the interpretability of our generated task executions from a machine\u2019s perspective using SOTA video understanding model, Video-LLaVA-7B [35]. We use two approaches: binary classification and descriptive generation. For classification, we feed the model with the task descriptions generated by VLM and ask question (is the video doing. . .?). In generation, we prompt Video-LLaVA-7B to articulate its interpretation of our task executions. To quantify the correspondence between the model\u2019s perception and the tasks, we use OpenCLIP cosine similarity score [15]. ", "page_idx": 7}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/6d01b237020c661f3af980d452f5f92270796bc8605ea34f6dd41bd4a5071376.jpg", "table_caption": ["Table 8: Ablation study. For quantitative IaOR-VQA, the accuracy is measured by the answers that fall within $0.75\\mathbf{x}$ to $1.33\\mathbf{x}$ of the ground truth value. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/d7eabcb54cd76e6d60581c3a2728df2dfd3de63d78d6be516d1edcd1f362d23c.jpg", "table_caption": ["Table 7: Results for machine understanding (classification and generation) on 278 task ex"], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/1c74d371ffbfcaab47814b1d027788f973ee2f5985b97c1df6df630a68b1e389.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the effectiveness of each module in our framework on IaOR-VQA by 1) seeking alternative designs of the overall pipeline and 2) removing each component in our ablations. ", "page_idx": 8}, {"type": "text", "text": "Overall Design To demonstrate our framework\u2019s generalization across a wide range of objects, We replace our $\\ensuremath{2\\mathrm{D}}+\\ensuremath{3\\mathrm{D}}$ pipeline with: 1) SOTA mesh-free single image object pose and size estimation model, ShAPO [27], 2) SOTA mesh-based single image object pose and size estimation model, SAM-6D [36], and feeds it with the object 3D model reconstructed by One-2- $.3{-}45{+}+$ [41], and 3) the data generation backbone of SpatialVLM [12]. Since models 1) and 2) do not provide language annotations for their outputs, we first summarize the numerical outputs using our approach in Sec. 3.3. Then, GPT-4V identifies QA pairs. ", "page_idx": 8}, {"type": "text", "text": "Removing 2D Understanding Module In this case, the VLM no longer examines the objects through prompting, and only the object name is input into the language-guided segmentation model. ", "page_idx": 8}, {"type": "text", "text": "Removing 3D Understanding Modules This means there is no scene size estimation, and the VLM does not conduct perspective canonicalization. During 3D scene reconstruction, we assume the image plane width to be 1 meter. ", "page_idx": 8}, {"type": "text", "text": "To validate the fine-grained 3D scene understanding module, we replace object mask raycasting with backprojection using the object\u2019s 2D center and remove the object scale calibration. ", "page_idx": 8}, {"type": "text", "text": "To demonstrate the overall effectiveness of our 3D understanding modules, we simply backproject the input image with the estimated metric depth. ", "page_idx": 8}, {"type": "text", "text": "Results Table 8 demonstrates the effectiveness of each module in our framework. The results also highlight the limitations of using off-the-shelf SOTA mesh-free and mesh-based single-image object pose and size estimation methods as our backbone. These methods are not language-driven and may struggle to generalize to novel objects in diverse input scenes. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with 3D priors in a zero-shot, training-free manner. We see our work as a step towards equipping VLMs with more generalized spatial reasoning capacities, demonstrated through applications in various forms of spatial VQA and both traditional and novel robotics tasks. ", "page_idx": 8}, {"type": "text", "text": "Limitations Readers may be curious about the inference speed of our framework. The bottleneck is the 3D object reconstruction process and the API call to closed-source VLMs $\\sim20$ seconds per image). However, we want to highlight that this process runs only once per image, and the speed is expected to improve with future versions of 3D foundation models. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Open AI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. ", "page_idx": 9}, {"type": "text", "text": "[2] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, and et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \n[4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias M\u00fcller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023.   \n[5] Reiner Birkl, Diana Wofk, and Matthias M\u00fcller. Midas v3.1 - A model zoo for robust monocular relative depth estimation. arXiv preprint arXiv:2307.14460, 2023.   \n[6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, and et al. RT-1: robotics transformer for real-world control at scale. In Robotics: Science and Systems, 2023.   \n[7] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402. IEEE, 2023.   \n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.   \n[9] Dingding Cai, Janne Heikkil\u00e4, and Esa Rahtu. OVE6D: object viewpoint encoding for depthbased 6d object pose estimation. In Conference on Computer Vision and Pattern Recognition, pages 6793\u20136803. IEEE, 2022.   \n[10] Berk \u00c7alli, Arjun Singh, Aaron Walsman, Siddhartha S. Srinivasa, Pieter Abbeel, and Aaron M. Dollar. The YCB object and model set: Towards common benchmarks for manipulation research. In International Conference on Advanced Robotics, pages 510\u2013517. IEEE, 2015.   \n[11] Thomas Carta, Cl\u00e9ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and PierreYves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pages 3676\u20133713, 2023.   \n[12] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. arXiv preprint arXiv:2401.12168, 2024.   \n[13] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geoffrey E. Hinton. Pix2seq: A language modeling framework for object detection. In International Conference on Learning Representations, 2022.   \n[14] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thapliyal, James Bradbury, and Weicheng Kuo. Pali: A jointly-scaled multilingual languageimage model. In International Conference on Learning Representations, 2023.   \n[15] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Conference on Computer Vision and Pattern Recognition, pages 2818\u20132829. IEEE, 2023.   \n[16] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon Sohn. DIML/CVL RGBD dataset: 2m RGB-D images of natural indoor and outdoor scenes. arXiv preprint arXiv:2110.11590, 2021.   \n[17] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.   \n[18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems, 2023.   \n[19] Murtaza Dalal, Tarun Chiruvolu, Devendra Singh Chaplot, and Ruslan Salakhutdinov. Plan-seqlearn: Language model guided RL for solving long horizon robotics tasks. In CoRL Workshop on Learning Effective Abstractions for Planning, 2023.   \n[20] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, pages 8469\u20138488, 2023.   \n[21] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research, pages 1231\u20131237, 2013.   \n[22] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, and Hao Su. Maniskill2: A unified benchmark for generalizable manipulation skills. In International Conference on Learning Representations, 2023.   \n[23] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, and Hao Su. Maniskill2: A unified benchmark for generalizable manipulation skills. In Conference on Learning Representations, 2023.   \n[24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross-attention control. In International Conference on Learning Representations, 2023.   \n[25] Ming-Kuei Hu. Visual pattern recognition by moment invariants. IRE Transactions on Information Theory, pages 179\u2013187, 1962.   \n[26] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. arXiv preprint arXiv:2403.08248, 2024.   \n[27] Muhammad Zubair Irshad, Sergey Zakharov, Rares Ambrus, Thomas Kollar, Zsolt Kira, and Adrien Gaidon. Shapo: Implicit representations for multi-object shape, appearance, and pose optimization. In European Conference on Computer Vision, pages 275\u2013292, 2022.   \n[28] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchmark & learning environment. Robotics and Automation Letters, pages 3019\u20133026, 2020.   \n[29] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, and David F. Fouhey. Perspective fields for single image camera calibration. In Conference on Computer Vision and Pattern Recognition, pages 17307\u201317316. IEEE, 2023.   \n[30] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Conference on Computer Vision and Pattern Recognition, pages 1988\u20131997. IEEE, 2017.   \n[31] Sertac Karaman and Emilio Frazzoli. Sampling-based algorithms for optimal motion planning. The International Journal of Robotics Research, pages 846\u2013894, 2011.   \n[32] Kourosh Khoshelham and Sander Oude Elberink. Accuracy and resolution of kinect depth data for indoor mapping applications. Sensors, pages 1437\u20131454, 2012.   \n[33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[34] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Aky\u00fcrek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. Pre-trained language models for interactive decision-making. In Advances in Neural Information Processing Systems, 2022.   \n[35] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.   \n[36] Jiehong Lin, Lihua Liu, Dekun Lu, and Kui Jia. SAM-6D: segment anything model meets zero-shot 6d object pose estimation. arXiv preprint arXiv:2311.15707, 2023.   \n[37] Dingning Liu, Xiaomeng Dong, Renrui Zhang, Xu Luo, Peng Gao, Xiaoshui Huang, Yongshun Gong, and Zhihui Wang. 3daxiesprompts: Unleashing the 3d spatial task capabilities of GPT-4V. arXiv preprint arXiv:312.09738, 2023.   \n[38] Fangchen Liu, Kuan Fang, Pieter Abbeel, and Sergey Levine. MOKA: open-vocabulary robotic manipulation through mark-based visual prompting. arXiv preprint arXiv:2403.03174, 2024.   \n[39] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 2023.   \n[40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, 2023.   \n[41] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3- $45++$ : Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023.   \n[42] Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. Gen6d: Generalizable model-free 6-dof object pose estimation from RGB images. In European Conference on Computer Vision, pages 298\u2013315, 2022.   \n[43] Chenyang Ma, Xinchi Qiu, Daniel Beutel, and Nicholas Lane. Gradient-less federated gradient boosting tree with learnable learning rates. In Proceedings of the 3rd Workshop on Machine Learning and Systems, pages 56\u201363, 2023.   \n[44] Luca Medeiros. Language segment-anything. https://github.com/luca-medeiros/ lang-segment-anything, 2023.   \n[45] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. OBJECT 3DIT: Language-guided 3d-aware image editing. In Advances in Neural Information Processing Systems, 2023.   \n[46] Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin, Vincent Lepetit, and Tomas Hodan. CNOS: A strong baseline for cad-based novel object segmentation. In International Conference on Computer Vision Workshops, pages 2126\u20132132. IEEE, 2023.   \n[47] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[48] Xinchi Qiu, Heng Hen, Wanru Zhao, Pedro Porto Buarque de Gusmao, and Nicholas Donald Lane. Efficient vertical federated learning with secure aggregation. In Federated Learning Systems (FLSys) Workshop@ MLSys 2023, 2023.   \n[49] Xinchi Qiu, Heng Pan, Wanru Zhao, Chenyang Ma, Pedro PB Gusmao, and Nicholas D Lane. vfedsec: Efficient secure aggregation for vertical federated learning via secure layer. arXiv preprint arXiv:2305.16794, 2023.   \n[50] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1623\u20131637, 2022.   \n[51] remyxai. Vqasynth, 2024. GitHub repository.   \n[52] Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. Visionlanguage models are zero-shot reward models for reinforcement learning. arXiv preprint arXiv:2310.12921, 2023.   \n[53] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from RGBD images. In European Conference on Computer Vision, pages 746\u2013760, 2012.   \n[54] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, C. Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, and Li Fei-Fei. BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pages 477\u2013490, 2021.   \n[55] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Winter Conference on Applications of Computer Vision, pages 3172\u20133182. IEEE, 2022.   \n[56] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: A dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023.   \n[57] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J. Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Conference on Computer Vision and Pattern Recognition, pages 2642\u20132651. IEEE, 2019.   \n[58] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. IRS: A large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation. In International Conference on Multimedia and Expo, pages 1\u20136. IEEE, 2021.   \n[59] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian A. Scherer. Tartanair: A dataset to push the limits of visual SLAM. In International Conference on Intelligent Robots and Systems, pages 4909\u20134916. IEEE, 2020.   \n[60] Yen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil Sreenath. Prompt a robot to walk with large language models. arXiv preprint arXiv:2309.09969, 2023.   \n[61] Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Zackory Erickson, David Held, and Chuang Gan. Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. arXiv preprint arXiv:2311.01455, 2023.   \n[62] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with llms enables open-world multi-task agents. In Advances in Neural Information Processing Systems, 2023.   \n[63] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems, 2022.   \n[64] Bowen Wen, Chaitanya Mitash, Baozhang Ren, and Kostas E. Bekris. se(3)-tracknet: Datadriven 6d pose tracking by calibrating image residuals in synthetic domains. In International Conference on Intelligent Robots and Systems, pages 10367\u201310373. IEEE, 2020.   \n[65] Tsung-Han Wu, Long Lian, Joseph E Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-controlled diffusion models. arXiv preprint arXiv:2311.16090, 2023.   \n[66] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao. Structure-guided ranking loss for single image depth prediction. In Conference on Computer Vision and Pattern Recognition, pages 608\u2013617. IEEE, 2020.   \n[67] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. In Advances in Neural Information Processing Systems, 2023.   \n[68] Fengyu Yang and Chenyang Ma. Sparse and complete latent organization for geospatial semantic segmentation. In Conference on Computer Vision and Pattern Recognition, pages 1809\u20131818, 2022.   \n[69] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan, and Andrew Owens. Touch and go: Learning from human-collected vision and touch. Advances in Neural Information Processing Systems, pages 8081\u20138103, 2022.   \n[70] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. arXiv preprint arXiv:2401.10891, 2024.   \n[71] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo networks. In Conference on Computer Vision and Pattern Recognition, pages 1787\u20131796. IEEE, 2020.   \n[72] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pages 1094\u20131100, 2019.   \n[73] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. In International Conference on Learning Representations, 2023.   \n[74] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In International Conference on Computer Vision. IEEE, 2023.   \n[75] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In SIGIR, pages 1097\u20131100. ACM, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix for SpatialPIN ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Overview ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This Appendix includes: 1) more technical details about our partial 3D scene reconstruction, 2) additional details, templates, and visualizations of our proposed two forms of SpatialVQA, 3) implementation details on our proposed application: discovering and planning for robotics tasks from a single image (task proposal, axes-constrained motion planning through waypoints, and trajectory generation and smoothing), 4) more experiments on our proposed application (dataset statistics, additional qualitative demonstrations, human understanding, and task diversity), and 5) prompt details for VLMs. ", "page_idx": 14}, {"type": "text", "text": "B Partial 3D Scene Reconstruction Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Metric Depth Estimation Because a significant portion of depth estimation model [70, 50, 4, 5, 68] is trained on depth datasets with depth data determined by sensors [21, 53] and stereo matching [71, 16, 58, 59, 66], we assume that the predicted normalized depth is the perpendicular distance to the camera plane, instead of a straight line from the object to the camera lens [32]. ", "page_idx": 14}, {"type": "text", "text": "Camera Intrinsic Estimation Given an RGB image, With the estimated vertical field of view (FOV), $\\theta_{v}$ , through perspective fields [29], the camera focal length $f$ can be found by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf=\\frac{H}{2\\tan\\left(\\frac{\\theta_{v}}{2}\\right)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $H$ is the image height in pixels. ", "page_idx": 14}, {"type": "text", "text": "Object 6D Pose Estimation Single-view 3D reconstruction model reconstructs mesh $O_{i}$ of object $i$ at the 3D origin, in the coordinate frame set by the input mask $M_{i}^{o f}$ , and captures $O_{i}$ by a pinhole camera. This camera, with 6D pose $P_{c}=[R_{c}\\ |\\ t_{c}^{o r i g i n}]$ , captures $O_{i}$ \u2019s canonical pose within the image. Thus, we can restore all objects\u2019 canonical poses across all images by identifying $P_{c}$ . ", "page_idx": 14}, {"type": "text", "text": "We use One-2 $\\mathrel{\\mathrel{\\left/{\\vphantom{\\Biggl3}}\\right.\\kern-\\nulldelimiterspace}}-3-45++$ [41], which provides the camera pose. For models without this information, we develop an efficient method to determine the pose by comparing object masks with rendered 3D model templates, inspired from matching-based 6D pose estimation works [36, 46, 9, 42]. ", "page_idx": 14}, {"type": "text", "text": "We generate a set of object templates, denoted as T ji}jJ=1, each rendered from the object\u2019s 3D model $O_{i}$ . These templates are created by positioning the camera at various locations on an icosphere surrounding the object in $S E(3)$ space, which simulates a spherical coverage around the object to capture its geometry from all angles uniformly. For each template $T_{j}^{i}$ , we compute a matching score against the occlusion-free object mask M io f. ", "page_idx": 14}, {"type": "text", "text": "We propose a simple yet effective score matching method. We draw a bounding rectangle around the segmented object inside $M_{i}^{o f}$ and across all $\\{T_{j}^{i}\\}_{j=1}^{J}$ , and crop the bounding rectangle. We then calculate the shape similarity between the contour line of cropped $M_{i}^{o f}$ and that of each cropped $T_{j}^{i}$ using Hu moments [25]. Additionally, we crop and resize the bounding rectangle to the same dimension, and evaluate the similarity based on the pixel area of the cropped and resized masks. Our score matching method can be formulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{i}^{A,h}=\\mathrm{Hu}(\\mathrm{findContour}(\\mathrm{crop}(M_{i}^{o f}))),\\qquad p a_{i}^{A}=\\mathrm{sum}(\\mathrm{resize}(\\mathrm{crop}(M_{i}^{o f}))),}\\\\ &{m_{i,j}^{B,h}=\\mathrm{Hu}(\\mathrm{findContour}(\\mathrm{crop}(T_{j}^{i}))),\\qquad p a_{i,j}^{B}=\\mathrm{sum}(\\mathrm{resize}(\\mathrm{crop}(T_{j}^{i}))),}\\\\ &{\\displaystyle\\Sigma(M_{i}^{o f},T_{j}^{i})=\\alpha\\left|1-\\frac{\\operatorname*{min}(p a_{i}^{A},p a_{i,j}^{B})}{\\operatorname*{max}(p a_{i}^{A},p a_{i,j}^{B})}\\right|+\\beta\\sum_{h=1}^{7}\\left|\\frac{1}{\\mathrm{sgn}(m_{i}^{A,h})\\cdot\\log(m_{i}^{A,h})}-\\frac{1}{\\mathrm{sgn}(m_{i,j}^{B,h})\\cdot\\log(m_{i,j}^{B,h})}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This dual approach allows for a comprehensive comparison that incorporates both the geometric configuration and the scale of the object representations. The best-matched template can be found by $\\begin{array}{r}{\\arg\\operatorname*{min}_{j=1}^{J}\\mathcal{L}(M_{i}^{o f},T_{j}^{i})}\\end{array}$ . ", "page_idx": 14}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/c28d212d9bb57de9ad37b20b9507f712d63fecdc365753c3910e2099b2b6aacb.jpg", "img_caption": ["Figure 6: Example input images of all forms of spatial VQA. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Object Scale Calibration After integrating all 3D object models $\\{O_{k}\\}_{k=1}^{K}$ into the 3D scene, each with a pose $\\{P_{k}=[R_{k}\\mid t_{k}]\\}_{k=1}^{K}$ and an initial scale nit}kK=1 set by the single-view 3D reconstruction model, we refine their scales to accurately reflect depth variations (e.g., moving an apple from close to the camera to a distant corner reduces its apparent size). Through the lens of the pinhole camera with pose $P_{c}$ , we render binary masks for each object and evaluate the length of their contour lines relative to their occlusion-free masks. The adjusted, final scale of object $i$ can be expressed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nS_{i}^{a d j}=S_{i}^{i n i t}\\times\\frac{\\mathrm{arcLength}(\\mathrm{findContour}(M_{i}^{o f}))}{\\mathrm{arcLength}(\\mathrm{findContour}(M_{i}^{r e n d}))},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $M_{i}^{r e n d}$ is the rendered mask of object $i$ ", "page_idx": 15}, {"type": "text", "text": "C Additional Experiments and Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Spatial Visual Question Answering ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Intra-Image Angular Discrepancies VQA (IaAD-VQA) For annotation, since YCBInEOAT [64] offers ground truth object 6D poses, we first determine the table/ground plane using the principal axes of objects resting on it (if present). Then, we calculate the angles between the principal axes of different objects to annotate a list of qualitative and quantitative QA pairs. We provide a subset of the question template below. ", "page_idx": 15}, {"type": "text", "text": "Qualitative questions: ", "page_idx": 15}, {"type": "text", "text": "Is [A] tilted?   \nIs [A] tilted to the left?   \nIs [A] inclined to the back?   \nIs [A] more tilted than [B]?   \nIs [A] more tilted than [B] to the back?   \nIs [A] more inclined than [B] to the right?   \nIs [A] leaning towards [B] vertically?   \nIs [A] straighter than [B]?   \nAlong which axis (W, D, H) is [A] more tilted?   \nWhich object(s) are not upright?   \nHow many object(s) are not upright? ", "page_idx": 15}, {"type": "text", "text": "Quantitative questions: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "What is the inclination angle of [A] along the vertical axis? How many degrees is [A] tilted horizontally? Calculate the angle of tilt for [A] towards back. ", "page_idx": 15}, {"type": "text", "text": "Measure the tilt of [A] relative to the front.   \nWhat is the relative angle between [A] and [B]?   \nMeasure the angle between [A] and [B].   \nWhat is the angle between the horizontal axis of [A] and [B]?   \nHow much is [A] inclined along the depth axis compared to [B]?   \nMeasure the inclination difference along the vertical axis for [A] and [B]. Measure the angular deviation of [A] and [B] along the vertical axis. Determine the angular difference between the depth axes of [A] and [B]. Determine the tilt difference between [A] and [B] along the horizontal axis. Compare the angles of tilt for [A] and [B] along the vertical axis. ", "page_idx": 16}, {"type": "text", "text": "Inter-Image Spatial Dynamics VQA (IrSD-VQA) For annotation, we manually measure the changes in objects\u2019 locations and angles between two photos. To measure the change in camera shot angle, we record the change in the angle of the tripod to which the iPhone 12 Pro Max is attached. We provide a subset of the question template below. ", "page_idx": 16}, {"type": "text", "text": "Qualitative questions: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Does [A] move?   \nDoes [A] rotate?   \nDoes [A] rotate clockwise?   \nDoes [A] incline?   \nDoes [A] move to the right?   \nDoes [A] move closer to [B]?   \nDoes [A] become more upright?   \nDoes [A] incline more to the back?   \nDoes the angle between [A] and [B] become smaller?   \nAlong which direction does [A] move?   \nAlong which axis (W, D, H) does [A] rotate?   \nWhich object(s) move?   \nHow many object(s) rotate?   \nHow many object(s) become more tilted to the back?   \nDoes the camera shot angle change?   \nAlong which axis (W, D, H) does the camera shot angle change? ", "page_idx": 16}, {"type": "text", "text": "Quantitative questions: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "How far does [A] move vertically?   \nHow far does [A] move horizontally?   \nHow far does [A] move towards the back?   \nHow many degrees does [A] rotate clockwise?   \nHow many degrees does [A] rotate counterclockwise?   \nWhat is the total distance [A] moves from its original position?   \nCalculate the angle of inclination of [A] in the second image.   \nMeasure the tilt of [A] relative to the first image.   \nWhat is the change in height of [A] from the first to the second image?   \nHow much does the distance between [A] and [B] change?   \nHow much does [A] incline towards the left compared to the first image?   \nWhat is the angular displacement of [A] towards the right?   \nMeasure the rotation angle of [A] about its own axis.   \nWhat is the new distance between [A] and [B] in the second image?   \nCalculate the difference in the inclination angle of [A] between the two images. Determine the change in angle of [A] relative to the ground plane.   \nWhat is the relative movement of [A] with respect to [B]?   \nMeasure the angular deviation of [A] and [B] along the vertical axis.   \nBy how many degrees has the camera shot angle changed?   \nAlong which axis/axes has the camera shot angle changed?   \nHow does [A] appear to move if we do not account for the camera shot angle change? What is the perceived change in orientation of [A] due to the camera angle change? ", "page_idx": 16}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/f68756f02bf1976f4383e741eef87e5136b37908f804aef20217f46e36f63f96.jpg", "img_caption": ["Figure 7: Pipeline for discovering and planning for robotics tasks from a single image. It incorporate the task proposal and motion planning modules based on SpatialPIN. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "How does the position of [A] change relative to the camera angle difference? What is the actual move distance of [A] when accounting for the camera shot angle change? ", "page_idx": 17}, {"type": "text", "text": "Dataset Examples We illustrate example input images of all forms of spatial VQA in Fig. 6. ", "page_idx": 17}, {"type": "text", "text": "C.2 Discovering and Planning for Robotics Tasks from a Single Image ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Given a single RGB image of a scene with unknown environments and objects, the VLM identifies potential tasks and plans their execution using full 3D trajectories, complete with visualization. Figure 7 shows the pipeline, which incorporates task proposal and axes-constrained motion planning modules into SpatialPIN\u2019s pipeline from our main paper (Fig. 2). ", "page_idx": 17}, {"type": "text", "text": "Task Proposal We query VLM to propose meaningful, diverse tasks, each with a one-sentence task description. Instead of directly querying VLM for task proposal, we employ a hybrid approach that integrates role-play and object-based initialization. In the role-play scenario, we prompt VLM to envision itself as a robotic/human hand working in the scene to perform household tasks. For the object-based initialization, we guide VLM to sequentially focus on each identified object within the scene. When the scene contains more than one identified objects, VLM is instructed to suggest two tasks emphasizing interactions between the manipulating object and any of the detected objects, and an additional task focused solely on the manipulating object. If only one object is detected, VLM is directed to propose a task involving just that object. This strategy guarantees a broad spectrum of task suggestions, ensuring comprehensive object engagement. ", "page_idx": 17}, {"type": "text", "text": "To further tailor the task proposals, we impose specific constraints, directing VLM to consider the practical affordances of objects while encouraging creative assumptions (e.g., a bowl\u2019s capacity to hold water) and potential interactions (e.g., transferring water from a cup into a bowl). Additionally, we delineate clear boundaries by excluding tasks that entail the disassembly of objects, functionality tests, or the involvement of imaginary objects, thereby focusing on feasible and meaningful tasks. ", "page_idx": 17}, {"type": "text", "text": "As a concrete example, given the image on the left of Fig. 2, with the manipulating object to be the red can, VLM will propose the following tasks: ", "page_idx": 17}, {"type": "text", "text": "\u201cTask name: Can to Bowl Transfer   \nDescription: Pick up the can and pour its contents into the bowl. Task name: Can Relocation   \nDescription: Pick up the can and place it inside the bowl.   \nTask name: Can Rotation   \nDescription: Rotate the can 90 degrees on its vertical axis. ", "page_idx": 17}, {"type": "text", "text": "Axes-Constrained Motion Planning through Waypoint We introduce a novel method to guide VLM to conduct motion planning within a 3D scene based on a proposed task by planning motion waypoints along the manipulating object\u2019s principal axes. More specifically, we define four types of manipulations that VLM can use: ", "page_idx": 17}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/c70316888c871874daf7c04a7a8e6ee613ff60c8b8931515c3aaa931f9b943e2.jpg", "img_caption": ["Figure 8: Dataset statistics. Our dataset presents 51 scenes\u201413 from NOCS and 38 captured from varied perspectives\u2014featuring a wide range of object categories, quantities, and a diverse set of tasks and planned trajectories. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Rotation type 1: Axial rotation. The object rotates around its principle axes. ", "page_idx": 18}, {"type": "text", "text": "Rotation type 2: Rotation relative to the target object.   \n- pitch: Tilt similar to pouring water, around a horizontal axis formed by the cross product of the connecting directional vector and the target\u2019s vertical axis.   \n- yaw: Horizontal rotation, like a camera panning, around a vertical axis formed by the cross product of the connecting directional vector and the pitch axis.   \n- roll: Rotation like drilling a surface, around the connecting directional vector. ", "page_idx": 18}, {"type": "text", "text": "Translation type 1: Defines the goal relative to the target object\u2019s principle axes, with translation values for its $[x,y,z]$ axes in centimeters. $[0,0,0]$ cm indicates the goal is the center of the target object. ", "page_idx": 18}, {"type": "text", "text": "Translation type 2: Sets the goal relative to a directional vector between two reference objects, specifying how far (in cm) object 1 should move towards or away from object 2 along this vector. ", "page_idx": 18}, {"type": "text", "text": "Since VLM inherently lacks the capability to provide 3D coordinates and low-level actions directly [60, 19], our method offers a practical workaround by translating natural language instructions into precise motion waypoints. This approach significantly enhances VLM\u2019s utility in spatial reasoning and manipulation tasks without requiring direct 3D coordinate generation capabilities. Also, the four types of manipulations we defined are both simple and comprehensive, covering a broad spectrum of manipulation tasks. ", "page_idx": 18}, {"type": "text", "text": "As a concrete example, given the image on the left of Fig. 7, with \u201cTask name: Can to Bowl Transfer\u201d, VLM will plan as follows: ", "page_idx": 18}, {"type": "text", "text": "\u201cTask Name: Can to Bowl Transfer   \nManipulating obj idx: 3   \nInteracting obj idx: 4   \n1. Move Manipulating Obj [3] to [6, 0, 7] cm relative to Target Obj [4]\u2019s local [x, y, z] axes. 2. rotate_wref: Rotate Manipulating Obj [3] relative to Target Obj [4] around [pitch] axis by [75] degrees.\u201d ", "page_idx": 18}, {"type": "text", "text": "In practice, the quality of motion planning by VLMs can be enhanced using various prompting techniques. One such technique is chain-of-thought (CoT) [63], where another LLM guides the VLM to plan each axes-constrained sparse waypoint step by step. ", "page_idx": 18}, {"type": "text", "text": "Dataset Statistics See Fig. 8 for statistics and visuals. We standardize all image dimensions by resizing all to $640\\times480$ . ", "page_idx": 18}, {"type": "text", "text": "More Qualitative Demonstration We present more qualitative examples spanning various scenes and tasks, as shown in Fig. 9. ", "page_idx": 18}, {"type": "image", "img_path": "YTHJ8O6SCB/tmp/f735d26c1af28819b19b30ec8d7002b9c280d33f7c9d57bfed51331e8da2349a.jpg", "img_caption": ["Figure 9: More qualitative examples. With diverse input scenes and proposed tasks, our framework produces 3D trajectories with geometric awareness that aligns with the task descriptions. Zoom in for better view. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/ed5a1ba35b4defba8d26d6fd79f5e43d124c454a195d95ae468c392e0dd4c4f9.jpg", "table_caption": ["Table 10: Comparison of task diversity. We sample 106 proposed tasks for fair comparison with RoboGen and previous RL benchmarks. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Experiment on Human Understanding ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We aim to understand human perception of our generated task executions by asking participants to provide a one-sentence description. We then evaluate the alignment between these descriptions and the ground truth task descriptions proposed by VLM, using OpenCLIP [15, 43]. Table. 9 reveals a high degree of alignment. Intriguingly, it appears humans understand our task executions more accurately than machines do (Table. 7). We hypothesize this discrepancy stems from the limitations of current video understanding models, whereas humans draw on their prior experiences for a deeper comprehension. ", "page_idx": 19}, {"type": "table", "img_path": "YTHJ8O6SCB/tmp/3bbb15a012c18819054556d89dcab510e3141e3282af126c6a1101a01a06ea87.jpg", "table_caption": ["Table 9: Results for machine understanding (generation) on 278 task executions and human understanding, where 25 users write descriptions for 10 tasks. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Experiment on Task Diversity We evaluate the diversity of the proposed tasks in terms of semantic meaning using Self-BLEU and the embedding similarity [75, 48, 49] following RoboGen [61], where lower scores mean higher diversity. We also compare with previous reinforcement learning (RL) benchmarks. From Table. 10, ours method generates most diverse tasks as it is open to all scenes with no constraint. ", "page_idx": 19}, {"type": "text", "text": "C.3 Trajectory Generation Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Trajectory Generation With the waypoints planned by VLM, we generate the manipulating object\u2019s trajectory using path planning algorithm, specifically rapidly-exploring random tree star (RRT\\*) [31]. To generate accurate collision-free path, we perform K-means clustering on the point clouds of object 3D model with a high number of clusters, segmenting the object mesh into discrete voxels and treating each voxel as an obstacle. Then, to accurately consider the manipulating object\u2019s dimensions, we grow the size of each voxel by its dimensions. ", "page_idx": 20}, {"type": "text", "text": "Handling VLM Planning Discrepancies The waypoints generated by VLM are typically accurate and practical. Nonetheless, there are instances where the waypoints suggested by VLM lead to collisions as determined by the RRT\\* planner. This discrepancy is less about VLM\u2019s misunderstanding of the objects\u2019 sizes and their spatial relationship and more about the precision level of the waypoints, which may not match the exacting standards of the $\\mathbf{RRT*}$ planner\u2019s outcomes. To resolve this, we implement Gaussian sampling around the initially planned waypoints whenever a collision is detected. The sampling strategy is guided by a predefined set of geometric rules. In our 3D coordinate system, positive $\\mathbf{X}$ -axis [1, 0, 0] points right, positive y-axis [0, 1, 0] is away from viewer, positive ${\\bf Z}$ -axis [0, 0, 1] is up. For translation type 1, we denote the goal pose relative to the target object\u2019s principle axes as [dx, dy, dz]. For translation type 2, we denote the distance that object 1 moves towards object 2 as dD. The set of geometric rules are as follows: ", "page_idx": 20}, {"type": "text", "text": "if type $1~\\&~[{\\mathsf{d x}},{\\mathsf{d y}},{\\mathsf{d z}}]=[0,0,0]$ : sample along [x, y, z] axes elif type $\\textsf{l}\\,\\&\\,\\mathsf{d}\\mathsf{x}=0\\ \\&\\ \\mathsf{d}\\mathsf{y}=0\\ \\&\\ \\mathsf{d}z\\ \\mathsf{l}=0\\,$ sample along [z] axis, $z_{\\mathsf{s a m p l e d}}\\cdot\\mathsf{d}z>1$ elif type $1\\ \\&\\ \\mathsf{d}z=0$ : sample along [x, y] axes elif type $1\\ \\&\\ \\mathsf{d}z\\ \\mathsf{l}{=}\\ 0$ : sample along [x, y, z] axes, zsampled\u00b7 dz > 1 ", "page_idx": 20}, {"type": "text", "text": "if type 2: sample along the connecting directional vector, Dsampled\u00b7 $\\mathsf{d D}>1$ ", "page_idx": 20}, {"type": "text", "text": "Trajectory Smoothing Finally, to ensure our trajectory is natural and smooth, we linearly interpolate rotation and interpolate translation using cubic spline. ", "page_idx": 20}, {"type": "text", "text": "D Prompt Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We show exact prompts for VLMs for our proposed application: discovering and planning for robotics tasks from a single image. ", "page_idx": 20}, {"type": "text", "text": "Understanding Objects by Constraining Prompt. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Input: RGB image (640, 480) $=$ (width, height) with multiple objects. ", "page_idx": 20}, {"type": "text", "text": "Your task is to identify and objects by precise color, texture, and 2D spatial locations (in words).   \nDo not use vague phrase like multi-colored. ", "page_idx": 20}, {"type": "text", "text": "Please write in the following format. Do not output anything else: Object idx (actual integer, start from 0): $\\tt x$ of color y, texture z at location $\\boldsymbol{\\mathtt{w}}$ . ", "page_idx": 20}, {"type": "text", "text": "Task Proposal Prompt. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Input:   \n1. RGB image (640, 480) $=$ (width, height) with multiple objects. 2. Detected objects with index. ", "page_idx": 20}, {"type": "text", "text": "You are a single robot hand working in this image scene to perform simple household tasks. Tasks must be discovered from the image. Consider objects\u2019 affordances and feel free to make assumptions (e.g., a bowl can contain water) and interactions with other objects (e.g., pouring water from a cup into a bowl). ", "page_idx": 20}, {"type": "text", "text": "translation $^+$ rotation).   \n2. Rotate manipulating object (involve rotation). Strictly follow constraints:   \n1. Exclude tasks involving disassembly of objects.   \n2. Exclude tasks involving cleaning or functionality testing.   \n3. Exclude tasks involving imaginary objects.   \n4. Manipulating object moves; interacting object static.   \n5. Assume all objects are rigid, without joints or moveable parts (i.e., cannot deform, disassemble, transform). This applies even to objects that are typically articulated (e.g., laptop). Propose 3 tasks (2 interaction, 1 rotation) for manipulating Object 5. Write in the following format. Do not output anything else:   \nTask Name: xxx   \nManipulating obj idx: 5   \nInteracting obj idx: obj_idx (actual integer, or manipulating obj idx)   \nDescription: basic descriptions. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Coarse 3D Understanding Prompt. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Inputs:   \n1. RGB image (640, 480) $=$ (width, height) with multiple objects   \n2. Detected objects with index.   \n3. Image scene size.   \n4. Maximum and minimum width, depth, and height. ", "page_idx": 21}, {"type": "text", "text": "Your task is to identify the camera shot angle (horizontal, top-down, bottom-up). Reason with respect to the visual cues, the image scene size, and maximums and minimums along each dimension. Choose horizontal if not severely angled. ", "page_idx": 21}, {"type": "text", "text": "Please write in the following format. Be concise. Do not output anything else: Visual cues reasoning: ...   \nSpatial data reasoning: ...   \nConclusion: horizontal/top-down/bottom-up. ", "page_idx": 21}, {"type": "text", "text": "Image and Spatial Context Understanding Prompt. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Inputs:   \n1. RGB image (640, 480) $=$ (width, height) with multiple objects and their visualized local axes ( $\\scriptstyle\\mathbf{x}$ red, y green, z blue).   \n2. Detected objects with index.   \n3. For each detected object, its 3D center, local xyz-axes, size, and spatial relationship relative to other objects. ", "page_idx": 21}, {"type": "text", "text": "The 3D coordinate system of the image is in centimeters and follows Blender. Positive $\\tt x$ -axis [1, 0, 0] right, positive $\\mathtt{y}$ -axis [0, 1, 0] away from viewer, positive z-axis [0, 0, 1] up. Positive rotation is counter-clockwise around all axes. ", "page_idx": 21}, {"type": "text", "text": "Your task is to learn the spatial context. Do not output. ", "page_idx": 21}, {"type": "text", "text": "Motion Planning Prompt. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Inputs:   \n1. RGB image (640, 480) $=$ (width, height) with multiple objects and their visualized local axes (x red, y green, $_\\mathrm{z}$ blue).   \n2. Detected objects with index.   \n3. Simple household tasks and descriptions to be performed by a single robot hand. ", "page_idx": 21}, {"type": "text", "text": "Your goal is to plan fine-grained motions for the manipulating object to complete the tasks using four manipulations, explained as follows: ", "page_idx": 22}, {"type": "text", "text": "Rotation: ", "page_idx": 22}, {"type": "text", "text": "rotate_self: Axial rotation. The object rotates around its local [x/y/z] axis by [degrees].   \nrotate_wref: Rotation relative to the target object:   \n- pitch: Tilt similar to pouring water, around a horizontal axis formed by the cross product of the connecting directional vector and the target\u2019s z-axis.   \n- yaw: Horizontal rotation, like a camera panning, around a vertical axis formed by the cross product of the connecting directional vector and the pitch axis.   \n- roll: Rotation like a drill entering a surface, around the connecting directional vector.   \nThe degrees can be specified in two ways:   \n- Exact [degrees]. Positive values rotate the manipulating object towards the target object.   \n- Fixed_towards/fixed_back. \u2019fixed_towards\u2019 orients the object towards the target, mimicking actions like pouring (pitch), facing (yaw), or drilling into (yaw+roll) the target. \u2019fixed_back\u2019 reverses this alignment. ", "page_idx": 22}, {"type": "text", "text": "Translation: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "translate_tar_obj: Defines the goal relative to the target object\u2019s local axes, with translation values for its [local_x, local_y, local_z] axes in centimeters. [0, 0, 0] cm indicates the goal is the center of the target object.   \ntranslate_direc_axis: Sets the goal relative to a directional vector between two reference objects, specifying how far (in cm) object 1 should move towards or away from object 2 along this vector (positive closer, negative away). Object indices must differ, and if one reference object is the manipulating object, its current location is used. Strictly follow caveats:   \n1. Apply rotate_wref thoughtfully and sequentially around different axes as needed.   \n2. Use the provided spatial information and image effectively for understanding and planning within the 3D scene.   \n3. Combine common physical understanding with the scene\u2019s spatial details (like relative positions and sizes of objects) for strategic planning.   \n4. Remember that objects\u2019 local axes\u2019 positive directions might require using negative values in rotation and translation for authentic motion planning. Plan as below. Fill in obj_idx based on the tasks.   \nrotate_self: Rotate Manipulating Object [obj_idx] around its local axis $[\\mathrm{x}/\\mathrm{y}/\\mathrm{z}]$ by [degrees]. rotate_wref: Rotate Manipulating Object [obj_idx] relative to Target Object [target_obj_idx] around [pitch/yaw/roll] axis by [degrees/fixed_towards/fixed_back].   \ntranslate_tar_obj: Move Manipulating Object [obj_idx] to [a, b, c] cm relative to Target Object [target_obj_idx]\u2019s local [x, y, z] axes.   \ntranslate_direc_axis: Move Manipulating Object [obj_idx] [a] cm along the directional vector from Reference Object [ref_obj_1_idx] to Reference Object [ref_obj_2_idx]. Here are some full examples. Please write in the following format. Do not output anything else: Task Category: Bear rotation   \nDescription: Rotate the toy bear 90 degrees on its vertical axis.   \nMotion Planning:   \nManipulating obj idx: bear_idx (actual integer)   \nInteracting obj idx: bear_idx (actual integer)   \n1. rotate_self: Rotate Manipulating Object [bear_idx] around its local axis [z] by [90] degrees. Task Name: Cup content transfer   \nDescription: Pick up the mug and pour its contents into the bowl.   \nMotion Planning:   \nManipulating obj idx: cup_idx (actual integer)   \nInteracting obj idx: bowl_idx (actual integer)   \n1. translate_tar_obj: Move Manipulating Object [cup_idx] to [5, -7, 5] cm relative to Target Object [bowl_idx]\u2019s local [x, y, z] axes. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "2. rotate_wref: Rotate Manipulating Object [obj_idx] relative to Target Object [bowl_obj_idx] around [pitch] axis by [fixed_towards]. ", "page_idx": 23}, {"type": "text", "text": "Task Name: Screwdriver penetration   \nDescription: Use a screwdriver to penetrate an avocado.   \nMotion Planning:   \nManipulating obj idx: screw_idx (actual integer)   \nInteracting obj idx: avocado_idx (actual integer)   \n1. translate_tar_obj: Move Manipulating Object [screw_idx] to [-5, -5, 0] cm relative to Target Object [avocado_idx]\u2019s local [x, y, z] axes.   \n2. rotate_wref: Rotate Manipulating Object [screw_idx] relative to Target Object [avocado_idx] around [yaw] axis by [fixed_towards].   \n3. rotate_wref: Rotate Manipulating Object [screw_idx] relative to Target Object [avocado_idx] around [roll] axis by [360] degrees. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conducted extensive empirical experiments to demonstrate it. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discussed limitations in the Discussion and Conclusion. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We did not have theoretical assumptions. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have thoroughly discussed implementations details in the main paper and Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have created our project page and Github repo. These will be updated soon. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discussed these in the Experiments and Appendix with a lot of details. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: In our experiments, large time cost is needed to do this. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discussed it in the Experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We followed the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work does not have broader social impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No such concern. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All properly cited. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA]   \nJustification: No new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work does not have such. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: This work does not have such. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]