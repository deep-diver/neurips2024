[{"type": "text", "text": "BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haohong Lin1, Wenhao Ding1, Jian Chen1, Laixi $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{2}$ , Jiacheng ${\\bf Z}{\\bf h}{\\bf u}^{3}$ , $\\bf{B o}\\,\\bf{L i}^{4}$ , Ding Zhao1 1CMU, 2Caltech, 3MIT, 4UChicago & UIUC {haohongl, wenhaod}@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline model-based reinforcement learning (MBRL) enhances data efficiency by utilizing pre-collected datasets to learn models and policies, especially in scenarios where exploration is costly or infeasible. Nevertheless, its performance often suffers from the objective mismatch between model and policy learning, resulting in inferior performance despite accurate model predictions. This paper first identifies the primary source of this mismatch comes from the underlying confounders present in offline data for MBRL. Subsequently, we introduce BilinEar CAUSal rEpresentation (BECAUSE), an algorithm to capture causal representation for both states and actions to reduce the influence of the distribution shift, thus mitigating the objective mismatch problem. Comprehensive evaluations on 18 tasks that vary in data quality and environment context demonstrate the superior performance of BECAUSE over existing offline RL algorithms. We show the generalizability and robustness of BECAUSE under fewer samples or larger numbers of confounders. Additionally, we offer theoretical analysis of BECAUSE to prove its error bound and sample efficiency when integrating causal representation into offilne MBRL. See more details in our project page: https://sites.google.com/view/be-cause. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offilne Reinforcement Learning (RL) has shown great promise in learning directly from pre-collected datasets, especially in scenarios where active interaction is expensive or infeasible [1]. Specifically, offline model-based reinforcement learning (MBRL) [2, 3, 4], learning policies with an estimated world model, generally perform better than their model-free counterparts in long-horizon tasks such as self-driving vehicles [5], robotics [6], and healthcare [7]. However, offline RL suffers from distribution shift because the rollout data is either sampled from some suboptimal behavior policies or sampled from slightly different training environments compared to the deployment time [8]. ", "page_idx": 0}, {"type": "text", "text": "Although identifying distribution shift issues, many of the current offilne MBRL works fail to model the shift in environment dynamics, which is ubiquitous and could cause catastrophic failure of trained policy at a slightly different deployment stage. Furthermore, since the learning objectives of the world models and policies are isolated from each other, a significant challenge in offline MBRL is objective mismatch [9, 10] problem (shown in Figure 1): models that achieve a lower training loss are not nec", "page_idx": 0}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/c8ebd1fef3243715c3d7b75ea6306f58a1ba407967fe6dd7eb7b794e30097468.jpg", "img_caption": ["Figure 1: The objective mismatch problem. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "essarily better for control performance. For example, a dynamics model achieve relatively low prediction loss, yet it may not be sufficient to guide the planner to the high-reward region. Previous works [9, 10] have attempted to reduce such objective mismatch by jointly learning the model and policy. However, the performance is suboptimal due to the lack of digging into the underlying cause of the mismatch issue [8]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we identify that the objective mismatch between model estimation and policy learning comes from two sources of distribution shift in offilne MBRL: (1) shift between the online optimal policy and offilne sub-optimal behavior policies, and (2) shift between the data collection environment and online testing environments. Unlike humans, who make decisions based on reasoning over taskrelevant factors, models in offline RL memorize correlations without learning the causality. The sub-optimal behavior policies introduce spurious correlations [11] between actions and states, making the model memorize specific actions. When online testing environments differ from the data collection environment, the model could overfti spurious correlations in the state and fail to generalize to unseen states. Based on the analysis of the above mismatch, our work differs from previous work of causal model-based RL [12, 13] in that we model causality in both model and policy learning., We aim to avoid spurious correlation by discovering underlying structures between abstracted states and actions. ", "page_idx": 1}, {"type": "text", "text": "To alleviate objective mismatch and generalize well, we introduce the BilinEar CAUSal rEpresentation (BECAUSE) that integrates the causal representation in both world model learning and planning of MBRL agents. Inspired by preliminary works that use bilinear MDPs to capture the structural representation in MBRL [14], we first approximate the causal representation to capture the low-rank structure in the world model, then use this learned representation to facilitate planning by quantifying the uncertainty of sampled transition pairs. Consequently, we factorize the spurious correlations and learn a unified representation for both the world model and planner. ", "page_idx": 1}, {"type": "text", "text": "In summary, the contribution of this paper is threefold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We formulate offline MBRL into the causal representation learning problem, highlighting the tight connection between structural causal models and low-rank structures in MDPs. To the best of our knowledge, this is the first work that systematically reveals the connection between causal representation learning and Bilinear MDPs.   \n\u2022 We propose BECAUSE, an empirical causal representation framework, based on the above formulation. BECAUSE first learns a causal world model, then fosters the generalizability of offilne RL agents by quantifying the uncertainty of the state transition, which facilitates conservative planning to mitigate the objective mismatch.   \n\u2022 We provide extensive empirical studies and performance analysis in tasks of multiple domains to demonstrate the superiority of BECAUSE over existing baselines, which illustrates its potential to improve the generalizability and robustness of offline MBRL algorithms. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To alleviate the objective mismatch problem and the degraded performance caused by the spurious correlation, we first provide our novel formulation of learning the underlying causal structures of Markov Decision Process (MDP) under the bilinear MDP setting, then introduce the causal discovery for MDP with confounders. ", "page_idx": 1}, {"type": "text", "text": "2.1 Preliminary: MDP and Bilnear MDP ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We denote an episodic finite-horizon MDP by $\\mathcal{M}=\\left\\{\\mathcal{S},\\mathcal{A},\\mathcal{T},H,r\\right\\}$ , which is composed of state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , a set of transition functions $\\bar{\\tau}$ , planning horizon $H$ and reward function $r$ associated with task preferences. Without loss of generality in many real-world practices, we assume that the reward function is bounded by $r_{h}\\in[0,1],\\forall h\\in[H]$ . Specifically, we are interested in a goal-conditioned reward setting, where $\\forall g\\in S$ , $r(s,a;g)=1$ if and only if $s=g$ . Given a policy $\\pi$ and the state-action pair $(s,a)\\in S\\times A$ , we then define the state-action value function in the timestep $h$ as $\\begin{array}{r}{Q_{h}^{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[\\sum_{i=h}^{H}r_{i}(s_{i},a_{i})|s_{h}=s,a_{h}=a\\right]}\\end{array}$ , and the value function $\\begin{array}{r}{V_{h}^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[\\sum_{i=h}^{H}r_{i}(s_{i},a_{i})|s_{h}=s\\right]\\!.}\\end{array}$ . The expectation $\\mathbb{E}_{\\pi}$ here is integrated into randomness throughout the trajectory, which is essentially induced by the random action of the policy $a_{i}\\sim\\pi(\\cdot|s_{i})$ ", "page_idx": 1}, {"type": "text", "text": "and the time-homogeneous transition dynamics of the environment $s_{i+1}\\sim T(\\cdot|s_{i},a_{i}),\\forall i\\in[h,H]$ . In the offilne dataset, the data rollouts can be seen as generated by some (mixed) behavior policy $\\pi_{\\beta}$ , resulting in a dataset $\\mathcal{D}$ with in total $n$ samples $\\{s_{i},\\bar{a_{i}},s_{i}^{\\prime},r_{i}\\}_{1\\leq i\\leq n}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Bilinear MDP [14]). For each $(s,a)\\in\\mathcal{S}\\times\\mathcal{A},s^{\\prime}\\in\\mathcal{S}$ , we have the corresponding feature vector $\\phi(\\cdot,\\cdot):\\mathbb{R}^{|S|}\\times\\mathbb{R}^{|A|}\\to\\mathbb{R}^{d},\\mu(\\cdot):\\mathbb{R}^{|S|}\\to\\mathbb{R}^{d^{\\prime}}$ . With some core matrix $M\\in\\mathbb{R}^{d\\times d^{\\prime}}$ , we can represent the transition function kernel $T(\\cdot|\\cdot,\\cdot)$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall s,a,s^{\\prime}\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S},\\;\\;T(s^{\\prime}|s,a)=\\phi(s,a)^{T}M\\mu(s^{\\prime}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi(s,a)$ and $\\mu(s^{\\prime})$ are embedding functions that map the original state and action to the latent space, $M$ is the core matrix that models the transition relationship between the previous timestep and next timestep in the latent space. Such a linear decomposition in the transition dynamics allows us to embed structures of the transition model without the loss of general function approximation capabilities to derive state and action representations. ", "page_idx": 2}, {"type": "text", "text": "2.2 Action State Confounded MDP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the existence of confounders in the MDP to represent the offline data collection process, and define action-state confounded MDP (ASC-MDP): ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (ASC-MDP). Besides the components in standard MDPs $\\mathcal{M}=\\left\\{S,\\mathcal{A},T,H,\\dot{r}\\right\\}$ , we introduce a set of unobserved confounders $u$ . In ASP-MDP, confounders are factorized as $u\\,=\\,\\{u_{\\pi},u_{c}\\}_{1\\leq h\\leq H}$ , where $u_{\\pi}\\,\\in\\,\\mathcal{U}$ denotes the confounders between $s$ and $a\\sim\\pi_{\\beta}(s)$ induced by behavior policies, and $u_{c}\\in\\mathcal{U}$ denotes the confounders within the state-action pairs of the environment transition, that is, the inherent structure between $(s,a)$ and $s^{\\prime}$ . Here we ", "page_idx": 2}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/54f83492d3b9ee6340cb68e229304a4de6373959495d4a9e830089ccea492389.jpg", "img_caption": ["Figure 2: Comparison of our ASC-MDP with two existing formulations. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "assume a time-invariant confounder distribution $u\\sim P_{u}(\\cdot),\\forall h\\in[H]$ , which is a common assumption [15, 16, 17] ", "page_idx": 2}, {"type": "text", "text": "The resulting causal relationship of ASC-MDP is demonstrated in Figure 2. Originating from the original MDP, ASC-MDP is different from the Confounded MDP [18] and State-Confounded MDP (SC-MDP) [19] in that it models both the spurious correlation between the current state $s$ and the current action $a$ , as well as those between the next state $s^{\\prime}$ and $(s,a)$ . Yet, confounded MDP and SC-MDP only model part of the possible confounders between states and actions. The factorization of the confounder in ASC-MDP aligns with the source of spurious correlation in offline MBRL. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method: BECAUSE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose BECAUSE, our core methodology for modeling, learning, and applying our causal representations for generalizable offline MBRL. Section 3.1 models the basic format of causal representations and analyzes their properties. Section 3.2 gives a compact way to learn the causal representation $\\phi(s,a)$ and $\\mu(s^{\\prime})$ , as well as the core mask estimation $M$ . Section 3.3 utilizes these learned causal representations in both world model learning and MBRL planning from offilne datasets. ", "page_idx": 2}, {"type": "text", "text": "3.1 Causal Representation for ASC-MDP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the presence of a hidden confounder $u$ , we model the confounder behind the transition dynamics as a linear confounded MDP [18]: ", "page_idx": 2}, {"type": "equation", "text": "$$\nT(s^{\\prime}|s,a,u)=\\widetilde{\\phi}(s,a,u)^{T}\\mu(s^{\\prime}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $u\\sim P_{u}(\\cdot)$ . Inspired by the Bilinear MDP in Definition 1, we decompose $\\widetilde{\\phi}(s,a,u)$ into a confounder-aware core matrix $M(u)$ and a feature mapping $\\phi(s,a)$ , which factorize  the influence of ", "page_idx": 2}, {"type": "text", "text": "the confounders. Given the factorization of confounder $\\boldsymbol{u}=\\{u_{c},u_{\\pi}\\}$ in Definition 2, we derive via $d$ -separation in the graphical model in Figure 2 that $s^{\\prime}$ \u22a5\u22a5 $u_{\\pi}|\\{s,a,u_{c}\\}$ . As a result, we only need to consider the confounder $u_{c}$ from the environment when decomposing the transition model: ", "page_idx": 3}, {"type": "equation", "text": "$$\nT(s^{\\prime}|s,a,u)=T(s^{\\prime}|s,a,u_{c})=\\phi(s,a)^{T}M(u_{c})\\mu(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Construction of causal graph $G$ ). In ASC-MDP, $G={\\binom{0^{d\\times d}}{0^{d^{\\prime}\\times d}}}_{\\begin{array}{c}{0^{d^{\\prime}\\times d^{\\prime}}}\\end{array}}$ 0d\u2032\u00d7d\u2032 . for all (sparse) core matrix $M$ , the causal graph $G$ is bipartite, thus $\\forall\\,G,G\\in\\mathrm{DAG}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3 reveals the connection between the core matrix $M$ and causal graph $G$ , as is formulated in the ASC-MDP. To reduce the influence of $u$ and estimate the unconfounded transition model $T(s^{\\prime}|s,a)$ , one way is to identify the causal structures induced by the confounder $u_{c}$ for the transition dynamics [20]. Existing methods in differentiable causal discovery [21, 22, 23] transform causal discovery on some causal graph $G$ , into a maximum likelihood estimation (MLE) with regularization: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{G}=\\underset{G\\in\\mathrm{DAG}}{\\arg\\operatorname*{max}}\\log p(\\mathcal{D};\\phi,\\mu,G)-\\lambda|G|\\quad\\Longrightarrow\\quad\\widehat{M}=\\underset{M}{\\arg\\operatorname*{max}}\\log p(\\mathcal{D};\\phi,\\mu,M)-\\lambda|M|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since in our case, $M$ is a sub-matrix of the causal graph $G$ . Given the Definition 3, $G$ automatically satisfies the formulation of ASC-MDP, thus discovering $G$ is essentially estimating the sparse submatrix $M$ without DAG constraints: $\\widehat{M}=\\arg\\operatorname*{max}_{M}\\log p(\\mathcal{D};\\phi,\\mu,M)-\\lambda|M|$ . We elaborate Definition 3 and show the relationship be tween core matrix $M$ and causal graph $G$ in Appendix A.3. ", "page_idx": 3}, {"type": "text", "text": "We also make the assumption that the sparse $G$ and $M$ remain invariant with different environment confounders in the offline training and online testing. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Invariant causal graph). We denote the causal graph $G$ under confounder $u$ as $G(u)$ . The generalization problem that we aim to solve satisfies the invariance in the causal graph $G(u_{c})$ , where $G(u_{c})=G(\\bar{u}_{c}^{\\prime}),M(u_{c})=M(u_{c}^{\\prime})$ , $u_{c}$ and $u_{c}^{\\prime}$ are the confounders in training and testing. ", "page_idx": 3}, {"type": "text", "text": "Remark 1. The assumption 1 can also be interpreted as task independence in [24], invariant state representation [25], or invariant action effect in [26]. See detailed comparison in appendix Table 4. ", "page_idx": 3}, {"type": "text", "text": "3.2 Learning Causal Representation from Offline Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first learn the causal world model $T(s^{\\prime}|s,a)$ in the presence of confounders $u$ in the offline datasets. As formulated in ASC-MDP 2, there are two sets of confounders: $u_{\\pi}$ and $u_{c}$ . To estimate an unconfounded transition model and remove the effect of confounder, we first remove the impact of $u_{c}$ which comes from the dynamics shift by estimating a batch-wise transition matrix $M(u_{c})$ , then we apply a reweighting formula to deconfound $u_{\\pi}$ induced by the behavior policies and mitigate the model objective mismatch. ", "page_idx": 3}, {"type": "text", "text": "As discussed in Definition 3, we only need to optimize the part of the parameters of the causal graph $G$ , i.e. $M$ . Thus, we can remove the constraints in (4), then transform the original causal discovery problem into a regularized MLE problem as follows: ", "page_idx": 3}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/38e4369f37ed2c3c984e5caf0abcfb36cf8da40afe58bdd7c3c6e00e0dc84cfc.jpg", "img_caption": ["Figure 3: BECAUSE learns a causality-aware representation from the buffer and uses it in both the world model and uncertainty quantification to obtain a pessimistic planning policy. "], "img_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{M}{\\operatorname*{min}}\\,\\mathcal{L}_{\\operatorname*{max}}(M)=\\underset{M}{\\operatorname*{min}}\\,(-\\log p(\\mathcal{D};\\phi,\\mu,M)+\\lambda|M|)}&{}\\\\ {=\\underset{M}{\\operatorname*{min}}\\,\\big(\\underbrace{\\mathbb{E}_{(s,a,s^{\\prime})\\in\\mathcal{D}}\\|\\mu^{T}(s^{\\prime})K_{\\mu}^{-1}-\\phi^{T}(s,a)M\\|_{2}^{2}}_{\\mathrm{Ward.Al.add.I.}}}&{+\\underset{\\mathrm{Soursiv}\\,\\mathrm{Reoularization}}{\\underbrace{\\lambda\\|M\\|_{\\mathcal{A}}}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{K_{\\mu}:=\\sum_{s^{\\prime}\\in{\\cal S}}\\mu(s^{\\prime})\\mu(s^{\\prime})^{T}}\\end{array}$ is an invertible matrix. The derivation of Equation (5) is elaborated in Appendix A.4. In practice, we use the $\\chi^{2}$ -test for discrete state and action space and the fast ", "page_idx": 3}, {"type": "text", "text": "Conditional Independent Test (CIT) [27] for continuous variables to estimate each entry in the core matrix $M$ . We regularize the sparsity of $M$ by controlling the $p$ -value threshold in CIT and provide a more detailed implementation in Appendix C.1. ", "page_idx": 4}, {"type": "text", "text": "Estimating the core mask provides a more accurate relationship between state and action representations, and we further refine the state action representation function $\\phi$ and $\\mu$ to help capture more accurate transition dynamics. We optimize them by solving the following problem, according to the transition model loss and spectral norm regularization [28] to satisfy the regularity constraints of the feature in Assumption 3: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi,\\mu}\\mathcal{L}_{\\mathrm{rep}}(\\phi,\\mu)=\\operatorname*{min}_{\\phi,\\mu}\\mathbb{E}_{(s,a,s^{\\prime})\\in\\mathcal{D}}\\|\\mu^{T}(s^{\\prime})K_{\\mu}^{-1}-\\phi^{T}(s,a)M\\|_{2}^{2}+\\lambda_{\\phi}\\|\\phi\\|_{2}+\\lambda_{\\mu}\\|\\mu\\|_{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The world model learning process is illustrated in Figure 3. The estimation of individual $M(u_{c})$ mitigates the spurious correlation brought by $u_{c}$ . To further deal with the spurious correlation in $u_{\\pi}$ induced by the behavior policy $\\pi_{\\beta}(a|s,u_{\\pi})$ , we utilize the conditional independence property in the ASC-MDP shown in Equation (3). The following equation shows that the true unconfounded transition $T$ can be rewritten in the reweighting formulas. This reweighting process in mask $M$ serves as the soft intervention approach [18, 29] to estimate the treatment effect in the transition function $T$ in an unconfounded way: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(s^{\\prime}|s,a)=\\frac{\\mathbb{E}_{p_{u}}\\left[\\phi(s,a)^{T}M(u_{c})\\mu(s^{\\prime})\\cdot\\pi_{\\beta}(a|s,u_{\\pi})\\right]}{\\mathbb{E}_{p_{u}}\\pi_{\\beta}\\left(a|s,u_{\\pi}\\right)}}\\\\ &{\\qquad\\qquad=\\phi(s,a)^{T}\\left[\\frac{\\mathbb{E}_{p_{u}}\\left[M(u_{c})\\pi_{\\beta}(a|s,u_{\\pi})\\right]}{\\mathbb{E}_{p_{u}}\\pi_{\\beta}(a|s,u_{\\pi})}\\right]\\mu(s^{\\prime})\\triangleq\\phi(s,a)^{T}\\,\\overline{{M}}(u)\\mu(s^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The derivation of Equation (7) is illustrated in Appendix A.5. Equation (7) basically shows a re-weighting process given the empirical estimation of $M(u_{c})$ in every batch of trajectories: Epu[M(u\u03c0c)(\u03c0a|\u03b2s(,au|s,)u\u03c0)]. Compared to the general reweighting strategies in previous MBRL literatures [18, 29] which reweights the entire value function, this re-weighting process is conducted only on the estimated matrix, while the representation $\\phi(s,a)$ and $\\mu(s^{\\prime})$ are subsequently regularized by weighted estimation of $\\overline{{M}}$ . The pipeline of causal world model learning is described in the first part of the Algorithm 1. We discuss more details of the implementation and experiment in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "3.3 Causal Representation for Uncertainty Quantification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To avoid entering OOD states in the online deployment, we further design a pessimistic planner according to the uncertainty of the predicted trajectories in the imagination rollout step to mitigate objective mismatch. ", "page_idx": 4}, {"type": "text", "text": "We use the feature embedding from bilinear causal representation to help quantify the uncertainty, denoted as $E_{\\theta}(s,a)$ . As we have access to the offline dataset, we learn an Energybased Model (EBM) [30, 31] based on the abstracted state representation $\\phi$ and core matrix $M$ . A higher output of the energy function $\\bar{E_{\\theta}}(\\cdot,\\cdot)$ indicates a higher uncertainty in the current state as they are visited by the behavior policies $\\pi_{\\beta}$ less frequently. In practice, the energy-based model usually suffers from a high-dimensional data space [32]. To mitigate this overhead of training a good uncertainty quantifier, we first embed the state ", "page_idx": 4}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/d14e747eec7eab5c0937f8d418c4a514f5eae62faa60d6d75e2913a42de3e550.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "samples through the abstract representation $\\mu(s^{\\prime})$ , and the state action pair via $\\phi(s,a)$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathtt{E B M}}(\\theta)=\\mathbb{E}_{\\hat{T}(\\cdot|s,a)}E_{\\theta}[\\mu(s^{+})|\\phi(s,a)]-\\mathbb{E}_{q(s,a)}E_{\\theta}[\\mu(s^{-})|\\phi(s,a)]+\\lambda_{\\mathtt{E B M}}\\|\\theta\\|_{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu(s)^{+}$ refers to the positive samples from the approximated transition dynamics $\\widehat{T}(\\cdot|s,a)$ , and $\\mu(s^{-})$ refers to the latent negative samples via the Langevin dynamics [30]. Additionally, we regularize the parameters of EBM to avoid overfitting issues. We attach more training details and results of EBMs in Appendix C.2 The learned energy function $E_{\\theta}(s,a)$ is used to quantify the uncertainty based on the offline data. ", "page_idx": 5}, {"type": "text", "text": "During the online planning stage, we use the learned EBM to adjust the reward estimation based on Model Predictive Control (MPC) [33]. At timestep $h$ , we basically subtract the original step return estimation $r_{h}(s,a)$ by its uncertainty $E_{\\theta}(s,a)$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{Q}}_{h}(s,a)=\\widehat{Q}_{h}(s,a)-E_{\\theta}(s,a)=\\underbrace{r_{h}(s,a)-E_{\\theta}(s,a)}_{\\mathrm{Adjusted~Return}}+\\sum_{s^{\\prime}\\in\\mathcal{S}}\\widehat{T}(s^{\\prime}|s,a)\\widehat{V}_{h+1}(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.4 Theoretical Analysis of BECAUSE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Then we move on to develop the theoretical analysis for the proposed method BECAUSE. Based on two standard Assumption 2 and 3 on the feature\u2019s existence and regularity, we achieve the finite-sample complexity guarantee \u2014 an upper bound of the suboptimality gap as follows, whose proof is postponed to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Performance guarantee). Consider any $0<\\delta<1$ and any initial state $\\widetilde s\\in S$ . Under the Assumption 2, 3 and that the transition model $T$ is an SCM (defined in 4), for any  a ccuracy level $0\\leq\\xi\\leq1$ , with probability at least $1-\\delta$ , the output policy $\\pi$ of BECAUSE (Algorithm 1) based on the historical dataset $\\mathcal{D}$ with $\\begin{array}{r}{n=\\sum_{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}n(s,a)}\\end{array}$ samples generated from a behavior policy $\\pi_{\\beta}$ satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{1}^{*}(\\widetilde{s})-V_{1}^{\\pi}(\\widetilde{s})\\lesssim\\operatorname*{min}\\big\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal{S}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\big)\\big\\}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\sqrt{\\frac{\\log(1/\\delta)}{n(s_{h},a_{h})}}\\ |\\ s_{1}=\\widetilde{s}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C_{1},C_{s}$ are some universal constants, $\\sigma$ is SCM\u2019s noise level (see Definition 4), and $M\\in\\mathbb{R}^{d\\times d^{\\prime}}$ is the optimal ground truth sparse transition matrix to be estimated. ", "page_idx": 5}, {"type": "text", "text": "The error bound shrinks as the offilne sample size $n$ over all state-action pairs increase. It also grows proportionally to the planning horizon $H$ , SCM\u2019s noise level $\\sigma$ , and the $\\ell_{0}$ norm of the ground true causal mask $M$ , which describes the intrinsic complexity of the world model. ", "page_idx": 5}, {"type": "text", "text": "Consequently, with Proposition 1 in the Appendix, we can achieve $\\xi$ -optimal policy $(V_{1}^{*}(\\widetilde{s})\\!-\\!V_{1}^{\\pi}(\\widetilde{s})\\leq$ $\\xi$ ) as long as the historical dataset satisfies the following conditions: $\\forall\\,0\\leq\\xi\\leq1$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{s,a,h\\in S\\times A\\times[H]}}\\mathbb{E}_{\\pi^{*}}\\left[n(s_{h},a_{h})\\mid s_{1}=\\widetilde s\\right]\\gtrsim\\frac{\\operatorname*{min}\\left\\{C_{1}^{2}\\log^{2}\\left(\\frac{\\|M\\|_{\\infty}}{\\xi}\\right)|S|,C_{s}^{2}\\sigma^{2}\\|M\\|_{0}\\right\\}\\cdot H^{2}\\log(1/\\delta)}{\\xi^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Experiment Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct a comprehensive empirical evaluation of BECAUSE\u2019s generalization performance in a diverse set of environments, covering different decision-making problems in the grid world, manipulation, and autonomous driving domains, shown in Figure 4. ", "page_idx": 5}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/d1776b28155ad4e8466a8848dcac86de93288c83d96ba84417ef382c23137f90.jpg", "img_caption": ["Figure 4: Three environments used in this paper. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Environment Design We design 18 tasks in 3 representative RL environments in Figure 4. Agents need to acquire reasoning capabilities to receive higher rewards and achieve goals. ", "page_idx": 5}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/5e6470b4c71b0f92324f4f4e3c61f46ededd945cad3c39bd021b90c2d303d46a.jpg", "img_caption": ["Figure 5: Results of BECAUSE and baselines in different tasks. (a) Average success rate in distribution and out of distribution. (b) Average success rate w.r.t. ratio of offline samples. (c) Average success rate w.r.t. spurious level in the environments. We evaluate the mean and standard deviation of the best performance among 10 random seeds and report task-wise results in Appendix Table 6. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "\u2022 Lift: Object manipulation environment in RoboSuite [34]. We designed this environment for the agent to lift an object with a specific color configuration on the table to a desired height. In the OOD environment Lift-O, there is an injected spurious correlation between the color of the cube and the position of the cube in the training phase. During the testing phase, the correlation between color and position is different from training. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Unlock: We designed this environment for the agent to collect a key to open doors in Minigrid [35]. In the OOD environment Unlock-O, there will be a different number of goals (doors to be opened) in the testing environments from the training environments. \u2022 Crash: Safety is critical in autonomous driving, which is reflected by the collision avoidance capability. We consider a risky scenario where an AV collides with a jaywalker because its view is blocked by another car [36]. We design such a crash scenario based on highway-env [37], where the goal is to create crashes between a pedestrian and AVs. In the OOD environment Crash- $o$ , the distribution of reward (number of pedestrians) is different in online testing environments. ", "page_idx": 6}, {"type": "text", "text": "For all three different environments, we set a specific subset of the state space as the goal $g\\in S$ , and the reward is defined as the goal-reaching reward $r(s,a,g)=\\mathbb{I}(r=g)$ . When the episode ends in the goal state within the task horizon $H$ , the episode is considered a success. We then use the average success rate as the general evaluation metrics for our BECAUSE and all baselines. ", "page_idx": 6}, {"type": "text", "text": "In each environment, we collect three types of offline data: random, medium, and expert based on the different levels of $u_{\\pi}$ in the behavior policies. In Unlock environments, we collect 200 episodes from each level of behavior policies as the offilne demonstration data, and the number of episodes is 1,000 in the environments $L i f t$ and Crash, which all have continuous state and action space. A more detailed view of the environment hyperparameters and behavior policy design is in Appendix C.5. ", "page_idx": 6}, {"type": "text", "text": "Baselines We compare our proposed BECAUSE with several offilne causal RL or MBRL baselines. ICIL [38] learns a dynamic-aware invariant causal representation learning to assist a generalizable policy learning from offline datasets. CCIL [39] conducts a soft intervention in our offline setting by jointly optimizing policy parameters and masks over the state. MnM [9] unifies the objective of jointly training the model and policy, which allocates larger weights in the state prediction loss in the high-reward region. Delphic [40] introduces delphic uncertainty to differentiate between uncertainties caused by hidden confounders and traditional epistemic and aleatoric uncertainties. $\\mathbf{TD3+BC}$ [41] is an offline model-free RL approach that combines the Twin Delayed Deep Deterministic Policy ", "page_idx": 6}, {"type": "text", "text": "Gradient (TD3) algorithm with Behavior Cloning (BC) to adopt both the actor-critic framework and supervised learning from expert demonstrations. MOPO [2] is an offline MBRL approach that uses flat latent space and count-based uncertainty quantification to maintain conservatism in online deployment. GNN [42] is a GNN-based baseline using a Relational Graph Convolutional Network to model the temporal dependency of state-action pairs in the dynamic model with message passing. CDL [24] uses causal discovery to learn a task-independent world model. Denoised MDP [12] and IFactor [13] conduct causal state abstraction based on their controllability and reward relevance. The last three methods are designed for online settings, so we only implement their model learning objectives. We attach more details of the baseline implementation in Appendix C.6. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experiment Results Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We empirically answer the following research questions. ", "page_idx": 7}, {"type": "text", "text": "\u2022 RQ1: How is the generalizability of BECAUSE in the online environments (which may be unseen)? Specifically, how does BECAUSE perform under diverse qualities of demonstration data (different level of $u_{\\pi}$ ), and different environment contexts (different $u_{c}$ )?   \n\u2022 RQ2: How does the design in BECAUSE contribute to the robustness of its final performance under different sample sizes or spurious levels?   \n\u2022 RQ3: How does BECAUSE scale up to visual RL tasks with image observation input compared to other visual RL baselines?   \n\u2022 RQ4: How does BECAUSE achieve the aforementioned generalizability by mitigating the objective mismatch problem in offline MBRL? ", "page_idx": 7}, {"type": "text", "text": "For RQ1, in Figure 5(a), we evaluate the success rate in the online environment against different baselines. The result shows that under different environments and different qualities of behavior policies $\\pi_{\\beta}$ (different $u_{\\pi}$ ), BECAUSE consistently achieves the best performance in 8 out of 9 for both the in-distribution (I) and out-of-distribution (O) for all the demonstration data quality (different level of $u_{\\pi}$ ). Where O here indicates the tasks under unseen environment with confounder $u_{c}^{\\prime}\\neq u_{c}$ different from offline training. Another finding is that model-based approaches generally perform better than model-free approaches at various levels of offline data, which shows the importance of world model learning for generalizable offline RL. We attach the detailed results in the Appendix Table 6, 7 and the causal masks discovered in each environment in Appendix Figure 8 for reference. ", "page_idx": 7}, {"type": "text", "text": "For RQ2, we compare different aspects of BECAUSE\u2019s robustness with MOPO without causal structures [2]. We compare their performance with different ratios of the entire offline dataset and illustrate the success rates in Figure 5(b). The result shows that, for any selected number of samples, BECAUSE consistently outperforms MOPO with a clear margin. We also evaluate BECAUSE performance at higher spurious levels in Figure 5(c). We add up to $8\\times$ of the original number of confounders in the environments to test the robustness of the agent\u2019s performance. BECAUSE consistently outperforms MOPO and the margin enlarges as the spurious level grows higher. ", "page_idx": 7}, {"type": "text", "text": "For RQ3, we conduct experiments with visual inputs in the Unlock environments with ICIL [38] and IFactor [13]. We parameterize the feature encoder as a three-layer CNN with 128 dimensions hidden size for all the baselines and our methods. The results in Table 1 show that BECAUSE can significantly improve both in-distribution and out-of-distribution performance under different quality of behavior policies. ", "page_idx": 7}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/a75f81daa47b2f2a3b3810eedda15fafda7962f71d3d120b5a8f1930ac505a09.jpg", "table_caption": ["Table 1: Comparison of visual RL performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "For RQ4, we aim to understand whether BECAUSE achieves higher performance by resolving the objective mismatch problem. We first collect two groups of trajectories: $\\tau_{p o s}$ and $\\tau_{n e g}$ , each with positive reward (success) and negative reward (failure) in Unlock task with sparse goal-reaching reward. We want to have a model whose loss is informative for discriminating control results, that is, we wish $\\mathcal{L}_{m o d e l}(\\tau_{p o s})\\,<\\,\\mathcal{L}_{m o d e l}(\\tau_{n e g})$ . According to our visualization in Figure 6, in Unlock-Expert and Unlock-Medium, the ratio of $\\tau_{p o s}$ is much higher in BECAUSE than MOPO among the trajectories with low model loss. In Unlock-Random, the mismatch of the model and control objective is more significant, since the demonstration is poor in state coverage. MOPO cannot succeed even when the model loss is low, whereas our methods can. We perform a hypothesis test with $H_{0}:\\mathcal{L}_{m o d e l}(\\tau_{p o s})<\\mathcal{L}_{m o d e l}(\\tau_{n e g})$ . In BECAUSE, this desired property is more significant than MOPO attributed to the causal representation we learn, indicating a reduction of objective mismatch. We attach detailed discussions for the mismatch evaluation in Appendix C.3 and Table 5. ", "page_idx": 7}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/2604be520c1736f5a357ac66e2634482ca36113cd1b1cc68b2190e5b46774255.jpg", "img_caption": ["Figure 6: Evaluation of the difference between the distribution of episodic model loss for success and failure trajectories. The higher difference indicates a reduction in model mismatch issues. An example of failure mode is trying to open the door without having the key. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted ablation studies with three variants of BECAUSE and report the average success rate across nine in-distribution and nine out-of-distribution tasks in Table 2. The Optimism variant conducts optimistic planning instead of pessimistic planning in Equation (9), which uses uniform sampling in the planner module. The Linear variant assumes a full connection to the causal matrix $M$ , then directly uses ", "page_idx": 8}, {"type": "text", "text": "Table 2: The ablation studies between BECAUSE and its variants. We report the overall Success rate $(\\%)$ over 9 in-distribution (I) and 9 out-of-distribution (O) tasks, respectively. Bold is the best. ", "page_idx": 8}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/3de3239aa8e083d6703cf5b00b46aed761d5ac41b825da5b20e8f829656f5bcf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "linear MDP to parameterize the dynamics model $T$ , which removes the causal discovery module in BECAUSE. The Full variant learns from the full batch of data to estimate the causal mask without iterative update. We report the results of the task-wise ablation with confidence interval and significance in Appendix Table 8 and 9. ", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Objective Mismatch in MBRL The objective mismatch in MBRL [43, 44] refers to the fact that pure MLE estimation of the world model does not align well with the control objective. Previous works [29, 45] propose reweighting during model training to alleviate this mismatch, [46] proposes a goal-aware prediction by redistributing model error according to their task relevance. These works essentially reweight loss for the entire model training, while our work conducts reweighting just over the estimated causal mask more efficiently. More recently, [9, 10] proposed a joint training between the world model and policies. Although joint optimization improves performance, they do not address the generalizability of the learned model under the distribution shift setting. In the offilne setting, Model-based RL [2, 3, 4, 47] employs model ensemble, pessimistic policy optimization or value iteration [48, 49], and an energy-based model for planning [50] to quantify uncertainty and improve test performance. To the best of our knowledge, no previous work explored or modeled the impact of distribution shift on the objective mismatch problem in MBRL. ", "page_idx": 8}, {"type": "text", "text": "Causal Discovery with Confounder Most of the existing causal discovery methods [51] can be categorized into constraint-based and score-based. Constraint-based methods [52] start from a complete graph and iteratively remove edges with statistical hypothesis testing [53, 54]. This type of method is highly data-efficient but not robust to noisy data. As a remedy, score-based methods [55, 56] use metrics such as the likelihood or BIC [57] as scores to manipulate edges in the causal graph. Recently, researchers have extended score-based methods with RL [58], order learning [59] or differentiable discovery [22, 60, 61]. To alleviate the non-identifiability under hidden confounders, active intervention methods have been explored [62], aiming to break spurious correlations in an online fashion. With extra assumptions on confounders, some recent works detect such correlations [63, 64, 65] so that models can effectively identify elusive confounders. ", "page_idx": 9}, {"type": "text", "text": "Causal Reinforcement Learning Recently, many RL algorithms have incorporated causality to improve reasoning capability [66] and generalizability. For instance, [67] and [68] explicitly estimate causal structures with the interventional data obtained from the environment in an online setting. These structures can be used to constrain the output space [19] or to adjust the buffer priority [69]. Building dynamic models in model-based RL [24, 70, 71] based on causal graphs is widely studied. Most existing causal MBRL works focus on estimating the causal world model by predicting transition dynamics and rewards. Existing methods learn this causal world model via structural regularization [23, 72, 73], conditional independence test [24, 70, 74], variational inference [75, 12], counterfactual data augmentation [76, 77], hierarchical skill abstraction [78, 79], uncertainty quantification [40], reward redistribution [24, 80], causal context modeling [81, 82] and structure-aware state abstraction [12, 13, 83, 84] based on the controllability and task or reward relevance. However, the presence of confounders during data collection can skew the learned policy, making it susceptible to spurious correlations. Deconfounding solutions have been proposed either between actions and states [39, 85, 86] or among different dimensions of state variables [19, 87]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study how to mitigate the objective mismatch problem in MBRL, especially under the offline settings where distribution shift occurs. We first propose ASC-MDP and the bilinear causal representation associated with it. Based on the formulation, we proposed how to learn this causal abstraction by alternating between causal mask learning and feature learning in fitting the world dynamics. In the planning stage, we applied the learned causal representation to an uncertainty quantification module based on EBM, which improves the robustness under uncertainty in the online planning stage. We theoretically justify BECAUSE\u2019s sub-optimality bound induced by the sparse matrix estimation problem and offline RL. Comprehensive experiments on 18 different tasks show that given a diverse level of demonstration as the offline dataset, BECAUSE has better generalizability than baselines in different online environments, and it robustly outperforms baselines under different spurious levels or sample sizes. We empirically show that BECAUSE mitigates the objective mismatch with causal awareness learned from offline data. One limitation of BECAUSE lies in its simplified assumption of time-homogeneous causal structure, which may not always hold in long-horizon or non-stationary settings. Besides, the current implementation is still based on vector observations. It will be interesting to scale up the causal reasoning framework into high-dimensional observations to discover concept factors in long-horizon visual RL settings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is partially supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR00112320012. The work of L. Shi is supported in part by the Resnick Institute and Computing, Data, and Society Postdoctoral Fellowship at California Institute of Technology. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[2] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142, 2020.   \n[3] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 21810\u201321823. Curran Associates, Inc., 2020.   \n[4] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in neural information processing systems, 34:28954\u201328967, 2021.   \n[5] Zeyu Zhu and Huijing Zhao. A survey of deep rl and il for autonomous driving policy learning. IEEE Transactions on Intelligent Transportation Systems, 23(9):14043\u201314065, 2021.   \n[6] Aviral Kumar, Anikait Singh, Frederik Ebert, Mitsuhiko Nakamoto, Yanlai Yang, Chelsea Finn, and Sergey Levine. Pre-training for robots: Offilne rl enables learning new tasks from a handful of trials. arXiv preprint arXiv:2210.05178, 2022.   \n[7] Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerations for healthcare settings. In Machine Learning for Healthcare Conference, pages 2\u201335. PMLR, 2021.   \n[8] Laixi Shi and Yuejie Chi. Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. arXiv preprint arXiv:2208.05767, 2022.   \n[9] Benjamin Eysenbach, Alexander Khazatsky, Sergey Levine, and Russ R Salakhutdinov. Mismatched no more: Joint model-policy optimization for model-based rl. Advances in Neural Information Processing Systems, 35:23230\u201323243, 2022.   \n[10] Shentao Yang, Shujian Zhang, Yihao Feng, and Mingyuan Zhou. A unified framework for alternating offilne model training and policy learning. Advances in Neural Information Processing Systems, 35:17216\u201317232, 2022.   \n[11] Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.   \n[12] Tongzhou Wang, Simon S Du, Antonio Torralba, Phillip Isola, Amy Zhang, and Yuandong Tian. Denoised mdps: Learning world models better than the world itself. arXiv preprint arXiv:2206.15477, 2022.   \n[13] Yu-Ren Liu, Biwei Huang, Zhengmao Zhu, Honglong Tian, Mingming Gong, Yang Yu, and Kun Zhang. Learning world models with identifiable factorization. arXiv preprint arXiv:2306.06561, 2023.   \n[14] Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In International Conference on Machine Learning, pages 10746\u201310756. PMLR, 2020.   \n[15] Weitong Zhang, Jiafan He, Dongruo Zhou, Q Gu, and A Zhang. Provably efficient representation selection in low-rank markov decision processes: from online to offline rl. In Uncertainty in Artificial Intelligence, pages 2488\u20132497. PMLR, 2023.   \n[16] Fan Feng and Sara Magliacane. Learning dynamic attribute-factored world models for efficient multi-object reinforcement learning. arXiv preprint arXiv:2307.09205, 2023.   \n[17] Angela Zhou. Reward-relevance-filtered linear offilne reinforcement learning. arXiv preprint arXiv:2401.12934, 2024.   \n[18] Lingxiao Wang, Zhuoran Yang, and Zhaoran Wang. Provably efficient causal reinforcement learning with confounded observational data. Advances in Neural Information Processing Systems, 34:21164\u201321175, 2021.   \n[19] Wenhao Ding, Laixi Shi, Yuejie Chi, and Ding Zhao. Seeing is not believing: Robust reinforcement learning against spurious correlation. arXiv preprint arXiv:2307.07907, 2023.   \n[20] Junzhe Zhang and Elias Bareinboim. Markov decision processes with unobserved confounders: A causal approach. Technical report, Technical report, Technical Report R-23, Purdue AI Lab, 2016.   \n[21] Karren Yang, Abigail Katcoff, and Caroline Uhler. Characterizing and learning equivalence classes of causal dags under interventions. In International Conference on Machine Learning, pages 5541\u20135550. PMLR, 2018.   \n[22] Philippe Brouillard, S\u00e9bastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable causal discovery from interventional data. Advances in Neural Information Processing Systems, 33:21865\u201321877, 2020.   \n[23] Zizhao Wang, Xuesu Xiao, Yuke Zhu, and Peter Stone. Task-independent causal state abstraction. Workshop on Robot Learning: Self-Supervised and Lifelong Learning, NeurIPS, 2021.   \n[24] Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal dynamics learning for task-independent state abstraction. arXiv preprint arXiv:2206.13452, 2022.   \n[25] Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block mdps. In International Conference on Machine Learning, pages 11214\u201311224. PMLR, 2020.   \n[26] Wenxuan Zhu, Chao Yu, and Qiang Zhang. Causal deep reinforcement learning using observational data. arXiv preprint arXiv:2211.15355, 2022.   \n[27] Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Fast conditional independence test for vector variables with large sample sizes. arXiv preprint arXiv:1804.02747, 2018.   \n[28] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.   \n[29] Nathan Lambert, Brandon Amos, Omry Yadan, and Roberto Calandra. Objective mismatch in model-based reinforcement learning. arXiv preprint arXiv:2002.04523, 2020.   \n[30] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in Neural Information Processing Systems, 32, 2019.   \n[31] Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu, and Dongsheng Li. Energy-based open-world uncertainty modeling for confidence calibration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9302\u20139311, 2021.   \n[32] Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space energy-based prior model. Advances in Neural Information Processing Systems, 33:21994\u2013 22008, 2020.   \n[33] Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer science & business media, 2013.   \n[34] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Mart\u00edn-Mart\u00edn, Abhishek Joshi, Soroush Nasiriany, and Yifeng Zhu. robosuite: A modular simulation framework and benchmark for robot learning. In arXiv preprint arXiv:2009.12293, 2020.   \n[35] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.   \n[36] Zenna Tavares, James Koppel, Xin Zhang, Ria Das, and Armando Solar-Lezama. A language for counterfactual generative models. In International Conference on Machine Learning, pages 10173\u201310182. PMLR, 2021.   \n[37] Edouard Leurent. An environment for autonomous driving decision-making. https://github. com/eleurent/highway-env, 2018.   \n[38] Ioana Bica, Daniel Jarrett, and Mihaela van der Schaar. Invariant causal imitation learning for generalizable policies. Advances in Neural Information Processing Systems, 34:3952\u20133964, 2021.   \n[39] Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[40] Aliz\u00e9e Pace, Hugo Y\u00e8che, Bernhard Sch\u00f6lkopf, Gunnar Ratsch, and Guy Tennenholtz. Delphic offline reinforcement learning under nonidentifiable hidden confounding. In The Twelfth International Conference on Learning Representations, 2023.   \n[41] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[42] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pages 593\u2013607. Springer, 2018.   \n[43] Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for model-based reinforcement learning. In Artificial Intelligence and Statistics, pages 1486\u20131494. PMLR, 2017.   \n[44] Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. arXiv preprint arXiv:1807.03858, 2018.   \n[45] Pierluca D\u2019Oro, Alberto Maria Metelli, Andrea Tirinzoni, Matteo Papini, and Marcello Restelli. Gradient-aware model-based policy search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3801\u20133808, 2020.   \n[46] Suraj Nair, Silvio Savarese, and Chelsea Finn. Goal-aware prediction: Learning to model what matters. In International Conference on Machine Learning, pages 7207\u20137219. PMLR, 2020.   \n[47] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. Model-bellman inconsistency for model-based offline reinforcement learning. In International Conference on Machine Learning, pages 33177\u201333194. PMLR, 2023.   \n[48] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137\u2013 2143. PMLR, 2020.   \n[49] Masatoshi Uehara and Wen Sun. Pessimistic model-based offilne reinforcement learning under partial coverage. arXiv preprint arXiv:2107.06226, 2021.   \n[50] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Learning iterative reasoning through energy minimization. In International Conference on Machine Learning, pages 5570\u2013 5582. PMLR, 2022.   \n[51] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. Frontiers in genetics, 10:524, 2019.   \n[52] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.   \n[53] Karl Pearson. X. on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 50(302):157\u2013175, 1900.   \n[54] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Kernel-based conditional independence test and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.   \n[55] David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507\u2013554, 2002.   \n[56] Alain Hauser and Peter B\u00fchlmann. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. The Journal of Machine Learning Research, 13(1):2409\u20132464, 2012.   \n[57] Andrew A Neath and Joseph E Cavanaugh. The bayesian information criterion: background, derivation, and applications. Wiley Interdisciplinary Reviews: Computational Statistics, 4(2):199\u2013203, 2012.   \n[58] Shengyu Zhu, Ignavier $\\mathrm{Ng}$ , and Zhitang Chen. Causal discovery with reinforcement learning. arXiv preprint arXiv:1906.04477, 2019.   \n[59] Dezhi Yang, Guoxian Yu, Jun Wang, Zhengtian Wu, and Maozu Guo. Reinforcement causal structure learning on order graph. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10737\u201310744, 2023.   \n[60] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Sch\u00f6lkopf, Michael C Mozer, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprint arXiv:1910.01075, 2019.   \n[61] Yunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox, and Animesh Garg. Causal discovery in physical systems from videos. Advances in Neural Information Processing Systems, 33:9180\u20139192, 2020.   \n[62] Nino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard Sch\u00f6lkopf, Michael C Mozer, Yoshua Bengio, Stefan Bauer, and Nan Rosemary Ke. Learning neural causal models with active interventions. arXiv preprint arXiv:2109.02429, 2021.   \n[63] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don\u2019t take the easy way out: Ensemblebased methods for avoiding known dataset biases. arXiv preprint arXiv:1909.03683, 2019.   \n[64] Divyansh Kaushik, Eduard Hovy, and Zachary C Lipton. Learning the difference that makes a difference with counterfactually-augmented data. arXiv preprint arXiv:1909.12434, 2019.   \n[65] Meike Nauta, Ricky Walsh, Adam Dubowski, and Christin Seifert. Uncovering and correcting shortcut learning in machine learning models for skin cancer diagnosis. Diagnostics, 12(1):40, 2021.   \n[66] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable reinforcement learning through a causal lens. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 2493\u20132500, 2020.   \n[67] Suraj Nair, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Causal induction from visual observations for goal directed tasks. arXiv preprint arXiv:1910.01751, 2019.   \n[68] Sergei Volodin, Nevan Wichers, and Jeremy Nixon. Resolving spurious correlations in causal models of environments via interventions. arXiv preprint arXiv:2002.05217, 2020.   \n[69] Maximilian Seitzer, Bernhard Sch\u00f6lkopf, and Georg Martius. Causal influence detection for improving efficiency in reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.   \n[70] Zheng-Mao Zhu, Xiong-Hui Chen, Hong-Long Tian, Kun Zhang, and Yang Yu. Offline reinforcement learning with causal structured world models. arXiv preprint arXiv:2206.01474, 2022.   \n[71] Mirco Mutti, Riccardo De Santi, Emanuele Rossi, Juan Felipe Calderon, Michael Bronstein, and Marcello Restelli. Provably efficient causal model-based reinforcement learning for systematic generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 9251\u20139259, 2023.   \n[72] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for nonstationary reinforcement learning. Advances in Neural Information Processing Systems, 35:31957\u201331971, 2022.   \n[73] Tianying Ji, Yongyuan Liang, Yan Zeng, Yu Luo, Guowei Xu, Jiawei Guo, Ruijie Zheng, Furong Huang, Fuchun Sun, and Huazhe Xu. Ace: Off-policy actor-critic with causality-aware entropy regularization. arXiv preprint arXiv:2402.14528, 2024.   \n[74] Jiaheng Hu, Zizhao Wang, Peter Stone, and Roberto Martin-Martin. Elden: Exploration via local dependencies. arXiv preprint arXiv:2310.08702, 2023.   \n[75] Wenhao Ding, Haohong Lin, Bo Li, and Ding Zhao. Generalizing goal-conditioned reinforcement learning with variational causal reasoning. Advances in Neural Information Processing Systems, 35:26532\u201326548, 2022.   \n[76] Lars Buesing, Theophane Weber, Yori Zwols, Nicolas Heess, Sebastien Racaniere, Arthur Guez, and Jean-Baptiste Lespiau. Woulda, coulda, shoulda: Counterfactually-guided policy search. In International Conference on Learning Representations, 2018.   \n[77] Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based counterfactual data augmentation. arXiv preprint arXiv:2210.11287, 2022.   \n[78] Tabitha E Lee, Jialiang Alan Zhao, Amrita S Sawhney, Siddharth Girdhar, and Oliver Kroemer. Causal reasoning in simulation for structure and transfer learning of robot manipulation policies. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 4776\u20134782. IEEE, 2021.   \n[79] Tabitha Edith Lee, Shivam Vats, Siddharth Girdhar, and Oliver Kroemer. Scale: Causal learning and discovery of robot manipulation skills using simulation. In Conference on Robot Learning, pages 2229\u20132256. PMLR, 2023.   \n[80] Yudi Zhang, Yali Du, Biwei Huang, Ziyan Wang, Jun Wang, Meng Fang, and Mykola Pechenizkiy. Interpretable reward redistribution in reinforcement learning: A causal approach. Advances in Neural Information Processing Systems, 36, 2024.   \n[81] Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. Adarl: What, where, and how to adapt in transfer reinforcement learning. In International Conference on Learning Representations, 2021.   \n[82] Peide Huang, Xilun Zhang, Ziang Cao, Shiqi Liu, Mengdi Xu, Wenhao Ding, Jonathan Francis, Bingqing Chen, and Ding Zhao. What went wrong? closing the sim-to-real gap via differentiable causal discovery. In Conference on Robot Learning, pages 734\u2013760. PMLR, 2023.   \n[83] Xiang Fu, Ge Yang, Pulkit Agrawal, and Tommi Jaakkola. Learning task informed abstractions. In International Conference on Machine Learning, pages 3480\u20133491. PMLR, 2021.   \n[84] Biwei Huang, Chaochao Lu, Liu Leqi, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Clark Glymour, Bernhard Sch\u00f6lkopf, and Kun Zhang. Action-sufficient state representation learning for control with structural constraints. In International Conference on Machine Learning, pages 9260\u20139279. PMLR, 2022.   \n[85] Zhihong Deng, Zuyue Fu, Lingxiao Wang, Zhuoran Yang, Chenjia Bai, Zhaoran Wang, and Jing Jiang. Score: Spurious correlation reduction for offilne reinforcement learning. arXiv preprint arXiv:2110.12468, 2021.   \n[86] Maxime Gasse, Damien Grasset, Guillaume Gaudron, and Pierre-Yves Oudeyer. Using confounded data in latent model-based reinforcement learning. Transactions on Machine Learning Research, 2023.   \n[87] Junzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved confounders. Advances in neural information processing systems, 33:12263\u201312274, 2020.   \n[88] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.   \n[89] Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Applied and computational harmonic analysis, 27(3):265\u2013274, 2009.   \n[90] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.   \n[91] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems, 21, 2008.   \n[92] Ryan Tibshirani and Larry Wasserman. Sparsity, the lasso, and friends. Lecture notes from \u201cStatistical Machine Learning,\u201d Carnegie Mellon University, Spring, 2017.   \n[93] Patrick E McKnight and Julius Najab. Mann-whitney u test. The Corsini encyclopedia of psychology, pages 1\u20131, 2010. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Auxiliary Details of BECAUSE Framework ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Notation Summary ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We illustrate all the notations used in the main paper and appendix in Table 3. ", "page_idx": 16}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/fc3634bffbf808aea8101058a7fc0518f3b21301e454e9cd78a25c69df7bc553.jpg", "table_caption": ["Table 3: Notations used in this paper and their corresponding meanings. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.2 Equivalence of the Assumptions with Previous Works ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.3 Derivation of Definition 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The node of this causal graph $G=\\left[\\stackrel{0^{d\\times d}}{0^{d\\times d^{\\prime}}}\\right.\\quad\\left.M\\right]$ contains two groups of entities: (1) The state action abstraction $\\phi(s,a)$ , and (ii) the next state abstraction $\\mu(s^{\\prime})$ . We denote $\\phi(\\cdot,\\cdot)^{(i)}$ as the $i^{t h}$ ", "page_idx": 16}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/17735d38efe2610281ec0f7fe04b33c381676893c2dc95439327c24f0f05bc02.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "factor in the abstracted state action representations, and $\\mu(\\cdot)^{(j)}$ for the $j^{t h}$ factor in the abstracted state representations. ", "page_idx": 17}, {"type": "text", "text": "The source node of all the nodes $G$ is $\\phi(s,a)^{(i)}$ , which is the abstracted state-action representation, and the sink node of all the edges in $G$ is the to $\\mu(s)^{\\prime(i)}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T(s^{\\prime}|s,a)=\\left[\\phi(s,a)^{T}\\right.}&{{}\\!\\!\\mu(s^{\\prime})^{T}\\right]\\left[\\!\\!\\begin{array}{c c}{0^{d\\times d}}&{M}\\\\ {0^{d\\times d^{\\prime}}}&{0^{d\\times d}}\\end{array}\\!\\!\\right]\\left[\\!\\!\\begin{array}{c c}{\\phi(s,a)}\\\\ {\\mu(s^{\\prime})}\\end{array}\\!\\!\\right]=\\phi(s,a)^{T}M\\mu(s^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $G$ is a bipartite graph, since there will be no edges between $\\phi(s,a)^{(i)},\\phi(s,a)^{(j)}$ , or $\\mu(s)^{(i)},\\mu(s)^{(j)}$ . ", "page_idx": 17}, {"type": "text", "text": "Consequently, we show that $G\\in{\\mathrm{DAG}}$ . ", "page_idx": 17}, {"type": "text", "text": "A.4 Derivation of Equation (5) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Definition 4 (Structured Causal Model). An SCM $\\theta\\;:=\\;(S,{\\mathcal{E}})$ consists of a collection $\\boldsymbol{S}$ of $d$ functions [11], ", "page_idx": 17}, {"type": "equation", "text": "$$\ns_{j}:=f_{j}(\\mathbf{PA}^{G}(s_{j}),\\epsilon_{j}),\\;\\;j\\in[d],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ${\\bf{P A}}_{j}^{G}\\subset\\{s_{1},\\ldots,s_{d}\\}\\backslash\\{s_{j}\\}$ are called parents of $x_{j}$ in the Directed Acyclic Graph (DAG) $G$ , and $\\mathcal{E}=\\{\\epsilon_{i}\\}_{i=1}^{d}$ are jointly independent. For instance, in continuous state and action space, we parameterize the world model with joint Gaussian Distribution, i.e. $\\epsilon\\sim\\mathcal{N}(0,\\sigma I_{d d^{\\prime}})$ . ", "page_idx": 17}, {"type": "text", "text": "We then use bilinear MDP to approximate the original likelihood function in Equation (4), i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\np(\\mathcal{D};\\phi,\\mu,M)\\propto\\prod_{(s,a,s^{\\prime})\\in\\mathcal{D}}\\exp(-\\|\\mu^{T}(s^{\\prime})K_{\\mu}^{-1}-\\phi^{T}(s,a)M\\|_{2}^{2}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where K\u00b5 :=  s\u2032\u2208S \u00b5(s)\u00b5(s) is an invertible matrix. Then we can apply an MLE in Equation (5). ", "page_idx": 17}, {"type": "text", "text": "In our BECAUSE algorithm, the optimization of the causal world model is conducted by solving the regularized MLE problem in Equation (13). The biggest difference between BECAUSE and the offilne version of [14, 15] is that it aims to apply $\\ell_{0}$ regression instead of ridge regression to estimate matrix $M$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{n}=\\underset{M}{\\arg\\operatorname*{max}}\\ [\\log p(\\mathcal{D};\\phi,\\mu,M)-\\lambda|M|]}\\\\ &{\\quad=\\underset{M}{\\arg\\operatorname*{min}}\\ \\underset{(s,a,s^{\\prime})\\in\\mathcal{D}}{\\sum}\\ \\|\\mu^{T}(s^{\\prime})K_{\\mu}^{-1}-\\phi^{T}(s,a)M\\|_{2}^{2}+\\underset{\\mathrm{Sparsiv\\,Regularization}}{\\underbrace{\\lambda\\|M\\|_{0}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.5 Proof of Equation (7) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The derivation depends on the following re-weighting formula in [18]: ", "page_idx": 18}, {"type": "equation", "text": "$$\nT(s^{\\prime}|s,a)=\\frac{\\mathbb{E}_{p_{u}}T(s^{\\prime}|s,a,u)\\pi_{\\beta}(a|s,u_{\\pi})}{\\mathbb{E}_{p_{u}}\\pi_{\\beta}(a|s,u_{\\pi})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we apply equation (14) to the decomposition in Equation (2) and Equation (3), which yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(s^{\\prime}|s,a)=\\frac{\\mathbb{E}_{p_{u}}\\left[T(s^{\\prime}|s,a,u)\\pi_{\\beta}(a|s,u_{\\pi})\\right]}{\\mathbb{E}_{p_{u}}\\pi_{\\beta}(a|s,u_{\\pi})}}\\\\ &{\\phantom{\\sum_{p_{u}}}=\\frac{\\mathbb{E}_{p_{u}}\\left[\\phi(s,a)^{T}M(u_{c})\\mu(s^{\\prime})\\pi_{\\beta}(a|s,u_{\\pi})\\right]}{\\mathbb{E}_{p_{u}}\\left[\\pi_{\\beta}(a|s,u_{\\pi})\\right]}}\\\\ &{\\phantom{\\sum_{p_{u}}}=\\phi(s,a)^{T}\\left[\\frac{\\mathbb{E}_{p_{u}}\\left[M(u_{c})\\pi_{\\beta}(a|s,u_{\\pi})\\right]}{\\mathbb{E}_{p_{u}}\\left[\\pi_{\\beta}(a|s,u_{\\pi})\\right]}\\right]\\mu(s^{\\prime})}\\\\ &{\\phantom{\\sum_{p_{u}}}=\\phi(s,a)^{T}\\left[\\frac{\\mathbb{E}_{p_{u}}\\left[\\pi_{\\beta}(a|s,u_{\\pi})\\right]}{\\mathbb{E}_{p_{u}}\\left[\\pi_{\\beta}(a|s,u_{\\pi})\\right]}\\right]\\mu(s^{\\prime})}\\\\ &{\\phantom{\\sum_{p_{u}}}=\\phi(s,a)^{T}\\,\\overline{{M}}(u)\\mu(s^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last equality holds by letting $\\begin{array}{r}{\\overline{{M}}(u):=\\frac{\\mathbb{E}_{p_{u}}[M(u_{c})\\pi_{\\beta}(a|s,u_{\\pi})]}{\\mathbb{E}_{p_{u}}[\\pi_{\\beta}(a|s,u_{\\pi})]}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide the proof of the sub-optimality upper bound in Theorem 1. We first show some useful definitions and lemmas in Section B.1. Armed with them, we provide the theoretical results tailored for the causal discovery setting in Section B.2. Furthermore, we give a detailed proof of the uncertainty set form in our causal discovery problems in Section B.3. ", "page_idx": 18}, {"type": "text", "text": "B.1 Preliminary ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this subsection, we first define the $\\delta$ -uncertainty quantifier $\\Gamma$ , then we refer to the lemmas in the previous literature to construct a suboptimality bound based on the defined uncertainty quantifier $\\Gamma$ . ", "page_idx": 18}, {"type": "text", "text": "First, we define the Bellman operator $\\mathbb{B}_{h}$ , for some value function $V:S\\mapsto\\mathbb{R}$ , the Bellman operator can be defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathbb{B}_{h}V)(s,a)=\\mathbb{E}[r_{h}(s_{h},a_{h})+V(s_{h+1})|s_{h}=s,a_{h}=a].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we denote the approximate Bellman operator of the empirical MDP constructed from the offline dataset $\\mathcal{D}$ as ${\\widehat{\\mathbb{B}}}_{h}$ for any $h\\in[H]$ . ", "page_idx": 18}, {"type": "text", "text": "Definition 5 $\\boldsymbol{\\delta}$ -Uncertainty Quantifier). We let $\\{\\Gamma_{h}\\}_{h=1}^{H}$ , $\\Gamma_{h}:S\\times A\\mapsto\\mathbb{R}$ to be a $\\delta$ -uncertainty quantifier with respect to data distribution $P_{\\cal D}$ if the event: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{|(\\widehat{\\mathbb{B}}_{h}\\widehat{V}_{h+1})(s,a)-(\\mathbb{B}_{h}\\widehat{V}_{h+1})(s,a)|\\leq\\Gamma_{h}(s,a),\\forall(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "satisfies $\\mathbb{P}_{\\mathcal{D}}(\\mathcal{E})\\ge1-\\delta$ . ", "page_idx": 18}, {"type": "text", "text": "As we consider the offilne model learning and planning, we define the model evaluation error at each step $h\\in[H]$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall(s,a)\\in{\\mathcal{S}}\\times{\\mathcal{A}}:\\quad\\iota_{h}(s,a)=(\\mathbb{B}_{h}\\widehat V_{h+1})(s,a)-\\widehat Q_{h}(s,a),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\iota_{h}$ is the error induced by the approximate Bellman operator, especially the transition kernel based on $\\mathcal{D}$ . We then identify the source of sub-optimality in our offline MBRL setting by decomposing the sub-optimality error in Lemma 1. ", "page_idx": 18}, {"type": "text", "text": "Lemma 1 (Decomposition of Suboptimality [88]). ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall s\\in\\mathcal{S}:\\quad V_{h}^{*}(s)-V_{h}^{\\pi}(s)=-\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\mathbb{E}_{\\pi}\\big[\\iota_{h}(s_{h^{\\prime}},a_{h^{\\prime}})\\big|s_{h}=s\\big]+\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\mathbb{E}_{\\pi^{*}}\\big[\\iota_{h^{\\prime}}(s_{h^{\\prime}},a_{h^{\\prime}})\\big|s_{h}=s\\big]}&{}\\\\ {+\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\mathbb{E}_{\\pi^{*}}[\\langle\\widehat{Q}_{h^{\\prime}}(s_{h^{\\prime}},\\cdot),\\pi^{*}(\\cdot,s_{h^{\\prime}})-\\widehat{\\pi}(\\cdot,s_{h^{\\prime}})\\rangle_{A}]s_{h}=s\\big],}&{}\\\\ {\\qquad}&{\\qquad+\\displaystyle\\sum_{h^{\\prime}=h}^{H}\\mathbb{E}_{\\pi^{*}}[\\langle\\widehat{Q}_{h^{\\prime}}(s_{h^{\\prime}},\\cdot),\\pi^{*}(\\cdot,s_{h^{\\prime}})-\\widehat{\\pi}(\\cdot,s_{h^{\\prime}})\\rangle_{A}]s_{h}=s\\big],}&{\\qquad\\dots}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pi$ is any learned policy, $\\pi^{*}$ is the optimal policy that maximizes the cumulative return as below: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi^{*}=\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{\\pi}\\Big[\\sum_{h^{\\prime}=1}^{H}\\gamma^{h^{\\prime}}r\\big(s_{h^{\\prime}},a_{h^{\\prime}}\\big)\\big|s_{h}\\Big].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Based on this decomposition, we will get the basic form of sub-optimality error bound for general offline RL settings in Lemma 2: ", "page_idx": 19}, {"type": "text", "text": "Lemma 2 (Suboptimality in standard MDP [88]). Suppose we have $\\{\\Gamma_{h}\\}_{h=1}^{H}$ as $\\delta$ -uncertainty quantifier. Under $\\mathcal{E}$ defined in Equation (5), the suboptimality error bound by conservative planning satisfies: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall s\\in S:\\quad V_{h}^{*}(s)-V_{h}^{\\pi}(s)\\leq2\\sum_{h^{\\prime}=h}^{H}\\mathbb{E}_{\\pi^{*}}[\\Gamma_{h^{\\prime}}(s_{h^{\\prime}},a_{h^{\\prime}})|s_{1}=s].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The basic form of sub-optimality bound in Lemma 2 involves an uncertainty quantifier $\\Gamma_{h}$ , which in our case will be further replaced by an exact bound in our sparse matrix estimation problem of causal discovery algorithms. ", "page_idx": 19}, {"type": "text", "text": "B.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The main results hold under the following two assumptions: ", "page_idx": 19}, {"type": "text", "text": "Assumption 2 (Existence of a core matrix given the feature embedding). For each $(s,a)\\in S\\times A,$ feature vectors $\\phi(s,a)\\in\\mathbb{R}^{d},\\mu(s)\\in\\mathbb{R}^{d^{\\prime}}$ are approximated as a priori. Given a specific confounder set $u$ , there exists an unknown matrix $M(u)^{*}\\in\\mathbb{R}^{d^{\\prime}\\times d}$ such that, ", "page_idx": 19}, {"type": "equation", "text": "$$\nT(s^{\\prime}|s,a,u)=\\phi(s,a)^{T}M(u)\\mu(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Assumption 3 (Feature regularity). We assume feature regularity [14, 15] for the following components of the confounded bilinear MDP: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\forall u,\\|M(u)\\|_{F}^{2}\\leq C_{M}d,}\\\\ &{\\bullet\\ \\forall(s,a)\\in\\mathcal{S}\\times\\mathcal{A},\\|\\phi(s,a)\\|_{2}^{2}\\leq C_{\\phi}d,}\\\\ &{\\bullet\\ \\forall s^{\\prime}\\in\\mathbb{R}^{|\\mathcal{S}|},\\|\\mu^{T}s^{\\prime}\\|_{2}\\leq C_{\\mu}\\|s^{\\prime}\\|_{\\infty},\\|\\mu K_{\\mu}^{-1}\\|_{2,\\infty}\\leq C_{\\mu}^{\\prime},}\\\\ &{\\bullet\\ \\forall s,a,s^{\\prime}\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S},\\|\\phi(s,a)\\mu(s^{\\prime})^{T}\\|_{1}\\leq C_{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{M},C_{\\phi},C_{\\mu},C_{\\mu}^{\\prime}$ are some universal constants. ", "page_idx": 19}, {"type": "text", "text": "Here, for any matrix $X$ $\\begin{array}{r}{\\mathrm{\\Delta}\\mathrm{Y},\\,\\|{X}\\|_{2,\\infty}:=\\operatorname*{max}_{i}\\sqrt{\\sum_{j}X_{i j}^{2}}}\\end{array}$ represents the operator $2\\mapsto\\infty$ norm. ", "page_idx": 19}, {"type": "text", "text": "Proof pipeline. Armed with the above assumptions, we turn to the bilinear MDP setting, which this work focuses on. We shall develop the finite-sample analysis by specifying the main error term \u2014- $\\delta$ -uncertainty quantifier $\\Gamma$ (see Lemma 2) for our time-homogeneous core matrix estimation problem in the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma 3 (Uncertainty bound for Bilinear Causal Representation). Under the Assumption 2, 3 and that $T$ is an SCM (defined in $^{4}$ ), for the BECAUSE algorithm, for the $\\xi$ -optimal policy $(V_{1}^{*}(\\widetilde s)-$ $V_{1}^{\\pi}(\\widetilde{s})\\leq\\xi)$ , $\\forall\\,0\\leq\\xi\\leq1$ , we have the $\\delta$ -uncertainty set as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{B E C A U S E}=\\Big\\{|(\\widehat{\\mathbb{B}}_{h}\\widehat{V}_{h+1})(s,a)-(\\mathbb{B}_{h}\\widehat{V}_{h+1})(s,a)|}\\\\ &{\\qquad\\qquad\\lesssim\\operatorname*{min}\\big\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal{S}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\big\\}\\sqrt{\\frac{\\log\\big(1/\\delta\\big)}{n(s,a)}},\\forall(s,a,h)\\in A\\times\\mathcal{S}\\times[H]\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{1}$ is some universal constants. ", "page_idx": 19}, {"type": "text", "text": "Armed with the above lemma, we complete the proof of Theorem 1 by showing that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{V_{1}^{*}(\\widetilde s)-V_{1}^{\\pi}(\\widetilde s)\\le2\\sum_{h^{\\prime}=1}^{H}\\mathbb{E}_{\\pi^{*}}[\\Gamma_{h^{\\prime}}(s_{h^{\\prime}},a_{h^{\\prime}})|s_{1}=\\widetilde s]}}\\\\ &{}&{\\lesssim2\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\operatorname*{min}\\left\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal S|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\right\\}\\sqrt{\\frac{\\log(1/\\delta)}{n(s_{h},a_{h})}}\\mid s_{1}=\\widetilde s\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This concludes the proof of Theorem 1. ", "page_idx": 20}, {"type": "text", "text": "B.3 Proof of Lemma 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The key to proving Theorem 1 is to prove Lemma 3. The proof pipeline of Lemma 3 is illustrated below. In Step 1, we derive the estimation of the causal transition matrix $M$ in BECAUSE as a sparsity regression problem. In Step 2, we decompose the error terms within $\\delta$ -uncertainty set into two parts: (a) error due to the under-explored dataset, (b) error due to optimization error in the structured causal model. Then we bound both error terms in Step 3 and Step 4, respectively. Finally, in Step 5, we sum up all the results and derive the form of $\\delta$ -uncertainty quantifier which will lead to our final results in Theorem 1. ", "page_idx": 20}, {"type": "text", "text": "Step 1: deriving the output model of BECAUSE. Recalling the original optimization problem in equation (13) to estimate the core matrix: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widehat{M}=\\underset{M}{\\arg\\operatorname*{max}}[\\log p(\\phi,\\mu,M)-\\lambda|M|]}\\\\ {=\\underset{M}{\\arg\\operatorname*{min}}\\underbrace{\\sum_{(s,a,s^{\\prime})\\in\\mathcal{D}}\\|\\mu(s^{\\prime})^{T}K_{\\mu}^{-1}-\\phi(s,a)^{T}M\\|_{2}^{2}}_{\\mathrm{Model~Learning}}+\\underbrace{\\lambda\\|M\\|_{0}}_{\\mathrm{Sparsity\\,Regularization}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This part of derivation aims to transform the above estimation problem into a linear regression problem, with the regression data pairs $(X,T)$ and some unknown parameters $\\beta$ associated with mask $M$ to be estimated. Eventually, we\u2019ll derive the representation of each part of $\\beta^{M},X,T$ , and eventually reach the following form: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\beta^{M}}\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}[\\|T_{\\pi_{\\beta}}(s_{i}^{\\prime}\\mid s_{i},a_{i})-X_{i}\\beta^{M}\\|_{2}^{2}+\\lambda\\|\\beta^{M}\\|_{0}].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We define each component of this target form of $\\ell_{0}$ regression as follows: ", "page_idx": 20}, {"type": "text", "text": "\u2022 For unknown parameters $\\beta^{M}$ : We first define $\\beta^{M}\\in[0,1]^{d d^{\\prime}}$ as a column dimensional vector consisting of all the entries in time-homogenous causal matrix $M$ , where $\\beta_{i}^{M}$ denotes the $i$ -th entry of $\\beta^{M}$ . Besides, we define $\\beta_{\\mathcal{D}}^{M}$ as the true core matrix given some offline dataset $\\mathcal{D}$ and corresponding data pairs $T_{d a t a},X_{d a t a}$ that satisfies $T_{d a t a}=\\beta_{\\mathcal{D}}^{M}X_{d a t a}+\\epsilon.$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 For dataset $\\mathcal{D}$ : Recall the transition pairs in the offline dataset $\\mathcal{D}=\\{s_{i},a_{i},s_{i}^{\\prime}\\}_{1\\leq i\\leq n}$ . Here, $n$ represents the sample size over certain state-action pairs in the rollout data by some behavior policy $\\pi_{\\beta}$ . For simplicity, we denote $n\\triangleq n(s,a)$ in the following derivation, which is mentioned in Section 2.1. ", "page_idx": 20}, {"type": "text", "text": "\u2022 For regression target $T_{\\pi_{\\beta}}$ : Then, we introduce the following transition targets $T_{\\pi_{\\beta}}$ induced by the offline dataset $\\mathcal{D}$ sampled with behavior policy $\\pi_{\\beta}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{T_{\\pi_{\\beta}}(s^{\\prime}|s,a):=\\displaystyle\\frac{\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}\\mathbf{1}(s_{i}=s,a_{i}=a,s_{i}^{\\prime}=s^{\\prime})}{\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}\\mathbf{1}(s_{i}=s,a_{i}=a)}}}\\\\ {{\\displaystyle=\\frac{1}{n(s,a)}\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}\\mathbf{1}(s_{i}=s,a_{i}=a,s_{i}^{\\prime}=s^{\\prime}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Under the $n$ finite samples in the offline dataset, we assume that $T_{\\pi_{\\beta}}\\sim\\mathcal{N}(\\mathbb{E}[T_{\\pi_{\\beta}}],\\sigma^{2}I_{n})$ . The above definition specifies the regression target in the $\\ell_{0}$ regression problem, and we denote ", "page_idx": 20}, {"type": "text", "text": "$T_{\\pi_{\\beta}}=[T_{\\pi_{\\beta}}(s_{1}^{\\prime}|s_{1},a_{1}),\\cdot\\cdot\\cdot\\cdot,T_{\\pi_{\\beta}}(s_{n}^{\\prime}|s_{n},a_{n})]^{T}\\in\\mathbb{R}^{n}$ as the empirical transition probabilities of certain transition pairs in the offline data $\\mathcal{D}=\\{s_{i},a_{i},s_{i}^{\\prime}\\}_{1\\leq i\\leq n}$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 For regression data $X$ : Next, we need to specify the data $X$ in the regression problem. We denote the i-th row of $X$ as the i-th sample in the offline transition pairs $X_{i}\\in\\mathcal{D}$ , which is a vector of Kronecker product between $\\phi(s_{i},a_{i})\\in\\mathbb{R}^{d}$ and normalized $\\frac{\\mu(s_{i}^{\\prime})}{C_{\\mu}}\\in\\mathbb{R}^{d^{\\prime}}$ (without loss of generality, we assume $C_{\\phi}=1$ and only need to normalize $\\mu(s_{i}^{\\prime})$ by $C_{\\mu}$ ): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{X_{i}=\\phi(s_{i},a_{i})\\otimes\\displaystyle\\frac{\\mu(s_{i}^{\\prime})}{C_{\\mu}}}}\\\\ {{\\displaystyle\\quad=\\displaystyle\\frac{1}{C_{\\mu}}[\\phi(s_{i},a_{i})^{(1)}\\mu(s_{i}^{\\prime})^{(1)},\\phi(s_{i},a_{i})^{(1)}\\mu(s_{i}^{\\prime})^{(2)},\\cdot\\cdot\\cdot,\\phi(s_{i},a_{i})^{(d)}\\mu(s_{i}^{\\prime})^{(d^{\\prime})}]^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As a result, $X_{i}\\in\\mathbb{R}^{d d^{\\prime}}$ , since there are in all $n$ samples in offline dataset, $X\\in\\mathbb{R}^{n\\times d d^{\\prime}}$ is the dataset-dependent matrix with all $n$ rows of samples, and $d$ and $d^{\\prime}$ are the latent dimension of $\\phi$ and $\\mu$ , respectively. Based on the feature regularity criteria in Assumption 3, we have $\\|X_{i}\\|_{2}\\leq\\|X_{i}\\|_{1}^{\\cdot}\\leq1,\\|\\dot{X}\\|_{\\infty}\\leq1$ . ", "page_idx": 21}, {"type": "text", "text": "The prior work [14] estimate the transition kernel of a bilinear MDP using the following ridge regression: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{M}\\mathbb{E}_{(s,a,s^{\\prime})\\in\\mathcal{D}}\\|\\mu(s^{\\prime})^{T}K_{\\mu}^{-1}-\\phi(s,a)^{T}M\\|_{2}^{2}+\\lambda\\|M\\|_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In this paper, in order to promote the sparsity of the matrix $M$ , we introduce the $\\ell_{0}$ regularization term and arrive at the following optimization problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\beta^{M}}\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}[\\|\\mu(s_{i}^{\\prime})^{T}K_{\\mu}^{-1}\\mu(s_{i}^{\\prime})-\\phi(s_{i},a_{i})^{T}M\\mu(s_{i}^{\\prime})\\|_{2}^{2}+\\lambda\\|\\beta^{M}\\|_{0}]}\\\\ &{\\displaystyle\\to\\operatorname*{min}_{\\beta^{M}}\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}[\\|T_{\\pi_{\\beta}}(s_{i}^{\\prime}\\mid s_{i},a_{i})-X_{i}\\beta^{M}\\|_{2}^{2}+\\lambda\\|\\beta^{M}\\|_{0}]=:\\widehat{\\beta}_{\\mathcal{D}}^{M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we denote the solution associated with the offilne dataset $\\mathcal{D}$ as $\\widehat{\\beta}_{\\mathcal{D}}^{M}$ . Here, we use the empirical version constructed by the finite samples in offline dataset $\\mathcal{D}$ . ", "page_idx": 21}, {"type": "text", "text": "Given the goal-conditioned reward setting, for a single episode $s\\sim\\tau$ , $r(s,a;g)=1$ if and only if $s=g$ , otherwise $r(s,a;g)=0$ , as is specified in Section 2.1. Since we are essentially predicting the probabilities (normalized to a sum of 1) of whether the next state is the goal state, i.e. $\\dot{\\textstyle\\sum_{s\\in S}}\\,\\widehat{V}(s)\\,{=}\\,1$ Therefore, we have $\\|\\widehat{V}(\\cdot)\\|_{1}\\leq1$ . ", "page_idx": 21}, {"type": "text", "text": "As is denoted by Equation (23), for the specific offilne dataset collected by some behavior policies $\\pi_{\\beta}$ , we have the regression target $\\begin{array}{r}{T_{\\pi_{\\beta}}(s^{\\prime}|s,a)=\\frac{\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}\\mathbf{1}\\left(s_{i}=s,a_{i}=a,s_{i}^{\\prime}=s^{\\prime}\\right)}{\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in\\mathcal{D}}\\mathbf{1}\\left(s_{i}=s,a_{i}=a\\right)}}\\end{array}$ i(sii,aii,s\u2032i)\u2208D 1(si=s,ai=a) , and the corresponding features induced by the dataset $\\begin{array}{r}{X_{i}=\\phi(s_{i},a_{i})\\otimes\\frac{\\mu(s_{i}^{\\prime})}{C_{\\mu}}}\\end{array}$ . We have the following equations hold: ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{\\pi_{\\beta}}=X\\beta_{\\mathcal{D}}^{M}+\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\beta_{\\ensuremath{\\mathcal{D}}}$ is the true underlying transition mask given the offilne dataset $D,\\epsilon\\sim\\mathcal{N}(0,\\sigma\\cdot I_{n})$ is some exogenous noise of the transition model. ", "page_idx": 21}, {"type": "text", "text": "Specifically, for the regression problem, we transform the original trajectory dataset $\\mathcal{D}$ as $[X,T_{\\pi_{\\beta}}]$ , as the representation of transition pairs rolled out by the behavior policy $\\pi_{\\beta}$ . Similarly, we can define some \u2019well-explored dataset\u2019 $\\mathcal{D}^{*}$ , which is an infinite dataset rollout by the behavior policy $\\pi_{\\beta}$ , ${\\cal D}^{*}\\;=\\;\\{s_{i},a_{i},r_{i}\\}_{i=0}^{\\infty}$ , similar to the definition of regression target $T$ in Equation (23) and representation data $X$ in Equation (24), we have $[X,\\operatorname{\\mathbb{E}}[T_{\\pi_{\\beta}}]]$ as the regression pairs with access to the true transition probability distribution for all the state action pairs $(s,a)$ . Here $X\\in\\mathbb{R}^{n\\times d d^{\\prime}}$ is the regression data defined in equation (22). Recalling the definition in Equation (23), we can further build the regression target under well-explored dataset $\\mathcal{D}^{*}$ as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[T_{\\pi_{\\beta}}(s^{\\prime}|s,a)]=\\frac{\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in{\\mathcal D}^{*}}\\mathbf{1}\\left(s_{i}=s,a_{i}=a,s_{i}^{\\prime}=s^{\\prime}\\right)}{\\sum_{(s_{i},a_{i},s_{i}^{\\prime})\\in{\\mathcal D}^{*}}\\mathbf{1}\\left(s_{i}=s,a_{i}=a\\right)}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{s^{\\prime}\\sim T(\\cdot|s,a)}[\\mathbf{1}(s_{i}=s,a_{i}=a,s_{i}^{\\prime}=s^{\\prime})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We denote $\\mathbb{E}[T_{\\pi_{\\beta}}]\\,=\\,\\left[\\mathbb{E}[T_{\\pi_{\\beta}}(s_{1}^{\\prime}|s_{1},a_{1})],\\cdot\\cdot\\cdot\\mathbb{E}[T_{\\pi_{\\beta}}(s_{n}^{\\prime}|s_{n},a_{n})]\\right]^{T}\\,\\in\\,\\mathbb{R}^{n}$ . In practice, with finite sample size $n$ , we have $\\mathbb{E}[T_{\\pi_{\\beta}}(s^{\\prime}|s,a)]=T(s^{\\prime}|s,a)+\\epsilon=X\\beta_{\\mathcal{D}^{*}}^{M}+\\epsilon$ . In addition, we also introduce a vector form $\\mathbf{T}\\in\\mathbb{R}^{|S||A|\\times|S|}$ so that $\\forall(s,a)\\in{\\mathcal{S}}\\ \\times{\\mathcal{A}}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{T}(\\cdot|s,a)=\\left[T(s_{1}^{\\prime}|s,a)\\quad T(s_{2}^{\\prime}|s,a)\\quad\\cdot\\cdot\\cdot\\quad T(s_{|S|}^{\\prime}|s,a)\\right]^{T}\\in\\mathbb{R}^{|S|},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the state space is denoted as ${\\cal S}\\;=\\;\\{s_{1}^{\\prime},\\cdots,s_{|S|}\\}$ are all possible states. Similar to the Kronecker product we define for $X$ in equation (26), we define $\\mathbf{X}$ in a matrix form for any state-action pair $(s,a)$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{X}(\\cdot|s,a)=\\bigl[\\phi(s,a)\\otimes\\frac{\\mu(s_{1}^{\\prime})}{C_{\\mu}},\\cdot\\cdot\\cdot\\phi(s,a)\\otimes\\frac{\\mu(s_{|S|}^{\\prime})}{C_{\\mu}}\\bigr]^{T}\\in\\mathbb{R}^{|S|\\times d d^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In addition, we let $\\mathbf{X}(s^{\\prime}|s,a)\\in\\mathbb{R}^{1\\times d d^{\\prime}}$ denote the $s^{\\prime}$ -th row of $\\mathbf{X}(\\cdot|s,a)$ associated with the state $s^{\\prime}\\in\\mathcal{S}$ . Consequently, the estimated transition kernel can be expressed as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{T}}(\\cdot|s,a)=\\boldsymbol{\\phi}^{T}(s,a)\\widehat{M}\\boldsymbol{\\mu}(\\cdot)=\\mathbf{X}(\\cdot|s,a)\\widehat{\\boldsymbol{\\beta}}_{\\mathcal{D}}^{M}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Step 2: decomposing the term of interest. To begin with, recalling the definition of Bellman operator $\\mathbb{B}_{h}$ in Equation (16) and applying H\u00f6lder\u2019s inequality, the term of interest for any time step $1\\leq h\\leq H$ and state-action pair $(s,a)\\in S\\times A$ can be controlled as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|(\\widehat{\\mathbb{B}}_{h}\\widehat{V}_{h+1})(s,a)-(\\mathbb{B}_{h}\\widehat{V}_{h+1})(s,a)|\\leq|\\langle\\widehat{\\mathbf{T}}(\\cdot|s,a)-\\mathbf{T}(\\cdot|s,a),\\widehat{V}_{h+1}\\rangle|}&{}\\\\ {\\leq\\|\\widehat{\\mathbf{T}}(\\cdot|s,a)-\\mathbf{T}(\\cdot|s,a)\\|_{\\infty}\\|\\widehat{V}_{h+1}\\|_{1}}&{}\\\\ {\\leq\\|\\widehat{\\mathbf{T}}(\\cdot|s,a)-\\mathbf{T}(\\cdot|s,a)\\|_{\\infty},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality is held given our goal-conditioned reward formulation in section 2.1. To continue, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|(\\widehat{\\mathbb{B}}_{h}\\widehat{V}_{h+1})(s,a)-(\\mathbb{B}_{h}\\widehat{V}_{h+1})(s,a)|\\leq\\|\\widehat{\\mathbf{T}}(\\cdot|s,a)-\\mathbf{T}(\\cdot|s,a)\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\|\\mathbf{X}(\\cdot|s,a)\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\mathbf{X}(\\cdot|s,a)\\beta^{M}\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, we recall $\\widehat{\\beta}_{\\mathcal{D}}^{M}$ represents the parameter vector in the estimated causal masks based on the offilne dataset $\\mathcal{D}$ sampled by $\\pi_{\\beta}$ . Similarly, we denote $\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}$ as the estimated causal mask outputted from equation (26) based on the infinite dataset $\\mathcal{D}^{*}$ generated by the behavior policy $\\pi_{\\beta}$ . Then, we can further control equation (30) as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{X}(\\cdot|s,a)\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\mathbf{X}(\\cdot|s,a)\\beta^{M}\\|_{\\infty}=\\|\\mathbf{X}(\\cdot|s,a)\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\mathbf{X}(\\cdot|s,a)\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}+\\mathbf{X}(\\cdot|s,a)\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}-\\mathbf{X}(\\cdot|s,a)\\beta^{M}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\|\\mathbf{X}(\\cdot|s,a)[\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}]\\|_{\\infty}+\\|\\mathbf{X}(\\cdot|s,a)[\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}-\\beta^{M}]\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathbf{X}(\\cdot|s,a)\\|_{\\infty}\\|\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}\\|_{\\infty}+\\|\\mathbf{X}(\\cdot|s,a)\\|_{\\infty}\\|\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}-\\beta^{M}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underbrace{\\|\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}\\|_{\\infty}}_{(\\mathrm{a})}+\\underbrace{\\|\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}-\\beta^{M}\\|_{\\infty}}_{(\\mathrm{b})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here the last inequality comes from the fact that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\mathbf{X}(\\cdot|s,a)\\|_{\\infty}=\\underset{i\\in|S|}{\\operatorname*{max}}\\sum_{j\\in[d d^{\\prime}]}|\\mathbf{X}(\\cdot|s,a)_{i j}|=\\underset{i\\in|S|}{\\operatorname*{max}}\\,\\|\\mathbf{X}(\\cdot|s,a)_{i}\\|_{1}}\\\\ {\\displaystyle=\\underset{i\\in|S|}{\\operatorname*{max}}\\,\\|\\phi(s,a)\\mu(s_{i}^{\\prime})^{T}\\|_{1}\\leq\\frac{C_{\\mu}}{C_{\\mu}}=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "based on the definition of $X$ in equation (24) and assumption 3. Here $(a)$ comes from the mismatch error between the demonstrated offline dataset and some optimal rollout datasets. And $(b)$ comes from the error of the $\\ell_{0}$ optimization of causal masks given the existence of exogenous noise $\\sigma$ defined by SCM in Definition 4. We will control them separately in the following. ", "page_idx": 22}, {"type": "text", "text": "Step 3: Controlling term (a). We need to consider the optimization process in the original regression problem in equation (22) to fully understand the difference between $\\widehat{\\beta}_{\\mathcal{D}}^{M}$ and $\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}$ , where the only difference is that the latter uses a perfect dataset with infinite sample s. The o ptimization problem we target (cf. equation (26)) can be solved by the iterative hard thresholding algorithm (IHT) proposed by [89] IHT offers an iterative solution for the $\\ell_{0}$ regression problem, armed with a hard thresholding operator as below: ", "page_idx": 23}, {"type": "equation", "text": "$$\n[g_{\\lambda}(\\beta)]_{j}={\\binom{\\operatorname*{max}\\{0,\\beta_{j}-\\lambda\\}}{0}}\\quad{\\mathrm{if~}}\\beta_{j}>\\lambda}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We denote $\\widehat{\\beta}_{\\mathcal{D}}^{M}(i)$ as the estimated causal mask parameters after $i$ -th iterations with dataset $\\mathcal{D}$ Similarly, we denote $\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}(i)$ as the estimation after $i$ -th iterations with dataset $\\mathcal{D}^{*}$ . We initialize the graph to be a full gra ph regardless of the datasets ( $\\mathcal{D}^{*}$ or $\\mathcal{D}$ ) used in the optimization process, leading to $\\bar{\\beta}_{\\mathcal{D}^{\\ast}}^{M}(0)=\\widehat{\\beta}_{\\mathcal{D}}^{M}\\bar{(0)}=\\mathbf{1}\\,\\,\\bar{\\in}\\,\\mathbb{R}^{d d^{\\prime}}$ . ", "page_idx": 23}, {"type": "text", "text": "Recall that $X\\in\\mathbb{R}^{n\\times d d^{\\prime}}$ , $T_{\\pi_{\\beta}}\\in\\mathbb{R}^{n},\\mathbb{E}[T_{\\pi_{\\beta}}]\\in\\mathbb{R}^{n}$ and $\\widehat{\\beta}_{\\mathcal{D}}^{M}$ is $[0,1]^{d d^{\\prime}}$ based on the definition in the original $\\ell_{0}$ optimization problem in equation (22), equa tion (23) and equation (28). The update rules of using either the dataset $\\mathcal{D}$ or $\\mathcal{D}^{*}$ can be written as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\beta}_{\\mathcal{D}}^{M}(i)=g_{\\lambda}(\\widehat{\\beta}_{\\mathcal{D}}^{M}(i-1)+\\eta X^{T}[T_{\\pi_{\\beta}}-X\\widehat{\\beta}_{\\mathcal{D}}^{M}(i-1)])}\\\\ &{\\beta_{\\mathcal{D}^{*}}^{M}(i)=g_{\\lambda}(\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}(i-1)+\\eta X^{T}[\\mathbb{E}[T_{\\pi_{\\beta}}]-X\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}(i-1)]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that the difference between the two parameters $\\widehat{\\beta}_{\\mathcal{D}}^{M}$ and $\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}$ essentially relies on the difference between two pairs of transition kernel estimation  datasets $[\\bar{X},\\mathbb{E}[T_{\\pi_{\\beta}}]]$ and $[X,T_{\\pi_{\\beta}}]$ . It is easily verified that the hard-thresholding operator $[g_{\\lambda}(\\cdot)]_{i}$ defined in equation (32) is $L\\,=\\,1$ -Lipschitz. According to [90], we can set the learning rate $\\begin{array}{r}{\\eta\\le\\frac{1}{L}=1}\\end{array}$ here. Using the Lipschitz property, at each iterative update step $i$ , we can control the difference between the estimated parameters obtained by using $\\mathcal{D}$ or $\\mathcal{D}^{*}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\beta}_{D}^{M}(i)-\\widehat{\\beta}_{D^{*}}^{M}(i)\\|_{2}}\\\\ &{=\\|g_{\\lambda}\\big(\\widehat{\\beta}_{D}^{M}(i-1)+\\eta X^{T}[T_{\\pi_{\\beta}}-X\\widehat{\\beta}_{D}^{M}(i-1)]\\big)}\\\\ &{\\qquad\\qquad-g_{\\lambda}\\big(\\widehat{\\beta}_{D^{*}}^{M}(i-1)+\\eta X^{T}[\\mathbb{E}[T_{\\pi_{\\beta}}]-X\\widehat{\\beta}_{D^{*}}^{M}(i-1)]\\big)\\|_{2}}\\\\ &{\\le\\|\\big(\\widehat{\\beta}_{D}^{M}(i-1)+\\eta X^{T}[T_{\\pi_{\\beta}}-X\\widehat{\\beta}_{D}^{M}(i-1)]\\big)-\\big(\\widehat{\\beta}_{D^{*}}^{M}(i-1)+\\eta X^{T}[\\mathbb{E}[T_{\\pi_{\\beta}}]-X\\widehat{\\beta}_{D^{*}}^{M}(i-1)]\\big)\\|}\\\\ &{\\le\\|(I_{d\\ell^{\\prime}}-\\eta X^{T}X)[\\widehat{\\beta}_{D}(i-1)-\\widehat{\\beta}_{D^{*}}(i-1)]+\\eta X^{T}[T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]]\\|_{2}}\\\\ &{\\le\\|I_{d\\ell^{\\prime}}-\\eta X^{T}X\\|_{2}\\|\\widehat{\\beta}_{D}(i-1)-\\widehat{\\beta}_{D^{*}}(i-1)\\|_{2}+\\eta\\|X\\|_{2}\\|T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]\\|_{2}}\\\\ &{\\le\\|\\widehat{\\beta}_{D}(i-1)-\\widehat{\\beta}_{D^{*}}(i-1)\\|_{2}+\\eta\\|T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, $I_{d d^{\\prime}}$ represent the identity matrix of size $d d^{\\prime}\\times d d^{\\prime}$ , the last inequality holds based on the fact that $\\|X\\|_{2}\\leq1$ defined in equation (24). Since $\\begin{array}{r}{\\eta\\le\\frac{1}{L}=1}\\end{array}$ , as a result, $\\|I_{d d^{\\prime}}-\\eta X^{T}X\\|_{2}=$ $\\operatorname*{max}(1-\\eta\\lambda(X^{T}X))\\leq1$ . We perform IHT for sufficient $K>0$ iterations and output the last step estimation as our solutions for either the offline dataset $\\mathcal{D}$ (we use for practical optimization) or the perfect dataset $\\mathcal{D}^{*}$ . In practice, for any dataset such as $\\mathcal{D}$ and any accuracy level $0\\leq\\xi\\leq1$ , we have the output of IHT gradually converges to the optimal solution $\\widehat{\\beta}_{\\mathcal{D}}^{M}$ of the problem in equation (26). Namely, after at most $\\begin{array}{r}{K\\simeq\\log\\left(\\frac{||\\boldsymbol{M}||_{0}}{\\xi}\\right)}\\end{array}$ steps [89, Corollary 1], the output satisfies $\\begin{array}{r}{\\left\\|\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\widehat{\\beta}_{\\mathcal{D}}^{M}(K)\\right\\|_{2}\\le\\frac{\\xi}{4}}\\end{array}$ . Similarly, based on the perfect dataset $\\mathcal{D}^{*}$ , the output of IHT gradually converges to $\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}$ at the same rate. Consequently, the term of interest (a) can be bounded recursively as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\beta}_{D}^{M}-\\widehat{\\beta}_{D^{*}}^{M}\\|_{\\infty}=\\left\\|\\widehat{\\beta}_{D}^{M}(K)-\\widehat{\\beta}_{D^{*}}^{M}(K)+\\left(\\widehat{\\beta}_{D}^{M}-\\widehat{\\beta}_{D}^{M}(K)\\right)+\\left(\\widehat{\\beta}_{D^{*}}^{M}-\\widehat{\\beta}_{D^{*}}^{M}(K)\\right)\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\widehat{\\beta}_{D}^{M}(K)-\\widehat{\\beta}_{D^{*}}^{M}(K)\\right\\|_{\\infty}+\\frac{\\xi}{4}+\\frac{\\xi}{4}}\\\\ &{\\qquad\\qquad\\leq\\left\\|\\widehat{\\beta}_{D}^{M}(K)-\\widehat{\\beta}_{D^{*}}^{M}(K)\\right\\|_{2}+\\frac{\\xi}{4}+\\frac{\\xi}{4}}\\\\ &{\\qquad\\qquad\\leq\\|\\widehat{\\beta}_{D}^{M}(0)-\\widehat{\\beta}_{D^{*}}^{M}(0)\\|_{2}+K\\eta\\|T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]\\|_{2}+\\frac{\\xi}{2}}\\\\ &{\\qquad=K\\eta\\|T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]\\|_{2}+\\frac{\\xi}{2}}\\\\ &{\\qquad\\qquad\\lesssim\\eta\\log\\left(\\frac{\\|M\\|_{0}}{\\xi}\\right)\\|T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]\\|_{2}+\\frac{\\xi}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality holds by recursively applying equation (34) to $K,K\\!-\\!1,\\cdots\\,,0$ iterations, the last equality is due to the fact that $\\widehat{\\beta}_{\\mathcal{D}}^{M}(0)=\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}(0)$ , and the last inequality holds by setting $K=\\log(\\frac{\\|M\\|_{2}}{\\xi})$ . ", "page_idx": 24}, {"type": "text", "text": "Now the remaining of the proof will focus on controlling $\\|T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]\\|_{\\infty}$ . Recall that we have defined the regression target in equation (23) and equation (28): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle{T_{\\pi_{\\beta}}(s^{\\prime}|s,a)=\\frac{1}{n(s,a)}\\sum_{i=1}^{n(s,a)}{\\bf1}(s_{i}=s,a_{i}=a,s_{i}^{\\prime}=s^{\\prime})},}\\\\ &{}&{\\mathbb{E}[T_{\\pi_{\\beta}}(s^{\\prime}|s,a)]=\\mathbb{E}_{s^{\\prime}\\sim T(\\cdot|s,a)}[{\\bf1}(s_{i}=s,a_{i}=a,s_{i}^{\\prime}=s^{\\prime})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proposition 1 (Well-explored dataset). With the offline dataset $\\mathcal{D}$ of in total $n$ samples with $n(s,a)$ samples generated independently conditioned on any $(s,a)$ . For any $0<\\delta<1$ , with probability at least $1-\\delta_{i}$ , one has ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall(s,a)\\in S\\times A:\\quad\\|T_{\\pi_{\\beta}}(\\cdot|s,a)-\\mathbb{E}[T_{\\pi_{\\beta}}(\\cdot|s,a)]\\|_{2}\\leq C_{\\beta}\\sqrt{\\frac{|S|}{n(s,a)}\\log\\left(\\frac{|S||A|}{\\delta}\\right)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some universal constant $C_{\\beta}$ . ", "page_idx": 24}, {"type": "text", "text": "The above proposition can be directly proved by applying [91, Lemma 17] over all $(s,a)\\in S\\times A$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{(s,a)\\in S\\times A}\\|T_{\\pi_{\\beta}}(\\cdot|s,a)-\\mathbb{E}[T_{\\pi_{\\beta}}(\\cdot|s,a)]\\|_{1}\\leq\\sqrt{\\frac{14|S|}{n(s,a)}\\log\\left(\\frac{2|S||A|}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As a result, we can further extend the results in equation (35) to bound the term (a) as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\beta}_{\\mathcal{D}}^{M}-\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}\\|_{\\infty}\\leq K\\eta\\|T_{\\pi_{\\beta}}-\\mathbb{E}[T_{\\pi_{\\beta}}]\\|_{2}+\\frac{\\xi}{2}}\\\\ &{\\qquad\\qquad\\lesssim\\eta C_{\\beta}C_{\\mu}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{\\frac{|\\mathcal{S}|}{n(s,a)}\\log\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)}+\\frac{\\xi}{2}}\\\\ &{\\qquad\\qquad\\leq C_{\\beta}C_{\\mu}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{\\frac{|\\mathcal{S}|}{n(s,a)}\\log\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)}+\\frac{\\xi}{2}}\\\\ &{\\qquad\\qquad\\triangleq C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{\\frac{|\\mathcal{S}|}{n(s,a)}\\log\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|}{\\delta}\\right)}+\\frac{\\xi}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $C_{1}=C_{\\beta}C_{\\mu}$ is some constant that is related to the feature regularity, $\\xi$ is the level of error tolerance in the cumulative value returns, in our setting, $0\\leq\\xi\\leq1$ . ", "page_idx": 24}, {"type": "text", "text": "Step 4: Controlling term (b). For (b) in Equation (31), which is $\\|\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}-\\beta^{M}\\|_{\\infty}$ , we are interested in what is the optimization error given finite well-explored offline dataset $\\mathcal{D}^{*}$ . Here the optimization error mainly originates from the Gaussian noise in the SCM formulation in Definition 4. Yet our causal discovery module, i.e. an $\\ell_{0}$ estimator, will always encounter some estimation error induced by the exogenous noise. ", "page_idx": 25}, {"type": "text", "text": "Firstly, we have the bounded relationship between $\\ell_{2}$ and $\\ell_{\\infty}$ norm: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lVert\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}-\\beta^{M}\\rVert_{\\infty}\\leq\\lVert\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}-\\beta^{M}\\rVert_{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then we can analyze the error bound for $\\ell_{0}$ regression in the sense of $\\ell_{2}$ norm. ", "page_idx": 25}, {"type": "text", "text": "The derivation below generally follows the $\\ell_{0}$ regularized linear regression bound in [92]. Based on Assumption 2 and 3, there exists $M$ , we denote this optimal solution in the vector form as $\\beta^{M}$ . ", "page_idx": 25}, {"type": "text", "text": "Besides the aforementioned optimization error in the iterative thresholding update process, according to the SCM in Definition 4 and Assumption 2 and Proposition 1, we denote a finite subset of observed transition probabilities of the well-explored data $\\mathbb{E}[T_{\\pi_{\\beta}}]$ with size $n$ : $T_{o b v}\\in\\mathbb{R}^{n}$ . By definition above, we can then assume the finite-sample regression target $T$ is generated by causal features of transition pairs (denoted by $\\begin{array}{r}{X=X_{\\pi^{*}}=X_{\\pi_{\\beta}},X\\in\\mathbb{R}^{n\\times d^{\\prime}})}\\end{array}$ and the ground-truth causal mask (represented by \u03b2M \u2208Rdd) with the following equation: ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{o b v}\\triangleq\\mathbb{E}[T_{\\pi_{\\beta}}]=T+\\epsilon=X\\beta^{M}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here $\\epsilon\\sim\\mathcal{N}(0,\\sigma I_{n})$ is the independent exogenous noise defined in Definition 4. We\u2019ll then use the above equation to bound term (b) in equation (31). ", "page_idx": 25}, {"type": "text", "text": "Specifically, we solved this $\\ell_{0}$ regression problem with its bounded form as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\beta^{M}}\\|T_{o b v}-X\\beta^{M}\\|_{2}^{2},\\quad s.t.\\|\\beta^{M}\\|_{0}\\leq s.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In BECAUSE, we select the sparsity level $s\\approx\\|\\beta^{M}\\|_{0}=\\|M\\|_{0}\\leq d d^{\\prime}$ . ", "page_idx": 25}, {"type": "text", "text": "For simplicity, we denote the approximate solution $\\widehat{\\beta}_{\\mathcal{D}^{*}}^{M}$ in Equation (39) as ${\\widehat{\\beta}}^{M}$ . By the virtue of optimality of the solution $\\beta^{M}$ in Equation (39), we find that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|T_{o b v}-X\\beta^{M}\\|_{2}^{2}\\leq\\|T_{o b v}-X\\widehat{\\beta}^{M}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then by expanding and shifting the terms, we can derive the basic inequality for the $\\ell_{0}$ estimator above: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{o b v}-X\\beta^{M}\\|_{2}^{2}\\leq\\|(T_{o b v}-X\\beta^{M})+(X\\beta^{M}-X\\widehat{\\beta}^{M})\\|_{2}^{2}}\\\\ &{\\qquad\\overset{(38)}{\\Longrightarrow}\\|\\epsilon\\|_{2}^{2}\\leq\\|\\epsilon+(X\\beta^{M}-X\\widehat{\\beta}^{M})\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|\\epsilon\\|_{2}^{2}+\\|X\\widehat{\\beta}^{M}-X\\beta^{M}\\|_{2}^{2}+2\\langle\\epsilon,X\\beta^{\\beta^{*}}-X\\widehat{\\beta}^{M}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since both ${\\widehat{\\beta}}^{M}$ and $\\beta^{M}$ are $s$ -sparse, the vector ${\\widehat{\\beta}}^{M}-\\beta^{M}$ is at most $2s$ -sparse. We denote the set of all $2s$ -sparse $d d^{\\prime}$ -dimensional vector set as $\\mathbb{T}^{d d^{\\prime}}(2s)$ , we denote $\\boldsymbol{v}\\,=\\,\\overset{\\cdot}{\\mathbb{I}}\\bigl(\\,\\widehat{\\beta}^{M}\\,-\\,\\beta^{M}\\,=\\,0\\bigr)$ as an indicator vector, $v_{i}=0$ if and only if $\\widehat{\\beta}_{i}^{M}-\\beta_{i}^{M},\\forall\\:i\\in[d d^{\\prime}]$ . ", "page_idx": 25}, {"type": "text", "text": "By shifting the terms, we use the sub-Gaussian assumption and further use H\u00f6lder\u2019s inequality to get the following results: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n(s,a)}\\|X{\\widehat{\\beta}}^{M}-X{\\beta}^{M}\\|_{2}^{2}\\leq\\displaystyle\\frac{2}{n(s,a)}\\langle X^{T}\\epsilon,{\\widehat{\\beta}}^{M}-\\beta^{M}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2}{n(s,a)}\\|{\\widehat{\\beta}}^{M}-\\beta^{M}\\|_{2}\\operatorname*{sup}_{v\\in\\mathbb{T}^{d^{\\prime}}(2s)}\\langle v,X^{T}\\epsilon\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\|{\\widehat{\\beta}}^{M}-\\beta^{M}\\|_{2}\\operatorname*{sup}_{|S|=2s}\\|\\frac{(X^{T}\\epsilon)_{S}}{n(s,a)}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\|{\\widehat{\\beta}}^{M}-\\beta^{M}\\|_{2}\\sigma\\sqrt{\\frac{2s\\log(d d^{\\prime}/\\delta)}{n(s,a)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}I_{n})$ is the exogenous noise variable in the SCM in Definition 4. ", "page_idx": 26}, {"type": "text", "text": "Then by simply applying restricted eigenvalue (RE) condition, for some $\\kappa(S)>0$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\kappa(S)\\|\\widehat{\\boldsymbol{\\beta}}^{M}-\\boldsymbol{\\beta}^{M}\\|_{2}^{2}\\leq\\frac{1}{n(s,a)}\\|X(\\widehat{\\boldsymbol{\\beta}}^{M}-\\boldsymbol{\\beta}^{M})\\|_{2}^{2}\\leq2\\|\\widehat{\\boldsymbol{\\beta}}^{M}-\\boldsymbol{\\beta}^{M}\\|_{2}\\sigma\\sqrt{\\frac{2s\\log(d d^{\\prime}/\\delta)}{n(s,a)}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\widehat{\\beta}^{M}-\\beta^{M}\\|_{2}^{2}\\leq\\frac{2\\sigma\\sqrt{2s}}{\\kappa(S)}\\sqrt{\\frac{\\log(d d^{\\prime}/\\delta)}{n}}\\triangleq C_{s}\\sigma\\sqrt{\\frac{\\|M\\|_{0}\\log(d d^{\\prime}/\\delta)}{n(s,a)}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with probability at least $1-\\delta$ , which bounds term (b) in Equation (31). ", "page_idx": 26}, {"type": "text", "text": "Step 5: Summing up the results Summarizing both bounds for terms (a) and (b) in Equation (31), we will get the following bounds with probability $1-\\delta$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|(\\widehat{\\mathbb{B}}_{h}\\widehat{V}_{h+1})(s,a)-(\\mathbb{B}_{h}\\widehat{V}_{h+1})(s,a)|\\leq\\|\\widehat{\\beta}_{D^{*}}^{M}-\\beta^{M}\\|_{\\infty}+\\|\\widehat{\\beta}_{D}^{M}-\\widehat{\\beta}_{D^{*}}^{M}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{\\frac{|\\mathcal{S}|}{n(s,a)}\\log\\Big(\\frac{|\\mathcal{S}||A|}{\\delta}\\Big)}+C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\sqrt{\\frac{\\log(d d^{\\prime}/\\delta)}{n(s,a)}}+\\frac{\\xi}{2}}\\\\ &{\\qquad\\qquad\\lesssim\\operatorname*{min}\\big\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal{S}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\big\\}\\sqrt{\\frac{\\log(1/\\delta)}{n(s,a)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we complete the proof of Lemma 3 by showing that for all $(s,a)\\in\\mathcal{A}\\times\\mathcal{A},h\\in[H]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{\\mathrm{BECAUSE}}\\Bigg\\{|(\\widehat{\\mathbb{B}}_{h}\\widehat{V}_{h+1})(s,a)-(\\mathbb{B}_{h}\\widehat{V}_{h+1})(s,a)|}\\\\ &{\\qquad\\quad\\lesssim\\operatorname*{min}\\left\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|S|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\right\\}\\sqrt{\\frac{\\log(1/\\delta)}{n(s,a)}}\\Bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The final bound of the term (b) includes a dependency of $\\textstyle O({\\frac{1}{\\sqrt{n}}})$ and logarithm of dimensionality $d d^{\\prime}$ in the estimated transition matrix. It also incurs a dependency of error tolerance $\\xi$ or SCM\u2019s noise level $\\sigma$ square root of sparsity level ${\\sqrt{s}}={\\sqrt{\\|M\\|_{0}}}$ . ", "page_idx": 26}, {"type": "text", "text": "So far, we prove the following bound in theorem 1: ", "page_idx": 26}, {"type": "equation", "text": "$$\nV_{1}^{*}(\\widetilde{s})-V_{1}^{\\pi}(\\widetilde{s})\\leq\\operatorname*{min}\\left\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|{\\cal{S}}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\right\\}\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[\\sqrt{\\frac{\\log(1/\\delta)}{n(s_{h},a_{h})}}\\ |\\ s_{1}=\\widetilde{s}\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In order to achieve $\\xi$ -optimal policy such that $V_{1}^{*}(\\widetilde{s})-V_{1}^{\\pi}(\\widetilde{s})\\lesssim\\xi$ , the RHS needs to satisfy: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|{\\cal{S}}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\right\\}\\sum_{h=1}^{H}\\mathbb{E}_{\\boldsymbol{\\pi}^{*}}\\left[\\sqrt{\\frac{\\log(1/\\delta)}{n(s_{h},a_{h})}}\\ |\\ s_{1}=\\tilde{s}\\right]\\lesssim\\xi\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We first multiply $\\begin{array}{r}{\\sqrt{\\operatorname*{min}_{(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\left[H\\right]}\\mathbb{E}_{\\pi^{\\star}}\\left[n(s_{h},a_{h})\\mid s_{1}=\\widetilde{s}\\right]},}\\end{array}$ , and then take the square on both sides. We have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{sin}\\left\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal{S}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\right\\}\\sum_{h=1}^{H}{\\mathbb E}_{\\pi^{*}}\\left[\\sqrt{\\frac{\\log\\!\\big(1/\\delta\\big)\\operatorname*{min}_{(s,a,h)\\in\\mathcal{S}\\times\\mathcal{A}\\times[H]}n\\big(s_{h},a_{h}\\big)\\big]}{n(s_{h},a_{h})}}\\mid\\alpha\\mid_{0}\\right.}&{{}}\\\\ {\\displaystyle\\lesssim\\xi\\sqrt{(s,a,h)\\!\\in\\!\\mathcal{S}\\!\\times\\!A\\times\\![H]}^{\\mathbb{B}_{\\pi^{*}}\\big[n(s_{h},a_{h})\\mid s_{1}=\\widetilde{s}\\big]}}&{{}\\displaystyle\\bigwedge_{(s,a_{h})\\in\\mathcal{S}\\times A\\times[H]}^{\\mathrm{Bin}}\\mathbb{E}_{\\pi^{*}}\\big[n(s_{h},a_{h})\\mid s_{1}=\\widetilde{s}\\big]}&{{}}\\\\ {\\displaystyle}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\Big\\{\\!\\left.\\frac{1}{}\\!\\in\\mathcal{S}\\!\\times\\!A\\times\\![H]\\!\\right\\}\\mathbb{E}_{\\pi^{*}}\\big[n(s_{h},a_{h})\\mid s_{1}=\\widetilde{s}\\big]}&{{}\\displaystyle\\bigwedge_{(s,a_{h})\\in\\mathcal{S}\\times A\\times[H]}^{\\mathrm{Bin}}\\mathbb{E}_{\\pi^{*}}\\big[n(s_{h},a_{h})\\big]\\big\\}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Given the definition, LHS in Equation (47) satisfies: ", "page_idx": 27}, {"type": "equation", "text": "$$\nL H S\\lesssim\\operatorname*{min}\\big\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal{S}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\big\\}\\sum_{h=1}^{H}\\mathbb{E}_{\\boldsymbol{\\pi}^{*}}\\left[\\sqrt{\\log(1/\\delta)}\\mid s_{1}=\\widehat{s}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To ensure the satisfaction of $\\xi$ -optimal policy, we thus would like the RHS of Equation (47) satisfies: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R H S}=\\xi\\sqrt{\\underset{(s,a,h)\\in S\\times A\\times[H]}{\\operatorname*{min}}\\mathbb{E}_{\\pi^{\\star}}\\left[n(s_{h},a_{h})\\mid s_{1}=\\widetilde{s}\\right]}}\\\\ &{\\qquad\\gtrsim\\operatorname*{min}\\big\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal{S}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\big)\\big\\}\\underset{h=1}{\\overset{H}{\\sum}}\\mathbb{E}_{\\pi^{\\star}}\\left[\\sqrt{\\log(1/\\delta)}\\mid s_{1}=\\widetilde{s}\\right]}\\\\ &{\\qquad=\\operatorname*{min}\\big\\{C_{1}\\log\\big(\\frac{\\|M\\|_{0}}{\\xi}\\big)\\sqrt{|\\mathcal{S}|},C_{s}\\sigma\\sqrt{\\|M\\|_{0}}\\big)\\big\\}H\\sqrt{\\log(1/\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Consequently, we can shift the terms and get the sample complexity bound for the $\\xi$ -optimal policy, $\\forall0\\leq\\xi\\leq1$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{s,a,h\\in S\\times A\\times[H]}}\\mathbb{E}_{\\pi^{*}}\\left[n(s_{h},a_{h})\\mid s_{1}=\\widetilde s\\right]\\gtrsim\\frac{\\operatorname*{min}\\left\\{C_{1}^{2}\\log^{2}\\left(\\frac{\\|M\\|_{\\infty}}{\\xi}\\right)|S|,C_{s}^{2}\\sigma^{2}\\|M\\|_{0}\\right\\}\\cdot H^{2}\\log(1/\\delta)}{\\xi^{2}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "C Additional Experiments Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide additional experiment results and algorithm implementation details. ", "page_idx": 27}, {"type": "text", "text": "C.1 Implementation of Causal Discovery ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We implement the causal discovery primarily based on Equation (5). However, in practice, how to control the coefficient before the sparsity regularization terms is crucial to the final performance. In practice, instead of controlling $\\lambda$ , we use $p$ -value as a threshold to determine the following conditional independence: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\phi(s,a)^{(i)}\\perp\\!\\!\\!\\perp\\mu(s^{\\prime})^{(j)}\\mid\\phi(s,a)^{-(i)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here $\\phi(\\cdot,\\cdot)^{(i)}$ means that this element is the $i^{t h}$ factor in the abstracted state action representation, similar to $\\mu(\\cdot,\\cdot)^{(j)}$ , $\\phi(s,a)^{-(i)}$ means all the other factors in the representation except for the $i^{t h}$ factor. ", "page_idx": 27}, {"type": "text", "text": "If the $\\mathbf{p}$ -value based on the above conditional independence test is less than a threshold, we can remove the edge by setting $M_{i j}\\,=\\,0$ . Please refer to the Appendix Table 13 for the selection of threshold in each environment. ", "page_idx": 27}, {"type": "text", "text": "C.2 Training Details of Energy-based Model ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We train the EBM according to the margin loss in Equation (8). In practice, we attach Tanh() to the output layer to clip the unnormalized score between $^{-1}$ and $+1$ . The energy networks take in both conditions and samples, then concatenate them together and sent it into MLP encoders. The detailed hyperparameters of EBM are listed in Table 12. ", "page_idx": 27}, {"type": "text", "text": "In the vanilla EBM, people follow the Langevin dynamics to effectively sample the negative samples. Here, as we discover the causal mask and identify the causal representation in the model learning stage, we find that we can use these representations in both the energy networks and the sampling process to get some effective negative samples, which is similar to the practice of augmentation of causality-guided counterfactual data [77]. ", "page_idx": 27}, {"type": "text", "text": "The interesting trick we employ here is the way to get our negative samples by mixing the latent factors from offline data. For example, for a positive sample array ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pmb{x}^{+}=\\left[\\begin{array}{c c c c}{\\!\\!\\left[\\mu(s_{1}^{\\prime})^{(1)}\\right.}&{\\mu(s_{1}^{\\prime})^{(2)}}&{.\\cdot\\cdot.}&{\\mu(s_{1}^{\\prime})^{(d^{\\prime})}}\\\\ {\\!\\!\\left[\\mu(s_{2}^{\\prime})^{(1)}\\right.}&{\\mu(s_{2}^{\\prime})^{(2)}}&{.\\cdot\\cdot}&{\\mu(s_{2}^{\\prime})^{(d^{\\prime})}}\\\\ {.\\cdot\\cdot}&{.\\cdot\\cdot}&{.\\cdot\\cdot}&{.\\cdot\\cdot}\\\\ {\\!\\!\\left[\\mu(s_{n}^{\\prime})^{(1)}\\right.}&{\\mu(s_{n}^{\\prime})^{(2)}}&{.\\cdot\\cdot}&{\\mu(s_{n}^{\\prime})^{(d^{\\prime})}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with conditions: ", "page_idx": 28}, {"type": "equation", "text": "$$\ny=\\left[\\begin{array}{c c c c}{\\!\\!\\left[\\phi(s_{1},a_{1})^{(1)}\\right.}&{\\phi(s_{1},a_{1})^{(2)}}&{\\cdot\\cdot\\cdot}&{\\phi(s_{1},a_{1})^{(d)}}\\\\ {\\!\\!\\left.\\phi(s_{1},a_{1})^{(1)}\\right.}&{\\phi(s_{1},a_{1})^{(2)}}&{\\cdot\\cdot\\cdot}&{\\phi(s_{1},a_{1})^{(d)}}\\\\ {\\cdot\\cdot\\cdot}&{\\cdot\\cdot\\cdot}&{\\cdot\\cdot\\cdot}&{\\cdot\\cdot\\cdot}\\\\ {\\!\\!\\phi(s_{1},a_{1})^{(1)}}&{\\phi(s_{1},a_{1})^{(2)}}&{\\cdot\\cdot\\cdot}&{\\phi(s_{1},a_{1})^{(d)}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, $s_{i},a_{i},s_{i}^{\\prime}$ denotes the timestep of the offline samples. $\\phi(\\cdot,\\cdot)^{(i)}$ means this element is the $i^{t h}$ factor in the abstracted state action representation, similar to the $\\mu(\\cdot,\\cdot)^{(i)}$ ", "page_idx": 28}, {"type": "text", "text": "as we already get the corresponding causal representation $\\mu(s^{\\prime})$ that is semantically meaningful, we can mix the columns to create useful counterfactual negative samples ", "page_idx": 28}, {"type": "equation", "text": "$$\nx_{\\mathrm{countefactual}}^{-}=\\left[\\!\\!\\begin{array}{c c c c}{\\mu(s_{1}^{\\prime})^{(1)}}&{\\mu(s_{2}^{\\prime})^{(2)}}&{\\cdot\\cdot\\cdot}&{\\mu(s_{n-1}^{\\prime})^{(d^{\\prime})}}\\\\ {\\mu(s_{2}^{\\prime})^{(1)}}&{\\mu(s_{1}^{\\prime})^{(2)}}&{\\cdot\\cdot}&{\\mu(s_{n}^{\\prime})^{(d^{\\prime})}}\\\\ {\\cdot\\cdot\\cdot}&{\\cdot\\cdot\\cdot}&{\\cdot\\cdot}&{\\cdot\\cdot}\\\\ {\\mu(s_{n}^{\\prime})^{(1)}}&{\\mu(s_{2}^{\\prime})^{(2)}}&{\\cdot\\cdot\\cdot}&{\\mu(s_{1}^{\\prime})^{(d^{\\prime})}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "These counterfactual negative samples seem to effectively use the causal representation and speed up the training, as we show in Figure 7. ", "page_idx": 28}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/474a353c765563a627ea435b6dda4f4a612903d322b1a1bcc2d0794288f369c7.jpg", "img_caption": ["Figure 7: Comparison of the convergence speed in EBM training. Compared to random negative samples, our approach enjoys a higher rate of convergence empirically. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "C.3 Additional Mismatch Analysis ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To evaluate the significance of the objective mismatch effect at three different levels of offilne datasets in Unlock environments, we collect 5,000 episodes in each of the Unlock environments for each method. Then we evaluate the mismatch via the following two metrics. ", "page_idx": 28}, {"type": "text", "text": "\u2022 We conduct hypothesis testing via Mann-Whitney U Test [93], with Null hypothesis ", "page_idx": 28}, {"type": "equation", "text": "$$\nH_{0}:\\mathcal{L}_{m o d e l}(\\tau_{p o s})<\\mathcal{L}_{m o d e l}(\\tau_{n e g}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u2022 To understand the exact difference in model loss between two groups of samples, we compute their Wasserstein-1 distance in the episodic model loss between the trajectories with positive and negative rewards, i.e. $W_{1}(\\tau_{p o s}\\|\\tau_{n e g})$ . ", "page_idx": 28}, {"type": "text", "text": "We report the results of $p$ -value and the $W_{1}$ distance of two groups of model loss samples in the following table. ", "page_idx": 28}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/96e978e01121720566a6003ae3f59dd995474585e16380ddc84a1d3ce9141c9a.jpg", "table_caption": ["Table 5: The comparison results of the $p$ -value and the $W_{1}$ distance $\\left(\\times10^{-4}\\right)$ ) between MOPO and BECAUSE. Bold means the better. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "C.4 Additional Experiment Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We report the results of the task-wise performance of all baselines in the main experiment and variants in the ablation studies in Table 6, 7, 8, and 9. ", "page_idx": 29}, {"type": "text", "text": "Table 6: Success rate $(\\%)$ for 18 tasks in three different environments. We evaluate the mean and $95\\%$ confidence interval given by the $t$ -test of the best performance among 10 random seeds, as well as the p-value between the overall performance. Bold is the best. ", "page_idx": 29}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/f9f9f7f364cf099e019bf5aeef2b78908d9ec1977636fff0054d0c19c1f85bbb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 7: $p$ -values of different methods (each has 10 random trials) against BECAUSE in various environments. Under the significance level 0.05, we mark all the baseline results that are significantly lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly outperforms 10 baselines in 18 tasks in $91.1\\%$ of the experiments (164 out of total 180 pairs of experiments). ", "page_idx": 30}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/7d596fcaa22be6547999e0097c3e12db5f7bc5826a96dade1f33f9ca623c6f91.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/1518d9bd193b4b4f70b430f4923d54ee40c13e831fdfb34cc20d50d5ca16a979.jpg", "table_caption": ["Table 8: Success rate $(\\%)$ for 18 tasks in three different environments. We evaluate the mean and $95\\%$ confidence interval of the test performance among 10 random seeds. Bold means the best. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/f697fd47b9d77592e6895e4661fe49581351ec75f891873f863f5c009955f11d.jpg", "table_caption": ["Table 9: $p$ -values of different methods (each has 10 random trials) against BECAUSE in various environments. Under the significance level 0.05, we mark all the baseline results that are significantly lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly outperforms 3 variants in 18 tasks $83.3\\%$ of the experiments (45 out of total 54 pairs of experiments). "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "C.5 Additional Environment Description ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We provide a more detailed description of the environments we use in the experiment, as shown in Table 10. ", "page_idx": 31}, {"type": "text", "text": "Lift The Lift environment based on the robosuite [34] contains 33 dimensions of state space, including the end effector pose, joint pose, joint velocity, cube pose as well as its relative position, cube color, and a contact flag. It contains 4 dimensions of hybrid action space that uses Operation Space Control (OSC) to control the 3D position and the 1D gripper movement. The task is counted as a success when the assigned block is lifted from the table over $0.1\\mathrm{m}$ . The generalization setting in the Lift environment is to use an unseen combination of position and color during online testing. This environment can be abstracted into 15 dimensions of factorizable state space and 4 dimensions of factorizable action space. The causal graph of this environment is recorded in Figure 8(a). ", "page_idx": 31}, {"type": "text", "text": "Unlock The Unlock environments based on the MiniGrid world [35] contain 110 dimensions of discrete state space, with 3 of 36-dimensional vector inputs representing the current position of the agent, key, and door in a 6x6 grid world. The rest 2 dimensions in the state space memorize the state of whether the agent has the key in hand. The action space is also discrete (with eight dimensions) to determine the movement (up/down/left/right) and the pick-key, open-door actions. An episode will be counted as a success when the agent holds the key and uses it to open the door in the right position. The generalization setting in the Unlock environment is to change the position of the door and increase the number of total goals in the environment. The agent will only successfully finish one episode by opening all the doors. The causal graph of this environment is recorded in Figure 8(b). ", "page_idx": 31}, {"type": "text", "text": "Crash The Crash environments are based on the Highway environment [37] which contains 22 dimensions of continuous state space, with four vector inputs representing the current position, velocity, and orientation of the surrounding vehicles and ego vehicles. There are two additional dimensions of state memorizing the collision type between the ego vehicles and surrounding vehicles or pedestrians. The 8-dimensional action space is continuous to determine the acceleration in the $x-y$ directions of the ego and surrounding agents. The generalization of the Crash environment is to add different numbers of pedestrians that may cause the crash. An episode will only end when the ego vehicles have a near-miss with both of the pedestrians at the scene. We visualize the causal graph of this environment in Figure 8(c). ", "page_idx": 31}, {"type": "text", "text": "All three environments are visualized in Figure 4. We list their basic configurations in Table 10. ", "page_idx": 32}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/00b2fb072fcc53eae48559d9907c5fbf1a7ba624fd1fe5cfaf72153d6361eee9.jpg", "table_caption": ["Table 10: Environment configurations used in experiments "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "4i9xuPEu9w/tmp/70e3f369bffebf99bdb65ebaba68853a1e9315b310b868449060e484c09e2501.jpg", "img_caption": ["Figure 8: Underlying causal graph $G$ in all 3 environments with expert demonstration. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "C.6 Additional Baseline Information ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We collect data on the above 3 different environments, thus forming 9 groups of offline datasets. ", "page_idx": 32}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/e526f381415143c8f218fa49f5f14f7054a8290e65b4f3ef0d1f8113665ed723.jpg", "table_caption": ["Table 11: Bahavior policies used to collect offline data in different environments. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "After collecting the data using scripted policies in different environments, we train all agents as well as BECAUSE under 10 different random seeds. Then we report the best performance of each trial and compute the mean and standard deviation over 10 seeds for each task in the Appendix 6. ", "page_idx": 32}, {"type": "text", "text": "We refer to the following codebase to implement all the baselines we use: ", "page_idx": 32}, {"type": "text", "text": "\u2022 Invariant Causal Imitation Learning (ICIL, [38]): https://github.com/ioanabica/InvariantCausal-Imitation-Learning, MIT License.   \n\u2022 Causal Confusion Imitation Learning (CCIL, [39]): reference link to the paper.   \n\u2022 TD3 with Behavior Cloning $\\mathbf{TD3+BC}$ , [41]): https://github.com/sfujim/TD3_BC, MIT License.   \n\u2022 Model-based Offline Policy Otimization (MOPO, [2]): https://github.com/junmingyang/mopo.git, MIT License.   \n\u2022 Relational Graph Neural Network (GNN, [42]): https://github.com/MichSchli/RelationPrediction.git, MIT License.   \n\u2022 Causal Dynamics Learning (CDL, [24]): https://github.com/wangzizhao/robosuite/tree/cdl, MIT License.   \n\u2022 Denoised MDP (Denoised, [12]): https://github.com/facebookresearch/denoised_mdp.git, CC BY-NC 4.0.   \n\u2022 Mismatch No More (MnM, [9]): reference link to the paper.   \n\u2022 World model with identifiable factorization (IFactor, [13]), reference link to the paper.   \n\u2022 Delphic Offline RL (Delphic, [40]): reference link to the paper. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "The detailed hyperparameters we use in BECAUSE and other baselines are listed in Table 12 and Table 13: ", "page_idx": 33}, {"type": "text", "text": "C.7 Experiment Support ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Our code is available at the anonymous repo: https://anonymous.4open.science/r/BECAUSE-NeurIPS ", "page_idx": 33}, {"type": "text", "text": "Computing resources The experiments are run on a server with $2\\!\\times\\!\\mathrm{AMD}$ EPYC 7542 32-Core Processor CPU, $2\\times$ NVIDIA RTX 3090 graphics and $2\\times$ NVIDIA RTX A6000 graphics, and 252 GB memory. For one single experiment, it takes BECAUSE and other baselines about 1.5 hours with 100, 000 iterations to train the world model and $1,000,000$ steps to train the energy-based models. ", "page_idx": 33}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/d4b2410a418756a746173f47908a34cd81fcc6d80763333d7eb401c0b775a2d7.jpg", "table_caption": ["Table 12: Hyper-parameters of models used in experiments of BECAUSE and baselines (Part I) "], "table_footnote": ["\\* Use the same planning parameters as BECAUSE. "], "page_idx": 33}, {"type": "table", "img_path": "4i9xuPEu9w/tmp/707a257991abd972156b770fe11d37a671c079ea9339ea416ffa3f8acf6fa842.jpg", "table_caption": ["Table 13: Hyper-parameters of models used in experiments of baselines (Continued) "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "C.8 Broader Impact ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "This work incorporates causality into reinforcement learning methods, which helps humans understand the underlying mechanism of algorithms and check the source of failures. However, the learned causal world model may contain human-readable private information about the environment and the dataset. To mitigate this potential negative societal impact, the causal world model should only be accessible to trustworthy users. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 36}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 36}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 36}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 36}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 36}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 36}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: They are provided in the abstract and introduction. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: They are discussed in the conclusion. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: They are provided in section 3.4 and Appendix A and B. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide necessary implementation details in section 3.2, 3.3 and Appendix C. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 37}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We provide an open access to the code and data collection scripts in the anonymous link. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 38}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide all the experiment settings and details in Appendix C.5 and C.6. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We report the mean and $95\\%$ confidence interval (CI) as well as the significance level (p value) in Table 6, 7, 8 and 9. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: We ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Provided in Appendix C.7. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Provided in the OpenReview form. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Provided in Appendix C.8 ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not contain pre-trained language models, image generators, scraped datasets, or similar assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All the baselines are properly cited and introduced in Appendix C.6. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not involve study participants. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]