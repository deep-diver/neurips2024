{"references": [{"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "publication_date": "2020-05-01", "reason": "This paper provides a comprehensive overview of offline reinforcement learning, which is the foundation of this paper's research."}, {"fullname_first_author": "Tianhe Yu", "paper_title": "Mopo: Model-based offline policy optimization", "publication_date": "2020-00-00", "reason": "This is a key model-based offline RL algorithm that this paper improves upon by incorporating causal representation."}, {"fullname_first_author": "Benjamin Eysenbach", "paper_title": "Mismatched no more: Joint model-policy optimization for model-based RL", "publication_date": "2022-00-00", "reason": "This paper addresses the objective mismatch problem in offline MBRL, a key challenge this paper also tackles."}, {"fullname_first_author": "Jonas Peters", "paper_title": "Elements of causal inference: foundations and learning algorithms", "publication_date": "2017-00-00", "reason": "This paper provides the theoretical foundations of causal inference, which are crucial to this paper's approach of using causal representation."}, {"fullname_first_author": "Lin Yang", "paper_title": "Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound", "publication_date": "2020-00-00", "reason": "This paper introduces bilinear MDPs, which are used in this paper to model the underlying causal structure."}]}