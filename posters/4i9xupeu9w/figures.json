[{"figure_path": "4i9xuPEu9w/figures/figures_0_1.jpg", "caption": "Figure 1: The objective mismatch problem.", "description": "The figure illustrates the objective mismatch problem in offline model-based reinforcement learning (MBRL).  In standard offline MBRL, a low model loss doesn't guarantee successful policy deployment because of a mismatch between the model training objective and the policy's success. The left panel shows this mismatch: low model loss is not correlated with policy success. The right panel illustrates how the proposed method, BECAUSE, addresses this mismatch by aligning model loss with policy success, improving generalizability.", "section": "1 Introduction"}, {"figure_path": "4i9xuPEu9w/figures/figures_2_1.jpg", "caption": "Figure 2: Comparison of our ASC-MDP with two existing formulations.", "description": "This figure compares three different causal models for Markov Decision Processes (MDPs) with confounders. (a) shows a confounded MDP where confounders affect both the state transition and the relationship between states and actions. (b) shows an SC-MDP (State-Confounded MDP) where confounders only influence the state transition. (c) shows the proposed ASC-MDP (Action-State-Confounded MDP), which models confounders affecting both the state transition and the relationship between states and actions, as well as the correlation between actions and states.", "section": "2.2 Action State Confounded MDP"}, {"figure_path": "4i9xuPEu9w/figures/figures_3_1.jpg", "caption": "Figure 3: BECAUSE learns a causality-aware representation from the buffer and uses it in both the world model and uncertainty quantification to obtain a pessimistic planning policy.", "description": "The figure illustrates the BECAUSE framework.  It shows how the algorithm learns a causal representation from offline data to improve the generalization and robustness of offline model-based reinforcement learning. The framework consists of three main components:\n\n1. **Causal Representation Learning**: The offline data buffer is processed using feature mappings \u03c6(.) and \u03bc(.) and the learned causal mask M to obtain a causal representation of states and actions that is less sensitive to confounding factors.\n2. **World Model**:  The causal representation is used to learn a more robust and generalizable world model T(s'|s,a) that accurately predicts the next state s' given the current state s and action a.\n3. **Uncertainty Quantification**: An energy-based model Eg(s, a) is used to quantify the uncertainty of the model's predictions.  This uncertainty is then incorporated into the planning process to make the policy more conservative and less likely to fail in unseen environments.  This uncertainty quantification helps mitigate the objective mismatch problem in offline RL.", "section": "3 Proposed Method: BECAUSE"}, {"figure_path": "4i9xuPEu9w/figures/figures_5_1.jpg", "caption": "Figure 4: Three environments used in this paper.", "description": "This figure shows three different reinforcement learning environments used in the paper's experiments.  The \"Lift\" environment involves a robotic arm manipulating objects. The \"Unlock\" environment is a grid world where an agent must navigate to collect a key and unlock a door. The \"Crash\" environment simulates an autonomous driving scenario where the agent must avoid collisions with pedestrians and other vehicles.", "section": "4 Experiment Results"}, {"figure_path": "4i9xuPEu9w/figures/figures_6_1.jpg", "caption": "Figure 5: Results of BECAUSE and baselines in different tasks. (a) Average success rate in distribution and out of distribution. (b) Average success rate w.r.t. ratio of offline samples. (c) Average success rate w.r.t. spurious level in the environments. We evaluate the mean and standard deviation of the best performance among 10 random seeds and report task-wise results in Appendix Table 6.", "description": "This figure displays the results of the BECAUSE algorithm and its baselines across various tasks.  Panel (a) compares the average success rate in distribution and out-of-distribution settings. Panel (b) shows how the success rate changes with varying ratios of offline samples used for training. Panel (c) illustrates the robustness of the algorithms by demonstrating the impact of different spurious correlation levels on the success rate.  The results are averaged over 10 trials, and detailed task-wise information can be found in Appendix Table 6.", "section": "4 Experiment Results"}, {"figure_path": "4i9xuPEu9w/figures/figures_8_1.jpg", "caption": "Figure 6: Evaluation of the difference between the distribution of episodic model loss for success and failure trajectories. The higher difference indicates a reduction in model mismatch issues. An example of failure mode is trying to open the door without having the key.", "description": "This figure compares the distribution of episodic model loss for successful and failed trajectories in the Unlock environment using two different methods: MOPO and BECAUSE.  The x-axis represents the model loss, and the y-axis shows the number of samples.  Separate distributions are shown for positive reward (success) and negative reward (failure) trajectories.  The key observation is that BECAUSE exhibits a greater separation between the loss distributions for successful and failed trajectories compared to MOPO. This indicates that BECAUSE is better at mitigating the objective mismatch problem, meaning that low model loss more reliably translates into successful outcomes. An example of a failure case is shown in the figure's inset, where the agent attempts to open a door without possessing the required key. The figure helps to illustrate the performance improvement of BECAUSE by showing that a lower model loss correlates more strongly with success than in the case of MOPO.", "section": "4 Experiment Results"}, {"figure_path": "4i9xuPEu9w/figures/figures_28_1.jpg", "caption": "Figure 7: Comparison of the convergence speed in EBM training. Compared to random negative samples, our approach enjoys a higher rate of convergence empirically.", "description": "This figure compares the convergence speed of two different methods for training an Energy-based Model (EBM) used in the BECAUSE algorithm.  The top row shows the training process using randomly sampled negative samples. The bottom row shows the training process using negative samples generated by mixing latent factors from the offline data, a technique leveraging causal representation learned by the BECAUSE algorithm. The images in each row visualize the distribution of the EBM outputs at different timesteps during training. The visualization demonstrates that the causally-informed negative sampling technique leads to faster convergence.", "section": "Additional Experiments Details"}, {"figure_path": "4i9xuPEu9w/figures/figures_32_1.jpg", "caption": "Figure 8: Underlying causal graph G in all 3 environments with expert demonstration.", "description": "This figure shows the causal graphs learned by BECAUSE for each of the three environments used in the experiments: Lift, Unlock, and Crash. Each graph visually represents the causal relationships between the state and action variables in the respective environment. The nodes represent variables, and edges represent causal relationships.  The graphs are bipartite, with one set of nodes representing state variables at a given time step, and the other set representing state variables at the next time step, along with action variables. The sparsity of the graphs highlights the focus on significant causal relationships, ignoring less influential correlations.", "section": "Additional Baseline Information"}]