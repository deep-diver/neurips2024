[{"type": "text", "text": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ilgee Hong\u2217 Georgia Institute of Technology ihong39@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Zichong Li\u2217 Georgia Institute of Technology zli911@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Alexander Bukharin Georgia Institute of Technology abukharin3@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Yixiao Li Georgia Institute of Technology yixiaoli@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Haoming Jiang Amazon jhaoming@amazon.com ", "page_idx": 0}, {"type": "text", "text": "Tianbao Yang Texas A&M University tianbao-yang@tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Tuo Zhao Georgia Institute of Technology tourzhao@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the field of artificial intelligence, aligning AI systems with human preferences has become increasingly crucial, particularly for applications involving complex data and models like large language models (LLMs) in natural language processing [38, 28]. Reinforcement learning from human feedback (RLHF) has gained popularity for customizing AI systems [12, 3, 47]. RLHF involves learning a reward function from human preference data, then using a reinforcement learning algorithm to train a policy to optimize the learned reward model. ", "page_idx": 0}, {"type": "text", "text": "A key challenge in RLHF lies in the complexity of reward modeling, which primarily stems from the reliance on preference labels. Since preference labels only provide comparative rankings of trajectory segments without quantifying the scale of underlying preference strengths, previous methods have employed the Bradley-Terry (BT) model [8] in conjunction with cross-entropy loss to learn the reward function from preference data [12, 38]. This approach assumes that the logit of the preference distribution scales linearly with the reward difference across all sample pairs. However, such linear scaling is often insufficient to account for the variations in preference strength among different pairs, restricting the reward function\u2019s ability to capture a broader range of reward differences. This restrictive approach to reward modeling limits the flexibility of the learned reward function, hindering its capacity to produce the versatile rewards essential for the downstream policy optimization. ", "page_idx": 1}, {"type": "text", "text": "To overcome this shortcoming, we introduce a novel adaptive preference loss function inspired by distributionally robust optimization (DRO) [16]. Our approach incorporates an instance-specific scaling factor to change the scaling between the preference distribution and the reward difference to be non-linear. These factors are learned during training and enable the model to accommodate varying uncertainties of preference strength, thereby enhancing the flexibility of the reward. For pairs showing strong preference (i.e., low preference uncertainty), our method learns a large scaling factor, which enables the model to learn a larger reward difference. In contrast, for pairs showing ambiguous preferences (i.e., high preference uncertainty), our method assigns a smaller scaling factor, enabling the model to learn a smaller reward difference. The additional computational overhead of involving this scaling factor into training is negligible, as the proposed loss function is strictly convex and univariate with respect to each scaling parameter. Therefore, it can be easily optimized by a simple second-order algorithm within a few iterations. ", "page_idx": 1}, {"type": "text", "text": "Our experiments on robotic control tasks [39] demonstrate that our method can learn a more flexible reward function, resulting in an improved policy. Surprisingly, we also discover that our method better aligns the learned reward function with downstream policy optimization. Specifically, when tuning hyperparameters for reward modeling, the simplest approach is to select the reward model according to preference prediction accuracy. However, the selected reward function (with the highest accuracy) often yields a downstream policy with poor performance. To address this misalignment, we usually have to jointly tune the parameters across both stages according to downstream policy performance, resulting in significant computational burden and tuning effort. Our proposed method can mitigate this misalignment: When using our adaptive loss, we can select the reward model based on preference prediction accuracy alone and yield a reasonably well-performing policy. This allows separate tuning of the two stages, easing tuning overhead. To our knowledge, the challenge of this misalignment issue is almost untouched in the RLHF literature, and we are the first to propose a principal approach to mitigate this issue. ", "page_idx": 1}, {"type": "text", "text": "Moreover, our method is generalizable and can be applied to other preference optimization algorithms. For instance, we implement it with direct preference optimization (DPO) [31] and evaluate its effectiveness on natural language generation tasks using Llama-2 7B [40]. Our results demonstrate that integrating adaptive preference scaling into DPO boosts policy performance, while preserving the benefits of alignment. Alignment is especially critical in this setting, where we employ proprietary models like Claude 3 [1] as judges for policy selection, which demands substantial costs for using the APIs. In the case without access to LLM assessment, we must select policy based solely on preference accuracy, under which our approach substantially outperforms other baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Loss functions for reward learning. Prior work on this topic is very limited. For example, Song et al. [37] propose using different loss functions for strong and ambiguous preference data in natural language generation tasks. They apply heavy-tailed loss functions for open-ended questions, where preference ambiguity is desirable, and light-tailed loss functions for close-ended questions requiring clear-cut rewards. However, their approach requires knowing the question type a priori, necessitating extra labeling effort, and may fail for complex questions containing both open and closed aspects. Zhao et al. [47] propose using a hinge loss, which results in zero gradient when the learned reward difference exceeds a margin of 1. This limits the ability to learn very large differences in rewards. Azar et al. [2] develop $\\Psi$ Preference Optimization with Identity Mapping (IPO), which modifies DPO with a loss function matching the scaling of KL-divergence between the learned policy and the initial policy to avoid overftiting due to weak regularization. In contrast to prior work, our method is more broadly applicable to complex preference learning tasks without needing additional labeling or sacrificing the ability to learn arbitrarily large reward differences. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Adaptive temperature scaling (ATS). Temperature scaling (TS) aims to adjust the entropy of probabilistic models by rescaling their logit outputs before the softmax function is applied. This simple method not only enables confidence calibration [19], but also plays a vital role in various machine learning methods, including knowledge distillation [20], reinforcement learning [25], and contrastive learning [43]. Building on TS, adaptive temperature scaling (ATS) enhances flexibility by using instance-specific scalars. Most ATS method trains an additional network for predicting the temperature parameter, which is further integrated into the softmax operator to calibrate the prediction probabilities [44, 14, 4, 21]. In contrast to the aforementioned ATS methods, the proposed adaptive preference scaling (APS) is not designed for classical confidence calibration, but is crafted specifically to enhance the training process of reward function in RLHF. Consequently, the interpretations of scaling factors in ATS and APS are opposite. In ATS, a larger scaling parameter is applied to data with higher uncertainty (e.g., data that the classifier is likely to misclassify), which reduces the magnitude of the corresponding logit. Conversely, in APS, a larger scaling factor is assigned to data with clearer preferences, resulting in a larger logit. This distinction clarifies why the scaling parameter in our approach does not correspond to the concept of \u201ctemperature\" from statistical physics. Additionally, we propose a principled framework for learning scaling parameter based on DRO, which avoids the complexities of designing specific temperature networks and does not rely on heuristically designed loss functions. ", "page_idx": 2}, {"type": "text", "text": "Distributionally robust optimization (DRO). DRO is a technique that trains machine learning models to be robust against uncertainty in the data distribution. Specifically, DRO finds a solution that performs well under the worst-case distribution within a specified uncertainty set around the empirical data distribution [5, 7, 23, 35, 16]. DRO has been applied in various AI/ML domains to improve generalization when the test distribution differs from the training distribution [27, 18, 26, 10, 45, 30]. Our framework is motivated by Qi et al. [30], which tackles KL-constrained DRO problem. However, our approach differs in two significant ways. First, instead of using a single KL constraint for the entire training dataset, we apply a separate KL constraint to each individual training data. Second, since each training data involves just two distributional variables, we can use a deterministic method to optimize these efficiently. Note that while our proposed method is inspired by DRO, it serves a distinct purpose: improving reward learning in RLHF, which is orthogonal to distributional robustness. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first outline the problem setup, derive the loss function with adaptive preference scaling, and then provide theoretical motivation for our proposed loss. At last, we present an optimization algorithm, extend the approach to direct preference optimization, and introduce a variant of our proposed loss that incorporates quadratic regularization. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a reward-free Markov decision process $\\mathcal{M}\\,=\\,(S,\\mathcal{A},p,\\gamma)$ with state $s\\ \\in{\\cal S}$ , action $a\\in{\\mathcal{A}}$ , state transition function $p$ , and discount factor $\\gamma$ . The ground truth reward function $r:$ $s\\times A\\,\\rightarrow\\,\\mathbb{R}$ is assumed to be unknown, but only human preferences over pairs of trajectory segments are observed. A trajectory segment is a sequence of consecutive state and action pairs $z\\stackrel{-}{=}\\{(s_{m},a_{m}),(s_{m+1},a_{m+1}),\\ldots,\\bar{(s_{k-1},a_{k-1})}\\}\\in(\\bar{S}\\times A)^{k-m}$ . We denote $z_{1}\\succ z_{2}$ to indicate that the human preferred trajectory segment $z_{1}$ over the trajectory segment $z_{2}$ and denote the preferred one with a subscript $w$ and the dispreferred one with a subscript $l$ (i.e., $z_{w}$ and $z_{l}$ ). Here, we are given a human preference dataset of trajectory segments $\\mathcal{D}_{\\mathrm{pref}}=(z_{w,i},z_{l,i})_{i=1}^{N}$ . Our goal is to find a reward $\\hat{r}(s,a)$ , which is well-aligned with human preferences. Once we learn the reward, we then find a policy $\\acute{\\pi}\\in\\Delta_{A}^{s}$ such that it maximizes the expected sum of discounted rewards, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\mathbb{E}_{z\\sim\\mathcal{D}_{\\pi}}\\left[\\hat{r}(z)\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{r}(z)=\\sum_{(s_{t},a_{t})\\in z}\\gamma^{t}\\hat{r}(s_{t},a_{t})}\\end{array}$ and ${\\mathcal{D}}_{\\pi}$ denotes the stationary distribution of the state-action pair induced by $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Reward learning with adaptive preference scaling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now focus on the reward learning phase in RLHF, a crucial stage for capturing human preferences across various trajectory segments. The standard reward learning procedure assumes that the reward function determines a preference distribution, also known as the Bradley-Terry (BT) model [8], ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{r}(z_{w}\\succ z_{l})=\\sigma(r(z_{w})-r(z_{l})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma$ denotes the sigmoid function. The reward function is then learned by minimizing the expectation of negative log-likelihood of $r$ over the preference data [12]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r}\\ \\mathcal{L}_{\\mathrm{pref}}(r)=-\\mathbb{E}_{(z_{w},z_{l})\\sim\\mathcal{D}_{\\mathrm{pref}}}\\left[\\log p_{r}(z_{w}\\succ z_{l})\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As can be seen from (1), the BT model essentially assumes that the logit of the preference distribution $\\sigma^{-1}(p_{r}(z_{w}\\;\\succ\\;z_{l}))$ scales linearly with the reward difference, regardless of the specific pair of samples. Such linear scaling, however, may not align well with downstream policy learning. Human preferences are often influenced by numerous factors that interact in non-linear ways, making the BT model suboptimal as a reward model. For example, when the reward difference is small, even slight changes in certain features might lead to significant shifts in preference. The BT model may struggle to capture such rapid shifts due to its slower transition. ", "page_idx": 3}, {"type": "text", "text": "To address this challenge, we propose an adaptive preference loss based on KL-constrained distributionally robust optimization formulation [30], which can implicitly change the scaling between the logit and the reward difference to be non-linear. Specifically, given a pair of trajectory segments $(z_{1},z_{2})$ , we denote $d_{r}(z_{1},z_{2})\\,=\\,{\\bf1}(z_{1}\\,\\succ\\,z_{2})\\cdot(r(z_{2})\\,-\\,r(z_{1}))$ and $\\boldsymbol{p}\\,=\\,\\left(p_{1},p_{2}\\right)$ . We define the following instance-level loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{r}(z_{1},z_{2}):=\\operatorname*{max}_{p\\in\\Delta_{2}}p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_{2},z_{1})-\\tau_{0}\\mathrm{KL}(p,1/2)\\qquad\\mathrm{s.t.}\\quad\\mathrm{KL}(p,1/2)\\leq\\rho_{0},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta_{2}=\\left\\{p\\in\\mathbb{R}^{2}:p_{1}+p_{2}=1,0\\le p_{1},p_{2}\\le1\\right\\}$ , $1/2$ is denoted for the uniform distribution, and $\\rho_{0},\\tau_{0}\\,>\\,0$ are shared prespecified parameters across all instances. $\\operatorname{KL}(\\cdot,\\cdot)$ denotes the KL divergence. Note that without the KL-constraint, (3) is reduced to the cross-entropy loss with $\\tau_{0}=1$ . Unlike general KL-constrained DRO formulation, which considers a distribution $p$ over all training samples, the distributional variable $p$ in (3) is associated specifically with binary preference comparisons for each pair. ", "page_idx": 3}, {"type": "text", "text": "We then convert (3) into an equivalent minimax formulation based on the Lagrangian duality, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda\\geq0}\\operatorname*{max}_{p\\in\\Delta_{2}}p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_{2},z_{1})-(\\lambda+\\tau_{0})(\\mathrm{KL}(p,1/2)-\\rho_{0}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda$ is the Lagrange multiplier. By defining $\\tau$ as $\\tau=\\lambda\\!+\\!\\tau_{0}$ and applying the optimality condition for $p$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\tau\\geq\\tau_{0}}\\ -\\ \\tau\\log p_{r,\\tau}(z_{w}\\succ z_{l})+(\\rho_{0}-\\log2)\\tau,}\\\\ &{\\quad\\quad p_{r,\\tau}(z_{w}\\succ z_{l})=\\sigma\\biggl(\\displaystyle\\frac{r(z_{w})-r(z_{l})}{\\tau}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "text", "text": "We refer to Appendix A.1 for the full derivation. Note that the preference scaling factor $\\tau$ in (4) and (5) serves as the Lagrange multiplier of (3). This scaling parameter $\\tau$ is used specifically for training the reward function $r$ , rather than calibrating the preference distribution $p_{r,\\tau}(z_{w}\\succ z_{l})$ . The scaler $\\tau$ is used exclusively during the reward learning phase and is no longer needed in subsequent policy optimization, where the reward function $r$ alone is used. ", "page_idx": 3}, {"type": "text", "text": "Moreover, the scaling parameter $\\tau$ is defined to be an instance-specific parameter corresponding to the pair of trajectory segments $(z_{w},z_{l})$ . Therefore, when applying our adaptive loss to reward learning, for each pair $(z_{w,i},z_{l,i})$ , we need to define a corresponding scaling parameter denoted by $\\tau_{i}$ The overall loss function over the training set $\\mathcal{D}_{\\mathrm{pref}}$ is as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{r,\\tau_{1},\\ldots,\\tau_{N}\\in\\Omega}}\\frac{1}{N}\\sum_{i=1}^{N}\\ell_{i}(r,\\tau_{i}):=\\frac{1}{N}\\sum_{i=1}^{N}\\big(-\\tau_{i}\\log p_{r,\\tau_{i}}(z_{w,i}\\succ z_{l,i})+\\rho\\tau_{i}\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T=(\\tau_{1},\\dots,\\tau_{N})$ , $\\Omega=\\{\\tau:\\tau_{0}\\leq\\tau\\leq\\tau_{\\mathrm{max}}\\}$ with $\\tau_{\\mathrm{max}}$ as another prespecified parameter, and $\\rho=\\rho_{0}-\\log2>-\\log2$ . Here, we also involve an upper bound $\\tau_{\\operatorname*{max}}>0$ in (6), and we will explain why it is needed in the next subsection. ", "page_idx": 3}, {"type": "text", "text": "3.3 Theoretical insights ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We next provide some theoretical insights on why the scaling parameter $\\tau$ can help gain adaptivity by a proposition. For simplicity, we only consider a pair of trajectories. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. Assume we have a pair of trajectories $z_{1},z_{2}$ , and the preference distribution $p(z_{1}\\,\\succ\\,z_{2})\\,=\\,p^{*}\\,\\in\\,(0,1)$ , i.e., the probability, that $z_{1}$ is preferred over $z_{2}$ , is $p^{*}$ . Consider the problem of minimizing the expectation of our adaptive loss function over the preference distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r,\\tau\\in\\Omega}-\\tau p^{*}\\log\\big(\\sigma\\big((r(z_{1})-r(z_{2}))/\\tau\\big)\\big)-\\tau(1-p^{*})\\log\\big(\\sigma\\big((r(z_{2})-r(z_{1}))/\\tau\\big)\\big)+\\rho\\tau.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then the minimizer $\\tau^{*}$ and $r^{*}$ of the expected loss satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau^{*}=\\left\\{\\!\\!\\begin{array}{l l}{\\tau_{0}}&{i f-p^{*}\\log(p^{*})-(1-p^{*})\\log(1-p^{*})+\\rho>0,}\\\\ {\\tau_{\\operatorname*{max}}}&{i f-p^{*}\\log(p^{*})-(1-p^{*})\\log(1-p^{*})+\\rho<0,}\\end{array}\\!\\!\\right.}\\\\ &{\\tau^{*}(z_{1})-r^{*}(z_{2})=\\tau^{*}\\sigma^{-1}(p^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\sigma^{-1}$ is the inverse of sigmoid function. ", "page_idx": 4}, {"type": "text", "text": "Note that the expected loss (7) is only for easing theoretical analysis, as $p^{*}$ is not accessible in practice. From Proposition 3.1, we can see that when $p^{*}$ is close enough to 0.5, (i.e., the uncertainty of preference is large), the corresponding optimal $\\tau^{*}$ is at the lower bound $\\tau_{0}$ . The resulting optimal reward difference is $\\tau_{0}\\sigma^{-1}(p^{*})$ , which is smaller than the counterpart obtained by the cross-entropy loss when $\\tau_{0}<1$ . Conversely, when $p^{*}$ is close to 0 or 1, (i.e., the uncertainty of preference is small), the resulting optimal $\\tau^{*}$ is at the upper bound $\\tau_{\\mathrm{max}}$ . Here, we introduce the upper bound $\\tau_{\\mathrm{max}}$ to ensure that the optimal $\\tau^{*}$ is bounded. The resulting reward difference in this case is $\\tau_{\\mathrm{max}}\\sigma^{-1}(p^{\\ast})$ , which is larger than the counterpart obtained by the cross-entropy loss when $\\tau_{\\operatorname*{max}}>1$ . Our theoretical analysis suggests that the adaptive scaling factor essentially changes the correspondence between the logit of preference distribution and the reward difference for each pair of trajectory segments, which could lead to a more flexible reward model. ", "page_idx": 4}, {"type": "text", "text": "We further visualize our adaptive preference loss in Figure 1, setting $\\tau_{0}$ and $\\tau_{\\mathrm{max}}$ to 0.1 and 5.0, respectively. As depicted, our adaptive preference loss behaves distinctly compared to the cross-entropy loss. With large learned reward differences, the cross-entropy tends to be very flat, while our loss maintains a non-trivial gradient, allowing us to continually decrease the loss function. In contrast, for small positive learned reward differences, our loss yields a smaller gradient, thereby ", "page_idx": 4}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/f1997e1a0a33d7183b6ecdacde298cdfa676c898614e619e4c28cc559299cd3d.jpg", "img_caption": ["Figure 1: Visualization of the loss function (left) and its gradient (right) on different reward differences. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "less encouraging the reward model to further distinguish pairs of ambiguous trajectory segments.   \nThis is consistent with our theoretical analysis. ", "page_idx": 4}, {"type": "text", "text": "3.4 Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present an efficient algorithm for solving (6). Suppose we parameterize $r$ as a neural network with parameter $\\phi$ . At the $m$ -th iteration, we have the iterate $\\phi^{(m)}$ , and we sample a pair of trajectory segments $z_{w,i}$ and $z_{l,i}$ . We initialize $\\tau_{i}^{(0)}=1$ and then optimize $\\tau_{i}$ by a projected Newton method subject to a simple interval constraint $\\Omega$ [6]. Specifically, for $k=0,...,K-1$ , we take ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau_{i}^{(k+1)}=\\prod_{\\tau_{i}\\in\\Omega}(\\tau_{i}^{(k)}+\\Delta_{i}^{(k)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\Delta}_{i}^{(k)}$ denotes the descent direction ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{i}^{(k)}=-\\frac{\\nabla_{\\tau_{i}}\\ell_{i}(\\phi^{(m)},\\tau_{i}^{(k)})}{\\nabla_{\\tau_{i}}^{2}\\ell_{i}(\\phi^{(m)},\\tau_{i}^{(k)})}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Once we get $\\tau_{i}^{(K)}$ , we update $\\phi$ by a stochastic gradient descent step ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi^{(m+1)}=\\phi^{(m)}-\\eta_{\\phi}\\nabla_{\\phi}\\ell_{i}(\\phi^{(m)},\\tau_{i}^{(K)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta_{\\phi}$ is the learning rate. We summarized our proposed algorithm in Algorithm 1, which is presented in a per-data manner for clarity but can be directly adapted for mini-batch learning. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Algorithm for reward learning with adaptive preference scaling ", "page_idx": 5}, {"type": "text", "text": "1: Input: $\\tau_{0}$ , $\\tau_{\\mathrm{max}}$ , $\\rho_{z}$ , $\\eta_{\\phi}$ ;   \n2: for $m=0,1,2,\\ldots,M-1$ do   \n3: Sample a pair of trajectory segements from $\\mathcal{D}_{\\mathrm{pref}}$ ;   \n4: Set $\\tau_{i}^{0}=1$ ;   \n5: for $k=0,1,2,\\ldots,K-1$ do   \n6: Compute $\\Delta_{i}^{(k)}$ using (9) and update $\\tau_{i}^{(k)}$ using (8);   \n7: end for   \n8: Update $\\phi^{(m)}$ using (10) or Adam-style step;   \n9: end for ", "page_idx": 5}, {"type": "text", "text": "Remark 3.1. Note that since $\\ell_{i}(\\phi,\\tau_{i})$ is strictly convex and univariate with respect to $\\tau_{i}$ , in each iteration $m$ , $\\tau_{i}^{(K)}$ is guaranteed to be near-optimal (i.e., $\\tau_{i}^{(K)}\\approx\\tau_{i}^{\\star})$ . Therefore, the convergence of Algorithm 1 can be guaranteed by the convergence of stochastic gradient descent on the reward model parameter $\\phi$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 3.2. Computationally, Algorithm 1 incurs negligible additional cost. The inner minimization problem (Lines 5-7 in Algorithm 1) can be solved to near-optimality efficiently within a few iterations (e.g. $K=5$ ) given its convex and univariate nature. The additional overhead of each update is minor compared to the overall RLHF pipeline. ", "page_idx": 5}, {"type": "text", "text": "3.5 Extension to direct preference optimization (DPO) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our adaptive preference scaling approach is generic and can be extended to DPO [31], which is another popular method for policy learning from human preferences. DPO directly learns the policy in supervised manner using the preference data of state-action pairs $\\mathcal{D}_{\\mathrm{pref}}=(s_{i},\\dot{a}_{w,i},a_{l,i})_{i=1}^{N}$ . This approach forgoes the need to learn the reward function explicitly by the reparameterization of reward function $r$ with respect to its optimal policy $\\pi_{r}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nr(s,a)=\\beta\\log(\\pi_{r}(a|s)/\\pi_{\\mathrm{ref}}(a|s))+\\beta\\log Z(s),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{Z(s)=\\sum_{a}\\pi_{\\mathrm{ref}}(a|s)\\exp\\left(r(s,a)/\\beta\\right)}\\end{array}$ and $\\pi_{\\mathrm{ref}}$ denotes the reference policy. By plugging in (11) back into  (2), we have the policy optimization problem ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi}\\mathcal{L}_{\\mathrm{DPO}}(\\pi)=-\\mathbb{E}_{(s,a_{w},a_{l})\\sim\\mathcal{D}_{\\mathrm{pref}}}\\log\\sigma\\big(\\beta r_{\\pi}(a_{w}|s)-\\beta r_{\\pi}(a_{l}|s)\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $r_{\\pi}(a|s)=\\log(\\pi(a|s)/\\pi_{\\mathrm{ref}}(a|s))$ denotes the log-probability ratio. ", "page_idx": 5}, {"type": "text", "text": "Similarly. we can integrate adaptive preference scaling into DPO by plugging in (11) into (6). By merging $\\beta$ with the $\\tau_{i}$ and $\\rho$ , we can further obtain the adaptive DPO (Ada-DPO) formulation as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau,\\tau_{1},\\ldots,\\tau_{N}\\in\\Omega}\\mathcal{L}_{\\mathrm{Ada-DPO}}(\\pi,\\tau_{1},...,\\tau_{N}):=\\frac{1}{N}\\sum_{i=1}^{N}\\Bigg[-\\tau_{i}\\log\\sigma\\Bigg(\\frac{r_{\\pi}(a_{w,i}|s_{i})-r_{\\pi}(a_{l,i}|s_{i})}{\\tau_{i}}\\Bigg)+\\rho\\tau_{i}\\Bigg].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 3.3. Note that the proposed adaptive preference loss can be further combined with other RLHF approaches, such as PEBBLE [24], SURF [29], and PARL [11], which still optimize the standard cross-entropy loss (see [24, Eq. (4)], [29, Eq. (3)], and [11, Eq. (5)]). ", "page_idx": 5}, {"type": "text", "text": "3.6 Extension to quadratic regularization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now introduce a variant of our adaptive preference loss that uses quadratic regularization for $\\tau$ . This modification removes the need for the hyperparameter $\\tau_{\\mathrm{max}}$ in $\\Omega$ , easing the tuning effort. We define the following instance-level adaptive preference loss with quadratic regularization: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\geq\\tau_{0}}\\,\\ell_{\\mathrm{quad}}(\\boldsymbol{r},\\tau):=-\\tau\\log p_{r,\\tau}(z_{w}\\succ z_{l})+\\rho_{0}\\tau^{2}-\\log2\\tau.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Compared to (4), which includes a linear regularization term of $(\\rho_{0}-\\log2)\\tau$ , (12) modifies the regularization term with coefficient $\\rho_{0}$ to be quadratic while keeping the term $\\log2\\tau$ linear. Additionally, in (12), the constraint on $\\tau$ only specifies a lower bound $\\tau_{0}$ and no longer includes an upper bound $\\tau_{\\mathrm{max}}$ . The following proposition provides theoretical insights for this modification. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.2. Assume we have a pair of trajectories $z_{1},z_{2}$ , and the preference distribution $p(z_{1}^{-}\\succ z_{2})=p^{*}\\in(0,1)$ . Consider the problem of minimizing the expectation of our adaptive loss function with quadratic regularization over the preference distribution:   \nr $\\operatorname*{min}_{\\tau\\geq\\tau_{0}}-\\tau p^{*}\\log{\\big(}\\sigma{\\big(}(r(z_{1})-r(z_{2}))/\\tau{\\big)}{\\big)}-\\tau(1-p^{*})\\log{\\big(}\\sigma{\\big(}(r(z_{2})-r(z_{1}))/\\tau{\\big)}{\\big)}+\\rho_{0}\\tau^{2}-\\log{2\\tau}.$ Then the minimizer $\\tau^{*}$ and $r^{*}$ of the expected loss satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau^{\\star}=\\operatorname*{max}\\{\\tau_{0},(p^{\\ast}\\log(p^{\\ast})+(1-p^{\\ast})\\log(1-p^{\\ast})+\\log2)/(2\\rho_{0})\\},}\\\\ &{r^{\\ast}(z_{1})-r^{\\ast}(z_{2})=\\tau^{\\ast}\\sigma^{-1}(p^{\\ast}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $\\sigma^{-1}$ is the inverse of sigmoid function. ", "page_idx": 6}, {"type": "text", "text": "Note that unlike the adaptive preference loss with linear regularization described in Proposition 3.1, the optimal value $\\tau^{\\star}$ for quadratic regularization does not involve the upper bound $\\tau_{\\mathrm{max}}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we examine the effectiveness of our adaptive preference loss based on robotic control and natural language generation tasks. Due to space limit, we defer the experiments with quadratic regularization, ablation studies, and discussions on hyperparameter selection to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.1 Robotic control ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experiment setup. We apply our proposed reward learning method on 3 robotic control tasks from the PyBullet [13] environments: HalfCheetah, Ant, and Hopper. These environments are similar to those available in OpenAI Gym [9] but they are known to be much harder to solve [34]. Similarly to Gao et al. [17], our setting is synthetic, where we use the ground truth rewards to provide preference labels on each pair of samples due to high expense of collecting human preferences. For the reward function, we use two-hidden-layer MLPs, each containing 64 hidden units. This configuration is aligned with the designs of both the policy and value networks. Following Christiano et al. [12], we repeat the following three steps for each stage: (i) We sample a set of trajectories by the policy $\\pi$ , and update the policy with proximal policy optimization (PPO, Schulman et al. [36]) alongside a reward function $\\hat{r}$ . (ii) We split the segments (the sequence of state-action pairs) into a training set and a testing set. Then, we randomly sample pairs of segments from the training set, and generate $\\mathcal{D}_{\\mathrm{pref}}$ with preference labels. We do the same to the testing set, and generate \u2032pref. (iii) We train the reward function $\\hat{r}$ on $\\mathcal{D}_{\\mathrm{pref}}$ , and use D\u2032pref for evaluating the preference prediction of r\u02c6. ", "page_idx": 6}, {"type": "text", "text": "For notational simplicity, we name our proposed adaptive preference scaling method for reward learning as \u201cAda-Pref\u201d. We compare Ada-Pref with the baseline method \u201cPref\u201d, which uses the standard cross-entropy loss for reward learning. For every 10000 timesteps the policy $\\pi$ runs, we evaluate the learned policy based on 20 test episodes. We also compute the average preference prediction accuracy of the learned reward function across stages. We set the budget to 3 million timesteps and perform training over 10 different seeds. For hyperparameter tuning in both reward learning and policy optimization, we apply two different criteria: 1) We identify the best policy based on its performance (the one with the highest return) and subsequently select the corresponding reward function. 2) We choose the best reward function based on its performance (the one with the highest average preference prediction accuracy) and then select the corresponding policy. Details of the implementations and hyperparameter tuning procedures are in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "Results. We summarize the results on three PyBullet tasks as follows: ", "page_idx": 6}, {"type": "text", "text": "Table 1 and Figure 2 illustrate the results for Pref and Ada-Pref on the PyBullet tasks, based on the first hyperparameter tuning criterion. In Table 1, we report the highest return of the best policy and the average preference accuracy of the corresponding reward function. We can see that Ada-Pref consistently outperforms Pref in terms of return on all three tasks and achieves comparable preference accuracy. The upper panel of Figure 2 shows the learning curve plots. We can see that Ada-Pref surpasses Pref at nearly every timestep and reaches a higher plateau across all tasks. The lower panel of Figure 2 presents percentile plots from different seeds to demonstrate individual run behaviors. As shown, we confirm that Ada-Pref consistently outperforms Pref at every percentile across all tasks. ", "page_idx": 6}, {"type": "text", "text": "Table 2 presents the results for Pref and Ada-Pref based on the second hyperparameter tuning criterion. From Table 2, we can see that both methods show a decrease in performance compared to Table 1, while Ada-Pref still outperforms Pref in terms of both preference accuracy and return on all three tasks. Furthermore, Ada-Pref demonstrates greater resistance to performance degradation than Pref, indicating its superior ability to align the learned reward function with policy optimization. This alignment allows for effective policy selection based on preference accuracy without the need to evaluate the policy using ground truth rewards. ", "page_idx": 6}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/6452a8221a5f32c0b5ebad93db183b6d85d4c35acc96e0a66464a4dfc90b3ba8.jpg", "img_caption": ["Figure 2: Learning curve plots (top) and percentile plots (bottom) for Pref and Ada-Pref. For the learning curve plots, returns at each timestep are averaged across 10 different seeds, then smoothed over timesteps using an exponential moving average (EMA) with a smoothing factor of $\\alpha=0.1$ . For the percentile plots, returns from 10 different seeds are sorted in ascending order. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "GnaFrZRHPf/tmp/115aa1a42fef73997e1b7dadbb7d39c550161f8f761cfb67072731ed83a82035.jpg", "table_caption": ["Table 1: Table for the highest return of the best policy and the average preference prediction accuracy of the corresponding reward function. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 2: Table for the average preference prediction accuracy of the best reward function and the highest return of the corresponding policy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Natural language generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experiment setup. We apply DPO with our proposed adaptive loss (Ada-DPO) method to two open-ended text generation tasks: summarization and single-turn dialogue. We adopt the Llama-2 7B model [40] as the backbone and conduct instruction tuning on each task to obtain the initial reference models. For summarization, the policy generates summaries given posts collected from Reddit. We use the flitered TL;DR summarization dataset [41] for instruction tuning, which contains more than 117K Reddit posts, each with a human-written summary. We apply the human preferences collected by Stiennon et al. [38] for preference optimization, where each transcript contains a pair of responses along with a preference label. For single-turn dialogue, the policy responds to various human queries ranging from simple questions to complex demands. We utilize the Anthropic Helpful and Harmless dialogue preferences dataset [3] for both instruction tuning and preference optimization. This dataset contains 170k human-AI dialogues, with each dialogue containing two AI responses and a human preference label. We use the preferred responses for instruction tuning and the full set of preferences for optimization. For instruction tuning stage, we fine-tune the entire Llama-2 model. For the alignment stage using Ada-DPO and different baselines, we apply LoRA fine-tuning for computational efficiency concerns, as we need to simultaneously tune multiple hyperparameters. ", "page_idx": 7}, {"type": "text", "text": "The rank of the LoRA adaptor is 64. We consider three baseline methods: DPO [31], $\\Psi$ Preference Optimization with Identity Mapping (IPO) [2] and Sequence Likelihood Calibration with Human Feedback (SLiC-HF) [47]. ", "page_idx": 8}, {"type": "text", "text": "As human evaluation is prohibitively expensive, we use Claude 3 [1], a proprietary large language model, to automatically evaluate responses based on summary quality and helpfulness/harmlessness for the summarization and dialogue tasks, respectively. Prior work has shown that Claude 3 and GPT-4 can effectively measure a quantitative improvement over the instruction-tuned model [15]. We split a small subset from each instruction tuning dataset for testing and calculate the win rate against the instruction-tuned reference model as the evaluation metric. The percentage of instances where the response generated by policy A is preferred over policy B is referred to as the win rate of A against B. We also split a subset from each preference optimization dataset to validate the preference prediction accuracy. Details of the implementations and hyperparameter selections are in Appendix B.2. ", "page_idx": 8}, {"type": "text", "text": "Results. We summarize the results on the two natural language generation tasks as follows: ", "page_idx": 8}, {"type": "text", "text": "In Figure 3, we select the model with the highest win rate and present the win rate and its preference accuracy for all baselines. We observe that Ada-DPO outperforms the other baselines on both tasks in terms of win rate and achieves comparable preference accuracy. In Figure 4, we display the performance of the model selected with the highest accuracy (not win rate). As shown, Ada-DPO achieves a significant improvement beyond the DPO baseline in terms of win rate and obtains a comparable preference accuracy. This again indicates that Ada-DPO yields better alignment between the learned reward function and policy optimization, allowing good policy selection based on preference accuracy without a proprietary LLM judge. ", "page_idx": 8}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/8b52e0546a47cab86d13168d2f86f2c1756830cd5ead44435de61df6c11b1267.jpg", "img_caption": ["prediction accuracy of the corresponding model. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/6c15cd9926c6aca695f4fbc5ad81372d0fb90704c88821608d53181580685377.jpg", "img_caption": ["and the win rate of the corresponding model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Detailed analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present detailed analyses of Ada-Pref and Ada-DPO for both the Ant and summarization tasks. Figure 5(a) presents a histogram of the learned scaling factors $\\tau$ for the Ant task. We can see that around $60\\%$ of these scaling factors reach the upper bound, while about $10\\%$ converge to the lower bound, and the rest are distributed across the region. In Figure 5(b), we explore the relationship between preference strength and the learned scaling factors $\\tau$ , and in Figure 5(c), we investigate the relationship between preference strength and the learned reward difference for Pref and Ada-Pref. We measure preference strength using the true reward difference, categorize it into five percentile bins, and then bin the scaling factors and the learned reward differences accordingly to compute the average. As can be seen, the learned scaling factor increases monotonically with preference strength, demonstrating that the our method successfully adapts the loss scaling to the varying degrees of preference in the data. Furthermore, Ada-Pref learns smaller reward differences for pairs with ambiguous preferences and learns larger reward differences for those with strong preferences, compared to Pref. This indicates that our method leads to a more flexible reward function. ", "page_idx": 8}, {"type": "text", "text": "In Figure 6(a), we plot a histogram of the learned scaling factors $\\tau$ for the summarization task. We can see that around $40\\%$ of the scaling factors converge to the upper bound, with the rest distributed across the region. We also display the relationship between the confidence scores and the scaling factors in Figure 6(b). The confidence score is an integer from 1 to 4 included in the dataset, and a higher score denotes a stronger preference. We bin the scaling factors based on confidence scores and compute the average. As shown, the scaling factors positively correlate with confidence scores, justifying that we learn larger $\\tau$ for strong preferences and smaller $\\tau$ for ambiguous ones. ", "page_idx": 8}, {"type": "text", "text": "We further present two pairs of preference samples where Ada-DPO assigns large or small scaling factors in Figure 7. We observe that the sample pair with a large scaling factor shows a strong ", "page_idx": 8}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/1efaefe4640c84f9e0fd3638c0a43dc8dbea409251e21a053d42e3e4c1c0f05e.jpg", "img_caption": ["Figure 5: Histogram of learned scaling factors, relationship between preference strength and the learned scaling factors, and relationship between preference strength and the learned reward difference. All plots are from the Ant task. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/429ddd183d3b281494c182463b49ccbd93dbdb51ad16fcc7b8d9a2374d841ee2.jpg", "img_caption": ["(a) Histogram of $\\tau$ (b) Confidence score and $\\tau$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 6: Histogram of learned scaling factors and relationship between the confidence scores and the learned scaling factors. Both plots are from the summarization task. ", "page_idx": 9}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/0d670b6dabc5c3eb9f94bda882f80127fb05da1142f3eb8295132d4f8448720d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 7: Examples of preference sample pairs with large (left) and small (right) scaling factors $\\tau$ , and the comparison of the learned reward difference. The preferred (chosen) responses are colored by green and the rejected responses are colored by red. ", "page_idx": 9}, {"type": "text", "text": "preference, as the rejected response is nonsensical while the chosen one is clear. Ada-DPO learns a larger reward difference for such data, while it is much smaller with DPO. Conversely, for the sample pair with a small scaling factor, the two responses are very similar, indicating its ambiguity. Ada-DPO learns a small reward difference on this pair, while DPO gets a large reward difference. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "RLHF is an emerging challenge in machine learning. Prior to the popularity of models like ChatGPT, research on designing proper loss functions for reward learning was limited. To bridge this gap, we explore uncertainties in underlying preference strengths and propose an adaptive preference loss function. This loss function incorporates instance-specific scaling factors to modulate the correspondence between reward differences and preference distributions. Taking the result in this paper as an initial start, we expect more sophisticated and stronger follow-up work that applies to RLHF with similar structures. All of these efforts may ultimately assist in developing more principled RLHF methods to better control risks associated with advanced AI systems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Anthropic. Claude, 2024. URL https://www.anthropic.com. ", "page_idx": 10}, {"type": "text", "text": "[2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 4447\u20134455. PMLR, 2024. [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learningvfrom human feedback. CoRR, abs/2204.05862, 2022. [4] Sergio A Balanya, Juan Maro\u00f1as, and Daniel Ramos. Adaptive temperature scaling for robust calibration of deep neural networks. Neural Computing and Applications, pages 1\u201323, 2024. [5] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341\u2013357, 2013.   \n[6] Dimitri P Bertsekas. Projected newton methods for optimization problems with simple constraints. SIAM Journal on control and Optimization, 20(2):221\u2013246, 1982. [7] Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. Mathematical Programming, 167:235\u2013292, 2018. [8] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. [9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n[10] Samuel Broscheit, Quynh Do, and Judith Gaspers. Distributionally robust finetuning bert for covariate drift in spoken language understanding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 1970\u20131985, 2022.   \n[11] Souradip Chakraborty, Amrit Bedi, Alec Koppel, Huazheng Wang, Dinesh Manocha, Mengdi Wang, and Furong Huang. Parl: A unified framework for policy alignment in reinforcement learning. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n[12] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[13] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning, 2016-2019. URL https://pybullet.org/.   \n[14] Zhipeng Ding, Xu Han, Peirong Liu, and Marc Niethammer. Local temperature scaling for probability calibration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6889\u20136899, 2021.   \n[15] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. Mathematics of Operations Research, 46(3): 946\u2013969, 2021.   \n[17] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835\u201310866. PMLR, 2023.   \n[18] Tejas Gokhale, Abhishek Chaudhary, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. Semantically distributed robust optimization for vision-and-language inference. In 60th Annual Meeting of the Association for Computational Linguistics, pages 1493\u20131513, 2022.   \n[19] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[21] Tom Joy, Francesco Pinto, Ser-Nam Lim, Philip HS Torr, and Puneet K Dokania. Sampledependent adaptive temperature scaling for improved calibration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 14919\u201314926, 2023.   \n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.   \n[23] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh ShafieezadehAbadeh. Wasserstein distributionally robust optimization: Theory and applications in machine learning. In Operations research & management science in the age of analytics, pages 130\u2013166. Informs, 2019.   \n[24] Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In International Conference on Machine Learning, 2021.   \n[25] Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, and Eduard Hovy. Softmax qdistribution estimation for structured prediction: A theoretical interpretation for raml. arXiv preprint arXiv:1705.07136, 2017.   \n[26] Paul Michel, Tatsunori Hashimoto, and Graham Neubig. Modeling the second player in distributionally robust optimization. International Conference on Learning Representations, 2021.   \n[27] Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust language modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4227\u20134237, 2019.   \n[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[29] Jongjin Park, Younggyo Seo, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. SURF: Semi-supervised reward learning with data augmentation for feedback-efficient preference-based reinforcement learning. In International Conference on Learning Representations, 2022.   \n[30] Qi Qi, Jiameng Lyu, Kung-Sik Chan, Er-Wei Bai, and Tianbao Yang. Stochastic constrained dro with a complexity independent of sample size. Transactions on Machine Learning Research, 2023.   \n[31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[32] Antonin Raffin. Rl baselines3 zoo, 2020. URL https://github.com/DLR-RM/ rl-baselines3-zoo.   \n[33] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1\u20138, 2021.   \n[34] Antonin Raffin, Jens Kober, and Freek Stulp. Smooth exploration for robotic reinforcement learning. In Conference on Robot Learning, pages 1634\u20131644. PMLR, 2022.   \n[35] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.   \n[36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[37] Ziang Song, Tianle Cai, Jason D. Lee, and Weijie J. Su. Reward collapse in aligning large language models. CoRR, abs/2305.17608, 2023.   \n[38] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. Advances in Neural Information Processing Systems, 2020.   \n[39] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \n[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[41] Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59\u201363, 2017.   \n[42] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning, 2020. URL https: //github.com/huggingface/trl.   \n[43] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2495\u20132504, 2021.   \n[44] Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, and Da-Chang Juan. Contextual temperature for language modeling. arXiv preprint arXiv:2012.13575, 2020.   \n[45] Hongyi Wen, Xinyang Yi, Tiansheng Yao, Jiaxi Tang, Lichan Hong, and Ed H Chi. Distributionally-robust recommendations for improving worst-case user experience. In Proceedings of the ACM Web Conference 2022, pages 3606\u20133610, 2022.   \n[46] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 2020.   \n[47] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback. CoRR, abs/2305.10425, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Derivation and proofs of Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Derivation of Equation (4) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this subsection, we present the full derivation of Equation (4). Recall the following loss: $\\ell_{r}(z_{1},z_{2})=\\operatorname*{max}_{p\\in\\Delta_{2}}\\dot{p}_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_{2},z_{1})-\\dot{\\tau}_{0}\\mathrm{KL}(p,1/2)$ s.t. $\\mathrm{KL}(p,\\bar{1}/2)\\le\\rho_{0}$ ", "page_idx": 13}, {"type": "text", "text": ". Using the Lagrangian duality, we have ", "page_idx": 13}, {"type": "equation", "text": "$\\operatorname*{max}_{p\\in\\Delta_{2}}\\operatorname*{min}_{\\lambda\\geq0}p_{1}d_{r}(z_{1}^{<},z_{2})+p_{2}d_{r}(z_{2},z_{1})-\\tau_{0}\\mathrm{KL}(p,1/2)-\\lambda(\\mathrm{KL}(p,1/2)-\\rho_{0}).$ ", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By strong duality theorem, we have ", "page_idx": 13}, {"type": "text", "text": "$\\operatorname*{min}_{\\lambda\\geq0}\\operatorname*{max}_{p\\in\\Delta_{2}}p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_{2},z_{1})-\\tau_{0}\\mathrm{KL}(p,1/2)-\\lambda(\\mathrm{KL}(p,1/2)-\\rho_{0}),$ ", "page_idx": 13}, {"type": "text", "text": "which is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\varepsilon\\in\\Delta_{2}}p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_{2},z_{1})-(\\lambda+\\tau_{0})(\\mathrm{KL}(p,1/2)-\\rho_{0})-\\tau_{0}\\rho_{0}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We let $\\tau=\\lambda+\\tau_{0}$ and obtain ", "page_idx": 13}, {"type": "equation", "text": "$\\begin{array}{r l}&{\\underset{\\b{\\geq}\\tau_{0}}{\\operatorname*{in}}\\underset{p\\in\\Delta_{2}}{\\operatorname*{max}}\\ p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_{2},z_{1})-\\tau(\\mathrm{KL}(p,1/2)-\\rho_{0})-\\tau_{0}\\rho_{0}.}\\\\ &{\\qquad\\qquad\\qquad\\ldots\\qquad\\ldots\\qquad\\ldots\\qquad\\ldots\\qquad\\quad.}\\end{array}$ ", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now, we consider the optimality conditions for the inner constrained maximization problem by defining the following Lagrangian function: ", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{r,\\tau}(p,\\mu)=p_{1}d_{r}(z_{1},z_{2})+p_{2}d_{r}(z_{2},z_{1})-\\tau(\\mathrm{KL}(p,1/2)-\\rho_{0})-\\mu\\biggl(\\sum_{k=1}^{2}p_{k}-1\\biggr),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mu$ is the Lagrange multiplier. The optimal solutions $p^{r,\\tau}$ to the inner maximization problem satisfy the following KKT conditions: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{d_{r}(z_{1},z_{2})}}-\\tau(\\log(p^{r,\\tau})+1)-\\mu=0,}\\\\ &{d_{r}(z_{2},z_{1})-\\tau(\\log(p^{r,\\tau})+1)-\\mu=0,}\\\\ &{\\mathrm{~and~}\\displaystyle\\sum_{k=1}^{2}p_{k}^{r,\\tau}=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{1}^{r,\\tau}=\\frac{\\exp\\left(d_{r}(z_{1},z_{2})/\\tau\\right)}{\\exp\\left(d_{r}(z_{1},z_{2})/\\tau\\right)+\\exp\\left(d_{r}(z_{2},z_{1})/\\tau\\right)}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Plugging in $p_{1}^{r,\\tau}$ and $p_{2}^{r,\\tau}$ back into the inner maximization problem, we have ", "page_idx": 13}, {"type": "equation", "text": "$\\operatorname*{min}_{r\\searrow r\\to\\infty}\\tau\\log\\Big\\langle\\exp\\big(d_{r}(z_{1},z_{2})/\\tau\\big)+\\exp\\big(d_{r}(z_{2},z_{1}\\big\\rangle/\\tau\\big)\\big)-\\tau\\log2+(\\tau-\\tau_{0})\\rho_{0}.$ ", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Without loss of generality, we let $z_{1}=z_{w}$ and $z_{2}=z_{l}$ , and obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\geq\\tau_{0}}\\;-\\tau\\log\\sigma\\Big(\\frac{r(z_{w})-r(z_{l})}{\\tau}\\Big)+(\\rho_{0}-\\log2)\\tau,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\sigma$ is the logistic function. This completes the derivation. ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first derive the expectation of the adaptive loss. By taking expectation of (4) with $\\rho=\\rho_{0}-\\log2$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathfrak{K}}{1,z}\\left\\{\\mathbf{1}(z_{1}\\sim z_{2})\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{1})-r(z_{2}))/\\tau\\big)\\big)+\\rho\\tau\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathbf{1}(z_{2}\\succ z_{1})\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{2})-r(z_{1}))/\\tau\\big)\\big)+\\rho\\tau\\big]\\right\\}}\\\\ &{=p^{*}\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{1})-r(z_{2}))/\\tau\\big)\\big)+\\rho\\tau\\big]+(1-p^{*})\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{2})-r(z_{1}))/\\tau\\big)\\big)+\\rho\\tau\\big]}\\\\ &{=-\\tau p^{*}\\log\\big(\\sigma\\big((r(z_{1})-r(z_{2}))/\\tau\\big)\\big)-\\tau(1-p^{*})\\log\\big(\\sigma\\big((r(z_{2})-r(z_{1}))/\\tau\\big)\\big)+\\rho\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the optimality condition of $r(z_{1})-r(z_{2})$ and $\\tau$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nr(z_{1})-\\stackrel{.}{r}(z_{2})=\\tau\\sigma^{-1}(p^{*}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "ere $\\sigma^{-1}$ is the inverse of sigmoid function. Plugging (13) into the objective in (7), we obtain $\\operatorname*{min}_{\\tau\\in\\Omega}\\tilde{[}-p^{*}\\log(p^{*})-(1-p^{*}\\tilde{)}\\log(1-p^{*})+\\tilde{\\rho}]\\,\\tau$ , ", "page_idx": 13}, {"type": "text", "text": "whose objective is essentially linear in $\\tau$ . Hence, when $-p^{*}\\log(p^{*})\\!-\\!(1\\!-\\!p^{*})\\log(1\\!-\\!p^{*})\\!+\\!\\rho>0$ , the corresponding optimal $\\tau^{*}$ is at the lower bound $\\tau_{0}$ and the optimal reward difference $r^{*}(z_{1})\\!-\\!r^{*}(z_{2})=$ $\\tau_{0}\\sigma^{-1}(p^{*})$ given the optimality condition. Conversely, when $-p^{*}\\log(p^{*})\\!-\\!(1\\!-\\!p^{*})\\log(1\\!-\\!p^{*})\\!+\\!\\rho<$ 0, we have ${\\tau}^{*}={\\tau}_{\\operatorname*{max}}$ and $r^{*}(z_{1})-r^{*}(z_{2})=\\tau_{\\mathrm{max}}\\bar{\\sigma}^{-1}(p^{*})$ . This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.3 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first derive the expectation of the adaptive loss with quadratic regularization. By taking expectation of (12), we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{1,z_{2}}{\\mathbb{R}}\\big\\{\\mathbf{1}(z_{1}\\sim z_{2})\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{1})-r(z_{2}))/\\tau\\big)\\big)+\\rho_{0}\\tau^{2}-\\log2\\tau\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbf{1}(z_{2}\\succ z_{1})\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{2})-r(z_{1}))/\\tau\\big)\\big)+\\rho_{0}\\tau^{2}-\\log2\\tau\\big]\\big\\}}\\\\ &{=p^{*}\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{1})-r(z_{2}))/\\tau\\big)\\big)+\\rho_{0}\\tau^{2}-\\log2\\tau\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,(1-p^{*})\\big[-\\tau\\log\\big(\\sigma\\big((r(z_{2})-r(z_{1}))/\\tau\\big)\\big)+\\rho_{0}\\tau^{2}-\\log2\\tau\\big]}\\\\ &{=-\\tau p^{*}\\log\\big(\\sigma\\big((r(z_{1})-r(z_{2}))/\\tau\\big)\\big)-\\tau(1-p^{*})\\log\\big(\\sigma\\big((r(z_{2})-r(z_{1}))/\\tau\\big)\\big)+\\rho_{0}\\tau^{2}-\\log2\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the optimality condition of $r(z_{1})-r(z_{2})$ and $\\tau$ , we have ", "page_idx": 14}, {"type": "text", "text": "where $\\sigma^{-1}$ is the inverse of sigmoid function. Plugging (15) into the objective in (14), we obtain $\\operatorname*{min}_{\\tau\\geq\\tau_{0}}\\big[-p^{*}\\log(p^{*})-(1-p^{*})\\operatorname{\\bar{l}o g}(\\bar{1}-p^{*})-\\log2\\big]\\bar{\\tau}+\\rho_{0}\\tau^{2}.$ ( ", "page_idx": 14}, {"type": "text", "text": "Note that (16) is always bounded without the need of an upper bound of $\\tau$ . Specifically, with any $p^{\\star}\\in(0,1)$ , we have $-p^{*}\\log(p^{*})\\!-(1\\!-\\!p^{*})\\log(1\\!-\\!p^{*})\\!-\\!\\log2\\leq0$ and $\\tau^{\\star}=\\operatorname*{max}\\{\\tau_{0},(p^{\\ast}\\log(p^{\\ast})+$ $\\left(1-p^{*}\\right)\\log(1-p^{*})+\\log2)/(2\\rho_{0})\\}$ . This completes the proof. ", "page_idx": 14}, {"type": "text", "text": "B Implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Robotic control ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our implementations of robotic control tasks are based on Stable-Baselines3 [33] and RL Zoo training framework [32]. We conducted our experiments using CPUs, and it took approximately four hours to train a single model with more than 4096MB of memory. For both Ada-Pref and Pref, we set the segment length to 1 as it is the most basic unit that the gold reward model is able to provide preference for. Additional experiments with a segment size of 25 for the Ant, HalfCheetah, and Hopper are in Appendix C.2. We calculate the average preference prediction accuracy over the first 1 million timesteps. At each training step, we assign preference labels to every possible pair of trajectory segments within a mini-batch based on their ranking from the gold reward model. We set the batch size to 64 for the HalfCheetah and Ant tasks and 4 for the Hopper task. We tune the number of epochs in $\\{1,3,5\\}$ . We use Adam optimizer [22] and tune the learning rate in $\\{5e-3,1e-3,5e-4,1\\bar{e}-4\\}$ for the Ant and HalfCheetah, and set the learning rate to $1e-2$ for the Hopper. For Ada-Pref, we tune the $\\tau_{\\mathrm{max}}$ in $\\{1.0,3.0\\}$ and the $\\rho_{0}$ in $\\{0.1,0.3,0.5\\}$ . We fix $\\tau_{0}=0.1$ and the number of Newton iterations to 3 for all experiments. Details of the chosen hyperparameters for reward learning for all three tasks are summarized in Tables 3 and 4. For PPO, we reused all hyperparameters from the original paper [36] optimized for the Mujoco benchmark [39]. Details of the hyperparameters for PPO are in Table 5. ", "page_idx": 14}, {"type": "table", "img_path": "GnaFrZRHPf/tmp/9c39e5de849f1d5ec80094101fbdb7ff3b979c87c9af0305c607433c6caed1a1.jpg", "table_caption": ["Table 3: Chosen hyperparameters for reward learning used for Table 1. ", "Table 4: Chosen hyperparameters for reward learning used for Table 2. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 Natural language generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our implementations of natural language generation tasks are based on transformers [46] and trl training framework [42]. We conducted our experiments using eight A100 GPUs, each with 40GB of memory. Training a single model took approximately two hours. We provide more details on each task as follows: ", "page_idx": 14}, {"type": "table", "img_path": "GnaFrZRHPf/tmp/4679b2113ace416af0afa6d2080ebe660cf67e211867e2a349e7ddb66a8e517f.jpg", "table_caption": ["Table 5: Chosen hyperparameters for PPO. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2.1 Summarization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For the instruction tuning stage, we randomly select 800 data from the flitered TL;DR summarization dataset [41] for testing the policy and leave the rest for supervised tuning. In the preference optimization stage, we split the preference dataset [38] into a training and testing set to evaluate the preference accuracy. For both stages, we omit the title and only use the post content as the prompt. The prompt format follows: \"POST: post content. $\\backslash\\mathfrak{n}\\backslash\\mathfrak{n}\\mathrm{TL};\\mathrm{DR};^{\\prime\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "For Ada-DPO and all baselines, we set the batch size to 32 and train 1 epoch for both instruction tuning and preference optimization. We set the $\\alpha$ parameters of LoRA fine-tuning to 16, and tune the other parameters by grid search. The learning rate is tuned in $\\{5e-6,5e-5,1e-4,5e-4\\}$ . SLiC-HF, IPO and DPO include parameter $\\beta$ , which is tuned in a range of $\\{0.01,0.1,0.3,0.5\\}$ . For Ada-DPO, we tune the $\\rho_{0}$ in $\\left\\lbrace0.05,0.1,0.3,0.5\\right\\rbrace$ and the $\\tau_{\\mathrm{max}}$ in $\\{1.0,4.0,5.0,10.0\\}$ . We fix $\\tau_{0}=0.1$ and the number of Newton iterations to 5 for all experiments. The best Ada-DPO is achieved with $l r=5e-5$ , $\\rho_{0}=0.1$ , and $\\tau_{\\operatorname*{max}}=4.0$ . ", "page_idx": 15}, {"type": "text", "text": "B.2.2 Single-turn dialogue ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the original training split in the Anthropic Helpful and Harmless dialogue preferences dataset [3] for training in both stages. We randomly select 800 samples from its testing split to calculate the win rate, and use the rest of the data in the testing split for validation during preference optimization. We use the original data format. ", "page_idx": 15}, {"type": "text", "text": "In the dialogue task, we use the same batch size of 32 and 1 epoch for training. The learning rate is tuned in $\\{5e-6,5e-5,1e-4\\}$ . The parameter $\\beta$ for baselines is tuned in a range of $\\{0.01,0.1,0.3\\}$ . For Ada-DPO, we tune the $\\rho_{0}$ in $\\{0.05,0.1,0.3,0.5\\}$ , $\\tau_{\\mathrm{max}}$ in $\\{1,5,10\\}$ and fix $\\tau_{0}=0.1$ . The best Ada-DPO is achieved with $l r=5e-5$ , $\\rho_{0}=0.05$ , and $\\tau_{\\operatorname*{max}}=5.0$ . ", "page_idx": 15}, {"type": "text", "text": "B.2.3 Evaluation prompt. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We apply Claude 3 for win rate judgments. Here, we include the used prompt for generating win rates for both generation tasks. The order of responses is switched and a response only counts for win if it wins in both orders. ", "page_idx": 15}, {"type": "text", "text": "Summarization win rate prompt. ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/7e9307d2ca6abdd89db53c95a6cdf00cbc366dc16597ae214e2be2abda68e225.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Dialogue win rate prompt. ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/bf36545009fceedad8e950416155a95a5d3ace210219710a59905b086dfc6586.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Additional experiments and discussions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Experiments with quadratic regularization ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/3bd1f3783b52dd421695d4d0f01dbe37e5ddcb033e245f67b4002e79ae84a7fe.jpg", "img_caption": ["Figure 8: Left: Learning curve and percentile plot for Pref and Ada-Pref-Quad. Middle: Histogram of the learned scaling factors. Right: Relationship between preference strength and the learned scaling factors, and relationship between preference strength and the learned reward difference. All plots are from the Ant task. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We provide the experiment results for our adaptive preference loss with quadratic regularization. Here, we name the method as \u201cAda-Pref-Quad\" and the one applied to DPO as \u201cAda-DPO-Quad\". Table 6 and Figure 8 show the results for Pref and Ada-Pref-Quad on the Ant task, and DPO and Ada-DPO-Quad on the single-turn dialogue. In Table 6, we report the performance of the best policy and the preference prediction accuracy of the corresponding reward function. From Table 6, we can see that Ada-Pref-Quad outperforms Pref on the Ant task, and Ada-DPO-Quad surpasses DPO on the single-turn dialogue in terms of return and win rate, respectively. Figures 8(a) and 8(b) present the learning curve and the percentile plot for the Ant task. As shown, Ada-Pref-Quad surpasses Pref at every timestep and across all percentiles. Figure 8(c) shows a histogram of the learned scaling factors $\\tau$ . Compared to Figure 5(a), we can see much smoother distribution of $\\tau$ due to the quadratic regularization. Figures 8(d) and 8(e) illustrate the relationship between the learned scaling factors $\\tau$ and preference strength, and the relationship between the learned reward difference and preference strength. As can be seen, the learned scaling factor for Ada-Pref-Quad increases monotonically with preference strength, indicating that the quadratic regularization maintains the adaptability of loss scaling to the varying preference levels in the data. Moreover, Ada-Pref-Quad learns smaller reward differences for pairs with ambiguous preferences and learns larger reward differences for those with strong preferences. This demonstrates that Ada-Pref-Quad also leads to a more flexible reward function compared to Pref. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "GnaFrZRHPf/tmp/9b714118970cb24ea535fb9c051591bb510bd82e994c94a75fb10676570868a6.jpg", "table_caption": ["Table 6: Table for the highest return (left) and the best win rate (right) of the best policy and the average preference prediction accuracy of the corresponding reward function. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 Ablation studies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this subsection, we present the results for three PyBullet tasks, using a segment size of 25. Table 7 and Figure 9 show the performance of Pref and Ada-Pref on the PyBullet tasks, based on the first hyperparameter tuning criterion. Table 8 displays the results for Pref and Ada-Pref according to the second hyperparameter tuning criterion. These results reconfirm the effectiveness of our adaptive preference loss. ", "page_idx": 17}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/7c1e66550b83328bb7ba07ea395d13483bd9d192c5e2dc8c261394dae1ad905f.jpg", "img_caption": ["Figure 9: Learning curve plots (top) and percentile plots (bottom) for Pref and Ada-Pref. For the learning curve plots, returns at each timestep are averaged across 10 different seeds, then smoothed over timesteps using an exponential moving average (EMA) with a smoothing factor of $\\alpha=0.1$ . For the percentile plots, returns from 10 different seeds are sorted in ascending order. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.3 Discussions on hyperparameter tuning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Compared to the cross-entropy loss, our method needs three additional hyperparameters: the bounds on the scaling factors $\\tau_{0}$ and $\\tau_{\\mathrm{max}}$ , and the regularization parameter $\\rho$ . In our experiments, we fixed $\\tau_{0}$ at 0.1 without tuning it, as this value worked well for all five tasks. We did tune $\\tau_{\\mathrm{max}}$ to adjust the scale of $\\tau$ , but this can be avoided by using the quadratic regularization formulation described in Section 3.6. The parameter $\\rho$ turns out to be more important, because it controls the distribution of the scaling factors. We performed a careful grid search to tune $\\rho$ in our experiments. Figure 10 shows the hyperparameter sensitivity of $\\rho$ on the Ant and summarization tasks. Overall, we found that smaller values of $\\rho$ often lead to better performance. ", "page_idx": 17}, {"type": "table", "img_path": "GnaFrZRHPf/tmp/8914706e63136dd635a631d00a89edb26994c0f3e0901a399da5f7b307af6a72.jpg", "table_caption": ["Table 7: Table for the highest return of the best policy and the average preference prediction accuracy of the corresponding reward function. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "GnaFrZRHPf/tmp/4a7b9e2a229486437f98b7e4b1029bcf0f5d8eb6d8d51cb7d89c23b51e18c743.jpg", "table_caption": ["Table 8: Table for the average preference prediction accuracy of the best reward function and the highest return of the corresponding policy. "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "GnaFrZRHPf/tmp/4986dd74157e671a431d7bd9a77ef0b2e6268db6a2b21fb7940cc77187a85f03.jpg", "img_caption": ["Figure 10: Hyperparameter sensitivity of $\\rho$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Contributions and scope are explained in the methodology section 3 and well-supported by results presented in section 4. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The full derivation of the proposed loss and the proofs for the propositions are provided in Appendix A. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The proposed algorithm is summarized in Appendix B and the experiment details are included in Section 4 and Appendix C. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] Justification: We will release the code after the submission deadline. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The basic experiment settings are included in Section 4 and more implementation details are in Appendix C. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Error bars are included in the robotic control experiments to indicate variability. Additionally, percentile plots are provided to support the validity of the experimental results. See Figures 2, 7, and 8 for reference. Only one seed is used for natural language tasks due to computational concerns. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See implementation details in Appendix C. ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This research adheres to the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted. ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper does not release data or models. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The used models and data are properly credited in experimental settings. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}]