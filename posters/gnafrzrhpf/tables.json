[{"figure_path": "GnaFrZRHPf/tables/tables_7_1.jpg", "caption": "Table 1: Table for the highest return of the best policy and the average preference prediction accuracy of the corresponding reward function.", "description": "This table presents the results of the robotic control experiments.  For each task (HalfCheetah, Ant, Hopper), it shows the highest return achieved by the best policy found using two different methods: Pref (baseline using cross-entropy loss) and Ada-Pref (the proposed method using adaptive preference scaling).  It also provides the average preference prediction accuracy of the corresponding reward function for each method.  The data demonstrates the improvement of Ada-Pref over Pref in terms of both policy performance (return) and reward function alignment (accuracy).", "section": "4.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/tables/tables_14_1.jpg", "caption": "Table 3: Chosen hyperparameters for reward learning used for Table 1.", "description": "This table lists the hyperparameters used for reward learning when obtaining the results shown in Table 1.  The hyperparameters shown are for the Pref and Ada-Pref methods across three different robotic control tasks (HalfCheetah, Ant, and Hopper) from the PyBullet environment.  The values represent the number of epochs, learning rate, T_max (maximum scaling factor), and \u03c1_0 (KL constraint parameter).", "section": "B.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/tables/tables_15_1.jpg", "caption": "Table 5: Chosen hyperparameters for PPO.", "description": "This table lists the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm in the robotic control experiments.  It specifies settings for the optimizer, discount factor, value function coefficient, entropy coefficient,  gradient norm, learning rate schedule, advantage normalization, clip range for the value function, number of steps per rollout, initial log standard deviation, learning rate, number of epochs, mini-batch size, non-linearity, generalized advantage estimation (GAE) coefficient, clip range, and orthogonal initialization.", "section": "B.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/tables/tables_17_1.jpg", "caption": "Table 6: Table for the highest return (left) and the best win rate (right) of the best policy and the average preference prediction accuracy of the corresponding reward function.", "description": "This table presents the results of the Ant and Dialogue tasks.  For the Ant task, it shows the return and preference accuracy for Pref and Ada-Pref-Quad. For the Dialogue task, it shows the win rate and preference accuracy for DPO and Ada-DPO-Quad.  It summarizes the performance of the best policies and their corresponding reward functions based on two different evaluation metrics (return and win rate).", "section": "C.1 Experiments with quadratic regularization"}, {"figure_path": "GnaFrZRHPf/tables/tables_18_1.jpg", "caption": "Table 7: Table for the highest return of the best policy and the average preference prediction accuracy of the corresponding reward function.", "description": "This table presents the results of the Pref and Ada-Pref methods on three robotic control tasks (HalfCheetah, Ant, and Hopper).  For each task, it shows the highest return achieved by the policy and the average preference prediction accuracy of the corresponding reward function.  The table allows comparison of the performance of the standard cross-entropy loss method (Pref) against the proposed adaptive preference scaling method (Ada-Pref).", "section": "4.1 Robotic control"}, {"figure_path": "GnaFrZRHPf/tables/tables_18_2.jpg", "caption": "Table 8: Table for the average preference prediction accuracy of the best reward function and the highest return of the corresponding policy.", "description": "This table presents the results of the experiments using the second hyperparameter tuning criterion.  The table shows the average preference prediction accuracy of the best reward function (selected based on accuracy) and the highest return achieved by the corresponding policy for the HalfCheetah, Ant, and Hopper robotic control tasks using both the Pref (baseline) and Ada-Pref methods.  It highlights the performance trade-off when prioritizing reward function accuracy over overall policy performance.", "section": "4.1 Robotic control"}]