[{"type": "text", "text": "SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vijay Lingam\u2020 \u00a7\u2217 Atula Tejaswi\u2020\u2217 Aditya Vavre\u2020\u2217 Aneesh Shetty\u2020\u2217 Gautham Krishna Gudur\u2020\u2217 Joydeep Ghosh\u2020 Alex Dimakis\u2020 Eunsol Choi\u2020 Aleksandar Bojchevski\u2021\u2217 Sujay Sanghavi\u2020\u2217 ", "page_idx": 0}, {"type": "text", "text": "\u2020University of Texas at Austin \u2021University of Cologne \u00a7CISPA Helmholtz Center for Information Security ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights W and inject learnable matrices \u2206W. These $\\Delta\\mathbf{W}$ matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically exhibit a performance gap compared to full fine-tuning. While recent PEFT methods have narrowed this gap, they do so at the expense of additional learnable parameters. We propose $\\bar{\\mathrm{SVFT}^{2}}$ , a simple approach that structures $\\Delta\\mathbf{W}$ based on the specific weight matrix W. SVFT updates W as a sparse combination $M$ of outer products of its singular vectors, training only the coefficients of these combinations. Crucially, we make additional off-diagonal elements in $M$ learnable, enabling a smooth trade-off between trainable parameters and expressivity\u2014an aspect that distinctly sets our approach apart from previous works leveraging singular values. Extensive experiments on language and vision benchmarks show that SVFT recovers up to $96\\%$ of full fine-tuning performance while training only 0.006 to $\\mathbf{0.25\\%}$ of parameters, outperforming existing methods that achieve only up to ${\\bf85}\\%$ performance with 0.03 to $0.8\\%$ of the trainable parameter budget. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale foundation models are often adapted for specific downstream tasks after pre-training. Parameter-efficient fine-tuning (PEFT) facilitates this adaptation efficiently by learning a minimal set of new parameters, thus creating an \"expert\" model. For instance, Large Language Models (LLMs) pre-trained on vast training corpora are fine-tuned for specialized tasks such as text summarization [13, 37], sentiment analysis [27, 21], and code completion [28] using instruction fine-tuning datasets. Although full fine-tuning (Full-FT) is a viable method to achieve this, it requires re-training and storing all model weights, making it impractical for deployment with large foundation models. ", "page_idx": 0}, {"type": "text", "text": "To address these challenges, PEFT techniques [14] (e.g., LoRA [15]) were introduced to significantly reduce the number of learnable parameters compared to Full-FT, though often at the cost of performance. DoRA [19] bridges this performance gap by adding more learnable parameters and being more expressive than LoRA. Almost all these methods apply a low-rank update additively to the frozen pre-trained weights, potentially limiting their expressivity. Furthermore, these adapters are agnostic to the structure and geometry of the weight matrices they modify. Finally, more expressive ", "page_idx": 0}, {"type": "image", "img_path": "uS0PwIBzC0/tmp/eea634f3c0546e49a56712bb01f4027a4ed5f5f281a50cc930a2e967ec307ea6.jpg", "img_caption": ["Figure 1: Performance vs total trainable parameters for GSM-8K (left) and Commonsense Reasoning (right) on Gemma-2B. $\\mathrm{SVFT}_{d=16}^{B/R}$ TdB=/1R6 outperforms DoRAr=8/16 with 75% less trainable parameters. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "PEFT methods (e.g., LoRA, DoRA, BOFT [20]) still accumulate a considerable portion of learnable parameters even in their most efficient configuration (e.g., setting rank $^{=1}$ in LoRA and DoRA). The storage requirements for the learnable adapters can grow very quickly when adapting to a large number of downstream tasks [17]. ", "page_idx": 1}, {"type": "text", "text": "Is it possible to narrow the performance gap between PEFT and Full-FT while being highly parameter-efficient? Yes, we propose SVFT: Singular Vectors guided Fine-Tuning \u2014 a simple approach that involves updating an existing weight matrix by adding to it a sparse weighted combination of its own singular vectors. The structure of the induced perturbation in SVFT depends on the specific matrix being perturbed. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce SVFT, a new PEFT method. Given a weight matrix $W$ , SVFT involves adapting it with a matrix $\\begin{array}{r}{\\Delta W\\,:=\\,\\sum_{(i,j)\\in\\Omega}m_{i j}\\pmb{u}_{i}\\pmb{v}_{j}^{T}}\\end{array}$ where the $\\{{\\pmb u}_{i}\\}$ and $\\{{v}_{j}\\}$ are the left and right singular vectors of $W$ , $\\Omega$ is an a-priori fixed sparsity pattern, and $m_{i j}$ for $(i,j)\\in\\Omega$ are learnable parameters. By controlling $|\\Omega|$ we can efficiently explore the accuracy vs parameters trade-off. \u2022 SVFT achieves higher downstream accuracy, as a function of the number of trainable parameters, as compared to several popular PEFT methods (see Figure 1) and over several downstream tasks across both vision and language tasks. For instance, on GSM-8K using Gemma-2B our method recovers up to $96\\%$ of full fine-tuning performance while training only 0.006 to $\\mathbf{0.25\\%}$ of parameters, outperforming existing methods that only recover up to ${\\bf85\\%}$ performance using 0.03 to ${\\bf0.8\\%}$ the trainable parameter budget (see Figure 1). ", "page_idx": 1}, {"type": "text", "text": "We introduce four simple variants for parameterizing weight updates, namely: Plain, Random, Banded, and Top- $k$ in SVFT (which differ in their choices of the fixed sparsity pattern $\\Omega$ ) and validate these design choices empirically. Additionally, we theoretically show that for any fixed parameters budget, SVFT can induce a higher rank perturbation compared to previous PEFT techniques. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recent advancements in large language models (LLMs) have emphasized the development of PEFT techniques to enhance the adaptability and efficiency of large pre-trained language models. ", "page_idx": 1}, {"type": "text", "text": "LoRA. A notable contribution in this field is Low-Rank Adaptation (LoRA) [15], which freezes the weights of pre-trained models and integrates trainable low-rank matrices into each transformer layer. For a pre-trained weight matrix $W_{0}\\stackrel{-}{\\in}\\mathbb{R}^{d\\times n}$ , LoRA constraints the weight update $\\Delta W$ to a low-rank decomposition: $h=W_{0}{\\pmb x}+\\Delta W{\\pmb x}=W_{0}{\\pmb x}+\\underline{{B}}A{\\pmb x}$ , where $B\\in\\breve{\\mathbb{R}^{d\\times r}}$ , $\\mathbf{\\bar{A}}\\in\\mathbb{R}^{r\\times n}$ and rank $r\\ll\\operatorname*{min}(\\bar{d},n)$ . We underline the (trainable) parameters that are updated via gradient descent. ", "page_idx": 1}, {"type": "text", "text": "LoRA variants. We highlight some recent approaches that further improve the vanilla LoRA architecture. Vector-based Random Matrix Adaptation (VeRA) [17] minimizes the number of trainable parameters by utilizing a pair of low-rank random matrices shared between layers and learning compact scaling vectors while maintaining performance comparable to LoRA. Formally, VeRA can be expressed as: $h=W_{0}{\\pmb x}+\\Delta W{\\pmb x}=W_{0}{\\pmb x}+{\\pmb\\Lambda}_{b}B{\\pmb\\Lambda}_{d}A{\\pmb x}$ , where $\\pmb{A}$ and $_B$ are initialized randomly, frozen, and shared across layers, while $\\mathbf{\\Delta}\\Lambda_{b}$ and $\\overline{{\\mathbf{A}}}_{d}$ are trainable diagonal matrices. ", "page_idx": 1}, {"type": "image", "img_path": "uS0PwIBzC0/tmp/dd7382d5fca82cf4a4048c7ccdb5d1dbaf61f663c03bc80289a1872ffa3f9f0f.jpg", "img_caption": ["Figure 2: Schematic comparison of LoRA, VeRA, DoRA, and SVFT (left to right). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "An alternative approach, Weight-Decomposed Low-Rank Adaptation (DoRA) [19], decomposes pretrained weight matrices into magnitude and direction components, and applies low-rank updates for directional updates, reducing trainable parameters and enhancing learning capacity and training stability. DoRA can be expressed as: $\\begin{array}{r}{h=\\!\\frac{\\mathbf{\\bar{m}}}{\\|W_{0}+\\Delta W\\|_{c}}x=\\!\\frac{\\bar{W}_{0}+\\underline{{B A}}}{\\|W_{0}+\\underline{{B A}}\\|_{c}}x}\\end{array}$ , where $\\|\\cdot\\|_{c}$ denotes the vector-wise norm of a matrix across each column. Similar to LoRA, $W_{0}$ remains frozen, whereas the magnitude vector $\\mathbf{\\nabla}m$ (initialized to $\\|\\mathbf{W}_{0}\\|_{c})$ and low-rank matrices $A,B$ contain trainable parameters. AdaLoRA [38] adaptively distributes the parameter budget across weight matrices based on their importance scores and modulates the rank of incremental matrices to manage this allocation effectively. PiSSA (Principal Singular Values and Singular Vectors Adaptation) [22] is another variant of LoRA, where matrices $A,B$ are initialized with principal components of SVD and the remaining components are used to initialize $W_{0}$ . FLoRA [34] enhances LoRA by enabling each example in a mini-batch to utilize distinct low-rank weights, preserving expressive power and facilitating efficient batching, thereby extending the domain adaptation benefits of LoRA without batching limitations. ", "page_idx": 2}, {"type": "text", "text": "Other PEFT variants. Orthogonal Fine-tuning (OFT) [26] modifies pre-trained weight matrices through orthogonal reparameterization to preserve essential information. However, it still requires a considerable number of trainable parameters due to the high dimensionality of these matrices. Butterfly Orthogonal Fine-tuning (BOFT) [20] extends OFT\u2019s methodology by incorporating Butterfly factorization thereby positioning OFT as a special case of BOFT. Unlike the additive low-rank weight updates utilized in LoRA, BOFT applies multiplicative orthogonal weight updates, marking a significant divergence in the approach but claims to improve parameter efficiency and fine-tuning flexibility. BOFT can be formally expressed as: $\\mathbf{\\boldsymbol{h}}=(\\underline{{\\boldsymbol{R}}}(\\boldsymbol{m},\\boldsymbol{b})\\cdot\\mathbf{\\boldsymbol{W}}_{0})\\mathbf{\\boldsymbol{x}}$ , where the orthogonal matrix $\\pmb{R}(m,b)\\in\\mathbb{R}^{d\\times d}$ is composed of a product of multiple orthogonal butterfly components. When $m=1$ , BOFT reduces to block-diagonal OFT with block size $b$ . When $m=1$ and $b=d$ , BOFT reduces to the original OFT with an unconstrained full orthogonal matrix. ", "page_idx": 2}, {"type": "text", "text": "SVD-based Variants. SVF [31], SVDiff [10], and SAM-Parser [25] also leverage the structure of $W$ matrices by decomposing them into three consecutive matrices via Singular Value Decomposition (SVD). However, these methods fine-tune only the singular values while keeping other components fixed, making them comparable to $\\mathrm{SVFT}^{P}$ . In Appendix C.1, we present a comparison of $\\mathring{\\mathbf{S}}\\mathbf{V}\\mathbf{F}\\mathbf{T}^{P}$ with SVF, confirming that their performance is similar, which supports our observations. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce Singular Vectors guided Fine-Tuning (SVFT). The main innovation in SVFT lies in applying structure/geometry-aware weight updates through sparse weighted combination of singular vectors. ", "page_idx": 2}, {"type": "image", "img_path": "uS0PwIBzC0/tmp/6b2499feb9e629773df6d96d949b84f1e81d306a0d40d6313927ec27c8c9f4bc.jpg", "img_caption": ["Figure 3: An Overview of SVFT. The original weights $W$ are decomposed into $U,\\Sigma,V$ . Here, $_M$ contains all the trainable parameters, which can be configured into patterns such as Plain, Random, Banded, and Top- $k$ , represented by patterns of trainable (orange) and zero (gray) elements. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 SVFT Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now formally describe our method, SVFT for parameter-efficient fine-tuning of a pre-trained model. Let $W_{0}\\stackrel{\\cdot}{\\in}\\mathbb{R}^{d_{1}\\times d_{2}}$ denote a weight matrix in the pre-trained model, such as a key matrix, query matrix, or an MLP matrix within a transformer block. To this matrix, we add a structured, learnable update $\\Delta W$ as follows. ", "page_idx": 3}, {"type": "text", "text": "As a first step, we compute the Singular Value Decomposition (SVD) of the given matrix: $W_{0}=$ $U{\\scriptstyle\\sum}V^{T}$ . That is, $U$ is the $d_{1}\\times d_{1}$ matrix of left singular vectors (i.e., its columns are orthonormal), $V^{T}$ is the $d_{2}\\times d_{2}$ matrix of right singular vectors (i.e., its rows are orthonormal), and $\\Sigma$ is a $d_{1}\\times d_{2}$ diagonal matrix. Then, we parameterize our weight update as $\\Delta W\\,=\\,U\\underline{{M}}\\dot{V}^{T}$ , where $U,V$ are fixed and frozen, while $\\underline{{\\boldsymbol{M}}}$ is a $d_{1}\\times d_{2}$ sparse trainable matrix with pre-determined and fixed sparsity pattern3. That is, we first pre-determine a small fixed set of elements in $_M$ that will be allowed to be non-zero and train only those elements. The forward pass for SVFT can be written as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{h}=W_{0}\\boldsymbol{x}+\\Delta W\\boldsymbol{x}=U(\\Sigma+\\underline{{M}})V^{T}\\boldsymbol{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We explore four simple choices for $\\Omega$ , the pre-determined sparsity pattern of $\\underline{{M}}$ . ", "page_idx": 3}, {"type": "text", "text": "Plain $\\bar{(\\mathbf{S}\\mathbf{V}\\mathbf{F}\\mathbf{T}^{P})}$ . In this variant, we constrain $\\underline{{M}}$ to be a diagonal matrix, which can be interpreted as adapting singular values and reweighting the frozen singular vectors. Since only the diagonal elements are learned, this is the most parameter-efficient SVFT variant. ", "page_idx": 3}, {"type": "text", "text": "Banded $\\left(\\boldsymbol{\\mathrm{SVFT}}_{d}^{B}\\right)$ . In this approach, we populate $\\underline{{\\boldsymbol{M}}}$ using a banded matrix, progressively making off-diagonals learnable. Specifically, for constants $z_{1}$ and $z_{2}$ , $\\underline{{M_{i j}}}=0$ if $j<i-z_{1}$ or $j>i+z_{2}$ , where $z_{1},z_{2}\\ge0$ . In our experiments, we set $z_{1}\\,=\\,z_{2}\\,=\\,d$ to induce off-diagonal elements that capture additional interactions beyond those represented by singular values. This banded perturbation induces local interactions, allowing specific singular values to interact with their immediate neighbors, ensuring smoother transitions. This method, although deviating from the canonical form of SVD, provides a mechanism to capture localized interactions. ", "page_idx": 3}, {"type": "text", "text": "Random $\\left(\\boldsymbol{\\mathrm{SVFT}}_{d}^{R}\\right)$ . A straightforward heuristic for populating $\\underline{{M}}$ involves randomly selecting $k$ elements to be learnable. ", "page_idx": 3}, {"type": "text", "text": "Top- $k$ $\\left(\\mathbf{S}\\mathbf{V}\\mathbf{F}\\mathbf{T}_{\\#p}^{T}\\right)$ . The final design choice we explore involves computing the alignment between the left and right singular vectors as ${\\pmb u}_{i}^{T}{\\pmb v}_{j}$ . We then select the top- $k$ elements and make them learnable. However, note that this only works when left and right singular vectors have the same size. A possible interpretation of this is we make only the top- $k$ strong interactions between singular vector directions learnable. The subscript $\\#p$ denotes the total number of learnable parameters. ", "page_idx": 3}, {"type": "text", "text": "We illustrate these SVFT design choices in Figure 3. Our empirical results demonstrate that these simple design choices significantly enhance performance compared to state-of-the-art PEFT methods. Note that $\\bar{\\mathbf{S}}\\mathbf{V}\\mathbf{F}\\mathbf{T}^{P}$ has a fixed number of learnable parameters, while the remaining variants are configurable. We hypothesize that further innovation is likely achievable through optimizing the sparsity pattern of $M$ , including efficient learned-sparsity methods. In this paper, we explore these four choices to validate the overall idea: determining a perturbation using the singular vectors of the matrix that is being perturbed. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Properties of SVFT ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We highlight some properties of SVFT in the following lemma and provide insights into how its specific algebraic structure compares and contrasts with baseline PEFT methods. ", "page_idx": 4}, {"type": "text", "text": "Lemma: Let $W_{0}$ be a matrix of size $d_{1}\\times d_{2}$ with SVD given by $U{\\scriptstyle\\sum}V^{T}$ . Consider an updated final matrix $\\pmb{W_{0}}+\\pmb{U}\\pmb{M}\\pmb{V}^{T}$ , where $_M$ is a matrix of the same size as $\\Sigma$ , which may or may not be diagonal. Then, the following holds: ", "page_idx": 4}, {"type": "text", "text": "(a) Structure: If $_M$ is also diagonal (i.e. the plain SVFT), then the final matrix $W_{0}+U M V^{T}$ has $U$ as its left singular vectors and $\\mathrm{sign}(\\Sigma+M)V^{T}$ as its right singular vectors. That is, its singular vectors are unchanged, except for possible sign flips. Conversely, if $_M$ is not diagonal (i.e., variants of SVFT other than plain), then $U$ and $V$ may no longer be the singular directions of the final matrix $\\pmb{W}_{0}+\\pmb{U}\\bar{\\pmb{M}}\\pmb{V}^{T}$ .   \n(b) Expressivity: Given any target matrix $_{P}$ of size $d_{1}\\times d_{2}$ , there exists an $_M$ such that $\\bar{\\mathbf{P}_{=}}\\mathbf{W}_{0}+U M V^{T}$ . That is, if $_M$ is fully trainable, any target matrix can be realized using this method.   \n(c) Rank: If $_M$ has $k$ non-zero elements, then the rank of the update $U M V^{T}$ is at most $\\operatorname*{min}\\{k,\\operatorname*{min}\\{d_{1},d_{2}\\}\\}$ . For the same number of trainable parameters, SVFT can produce a much higher rank perturbation than LoRA (eventually becoming full rank), but in a constrained structured subspace. ", "page_idx": 4}, {"type": "text", "text": "We provide our proofs in Appendix A. Building on this lemma, we now compare the form of the SVFT update with LoRA and VeRA. SVFT\u2019s $\\Delta W$ can be written as a sum of rank-one matrices: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta W\\ =\\ \\sum_{(i,j)\\in\\Omega}\\frac{m_{i j}}{{\\bf\\nabla}}u_{i}v_{j}^{T}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{u}_{i}$ is the $i^{t h}$ left singular vector, $\\pmb{v}_{j}$ is the $j^{t h}$ right singular vector, and $\\Omega$ is the set of non-zero elements in $_M$ . Thus, our method involves adding a weighted combination of specific rank-one perturbations of the form $\\pmb{u}_{i}\\pmb{v}_{j}^{T}$ . ", "page_idx": 4}, {"type": "text", "text": "LoRA and VeRA updates can also be expressed as sums of rank-one matrices. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta W_{\\mathrm{LoRA}}\\;=\\;\\sum_{i=1}^{r}\\frac{a_{i}}{\\frac{}{}}\\frac{b_{i}{}^{T}}{}\\quad\\mathrm{and}\\quad\\Delta W_{\\mathrm{VeRA}}\\;=\\;\\sum_{i=1}^{r}\\frac{}{}\\alpha_{i}\\bigl(\\hat{a}_{i}\\odot\\underline{{{\\beta}}}\\bigr)\\hat{b}_{i}^{T}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\underline{{\\pmb{a}_{i}}}$ and $\\underline{{b_{j}}}$ are the trainable columns of $\\pmb{A}$ and $_B$ matrices in LoRA. In VeRA, $\\hat{\\pmb{a}}_{i}$ and $\\hat{\\pmb{b}}_{i}$ are random and fixed vectors, while $\\underline{{\\pmb{\\alpha}}}$ and $\\underline{{\\beta}}$ represent the diagonal elements of $\\Lambda_{d}$ and $\\mathbf{\\nabla}\\Lambda_{b}$ respectively. ", "page_idx": 4}, {"type": "text", "text": "Note that LoRA requires $d_{1}+d_{2}$ trainable parameters per rank-one matrix, while SVFT and VeRA require only one. Although LoRA can potentially capture directions different from those achievable by the fixed $\\{u_{i},v_{j}^{T}\\}$ pairs, each of these directions incurs a significantly higher parameter cost. ", "page_idx": 4}, {"type": "text", "text": "VeRA captures new directions at a parameter cost similar to SVFT; however, there is a key distinction: in VeRA, each vector $\\hat{\\pmb{a}}_{i}$ or $\\hat{\\pmb{b}}_{i}$ appears in only one of the rank-one matrices. In contrast, in SVFT, the same vector $\\pmb{u}_{i}$ can appear in multiple terms in the summation, depending on the sparsity pattern of $_M$ . This results in an important difference: unlike SVFT, VeRA is not universally expressive \u2013 it cannot represent any target matrix $_{P}$ . Moreover, $\\hat{\\pmb{a}}_{i},\\hat{\\pmb{b}}_{i}$ are random, while ${\\pmb u}_{i},{\\pmb v}_{j}$ depend on $W_{0}$ . ", "page_idx": 4}, {"type": "text", "text": "Note. SVFT requires storing both left and right singular vectors due to its computation of the SVD on pre-trained weights. While this increases memory usage compared to LoRA, it remains comparable to or lower than DoRA and BOFT. We present a memory analysis in Section 5.3. Further exploration of memory-reduction techniques, such as quantization, is planned as future work. Importantly, inference time and memory consumption remain the same across all methods, including SVFT, as the weights can be fused. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Base Models & Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We adapt widely-used language models, encoder-only model (DeBERTaV3base [11]) and two decoder-only models (Gemma-2B/7B [32], LLaMA-3-8B [1]). We also experiment with vision transformer models (ViT-B/16 and ViT-L/16) [9]) pre-trained on ImageNet-21k [8], following prior work [17]. The complete details of our experimental setup and hyperparameter configurations are provided in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We compare with Full Fine-Tuning (FT) updating all learnable parameters in all layers, along with LoRA [15], DoRA [19], BOFT [20] and VeRA [17].4 ", "page_idx": 5}, {"type": "text", "text": "Target Modules. We adapt all weight matrices for SVFT, as it does not increase trainable parameters at the same rate as baseline methods. For baselines, we adapt the target modules recommended in [19]: QKVUD matrices for LoRA and DoRA, compatible matrices for VeRA, and QV matrices for BOFT to stay within GPU memory limits. Additional details can be found in Appendix C.7 and C.8. We also conduct experiments adapting QKVUD modules across methods and observe similar trends, as discussed in Appendix C.2. ", "page_idx": 5}, {"type": "text", "text": "4.2 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Language. For natural language generation (NLG) tasks, we evaluate on GSM-8K [7] and MATH [12] by fine-tuning on MetaMathQA-40K [35], following [20]. We also evaluate on 8 commonsense reasoning benchmarks (BoolQ [5], PIQA [3], SIQA [30], HellaSwag [36], Winogrande [29], ARC-easy/challenge [6], and OpenBookQA [23]). We follow the setting outlined in prior work [19, 16], where the training sets of all benchmarks are amalgamated for fine-tuning. We fine-tune on 15K examples from this training set. For natural language understanding (NLU), we evaluate on the General Language Understanding Evaluation (GLUE) benchmark consisting of classification and regression tasks, in line with [17, 15]. ", "page_idx": 5}, {"type": "text", "text": "Vision. Our experiments on vision tasks consist of 4 benchmarks: CIFAR-100 [18], Food101 [4], RESISC45 [33], and Flowers102 [24]. We follow the setup from [17], and fine-tune on a subset comprising 10 samples from each class. ", "page_idx": 5}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/c7e16ba8f2b7b7eeba13dd3c6f0cecdf50a92568c7d996565926a8cc82a37152.jpg", "table_caption": ["Table 1: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). #Params denote the number of trainable parameters. bold and underline represent the best and second best performing PEFT methods, respectively. SVFT offers superior/competitive performance at much lower #Params. For $\\operatorname{SVFT}_{d}^{R}$ , we set $d=16$ for Gemma and $d=12$ for LLaMA-3 models. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 2: Evaluation results on eight commonsense reasoning benchmarks with Gemma-7B. We follow [19] for hyperparameter configurations, and report accuracy for all tasks. HS and WG denote HellaSwag [36] and WinoGrande [29], respectively. $\\operatorname{SVFT}^{P}$ offers competitive performance at a fraction of #Params. $\\mathrm{SVFT}_{d=8}^{B}$ can match $\\mathrm{LoRA}_{r=32}$ with ${\\sim}7\\mathbf{x}$ fewer parameters. ", "page_idx": 6}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/d79f2257bb7899e6045cd68fb4452a76b8a437f8ea84edc58129d6b8af398830.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/ef886d3e8f6921cce45d94f8374c5fc5d0c1b693f777bcdd478878b10a2bb218.jpg", "table_caption": ["Table 3: DeBERTaV3base with different adaptation methods on the GLUE benchmark. We report matched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all tasks. \\* indicates values reported in [20]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Performance on Language Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Natural Language Generation. We present results on mathematical question answering against baseline PEFT techniques across three base models \u2013 varying from 2B to 8B parameters in Table 1. To ensure a comprehensive comparison, we test baseline techniques (LoRA, DoRA) with different configurations, and varying hyper-parameters like rank to cover a range of learnable parameters from low to high. Note that even when the rank is as low as 1, both methods yield more trainable parameters than $\\mathrm{SVFT}^{P}$ . $\\operatorname{SVFT}^{P}$ $(\\sim\\!0.2\\mathbf{M})$ shows as much as $18\\%$ relative improvement over techniques that use $6\\times$ more trainable parameters $(\\mathbf{BOFT}_{m=2}^{b=8}$ , $\\mathrm{LoRA}_{r=1}\\mathrm{~}$ ). Against techniques of comparable size (VeRA), $\\operatorname{SVFT}^{P}$ achieves $15.5\\%$ relative improvement on average. Even in the default regime, $\\operatorname{SVFT}_{d}^{R}$ matches techniques with at least $3\\times$ more trainable parameters. Notably, on GSM-8K, $\\operatorname{SVFT}_{d}^{R}$ again achieves $96\\%$ of full fine-tuning performance, while $\\mathrm{DoRA}_{r=16}$ recovers $86\\%$ with $2\\times$ more parameters than $\\operatorname{SVFT}_{d}^{R}$ . ", "page_idx": 6}, {"type": "text", "text": "Commonsense Reasoning. In Table 2, we compare performance on commonsense reasoning benchmarks with Gemma-7B, and observe similar trends. In the lower and moderately parameterized regime $(\\sim\\!0.43\\mathrm{M})$ , $\\operatorname{SVFT}^{P}$ shows competitive performance in comparison to $\\mathrm{LoRA}_{r=1}$ and $\\mathrm{DoRA}_{r=1}$ , which have $1.9\\times$ and $7.7\\times$ more parameters, respectively. Against VeRA, which trains $3.5\\times$ more parameters, $\\operatorname{SVFT}^{P}$ shows a relative improvement of ${\\sim}1.16\\%$ . Similarly, $\\mathrm{SVFT}_{d=8}^{B}$ also matches or exceeds methods that use up to $7\\times$ more trainable parameters. For instance, $\\mathrm{SVFT}_{d=8}^{B}$ attains an average performance of $83.35\\%$ with only 9.8M parameters, closely matching $\\mathrm{LoRA}_{r=16}$ $83.69\\%$ , 68.8M parameters). We observe similar trends with Gemma-2B (refer Table 11). ", "page_idx": 6}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/3eafddaf272e3f4ca8ac5af2e06f64c364a797fe1e15f28d8573e8ed3180a44b.jpg", "table_caption": ["Table 4: Performance on image classification benchmarks \u2013 CIFAR-100 (C100), Food101 (F101), Flowers102 (F102), and Resisc-45 (R45). We only adapt $Q,V$ matrices for all methods, following prior work [17]. We report accuracy for all tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Natural Language Understanding. Results on the GLUE benchmark are summarized in Table 3. SVFT matches $\\mathrm{LoRA}_{r=8}$ and $\\mathrm{DoRA}_{r=4}$ which use $12{-}22{\\times}$ more trainable parameters. Similarly, when compared to OFT and BOFT, $\\mathrm{SVFT}^{P}$ maintains a comparable average performance despite being $12\\times$ smaller. These results highlight SVFT\u2019s ability to strike a balance between parameter efficiency and performance, making it an attractive PEFT choice for simple classification tasks. ", "page_idx": 7}, {"type": "text", "text": "Parameter efficiency. In Figure 1, we plot the performance of SVFT on mathematical reasoning and commonsense reasoning against other PEFT techniques across a range of configurations. Across trainable parameter budgets ranging from lowest to highest, SVFT obtains the best overall performance, matching methods that require significantly more trainable parameters. These results establish SVFT as a pareto-dominant approach for parameter-efficient fine-tuning. ", "page_idx": 7}, {"type": "text", "text": "5.2 Performance on Vision Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 4 presents a comparison between SVFT and other PEFT techniques on image classification benchmarks, using ViT-B and ViT-L models. The results show that SVFT variants achieve a strong balance between performance and parameter efficiency, often surpassing or matching the performance of other methods with fewer learnable parameters. Notably, the $\\mathrm{SVFT}^{B}$ variant attains an average accuracy of $\\mathrm{83.87\\%}$ across tasks with ViT-Base, outperforming Full-FT, which achieves a close $83.72\\%$ . ", "page_idx": 7}, {"type": "image", "img_path": "uS0PwIBzC0/tmp/ba5976b7a0ad12cbdd3f601beed1368aac03fa26a5f1b9e465d0e6459f3e4cb7.jpg", "img_caption": ["Figure 4: Performance variation with $\\bar{\\mathbf{\\nabla}}\\bar{\\mathbf{V}}\\mathbf{F}\\mathbf{T}_{d}^{B}$ based on the adapted weight matrices \u2013 GSM-8K with Gemma-2B. Adapting more target weight types results in greater gains in performance. Interestingly, for a fixed parameter budget, adapting $U$ and $_{D}$ weight types gives greater lifts than adapting $Q$ and $V$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Additionally, it\u2019s important to note that in these vision experiments, both classifier head parameters and other learnable parameters are trained. ", "page_idx": 7}, {"type": "text", "text": "5.3 Memory Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Although SVFT reduces trainable parameters, it results in higher overall GPU memory usage compared to LoRA. However, fewer trainable parameters lower the memory demands for gradients, activations, optimizer states, and other buffers. To validate this, we used HuggingFace\u2019s internal memory proflier to measure peak GPU memory usage. Our results, along with the adapted modules for all baselines, are summarized in Table 5. We observe that SVFT uses approximately $1.2\\mathbf{x}$ more memory than LoRA but remains comparable to or more efficient than DoRA. We present additional analysis in Appendix C.5. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/b036414f6ad7d7e3606b49433d7a334b72061dec1e083be1a5bbbf8a70c541f0.jpg", "table_caption": ["Table 5: GPU Memory analysis, measured in gigabytes (GB). We report the average performance on GSM-8K and MATH. SVFT outperforms both LoRA and DoRA in terms of performance while requiring lesser GPU memory than DoRA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Contribution of Each Weight Type ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 4, we investigate the contribution of each weight type. Starting with the base configuration, we apply $\\operatorname{SVFT}_{d}^{B}$ to the $Q$ and $V$ weights in each transformer block and report the performance. We then incrementally add the remaining weight modules $(K,U,D,O,G)$ and observe the changes in performance. For each configuration, we also vary the trainable parameters by incrementing the total learnable off-diagonals. ", "page_idx": 8}, {"type": "text", "text": "Note that applying $\\mathrm{SVFT}_{d}^{B}$ to ${\\cal U},{\\cal D},{\\cal O}$ , and $\\pmb{G}$ does not increase trainable parameters as much as applying LoRA/DoRA to these modules (Table 8). For example, for a large matrix of shape $d_{1}\\times d_{2}$ , $\\mathrm{LoRA}_{r=1}$ learns $d_{1}+d_{2}$ parameters, while $\\mathrm{SVFT}^{P}$ learns $\\operatorname*{min}(d_{1},d_{2})$ parameters. We observe that adapting only $U$ and $_{D}$ with SVFT yields up to a $10\\%$ relative improvement over adapting $Q$ and $V$ for the same parameter budget $(\\sim0.8M)$ ). Our findings indicate that adapting more weight types enhances performance. ", "page_idx": 8}, {"type": "text", "text": "5.5 Impact of $M$ \u2019s Structure on Performance ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyze the impact of various parameterizations of $_M$ (Plain, Banded, Random, Top- $\\cdot k$ ) on downstream performance. To ensure a fair comparison, we align the number of trainable coefficients across all variants whenever possible. As shown in Table 7, the Banded variant outperforms the others, followed closely by the Random variant, across different models and tasks. This trend is also evident in the average rank column of the table. Based on these empirical findings, we recommend using the Banded variant. ", "page_idx": 8}, {"type": "text", "text": "5.6 Impact of Pre-trained Weight Quality ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A key feature of SVFT is that the weight update depends on the pre-trained weights $W$ . We therefore ask the following question: Does the quality of pre-trained weights have a disproportionate impact on SVFT? To answer this, we consider two checkpoints from the Pythia suite [2] at different stages of training, i.e., 39K steps and 143K steps, respectively. We fine-tune each of these checkpoints independently with Full-FT, LoRA, and SVFT. We then compare the increase in performance (\u2206Perf). As shown in Table 6, compared to LoRA, SVFT benefits more from better pre-trained weights. We also note that SVFT outperforms LoRA in both settings, suggesting that the benefits of inducing a $\\Delta W$ that explicitly depends on $W$ are beneficial even when $W$ is sub-optimal. ", "page_idx": 8}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/9da8640c2155c0066341fc430ed24f7b47f9f8695ce669b2369842979cef3c55.jpg", "table_caption": ["Table 6: Results on GSM-8K after finetuning on Pythia-2.8B checkpoints at different stages of pre-training (PT). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/1985e628ed64c84d9bd3658f1eee5e8efb01fa5003a88a36e63a91bd16e98f04.jpg", "table_caption": ["Table 7: Results on fine-tuning with SVFT using different $_M$ parameterizations. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. Despite significantly reducing learnable parameters and boosting performance, SVFT incurs some additional GPU memory usage. Unlike LoRA, SVFT necessitates computing the SVD and storing both left and right singular vectors. While memory consumption remains comparable to or lower than DoRA and BOFT, it\u2019s roughly $1.2\\times$ that of LoRA. However, similar to the scaling explored in [34], memory usage should amortize with the increasing scale of adaptation tasks. In future work we will explore quantization and other techniques to address memory concerns. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. Our work enables easier personalization of foundational models, which can have both positive and negative societal impacts. Since our method provides computational efficiency (smaller parameter footprint), it will be less expensive to enable personalization. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduces SVFT, a novel and efficient PEFT approach that leverages the structure of pretrained weights to determine weight update perturbations. We explore four simple yet effective sparse parameterization patterns, offering flexibility in controlling the model\u2019s expressivity and the number of learnable parameters. Extensive experiments on language and vision tasks demonstrate SVFT\u2019s effectiveness as a PEFT method across diverse parameter budgets. Furthermore, we theoretically show that SVFT can induce higher-rank perturbation updates compared to existing methods, for a fixed parameter budget. In future work, we aim to develop principled methods to generate sparsity patterns, potentially leading to further performance improvements. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Greg Kuhlmann for helping support this research. This work was also supported by the NSF institutes ENCORE and IFML. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Meta AI. Introducing meta llama 3: The most capable openly available llm to date. April 2024. ", "page_idx": 9}, {"type": "text", "text": "[2] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023. ", "page_idx": 9}, {"type": "text", "text": "[3] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.   \n[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative components with random forests. In European Conference on Computer Vision, 2014.   \n[5] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.   \n[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[10] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 7323\u20137334, 2023.   \n[11] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electrastyle pre-training with gradient-disentangled embedding sharing, 2023.   \n[12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.   \n[13] Karl Moritz Hermann, Tom\u00e1\u0161 Koc\u02c7isk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915, page 1693\u20131701. MIT Press, 2015.   \n[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2019.   \n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[16] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023.   \n[17] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. ELoRA: Efficient low-rank adaptation with random matrices. In The Twelfth International Conference on Learning Representations, 2024.   \n[18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[19] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024.   \n[20] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard Sch\u00f6lkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The Twelfth International Conference on Learning Representations, 2024.   \n[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.   \n[22] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024.   \n[23] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.   \n[24] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.   \n[25] Zelin Peng, Zhengqin Xu, Zhilin Zeng, Xiaokang Yang, and Wei Shen. Sam-parser: Fine-tuning sam efficiently by parameter space reconstruction. Proceedings of the AAAI Conference on Artificial Intelligence, 38:4515\u20134523, 03 2024.   \n[26] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch\u00f6lkopf. Controlling text-to-image diffusion by orthogonal finetuning. In Thirty-seventh Conference on Neural Information Processing Systems, volume 36, pages 79320\u201379362, 2023.   \n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[28] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.   \n[29] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.   \n[30] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions, 2019.   \n[31] Yanpeng Sun, Qiang Chen, Xiangyu He, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jian Cheng, Zechao Li, and Jingdong Wang. Singular value fine-tuning: Few-shot segmentation requires few-parameters fine-tuning. In Advances in Neural Information Processing Systems, volume 35, pages 37484\u201337496, 2022.   \n[32] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[33] Ihsan Ullah, Dustin Carrion, Sergio Escalera, Isabelle M Guyon, Mike Huisman, Felix Mohr, Jan N van Rijn, Haozhe Sun, Joaquin Vanschoren, and Phan Anh Vu. Meta-album: Multidomain meta-dataset for few-shot image classification. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.   \n[34] Yeming Wen and Swarat Chaudhuri. Batched low-rank adaptation of foundation models. In The Twelfth International Conference on Learning Representations, 2024.   \n[35] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023.   \n[36] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.   \n[37] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11328\u201311339. PMLR, 13\u201318 Jul 2020.   \n[38] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The appendix is organized as follows. ", "page_idx": 13}, {"type": "text", "text": "\u2022 In Appendix A, we give proofs for the lemmas outlined in 3.2.   \n\u2022 In Appendix B, we compare how the trainable parameters count for different PEFT techniques (LoRA, DoRA, VeRA) versus our method SVFT.   \n\u2022 In Appendix C, we describe results for additional experiments and provide implementation details for all the experiments. ", "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide brief proofs for the Structure, Expressivity and the Rank lemmas for SVFT: ", "page_idx": 13}, {"type": "text", "text": "(a) Structure: If $_M$ is diagonal, then the final matrix $W_{0}+U M V^{T}$ can be written as $U(\\Sigma+M)V^{T}$ since $\\mathbf{\\bar{W}}_{0}\\,=\\,U\\Sigma V^{T}$ , where $(\\Sigma+M)$ is also a diagonal matrix. Thus, $U(\\Sigma+M)V^{T}$ is a valid and unique SVD of $\\dot{\\pmb{W_{0}}}+U\\dot{\\pmb{M}}V^{T}$ up to sign filps in the singular vectors.   \n(b) Expressivity: Finding $_M$ for any target matrix $P$ of size $d_{1}\\times d_{2}$ such that $P\\,=\\,W_{0}\\,+$ $U\\dot{M}V^{T}$ is the same as finding $_M$ for a new target matrix $P^{\\prime}\\;=\\;P\\,-\\,W_{0}$ such that $P^{\\prime}=U M V^{T}$ . For a full SVD, the dimension of $_M$ is $d_{1}\\times d_{2}$ and since the dimension of $P^{\\prime}$ is also $d_{1}\\times d_{2}$ , $P^{\\prime}=U M V^{T}$ is a bijection and $M=U^{T}(P-W_{0})V$ (since $U$ and $V$ are orthogonal).   \n(c) Rank: If $_M$ has $k$ non-zero elements, then the rank of the update $U M V^{T}$ will be upper bounded by $k$ (since by Gaussian elimination, $k$ or less elements will remain, the best case being all $k$ elements in the diagonal). We also know that the rank is upper bounded by $\\operatorname*{min}\\{d_{1},d_{2}\\}$ , giving an achievable upper bound on the rank as $\\operatorname*{min}\\{k,\\operatorname*{min}\\{d_{1},d_{2}\\}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "B Parameter Count Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 8: Parameter count analysis. $L_{\\mathrm{uned}}$ , $D_{\\mathrm{model}}$ , $r$ , $k$ denote total layers being adapted, hidden dimension, rank, and additional off-diagonals respectively. ", "page_idx": 13}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/5bf07cbc09d9c9667e186408d50edada7d3e21a929bde591cdc427e03081e98f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Additional Experiments and Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "All of our experiments are conducted on a Linux machine (Debian GNU) with the following specifications: $2\\!\\times\\!\\mathrm{Al}00\\;\\mathrm{80}\\,\\mathrm{GB}$ , Intel Xeon CPU $@$ $2.20\\mathrm{GHz}$ with 12 cores, and 192 GB RAM. For all our experiments (including baseline experiments), we utilize hardware-level optimizations like mixed weight precision (e.g., bfloat16) whenever possible. ", "page_idx": 13}, {"type": "text", "text": "C.1 Comparison against SVD-based Variants ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We compare SVF [31] and our proposed method, SVFT, on the GSM-8K and MATH benchmarks using Gemma-2B. The results are presented in Table 9. The results indicate that SVF and $\\mathrm{SVFT}^{P}$ exhibit comparable performance on these benchmarks, as expected due to their design equivalence. This ", "page_idx": 13}, {"type": "text", "text": "finding also applies to SVDiff [10] and SAM-parser [25] for the same reason. Additionally, the table highlights a significant performance improvement when comparing SVF to $\\operatorname{SVFT}^{R}$ , demonstrating the advantage of learning the off-diagonal elements. ", "page_idx": 14}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/73443f6560897caee02d827590070befa99d6fc28a821d08794adcdf739891c4.jpg", "table_caption": ["Table 9: Results of SVF and SVFT on GSM-8K and MATH with Gemma-2B. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C.2 Performance Evaluation with Fixed Target Module Adaptation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We compare SVFT to baseline methods, adapting the same target modules to ensure a consistent evaluation. Results are presented in Table 10, showing that SVFT outperforms other methods in this setup. ", "page_idx": 14}, {"type": "text", "text": "Table 10: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). All methods are applied on the target modules {Q,K,V,U,D} with SVFT demonstrating superior performance. When applying $\\operatorname{SVF}\\bar{\\boldsymbol{\\Gamma}}^{R}$ on Gemma-2B and LLaMA-3-8B we use $d=12$ and $d=24$ respectively. ", "page_idx": 14}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/96039ca34e73ec43ab2830ce4f1d7a7a3bf96c585cd1d5041e2adb6ebe754d19.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C.3 Commonsense Reasoning Gemma-2B ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We evaluate and compare SVFT variants against baseline PEFT methods on commonsense reasoning tasks with Gemma-2B model and tabulate results in Table 11. ", "page_idx": 14}, {"type": "text", "text": "C.4 Are All Singular Vectors Important? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To determine the importance of considering all singular vectors and singular values during finetuning, we reduce the rank of $U$ and $V$ , and truncate $\\Sigma$ and $_M$ to an effective rank of $r$ . If the original weight matrix $W\\in\\mathbb{R}^{m\\times n}$ , then after truncation, $U\\in\\mathbb{R}^{m\\times r}$ , $V\\in\\ensuremath{\\mathbb{R}}^{n\\times r}$ . This truncation significantly reduces the number of trainable parameters, so we compensate by increasing the number of off-diagonal coefficients $(d)$ in $_M$ . ", "page_idx": 14}, {"type": "text", "text": "Table 11: Results with Gemma-2B on eight commonsense reasoning benchmarks. We follow [19] for hyperparameter configurations, and report accuracy for all tasks. ", "page_idx": 15}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/0eaf167d7685ccf9e859a1314cf90b6295230fbc0d04930881aba6eb4740051c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 12: Performance with varying rank $(r)$ and the off-diagonal elements $(d)$ of $_M$ . When $r=2048$ , the update is full-rank. ", "page_idx": 15}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/ae62ffa1c9bf84ee651d31ef2adc902d2b21b9a1ad740fb91b487effdd87ede8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Our results, with four different configurations of $r$ and $d$ , are presented in Table 12. The findings show that a very low rank $r=128)$ ) leads to poor performance, even when parameters are matched. A reasonably high rank of $r=1536$ , which is $75\\%$ of the full rank, still fails to match the performance of the full-rank variant that has $0.25\\times$ the trainable parameters. This indicates that all singular vectors significantly contribute to the end task performance when fine-tuning with SVFT, and that important information is lost even when truncating sparingly. ", "page_idx": 15}, {"type": "text", "text": "C.5 Additional Memory Analysis Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present additional memory analysis experiments for Gemma-2B and LLaMA-3-8B in Table 13 and Table 14. SVFT variants consume lesser memory than DoRA and $1.2\\times$ more memory than LoRA. ", "page_idx": 15}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/e6f5b9fc897c94d6bfb3647cae2f997d5611a6536abf8202c9c500ee95b969ad.jpg", "table_caption": ["Table 13: Memory analysis for Gemma-2B. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/97d541bed145a9d8ea0b92e6fd56aebf56bebfbe753bd3f81a91243ef55f9109.jpg", "table_caption": ["Table 14: Memory analysis for LLaMA-3-8B. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.6 Performance vs Total Trainable Parameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to the experiments performed in Figure 1 (main paper) for Gemma-2B on challenging natural language generation (NLG) tasks like GSM-8K and Commonsense Reasoning, we also plot the performance vs total trainable parameters for larger state-of-the-art models like Gemma-7B and LLaMA-3-8B on GSM-8K. Figure 5 further demonstrates SVFT\u2019s pareto-dominance. On larger models, we observe that full-finetuning overfits, leading to sub-optimal performance in comparison to PEFT methods. ", "page_idx": 16}, {"type": "image", "img_path": "uS0PwIBzC0/tmp/7c4d2e8a76c578e4701e0910ffea2d68631e8314673671063b93f9ea871605bb.jpg", "img_caption": ["Figure 5: Performance versus total trainable parameters for GSM-8K on Gemma-7B (left) and LLaMA-3-8B (right). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.7 Settings for Language Tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Natural Language Understanding. We fine-tune the DeBERTa $_\\mathrm{V3}_{\\mathrm{base}}$ [11] model and apply SVFT to all linear layers in every transformer block of the model. We only moderately tune the batch size, learning rate, and number of training epochs. We use the same model sequence lengths used by [20] to keep our comparisons fair. The hyperparameters used in our experiments can be found in Table 15. ", "page_idx": 16}, {"type": "text", "text": "Natural Language Generation. See the hyperparameters used in our experiments in Table 16. For LoRA, DoRA, we adapt $Q,K,V,U,D$ matrices. We apply BOFT on $Q,V$ matrices since applying on multiple modules is computationally expensive. For VeRA, which enforces a constraint of uniform internal dimensions for shared matrices, we apply on $G,U$ projection matrices as it yields the highest number of learnable parameters. We apply SVFT on $Q,K,V,U,D,O,G$ for the Gemma family of models, and $U,D,O,G$ for LLaMA-3-8B. Note that applying SVFT on these modules does not increase trainable parameters at the same rate as applying LoRA or DoRA on them would. We adopt the code base from https://github.com/meta-math/MetaMath.git for training scripts and evaluation setups and use the fine-tuning data available at https://huggingface.co/datasets/ meta-math/MetaMathQA-40K. ", "page_idx": 16}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/ed75fb671b965d95c83f1805a242c8fb7fd4dd5ce38db55283d1d529c77c53a7.jpg", "table_caption": ["Table 15: Hyperparameter setup used for DeBERTaV3base on the GLUE benchmark. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/8c3def02a3e77aa63c712c2b2421bf8b7068f3df0c2e0c97d015b1b1f5df529a.jpg", "table_caption": ["Table 16: Hyperparameter setup used for fine-tuning on MetaMathQA-40K. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Commonsense Reasoning. See the hyperparameters used in our experiments in Table 17. We adopt the same set of matrices as that of natural language generation tasks. We use the code base from https://github.com/AGI-Edgerunners/LLM-Adapters, which also contains the training and evaluation data. ", "page_idx": 17}, {"type": "text", "text": "C.8 Settings for Vision Tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For each dataset in the vision tasks, we train on 10 samples per class, using 2 examples per class for validation, and test on the full test set. Similar to previous literature, we always train the classifier head for these methods since the number of classes is large. The parameter counts do not include the number of parameters in the classification head. The hyperparameters are mentioned in Table 18. We tune the learning rates for SVFT and BOFT select learning rates for other methods from [17], run training for 10 epochs, and report test accuracy for the best validation model. For all methods, since the classification head has to be fully trained, we report the parameter count other than the classification head. ", "page_idx": 17}, {"type": "text", "text": "Table 17: Hyperparameter setup used for fine-tuning on commonsense-15K. ", "page_idx": 18}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/55bb3f699d0a5527fad8feba889e877a1893771084f69d19f2501004f743b293.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "uS0PwIBzC0/tmp/3547b530c283cb72db0efd3d5a0d5f35411fb87e65cfb6ef5937a7c63ceed735.jpg", "table_caption": ["Table 18: Hyperparameter setup used for fine-tuning on all vision tasks. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction are motivated using extensive experiments (see section 4). A Lemma and its proof are introduced where required. We include some limitations of our work in the Limitations section (see section 6). ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We run extensive experiments on a range of models to study and analyze our method\u2019s performance. See section 6 for some limitations. We include parameter count analysis in Appendix B. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We introduce our Lemma in subsection 3.2 and its corresponding proof in Appendix A. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide our method description in section 3, experimental evaluation in section 4 and hyper-parameter ranges in Appendix C. Additionally, we provide code and scripts to replicate our experiments as part of supplementary material to provide more details. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our code is publicly available at https://github.com/vijaylingam95/ svft \u2013 also referenced in our abstract. We include all details on hyper-parameters ranges and methods in Appendix C. All our experiments are run on publicly available datasets. We provide references to the dataset source in our code. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We provide details on our method in section 3. We provide comprehensive details on hyper-parameter ranges and datasets/model names in Appendix C. Additional details on model implementation and experiments are available in the code submitted as part of supplementary material. We rely on public datasets, splits, and evaluation strategies from previously published literature. We refer and cite these works in section 4. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Since we experiment with large models, it is often difficult to run these experiments with multiple seeds and report error bars. We follow the same setup from previously published research \u2013 these research works also do not compute error bars or report them in any of their results tables. E.g, DoRA: Weight-Decomposed Low-Rank Adaptation (ICML 2024 Accepted paper). Our limited compute resources have hindered us from running multiple seed experiments. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We use a Linux machine (Debian GNU/Linux 10) with the following configuration: 2x A100 80GB, 192GB RAM, Intel Xeon CPU $\\textcircled{a}\\ 2.20\\mathrm{GHz}$ with 12 cores. Here are some more information on the compute time for our language experiments: 2B models take around 6 hours and 7B/8B models take around 14 hours for training. For vision experiments using vision transformers (ViTs), it takes up to 4 hours for training. ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We follow and abide by the ethics guidelines laid out by NeurIPS. We also preserve and conform to anonymity policies. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See section 6. Our work can allow more easy personalization of foundational models which can have both positive and negative societal impact. This is because our method provides computational efficiency (smaller memory footprint), making it less expensive to enable personalization. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: To the best of our knowledge, the paper poses no such risks on safeguards. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We cite the original papers for utilizing the assets like code (in supplementary material), datasets (subsection 4.2, and models (subsection 4.1) including their appropriate versions. We also abide by different licenses for code bases, models, and datasets used in our work like CC, MIT, META LLAMA 3 COMMUNITY LICENSE AGREEMENT, Gemma Terms of Use, and more. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide assets in the form of code and multiple scripts as a part of the supplementary material for the experiments and ablations performed in the paper. The details about model training, their respective hyperparameters, and limitations are furnished in section 4, Appendix C, and section 6 respectively. We also add the appropriate licenses for disseminating our assets, particularly code and scripts in the supplementary material. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing, nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing, nor research with human subjects. ", "page_idx": 21}]