[{"figure_path": "uS0PwIBzC0/tables/tables_5_1.jpg", "caption": "Table 1: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). #Params denote the number of trainable parameters. bold and underline represent the best and second best performing PEFT methods, respectively. SVFT offers superior/competitive performance at much lower #Params. For SVFT, we set d = 16 for Gemma and d = 12 for LLaMA-3 models.", "description": "This table presents the performance (accuracy) achieved by various parameter-efficient fine-tuning (PEFT) methods and full fine-tuning on two mathematical reasoning benchmarks: GSM-8K and MATH.  The performance is measured in terms of accuracy.  The table also shows the number of trainable parameters used by each method.  The results highlight SVFT's superior or competitive performance while using significantly fewer parameters compared to other PEFT techniques.", "section": "4.2 Datasets"}, {"figure_path": "uS0PwIBzC0/tables/tables_6_1.jpg", "caption": "Table 2: Evaluation results on eight commonsense reasoning benchmarks with Gemma-7B. We follow [19] for hyperparameter configurations, and report accuracy for all tasks. HS and WG denote HellaSwag [36] and WinoGrande [29], respectively. SVFTP offers competitive performance at a fraction of #Params. SVFT-8 can match LoRA,=32 with ~7x fewer parameters.", "description": "This table presents the performance comparison of different parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning benchmarks using the Gemma-7B language model.  It shows accuracy scores for each benchmark and highlights that SVFT (particularly SVFT with plain sparsity pattern) achieves competitive performance while using significantly fewer trainable parameters than other methods like LoRA and DORA.", "section": "5 Results"}, {"figure_path": "uS0PwIBzC0/tables/tables_6_2.jpg", "caption": "Table 3: DeBERTaV3base with different adaptation methods on the GLUE benchmark. We report matched accuracy for MNLI, Matthew\u2019s correlation for COLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all tasks. * indicates values reported in [20].", "description": "This table presents the performance of different parameter-efficient fine-tuning (PEFT) methods, including SVFT, on the GLUE benchmark using the DeBERTaV3base model.  The results show accuracy scores for various tasks within the benchmark, highlighting SVFT's performance compared to other techniques like LoRA, DORA, and BOFT.  The table provides a quantitative assessment of the different models' effectiveness in adapting to downstream tasks, focusing on the tradeoff between parameter efficiency and accuracy.", "section": "5 Results"}, {"figure_path": "uS0PwIBzC0/tables/tables_7_1.jpg", "caption": "Table 4: Performance on image classification benchmarks \u2013 CIFAR-100 (C100), Food101 (F101), Flowers102 (F102), and Resisc-45 (R45). We only adapt Q, V matrices for all methods, following prior work [17]. We report accuracy for all tasks.", "description": "This table presents the performance of different parameter-efficient fine-tuning (PEFT) methods on image classification tasks using two different vision transformer models (ViT Base and ViT Large).  The accuracy of each method is reported for four different benchmark datasets (CIFAR-100, Food101, Flowers102, and Resisc-45). Only the Q and V matrices in each model are fine-tuned for the experiments.  The results show the accuracy achieved for each method, the number of trainable parameters used, and the model variant used (ViT Base or ViT Large).", "section": "5.2 Performance on Vision Tasks"}, {"figure_path": "uS0PwIBzC0/tables/tables_8_1.jpg", "caption": "Table 5: GPU Memory analysis, measured in gigabytes (GB). We report the average performance on GSM-8K and MATH. SVFT outperforms both LoRA and DoRA in terms of performance while requiring lesser GPU memory than DoRA.", "description": "This table compares the GPU memory usage and performance of different parameter-efficient fine-tuning (PEFT) methods, including LoRA, DoRA, and SVFT variants, on the GSM-8K and MATH benchmarks using Gemma-2B and Gemma-7B language models.  It demonstrates that SVFT achieves comparable or better performance than LoRA and DoRA while using less GPU memory than DoRA.", "section": "5.3 Memory Analysis"}, {"figure_path": "uS0PwIBzC0/tables/tables_8_2.jpg", "caption": "Table 6: Results on GSM-8K after fine-tuning on Pythia-2.8B checkpoints at different stages of pre-training (PT).", "description": "This table shows the performance improvement (\u0394Perf) achieved by fine-tuning three different methods (Full-FT, LoRA, and SVFT) on the GSM-8K benchmark using Pythia-2.8B checkpoints at two different pre-training stages (39K and 143K steps).  The table displays the number of parameters (#Params) used by each method and their respective performance gains. The results demonstrate that SVFT shows a greater improvement with better pre-trained weights and that SVFT outperforms LoRA in both settings.", "section": "5.6 Impact of Pre-trained Weight Quality"}, {"figure_path": "uS0PwIBzC0/tables/tables_9_1.jpg", "caption": "Table 7: Results on fine-tuning with SVFT using different M parameterizations.", "description": "This table presents the results of fine-tuning experiments using the Singular Vectors guided Fine-Tuning (SVFT) method with four different sparsity patterns for the trainable matrix M: Plain, Banded, Random, and Top-k.  The table shows the number of parameters used, and the performance (accuracy) on the GSM-8K and MATH benchmarks for three different language models: Gemma-2B, Gemma-7B, and LLaMA-3-8B. The average rank achieved by each sparsity pattern across the three models is also provided. This allows for a comparison of performance and parameter efficiency for the different sparsity patterns.", "section": "5 Results"}, {"figure_path": "uS0PwIBzC0/tables/tables_13_1.jpg", "caption": "Table 8: Parameter count analysis. Ltuned, Dmodel, r, k denote total layers being adapted, hidden dimension, rank, and additional off-diagonals respectively.", "description": "This table compares the number of trainable parameters for different parameter-efficient fine-tuning (PEFT) methods.  It shows the formulas for calculating the number of trainable parameters for LoRA, DoRA, VeRA, and two variants of SVFT (SVFTP and SVFTB).  The formulas take into account the number of layers being adapted (Ltuned), the model dimension (Dmodel), the rank (r), and the number of additional off-diagonal elements (k).  This provides a quantitative comparison of the parameter efficiency of different PEFT methods.", "section": "B Parameter Count Analysis"}, {"figure_path": "uS0PwIBzC0/tables/tables_14_1.jpg", "caption": "Table 9: Results of SVF and SVFT on GSM-8K and MATH with Gemma-2B.", "description": "This table compares the performance of SVF and SVFT on the GSM-8K and MATH benchmarks using the Gemma-2B model.  It shows that SVFT, especially with off-diagonal elements, outperforms SVF. The results highlight the advantage of learning the off-diagonal elements in SVFT for improved performance.", "section": "C.1 Comparison against SVD-based Variants"}, {"figure_path": "uS0PwIBzC0/tables/tables_14_2.jpg", "caption": "Table 10: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). All methods are applied on the target modules {Q,K,V,U,D} with SVFT demonstrating superior performance. When applying SVFTR on Gemma-2B and LLaMA-3-8B we use d = 12 and d = 24 respectively.", "description": "This table presents the accuracy results on GSM-8K and MATH benchmark datasets for different parameter-efficient fine-tuning (PEFT) methods.  The methods are compared based on their performance, measured by accuracy, and the number of trainable parameters used.  The target modules (Q, K, V, U, D) are the same across all methods, allowing for a fairer comparison.  The table shows that SVFT achieves better accuracy than other methods with similar or fewer parameters.", "section": "4.2 Datasets"}, {"figure_path": "uS0PwIBzC0/tables/tables_15_1.jpg", "caption": "Table 2: Evaluation results on eight commonsense reasoning benchmarks with Gemma-7B. We follow [19] for hyperparameter configurations, and report accuracy for all tasks. HS and WG denote HellaSwag [36] and WinoGrande [29], respectively. SVFTP offers competitive performance at a fraction of #Params. SVFT-8 can match LoRAr=32 with ~7x fewer parameters.", "description": "This table presents the performance of various parameter-efficient fine-tuning (PEFT) methods on eight commonsense reasoning benchmarks using the Gemma-7B language model.  It compares the accuracy achieved by Full Fine-Tuning (Full-FT), LoRA (with different ranks), DoRA (with different ranks), BOFT, VeRA, and SVFT (both plain and banded versions) across the benchmarks. The table highlights SVFT's competitive performance while using significantly fewer trainable parameters than other methods.", "section": "5 Results"}, {"figure_path": "uS0PwIBzC0/tables/tables_15_2.jpg", "caption": "Table 12: Performance with varying rank (r) and the off-diagonal elements (d) of M. When r = 2048, the update is full-rank.", "description": "This table presents the performance of SVFT on GSM-8K and MATH datasets with different rank (r) and number of off-diagonal elements (d) in the trainable matrix M.  It shows how performance changes as the model's expressivity and the number of trainable parameters are varied. The full-rank model (r=2048) achieves the best performance, highlighting the importance of considering all singular vectors and values during fine-tuning.", "section": "5 Results"}, {"figure_path": "uS0PwIBzC0/tables/tables_15_3.jpg", "caption": "Table 5: GPU Memory analysis, measured in gigabytes (GB). We report the average performance on GSM-8K and MATH. SVFT outperforms both LoRA and DoRA in terms of performance while requiring lesser GPU memory than DoRA.", "description": "This table compares the GPU memory usage and performance (measured by accuracy on GSM-8K and MATH datasets) of different parameter-efficient fine-tuning (PEFT) methods.  The methods compared are LoRA, DoRA, and SVFT with varying numbers of parameters.  The results show that SVFT achieves comparable or better performance while using less GPU memory than DoRA, although using slightly more memory than LoRA.", "section": "5.3 Memory Analysis"}, {"figure_path": "uS0PwIBzC0/tables/tables_16_1.jpg", "caption": "Table 14: Memory analysis for LLaMA-3-8B.", "description": "This table presents the GPU memory usage (in GB) and performance (accuracy) on GSM-8K and MATH benchmarks for different parameter-efficient fine-tuning (PEFT) methods, including LoRA, DoRA, and SVFT variants.  The table shows the number of parameters (#Params) used by each method and the corresponding GPU memory consumption. It provides a comparison of memory efficiency and performance across various PEFT techniques, highlighting the trade-off between model size and accuracy.  The target modules adapted are also specified for each method.", "section": "5.3 Memory Analysis"}, {"figure_path": "uS0PwIBzC0/tables/tables_17_1.jpg", "caption": "Table 15: Hyperparameter setup used for DeBERTaV3base on the GLUE benchmark.", "description": "This table shows the hyperparameter settings used for fine-tuning the DeBERTaV3base model on the GLUE benchmark.  It includes details for both SVFTP and SVFT2 (with d=2), specifying the optimizer (AdamW), warmup ratio (0.1), learning rate schedule (linear), learning rates (for the head and the main model), maximum sequence length, number of epochs, and batch size for each task within the GLUE benchmark (MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, STS-B).  The table provides a detailed configuration of the hyperparameters used in the experiments described in the paper, facilitating reproducibility and comparison.", "section": "C.7 Settings for Language Tasks"}, {"figure_path": "uS0PwIBzC0/tables/tables_17_2.jpg", "caption": "Table 1: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). #Params denote the number of trainable parameters. bold and underline represent the best and second best performing PEFT methods, respectively. SVFT offers superior/competitive performance at much lower #Params. For SVFT, we set d = 16 for Gemma and d = 12 for LLaMA-3 models.", "description": "This table compares the performance of SVFT against other parameter-efficient fine-tuning (PEFT) methods and full fine-tuning on two mathematical reasoning benchmarks: GSM-8K and MATH.  The results are presented in terms of accuracy and the number of trainable parameters used by each method.  The table highlights that SVFT achieves either the best or second-best accuracy while using significantly fewer parameters than the other methods.", "section": "4.1 Base Models & Setup"}, {"figure_path": "uS0PwIBzC0/tables/tables_18_1.jpg", "caption": "Table 17: Hyperparameter setup used for fine-tuning on commonsense-15K.", "description": "This table shows the hyperparameter settings used for fine-tuning on the commonsense-15K dataset.  It lists the hyperparameters for different models (Gemma-2B and Gemma-7B) and variations of the SVFT method (SVFT<sup>P</sup> and SVFT<sup>B</sup><sub>d=8</sub>).  The hyperparameters include the optimizer, warmup steps, learning rate schedule, maximum sequence length, number of epochs, batch size, and the learning rate itself. These settings are crucial for reproducibility of the experiments in this section of the paper.", "section": "C.3 Commonsense Reasoning Gemma-2B"}, {"figure_path": "uS0PwIBzC0/tables/tables_18_2.jpg", "caption": "Table 1: Performance (Accuracy) on Mathematical Reasoning (GSM-8K and MATH). #Params denote the number of trainable parameters. bold and underline represent the best and second best performing PEFT methods, respectively. SVFT offers superior/competitive performance at much lower #Params. For SVFT, we set d = 16 for Gemma and d = 12 for LLaMA-3 models.", "description": "This table presents the performance (accuracy) of different parameter-efficient fine-tuning (PEFT) methods and full fine-tuning on two mathematical reasoning benchmarks: GSM-8K and MATH.  It compares the accuracy achieved by various methods (Full-FT, LoRA, DoRA, BOFT, VeRA, and SVFT) while considering the number of trainable parameters used. The table highlights SVFT's superior or competitive performance with significantly fewer trainable parameters compared to other methods.  The hyperparameter 'd' used in SVFT is specified for different base models.", "section": "4 Experiments"}]