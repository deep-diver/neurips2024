[{"type": "text", "text": "Learning-Augmented Approximation Algorithms for Maximum Cut and Related Problems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vincent Cohen-Addad Tommaso d\u2019Orsi\u2217 Anupam Gupta\u2020 Google Research Bocconi University New York University France Italy New York NY 10012   \ncoheaddad@google.com tommaso.dorsi@unibocconi.it anupam.g@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Euiwoong Lee University of Michigan Ann Arbor MI 48105 euiwoong@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Debmalya Panigrahi   \nDuke University   \nDurham NC 27708   \ndebmalya@cs.duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, there has been a surge of interest in the use of machine-learned predictions to bypass worst-case lower bounds for classical problems in combinatorial optimization. So far, the focus has mostly been on online algorithms, where information-theoretic barriers are overcome using predictions about the unknown future. In this paper, we consider the complementary question of using learned information to overcome computational barriers in the form of approximation hardness of polynomial-time algorithms for NP-hard (offilne) problems. We show that noisy predictions about the optimal solution can be used to break classical hardness results for maximization problems such as the max-cut problem and more generally, maximization versions of constraint satisfaction problems (CSPs). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The design and analysis of algorithms beyond the classical worst-case paradigm has been an active area of research (see, e.g., the collection of surveys by Roughgarden [2020]). In recent years, this has been accelerated by the success and widespread adoption of machine learning models across application domains, leading researchers to ask: can we use machine-learned information to solve typical instances of a problem better than what we can hope for in the worst case? This meta-question has been particularly influential in the realm of online optimization, where the goal is to design algorithms for inputs that are revealed sequentially over time. Indeed, assuming that the future unfolds in a typical rather than worst-case manner, we can compensate for the information deficit of the online algorithm with predictions learned based on past data, thereby helping it bypass information-theoretic lower bounds. This principle has been successfully applied to a broad range of online problems, such as caching [Lykouris and Vassilvitskii, 2021], rent-or-buy [Purohit et al., 2018], covering Bamas et al. [2020b], network design Azar et al. [2022], scheduling Azar et al. [2021], matching Dinitz et al. [2021], and many others (see related work for more references). ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study the role of machine-learned predictions in offline NP-hard problems. For offline problems, an algorithm has no information disadvantage compared to an optimal solution: the disadvantage is computational. The NP-hardness of problems makes exact algorithms that run in polynomial time unlikely. This has led to the field of approximation algorithms, where the goal is to obtain polynomial-time algorithms that output solutions that are approximately optimal. In particular, an approximation algorithm for an optimization problem has an approximation factor of $\\alpha$ if the solution it produces on every instance is within a factor of $\\alpha$ of that of an optimal solution. The best approximation factor for an NP-hard offline problem is also subject to computational barriers. For instance, for the classical MAXCUT problem, it is known that the approximation factor of $\\alpha_{\\mathrm{GW}}\\approx0.878$ obtained in the celebrated work of Goemans and Williamson [1995] is the best possible under the Unique Games Conjecture (UGC). This raises a natural question: can we use machine-learned predictions to overcome computational barriers to approximation algorithms for NP-hard problems? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Suppose we are given a noisy prediction that is mildly correlated with an optimal solution for a given problem instance. Can we use such a prediction to recover a better approximation to the optimal solution than is possible without any prediction? For (strongly) NP-hard problems, we typically know barriers for the best approximation factor $\\alpha$ that can be achieved by polynomial-time algorithms. Using a machine-learned prediction of an optimal solution, we seek to go beyond this barrier: to obtain a polynomial-time algorithm with an approximation factor strictly better than $\\alpha$ . The quantum of improvement naturally depends on the quality of the prediction: if the prediction is $\\varepsilon$ -correlated with the target solution, can we get an $\\alpha+f(\\varepsilon)$ approximation? ", "page_idx": 1}, {"type": "text", "text": "To make these questions concrete, we first consider the MAXCUT problem in the learning-augmented setting. Given an undirected, weighted graph, the MAXCUT problem asks for a bi-partition of the vertices such that the total weight of edges in the cut is maximized. Assuming the widely-accepted unique games conjecture, the Goemans-Williamson approximation bound of $\\alpha_{\\mathrm{GW}}\\approx0.878$ is the best possible for a polynomial-time algorithm. But, suppose we are given a prediction for the optimal max-cut that is independently correct for every vertex with probability $^{1}\\!/\\!_{2}+\\varepsilon$ , for any $\\varepsilon>0$ . (Note that random guessing achieves correctness of $^1\\!/\\!2$ ; so, we assume that the prediction is only $\\varepsilon$ -better than random guesses.) Can this prediction be used to obtain an approximation factor better than $\\alpha_{\\mathrm{GW}}$ in polynomial time? This question was posed by Svensson in his SODA 2023 plenary lecture. ", "page_idx": 1}, {"type": "text", "text": "Our first result is to unconditionally improve on worst-case performance for the MAXCUT problem using an $\\varepsilon$ -correlated prediction. This answers the question posed by Svensson in the affirmative. Specifically, we give an algorithm that obtains an $(\\bar{\\alpha_{\\mathrm{GW}}}+\\tilde{\\Omega}\\bar{(\\varepsilon^{4})})$ -approximate MAXCUT solution. This also quantifies the dependence between the improvement in the approximation factor and the correlation of the prediction with the actual optimal solution. Interestingly, we also relax the independence requirement to just pairwise independence of the predictions on the vertices. This is significant because in practice, the predictions for the different vertices are likely to be obtained from a machine learning model or a human expert, either of which sources are unlikely to output completely independent predictions for different vertices. ", "page_idx": 1}, {"type": "text", "text": "We further complement this result by considering another natural prediction model where instead of a noisy prediction for every vertex, we get a correct prediction but only for an $\\varepsilon$ -fraction of randomly chosen vertices. (To distinguish between these models, we call the former noisy predictions and the latter partial predictions.) In this case, we obtain an $\\left(\\alpha_{\\mathrm{RT}}+\\Omega(\\varepsilon)\\right)$ -approximate solution to MAXCUT, where $\\alpha_{\\mathrm{RT}}\\simeq0.858$ is the approximation factor obtained by Raghavendra and Tan for the MAXBISECTION problem [Raghavendra and Tan, 2012]. Note that $\\alpha_{\\mathrm{{RT}}}$ is slightly smaller than $\\alpha_{\\mathrm{GW}}$ but we get a better advantage of $\\Omega(\\varepsilon)$ instead of $\\Omega(\\varepsilon^{4})$ . ", "page_idx": 1}, {"type": "text", "text": "Next, we show that our algorithmic framework is applicable beyond the MAXCUT problem. Constraint Satisfaction Problems $(C S P s)$ are a broad class of optimization problems that includes fundamental optimization tasks such as MAX- $k$ -SAT, MAX- $k$ -LIN, MAX- $k$ -AND, etc. In particular, 2-CSPs are a subclass of CSPs where each constraint contains only two variables. This includes problems such as MAXCUT, MAXDICUT, and MAX-2-SAT. A classical result of Arora et al. [1999] showed that it is possible to obtain an approximation factor arbitrarily close to 1 for \u201cdense\u201d instances of all 2-CSPs including MAXCUT. We show that $\\varepsilon$ -correlated predictions of the optimal solution is a useful tool for dense instances of all 2-CSPs. In particular, we use the prediction to lower the \u201cdensity threshold\u201d for obtaining an arbitrarily small approximation factor for dense instances of 2-CSPs. In other words, the assumption on the density of the instance (which is a function of the prediction bias and the approximation error) for our result in the learning-augmented setting is weaker than that of Arora et al. [1999], i.e., our result applies to a broader set of instances. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In recent years, the abundance of data and the tremendous success of machine learning has led to a variety of attempts at going beyond traditional worst-case analysis for combinatorial optimization by taking advantage of learned information. In clustering, Ashtiani et al. [2016] introduced a setting where an algorithm can query an external oracle (e.g., a machine learning model) to learn if a pair of points are in the same cluster in the optimal clustering (same-cluster queries). The goal then is to recover an optimal solution (or a sufficiently good approximation) while minimizing the number of oracle queries. Tight bounds have been obtained for various clustering objectives in this setting, from $k$ -means [Ailon et al., 2018] to correlation clustering [Mazumdar and Saha, 2017]. Moreover, robust settings that incorporate noise in the oracle answers have also been studied Larsen et al. [2020], Del Pia et al. [2022]. Another line of work Ergun et al. [2022], Gamlath et al. [2022] considers $k$ -means and related clustering problems where the algorithm is provided noisy node labels. For instance, Gamlath et al. [2022] showed that even when the labels provided by the oracle are correct with a tiny probability (say $1\\%$ ), it is possible to obtain a $(1+o(1))$ -approximation to the $k$ -means objective as long as the clusters are not too small. ", "page_idx": 2}, {"type": "text", "text": "A different line of work has aimed to incorporate machine-learned predictions in the design of online algorithms (see, e.g., the surveys Mitzenmacher and Vassilvitskii [2020, 2022]). This model was introduced by Lykouris and Vassilvitskii for the caching problem Lykouris and Vassilvitskii [2021] and has since been studied in many problem domains such as rent or buy Purohit et al. [2018], Gollapudi and Panigrahi [2019], Anand et al. [2020], covering Bamas et al. [2020b], Anand et al. [2022], Gupta et al. [2022], scheduling Purohit et al. [2018], Wei and Zhang [2020], Bamas et al. [2020a], Lattanzi et al. [2020], Mitzenmacher [2020], Azar et al. [2021], Cohen and Panigrahi [2023], Lindermayr et al. [2023], Lassota et al. [2023], caching Lykouris and Vassilvitskii [2021], Wei [2020], Jiang et al. [2022b], Bansal et al. [2022], matching Dinitz et al. [2021], Lavastida et al. [2021], secretary problems Antoniadis et al. [2020b], D\u00fctting et al. [2021], graph problems Antoniadis et al. [2020a], Jiang et al. [2022a], Almanza et al. [2021], Antoniadis et al. [2023], Bernardini et al. [2022], Anand et al. [2022], Fotakis et al. [2021], Azar et al. [2022], and many others. (The reader is referred to ALP [2023] for a compendium of papers in this area.) The main goal in this line of work is to overcome information-theoretic lower bounds for online problems using machine-learned predictions about the unknown future. ", "page_idx": 2}, {"type": "text", "text": "Two recent works (concurrent and independent of ours) have considered the computational complexity of MAXCUT and CSPs in the context of noisy predictions. The first of these is Bampis et al. [2024], who tackle the problem of speeding up approximation schemes for dense CSP instances using noisy predictions. Namely, they provide an algorithm that achieves a $(1-\\varepsilon)$ approximation whose running time depends on the density of the instance and the error in the prediction. The algorithm runs in polynomial time if the number of edges in the instance is $\\Omega(n^{2}/\\log{n})$ , assuming the prediction label of each vertex is correct with constant probability. The second work is by Ghoshal et al. [2024], who consider both the partial a\u221and the noisy predictions models under full independence of the predictions. They provide a $(1^{\\overline{{-}}}O((\\varepsilon{\\sqrt{\\Delta}})^{-1}))$ -approximation for both models, where $\\Delta$ is the average degree of the graph. Alternatively, they show that if the edges are weighted, the value of the solution computed is at least opt $-\\,\\sqrt{n\\sum_{i j}w_{i j}^{2}}\\varepsilon^{-1}$ . ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The MAXCUT Problem. We start by describing the MAXCUT problem. In this problem, we are given a weighted graph $G=(V,E)$ represented by a (symmetric) $n\\times n$ adjacency matrix $A$ , where $A_{i j}\\,=\\,w_{i j}$ , the weight of edge $\\{i,j\\}$ if it exists, and 0 otherwise. (We assume the graph has no self-loops, and hence $A$ has zeroes on the diagonal.) We let $\\begin{array}{r}{W_{i}=\\sum_{j}w_{i j}}\\end{array}$ denote the weighted degree of vertex $i$ . We use $D:=\\operatorname{diag}(W_{1},\\ldots,W_{n})$ to denote the diagonal matrix with these weighted degrees, and $L=D-A$ to denote the (unnormalized) Laplacian matrix of the graph. Note that $x\\in\\{-1,1\\}^{n}$ denotes a cut in the graph, and the quadratic form $\\begin{array}{r}{\\langle x,L x\\rangle=\\sum_{\\{i,j\\}\\in E}\\dot{w}_{i j}(x_{i}-x_{j})^{2}}\\end{array}$ counts (four times) the weight of edges crossing the cut between the vertices labeled 1, and those labeled $-1$ . ", "page_idx": 2}, {"type": "text", "text": "The MAXCUT problem asks for the cut with maximum edge weight. Hence, MAXCUT can be rephrased as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{MAXCUT}(G):=\\operatorname*{max}_{x\\in\\{-1,1\\}^{n}}1/4\\cdot\\langle x,L x\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We defer the formal definition of CSPs to Section 5. ", "page_idx": 3}, {"type": "text", "text": "The Noisy/Partial Predictions Framework. We describe the prediction models for the MAXCUT problem. We extend the noisy predictions model to general Max-2-CSPs in Section 5. ", "page_idx": 3}, {"type": "text", "text": "1. In the noisy predictions model for MAXCUT, we assume there is some fixed and unknown optimal solution $x^{*}~\\in~\\{-1,1\\}^{n}$ . The algorithm has access to a prediction vector $Y\\ \\in\\ \\{-1,1\\}^{n}$ , such that for each vertex $i.$ , $Y_{i}$ is the correct label $\\boldsymbol{x}_{i}^{*}$ with some (unknown) probability $^{1/2}+\\varepsilon,$ , and is the other (incorrect) label with probability $1/2-\\varepsilon$ . Here we only assume pairwise independence; for any two vertices $i$ and $j$ , $\\mathrm{Pr}[i,j$ both give their correct labels] $\\mathbf{\\bar{\\rho}}=(1/2+\\varepsilon)^{2}$ . 2. In the partial predictions model for MAXCUT, the algorithm has access to a prediction vector $Y\\,\\in\\,\\{-1,0,1\\}^{n}$ such that for each vertex $i.$ , $Y_{i}$ is the correct label $\\boldsymbol{x}_{i}^{*}$ with some (unknown) probability $\\varepsilon$ and is 0 otherwise. Again, we only assume pairwise independence; for any two vertices $i$ and $j$ , $\\mathrm{Pr}[i,j$ both give their correct labels] $\\c=\\varepsilon^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "3 MAXCUT in the Noisy Prediction Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall that in the noisy prediction model, the predicted label of each vertex is its correct label in a fixed max-cut with probability $1/2+\\varepsilon$ . Moreover, the labels are pairwise independent. One can show that in this model, if one were to simply output the prediction itself, then its value would be at least $O(m/2+\\varepsilon^{2}(\\mathrm{opt}-m/2))$ for an unweighted graphs with $m$ edges. But, this is generally worse than the $\\alpha_{\\mathrm{GW}}\\cdot\\mathrm{opt}\\approx0.878$ \u00b7 opt bound obtained by the Goemans-Williamson MAXCUT algorithm. Our main result is to give an algorithm that uses a noisy prediction to outperform $\\alpha_{\\mathrm{GW}}$ in the approximation bound by an additive $\\mathrm{poly}(\\varepsilon)$ factor: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Noisy Predictions). Given noisy predictions with a bias of $\\varepsilon$ , there is a polynomial-time randomized algorithm that obtains an approximation factor of $\\alpha_{G W}+\\tilde{\\Omega}(\\varepsilon^{4})$ in expectation for the MAXCUT problem. ", "page_idx": 3}, {"type": "text", "text": "The rest of this section is devoted to proving the above theorem. A basic distinction that we will use throughout this section is that of $\\Delta$ -wide and $\\Delta$ -narrow graphs; these should be thought of as weighted analogs of high-degree and low-degree graphs. We first define these and related concepts below, then we present an algorithm for the MAXCUT problem on $\\Delta$ -wide graphs in $\\S3.1$ , followed by the result for $\\Delta$ -narrow graphs in $\\S3.2$ . We finally wrap up with the proof of Theorem 3.1. ", "page_idx": 3}, {"type": "text", "text": "We partition the edges incident to vertex $i$ into two sets: the $\\Delta$ -prefix for $i$ comprises the $\\Delta$ heaviest edges incident to $i$ (breaking ties arbitrarily), while the remaining edges make up the $\\Delta$ -suffix for $i$ . We fix a parameter $\\eta\\in\\,(0,1/2)$ . We will eventually set $\\Delta=\\overline{{\\Theta(1/\\varepsilon^{2})}}$ and $\\eta$ to be an absolute constant. Recall that $\\begin{array}{r}{W_{i}=\\sum_{j\\in[n]}A_{i j}}\\end{array}$ is the weighted degree of $i$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 ( $\\Delta$ -Narrow/Wide Vertex). $A$ vertex $i$ is $\\Delta$ -wide if the total weight of edges in its $\\Delta$ -prefix is at most $\\eta W_{i}$ , and so the weight of edges in its $\\Delta$ -suffix is at least $(1-\\eta)W_{i}$ . Otherwise, the vertex $i$ is $\\Delta$ -narrow. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, a $\\Delta$ -wide vertex is one where most of its weighted degree is preserved even if we ignore the $\\Delta$ heaviest edges incident to the vertex. ", "page_idx": 3}, {"type": "text", "text": "We partition the vertices $V=[n]$ into the $\\Delta$ -wide and $\\Delta$ -narrow sets; these are respectively denoted $V_{>\\Delta}$ and $V_{<\\Delta}$ . We define $\\begin{array}{r}{\\overbar{W_{>\\Delta}}:=\\sum_{i\\in V_{>\\Delta}}W_{i}}\\end{array}$ >\u2206Wi and W<\u2206:= i\u2208V<\u2206 , and hence the sum of weighted degrees of all vertices is $\\begin{array}{r}{W:=\\sum_{i=1}^{n}W_{i}=W_{>\\Delta}+W_{<\\Delta}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3 ( $\\Delta$ -Narrow/Wide Graph). $A$ graph is $\\Delta$ -wide if the sum of weighted degrees of $\\Delta$ -wide vertices accounts for at least $1\\!-\\!\\eta$ fraction of that of all vertices; i.e., i $f W_{>\\Delta}\\geq(1\\!-\\!\\eta)W$ . Otherwise, it is $\\Delta$ -narrow. ", "page_idx": 3}, {"type": "text", "text": "3.1 Solving MAXCUT for $\\Delta$ -wide graphs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For $\\Delta$ -wide graphs, we show: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. Fix $\\varepsilon^{\\prime}\\,\\in\\,(0,1)$ . Given noisy predictions with bias \u03b5, there is a polynomial-time randomized algorithm that, given any $\\Delta$ -wide graph, outputs a cut of value at least the maximum cut minus $(5\\eta+2\\Bar{\\varepsilon}^{\\prime})W$ , where $\\bar{\\Delta}:=\\dot{O(1/(\\varepsilon\\cdot\\varepsilon^{\\prime})^{2})}$ , with probability 0.98. ", "page_idx": 4}, {"type": "text", "text": "Since the graph is $\\Delta$ -wide, most vertices have their weight spread over a large number of their neighbors. In this case, the prediction vector allows us to obtain a good estimate $\\hat{r}$ of the optimal neighborhood imbalance $r^{*}$ (the difference between the number of neighbors a vertex has on its side versus the other side of the optimal cut). We can then write an LP to assign fractional labels to vertices that maximize the cut value while remaining faithful to these estimates $\\hat{r}$ ; finally rounding the LP gives the solution. ", "page_idx": 4}, {"type": "text", "text": "3.1.1 The $\\Delta$ -wide Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Define an $n\\times n$ matrix $\\tilde{A}$ from the adjacency matrix $A$ as follows: for each row corresponding to the edges incident to a vertex $i$ , we set the entry $\\tilde{A}_{i j}=0$ if the edge $(i,j)$ is in the $\\Delta$ -prefix of vertex $i$ ; otherwise, $\\tilde{A}_{i j}=A_{i j}$ . Now, define an $n$ -dimensional vector $\\hat{r}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{r}_{i}=\\left\\{\\overset{1}{2}(\\overset{\\cdot}{A}Y)_{i}\\quad\\mathrm{if~}i\\mathrm{~is~}\\Delta\\mathrm{-wide}\\right.\\quad}\\\\ {0\\quad\\quad\\quad\\quad\\,\\mathrm{if~}i\\mathrm{~is~}\\Delta\\mathrm{-narrow}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Y$ is the prediction vector. Solve the linear program: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in[-1,1]^{n}}\\left\\langle\\hat{r},x\\right\\rangle\\quad s.t.\\quad\\|\\hat{r}-A x\\|_{1}\\leq(\\varepsilon^{\\prime}+2\\eta)W.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\hat{x}\\in[-1,1]^{n}$ be the optimal LP solution. ", "page_idx": 4}, {"type": "text", "text": "Finally, do the following $O(^{1}/\\eta)$ times independently, and output the best cut $X^{*}$ among them: randomly round the fractional solution $\\hat{x}$ independently for each vertex to get a cut $X\\in\\{-1,1\\}^{n}$ ; namely, $\\mathrm{Pr}[X_{i}=1]=(1{+}\\hat{x}_{i})/2$ and $\\operatorname*{Pr}[X_{i}={\\bar{-}}1]=(1{\\bar{-}}\\hat{x}_{i})/2$ . ", "page_idx": 4}, {"type": "text", "text": "3.1.2 The Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For a labeling $x\\in\\{-1,1\\}^{n}$ , the neighborhood imbalance for vertex $i$ is defined as $\\textstyle\\sum_{j}A_{i j}x_{j}=$ $(A x)_{i}$ . This denotes the (signed) difference between the total weight of edges incident to $i$ that appear and do not appear in the cut defined by the labeling $x$ . The maximality of the optimal cut $x^{*}\\,\\in\\,\\{-1,1\\}^{n}$ ensures that $x_{i}^{*}\\cdot\\mathrm{sign}((A x^{*})_{i})\\,\\leq\\,0$ for all $i$ ; else, switching $x_{i}$ from 1 to $-1$ or vice-versa increases the objective. Define $r^{*}:=A x^{*}$ as the vector of imbalances for the optimal cut. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.5. The vector $\\hat{r}$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\hat{r}-r^{*}\\right\\rVert_{1}\\right]:=\\mathbb{E}\\left[\\sum_{i=1}^{n}\\left|\\hat{r}_{i}-r_{i}^{*}\\right|\\right]\\leq O\\bigg(\\frac{W}{\\varepsilon\\sqrt{\\Delta}}\\bigg)+2\\eta W.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[Y_{i}]=x_{i}^{*}\\cdot\\operatorname*{Pr}[Y_{i}=x_{i}^{*}]-x_{i}^{*}\\cdot\\operatorname*{Pr}[Y_{i}=-x_{i}^{*}]=x_{i}^{*}(1/2+\\varepsilon)-x_{i}^{*}(1/2-\\varepsilon)=2\\varepsilon x_{i}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Define $\\begin{array}{r}{Z:={\\frac{1}{2\\varepsilon}}Y}\\end{array}$ . Then, $\\mathbb{E}[Z]=x^{*}$ , and so $\\mathbb{E}[A Z]=r^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "First, we consider a $\\Delta$ -narrow vertex $i$ . Since $\\hat{r}_{i}=0$ , we have $\\left|\\hat{r}_{i}-r_{i}^{*}\\right|=|r_{i}^{*}|\\leq W_{i}$ . So summing over all $\\Delta$ -narrow vertices gives ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{i\\in V_{<\\Delta}}|\\hat{r}_{i}-r_{i}^{*}|\\leq\\sum_{i\\in V_{<\\Delta}}W_{i}\\leq\\eta W,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "since the graph is $\\Delta$ -wide. ", "page_idx": 4}, {"type": "text", "text": "Now, we consider a $\\Delta$ -wide vertex $i$ . We have ", "page_idx": 4}, {"type": "equation", "text": "$$\n|\\hat{r}_{i}-r_{i}^{*}|=|(\\tilde{A}Z)_{i}-r_{i}^{*}|\\leq|\\mathbb{E}[(\\tilde{A}Z)_{i}]-r_{i}^{*}|+|(\\tilde{A}Z)_{i}-\\mathbb{E}[(\\tilde{A}Z)_{i}]|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To bound the first term in the RHS of (3), recall that $r_{i}^{*}=\\mathbb{E}[(A Z)_{i}]$ . Thus, ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\mathbb{E}[(\\tilde{A}Z)_{i}]-r_{i}^{*}|=|\\mathbb{E}[(\\tilde{A}Z)_{i}]-\\mathbb{E}[(A Z)_{i}]|=\\langle(\\tilde{A}-A)_{i},\\mathbb{E}[Z]\\rangle.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $\\mathbb{E}[Z]=x^{*}\\in\\{-1,1\\}^{n}$ , we get ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\mathbb{E}[(\\tilde{A}Z)_{i}]-r_{i}^{*}|=(\\tilde{A}-A)_{i}\\cdot x^{*}\\leq\\|(\\tilde{A}-A)_{i}\\|_{1}\\|x^{*}\\|_{\\infty}\\leq\\eta W_{i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where in the last step, we used the fact that $i$ is a $\\Delta$ -wide vertex. ", "page_idx": 5}, {"type": "text", "text": "Now, we bound the second term in the RHS of (3). Using Chebyshev\u2019s inequality on the sum $\\begin{array}{r}{(\\tilde{A}Z)_{i}=\\sum_{j}\\tilde{A}_{i j}Z_{j}}\\end{array}$ , we get ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[|(\\tilde{A}Z)_{i}-\\mathbb{E}[(\\tilde{A}Z)_{i}]|\\ge\\lambda_{i}]\\le\\frac{\\mathrm{var}((\\tilde{A}Z)_{i})}{\\lambda_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the variables $Z_{j}$ are pairwise independent, the variance $\\begin{array}{r}{\\mathrm{var}((\\tilde{A}Z)_{i})=\\sum_{j}\\tilde{A}_{i j}^{2}\\mathrm{var}(Z_{j})}\\end{array}$ . The variance of each $Z_{j}$ is ${\\cal O}(1/\\varepsilon^{2})$ . For $\\sum_{j}\\tilde{A}_{i j}^{2}$ , we know ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{j\\in[n]}\\tilde{A}_{i j}^{2}=\\|\\tilde{A}_{i}\\|_{2}^{2}\\leq\\|\\tilde{A}_{i}\\|_{1}\\cdot\\|\\tilde{A}_{i}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the weight of any edge in the $\\Delta$ -suffix of $i$ is at most $W_{i}/\\Delta$ . Therefore, by our definition of $\\tilde{A}$ , we have $\\|\\tilde{A}_{i}\\|_{\\infty}\\leq W_{i}/\\Delta$ . Since $\\tilde{A}_{i j}\\leq A_{i j}$ for all $j\\in[n]$ , we also have $\\|\\tilde{A}_{i}\\|_{1}\\leq\\|A_{i}\\|_{1}=W_{i}$ . Applying these bounds, we get: $\\begin{array}{r}{\\sum_{j\\in[n]}\\tilde{A}_{i j}^{2}\\le W_{i}^{2}/\\Delta}\\end{array}$ . Therefore, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[|(\\tilde{A}Z)_{i}-\\mathbb{E}[(\\tilde{A}Z)_{i}]|]\\le\\sqrt{\\mathrm{var}((\\tilde{A}Z)_{i})}\\le O(W_{i}/(\\varepsilon\\sqrt{\\Delta})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Summing over all $\\Delta$ -wide vertices, we get ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\sum_{i\\in V_{>\\Delta}}\\left|\\hat{r}_{i}-r_{i}^{*}\\right|\\bigg]\\leq O\\left(\\frac{W_{>\\Delta}}{\\varepsilon\\sqrt{\\Delta}}\\right)+\\eta W_{>\\Delta}\\leq O\\left(\\frac{W}{\\varepsilon\\sqrt{\\Delta}}\\right)+\\eta W_{\\varepsilon}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Combining with (2) for $\\Delta$ -narrow vertices, we get ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\hat{r}_{i}-r_{i}^{*}\\|_{1}\\right]\\leq O\\left(\\frac{W}{\\varepsilon\\sqrt{\\Delta}}\\right)+2\\eta W.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now using Markov\u2019s inequality with Lemma 3.5, we get that setting $\\Delta=\\Omega(1/(\\varepsilon\\varepsilon^{\\prime})^{2})$ for any fixed constant $\\varepsilon^{\\prime}>0$ ensures that we get a vector of empirical imbalances $\\hat{r}$ satisfying ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lVert\\widehat{\\boldsymbol{r}}-\\boldsymbol{r}^{*}\\rVert_{1}\\leq(\\varepsilon^{\\prime}+2\\eta)W.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with probability at least 0.99. (Since the $2\\eta W$ losses are deterministically bounded, we can use Markov\u2019s inequality only on the random variable $\\begin{array}{r}{\\sum_{i\\in V_{>\\Delta}}|(\\tilde{A}Z)_{i}-\\mathbb{E}[(\\tilde{A}\\dot{Z})_{i}]|}\\end{array}$ .) Hence, when the event in (4) happens, the vector $x^{*}$ is a feasible solution to LP (1). ", "page_idx": 5}, {"type": "text", "text": "Next, we need to analyze the quality of the cut produced by randomly rounding the solution of LP (1). Recall that for the (unnormalized) Laplacian $L$ and some $x\\in\\{-1,1\\}^{n}$ , the cut value is ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(x):=1/4\\cdot\\langle x,L x\\rangle=1/4\\cdot(\\langle x,D x\\rangle-\\langle x,A x\\rangle)=1/4\\cdot(W-\\langle x,A x\\rangle).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 3.6. For any $\\Delta$ -wide graph, the algorithm from $\\S3.1.1$ outputs $X^{*}\\in\\{-1,1\\}^{n}$ that satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(X^{*})\\geq f(x^{*})-(2\\varepsilon^{\\prime}+5\\eta)W\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with probability at least 0.98. ", "page_idx": 5}, {"type": "text", "text": "Proof. Recall that the cut $X^{*}$ is the best among $T\\,:=\\,O({^1}/\\eta)$ independent roundings of cut $\\hat{x}$ . Consider one of the roundings $X$ , and write: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\langle X,A X\\rangle=\\langle\\hat{x},\\hat{r}\\rangle+(\\langle\\hat{x},A\\hat{x}\\rangle-\\langle\\hat{x},\\hat{r}\\rangle)+(\\langle X,A X\\rangle-\\langle\\hat{x},A\\hat{x}\\rangle).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let us first bound the expectation of each of the terms in the RHS of (6) separately. ", "page_idx": 5}, {"type": "text", "text": "To bound the first term $\\langle{\\hat{x}},{\\hat{r}}\\rangle$ , note that given (4) (which happens with probability 0.99), the solution $x^{*}$ is feasible for the LP in (1). This means the optimal solution $\\hat{x}$ has objective function value ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\hat{r},\\hat{x}\\rangle\\leq\\langle\\hat{r},x^{*}\\rangle=\\langle r^{*},x^{*}\\rangle+\\langle\\hat{r}-r^{*},x^{*}\\rangle\\leq\\langle x^{*},A x^{*}\\rangle+\\|\\hat{r}-r^{*}\\|_{1}\\|x^{*}\\|_{\\infty}}\\\\ &{\\quad\\quad\\leq\\langle x^{*},A x^{*}\\rangle+(\\varepsilon^{\\prime}+2\\eta)W.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Next, we bound the second term $\\bigl(\\langle\\hat{x},A\\hat{x}\\rangle-\\langle\\hat{x},\\hat{r}\\rangle\\bigr)$ by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\hat{\\boldsymbol{x}}\\|_{\\infty}\\|\\boldsymbol{A}\\hat{\\boldsymbol{x}}-\\hat{\\boldsymbol{r}}\\|_{1}\\leq(\\varepsilon^{\\prime}+2\\eta)W,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "by feasibility of $\\hat{x}$ for the LP in (1). Finally, we bound the third term $(\\langle X,A X\\rangle-\\langle{\\hat{x}},A{\\hat{x}}\\rangle)$ , this time in expectation: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle X,A X\\rangle]-\\langle\\hat{x},A\\hat{x}\\rangle=0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Chaining eqs. (7) to (9) for the various parts of (6), we get ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\langle X,A X\\rangle]\\leq\\langle x^{*},A x^{*}\\rangle+(2\\varepsilon^{\\prime}+4\\eta)W.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, using that $\\langle X,A X\\rangle\\in[-W,W]$ , we get ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\langle X,A X\\rangle\\geq\\mathbb{E}[\\langle X,A X\\rangle]+\\eta W\\right]=\\operatorname*{Pr}\\left[\\langle X,A X\\rangle+W\\geq\\mathbb{E}[\\langle X,A X\\rangle]+(1+\\eta)W\\right]}\\\\ &{\\quad\\leq\\operatorname*{Pr}\\left[\\langle X,A X\\rangle+W\\geq(1+\\eta/2)\\,\\left(\\mathbb{E}[\\langle X,A X\\rangle]+W\\right]\\right)}\\\\ &{\\quad\\leq1/(1+\\eta/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If $X^{*}$ is the cut with the smallest value of $\\langle X,A X\\rangle$ among all the independent roundings: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left[\\langle X^{*},A X^{*}\\rangle\\leq\\langle x^{*},A x^{*}\\rangle+(2\\varepsilon^{\\prime}+5\\eta)W\\right]\\geq1-\\left(1/(1+\\eta/2)\\right)^{T}\\geq0.99.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Substituting into the definition of $f(\\cdot)$ completes the proof. ", "page_idx": 6}, {"type": "text", "text": "This proves Lemma 3.6, and hence also Theorem 3.4. ", "page_idx": 6}, {"type": "text", "text": "Deterministic Rounding. We observe that we can replace the repetition by a simple pipeage rounding algorithm to round the fractional solution $\\hat{x}$ to an integer solution $X^{*}$ without suffering any additional loss. Indeed, viewing $\\langle x,A x\\rangle$ as a function of some $x_{i}$ keeping the remaining $\\{x_{1},\\cdot\\cdot\\cdot,x_{n}\\}\\setminus\\{x_{i}\\}$ fixed gives us a linear function of $x_{i}$ (since the diagonals of $A$ are zero). Hence we can increase or decrease the value of $x_{i}$ to decrease $\\langle x,A x\\rangle$ until $x_{i}\\in\\{-1,1\\}$ . Iterating over the variables gives the result. However, this does not change the result qualitatively. ", "page_idx": 6}, {"type": "text", "text": "3.2 Solving MAXCUT for $\\Delta$ -narrow graphs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we consider $\\Delta$ -narrow graphs. We show: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.7. For any $\\Delta\\in\\mathbb{N},$ , there is a randomized algorithm for the MAXCUT problem with an (expected) approximation factor of $\\alpha_{G W}+\\tilde{\\Omega}(\\eta^{5}/\\Delta^{2})$ on any $\\Delta$ -narrow graph. ", "page_idx": 6}, {"type": "text", "text": "For the case of $\\Delta$ -narrow graphs, we do not use predictions; rather, we adapt an existing algorithm for the MAXCUT problem for low-degree graphs by Feige et al. [2002] and its refinement due to Hsieh and Kothari [2022]. Note that any graph with maximum degree $\\Delta$ is clearly $\\Delta$ -narrow (even when $\\eta=1$ ). ", "page_idx": 6}, {"type": "text", "text": "3.2.1 The $\\Delta$ -narrow Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We show that Theorem 3.7 holds for the Feige, Karpinski, and Langberg (FKL) MAXCUT algorithm [Feige et al., 2002]. We briefly recall this algorithm first. Consider the MAXCUT SDP with triangle inequalities: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{\\substack{v_{i}\\in S_{n}\\,\\forall i\\in[n]}}\\displaystyle\\sum_{i<j\\in[n]}A_{i,j}\\cdot\\Big(\\frac{1-\\langle v_{i},v_{j}\\rangle}{2}\\Big)}&{}\\\\ {\\mathit{s.t.}\\quad\\|a v_{i}-b v_{j}\\|_{2}^{2}+\\|b v_{j}-c v_{k}\\|_{2}^{2}\\geq\\|a v_{i}-c v_{k}\\|_{2}^{2}}&{\\forall i,j,k\\in[n],a,b,c\\in\\{-1,1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $S_{n}$ is the unit sphere of $n$ dimensions. Let $\\hat{v}$ be an optimal solution to this SDP. ", "page_idx": 6}, {"type": "text", "text": "Let $g$ be a random vector where each coordinate is sampled independently from a standard normal distribution. We use random hyperplane rounding from the MAXCUT algorithm of Goemans and Williamson [1995] to round $\\hat{v}$ to $\\hat{x}\\in\\{-1,1\\}^{n}$ as follows: if $\\langle\\hat{v}_{i},g\\rangle>0$ , then $\\hat{x}_{i}=1$ ; else, $\\hat{x}_{i}=-1$ . ", "page_idx": 7}, {"type": "text", "text": "Now, define $F\\;=\\;\\{i\\;\\in\\;[n]\\;:\\;\\langle\\hat{v}_{i},g\\rangle\\;\\in\\;[-\\delta,\\delta]\\}$ for some $\\delta\\,=\\,\\Theta(1/((\\Delta/\\eta)\\sqrt{\\log(\\Delta/\\eta)}))$ ). We partition $N_{i}:=[n]\\backslash\\{i\\}$ as follows: $B_{i}:=\\bar{\\{j\\in\\bar{N_{i}}\\backslash F:\\hat{x}_{j}=\\hat{x}_{i}\\}}$ , and $C_{i}:=\\{j\\in N_{i}\\backslash F:\\hat{x}_{j}\\neq\\hat{x}_{i}\\}$ and $D_{i}:=N_{i}\\cap F$ . We define $F^{\\prime}\\subseteq F$ as follows: $i\\in F^{\\prime}$ if $i\\in F$ and $w(B_{i})>w(C_{i})+w(D_{i})$ where $\\textstyle w(S):=\\sum_{j\\in S}A_{i j}$ . In the final output $X\\in\\{-1,1\\}^{n}$ , we flip the vertices in $F^{\\prime}$ , namely $X_{i}=-\\hat{x}_{i}$ if $i\\in F^{\\prime}$ , else $X_{i}=\\hat{x}_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "We now give an analysis for the FKL algorithm establishing Theorem 3.7. ", "page_idx": 7}, {"type": "text", "text": "The \u201clocal gain\u201d for a vertex $i\\in\\textit{F}$ is defined as $\\Delta_{i}:=(|B_{i}|-(|D_{i}|+|C_{i}|))^{+}$ , where $z^{+}=$ $\\operatorname*{max}(z,0)$ . We now state the following key lemmas: ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.8. For any vertex $i\\in[n],\\,\\mathrm{Pr}[i\\in F]=\\Omega(\\delta)$ . ", "page_idx": 7}, {"type": "text", "text": "Proof. This lemma immediately follows from [Hsieh and Kothari, 2022, Fact 3]. ", "page_idx": 7}, {"type": "text", "text": "Let $i$ be a $\\Delta$ -narrow vertex, and $w\\,\\in\\,\\mathbb{R}^{n}$ be its weight vector $w_{i}\\,=\\,A_{i j}$ for all $j\\in[n])$ so that $W_{i}=\\lVert w_{i}\\rVert_{1}$ . Let $w^{\\prime}\\in\\mathbb{R}^{n}$ be the projection of $w$ onto its top $\\Delta$ coordinates. The narrowness of $i$ implies that $\\|w^{\\prime}\\|_{1}\\geq\\eta\\|w\\|_{1}$ , which implies that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{w}\\|_{2}^{2}\\geq\\|\\boldsymbol{w}^{\\prime}\\|_{2}^{2}\\geq\\frac{\\|\\boldsymbol{w}^{\\prime}\\|_{1}^{2}}{\\Delta}\\geq\\frac{\\eta^{2}\\|\\boldsymbol{w}\\|_{1}^{2}}{\\Delta}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It turns out that the analysis of Hsieh and Kothari [2022] still holds under the above bound between $\\ell_{1}$ and $\\ell_{2}$ norms of weight vectors. So we have the following slight generalization of their Lemma 8. Lemma 3.9 (extends Lemma 8 of Hsieh and Kothari [2022]). There is a large enough constant $C$ such that for any $d\\geq3$ and Cd\u221a1log d, for any vertex i whose weight vector w satisfies $\\|w\\|_{1}^{2}\\leq d\\|w\\|_{2}^{2}$ , it holds that the expected local gain of a vertex $i$ satisfies: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta_{i}|i\\in F]=\\Omega\\left(\\frac{W_{i}}{d\\sqrt{\\log d}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof. In Hsieh and Kothari [2022], the only place where the degree bound $d$ is used is $\\|w\\|_{1}^{2}\\leq$ $d\\|w\\|_{2}^{2}$ at the end of the proof of Lemma 7. ", "page_idx": 7}, {"type": "text", "text": "Proof of Theorem 3.7. Note that the value of the cut $X$ exceeds that of $\\hat{x}$ by $\\sum_{i\\in F^{\\prime}}\\Delta_{i}$ , i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\langle X,L X\\rangle]=\\mathbb{E}[\\langle\\hat{x},L\\hat{x}\\rangle]+\\displaystyle\\sum_{i\\in[n]}\\mathbb{E}[\\Delta_{i}|i\\in F]\\cdot\\operatorname*{Pr}[i\\in F]}\\\\ &{~~~~~~~~~~~~~~~\\geq\\mathbb{E}[\\langle\\hat{x},L\\hat{x}\\rangle]+\\displaystyle\\sum_{i:\\Delta\\mathrm{-narrow}}\\mathbb{E}[\\Delta_{i}|i\\in F]\\cdot\\operatorname*{Pr}[i\\in F].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Let the approximation factor of the cut $\\hat{x}$ output by the Goemans-Williamson algorithm be denoted $\\alpha_{\\mathrm{GW}}$ and let opt be the size of the maximum cut. Then, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle\\hat{x},L\\hat{x}\\rangle]\\ge\\alpha_{\\mathrm{GW}}\\cdot\\mathrm{opt}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From Lemmas 3.8 and 3.9 with $d=\\Delta/\\eta^{2}$ , we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle X,L X\\rangle]\\geq\\alpha_{\\mathrm{GW}}\\cdot\\mathrm{opt}+\\Omega\\left(\\frac{1}{(\\Delta/\\eta^{2})\\sqrt{\\log(\\Delta/\\eta^{2})}}\\cdot\\sum_{i:\\Delta\\mathrm{-narrow}}\\frac{W_{i}}{(\\Delta/\\eta^{2})\\sqrt{\\log(\\Delta/\\eta^{2})}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since $\\sum_{i:\\Delta}$ -narrow $W_{i}\\geq\\eta W\\geq2\\eta\\cdot\\mathrm{opt}$ , we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle X,L X\\rangle]\\ge(\\alpha_{\\mathrm{GW}}+\\tilde{\\Omega}(\\eta^{5}/\\Delta^{2}))\\cdot\\mathrm{opt}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "3.3 Wrapping up: Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For $\\Delta$ -wide graphs, Theorem 3.4 returns a cut with value at least opt $-\\,(2\\eta+\\varepsilon^{\\prime})W$ with probability 0.98. Since we can always find a cut of value $\\alpha_{\\mathrm{GW}}\\cdot\\mathrm{opt}$ , and opt $\\ge W/2$ , this means the expected cut value is at least $\\left[0.98\\cdot\\left(1-6\\eta-2\\varepsilon^{\\prime}\\right)+0.02\\cdot\\alpha_{\\mathrm{GW}}\\right]\\mathrm{opt}.$ . And for $\\Delta$ -narrow graphs, Theorem 3.7 finds a cut with expected value $\\left[\\alpha_{\\mathrm{GW}}+\\tilde{\\Omega}(\\eta^{5}/\\Delta^{2})\\right]$ \u00b7 opt. Moreover, recall that $\\Delta=O(1/(\\varepsilon\\varepsilon^{\\prime})^{2})$ . Setting $\\eta,\\varepsilon^{\\prime}$ to be suitably small universal constants gives us that both the above approximation factors are at least $\\alpha_{\\mathrm{GW}}+\\tilde{\\Omega}(\\varepsilon^{4})$ , which proves Theorem 3.1. ", "page_idx": 7}, {"type": "text", "text": "4 MaxCut in the Partial Prediction Model ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now consider the partial prediction model, where each vertex pairwise-independently reveals their correct label with probability $\\varepsilon$ . Intuitively, this prediction model provides more information than the noisy prediction model since all predictions are guaranteed to be correct. Indeed, this is reflected in out first bound: we show that since an $\\varepsilon^{2}$ fraction of the edges are induced by the vertices with the given labels, it is easy to get an approximation ratio of almost $\\alpha_{\\mathrm{GW}}+\\Omega(\\varepsilon^{2})$ . (We give details in Appendix A.) ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1. Given noisy predictions with a rate of $\\varepsilon$ , there is a polynomial-time randomized algorithm that obtains an (expected) approximation factor of $\\alpha_{G W}+\\varepsilon^{2}$ for the MAXCUT problem ", "page_idx": 8}, {"type": "text", "text": "Although the $\\Omega(\\varepsilon^{2})$ advantage in this theorem is already better than $\\tilde{\\Omega}(\\varepsilon^{4})$ in Theorem 3.1, we ask if can we do even better given the more informative predictions. Ideally, we could get an $\\Omega(\\varepsilon)$ -advantage if the hyperplane rounding performs better than $\\alpha_{\\mathrm{GW}}$ for the edges with only one endpoint\u2019s label revealed. One naive way to achieve this is to hope that the rounding preserves the marginals; i.e., $\\mathbb{E}[x_{i}]=\\langle v_{0},v_{i}\\rangle$ for all $i\\in[n]$ . In that case, if we consider $(i,j)$ where if $v_{i}=\\pm v_{0}$ , the probability that $(i,j)$ is cut is exactly their contribution to the SDP $(1-\\langle v_{i},v_{j}\\rangle)$ . ", "page_idx": 8}, {"type": "text", "text": "Since the hyperplane rounding does not satisfy this property, we use the rounding scheme developed by Raghavendra and Tan [2012] for max-bisection that has an approximation ratio $\\alpha_{\\mathrm{RT}}\\approx0.858$ while preserving the marginals. The proof of this theorem is deferred to Appendix A. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2 (Partial Predictions). Given partial predictions with a rate of $\\varepsilon$ , there is a polynomialtime randomized algorithm that obtains an (expected) approximation factor of $\\alpha_{R T}+(1-\\alpha_{R T}-$ $o(1))(2\\varepsilon-\\varepsilon^{2})$ for the MAXCUT problem. ", "page_idx": 8}, {"type": "text", "text": "5 2-CSPs in the Noisy Prediction Model ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we go beyond the MAXCUT problem and consider general 2-CSPs. In particular, we extend Theorem 3.4 to this broader class of problems. Let us first define Max-2-CSPs (Constraint Satisfaction Problems). Each constraint is a predicate on two variables: e.g., AND, OR, Not-Equals, or Xor. We are given a collection of such constraints (each on two variables), and the goal is to find an binary assignment to the variables that satisfy the maximum number of constraints. (E.g., if the predicate is Not-Equals, and constraints form the edges of some graph, we get the MAXCUT problem.) ", "page_idx": 8}, {"type": "text", "text": "Formally, for a multi-index $\\alpha\\in[n]^{2}$ we denote by $\\alpha(i)$ its $i$ -th index and by $x^{\\alpha}$ the pair $(x_{\\alpha(1)},x_{\\alpha(2)})$ . For variables $x_{1},\\ldots,x_{n}$ , we then write $\\chi_{\\alpha}(x)$ for the monomial $\\textstyle\\prod_{i\\in\\alpha}x_{i}$ . Given a predicate $P$ : $\\{-1,+1\\}^{2}\\rightarrow\\{0,1\\}$ , an instance $\\mathcal{T}$ of the CSP(P) problem over variables $x_{1},\\ldots,x_{n}$ is a multi-set of triplets $(w,c,\\alpha)$ representing constraints of the form $P(c_{1}x_{\\alpha(1)},c_{2}x_{\\alpha(2)})=1$ where $\\alpha\\in[n]^{2}$ is the scope, $c\\in\\{\\pm1\\}^{2}$ is the negation pattern and $w\\geq0$ is the weight of the constraint. For brevity we often write $P(c\\circ x^{\\alpha})$ in place of $P(c_{1}x_{\\alpha(1)},c_{2}x_{\\alpha(2)})$ . We let $\\begin{array}{r}{W=\\sum_{(w,c,\\alpha)\\in\\mathcal{T}}w}\\end{array}$ . We can represent the predicate $P$ as the multilinear polynomial of degree 2 in indeterminates $x_{\\alpha(1)},x_{\\alpha(2)}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\nP(c\\circ x^{\\alpha})=\\sum_{\\alpha^{\\prime}\\subseteq\\alpha}c^{\\alpha^{\\prime}}\\cdot\\hat{p}(\\alpha^{\\prime})\\cdot\\chi_{\\alpha^{\\prime}}(x)\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\hat{p}(\\alpha^{\\prime})$ is the coefficient in $P$ of the monomial $\\chi_{\\alpha^{\\prime}}(x)$ . Notice that this formulation does not rule out predicates with the same multi-index but different negation patterns or multi-indices in which an index appears multiple times. Given a predicate $P$ , an instance $\\mathcal{T}$ of CSP(P) with $m$ constraints and $x\\in\\{\\pm1\\}^{n}$ , we define ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{val}_{\\mathcal{Z}}(x):=\\frac{1}{W}\\sum_{(w,c,\\alpha)\\in\\mathcal{Z}}w\\cdot P(c\\circ x^{\\alpha})\\qquad\\mathrm{and}\\qquad\\mathrm{opt}_{\\mathcal{Z}}:=\\operatorname*{max}_{x\\in\\{\\pm1\\}^{n}}\\mathrm{val}_{\\mathcal{Z}}(x)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For instance, MAXCUT on a graph $G=([n],E)$ can be captured in this framework where $P(x,y)=$ $(1-x y)/2$ , each edge $(i,j)\\bar{\\in}\\,[n]^{2}$ yields a triplet $(w,c,\\alpha)$ where $w=1$ , $c=(1,1)$ and $\\alpha=(i,j)$ . In the noisy prediction model for CSPs, for an instance $\\mathcal{T}$ of CSP(P), we assume there is some fixed assigment $x^{*}$ with value $\\mathrm{val}_{\\mathcal{T}}(x^{*})=\\mathrm{opt}_{\\mathcal{T}}$ . The algorithm has access to a prediction vector ", "page_idx": 8}, {"type": "text", "text": "$Y\\,\\in\\,\\{\\pm1\\}^{n}$ such that predictions $y_{i}$ \u2019s are 2-wise independently correct with probability $\\textstyle{\\frac{1+\\varepsilon}{2}}$ for unknown bias $\\varepsilon$ . We let $\\textstyle Z\\,=\\,{\\frac{Y}{2\\varepsilon}}$ . With a slight abuse of notation we also write $P(c\\circ Z^{\\alpha})$ even though $Z$ is a rescaled boolean vector. ", "page_idx": 9}, {"type": "text", "text": "For a literal $i\\;\\in\\;[n]$ and an instance $\\mathcal{T}$ of CSP(P) we let $S_{i}\\,:=\\,\\{(w,c,\\alpha)\\,\\in\\,{\\mathcal{T}}\\,|\\,\\alpha(1)\\,=\\,i\\}$ . As in Section 3.1.1, we can define $\\Delta$ -wide literals and instances. For an instance $\\mathcal{T}$ , we partition the constraints in $S_{i}$ into two sets: the $\\Delta$ -prefix for $i$ comprises the $\\Delta$ heaviest constraints in $S_{i}$ (breaking ties arbitrarily), while the remaining constraints make up the $\\Delta$ -suffix for $i$ , which we denote by ${\\tilde{S}}_{i}$ . We fix a parameter \u03b7 \u2208(0, 1/2). We let Wi = (w,c,\u03b1)\u2208S wi . ", "page_idx": 9}, {"type": "text", "text": "Definition 5.1 ( $\\Delta$ -Narrow/Wide). A literal $i$ is $\\Delta$ -wide if the total weight of its in its $\\Delta$ -prefix is at most $\\eta W_{i}$ , and so the weight of edges in its $\\Delta$ -suffix is at least $(1-\\eta)W_{i}$ . Otherwise, the literal $i$ is $\\Delta$ -narrow. An instance $\\mathcal{T}$ of $C S P(P)$ is $\\Delta$ -wide $i f\\sum_{i\\in[n]}\\;W_{i}\\geq(1-\\eta)W.$ . $\\Delta$ -wide ", "page_idx": 9}, {"type": "text", "text": "We are now ready to state the main theorem of the section. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.2. Let $P\\;:\\;\\{\\pm1\\}^{2}\\;\\rightarrow\\;\\{0,1\\}$ be a predicate. Let $\\varepsilon^{\\prime}\\;\\in\\;(0,1)$ , $\\eta~\\in~(0,1/2)$ and $\\Delta\\geq O(1/(\\varepsilon^{\\prime}\\cdot\\varepsilon)^{2})$ . There exists a polynomial-time randomized algorithm that, given a $\\Delta$ -wide $\\mathcal{T}$ in $C S P(P)$ and noisy predictions with bias $\\varepsilon$ , returns a vector $\\hat{x}\\in\\{\\pm1\\}^{n}$ satisfying ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathrm{val}_{\\mathcal{T}}(x)\\ge\\mathrm{opt}_{\\mathcal{T}}-5\\eta-O\\left(\\varepsilon^{\\prime}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "with probability at least 0.98 . ", "page_idx": 9}, {"type": "text", "text": "The proof of Theorem 5.2 follows closely that of Theorem 3.4, and is deferred to Appendix B.2. ", "page_idx": 9}, {"type": "text", "text": "6 Closing Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work suggests many directions for future research. One immediate question is to quantitatively improve the exponent of $\\varepsilon$ for noisy predictions, and the constants. Here are some broader questions: ", "page_idx": 9}, {"type": "text", "text": "1. We assume that our noisy predictions are correct with probability equal to $1/2+\\varepsilon$ ; we can easily extend to the case where each node has a prediction that is correct with some probability $1/2+\\varepsilon_{i}$ , and each $\\varepsilon_{i}\\,\\in\\,\\Theta(\\varepsilon)$ . Can we extend to the case when we are only guaranteed $\\varepsilon_{i}\\geq\\varepsilon$ for every $i$ ?   \n2. For which other problems can we improve the performance of the state-of-the-art algorithms using noisy predictions? As we showed, the ideas used for the $\\Delta$ -wide case extend to more general maximization problems on 2-CSPs with \u201chigh degree\u201d, but can we extend the results for the \u201clow-degree\u201d case where each variable does not have a high-enough degree to infer a clear signal? Can we extend these to minimization versions of 2-CSPs?   \n3. What general lower bounds can we give for our prediction models? We feel that $\\alpha_{G W}\\!+\\!O(\\varepsilon)$ is a natural barrier. One \u201cevidence\u201d we have is the following integrality gap for the SDP used in the partial information model; starting from a gap instance and an SDP solution exhibiting opt $\\leq\\alpha_{G W}$ \u00b7 sdp for the standard SDP (without incorporating revealed information), given labels for an $\\varepsilon n$ vertices, our new SDP simply fixes the positions of the corresponding $\\varepsilon n$ vectors, but doing that from the given SDP solution decreases the SDP value by at most $O(\\varepsilon)$ in expectation, which still yields opt $\\leq(\\alpha_{G W}+O(\\varepsilon))\\mathrm{sdp}.$ . (Note that you can replace the SDP gap with any hypothetical gap instance for stronger relaxations.) Given that the partial predictions model is easier than the noisy predictions model and our entire algorithm for the partial model is based on this SDP, this might be considered as a convincing lower bound, but it would be nicer to have more general lower bounds against all polynomial-time algorithms.   \n4. Our models only assume pairwise independence between vertices: can we extend our results to other ways of modeling correlations between the predictions? In addition to stochastic predictions, can we incorporate geometric predictions (e.g., in random graph models where the probability of edges depend on the proximity of the nodes)? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Ola Svensson for enjoyable discussions. AG was supported in part by NSF awards CCF-2006953 and CCF-2224718, and by Google, Inc. EL was supported in part by NSF award CCF-2236669 and by Google, Inc. DP was supported in part by NSF awards CCF-1955703 and CCF-2329230, and by Google, Inc. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Online index for algorithms with predictions, 2023. https://algorithms-with-predictions. github.io/. ", "page_idx": 10}, {"type": "text", "text": "Nir Ailon, Anup Bhattacharya, Ragesh Jaiswal, and Amit Kumar. Approximate clustering with same-cluster queries. In Anna R. Karlin, editor, 9th Innovations in Theoretical Computer Science Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, volume 94 of LIPIcs, pages 40:1\u201340:21. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2018. ", "page_idx": 10}, {"type": "text", "text": "Matteo Almanza, Flavio Chierichetti, Silvio Lattanzi, Alessandro Panconesi, and Giuseppe Re. Online facility location with multiple advice. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 4661\u20134673, 2021. ", "page_idx": 10}, {"type": "text", "text": "Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ML predictions for online algorithms. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 303\u2013313. PMLR, 2020. ", "page_idx": 10}, {"type": "text", "text": "Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online algorithms with multiple predictions. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 582\u2013598. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "Antonios Antoniadis, Christian Coester, Marek Eli\u00e1s, Adam Polak, and Bertrand Simon. Online metric algorithms with untrusted predictions. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 345\u2013355. PMLR, 2020a. ", "page_idx": 10}, {"type": "text", "text": "Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching problems with machine learned advice. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020b. ", "page_idx": 10}, {"type": "text", "text": "Antonios Antoniadis, Christian Coester, Marek Eli\u00e1s, Adam Polak, and Bertrand Simon. Mixing predictions for online metric algorithms. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 969\u2013983. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "Sanjeev Arora, David R. Karger, and Marek Karpinski. Polynomial time approximation schemes for dense instances of np-hard problems. J. Comput. Syst. Sci., 58(1):193\u2013210, 1999. ", "page_idx": 10}, {"type": "text", "text": "Hassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries. Advances in neural information processing systems, 29, 2016. ", "page_idx": 10}, {"type": "text", "text": "Yossi Azar, Stefano Leonardi, and Noam Touitou. Flow time scheduling with uncertain processing time. In Samir Khuller and Virginia Vassilevska Williams, editors, STOC \u201921: 53rd Annual ACM SIGACT Symposium on Theory of Computing, Virtual Event, Italy, June 21-25, 2021, pages 1070\u20131080. ACM, 2021. ", "page_idx": 10}, {"type": "text", "text": "Yossi Azar, Debmalya Panigrahi, and Noam Touitou. Online graph algorithms with predictions. In Joseph (Seffi) Naor and Niv Buchbinder, editors, Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022, pages 35\u201366. SIAM, 2022.   \n\u00c9tienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning augmented energy minimization via speed scaling. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020a.   \n\u00c9tienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020b.   \nEvripidis Bampis, Bruno Escoffier, and Michalis Xefteris. Parsimonious learning-augmented approximations for dense instances of ${\\mathcal{N P}}$ -hard problems, 2024.   \nNikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, and Erik Vee. Learning-augmented weighted paging. In Joseph (Seffi) Naor and Niv Buchbinder, editors, Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022, pages 67\u201389. SIAM, 2022.   \nGiulia Bernardini, Alexander Lindermayr, Alberto Marchetti-Spaccamela, Nicole Megow, Leen Stougie, and Michelle Sweering. A universal error measure for input predictions applied to online graph problems. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.   \nIlan Reuven Cohen and Debmalya Panigrahi. A general framework for learning-augmented online allocation. In Kousha Etessami, Uriel Feige, and Gabriele Puppis, editors, 50th International Colloquium on Automata, Languages, and Programming, ICALP 2023, July 10-14, 2023, Paderborn, Germany, volume 261 of LIPIcs, pages 43:1\u201343:21. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2023.   \nAlberto Del Pia, Mingchen Ma, and Christos Tzamos. Clustering with queries under semi-random noise. In Conference on Learning Theory, pages 5278\u20135313. PMLR, 2022.   \nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster matchings via learned duals. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 10393\u201310406, 2021.   \nPaul D\u00fctting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with advice. In P\u00e9ter Bir\u00f3, Shuchi Chawla, and Federico Echenique, editors, EC \u201921: The 22nd ACM Conference on Economics and Computation, Budapest, Hungary, July 18-23, 2021, pages 409\u2013429. ACM, 2021.   \nJon C. Ergun, Zhili Feng, Sandeep Silwal, David P. Woodruff, and Samson Zhou. Learning-augmented $\\mathbb{S}\\mathbf{k}\\mathbb{S}$ -means clustering. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \nUriel Feige, Marek Karpinski, and Michael Langberg. Improved approximation of max-cut on graphs of bounded degree. J. Algorithms, 43(2):201\u2013219, 2002.   \nDimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, and Nikolas Patris. Learning augmented online facility location. CoRR, abs/2107.08277, 2021. URL https://arxiv.org/abs/2107. 08277.   \nBuddhima Gamlath, Silvio Lattanzi, Ashkan Norouzi-Fard, and Ola Svensson. Approximate cluster recovery from noisy labels. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of Machine Learning Research, pages 1463\u20131509. PMLR, 2022.   \nSuprovat Ghoshal, Konstantin Makarychev, and Yury Makarychev. Constraint satisfaction problems with advice. CoRR, abs/2403.02212, 2024. doi: 10.48550/ARXIV.2403.02212. URL https: //doi.org/10.48550/arXiv.2403.02212.   \nMichel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. J. ACM, 42(6):1115\u20131145, 1995.   \nSreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2319\u20132327. PMLR, 2019.   \nAnupam Gupta, Debmalya Panigrahi, Bernardo Subercaseaux, and Kevin Sun. Augmenting online algorithms with $\\varepsilon$ -accurate predictions. In NeurIPS, Dec 2022.   \nJun-Ting Hsieh and Pravesh K. Kothari. Approximating max-cut on bounded degree graphs: Tighter analysis of the FKL algorithm. CoRR, abs/2206.09204, 2022.   \nShaofeng H.-C. Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang. Online facility location with predictions. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a.   \nZhihao Jiang, Debmalya Panigrahi, and Kevin Sun. Online algorithms for weighted paging with predictions. ACM Trans. Algorithms, 18(4):39:1\u201339:27, 2022b.   \nKasper Green Larsen, Michael Mitzenmacher, and Charalampos E. Tsourakakis. Clustering with a faulty oracle. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen, editors, WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, pages 2831\u20132834. ACM / IW3C2, 2020.   \nAlexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schl\u00f6ter. Minimalistic predictions to schedule jobs with online precedence constraints. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 18563\u201318583. PMLR, 2023.   \nSilvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling via learned weights. In Shuchi Chawla, editor, Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020, pages 1859\u20131877. SIAM, 2020.   \nThomas Lavastida, Benjamin Moseley, R. Ravi, and Chenyang Xu. Learnable and instance-robust predictions for online matching, flows and load balancing. In Petra Mutzel, Rasmus Pagh, and Grzegorz Herman, editors, 29th Annual European Symposium on Algorithms, ESA 2021, September 6-8, 2021, Lisbon, Portugal (Virtual Conference), volume 204 of LIPIcs, pages 59:1\u201359:17. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2021.   \nAlexander Lindermayr, Nicole Megow, and Martin Rapp. Speed-oblivious online scheduling: Knowing (precise) speeds is not necessary. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 21312\u201321334. PMLR, 2023.   \nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. J. ACM, 68(4):24:1\u201324:25, 2021.   \nArya Mazumdar and Barna Saha. Clustering with noisy queries. Advances in Neural Information Processing Systems, 30, 2017.   \nMichael Mitzenmacher. Scheduling with predictions and the price of misprediction. In Thomas Vidick, editor, 11th Innovations in Theoretical Computer Science Conference, ITCS 2020, January 12-14, 2020, Seattle, Washington, USA, volume 151 of LIPIcs, pages 14:1\u201314:18. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2020.   \nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Roughgarden, editor, Beyond the Worst-Case Analysis of Algorithms, pages 646\u2013662. Cambridge University Press, 2020.   \nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Commun. ACM, 65(7): 33\u201335, 2022.   \nRyan O\u2019Donnell. Analysis of boolean functions. Cambridge University Press, 2014.   \nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 9684\u20139693, 2018.   \nPrasad Raghavendra and Ning Tan. Approximating CSPs with global cardinality constraints using sdp hierarchies. In Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms, pages 373\u2013387. SIAM, 2012.   \nTim Roughgarden, editor. Beyond the Worst-Case Analysis of Algorithms. Cambridge University Press, 2020.   \nAlexander Wei. Better and simpler learning-augmented online caching. In Jaroslaw Byrka and Raghu Meka, editors, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM 2020, August 17-19, 2020, Virtual Conference, volume 176 of LIPIcs, pages 60:1\u201360:17. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2020.   \nAlexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learning-augmented online algorithms. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Missing proofs for MAXCUT in the partial prediction model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 4.1. Given a graph $G\\,=\\,(V,E)$ with the optimal cut $(A^{*},B^{*})$ that cuts $E^{*}=$ $E\\cap E(A^{*},B^{*})$ , let $S$ be the set of vertices whose label is given, and let $A=A^{*}\\cup S$ , $B=B^{*}\\cup S$ Consider the following MAXCUT SDP that fixes the vertices with the revealed labels. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{v_{i}\\in S_{n}}\\operatorname*{max}_{\\forall i\\in[n]}\\sum_{i,j\\in[n]}\\frac{A_{i,j}(1-\\langle v_{i},v_{j}\\rangle)}{2}\\quad s.t.\\ v_{i}=v_{0}\\,\\forall i\\in A\\ \\mathrm{and}\\ v_{i}=-v_{0}\\,\\forall i\\in B.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that this is still a valid relaxation so the optimal SDP value sdp is at least opt. For each edge $e\\,\\in\\,E^{*}$ , $e\\in E(A,B)$ with probability $\\varepsilon^{2}$ ; in other words, both of its endpoints will reveal their labels. Let $\\tau$ denotes the total weight of such edges, so that $\\mathbb{E}[\\tau]=\\varepsilon^{2}\\mathrm{opt}$ . Note that sdp $\\geq$ opt for every partial prediction. ", "page_idx": 14}, {"type": "text", "text": "Perform the standard hyperplane rounding. For each $e\\in E^{*}\\cap E(A,B)$ , the rounding will always cut $e$ . For all other edges, we have an approximation ratio of $\\alpha_{\\mathrm{GW}}$ . Therefore, the expected weight of the cut edges is at least ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\tau W+\\alpha_{\\sf G W}(\\mathrm{sdp}-\\tau W)]\\geq\\varepsilon^{2}\\mathrm{opt}+\\alpha_{\\sf G W}(1-\\varepsilon^{2})\\mathrm{opt}=(\\alpha_{\\sf G W}+(1-\\alpha_{\\sf G W})\\varepsilon^{2})\\cdot\\mathrm{opt}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 4.2. Given a graph $G\\,=\\,(V,E)$ with the optimal cut $(A^{*},B^{*})$ that cuts $E^{*}=$ $E\\cap E(A^{*},B^{*})$ , let $S$ be the set of vertices whose label is given, and let $A=A^{*}\\cup S$ , $B=B^{*}\\cup S$ Let $E^{\\prime}$ be the set of the edges that are incident on $A\\,{\\underline{{o r}}}\\;B$ . Each edge cut in the optimal solution will be in $E^{\\prime}$ with probability $2\\varepsilon-\\varepsilon^{2}$ . Let $\\tau$ be the total weight of the edges in $E^{*}\\cap E^{\\prime}$ so that $\\mathbb{E}[\\tau]=(2\\varepsilon-\\varepsilon^{2})\\mathrm{opt}$ . Guess the value of $\\tau$ (up to a $o(1)$ multiplicative error that we will ignore in the proof), and consider the following MAXCUT SDP that fixes the vertices with the revealed labels and requires a large SDP contribution from $E^{\\prime}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall i\\in A}\\\\ {\\forall i\\in B}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{v_{i}\\in S_{n}\\ \\forall i\\in[n]}{\\operatorname*{max}}\\ \\sum_{i,j\\in[n]}\\frac{A_{i,j}(1-\\langle v_{i},v_{j}\\rangle)}{2}}&{}&\\\\ {\\mathrm{~s.t.~}v_{i}=v_{0}}&{}&\\\\ {v_{i}=-v_{0}}&{}&\\\\ {\\displaystyle\\sum_{(i,j)\\in E^{\\prime}}\\frac{A_{i,j}(1-\\langle v_{i},v_{j}\\rangle)}{2}\\geq\\tau.}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given the correctly guessed value of $\\tau$ , the optimal solution is still feasible for the above SDP, so sdp $\\geq$ opt. We use Raghavendra and Tan [2012]\u2019s rounding algorithm, which is briefly recalled below. ", "page_idx": 14}, {"type": "text", "text": "\u2022 For each $i\\in[n]$ , define $\\mu_{i}\\in[-1,+1]$ and $w_{i}\\in\\mathbb{R}^{n}$ such that $v_{i}=\\mu_{i}v_{0}+w_{i}$ and $w_{i}\\perp v_{0}$ . Let $\\overline{{w_{i}}}=w_{i}/\\lVert\\dot{w}_{i}\\rVert$ . $w_{i}=0$ if and only if $v_{i}=\\pm v_{0}$ . Then define $\\overline{{w_{i}}}=0.$ .)   \n\u2022 Pick a random Gaussian vector $g$ orthogonal to $v_{0}$ . Let $\\xi_{i}:=\\left<g,\\overline{{w_{i}}}\\right>$ . Note that each $\\xi_{i}$ is a standard Gaussian.   \n\u2022 Let the threshold $t_{i}:=\\Phi^{-1}(\\mu_{i}/2+1/2)$ where $\\Phi$ is the CDF of a standard Gaussian.   \n\u2022 If $\\xi_{i}\\leq t_{i}$ , set $x_{i}=1$ and otherwise set $x_{i}=-1$ . ", "page_idx": 14}, {"type": "text", "text": "Raghavendra and Tan showed that this rounding achieves an $\\Delta_{\\mathrm{RT}}\\approx0.858)$ )-approximation for every edge. ", "page_idx": 14}, {"type": "text", "text": "Consider an edge $(i,j)\\,\\in\\,E^{\\prime}$ and without loss of generality, assume $i\\in\\mathit{B}$ , which implies that $v_{i}=-v_{0}$ . The contribution of this edge to the SDP objective is $\\mu_{j}/2+1/2$ . Note that $\\operatorname*{Pr}[x_{j}=1]$ is exactly $\\mu_{j}/2+1/2$ and $\\mathbb{E}[x_{j}]=(\\mu_{j}/2+1/2)-(1/2-\\mu_{j}/2)\\stackrel{,}{=}\\mu_{j}$ . So, we get a 1-approximation from this edge. Since other edges still have an $\\alpha_{\\mathrm{{RT}}}$ -approximation, the total expected weight of the edges cut is at least ", "page_idx": 14}, {"type": "text", "text": "$\\mathfrak{L}[\\tau+\\alpha_{\\mathrm{RT}}(\\mathrm{sdp}-\\tau)]\\ge(2\\varepsilon-\\varepsilon^{2})\\mathrm{opt}+\\alpha_{\\mathrm{RT}}(1-(2\\varepsilon-\\varepsilon^{2}))\\mathrm{opt}=\\alpha_{\\mathrm{RT}}\\cdot\\mathrm{opt}+(1-\\alpha_{\\mathrm{RT}})(2\\varepsilon-\\varepsilon^{2})\\mathrm{opt}.$Hence the proof of Theorem 4.2. \u53e3", "page_idx": 14}, {"type": "text", "text": "B Missing details for 2-CSPs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The goal of this section is to establish Theorem 5.2 for dense instances of 2-CSPs. ", "page_idx": 15}, {"type": "text", "text": "B.1 The 2-CSP Algorithm for Dense Instances ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "First observe that we may assume without loss of generality that each $(w,c,\\alpha)$ appears exactly twice in $\\mathcal{T}$ . This is convenient so that for all $x\\in\\{\\pm1\\}^{\\bar{n}}$ , $\\begin{array}{r}{\\mathrm{val}_{\\mathbb{Z}}(x)=\\sum_{i\\in[n]}\\sum_{(w,c,\\alpha)\\in S_{i}}w\\cdot P(c\\circ x^{\\alpha})}\\end{array}$ . With a slight abuse of notation, for all $(w,c,\\alpha)\\in S_{i}$ , we let ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(c\\circ(x_{i}\\cdot Z^{\\alpha\\setminus i})):=\\sum_{\\stackrel{\\alpha^{\\prime}\\subseteq\\alpha\\;s.\\mathrm{t}.}{\\alpha^{\\prime}(1)=i}}\\hat{p}_{\\alpha^{\\prime}}c^{\\alpha^{\\prime}}x_{i}\\cdot\\chi_{\\alpha^{\\prime}\\setminus\\alpha^{\\prime}(1)}(Z)+\\sum_{\\stackrel{\\alpha^{\\prime}\\subseteq\\alpha\\;s.\\mathrm{t}.}{\\alpha^{\\prime}(1)\\neq i}}\\hat{p}_{\\alpha^{\\prime}}c^{\\alpha^{\\prime}}\\chi_{\\alpha^{\\prime}}(Z)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(c\\circ x^{\\alpha\\setminus i}):=\\sum_{\\stackrel{\\alpha^{\\prime}\\subseteq\\alpha\\;s.t.}{\\alpha^{\\prime}(1)=i}}\\hat{p}_{\\alpha^{\\prime}}c^{\\alpha^{\\prime}}\\chi_{\\alpha^{\\prime}\\setminus\\alpha^{\\prime}(1)}(x)+\\sum_{\\stackrel{\\alpha^{\\prime}\\subseteq\\alpha\\;s.t.}{\\alpha^{\\prime}(1)\\neq i}}\\hat{p}_{\\alpha^{\\prime}}c^{\\alpha^{\\prime}}\\chi_{\\alpha^{\\prime}}(x)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We further define $\\tilde{S}_{i}\\subseteq S_{i}$ to be subset of constraints in $S_{i}$ that are not part of the $\\Delta$ -prefix of $i$ .   \nWe can now state the algorithm behind Theorem 5.2, which amounts to the following two steps. ", "page_idx": 15}, {"type": "text", "text": "1. Solve the linear program ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in[-1,+1]^{n}}\\;\\sum_{i\\in[n]\\atop\\Delta\\cdot\\mathrm{wide}}\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w P(c\\circ(x_{i}\\cdot Z^{\\alpha\\backslash i}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "subject to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in[n]}\\ \\left|\\sum_{(w,c,\\alpha)\\in S_{i}}w P(c\\circ x^{\\alpha\\backslash i})\\right|+\\sum_{i\\in[n]}\\left|\\sum_{(w,c,\\alpha)\\in S_{i}\\backslash\\tilde{S}_{i}}w P(c\\circ x^{\\alpha\\backslash i})\\right|}\\\\ &{+\\displaystyle\\sum_{i\\in[n]}\\ \\left|\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w\\left(P(c\\circ x^{\\alpha\\backslash i})-P(c\\circ Z^{\\alpha\\backslash i})\\right)\\right|\\leq C(\\varepsilon^{\\prime}+2\\eta)W}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for some large enough absolute constant $C>0$ . Let $\\hat{x}\\in[-1,+1]^{n}$ be the found optimal solution. ", "page_idx": 15}, {"type": "text", "text": "2. Repeat ${\\cal O}(1/\\eta)$ times independently and output the best assignment $X^{*}$ : independently for each $i\\in[n]$ set $X_{i}=1$ with probability $(1\\bar{+}\\,\\hat{x}_{i})/2$ and $X_{i}=-1$ otherwise. ", "page_idx": 15}, {"type": "text", "text": "The LP above generalize the one in eq. (1), which comes as a special case where $\\hat{p}_{\\alpha^{\\prime}}=0$ for all $\\alpha^{\\prime}\\subset\\alpha\\in[n]^{2}$ . Indeed, since predicates contain only two literals, the program is linear. ", "page_idx": 15}, {"type": "text", "text": "B.2 The Analysis of the 2-CSP Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We obtain here the proof of Theorem 5.2. ", "page_idx": 15}, {"type": "text", "text": "Feasibility of the best assignment As in Lemma 3.5, we first prove that, in expectation over the prediction $Y,x^{*}$ is a feasible solution to the program. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. Consider the settings of Theorem 5.2. Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\displaystyle\\sum_{i\\in[n]}\\left\\vert\\sum_{(w,c,\\alpha)\\in S_{i}}w P(c\\circ x^{*\\alpha\\backslash i})\\right\\vert+\\displaystyle\\sum_{i\\in[n]}\\left\\vert\\sum_{(w,c,\\alpha)\\in S_{i}\\setminus\\tilde{S}_{i}}w P(c\\circ x^{*\\alpha\\backslash i})\\right\\vert}\\\\ &{+\\displaystyle\\sum_{i\\in[n]}\\left\\vert\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w\\left(P(c\\circ x^{*\\alpha\\backslash i})-P(c\\circ Z^{\\alpha\\backslash i})\\right)\\right\\vert\\leq W(2\\eta+O(1/\\varepsilon\\sqrt{\\Delta}))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. First, by definition of $\\Delta$ -wide instance, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]}\\left|\\sum_{(w,c,\\alpha)\\in S_{i}}w P(c\\circ x^{*\\alpha\\setminus i})\\right|\\leq\\eta W\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Second, by definition for any $\\Delta$ -wide vertex $i$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\sum_{(w,c,\\alpha)\\in S_{i}\\setminus\\tilde{S}_{i}}w P(c\\circ x^{*\\alpha\\setminus i})\\right|\\leq\\eta W_{i}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence it remains to show ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{i\\in[n]\\atop\\Delta\\to\\mathrm{wide}}\\left|\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w\\left(P(c\\circ x^{*\\alpha\\backslash i})-P(c\\circ Z^{\\alpha\\backslash i})\\right)\\right|\\leq O(W/(\\varepsilon\\sqrt{\\Delta})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, recall that $\\mathbb{E}[Y_{i}]\\,=\\,2\\varepsilon x_{i}^{*}$ and thus $\\mathbb{E}[Z]\\,=\\,x^{*}$ . So for any $\\left(c,\\alpha\\right)\\,\\in\\,{\\mathcal{Z}}\\,,\\,\\mathbb{E}[P(c\\circ Z^{\\alpha})]\\,=$ $\\mathbb{E}[P(c\\circ x^{*\\alpha})]$ by pair-wise independence of the predictions. Thus it suffices to study, for each $\\Delta$ -wide $i$ , var $\\begin{array}{r}{\\left(\\sum_{(w,c,\\alpha)\\in S_{i}}w P(c\\circ Z^{\\alpha\\backslash i})\\right)}\\end{array}$ . To this end, notice that for any $\\alpha,\\alpha^{\\prime}\\in S_{i}$ with $\\alpha\\cap\\alpha^{\\prime}=\\{i\\}$ it holds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[P(c\\circ Y^{\\alpha\\backslash i})P(c\\circ Y^{\\alpha^{\\prime}\\backslash i})\\right]=\\mathbb{E}\\left[P(c\\circ Y^{\\alpha\\backslash i})\\right]\\mathbb{E}\\left[P(c\\circ Y^{\\alpha^{\\prime}\\backslash i})\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, since $|\\alpha|=2$ , there are at most 4 distinct negation patterns. Therefore, by the AM-GM inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{var}\\left(\\displaystyle\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w P(c\\circ Z^{\\alpha\\backslash i})\\right)\\leq\\displaystyle\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}O(w^{2})\\,\\mathrm{var}\\left(P(c\\circ Z^{\\alpha\\backslash i})\\right)}\\\\ {\\leq\\displaystyle\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}O\\left(\\frac{w^{2}}{\\varepsilon^{2}}\\right)\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used the fact that entries of $Z$ are bounded by $1/\\varepsilon$ and the coefficients of a boolean predicate are bounded by 1 (by Parseval\u2019s Theorem, see O\u2019Donnell [2014]). By construction of ${\\tilde{S}}_{i}$ , each $(w,c,\\alpha)\\in\\tilde{S}_{i}$ must satisfy $w\\leq W_{i}/\\Delta$ . Using Holder\u2019s inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{var}\\left(\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w P(c\\circ Z^{\\alpha\\backslash i})\\right)\\leq O\\left(\\frac{W_{i}^{2}}{\\Delta\\cdot\\varepsilon^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can use this bound on the variance in combination with Chebishev\u2019s inequality to obtain, for $\\lambda>0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{(w,c,\\alpha)\\in S_{i}}w\\left(P(c\\circ x^{*\\alpha\\setminus i})-P(c\\circ Z^{\\alpha\\setminus i})\\right)\\right|\\geq\\lambda\\right)\\leq O\\left(\\frac{W_{i}^{2}}{\\varepsilon^{2}\\cdot\\Delta\\cdot\\lambda^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\lambda:=O(W_{i}/(\\varepsilon\\sqrt{\\Delta}))$ . A peeling argument now completes the proof: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left|\\displaystyle\\sum_{(w,c,\\alpha)\\in S_{i}}w\\left(P(c\\circ x^{*\\alpha\\backslash i})-P(c\\circ Z^{\\alpha\\backslash i})\\right)\\right|\\right]}\\\\ &{\\quad\\leq\\lambda+\\displaystyle\\sum_{t\\geq0}2^{t+1}\\lambda\\cdot\\mathbb{P}\\left(\\left|\\displaystyle\\sum_{(w,c,\\alpha)\\in S_{i}}w\\left(P(c\\circ x^{*\\alpha\\backslash i})-P(c\\circ Z^{\\alpha\\backslash i})\\right)\\right|\\geq2^{t}\\lambda\\right)\\leq O(\\lambda)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Analysis of the algorithm We can use Lemma B.1 to obtain our main theorem for CSPs. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 5.2. We follow closely the proof of Lemma 3.6. Consider one of the assignments $X\\in\\{\\pm1\\}^{n}$ found in the second step of the algorithm. Recall $\\hat{x}\\in[-1,+1]^{n}$ denotes the optimal fractional solution found by the algorithm. We may rewrite for each $\\Delta$ -wide $i$ and $(w,c,\\alpha)\\in\\tilde{S}_{i}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]}\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w P(c\\circ X^{\\alpha})=\\sum_{i\\in[n]\\atop\\Delta\\to\\mathrm{wide}}\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w\\left[P(c\\circ(\\hat{x}_{i}\\cdot Z^{\\alpha\\backslash i}))\\right]}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\qquad\\qquad\\qquad\\qquad+\\ P(c\\circ X^{\\alpha})-P(c\\circ\\hat{x}^{\\alpha})}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad+\\ P(c\\circ\\hat{x}^{\\alpha})-P(c\\circ(\\hat{x}_{i}\\cdot Z^{\\alpha\\backslash i}))\\right]\\ .}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We bound each term in Equation (11) separately. First, notice that by Markov\u2019s inequality and Lemma B.1, with probability 0.99 , $x^{*}$ is a feasible solution to the LP. Conditioning on this event $\\mathcal{E}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i\\in[n]}\\displaystyle\\sum_{(w,c,\\alpha)\\in\\bar{S}_{i}}w P(c\\circ(\\hat{x}_{i}\\cdot Z^{\\alpha\\backslash i}))\\geq\\displaystyle\\sum_{i\\in[n]}\\displaystyle\\sum_{(w,c,\\alpha)\\in\\bar{S}_{i}}w P(c\\circ(x_{i}^{*}\\cdot Z^{\\alpha\\backslash i}))}&{}\\\\ {\\Delta\\widehat{\\cdots\\kappa\\mathfrak{i d e}}^{(w,c,\\alpha)\\in\\bar{S}_{i}}}&{}\\\\ {=\\displaystyle\\sum_{i\\in[n]}\\displaystyle\\sum_{(w,c,\\alpha)\\in\\bar{S}_{i}}w P(c\\circ x^{*\\alpha})}&{}\\\\ {+\\displaystyle\\sum_{i\\in[n]}\\displaystyle\\sum_{(w,c,\\alpha)\\in\\bar{S}_{i}}w\\left(P(c\\circ(x_{i}^{*}\\cdot Z^{\\alpha\\backslash i}))-P(c\\circ x^{*\\alpha})\\right)}&{}\\\\ {\\Delta\\widehat{\\cdots\\kappa\\mathfrak{i d e}}^{(w,c,\\alpha)\\in\\bar{S}_{i}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Holder\u2019s inequality and the fact that $x^{*}$ is feasible, for $\\Delta$ -wide $i$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in[n]}\\ \\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w\\left(P(c\\circ(x_{i}^{*}\\cdot Z^{\\alpha\\setminus i}))-P(c\\circ x^{*\\alpha})\\right)}\\\\ &{\\Delta\\mathrm{-wide}}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\sum_{i\\in[n]}\\,\\left\\vert\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w\\left(P(c\\circ Z^{\\alpha\\setminus i})-P(c\\circ x^{*\\alpha\\setminus i})\\right)\\right\\vert\\le(O(\\varepsilon^{\\prime})+2\\eta)W\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since by construction $\\hat{x}$ is feasible, another application of Holder\u2019s inequality also yields the following bound on the third term, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]}\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w\\left(P(c\\circ(\\hat{x}_{i}\\cdot Z^{\\alpha\\setminus i}))-P(c\\circ\\hat{x}^{\\alpha})\\right)\\leq(O(\\varepsilon^{\\prime})+2\\eta)W\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the second term in Equation (11), by construction of $X$ we have $\\mathbb{E}\\left[P(c\\circ X^{\\alpha})\\,|\\,\\mathcal{E}\\right]=P(c\\circ\\hat{x}^{\\alpha})$ Combining the three bounds, we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{opt}_{\\mathbb{Z}}\\geq E\\left[\\frac{1}{W}\\sum_{\\stackrel{i\\in[n]}{\\Delta\\neg\\mathrm{wide}}}\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w P(c\\circ X^{\\alpha})\\Bigg|\\mathcal{E}\\right]\\geq\\mathrm{opt}_{\\mathbb{Z}}-(O(\\varepsilon^{\\prime})+4\\eta)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying Markov\u2019s inequality on the random variable $\\begin{array}{r}{\\mathrm{opt}_{\\mathbb{Z}}-\\frac{1}{W}\\sum_{i\\in[n]}\\sum_{(w,c,\\alpha)\\in\\tilde{S}_{i}}w P(c\\circ X^{\\alpha}),}\\end{array}$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left.\\frac{1}{W}\\sum_{\\stackrel{i\\in[n]}{\\Delta\\neg\\neg\\alpha}\\!\\!\\right)\\!\\!\\in\\!\\!\\!\\left(w,c,\\alpha\\right)\\!\\in\\tilde{S}_{i}}w P(c\\circ X^{\\alpha})\\leq\\mathrm{opt}_{\\mathbb{Z}}-(O(\\varepsilon^{\\prime})+5\\eta)\\right|\\varepsilon\\right)\\leq\\frac{1}{1+\\eta}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The theorem follows since we sample ${\\cal O}(1/\\eta)$ independent assignments $X$ and pick the best. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Theorems 3.1, 4.1 and 5.2, which are proved in the paper, formalize the claims in the abstract. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The models\u2019 assumptions are accurately described (see Section 2). ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All assumptions are clearly stated. All theorems are formally proved in the main body or in the appendix. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: This is a theoretical work; we do not foresee any societal impact of this work. ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not release new assets. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}]