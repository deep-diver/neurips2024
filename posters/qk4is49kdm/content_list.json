[{"type": "text", "text": "Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jason D. Lee1, Kazusato $\\mathbf{Ok0^{2,4}}$ , Taiji Suzuki3,4, Denny Wu5,6 ", "page_idx": 0}, {"type": "text", "text": "1Princeton University, 2University of California, Berkeley, 3University of Tokyo 4RIKEN AIP, 5New York University, 6Flatiron Institute ", "page_idx": 0}, {"type": "text", "text": "jasonlee@princeton.edu, oko@berkeley.edu, taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of gradient descent learning of a single-index target function $f_{*}(\\pmb{x})\\stackrel{!}{=}\\sigma_{*}(\\bar{\\langle}\\pmb{x},\\pmb{\\theta}\\rangle)$ under isotropic Gaussian data in $\\mathbb{R}^{d}$ , where the unknown link function $\\sigma_{*}:\\mathbb{R}\\rightarrow\\mathbb{R}$ has information exponent $p$ (defined as the lowest degree in the Hermite expansion). Prior works showed that gradient-based training of neural networks can learn this target with $n\\gtrsim d^{\\Theta(p)}$ samples, and such complexity is predicted to be necessary by the correlational statistical query lower bound. Surprisingly, we prove that a two-layer neural network optimized by an SGD-based algorithm (on the squared loss) learns $f_{*}$ with a complexity that is not governed by the information exponent. Specifically, for arbitrary polynomial single-index models, we establish a sample and runtime complexity of $n\\simeq T=\\Theta(d{\\cdot}\\mathrm{polylog}d)$ , where $\\Theta(\\cdot)$ hides a constant only depending on the degree of $\\sigma_{*}$ ; this dimension dependence matches the information theoretic limit up to polylogarithmic factors. More generally, we show that $n\\gtrsim d^{(p_{*}-1)\\vee1}$ samples are sufficient to achieve low generalization error, where $p_{*}\\leq p$ is the generative exponent of the link function. Core to our analysis is the reuse of minibatch in the gradient computation, which gives rise to higher-order information beyond correlational queries. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Single-index models are a classical class of functions that capture low-dimensional structure in the learning problem. To efficiently estimate such functions, the learning algorithm should extract the relevant (one-dimensional) subspace from high-dimensional observations; hence this problem setting has been extensively studied in deep learning theory [BL20, $\\mathrm{BES}^{+}22$ , BBSS22, $\\mathrm{MHPG}^{+}23$ , ${\\tt M}\\bar{\\cal Z}{\\tt D}^{+}23$ , WWF24], to examine the adaptivity to low-dimensional targets and benefit of representation learning in neural networks (NNs) optimized by gradient descent (GD). In this work we study the learning of a single-index target function under isotropic Gaussian data: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{i}=f_{*}(\\pmb{x}_{i})+\\varsigma_{i},\\quad f_{*}(\\pmb{x}_{i})=\\sigma_{*}(\\langle\\pmb{x}_{i},\\pmb{\\theta}\\rangle),\\quad\\pmb{x}_{i}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,\\pmb{I}_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\varsigma_{i}$ is i.i.d. label noise, $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ is the direction of index features, and we assume the link function $\\sigma_{*}:\\mathbb{R}\\rightarrow\\mathbb{R}$ has information exponent $p\\in\\mathbb{N}_{+}$ defined as the index of the first non-zero coefficient in the Hermite expansion (see Definition 1). ", "page_idx": 0}, {"type": "text", "text": "Equation (1.1) requires the estimation of the one-dimensional link function $\\sigma_{*}$ and the relevant direction $\\pmb{\\theta}$ ; it is known that learning is information theoretically possible with $n\\gtrsim d$ training examples [DH24, DPVLB24]. Indeed, when $\\sigma_{*}$ is polynomial, such statistical complexity can be achieved up to logarithmic factors by a tailored algorithm that exploit the structure of low-dimensional target [CM20]. On the other hand, for gradient-based training of two-layer NNs, existing works established a sample complexity of $n\\gtrsim\\,d^{\\Theta(p)}$ [BAGJ21, BBSS22, DNGL23], which presents a gap between the information theoretic limit and what is computationally achievable by (S)GD. Such a gap is also predicted by the correlational statistical query (CSQ) lower bound [DLS22, AAM23], which roughly states that for a CSQ algorithm to learn (isotropic) Gaussian single-index models using less than exponential compute, a sample size of $n\\gtrsim d^{p/2}$ is necessary. ", "page_idx": 0}, {"type": "image", "img_path": "qK4iS49KDm/tmp/1ac18f9685f00b96fda53e69ed5efb66d5e415bb3dd6b57b7e2ee2150b5187c5.jpg", "img_caption": ["Figure 1: We train a ReLU NN (3.1) with $N=1024$ neurons using SGD (squared loss) with step size $\\eta=1/d$ to learn a single-index target $f_{*}(\\pmb{x})\\,=\\,{\\sf H e}_{3}(\\langle\\pmb{x},\\pmb{\\theta}\\rangle)$ ; heatmaps are values averaged over 10 runs. $(a)$ online SGD with batch size $B\\,=\\,8$ ; $(b)$ GD on the same batch of size $_n$ for $T=2^{14}$ steps. We only report weak recovery (i.e., overlap between parameters $\\pmb{w}$ and target $\\pmb{\\theta}$ , averaged across neurons) for online SGD since the test error does not drop. ", ""], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Although CSQ lower bounds are frequently cited to imply a fundamental barrier of learning via SGD (with the squared/correlation loss), strictly speaking, the CSQ model does not include empirical risk minimization with gradient descent, due to the non-adversarial noise and existence of noncorrelational terms in the gradient computation. Very recently, $[\\mathrm{DTA}^{+}24]$ exploited higher-order terms in the gradient update arising from the reuse of the same training data, and showed that for certain link functions with high information exponent $(p\\,>\\,2)$ , two-layer NNs may still achieve weak recovery (i.e., nontrivial overlap with $\\pmb{\\theta}$ ) after two GD steps with $\\Theta(d)$ batch size. While this presents evidence that GD-trained NNs can learn $f_{*}$ beyond the sample complexity suggested by the CSQ lower bound, the weak recovery statement in $\\mathrm{[DTA^{+}24]}$ may not translate to statistical guarantees; moreover, the class of functions where SGD can achieve vanishing generalization error is not fully characterized, as only a few specific examples of link functions are discussed. ", "page_idx": 1}, {"type": "text", "text": "Given the existence of (non-NN) algorithms that learn any single-index polynomials in $n=\\tilde{O}(d)$ samples [CM20] regardless of the information exponent $p$ , and more generally, non-CSQ algorithms with a sample complexity surpassing the CSQ lower bound [DPVLB24], it is natural to ask if gradient-based training of NNs can achieve similar statistical efficiency for this function class. Motivated by observations in $[\\mathrm{DTA}^{+}24]$ that SGD with reused data may break the \u201ccurse of information exponent\u201d, we aim to address the question: ", "page_idx": 1}, {"type": "text", "text": "Can NN optimized by SGD with reused batch learn single-index $f_{*}$ beyond the CSQ lower bound? And for polynomial $\\sigma_{*}$ , can learning succeed near the information-theoretic limit $n\\simeq d$ ? ", "page_idx": 1}, {"type": "text", "text": "Empirically, the separation between one-pass (online) and multi-pass SGD is clearly observed in Figure 1, where we trained the same two-layer ReLU neural network to learn a single-index polynomial with information exponent $p=3$ . We see that SGD with reused data (Figure 1(b)) reaches low test error using roughly $n\\simeq d$ samples, whereas online SGD fails to achieve even weak recovery with much larger sample size $n=\\bar{\\Omega}(d^{2})$ . Our main contribution is to establish this improved statistical complexity for two-layer NNs trained by a variant of SGD with reused training data. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We answer the above question in the affirmative by showing that SGD training (with the squared loss) on a natural class of shallow NNs can achieve small generalization error using polynomial compute and a sample complexity that is not governed by the information exponent, if we employ a layer-wise optimization procedure (analogous to that in $[\\mathbf{B}\\mathbf{E}\\mathbf{S}^{+}22$ , DLS22, AAM23]) and reuse of the same minibatch. The core insight is that SGD can implement a full statistical query (SQ) ", "page_idx": 1}, {"type": "text", "text": "algorithm that goes beyond CSQ, despite the correlational structure of the squared loss. Our main finding is summarized by the following theorem. ", "page_idx": 2}, {"type": "text", "text": "Theorem (informal). A shallow NN with $N\\;=\\;\\tilde{O}_{d}(1)$ neurons can learn arbitrary single-index models up to small population loss: $\\mathbb{E}_{\\pmb{x}}[(f_{\\pmb{\\Theta}}(\\pmb{x})-f_{*}(\\pmb{x}))^{2}]=o_{d,\\mathbb{P}}(1),$ , if we employ an SGD-based algorithm (with reused training data) to minimize the squared loss objective, with a sample and runtime complexity of $n,T=\\bar{\\tilde{O}}_{d}(d^{(p_{*}-1)\\vee1})$ , where $p_{*}$ is the generative exponent of the link $\\sigma_{*}$ . ", "page_idx": 2}, {"type": "text", "text": "Note that the generative exponent [DPVLB24] is defined as the minimum information exponent of the link function $\\sigma_{*}$ after arbitrary $L^{2}$ transformation, and hence by definition $p_{*}\\leq p$ (equality is achieved by the identity transformation). We make the following remarks on our main result. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We know that $p_{*}\\leq2$ for arbitrary polynomial link functions. Therefore, the theorem suggests that $\\mathrm{NN}+\\mathrm{SGD}$ with reused batch can learn single-index polynomials with a sample complexity $n=$ ${\\tilde{O}}_{d}(d)$ which is information theoretically optimal up to polylogarithmic factors, hence matching the efficiency of SQ algorithms tailored for low-dimensional polynomial regression [CM20]. \u2022 For non-polynomial $\\sigma_{*}$ with high generative exponent $p_{*}>2$ , our sample complexity $n\\gtrsim d^{p_{*}-1}$ can be interpreted as an SQ version of the online SGD result in [BAGJ21]. Since the information exponent $p$ can be arbitrarily larger than the generative exponent $p_{*}$ , our main theorem disproves a conjecture in [AAM23] stating that $n\\,\\asymp\\,\\bar{d}^{p/2}$ is the optimal sample complexity for empirical risk minimization with SGD on the squared loss / correlation loss. \u2022 A key observation in our analysis is that with suitable activation function, SGD with reused batch can go beyond correlational queries and implement (a subclass of) SQ algorithms. This enables polynomial transformations to the labels that reduce the information exponent, and therefore optimization can escape the high-entropy \u201cequator\u201d at initialization in polylogarithmic time. ", "page_idx": 2}, {"type": "text", "text": "Upon completion of this work, we became aware of the preprint $[\\mathrm{ADK^{+}24}]$ showing weak recovery (for polynomial targets with $p_{*}\\leq2\\!,$ ) with similar sample complexity, also by exploiting the reuse of training data. Our work was conducted independently and simultaneously. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setting and Prior Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. $\\|\\cdot\\|$ denotes the $\\ell_{2}$ norm for vectors and the $\\ell_{2}\\to\\ell_{2}$ operator norm for matrices. $O_{d}(\\cdot)$ and $o_{d}(\\cdot)$ stand for the big-O and little-o notations, where the subscript highlights the asymptotic variable $d$ and suppresses dependence on $p,q$ ; we write $\\tilde{O}(\\cdot)$ when (poly-)logarithmic factors are ignored. $O_{d,\\mathbb{P}}(\\cdot)$ (resp. $o_{d,\\mathbb{P}}(\\cdot),$ ) represents big-O (resp. little-o) in probability as $d\\to\\infty$ . $\\Omega(\\cdot),\\Theta(\\cdot)$ are defined analogously. $\\gamma$ is the standard Gaussian distribution in $\\mathbb{R}$ . We denote the $L^{2}$ -norm of a function $f$ with respect to the data distribution (which will be specified) as $\\|f\\|_{L^{2}}$ . For $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ , we denote $g^{i}$ as its $i$ -th exponentiation, and $g^{(i)}$ is the $i$ -th derivative. We say an event happens with high probability when the failure probability is bounded by $\\exp(-C\\log d)$ for large constant $C$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Complexity of Learning Single-index Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to learn a single-index model (1.1) where the link function $\\sigma_{*}:\\mathbb{R}\\rightarrow\\mathbb{R}$ has information exponent $p$ defined as follows [DH18, BAGJ21]. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Information exponent). Let $\\{\\mathsf{H e}_{j}\\}_{j=0}^{\\infty}$ denote the normalized Hermite polynomials. The information exponent of $g\\,\\in\\,L^{2}(\\gamma)$ , denoted by $\\operatorname{IE}(g)\\;:=\\;p\\;\\in\\;\\mathbb{N}_{+}$ , is the index of the first non-zero Hermite coefficient of $g$ , that is, given $\\begin{array}{r}{g(z)=\\sum_{i=0}^{\\infty}\\alpha_{i}\\mathsf{H}\\mathsf{e}_{i}(z)}\\end{array}$ , $p:=\\operatorname*{min}\\{i>0:\\alpha_{i}\\neq0\\}$ . ", "page_idx": 2}, {"type": "text", "text": "By definition, when $\\sigma_{*}$ is a degree- $\\cdot q$ polynomial, we always have $p\\,\\leq\\,q$ . Note that $f_{*}$ contains $\\Theta(d)$ parameters to be estimated, and hence information theoretically $n\\gtrsim d$ samples are both sufficient and necessary for learning [MM18, $\\mathbf{B}\\mathbf{K}\\mathbf{\\dot{M}}^{+}19$ , DPVLB24]; however, the sample complexity achieved by different (polynomial time) algorithms depends on structure of the link function. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Kernel Methods. Rotationally invariant kernels cannot adapt to the low-dimensional structure of single-index $f_{*}$ and hence suffer from the curse of dimensionality [YS19, GMMM21, DWY21, $\\mathrm{BE}\\bar{\\mathbf{S}}^{+}22]$ . By a standard dimension argument [KMS20, HSSVG21, AAM22], we know that in the isotropic data setting, kernel methods (including neural networks in the lazy regime [JGH18, COB19]) require $n\\gtrsim d^{q}$ samples to learn degree- $\\cdot q$ polynomials in $\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Gradient-based Training of NNs. While NNs can easily approximate a single-index model [Bac17], the sample complexity of gradient-based learning established in prior works typically scales as $n\\gtrsim d^{\\Theta(p)}$ : in the well-specified setting, [BAGJ21] proved a sample complexity of $n=$ $\\tilde{\\Theta}(d^{p-1})$ for online SGD, which is later improved to $\\tilde{\\Theta}(d^{p/2})$ by a smoothed objective [DNGL23]; as for the misspecified setting, [BBSS22, $\\mathrm{DKL}^{+}23]$ showed that $n\\gtrsim d^{p}$ samples suffice, and in some cases a $\\tilde{\\Theta}(d^{p-1})$ complexity is achievable [AAM23, OSSW24]. Consequently, at the information-theoretic limit $(n\\asymp d)$ , existing results can only cover the learning of low information exponent targets [AAM22, BMZ23, $\\mathrm{BES}^{+}23^{\\cdot}$ . This exponential dependence on $p$ also appears in the CSQ lower bounds [DLS22, AAM22], which is often considered to be indicative of the performance of SGD learning with the squared loss (see Section 2.2). ", "page_idx": 3}, {"type": "text", "text": "Statistical Query Learners. If we do not restrict ourselves to correlational queries, the sample complexity of learning (1.1) can be drastically improved. Specifically, for polynomial $\\sigma_{*}$ , [CM20] gave an SQ algorithm that achieves low generalization error in $n=\\tilde{O}(d)$ samples, which is near the information-theoretic limit; the key ingredient is to construct nonlinear transformations to the labels that lowers the information exponent to 2; similar preprocessing also appeared in context of phase retrieval [MM18, $\\mathbf{BKM}^{+}19]$ . Such transformations do not belong to CSQ, but can be utilized by a full SQ learner to enhance the statistical efficiency. Recently, [DPVLB24] introduced the generative exponent which governs the complexity of SQ algorithms. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Generative exponent). The generative exponent $(G E)$ of $g\\,\\in\\,L^{2}(\\gamma)$ is defined as the lowest information exponent $(I E)$ after arbitrary $L^{2}$ transformation, that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{*}=:\\mathrm{GE}(g)=\\operatorname*{inf}_{T\\in L^{2}(P_{y})}\\mathrm{IE}(T\\circ g).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The generative exponent is the smallest information exponent obtained by all possible label transformations. By definition we always have $p^{*}\\leq p$ , and the gap between the two indices can be arbitrarily large; for example, for the Hermite polynomials we have $\\mathrm{IE}({\\mathsf{H e}}_{k})=k$ whereas $\\mathrm{GE}(\\mathsf{H e}_{k})\\leq2$ . ", "page_idx": 3}, {"type": "text", "text": "[DPVLB24] established a sample complexity lower bound of $n=\\Omega(d^{p_{*}/2\\vee1})$ for full SQ learners with polynomial compute, and obtained matching upper bounds by a tensor partial-trace algorithm. The goal of our work is to show that SGD training of a two-layer neural network can also achieve a sample and computational complexity that scales with $n\\simeq d^{\\Theta(p_{*})}$ , where the dimension dependence is governed by the generative exponent $p_{*}$ instead of the information exponent $p$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Can Gradient Descent Go Beyond Correlational Queries? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Correlational statistical query. A statistical query (SQ) learner [Kea98, Rey20] accesses the target $f_{*}$ through noisy queries $\\tilde{\\phi}$ with error tolerance $\\tau$ : $|\\tilde{\\phi}-\\mathbb{E}_{\\mathbf{x},y}[\\phi(\\mathbf{x},y)]|\\,\\leq\\,\\tau$ . Lower bound on the performance of SQ algorithm is a classical measure of computational hardness. In the context of gradient-based optimization, an often-studied subclass of SQ is the correlational statistical query (CSQ) [BF02] where the query is restricted to (noisy version of) $\\mathbb{E}_{{\\pmb x},{\\pmb y}}[\\phi({\\pmb x}){\\pmb y}]$ . To see the connection between CSQ and SGD, consider the gradient of expected squared loss for one neuron $f_{w}(x)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{w}\\mathbb{E}_{x,y}(f_{w}(x)-y)^{2}\\propto-\\mathbb{E}_{x,y}[\\underbrace{y\\cdot\\nabla_{w}f_{w}(x)}_{\\mathrm{correlational~query}}]\\,+\\,\\mathbb{E}_{x}[\\underbrace{f_{w}(x)\\cdot\\nabla_{w}f_{w}(x)}_{\\mathrm{can~be~evaluated~without~}y}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "One can see that information of the target function is encoded in the correlation term in the gradient. To infer the statistical efficiency of GD in the empirical risk minimization setting, we replace the population gradient with the empirical average $\\begin{array}{r}{\\nabla_{\\pmb{w}}\\mathring{(\\frac{1}{n}\\sum_{i=1}^{n}(f_{\\pmb{w}}(\\pmb{x}_{i})-y_{i})^{2})}}\\end{array}$ , and heuristically equate the CSQ tolerance \u03c4 with the scale of i.i.d. concentration error n\u22121/2. ", "page_idx": 3}, {"type": "text", "text": "For the Gaussian single-index model class with information exponent $p$ , [DLS22] proved a lower bound stating that a CSQ learner either has access to queries with tolerance $\\tau\\lesssim d^{-p\\bar{/}4}$ , or exponentially many queries are needed to learn $f_{*}$ with small population loss. Using the heuristic $\\tau\\approx n^{-1/2}$ , this suggests a sample complexity lower bound $n\\gtrsim d^{p/2}$ for polynomial time CSQ algorithm. This lower bound can be achieved by a landscape smoothing procedure [DNGL23] (in the well-specified setting), and is conjectured to be optimal for empirical risk minimization with SGD [AAM23]. ", "page_idx": 4}, {"type": "text", "text": "SGD with reused data. As previously discussed, the gap between SQ and CSQ algorithms primarily stems from the existence of label transformations that decrease the information exponent. While such transformation cannot be utilized by a CSQ learner, $[\\mathrm{DTA}^{+}24]$ argued that they may arise from two consecutive gradient updates using the same minibatch. For illustrative purposes, consider one neuron $f_{{\\pmb w}}({\\pmb x})=\\bar{\\sigma}(\\langle{\\pmb x},{\\pmb w}\\rangle)$ updated by two GD steps using the same data point $(\\pmb{x},y)$ , starting from zero initialization ${\\boldsymbol w}^{0}={\\bf0}$ (we focus on the correlational term in the loss for simplicity): ", "page_idx": 4}, {"type": "equation", "text": "$$\nw^{2}=w^{1}+\\eta\\cdot y\\sigma^{\\prime}(\\langle x,w^{1}\\rangle)x=\\eta\\sigma^{\\prime}(0)\\underbrace{y\\cdot x}_{\\mathrm{CsQ\\;term}}+\\eta\\underbrace{y\\sigma^{\\prime}(\\eta\\sigma^{\\prime}(0)\\|x\\|^{2}\\cdot y)x}_{\\mathrm{non-CSQ\\;term}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Under appropriate learning rate scaling $\\eta\\cdot\\left\\|\\pmb{x}\\right\\|^{2}=\\Theta(1)$ , one can see that in the second gradient step, the label $y$ is transformed by the nonlinearity $\\sigma^{\\prime}$ , even though the loss function itself is not modified. Based on this observation, $[\\mathrm{DTA}^{+}24]$ showed that if the non-CSQ term in (2.1) reduces the information exponent to 1, then weak recovery (i.e., nontrivial overlap between the first-layer parameters $\\mathbf{\\nabla}w$ and index features $\\pmb{\\theta}$ ) can be achieved after two GD steps with $n=\\Theta(d)$ samples. ", "page_idx": 4}, {"type": "text", "text": "2.3 Challenges in Establishing Statistical Guarantees ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Importantly, the analysis in $[\\mathrm{DTA}^{+}24]$ does not lead to concrete learnability guarantees for the class of single-index polynomials for the following reasons: $(i)$ it is not clear if an appropriate nonlinear transformation that lowers the information exponent can always be extracted from SGD with reused data, and $(i i)$ the weak recovery guarantee may not translate to a sample complexity for the trained NN to achieve small generalization error. We elaborate these technical challenges below. ", "page_idx": 4}, {"type": "text", "text": "SGD decreases information exponent. To show weak recovery, $\\mathrm{[DTA^{+}24}$ , Definition 3.1] assumed that the student activation $\\sigma$ can reduce the information exponent of the labels to 1; while a few examples are given, the existence of such transformations in SGD is not guaranteed: ", "page_idx": 4}, {"type": "text", "text": "\u2022 The label transformation employed in prior SQ algorithms [CM20] is based on thresholding, which reduces the information exponent to 2 for any polynomial $\\sigma_{*}$ ; however, isolating such function from SGD updates on the squared loss is challenging. Instead, we make use of monomial transformation which can be extracted from SGD via Taylor expansion. \u2022 If the link function satisfies $p_{*}\\geq2$ , its information exponent after arbitrary nonlinear transformation is at least 2; such functions are predicted not be not learnable by SGD in the $n\\asymp d$ regime $[\\mathrm{DTA}^{+}24]$ . To handle this setting, we analyze the SGD update up to $\\mathrm{poly}(d)$ time, at which a nontrivial overlap can be established by a Gro\u00a8nwall-type argument similar to [BAGJ21]. For $p_{*}=2$ , this recovers results on phase retrieval when $\\sigma_{*}(z)^{*}\\!=z^{2}$ which requires $n=\\Omega(d\\log d)$ . ", "page_idx": 4}, {"type": "text", "text": "From weak recovery to sample complexity. Note that weak recovery (i.e., $|\\langle{\\pmb w},{\\pmb\\theta}\\rangle|\\;>\\;\\varepsilon$ for some small constant $\\varepsilon\\;>\\;0$ ) is generally insufficient to establish low generalization error of the trained NN. Therefore, we need to show that starting from a nontrivial overlap, subsequent gradient steps can achieve strong recovery of the index features (i.e., $|\\langle{\\pmb w},{\\pmb\\theta}\\rangle|\\,>\\,1\\,-\\,\\varepsilon)$ , despite the link misspecification. After the first-layer parameters align with the target function, we can train the second-layer parameters with SGD to learn the link function $\\sigma_{*}$ with the aid of random bias units. ", "page_idx": 4}, {"type": "text", "text": "3 Learning Polynomial $f_{*}$ in Linear Sample Complexity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first consider the setting where $\\sigma_{*}$ is polynomial with degree $q$ specified as follows. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. The target function is given as $f_{*}(\\pmb{x})\\,=\\,\\sigma_{*}(\\langle\\pmb{x},\\pmb{\\theta}\\rangle)$ , where the link function $\\sigma_{*}$ : $\\mathbb{R}\\to\\mathbb{R}$ admits the Hermite decomposition $\\begin{array}{r}{\\sigma_{*}(z)=\\sum_{i=p}^{q}\\alpha_{i}\\mathsf{H}\\mathsf{e}_{i}(z)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "For single-index polynomials, we do not expect a computational-to-statistical gap under the SQ class [CM20] \u2014 indeed, we will establish learning guarantees near the information theoretic limit $n\\asymp d$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Gradient-based training of two-layer neural network ", "page_idx": 5}, {"type": "text", "text": "Input : Step sizes $\\eta^{t}$ ; momentum parameters $\\xi^{t}$ ; training time $T_{1},T_{2};\\ell_{2}$ regularization $\\lambda$ .   \nInitialize $w_{j}^{\\bar{0}}\\sim\\mathbb{S}^{d-1}(1)$ , $a_{j}\\sim\\mathrm{Unif}\\{\\pm c_{a}\\}$ .   \nPhase I: normalized SGD on first-layer parameters for $t=0$ to $T_{1}$ do if $t$ is even then Draw i.i.d. sample $(\\pmb{x},y)$ . Interpolate $\\pmb{w}_{j}^{t}\\leftarrow\\pmb{w}_{j}^{t}-\\xi_{j}^{t}(\\pmb{w}_{j}^{t}-\\pmb{w}_{j}^{t-2})$ (when $t>0$ ). Normalize $\\pmb{w}_{j}^{t}\\leftarrow\\pmb{w}_{j}^{t}/\\|\\pmb{w}_{j}^{t}\\|$ . $\\pmb{w}_{j}^{t+1}\\leftarrow\\pmb{w}_{j}^{t}-\\eta^{t}\\tilde{\\nabla}_{\\pmb{w}}(f_{\\pmb{\\Theta}}(\\pmb{x})-y)^{2},\\quad(j=1,\\ldots,N).$   \nInitialize $b_{j}\\sim\\mathrm{Unif}([-C_{b},C_{b}])$ .   \nPhase II: SGD on second-layer parameters   \nO $\\begin{array}{r l}&{\\quad\\hat{a}\\gets\\operatorname*{argmin}_{a\\in\\mathbb{R}^{N}}\\frac{1}{T_{2}}\\sum_{i=1}^{\\tilde{T}_{2}}(\\dot{f}_{\\Theta}({\\boldsymbol x}_{i})-{\\boldsymbol y}_{i})^{2}+\\lambda\\|{\\boldsymbol a}\\|^{2}.}\\\\ &{\\qquad\\mathbf{utput}\\colon\\operatorname*{Prediction}\\,\\operatorname{function}{\\boldsymbol x}\\mapsto f_{\\hat{\\Theta}}({\\boldsymbol x})\\,\\operatorname{with}\\hat{\\Theta}=(\\hat{a}_{j},{\\boldsymbol w}_{j}^{T_{1}},{\\boldsymbol b}_{j})_{j=1}^{N}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "3.1 Training Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We train the following two-layer network with $N$ neurons using SGD to minimize the squared loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{\\Theta}(\\pmb{x})=\\frac{1}{N}\\sum_{j=1}^{N}a_{j}\\sigma_{j}(\\langle\\pmb{x},\\pmb{w}_{j}\\rangle+b_{j}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{\\Theta}\\,=\\,(\\pmb{w}_{j},a_{j},b_{j})_{j=1}^{N}$ are trainable parameters, and $\\sigma_{j}:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the activation function defined as the sum of Hermite polynomials to degree $\\begin{array}{r}{C_{\\sigma}\\colon\\sigma_{j}(z):=\\sum_{i=0}^{C_{\\sigma}}\\beta_{j,i}\\mathsf{H}\\mathsf{e}_{i}(z)}\\end{array}$ , where $C_{\\sigma}$ $\\sigma_{*}$   \nnonlinearity as indicated by the subscript in $\\sigma_{j}$ ; this subscript is omitted when we focus on the dynamics of one single neuron. Our SGD training procedure is described in Algorithm 1, and below we outline the key ingredients of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Algorithm 1 employs a layer-wise training strategy common in the recent feature learning theory literature [DLS22, $\\bar{\\mathbf{B}}\\mathbf{E}\\mathbf{S}^{+}\\bar{2}2$ , BBSS22, AAM23, MHWSE23], where in the first stage, we optimize the first-layer parameters $\\{w_{j}\\}_{j=1}^{N}$ with normalized SGD to learn the low-dimensional latent representation (index features $\\pmb{\\theta}$ ), and in the second phase, we train the second-layer $\\{a_{j}\\}_{j=1}^{N}$ to fit the unknown link function \u03c3\u2217. ", "page_idx": 5}, {"type": "text", "text": "\u2022 The most crucial part in Phase I of Algorithm 1 is the reuse of the same minibatch in the gradient computation. Specifically, we sample a fresh batch of training examples in every two $G D$ steps; this enables us to extract non-CSQ terms from two consecutive gradient updates outlined in (2.1). ", "page_idx": 5}, {"type": "text", "text": "\u2022 We introduce an interpolation step between the current and previous iterates with hyperparameter $\\xi$ to stabilize the training dynamics; this resembles a negative momentum often seen in optimization algorithms [AZ18, ZLBH19]; the role of this interpolation is discussed in Section 4.2. We use a projected gradient update $\\tilde{\\nabla}_{w}\\mathcal{L}(\\pmb{w})=(\\pmb{I}_{d}-\\pmb{w}^{2t}\\pmb{w}^{2\\bar{t}^{\\top}})\\nabla_{\\pmb{w}}\\mathcal{L}(\\pmb{w})$ for steps $2t$ and $2t+1$ , where $\\nabla_{w}$ is the Euclidean gradient; similar use of projection also appeared in [DNGL23, AAM23]. ", "page_idx": 5}, {"type": "text", "text": "3.2 Convergence and Sample Complexity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Weak Recovery Guarantee. We first consider the \u201csearch phase\u201d of SGD, and show that after running Phase I of Algorithm 1 for $T={\\mathrm{polylog}}(d)$ steps, a subset of parameters $\\mathbf{\\nabla}w$ achieve nontrivial overlap with the target direction $\\pmb{\\theta}$ . We denote $H(g;j)$ as the $j$ -th Hermite coefficient of some $g\\in L^{2}(\\gamma)$ . Our main theorems handle polynomial activations satisfying the following condition. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. We require the activation function to be a polynomial $\\begin{array}{r}{\\sigma(z)=\\sum_{i=0}^{C_{\\sigma}}\\beta_{i}\\mathsf{H}\\mathsf{e}_{i}(z)}\\end{array}$ and its degree to be sufficiently large so that holds is defined in Pr oposition $^{6}$ ). For all $2\\leq\\ell\\leq C_{\\sigma}$ and $k=0,1$ , we assume that $H\\big(\\sigma^{(\\ell)}(\\sigma^{(1)})^{\\ell-1};k\\big)>0$ . ", "page_idx": 5}, {"type": "text", "text": "As discussed in Appendix B.1, a given $\\sigma_{*}$ , the above condition only needs to be met for one pair of $(k,\\ell)$ . Also, Appendix B.1.3 states that $H\\big(\\sigma^{(\\ell)}(\\sigma^{(1)})^{\\ell-1};k\\big)\\,\\stackrel{\\cdot}{\\neq}\\,0$ also works by choosing $\\xi$ differently. We show that this condition is satisfied for a wide range of polynomials with degree $C_{\\sigma}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 3. Given $\\ell\\geq2$ and $k\\geq0$ . For $\\begin{array}{r}{C_{\\sigma}\\geq\\frac{2\\ell+k-1}{\\ell}}\\end{array}$ , if we choose $\\{\\beta_{i}\\}_{i=0}^{C_{\\sigma}}$ where $\\beta_{i}$ is randomly drawn from some non-empty interval $[a_{i},b_{i}]$ , then ${\\bf\\,\\tilde{\\cal H}}(\\sigma^{(\\ell)}(\\sigma^{(1)})^{\\ell-1};k)\\neq0$ with probability $^{\\,l}$ . $n={\\tilde{\\Theta}}(d)$ ", "page_idx": 6}, {"type": "text", "text": "The following theorem states that samples are sufficient to achieve weak recovery. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Under Assumptions $^{\\,l}$ and 2, for suitable choices of hyperparameters $\\eta^{t}=\\Tilde{O}_{d}(N d^{-1})$ and $1-\\xi^{t}\\,=\\,o_{d}(1)$ , there exists constant $C(q)$ such that after Phase $I$ of Algorithm 1 is run for $2T_{1,1}=C(q)\\cdot d\\mathrm{polylog}(d)$ steps, with high probability, there exists a subset of neurons $\\pmb{w}_{j}^{2T_{1}}\\in\\mathcal{W}$ with $|\\mathcal{W}|=\\tilde{\\Theta}(N)$ such that $\\left|\\left\\langle\\pmb{w}_{j}^{2T_{1}},\\pmb{\\theta}\\right\\rangle\\right|>c$ for some $c\\gtrsim1/\\mathrm{polylog}(d)$ . ", "page_idx": 6}, {"type": "text", "text": "Recall that at random initialization we have $\\langle{\\pmb w},{\\pmb\\theta}\\rangle\\approx d^{-1/2}$ with high probability. The theorem hence implies that SGD \u201cescapes from mediocrity\u201d after seeing $n=\\tilde{O}(d)$ samples, analogous to the information exponent $p\\,=\\,2$ setting studied in [BAGJ21]. We remark that due to the small second-layer initialization, the squared loss is dominated by the correlation loss, which allows us to track the evolution of each neuron independently; similar use of vanishing initialization also appeared in $[\\mathbf{B}\\mathbf{E}\\mathbf{S}^{+}22$ , AAM23]. This will be formally proved in Appendix B.4. ", "page_idx": 6}, {"type": "text", "text": "Strong recovery and sample complexity. After weak recovery is achieved, we continue Phase I to amplify the alignment. Due to the nontrivial overlap between $\\pmb{w}$ and $\\pmb{\\theta}$ , the objective is no longer dominated by the lowest degree in the Hermite expansion. Therefore, to establish strong recovery $(\\langle{\\pmb w},{\\pmb\\theta}\\rangle>1-\\varepsilon)$ , we place an additional assumption on the activation function. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. Given the Hermite expansions $\\begin{array}{r}{\\sigma_{\\ast}(z)=\\sum_{i=p}^{q}\\alpha_{i}\\mathsf{H e}_{i}(z),\\sigma_{j}(z)=\\sum_{i=0}^{C_{\\sigma}}\\beta_{j,i}\\mathsf{H e}_{i}(z),}\\end{array}$ we assume the coefficients satisfy $\\alpha_{i}\\beta_{j,i}\\geq0$ for $p\\leq i\\leq q$ . ", "page_idx": 6}, {"type": "text", "text": "This assumption is easily verified in the well-specified setting $\\sigma_{*}=\\sigma$ [BAGJ21] since $\\alpha_{i}=\\beta_{i}$ , and under link misspecification, it has been directly assumed in prior work [MHWSE23]. We follow [OSSW24] and show that by randomizing the Hermite coefficients of the activation function, a subset of neurons satisfy the above assumption for any degree- $q$ polynomial link function $\\sigma_{*}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 4. If we set $\\begin{array}{r}{\\sigma_{j}(z)~=~\\sum_{i=0}^{C_{\\sigma}}\\beta_{j,i}\\mathsf{H}\\mathsf{e}_{i}(z),}\\end{array}$ , where for each neuron we sample $\\beta_{j,i}$ i.i\u223c.d. $\\operatorname{Unif}(\\{\\pm r_{i}\\})$ with appropriate c onstant $r_{i}$ , then Assumption 2 and 3 are satisfied in $\\exp(-\\Theta(q))$ - fraction of neurons. ", "page_idx": 6}, {"type": "text", "text": "Note that in our construction of activation functions for both assumptions, we do not exploit knowledge of the link function $\\sigma_{*}$ other than its degree $q$ which decides the constant $C_{\\sigma}$ . See Appendix B.1 for more discussion of Assumption 3 and Lemma 4. The next theorem shows that by running Phase I for $\\tilde{\\Theta}(d)$ more steps, a subset of neurons can achieve almost perfect overlap with the index features. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. For student neurons with activation satisfying Assumptions 2 and 3 and parameter ${\\pmb w}_{j}$ starting from a nontrivial overlap c specified in Theorem $^{\\,l}$ , for suitable hyperparameters $\\eta^{t}=$ $\\tilde{O_{d}}(N d^{-1}\\varepsilon)$ and $\\xi^{t}=1$ , if we continue to run Phase I of Algorithm 1 for $2T_{1,2}=\\tilde{\\Theta}_{d}(d\\varepsilon^{-2})$ steps, then we achieve $\\langle w_{j}^{2(T_{1,1}+T_{1,2})},\\pmb{\\theta}\\rangle>1-\\varepsilon$ with high probability. ", "page_idx": 6}, {"type": "text", "text": "The following proposition shows that after strong recovery, training the second-layer parameters in Phase $\\mathrm{II}$ is sufficient for the NN model (3.1) to achieve small generalization error. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5. After Phase I terminates, for suitable $\\lambda>0$ , the output of Phase II satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}}[(f_{\\hat{\\Theta}}(\\mathbf{x})-f_{*}(\\mathbf{x}))^{2}]\\lesssim\\varepsilon^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability $^{\\,l}$ as $d\\to\\infty$ , $i f$ we set $T_{2}=C(q)N^{4}\\mathrm{polylog}(d)\\varepsilon^{-4}$ , $N=C(q)\\mathrm{polylog}(d)\\varepsilon^{-1}$ for some constant $C(q)$ depending on the target degree $q$ . ", "page_idx": 6}, {"type": "text", "text": "Putting things together. Combining the above theorems, we conclude that in order for two-layer NN (3.1) trained by Algorithm 1 to achieve $\\varepsilon$ population squared loss, it is sufficient to set ", "page_idx": 6}, {"type": "equation", "text": "$$\nn=T_{1}+T_{2}\\asymp C(q)\\cdot(d\\varepsilon^{-2}\\vee\\varepsilon^{-8})\\mathrm{polylog}(d),\\quad N\\asymp C(q)\\cdot\\varepsilon^{-1}\\mathrm{polylog}(d),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where constant $C(q)$ only depends on the target degree $q$ (although exponentially). Hence we may set $\\varepsilon^{-1}\\asymp\\mathrm{polylog}\\tilde{d}$ to conclude an almost-linear sample and computational complexity for learning arbitrary single-index polynomials up to $o_{d}(1)$ population error. ", "page_idx": 6}, {"type": "text", "text": "4 Proof Sketch ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we outline the high-level ideas and key steps in our derivation. ", "page_idx": 7}, {"type": "text", "text": "4.1 Monomial Transformation Reduces Information Exponent ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To prove the main theorem, we first establish the existence of nonlinear label transformation that $(i)$ reduces the information exponent, and $(i i)$ can be easily extracted from SGD updates. If we ignore desideratum $(i i)$ , then for polynomial link functions, transformations that decrease the information exponent to at most 2 have been constructed in [CM20, Section 2.1]. However, prior results are based on the thresholding function, and it is not clear if such function naturally arises from SGD with batch reuse. The following proposition shows that the effect of thresholding can also be achieved by a simple monomial transformation where the required degree can be uniformly upper bounded. ", "page_idx": 7}, {"type": "text", "text": "Proposition 6. Let $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ be any polynomial with degree up to p and $\\left\\|g\\right\\|_{L^{2}(\\gamma)}^{2}=1$ , then (i) There exists some $i\\leq C_{q}\\in\\mathbb{N}_{+}$ such that $\\operatorname{IE}(g^{i})\\leq2$ , where constant $C_{q}$ only depends on $q$ . (ii) Let $g^{\\mathrm{odd}}:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ be the odd part of $g$ with $\\|g^{\\mathrm{odd}}\\|_{L^{2}(\\gamma)}^{2}\\,\\geq\\,\\rho\\,>\\,0$ . Then there exists some $i\\leq C_{q,\\rho}\\in\\mathbb{N}_{+}$ such that $\\operatorname{IE}(g^{i})=1$ , where constant $C_{q,\\rho}$ only depends on q and $\\rho$ . ", "page_idx": 7}, {"type": "text", "text": "The proof can be found in Appendix A. We make the following remarks. ", "page_idx": 7}, {"type": "text", "text": "\u2022 The proposition implies that for any polynomial link function that is not even, there exists $i\\in\\mathbb{N}_{+}$ only depending on the degree of $\\sigma_{*}$ such that raising the function to the $i$ -the power reduces the information exponent to 1. For even $\\sigma_{*}$ , the information exponent after arbitrary transformation is at least 2, which can also be attained by monomial transformation. Furthermore, we provide a uniform upper-bound on the required degree of transformation $i$ via a compactness argument. \u2022 The advantage of working with monomial transformations is that they can be obtained from two GD steps on the same training example, by Taylor expanding the activation $\\sigma^{\\prime}$ . In Section 4.2, we build upon this observation to show that Phase I of Algorithm 1 achieves weak recovery using $n\\gtrsim d\\,\\mathrm{polylog}(d)$ samples. ", "page_idx": 7}, {"type": "text", "text": "Intuition behind the analysis. Our proof is inspired by [CM20] which introduced a (nonpolynomial) label transformation that reduces the information exponent of any degree- $\\cdot q$ polynomial to at most 2. To prove the existence of monomial transformation for the same purpose, we first show that for a fixed link function $\\sigma_{*}$ , there exists some $i$ such that the $i$ -th power of the link function has information exponent 2, which mirrors the transformation used in [CM20]. Then, we make use of the compactness of the space of link functions to define a test function and obtain a uniform bound on $i$ . As for the polynomial transformation for non-even functions, we exploit the asymmetry of $\\sigma_{*}$ to further reduce the information exponent to 1. ", "page_idx": 7}, {"type": "text", "text": "4.2 SGD with Batch Reuse Implements Polynomial Transformation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now we present a more formal discussion of (2.1) to illustrate how polynomial transformation can be utilized in batch reuse SGD. We let $\\eta^{t}\\equiv\\eta$ . When one neuron $f_{{\\pmb w}}\\bar{(}{\\pmb x})=\\sigma(\\langle{\\pmb x},{\\pmb w}\\rangle)$ is updated by two GD steps using the same sample $(\\boldsymbol{x},\\boldsymbol{y})$ , starting from $w^{0}:=\\omega$ , the alignment with $\\pmb{\\theta}$ becomes ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta,w^{2}\\rangle=\\langle\\theta,\\big[w^{1}+\\eta\\cdot y\\sigma^{\\prime}(\\langle x,w^{1}\\rangle)x\\big]\\rangle=\\langle\\theta,\\omega\\rangle+}\\\\ &{\\eta\\bigg[y\\sigma^{\\prime}(\\langle\\omega,x\\rangle)\\langle\\theta,x\\rangle+\\sum_{i=0}^{C_{\\sigma}-1}\\underbrace{(\\eta\\|x\\|^{2})^{i}y^{i+1}(i!)^{-1}(\\sigma^{\\prime}(\\langle\\omega,x\\rangle))^{i}\\sigma^{(i+1)}(\\langle\\omega,x\\rangle)\\langle\\theta,x\\rangle}_{=:\\psi_{i}}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We take $\\eta\\leq c_{\\eta}d^{-1}$ with a small constant $c_{\\eta}$ so that $\\eta\\|\\pmb{x}\\|^{2}\\ll1$ with high probability. Crucially, the strength of each term in (4.1) can vary depending on properties of the unknown link function $\\sigma_{*}$ . Hence a careful analysis is required to ensure that a suitable monomial transformation is always singled out from the gradient. We establish the following lemma on the evolution of alignment. ", "page_idx": 7}, {"type": "text", "text": "Lemma 7. Under the assumptions per Theorem $^{\\,l}$ , the following holds for either $p_{*}=1,2$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\langle\\pmb{\\theta},w^{2(t+1)}\\rangle\\geq\\langle\\pmb{\\theta},w^{2t}\\rangle+c_{\\eta}^{I}c_{\\xi}c_{\\sigma}d^{-\\frac{p_{*}}{2}\\vee1}(\\kappa^{2t})^{p_{*}-1}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}\\nu^{2t}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "See Lemma 16 for the formal version. For $p_{*}=1$ , taking expectation immediately yields that weak recovery within $(\\eta(1-\\xi)\\gamma)^{-1}=O(d)$ steps. For $p_{*}\\,=\\,2$ , $\\langle\\pmb{\\theta},\\pmb{w}_{j}^{2t}\\rangle=:\\kappa^{t}$ can be approximated by a differential equation $\\begin{array}{r}{\\frac{\\mathrm{d}\\kappa^{t}}{\\mathrm{d}t}\\,=\\,\\eta(1-\\xi)\\gamma\\kappa^{t}}\\end{array}$ . Solving this yields $\\kappa^{t}\\,=\\,\\kappa^{0}\\exp(\\eta(1\\,-\\,\\xi)\\gamma t)\\,\\approx$ $d^{-\\frac{1}{2}}\\exp(\\eta(1-\\xi)\\gamma t)$ , and weak recovery is obtained within $t\\lesssim(\\eta(1-\\xi)\\gamma)^{-1}\\cdot\\log d=O(d\\log d)$ steps, similar to the analysis in [BAGJ21]. ", "page_idx": 8}, {"type": "text", "text": "Why interpolation is needed. In our setting, the signal strength may not dominate the error coming from discarding the effect of normalization. Usually, given the gradient $-\\pmb{g}$ and projection $P_{w}=I_{d}-w w^{\\top}$ , the spherical gradient affects the alignment as $\\begin{array}{r}{\\langle\\pmb{\\theta},\\pmb{w}^{t+1}\\rangle=\\left\\langle\\pmb{\\theta},\\frac{\\pmb{w}^{t}+\\eta P_{w}\\pmb{g}}{\\lVert\\pmb{w}^{t}+\\eta P_{w}\\pmb{g}\\rVert}\\right\\rangle\\geq}\\end{array}$ $\\begin{array}{r}{\\langle\\pmb{\\theta},\\pmb{w}^{t}\\rangle+\\eta\\langle\\pmb{\\theta},\\pmb{g}\\rangle-\\frac{1}{2}\\eta^{2}\\|\\pmb{g}\\|^{2}\\langle\\pmb{\\theta},\\pmb{w}^{t}\\rangle+\\epsilon}\\end{array}$ (negligible terms), see [BAGJ21, DNGL23]. Here $\\eta\\langle\\pmb\\theta,\\pmb g\\rangle$ corresponds to the signal, and $-\\textstyle{\\frac{1}{2}}\\eta^{2}\\|g\\|^{2}\\langle\\pmb{\\theta},\\pmb{w}^{t}\\rangle$ comes from normalization. Thus, taking $\\eta$ sufficiently small, the normalization error shrinks faster than the signal. However, in our case the signal shrinks at the rate of $c_{\\eta}^{I}$ (recall that $\\eta=c_{\\eta}d^{-1},$ ), and hence taking a smaller step may not improve the signal-to-noise ratio. The interpolation step in Algorithm 1 allows us to reduce the effect of normalization without shrinking the signal too much, by ensuring $\\pmb{w}^{2(t+1)}$ stays close to $w^{2t}$ . In particular, setting $\\xi=1-\\tilde{\\eta}$ , we see that the signal is affected by a factors of $\\tilde{\\eta}$ whereas the normalization error shrinks by $\\tilde{\\eta}^{2}$ ; this allows us to boost the signal-to-noise ratio by taking $\\tilde{\\eta}$ sufficiently small. ", "page_idx": 8}, {"type": "text", "text": "4.3 Analysis of Phase II and Statistical Guarantees ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Once strong recovery is achieved for the first-layer parameters, we turn to Phase $\\mathrm{II}$ and optimize the second-layer with $\\ell_{2}$ regularization. Since the objective is strongly convex, gradient-based optimization can efficiently minimize the empirical loss. In Appendix B.6, the learnability guarantee follows from standard analysis analogous to that in [AAM22, DLS22, $\\mathrm{BES}^{+}22]$ , where we construct a \u201ccertificate\u201d second-layer $\\pmb{a}^{*}\\in\\mathbb{R}^{N}$ that achieves small loss and small norm: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}}\\Big(f_{*}(\\mathbf{x})-\\frac{1}{N}\\sum_{j=1}^{N}a_{j}^{*}\\sigma_{j}\\big(\\langle\\mathbf{w}_{j}^{T_{1}},\\mathbf{x}\\rangle+b_{j}\\big)\\Big)^{2}\\leq\\varepsilon^{*},\\quad\\|\\mathbf{a}^{*}\\|\\lesssim r^{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "from which the population loss of the regularized empirical risk minimizer can be bounded via standard Rademacher complexity argument. To construct such a certificate, we make use of the random bias units $\\{b_{j}\\}_{j=1}^{N}$ to approximate the link function $\\sigma_{*}$ as done in [DLS22, BBSS22, OSSW24]. ", "page_idx": 8}, {"type": "text", "text": "5 Beyond Polynomial Link Functions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Thus far we have shown that for polynomial single-index target functions (which satisfy $p_{*}\\leq2\\,$ ), SGD with data reuse can implement a polynomial transformation to the labels that reduces the information exponent to at most 2; consequently, the trained two-layer neural network can achieve small generalization error with $n=d\\,\\mathrm{polylog}(d)$ samples. However, as shown in [DPVLB24], there exists (non-polynomial) $\\sigma_{*}$ with generative exponent $p_{*}>2$ (i.e., label transformations cannot lower the information exponent to 2) and thus not learnable by SQ algorithms in linear sample complexity. ", "page_idx": 8}, {"type": "text", "text": "Nevertheless, for a single-index model with generative exponent $p_{*}$ , we know there exists an \u201coptimal\u201d label transformation that reduces the information exponent to $p_{*}$ . If SGD can make use of such transformation, then from the arguments in [BAGJ21], it is natural to conjecture that a sample size of $n\\simeq d^{p_{*}-1}$ is sufficient. In this section we confirm this intuition by proving that SGD with batch reuse (Algorithm 1) indeed matches this complexity. The following lemma is an analogue of Proposition 6 stating that polynomial transformations are sufficient to lower the information exponent. ", "page_idx": 8}, {"type": "text", "text": "Lemma 8. Given link function $\\sigma_{*}$ with generative exponent $\\smash{p_{*}\\in\\mathbb{N}_{+}^{}}$ . Suppose we can take an orthonormal polynomial basis $\\{\\phi_{k}\\}_{k}$ for the space $L^{2}(\\bar{P}_{y})$ with inner product $\\begin{array}{r l}{\\langle f,g\\rangle}&{{}=}\\end{array}$ $\\mathbb{E}_{y=\\sigma_{*}(z)}[f(y)g(y)]$ . Then there exists some degree $I\\in\\mathbb{N}$ such that $\\mathrm{IE}(\\sigma_{*}^{I})=p_{*}$ . ", "page_idx": 8}, {"type": "text", "text": "We outline the differences and additional technical challenges to handle the $\\mathrm{GE}(\\sigma_{\\ast})>2$ setting. ", "page_idx": 8}, {"type": "text", "text": "\u2022 For general $L^{2}$ link functions $\\sigma_{*}$ , we can no longer make use of the compactness argument (see proof of Proposition 6) to upper bound the degree of monomial transformation. Hence in Lemma 8 we do not state a uniform upper bound on the required degree $I$ , unlike the polynomial setting. \u2022 Any link function with $p_{*}>2$ cannot be polynomial, and hence we cannot achieve low generalization error using a neural network with polynomial nonlinearity. We therefore need to use an activation function with universal function approximation ability. ", "page_idx": 8}, {"type": "text", "text": "5.1 Sample Complexity for Weak Recovery ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We first show that Algorithm 1 achieves weak recovery with a complexity governed by the generative exponent of the link function $p_{*}\\,=\\,\\mathrm{GE}(\\sigma_{*})$ . Similar to Section 3.2, we make use of randomized activation functions to ensure the desired label transformation is encoded \u2014 we defer the conditions on the student activation to Appendix B.1.2. ", "page_idx": 9}, {"type": "text", "text": "Proposition 9. Suppose the link function $\\sigma_{*}$ has generative exponent $p_{*}$ , and let $I\\ \\in\\ \\mathbb{N}_{+}$ be the smallest degree of monomial transformation that lowers the information exponent to $p_{*}$ (i.e., $\\mathrm{IE}(\\sigma_{*}^{I})=p)$ . We can find a student activation function $\\sigma$ depending only on $p,p_{*}$ and $I$ , such that if we take $\\eta^{2t},\\eta^{2t+1}=c_{\\eta}N d^{-1}$ , $\\xi^{2(t+1)}=1-c_{\\xi}d^{-(p_{*}-2)}{+}/2$ for small $c_{\\eta},c_{\\xi}=o_{d}(1)$ , and set ", "page_idx": 9}, {"type": "equation", "text": "$$\nT_{1,1}\\simeq c_{\\xi}^{-1}\\left\\{\\!\\!\\begin{array}{l l}{{d}}&{{(i f\\,p_{*}=1)}}\\\\ {{d(\\log d)}}&{{(i f\\,p_{*}=2)}}\\\\ {{d^{p_{*}-1}}}&{{(i f\\,p_{*}\\geq3),}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "then if the initial alignment $\\langle{\\pmb w}^{0},{\\pmb\\theta}\\rangle\\ge2c_{\\eta}^{-1}d^{-1/2}$ , there exists $\\tau_{*}\\leq T_{1,1}$ such that for all $\\tau\\geq\\tau_{*}$ , ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\langle{\\pmb w}^{2\\tau},{\\pmb\\theta}\\rangle\\geq\\tilde{\\Theta}(1),\\quad w i t h\\,p r o b a b i l i t y\\;1-o_{d}(1).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Proposition 9 is a generalization of Theorem 1 beyond polynomial $\\sigma_{*}$ (the proof of both results are presented in Appendix B.3,B.4), and can be interpreted as an SQ counterpart to [BAGJ21]: we establish a sufficient sample size of $n\\simeq d^{(p_{*}-1)\\vee1}$ for Algorithm 1 to exit the search phase, which is parallel to the $n\\simeq d^{(p\\overline{{{-}}}1)\\vee1}$ rate for one-pass SGD (note that our rates are slightly sharper due to logarithmic factors removed, since c\u03be\u2212 can grow arbitrarily slowly with $d$ ). For high generative exponent $\\sigma_{*}$ with $p_{*}>2$ , we no longer match the information theoretically optimal sample complexity $n\\asymp d$ , which is consistent with the computational-to-statistical gap observed in [DPVLB24]. ", "page_idx": 9}, {"type": "text", "text": "5.2 Generalization Error Guarantee ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "After Phase I of Algorithm 1, we learn the unknown link function $\\sigma_{*}$ via training the second-layer. To approximate non-polynomial functions, we introduce a ReLU component in the student nonlinearity $\\sigma$ (see Lemma 12 for discussions), and make use of the approximation result for the (univariate) ReLU kernel in [BBSS22], which handles general $\\sigma_{*}$ whose second derivative has bounded 4th moment. Combining the above, we arrive at the following end-to-end guarantee for learning single-index models with arbitrary generative exponent using SGD training of neural network. ", "page_idx": 9}, {"type": "text", "text": "Proposition 10 (Informal). Suppose the link function $\\sigma_{*}$ has generative exponent $p_{*}\\ \\in\\ \\mathbb{N}_{*}$ and satisfies $\\sigma_{*},\\sigma_{*}^{\\prime\\prime}\\;\\in\\;L^{4}(\\gamma)$ . For appropriately chosen activation function $\\sigma$ (see Appendix B.1.2), the neural network (3.1) optimized by Algorithm 1 achieves small population loss $\\mathbb{E}_{\\pmb{x}}[(f_{\\hat{\\pmb{\\Theta}}}(\\pmb{x})-$ $f_{*}({\\pmb x}))^{2}]=o_{d,\\mathbb{P}}(1),$ , with a sample complexity of $n=\\tilde{\\Theta}(d^{(p_{*}-1)\\vee1})$ . ", "page_idx": 9}, {"type": "text", "text": "See Appendix B.6 for the full statement with $\\varepsilon$ dependence. This proposition confirms that weak recovery (established in Proposition 9) is the bottleneck in learning single-index models, as the total sample size required for Algorithm 1 to achieve vanishing test error also scales with $d^{(p_{*}-1)\\vee1}$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We showed that a two-layer neural network (3.1) trained by SGD with reused batch can learn singleindex model (with generative exponent $p_{*}$ ) using $n\\simeq d^{(p_{*}-1)\\vee1}$ samples and compute; in particular, when the link function $\\sigma_{*}$ is polynomial, we established a sample complexity of $n=\\tilde{O}(d\\varepsilon^{-2})$ to achieve $\\varepsilon$ population loss, which is almost information theoretically optimal. Our analysis is based on the observation that by reusing the same training data twice in the gradient computation, a noncorrelational term arises in the SGD update that transforms the labels (despite the loss function not modified). We proved that monomial transformations that lower the information exponent of $\\sigma_{*}$ can be extracted by Taylor-expanding the SGD update; then we showed via careful analysis of the trajectory that strong recovery and low population loss is achieved under suitable activation function. ", "page_idx": 9}, {"type": "text", "text": "Interesting future directions include extension to multi-index models [BAGJ22, ABA22, BBPV23, CWPPS23], hierarchical polynomials [AZL19, NDL23], and additive models [OSSW24]. Also, the SGD algorithm that we employ requires a layer-wise training procedure and a specific batch reuse schedule; one may therefore ask if standard multi-pass SGD training of all parameters simultaneously [Gla23] (as reported in Figure 1) also achieves the same statistical efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors thank Gerard Ben Arous, Joan Bruna, Alex Damian, Marco Mondelli, and Eshaan Nichani for the discussions and feedback on the manuscript. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-0304, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. KO was partially supported by JST ACT-X (JPMJAX23C4). TS was partially supported by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015). This research is unrelated to DW\u2019s work at xAI. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The mergedstaircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022.   \n[AAM23] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory, pages 2552\u20132623. PMLR, 2023.   \n[ABA22] Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. Advances in Neural Information Processing Systems, 35:17188\u201317201, 2022.   \n$[\\mathrm{ADK}^{+}24]$ Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. arXiv preprint arXiv:2405.15459, 2024. [AZ18] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal of Machine Learning Research, 18(221):1\u201351, 2018.   \n[AZL19] Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? Advances in Neural Information Processing Systems, 32, 2019. [Bac17] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine Learning Research, 18(1):629\u2013681, 2017.   \n[BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. The Journal of Machine Learning Research, 22(1):4788\u20134838, 2021.   \n[BAGJ22] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for sgd: Effective dynamics and critical scaling. Advances in Neural Information Processing Systems, 35:25349\u201325362, 2022.   \n[BBPV23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multiindex models with gradient flow. arXiv preprint arXiv:2310.19793, 2023.   \n[BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning singleindex models with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768\u20139783, 2022.   \n$[\\mathbf{BES}^{+}22]$ Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems, 35:37932\u201337946, 2022.   \n$[\\mathbf{B}\\mathbf{E}\\mathbf{S}^{+}23]$ Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning in the presence of low-dimensional structure: A spiked random matrix perspective. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [BF02] Nader H Bshouty and Vitaly Feldman. On using extended statistical queries to avoid membership queries. Journal of Machine Learning Research, 2(Feb):359\u2013395, 2002.   \n$[\\mathbf{B}\\mathbf{K}\\mathbf{M}^{+}19]$ Jean Barbier, Florent Krzakala, Nicolas Macris, Le\u00b4o Miolane, and Lenka Zdeborova\u00b4. Optimal errors and phase transitions in high-dimensional generalized linear models. Proceedings of the National Academy of Sciences, 116(12):5451\u20135460, 2019. [BL20] Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. In International Conference on Learning Representations, 2020. [BMZ23] Raphae\u00a8l Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers neural networks. arXiv preprint arXiv:2303.00055, 2023. [CCM11] Seok-Ho Chang, Pamela C Cosman, and Laurence B Milstein. Chernoff-type bounds for the Gaussian error function. IEEE Transactions on Communications, 59(11):2939\u20132944, 2011. [CM20] Sitan Chen and Raghu Meka. Learning polynomials in few relevant dimensions. In Conference on Learning Theory, pages 1161\u20131227. PMLR, 2020. [COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances in Neural Information Processing Systems, 32, 2019.   \n[CWPPS23] Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi. Hitting the high-dimensional notes: An ode for sgd learning dynamics on glms and multi-index models. arXiv preprint arXiv:2308.08977, 2023. [DH18] Rishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In Conference On Learning Theory, pages 1887\u20131930. PMLR, 2018. [DH24] Rishabh Dudeja and Daniel Hsu. Statistical-computational trade-offs in tensor pca and related problems via communication complexity. The Annals of Statistics, 52(1):131\u2013156, 2024.   \n$[\\mathrm{DKL}^{+}23]$ Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time. arXiv preprint arXiv:2305.18270, 2023. [DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413\u20135452. PMLR, 2022.   \n[DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D. Lee. Smoothing the landscape boosts the signal for SGD: Optimal sample complexity for learning single index models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[DPVLB24] Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The computational complexity of learning gaussian single-index models. arXiv preprint arXiv:2403.05529, 2024. $[\\mathrm{DTA}^{+}24]$ Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova\u00b4, and Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024. [DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance of common kernels prevents generalization in high dimensions. In International Conference on Machine Learning, pages 2804\u20132814. PMLR, 2021. [Gla23] Margalit Glasgow. Sgd finds then tunes features in two-layer neural networks with near-optimal sample complexity: A case study in the xor problem. arXiv preprint arXiv:2309.15111, 2023.   \n[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. The Annals of Statistics, 49(2):1029\u20131054, 2021.   \n[HSSVG21] Daniel Hsu, Clayton H Sanford, Rocco Servedio, and Emmanouil Vasileios VlatakisGkaragkounis. On the approximation power of two-layer networks of random relus. In Conference on Learning Theory, pages 2423\u20132461. PMLR, 2021. [JGH18] Arthur Jacot, Franck Gabriel, and Cle\u00b4ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571\u20138580, 2018. [Kea98] Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983\u20131006, 1998. [KMS20] Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Probabilistic variants of dimensional and margin complexity. In Conference on Learning Theory, pages 2236\u20132262. PMLR, 2020.   \n$[\\mathrm{MHPG^{+}}23]$ Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with SGD. In The Eleventh International Conference on Learning Representations, 2023.   \n[MHWSE23] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A. Erdogdu. Gradient-based feature learning under structured data. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023), 2023. [MM18] Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with applications to phase retrieval. In Conference On Learning Theory, pages 1445\u20131450. PMLR, 2018.   \n$[\\mathbf{M}Z\\mathbf{D}^{+}23]$ Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma. Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time. Advances in Neural Information Processing Systems, 36, 2023. [NDL23] Eshaan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks. Advances in Neural Information Processing Systems, 36, 2023. [O\u2019D14] Ryan O\u2019Donnell. Analysis of Boolean Functions. Cambridge University Press, 2014. [OSSW24] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations. In Conference on Learning Theory. PMLR, 2024. [Rey20] Lev Reyzin. Statistical queries and statistical algorithms: Foundations and applications. arXiv preprint arXiv:2004.00557, 2020. [Sch80] Jacob T Schwartz. Fast probabilistic algorithms for verification of polynomial identities. Journal of the ACM (JACM), 27(4):701\u2013717, 1980. [TS24] Shokichi Takakura and Taiji Suzuki. Mean-field analysis on two-layer neural networks from a kernel perspective. arXiv preprint arXiv:2403.14917, 2024. [WWF24] Zhichao Wang, Denny Wu, and Zhou Fan. Nonlinear spiked covariance matrices and signal propagation in deep neural networks. In Conference on Learning Theory. PMLR, 2024. [YS19] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. Advances in Neural Information Processing Systems, 32, 2019. [ZLBH19] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps forward, 1 step back. Advances in neural information processing systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Introduction 1.1 Our Contributions 2 ", "page_idx": 13}, {"type": "text", "text": "2 Problem Setting and Prior Works 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "2.1 Complexity of Learning Single-index Models 3   \n2.2 Can Gradient Descent Go Beyond Correlational Queries? . . 4   \n2.3 Challenges in Establishing Statistical Guarantees 5 ", "page_idx": 13}, {"type": "text", "text": "3 Learning Polynomial $f_{*}$ in Linear Sample Complexity 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "3.1 Training Algorithm 6   \n3.2 Convergence and Sample Complexity 6 ", "page_idx": 13}, {"type": "text", "text": "4 Proof Sketch 8 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "4.1 Monomial Transformation Reduces Information Exponent 8   \n4.2 SGD with Batch Reuse Implements Polynomial Transformation 8   \n4.3 Analysis of Phase II and Statistical Guarantees 9 ", "page_idx": 13}, {"type": "text", "text": "5 Beyond Polynomial Link Functions 9 ", "page_idx": 13}, {"type": "text", "text": "5.1 Sample Complexity for Weak Recovery 10   \n5.2 Generalization Error Guarantee . 10 ", "page_idx": 13}, {"type": "text", "text": "6 Conclusion and Future Directions 10 ", "page_idx": 13}, {"type": "text", "text": "A Polynomial Transformation 15 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof for Even Functions (i) 15   \nA.2 Proof for Non-even Functions (ii) 16   \nA.3 Proof for Non-Polynomial Functions 17 ", "page_idx": 13}, {"type": "text", "text": "B SGD with Reused Batch 17 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Assumptions on Link Function 18   \nB.2 Initialization . . . 22   \nB.3 Weak Recovery: Population Update 22   \nB.4 Weak Recovery: Stochastic Update . 26   \nB.5 From Weak Recovery to Strong Recovery 29   \nB.6 Second Layer Training . . 31 ", "page_idx": 13}, {"type": "text", "text": "A Polynomial Transformation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 6. We use a thresholding and compactness argument inspired by [CM20]. ", "page_idx": 14}, {"type": "text", "text": "A.1 Proof for Even Functions $(i)$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We divide the analysis into the following steps. ", "page_idx": 14}, {"type": "text", "text": "(i-1): Monomials reducing the information exponent. Define $\\tau(f)\\,=\\,\\mathrm{max}_{-2\\leq t\\leq2}\\left|f(t)\\right|$ . This entails that if $|f(t)|\\geq\\tau(f)$ , then we have $|t|>2$ . ", "page_idx": 14}, {"type": "text", "text": "Consider the following expectation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\left[\\left(\\frac{f(t)}{2\\tau(f)}\\right)^{i}(t^{2}-1)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We evaluate the case when $i$ is even. (A.1) can be lower bounded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{A}.1)=\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\bigg[1\\big1/(t(t))\\geq2\\tau(f)\\big|\\bigg(\\frac{f(t)}{2\\tau(f)}\\big)^{i}(t^{2}-1)\\bigg]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\bigg[1\\big1/(t/t)\\leq|f(t)|<2\\tau(f)\\big|\\bigg(\\frac{f(t)}{2\\tau(f)}\\big)^{i}(t^{2}-1)\\bigg]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\bigg[1\\big1/\\big(t/t)|<\\tau(f)\\big|\\bigg(\\frac{f(t)}{2\\tau(f)}\\big)^{i}(t^{2}-1)\\bigg]}\\\\ &{\\qquad\\geq\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\bigg[1\\big1/\\big(t/t\\big)\\geq2\\tau(f)\\big|\\bigg(\\frac{2\\tau(f)}{2\\tau(f)}\\big)^{i}(t^{2}-1)\\bigg]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\bigg[1\\big/\\big(f)\\leq|f(t)|<2\\tau(f)\\big|\\bigg(\\frac{f(t)}{2\\tau(f)}\\bigg)^{i}(2^{2}-1)\\bigg]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\bigg[1\\big1/\\big(f)|<\\tau(f)\\big|\\bigg(\\frac{\\tau(f)}{2\\tau(f)}\\bigg)^{i}(0^{2}-1)\\bigg]}\\\\ &{\\qquad\\geq3\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\big[1\\big/\\big(t)\\geq2\\tau(f)\\big]-2\\tau^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\mathbb{P}[|f(t)|\\,\\geq\\,2\\tau(f)]$ is positive (since $f$ is polynomial) and independent of $i$ , while $2^{-i}$ decays to 0 as $i$ increases. Therefore, for sufficiently large $\\textit{i}\\in\\mathbb{N}$ , (A.1) is positive and hence $\\operatorname{IE}(\\bar{f}^{i})\\leq2$ . The subsequent analysis aims to provide an upper bound on $i$ . ", "page_idx": 14}, {"type": "text", "text": "(i-2): Construction of test function. We introduce the notation $H(\\cdot;j)$ which takes any function (in $L^{1}$ ) and returns its $j$ -th Hermite coefficient. We consider the following test function: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{H}(f):=\\sum_{i=2}^{\\infty}\\bigg(\\frac{H(f^{i};2)}{2^{\\frac{i}{2}}(2i-1)^{\\frac{i q}{2}}}\\bigg)^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(i-3): Lower bound of test function via compactness. Let ${\\mathcal{F}}_{q}$ be a set of polynomials with degree up to $q$ with unit $L^{2}$ norm. Because $\\mathcal{H}(f)$ is positive for any $f\\in\\mathcal F_{q}$ , $H(f^{i};2)$ is continuous with respect to $f$ , and ${\\mathcal{F}}_{q}$ is a compact set, $\\operatorname{inf}_{f\\in\\mathcal{F}_{q}}\\mathcal{H}(f)$ admits a minimum value $\\mathcal{H}_{0}$ which is positive. ", "page_idx": 14}, {"type": "text", "text": "(i-4): Conclusion via hypercontractivity. Because $f$ is a polynomial with degree at most $q$ , Gaussian hypercontractivity [O\u2019D14] yields that ", "page_idx": 14}, {"type": "equation", "text": "$$\n2H(f^{i};2)^{2}\\leq\\mathbb{E}_{t\\sim N(0,1)}\\left[(f(t))^{2i}\\right]\\leq(2i-1)^{i q}\\big(\\mathbb{E}_{t\\sim N(0,1)}\\left[f(t)^{2}\\right]\\big)^{i}=(2i-1)^{i q}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, for all polynomials in ${\\mathcal{F}}_{q}$ , a partial sum of (A.2) is uniformly bounded by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigg|\\sum_{i=j}^{\\infty}\\left(\\frac{H(f^{i};2)}{2^{\\frac{i}{2}}(2i-1)^{\\frac{i q}{2}}}\\right)^{2}\\bigg|\\le\\sum_{i=j}^{\\infty}2^{-i-1}=2^{-j}\\rightarrow0\\quad(j\\rightarrow\\infty).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining this with the fact that $\\mathcal{H}(f)\\,\\geq\\,\\mathcal{H}_{0}\\,>\\,0$ , we know that there exists some $C_{q}\\,\\leq\\,1\\,+$ $\\log_{2}(\\mathcal{H}_{0}^{-1})$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i=2}^{C_{q}}\\left(\\frac{H(f^{i};2)}{2^{\\frac{i}{2}}(2i-1)^{\\frac{i q}{2}}}\\right)^{2}>\\frac{1}{2}\\mathcal{H}_{0}>0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all polynomials in ${\\mathcal{F}}_{q}$ . This means that there is at least one $i\\leq C_{q}$ such that $H(f^{i};2)\\neq0$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof for Non-even Functions $(i i)$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "(ii-1): Monomials reducing the information exponent. We prove that some exponentiation of $g\\;:=\\;f^{2}$ has non-zero first Hermite coefficient. Denote $g^{\\mathrm{odd}}$ as the odd part of $g$ , and similarly $g^{\\mathrm{even}}$ . Let $v(g)\\in\\mathbb{R}_{+}$ be the value at which the followings hold: ", "page_idx": 15}, {"type": "text", "text": "(a) $g^{\\mathrm{odd}}(t)>0$ for all $t\\geq v(g)$ and $g^{\\mathrm{odd}}(t)<0$ for all $t\\leq-\\upsilon(g)$ .   \n(b) $g^{\\mathrm{even}}(t)>|g^{\\mathrm{odd}}(t)|$ for all $t\\geq\\upsilon(g)$ and $t\\leq-\\upsilon(g)$ .   \n(c) For for all $t\\geq\\upsilon(g)$ and $t\\leq-\\upsilon(g)$ , $g(s)=g(t)$ (as an equation of $s$ ) only has two real-valued solutions with opposing signs. ", "page_idx": 15}, {"type": "text", "text": "Such threshold $\\ensuremath{\\boldsymbol{\\upsilon}}(\\ensuremath{\\boldsymbol{g}})$ exists because the tail of $g\\,=\\,f^{2}$ is dominated by the highest degree which is even. Then, we let $\\tau(g)=\\operatorname*{max}_{-\\upsilon(g)\\leq t\\leq\\upsilon(g)}|g(t)|$ . ", "page_idx": 15}, {"type": "text", "text": "Consider the following expectation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\left[\\left(\\frac{g(t)}{2\\tau(g)}\\right)^{i}\\!t\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(A.3) is decomposed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{A.3})=\\mathbb{E}_{t\\sim N(0,1)}\\Bigg[\\mathbb{I}\\big[|g(t)|\\geq3\\tau(f)\\big]\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}t\\Bigg]}\\\\ &{\\qquad\\qquad+\\,\\mathbb{E}_{t\\sim N(0,1)}\\Bigg[\\mathbb{I}\\big[2\\tau(g)\\leq|g(t)|<3\\tau(g)\\big]\\bigg(\\frac{g(t)}{3\\tau(f)}\\bigg)^{i}t\\Bigg]}\\\\ &{\\qquad\\qquad+\\,\\mathbb{E}_{t\\sim N(0,1)}\\Bigg[\\mathbb{I}\\big[|g(t)|<2\\tau(g)\\big]\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}t\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We first evaluate the first term. Because of (c), $g(t)\\,=\\,3\\tau(f)$ has two real-valued solutions $\\alpha\\,<$ $0<\\beta$ . Because of (a) and (b), $g(\\beta)=g^{\\mathrm{even}}(\\beta)+g^{\\mathrm{odd}}(\\beta)=3\\tau(f)>g^{\\mathrm{even}}(-\\beta)+g^{\\mathrm{odd}}(-\\beta)=$ $g^{\\mathrm{odd}}(-\\beta)$ . Because $\\begin{array}{r}{\\operatorname*{lim}_{t\\to-\\infty}g^{\\mathrm{odd}}(t)=+\\infty}\\end{array}$ , and $\\alpha$ is the only solution in $t<0$ , we have $\\alpha<-\\beta$ . Moreover, for all $t>\\beta$ , we have $g(t)=g^{\\mathrm{even}}(t)+g^{\\mathrm{odd}}(t)>g^{\\mathrm{even}}(-t)+g^{\\mathrm{odd}}(-t)=g^{\\mathrm{odd}}(-t)$ . Combining the above, the first term of (A.4) is bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t\\sim N(0,1)}\\Bigg[\\mathbb{1}[|g(t)|\\geq3\\tau(f)]\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}t\\Bigg]}\\\\ &{=\\mathbb{E}_{t\\sim N(0,1)}\\Bigg[\\mathbb{1}[\\beta\\leq t\\leq-\\alpha]\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}t\\Bigg]+\\mathbb{E}_{t\\sim N(0,1)}\\Bigg[\\mathbb{1}[t\\geq-\\alpha]\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}t\\Bigg]}\\\\ &{\\quad+\\mathbb{E}_{t\\sim N(0,1)}\\Bigg[\\mathbb{1}[t\\leq\\alpha]\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}t\\Bigg]}\\\\ &{=\\mathbb{E}_{t\\sim N(0,1)}\\big[\\mathbb{1}[\\beta\\leq t\\leq-\\alpha]t\\big]+\\mathbb{E}_{t\\sim N(0,1)}\\bigg[\\mathbb{1}[t\\geq-\\alpha]\\bigg(\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}-\\bigg(\\frac{g(-t)}{3\\tau(g)}\\bigg)^{i}\\bigg)t\\Bigg]}\\\\ &{>\\beta\\mathbb{P}_{t\\sim N(0,1)}\\big[\\beta\\leq t\\leq-\\alpha\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Following the exact same reasoning, we know that the second term of (A.4) is positive. Finally, the third term which is bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\left[\\mathbb{1}[|g(t)|<2\\tau(g)]\\bigg(\\frac{g(t)}{3\\tau(g)}\\bigg)^{i}t\\right]\\ge-\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\left[\\mathbb{1}[|g(t)|<2\\tau(g)]|t|\\right]\\bigg(\\frac{2}{3}\\bigg)^{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Putting things together, ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\mathrm{A}.4)>\\beta\\mathbb{P}_{t\\sim\\mathcal{N}(0,1)}\\left[\\beta\\leq t\\leq-\\alpha\\right]-\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}\\left[\\mathbb{1}\\left[\\vert g(t)\\vert<2\\tau(g)\\vert\\vert t\\vert\\right]\\left(\\frac{2}{3}\\right)^{i}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first term is independent of $i$ and positive, while the second term goes to zero as $i$ grows.   \nTherefore, there exists some $i$ such that $\\operatorname{\\bar{IE}}(g^{i};1)=1$ . ", "page_idx": 15}, {"type": "text", "text": "(ii-2): Construction of test function. This time we consider the following function: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{H}(f):=\\sum_{i=2}^{\\infty}\\bigg(\\frac{H(f^{i};1)}{2^{\\frac{i}{2}}\\big(2i-1\\big)^{\\frac{i q}{2}}}\\bigg)^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(ii-3): Lower bound of test function via compactness. Let ${\\mathcal{F}}_{q}$ be a set of unit $L^{2}$ -norm polynomials with degree up to $q$ and $\\mathbb{E}_{t\\sim\\mathcal{N}(0,1)}[f^{\\mathrm{odd}}(t)^{2}]\\geq c$ . Since $\\mathcal{H}(f)$ is always positive for ${\\mathcal{F}}_{q}$ , $\\mathcal{H}(f)$ is continuous with respect to $f$ , and ${\\mathcal{F}}_{q}$ is a compact set, $\\operatorname{inf}_{f\\in\\mathcal{F}_{q}}\\mathcal{H}(f)$ has the minimum value $\\mathcal{H}_{0}$ that is positive. Note that $\\mathcal{H}(f)$ might depends on $c$ . ", "page_idx": 16}, {"type": "text", "text": "(ii-4): Conclusion via hypercontractivity. Using the same argument as in (i), we conclude that there exists some $C_{q,c}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=2}^{C_{q}}\\left(\\frac{H(f^{i};1)}{2^{i}(2i-1)^{\\frac{i q}{2}}}\\right)^{2}>\\frac{1}{2}\\mathcal{H}_{0}>0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Because $\\mathcal{H}_{0}$ depends on $c$ , $C_{q,c}$ depends on $c$ as well as $q$ . ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof for Non-Polynomial Functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For non-polynomial link functions, we note that similar to [DPVLB24], the existence of polynomial basis is needed to exclude extreme cases, and we cannot upper bound the required degree $I$ because general link functions are not included in a compact space. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 8. The derivation is analogous to [DPVLB24, Lemma F.14]. Let $z\\sim\\mathcal{N}(0,1)$ and $\\textit{y}=\\textit{\\textbf{\\sigma}}\\big(z\\big)$ . We define $\\begin{array}{r}{\\zeta_{p_{*}}(y)~=~\\mathbb{E}[\\frac{1}{\\sqrt{p_{*}!}}{\\mathsf{H}}\\mathsf{e}_{p_{*}}(z)|y]}\\end{array}$ and its basis expansion $\\zeta_{p_{\\ast}}(y)\\;=$ $\\textstyle\\sum_{k=0}^{\\infty}v_{k}\\phi_{k}$ . Let $K$ be a smallest integer such that $\\boldsymbol{v}_{k}~\\neq~0$ . Then, there exists an integer with $I\\leq K$ such that $\\mathrm{IE}(y^{I})=p_{*}$ . Indeed, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\phi_{K}(y)\\mathsf{H}\\mathsf{e}_{p_{*}}(z)]=\\mathbb{E}_{y}[\\Phi_{K}(y)\\mathbb{E}_{z|y}[\\mathsf{H}\\mathsf{e}_{p_{*}}(z)|y]]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{y}\\bigg[\\Phi_{K}(y)\\displaystyle\\sum_{k=0}^{K}v_{k}\\phi_{k}(y)\\bigg]=v_{K}\\neq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means that at least one of $y,y^{2},\\cdot\\cdot\\cdot\\,,y^{K}$ yields a non-zero $p_{*}$ -th Hermite coefficient. ", "page_idx": 16}, {"type": "text", "text": "B SGD with Reused Batch ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we show that Algorithm 1 learns single-index models in $\\tilde{O}(d^{1\\vee(p_{*}-1)})$ samples with high probability. The algorithm trains the first layer for $T_{1}$ SGD steps, where we sample a new data point in every two steps. The first layer training is further divided into two phases: weak recovery $(w^{\\top}\\pmb{\\theta}\\gtrsim1)$ and strong recovery $(\\|w-\\pmb\\theta\\|\\lesssim\\varepsilon)$ . Then, we learn the second layer parameters. ", "page_idx": 16}, {"type": "text", "text": "Specifically, Section B.2 shows that at initialization, a (nearly) constant fraction of neurons has alignment $\\bar{\\b{w}}^{\\top}\\pmb{\\theta}$ beyond a certain threshold. We focus on such neurons in the first phase of training. Section B.3 lower bounds the expected update of alignment ${\\pmb w}^{\\top}{\\pmb\\theta}$ of two gradient steps, and Section B.4 establishes that the neurons achieve weak recovery within $2T_{1,1}=\\bar{\\tilde{O}}(d^{1\\vee(p_{*}-1)}\\big)$ steps. Section B.5 discusses how to convert weak recovery to strong recovery using $2T_{1,2}\\,=\\,\\tilde{O}(d\\varepsilon^{-2})$ more steps. We let $T_{1}\\,=\\,2T_{1,1}\\,+\\,2T_{1,2}$ . Finally, Section B.6 analyzes second layer training and concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "In the following proofs, we use several constants, which depends on $d$ at most at most polylogarithmically. Specifically, asymptotic strength of the constants is ordered as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1\\simeq c_{\\sigma}\\simeq C_{1}\\lesssim\\left\\{c_{\\eta}^{-1}\\simeq C_{2}\\lesssim\\mathrm{poly}(c_{\\eta}^{-1})\\lesssim\\left\\{\\stackrel{c_{1}^{-1}\\simeq C_{3}}{c_{2}^{-1}}\\right\\}\\lesssim\\left\\{\\stackrel{\\delta^{-1}\\mathrm{poly}(c_{\\eta}^{-1})\\lesssim c_{\\xi}^{-1}}{\\mathrm{poly}(c_{1}^{-1})\\lesssim\\bar{c}_{\\eta}^{-1}}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\lesssim\\mathrm{polylog}(d)=C_{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $c_{\\eta}$ and $\\delta$ should satisfy $\\begin{array}{r}{\\operatorname*{lim}_{d\\rightarrow\\infty}c_{\\eta}=\\operatorname*{lim}_{d\\rightarrow\\infty}\\delta=0}\\end{array}$ , but the convergence can be arbitrarily slow, (e.g., as slow as $1/\\log\\log\\log\\cdot\\cdot\\cdot\\log d)$ . This requirement comes from the fact that we do not know the exact value of $H(\\sigma_{*}^{I};p_{*})$ . To ensure that one signal term (from the Taylor series) is isolated, taking $\\eta\\asymp d^{-1}$ with a sufficiently small constant is insufficient but $\\eta\\asymp\\dot{c}_{\\eta}d^{-1}$ with arbitrarily slow $c_{\\eta}$ suffices. Also, to guarantee that the failure probability is $o_{d}(1)$ , we require $\\delta$ to be $o_{d}(1)$ . $c_{\\xi}$ can also decay arbitrarily slowly, as long as it satisfies $c_{\\xi}\\lesssim\\delta\\mathrm{poly}(c_{\\eta}^{-1})$ . $C_{4}=\\mathrm{polylog}(d)$ will be used to represent any polylogarithmic factor that comes from high probability bounds. ", "page_idx": 17}, {"type": "text", "text": "For the first-layer training, we can reduce the argument into training of one neuron using the correlation loss as follows. At each step, the gradient update (Line 8 of Algorithm 1) is written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{j}^{t+1}\\leftarrow w_{j}^{t}-\\eta^{t}\\tilde{\\nabla}_{w}\\left((\\boldsymbol{f}_{\\Theta}(\\boldsymbol{x})-\\boldsymbol{y})^{2}\\right)}\\\\ &{\\qquad=w_{j}^{t}-\\eta^{t}\\tilde{\\nabla}_{w}\\bigg(\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}a_{j}\\sigma_{j}({w_{j}^{t}}^{\\top}\\boldsymbol{x})\\bigg)^{2}+2\\eta_{j}^{t}\\tilde{\\nabla}_{w}\\bigg(y\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}a_{j}\\sigma_{j}({w_{j}^{t}}^{\\top}\\boldsymbol{x})\\bigg)}\\\\ &{\\qquad=w_{j}^{t}-\\frac{2\\eta^{t}c_{a}^{2}}{N}\\bigg(\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{j=1}^{N}\\sigma_{j}({w_{j}^{t}}^{\\top}\\boldsymbol{x})\\bigg)\\big(\\tilde{\\nabla}_{w}\\sigma_{j}({w_{j}^{t}}^{\\top}\\boldsymbol{x})\\big)+\\frac{2\\eta^{t}c_{a}}{N}y\\big(\\tilde{\\nabla}_{w}\\sigma_{j}({w_{j}^{t}}^{\\top}\\boldsymbol{x})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "While the second term scales with $\\eta^{t}c_{a}^{2}N^{-1}$ , the third term scales with $\\eta^{t}c_{a}N^{-1}$ . Thus, by setting $c_{a}$ sufficiently small, we can ignore the interaction between neurons. We will show that the strength of the signal in the direction of $\\pmb{\\theta}$ is at least $(\\kappa_{j}^{t})^{p_{*}-1}\\gtrsim d^{-\\frac{p_{*}-1}{2}}$ (up to a polylogarithmic factor, and $p_{*}=\\mathrm{GE}(\\sigma_{*}))$ ). On the other hand, we can easily see that $\\begin{array}{r}{\\pmb{\\theta}^{\\top}\\big(\\frac{1}{N}\\sum_{j=1}^{N}\\sigma_{j}\\big(\\pmb{w}_{j}^{t}^{\\top}\\pmb{x}\\big)\\big)\\big(\\tilde{\\nabla}_{\\pmb{w}}\\sigma_{j}\\big(\\pmb{w}_{j}^{t}^{\\top}\\pmb{x}\\big)\\big)}\\end{array}$ is bounded by $\\tilde{O}(1)$ with high probability. Therefore, by simply letting $c_{a}\\,=\\,\\tilde{\\Theta}(d^{-\\,\\frac{p_{*}-1}{2}})$ , we can ignore the effect of the second term in (B.1). Moreover, for simplicity, we will reparameterize $\\frac{2\\eta^{t}c_{a}}{N}$ as $\\eta^{t}$ below. Consequently, we may analyze the following update ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{w}_{j}^{t+1}\\leftarrow\\pmb{w}_{j}^{t}+\\eta^{t}\\tilde{\\nabla}_{\\pmb{w}}\\big(y\\sigma_{j}(\\pmb{w}_{j}^{t}^{\\top}\\pmb{x})\\big),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "instead of Line 8 of Algorithm 1. Since there is no interaction between neurons now, we omit the subscript $j$ when the context is clear. ", "page_idx": 17}, {"type": "text", "text": "B.1 Assumptions on Link Function ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The analysis consists of three different phases: weak recovery and strong recovery of the first-layer weights, and approximation of the link function (ridge regression of the second-layer). Each phase requires different assumptions on the activation functions, depending on the link function. Before starting the analysis, we decompose Assumptions 2 and 3 and clarify which conditions are needed in each phase. We prove that instead of using a specific activation function tailored to different link functions, a randomized activation function satisfies all required assumptions with probability $\\Omega(1)$ . In the following, we write the student activation function as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{j}(s):=\\sum_{i=0}^{\\infty}\\beta_{j,i}\\mathsf{H}\\mathsf{e}_{i}(s)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with coefficients $\\{\\beta_{j,i}\\}_{i=0}^{C_{\\sigma}}$ (sometimes the subscript $j$ , which is the index of the neurons, is omitted). ", "page_idx": 17}, {"type": "text", "text": "B.1.1 For polynomial link functions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the following, we summarize the precise conditions to be satisfied by the activation functions (these conditions are weaker than Assumptions 2 and 3). For polynomial link functions, we focus on polynomial activation functions (with bounded degree) for simplicity, but non-polynomial activation functions would not change the proof significantly. ", "page_idx": 17}, {"type": "text", "text": "Let $p$ and $q$ be the minimum and maximum degree of non-zero Hermite coefficients of $\\sigma_{*}$ . Note that $\\mathrm{GE}(\\sigma_{*})=1$ or 2 holds (see Proposition 6). Let $I\\leq C_{q}$ (according to Proposition 6) be the smallest integer such that $\\mathrm{IE}(\\sigma_{*}^{I})=\\mathrm{GE}(\\sigma_{*})=p_{*}$ and $C_{\\sigma}$ be the degree of the activation function. ", "page_idx": 17}, {"type": "text", "text": "(I) If $I=1\\Leftrightarrow\\mathrm{IE}(\\sigma_{*})=\\mathrm{GE}(\\sigma_{*})=p_{*}.$ ", "page_idx": 17}, {"type": "text", "text": "Weak recovery: $\\alpha_{p_{*}}\\beta_{p_{*}}>0$ (covered by Assumption 3). ", "page_idx": 17}, {"type": "text", "text": "Strong recovery: $\\begin{array}{r}{\\sum_{j=p_{*}}^{q}j!\\alpha_{j}\\beta_{j}s^{j-1}>0}\\end{array}$ for all $s>0$ (covered by Assumption 2). ", "page_idx": 18}, {"type": "text", "text": "Approximation (ridge regression): $\\beta_{i}\\neq0$ for some $i\\geq q$ (covered by Assumption 3). ", "page_idx": 18}, {"type": "text", "text": "(II) If $2\\leq I=\\{\\operatorname*{min}\\;i\\;|\\;\\mathrm{IE}(\\sigma_{*}^{I})=\\mathrm{GE}(\\sigma_{*})=p_{*}\\}\\leq C_{\\sigma}.$ . Weak recovery: $H((\\sigma_{*})^{I};p_{*})H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)>0$ (covered by Assumption 3). Strong recovery: $\\begin{array}{r}{\\sum_{j=p_{*}+1}^{q}j!\\alpha_{j}\\beta_{j}s^{j-1}>0}\\end{array}$ for all $s>0$ (covered by Assumption 2). Approximation: $\\beta_{i}\\neq0$ for some $i\\geq q$ (covered by Assumption 3). ", "page_idx": 18}, {"type": "text", "text": "Note that it is difficult to construct a deterministic activation function that satisfies all of the assumptions for any link function $\\sigma_{*}$ (the simplest counterexample is to consider $-\\sigma_{*}$ which flips the Hermite coefficients). Instead, we show the existence of randomized construction of such an activation function that satisfies all of the assumptions on the activation function simultaneously with constant probability, which entails that a subset of neurons can achieve strong recovery. The construction does not depend on properties of the link function itself except for its degree $q$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 11. There exists a randomized activation function sampled from a discrete set such that the above conditions hold with constant probability. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $c$ be a sufficiently small constant only used in this proof and $C_{\\sigma}$ be the minimum odd integer with $C_{\\sigma}\\geq\\operatorname*{max}\\{C_{q}+1,q+2,3\\}$ , where $C_{q}$ was introduced in Proposition 6. With probability $\\frac{1}{2}$ , we let $\\beta_{1}\\sim\\mathrm{Unif}(\\{\\pm1\\})$ , and $\\beta_{j}\\sim\\mathrm{Unif}(\\{\\pm c\\})$ for $2\\le j\\le C_{\\sigma}$ . With probability $\\frac{1}{2}$ , we let $\\beta_{j}\\ {\\overset{=}{\\sim}}\\ \\mathrm{Unif}(\\{\\pm c\\})$ for $1\\leq j\\leq C_{\\sigma}-2$ and $\\beta_{C_{\\sigma}-1}=\\beta_{C_{\\sigma}}\\sim\\mathrm{Unif}(\\{\\pm1\\})$ . ", "page_idx": 18}, {"type": "text", "text": "We first consider (I). When $\\beta_{1}\\sim\\mathrm{Unif}(\\{\\pm1\\})$ , and $\\beta_{j}\\sim\\mathrm{Unif}(\\{\\pm c\\})$ for $2\\le j\\le C_{\\sigma}$ , it is easy to see $\\mathrm{sign}(\\alpha_{j})=\\mathrm{sign}(\\beta_{j})$ for all $j=1,\\cdot\\cdot\\cdot,q$ hold with probability at least $2^{-q}$ , which is sufficient to satisfy (I). ", "page_idx": 18}, {"type": "text", "text": "We then consider (II). First focus on the case when $p_{*}=1$ and $I$ is even. When $\\beta_{1}\\sim\\mathrm{Unif}(\\{\\pm1\\})$ and $\\beta_{j}\\sim\\mathrm{Unif}(\\{\\pm c\\})$ for $2\\le j\\le C_{\\sigma}$ , by taking $c$ sufficiently small, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nH(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};0)=\\underbrace{I!\\beta_{I}(\\beta_{1})^{I-1}}_{\\asymp\\ c}+O(c^{2}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When $I$ is even, by adjusting the sign of $\\beta_{1}$ , $H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};0)$ is non-zero and has the same sign as $H((\\sigma_{*})^{I};\\dot{1})$ with probability $\\frac{1}{2}$ . Note that the sign of $\\beta_{1}$ is independent from whether $\\begin{array}{r}{\\sum_{j=2}^{q}j!\\alpha_{j}\\beta_{j}s^{j-1}\\;>\\;0}\\end{array}$ for all $s~>~0$ holds. This holds with probability at least $2^{-q+1}$ . Thus we verified (II) for $p_{*}=1$ and even $I$ . ", "page_idx": 18}, {"type": "text", "text": "For $p_{*}~=~1$ and odd $I$ , consider $\\beta_{j}\\,\\sim\\,\\mathrm{Unif}(\\{\\pm c\\})$ for $1\\,\\leq\\,j\\,\\leq\\,C_{\\sigma}\\,-\\,2$ and $\\beta_{C_{\\sigma}-1}\\,=\\,\\beta_{C_{\\sigma}}\\,\\sim$ $\\operatorname{Unif}(\\{\\pm1\\})$ . Note that $\\begin{array}{r}{\\sum_{j=2}^{q}j!\\alpha_{j}\\beta_{j}s^{j-1}\\ >\\ 0}\\end{array}$ for all $s~>~0$ (this is the condition for strong recovery) and the condition for ridge regression also holds. Furthermore, the sign of $H((\\mathsf{H e}_{C_{\\sigma}}+$ $\\mathsf{H e}_{C_{\\sigma}-1})^{(I)}((\\mathsf{H e}_{C_{\\sigma}}+\\mathsf{H e}_{C_{\\sigma}-1})^{(1)})^{I-1};0)$ is $\\pm1$ with equiprobability, independent of $\\beta_{2},\\ldots,\\beta_{q}$ . Therefore, by taking $c$ sufficiently small, we can obtain the desired sign of $H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};0)$ . Thus we proved (II) for $p_{*}=1$ and odd $I$ . ", "page_idx": 18}, {"type": "text", "text": "Regarding (II) for $p_{*}=2$ and even $I$ , when $\\beta_{1}\\sim\\mathrm{Unif}(\\{\\pm1\\})$ and $\\beta_{j}\\sim\\mathrm{Unif}(\\{\\pm c\\})$ for $2\\leq j\\leq$ $C_{\\sigma}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nH(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};1)=\\underbrace{(I+1)!\\beta_{I+1}(\\beta_{1})^{I-1}}_{\\asymp\\epsilon}+O(c^{2}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, similar to (II) with $p_{*}=1$ and even $I$ , we get (II) for $p_{*}=2$ and even $I$ . ", "page_idx": 18}, {"type": "text", "text": "Finally, consider (II) for $p_{*}\\,=\\,2$ and odd $I$ . When $\\beta_{j}\\,\\sim\\,\\mathrm{Unif}(\\{\\pm c\\})$ for $1\\,\\le\\,j\\,\\le\\,C_{\\sigma}\\,-\\,2$ and $\\beta_{C_{\\sigma}-1}=\\beta_{C_{\\sigma}}\\sim\\mathrm{Unif}(\\{\\pm1\\})$ , the sign of $H((\\mathsf{H e}_{C_{\\sigma}}+\\mathsf{H e}_{C_{\\sigma}-1})^{(I)}((\\mathsf{H e}_{C_{\\sigma}}+\\mathsf{H e}_{C_{\\sigma}-1})^{(1)})^{I-1};1)$ is $\\pm1$ with equiprobability when $I$ is odd, and this term dominates the others in $H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};1)$ . Thus, (II) for $p_{*}=2$ and odd $I$ holds similarly to (II) for $p_{*}=1$ and odd $I$ . ", "page_idx": 18}, {"type": "text", "text": "Now we have obtained the assertion for all cases. ", "page_idx": 18}, {"type": "text", "text": "B.1.2 For general link functions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Now we consider non-polynomial link functions with potentially large generative exponent $p_{*}=$ $\\mathrm{GE}(\\sigma_{\\ast})\\geq2$ . For weak and strong recovery to succeed, the conditions on the activation function are essentially the same as those for polynomial link functions: ", "page_idx": 18}, {"type": "text", "text": "(I) If $I=1\\Leftrightarrow{\\mathrm{IE}}(\\sigma_{*})={\\mathrm{GE}}(\\sigma_{*})=p_{*}.$ ", "page_idx": 19}, {"type": "text", "text": "Weak recovery: $\\alpha_{p_{*}}\\beta_{p_{*}}>0$ . Strong recovery: $\\begin{array}{r}{\\sum_{j=p_{*}}^{\\infty}j!\\alpha_{j}\\beta_{j}s^{j-1}>0}\\end{array}$ for all $s>0$ , (II) If $:2\\leq I=\\{\\operatorname*{min}\\;i\\;|\\;\\mathrm{IE}(\\sigma_{*}^{I})=\\mathrm{GE}(\\sigma_{*})=p_{*}\\}\\leq C_{\\sigma}.$ Weak recovery: $H((\\sigma_{*})^{I};p_{*})H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)>0,$ Strong recovery: $\\begin{array}{r}{\\sum_{j=p_{*}+1}^{\\infty}j!\\alpha_{j}\\beta_{j}s^{j-1}>0}\\end{array}$ for all $s>0$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Due to the proof strategy (which uses Taylor expansion), we also require that all differentials and sum of expectations appearing in the following proofs are well-defined and bounded. ", "page_idx": 19}, {"type": "text", "text": "To approximate a non-polynomial $\\sigma_{*}$ , we introduce the following condition on the activation function. We sample $\\sigma_{j}$ from a discrete set (with bounded cardinality). Let $J$ be an index set such that the coefficients of $\\sigma_{j}$ $(j\\in J)$ satisfy the conditions above. Because we are selecting $\\sigma_{j}$ from a discrete set, $|J|\\,\\simeq\\,N$ holds. We introduce the following condition, which states that the target single-index model can be well-approximated by a linear combination of student neurons. ", "page_idx": 19}, {"type": "text", "text": "Assumption 4. When $b_{j}\\ \\sim\\ \\mathrm{Unif}([-C_{b},C_{b}])$ where $(C_{b}\\ =\\ \\mathrm{polylog}(d))$ and $x_{1},\\ldots,x_{T_{2}}\\;\\;\\sim$ $\\mathcal{N}(0,\\bar{\\boldsymbol{I_{d}}})$ , there exists a set of coefficients $a_{1},\\dotsc,a_{|J|}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{T_{2}}\\sum_{i=1}^{T_{2}}\\left(\\frac{1}{|J|}\\sum_{j\\in J}a_{j}\\sigma_{j}(\\pmb{\\theta}_{j}^{\\top}\\pmb{x}_{i}+b_{j})-\\sigma_{*}(\\pmb{\\theta}^{\\top}\\pmb{x}_{i})\\right)^{2}\\lesssim\\varepsilon^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds with coefficients of reasonable magnitudes $\\begin{array}{r}{\\sum_{j\\in J}a_{j}^{2}\\,=\\,\\Theta(|J|)}\\end{array}$ with high probability $(w.r t.$ the randomness of $b_{j}$ and $\\pmb{x}_{i}$ ). Moreover, $\\mathbb{E}_{\\mathbf{x}}[\\sigma_{j}(\\pmb{\\theta}_{j}^{\\top}\\pmb{x}+b_{j})^{4}]\\ \\le$ polylog $(d)$ for all $j$ with high probability (w.r.t. the randomness of $b_{j}$ ). ", "page_idx": 19}, {"type": "text", "text": "The following lemma states that we can design a randomized activation function that satisfies all of the above assumptions with probability $\\Omega(1)$ , as long as the link function $\\sigma$ satisfies Assumption 4 for $\\sigma\\,=\\,\\mathrm{ReLU}$ . In other words, we are able to cover the class of link functions $\\sigma$ that can be efficiently approximated by a two-layer ReLU network. Since the general link functions are not included in a compact space, we do not have an upper bound of exponent to obtain $\\mathrm{LE}(\\sigma_{*}^{I})\\ =$ $\\mathrm{GE}(\\sigma_{*})$ as we had $C_{q}$ in the polynomial case. Consequently, our student activation is not entirely agnostic to the link function $\\sigma_{*}$ , as we require knowledge of $p$ (information exponent), $p_{*}$ and $I$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma 12. Suppose that the target link function $\\sigma_{*}$ satisfies Assumption 4 for $\\sigma_{j}=\\mathrm{ReLU}$ . There exists a randomized activation function sampled from a discrete set such that the above conditions hold with constant probability. ", "page_idx": 19}, {"type": "text", "text": "Before we sketch the design of activation function, we present the following approximation result from [BBSS22], which establishes that Assumption 4 with $\\sigma_{j}=\\mathrm{ReLU}$ is satisfied for broad class of functions, according to Lemma 4.4 and 4.5 of [BBSS22]. Specifically, taking $\\tau=1/2$ and $\\lambda=N^{-1}$ yields that $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}}[(\\frac{1}{|J|}\\sum_{j\\in J}a_{j}\\sigma_{j}(\\pmb{\\theta}^{\\top}\\pmb{x}+b_{j})-\\sigma_{*}(\\pmb{\\theta}^{\\top}\\pmb{x}))^{2}]\\leq N^{-\\frac{2}{7}}}\\end{array}$ . Although they sample $b_{j}$ from the Gaussian $\\mathcal N(0,2)$ , the result translates to uniform sampling of the bias units from $[-C_{b},C_{b}]$ by introducing additional logarithmic factor. ", "page_idx": 19}, {"type": "text", "text": "Lemma 13 (Lemma 4.4, 4.5 of [BBSS22]). Suppose that $\\mathbb{E}_{z\\sim\\mathcal{N}(0,1)}[\\sigma_{*}(z)^{4}],\\mathbb{E}_{z\\sim\\mathcal{N}(0,1)}[\\sigma_{*}^{\\prime\\prime}(z)^{4}]<$ $\\infty$ . Then, Assumption $^{4}$ with $\\sigma_{j}=\\mathrm{ReLU}$ holds with $\\varepsilon=N^{-\\frac{1}{7}}$ and $C_{b}\\simeq\\sqrt{\\log d}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 12. We show the existence of suitable $\\sigma$ in two steps: first we construct a randomized polynomial activation function that satisfies conditions (I)(II) with constant probability; then we add a small ReLU perturbation so that the activation can approximate non-polynomial $\\sigma_{*}$ . ", "page_idx": 19}, {"type": "text", "text": "Recall $p\\,\\in\\,\\mathbb{N}_{+}$ is the information exponent of $\\sigma_{*}$ . We first show that there exists a randomized polynomial activation that satisfies the conditions for weak and strong recovery with probability $\\Omega(1)$ . Note that the issue of differentiability and bounded moment is avoided when we focus on the polynomial activation functions. We specify the following two distributions. With probability $\\frac{1}{2}$ , let $\\beta_{1}\\sim\\operatorname{Unif}(\\{-1,1\\})$ , $\\beta_{j}\\,\\sim\\,\\mathrm{Unif}(\\{-c,c\\})$ for $j\\,=\\,1,\\cdot\\cdot\\cdot\\,,p_{*}\\,+\\,I\\mathrm{~-~}1$ and $\\beta_{j}\\,=\\,0$ otherwise, where $c>0$ is a sufficiently small constant. With probability $\\frac{1}{2}$ , let $\\beta_{1}\\,=\\,\\mathrm{Unif}(\\{-1,1\\})$ , $\\beta_{2}=$ $\\operatorname{Unif}(\\{-c,c\\})$ , $\\beta_{j}=\\operatorname{Unif}(\\{-c^{2},c^{2}\\})$ for all $2\\leq j\\leq(p_{*}+I)\\vee p$ for a sufficiently small constant $c>0$ , and $\\beta_{j}=0$ otherwise. ", "page_idx": 19}, {"type": "text", "text": "Regarding (I), consider the case when the coefficients are sampled with the first distribution, and $|\\beta_{j}\\bar{|}\\ll1$ except for $j=p_{*}$ . Then, $\\begin{array}{r}{\\sum_{j=p_{*}}^{\\infty}j!\\alpha_{j}\\beta_{j}s^{j-1}\\approx p_{*}!\\alpha_{p_{*}}^{\\bigstar}\\beta_{p_{*}}s^{p_{*}}}\\end{array}$ . Choosing the sign of $\\beta_{p_{*}}$ , we have that the assumption holds with probability $\\Omega(1)$ . ", "page_idx": 20}, {"type": "text", "text": "Regarding (II) with even $I$ , consider when the coefficients are sampled with the first distribution, and $\\mathrm{Sign}(\\beta_{j})=\\mathrm{Sign}(\\alpha_{j})$ for $j\\le(p_{*}+I-1)\\lor p$ . Then, $\\begin{array}{r}{\\sum_{j=p_{*}+1}^{\\infty}j!\\alpha_{j}\\beta_{j}s^{j-1}>0}\\end{array}$ for all $s>0$ Also, similarly to (B.2), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)=\\underbrace{i!\\beta_{I+p_{*}-1}(\\beta_{1})^{I-1}}_{\\asymp\\ c}+O(c^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By flipping the sign of $\\beta_{1}$ , we can change the sign of $H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)$ . Thus, (II) for even $I$ is satisfied by a randomized choice of $\\beta_{1}$ . ", "page_idx": 20}, {"type": "text", "text": "For (II) with odd $I$ , consider the case when the coefficients are sampled with the second distribution, and $\\mathrm{Sign}(\\beta_{j})=\\mathrm{Sign}(\\alpha_{j})$ for $j\\leq(p_{*}+I)\\vee p$ . Then, $\\begin{array}{r}{\\sum_{j=p_{*}+1}^{\\infty}j!\\alpha_{j}\\beta_{j}s^{j-1}>0}\\end{array}$ for all $s>0$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)=\\cfrac{1}{(p_{*}-1)!}\\mathbb{E}[\\sigma^{(I)}(\\sigma^{(1)})^{I-1}\\mathsf{H e}_{p_{*}-1}]}\\\\ &{\\,=\\cfrac{1}{(p_{*}-1)!}\\mathbb{E}[(I-1)(\\beta_{p_{*}+I}\\mathsf{H e}_{p_{*}+I})^{(I)}(\\beta_{2}\\mathsf{H e}_{2})^{(1)}(\\beta_{1})^{I-2}\\mathsf{H e}_{p_{*}-1}]+O(c^{4}).}\\\\ &{\\,=\\cfrac{2(I-1)\\beta_{p_{*}+I}\\beta_{2}(\\beta_{1})^{I-2}(p_{*}+I)!}{(p_{*}-1)!}+O(c^{4}).}\\\\ &{\\,=\\cfrac{\\hphantom{-}\\sum_{j={1}}^{2}\\hphantom{-}}{(p_{*}-1)!}\\qquad\\cfrac{1}{c^{3}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By flipping the sign of $\\beta_{1}$ , we can change the sign of $H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)$ . Thus, (II) for odd $I$ is satisfied by a randomized choice of $\\beta_{1}$ . ", "page_idx": 20}, {"type": "text", "text": "Therefore, we have constructed a randomized polynomial activation $\\sigma$ that satisfies all of the conditions for weak and strong recovery. Now we provide a sketch of reasoning that when the link function $\\sigma_{*}$ is well-approximated by ReLU as specified in Assumption 4, we can find some $\\sigma$ that additionally satisfies Assumption 4 by introducing a small ReLU component. Specifically, we add $c_{\\mathrm{R}}\\cdot\\mathrm{ReLU}$ to the activation function with probability $\\frac{1}{2}$ , with a sufficiently small $c_{\\mathrm{R}}=\\dot{\\tilde{\\Omega}}(1)$ , e.g., $c_{\\mathrm{R}}=(\\log d)^{-C}$ for some $C>0$ . When a two-layer ReLU network approximates $\\sigma_{*}$ that satisfies Assumption 4, by using the neurons with added ReLU component, $\\sigma_{*}$ can be approximated up to some polynomial residual with degree $(p_{*}+I)\\vee p$ . And by using the remaining polynomial neurons, we can approximate the additional polynomial terms in $\\sigma_{*}$ (see Lemmas 23 and 22). Subtracting the latter from the former, we obtain the desired approximation result. When $c_{\\mathrm{R}}$ is sufficiently small, this additional term does not impact the conditions for weak and strong recovery and the moment calculations; similarly, since $c_{\\mathrm{R}}\\ll1$ we may discard this non-smooth term before Taylor expansion without affecting the analysis of optimization dynamics. We remark that to avoid such unnatural design of activation function, we can also train the first-layer parameters using a polynomial activation specified above, and then perturb it before the second-layer training to enhance the approximation ability \u2014 such strategy has also been employed in prior layer-wise training analysis [AAM22]. ", "page_idx": 20}, {"type": "text", "text": "B.1.3 More Discussion on Assumption 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Assumption 2 requires $H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}\\,-\\,1)$ is not zero and has the same sign as $H(\\sigma_{*}^{I};p_{*})$ . We remark that if we allow a negative momentum parameter larger than 1, i.e., setting $\\xi^{2(t+1)}\\,=$ $1+c_{\\xi}d^{-\\frac{(p_{*}-2)}{2}+}$ , we can negate the opposite sign of $H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)$ (see Lemma 16), and the subsequent analysis still holds. Therefore, what we essentially need is ${\\cal H}(\\sigma^{(i)}(\\sigma^{(1)})^{i-1};k)\\neq0$ . Lemma 3 confirms that it is satisfied by almost all polynomials: ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 3. We note that $H(\\sigma^{(i)}(\\sigma^{(1)})^{i-1};k)=\\mathbb{E}[\\sigma^{(i)}(\\sigma^{(1)})^{i-1}\\mathsf{H}\\mathsf{e}_{k}]$ is a polynomial of $\\{\\beta_{j}\\}_{j=0}^{C_{\\sigma}}$ . This polynomial is not identically equal to zero. To confirm this, consider $\\sigma\\,=\\,x^{C_{\\sigma}}\\,+$ $x^{C_{\\sigma}-1}$ . Because $\\sigma^{(i)}(\\sigma^{(1)})^{i-1}$ is expanded as a sum of $x^{l}(i(C_{\\sigma}-3)\\le l\\le i(C_{\\sigma}-2)+1$ with positive coefficients and each $x^{l}$ is a sum of ${\\mathsf{H e}}_{l},{\\mathsf{H e}}_{l-2}\\cdot\\cdot\\cdot$ with positive coefficients, $\\sigma^{(i)}(\\sigma^{(1)})^{i-1}$ has all positive Hermite coefficients for degree $0,1,\\cdot\\cdot\\cdot,i(C_{\\sigma}\\,-\\,2)\\,+\\,1$ . If $k\\,\\leq\\,i(C_{\\sigma}\\,-\\,2)\\,+\\,1.$ , this choice of $\\sigma$ yields $H(\\sigma^{(i)}(\\sigma^{(1)})^{i-1};k)\\;>\\;0$ , which confirms that ${\\cal H}(\\sigma^{(i)}(\\sigma^{(1)})^{i-1};k)$ as a polynomial of $\\{\\beta_{j}\\}_{j=0}^{C_{\\sigma}}$ is not identically equal to zero. Hence the assertion follows from so-called Schwartz\u2013Zippel Lemma [Sch80], or the fact that zeros of a non-zero polynomial form a measurezero set. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "B.2 Initialization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first consider the initial alignment. In the following sections, we focus on the neurons that satisfy $\\kappa_{j}^{0}\\,=\\,\\pmb{\\theta}^{\\top}\\pmb{w}_{j}^{0}\\,\\geq\\,2c_{\\eta}^{-1}d^{-\\frac{1}{2}}$ at the initialization. The following lemma states that roughly a constant portion of the neurons satisfy the initial alignment condition upon random initialization. In particular, if we take $c_{\\eta}=\\Omega((\\log\\log d)^{-\\frac{1}{2}})$ , the fraction of neurons that satisfy the initial alignment condition is at least $e^{-16c_{\\eta}^{-2}}=\\tilde{\\Omega}(1)$ . Let us write $C_{2}=c_{\\eta}^{-1}$ for simplicity in the following. ", "page_idx": 21}, {"type": "text", "text": "Lemma 14. At the time of initialization, $\\kappa_{j}^{0}=\\pmb{\\theta}^{\\top}\\pmb{w}^{0}$ satisfies the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\kappa_{j}^{0}\\geq2C_{2}d^{-\\frac{1}{2}}]=\\mathbb{P}[\\kappa_{j}^{0}\\leq-2C_{2}d^{-\\frac{1}{2}}]\\gtrsim e^{-16C_{2}^{2}}=\\tilde{\\Omega}(1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We make use of the following lemma. ", "page_idx": 21}, {"type": "text", "text": "Lemma 15 (Theorem 2 of [CCM11]). For any $\\beta>1$ and $s\\in\\mathbb{R}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{2e(\\beta-1)}}{2\\beta\\sqrt{\\pi}}e^{-\\frac{\\beta s^{2}}{2}}\\leq\\int_{s}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 14. Because \u03ba0 = v\u22a4w d= e\u22251\u22a4g \u2225g, where g \u223cN(0, Id), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\kappa_{j}^{0}\\geq2C_{2}d^{-\\frac{1}{2}}]=\\mathbb{P}_{g\\sim{\\mathcal N}(0,I_{d})}\\bigg[e_{1}^{\\top}g\\geq4C_{2}\\wedge\\|g\\|\\leq2d^{\\frac{1}{2}}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathbb{P}_{g\\sim{\\mathcal N}(0,I_{d})}\\bigg[e_{1}^{\\top}g\\geq4C_{2}\\bigg]-\\mathbb{P}_{g\\sim{\\mathcal N}(0,I_{d})}\\bigg[\\|g\\|\\geq2d^{\\frac{1}{2}}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim\\frac{\\sqrt{2e(\\beta-1)}}{2\\beta\\sqrt{\\pi}}e^{-8\\beta C_{2}^{2}}-e^{-\\Omega(d)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used Lemma 15 for the final inequality. By letting $\\beta=2$ , we have that $\\mathbb{P}[\\kappa_{j}^{0}\\geq C_{2}d^{-\\frac{1}{2}}]\\gtrsim$ $e^{-16C_{2}^{2}}$ . Because of the symmetry, $\\mathbb{P}[\\kappa_{j}^{0}\\leq2C_{2}d^{-\\frac{1}{2}}]=\\mathbb{P}[\\kappa_{j}^{0}\\geq2C_{2}d^{-\\frac{1}{2}}]$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "B.3 Weak Recovery: Population Update ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We divide the first layer training into the first phase (weak recovery) and the second phase (strong recovery). We first evaluate the expected update of two gradient steps with the same training example. ", "page_idx": 21}, {"type": "text", "text": "Lemma 16. Let $\\eta^{2t},\\eta^{2t+1}=\\eta=c_{\\eta}d^{-1}$ , $\\xi^{2(t+1)}=\\xi=1-c_{\\xi}d^{-\\frac{(p_{*}-2)}{2}+}$ . Suppose that the link function satisfies $\\mathrm{IE}(\\sigma_{*}^{I})=\\mathrm{GE}(\\sigma_{*})=p_{*}$ (we choose the smallest such $I$ ) and activation functions satisfy all of the assumptions in Section B.1 for weak recovery. Then, for $w^{2t}$ with $c_{\\eta}^{-1}d^{-\\frac{1}{2}}\\;\\leq$ $\\pmb{\\theta}^{\\top}\\pmb{w}^{2t}\\leq c_{\\eta}^{I},$ , the alignment $\\pmb{\\theta}^{\\top}\\pmb{w}^{2(t+1)}$ can be evaluated as, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\theta^{\\top}w^{2(t+1)}\\geq\\theta^{\\top}w^{2t}+c_{\\eta}^{I}c_{\\xi}c_{\\sigma}d^{-\\frac{p_{*}}{2}\\vee1}(\\kappa^{2t})^{p_{*}-1}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}\\nu^{2t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here $c_{\\sigma}=p_{*}!\\alpha_{p_{*}}\\beta_{p_{*}}$ $(w h e n\\operatorname{IE}(\\sigma_{*})=\\operatorname{GE}(\\sigma_{*}))$ or $\\begin{array}{r}{c_{\\sigma}\\,=\\,\\frac{p_{*}!H(\\sigma_{*}^{I};p_{*})H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)}{2(I-1)!}}\\end{array}$ (otherwise), and $\\nu^{2t}$ is a mean-zero sub-exponential random variable. ", "page_idx": 21}, {"type": "text", "text": "Proof. The expected alignment $\\pmb{\\theta}^{\\top}\\pmb{w}^{2(t+1)}$ after two gradient steps from $w^{2t}=\\omega$ using the same sample $(\\pmb{x},y)$ , step size $\\eta^{2t}\\,=\\,\\eta^{2t+1}\\,=\\,\\eta\\,=\\,c_{\\eta}d^{-1}$ and momentum parameter $\\xi^{2(t+\\bar{1})}\\,=\\,\\xi\\,=$ $1-c_{\\xi}d^{-\\frac{(p_{*}-2)}{2}}$ is evaluated as follows. With a projection matrix $\\boldsymbol{P}_{\\omega}=\\boldsymbol{I}-\\omega\\omega^{\\top}$ , the first step updates the weight as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb w}^{2t+1}\\leftarrow{\\pmb w}^{2t}+\\eta\\tilde{\\nabla}_{\\pmb w}y\\sigma({\\pmb w}^{2t}{^{\\top}}{\\pmb x})=\\omega+\\eta y\\sigma^{\\prime}({\\pmb\\omega}^{\\top}{\\pmb x}){\\pmb P}_{\\omega}{\\pmb x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the next gradient step with the same sample is computed as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\nabla}_{w}y\\sigma({w^{2t+1}}^{\\top}{x})=y\\sigma^{\\prime}({w^{2t+1}}^{\\top}{x}){x}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=y\\sigma^{\\prime}\\big((\\omega+\\eta y\\sigma^{\\prime}({\\omega}^{\\top}x)P_{\\omega}x)^{\\top}x\\big)P_{\\omega}x}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=y\\sigma^{\\prime}\\big({\\omega}^{\\top}x+\\eta\\|x\\|_{P_{\\omega}}^{2}\\sigma^{\\prime}({\\omega}^{\\top}x)y\\big)P_{\\omega}x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "here we used the notation $\\|\\pmb{\\theta}\\|_{A}^{2}\\,=\\,\\pmb{\\theta}^{\\top}A\\pmb{\\theta}$ for a vector $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ and a positive symmetric matric $A\\in\\mathbb{R}^{d\\times d}$ . From (B.3) and (B.4), the parameter after the two steps is obtained as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w^{2(t+1)}\\gets w^{2t+1}+\\eta\\tilde{\\nabla}_{w}y\\sigma(w^{2t+1}{^{\\top}}x)}\\\\ &{\\qquad\\qquad=\\omega+\\eta y\\sigma^{\\prime}(\\omega^{\\top}x)P_{\\omega}x+\\eta y\\sigma^{\\prime}\\big(\\omega^{\\top}x+\\eta\\|x\\|_{P_{\\omega}}^{2}\\sigma^{\\prime}(\\omega^{\\top}x)y\\big)P_{\\omega}x.}\\\\ &{\\qquad\\qquad=\\omega+\\eta g^{2t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\ng^{2t}=y\\sigma^{\\prime}(\\omega^{\\top}x)P_{\\omega}x+y\\sigma^{\\prime}(\\omega^{\\top}x+\\eta\\|x\\|_{P_{\\omega}}^{2}\\sigma^{\\prime}(\\omega^{\\top}x)y)P_{\\omega}x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, the normalization step yields ", "page_idx": 22}, {"type": "equation", "text": "$$\nw^{2(t+1)}\\gets\\frac{w^{2(t+1)}-\\xi^{2(t+1)}(w^{2(t+1)}-w^{2t})}{\\lVert w^{2(t+1)}-\\xi^{2(t+1)}(w^{2(t+1)}-w^{2t})\\rVert}=\\frac{\\omega+\\eta\\xi g^{2t}}{\\lVert\\omega+\\eta\\xi g^{2t}\\rVert}=\\frac{\\omega+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}g^{2t}}{\\lVert\\omega+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}g^{2t}\\rVert}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, by writing $\\pmb{\\theta}^{\\top}\\pmb{w}^{2t}=\\kappa^{2t}$ , the update of the alignment is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\kappa^{2(t+1)}=\\theta^{\\top}w^{2(t+1)}}\\\\ &{=\\frac{\\kappa^{2t}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}\\theta^{\\top}g^{2t}}{\\|\\omega+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}g^{2t}\\|}}\\\\ &{\\geq\\kappa^{2t}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}\\theta^{\\top}g^{2t}-\\frac12\\kappa^{2t}c_{\\eta}^{2}c_{\\xi}^{2}d^{-p_{*}\\vee2}\\|g^{2t}\\|^{2}-\\frac12c_{\\eta}^{3}c_{\\xi}^{3}d^{-\\frac{3p_{*}}{2}\\vee3}|\\theta^{\\top}g^{2t}|\\|g^{2t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We can easily see that $\\mathbb{E}[\\|g^{2t}\\|^{2}]\\,\\lesssim\\,d$ and $\\mathbb{E}[|\\pmb{\\theta}^{\\top}\\pmb{g}^{2t}|\\|\\pmb{g}^{2t}\\|^{2}]\\,\\lesssim\\,d$ , which implies that the expectation of the last two terms of (B.5) is bounded by $\\lesssim\\,\\kappa^{2t}c_{\\eta}^{2}c_{\\xi}^{2}d^{-(p_{*}-1)\\vee1}\\,\\vee\\,c_{\\eta}^{3}c_{\\xi}^{3}d^{-(\\frac{3p_{*}}{2}-1)\\vee2}\\,\\le\\,.$ $c_{\\eta}^{2}c_{\\xi}^{2}d^{-(p_{*}-1)\\vee1}\\kappa^{2t}$ . ", "page_idx": 22}, {"type": "text", "text": "Now we bound $\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}^{2t}]$ by $\\gtrsim\\,c_{\\eta}^{I-1}\\kappa^{p_{*}-1}$ . Let $C_{\\sigma}$ be the maximum degree of the activation function with non-zero coefficients of Hermite expansion, which may be infinity when we consider general link functions, and there appear some infinite sums. For these cases we simply assume the sums converge \u2013 we discuss the validity of this condition in Section B.1.2. We omit the subscript $2t$ in the following for simplicity. We divide the analysis into the two cases. ", "page_idx": 22}, {"type": "text", "text": "(I) If $I=1\\Leftrightarrow\\mathrm{IE}(\\sigma_{*})=\\mathrm{GE}(\\sigma_{*})=p_{*}$ . For the first term of $\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}]$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta^{\\top}\\mathbb{E}[y\\sigma^{\\prime}(\\omega^{\\top}x)P_{\\omega}x]=\\theta^{\\top}P_{\\omega}\\mathbb{E}\\Bigg[\\Bigg(\\displaystyle\\sum_{j=p_{*}}^{\\infty}\\alpha_{j}\\mathsf{H e}_{j}(\\theta^{\\top}x)\\Bigg)\\Bigg(\\displaystyle\\sum_{j=1}^{C_{\\sigma}}j\\beta_{j}\\mathsf{H e}_{j-1}\\big(\\omega^{\\top}x\\big)\\Bigg)x\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\theta^{\\top}P_{\\omega}\\displaystyle\\sum_{j=p_{*}}^{\\infty}\\Bigg[j\\mathsf{l}\\alpha_{j}\\beta_{j}\\big(\\theta^{\\top}\\omega\\big)^{j-1}\\theta+(j+2)\\mathsf{l}\\alpha_{j}\\beta_{j+2}\\big(\\theta^{\\top}\\omega\\big)^{j}\\omega\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=p_{*}}^{C_{\\sigma}}j\\mathsf{l}\\alpha_{j}\\beta_{j}\\big(\\theta^{\\top}\\omega\\big)^{j-1}\\theta^{\\top}P_{\\omega}\\theta}\\\\ &{\\qquad\\qquad\\qquad\\qquad=p_{*}\\lvert\\alpha_{p_{*}}\\beta_{p_{*}}\\kappa^{p_{*}-1}+O\\big(\\kappa^{p_{*}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the second term of $\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}]$ , the following decomposition can be made. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta^{\\top}\\mathbb{E}[y\\sigma^{\\prime}\\big(\\omega^{\\top}x+\\eta\\|x\\|_{P_{\\omega}}^{2}\\sigma^{\\prime}(\\omega^{\\top}x)y\\big)P_{\\omega}x]}\\\\ &{=\\displaystyle\\sum_{i=1}^{C_{\\sigma}-1}(i!)^{-1}\\theta^{\\top}P_{\\omega}\\mathbb{E}\\bigg[y\\sigma^{(i+1)}\\big(\\omega^{\\top}x\\big)\\big(\\eta\\|x\\|_{P_{\\omega}}^{2}y\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{i}x\\bigg]+\\theta^{\\top}\\mathbb{E}[y\\sigma^{\\prime}(\\omega^{\\top}x)P_{\\omega}x]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{i=1}^{C_{\\sigma}-1}(i!)^{-1}\\eta^{i}\\pmb{\\theta}^{\\top}P_{\\omega}\\mathbb{E}\\bigg[\\|\\pmb{x}\\|_{P_{\\omega}}^{2i}y^{i+1}\\sigma^{(i+1)}\\big(\\pmb{\\omega}^{\\top}\\pmb{x}\\big)\\big(\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{i}x\\bigg]}\\\\ &{~~+\\displaystyle p_{*}!\\alpha_{p_{*}}\\beta_{p_{*}}\\kappa^{p_{*}-1}+O\\big(\\kappa^{p_{*}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We evaluate each term in the summation. We need to show that although $\\|\\pmb{x}\\|_{P_{\\omega}}^{2}$ is a function of $\\pmb{\\theta}^{\\top}\\pmb{x}$ and $\\omega^{\\top}x$ , it is mostly independent from the two quantities. To verify this, let $\\begin{array}{r}{\\pmb{e}=\\frac{\\pmb{\\theta}-(\\pmb{\\theta}^{\\top}\\pmb{\\omega})\\pmb{\\omega}}{\\|\\pmb{\\theta}-(\\pmb{\\theta}^{\\top}\\pmb{\\omega})\\pmb{\\omega}\\|}}\\end{array}$ be the orthogonal component of $\\pmb{\\theta}$ to $\\omega$ . Then, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x\\|_{P_{\\omega}}^{2}=x^{\\top}(I-\\omega\\omega^{\\top}-e e^{\\top})x+(e^{\\top}x)^{2}}\\\\ &{\\qquad\\quad=\\underset{{\\sim}\\chi_{d-2}^{\\top},\\,\\mathrm{independent}\\,\\mathrm{from}\\,\\omega^{\\top}x\\mathrm{~\\and}\\,\\theta^{\\top}x}{=\\sum_{d-2}^{\\top}\\left(I-\\omega\\omega^{\\top}-e e^{\\top}\\right)x}+\\biggr(\\frac{\\theta^{\\top}x-(\\theta^{\\top}\\omega)\\omega^{\\top}x}{\\|\\theta-(\\theta^{\\top}\\omega)\\omega\\|}\\biggr)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With $P_{\\omega,\\theta}=I_{d}-\\omega\\omega^{\\top}-e e^{\\top}$ , (B.7) is expanded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{C_{\\sigma}-1}\\sum_{j=0}^{i+1}\\sum_{l=0}^{i+1}\\frac{\\binom{i}{j}\\binom{i+1}{l}\\eta^{i}}{i!\\|\\theta-(\\theta^{\\top}\\omega)\\omega\\|^{2j}}}\\\\ &{\\displaystyle\\quad\\theta^{\\top}P_{\\omega}\\mathbb{E}\\bigg[\\big(x^{\\top}P_{\\omega,\\theta^{\\mathbf{X}}}\\big)^{i-j}\\zeta^{i+1-l}\\big(\\sigma_{*}(\\theta^{\\top}x)\\big)^{l}\\big(\\theta^{\\top}x-(\\theta^{\\top}\\omega)\\omega^{\\top}x\\big)^{2j}\\sigma^{(i+1)}\\big(\\omega^{\\top}x\\big)\\big(\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{i}x\\bigg]}\\\\ &{\\displaystyle\\sum_{i=1}^{C_{\\sigma}-1}\\sum_{j=0}^{i+1}\\sum_{l=0}^{2j}\\frac{\\binom{i}{l}\\binom{i+1}{l}\\binom{2j}{k}\\eta^{i}\\kappa^{k}\\big(-1\\big)^{k}\\mathbb{E}[\\zeta^{i+1-l}]\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}\\left[\\varepsilon^{i-j}\\right]}{i!\\|\\theta-(\\theta^{\\top}\\omega)\\omega\\|^{2j}}}\\\\ &{\\displaystyle\\qquad\\mathbb{E}\\bigg[\\big(\\sigma_{*}(\\theta^{\\top}x)\\big)^{l}\\big(\\theta^{\\top}x\\big)^{2j-k}\\big(\\omega^{\\top}x\\big)^{k}\\sigma^{(i+1)}\\big(\\omega^{\\top}x\\big)\\big(\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{i}\\big(\\theta^{\\top}x-\\theta^{\\top}\\omega\\omega^{\\top}x\\big)\\bigg]\\mathbb{B}.9)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For a general differentiable function $g(x)$ , we have $\\begin{array}{r}{\\mathbb{E}[\\mathsf{H}\\{\\mathbf{e}_{t}(x_{1})g(\\pmb x)\\}=\\mathbb{E}[\\frac{\\mathrm{d}^{t}}{\\mathrm{d}x_{1}^{t}}g(\\pmb x)]}\\end{array}$ . If $g(x)$ is a polynomial (with a bounded coefficients) of $x_{1}$ and ${\\pmb u}^{\\top}{\\pmb x}$ and its degree with respect to $x_{1}$ is at most $s(\\underline{{\\<}}\\ t),\\,|\\mathbb{E}[\\mathsf{H e}_{t}(x_{1})g(\\pmb{x})]|\\lesssim|u_{1}|^{t-s}$ , because differentiation of $g(\\pmb{x})=\\bar{g}(x_{1},\\pmb{\\dot{u}}^{\\top}\\pmb{x})$ is taken with respect to the first variable at most $s$ times. Each term of (B.9) is an expectation of $(\\sigma_{*}(\\pmb{\\theta}^{\\top}x))^{l}$ , multiplied by the polynomial of $\\pmb{\\theta}^{\\top}\\pmb{x}$ and $\\omega^{\\top}x$ , where its degree with respect to $\\pmb{\\theta}^{\\top}\\pmb{x}$ is at most $2j-k$ . Thus each term of (B.9) is evaluated as (here we omit the constants) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\eta^{i}\\kappa^{k}(-1)^{k}\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}[z^{i-j}]}{i!\\|\\theta-(\\theta^{\\top}\\omega)\\omega\\|^{2j}}\\mathbb{E}\\bigg[\\underbrace{(\\sigma_{*}(\\theta^{\\top}x))^{l}}_{\\mathrm{IE}\\geq p_{*}}\\underbrace{(\\theta^{\\top}x)^{2j-k}(\\omega^{\\top}x)^{k}\\sigma^{(i+1)}\\big(\\omega^{\\top}x\\big)\\big(\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{i}(\\theta^{\\top}x-\\theta^{\\top}x)}_{\\mathrm{degre~w.t.~}\\theta^{\\top}x\\mathrm{~is~at~mos~}2j\\mathrm{~}-k+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lesssim c_{\\eta}^{i}d^{-i}d^{i-j}\\kappa^{k}\\kappa^{((p_{*}-2j+k-1)\\vee0)}\\lesssim c_{\\eta}d^{-j}\\kappa^{((p_{*}-2j-1)\\vee0)}\\leq c_{\\eta}\\kappa^{p_{*}-1}(d/\\kappa^{2})^{-j}\\leq c_{\\eta}\\kappa^{p_{*}-1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The lower bound follows in the same fashion. Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|(\\mathrm{B}.9)|\\lesssim c_{\\eta}\\kappa^{p_{*}-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, $\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}]$ can be evaluated as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\theta^{\\top}g]=(\\mathrm{B}.6)+(\\mathrm{B}.7)+(\\mathrm{B}.8)=2p_{*}!\\alpha_{p_{*}}\\beta_{p_{*}}\\kappa^{p_{*}-1}+O\\big(c_{\\eta}\\kappa^{p_{*}-1}+\\kappa^{p_{*}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$(\\mathbf{II})$ If $I=\\{\\operatorname*{min}\\;i\\;|\\;\\mathrm{IE}(\\sigma_{*}^{i})=\\mathrm{GE}(\\sigma_{*})=p_{*}\\}\\geq2.$ Note that $\\alpha_{j}\\,=\\,0$ for all $j\\leq p_{*}$ from the assumption. Following (B.6), the first term of $\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}]$ is evaluated as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{\\top}\\mathbb{E}[y\\sigma^{\\prime}(\\omega^{\\top}x)\\pmb{P}_{\\omega}\\pmb{x}]=\\sum_{j=p}^{C_{\\sigma}}j!\\alpha_{j}\\beta_{j}\\big(\\pmb{\\theta}^{\\top}\\omega\\big)^{j-1}\\pmb{\\theta}^{\\top}\\pmb{P}_{\\omega}\\pmb{\\theta}=O(\\kappa^{p_{*}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the second term of $\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}]$ , similarly to (B.9), the following decomposition can be made. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\theta^{\\top}\\mathbb{E}[y\\sigma^{\\prime}\\big(\\omega^{\\top}x+\\eta\\|x\\|_{P_{\\omega}}^{2}\\sigma^{\\prime}(\\omega^{\\top}x)y\\big)P_{\\omega}x]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!C_{\\neq0}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{i=0}^{C_{\\sigma}-1}\\sum_{j=0}^{i}\\sum_{l=0}^{2j}\\frac{{\\binom{i}{j}}{\\binom{i+1}{l}}{\\binom{2j}{l}}{\\binom{2j}{k}}\\eta^{i}\\kappa^{k}(-1)^{k}\\mathbb{E}[\\varsigma^{i+1-l}]\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}{\\left[z^{i-j}\\right]}}{i!\\!\\|\\theta-(\\theta^{\\top}\\omega)\\omega\\|^{2j}}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbb{E}\\bigg[\\big(\\sigma_{*}(\\theta^{\\top}x)\\big)^{l}(\\theta^{\\top}x)^{2j-k}(\\omega^{\\top}x)^{k}\\sigma^{(i+1)}\\big(\\omega^{\\top}x\\big)\\big(\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{i}(\\theta^{\\top}x-\\theta^{\\top}\\omega\\omega^{\\top}x)\\bigg]\\!\\!\\!\\!\\bigg\\mathrm{[B}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Each term of (B.11) (omitting constants) is evaluated as ", "page_idx": 24}, {"type": "equation", "text": "$$\ni\\kappa^{k}\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}[z^{i-j}]\\mathbb{E}\\bigg[(\\sigma_{*}(\\theta^{\\top}x))^{l}(\\theta^{\\top}x)^{2j-k}(\\omega^{\\top}x)^{k}\\sigma^{(i+1)}\\big(\\omega^{\\top}x\\big)\\big(\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{i}(\\theta^{\\top}x-\\theta^{\\top}\\omega\\omega^{\\top}x)\\bigg].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{(\\mathrm{B.1}2)}\\\\ {=c_{\\eta}^{i}\\kappa^{k}d^{-j}\\mathbb{E}\\Bigg[\\underbrace{\\left(\\sigma_{*}(\\theta^{\\top}x)\\right)^{l}}_{\\mathrm{IE}\\geq\\Big\\{l\\geq I\\Big\\}}}&{\\underbrace{\\left(\\theta^{\\top}x\\right)^{2j-k}(\\omega^{\\top}x)^{k}\\sigma^{(i+1)}\\left(\\omega^{\\top}x\\right)\\left(\\sigma^{(1)}(\\omega^{\\top}x)\\right)^{i}\\left(\\theta^{\\top}x-\\theta^{\\top}\\omega\\right)}_{\\mathrm{degre~w.r.t},\\ \\theta^{\\top}x\\mathrm{~is~at~most~}2j-k+1}}\\\\ {\\qquad}&{\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lesssim c_{\\eta}^{i}\\kappa^{k}d^{-j}\\kappa^{(\\mathrm{IE}(\\sigma_{*}^{l})-2j+k-1)\\vee0}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $i\\leq I-2$ and $\\eta=c_{\\eta}d^{-1}$ , we have $l\\leq i+1<I$ and $\\operatorname{IE}((\\sigma_{*}(\\pmb{\\theta}^{\\top}\\pmb{x}))^{l})\\geq p_{*}+1$ . Thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathbf{B}.13)\\lesssim\\kappa^{p_{*}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $i\\geq I,\\mathrm{IE}((\\sigma_{*}(\\pmb{\\theta}^{\\top}\\pmb{x}))^{l})\\geq p_{*}$ and we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathrm{B}.13)\\lesssim c_{\\eta}^{I}\\kappa^{p_{*}-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now the case of $i=I-1$ . When $i=I-1$ and $j\\neq0$ , and using the assumption that $\\kappa\\leq c_{\\eta}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathrm{B}.13)\\lesssim c_{\\eta}^{I-1}\\kappa^{p_{*}-1}(\\kappa^{-2}/d)\\leq c_{\\eta}^{I}\\kappa^{p_{*}-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $i=I-1$ , $j=0$ , and $k\\neq0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathrm{B}.13)\\lesssim c_{\\eta}^{I-1}\\kappa^{p_{*}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $i=I-1,j=0,k=0$ , and $l\\leq I-1$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\mathrm{B}.13)\\lesssim c_{\\eta}^{I-1}\\kappa^{p_{*}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, except for $i\\;=\\;I\\,-\\,1,\\;j\\;=\\;0,\\;k\\;=\\;0.$ , and $l\\,\\leq\\,I\\,-\\,1$ , we can bound (B.13) by $\\lesssim$ $c_{n}^{I}\\kappa^{p_{*}-1}+\\kappa^{p_{*}}$ . The lower bound follows in the same way. Finally, consider the case of $i=I-1$ , $j^{'}\\!\\!=0,k=0$ , and $l=I$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n)=\\eta^{I-1}\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}[z^{I-1}]\\mathbb{E}\\bigg[(\\sigma_{*}(\\theta^{\\top}x))^{I}\\sigma^{(I+1)}\\big(\\omega^{\\top}x\\big)\\big(\\sigma^{(1)}(\\omega^{\\top}x)\\big)^{I-1}(\\theta^{\\top}x-\\theta^{\\top}\\omega\\omega^{\\top}x)\\bigg]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\eta^{I-1}\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}[z^{I-1}]\\sum_{m=p_{*}}^{C_{\\sigma}I}m!H(\\sigma_{*}^{I};m)H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};m-1)(1-\\kappa^{2})\\kappa^{m-1}}\\\\ &{=c_{\\eta}^{I-1}p_{*}!d^{-(I-1)}\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}[z^{I-1}]H(\\sigma_{*}^{I};p_{*})H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)(1-\\kappa^{2})\\kappa^{p_{*}-1}+O(c_{\\eta}^{I-1}\\kappa^{p_{*}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Putting it all together (recovering the constants omitted in (B.12) again), ", "page_idx": 24}, {"type": "text", "text": "(B.11) ", "page_idx": 24}, {"type": "equation", "text": "$$\n=c_{\\eta}^{I-1}\\frac{p_{*}!d^{-(I-1)}\\mathbb{E}_{z\\sim\\chi_{d-2}^{2}}[z^{I-1}]}{(I-1)!}H(\\sigma_{*}^{I};p_{*})H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)\\kappa^{p_{*}-1}+O(c_{\\eta}^{I}\\kappa^{p_{*}-1}+\\kappa^{p_{*}}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\varepsilon}[\\theta^{\\top}g]=(\\mathrm{B}.10)+(\\mathrm{B}.11)}\\\\ &{=c_{\\eta}^{I-1}\\underbrace{\\frac{p_{\\ast}!d-(I-1)\\mathbb{E}_{z\\sim\\mathcal{X}_{d-2}^{2}}\\left[z^{I-1}\\right]}{(I-1)!}}_{=\\Theta(1)}H(\\sigma_{\\ast}^{I};p_{\\ast})H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{\\ast}-1)\\kappa^{p_{\\ast}-1}+O(c_{\\eta}^{I}\\kappa^{p_{\\ast}-1}+\\kappa^{p_{\\ast}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining (i) and (ii), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}]\\ge2c_{\\eta}^{I-1}c_{\\sigma}\\kappa^{p_{*}-1}+O(c_{\\eta}^{I}\\kappa^{p_{*}-1}+\\kappa^{p_{*}})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for a positive constant $c_{\\sigma}~=~\\Theta(1)$ . Here $c_{\\sigma}~>~0$ satisfies $2c_{\\sigma}\\ =\\ 2p_{*}!\\alpha_{p_{*}}\\beta_{p_{*}}$ (for (i)) or $\\begin{array}{r}{2c_{\\sigma}\\,=\\,\\frac{p_{*}!H(\\sigma_{*}^{I};p_{*})H(\\sigma^{(I)}(\\sigma^{(1)})^{I-1};p_{*}-1)}{(I-1)!}}\\end{array}$ (for (ii)). Going back to (B.5), by setting $\\nu^{2t}=(\\theta^{\\top}g^{2t}-$ $\\mathbb{E}[\\pmb{\\theta}^{\\top}\\pmb{g}^{2t}])$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{s}^{2(t+1)}\\ge\\kappa^{2t}+2c_{\\eta}c_{\\xi}d^{-\\frac{p_{\\pi}}{2}\\vee1}\\mathbb{E}[\\theta^{\\top}g^{2t}]+c_{\\eta}c_{\\xi}d^{-\\frac{p_{\\pi}}{2}\\vee1}(\\theta^{\\top}g^{2t}-\\mathbb{E}[\\theta^{\\top}g^{2t}])+O(c_{\\eta}^{2}c_{\\xi}^{2}d^{-(p_{*}-1)\\vee1}\\kappa^{2t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\kappa^{2t}+2c_{\\eta}^{I}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}c_{\\sigma}(\\kappa^{2t})^{p_{*}-1}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}\\nu^{2t}}\\\\ &{\\qquad+\\,O\\Big(c_{\\eta}^{2}c_{\\xi}^{2}d^{-(p_{*}-1)\\vee1}(\\kappa^{2t})^{2t}+c_{\\eta}^{I+1}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}(\\kappa^{2t})^{p_{*}-1}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}(\\kappa^{2t})^{p_{*}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "When $c_{\\xi}\\ \\ \\leq\\ c_{\\eta}^{I}$ and $c_{\\eta}^{-1}d^{-\\frac{1}{2}}\\;\\;\\leq\\;\\;\\kappa\\;\\;\\leq\\;\\;c_{\\eta}^{I}$ , terms in the big- $O$ notation is smaller than $c_{\\eta}^{I}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}c_{\\sigma}(\\kappa^{\\dot{2}t})^{p_{*}-1}$ and we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\kappa^{2(t+1)}\\geq\\kappa^{2t}+c_{\\eta}^{I}c_{\\xi}c_{\\sigma}d^{-\\frac{p_{*}}{2}\\vee1}(\\kappa^{2t})^{p_{*}-1}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}\\nu^{2t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is straightforward to check $\\nu^{2t}$ has sub-Weibull tail. ", "page_idx": 25}, {"type": "text", "text": "B.4 Weak Recovery: Stochastic Update ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This subsection proves weak recovery using the results on population update from the previous section. Specifically, from the previous section, we know that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\boldsymbol{\\theta}^{\\top}\\boldsymbol{w}^{2(t+1)}\\ge\\boldsymbol{\\theta}^{\\top}\\boldsymbol{w}^{2t}+c_{\\eta}^{I}c_{\\xi}c_{\\sigma}d^{-\\frac{p_{*}}{2}\\vee1}(\\kappa^{2t})^{p_{*}-1}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}\\vee1}\\nu^{2t},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with the mean-zero sub-Weibull random variable $\\nu^{2t}$ and a positive $c_{\\sigma}\\;=\\;\\Theta(1)$ . For notational simplicity we write $c_{\\eta}^{I}c_{\\sigma}=c_{1}$ . The following lemma is a detailed version of Proposition 9. ", "page_idx": 25}, {"type": "text", "text": "Lemma 17. Take $\\eta^{2t},\\eta^{2t+1}=\\eta=c_{\\eta}d^{-1}$ , $\\xi^{2(t+1)}=\\xi=1-c_{\\xi}d^{-\\frac{(p_{*}-2)}{2}+}$ . Suppose that the link function satisfies $\\mathrm{IE}(\\sigma_{*}^{I})=\\mathrm{GE}(\\sigma_{*})=p_{*}$ (we choose the smallest such $I$ ) and activation functions satisfy all of the assumptions in Section B.1 for weak recovery. Let ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{1,1}=C_{3}c_{\\xi}^{-1}\\!\\left\\{\\!\\!\\begin{array}{l l}{{d}}&{{(i f p_{*}=\\mathrm{GE}(\\sigma_{*})=1)}}\\\\ {{d(\\log d)}}&{{(e l s e\\,i f p_{*}=\\mathrm{GE}(\\sigma_{*})=2)}}\\\\ {{d^{p_{*}-1}}}&{{(e l s e\\,p_{*}=\\mathrm{GE}(\\sigma_{*})\\geq3),}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and take $c_{\\xi}\\,\\lesssim\\,\\delta\\mathrm{poly}(c_{\\eta}),\\,c_{2}\\,\\gtrsim\\,\\mathrm{poly}(c_{\\eta}),$ , and $C_{3}\\,\\simeq\\,c_{1}^{-1}$ . If $\\kappa^{0}\\,\\geq\\,2c_{\\eta}^{-1}d^{-\\frac{1}{2}}$ , there exists some $\\tau_{*}\\le T_{1,1}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\kappa^{2\\tau_{*}}\\geq2c_{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with probability at least $1-\\delta$ , and $\\kappa^{2\\tau}\\geq2c_{2}$ for all $\\tau_{*}\\le\\tau\\le T_{1,1}.$ , with high probability. ", "page_idx": 25}, {"type": "text", "text": "We may take $\\delta\\:=\\:o_{d}(1)$ with arbitrarily slow decay. The proof is adapted from [BAGJ21], but our bound on $T_{1,1}$ is slightly sharper (by a $\\log d$ factor for $p_{*}~=~2$ and by a $(\\log d)^{2}$ factor for $p_{*}~\\geq~3)$ . For $p_{*}\\,=\\,2$ , this is because of a trick that we carefully \u201crestart\u201d the dynamics, whose failure probability exponentially decays. ", "page_idx": 25}, {"type": "text", "text": "Proof. We divide the proof into the following cases. ", "page_idx": 26}, {"type": "text", "text": "(i) When $p_{*}=1$ . Note that $\\{\\sum_{s=0}^{\\tau}\\nu^{2s}\\}_{\\tau}$ is Martingale with $\\mathbb{E}[(\\nu^{2s})^{2}]\\lesssim1$ . By Doob\u2019s maximal inequality and Markov\u2019s inequality, with probability $1-\\delta$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{0\\leq\\tau\\leq T}\\bigg|\\sum_{s=0}^{\\tau}\\nu^{2s}\\bigg|^{2}\\leq\\delta^{-1}\\mathbb{E}[(\\sum_{s=0}^{T}\\nu^{2s})^{2}]\\leq\\delta^{-1}\\sum_{s=0}^{T}\\mathbb{E}[(\\nu^{2s})^{2}]\\leq C_{1}\\delta^{-1}(T+1)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for any fixed $T\\geq0$ , with a sufficiently large constant $C_{1}=\\Theta(1)$ . In the following we consider the case when (B.14) holds for $T=c_{1}^{-1}c_{\\xi}^{-1}d-1$ . ", "page_idx": 26}, {"type": "text", "text": "If $c_{\\eta}^{-1}d^{-\\frac{1}{2}}\\leq\\kappa^{2t}\\leq c_{\\eta}^{I}$ for all $t=0,1,\\cdot\\cdot\\cdot,\\tau$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\kappa^{2(\\tau+1)}\\ge\\kappa^{2\\tau}+c_{1}c_{\\xi}d^{-1}+c_{\\eta}c_{\\xi}d^{-1}\\nu^{2\\tau}}}\\\\ &{}&{\\ge2c_{\\eta}^{-1}d^{-\\frac{1}{2}}+c_{1}c_{\\xi}(\\tau+1)d^{-1}\\gamma-c_{\\eta}c_{\\xi}d^{-1}\\bigg|\\sum_{s=0}^{\\tau}\\nu^{2s}\\bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, applying (B.14) to get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\kappa^{2(\\tau+1)}\\geq(\\mathrm{B}.15)\\geq2c_{\\eta}^{-1}d^{-\\frac{1}{2}}+c_{1}c_{\\xi}(\\tau+1)d^{-1}-c_{\\eta}c_{\\xi}^{\\frac{1}{2}}c_{1}^{-\\frac{1}{2}}C_{1}^{\\frac{1}{2}}\\delta^{-\\frac{1}{2}}d^{-\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "when $\\tau\\,\\leq\\,c_{1}^{-1}c_{\\xi}^{-1}d-1$ . By letting $c_{\\xi}\\,\\leq\\,c_{\\eta}^{-4}c_{1}C_{1}^{-1}\\delta$ , we have $c_{\\eta}^{-1}d^{-\\frac{1}{2}}\\leq c_{\\eta}c_{\\xi}^{\\frac{1}{2}}c_{1}^{-\\frac{1}{2}}C_{1}^{\\frac{1}{2}}\\delta^{-\\frac{1}{2}}d^{-\\frac{1}{2}}$ , and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\kappa^{2(\\tau+1)}\\geq c_{\\eta}^{-1}d^{-\\frac{1}{2}}+c_{1}c_{\\xi}(\\tau+1)d^{-1},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which verifies $c_{\\eta}^{-1}d^{-\\frac{1}{2}}\\leq\\kappa^{2t}$ for $t=\\tau+1$ . Thus, there exists some $\\tau_{*}\\leq c_{1}^{-1}c_{\\xi}^{-1}d$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\kappa^{2\\tau_{*}}\\geq4c_{2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $c_{1}\\leq\\textstyle\\frac{1}{4}c_{\\eta}^{I}$ , with probability $1-\\delta$ . ", "page_idx": 26}, {"type": "text", "text": "Now we prove that $\\kappa^{2t}\\geq2c_{2}$ holds for all $\\tau_{*}\\,\\leq\\,t\\,\\leq\\,T_{1,1}\\,=\\,C_{3}c_{\\xi}^{-1}d$ . Because $\\nu^{2t}$ are mean-zero sub-Weibull random variables, we also have that $\\begin{array}{r}{|\\sum_{s=\\tau}^{\\tau+\\tau^{\\prime}-1}\\nu^{2s}|\\ \\leq\\ C_{4}\\sqrt{\\tau^{\\prime}}}\\end{array}$ for all $0\\;\\leq\\;\\tau,\\tau^{\\prime}\\;\\leq$ $T_{1,1}$ with high probability. Also, because $\\eta^{t}\\ll\\,d^{-1}$ and $|1-\\xi^{t}|\\ll1$ , we can easily see that $|\\kappa^{2(\\tau+1)}-\\kappa^{2\\tau}|=\\tilde{O}(d^{-1})$ for all $\\tau=0,1,\\cdot\\cdot\\cdot\\,,T_{1,1}-1$ , with high probability. Thus, when there exists $\\tau\\geq\\tau_{*}$ such that $\\kappa^{2(\\tau-1)}\\geq4c_{2}$ and $\\kappa^{2\\tau}<4c_{2}$ , we have $\\kappa^{2\\tau}\\geq3c_{2}$ with high probability. Moreover, following the above argument, we can inductively show that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\kappa^{2(\\tau+\\tau^{\\prime})}\\geq3c_{2}+c_{1}c_{\\xi}\\tau^{\\prime}d^{-1}-c_{\\eta}c_{\\xi}d^{-1}C_{4}\\sqrt{\\tau^{\\prime}}}\\\\ &{\\qquad\\qquad\\geq3c_{2}+c_{1}c_{\\xi}\\tau^{\\prime}d^{-1}-\\left\\{c_{2}\\vphantom{\\frac{1}{c_{2}c_{\\xi}}}\\left(\\tau^{\\prime}\\leq c_{\\eta}^{-2}c_{\\xi}^{-2}C_{4}^{-2}c_{2}^{2}d^{2}\\right)\\right.}\\\\ &{\\qquad\\qquad\\geq\\left.2c_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $\\tau^{\\prime}\\leq T_{1,1}=C_{3}c_{\\xi}^{-1}d$ or until $\\kappa^{2(\\tau+\\tau^{\\prime})}\\geq4c_{2}$ holds again. By repeating this argument (if there are multiple such $\\tau$ ), we see that $\\kappa^{2t}~\\geq~2c_{2}$ holds for all $\\tau_{*}\\,\\leq\\,t\\,\\leq\\,T_{1,1}\\,=\\,C_{3}c_{\\xi}^{-1}d$ with high probability. ", "page_idx": 26}, {"type": "text", "text": "(ii) When $p_{*}\\,=\\,2$ . We define $\\iota_{0}\\,=\\,0,\\iota_{1}\\,=\\,\\log_{(1+c_{1}c_{\\xi}d^{-1})}(4),\\iota_{2}\\,=\\,2\\log_{(1+c_{1}c_{\\xi}d^{-1})}(4),\\cdot\\cdot\\cdot$ . We show that, for each $i$ , if $\\kappa^{2\\iota_{i}}\\;\\geq\\;2c_{\\eta}^{-1}d^{-\\frac{1}{2}}$ , we have $\\kappa^{2(\\iota_{i+1})}~\\geq~2\\kappa^{2\\iota_{i}}$ , with probability at least $1-\\delta4^{-i}$ , or there exists some $t$ $(\\iota_{i}<t\\leq\\iota_{i+1})$ with $\\kappa^{2t}>c_{\\eta}^{I}$ . ", "page_idx": 26}, {"type": "text", "text": "Assume that the above statement holds until some $i-1\\geq0$ (we do not need to assume anything for $i=0$ ). Then, we have $\\kappa_{\\ j}^{2\\iota_{i}}\\geq2^{i}\\kappa^{0}\\geq2c_{\\eta}^{-1}d^{-\\frac{1}{2}}$ . Similarly to (B.15), if $c_{\\eta}^{-1}d^{-\\frac{1}{2}}\\leq\\kappa^{2t}\\leq c_{\\eta}^{I}$ for all $t=\\iota_{i},\\iota_{i}+1,\\cdots,\\tau$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\kappa^{2(\\tau+1)}\\geq\\kappa^{2\\iota_{i}}+c_{1}c_{\\xi}d^{-1}\\sum_{s=\\iota_{i}}^{\\tau}\\kappa^{2s}-c_{\\eta}c_{\\xi}d^{-1}\\bigg|\\sum_{s=\\iota_{i}}^{\\tau}\\nu^{2s}\\bigg|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Applying (B.14) with $\\delta=\\delta/4^{i}$ and $T={\\textstyle\\frac{1}{4}}c_{\\eta}^{-2}c_{\\xi}^{-2}C^{-1}(\\delta/4^{i})(\\kappa^{2\\iota_{i}})^{2}d^{2}-1$ to get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\kappa^{2(\\tau+1)}\\geq\\kappa^{2\\iota_{i}}+c_{1}c_{\\xi}d^{-1}\\displaystyle\\sum_{s=\\iota_{i}}^{\\tau}\\kappa^{2s}-c_{\\eta}c_{\\xi}d^{-1}C^{\\frac{1}{2}}\\delta^{-\\frac{1}{2}}\\sqrt{\\tau+1-\\iota_{i}}}}\\\\ {{\\qquad\\geq\\kappa^{2\\iota_{i}}+c_{1}c_{\\xi}d^{-1}\\displaystyle\\sum_{s=\\iota_{i}}^{\\tau}\\kappa^{2s}-\\displaystyle\\frac{1}{2}\\kappa^{2\\iota_{i}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r}{\\tau\\,\\le\\,\\iota_{i}\\,+\\,\\frac{1}{4}c_{\\eta}^{-2}c_{\\xi}^{-2}C^{-1}(\\delta/4^{i})(\\kappa^{2\\iota_{i}})^{2}d^{2}\\,-\\,1}\\end{array}$ , which verifies $c_{\\eta}^{-1}d^{-\\frac{1}{2}}~\\leq~\\frac{1}{2}\\kappa^{2\\iota_{i}}~\\leq~\\kappa^{2t}$ for $t=\\tau+1$ . ", "page_idx": 27}, {"type": "text", "text": "This implies that, with probability $1-\\delta/4^{i}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa^{2(\\tau+1)}\\geq\\frac{1}{2}\\kappa^{2\\iota_{i}}+c_{1}c_{\\xi}d^{-1}\\sum_{s=\\iota_{i}}^{\\tau}\\kappa^{2s}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $\\begin{array}{r}{\\tau=\\frac{1}{4}c_{\\eta}^{-2}c_{\\xi}^{-2}C^{-1}(\\delta/4^{i})(\\kappa^{2\\iota_{i}})^{2}d^{2}-1}\\end{array}$ , which is equivalent to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa^{2\\tau}\\geq(1+c_{1}c_{\\xi}d^{-1})^{\\tau-\\iota_{i}}\\frac{1}{2}\\kappa^{2\\iota_{i}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $\\begin{array}{r l r}{\\tau}&{{}=}&{\\iota_{i},\\iota_{i}\\ +\\ 1,\\cdots,\\iota_{i}\\ +\\ \\frac{1}{4}c_{\\eta}^{-2}c_{\\xi}^{-2}C^{-1}(\\delta/4^{i})(\\kappa^{2\\iota_{i}})^{2}d^{2}}\\end{array}$ . By taking $\\mathit{c}_{\\xi}\\quad\\ll$ $c_{1}c_{\\eta}^{-2}C^{-1}(\\delta/4^{i})(\\kappa^{2\\iota_{i}})^{2}d$ , we have $\\begin{array}{r}{\\frac{1}{4}c_{\\eta}^{-2}c_{\\xi}^{-2}C^{-1}(\\delta/4^{i})(\\kappa^{2\\iota_{i}})^{2}d^{2}\\geq\\log_{(1+c_{1}c_{\\xi}d^{-1})}(4)}\\end{array}$ , and we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa^{2\\iota_{i+1}}\\geq2\\kappa^{2\\iota_{i}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with probability $1-\\delta/4^{i}$ (or there exists $t\\leq\\iota_{i+1}$ such that $\\kappa^{2t}>c_{\\eta}^{I}.$ ). ", "page_idx": 27}, {"type": "text", "text": "Thus, by induction, for all $i$ , we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa^{2\\iota_{i}}\\geq2^{i}\\kappa^{0},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "or that there exists some $t\\leq\\iota_{i}$ such that $\\kappa^{2t}$ is larger than $c_{\\eta}^{I}$ , with probability $1-\\delta$ . ", "page_idx": 27}, {"type": "text", "text": "The LHS of (B.16) becomes larger than $c_{\\eta}^{I}$ for some $i\\leq\\log d$ . Because $\\iota_{i}=\\Theta(i c_{1}^{-1}c_{\\xi}^{-1}d)$ , within $O(c_{1}^{-1}c_{\\xi}d\\log d)$ steps, there exists at least one $\\tau_{*}\\,=\\,O(c_{1}^{-1}c_{\\xi}^{-1}d\\log d)$ such that $\\kappa^{2\\tau_{*}}\\,\\geq\\,4c_{2}$ for $c_{2}\\leq\\textstyle{\\frac{1}{4}}c_{\\eta}^{I}$ , with probability $1-\\delta$ . ", "page_idx": 27}, {"type": "text", "text": "Once such $\\tau_{*}$ is obtained, following the last paragraph of (i), we can see that $\\kappa^{2t}\\geq2c_{2}$ holds until $t=T_{1,1}$ with high probability. ", "page_idx": 27}, {"type": "text", "text": "(iii) When $p_{*}\\geq3$ . We apply (B.14) with p\u22171\u22122c1\u2212 1c\u03be\u2212 1dp2\u2217 (\u03ba0)\u2212(p\u2217\u22122) to obtain that ", "page_idx": 27}, {"type": "equation", "text": "$$\nc_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}}\\bigg|\\sum_{s=0}^{\\tau}\\nu^{2s}\\bigg|\\le c_{\\eta}c_{\\xi}^{\\frac{1}{2}}c_{1}^{-\\frac{1}{2}}C^{\\frac{1}{2}}\\delta^{-\\frac{1}{2}}d^{-\\frac{p_{*}}{4}}\\big(\\kappa^{0}\\big)^{-\\frac{p_{*}-2}{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $\\tau=0,1,\\cdots\\,,T-1$ , with probability $1-\\delta$ . ", "page_idx": 27}, {"type": "text", "text": "We take $c_{\\xi}\\ll c_{\\eta}^{-2}c_{1}C^{-1}\\delta d^{\\frac{p_{*}}{4}}(\\kappa^{0})^{\\frac{p_{*}}{2}}$ so that (B.17) is bounded by $c_{1}^{-1}d^{-\\frac{1}{2}}$ . Then, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\kappa^{2(\\tau+1)}\\geq\\kappa^{0}+c_{1}c_{\\xi}d^{-\\frac{p_{*}}{2}}\\displaystyle\\sum_{s=0}^{\\tau}(\\kappa^{2s})^{p_{*}-1}+c_{\\eta}c_{\\xi}d^{-\\frac{p_{*}}{2}}\\displaystyle\\sum_{s=0}^{\\tau}\\nu^{2s}}}\\\\ {{\\geq c_{\\eta}^{-1}d^{-\\frac{1}{2}}+c_{1}c_{\\xi}d^{-\\frac{p_{*}}{2}}\\displaystyle\\sum_{s=0}^{\\tau}(\\kappa^{2\\tau})^{p_{*}-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It is easy to see that $\\kappa^{2(\\tau+1)}$ is lower bounded by $a^{\\tau+1}$ , where $a^{0}\\,=\\,c_{\\eta}^{-1}d^{-\\frac{1}{2}}$ and $a^{\\tau+1}\\,=\\,a^{\\tau}\\,+$ $c_{1}c_{\\xi}d^{-\\frac{p_{*}}{2}}(a^{\\tau})^{p_{*}-1}$ . By applying Lemma 18, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\kappa^{2\\tau}\\geq\\frac{\\kappa^{0}}{\\left(1-c_{1}c_{\\xi}d^{-\\frac{p_{*}}{2}}(p_{*}-2)(\\kappa^{0})^{(p_{*}-2)}t\\right)^{\\frac{1}{p_{*}-2}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, until $\\tau\\leq\\big(c_{1}c_{\\xi}d^{-\\frac{p_{*}}{2}}(p_{*}-2)\\big(\\kappa^{0}\\big)^{(p_{*}-2)}\\big)^{-1}\\leq T+1\\ll d^{p_{*}-1}$ , with probability at least $1-\\delta$ , there exists some $\\tau_{*}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\kappa^{2\\tau_{*}}\\geq4c_{2}\\geq c_{\\eta}^{I}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "when $\\begin{array}{r}{c_{2}\\leq\\frac{1}{4}c_{\\eta}^{I}}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Once such $\\tau_{*}$ is obtained, following the last paragraph of (i), we can see that $\\kappa^{2t}\\geq2c_{2}$ holds until $t=T_{1,1}$ with high probability. ", "page_idx": 28}, {"type": "text", "text": "In the above proof we used the (discrete version of) Bihari\u2013LaSalle inequality from [BAGJ22]. ", "page_idx": 28}, {"type": "text", "text": "Lemma 18. For $p\\geq3$ and $c>0$ , consider a positive sequence $(a^{t})_{t\\geq0}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\na^{t+1}=a^{t}+c(a^{t})^{p-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\na^{t}\\geq\\frac{a^{0}}{\\left(1-c(p-2)(a^{0})^{(p-2)}t\\right)^{\\frac{1}{p-2}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. From definition, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nc=\\frac{a^{t+1}-a^{t}}{(a^{t})^{p-1}}\\leq\\int_{t=a^{t}}^{a^{t+1}}\\frac{1}{x^{p-1}}\\leq\\frac{1}{p-2}\\left[\\frac{1}{(a^{t})^{p-2}}-\\frac{1}{(a^{t+1})^{p-2}}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking the summation and re-arranging the terms yield ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{(a^{t})^{-(p-2)}\\leq(a^{0})^{-(p-2)}-c(p-2)t,}\\\\ {\\therefore a^{t}\\geq\\displaystyle\\frac{a^{0}}{\\left(1-c(p-2)(a^{0})^{(p-2)}t\\right)^{\\frac{1}{p-2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which gives the lower bound. ", "page_idx": 28}, {"type": "text", "text": "B.5 From Weak Recovery to Strong Recovery ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the previous subsection, we proved that after $t\\,=\\,2T_{1,1}\\,=\\,\\tilde{\\Theta}(d)$ steps, with probability $\\tilde{\\Omega}(1)$ over the randomness of initialization, we obtain nontrivial alignment \u03baj2T $\\kappa_{j}^{2T_{1,1}}\\geq2c_{2}$ . This subsection discusses how to convert the weak recovery into the strong recovery. ", "page_idx": 28}, {"type": "text", "text": "Lemma 19. Suppose the neuron satisfies $\\kappa^{2T_{1,1}}~\\geq~2c_{2}$ . Take $\\eta^{2t}\\,=\\,\\eta\\,=\\,\\bar{c}_{\\eta}\\varepsilon d^{-1}$ , $\\eta^{2t+1}\\,=\\,0,$ , $\\xi^{2(t+1)}\\,=\\,0$ for all $t\\,\\geq\\,T_{1,1}$ , where $\\bar{c}_{\\eta}\\,\\lesssim\\,\\mathrm{poly}(c_{1})$ . If the activation functions satisfy all of the assumptions in Section B.1 for strong recovery, then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{\\top}\\pmb{w}^{2(T_{1,1}+\\tau_{*})}\\geq1-\\varepsilon,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with high probability, where $\\tau_{*}\\;\\leq\\;T_{1,2}\\;=\\;C_{3}d\\varepsilon^{-2}$ . Moreover, $\\pmb{\\theta}^{\\top}\\pmb{w}^{2(T_{1,1}+t)}\\ \\geq\\ 1-\\varepsilon$ for all $\\tau_{*}\\le t\\le T_{1,2}=C_{3}d\\varepsilon^{-2}$ , with high probability. ", "page_idx": 28}, {"type": "text", "text": "Proof. Consider the Hermite expansions of $\\sigma_{*}$ and $\\sigma$ . Let $p$ be the smallest degree that both $\\sigma_{*}$ and $\\sigma$ have non-zero coefficients. First we compute the population gradient (of the correlation term) as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\tilde{\\nabla}_{w}y\\sigma\\big({w^{2t}}^{\\top}x\\big)\\big]=\\mathbb{E}\\bigg[\\tilde{\\nabla}_{w}\\bigg(\\displaystyle\\sum_{j=p}^{\\infty}\\alpha_{j}\\mathsf{H e}_{j}(\\theta^{\\top}x)\\bigg)\\bigg(\\displaystyle\\sum_{j=0}^{\\infty}\\beta_{j}\\mathsf{H e}_{j}({w^{2t}}^{\\top}x)\\bigg)\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=p}^{\\infty}\\big[j!\\alpha_{j}\\beta_{j}(\\theta^{\\top}w^{2t})^{j-1}\\theta+(j+2)!\\alpha_{j}\\beta_{j+2}(\\theta^{\\top}w^{2t})^{j}w^{2t}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying $P_{w^{2t}}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[P_{w^{2t}}\\tilde{\\nabla}_{w}y\\sigma({w^{2t}}^{\\top}{x})\\big]=(\\pmb{\\theta}-({w^{2t}}^{\\top}\\pmb{\\theta}){w^{2t}})\\sum_{j=p}^{\\infty}j!\\alpha_{j}\\beta_{j}(\\pmb{\\theta}^{\\top}{w^{2t}})^{j-1}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, the update of the alignment $\\kappa^{2t}=\\pmb{\\theta}^{\\top}\\pmb{w}^{2t}$ is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\kappa^{2(t+1)}\\geq\\kappa^{2t}+\\eta\\pmb{\\theta}^{\\top}\\pmb{g}-\\frac{1}{2}\\eta^{2}\\kappa^{2t}\\|\\pmb{g}\\|^{2}-\\frac{1}{2}\\eta^{3}\\tilde{\\eta}^{3}|\\pmb{\\theta}^{\\top}\\pmb{g}|\\|\\pmb{g}\\|^{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{g}=P_{\\pmb{w}^{2t}}y\\sigma^{\\prime}(\\pmb{w}^{2t^{\\top}}\\pmb{x})\\pmb{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From (B.18), the expectation of (B.19) is bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\kappa^{2(t+1)}]\\ge\\kappa^{2t}+\\eta(1-(\\kappa^{2t})^{2})\\displaystyle\\sum_{j=p}^{\\infty}j!\\alpha_{j}\\beta_{j}(\\pmb{\\theta}^{\\top}\\pmb{w}^{2t})^{j-1}-\\eta^{2}C_{4}d(\\kappa^{2t}+\\eta)}\\\\ &{\\qquad\\qquad\\ge\\kappa^{2t}+\\eta(1-(\\kappa^{2t})^{2})\\displaystyle\\sum_{j=p}^{\\infty}j!\\alpha_{j}\\beta_{j}(\\kappa^{2t})^{p-1}-\\eta^{2}C_{4}d(\\kappa^{2t}+\\eta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By letting $\\eta\\leq c_{1}^{p-1}\\varepsilon d^{-1}$ , when $\\kappa^{2t}\\leq1-\\varepsilon$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\kappa^{2(t+1)}]\\ge\\kappa^{2t}+\\frac{1}{2}\\eta\\varepsilon\\sum_{j=p}^{\\infty}j!\\alpha_{j}\\beta_{j}(\\kappa^{2t})^{p-1}\\ge\\kappa^{2t}+\\eta\\varepsilon c_{1}^{p}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It is easy to see that the noise $\\nu^{2t}$ has sub-Weibull tail and we obtain that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\kappa^{2(t+1)}\\geq\\kappa^{2t}+\\frac{1}{2}\\eta\\varepsilon\\sum_{j=p}^{\\infty}j!\\alpha_{j}\\beta_{j}(\\kappa^{2t})^{p-1}+\\eta\\nu^{2t}\\geq\\kappa^{2t}+\\eta\\varepsilon c_{1}^{p}+\\eta\\nu^{2t}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Suppose that $2c_{2}\\,\\leq\\,\\kappa^{2(T1,1+\\tau)}\\,\\leq\\,1-\\varepsilon$ for all $t=0,1,\\dots,\\tau-1$ . By taking the summation of (B.20), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\kappa^{2(T1,1+\\tau)}\\ge\\kappa^{2T1,1}+\\eta\\varepsilon t c_{1}^{p}+\\eta\\sum_{s=T1,1}^{T1,1+\\tau-1}\\nu^{2t}\\ge2c_{2}+\\eta\\varepsilon\\tau c_{1}^{p}-C_{4}\\eta\\sqrt{\\tau},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with high probability. The third term is bounded by $C_{4}\\eta\\sqrt{\\tau}~\\leq~c_{2}$ when $\\tau~\\leq~c_{2}^{2}C_{4}^{-2}\\eta^{-2}~=~$ $c_{2}^{2}C_{4}^{-2}\\bar{c}_{\\eta}^{-2}\\varepsilon^{-2}d^{2}$ and by $\\scriptstyle{\\frac{1}{2}}\\eta\\varepsilon\\tau c_{1}^{p}$ when $\\tau\\;\\;\\geq\\;\\;4\\varepsilon^{-2}c_{1}^{-2p}C_{4}^{2}$ . Because $c_{2}^{2}C_{4}^{-2}\\bar{c}_{\\eta}^{-2}\\varepsilon^{-2}d^{2}\\;\\;\\;\\geq$ $4\\varepsilon^{-2}c_{1}^{-2p}C_{4}^{2}$ , we can bound (B.21) by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\kappa^{2(T1,1+\\tau)}\\geq c_{2}+\\frac{1}{2}\\eta\\varepsilon\\tau c_{1}^{p},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which verifies $2c_{2}\\leq\\kappa^{2(T1,1+\\tau)}$ . ", "page_idx": 29}, {"type": "text", "text": "Therefore, by induction, until $\\kappa^{2t}\\geq1-\\varepsilon$ , we have the lower bound (B.22), whose RHS exceeds $1\\,-\\,\\varepsilon$ when $\\tau\\;\\geq\\;2\\eta^{-1}\\varepsilon^{-1}c_{1}^{-p}\\;\\leq\\;C_{3}d\\varepsilon^{-2}$ . Thus, there exists $\\tau_{*}\\,\\le\\,T_{1,2}\\,=\\,C_{3}d\\varepsilon^{-2}$ such that \u03ba2(T 1,1+\u03c4\u2217) \u22651 \u2212\u03b5, with high probability. ", "page_idx": 29}, {"type": "text", "text": "Now, what remains is to prove that $\\kappa^{2(T1,1+\\tau)}\\,\\geq\\,1-\\,3\\varepsilon$ holds for all $\\tau_{*}\\,\\le\\,t\\,\\le\\,T_{1,2}\\,=\\,C_{3}d\\varepsilon^{-2}$ . Because $\\nu^{2t}$ are mean-zero sub-Weibull random variables, we have that $\\begin{array}{r}{|\\sum_{s=\\tau}^{\\tau+\\tau^{\\prime}-1}\\nu^{2s}|\\le C_{4}\\sqrt{\\tau^{\\prime}}}\\end{array}$ for all $0\\,\\leq\\,\\tau,\\tau^{\\prime}\\,\\leq\\,T_{1,1}$ with high probability. Also, because $\\eta^{t}\\ll\\varepsilon d^{-1}$ , we can easily see that $|\\kappa^{2(\\tau+1)}-\\kappa^{2\\tau}|=\\tilde{O}(\\varepsilon d^{-1})$ for all $\\tau=0,1,\\cdot\\cdot\\cdot\\,,T_{1,1}-1$ , with high probability. Thus, when there exists $\\tau\\geq\\tau_{*}$ such that $\\kappa^{2(T_{1,1}+\\tau-1)}\\geq1-\\varepsilon$ and $\\kappa^{2(T_{1,1}+\\tau)}<1-\\varepsilon$ , we have $\\kappa^{2(T_{1,1}+\\tau)}\\geq1-2\\varepsilon$ with high probability. Moreover, following the above argument, we can inductively show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\kappa^{2(T1,1+\\tau+\\tau^{\\prime})}\\geq1-2\\varepsilon+\\eta\\varepsilon\\tau^{\\prime}c_{1}^{p}-C_{4}\\eta\\sqrt{\\tau^{\\prime}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq1-2\\varepsilon+\\eta\\varepsilon\\tau^{\\prime}c_{1}^{p}-\\left\\{\\!\\!\\!\\begin{array}{l l}{\\varepsilon}&{(\\tau^{\\prime}\\leq\\bar{c}_{\\eta}^{-2}C_{4}^{-2}d^{2})}\\\\ {\\eta\\varepsilon\\tau^{\\prime}c_{1}^{p}}&{(\\tau^{\\prime}\\geq\\varepsilon^{-2}C_{4}^{2}c_{1}^{-2p})}\\end{array}\\!\\!\\right.}\\\\ &{\\geq1-3\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $\\tau^{\\prime}\\le T_{1,2}$ or until $\\kappa^{2(T1,1+\\tau+\\tau^{\\prime})}\\geq1-\\varepsilon$ holds again. Note that the last inequality follows from $\\bar{c}_{\\eta}^{-2}C_{4}^{-2}d^{2}\\geq\\varepsilon^{-2}C_{4}^{2}c_{1}^{-2p}$ . By repeating this argument (if there are multiple such $\\tau$ ), we can see that $\\kappa^{2(T1,1+t)}\\geq1-\\varepsilon$ holds for all $\\tau_{*}\\le t\\le T_{1,2}=C_{3}d\\varepsilon^{-2}$ with high probability. ", "page_idx": 30}, {"type": "text", "text": "Adjusting hidden constants to remove a factor of 3 from $3\\varepsilon$ yields the desired result. ", "page_idx": 30}, {"type": "text", "text": "B.6 Second Layer Training ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "From the previous analysis, we know that at least $\\Omega(1)$ portion of the neurons will satisfy the weak and strong recovery conditions (Appendix B.1), at least $\\tilde{\\Omega}(1)$ portion of the neurons (independent from the choice of $\\sigma_{j}$ ) satisfy initial alignment conditions (Appendix B.2), and at least $\\bar{1^{\\mathit{\\Pi}}}-o(1)$ fraction of them achieves strong recovery. ", "page_idx": 30}, {"type": "text", "text": "This subsection proves a generalization error bound after second-layer training. Let $f_{\\pmb{a}}(\\pmb{x})=f_{\\pmb{\\Theta}}(\\pmb{x})$ for $\\pmb{\\Theta}=(\\hat{\\pmb{w}}_{j},a_{j},\\hat{b}_{j})_{j=1}^{N}$ where $\\pmb{a}\\in\\mathbb{R}^{N}$ and $(\\hat{\\pmb{w}}_{j},\\hat{b}_{j})_{j=1}^{N}$ are the parameters trained in the first stage. Let $\\pmb{a}^{*}\\in\\mathbb{R}^{N}$ be the \u201ccertificate\u201d with $\\lVert\\boldsymbol{a}^{*}\\rVert^{2}=\\tilde{O}(N)$ that is shown to exist in Lemma 22. ", "page_idx": 30}, {"type": "text", "text": "Polynomial Link Functions. The following lemma is a complete version of Proposition 5. ", "page_idx": 30}, {"type": "text", "text": "Lemma 20. There exists a 4q-th order polynomial $Q(R_{w},b,q^{\\prime})$ of $R_{w}=\\operatorname*{max}_{j}\\|\\pmb{w}_{j}\\|$ , $\\boldsymbol{b}=(b_{j})_{j=1}^{N}$ such that, if we set \u03bb = \u0398  T22\u03b40 $\\begin{array}{r}{\\lambda=\\Theta\\Big(\\sqrt{\\frac{2}{T_{2}\\delta_{0}}N^{2}Q(R_{w},b,q^{\\prime})}\\Big)}\\end{array}$ for some $\\delta_{0}>0$ , the ridge estimator $\\hat{\\pmb{a}}$ satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|f_{\\hat{\\alpha}}-f_{*}\\|_{L^{2}(P_{x})}^{2}\\lesssim(N^{-2}+\\varepsilon^{2})+\\frac{1}{T_{2}\\lambda\\delta_{0}}\\big(2N^{2}Q(R_{w},b,q^{\\prime})+\\mathbb{E}_{x}[(f_{*})^{4}]\\big)+\\frac{3\\lambda}{2}\\|a^{*}\\|^{2}(\\mathrm{B}\\cdot\\mathrm{B}^{2})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with probability $1-\\delta_{0}$ . Therefore, by taking $T_{2}\\,=\\,\\tilde{\\Theta}\\big((N^{4}Q^{2}(R_{w},b,q^{\\prime})+\\mathbb{E}[f_{*}({\\pmb x})^{4}]^{2})\\varepsilon^{-4}\\big)$ and $N=\\tilde{\\Theta}(\\varepsilon^{-1})$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pmb{x}}[(f_{\\hat{\\pmb{a}}}(\\pmb{x})-f_{*}(\\pmb{x}))^{2}]\\lesssim\\varepsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Let $P_{T_{2}}$ be the empirical distribution of the second stage: $\\begin{array}{r}{P_{T_{2}}:=\\frac{1}{T_{2}}\\sum_{i=1}^{T_{2}}\\delta_{\\mathbf{x}_{i}}}\\end{array}$ . Let $\\psi(\\pmb{x})=$ $(\\sigma(\\langle\\pmb{x},\\widehat{\\pmb{w}}_{j})\\rangle+b_{j}))_{j=1}^{N}$ so that $f_{\\pmb{a}}(\\pmb{x})=\\langle\\pmb{a},\\psi(\\pmb{x})\\rangle$ . ", "page_idx": 30}, {"type": "text", "text": "Part (1). Here, we first bound the second term $\\|f_{\\hat{a}}\\,-\\,f_{*}\\|_{L^{2}(P_{T_{2}})}$ . Since $\\hat{\\mathcal{L}}(f_{\\hat{\\pmb{a}}})+\\lambda\\|\\hat{\\pmb{a}}\\|^{2}\\,\\leq$ $\\hat{\\mathcal L}(f_{a^{*}})+\\lambda\\|\\pmb{a}^{*}\\|^{2}$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|f_{\\hat{\\boldsymbol{a}}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}+\\lambda\\|\\hat{\\boldsymbol{a}}\\|^{2}}\\\\ {\\displaystyle\\leq\\|f_{\\boldsymbol{a}^{*}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}+\\frac{2}{T_{2}}\\sum_{i=1}^{T_{2}}(f_{\\boldsymbol{a}^{*}}(\\boldsymbol{x}_{i})-f_{\\hat{\\boldsymbol{a}}}(\\boldsymbol{x}_{i}))\\varepsilon_{i}+\\lambda\\|\\boldsymbol{a}^{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, by the Cauchy-Schwarz inequality, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{2}{T_{2}}\\sum_{i=1}^{T_{2}}(f_{a^{*}}(x_{i})-f_{\\hat{a}}(x_{i}))\\varepsilon_{i}=({a^{*}}-\\hat{a})^{\\top}\\frac{2}{T_{2}}\\sum_{i=1}^{T_{2}}\\psi(x_{i})\\varepsilon_{i}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\|{a^{*}}-\\hat{a}\\|\\sqrt{\\frac{\\sum_{i,j}\\varepsilon_{i}\\varepsilon_{j}\\psi(x_{i})^{\\top}\\psi(x_{j})}{T_{2}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By applying Markov\u2019s inequality to the right hand side, it can be further bounded by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\pmb{a}^{*}-\\hat{\\pmb{a}}\\|\\sqrt{\\frac{\\mathbb{E}_{\\pmb{x}}[\\|\\psi(\\pmb{x})\\|^{2}]}{T_{2}\\delta_{1}}}\\leq\\frac{\\lambda}{2}\\|\\hat{\\pmb{a}}\\|^{2}+\\frac{\\lambda}{2}\\|\\pmb{a}^{*}\\|^{2}+\\frac{\\mathbb{E}_{\\pmb{x}}[\\|\\psi(\\pmb{x})\\|^{2}]}{T_{2}\\delta_{1}\\lambda},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with probability $1-\\delta_{1}$ . Thus, by combining with (B.24), we arrive at ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}+\\frac{\\lambda}{2}\\|\\hat{a}\\|^{2}\\leq\\|f_{a^{*}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}+\\frac{\\mathbb{E}_{x}[\\|\\psi(\\pmb{x})\\|^{2}]}{T_{2}\\delta_{1}\\lambda}+\\frac{3\\lambda}{2}\\|\\pmb{a}^{*}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here, by using the evaluation $\\|f_{\\pmb{a}^{*}}-f_{*}\\|_{L^{2}(P_{T_{2}})}=\\tilde{O}(N^{-1}+\\varepsilon)$ in Lemma 22, the right hand side can be further bounded by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}+\\frac{\\lambda}{2}\\|\\hat{a}\\|^{2}\\le\\tilde{O}(N^{-2}+\\varepsilon^{2})+\\frac{\\mathbb{E}_{x}[\\|\\psi(x)\\|^{2}]}{T_{2}\\delta_{1}\\lambda}+\\frac{3\\lambda}{2}\\|a^{*}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Part (2). Next we lower bound $\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}$ by noticing that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}}\\\\ {\\displaystyle=\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{x})}^{2}+\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{x})}^{2}}\\\\ {\\displaystyle=\\|f_{\\hat{a}}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{\\hat{a}}\\|_{L^{2}(P_{x})}^{2}-2\\Bigg(\\frac{1}{T_{2}}\\sum_{i=1}^{T_{2}}f_{\\hat{a}}(x_{i})f_{*}(x_{i})-\\mathbb{E}[f_{\\hat{a}}(x_{i})f_{*}(x_{i})]\\Bigg)}\\\\ {\\displaystyle+\\|f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{*}\\|_{L^{2}(P_{x})}^{2}+\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{x})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The first two terms of Eq. (B.25) can be bounded by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\|f_{\\pmb{\\hat{a}}}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{\\pmb{\\hat{a}}}\\|_{L^{2}(P_{x})}^{2}\\bigg|=\\bigg|\\hat{a}^{\\top}\\Biggl(\\frac{\\sum_{i=1}^{T_{2}}\\psi\\left(\\pmb{x}_{i}\\right)\\psi\\left(\\pmb{x}_{i}\\right)^{\\top}}{T_{2}}-\\mathbb{E}_{\\mathbf{x}}[\\psi(\\pmb{x})\\psi(\\pmb{x})^{\\top}]\\Biggr)\\hat{a}\\bigg|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\|\\hat{a}\\|_{\\mathbf{\\hat{a}}:\\|\\mathbf{a}\\|\\leq1}^{2}\\bigg|\\|f_{\\pmb{a}}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{\\pmb{a}}\\|_{L^{2}(P_{x})}^{2}\\bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The standard Rademacher complexity bound yields that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(x_{i})_{i=1}^{T_{2}}}\\left[\\underset{u\\in\\mathbb{R}^{n}\\setminus[\\alpha]\\leq1]}{\\operatorname*{sup}}\\left\\lbrace\\|f_{\\alpha}\\|_{L^{2}(P_{\\alpha})}^{2}-\\|f_{\\alpha}\\|_{L^{2}(P_{\\alpha})}^{2}\\right\\rbrace\\right]}\\\\ &{\\leq2\\mathbb{E}_{(x_{i},\\sigma_{i})_{i=1}^{T_{2}}}\\left[\\underset{u\\in\\mathbb{R}^{n}\\setminus[\\alpha]\\leq1}{\\operatorname*{sup}}\\left|\\frac{1}{T_{2}}\\sum_{t=1}^{T_{2}}\\sigma_{t}f_{\\alpha}(x_{i})^{2}\\right|\\right]}\\\\ &{\\leq2\\sqrt{\\mathbb{E}_{(\\alpha_{i})_{i=1}^{T_{2}}}\\left[\\underset{u\\in\\mathbb{R}^{n}\\setminus[\\alpha]\\leq1}{\\operatorname*{sup}}\\frac{1}{T_{2}}\\sum_{i=1}^{T_{2}}(\\alpha^{\\top}\\psi(x_{i}))^{4}\\right]}}\\\\ &{\\leq2\\sqrt{\\mathbb{E}_{(\\alpha_{i})_{i=1}^{T_{2}}}\\left[\\frac{1}{T_{2}}\\frac{T_{2}}{\\sum_{i=1}^{T_{2}}\\|\\psi(x_{i})\\|^{4}}\\right]}}\\\\ &{=2\\sqrt{\\frac{1}{T_{2}}\\mathbb{E}_{\\alpha}\\|\\psi(x)\\|^{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(\\sigma_{i})_{i=1}^{T_{2}}$ is the i.i.d. Rademacher sequence which is independent of $({\\pmb x}_{i})_{i=1}^{T_{2}}$ . Hence, Markov\u2019s inequality yields that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\bigg|\\|f_{\\hat{a}}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{\\hat{a}}\\|_{L^{2}(P_{x})}^{2}\\bigg|=2\\|\\hat{a}\\|^{2}\\sqrt{\\frac{1}{T_{2}\\delta_{2}}\\mathbb{E}_{x}[\\|\\psi(\\pmb{x})\\|^{4}]},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probabilty $1-\\delta_{2}$ . ", "page_idx": 31}, {"type": "text", "text": "The third term in Eq. (B.25) can be evaluated as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{2\\Bigg(\\displaystyle\\frac{1}{T_{2}}\\sum_{i=1}^{T_{2}}f_{\\hat{a}}(\\pmb{x}_{i})f_{\\ast}(\\pmb{x}_{i})-\\mathbb{E}_{\\pmb{x}}[f_{\\hat{a}}(\\pmb{x})f_{\\ast}(\\pmb{x})]\\Bigg)}}\\\\ {{\\displaystyle=\\hat{\\pmb{a}}^{\\top}\\Bigg(\\frac{1}{T_{2}}\\sum_{i=1}^{T_{2}}(\\psi(\\pmb{x}_{i})f_{\\ast}(\\pmb{x}_{i})-\\mathbb{E}_{\\pmb{x}}[\\psi(\\pmb{x})f_{\\ast}(\\pmb{x})])\\Bigg)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\|\\hat{a}\\|\\sqrt{\\displaystyle\\frac{1}{T_{2}^{2}}\\sum_{i=1}^{T_{2}}\\sum_{j=1}^{T_{2}}(\\psi(x_{i})f_{*}(x_{i})-\\mathbb{E}_{x}[\\psi(x)f_{*}(x)])^{\\top}(\\psi(x_{j})f_{*}(x_{j})-\\mathbb{E}_{x}[\\psi(x)f_{*}(x)])}}\\\\ &{\\leq\\|\\hat{a}\\|\\sqrt{\\displaystyle\\frac{1}{T_{2}\\delta_{3}}\\mathbb{E}_{x}[\\|\\psi(x)f_{*}(x)-\\mathbb{E}_{x}[\\psi(x)f_{*}(x)]\\|^{2}]}}\\\\ &{\\leq\\|\\hat{a}\\|\\sqrt{\\displaystyle\\frac{1}{T_{2}\\delta_{3}}\\mathbb{E}_{x}[\\|\\psi(x)\\|^{4}+\\|f_{*}(x)\\|^{4}]}}\\\\ &{\\leq\\frac{\\lambda}{4}\\|\\hat{a}\\|^{2}+\\frac{1}{\\lambda T_{2}\\delta_{3}}\\mathbb{E}_{x}[\\|\\psi(x)\\|^{4}+\\|f_{*}(x)\\|^{4}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability $1-\\delta_{3}$ where we used Markov\u2019s inequality again in the second inequality. Finally, the fourth and fifth term in Eq. (B.25) can be bounded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\|f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{*}\\|_{L^{2}(P_{x})}^{2}\\bigg|=\\sqrt{\\bigg(\\|f_{*}\\|_{L^{2}(P_{T_{2}})}^{2}-\\|f_{*}\\|_{L^{2}(P_{x})}^{2}\\bigg)^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{1}{T_{2}\\delta_{4}}}\\mathbb{E}_{x}[(f^{*}(x)^{4}-\\|f_{*}\\|_{L^{2}(P_{x})}^{2})^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{1}{T_{2}\\delta_{4}}}\\mathbb{E}_{x}[(f^{*}(x))^{4}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability $1-\\delta_{4}$ where we used Markov\u2019s inequality in the last inequality. Combining these inequalities, we finally arrive at ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|f_{\\widehat{\\alpha}}-f_{*}\\|_{L^{2}(P_{x})}^{2}+\\left(\\frac{\\lambda}{4}-\\sqrt{\\frac{2}{T_{2}\\delta_{2}}\\mathbb{E}_{x}[\\|\\psi({\\boldsymbol x})\\|^{4}]}\\right)\\|\\widehat{\\alpha}\\|^{2}}\\\\ {\\displaystyle\\leq\\tilde{O}(N^{-2}+\\varepsilon^{2})+\\frac{1}{T_{2}\\lambda}\\Big(\\frac{\\mathbb{E}_{x}[\\|\\psi({\\boldsymbol x})\\|^{2}]}{\\delta_{1}}+\\frac{\\mathbb{E}_{x}[\\|\\psi({\\boldsymbol x})\\|^{2}]}{\\delta_{3}}+\\frac{\\mathbb{E}_{x}[(f^{*}({\\boldsymbol x}))^{4}]}{\\delta_{3}}\\Big)+\\frac{3\\lambda}{2}\\|{\\boldsymbol a}^{*}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability $\\textstyle1-\\sum_{j=1}^{4}\\delta_{j}$ . Hence, by setting $\\begin{array}{r}{\\lambda\\geq8\\sqrt{\\frac{2}{T_{2}\\delta_{2}}\\mathbb{E}_{\\pmb{x}}[||\\psi(\\pmb{x})||^{4}]}}\\end{array}$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f_{\\hat{\\boldsymbol a}}-f_{*}\\|_{L^{2}(P_{x})}^{2}}\\\\ &{\\leq\\tilde{O}(N^{-2}+\\varepsilon^{2})+\\cfrac{1}{T_{2}\\lambda}\\Big(\\cfrac{\\mathbb{E}_{x}[\\|\\psi(\\boldsymbol x)\\|^{2}]}{\\delta_{1}}+\\cfrac{\\mathbb{E}_{x}[\\|\\psi(\\boldsymbol x)\\|^{4}]}{\\delta_{3}}+\\cfrac{\\mathbb{E}_{x}[(f^{*}(\\boldsymbol x))^{4}]}{\\delta_{3}}\\Big)+\\frac{3\\lambda}{2}\\|\\boldsymbol a^{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "When the activation function $\\sigma$ is a polynomial, then each $\\psi_{j}(\\pmb{x})~=~\\sigma(\\langle\\pmb{x},\\pmb{w}_{j}\\rangle~+~b_{j})$ is an order $q$ -polynomial of a Gaussian random variable $\\langle{\\pmb x},{\\pmb w}_{j}\\rangle$ which has mean 0 and variance $\\mathbb{E}[\\langle\\pmb{x},\\pmb{w}_{j}\\rangle^{2}]\\ =\\ \\|\\pmb{w}_{j}\\|^{2}\\ =\\ \\tilde{O}(1)$ . Then, if we let $\\begin{array}{r l r}{R_{w}\\!}&{{}:=}&{\\!\\operatorname*{max}_{j}\\|w_{j}\\|\\;\\,=\\;\\,\\tilde{\\cal O}(1)}\\end{array}$ , the term $\\operatorname*{max}_{j}\\operatorname*{max}\\{\\mathbb{E}_{\\pmb{x}}[\\psi(\\pmb{x})_{j}^{2}],\\mathbb{E}_{\\pmb{x}}[\\psi(\\pmb{x})_{j}^{4}]\\}$ can be bounded by a $4q$ -th order polynomial of $R_{w}$ and $b$ , which can be denoted by $Q(R_{w},b,4q)$ . ", "page_idx": 32}, {"type": "text", "text": "Part (3). By combining evaluations of (1) and (2) together, if we let $\\begin{array}{r}{\\lambda=8\\sqrt{\\frac{2}{T_{2}\\delta_{0}}\\mathbb{E}_{\\pmb{x}}[||\\pmb{\\psi}(\\pmb{x})||^{4}]}}\\end{array}$ for some $\\delta_{0}>0$ , (by ignoring polylogarithmic factors) we obtain that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|f_{\\hat{\\alpha}}-f_{*}\\|_{L^{2}(P_{x})}^{2}\\lesssim(N^{-2}+\\varepsilon^{2})+\\frac{1}{T_{2}\\lambda\\delta_{0}}\\big(2N^{2}Q(R_{w},b,q^{\\prime})+\\mathbb{E}_{x}[(f_{*}(x))^{4}]\\big)+\\frac{3\\lambda}{2}\\|a^{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability $1-4\\delta_{0}$ . Thus, since $\\lVert{\\boldsymbol a}^{*}\\rVert^{2}\\,=\\,\\tilde{O}(N)$ , by setting $T_{2}\\,=\\,\\tilde{\\Theta}\\bigl((N^{4}Q^{2}(R_{w},b,q^{\\prime})\\,+$ $\\mathbb{E}[f_{*}({\\pmb x})^{4}]^{2})\\varepsilon^{-4})$ , and $N=\\tilde{\\Theta}(\\varepsilon^{-1})$ , we obtain that $(\\mathbf{B}.23)\\lesssim\\varepsilon^{2}$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Higher Generative Exponent Functions. For general link functions, under Assumption 4 and the bounded fourth moment of the link function, we have the following counterpart of Lemma 20, which provides the formal statement of Proposition 10. ", "page_idx": 32}, {"type": "text", "text": "Lemma 21. Suppose that $\\mathbb{E}[\\sigma_{*}(\\pmb{\\theta}^{\\top}x)^{4}]\\ <\\ \\infty$ and Assumption $^{4}$ hold. Then, by setting $\\lambda\\,=$ $\\tilde{\\Theta}\\bigg(\\sqrt{\\frac{N^{2}}{T_{2}\\delta_{0}}}\\bigg)$ for some $\\delta_{0}>0$ , the ridge estimator $\\hat{\\pmb{a}}$ satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|f_{\\hat{a}}-f_{*}\\|_{L^{2}(P_{x})}^{2}\\lesssim\\varepsilon^{2}+\\frac{1}{\\sqrt{T_{2}\\delta_{0}}}\\big(N^{2}C_{4}+\\mathbb{E}_{x}[(f_{*})^{4}]\\big)+\\frac{1}{\\sqrt{T_{2}\\delta_{0}}}\\|a^{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with probability $1-\\delta_{0}$ . By taking $T_{2}=\\tilde{\\Theta}((N^{4}+N^{2})\\varepsilon^{-4})$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pmb{x}}[(f_{\\hat{\\pmb{a}}}(\\pmb{x})-f_{*}(\\pmb{x}))^{2}]\\lesssim\\varepsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Furthermore, applying Lemma 12 and 13 yields that, when $\\begin{array}{r l r}{\\sigma_{*}}&{{}=}&{\\sum_{j=0}^{\\infty}\\alpha_{j}\\mathsf{H}\\mathrm{e}_{j}}\\end{array}$ satisfies $\\textstyle\\sum_{j=0}^{\\infty}j^{2}j!\\alpha_{j}^{2}$ and $\\mathbb{E}[\\sigma_{*}(\\pmb{\\theta}^{\\top}x)^{4}]$ are bounded, with a properly designed randomized activation in Lemma $^{12}$ , by taking $N=\\tilde{\\Theta}(\\varepsilon^{-7})$ and $T_{2}=\\tilde{\\Theta}(\\varepsilon^{-32})$ , Algorithm $^{\\,l}$ yields ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}}[(f_{\\hat{\\mathbf{a}}}(\\mathbf{x})-f_{*}(\\mathbf{x}))^{2}]\\lesssim\\varepsilon^{2}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with probability $1-o_{d}(1)$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. The proof is identical to that of Lemma 20, with the difference being that we replace the bounded moment assumptions with $\\mathbb{E}[\\sigma_{*}(\\pmb{\\theta}^{\\top}\\pmb{x})^{4}]<\\infty$ or Assumption 4. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Approximation Guarantee. Note that for non-polynomial link function with generative exponent $p_{*}\\geq3$ , the approximation error is already controlled in Assumption 4 (based on [BBSS22, Lemma 4.4, 4.5]). If $\\sigma_{*}$ is a degree- $q$ polynomial, we have the following characterization, which follows Lemmas 29 and 30 of [OSSW24]. ", "page_idx": 33}, {"type": "text", "text": "Lemma 22. Suppose that there exist at least $N^{\\prime}=\\tilde{\\Theta}(N)$ neurons that satisfy $\\lvert|\\pmb{w}_{j}^{2T_{1}}-\\pmb{\\theta}\\rvert\\rvert\\leq\\varepsilon$ and $\\sigma$ is a polynomial link function with degree at least $q$ . Let $b_{j}\\sim\\mathrm{Unif}([-C_{b},C_{b}])$ with $C_{b}=\\tilde{O}(1)$ , and consider approximation of a ridge function $h(\\pmb\\theta^{\\top}\\pmb x)$ with its degree at most $q$ . Then, there exists $a_{1},\\dots,a_{N}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{j=1}^{N}a_{j}\\sigma_{j}\\left({\\pmb w}_{j}^{2{T_{1}}}^{\\top}{\\pmb x}+b_{j}\\right)-h(\\pmb\\theta^{\\top}{\\pmb x})\\right|=\\tilde{O}(N^{-1}+\\varepsilon)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "with high probability, where $(\\pmb{x},y)$ is a random sample, and we omit dependence on the degree q in the big- $O$ notation. Moreover, we have $\\begin{array}{r}{\\sum_{j=1}^{N}a_{j}^{2}=\\tilde{O}(N)}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "To prove Lemma 22, we rely on the following result. ", "page_idx": 33}, {"type": "text", "text": "Lemma 23. Suppose that $C_{b}\\geq q$ . For any polynomial $h(s)$ with its degree at most $q$ , there exists $\\bar{v}(b;h)$ with $|\\bar{v}(b;h)|\\lesssim C_{b}$ such that for all $s$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{v}(b;h)\\sigma(\\delta s+b)]=h(s).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. When $g_{q}(s)=\\sigma(s)$ is a degree- $q$ polynomial, ", "page_idx": 33}, {"type": "equation", "text": "$$\ng_{q}(s)=\\int_{b=-q}^{0}\\sigma(s+b)\\mathrm{d}b\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "is also a degree- $q$ polynomial. Let us repeatedly define ", "page_idx": 33}, {"type": "equation", "text": "$$\ng_{q-i}(s):=g_{q-(i-1)}(s+1)-g_{q-(i-1)}(s)\\quad(i=1,2,\\cdots,q),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and let $(c_{i,j})$ be coefficients so that $\\textstyle(s-1)^{i}=\\sum_{j=0}^{i}c_{i,j}s^{j}$ holds for all $z$ . Then, by induction, $g_{i}(s)$ is a degree- $^{\\,\\cdot\\,i}$ polynomial. Moreover, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\ng_{q-i}(s)=\\sum_{j=0}^{i}c_{i,j}\\int_{b=-q}^{0}\\sigma(s+b+j)\\mathrm{d}b\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n=2C_{b}\\mathbb{E}_{b\\sim\\mathrm{Unif}([-C_{b},C_{b}])}\\bigg[\\bigg(\\sum_{j=0}^{i}c_{i,j}\\mathbb{1}[j-q\\leq b\\leq j]\\bigg)\\sigma(s+b)\\bigg],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "when $C_{b}\\geq q$ . Therefore, for any polynomial $h(s)$ with its degree at most $q$ , there exists $\\bar{v}(b;h)$ with $|\\bar{v}(b;h)|\\lesssim C_{b}$ such that for all $s$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{v}(b;h)\\sigma(\\delta s+b)]=h(s).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma 22. We now discretize Lemma 23. Although we may use randomized and different $\\sigma_{j}$ , because we sample the activation function from the finite set, the discussion is reduced to the case when the same $\\sigma_{j}$ is the same for all $j$ . We focus on $N^{\\prime}$ neurons that satisfy $\\lVert\\pmb{w}_{j}^{2T_{1}}-\\pmb{\\theta}\\rVert\\leq$ $\\varepsilon$ (by letting $a_{j}\\;=\\;0$ otherwise). For $A\\,=\\,\\tilde{\\Theta}(N^{\\prime})\\,=\\,\\tilde{\\Theta}(N)$ (with a small hidden constant), we consider $2A$ intervals $[-C_{b},C_{b}(-1+\\textstyle{\\frac{1}{A}})),[C_{b}(-1+\\textstyle{\\frac{1}{A}}),C_{b}(-1+\\textstyle{\\frac{2}{A}})),\\cdots\\,,[C_{b}(1-\\textstyle{\\frac{1}{A}}),C_{b}]$ . By taking the hidden constant sufficiently small, for each interval there exists at least one $b_{j}$ . Then, for bj corresponding to [Cb(\u22121 + Ai), Cb(\u22121 + i+A1 )), we set aj = N2  CCbb((\u2212\u221211++  Ai A) $\\begin{array}{r}{a_{j}\\,=\\,\\frac{N}{2}\\int_{C_{b}(-1+\\frac{i}{A})}^{C_{b}(-1+\\frac{i+1}{A}))}\\bar{v}(b;h)\\mathrm{d}b}\\end{array}$ we note that $|a_{j}|=\\tilde{O}(1)$ holds for all $j$ . If each interval contains more than one $b_{j}$ , we ignore all but one by letting $a_{j}=0$ except for the one. By (local) Lipschitzness of $\\sigma$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{j=1}^{N}a_{j}\\sigma(s+b_{j})-h(s)\\right|=\\tilde{O}(N)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $s=\\tilde{O}(1)$ . Because $|\\pmb{\\theta}^{\\top}\\pmb{x}^{t}|=\\tilde{O}(1)$ with high probability, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{j=1}^{N}a_{j}\\sigma(\\pmb\\theta^{\\top}\\pmb x+b_{j})-h(\\pmb\\theta^{\\top}\\pmb x)\\right|=\\tilde{O}(N^{-1})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "with high probability. Finally, because $\\lvert|\\pmb{w}_{j}^{2T_{1}}-\\pmb{\\theta}\\rvert|\\leq\\varepsilon$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{j=1}^{N}a_{j}{\\sigma}\\big({w_{j}^{2T_{1}}}^{\\top}x+b_{j}\\big)-\\frac{1}{N}\\sum_{j=1}^{N}a_{j}{\\sigma}(\\theta^{\\top}x+b_{j})\\right|=\\tilde{O}\\big((w_{j}^{2T_{1}}-\\theta)^{\\top}x\\big)=\\tilde{O}(\\varepsilon)(\\theta^{\\top}x+b_{j})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining (B.26) and (B.27), we obtain the assertion. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 35}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 35}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 35}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 35}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \u201d[Yes] \u201d is generally preferable to \u201d[No] \u201d, it is perfectly acceptable to answer \u201d[No] \u201d provided a proper justification is given (e.g., \u201derror bars are not reported because it would be too computationally expensive\u201d or \u201dwe were unable to find the license for the dataset we used\u201d). In general, answering \u201d[No] \u201d or \u201d[NA] \u201d is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: ", "page_idx": 35}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\u201d, \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 35}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper\u2019s main contribution is studying the computational complexity of learning single-index models with polynomial link functions by training neural networks with stochastic gradient descent, which is what we claim in the abstract and introduction. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We discussed the limitations of our results in Section 4. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: All assumptions required in our work are stated in Assumptions 1,2, and 3.   \nAll proofs are presented in the appendix. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper is theoretical and experiments are only used for the purpose of illustration in the Introduction. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: This paper is theoretical and experiments are only used for the purpose of illustration in the Introduction. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 37}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This is a theory paper. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This is a theory paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This is a theory paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The authors have read and followed the NeurIPS Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper carries out a theoretical study, and we believe our work does not have specific societal impacts that require a discussion. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper is theoretical and the experiment is only about two-layer neural network and synthetic data, which we do not have such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper is theoretical and does not use any assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper is theoretical and does not introduce new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper is theoretical and does not include crowdsourcing experiments nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper is theoretical and does not include crowdsourcing experiments nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]