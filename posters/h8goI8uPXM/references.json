{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models and is frequently cited in subsequent work on LLMs, including this paper on quantization."}, {"fullname_first_author": "Jerry Chee", "paper_title": "Quip: 2-bit quantization of large language models with guarantees", "publication_date": "2023-07-13", "reason": "This paper is highly relevant as it focuses on achieving high accuracy in 2-bit quantization of LLMs, a key challenge directly addressed by the current paper."}, {"fullname_first_author": "Elias Frantar", "paper_title": "Optq: Accurate quantization for generative pre-trained transformers", "publication_date": "2022-01-01", "reason": "This paper directly tackles post-training quantization for LLMs and proposes an approach compared to in this current work."}, {"fullname_first_author": "Itay Hubara", "paper_title": "Accurate post training quantization with small calibration sets", "publication_date": "2021-01-01", "reason": "This work is important as it explores accurate post-training quantization which is the primary focus of the current paper."}, {"fullname_first_author": "Yuhang Li", "paper_title": "BRECQ: Pushing the limit of post-training quantization by block reconstruction", "publication_date": "2021-02-05", "reason": "This paper focuses on post-training quantization and block reconstruction methods, both relevant to the techniques explored in the target paper."}]}