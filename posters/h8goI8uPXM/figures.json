[{"figure_path": "h8goI8uPXM/figures/figures_5_1.jpg", "caption": "Figure 1: The latency (in 1e-6 seconds) of the four GEMMs in transformer block on L4 GPU, (The three GEMMs for query, key and value are concatenated into GEMM 1), with hidden_dim = 5120, batch_size = 4.", "description": "The figure shows the latency, measured in microseconds, of four GEMM (General Matrix Multiply) operations within a transformer block on an L4 GPU.  GEMM 1 represents the combined latency of three GEMMs used for query, key, and value calculations. The experiment was performed with a hidden dimension of 5120 and a batch size of 4. Different quantization schemes (BF16, W8A16, W4A16g64, and W2A16g64) are compared to demonstrate their impact on latency.", "section": "Experiments"}, {"figure_path": "h8goI8uPXM/figures/figures_7_1.jpg", "caption": "Figure 2: The Top-1 accuracy of ResNet-18 on ImageNet. Solid and dashed lines are for approximation 10 and 11 respectively.", "description": "This figure shows the Top-1 accuracy of the ResNet-18 model on the ImageNet dataset as a function of the number of iterations (K) in the optimization process.  Two lines are presented: one for the model without supervised fine-tuning (\"w/o sft\") and one with supervised fine-tuning (\"w/ sft\").  The solid lines represent the results using the first-level approximation (10) while the dashed lines represent the results using the second-level approximation (11).  The figure demonstrates that accuracy improves with more iterations and that supervised fine-tuning provides further accuracy gains.  The difference between the approximation methods is also highlighted.", "section": "4.3 Ablation studies"}, {"figure_path": "h8goI8uPXM/figures/figures_7_2.jpg", "caption": "Figure 2: The Top-1 accuracy of ResNet-18 on ImageNet. Solid and dashed lines are for approximation 10 and 11 respectively.", "description": "This figure shows the Top-1 accuracy of ResNet-18 on ImageNet, comparing two different approximation methods (10 and 11) used in the decoupleQ algorithm for 2-bit quantization.  The x-axis represents the number of iterations in the optimization process, and the y-axis shows the achieved Top-1 accuracy. The solid line represents the results using approximation method 10, while the dashed line represents approximation method 11. The figure demonstrates that approximation method 10 generally leads to better accuracy, although it is more computationally expensive.", "section": "4.3 Ablation studies"}, {"figure_path": "h8goI8uPXM/figures/figures_8_1.jpg", "caption": "Figure 4: The PPL of Llama-7B on WikiText2 and the loss of the first block between pre-and post-quantization. Solid and dashed lines are for approximation 10 and 11 respectively.", "description": "This figure shows the perplexity (PPL) of the Llama-7B language model on the WikiText2 dataset and the loss of its first transformer block during the 2-bit quantization process using decoupleQ.  Two approximation methods (10 and 11) are compared. The solid lines represent approximation 10, and the dashed lines represent approximation 11. The x-axis represents the number of iterations in the optimization process, and the y-axis shows both the PPL and the loss, with two different scales for clarity.  The plot illustrates the relationship between the number of iterations, the resulting PPL, and the loss achieved by each approximation method during the quantization. This helps to evaluate the effectiveness and convergence behavior of the two different approximation strategies of the decoupleQ algorithm.", "section": "4.3 Ablation studies"}, {"figure_path": "h8goI8uPXM/figures/figures_8_2.jpg", "caption": "Figure 1: The latency (in 1e-6 seconds) of the four GEMMs in transformer block on L4 GPU, (The three GEMMs for query, key and value are concatenated into GEMM 1), with hidden_dim = 5120, batch_size = 4.", "description": "This figure compares the latency of different GEMM (General Matrix Multiply) operations within a transformer block on an L4 GPU.  It shows the execution time for four GEMMs: GEMM1 (combining query, key, and value matrix multiplications), GEMM2, GEMM3, and GEMM4.  The experiment is run with hidden dimension (hidden_dim) set to 5120 and batch size set to 4. The different quantization methods are compared: BF16, W8A16, W4A16g64, and W2A16g64, highlighting the impact of quantization on performance.", "section": "4.1 Private Experiments"}]