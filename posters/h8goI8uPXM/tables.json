[{"figure_path": "h8goI8uPXM/tables/tables_4_1.jpg", "caption": "Table 1: The results of our two ASR models. The models are quantized into W2A16g64. runtime for the quantization process is measured in hours. There are two sub-domains in task B, and we report the WER of both.", "description": "This table shows the Word Error Rate (WER) results for two Automatic Speech Recognition (ASR) models after quantization.  It compares the performance of the models using BF16 (baseline) against decoupleQ with and without subsequent fine-tuning. The runtime for the quantization process is also indicated, along with WER scores for two sub-domains in task B.", "section": "4.1 Private Experiments"}, {"figure_path": "h8goI8uPXM/tables/tables_6_1.jpg", "caption": "Table 1: The results of our two ASR models. The models are quantized into W2A16g64. runtime for the quantization process is measured in hours. There are two sub-domains in task B, and we report the WER of both.", "description": "This table presents the Word Error Rate (WER) and quantization runtime for two Automatic Speech Recognition (ASR) models, each tested on a different task (A and B).  Task B has two sub-domains, and WER is reported for both.  The models are quantized using the W2A16g64 method. The table compares the performance of the models using BF16 (baseline) and decoupleQ, with and without supervised fine-tuning (sft).", "section": "4.1 Private Experiments"}, {"figure_path": "h8goI8uPXM/tables/tables_6_2.jpg", "caption": "Table 2: Comparison of decoupleQ with other methods. In decoupleQ, we only use the first stage, layer-wise minimization. All the models are quantized into W2A16 without groups. In decoupleQ+sft, we train the (s, z) and norm layers for one epoch, using the regular labeled dataset containing 1.2 million images.", "description": "This table compares the performance of decoupleQ against other quantization methods (GPTQ, OBQ, BRECQ) on ResNet18 and ResNet50 models for 2-bit, 3-bit, and 4-bit quantization.  It shows the Top-1 accuracy for each method and bit-depth.  The decoupleQ+sft row indicates results after further fine-tuning with a labeled dataset.", "section": "4.2 Public Comparison"}, {"figure_path": "h8goI8uPXM/tables/tables_7_1.jpg", "caption": "Table 3: The results of PPL of wikitext-2 on Llama-1/2. We also report the runtime (measured in hours) for the W2 quantization via decoupleQ in the gray background row. The results other than decoupleQ are copied from OmniQuant (29). All the results of decoupleQ use the approximation 11.", "description": "This table presents the perplexity (PPL) results for different quantization methods on the WikiText-2 benchmark using Llama language models of various sizes (1-7B, 1-13B, 1-30B, 1-65B, 2-7B, 2-13B, and 2-70B parameters).  It compares the performance of decoupleQ against GPTQ and OmniQuant. The runtime for decoupleQ's W2 quantization is also shown (in gray).  All decoupleQ results utilize approximation 11.", "section": "4.2 Public Comparison"}, {"figure_path": "h8goI8uPXM/tables/tables_8_1.jpg", "caption": "Table 4: The perplexity of Llama on WikiText2 with and without the block-wise minimization. All the models are quantized into W2A16.", "description": "This table compares the perplexity scores achieved on the WikiText2 benchmark using two different quantization methods: with and without block-wise minimization.  The results are shown for various Llama model sizes (1-7B, 1-13B, 1-30B, 2-7B, and 2-13B parameters).  The \"w/o\" row represents the perplexity without block-wise minimization, while the \"w\" row shows the perplexity with block-wise minimization. Lower perplexity indicates better performance.  This table highlights the improvement in model accuracy that block-wise minimization provides.", "section": "4.3 Ablation studies"}]