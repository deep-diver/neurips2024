[{"figure_path": "2YSHEBRRol/figures/figures_3_1.jpg", "caption": "Figure 1: Trajectories of optimization in a two-player DMG (as delineated in Example 4.1). Fig. 1a displays the trajectories over the collective reward landscape - deeper orange equates to higher rewards. Remarkably, only Simul-Co and AgA make successful strides towards the social optimum. Fig. 1b and Fig. 1c delineate trajectories on the individual reward contour, underscoring Simul-Co's neglect for Player 1's interests as it navigates through the crests and troughs of its reward. Conversely, our AgA optimizes along the summit of Player 1's reward while also maximizing the collective reward, demonstrating successful alignment.", "description": "This figure shows the optimization trajectories of different algorithms in a two-player differentiable mixed-motive game.  Panel (a) displays the collective reward landscape, illustrating how different methods approach the social optimum. Panel (b) and (c) show the individual reward contours for players 1 and 2, respectively, highlighting the differences in how the algorithms balance individual and collective objectives.  AgA is shown to successfully align both objectives.", "section": "4 Method"}, {"figure_path": "2YSHEBRRol/figures/figures_3_2.jpg", "caption": "Figure 1: Trajectories of optimization in a two-player DMG (as delineated in Example 4.1). Fig. 1a displays the trajectories over the collective reward landscape - deeper orange equates to higher rewards. Remarkably, only Simul-Co and AgA make successful strides towards the social optimum. Fig. 1b and Fig. 1c delineate trajectories on the individual reward contour, underscoring Simul-Co's neglect for Player 1's interests as it navigates through the crests and troughs of its reward. Conversely, our AgA optimizes along the summit of Player 1's reward while also maximizing the collective reward, demonstrating successful alignment.", "description": "This figure shows the optimization trajectories of different algorithms in a two-player differentiable mixed-motive game.  Subfigure (a) displays the trajectories on the collective reward landscape, highlighting that only Simul-Co and AgA successfully reach the social optimum. Subfigures (b) and (c) show the trajectories on the individual reward contours for Player 1 and Player 2, respectively.  These subfigures demonstrate that Simul-Co prioritizes collective reward and neglects Player 1's individual interests, while AgA successfully balances both individual and collective objectives.", "section": "4.1 Differentiable Mixed-motive Game"}, {"figure_path": "2YSHEBRRol/figures/figures_3_3.jpg", "caption": "Figure 1: Trajectories of optimization in a two-player DMG (as delineated in Example 4.1). Fig. 1a displays the trajectories over the collective reward landscape - deeper orange equates to higher rewards. Remarkably, only Simul-Co and AgA make successful strides towards the social optimum. Fig. 1b and Fig. 1c delineate trajectories on the individual reward contour, underscoring Simul-Co's neglect for Player 1's interests as it navigates through the crests and troughs of its reward. Conversely, our AgA optimizes along the summit of Player 1's reward while also maximizing the collective reward, demonstrating successful alignment.", "description": "This figure shows the optimization trajectories for different algorithms in a two-player differentiable mixed-motive game.  The trajectories are shown in three different views: a 3D plot of the collective reward landscape, and 2D contour plots of the individual rewards for players 1 and 2. The figure highlights that the proposed AgA algorithm successfully aligns individual and collective objectives, unlike other methods, by optimizing along the summit of Player 1's reward while maximizing the collective reward.", "section": "4 Method"}, {"figure_path": "2YSHEBRRol/figures/figures_4_1.jpg", "caption": "Figure 1: Trajectories of optimization in a two-player DMG (as delineated in Example 4.1). Fig. 1a displays the trajectories over the collective reward landscape - deeper orange equates to higher rewards. Remarkably, only Simul-Co and AgA make successful strides towards the social optimum. Fig. 1b and Fig. 1c delineate trajectories on the individual reward contour, underscoring Simul-Co's neglect for Player 1's interests as it navigates through the crests and troughs of its reward. Conversely, our AgA optimizes along the summit of Player 1's reward while also maximizing the collective reward, demonstrating successful alignment.", "description": "This figure shows the optimization trajectories for different algorithms in a two-player differentiable mixed-motive game.  Panel (a) displays trajectories on a collective reward landscape, showing that only Simul-Co and AgA successfully reach the social optimum. Panels (b) and (c) show trajectories on individual reward contours, highlighting that Simul-Co prioritizes the collective objective at the expense of individual player interests, whereas AgA effectively balances both.", "section": "4 Method"}, {"figure_path": "2YSHEBRRol/figures/figures_8_1.jpg", "caption": "Figure 1: Trajectories of optimization in a two-player DMG (as delineated in Example 4.1). Fig. 1a displays the trajectories over the collective reward landscape - deeper orange equates to higher rewards. Remarkably, only Simul-Co and AgA make successful strides towards the social optimum. Fig. 1b and Fig. 1c delineate trajectories on the individual reward contour, underscoring Simul-Co's neglect for Player 1's interests as it navigates through the crests and troughs of its reward. Conversely, our AgA optimizes along the summit of Player 1's reward while also maximizing the collective reward, demonstrating successful alignment.", "description": "This figure shows the optimization trajectories of different algorithms in a two-player differentiable mixed-motive game.  Panel (a) shows the trajectories in the collective reward landscape, highlighting that only Simul-Co and AgA successfully reach the social optimum. Panels (b) and (c) show the trajectories in the individual reward landscapes for players 1 and 2 respectively, demonstrating that AgA successfully aligns individual and collective objectives, while Simul-Co prioritizes collective rewards at the expense of player 1's rewards.", "section": "4.1 Differentiable Mixed-motive Game"}, {"figure_path": "2YSHEBRRol/figures/figures_17_1.jpg", "caption": "Figure 4: The scatter of actions in a two-player public goods game achieved through different optimization methods. Each circle represents the position attained within a maximum of 100 steps, with the color indicating the corresponding method. The 'X' mark represents the mean actions of 50 random runs. With the exception of Simul-Co, the baseline methods converge towards the Nash equilibrium (0,0). Notably, while both AgA and Simul-Co display altruistic behavior, the actions of AgA are more tightly clustered around the (1,1) point compared to Simul-Co.", "description": "This figure shows the results of a two-player public goods game experiment comparing several multi-agent reinforcement learning methods, including the proposed Altruistic Gradient Adjustment (AgA) method. Each point represents the actions taken by two players in a single game instance, with the color of the point indicating the method used. The 'X' marks show the average actions for each method.  The figure highlights that most baselines converge to the Nash equilibrium (0,0), where neither player contributes. However, both AgA and Simul-Co show a tendency toward altruistic behavior (1,1), where both players contribute; AgA's actions are more consistently clustered around the optimal outcome.  This illustrates AgA's superior alignment toward the socially optimal point.", "section": "D.1 Result Visualization of Two-player Public Goods Game"}, {"figure_path": "2YSHEBRRol/figures/figures_18_1.jpg", "caption": "Figure 5: Semantic Diagram of MMM2 map in SMAC", "description": "This figure shows a simplified schematic of the Selfish-MMM2 environment used in the paper.  It depicts the layout of the map in the StarCraft II game. It highlights the division of units between the controlled agents (teammates) and the enemy units.  The controlled team consists of 1 Medivac, 2 Marauders, and 7 Marines.  The opposing enemy team consists of 1 Medivac, 3 Marauders, and 8 Marines. This diagram visually represents the asymmetrical setup of the environment used to test the proposed algorithm.", "section": "5.3 Selfish-MMM2"}]