[{"figure_path": "VUWvVvNi6r/figures/figures_4_1.jpg", "caption": "Figure 1: Projection loss vs. training epochs of ViT-tiny model. The reconstruction loss is averaged over the batch, heads, and layers. The downward trend suggests that the model is implicitly minimizing this projection loss.", "description": "This figure shows the training and testing reconstruction loss of a Vision Transformer (ViT-tiny) model trained on the ImageNet-1K dataset. The reconstruction loss is calculated as the mean squared distance between the original data points and their projections onto the principal component axes.  The plot shows that the reconstruction loss decreases over time, indicating that the model learns to perform kernel PCA by implicitly minimizing the projection loss. This supports the paper's claim that self-attention layers in transformers learn to perform kernel PCA.", "section": "2.2 Analysis on the Convergence of Self-Attention Layers to Kernel PCA"}, {"figure_path": "VUWvVvNi6r/figures/figures_5_1.jpg", "caption": "Figure 2: Mean and standard deviation of the absolute differences of elements in the constant vector \u03b3d, d = 1,..., Dv. The mean should be 0 with small standard deviations when vdj are close to the values predicted in Theorem 1. For comparison, we observe that the max, min, mean, and median of the absolute values of all the eigenvalues, averaged over all attention heads and layers, are 648.46, 4.65, 40.07, and 17.73, respectively, which are much greater than the values of |\u03b3i \u2212 \u03b3j |.", "description": "This figure shows empirical results supporting Equation 11 in the paper.  It plots the average pairwise absolute differences between elements of the vector \u03b3 (which should be constant if the value vectors vj capture the eigenvectors of the Gram matrix K\u03c6). The small standard deviation and near-zero mean support the hypothesis that vj captures these eigenvectors after training.  The comparison with the significantly larger magnitudes of the eigenvalues themselves further emphasizes the findings.", "section": "2.2 Analysis on the Convergence of Self-Attention Layers to Kernel PCA"}, {"figure_path": "VUWvVvNi6r/figures/figures_8_1.jpg", "caption": "Figure 3: Left: Top-1 accuracy of RPC-SymViT vs. baseline SymViT evaluated on PGD/FGSM attacked ImageNet-1K validation set across increasing perturbation budgets. Right: Validation top-1 accuracy (%) and loss of Scaled Attention vs. the baseline asymmetric softmax attention in ViT for the first 50 training epochs.", "description": "The figure shows two plots. The left plot compares the top-1 accuracy of RPC-SymViT and SymViT models on the ImageNet-1K validation set under PGD and FGSM attacks with varying perturbation budgets. It demonstrates the improved robustness of RPC-SymViT against adversarial attacks. The right plot compares the validation top-1 accuracy and loss curves of Scaled Attention and baseline asymmetric softmax attention models during the first 50 training epochs on the ImageNet-1K dataset. It illustrates the superior performance of Scaled Attention in terms of accuracy and convergence speed.", "section": "4 Experimental Results"}, {"figure_path": "VUWvVvNi6r/figures/figures_21_1.jpg", "caption": "Figure 3: Left: Top-1 accuracy of RPC-SymViT vs. baseline SymViT evaluated on PGD/FGSM attacked ImageNet-1K validation set across increasing perturbation budgets. Right: Validation top-1 accuracy (%) and loss of Scaled Attention vs. the baseline asymmetric softmax attention in ViT for the first 50 training epochs.", "description": "The left plot shows the robustness of RPC-SymViT against PGD and FGSM attacks compared to the baseline SymViT on the ImageNet-1K dataset. The right plot illustrates the performance of Scaled Attention and baseline asymmetric softmax attention on the validation dataset during the training of ViT-tiny model for the first 50 epochs. ", "section": "4 Experimental Results"}, {"figure_path": "VUWvVvNi6r/figures/figures_22_1.jpg", "caption": "Figure 2: Mean and standard deviation of the absolute differences of elements in the constant vector \u03bba, d = 1,..., Dv. The mean should be 0 with small standard deviations when \u03b3dj are close to the values predicted in Theorem 1. For comparison, we observe that the max, min, mean, and median of the absolute values of all the eigenvalues, averaged over all attention heads and layers, are 648.46, 4.65, 40.07, and 17.73, respectively, which are much greater than the values of \u03b3i - \u03b3j.", "description": "This figure shows the empirical results to verify equation (11) in the paper. The mean and standard deviation of the absolute differences between each pair of elements in the constant vector  \u03bba (where a is the eigenvector of the Gram matrix) are plotted for each principal component axis. The plot shows that the differences are close to 0, with small standard deviations across all layers and heads. This indicates that after training, the value matrix V captures the eigenvectors of the Gram matrix, supporting the theory that self-attention performs kernel PCA.", "section": "Analysis on the Convergence of Self-Attention Layers to Kernel PCA"}]