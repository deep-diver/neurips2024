[{"figure_path": "hwuUBsMlBf/tables/tables_5_1.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares CuMo's performance against other state-of-the-art multimodal LLMs across various benchmarks.  Models are grouped by their base LLM size (7B, 13B, and 7B MoE).  The table shows metrics for various tasks, including image understanding and question answering, and highlights CuMo's superior performance within each model size group.  The 'Act.' column represents the number of activated parameters during inference.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_6_1.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares CuMo's performance against other state-of-the-art multimodal LLMs across various benchmarks.  Models are grouped by the size of their base LLMs (7B, 13B, and 7B MoE models).  The table highlights CuMo's superior performance in many cases, particularly considering the number of activated parameters during inference.  Some results use an average of three GPT API queries.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_7_1.jpg", "caption": "Table 3: Ablation Studies during building CuMo. Each row represents a different configuration, with changes or additions marked using = and + symbols, respectively. Settings highlighted with a light blue background are those adapted for final model in Table 1. For (b): all MoE blocks in CLIP are initialized with upcycling.", "description": "This ablation study investigates the impact of different design choices within CuMo, focusing on the incorporation of Mixture-of-Experts (MoE) blocks in different components, including the MLP connector, CLIP vision encoder, and LLM.  The table shows the impact on various metrics (SQA, VQAT, MMVet, SEED) for each configuration, comparing scratch training vs upcycling, and including or excluding auxiliary losses. The results illustrate the effectiveness of co-upcycling in stabilizing training and improving performance, as well as the impact of different MoE block configurations and auxiliary loss functions.", "section": "4.3 Ablation Study"}, {"figure_path": "hwuUBsMlBf/tables/tables_7_2.jpg", "caption": "Table 3: Ablation Studies during building CuMo. Each row represents a different configuration, with changes or additions marked using = and + symbols, respectively. Settings highlighted with a light blue background are those adapted for final model in Table 1. For (b): all MoE blocks in CLIP are initialized with upcycling.", "description": "This table presents the ablation study results for the CuMo model. It shows the impact of different design choices on the performance of the model on various benchmarks, such as SQA, VQA, MMVet, and SEED. The rows indicate adding different MoE blocks and using different training strategies.  The table helps understand the contribution of each component (MLP connector, CLIP vision encoder, and the LLM itself) to the overall model's performance.", "section": "4.3 Ablation Study"}, {"figure_path": "hwuUBsMlBf/tables/tables_7_3.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares the performance of CuMo against other state-of-the-art multimodal LLMs on several benchmark datasets.  The models are grouped by their base LLM size (7B, 13B parameters, and 7B MoE models).  The table highlights CuMo's superior performance across multiple benchmarks within each size group, indicating the effectiveness of the proposed method. The \"Act.\" column refers to activated parameters during inference, and the \u2020 symbol denotes results averaged over three GPT API queries.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_7_4.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares the performance of CuMo against other state-of-the-art multimodal LLMs on various benchmarks.  Models are categorized by their base LLM size (7B, 13B, and 7B MoE models).  The table highlights CuMo's superior performance across multiple benchmarks, often matching or exceeding the performance of larger models.  The \"Act.\" column shows the number of activated parameters during inference, and the \u2020 symbol indicates results averaged from three GPT API queries for greater accuracy. ", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_7_5.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares the performance of CuMo against other state-of-the-art multimodal LLMs across various benchmarks.  Models are grouped by base LLM size (7B, 13B, and 7B MoE).  The best performance for each benchmark is highlighted in bold. The \"Act.\" column indicates the number of activated parameters during inference, and numbers with a dagger (\u2020) represent averages from three GPT API queries.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_7_6.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares CuMo's performance against other state-of-the-art multimodal LLMs on various benchmarks.  Models are grouped by base LLM size (7B, 13B, and 7B with Mixture-of-Experts).  The best performance for each benchmark is highlighted in bold.  'Act.' indicates the number of activated parameters during inference.  Results marked with a dagger (\u2020) represent averages from three GPT API queries.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_13_1.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares CuMo's performance against other state-of-the-art multimodal LLMs on various benchmarks.  Models are categorized by base LLM size (7B, 13B, and 7B MoE models).  The best performance for each benchmark is highlighted in bold.  'Act.' refers to the number of activated parameters during inference, and numbers marked with a dagger symbol (+) represent averages from three inference runs using the GPT API.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_13_2.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares CuMo's performance against other state-of-the-art multimodal LLMs on various benchmarks.  Models are grouped by base LLM size (7B, 13B, and 7B MoE).  The best performance for each benchmark is highlighted in bold.  The table also shows the number of activated parameters during inference for each model.  Note that some numbers are averages from three runs using the GPT API.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_14_1.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares CuMo's performance against other state-of-the-art multimodal LLMs across various benchmarks.  Models are grouped by base LLM size (7B, 13B, and 7B MoE).  The best performance for each benchmark is highlighted in bold.  'Act.' refers to activated parameters during inference, and numbers marked with a dagger symbol (+) represent averages from three GPT API queries.", "section": "4.1 Implementation Details"}, {"figure_path": "hwuUBsMlBf/tables/tables_14_2.jpg", "caption": "Table 1: Comparisons between CuMo and other state-of-the-art multimodal LLMs on competitive benchmarks. These models are grouped by the size of the base LLM and bold indicates the best performance on a certain benchmark. Act.: activated parameters during inference. Numbers with \u2020 are averaged by three inference runs of querying GPT API.", "description": "This table compares CuMo's performance against other state-of-the-art multimodal LLMs on various benchmarks.  Models are grouped by the size of their base LLM (7B, 13B, and 7B MoE models).  The best performance on each benchmark is highlighted in bold.  The table also shows the number of activated parameters during inference and notes that some numbers are averages from three GPT API queries.", "section": "4.1 Implementation Details"}]