[{"figure_path": "bmoS6Ggw4j/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of Task Planning in Language Agents (e.g., HuggingGPT [46])", "description": "This figure illustrates the task planning process in language agents, using HuggingGPT as an example.  A user request (e.g., \"Please generate an image where a girl is reading a book, and her pose is the same as the boy in 'example.jpg', then please describe the new image with your voice.\") is decomposed into a sequence of sub-tasks represented as a task graph. Each node in the graph represents a sub-task (e.g., Pose Detection, Pose-to-Image, Image-to-Text, Text-to-Speech), and the edges denote the dependencies between them.  The task planning process involves selecting a path through the graph (a sequence of sub-tasks) that fulfills the user's request. The selected path is then executed by invoking the corresponding APIs (from HuggingFace, in this case) to achieve the final result. This showcases the graph-based nature of task planning, a key concept explored in the paper.", "section": "2 Preliminaries"}, {"figure_path": "bmoS6Ggw4j/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of (a) LLMs' planning performance and hallucination in HuggingGPT, and (b) hallucination in relation to task graph size.", "description": "This figure shows the performance and hallucination rate of different LLMs on task planning.  Panel (a) compares the performance (Task F1 score) and hallucination (node and edge hallucination ratios) across several LLMs on the HuggingFace task planning benchmark. Panel (b) shows the relationship between the hallucination rates and the size of the task graph (number of subtasks) across multiple datasets, highlighting a correlation between larger graphs and increased hallucinations.", "section": "Failures of LLMs in Planning: Empirical Findings"}, {"figure_path": "bmoS6Ggw4j/figures/figures_9_1.jpg", "caption": "Figure 3: Orthogonal Effectiveness to both Improved Prompts and Fine-tuned LLMs", "description": "This figure demonstrates the orthogonal improvements achieved by integrating GNNs with LLMs for task planning.  Panel (a) shows the impact of different prompt engineering techniques on the performance of LLMs (CodeLlama-13B and Mistral-7B) across three different datasets (HuggingFace, Multimedia, and Daily Life). The results indicate consistent gains from using GraphSAGE regardless of the prompt style. Panel (b) compares the performance of LLMs (CodeLlama-7B and Vicuna-13B) before and after fine-tuning, illustrating again the consistent benefits of using GraphSAGE.  In summary, the results show that improvements from GraphSAGE are consistently observed, even under different prompt styles or after fine-tuning LLMs.", "section": "5.5 Improved Prompts and Fine-tuned LLMs"}, {"figure_path": "bmoS6Ggw4j/figures/figures_19_1.jpg", "caption": "Figure 6: Illustration of LLM's Direct Inference and GraphSearch Method.", "description": "This figure illustrates the two baselines used in the paper for comparison: LLM's Direct Inference and GraphSearch.  LLM's Direct Inference uses an LLM to directly generate the task invocation path. GraphSearch employs a greedy search algorithm on the task graph and leverages LLMs to score each candidate task's suitability for the current step. The figure visually represents the process flow, highlighting the differences in their approaches.", "section": "3 Graph Formulation and Insights"}, {"figure_path": "bmoS6Ggw4j/figures/figures_21_1.jpg", "caption": "Figure 6: Illustration of LLM's Direct Inference and GraphSearch Method.", "description": "This figure illustrates the two training-free baselines used in the paper for comparison against the proposed method.  (a) shows the \"Direct Inference\" method, where an LLM directly infers the task invocation path from the user request. (b) shows the \"GraphSearch\" method, which leverages a graph search algorithm guided by an LLM to explore and select the optimal task invocation path from the task graph. The figure highlights the steps involved in each method, including task decomposition, task assessment (for GraphSearch), and path selection.", "section": "Supplementary Materials for Training-free Methods"}, {"figure_path": "bmoS6Ggw4j/figures/figures_23_1.jpg", "caption": "Figure 4: Illustrative details of experimental datasets.", "description": "This figure shows example user requests, corresponding decomposed steps, and task invocation paths from four different datasets used in the paper: HuggingFace, Multimedia, Daily Life, and TMDB. Each example illustrates how a complex user request is broken down into smaller, manageable sub-tasks, highlighting the dependencies between them.  The figure provides concrete examples of the types of tasks and their relationships that are used to evaluate the task planning performance of different LLMs and GNN-based methods.", "section": "3 Graph Formulation and Insights"}, {"figure_path": "bmoS6Ggw4j/figures/figures_26_1.jpg", "caption": "Figure 5: Illustrative Examples of LLMs Failure to Solve Graph Computational Problems under Permutation (i.e., node re-odering). Experiments were conducted for 30 times.", "description": "This figure shows three examples where different node orderings of the same graph lead to different diameter predictions by LLMs.  This demonstrates the lack of permutation invariance in LLMs, a significant limitation when dealing with graph-structured data. The experiments were repeated 30 times to highlight the consistency of this issue.", "section": "Failures of LLMs in Planning: Empirical Findings"}, {"figure_path": "bmoS6Ggw4j/figures/figures_27_1.jpg", "caption": "Figure 6: Illustration of LLM's Direct Inference and GraphSearch Method.", "description": "This figure illustrates the two baselines used in the paper for comparison: LLM's Direct Inference and GraphSearch.  LLM's Direct Inference uses the LLM directly to predict the task sequence from the user request. GraphSearch uses an iterative search algorithm on the task graph, where at each iteration, the LLM scores the suitability of neighboring tasks to proceed in the search.  Both methods highlight the challenge of accurately discerning task dependencies and relations when solely relying on LLMs.", "section": "3 Graph Formulation and Insights"}, {"figure_path": "bmoS6Ggw4j/figures/figures_30_1.jpg", "caption": "Figure 7: Illustration of our GNN-enhanced Task Planning. First, LLMs interpret user request into several manageable steps. Then, we leverage GNNs for task retrieval, sequentially matching each step description to a suitable task, finally generating the invocation path.", "description": "This figure illustrates the proposed GNN-enhanced task planning framework.  It shows a two-stage process: 1) LLMs decompose the user request into a sequence of manageable steps and 2) GNNs select the appropriate tasks from a task graph to fulfill each step, thereby generating the final task invocation path. The optional GNN training uses a Bayesian Personalized Ranking (BPR) loss to learn from implicit sub-task rankings. This framework combines the strengths of LLMs for natural language understanding and GNNs for graph-based decision-making.", "section": "4 Integrating GNNs and LLMs for Planning"}, {"figure_path": "bmoS6Ggw4j/figures/figures_31_1.jpg", "caption": "Figure 8: LM+GNN Configuration. We offer two configurations: only training the GNN while keeping the LM frozen (Table 11), or co-training both the LM and GNN (Table 12).", "description": "This figure illustrates the two different model configurations used in the training-based approach.  The top configuration shows the model architecture where only the Graph Neural Network (GNN) is trained, while the Language Model (LM) is frozen (weights are not updated during training). The bottom configuration shows a co-training setup, where both the GNN and LM are trained simultaneously using the Bayesian Personalized Ranking (BPR) loss function.  Both models use a similar pipeline for inference:  the LM extracts text from an image, which serves as input to the GNN.  The GNN then interacts with the task graph to select a task, and the LM is used again to process the output of the GNN.  The difference is that in the second approach, the LM is also fine-tuned alongside the GNN.", "section": "4 Integrating GNNs and LLMs for Planning"}, {"figure_path": "bmoS6Ggw4j/figures/figures_37_1.jpg", "caption": "Figure 6: Illustration of LLM's Direct Inference and GraphSearch Method.", "description": "This figure illustrates the difference between LLM's Direct Inference and GraphSearch methods for task planning.  LLM's Direct Inference uses LLMs to directly generate task invocation paths from user requests.  In contrast, GraphSearch employs an iterative search strategy that evaluates the suitability of potential task sequences using LLMs before selecting an optimal path.  The figure shows a detailed breakdown of each step involved in both methods, including prompt engineering, task assessment, path selection and final path generation.", "section": "Supplementary Materials for Training-free Methods"}, {"figure_path": "bmoS6Ggw4j/figures/figures_38_1.jpg", "caption": "Figure 10: Failure cases of GNN. Our framework relies heavily on the quality of decomposed task steps. Ambiguous steps (step 2 which actually incorporates two steps) may mislead GNN to select the wrong task.", "description": "The figure shows a case where the GNN framework fails due to ambiguous step descriptions in the task decomposition phase.  An ambiguous step leads the GNN to choose an incorrect task, which then propagates to subsequent steps in the sequential task selection process.  This illustrates a limitation of the approach where the accuracy of the GNN depends on the clarity of the input descriptions.", "section": "Failures of LLMs in Planning: Empirical Findings"}, {"figure_path": "bmoS6Ggw4j/figures/figures_38_2.jpg", "caption": "Figure 9: Case Study of GNNs' Effectiveness. Nodes colored in pink and red denote wrongly predicted task or hallucinated task, respectively. Due to space limitations, we only show the first four valid searched paths of BeamSearch for illustration. Even though LLM can explore ground-truth paths during searching, they lack certain instruction-following and reasoning abilities to consistently choose the optimal path. On the contrary, GNN can correctly align decomposed steps with suitable tasks, hitting the ground-truth result.", "description": "This figure demonstrates two examples to compare the performance of LLMs' direct inference, BeamSearch, and GraphSAGE. The results show that although LLMs can explore ground-truth paths, they often fail to select the optimal paths due to limited instruction-following and reasoning abilities. In contrast, GraphSAGE consistently selects the correct tasks and achieves the ground-truth results, highlighting its effectiveness in task planning.", "section": "H Case Studies"}]