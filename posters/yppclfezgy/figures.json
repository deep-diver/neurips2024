[{"figure_path": "yppcLFeZgy/figures/figures_2_1.jpg", "caption": "Figure 1: Model architecture of MutaPLM. (a) The encoding branch of the protein delta network. The delta encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate z\u2206. (b) The decoding branch of the protein delta network. The key components involve a delta decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation.", "description": "This figure illustrates the architecture of MutaPLM, a framework for interpreting and navigating protein mutations using protein language models.  The figure is divided into two parts: (a) the encoding branch and (b) the decoding branch of a protein delta network. The encoding branch takes the difference between the representations of mutant and wild-type proteins (generated by a Protein Language Model, PLM) to create a \"delta\" representation (z\u2206).  This delta representation captures the essence of the mutation. The decoding branch uses this delta representation, along with the wild-type protein representation and a Large Language Model (LLM), to generate a description of the mutation's effects and predict the new amino acid sequence.  The use of both PLM and LLM allows for cross-modal learning and generation of human-understandable textual explanations.", "section": "3 Methods"}, {"figure_path": "yppcLFeZgy/figures/figures_3_1.jpg", "caption": "Figure 2: Training pipeline of MutaPLM. (a) Workflow of pre-training on protein-related literature. We perform next token prediction for the encoding workflow and conditional masked language modeling for the decoding workflow. (b) Workflow of fine-tuning with chain-of-thought (CoT). We employ a two-round dialog that involves describing the functions of a wild-type protein, explaining the effects of its mutation, and predicting the mutation based on the mutational effects.", "description": "This figure illustrates the training pipeline of the MutaPLM model.  Panel (a) shows the pre-training stage on protein literature, where the model learns from both protein sequences and their associated textual descriptions.  The encoding workflow uses next token prediction, while the decoding workflow uses conditional masked language modeling. Panel (b) depicts the fine-tuning stage using chain-of-thought prompting. This involves a two-round dialog: first, describing the wild-type protein's function; second, explaining the mutation's effects and predicting the mutation based on those effects.", "section": "3 Methods"}, {"figure_path": "yppcLFeZgy/figures/figures_8_1.jpg", "caption": "Figure 5: Visualization of fitness scores for multi-round protein optimization. The curves indicate the average results, and the shaded regions indicate the standard deviation.", "description": "This figure visualizes the results of multi-round protein optimization using different methods.  The x-axis represents the number of mutation rounds, and the y-axis represents the fitness score.  Multiple lines represent different methods: Random, EvoProtGrad (OntoProtein), EvoProtGrad (ESM-2), and MutaPLM (Ours). Shaded areas show standard deviation. The figure demonstrates MutaPLM's superior performance in achieving higher fitness scores across multiple rounds and different proteins.", "section": "4.3 Performance Evaluation on Mutation Engineering"}, {"figure_path": "yppcLFeZgy/figures/figures_9_1.jpg", "caption": "Figure 6: Performance analysis for mutation explanation (blue) and engineering (orange) on pre-training and fine-tuning. w/o pt: without pre-training. w/ pt: with pre-training.", "description": "This figure displays the results of experiments evaluating the effects of pre-training and fine-tuning on MutaPLM's performance in mutation explanation and engineering tasks.  The x-axis represents the number of training steps, while the y-axis shows the ROUGE-L score (for explanation) and Recall@50 (for engineering). Separate lines are shown for models with and without pre-training.  The figure shows that pre-training improves performance, particularly in the later stages of training, but also that fine-tuning can lead to overfitting if not carefully managed.", "section": "4.4 In-depth Analysis"}, {"figure_path": "yppcLFeZgy/figures/figures_21_1.jpg", "caption": "Figure 1: Model architecture of MutaPLM. (a) The encoding branch of the protein delta network. The delta encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate z\u2206. (b) The decoding branch of the protein delta network. The key components involve a delta decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation.", "description": "This figure shows the architecture of MutaPLM, a framework for interpreting and navigating protein mutations.  It consists of two main branches: an encoding branch and a decoding branch. The encoding branch uses a protein delta network to capture explicit protein mutation representations, taking the difference between mutant and wild-type protein representations as input. The decoding branch reconstructs mutant features from the encoded differences and predicts the position and amino acid of the mutation.", "section": "3 Methods"}, {"figure_path": "yppcLFeZgy/figures/figures_26_1.jpg", "caption": "Figure 3: Human-AI collaborative evaluation results for mutation explanation on the test sets of MutaDescribe. We show the number of accurate, relevant, opposite, and irrelevant predictions.", "description": "This figure presents a human-AI collaborative evaluation of MutaPLM's mutation explanation capabilities on the MutaDescribe dataset.  It displays a heatmap showing the counts of predictions categorized by a human expert as Accurate, Relevant, Opposite, and Irrelevant.  The results are broken down across three difficulty levels (Easy, Medium, Hard) of the test set, demonstrating the model's performance across different complexities of mutations.", "section": "4.2 Performance Evaluation on Mutation Explanation"}, {"figure_path": "yppcLFeZgy/figures/figures_28_1.jpg", "caption": "Figure 1: Model architecture of MutaPLM. (a) The encoding branch of the protein delta network. The delta encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate z\u2206. (b) The decoding branch of the protein delta network. The key components involve a delta decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation.", "description": "This figure illustrates the architecture of MutaPLM, a framework for interpreting and navigating protein mutations using protein language models.  Panel (a) shows the encoding branch of the protein delta network, which takes the difference between the mutant and wild-type protein representations from a pre-trained PLM (protein language model) and generates a 'delta feature' (z\u2206) representing the mutation. Panel (b) depicts the decoding branch, where the delta feature is used by a delta decoder to reconstruct the mutant protein features. This information is then processed by two prediction heads that determine the mutation's amino acid and its location in the protein sequence. In essence, the model uses the difference in representation between the wild type and the mutant as input to predict the mutation.", "section": "3 Methods"}]