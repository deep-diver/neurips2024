{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduced the concept of in-context learning, a core concept explored in the current paper."}, {"fullname_first_author": "E. Aky\u00fcrek", "paper_title": "In-context language learning: Arhitectures and algorithms", "publication_date": "2024-01-16", "reason": "This paper provides a comprehensive overview of in-context learning architectures and algorithms, directly relevant to the current work."}, {"fullname_first_author": "S. Garg", "paper_title": "What can transformers learn in-context? a case study of simple function classes", "publication_date": "2022-12-01", "reason": "This paper delves into the mechanistic understanding of in-context learning, complementing the current study\u2019s investigation of the internal mechanisms of LLMs."}, {"fullname_first_author": "J. Hewitt", "paper_title": "Designing and interpreting probes with control tasks", "publication_date": "2019-09-01", "reason": "This paper introduces the concept of interpretability probes, which the current work utilizes to analyze the internal workings of LLMs."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, foundational to the large language models studied in this work."}]}