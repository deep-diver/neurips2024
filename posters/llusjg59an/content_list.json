[{"type": "text", "text": "Where does In-context Learning Happen in Large Language Models? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Suzanna Sia \u2217 David Mueller Kevin Duh Johns Hopkins University Johns Hopkins University Johns Hopkins University ssia1@jh.edu dam@cs.jhu.edu kevinduh@cs.jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Self-supervised large language models have demonstrated the ability to perform various tasks via in-context learning, but little is known about where the model locates the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from recognizing the task to performing the task. Through a series of layer-wise context-masking experiments on GPTNEO2.7B, BLOOM3B, LLAMA2- 7B, LLAMA2-7B-CHAT, STARCODER2-3B and STARCODER2-7B on Machine Translation and Code generation, we demonstrate evidence of a \"task recognition\" point where the task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in $45\\%$ computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32 using an example with Machine Translation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In-context learning (ICL) refers to the phenomenon in which large generative pretrained transformers (GPTs) perform tasks with no gradient updates when shown task examples or descriptions in their context [13, 12]. There are many works on in-context learning which focused on prompt-engineering, treating GPT models as black boxes by focusing on which examples to provide in-context [47, 14, 19, 20, 51]. However, many of these works apply surface level interventions leaving the internal mechanism of task recognition in GPT models largely not understood. ", "page_idx": 0}, {"type": "text", "text": "In this work, we ask where does in-context Learning occur in GPT models? Our view of In-context Learning is that of \u201ctask recognition\" not \u201ctask learning\" [64, 41]. While in-context learning in GPT models appears to be generally applicable to any natural language task, to study task location, we focus on two tasks, Machine Translation (MT) and Code generation, as there is little to no ambiguity in evaluating whether the model has recognised the task. For MT, the model must generate tokens in a different language. For Code generation, the model must produce a working program which can be executed correctly. These two tasks are unlikely to be \u201clearnt\" from following patterns, and are more complex than a lookup in associative memory for simple Question-Answer tasks. ", "page_idx": 0}, {"type": "text", "text": "We focus on multi-head attention layers as a unit of study, as the self-attention mechanism is designed to allow the model to attend to it\u2019s context during generation of the target sentence [54]. Using causal masking over different parts of the context we demonstrate that there exists a \"task-recognition\" point after which attention to the context is no longer necessary (Section 4). The potential implications are large computational savings when the context is several times longer than the test source sentence (Section 5). Having identified the layers in which \"task recognition\" occurs, we also conduct exploratory studies into the extent to which subsequent layers are either redundant or corresponding to the \"task recognition\" layers. Simple layer-wise masking shows that for smaller models, removing attention around the \"task-recognition\" layers can cause the model to fail to perform translation all-together, whereas layers towards the end of the model are much more redundant (Section 6.2). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Next, we observe that very lightweight fine-tuning of LoRA parameters [29] are most effective at earlier layers of the model compared to the later ones (Section 7.2). While there is not a strict 1-1 correlation in layers, this provides loose supports for the conjecture that earlier layers are more important for the task. ", "page_idx": 1}, {"type": "text", "text": "We further investigate the extent of MT task redundancy using differentiable $L_{0}$ regularisation to train discrete attention head gates (Section A.7). We find that around $10\\%$ of the attention heads can be masked, which indicates that the attention-heads themselves are not redundant, it is attention over all of the context that can be redundant. This fundamentally differs from the literature in supervised NMT where more than half of the attention heads can be pruned, and Transformers are highly specialised for MT [56, 38, 7]. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In-Context Learning was first demonstrated by [13] who showed that GPT-3 could be used to perform a huge variety of tasks without any task-specific parameters or training, by conditioning the model\u2019s generation on a prompt which included a few labeled examples of the task of interest. Since then, interest in using GPT models for ICL has grown significantly [39, 3, 61], with several recent works introducing methods such as instruction-tuning [49, 58] or chain-of-thought prompting [59] to improve downstream ICL accuracy. One key characteristic of In-context Learning is its reliance on prompt examples demonstrating the task that the model should carry out [46]. ", "page_idx": 1}, {"type": "text", "text": "In-context Learning as Task Recognition. Ostensibly, ICL can work for nearly any task that can be defined or described in natural language, and therefore has potential for incredibly broad impact. However, ICL can often still underperform supervised fine-tuning [9], prompting research in analyzing the mechanisms underlying ICL. One line of work studies in-context learning with linear functions, typically linear regression, characterizing the learnability of these functions with ICL [33, 24] and even the learning algorithm a transformer uses [2, 17, 57]. A second body of work suggests that in-context learning locates existing latent concepts (tasks) which have been already learnt during pretraining [64, 60]. Notably, [52] describe function vectors which are robust to changes in context. [26] try to characterise the extent of task recognition from the pre-training data. Although there have been many studies on task recognition, our work presents a complementary perspective for task recognition, by demonstrating that there exists a point in the model\u2019s layers where the task has been located and causal self-attention onto the context is no longer needed for the model to perform the task.2 ", "page_idx": 1}, {"type": "text", "text": "Transformer Layers and Self-attention as the Unit of Study. Many works study layers of the model as a natural unit of analysis for interpretability [28, 18, 42, 21, 8, 48]. We highlight some of the work which is more closely related to task performance. [63] study the layer-wise adaptability by a hidden-state variability ratio while [55] study evolution of representations in MT-supervised transformer models. [43] studies when model layers can be skipped by feeding intermediate representations into the final output layer of a pre-trained supervised model. Our work adds to this body of work by considering the perspective of when and where layers are responsible for task location in in-context learning models. ", "page_idx": 1}, {"type": "text", "text": "The self-attention mechanism specifically has been highlighted as a source of redundancy by many previous and concurrent works [10, 40, 27]. This is due to it\u2019s causal structure over the input symbols under the specific context of the input sequence within it\u2019s context window [45]. In this paper, we study a major source of causal redundancy in the input, the \"prompt examples\" that are provided as input-output demonstrations to the model for \"in-context learning\". ", "page_idx": 1}, {"type": "text", "text": "3 Data and Settings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Models We use GPTNEO2.7B [11], BLOOM3B [50], LLAMA2-7B and LLAMA2-7B-chat [53] in all of our experiments with Machine Translation. For code generation, we used LLAMA2-7B and LLAMA2-7B-chat, STARCODER2-3B and STARCODER2-7B[35]. ", "page_idx": 2}, {"type": "text", "text": "GPTNEO2.7B has 32 layers and 20 heads, BLOOM3B has 30 layers and 32 heads, LLAMA2-7B has 32 layers and 32 heads and STARCODER2 has 30 layers and 24 heads. The checkpoints we use are from Meta AI (for LLAMA) and the transformers library [62]. Starcoder2 utilises grouped-query attention [1], while the rest of the models use \u201cregular\" multi-head self-attention. ", "page_idx": 2}, {"type": "text", "text": "GPTNEO was trained on The PILE [23], an 825GB text dataset which consists of roughly $98\\%$ English data. Despite being mostly monolingual, The PILE contains Europarl which GPTNEO was trained on at a document level (rather than a sentence level). Conversely, BLOOM was trained on the ROOTS corpus [32], a composite collection of 498 datasets that were explicitly selected to be multilingual, representing 46 natural languages and 13 programming languages. LLAMA training data consists primarily of common crawl, C4, wikipedia, stackexchange as major sources. STARCODER2 was trained on Github as well as Arxiv and Wikipedia. To our knowledge, there has not been any reports of sentence level parallel corpora in the training datasets of these models. ", "page_idx": 2}, {"type": "text", "text": "Data We test our models using two datasets, FLORES [25] for Translation and HUMANEVAL for Code generation. For FLORES, we experiment with $\\in\\ n\\leftrightarrow\\pounds\\,\\pounds$ (main paper) and $\\mathtt{e n}\\leftrightarrow\\mathtt{p t}$ (appendix). Prompt examples are drawn from the development set. We evaluate the generations using BLEU scores, following the implementation from [44]. For HUMANEVAL[15], we evaluate on the execution accuracy of the generated code using the $\\mathrm{Pass}(\\varpi1$ metric. As HUMANEVAL does not have an explicit train set, the prompt set is drawn from the Mostly Basic Python Program (MBPP) dataset [4]. To account for example selection and ordering effects,3 all inference runs were repeated with 5 randomly sampled prompt example sets. ", "page_idx": 2}, {"type": "text", "text": "Prompt Format Our prompts may consist of instructions, examples, both, or none. Importantly, we adopt neutral delimiters, \"Q:\" and \"A:\" to separate the prompt and the start of machine generated text. This ensures that the models do not have any information from the delimiters on what the task is and must recognise the task from examples. 4 ", "page_idx": 2}, {"type": "text", "text": "For the translation task, when no natural language instructions are used the model input will be Q: {source_sentence} A: Instructions are given in natural language and take the form: Translate from {L1} to {L2}: Q: {source_sentence} A:, where $\\mathrm{~\\tt~{~L~1~}~}=$ English and $\\begin{array}{r l}{\\mathbb{L}{\\mathcal{Z}}}&{{}=}\\end{array}$ French if the source and target languages are English and French respectively. Examples are given after instructions, and similarly delimited by Q: and A:. See Appendix: Table 1 for an example. ", "page_idx": 2}, {"type": "text", "text": "For the code generation task, when no natural language instructions are used, the model input will be $\\boldsymbol{\\mathrm{\\Omega}}$ : {program_description}, where the program_description is Instructions are given in natural language and take the form: \"Write a program for the following task:\". ", "page_idx": 2}, {"type": "text", "text": "4 Where does In-context MT happen? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "4.1 Analysis Methodology: Layer-from Context Masking ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In-context learning differs from task-specific supervised learning in that, during test time, the desired task must be identified from the context first, then executed. At what stage in the feed-forward computation does a GPT-style model transition from an in-context learner to a translation model? To explore this question, we introduce layer-from context-masking which masks out all attention weights to the context (instructions or prompts) from a certain layer onwards (see Figure 1 for a graphical description). ", "page_idx": 2}, {"type": "image", "img_path": "LLuSjg59an/tmp/b0e4d8d658e453ad07de3cfa767139f967042c378a9de207fd6f9d6288f46292.jpg", "img_caption": [], "img_footnote": ["Figure 1: Graphical explanation of Masking the Attention over Instructions and Examples. The leftmost image has instructions and masks examples (Instr, $\\overline{{\\mathbb{E}{\\mathbb{X}}}}^{M a s k\\setminus}$ , while the right image has both instructions and examples masked $(\\overline{{\\mathrm{I}\\,\\mathrm{n}\\,\\mathrm{s}\\,\\mathrm{t}\\,\\mathrm{r}}},\\mathrm{E}\\mathbf{x}^{M a s k})$ xMask). In the table, the overline corresponds to the yellow highlights. $N$ and $Y$ refer to absence and presence of either Instruction of Examples. Instr: Instructions and $\\mathrm{Ex}$ : Examples. "], "page_idx": 3}, {"type": "text", "text": "For Causal Decoder-only Transformer Language Models, given each position $i$ , the Attention weight \u03b1ij over context positions j, j < i can be computed by a \u03b1ij = softmax( Q\u221aKdk ) ij. Each element in $(Q K^{T})$ is the dot product between a query vector and key vector $q_{i}\\cdot k_{j}$ , where $q_{i}=W_{q}x_{i},k_{j}=$ $W_{k}x_{j}$ for trained weight matrices $W_{k}$ and $W_{q}$ .5 We apply the attention mask over the context so that the attention score is $(q_{i}\\cdot k_{j})+m(j,{\\bf u})$ . Here $\\mathbf{u}$ is the context that we wish to mask, and ", "page_idx": 3}, {"type": "equation", "text": "$$\nm(j,\\mathbf{u})=\\left\\{0\\begin{array}{l l}{{\\mathrm{if}}\\;x_{j}\\notin\\mathbf{u}}\\\\ {-\\infty}&{{\\mathrm{if}}\\;x_{j}\\in\\mathbf{u}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "All masks operate from the $j$ -th layer $(\\ell_{j})$ onwards, i.e. masking from $\\ell_{20}$ means causally masking out attention to all context positions from $\\ell_{20:n_{\\ell}}$ , where $n_{\\ell}$ is the total number of layers. To construct Fig 2, we increment $\\ell_{j}$ from 1 to $n_{\\ell}$ and apply the set of masks $\\{m(j,{\\bf u})\\}^{\\ell_{j}:n_{\\ell}}$ in each experiment and observe the performance of the model. ", "page_idx": 3}, {"type": "text", "text": "Under this causal masking treatment masking from layer $\\ell$ , the model must rely on the hidden state representations of the target input sentence from layer $\\ell+1$ only and having self-attention on only the positions of the task test input only to complete the task; if the test input representations do not already encode the target task (e.g., Machine translation or Code generation) then the model will fail to generate the correct output. ", "page_idx": 3}, {"type": "text", "text": "In other words, the goal is to characterise where the model has \"located\u201d the task. In all experiments we mask the examples provided in the context, but to control for the effect of semantic instructions, we ablate over different treatments of the instructions by removing instructions entirely $(\\overline{{\\mathrm{Ex}}}^{M a s k})$ , leaving them unmasked $(\\mathrm{In}\\,\\mathrm{s}\\,\\mathrm{t}\\,\\mathtt{r}\\overline{{\\mathrm{Ex}}}^{M a s k})$ , or masking them together with the examples $(\\overline{{\\mathrm{In}\\,\\mathbf{s}\\,\\mathsf{t}\\,\\mathtt{r}\\,\\mathtt{E}\\,\\mathbf{x}}}^{M a s k})$ ExMask). The overline notation indicates the context which are masking over ( Figure 1). ", "page_idx": 3}, {"type": "text", "text": "We run this experiment for all layers in the model, $j=1,\\cdot\\cdot\\cdot,n_{\\ell}$ and observe how task performance is affected, using 5 examples per prompt. ", "page_idx": 3}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We discuss the central findings of the paper: Models do not need to maintain attention over all of the context across every layer to perform the task. ", "page_idx": 3}, {"type": "image", "img_path": "LLuSjg59an/tmp/e74eab6c345a4b8776f1918c9995f47dffbd39cc1fa5de125a668b2882e37d04.jpg", "img_caption": ["Figure 2: Layer-from context-masking experiments for GPTNeo2.7B, BLOOM3B, Llama7b, Llama7bchat on $\\in\\ n\\leftrightarrow\\pounds\\,\\pounds$ . The graphs show translation performance when masking contexts from the $j^{\\mathrm{th}}$ layer onwards. Different lines indicate different treatments of the instruction, as described in Figure 1. The dashed black line is the performance when shown both examples and instructions without masking. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "LLuSjg59an/tmp/575a2bc7e443423d730ef3c69ed9cbfbedd622c1f0debd97abd5ff2bc43a4283.jpg", "img_caption": ["Figure 3: Layer-from context-masking experiments for Starcoder2-3B, Starcoder2-7B, Llama7b, Llama7b-chat on a text to code generation task. The graphs show translation performance when masking contexts from the $j^{\\mathrm{th}}$ layer onwards. Different lines indicate different treatments of the instruction, as described in Figure 1. The dashed black line is the performance when shown both examples and instructions without masking. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "In all models we observe that when applying masking from $\\{m(j,{\\bf u})\\}^{\\ell:n_{\\ell}}$ over the context, performance plateaus before the final layer, i.e., when $\\ell=n_{\\ell}$ . The results of our experiment for $\\mathtt{e n\\to f r}$ and $\\mathtt{f}\\,\\mathtt{r}\\to\\mathtt{e}\\,\\mathtt{n}$ are shown in Figure 3, and additional experiments for GPTNeo and Bloom on $\\mathtt{e n\\to p t}$ and $\\mathtt{p t\\to e n}$ are shown in Section A.2. ", "page_idx": 4}, {"type": "text", "text": "Different models reach this plateau point at different layers. In GPTNEO this point occurs around layer 25, in BLOOM this point occurs around layer 15-20, and in LLAMA models this occurs around layer 13-15. As English is the dominant language, as expected models can successfully perform translation into English upon earlier layers of masking, than translation out of English. ", "page_idx": 4}, {"type": "text", "text": "At this point, the models beneftis only marginally, if at all, from attending to the context, suggesting most of the task \"location\" has already occurred. ", "page_idx": 4}, {"type": "text", "text": "There exists critical layers for task location. Prior to the task recognition point, around the middle layers of the models, moving the context mask up a layer results in a significant increase to performance. We consider these critical layers, as instead of a gradual increase in performance, we observe very steep jumps, accounting for more than $80\\%$ of the model\u2019s ceiling performance for that task. We conjecture that the model is locating the correct task during processing in these middle layers, after which the context is no longer necessary to perform the task. ", "page_idx": 4}, {"type": "text", "text": "Overall, our findings suggest a 3-phase process to in-context learning: in the first phase, moving the mask up makes little difference in performance, which is close to 0. This suggests that the context has not influenced task location at all. In the second phase, shifting the mask upwards makes a large difference in performance, suggesting that the model has started to locate the task but can improve significantly with more processing of the context. Finally, in the third phase, shifting the mask upwards again has little-to-no effect on the performance, suggesting that the model has fully recognized the task as translation and no longer requires the context to interpret the task. ", "page_idx": 4}, {"type": "text", "text": "4.3 Instruction-tuned vs Non-instruction Tuned Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "When comparing non-instruction tuned LLAMA2-7B vs instruction-tuned models LLAMA2-7BCHAT, we do not observe any noticeable difference in where performance plateaus, i.e., where the model no longer requires attention over the context. This occurs around layers 18 for both LLAMA models in $\\mathtt{e n}\\to\\mathtt{f r}$ and around layer 14 for $\\mathtt{f}_{\\mathtt{T}}\\to\\mathtt{e}_{\\mathtt{n}}$ . The main difference is that instruction-tuned model is able to achieve better performance in the earlier layers for the setting where instructions are present and examples are masked (Instr, xMask). This is to be expected as these models are tuned towards following instructions. ", "page_idx": 5}, {"type": "text", "text": "Overall we find that the observation of task recognition layers and a task recognition point is present across both non-instruction tuned and instruction tuned models, and that this presents itself similarly in both types of models. ", "page_idx": 5}, {"type": "text", "text": "4.4 Do models have a distinct task recognition region regardless of the type of task? (Experiments on Code Generation) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For tasks that the model does not perform fluently, we do not observe a sharp increase at any particular layer. For instance, for code generation (HUMANEVAL) where the LLAMA2 model performs poorly, we can observe only a very gradual effect of masking the self-attention layers, and not a distinct increase as compared to the LLAMA2\u2019s performance on Translation. ", "page_idx": 5}, {"type": "text", "text": "However when we consider STARCODER2 while masking instructions or no instructions, i.e., the Instr, $\\overline{{\\mathrm{Ex}}}^{M a s k}$ and ExMask, we again see the same pattern demonstrating the task recognition phenomena on layer 19 of the 3B model, and layer 20-23 of the 7B model. ", "page_idx": 5}, {"type": "text", "text": "To understand Starcoder2\u2019s strong performance on the (Instr, $\\overline{{\\mathbb{E}{\\mathbb{X}}}}^{M a s k\\,,}$ ) condition, investigations found that the instructions and the test prompt alone contain sufficient information for the model to recognise that the task is to generate a Python program, even though the model is not instruction tuned. This happens as the model is very specialised towards code generation and has a strong prior to generate python code given its prevalence in it\u2019s training data. ", "page_idx": 5}, {"type": "text", "text": "4.5 The Role of Instructions vs Examples ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In separate experiments, we found that when shown only instructions and no examples, GPTNEO and BLOOM models are unable to translate, and their performance is nearly at 0 BLEU Score. For GPTNEO and BLOOM we see that the behavior of the model is similar when no instructions are present (ExMask) and when instructions are masked $(\\overline{{\\mathrm{I}\\,\\mathrm{n}\\,\\mathrm{s}\\,\\mathrm{t}\\,\\mathrm{r}\\,,\\,\\mathrm{E}\\,\\mathrm{x}}}^{M a s k})$ . However, if the model is given complete access to instructions $(\\mathrm{Inst}\\,\\mathtt{r}\\overline{{\\mathrm{Ex}}}^{M a s k})$ ), it can use the intermediate processing of examples to reach baseline performance earlier. ", "page_idx": 5}, {"type": "text", "text": "5 Inference Efficiency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Speeding up transformer inference is of great interest to the community [22]. We highlight the potential of speeding up inference time as a direct consequence of identifying where task recognition occurs in the model and redundancy of self-attention processing. Our results indicate that we can achieve significant speedups in inference by removing the processing of context-tokens all-together after a certain point in the model, with little to no impact on downstream performance. Let $\\ell_{r}$ be the $r^{\\mathrm{th}}$ layer where we can mask out the attention of the context across subsequent layers and match the \u201cceiling\" performance. Let $k$ be the number of prompt examples, where each example consists of a pair of parallel sentences. Then, for a model with $n_{\\ell}$ layers, the amount of processing in terms of speed and memory saved is approximately $(n\\ell-r)/n_{\\ell}\\times(k/k+1)$ . ", "page_idx": 5}, {"type": "text", "text": "Using the example of LLAMA7B (32 layers) on $\\scriptstyle\\mathtt{e n\\to f r}$ , we see from Figure 3 that the model is very close to it\u2019s ceiling score after processing the examples at layer 14 ( $\\mathcal{E}=14$ ). If we no longer need to process examples after $\\ell=14$ , under a prompt size of 5 the savings are approximately ${\\bf45}\\%$ . ", "page_idx": 5}, {"type": "text", "text": "For instruction-tuned models which are typically deployed in production, even if we assume that no examples are provided, savings can be non-trivial as very long-form instructions are typically provided to the model in an attempt to control it\u2019s behavior (prompt engineering). ", "page_idx": 5}, {"type": "text", "text": "Although we had demonstrated the redundancy of self-attention over the input context, the significance of this computational savings extends to all components of the transformer during forward inference. Since all subsequent layers of forward inference no longer rely on computations on previous token positions, all processing related to those redundant token positions (from the task recognition layer onwards) can be effectively removed. ", "page_idx": 6}, {"type": "text", "text": "6 Characterising Redundancy in Layers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recently, [48] found that many layers in pre-trained transformers can be dropped with little harm to downstream tasks; moreover, it is well known neural MT transformer models are known have several redundant heads which are not necessary during test time [56, 38, 7]. However, it is not clear if the same trends hold for in-context MT models, and how that redundancy is related to task location versus task execution. We focus on the task of $\\mathtt{e n}\\to\\mathtt{f r}$ in this set of experiments. ", "page_idx": 6}, {"type": "image", "img_path": "LLuSjg59an/tmp/882be982dc723164d6c45d7c545a4f3bdef2557278aed94e741b3cb429510868.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Layer-from experiments for GPTNEO2.7B, BLOOM3B, LLAMA and LLAMA7B-CHAT on $\\mathtt{e n}\\to\\mathtt{f r}$ when masking out from layer $j$ onwards. Orange and blue dashed lines refer to the baselines (without masking) of 0 and 5 prompts with instructions. True/False refers to whether there are instructions provided (True) vs not provided (False). In view of the smaller models failure to translate at all under the format Q: A: with no examples, we adopt \"English:\", \"French:\" as delimiters instead of QA in generating this figure. ", "page_idx": 6}, {"type": "text", "text": "One possible explanation for the results in Figure 3 is that, rather than identifying the point at which the task is recognized (no longer requires attending to instructions and examples), we have identified the point at which the model no longer requires attending to any other input tokens (instructions, examples and source sentence). To explore this, we run experiments in the $\\mathtt{e n}\\to\\mathtt{f r}$ direction where we mask attention to all inputs from a certain layer onwards. This does not include masking over the text the model has generated. ", "page_idx": 6}, {"type": "text", "text": "We plot the results in Figure 4; we find that for all models, the layer at which attention can be fully removed is much higher than the layer at which we can remove attention to the context. For GPTNEO and LLAMA, translation performance is never comparable to the baseline with no masking. Conversely, when masking only the context, translation performance improves as early as layer 10 and plateaus at the no-mask baseline much earlier. This supports the interpretation that the curves we observe in Figure 3 are due to the model still requiring attention to the source sentence input. ", "page_idx": 6}, {"type": "text", "text": "We study the contributions of individual attention-layers by performing a simple layer-wise masking of all self-attention heads for a single layer. When we mask layer $j$ , we are masking the attention mechanism of layer $j$ , that is the MLP of layer $j$ acts directly on the output of layer $j-1$ , rather than the output of the attention-head of layer $j$ . Doing so allows us to study how critical each layer is, where critical layers is loosely defined as those that have a large negative impact when masked. ", "page_idx": 6}, {"type": "text", "text": "We plot results for each layer all models, using the three combinations of {0 examples, no instructions}, {5 examples, instructions}, {5 examples, no instructions} in Figure 5.6 ", "page_idx": 6}, {"type": "image", "img_path": "LLuSjg59an/tmp/2801cae7d3c157ab4244cc6df4f5f2df93079ff22bfb6c691559946c8df4ff7a.jpg", "img_caption": ["Figure 5: Layer-wise masking of self-attention heads for GPTNEO2.7B, BLOOM3B, LLAMA and LLAMA-CHAT on $\\in\\Omega\\to f r$ . The orange and blue dotted lines refer to the baselines (without masking) of 0 and 5 prompts with instructions. True/False refers to whether there are instructions provided (True) vs not provided (False). We observe critical layers near the middle and redundant layers towards the end of the model. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Are \u201cCritical\" Layers Task Locating Layers? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 4, we observed that there are layers for task location. In this section, we observe evidence that there are critical layers which correspond to the task locating layers, providing support for our earlier observations. ", "page_idx": 7}, {"type": "text", "text": "For instance for $\\mathrm{LLAMA7B}\\in\\Omega\\to\\mathrm{f}\\,\\mathrm{r}$ , even in the scenarios when examples are provided, we can see a dip in performance around layer 15 to 18. Refering back to Figure 3, we see that this is where most of the task location with large jumps in performance had occurred. ", "page_idx": 7}, {"type": "text", "text": "For GPTNeo, we obseve a large set of contiguous layers which significantly decrease performance at around layer 10 to 15. This also corresponds to where most of the task location (large jumps in performance) had occurred for this model in Figure 3. ", "page_idx": 7}, {"type": "text", "text": "We note that the critical layers in different models have varying degrees of severity. It is not immediately clear why GPTNEO has such critical layers and suffers compared to the other models, although we note that this is unlikely to be due to size or model architecture as BLOOM is also around the same size as GPTNEO and performs more similarly to LLAMA. We suspect that it could be due to training data or some other factor related to the training dynamics but leave this for future work. ", "page_idx": 7}, {"type": "text", "text": "With regard to redundancy, we find that layers can be more safely removed towards the end without a noticeable loss in performance. We observe that for the less stable models, the model achieves close to baseline performance by layer-wise masking from $\\ell_{15}$ for GPTNEO, $\\ell_{26}$ for BLOOM and $\\ell_{20}$ for LLAMA. This suggests that these later layers contain redundancy for translation. ", "page_idx": 7}, {"type": "text", "text": "Overall, observing redundancy in layers is not suprising. To explain why models can have redundant layers, we refer to [16] who identify a phenomena where attention heads attend almost exclusively to delimiter and separator tokens such as [SEP], periods and commas. This is thought to act as a \u201cno-op\" as the value of such tokens in changing the current hidden representation is very small. Note that it is then possible to mask entire Transformer layers and still achieve a sensible output due to residual connections in the Transformer architecture at every layer. ", "page_idx": 7}, {"type": "text", "text": "7 Further Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the following sections, we focus on GPTNEO and BLOOM to conduct deeper analysis on the main phenomena presented in the paper. ", "page_idx": 7}, {"type": "text", "text": "7.1 Does the Number of Prompts Affect Task Recognition? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 4 we study context-masking with a fixed number of prompts. However, it is not clear if the number of prompts affects how fast, layer-wise, the model is able to recognize the task. We plot these results for $\\mathtt{e n\\to f r}$ in Figure 6, for both GPTNEO and BLOOM. In general, we find that the number of prompt examples has little effect on which layer the task is recognized at. While there is some variation in performance when the context is masked around the middle layers of the model, the final performance plateau occurs at the same layer regardless of the number of prompts. ", "page_idx": 7}, {"type": "image", "img_path": "LLuSjg59an/tmp/866cea672af391691645832940da608bf74721d13039d13810eae5309810c8bb.jpg", "img_caption": ["Figure 6: Layer-from context-masking experiments for GPTNeo and BLOOM on $\\mathtt{e n\\to f r}$ investigating number of examples in the $\\overline{{\\mathbb{E}\\mathbb{X}}}^{M\\bar{a}s k}$ mask setting. The dashed black line refers to no instructions and no examples. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "LLuSjg59an/tmp/bd2dd4c7b70b41d5d0707d05f2cb846265e6eb4aa12fcc361d3c53f50fa8e407.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 7: Performance of no-instructions trained Lora layers for GPTNeo and BLOOM on $\\mathsf{e n}\\!\\leftrightarrow\\!\\pm\\!\\pmb{\\Sigma}$ . The dashed black line refers to training of all layers together, while the orange (test without instructions) and blue (test with instructions) dashed lines refers to no training. The layers which are most amenable to lightweight fine-tuning occur in the earlier layers before the \"task recognition\" point. ", "page_idx": 8}, {"type": "text", "text": "7.2 The Adaptability of Task Layers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Intuitively, the layers prior to \"task recognition\" should contain information about locating the MT task. To test this intuition, we further explore the adaptability of these layers by lightweight finetuning experiments. We trained a single Low-rank Adaptation matrix (LoRA; [29]) for each layer of the output projection while keeping the rest of the network frozen.7 The model was shown parallel sentences as input, and layers were trained with no explicit translation instructions. We split the dev set of FLORES into 800 training examples and 200 dev examples. Note that this setup is designed to tune the layers for task location. It is highly unlikely that the model can learn translation knowledge from this small amount of supervision. The LoRA layers were trained for up to 50 epochs with early stopping patience $=5$ and threshold $=0.001$ , with $\\alpha=32,r=32$ and dropout $=0.1$ . The cross-entropy loss was computed only on the target sentence (see Section A.4 for details) and we used the best checkpoint on the 200 held out dev examples for evaluation. ", "page_idx": 8}, {"type": "text", "text": "While each layer can be trained to perform better than no fine-tuning at all, tuning different layers have different impacts on performance Figure 7. In particular, we find that high performing layers occur at the earlier to middle parts of the network, with the peak often occurring near the start of the \"task-locating\" layers from Section 4. In contrast to common fine-tuning wisdom, additional tuning on the later layers has a much smaller impact on final performance for $\\mathtt{e n}\\to\\mathtt{f r}$ . ", "page_idx": 8}, {"type": "text", "text": "7.3 Are There Specialised Attention Heads? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Section 4, we found that the earlier part of the model is critical for task location from the prompt context, and in Section 6.2 we found both critical and redundant layers to the MT task. In this section, we increase the level of granularity to that of attention heads instead of layers. ", "page_idx": 8}, {"type": "text", "text": "A well established finding for supervised encoder-decoder MT models, is that up to $90\\%$ of the attention heads can be pruned while minimising fall in translation performance [56, 6, 38]. We note that asking about the extent of pruning is a slightly ill-formed research question, as it depends on the type of pruning technique used. However broad trends of highly prunable models have been observed in the supervised MT paradigm. For instance, [5] studied attention-head importance for a broader set of ICL tasks, finding that the most important heads for ICL occur in the middle layers of the model. We train discrete attention head gates with $L_{0}$ regularisation for GPTNEO and BLOOM on $\\mathtt{e n}\\to\\mathtt{f r}$ (see Section A.5.1). Overall, we report that there are no \"few\" specialised heads, which directly contrasts with the literature on compression in supervised MT models [56, 38]. Potential reasons for this difference might be due to cross-entropy loss associated with task tuning for MT vs non-specific training on large corpora. We leave this as an open question for future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We demonstrate evidence that In-context Causal Decoder models locate their task at a specific layers during forward inference. To study this, we introduced causal masking of self-attention over the context from layer $\\ell$ onwards (Section 4). The findings generalise across 4 models of different sizes and in both non instruction-tuned and instruction-tuned models. We further identify certain layers as task critical, and show that this corresponds to the task recognition point of the model (Section 6.2) and is not influenced by increasing number of examples (Section 7.1) shown to the models. ", "page_idx": 9}, {"type": "text", "text": "Our central finding that models do not need to maintain attention over all of the context across every layer has direct implications for inference efficiency of transformers, with estimated up to $45\\%$ cost-savings for llama model with 5 examples (Section 5). ", "page_idx": 9}, {"type": "text", "text": "Contrary to common fine-tuning wisdom, we show that it is sometimes beneficial to target middle layers for fine-tuning the model which could be associated with task recognition ( Section 7.2). Finally, we trained attention head gates using differentiable $L_{0}$ regularisation (Section 7.3), and found that around $10\\%$ of attention heads can be masked. These are mostly distributed across the later layers of the model, providing some support for the idea that later layers are redundant. Although we have characterised this phenomena using Machine Translation and Code Generation, we believe that the broad findings are likely to generalise to other tasks. ", "page_idx": 9}, {"type": "text", "text": "8.1 Reproducibility ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The Machine Translation dataset that we use, FLORES [25] and the code generation datasets HumanEval[15] are fully open-source and well-known in the community. All models are opensource and freely available on Huggingface [62]. We used models of \"reasonable\" size (3B and 7B parameters) that can be run with consumer or academic grade GPUs, making our work reproducible to most academic institutions. ", "page_idx": 9}, {"type": "text", "text": "8.2 Impact Statement (Ethics and Societal Consequences) ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "There are no known ethical concerns as these are exploratory studies on open-source LLMs. ", "page_idx": 9}, {"type": "text", "text": "8.3 Limitations (and Future Work) ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "\u2022 There is limited exploration of why different models exhibit varying behaviors in terms of their \"task recognition point\" and critical layers. Unfortunately, the differences are not due to easily observable hyperparameters like model size or architecture. To put in another way, why do large models exhibit different characteristics?   \n\u2022 This paper focuses on empirical analysis, and lacks a theoretical framework that could explain why this phenomena is being observed. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank all the anonymous reviewers for their invaluable comments and suggestions, as well as Daniel Kashabi and Marc Marone for feedback on earlier drafts. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895\u20134901, Singapore, Dec. 2023. Association for Computational Linguistics.   \n[2] E. Aky\u00fcrek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.   \n[3] E. Aky\u00fcrek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Arhitectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.   \n[4] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \n[5] H. Bansal, K. Gopalakrishnan, S. Dingliwal, S. Bodapati, K. Kirchhoff, and D. Roth. Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11833\u201311856, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[6] M. Behnke and K. Heafield. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2664\u20132674, 2020.   \n[7] M. Behnke and K. Heafield. Pruning neural machine translation for speed using group lasso. In Proceedings of the sixth conference on machine translation, pages 1074\u20131086, 2021.   \n[8] I. Ben-Shaul and S. Dekel. Nearest class-center simplification through intermediate layers. In Topological, Algebraic and Geometric Learning Workshops 2022, pages 37\u201347. PMLR, 2022.   \n[9] K. Bhatia, A. Narayan, C. D. Sa, and C. R\u00e9. Tart: A plug-and-play transformer module for task-agnostic reasoning, 2023.   \n[10] Y. Bian, J. Huang, X. Cai, J. Yuan, and K. Church. On attention redundancy: A comprehensive study. In Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 930\u2013945, 2021.   \n[11] S. Black, G. Leo, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large scale autoregressive language modeling with Mesh-Tensorflow, Mar. 2021.   \n[12] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[14] B. Chen, Z. Zhang, N. Langren\u00e9, and S. Zhu. Unleashing the potential of prompt engineering in large language models: a comprehensive review. arXiv preprint arXiv:2310.14735, 2023.   \n[15] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[16] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does BERT look at? an analysis of BERT\u2019s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276\u2013286, Florence, Italy, Aug. 2019. Association for Computational Linguistics.   \n[17] D. Dai, Y. Sun, L. Dong, Y. Hao, S. Ma, Z. Sui, and F. Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.   \n[18] N. De Cao, M. S. Schlichtkrull, W. Aziz, and I. Titov. How do decisions emerge across layers in neural models? interpretation with differentiable masking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3243\u20133255, 2020.   \n[19] P. Denny, V. Kumar, and N. Giacaman. Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language. In Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1, pages 1136\u20131142, 2023.   \n[20] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[21] N. Durrani, H. Sajjad, F. Dalvi, and F. Alam. On the transformation of latent space in fine-tuned nlp models. arXiv preprint arXiv:2210.12696, 2022.   \n[22] Q. Fournier, G. M. Caron, and D. Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, 55(14s):1\u201340, 2023.   \n[23] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[24] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022.   \n[25] N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzm\u00e1n, and A. Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. 2021.   \n[26] X. Han, D. Simig, T. Mihaylov, Y. Tsvetkov, A. Celikyilmaz, and T. Wang. Understanding in-context learning via supportive pretraining data. arXiv preprint arXiv:2306.15091, 2023.   \n[27] S. He, G. Sun, Z. Shen, and A. Li. What matters in transformers? not all attention is needed. arXiv preprint arXiv:2406.15786, 2024.   \n[28] J. Hewitt and P. Liang. Designing and interpreting probes with control tasks. arXiv preprint arXiv:1909.03368, 2019.   \n[29] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[30] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.   \n[31] P. Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of machine translation summit x: papers, pages 79\u201386, 2005.   \n[32] H. Lauren\u00e7on, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao, L. Von Werra, C. Mou, E. Gonz\u00e1lez Ponferrada, H. Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:31809\u2013 31826, 2022.   \n[33] S. Li, Z. Song, Y. Xia, T. Yu, and T. Zhou. The closeness of in-context learning and weight shifting for softmax regression. arXiv preprint arXiv:2304.13276, 2023.   \n[34] C. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312, 2017.   \n[35] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.   \n[36] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland, May 2022. Association for Computational Linguistics.   \n[37] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.   \n[38] P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019.   \n[39] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.   \n[40] B. Pan, R. Panda, Y. Jiang, Z. Wang, R. Feris, and A. Oliva. Ia-red\u03982: Interpretability-aware redundancy reduction for vision transformers. Advances in Neural Information Processing Systems, 34:24898\u201324911, 2021.   \n[41] J. Pan. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. PhD thesis, Princeton University, 2023.   \n[42] A. Pasad, J.-C. Chou, and K. Livescu. Layer-wise analysis of a self-supervised speech representation model. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 914\u2013921. IEEE, 2021.   \n[43] J. Phang, H. Liu, and S. R. Bowman. Fine-tuned transformers show clusters of similar representations across layers. arXiv preprint arXiv:2109.08406, 2021.   \n[44] M. Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018.   \n[45] R. Y. Rohekar, Y. Gurwicz, and S. Nisimov. Causal interpretation of self-attention in pre-trained transformers. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. In M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655\u20132671, Seattle, United States, July 2022. Association for Computational Linguistics.   \n[47] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha. A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024.   \n[48] H. Sajjad, F. Dalvi, N. Durrani, and P. Nakov. On the effect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023.   \n[49] V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022.   \n[50] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni, F. Yvon, M. Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.   \n[51] S. Sia and K. Duh. In-context learning as maintaining coherency: A study of on-the-fly machine translation using large language models. arXiv preprint arXiv:2305.03573, 2023.   \n[52] E. Todd, M. L. Li, A. S. Sharma, A. Mueller, B. C. Wallace, and D. Bau. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.   \n[53] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[54] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[55] E. Voita, R. Sennrich, and I. Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. arXiv preprint arXiv:1909.01380, 2019.   \n[56] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797\u20135808, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[57] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent, 2023.   \n[58] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. Reddy A, S. Patro, T. Dixit, and X. Shen. Super-NaturalInstructions: Generalization via declarative instructions on $1600+$ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics.   \n[59] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[60] N. Wies, Y. Levine, and A. Shashua. The learnability of in-context learning. arXiv preprint arXiv:2303.07895, 2023.   \n[61] N. Wies, Y. Levine, and A. Shashua. The learnability of in-context learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[62] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[63] S. Xie, J. Qiu, A. Pasad, L. Du, Q. Qu, and H. Mei. Hidden state variability of pretrained language models can guide computation reduction for transfer learning. arXiv preprint arXiv:2210.10041, 2022.   \n[64] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "LLuSjg59an/tmp/5bbaae96a85ac371ec94d4b19347c10108a80d4fa194172f1809c241c5ee8dfc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 8: Context-masking and Layer-masking results on the English $\\leftrightarrow$ Portugese translation task. Critically, we see nearly identical trends to what we see in Figure 3 and Figure 5 on the English to French translation task, suggesting our results generalize across language pairs. ", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "LLuSjg59an/tmp/a55aeebbadd0255081dda308187bd1d1fccd12433565a24fe29c4e4b720ebd03.jpg", "table_caption": ["A.1 Prompt Format "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 1: A single continuous input sequence presented to the model for decoding a single test source sentence \u201cAfter you become comfortable with formatting..\u201d. Given the entire sequence as input, the model proceeds to generate the target sequence. ", "page_idx": 14}, {"type": "text", "text": "A.2 Additional Results on English & Portugese ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In addition to the language pairs $\\mathtt{e n}\\to\\mathtt{f r}$ and $\\mathtt{f}_{\\mathtt{T}}\\to\\mathtt{e}_{\\mathtt{n}}$ , we also run experiments on English and Portugese language pairs, both $\\mathtt{e n}\\to\\mathtt{p t}$ and $\\mathtt{p t}\\to\\mathtt{e n}$ . Due to space limitations, we plot the results of those experiments here. Overall, we see largely identical trends on both directions of English and Portugese to what we observe on English and French translation tasks, leading us to conclude that our conclusions generalize across different translation tasks. ", "page_idx": 14}, {"type": "text", "text": "A.3 Autoregressive Decoder only Transformer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The transformer consists of stacked blocks of self-attention, which itself consists of smaller units of self-attention heads that are concatenated before being fed through a fully connected layer. In autoregressive decoder-only transformers, training and inference adopts a causal mask, where current positions are only able to attend to previous timesteps, instead of being able to attend to the entire input sequence. Unlike encoder-decoder NMT models where source and target sentence have separate processing transformer blocks, decoder-only means that the same model weights are both used to \u201cencode\" the source sentence and \u201cdecode\" the target sentence in a single continuous sequence. ", "page_idx": 14}, {"type": "text", "text": "A.4 Training with Autoregressive Translation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The original language modeling objective in GPT training involves predicting the entire input token sequence which consists of both the source and target sentence (shifted by 1 position). We found this to produce slightly worse results than only minimising the negative log likelihood of predicting the target sentence to be translated, and not the entire sequence. We consider this autoregressive translation training. ", "page_idx": 14}, {"type": "text", "text": "A.5 $L_{0}$ Attention Gate Training ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Training Details For Section A.7, We train using Adam Optimizer $(\\beta_{1}\\;=\\;0.9,\\beta_{2}\\;=\\;0.999)$ with a batch size of 32, and learning rate of 0.001, early stopping patience of 10 and threshold of 0.01. We initialise attention head gates to be 1 instead of random or 0.5 as this leads to faster convergence. We experiment with two different training settings, the 0-prompts Train setting and the 5-prompts Train setting. As described in Section A.4, we train the model by predicting only the target sentence, conditioned on the context. In the 0-prompt setting, the context consists of the instructions and the source sentence to be translated. In the 5-prompt setting, the context consists of the instructions, 5 prompt examples, and the source sentence to be translated. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "In the 0-prompt setting, the conditional prefix consists of the instructions and the source sentence to be translated. In the 5-prompt setting, the conditional prefix consists of the instruction, 5 source target sentence pairs, and the source sentence to be translated. ", "page_idx": 15}, {"type": "text", "text": "Data We used the first 10,000 lines of $\\mathtt{e n\\to f r}$ from WMT06 Europarl [31] for training.8 To test the generalisability of trained attention head gates, we use a different test domain, FLORES [25] to reflect the scarcity of in-domain data. We also test an additional language direction $\\mathtt{e n\\to p t}$ in FLORES to see if training can generalise across languages. ", "page_idx": 15}, {"type": "text", "text": "Training Details We train using Adam Optimizer $(\\beta_{1}=0.9,\\beta_{2}=0.999)$ with a batch size of 32, and learning rate of 0.001. We use a large early stopping patience of 10 and threshold of 0.01, and train for up to 100 epochs. This is due to the nature of $L_{0}$ training; we do not expect performance to improve over many iterations and would like the attention gates to keep training as long as there is no large loss in performance. We initialise attention head gates to be 1 instead of random or 0.5 as this leads to much faster convergence and better performance. For the regularisation weight $\\lambda$ , we search over a hyperparameter set of $\\{0.1,0.01,0.001,0.0001\\}$ and found 0.01 performs best on the validation set. ", "page_idx": 15}, {"type": "text", "text": "A.5.1 $L_{0}$ head masking experiments. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Additional experiments on L0 head masking in the $\\exp\\to$ fr direction. ", "page_idx": 15}, {"type": "text", "text": "A.5.2 Training Attention Head Gates with $L_{0}$ regularisation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For a scalable approach to pruning, we opt to train self-attention head gates following [56] using the technique of differentiable $L_{0}$ regularization [34]. Let the attention head gates $g\\in\\mathbb{R}^{n_{h}\\times n_{\\ell}}$ be a set of trainable parameters, where $n_{h}$ is the number of attention heads per layer, and $n_{\\ell}$ is the number of layers. Let the original output of each attention head be $v_{j}$ , gated outputs $\\tilde{v}_{j}$ are obtained by elementwise multiplication of the gate value $g_{j}$ , i.e., $\\tilde{v}_{j}=g_{j}\\odot v_{j}$ . For $\\{(x,\\bar{y)}\\}^{n}$ source sentence $(x)$ and target sentence $(y)$ training pairs, a model $f$ and loss function $\\mathcal{L}$ , $L_{p}$ regularisation adds a $\\lambda$ weighted penalty associated with the complexity of the parameters. 9 The $L_{0}$ loss is non-differentiable as it involves raw counts of parameters. As a work around, $g$ can be approximated with random variables drawn from a Binary concrete distribution [37, 30].10 We refer the reader to [34] for the relevant technical exposition. Details of training are provided in Section A.5. ", "page_idx": 15}, {"type": "text", "text": "A.5.3 Generalisability of $L_{0}$ gate training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We experiment with $0\\,{\\-\\,}\\mathrm{prompt}\\,\\mathrm{s}$ and 5-prompts in training and using $\\lambda\\!=\\!0$ (no regularisation) and $\\lambda\\!=\\!0.01$ . $L_{0}$ training for the 0-prompts shows some gains for the 0-prompts test case, and with no loss on the 5-prompts test case (Table 2). Notably, this persists in $\\mathtt{e n}\\to\\mathtt{p t}$ , a different language direction from training. ", "page_idx": 15}, {"type": "text", "text": "The robustness of translation performance under multiple testing conditions (number of prompts, datasets, language directions) gives some confidence that the trained discrete attention head gates from $L_{0}$ support a general ability to translate (Table 2). In contrast, the soft attention head gates without regularisation ( $\\lambda=0$ ) appear to overfit as they perform well on some conditions but deteriorate in others. ", "page_idx": 15}, {"type": "text", "text": "We observe that $0{\\mathrm{-prompt}}$ training for $L_{0}(\\lambda=0.01)$ also outperforms 5-prompts which is slightly suprising since 5-prompts has more information in the prefix to locate the translation task. ", "page_idx": 15}, {"type": "image", "img_path": "LLuSjg59an/tmp/cac83474af2cfb3144efa19aaa6eaaf1e1cee0eab9fe59d8310689fefeb5e160.jpg", "img_caption": ["Figure 9: Visualisation of attention head masks for GPTNeo and BLOOM, learned with $L_{0}(\\lambda=0.01)$ regularisation under a 0-prompt train scheme in $\\mathtt{e n}\\to\\mathtt{f r}$ . A value of 0 (in black) indicates that the attention head is effectively masked out by the trained attention gate. Around $10\\%$ of attention heads are masked out i.e., redundant, with a majority of them occuring at the later layers for GPTNeo and distributed across layers for BLOOM. $\\mathtt{f}_{\\mathtt{T}}\\to\\mathtt{e}_{\\mathtt{n}}$ is availble in Section A.5.1 "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "LLuSjg59an/tmp/44ce1bfe6b9430c3e7560f02e073f0282b8f472cadda7676a96c521fc292455f.jpg", "table_caption": [], "table_footnote": ["Train: $\\mathtt{e n\\to f r}$ , Test: $\\mathtt{e n\\to f r}$ Train: $\\mathsf{e n\\to f r}$ , Test: $\\mathtt{e n\\to p t}$ "], "page_idx": 16}, {"type": "text", "text": "Table 2: Performance when using trained attention head gates for $L_{0}$ with regularisation $\\lambda=.01$ $\\lambda=0$ refers to training without regularisation. 0 and 5 prompts were used in the context for training. We highlight values which are greater or worse than 0.5 BLEU points from baseline. Note that as these are compression experiments, we do not expect $L_{0}$ to perform better than baseline. ", "page_idx": 16}, {"type": "text", "text": "One possibility is that the model overfit to the Europarl domain where the training prompts were drawn from. ", "page_idx": 16}, {"type": "text", "text": "A.6 Qualitative Analysis of Layer-wise Masking ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "GPTNEO Masking $\\ell_{4:8}$ results in a drop in performance for the 0-prompt setting but not the 5-prompt setting (Figure 5), which suggests that $\\ell_{4:8}$ are not related to the processing of prompt examples. We emphasise that this interpretation mostly holds at an aggregate level and is not strictly for each instance. For Test Instance ID 575, the model still generates a copy of the English source sentence up to the masking of $\\ell_{25}$ for the 0-prompts without instructions setting (Table 4). This suggests that uncertainty over the task is maintained across layers even though the contributions towards task location may be greater from specific layers. ", "page_idx": 16}, {"type": "text", "text": "BLOOM is observed to be more robust to masking of layers; suggesting that task location is more distributed. For the 5-prompt setting, the performance only decreases very slightly. For the 0-prompt setting, we observe that similar to GPTNEO, performance drops when masking out the middle layers. At the aggregate level, BLOOM appears to still be translating $\\mathit{\\Pi}^{?}>0$ BLEU) even when layers are masked. However we observe that the drop in performance is due to around 40 to $50\\%$ of the test sentences scoring $<5$ BLEU points. There is a clear failure to translate, not simply producing poorer translations. ", "page_idx": 16}, {"type": "table", "img_path": "LLuSjg59an/tmp/4e5fe12c53a2ed57a244384987bcb617808075e8756ce5e031d353d545079880.jpg", "table_caption": ["Table 3: 0-prompts with instructions, masking layer by layer of GPTNEO2.7B "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "LLuSjg59an/tmp/60d8e312d4e8d98bdfee9a1ec3c8e16bdfd7906ea2e4b9e0cbfbf6105853758f.jpg", "table_caption": ["Table 4: 0-prompts without instructions, masking layer by layer of GPTNEO2.7B "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "LLuSjg59an/tmp/05e8ae824d553176b33fa61916e2945bf5b6c09f61a77067f36362ded7fabb0b.jpg", "table_caption": ["Table 5: 5-prompts with instructions, masking layer by layer of GPTNEO2.7B "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "LLuSjg59an/tmp/3af9f72f78478f65f83bc94969c373d26fdb08df5cba1a9612148264ac4807de.jpg", "table_caption": ["Table 6: 5-prompts without instructions, masking layer by layer of GPTNEO2.7B "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.7 Studying Redundancy via Compression ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We noted that GPTNEO has some critical differences from BLOOM and LLAMA in terms of having critical layers (see Section 6.2). To what extent are there specialised attention heads for MT in the GPT-style models? If there were specialised heads, we would expect the model to be highly compressable/prunable to a select few heads. We plot a grid map of learned attention gate values for $\\mathtt{e n}\\to\\mathtt{f r}$ , where 0 indicates that the head is masked out (Figure 9). We find that most of the masked heads are distributed at the later layers for GPTNeo and are distributed across layers for BLOOM. This appears consistent with Section 6.2\u2019s observations that redundancy is more focused at certain layers in GPTNeo, and more spread out across the layers for Bloom. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We outline our contributions precisely in our abstract and introduction. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss our limitations in the conclusion section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not present theoretical results in this work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We describe hyperparameters, models, and methods clearly. Our models are downloaded from Huggingface and are all opensource. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 23}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We will provide access to all of the code used to run experiments. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We use publicly available dataset splits and our code will detail precisely what samples are used during in-context learning. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our core results present error bars over different random seeds and sets of in-context exemplars when applicable. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide sufficient information to reproduce all of our experiments (all of our experiments can be run on consume-grade GPUs such as RTX 6000). ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We adhere to the NeurIPS code of ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work contains an Impact Statement in the Appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: NA ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All of our data and models are publicly available, and we reference their sources in the main paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: NA Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: N/A Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]