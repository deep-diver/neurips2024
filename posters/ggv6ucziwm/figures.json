[{"figure_path": "GgV6UczIWM/figures/figures_1_1.jpg", "caption": "Figure 1: Transformers learn increasingly higher-order interactions from their data. Left: We illustrate the idea of a statistical \u201cclone\u201d of a data set, which approximates the underlying data distribution by keeping only interactions between tokens up to a fixed degree (in this case, three-body interactions). We introduce a principled approach to create clones by training a transformer with multiple layers of factored self-attention [17] with x\u00b2 activation function between layers. The depth of the architecture controls the degree of the approximation. Clones can then be sampled from these models. Right: Test loss of a standard BERT-like transformer encoder [18, 19] with four attention blocks trained on the WikiText-103 [20] data set and tested on clones of this data set with a truncated maximum degree of many-body interactions between tokens. We show the average over five training runs starting from the same initial condition. The shaded area indicates one standard deviation.", "description": "This figure demonstrates that transformers learn higher-order interactions sequentially.  The left panel illustrates the creation of \"clones\" of a dataset, which are simplified versions that only include interactions up to a specified order (here, three-body interactions). These clones are generated using a transformer with factored self-attention and a quadratic activation function, where the depth of the network controls the maximum interaction order. The right panel shows the training loss of a standard BERT-like transformer on WikiText-103, tested on clones with varying maximum interaction orders. The results indicate that the transformer initially learns low-order interactions, reaching a saturation point in the loss before continuing to learn higher-order interactions, highlighting the sequential nature of its learning process.", "section": "1 Introduction"}, {"figure_path": "GgV6UczIWM/figures/figures_4_1.jpg", "caption": "Figure 2: a) Multi-layer factored self-attention architecture with x\u00b2 activation function. b) Test loss learning curves of one, two and three factored self-attention layers with x\u00b2 activation function. The models were trained on a synthetic data set generated from a four-body Hamiltonian. The dashed horizontal lines correspond to the convergence value of the loss for two, three and four bodies energy based models trained on the same data set. c) Mean Square Displacement of the weights across different layers in a three-layers factored attention architecture. In these experiments, the size of the vocabulary was set to |V| = 10 and the sequence length to L = 20. We used a training set of M = 25600 samples, training the models with SGD, choosing a mini-batch size of 256. The initial learning rate is chosen to be 0.1.", "description": "This figure shows the results of experiments using a multi-layer factored self-attention architecture with a quadratic activation function (x\u00b2). Panel (a) illustrates the architecture. Panel (b) displays the test loss learning curves for models with 1, 2, and 3 layers, trained on a synthetic dataset with four-body interactions. The dashed lines represent the convergence values of the test loss for 2, 3, and 4-body interaction models. Finally, Panel (c) depicts the Mean Square Displacement (MSD) of the weights across layers in the 3-layer model, showcasing the sequential activation of layers during training.", "section": "Learning many-body interactions with factored attention"}, {"figure_path": "GgV6UczIWM/figures/figures_6_1.jpg", "caption": "Figure 3: Three steps for cloning a data set using factored-attention based generative models. a) Train factored-attention models on TinyStories. Test loss curves of different factored-attention based architectures trained on TinyStories and tested on TinyStories. Specifically, we consider architectures with two, four and six factored self-attention layers with x\u00b2 activation function. For comparison, also the test loss of a four-layers BERT is shown. b) Sample factored models. Mean score of a batch of sentences taken from the test set of the TinyStories data set and evolved with the Metropolis-Hasting sampling scheme described in appendix A.3. c) Check generated clones. Test loss curves of a standard four layers transformer encoder, trained on TinyStories and tested on clones generated after 20 and 70 Metropolis-Hasting sweeps. The clones were generated from a four layers standard BERT and from an architecture with four layers of factored self-attention and x2 activation function (associated with a nine bodies approximation of TinyStories).", "description": "This figure shows the three steps involved in cloning a dataset using factored-attention-based generative models.  Panel (a) compares the performance of different models (BERT and models with varying layers of factored attention) trained on the TinyStories dataset and evaluated on the original TinyStories dataset. Panel (b) illustrates the sampling process used to generate the clones. Finally, panel (c) presents the performance of these different models trained on the original TinyStories and evaluated on the generated clones.", "section": "3 Learning many-body interactions with factored attention"}, {"figure_path": "GgV6UczIWM/figures/figures_8_1.jpg", "caption": "Figure 4: BERT models trained on masked-language modelling learn increasingly higher-order interactions during training. Left panel: In an experiment analogous to the one shown in fig. 1, we show the test loss of a standard BERT-like transformer encoder trained trained on the TinyStories data set [22] and tested on clones of this data set with a truncated maximum degree of many-body interactions between tokens. The inset shows the corresponding test accuracy. We show the average over five different training runs, all starting from the same initial condition. The shaded area indicates one standard deviation. Right panel: An alternative way to visualise the data from the left panel is to plot the test loss at steps 104, 3 \u00d7 104, and 105 (blue, green and orange points respectively). This visualisation highlights the sequential learning of higher-order interactions, showing that for the clones derived from two- and four-layer factored architectures the loss saturates after 3 \u00d7 104 training steps, while on the clones derived from a six-layer architecture, as well as for the clone sampled from a BERT model, the test loss continues to decrease, as indicated by the black arrows.", "description": "This figure shows the results of training a BERT model on the TinyStories dataset and evaluating its performance on clones of the dataset with varying degrees of many-body interactions. The left panel shows the test loss curves over training steps for BERT and for clones generated using factored self-attention models with 2, 4, and 6 layers. The inset shows the corresponding test accuracy. The right panel provides an alternative visualization, highlighting the sequential learning of higher-order interactions by BERT.", "section": "4 Sequential learning in NLP"}, {"figure_path": "GgV6UczIWM/figures/figures_9_1.jpg", "caption": "Figure 4: BERT models trained on masked-language modelling learn increasingly higher-order interactions during training. Left panel: In an experiment analogous to the one shown in fig. 1, we show the test loss of a standard BERT-like transformer encoder trained trained on the TinyStories data set [22] and tested on clones of this data set with a truncated maximum degree of many-body interactions between tokens. The inset shows the corresponding test accuracy. We show the average over five different training runs, all starting from the same initial condition. The shaded area indicates one standard deviation. Right panel: An alternative way to visualise the data from the left panel is to plot the test loss at steps 104, 3 \u00d7 104, and 105 (blue, green and orange points respectively). This visualisation highlights the sequential learning of higher-order interactions, showing that for the clones derived from two- and four-layer factored architectures the loss saturates after 3 \u00d7 104 training steps, while on the clones derived from a six-layer architecture, as well as for the clone sampled from a BERT model, the test loss continues to decrease, as indicated by the black arrows.", "description": "This figure shows the results of an experiment where a BERT model was trained on the TinyStories dataset and tested on different clones of the dataset, where each clone has a different maximum order of interactions between tokens. The left panel shows the test loss curves for different models (BERT and factored attention with 2, 4, and 6 layers), indicating that BERT continues to improve even after other models plateau. The right panel provides an alternative visualization, emphasizing that higher-order interactions are learned sequentially.", "section": "4 Sequential learning in NLP"}]