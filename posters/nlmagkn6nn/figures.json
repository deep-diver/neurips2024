[{"figure_path": "NLmAGkN6nn/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Illustration of salient channels in activation and weight. Note that salient activation channels exhibit variations over different timesteps (e.g., t = t1, t2, t3.), posing non-trivial quantization challenges. To mitigate the overall quantization difficulty, our method leverages the complementarity (activation and weight channels do not have extreme magnitude simultaneously) to redistribute channel salience between weights and activations across various timesteps. (Right) Quantization performance on W8A8 and W4A8, employing FID (lower is better) and IS (higher is better) metrics on ImageNet 256x256 [41]. The circle size indicates the model size.", "description": "The figure demonstrates two key challenges in quantizing Diffusion Transformers: the presence of salient channels (with extreme magnitudes) in both activations and weights, and the temporal variability of salient activation distributions across multiple timesteps.  The left panel illustrates these challenges, while the right panel shows the effectiveness of the proposed PTQ4DiT method compared to other quantization techniques on ImageNet in terms of FID and IS scores for different bit-widths (W8A8 and W4A8).", "section": "1 Introduction"}, {"figure_path": "NLmAGkN6nn/figures/figures_2_1.jpg", "caption": "Figure 2: (Left) Overview of the Diffusion Transformer (DiT) Block [37]. (Middle) Illustration of the linear layer in Multi-Head Self-Attention (MHSA) and Pointwise Feedforward (PF) modules, which incorporates our proposed Channel-wise Salience Balancing (CSB) and Spearman's p-guided Salience Calibration (SSC) to address quantization difficulties for both activation X and weight W. Appendix A depicts detailed structures of the MHSA and PF modules with adjusted linear layers. (Right) Illustration of CSB and SSC in PTQ4DiT. CSB redistributes salient channels between weights and activations from various timesteps to reduce overall quantization errors. SSC calibrates the activation salience across multiple timesteps via selective aggregation, with more focus on timesteps where quantization errors can be significantly reduced by CSB.", "description": "This figure illustrates the architecture of the DiT block, focusing on the linear layer within the MHSA and PF modules.  It shows how the proposed methods, Channel-wise Salience Balancing (CSB) and Spearman's p-guided Salience Calibration (SSC), are integrated to handle the challenges of quantizing diffusion transformers. The right panel visually explains how CSB redistributes salient channels across timesteps to minimize quantization errors, and how SSC dynamically adjusts the focus on specific timesteps based on error levels.", "section": "4 PTQ4DIT"}, {"figure_path": "NLmAGkN6nn/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of maximal absolute magnitudes of activation (left) and weight (right) channels in a DiT linear layer, alongside their corresponding quantization Error (MSE). Channels with greater maximal absolute values tend to incur larger errors, presenting a fundamental quantization difficulty.", "description": "This figure shows the distribution of maximal absolute values for both activation and weight channels in a linear layer of a Diffusion Transformer (DiT). The left panel shows activation channels while the right panel shows weight channels.  The y-axis represents the maximum absolute values, and the x-axis represents the channel index.  Overlaid on the channel values are bars representing the quantization error (MSE) for each channel.  The figure highlights that channels with larger maximal absolute values (marked with stars) tend to experience significantly higher quantization errors. This observation demonstrates a key challenge in quantizing DiTs, where channels with extreme magnitudes cause substantial quantization errors.", "section": "3 Diffusion Transformer Quantization Challenges"}, {"figure_path": "NLmAGkN6nn/figures/figures_4_1.jpg", "caption": "Figure 4: Boxplot of maximal absolute magnitudes of activation channels in a linear layer within DiT over different timesteps, which exhibit significant temporal variations.", "description": "This figure shows how the maximum absolute values of activation channels in a linear layer of a Diffusion Transformer model change over different timesteps during the image generation process.  The box plots illustrate the distribution of these maximum values for each timestep.  The significant variation across timesteps highlights a key challenge in applying post-training quantization to DiTs: the distributions of salient channels (those with extreme magnitudes) are not static but change dynamically throughout the inference process.", "section": "3 Diffusion Transformer Quantization Challenges"}, {"figure_path": "NLmAGkN6nn/figures/figures_6_1.jpg", "caption": "Figure 5: Random samples generated by PTQ4DiT and two strong baselines: RepQ* [21] and Q-Diffusion [18], with W4A8 quantization on ImageNet 512x512 and 256x256. Our method can produce high-quality images with finer details. Appendix E presents more visualization results.", "description": "This figure compares the image generation quality of PTQ4DiT with two other state-of-the-art post-training quantization methods (RepQ* and Q-Diffusion) for diffusion transformers.  The images show that PTQ4DiT generates images with better details and overall quality, particularly noticeable in the W4A8 (4-bit weight, 8-bit activation) quantization setting.  The comparison is done using the ImageNet dataset at both 512x512 and 256x256 resolutions.", "section": "5 Experiments"}, {"figure_path": "NLmAGkN6nn/figures/figures_8_1.jpg", "caption": "Figure 6: Quantization performance on W8A8. The circle size represents the computational load (in Gflops).", "description": "This figure shows the quantization performance results on W8A8 for different numbers of sampling steps (250, 100, and 50) on ImageNet datasets with resolutions of 256x256 and 512x512.  The x-axis represents the SFID (Spatial Fr\u00e9chet Inception Distance), and the y-axis represents the FID (Fr\u00e9chet Inception Distance).  The circle size of each point indicates the model size and correlates to computational cost. The results demonstrate that PTQ4DiT achieves comparable performance to the full-precision (FP) model while significantly reducing computational costs, especially at higher resolutions and fewer sampling steps.", "section": "5.3 Ablation Study"}, {"figure_path": "NLmAGkN6nn/figures/figures_13_1.jpg", "caption": "Figure 7: Illustration of structures of the MHSA and PF modules within DiT Blocks [37]. Our proposed CSB and SSC are embedded in their linear layers, including Projection1, Projection2, and FC1. CSB and SSC collectively mitigate the quantization difficulties by transforming both activations and weights using Salience Balancing Matrices, BW and BX. To prevent extra computational burdens at inference time, BW is absorbed into the weight matrix of the linear layer f. Meanwhile, BX is integrated offline into the MLPs layer prior to adaLN modules for Projection1 and FC1, and into the preceding matrix multiplication operation for Projection2.", "description": "This figure illustrates how Channel-wise Salience Balancing (CSB) and Spearman's \u03c1-guided Salience Calibration (SSC) are integrated into the Multi-Head Self-Attention (MHSA) and Pointwise Feedforward (PF) modules of Diffusion Transformer (DiT) blocks to address the challenges of salient channels and temporal variability in DiT quantization.  It shows how the salience balancing matrices are incorporated into the linear layers (Projection1, Projection2, FC1) to redistribute salience between activations and weights, thereby mitigating quantization errors.  The offline integration of the matrices into the weight matrix and MLPs prevents added computational cost during inference.", "section": "A Structures of MHSA and PF with Adjusted Linear Layers"}, {"figure_path": "NLmAGkN6nn/figures/figures_16_1.jpg", "caption": "Figure 5: Random samples generated by PTQ4DiT and two strong baselines: RepQ* [21] and Q-Diffusion [18], with W4A8 quantization on ImageNet 512x512 and 256x256. Our method can produce high-quality images with finer details. Appendix E presents more visualization results.", "description": "This figure compares the image generation quality of PTQ4DiT with two other state-of-the-art post-training quantization methods (RepQ* and Q-Diffusion) and the full-precision model.  The comparison is made using W4A8 quantization (4-bit weights, 8-bit activations) on the ImageNet dataset at 256x256 and 512x512 resolution.  The figure shows example image samples from each method, highlighting that PTQ4DiT produces images with greater detail and quality compared to the baselines.  Appendix E contains additional visualization results.", "section": "5 Experiments"}, {"figure_path": "NLmAGkN6nn/figures/figures_17_1.jpg", "caption": "Figure 9: Random samples generated by different PTQ methods with W8A8 quantization, alongside the full-precision DiTs [37], on ImageNet 256x256.", "description": "This figure compares image samples generated using different post-training quantization (PTQ) methods, including RepQ*, Q-Diffusion, PTQD, and the proposed PTQ4DiT, with a weight and activation precision of 8 bits (W8A8).  It also includes samples from the full-precision DiT model for comparison.  The images were generated using the ImageNet 256x256 dataset. The purpose is to visually demonstrate the relative quality of images produced by each PTQ method compared to the original, unquantized model.  The visual comparison helps assess the impact of each quantization technique on the generative capabilities of the diffusion transformer model.", "section": "5.2 Quantization Performance"}]