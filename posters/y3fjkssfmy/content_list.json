[{"type": "text", "text": "Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junlin He   \nThe Hong Kong Polytechnic University Hong Kong SAR, China   \njunlinspeed.he@connect.polyu.hk ", "page_idx": 0}, {"type": "text", "text": "Jinxiao Du The Hong Kong Polytechnic University Hong Kong SAR, China jinxiao.du@connect.polyu.hk ", "page_idx": 0}, {"type": "text", "text": "Wei Ma\u2217 The Hong Kong Polytechnic University Hong Kong SAR, China wei.w.ma@polyu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Self-supervised learning (SSL) has rapidly advanced in recent years, approaching the performance of its supervised counterparts through the extraction of representations from unlabeled data. However, dimensional collapse, where a few large eigenvalues dominate the eigenspace, poses a significant obstacle for SSL. When dimensional collapse occurs on features (e.g. hidden features and representations), it prevents features from representing the full information of the data; when dimensional collapse occurs on weight matrices, their filters are self-related and redundant, limiting their expressive power. Existing studies have predominantly concentrated on the dimensional collapse of representations, neglecting whether this can sufficiently prevent the dimensional collapse of the weight matrices and hidden features. To this end, we first time propose a mitigation approach employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices, thus safeguarding against the dimensional collapse of weight matrices, hidden features, and representations. Our empirical investigations demonstrate that OR significantly enhances the performance of SSL methods across diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based architectures. Our code will be released at https://github.com/Umaruchain/OR_in_SSL.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Self-supervised learning (SSL) has established itself as an indispensable paradigm in machine learning, motivated by the expensive costs of human annotation and the abundant quantities of unlabeled data. SSL endeavors to produce meaningful representations without the guidance of labels. Recent developments have witnessed joint-embedding SSL methods achieving, or even exceeding the supervised counterparts (Misra & Maaten 2020, Bardes et al. 2022, Caron et al. 2020, Chen, Fan, Girshick & He 2020, Chen, Kornblith, Norouzi & Hinton 2020, Chen & He 2021, Dwibedi et al. 2021, HaoChen et al. 2021, He et al. 2020, He & Ozay 2022, Jing et al. 2021, Li, Zhou, Xiong & Hoi 2020, Jing et al. 2020, Balestriero et al. 2023, Grill et al. 2020, Zbontar et al. 2021, Chen et al. 2021). The efficacy of these methods hinges on two pivotal principles: 1) the ability to learn augmentation-invariant representations, and 2) the prevention of complete collapse, where all inputs are encoded to a constant vector. ", "page_idx": 0}, {"type": "text", "text": "Efforts to forestall complete collapse have been diverse, including contrastive methods with both positive and negative pairs (He et al. 2020, Chen, Kornblith, Norouzi & Hinton 2020, Chen et al. 2021) and non-contrastive methods utilizing techniques such as self-distillation (Caron et al. 2021, Grill et al. 2020, Chen & He 2021), clustering (Caron et al. 2018, 2020, Pang et al. 2022) and feature whitening (Bardes et al. 2022, Zbontar et al. 2021, Weng et al. 2022, 2023). Notwithstanding, these methods are prone to dimensional collapse, a phenomenon where a few large eigenvalues dominate the eigenspace. Dimensional collapse can occur on both features (e.g. hidden features and representations) and weight matrices. ", "page_idx": 1}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/a7ac5c71834b70ad2bccc5fff6311f418807e8274e73bb9caf93e14f2f1b1b94.jpg", "img_caption": ["Figure 1: Illustration of dimensional collapse in SSL. We use one augmented input $X_{a u g1}$ as an example: we assume that the encoder contains two basic blocks, each containing a linear operation (e.g., a linear layer or convolutional layer) and an activation function. Dimensional collapse can occur in weight matrices $(W_{1},W_{2})$ , hidden features, and the finally obtained representations. Existing methods act directly on representations and expect to affect hidden features and weight matrices indirectly, which has no guarantee in theory; our method directly constrains weight matrices and indirectly influences hidden features and representations, which can be guaranteed by theoretical analysis. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To prevent dimensional collapse of representations, as depicted in Figure 1, existing methods include modifying representations in downstream tasks (He & Ozay 2022), whitening representations directly (i.e. removing the projector) (Jing et al. 2021), incorporating regularizers on representations during pretraining (Huang et al. 2024, Hua et al. 2021). However, whether they sufficiently prevent the dimensional collapse of weight matrices and hidden features remains unknown (i.e., no theoretical guarantee) (Pasand et al. 2024). In Appendix A.1, we further demonstrate that whitening representations directly to eliminate the dimensional collapse of representations cannot adequately remove the dimensional collapse of weight matrices. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we first time propose a mitigation approach employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. It is natural that OR prevents the dimensional collapse of weight matrices as it ensures weight matrices orthogonality, keeps the correlation between its fliters as low as possible, and lets each fliter have a norm of 1. For features (e.g. hidden features and representations), orthogonal weight matrices can promote uniform eigenvalue distributions and thus prevent the domination of eigenspaces by a limited number of large eigenvalues, as theoretically substantiated by Huang et al. (2018), Yoshida & Miyato (2017), Rodr\u00edguez et al. (2016). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In our study, we introduce and assess the effects of two leading orthogonality regularizers, Soft Orthogonality (SO) and Spectral Restricted Isometry Property Regularization (SRIP), on SSL methods. We examine their integration with 13 modern SSL methods from Solo-learn and LightSSL, spanning both contrastive and non-contrastive methods (Chen, Fan, Girshick & He 2020, Chen et al. 2021, Grill et al. 2020, Caron et al. 2021, Dwibedi et al. 2021). Our findings indicate a consistent enhancement in linear probe accuracy on CIFAR-100 using both CNNs and Transformer-based architectures and OR exhibits a good scaling law at the model scale. Furthermore, when applied to BYOL trained on IMAGENET-1k, OR significantly improves the downstream performance on both classification and object detection tasks, suggesting its applicability to large-scale SSL settings. Remarkably, OR achieves these enhancements without necessitating modifications to existing SSL architectures or hyperparameters. ", "page_idx": 2}, {"type": "text", "text": "In summary, we present three major contributions: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We systematically study the phenomenon of dimensional collapse in SSL, including how feature whitening and network depth affect the dimensional collapse of weight matrices and hidden features.   \n\u2022 We first time introduce orthogonal regularization (OR) as a solution to prevent the dimensional collapse of weight matrices, hidden features, and representations during SSL pretraining.   \n\u2022 Our extensive experimental analysis demonstrates OR\u2019s substantial role in enhancing the performance of state-of-the-art joint-embedding SSL methods with a wide spectrum of backbones. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Self-Supervised Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Self-supervised Learning (SSL) aims to learn meaningful representations from unlabeled data. Existing SSL methods can be broadly classified into two categories: generative and joint embedding methods. This paper concentrates on joint-embedding methods, which learn representations by aligning the embeddings of different augmented views of the same instance. Joint-embedding methods further subdivide into contrastive and non-contrastive methods. Contrastive methods, such as those proposed by He et al. (2020), Chen, Kornblith, Norouzi & Hinton (2020), Chen et al. (2021), treat each sample as a distinct class and leverage the InfoNCE loss (Oord et al. 2018) to bring representations of positive pairs closer together while distancing those of negative pairs in the feature space. These methods generally require a substantial number of negative samples for effective learning. In contrast, non-contrastive methods eschew the use of negative samples. They instead employ various techniques such as self-distillation (Caron et al. 2021, Grill et al. 2020, Chen & He 2021), clustering (Caron et al. 2018, 2020, Pang et al. 2022) and feature whitening (Bardes et al. 2022, Zbontar et al. 2021, Weng et al. 2022, 2023). Our empirical findings indicate that incorporating OR enhances the performance of both contrastive and non-contrastive SSL methods. The exploration of its effects on generative methods remains for future work. ", "page_idx": 2}, {"type": "text", "text": "2.2 Dimensional Collapse in SSL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Dimensional collapse plagues both generative and joint embedding SSL methods (Zhang et al. 2022, Jing et al. 2021, Zhang et al. 2021, Tian et al. 2021). To prevent the dimensional collapse of representations, existing work has typically focused on imposing constraints on the covariance matrix of the representations, including modifying representations in downstream tasks (He & Ozay 2022), removing the projector (Jing et al. 2021), incorporating regularizers on representations during pretraining (Huang et al. 2024, Hua et al. 2021). However, these strategies face challenges such as performance degradation upon removing the projector, not addressing collapse during pre-training, and failing to prevent dimensional collapse in hidden features and weight matrices within the encoder (referred to Appendix A.1). This motivates us to regularize the weight matrices of the DNNs directly in SSL. ", "page_idx": 2}, {"type": "text", "text": "2.3 Orthogonality Regularization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Orthonormality regularization, which is applied in linear transformations, can improve the generalization and training stability of DNNs (Xie et al. 2017, Huang et al. 2018, Saxe et al. 2013). OR has demonstrated its effects on tasks including supervised/semi-supervised image classification, image retrieval, unsupervised inpainting, image generation, and adversarial training (Bansal et al. 2018, Balestriero et al. 2018, Balestriero & Baraniuk 2020, Xie et al. 2017, Huang et al. 2018). Efforts to utilize orthogonality in network training have included penalizing the deviation of the gram matrix of each weight matrice from the identity matrix (Xie et al. 2017, Bansal et al. 2018, Balestriero et al. 2018, Kim & Yun 2022) and employing orthogonal initialization (Xie et al. 2017, Saxe et al. 2013). For more stringent norm preservation, some studies transform the convolutional layer into a doubly block-Toeplitz (DBT) matrix and enforce orthogonality (Qi et al. 2020, Wang et al. 2020). ", "page_idx": 3}, {"type": "text", "text": "In this work, we first time investigate the efficacy of two orthogonality regularizers, Soft Orthogonality (SO) and Spectral Restricted Isometry Property (SRIP) in SSL (Bansal et al. 2018). These regularizers aim to minimize the distance between the gram matrix of each weight matrix and the identity matrix\u2014measured in Frobenius and spectral norms, respectively. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Settings of Self-supervised Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the general settings for joint-embedding SSL methods. We consider a large unlabelled dataset X \u2208RN\u00d7D, comprising $N$ samples each of dimensionality $D$ . The objective of SSL methods is to construct an effective encoder $f$ that transforms raw data into meaningful representations $Z=f(X)$ , where $Z\\in\\mathbb{R}^{N\\times M}$ and $M$ denotes the representation dimensionality. The learning process of SSL methods is visually represented in Figure 2, where data augmentations transform $X$ into two augmented views $X_{\\mathrm{augl}},X_{\\mathrm{aug2}}^{^{\\bullet}}\\,\\in\\,\\mathbb{R}^{D\\times N}$ . A typical joint-embedding SSL architecture encompasses an encoder $f$ and a projector $p$ . These components yield encoder features $Z_{\\mathrm{augl}}~=~f(X_{\\mathrm{augl}})$ and $Z_{\\mathrm{aug}2}~=~f(X_{\\mathrm{aug}2})$ , as well as projection features $H_{\\mathrm{augl}}\\ =\\ p(Z_{\\mathrm{augl}})\\mathrm{and}$ $\\bar{H_{\\mathrm{aug2}}}=p(Z_{\\mathrm{aug2}})$ . During training, the parameters of $f$ and $p$ are optimized via backpropagation to minimize the discrepancy between $H_{\\mathrm{augl}}$ and $H_{\\mathrm{aug2}}$ . To prevent the encoder $f$ from producing a constant feature vector, contrastive methods utilize negative samples, and non-contrastive methods employ strategies such as the self-distillation technique. ", "page_idx": 3}, {"type": "text", "text": "The efficacy of the encoder $f$ is usually assessed by the performance of the $C$ -class classification as downstream tasks. Specifically, given a labeled dataset containing samples $X_{s}\\,\\in\\,\\mathbb{R}^{S\\times D}$ and their corresponding labels $Y_{s}\\,\\in\\,\\dot{\\mathbb{R}}^{\\bar{S}\\times C}$ , where $S$ is the sample number. Then, a linear layer $g$ parameterized by $\\bar{W_{c}}\\in\\mathbb{R}^{C\\times M}$ is appended on top of the learned representations $Z_{s}=f(X_{s})$ , and thus the classification task can be fulfliled by minimizing the cross-entropy between softmax $(g(Z_{s}))$ and $Y_{s}$ . There are two strategies for the fine-tuning: 1) non-linear fine-tuning, which trains both $g$ and $f$ in the downstream tasks, and 2) linear evaluation, which freezes $f$ and only trains $g$ (referred to as the linear probe). ", "page_idx": 3}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/caa2466304f7cf0837aa41861d32e94e2b4290e955a17b004bf7030992e15c03.jpg", "img_caption": ["Figure 2: Illustration of joint-embedding SSL methods. This is a general structure. Different augmented inputs can be passed either by shared weight Encoder and Projector or by independent Encoder and Projector, depending on different SSL methods. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Orthogonality Regularizers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We introduce two orthogonality regularizers: Soft Orthogonality (SO) and Spectral Restricted Isometry Property Regularization (SRIP), which are seamlessly integrable with linear and convolutional layers. ", "page_idx": 4}, {"type": "text", "text": "Consider a weight matrix $W\\,\\in\\,\\mathbb{R}^{i n p u t\\times o u t p u t}$ in a linear layer, where input and output denote the number of input and output features, respectively. In line with Bansal et al. (2018), Xie et al. (2017), Huang et al. (2018), we reshape the convolutional filter to a two-dimensional weight matrix $W\\ \\in\\ \\mathbb{R}^{i n p u\\bar{t}\\times o u t p u t}$ , while we still use the same notation $W$ for consistency. To be specific, $i n p u t=S\\times H\\times C_{i n}$ and outpu $\\mathbf{\\chi}_{\\prime}^{\\prime}=C_{o u t}$ , with $C_{i n}$ and $C_{o u t}$ being the number of input and output channels, and $S$ and $H$ representing the width and height of the filter, respectively. ", "page_idx": 4}, {"type": "text", "text": "The SO regularizer encourages the weight matrix $W$ to approximate orthogonality by minimizing the distance between its Gram matrix and the identity matrix. This is quantified by the Frobenius norm as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{SO}(W)=\\left\\{\\begin{array}{l l}{\\left\\|W^{T}W-I\\right\\|_{F}^{2},\\quad\\mathrm{if}\\ i n p u t>o u t p u t,}\\\\ {\\quad}\\\\ {\\left\\|W W^{T}-I\\right\\|_{F}^{2},\\quad\\mathrm{otherwise},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $I$ is the identity matrix of appropriate size. ", "page_idx": 4}, {"type": "text", "text": "The SRIP regularizer employs the spectral norm to measure the deviation from orthogonality, which is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{SRIP}(W)=\\left\\{\\begin{array}{l l}{\\sigma(W^{T}W-I),}&{\\mathrm{if~}i n p u t>o u t p u t,}\\\\ {}&{}\\\\ {\\sigma(W W^{T}-I),}&{\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma(\\cdot)$ denotes the spectral norm operator. Due to the high computational cost posed by the spectral norm, the power iteration method (Yoshida $\\&$ Miyato 2017, Bansal et al. 2018) with two iterations is used for the estimation. The process for estimating $\\sigma(W^{T}W-I)$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{u}=(\\boldsymbol{W}^{T}\\boldsymbol{W}-\\boldsymbol{I})\\boldsymbol{v},\\quad\\boldsymbol{v}=(\\boldsymbol{W}^{T}\\boldsymbol{W}-\\boldsymbol{I})\\boldsymbol{u},\\quad\\sigma(\\boldsymbol{W}^{T}\\boldsymbol{W}-\\boldsymbol{I})=\\frac{\\|\\boldsymbol{v}\\|_{2}}{\\|\\boldsymbol{u}\\|_{2}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{v}\\in\\mathbb{R}^{i n p u t}$ is a vector initialized randomly from a normal distribution. ", "page_idx": 4}, {"type": "text", "text": "4 Incorporating OR into SSL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section details the integration of OR with SSL methods. To be specific, we employ OR across the encoder, targeting both convolutional and linear layers during pretraining. We represent the SSL method\u2019s loss function as $L o s s_{S S L}$ . Our overall optimization objective is the minimization of the combined loss equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL o s s=L o s s_{S S L}+\\gamma\\cdot L o s s_{O R},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $L o s s_{O R}$ is defined as $\\textstyle\\sum_{W\\in f}S O(W)$ or $\\textstyle\\sum_{W\\in f}S R I P(W)$ , depending on the selected orthogonality regularizers. The term $\\gamma$ serves as a hyperparameter that balances the SSL objective and OR loss. Notably, we only perform OR on the weight matrices located within the linear and convolutional layers of the encoder $f$ . OR provides a versatile regularization strategy for the encoder $f$ , facilitating its application across various SSL methods without necessitating modifications to the network designs or existing training protocols. ", "page_idx": 4}, {"type": "text", "text": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we show that dimensional collapse happens not only to the representations (i.e. output of the encoder), but also to weight matrices and hidden features of the encoder. We also compare the feature whitening technique used by one previous method, VICREG (Bardes et al. 2022), with OR. Migrating to BYOL, we find that the feature whitening technique only solves the dimensional collapse at the feature level, but instead accelerates the collapse of the weight matrices, and it even leads to lower performance of the downstream tasks as shown in Table 1. In contrast, OR can eliminate the dimensional collapse of weight matrices and thus the dimensional collapse of hidden features and representations. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In Appendix A.1, we further reveal that the original VICREG has a dimensional collapse problem with its weight matrices, which could not be solved by removing its projector. Adding OR to VICREG eliminates this problem and boosts the performance. ", "page_idx": 5}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/fd0380c5cf96417e78ec31b98bccfa5f1bfd4269351f7fbca09920ac1a815450.jpg", "table_caption": ["Table 1: Comparison of the feature whitening technique from VICREG and SO on CIFAR-10. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "To study the eigenspace of a matrix, we utilize the normalized eigenvalues defined in He & Ozay (2022): ", "page_idx": 5}, {"type": "text", "text": "Definition 1 (Normalized eigenvalues) Given a specific matrix $T\\in\\mathbb{R}^{N\\times D}$ , where $N$ is the sample number and $D$ is the feature dimension, we first obtain its covariance matrix $\\Sigma_{T}\\,\\in\\,\\mathbb{R}^{D\\times D}$ . Then we perform an eigenvalue decomposition on $\\Sigma_{T}$ to obtain its eigenvalues $\\{\\lambda_{i}^{T}\\}_{i=1}^{D}\\ =$ $\\left\\{\\lambda_{1}^{T},\\cdot\\cdot\\cdot,\\lambda_{i}^{T},\\cdot\\cdot\\cdot,\\lambda_{D}^{T}\\right\\}$ in descending order. And we obtain the normalized eigenvalues by $d i$ - viding all eigenvalues by the max eigenvalue $\\lambda_{1}^{T}$ , denoted as $\\{\\lambda_{1}^{T}/\\lambda_{1}^{T},\\cdot\\cdot\\cdot\\,,\\lambda_{i}^{T}/\\lambda_{1}^{\\bar{T}},\\cdot\\cdot\\cdot\\,,\\lambda_{D}^{T}/\\lambda_{1}^{T}\\}$ . To simplify the denotation, we reuse $\\{\\lambda_{i}^{T}\\}_{i=1}^{D}$ to denote normalized eigenvalues of $T$ . ", "page_idx": 5}, {"type": "text", "text": "These normalized eigenvalues are less than or equal to 1, where a larger value in one dimension indicates more information contained, and vice versa. We argue that if normalized eigenvalues drop very quickly, this means that only a few dimensions in the eigenspace contain meaningful information and also means that dimensional collapse has occurred. ", "page_idx": 5}, {"type": "text", "text": "We train three BYOL (Grill et al. 2020) models, without OR, with the feature whitening technique (e.g. Variance and Covariance regularization) from VICREG and with OR, respectively. We choose randomly initialized ResNet18 as the backbone (i.e. encoder) and train the three models on CIFAR-10 for 1,000 epochs, following the same recipe of Da Costa et al. (2022). Importantly, SO is selected as the orthogonality regularize, and $\\gamma$ is set to $1e-6$ . For the feature whitening technique, we impose the Variance and Covariance regularization from VICREG on the output of the predictor in BYOL as two additional loss terms, the former to ensure the informativeness of individual dimensions and the latter to reduce the correlation between dimensions. Following the solo-learn settings, we set the two loss term hyperparameters to $\\gamma_{v i c}$ and $\\gamma_{v i c}*0.004$ , and then tune the $\\gamma_{v i c}$ from $1e-3$ to $1e-5$ . ", "page_idx": 5}, {"type": "text", "text": "After training, we calculate the normalized eigenvalues of both weight matrices and features (e.g. input features, hidden features, representations). ResNet18 contains four basic blocks, each containing four convolutional layers, and we visualize the normalized eigenvalues of the last convolutional layer in each block. Hidden features are the outputs of four basic blocks in ResNet18. We use the first batch in the test set as input features with batchsize 4,096 and feature dimension 3072 $(32^{*}32^{*}3)$ . Results are analyzed in the following sections. ", "page_idx": 5}, {"type": "text", "text": "5.1 Dimensional Collapse of Weight Matrices ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first examine the weight matrices of the encoder. Similar to 5.1, all the weight matrices are viewed as two-dimensional matrices, and on top of them, we can calculate their normalized eigenvalues. For a specific weight matrix $W\\in\\mathbb{R}^{i n p u t\\times o u t p u t}$ in a neural network layer, we denote $\\{\\lambda_{i}^{W}\\}_{i=i}^{o u t p u t}$ as the normalized eigenvalues of this layer. ", "page_idx": 5}, {"type": "text", "text": "As shown in Figure 3, X and $\\mathrm{\\bfY}$ axis is the $i$ index and $i$ -th values of $\\{\\lambda_{i}^{W}\\}_{i=i}^{o u t p u t}$ , respectively. It is clear that with OR, the eigenvalues of the convolutional layers of different depths decay more slowly, which means that their fliters are less redundant and more diverse. In particular, we note that the deepest convolutional layer (i.e. layer4_512) has a much faster decay rate of the eigenvalues compared to the other convolutional layers in the absence of OR. Notably, the feature whitening technique does not alleviate this phenomenon. OR could significantly improve this. OR requires the weight matrix to be as orthogonal as possible, which means that the diagonal elements of its covariance matrix are as identical as possible, and the off-diagonal elements will be close to 0. Then ", "page_idx": 5}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/0385ab59173b241f78135f0281e535af6a34506abcafa1c427d7289c01dd333e.jpg", "img_caption": ["(a) Eigenvalues of weights (without OR) "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/695add76354936380e0f30ed4a3c887ae4a1a8cb0af2844bdf0f3b0c48e15916.jpg", "img_caption": ["(b) Eigenvalues of features (without OR) "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/643fc39bbf1698b604622a1526c1dfb150c382b29de1cd03b89faf52e5c85d6b.jpg", "img_caption": ["(c) Eigenvalues of weights (with feature whitening) "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/f2ca69c3af57805c3d42449067961c9508d6d5db646db4d6f883bc43be2ad234.jpg", "img_caption": ["(d) Eigenvalues of features (with feature whitening) "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/2141954a734b3e48840fd8137248f0b6e84e002846bcd83c57f774d1752ec9f1.jpg", "img_caption": ["(e) Eigenvalues of weights (with OR) "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/2c3897de295d7764203c0b4f96c0c530d559d311fd4ae77b8ebc0a1b37e1f6d1.jpg", "img_caption": ["(f) Eigenvalues of features (with OR) "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The $\\mathbf{X}$ -axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline. ", "page_idx": 6}, {"type": "text", "text": "tahlle  celiogseen tvoa l1u e( vdeercifoiemdp ions itAiponp eonf dsiux cAh .a2 )c.ovariance matrix will have elements in $\\{\\lambda_{i}^{W}\\}_{i=i}^{o u t p u t}$ that are ", "page_idx": 6}, {"type": "text", "text": "5.2 Dimensional Collapse of Features ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As the weight matrices become orthogonal, the distribution of their outputs stabilizes, thereby preventing the dimensional collapse of hidden features and representations. This property has been demonstrated in vector form by Huang et al. (2018) and is now presented in matrix form: ", "page_idx": 6}, {"type": "text", "text": "Proposition 1 For a specific weight matrix $W\\in\\mathbb{R}^{i n p u t\\times o u t p u t}$ and $X\\in\\mathbb{R}^{N\\times i n p u t}$ , comprising $N$ samples each of dimensionality input. We denote $\\bar{X}$ and $\\bar{S}$ as the sample means of $X$ and $S$ , respectively. Let $S=X W$ , where $W^{T}W=I$ . The covariance matrix of $X$ is $\\begin{array}{r}{\\Sigma_{X}=\\frac{(\\stackrel{\\cdot}{X}-\\bar{X})^{T}\\cdot(X-\\bar{X})}{N-1}}\\end{array}$ . $(I)$ If $\\bar{X}\\,=\\,0$ and $\\Sigma_{X}\\,=\\,\\sigma^{2}I_{\\mathrm{,}}$ , then $\\bar{S}\\,=\\,0$ and $\\Sigma_{S}\\ =\\ \\sigma^{2}I$ . (2) If input = output, we have $\\left\\|S\\right\\|_{F}^{2}=\\left\\|X\\right\\|_{F}^{2}$ . (3) Given the back-propagated gradient $\\frac{\\partial L}{\\partial S}$ , we have $\\begin{array}{r}{\\left\\|\\frac{\\partial\\bar{L}}{\\partial S}\\right\\|_{F}^{2}=\\left\\|\\frac{\\partial\\bar{L}}{\\partial X}\\right\\|_{F}^{2}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "The first point of Proposition 1 illustrates that in each layer of DNNs, the orthogonal weight matrix preserves the normalization and de-correlation of the output $S$ , assuming the input is whitened. This reveals that as the network gets deeper, the hidden features of each layer and the final representations do not tend to collapse. Moreover, orthogonal filters maintain the norm of both the output and the back-propagated gradient information in DNNs, as demonstrated by the second and third points of Proposition 1. ", "page_idx": 7}, {"type": "text", "text": "To verify that the dimensional collapse of features can be eliminated by OR, we visualize the normalized eigenvalues of features (input features, hidden features, and representations) as shown in Figure 3. Without the OR constraint, features located in deeper layers (i.e. representations) will have the fastest eigenvalues decay rate, and the distributions of eigenvalues of hidden features vary considerably at different depths. After adding the OR, it can be seen that the decay rates of hidden features at layers 3 and 4 are almost the same, while the eigenvalues of representations decay much more slowly. We also visualize the representations in Appendix A.3, which also verifies that the dimensional collapse of the representations is mitigated. Interestingly, the effect of OR on the feature level is similar to that of the feature whitening technique, however, the latter is unable to eliminate the dimensional collapse in the weight matrices. ", "page_idx": 7}, {"type": "text", "text": "6 Numerical Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We study the effects of OR on SSL methods through extensive experiments. we first demonstrate that OR improves the classification accuracy on CIFAR-10, CIFAR-100, and IMAGENET100, and the improvement is consistent across different backbones and SSL methods. On the large-scale dataset IMAGENET-1k (Deng et al. 2009), OR boosts the classification accuracy on both in-distribution and out-distribution datasets (i.e. transfer learning datasets), demonstrating consistent improvement. Moreover, OR also enhances the performance in downstream tasks(e.g. object detection). ", "page_idx": 7}, {"type": "text", "text": "Baseline methods and datasets. We evaluated the effect of adding OR to 13 modern SSL methods, including 6 methods implemented by solo-learn (MOCOv2plus, MOCOv3, DINO, NNBYOL, BYOL, VICREG) (Chen & He 2021, Chen et al. 2021, Grill et al. 2020, Dwibedi et al. 2021, Caron et al. 2021) and 10 methods implemented by LightlySSL (BarlowTwins, BYOL, DCL, DCLW, DINO, Moco, NNCLR, SimCLR, SimSiam, SwaV) (Zbontar et al. 2021, Yeh et al. 2022, Caron et al. 2020, Chen & He 2021). We pretrain SSL methods on CIFAR-10, CIFAR-100, IMAGENET-100 and IMAGENET-1k and evaluate transfer learning scenarios on datasets including CIFAR-100, CIFA-10 (Krizhevsky et al. 2009), Food-101 (Bossard et al. 2014), Flowers-102 (Xia et al. 2017), DTD (Sharan et al. 2014), GTSRB (Haloi 2015). We evaluate the objection detection task on PASCAL VOC2007 and VOC2012 (Everingham et al. 2010). Detailed descriptions of datasets and baseline SSL methods are shown in Appendix A.4 and A.5, respectively. ", "page_idx": 7}, {"type": "text", "text": "Training and evaluation settings. For each SSL method, we use the original settings in sololearn (Da Costa et al. 2022) and LightlySSL. These settings include the network structure, loss function, training policy (training epochs, optimizers, and learning rate schedulers) and data augmentation policy. The splits of the training and test set follow torchvision Marcel & Rodriguez (2010). For all the classification tasks, we report the linear probe or KNN accuracy; for the objection detection task, we perform non-linear fine-tuning. Details of training, parameter tuning, and evaluation are presented in Appendix A.6. It is worth noting that the Solo-learn and LightlySSL setups are not the same as the official implementation of the SSL methods, e.g., there is no use of multi-crop augmentation in DINO, and there is no exceptionally long training epoch. We leave experiments on migrating OR to the official implementation for future work. ", "page_idx": 7}, {"type": "text", "text": "Recipe of adding OR. For OR, $\\gamma$ of SRIP is tuned from $\\left\\{1e-3,1e-4,1e-5\\right\\}$ and $\\gamma$ of SO is tuned from $\\{1e-5,1e-6,1e-7\\}$ on a validation set. When you want to add OR to your SSL pre-training, you simply pass the encoder into the loss function, and then you just need to set $\\gamma$ of the OR according to the backbone and regularizer you use as shown in Table 9 of Appendix A.6. ", "page_idx": 7}, {"type": "text", "text": "6.1 OR is Suitable for Different Backbones and SSL Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "After pretraining on CIFRA-100, for each SSL method, we report the corresponding classification accuracy as shown in Table 2. Both two orthogonality regularizers consistently improve the linear classification accuracy. Note that OR boosts the performance of both constrastive (MoCov2plus, ", "page_idx": 7}, {"type": "text", "text": "Mocov3) and non-contrastive methods(BYOL, NNBYOL, DINO) as they are all susceptible to dimensional collapse (Zhang et al. 2022, Jing et al. 2021, Zhang et al. 2021, Tian et al. 2021). Non-contrastive methods gain more improvements in contrast to contrastive methods. When we use ResNet18, MOCOv3 improves $3\\%$ on Top-1 accuracy while DINO and NNBYOL improve $6\\%$ and $5\\%$ , respectively. OR can also boost the performance of the Sota method like BYOL by $1\\%$ . When we scale to ResNet 50 and WideResnet28w2, OR consistently boosts their performance. Moreover, the additional time overhead of adding OR to SSL is low compared to the original training time (referred to A.7). ", "page_idx": 8}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/66be8f38f67d6bf1fe8314dd8a3fd013d00a16aa5b680e49259d47e904e22bf1.jpg", "table_caption": ["Table 2: Classification accuracy on CIFAR-100 (CNN backbones). SSL methods (in Solo-learn) are trained with or without OR on CIFAR-100. The best results are in bold, the second best in italics. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In addition to CNN backbones, OR is also able to improve SSL performance on Transformer-based backbones (e.g., VIT) (Han et al. 2022, Dosovitskiy et al. 2020, Zhou et al. 2021). We pretrain DINO on CIFAR-100 with different depths of VITs. As shown in Table 3, with the increasing depth of the VIT, the original DINO performance is increasing, and OR is able to increase their performance even further, which exhibits a good scaling law. Interestingly, under the Transformer-based architecture, OR is able to improve performance more (up to $12\\%$ ) compared to CNN backbones. This is consistent with some existing studies (RoyChowdhury et al. 2017) that linear layers are more likely to have redundant filters than convolutional layers in DNNs, i.e., more prone to dimensional collapse. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Classification accuracy on CIFAR-100 (VITs). DINO (in Solo-learn) is trained with or without OR on CIFAR-100. ", "page_idx": 8}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/6167b10b0412b1f61801f3050777c8777c17c6a6c9c4f2fe42fdd26a8c1e1d8b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "To evaluate OR on more SSL methods, under the LightSSL framework, we test SO on CIFAR-10, IMAGENET-100, and IMAGENET-1k. OR still consistently improves the performance of various SSL methods. ", "page_idx": 8}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/6f01786f557c493d8da7df5f0f98eb0c69e472dfa73482a3856446444136487e.jpg", "table_caption": ["Table 4: Performance of SSL methods on LightlySSL. ResNet18 is used on CIFRA-10, CIFAR-100 and IMAGENET-100, and ResNet50 is employed on IMAGENET-1K. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.2 OR Works on Large-scale Dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We demonstrate the effects of OR on the large-scale dataset IMAGENET-1k. Specifically, we pre-train three BYOL models- BYOL without OR, BYOL with SO, and BYOL with SRIP on IMAGENET-1k (with ResNet50). For each testing classification dataset, we report the accuracy as shown in Table 5 and 6. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Table 5: Classification and objection detection performance. BYOL is trained with or without OR on IMAGENET-1k (ResNet50 with batchsize 128, Epoch 100). The best results are in bold, the second best in italics. ", "page_idx": 9}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/c5aa6bdf43a59352031db6ee9943662b19d10509ebdd4bc7291c23d5849ac650.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/3ee938ffc484b71aa941186bd3f9727093d972d23f72ac985e6ba62129a26458.jpg", "table_caption": ["Table 6: Classification accuracy on transfer learning datasets. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "We can observe that OR not only improves the accuracy on IMAGENET-1k but also on all the transfer learning datasets. OR improves TOP-1 accuracy by $3\\%$ in IMAGENET-1k and by $3\\%$ to $9\\%$ in each transfer learning datasets. The transfer learning task evaluates the generality of the encoder as it has to encode samples from various out-of-distribution domains with categories that it may not have seen during pretraining. OR also significantly improves SSL\u2019s performance in the objection detection task by $20\\%$ on AP. The above results are close to Chen et al. (2021), Da Costa et al. (2022), Weng et al. (2023) where also training only 100 epochs. ", "page_idx": 9}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/f506580fb75ef2300ac9161af5981d865a782ed1060af7c0a0ea9f166a47b28f.jpg", "table_caption": ["Table 7: Classification accuracy on IMAGENET-1k (pretrained with different epochs and batchsizes). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Considering that training epoch and batchsize during pretraining significantly impact the performance Chen et al. (2021), Huang et al. (2024), Lavoie et al. (2022), we further increase the training epoch(200) and batchsize(256) and scale up the learning rate accordingly. As shown in Table 7, OR consistently improves the performance. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The existing studies focus on the dimensional collapse of representations and overlook whether weight matrices and hidden features also undergo dimensional collapse. We first time propose a mitigation approach to employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices, thus safeguarding against the dimensional collapse of weights, hidden features, and representations. Our empirical investigations demonstrate that OR significantly enhances SSL method performance across diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based architectures as the backbones. Importantly, the time complexity and required efforts on fine-tuning are low and the performance improvement is significant, enabling it to become a useful plug-in in various SSL methods. ", "page_idx": 9}, {"type": "text", "text": "In terms of future research, we wish to examine the effect of OR on other pre-training foundation models, such as vision generative SSL models such as MAE (He et al. 2022), auto-regression models like GPTs and LLaMAs (Radford et al. 2018, 2019, Brown et al. 2020, Touvron et al. 2023), and Contrastive Language-Image Pre-training models (Radford et al. 2021, Li et al. 2022). We believe OR is a pluggable and useful module to boost the performance of vision and language foundation models. In fact, this paper is the first to test the effectiveness of OR in a Transformer-based architecture and it is reasonable to believe that it will perform well in these domains. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work described in this paper was supported by the National Natural Science Foundation of China (No. 52102385), grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU/25209221 and PolyU/15206322), and grants from the Otto Poon Charitable Foundation Smart Cities Research Institute (SCRI) at the Hong Kong Polytechnic University (Project No. P0043552). The contents of this article reflect the views of the authors, who are responsible for the facts and accuracy of the information presented herein. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Balestriero, R. & Baraniuk, R. G. (2020), \u2018Mad max: Affine spline insights into deep learning\u2019, Proceedings of the IEEE 109(5), 704\u2013727. ", "page_idx": 11}, {"type": "text", "text": "Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T., Bordes, F., Bardes, A., Mialon, G., Tian, Y. et al. (2023), \u2018A cookbook of self-supervised learning\u2019, arXiv preprint arXiv:2304.12210 .   \nBalestriero, R. et al. (2018), A spline theory of deep learning, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 374\u2013383.   \nBansal, N., Chen, X. & Wang, Z. (2018), \u2018Can we gain more from orthogonality regularizations in training deep networks?\u2019, Advances in Neural Information Processing Systems 31.   \nBardes, A., Ponce, J. & LeCun, Y. (2022), \u2018Variance-invariance-covariance regularization for selfsupervised learning\u2019, ICLR, Vicreg 1, 2.   \nBossard, L., Guillaumin, M. & Van Gool, L. (2014), Food-101\u2013mining discriminative components with random forests, in \u2018Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI $13^{\\circ}$ , Springer, pp. 446\u2013461.   \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. et al. (2020), \u2018Language models are few-shot learners\u2019, Advances in neural information processing systems 33, 1877\u20131901.   \nCaron, M., Bojanowski, P., Joulin, A. & Douze, M. (2018), Deep clustering for unsupervised learning of visual features, in \u2018Proceedings of the European conference on computer vision (ECCV)\u2019, pp. 132\u2013149.   \nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P. & Joulin, A. (2020), \u2018Unsupervised learning of visual features by contrasting cluster assignments\u2019, Advances in neural information processing systems 33, 9912\u20139924.   \nCaron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P. & Joulin, A. (2021), Emerging properties in self-supervised vision transformers, in \u2018Proceedings of the IEEE/CVF international conference on computer vision\u2019, pp. 9650\u20139660.   \nChen, T., Kornblith, S., Norouzi, M. & Hinton, G. (2020), A simple framework for contrastive learning of visual representations, in \u2018International conference on machine learning\u2019, PMLR, pp. 1597\u20131607.   \nChen, X., Fan, H., Girshick, R. & He, K. (2020), \u2018Improved baselines with momentum contrastive learning\u2019, arXiv preprint arXiv:2003.04297 .   \nChen, X. & He, K. (2021), Exploring simple siamese representation learning, in \u2018Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\u2019, pp. 15750\u201315758.   \nChen, X., Xie, S. & He, K. (2021), An empirical study of training self-supervised vision transformers, in \u2018Proceedings of the IEEE/CVF international conference on computer vision\u2019, pp. 9640\u20139649.   \nDa Costa, V. G. T., Fini, E., Nabi, M., Sebe, N. & Ricci, E. (2022), \u2018solo-learn: A library of selfsupervised methods for visual representation learning\u2019, Journal of Machine Learning Research 23(56), 1\u20136.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K. & Fei-Fei, L. (2009), Imagenet: A large-scale hierarchical image database, in \u20182009 IEEE conference on computer vision and pattern recognition\u2019, Ieee, pp. 248\u2013255.   \nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. et al. (2020), \u2018An image is worth 16x16 words: Transformers for image recognition at scale\u2019, arXiv preprint arXiv:2010.11929 .   \nDwibedi, D., Aytar, Y., Tompson, J., Sermanet, P. & Zisserman, A. (2021), With a little help from my friends: Nearest-neighbor contrastive learning of visual representations, in \u2018Proceedings of the IEEE/CVF International Conference on Computer Vision\u2019, pp. 9588\u20139597.   \nEveringham, M., Van Gool, L., Williams, C. K., Winn, J. & Zisserman, A. (2010), \u2018The pascal visual object classes (voc) challenge\u2019, International journal of computer vision 88, 303\u2013338.   \nGirshick, R., Donahue, J., Darrell, T. & Malik, J. (2014), Rich feature hierarchies for accurate object detection and semantic segmentation, in \u2018Proceedings of the IEEE conference on computer vision and pattern recognition\u2019, pp. 580\u2013587.   \nGrill, J.-B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M. et al. (2020), \u2018Bootstrap your own latent-a new approach to self-supervised learning\u2019, Advances in neural information processing systems 33, 21271\u201321284.   \nHaloi, M. (2015), \u2018Traffic sign classification using deep inception based convolutional networks\u2019, arXiv preprint arXiv:1511.02992 .   \nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C., Xu, Y. et al. (2022), \u2018A survey on vision transformer\u2019, IEEE transactions on pattern analysis and machine intelligence 45(1), 87\u2013110.   \nHaoChen, J. Z., Wei, C., Gaidon, A. & Ma, T. (2021), \u2018Provable guarantees for self-supervised deep learning with spectral contrastive loss\u2019, Advances in Neural Information Processing Systems 34, 5000\u20135011.   \nHe, B. & Ozay, M. (2022), Exploring the gap between collapsed & whitened features in selfsupervised learning, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 8613\u20138634.   \nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P. & Girshick, R. (2022), Masked autoencoders are scalable vision learners, in \u2018Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\u2019, pp. 16000\u201316009.   \nHe, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. (2020), Momentum contrast for unsupervised visual representation learning, in \u2018Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\u2019, pp. 9729\u20139738.   \nHua, T., Wang, W., Xue, Z., Ren, S., Wang, Y. & Zhao, H. (2021), On feature decorrelation in self-supervised learning, in \u2018Proceedings of the IEEE/CVF International Conference on Computer Vision\u2019, pp. 9598\u20139608.   \nHuang, H., Campello, R. J., Erfani, S. M., Ma, X., Houle, M. E. & Bailey, J. (2024), \u2018Ldreg: Local dimensionality regularized self-supervised learning\u2019, arXiv preprint arXiv:2401.10474 .   \nHuang, L., Liu, X., Lang, B., Yu, A., Wang, Y. & Li, B. (2018), Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks, in \u2018Proceedings of the AAAI Conference on Artificial Intelligence\u2019, Vol. 32.   \nJing, L., Vincent, P., LeCun, Y. & Tian, Y. (2021), \u2018Understanding dimensional collapse in contrastive self-supervised learning\u2019, arXiv preprint arXiv:2110.09348 .   \nJing, L., Zbontar, J. et al. (2020), \u2018Implicit rank-minimizing autoencoder\u2019, Advances in Neural Information Processing Systems 33, 14736\u201314746.   \nKim, T. & Yun, S.-Y. (2022), \u2018Revisiting orthogonality regularization: a study for convolutional neural networks in image classification\u2019, IEEE Access 10, 69741\u201369749.   \nKrizhevsky, A., Hinton, G. et al. (2009), \u2018Learning multiple layers of features from tiny images\u2019.   \nLavoie, S., Tsirigotis, C., Schwarzer, M., Vani, A., Noukhovitch, M., Kawaguchi, K. & Courville, A. (2022), \u2018Simplicial embeddings in self-supervised learning and downstream classification\u2019, arXiv preprint arXiv:2204.00616   \nLi, J., Li, D., Xiong, C. & Hoi, S. (2022), Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in \u2018International conference on machine learning\u2019, PMLR, pp. 12888\u201312900.   \nLi, J., Zhou, P., Xiong, C. & Hoi, S. C. (2020), \u2018Prototypical contrastive learning of unsupervised representations\u2019, arXiv preprint arXiv:2005.04966 .   \nLi, X., Chen, S. & Yang, J. (2020), Understanding the disharmony between weight normalization family and weight decay, in \u2018Proceedings of the AAAI Conference on Artificial Intelligence\u2019, Vol. 34, pp. 4715\u20134722.   \nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P. & Zitnick, C. L. (2014), Microsoft coco: Common objects in context, in \u2018Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13\u2019, Springer, pp. 740\u2013755.   \nMarcel, S. & Rodriguez, Y. (2010), Torchvision the machine-vision package of torch, in \u2018Proceedings of the 18th ACM international conference on Multimedia\u2019, pp. 1485\u20131488.   \nMcInnes, L., Healy, J. & Melville, J. (2018), \u2018Umap: Uniform manifold approximation and projection for dimension reduction\u2019, arXiv preprint arXiv:1802.03426 .   \nMisra, I. & Maaten, L. v. d. (2020), Self-supervised learning of pretext-invariant representations, in \u2018Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\u2019, pp. 6707\u2013 6717.   \nOord, A. v. d., Li, Y. & Vinyals, O. (2018), \u2018Representation learning with contrastive predictive coding\u2019, arXiv preprint arXiv:1807.03748 .   \nPang, B., Zhang, Y., Li, Y., Cai, J. & Lu, C. (2022), Unsupervised visual representation learning by synchronous momentum grouping, in \u2018European Conference on Computer Vision\u2019, Springer, pp. 265\u2013282.   \nPasand, A. S., Moravej, R., Biparva, M. & Ghodsi, A. (2024), \u2018Werank: Towards rank degradation prevention for self-supervised learning using weight regularization\u2019, arXiv preprint arXiv:2402.09586   \nQi, H., You, C., Wang, X., Ma, Y. & Malik, J. (2020), Deep isometric learning for visual recognition, in \u2018International conference on machine learning\u2019, PMLR, pp. 7824\u20137835.   \nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. et al. (2021), \u2018Clip: Learning transferable visual models from natural language supervision\u2019, arXiv preprint arXiv:2103.00020 .   \nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I. et al. (2018), \u2018Improving language understanding by generative pre-training\u2019.   \nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. et al. (2019), \u2018Language models are unsupervised multitask learners\u2019, OpenAI blog 1(8), 9.   \nRodr\u00edguez, P., Gonzalez, J., Cucurull, G., Gonfaus, J. M. & Roca, X. (2016), \u2018Regularizing cnns with locally constrained decorrelations\u2019, arXiv preprint arXiv:1611.01967 .   \nRoyChowdhury, A., Sharma, P., Learned-Miller, E. & Roy, A. (2017), Reducing duplicate filters in deep neural networks, in \u2018NIPS workshop on deep learning: Bridging theory and practice\u2019, Vol. 1, p. 6.   \nSaxe, A. M., McClelland, J. L. & Ganguli, S. (2013), \u2018Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\u2019, arXiv preprint arXiv:1312.6120 .   \nSharan, L., Rosenholtz, R. & Adelson, E. H. (2014), \u2018Accuracy and speed of material categorization in real-world images\u2019, Journal of vision 14(9), 12\u201312.   \nTian, Y., Chen, X. & Ganguli, S. (2021), Understanding self-supervised learning dynamics without contrastive pairs, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 10268\u201310278.   \nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F. et al. (2023), \u2018Llama: Open and efficient foundation language models\u2019, arXiv preprint arXiv:2302.13971 .   \nWang, J., Chen, Y., Chakraborty, R. & Yu, S. X. (2020), Orthogonal convolutional neural networks, in \u2018Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\u2019, pp. 11505\u2013 11515.   \nWeng, X., Huang, L., Zhao, L., Anwer, R., Khan, S. H. & Shahbaz Khan, F. (2022), \u2018An investigation into whitening loss for self-supervised learning\u2019, Advances in Neural Information Processing Systems 35, 29748\u201329760.   \nWeng, X., Ni, Y., Song, T., Luo, J., Anwer, R. M., Khan, S., Khan, F. S. & Huang, L. (2023), \u2018Modulate your spectrum in self-supervised learning\u2019, arXiv preprint arXiv:2305.16789 .   \nWu, Y., Kirillov, A., Massa, F., Lo, W.-Y. & Girshick, R. (2019), \u2018Detectron2\u2019, https://github. com/facebookresearch/detectron2.   \nXia, X., Xu, C. & Nan, B. (2017), Inception-v3 for flower classification, in \u20182017 2nd international conference on image, vision and computing (ICIVC)\u2019, IEEE, pp. 783\u2013787.   \nXie, D., Xiong, J. & Pu, S. (2017), All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation, in \u2018Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\u2019, pp. 6176\u2013 6185.   \nYeh, C.-H., Hong, C.-Y., Hsu, Y.-C., Liu, T.-L., Chen, Y. & LeCun, Y. (2022), Decoupled contrastive learning, in \u2018European conference on computer vision\u2019, Springer, pp. 668\u2013684.   \nYoshida, Y. & Miyato, T. (2017), \u2018Spectral norm regularization for improving the generalizability of deep learning\u2019, arXiv preprint arXiv:1705.10941 .   \nZbontar, J., Jing, L., Misra, I., LeCun, Y. & Deny, S. (2021), Barlow twins: Self-supervised learning via redundancy reduction, in \u2018International conference on machine learning\u2019, PMLR, pp. 12310\u2013 12320.   \nZhang, Q., Wang, Y. & Wang, Y. (2022), \u2018How mask matters: Towards theoretical understandings of masked autoencoders\u2019, Advances in Neural Information Processing Systems 35, 27127\u201327139.   \nZhang, S., Zhu, F., Yan, J., Zhao, R. & Yang, X. (2021), Zero-cl: Instance and feature decorrelation for negative-free symmetric contrastive learning, in \u2018International Conference on Learning Representations\u2019.   \nZhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Jiang, Z., Hou, Q. & Feng, J. (2021), \u2018Deepvit: Towards deeper vision transformer\u2019, arXiv preprint arXiv:2103.11886 . ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Effects of Representation Whitening on the Encoder ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we explore the effect of whitening representations on hidden features and weight matrices in the encoder. To be specific, similar to the settings of 5, we train three VICREG Bardes et al. (2022) models: original VICREG, VICREG without projector (Li, Chen & Yang 2020), and VICREG with OR. ", "page_idx": 15}, {"type": "text", "text": "Original VICREG adds two regularization terms (variance and covariance regularization) to whiten the projector features. We use $X_{a u g1}$ as an example to introduce them. ", "page_idx": 15}, {"type": "text", "text": "Variance regularization. The variance regularization term ensures that each dimension of the learned representation $Z$ maintains a non-trivial variance. This is critical to prevent the collapse of dimensions, where a model might ignore certain informative variations in the data. Mathematically, the variance regularization can be expressed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{\\mathrm{var}}=\\frac{1}{D}\\sum_{d=1}^{D}\\mathrm{max}(0,\\gamma-\\mathbf{S}(z_{d},\\epsilon))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $D$ is the dimensionality of $Z_{a u g1}=f(X_{a u g1})$ , $z_{d}$ represents the $d$ -th dimension of $Z_{a u g1}$ , $\\mathbf{S}(z_{d},\\epsilon)=\\sqrt{\\mathbf{V}\\mathrm{ar}(z_{d}))+\\epsilon}$ is the regularized standard deviation of $z_{d}$ across different samples, and $\\gamma$ is a threshold parameter that dictates the minimum desired standard deviation for each dimension. ", "page_idx": 15}, {"type": "text", "text": "Covariance regularization. The covariance regularization term is designed to decorrelate the different dimensions of $Z_{a u g1}$ . By minimizing the off-diagonal elements of the covariance matrix of $Z_{a u g1}$ , this term helps ensure that different dimensions capture distinct aspects of the data, thereby preventing redundancy in the representation. The covariance regularization is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{\\mathrm{cov}}=\\sum_{i\\neq j}\\left(\\operatorname{Cov}(z_{i},z_{j})\\right)^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\operatorname{Cov}(z_{i},z_{j})$ denotes the covariance between the $i^{\\th}$ -th and $j$ -th dimensions of $Z_{a u g1}$ . This term effectively encourages the representation to have orthogonal dimensions, which is beneficial for learning independent features. ", "page_idx": 15}, {"type": "text", "text": "As for the VICREG without projector, we discard the projector and apply the SSL objective directly to the representations, which ensures that the representations are whitened (no dimensional collapse in representations), i.e., minimize the correlation among dimensions and make each dimension rich in information. For OR, we choose SO as the regularizer and set $\\gamma$ as $1e-6$ . We then experimentally observe that guaranteeing that dimensional collapses do not occur in representations or projector features (i.e., VICREG without projector and projector features) does not guarantee that dimensional collapses do not occur in weight matrices in the encoder. Moreover, discarding the projector even damages the performance of the original VICREG, while OR still boosts the performance as shown in Table 8. ", "page_idx": 15}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/579f322d3f843a8c43d3686daa7fe7188204f2e0fd242bb0fc3efd18b875a478.jpg", "table_caption": ["Table 8: Performance comparison of different VICREG configurations. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "As shown in Figure 4, the weight matrices of the original VICREG suffer from dimensional collapse and whitening representations directly (i.e., getting rid of the projector) even makes the eigenvalue decay faster for the weight matrices. OR can alleviate this phenomenon. ", "page_idx": 15}, {"type": "text", "text": "A.2 Visualization of Weight Matrices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, layer4 of ResNet18 pretrained with BYOL on CIFAR-10 is visualized. To be specific, we calculate the correlation coefficient matrix of the weight matrix and then plot the HeatMap of the correlation coefficient matrix and the results of Spectral Biclustering. As shown in Figure 5, ", "page_idx": 15}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/8cfb252db506083e8e59f55cd3662dceef74e5f20bbc7b0f380fc9343b6b3c47.jpg", "img_caption": ["(c) Eigenvalues of weights(VICREG without projector) (d) Eigenvalues of features(VICREG without projector) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/d657d1ba390e40459dba21c9b877f59e8352af6a17793a20d828496d1e02d583.jpg", "img_caption": ["(e) Eigenvalues of weights(VICREG with SO) "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/17cf48ef9aa20f7d4a8aaea87ae2eb54d84ce6e21ee28e3407ebaaf9db453391.jpg", "img_caption": ["(f) Eigenvalues of features(VICREG with SO) "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4096). We pretrain original VICREG, VICREG without projecto, and VICREG with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline. ", "page_idx": 16}, {"type": "text", "text": "HeatMap can intuitively indicate that the correlation of non-diagonal elements is constrained to 0 by OR. The Biclustering results show an obvious blocky structure, which means that there is clustering between filters, and the weight matrix is low-rank and redundant. ", "page_idx": 16}, {"type": "text", "text": "A.3 Visualization of Representations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We used BYOL (ResNet18) for pretraining on CIFAR-10. After pretraining, we perform dimension reduction and visualization of learned representations using UMAP (McInnes et al. 2018). As shown in Figure 6, in the absence of OR, there is a tendency for the cluster centers of each category to move closer together and more outliers appear. This is due to the fact that in the absence of OR, BYOL produces representations dominated by some extremely large eigenvalues (i.e. dimensional collapse), which is consistent with results in Section 5.2. ", "page_idx": 16}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/3188e6db0a86d692c80abb1f5aafcf8a213d33bd50e728cb5956c40b476a00a3.jpg", "img_caption": ["Figure 5: HeatMap is the visualization of the absolute value of the correlation coefficients among filters of the weight matrix (layer4). Biclustering is the visualization of the results of spectral biclustering. It can be seen that OR significantly reduces the correlation and removes the clustering patterns among filters from the heatmap and biclustering, respectively. ", "(c) Biclustering (without OR) ", "(d) Biclustering (with OR) "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Y3FjKSsfmy/tmp/d2ed9a25c5c27532e39513bba7f55c1e22eaafd1e202bf52911650f0d5727299.jpg", "img_caption": ["Figure 6: Visualization of Representations "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.4 Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We utilized several datasets for pretraining and evaluating SSL methods. Below we provide a detailed description of these datasets: ", "page_idx": 19}, {"type": "text", "text": "\u2022 IMAGENET-1k (Deng et al. 2009): A large dataset contains 1,281,167 training images, 50,000 validation images, and 100,000 test images, which spans 1000 object classes.   \n\u2022 IMAGENET-100 (Deng et al. 2009): A subdataset of IMAGENET-1K, containing 100 classes with 1000 training data and 300 test data per class.   \n\u2022 CIFAR-10 (Krizhevsky et al. 2009): Comprising 60,000 images in 10 classes, with each class containing 6,000 images. The split includes 50,000 training images and 10,000 test images.   \n\u2022 CIFAR-100 (Krizhevsky et al. 2009): This dataset consists of 60,000 images divided into 100 classes, with 600 images per class. The dataset is split into 50,000 training images and 10,000 test images.   \n\u2022 Food-101 (Bossard et al. 2014): This dataset includes 101,000 images of food dishes categorized into 101 classes, with each class having approximately 1,000 images.   \n\u2022 Flowers-102 (Xia et al. 2017): Contains 8,189 images of flowers from 102 different categories. Each class consists of between 40 and 258 images.   \n\u2022 DTD (Sharan et al. 2014): The Describable Textures Dataset (DTD) includes 5,640 images categorized into 47 different texture categories.   \n\u2022 GTSRB (Haloi 2015): The German Traffic Sign Recognition Benchmark (GTSRB) dataset consists of over 50,000 images of traffic signs across 43 categories.   \n\u2022 PASCAL VOC2007 and VOC2012 (Lin et al. 2014): Used for evaluating objection tasks, this dataset includes complex everyday scenes with annotated objects in their natural context. The objection detection task contains 20 categories. We use the VOC2007 and VOC2012 train-val (16551 images) as the training set and then report the performance on the VOC2007 test set (4952 images). ", "page_idx": 19}, {"type": "text", "text": "Each dataset was carefully curated to support the training and validation of our models, ensuring a comprehensive evaluation across various image classification and segmentation tasks. ", "page_idx": 19}, {"type": "text", "text": "A.5 Joint-embedding SSL methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without the need for labeled data. This appendix provides a concise overview of several SSL methods used in this paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 MOCOv2, introduced by Chen & He (2021), on top of MOCO\u2019s momentum encoder and the use of the dynamic dictionary with a queue to store negative samples (He et al. 2020), adds the MLP projection head and more data augmentation. Compared to MOCOv2, MOCOv2plus uses a symmetric similarity loss.   \n\u2022 MoCov3 (Chen et al. 2021) makes some improvements on the basis of v1/2, firstly, because the batchsize is large enough when training V3, the memory queue is removed, and the negative samples are sampled directly from the batch. Secondly, symmetric contrastive loss is used, and finally, an extra prediction head is added to the original encoder, which is a two-layer fully connected layer.   \n\u2022 Bootstrap Your Own Latent (BYOL), proposed by Grill et al. (2020), introduces a novel approach to SSL that does not rely on negative pairs. Instead, BYOL employs a dual-network architecture where the encoder learns to predict the representations of the momentum encoder. Through a series of updates (i.e. EMA), where the momentum encoder gradually assimilates the encoder\u2019s weights, BYOL effectively learns robust representations. The success of BYOL depends not only on the EMA, but also on its additional projector and the BN in the projector to avoid a complete collapse of the encoder. This method challenges the conventional wisdom that contrastive learning requires negative pairs, opening new avenues for SSL research. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Expanding on the ideas of BYOL, NNBYOL (Dwibedi et al. 2021) introduces the concept of using nearest neighbors to augment the learning process. By leveraging the similarities between different instances in the dataset, NNBYOL aims to refine the quality of the learned representations further. This approach underscores the potential of incorporating instancelevel information into the SSL framework, enhancing the discriminability and robustness of the resulting models. ", "page_idx": 20}, {"type": "text", "text": "\u2022 DINO (Caron et al. 2021) uses a self-distilling architecture. The outputs of the teacher networks (i.e. the momentum encoder) are subjected to a centering operation by averaging over a batch, and each network outputs a K-dimensional feature that is normalized using Softmax. The similarity between the student model (i.e. the encoder) and the teacher model is then computed using cross-entropy loss as the objective function. A stop-gradient operator is used on the teacher model to block the propagation of the gradient, and only the gradient is passed to the student model to make it update its parameters. The teacher model is updated using the weights of the student model (i.e. EMA). ", "page_idx": 20}, {"type": "text", "text": "\u2022 Barlow Twins computes the correlation matrix between the embeddings of two different views of the same sample and avoids collapse by making it as close as possible to the unit matrix. This approach makes the embeddings between the two views of the sample as similar as possible while minimizing the redundancy between vector components.   \n\u2022 VICREG avoids the complete collapse problem with variance and covariance regularization.   \n\u2022 DCL and DCLW removes the NPC effect of infoNCE loss by getting rid of the positive term from the denominator and thus significantly improves the learning efficiency   \n\u2022 SimSiam achieves a very strong baseline without using large batchsize, negative samples, or momentum encoder using only the stop-gradient operation.   \n\u2022 SwaV is trained by predicting the clustering assignment of another view and also introduces multi-crop, which increases the number of views by reducing the image size without increasing the extra memory and computational requirements.   \n\u2022 SimCLR establishes a simple and effective architecture for contrastive learning by increasing the batchsize, augmenting the data and adding a nonlinear projector after the representation. ", "page_idx": 20}, {"type": "text", "text": "A.6 Hyper-parameters of Pretraining and Evaluation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For each SSL method, we use the original settings of Solo-learn and LightlySSL (Da Costa et al. 2022). These settings include the network structure, loss function, training policy, and data augmentation policy. Considering that we use numerous SSL methods and that our setup is exactly the same as them, please go to their official implementation. ", "page_idx": 20}, {"type": "text", "text": "For OR, the appropriate regularization term $\\gamma$ generally depends only on the backbone used by SSL and the orthogonality regularizer (SRIP or SO) chosen. As shown in Table 9, when you want to add OR to your SSL pre-training, you simply pass the encoder into the loss function, and then you just need to set $\\gamma$ of the OR according to the backbone and regularizer you use. ", "page_idx": 20}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/6ca899effffc5fe6e7ba13d5986e249c137fa08484666aa3929cb1ba2e65de58.jpg", "table_caption": ["Table 9: The recipe of adding OR "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "For the classification tasks, due to computational constraints, we do not perform non-linear fine-tuning in classification tasks. Instead, we perform a linear probe or KNN to evaluate the quality of obtained representations as typically done in the literature (Huang et al. 2024, Li, Chen & Yang 2020, Lavoie et al. 2022). To be specific, for each SSL method and dataset, after pretraining, We train a linear classifier on top of frozen representations of the training set. Then we report the Top-1 and Top-5 linear classification accuracy on the test set. When training the linear classifier, we use 100 epochs, weight decay to 0.0005, learning rate 0.1 (we divide the learning rate by a factor of 10 on Epoch 60 and 100), batchsize 256, and SGD with Nesterov momentum as optimizer (In IMAGENET-1k, we use batchsize 128 and learning rate 0.2). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "For the object detection task, we perform nonlinear fine-tuning on ResNet50 in RCNN-C4 (Girshick et al. 2014) with batchsize 9 and base learning rate 0.01. We use the detectron2 (Wu et al. 2019), following the MOCO-v1 (He et al. 2020) official implementation exactly. ", "page_idx": 21}, {"type": "text", "text": "A.7 Time Cost of OR ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Implementing OR requires computing the OR loss in the backbone at each gradient update, we count the time overhead required by the different backbones to compute OR at one time, and we have averaged over 10 times as shown in Table A.7. In the pre-training phase, the time overhead of OR is only related to the backbone and the steps that need to be updated, IMAGENET-1k (100 epochs, batchsize 128) has a total of 62599 steps, and CIFAR-100 (1000 epochs, batchsize 256) has a total of 194999 steps. As you can see, compared to the original pre-training overhead of dozens and hundreds of hours, the additional time added by OR is very small, steadily improving SSL\u2019s performance. Notably, if we use a larger batchsize such as 4096, our time overhead will be reduced by 64 on IMAGENET-1k and 16 on CIFAR-100. ", "page_idx": 21}, {"type": "table", "img_path": "Y3FjKSsfmy/tmp/d0ea47e841dadd8f974ee361e515f0bd7c6c1cbb03dbbbc00798268bef1c576e.jpg", "table_caption": ["Table 10: Time cost of OR "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: In the abstract, we explained that dimensional collapse would exist in weight matrices and features, which would damage the performance of SSL. We used OR to eliminate the dimensional collapse in SSL. In addition, consistent improvement results were achieved under each benchmark.In the introduction, we list our 3 contributions in detail. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our approach requires the introduction of additional computational overhead, which is discussed in Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We just expressed the existing theory in the vector form to the matrice form. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: All of our experimental codes including how to divide the generated data, training and testing are all the same with open-sourced Solo-learn. And we\u2019ve also attached them in the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We include instructions in the appendix, including how to download and divide the data set, how to execute training and test scripts, and how to visualize the results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have explained data division and hyperparameters in detail in both Appendix and code. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: We only conducted multiple averaging operations in the statistical OR time overhead, and the experiments in the main paper did not carry out multiple averaging operations. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We compared the time cost of OR in Appendix, and explained that our experiments were all completed on 4 3090 GPUs. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The data sets we use are all public data sets, and we strictly follow existing studies, which will not have a bad impact on the society. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our goal is to improve the performance of the vision foundation model, which will have a positive impact on various downstream tasks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our approach addresses the potential dimensional collapse in SSL without raising their risk. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We referenced the framework we used and the data set we used. In the code, we listed the URL of each data set and the code environment we needed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not introduce new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]