[{"figure_path": "Y3FjKSsfmy/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of dimensional collapse in SSL. We use one augmented input Xaug1 as an example: we assume that the encoder contains two basic blocks, each containing a linear operation (e.g., a linear layer or convolutional layer) and an activation function. Dimensional collapse can occur in weight matrices (W1, W2), hidden features, and the finally obtained representations. Existing methods act directly on representations and expect to affect hidden features and weight matrices indirectly, which has no guarantee in theory; our method directly constrains weight matrices and indirectly influences hidden features and representations, which can be guaranteed by theoretical analysis.", "description": "This figure illustrates the concept of dimensional collapse in self-supervised learning (SSL).  It shows how dimensional collapse can occur in three places within the encoder: the weight matrices (W1, W2), the hidden features, and the final representations. It also compares existing methods, which act only on the representations, with the proposed orthogonality regularization method, which acts directly on weight matrices and has theoretical guarantees on preventing collapse.", "section": "1 Introduction"}, {"figure_path": "Y3FjKSsfmy/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of dimensional collapse in SSL. We use one augmented input Xaug1 as an example: we assume that the encoder contains two basic blocks, each containing a linear operation (e.g., a linear layer or convolutional layer) and an activation function. Dimensional collapse can occur in weight matrices (W1, W2), hidden features, and the finally obtained representations. Existing methods act directly on representations and expect to affect hidden features and weight matrices indirectly, which has no guarantee in theory; our method directly constrains weight matrices and indirectly influences hidden features and representations, which can be guaranteed by theoretical analysis.", "description": "This figure illustrates how dimensional collapse can occur in self-supervised learning (SSL) models.  It shows an encoder with two blocks, each containing a linear layer and activation function.  Dimensional collapse can happen at various points: within the weight matrices (W1, W2), the hidden features, and the final representations. Existing approaches focus on modifying the representations, lacking a theoretical guarantee that it would fix the issue in the weight matrices and hidden features.  In contrast, this paper's approach directly constrains the weight matrices, ensuring orthogonality, which theoretically prevents collapse across all three areas.", "section": "1 Introduction"}, {"figure_path": "Y3FjKSsfmy/figures/figures_6_1.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure displays the eigenvalue distribution of both weight matrices and features within a ResNet18 encoder for three BYOL training scenarios: without orthogonality regularization (OR), with VICREG's feature whitening, and with OR.  The plots show how OR helps prevent eigenvalues from decaying rapidly, indicating a more even distribution of information across the feature space and suggesting a reduction in dimensional collapse. This contrasts with the rapid decay observed in the other scenarios.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_6_2.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure displays the eigenvalue distribution of both weight matrices and features (input features, hidden features, and representations) within a ResNet18 encoder, comparing three BYOL training scenarios: without orthogonality regularization (OR), with feature whitening (from VICREG), and with OR. The plots reveal that OR effectively mitigates the dimensional collapse issue, resulting in more uniform eigenvalue distributions.", "section": "Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_6_3.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure compares the eigenvalue distributions of weight matrices and features (input, hidden, and representations) within a ResNet18 encoder, trained with three different methods: BYOL without orthogonality regularization (OR), BYOL with VICREG's feature whitening, and BYOL with OR.  The plots show how the eigenvalue decay rate changes across different layers and methods, illustrating the impact of OR in preventing dimensional collapse.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_6_4.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure visualizes the eigenvalue distribution of weight matrices and features (input, hidden, and representation) within the ResNet18 encoder of a BYOL model trained on CIFAR-10 under three conditions: without orthogonal regularization (OR), with feature whitening from VICREG, and with OR.  The plots show how the eigenvalues decay across different ranks, indicating the extent of dimensional collapse. A slower decay suggests a more uniform distribution of information across feature dimensions, whereas a rapid decay signifies dimensional collapse where a few dimensions dominate. The figure aims to demonstrate OR's effectiveness in mitigating dimensional collapse in both weight matrices and features.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_6_5.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure displays the eigenvalue distribution of both weight matrices and feature maps within the ResNet18 encoder of a BYOL model trained on CIFAR-10. Three training scenarios are compared: without orthogonal regularization (OR), with feature whitening from VICREG, and with OR. The plots show that OR leads to a more uniform eigenvalue distribution, indicating a reduction in dimensional collapse.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_6_6.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure visualizes the eigenvalue distributions of both weight matrices and features (input features, hidden features, and representations) within a ResNet18 encoder pretrained using BYOL on CIFAR-10.  Three different scenarios are shown: BYOL without orthogonal regularization (OR), BYOL with feature whitening (from VICREG), and BYOL with OR.  The plots show that OR effectively prevents dimensional collapse, evidenced by the slower decay of eigenvalues and fewer negative values, indicating a more uniform distribution of information across dimensions compared to other methods.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_16_1.jpg", "caption": "Figure 1: Illustration of dimensional collapse in SSL. We use one augmented input Xaug1 as an example: we assume that the encoder contains two basic blocks, each containing a linear operation (e.g., a linear layer or convolutional layer) and an activation function. Dimensional collapse can occur in weight matrices (W1, W2), hidden features, and the finally obtained representations. Existing methods act directly on representations and expect to affect hidden features and weight matrices indirectly, which has no guarantee in theory; our method directly constrains weight matrices and indirectly influences hidden features and representations, which can be guaranteed by theoretical analysis.", "description": "This figure illustrates how dimensional collapse can affect different parts of a self-supervised learning (SSL) model. It shows that existing methods primarily focus on addressing collapse in the final representations, leaving the weight matrices and hidden features potentially uncontrolled. In contrast, the proposed method directly addresses collapse in the weight matrices, which indirectly helps control collapse in hidden features and representations.", "section": "1 Introduction"}, {"figure_path": "Y3FjKSsfmy/figures/figures_16_2.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure visualizes the eigenvalue distribution of weight matrices and features (input features, hidden features, and representations) within a ResNet18 encoder using BYOL pre-training.  Three scenarios are compared: BYOL without orthogonality regularization (OR), BYOL with feature whitening (from VICREG), and BYOL with OR.  The plots show how OR affects eigenvalue distribution, indicating improved feature diversity and reduced dimensional collapse.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_16_3.jpg", "caption": "Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.", "description": "This figure visualizes the eigenvalue distributions of both weight matrices and features (input, hidden, and representation) within a ResNet18 encoder when using BYOL for pre-training on CIFAR-10, with and without Orthogonal Regularization (OR) and with VICREG's feature whitening.  The plots show how OR and feature whitening techniques impact the eigenspectra, indicating the presence or absence of dimensional collapse in different model components.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}, {"figure_path": "Y3FjKSsfmy/figures/figures_17_1.jpg", "caption": "Figure 1: Illustration of dimensional collapse in SSL. We use one augmented input Xaug1 as an example: we assume that the encoder contains two basic blocks, each containing a linear operation (e.g., a linear layer or convolutional layer) and an activation function. Dimensional collapse can occur in weight matrices (W1, W2), hidden features, and the finally obtained representations. Existing methods act directly on representations and expect to affect hidden features and weight matrices indirectly, which has no guarantee in theory; our method directly constrains weight matrices and indirectly influences hidden features and representations, which can be guaranteed by theoretical analysis.", "description": "This figure illustrates the concept of dimensional collapse in self-supervised learning (SSL).  It shows how the encoder, consisting of multiple blocks with linear or convolutional layers, can lead to collapse in three different places:  the weight matrices (W1, W2), hidden features, and the final representations.  Existing approaches primarily focus on fixing issues with the representations, but this figure argues that this indirect approach lacks theoretical guarantees.  Their method aims to directly regularize the weight matrices, providing theoretical guarantees of preventing collapse in the weight matrices and indirectly improving hidden features and representations.", "section": "1 Introduction"}, {"figure_path": "Y3FjKSsfmy/figures/figures_18_1.jpg", "caption": "Figure 6: Visualization of Representations", "description": "This figure visualizes the learned representations from BYOL with and without orthogonal regularization (OR).  It uses UMAP for dimensionality reduction to project the high-dimensional representations into a 2D space for visualization. Each point represents a data sample, and the color indicates its class label. The plots show that the BYOL model without OR shows more cluster overlap and scattered points, indicating dimensional collapse (where data points are mapped to a limited number of dimensions). Conversely, BYOL with OR has more clearly separated clusters of data points, demonstrating its effectiveness in preventing dimensional collapse and preserving more of the data's inherent structure.", "section": "5 Analysis of Dimensional Collapse and the Effects of OR in SSL"}]