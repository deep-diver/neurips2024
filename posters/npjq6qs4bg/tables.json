[{"figure_path": "npJQ6qS4bg/tables/tables_4_1.jpg", "caption": "Table 1: OP matches Pre LN performance at scales up to 1.2B params, on Languini Books [39].7", "description": "This table demonstrates the performance of the Outlier Protected (OP) block compared to the Pre-LN block across three different model sizes (100M, 320M, and 1.2B parameters). The evaluation metric used is perplexity (PPL) on the Languini Books dataset.  The results show that the OP block achieves comparable perplexity to the Pre-LN block across all model sizes, indicating that the OP block can effectively minimize outlier features without compromising performance. The model sizes are measured by the number of parameters.", "section": "Additional Experiments"}, {"figure_path": "npJQ6qS4bg/tables/tables_9_1.jpg", "caption": "Table 2: Average kurtosis across layers, plus standard precision (FP16/32) and quantised int8 (W8A8) perplexity of various 125m OPT models [6] trained on BookCorpus+Wikipedia [15]. Kurtosis strongly correlates with int8 error across settings, and the best int8 setup combines our architectural (OP) and optimiser (SOAP) suggestions, with only 1.2 kurtosis and 0.16 perplexity increase.", "description": "This table shows the results of quantization experiments on the OPT-125m model.  Different model architectures (Pre-LN, Gated Attention, OP) and optimizer hyperparameters are compared. The table reports the average kurtosis across layers, standard precision perplexity (FP16/32), and quantized int8 perplexity (W8A8).  The key finding is that the combination of the OP architecture and the SOAP optimizer achieves the lowest quantized perplexity, demonstrating the effectiveness of the proposed methods for minimizing outlier features and improving quantization.", "section": "6 Additional Experiments"}, {"figure_path": "npJQ6qS4bg/tables/tables_42_1.jpg", "caption": "Table 2: Average kurtosis across layers, plus standard precision (FP16/32) and quantised int8 (W8A8) perplexity of various 125m OPT models [6] trained on BookCorpus+Wikipedia [15]. Kurtosis strongly correlates with int8 error across settings, and the best int8 setup combines our architectural (OP) and optimiser (SOAP) suggestions, with only 1.2 kurtosis and 0.16 perplexity increase.", "description": "This table presents the results of an experiment on quantizing OPT-125m models.  It shows the average kurtosis across layers, standard precision perplexity (FP16/32), and quantized int8 perplexity (W8A8) for various model architectures and optimization choices. The results demonstrate a strong correlation between kurtosis and int8 error. The best performing int8 setup uses the proposed Outlier Protected (OP) block and SOAP optimizer.", "section": "6 Additional Experiments"}, {"figure_path": "npJQ6qS4bg/tables/tables_44_1.jpg", "caption": "Table 2: Average kurtosis across layers, plus standard precision (FP16/32) and quantised int8 (W8A8) perplexity of various 125m OPT models [6] trained on BookCorpus+Wikipedia [15]. Kurtosis strongly correlates with int8 error across settings, and the best int8 setup combines our architectural (OP) and optimiser (SOAP) suggestions, with only 1.2 kurtosis and 0.16 perplexity increase.", "description": "This table presents the results of experiments evaluating different model architectures and optimization strategies on the task of quantizing OPT-125m language models. The table compares standard precision perplexity (FP16/32) and int8 quantized perplexity (W8A8), along with the average kurtosis across layers.  The results show a strong correlation between kurtosis (a measure of outlier features) and int8 quantization error.  The combination of the Outlier Protected (OP) block and the SOAP optimizer yields the lowest kurtosis and the smallest increase in perplexity after quantization.", "section": "6 Additional Experiments"}]