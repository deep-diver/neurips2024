[{"figure_path": "IlIDNMvwmX/tables/tables_6_1.jpg", "caption": "Table 1: Ablation study for the LM-HT model on a subset of ImageNet-1k.", "description": "This table presents the results of an ablation study conducted on a subset of the ImageNet-1k dataset to evaluate the impact of different model configurations on the LM-HT model.  The study varies the number of thresholds (L) and the number of time steps (T) considered by the model, with and without the Temporal-Global Information Matrix (T-GIM).  For each configuration, the table reports the achieved accuracy (Acc.), the number of synaptic operations (SOPs) in Giga, and the energy consumption (E.) in millijoules.  The results allow assessing the trade-off between accuracy, computational cost, and energy efficiency for various LM-HT model setups.", "section": "5 Experiments"}, {"figure_path": "IlIDNMvwmX/tables/tables_6_2.jpg", "caption": "Table 2: Validation for the reparameterization procedure.", "description": "This table presents the accuracy, synaptic operations (SOPs), and energy consumption (E) for VGG-13 and ResNet-18 models before and after the reparameterization process. The reparameterization transforms a multi-hierarchical threshold (M-HT) model into a vanilla single-threshold model, demonstrating the efficiency and flexibility of the proposed method. The results show that the performance remains almost identical after reparameterization, indicating a successful transformation without significant loss in accuracy or computational cost.", "section": "4.5 Reparameterize the LM-HT model to vanilla LIF model"}, {"figure_path": "IlIDNMvwmX/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with previous state-of-the-art works.", "description": "This table compares the performance of the proposed LM-HT model against other state-of-the-art methods on various datasets (CIFAR-10, CIFAR-100, ImageNet-200, ImageNet-1k, and CIFAR10-DVS) using different network architectures and numbers of time steps.  The table presents the accuracy achieved by each method and highlights the superior performance of the LM-HT model, especially when using only two time steps.", "section": "5.2 Comparison with Previous SoTA Works"}, {"figure_path": "IlIDNMvwmX/tables/tables_8_1.jpg", "caption": "Table 4: The performance of hybrid training based on the LM-HT model for CIFAR-100 dataset.", "description": "This table presents the results of hybrid training experiments using the LM-HT model on the CIFAR-100 dataset.  It compares the accuracy achieved by different methods (RMP, SNM, SRP, QCFS, and LM-HT with varying parameters) after hybrid training, where the pre-trained ANN models are further fine-tuned using STBP for enhancing the performance of converted SNNs under low time latency. The results are broken down for two architectures, VGG-16 and ResNet-20, and for multiple time steps.  The table shows the improvement in accuracy achieved using the LM-HT model over other methods.", "section": "5.3 Performance Analysis of Hybrid Training"}, {"figure_path": "IlIDNMvwmX/tables/tables_15_1.jpg", "caption": "Table S1: Comparison with previous methods based on advanced backbones and attention mechanism.", "description": "This table compares the performance of the proposed LM-HT model with other state-of-the-art methods on CIFAR-10 and CIFAR-100 datasets.  The comparison considers different network architectures (MS-ResNet-18 and Transformer-4-384) and the number of time steps used for training.  The results demonstrate the effectiveness of the LM-HT model in achieving high accuracy across various backbones and time steps.", "section": "A.2 Comparison with Other Advanced Network Backbones"}]