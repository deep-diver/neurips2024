[{"type": "text", "text": "Elliptical Attention ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stefan K. Nielsen\u2217   \nFPT Software AI Center   \nHa Noi, Vietnam   \nstefannvkp@fpt.com ", "page_idx": 0}, {"type": "text", "text": "Laziz U. Abdullaev\u2217 Department of Mathematics National University of Singapore Singapore 119077, Singapore laziz.abdullaev@u.nus.edu ", "page_idx": 0}, {"type": "text", "text": "Rachel S.Y. Teo ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Mathematics National University of Singapore Singapore 119077, Singapore rachel.teo@u.nus.edu ", "page_idx": 0}, {"type": "text", "text": "Tan M. Nguyen Department of Mathematics National University of Singapore Singapore 119077, Singapore tanmn@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pairwise dot-product self-attention is key to the success of transformers that achieve state-of-the-art performance across a variety of applications in language and vision. This dot-product self-attention computes attention weights among the input tokens using Euclidean distance, which makes the model prone to representation collapse and vulnerable to contaminated samples. In this paper, we propose using a Mahalanobis distance metric for computing the attention weights to stretch the underlying feature space in directions of high contextual relevance. In particular, we define a hyper-ellipsoidal neighborhood around each query to increase the attention weights of the tokens lying in the contextually important directions. We term this novel class of attention Elliptical Attention. Our Elliptical Attention provides two benefits: 1) reducing representation collapse, and 2) enhancing the model\u2019s robustness as Elliptical Attention pays more attention to contextually relevant information, rather than focusing on some small subset of informative features. We empirically demonstrate the advantages of Elliptical Attention over the baseline dot-product attention and state-of-the-art attention methods on various practical tasks, including object classification, image segmentation, and language modeling across different data modalities. The code is publicly available at https://github.com/stefvk/Elliptical-Attention. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Attention mechanisms and transformers [82] have achieved state of the art performance across a wide variety of tasks in machine learning [27, 35, 75] and, in particular, within natural language processing [1, 2, 13, 65, 12], computer vision [16, 39, 78, 66, 62], and reinforcement learning [25, 5]. They have also demonstrated strong performance in knowledge transfer from pretraining tasks to various downstream tasks with weak or no supervision [63, 64, 15]. At the core of these models is the dotproduct self-attention mechanism, which learns self-alignment between tokens in an input sequence by estimating the relative importance of each token with respect to all others. The mechanism then transforms each token into a weighted average of the feature representations of the other tokens with weights proportional to the learned importance scores. The relative importance scores capture contextual information among tokens and are key to the success of the transformer architecture [83, 76, 8, 59, 36, 55]. ", "page_idx": 0}, {"type": "image", "img_path": "Ejg4d4FVrs/tmp/3ac99a77dffaf6f81071ff0e9988489bf3027fd744a282ffd1c309f803b9a4b4.jpg", "img_caption": ["Figure 1: Comparison of Attention Heatmaps. Elliptical pays attention to more relevant information. DeiT focuses on just a subset of informative features while Elliptical considers a wider set of contextually relevant information, helping to produce more accurate and robust predictions. Attention scores are min-max scaled for visualization purposes. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Recent work has begun exploring the connections between self-attention and non-parametric kernel regression [54, 23]. Under this interpretation, there is an unknown, underlying function $f$ mapping the tokens in the input sequence to the output sequence. The self-attention mechanism estimates $f$ by performing Nadaraya-Watson (NW) regression with isotropic Gaussian kernels. Our work leverages this perspective on self-attention, where we notice that Gaussian isotropic kernels are spherically invariant. This has the drawback of assuming all dimensions of the feature space are equal in terms of importance, meaning nearby tokens are assigned contextual relevance weights dependant only on their Euclidean distance from a query, regardless of direction. From the non-parametric regression perspective, we show that spherical invariance in the kernel causes the estimator to suffer provably higher variance. This causes two connected disadvantages in the self-attention setting. First, high variance in the estimator impairs robustness as small contaminations in the input cause large, erroneous changes in the self-attention output. Second, the high variance of the estimator reduces the capacity of the self-attention mechanism as hidden representations passing through the model are increasingly composed of uninformative noise. ", "page_idx": 1}, {"type": "text", "text": "Contribution. In this work, we propose Elliptical Attention, a new class of self-attention that constructs hyper-ellipsoidal, rather than hyper-spherical, neighborhoods around the attention queries. The key idea is to stretch the neighborhoods around the queries to upweight keys in directions of high importance. We achieve this by computing a Mahalanobis transformation that stretches the axes of the underlying feature space according to a learned measure of coordinate-wise relevance. Constructing hyper-ellipsoidal neighborhoods following this scheme allows the self-attention mechanism to learn higher-quality contextual representations that prevent representation collapse while simultaneously exhibiting stronger robustness. We additionally propose an estimator of coordinate-wise relevance in the self-attention mechanism that can be computed highly efficiently and with no learnable parameters. We theoretically prove that our estimator accurately estimates the relative coordinate-wise relevance in the feature space. Finally, our approach of constructing hyper-ellipsoidal neighborhoods is linked to theoretical improvements in the mean squared error (MSE) of non-parametric estimators by reducing variance without introducing bias. We demonstrate that this provable reduction in variance is related to both representation collapse and robustness, proposing a unifying framework for both phenomena. This framework is based on the geometry of the predictive neighborhood around queries in the attention mechanism. In summary, our contributions are three-fold: ", "page_idx": 1}, {"type": "text", "text": "1. We develop the novel Elliptical Attention, which learns better contextual representations by constructing hyper-ellipsoidal neighborhoods around queries.   \n2. We propose an efficient estimator of the coordinate-wise relevance in the self-attention mechanism, which requires no learnable parameters, and provide theoretical guarantees for this estimator.   \n3. We derive a theoretical framework unifying representation collapse and robustness in transformers based only on the implicit geometry of the attention mechanism. ", "page_idx": 1}, {"type": "text", "text": "We empirically demonstrate that 1) Elliptical Attention outperforms baseline self-attention models in terms of accuracy and robustness on a variety of practical benchmarks, including WikiText-103 language modelling, ImageNet-1K object classification, LRA long sequence modeling, and ADE20K image segmentation, 2) Elliptical Attention attains robust improvements with lower memory requirements and faster computational speed than baseline robust transformers, and 3) Elliptical Attention can be combined with state-of-the-art robust transformers to further boost robust performance in ImageNet-1K under adversarial attack. ", "page_idx": 1}, {"type": "image", "img_path": "Ejg4d4FVrs/tmp/28ef34659751d0682746a3a5c28bd0c57c4d5209b2aed0725e60e23ba90f9bb5.jpg", "img_caption": ["Figure 2: Left: The function does not vary in the $x_{2}$ axis so we stretch the neighborhood in that direction. Right: The stretched ellipsoidal neighborhood includes 4 more keys. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Organization. We structure this paper as follows: In Section 2, we present preliminaries on selfattention and non-parametric kernel regression. In Section 3, we illustrate the theoretical benefits of hyper-ellipsoidal neighborhoods, demonstrate how we build the required transformation, and provide the full technical formulation of Elliptical Attention. We empirically validate the advantages of the Elliptical Attention in Section 4. Related work is discussed in Section 5 before presenting concluding remarks in Section 6. Proofs, technical details, and further experiments are provided in the Appendix. ", "page_idx": 2}, {"type": "text", "text": "2 Background: Self-Attention and Non-Parametric Regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first provide preliminaries on the self-attention mechanism followed by background on its connection to the Nadaraya-Watson (NW) estimator in non-parametric regression [48]. ", "page_idx": 2}, {"type": "text", "text": "2.1 Self-Attention Mechanism ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given an input sequence ${\\pmb X}=[{\\pmb x}_{1},\\dots,{\\pmb x}_{N}]^{\\top}\\,\\in\\,\\mathbb{R}^{N\\times D_{x}}$ of $N$ feature vectors, the self-attention mechanism transforms the input to $H:=[\\pmb{h}_{1},\\dots,\\pmb{h}_{N}]^{\\top}\\in\\mathbb{R}^{N\\times D_{x}}$ as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{h}_{i}=\\sum_{j\\in[N]}\\mathrm{softmax}\\left(\\frac{\\pmb{q}_{i}^{\\top}\\pmb{k}_{j}}{\\sqrt{D}}\\right)\\pmb{v}_{j},\\;\\mathrm{for}\\;i=1,\\ldots,N.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The vectors ${\\pmb q}_{i},{\\pmb k}_{j}$ , and $\\pmb{v}_{j}$ are the query, key, and value vectors, respectively. They are computed as $[\\pmb{q}_{1},\\dots,\\pmb{q}_{N}]^{\\top}:=\\pmb{Q}\\,=\\,\\pmb{X}\\pmb{W}_{Q}^{\\top}\\,\\in\\,\\mathbb{R}^{N\\times D}$ , $[\\pmb{k}_{1},\\dots,\\pmb{k}_{N}]^{\\top}\\,:=\\,\\pmb{K}\\,=\\,\\pmb{X}\\pmb{W}_{K}^{\\top}\\,\\in\\,\\mathbb{R}^{N\\times D}$ , and $[\\pmb{v}_{1},\\ldots,\\pmb{v}_{N}]^{\\top}:=\\pmb{V}=\\pmb{X}\\pmb{W}_{V}^{\\top}\\in\\mathbb{R}^{N\\times D_{v}}$ where $\\pmb{W}_{Q},\\pmb{W}_{K}\\in\\mathbb{R}^{D\\times D_{x}}$ , $\\pmb{W}_{V}\\in\\mathbb{R}^{D_{v}\\times D_{x}}$ are the weight matrices. Eqn. 1 can be expressed in matrix form as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nH=\\mathrm{softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{D}}\\right)V,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the softmax function is applied row-wise to the matrix $Q K^{\\top}/\\sqrt{D}$ . We refer to transformers built with Eqn. 2 as standard transformers or just transformers. ", "page_idx": 2}, {"type": "text", "text": "2.2 A Non-Parametric Regression Perspective of Self-Attention ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now present the connection between self-attention as described in Eqn. 1 and non-parametric regression. We first assume key and value vectors $\\{k_{j},\\boldsymbol{v}_{j}\\}_{j\\in[N]}$ are obtained from the following data generating process: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{v}=f(\\pmb{k})+\\pmb{\\epsilon},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\epsilon$ is random zero-mean noise $\\mathbb{E}[\\epsilon]=0$ , and $f$ is the unknown function to be estimated. We consider the random design setting where the keys $\\{k_{j}\\}_{j\\in[N]}$ are i.i.d samples drawn from the marginal distribution $p(k)$ . We use $p(v,k)$ to denote the joint distribution of pairs $(v,k)$ as obtained according to Eqn. 3. At any given new query $\\pmb q$ , we aim to estimate the unknown function $f({\\pmb q})$ . ", "page_idx": 2}, {"type": "text", "text": "The NW estimator is a non-parametric estimator of the unknown $f$ described by ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\pmb{k})=\\mathbb{E}[\\pmb{v}|\\pmb{k}]=\\int_{\\mathbb{R}^{D}}\\pmb{v}\\cdot\\pmb{p}(\\pmb{v}|\\pmb{k})d\\pmb{v}=\\int_{\\mathbb{R}^{D}}\\frac{\\pmb{v}\\cdot\\pmb{p}(\\pmb{v},\\pmb{k})}{p(\\pmb{k})}d\\pmb{v},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we apply zero-mean noise for the first equality and the definitions of conditional expectation and density for the second and final. Then, it can be shown that by estimating the joint density $p(v,k)$ and marginal density $p(k)$ using isotropic Gaussian kernels with bandwidth $\\sigma$ and evaluating the NW estimator at a new query $\\mathbf{{q}}_{i}$ , we obtain ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}_{\\sigma}(\\boldsymbol{q}_{i})=\\frac{\\sum_{j\\in[N]}v_{j}\\exp\\left(-\\|\\boldsymbol{q}_{i}-\\boldsymbol{k}_{j}\\|^{2}/2\\sigma^{2}\\right)}{\\sum_{j\\in[N]}\\exp\\left(-\\|\\boldsymbol{q}_{i}-\\boldsymbol{k}_{j}\\|^{2}/2\\sigma^{2}\\right)}}\\\\ &{\\quad\\quad=\\frac{\\sum_{j\\in[N]}v_{j}\\exp\\left(\\boldsymbol{q}_{i}^{\\top}\\boldsymbol{k}_{j}/\\sigma^{2}\\right)}{\\sum_{j\\in[N]}\\exp\\left(\\boldsymbol{q}_{i}^{\\top}\\boldsymbol{k}_{j}/\\sigma^{2}\\right)}=\\sum_{j\\in[N]}\\mathrm{softmax}(\\boldsymbol{q}_{i}^{\\top}\\boldsymbol{k}_{j}/\\sigma^{2})v_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where choosing $\\sigma^{2}=\\sqrt{D}$ as the isotropic variance recovers the full attention mechanism. We present the full derivation in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Limitation of self-attention. We see in Eqn. 5 that standard self-attention computes the relative importance scores between queries and keys via Euclidean distance. Euclidean distances are spherically invariant and therefore fail to consider coordinate-wise significance in the feature space, meaning the proximity of $k_{j}$ from $\\mathbf{\\Delta}q_{i}$ influences its contextual relevance equally regardless of direction. ", "page_idx": 3}, {"type": "text", "text": "3 Elliptical Attention: Leveraging Hyper-Ellipsoids to Pay More Attention Without Losing Focus ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we first present how NW regression obtains a lower MSE by taking hyper-ellipsoidal neighborhoods around queries. We then construct the required hyper-ellipsoidal transformation via a Mahalanobis metric. We present the framework relating robustness and representation collapse to the geometry of the query neighborhoods and show how our proposed scheme offers improvements in both areas. We then provide an efficient estimator of the coordinate-wise relevance before finally giving the full technical formulation of Elliptical Attention. Technical details on the implementation procedure are in Appendix E. ", "page_idx": 3}, {"type": "text", "text": "3.1 Improving NW Regression with Hyper-Ellipsoids ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Distance-based estimators, such as the NW estimator, can obtain a lower MSE by taking hyperellipsoidal neighborhoods around queries [29, 30]. The key idea is that we wish to stretch the axes of the underlying space in directions for which the true $f$ in Eqn. 3 varies least. ", "page_idx": 3}, {"type": "text", "text": "Figure 2 shows a situation in which $f$ does not vary equally in all directions. This is actually a limiting case in which the function is sparse in the $x_{2}$ direction. In the left sub-figure, we show the result of stretching the Euclidean circular neighborhoods around each query in the $x_{2}$ direction for which the function does not vary. The right sub-figure then shows how the resulting ellipse in the $x_{2}$ direction can include additional data points without adding additional bias into the model. It is a well-established result that the variance of non-parametric estimates at a point is inversely proportional to the number of samples in that point\u2019s neighborhood, as the additional samples smooth out the effect of noise. As a result, stretching the neighborhood, as shown in the right sub-figure, decreases the variance. Crucially, including these additional samples does not cause the estimate to miss the true variation in the function, as there is no variation in the $x_{2}$ direction. By including points in this direction, we do not introduce bias into the estimate. Hence, we lower variance without the introduction of bias, obtaining a lower MSE estimator. This intuition is formalized in Theorem 1 in Appendix C, which shows that the best achievable rate of convergence for estimators of non-sparse Lipschitz functions is of the order ${\\mathcal{O}}(n^{-2/(2+d)})$ for a $d$ dimensional feature space. However, when the function only depends on $R\\subseteq[d]$ coordinates, the rate improves to ${\\mathcal{O}}(n^{-2/(2+|R|)})$ . In the case of approximate sparsity, when coordinate directions exhibit differing variability, the same intuition carries over as shown by the improvement in convergence rates in Theorem 2 in Appendix C. ", "page_idx": 3}, {"type": "image", "img_path": "Ejg4d4FVrs/tmp/d075a3419c6f04e70aa98a1e60a1b85cbaef253a9dd8040381f0fef0a84a128a.jpg", "img_caption": ["Figure 3: Representation Collapse on WikiText-103. Elliptical Attention learns more diverse representations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We leverage this analysis from non-parametric regression to motivate our Elliptical Attention. From the regression perspective, the self-attention mechanism, which performs NW regression, is able to learn a lower MSE estimator of the true underlying $f$ by reducing the variance of the estimator without (or with minimal) introduction of bias. From the attention perspective, this means queries pay higher attention to more relevant keys, producing more contextually meaningful attention scores and better, more robust learned representations. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Capturing Coordinate-wise Variability and Building the Mahalanobis Transformation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We measure the variation in $f$ in the $i^{t h}$ coordinate direction by the expectation of the ${\\mathcal{L}}_{1}$ norm of the $i^{t h}$ directional derivative taken over all $k\\in\\mathcal{X}_{k}$ , where $\\mathcal{X}_{k}^{\\overline{{\\ }}}\\subseteq\\mathbb{R}^{D}$ denotes the feature space. Roughly speaking, this quantity corresponds to the average absolute gradient of $f$ in the $i^{t h}$ direction throughout the space. Formally, this quantity is defined as ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Coordinate-wise Variability of $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D_{v}})$ The coordinate-wise variability of $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D_{v}}$ with Jacobian matrix $\\boldsymbol{J}_{f}\\in\\mathbb{R}^{D_{v}\\times D}$ in the $i^{t h}$ direction is given by the quantity $\\|f_{i}^{\\prime}\\|_{1,\\mu}\\,:=\\,\\mathbb{E}_{k\\sim\\mu}\\|J_{f}(k)e_{i}\\|_{1},i\\,\\in\\,[D]$ , where $e_{i}$ is an all-zero vector with a single $^{\\,l}$ in the $i^{t h}$ coordinate and $\\mu$ is the marginal distribution of $k$ over support $\\scriptstyle{\\mathcal{X}}_{k}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 1 This definition is one of many possible. One could also take the supremum rather than the expectation or consider second derivatives. We select this definition as averages over first derivatives are more easily estimated and the definition still captures the intuitive properties of variability. ", "page_idx": 4}, {"type": "text", "text": "Denoting estimates of the coordinate-wise variability $\\|f_{i}^{\\prime}\\|_{1,\\mu}$ by $m_{i}$ , we can then incorporate these quantities into a distance function of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(\\pmb{q},\\pmb{k}):=\\sqrt{(\\pmb{q}-\\pmb{k})^{\\top}\\pmb{M}(\\pmb{q}-\\pmb{k})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $M=\\mathrm{diag}(m_{1},m_{2},\\dots,m_{D})$ is a diagonal matrix whose diagonal elements are the estimates of $\\|f_{i}^{\\prime}\\|_{1,\\mu}$ for $i\\in[D]$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 2 The metric described in Eqn. 7 is a form of Mahalanobis distance metric, which can be interpreted as first applying a transformation to the underlying space in which we stretch the coordinate axes by the diagonal elements of $_M$ . Therefore using this metric within the self-attention computation produces the desired hyper-ellipsoidal neighborhoods around queries. ", "page_idx": 4}, {"type": "text", "text": "Remark 3 In practice, we maxscale the estimates to obtain $m_{i}\\leftarrow m_{i}/m_{m a x}$ where $m_{m a x}\\geq m_{i}$ for all $i\\in[D]$ . This is because we care about the relative magnitudes of the direction-wise variability as opposed to the absolute magnitudes. Under this interpretation, we identify the most variable dimension and stretch all others relative to this direction. ", "page_idx": 4}, {"type": "text", "text": "3.3 Promoting Robustness and Avoiding Representation Collapse ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Before providing the technical procedure for estimating $_M$ and the full technical formulation of Elliptical Attention in Section 3.5, we first theoretically analyze in Propositions 1 and 2 how the hyper-ellipsoidal transformation in Eqn.7 improves robustness and alleviates representation collapse. ", "page_idx": 4}, {"type": "text", "text": "Dimension-wise input sensitivity of Elliptical Attention and robustness. In Lemma 1, we show that when each input component is weighted according to the Mahalanobis transformation in Eqn. 7, the impact of perturbing the $i^{t h}$ input coordinate on any coordinate of the output is proportional to the corresponding weighting parameter with proportionality coefficient depending on the indices $i$ and $j$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 1 Let $\\mathcal{M}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{N}$ denote the transformed Elliptical softmax operator for a given set of keys as $\\begin{array}{r}{\\mathcal{M}(\\boldsymbol{x})\\,:=\\,\\frac{1}{\\sum_{j\\in[N]}\\exp(\\boldsymbol{x}^{\\intercal}M k_{j})}\\left[\\exp(\\boldsymbol{x}^{\\intercal}M k_{1}),\\exp(\\boldsymbol{x}^{\\intercal}M k_{2}),\\dots,\\exp(\\boldsymbol{x}^{\\intercal}M k_{N})\\right]^{\\intercal}.}\\end{array}$ for weight matrix $_M$ as in Eqn. 7. Then, the achievable rate of change of $\\mathcal{M}(\\pmb{x})$ in $i^{t h}$ input dimension is proportional to $m_{i}$ , that is, $\\operatorname*{sup}_{\\pmb{x}\\in\\mathcal{X}}|J_{\\mathcal{M}}(\\pmb{x})_{j i}|\\propto m_{i}$ , for all $i\\in[D]$ and $j\\in[N]$ where $J_{\\mathcal{M}}$ is the Jacobian matrix of $\\mathcal{M}$ . ", "page_idx": 4}, {"type": "text", "text": "By virtue of Lemma 1, which is proven in Appendix B.1, we show in Proposition 1 that choosing the weights as properly scaled estimates of the underlying function variability, as in Elliptical Attention, the output vectors become less prone to large errors caused by noisy input while simultaneously respecting the dimension-wise variability pattern of the true self-attention function. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 (Robustness of Elliptical Attention) Let $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D_{v}}$ be the true self-attention function, $\\hat{f}_{d}$ be the Elliptical Attention estimator with metric $d$ as described in Eqn. 7. Then for any index $i\\in[N]$ and noise $\\epsilon\\in\\mathbb{R}^{D}$ , the following error bound holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\hat{f}_{d}(\\boldsymbol{q}_{i})-\\hat{f}_{d}(\\boldsymbol{q}_{i}+\\boldsymbol{\\epsilon})\\|\\leq\\left(\\sum_{j\\in[N]}\\sqrt{\\mathrm{tr}(K_{j}^{2}M^{2})}\\|\\boldsymbol{v}_{j}\\|\\right)\\|\\boldsymbol{\\epsilon}\\|\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\{K_{j}\\}_{j\\in[N]}$ are constant diagonal matrices that depend only on the key vectors. ", "page_idx": 5}, {"type": "text", "text": "Note that when the estimates are maxscaled so that $m_{i}\\leq1$ , the achievable output error of Elliptical Attention is lower than that of standard self-attention where $m_{i}=1$ for all $i\\in[D]$ . Besides, when the true function exhibits approximate sparisity in some number of dimensions (i.e. $m_{i}\\to0^{+}$ for majority of indices), the error bound in Eqn. 8 becomes significantly tighter for Elliptical Attention. The proof of Proposition 1 is provided in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "Input smoothing and representation collapse. In each layer, the standard self-attention mechanism fits a noisy estimate of the true function $f$ , which is then fed into subsequent layers and iteratively refti. The input to each attention layer is then partially composed of noise, which is equivalently the common regularization method of random input smoothing. We show that by reducing the noise component in each layer, Elliptical Attention maintains expressive power and resists representation collapse. This is formalized in the following proposition: ", "page_idx": 5}, {"type": "text", "text": "Proposition 2 (Elliptical Attention maintains expressive power by reducing noise) Let $h_{d}^{\\ell}$ denote the output of a transformer using Elliptical Attention with metric d as described in Eqn. 7 and $h^{\\ell}$ denote the output of a transformer using standard self-attention at layer $\\ell$ . Let $\\mathcal{D}$ be the sampling distribution of the data and let $c\\in\\mathbb{R}^{D}$ . Then, for any $h,h_{d}$ and layer $\\ell$ , in expectation a standard self-attention transformer attenuates towards $^c$ faster than Elliptical Attention. Formally, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\cal{D}}\\|h_{d}^{\\ell}-c\\|\\geq\\mathbb{E}_{\\cal{D}}\\|h^{\\ell}-c\\|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof is provided in Appendix B.3. Proposition 2 shows Elliptical Attention maintains better expressive power than standard self-attention. We find this empirically supported as shown in Fig 3. ", "page_idx": 5}, {"type": "text", "text": "3.4 An Efficient Estimator of the Coordinate-wise Variability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We propose a simple difference-based estimator that effectively captures the coordinate-wise variability of the underlying function. Our estimator is easily and efficiently computed. It requires no additional learnable parameters and demands negligible additional memory. Let $\\mathbb{E}_{n}$ denote empirical mean over $n$ samples, ${\\pmb v}^{\\ell}(i)$ denote the $i^{t h}$ component of the vector $\\pmb{v}$ at the $\\ell^{t h}$ layer, and ${\\mathcal{X}}_{v}^{\\ell,\\ell+1}=\\{({\\boldsymbol{v}}^{\\ell+1},{\\boldsymbol{v}}^{\\ell}):{\\boldsymbol{v}}^{\\ell}=f({\\boldsymbol{k}}^{\\ell})+\\epsilon\\}$ be the value feature space at neighboring layers $\\ell$ and $\\ell+1$ where values are generated according to the process described in Eqn. 3. Then, our approach to estimating the $i^{t h}$ coordinate-wise variability is described in the following proposition. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3 (Coordinate-wise Variability Estimator) Given a function $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D_{v}}$ with $i^{t h}$ directional variation $\\|f_{i}^{\\prime}\\|_{1,\\mu},i\\in[D]$ and some $\\delta>0$ , the directional variation can be estimated by the quantity ", "page_idx": 5}, {"type": "equation", "text": "$$\nm_{i}:=\\mathbb{E}_{n}\\underline{{\\mathbb{1}}}_{s}1\\underline{{\\mathbf{\\eta}}}_{\\delta}^{\\ell+1}(i)-\\mathbf{\\eta}^{v^{\\ell}(i)|}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 4 For the purposes of improving the performance of transformers by stretching the feature space according to the direction-wise variability of $f$ , we note that consistent estimators of $\\|f_{i}^{\\prime}\\|_{1,\\mu}$ for all $i\\in[D]$ are sufficient but not necessary. Instead, we require only the weaker objective of accurately estimating the relative magnitudes of the direction-wise variability. That is, $i f\\|f_{i}^{\\prime}\\|_{1,\\mu_{..}}\\geq\\|f_{j_{.}}^{\\prime}\\|_{1,\\mu},$ , we need only that $m_{i}\\geq m_{j}$ . This is because the theory requires us only to identify coordinate directions of more or less variability and shrink or stretch the space accordingly. ", "page_idx": 5}, {"type": "text", "text": "The intuition behind our estimator in Eqn. 10 lies in prior lines of research studying transformers as an Euler discretization of a continuous-time dynamic, usually as a system of first-order ordinary differential equations (ODEs) [40, 21, 53]. In fact, our estimator resembles the absolute value of a forward Euler discretization of the variability of the $i^{t h}$ component of a value vector over time $\\partial{\\pmb v}(t,t)/\\partial t$ , where the layers $\\ell$ and $\\ell\\!+\\!1$ represent consecutive time points in an interval partition with the step size $\\delta$ . We prove that our estimator in Eqn. 10 effectively estimates the relative magnitudes of the coordinate-wise variability of $f$ in Appendix B.5. ", "page_idx": 5}, {"type": "text", "text": "3.5 Full Technical Formulation of Elliptical Attention ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present the full formulation of Elliptical Attention. Given the distance function $d(\\cdot,\\cdot)$ as in Eqn. 7, where $M=\\operatorname{diag}(m_{1},\\dots,m_{D})$ is a diagonal matrix with elements $m_{i}$ as in Prop. 3, the $_M$ -norm can be defined as $\\Vert x\\Vert_{M}:=\\sqrt{\\mathbf{x}^{T}M x}$ , which produces hyper-ellipsoidal stretching in the feature space. Then, Elliptical Attention is defined as follows. ", "page_idx": 6}, {"type": "text", "text": "Definition 2 (Elliptical Attention Computation) Let $\\varphi_{d,\\sigma}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$ denote the Gaussian density kernel with variance $\\sigma^{2}I$ equipped with the $_M$ -norm as defined above. Then the corresponding NW estimator at $\\mathbf{{q}}_{i}$ becomes ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{f}_{d,D}(\\pmb{q}_{i}):=\\frac{\\sum_{j\\in[N]}\\pmb{v}_{j}\\exp\\left(-\\|\\pmb{q}_{i}-\\pmb{k}_{j}\\|_{M}^{2}/2\\sigma^{2}\\right)}{\\sum_{j\\in[N]}\\exp\\left(-\\|\\pmb{q}_{i}-\\pmb{k}_{j}\\|_{M}^{2}/2\\sigma^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, the Elliptical Attention output for the $i^{t h}$ que\u221ary $\\pmb q_{i}$ given keys $\\{k_{i}\\}_{i=1}^{N}$ and values $\\{{\\pmb v}_{i}\\}_{i=1}^{N}$ corresponding to the NW estimator $(I I)$ with $\\sigma^{2}=\\sqrt{D}$ is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{i}=\\sum_{j\\in[N]}\\frac{\\mathrm{exp}\\Big(q_{i}^{\\top}M k_{j}/\\sqrt{D}\\Big)v_{j}}{\\sum_{j\\in[N]}\\mathrm{exp}\\Big(q_{i}^{\\top}M k_{j}/\\sqrt{D}\\Big)}=\\sum_{j\\in[N]}\\mathrm{softmax}(q_{i}^{\\top}M k_{j}/\\sqrt{D})v_{j},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $M=\\mathrm{diag}(m_{1},\\dots,m_{D})$ with $m_{i}$ defined as Eqn. 10 for all $i\\in[D]$ . ", "page_idx": 6}, {"type": "text", "text": "Eqn. 12 is equivalently expressed in matrix form as ", "page_idx": 6}, {"type": "equation", "text": "$$\nH=\\mathrm{softmax}\\left(\\frac{Q M K^{\\top}}{\\sqrt{D}}\\right)V.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 5 We see from the form of Eqns. 12, 13 that standard self-attention is recoverable by setting $M=I_{D}$ . Under our framework, this implies that standard self-attention assumes the underlying regression function to have exactly equal variability in all coordinate directions. ", "page_idx": 6}, {"type": "text", "text": "Pseudocode for the Elliptical Attention computation is provided in Appendix F.12. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we empirically justify the advantage of Elliptical Attention over baseline transformers that take hyper-spheres around queries. We evaluate our method on robust Wikitext-103 modeling under Word Swap contamination [45], ImageNet classification under a wide range of attacks [14, 67], the LRA benchmark [74], and ADE20K image segmentation [87]. We compare Elliptical Attention with state-of-the-art (SOTA) clean and robust models, including Performer [9], FourierFormer [54], Robust Vision Transformer [44], Fully Attentional Network (FAN) [89], Mixture of Gaussian Keys (MGK) [52], Mixture-of-Expert (MoE) based transformers, such as Switch transformer [18] and Generalist Langauge Model (GLaM) [17], and robust kernel density estimation (KDE) based transformers, such as Median of Means (MoM) and Scaled Projected KDE (SPKDE) [23]. We aim to show that i) Elliptical Attention offers substantive improvements over baseline models across tasks on both clean and contaminated data; ii) Elliptical Attention attains these improvements on contaminated data while reducing memory requirements and increasing computational speed compared to comparative robust models; iii) Elliptical Attention can be combined with SOTA robust transformers to further improve robustness with negligible increase in computational overhead. We compare Elliptical Attention with baselines of the same configuration. Results are averaged over 5 runs with different seeds. Additional results and full details on experimental setup are in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "4.1 Robust Language Modelling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental setup. We adopt the experimental setup in [54, 23]. We pretrain and evaluate our models on the WikiText-103 benchmark in comparison with the standard baseline Transformer [82], Performer [9], Transformer-MGK [52], FourierFormer [54], and the robust kernel density estimationbased Transformers including Transformer-SPKDE and Transformer-MoM [23]. All models use the 44M-parameter Transformer backbone. We pretrain all models on clean data for 125 epochs before attacking only the test set using a Word Swap Attack, which substitues random words with a generic \u2018AAA\u2019 token at a $2.5\\%$ swap rate. We report test perplexity (PPL) as the performance metric. ", "page_idx": 6}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/5edf15b799730efb639ea238dd78c01e2e4903c5dd65040b2d36f73fbeb5df6a.jpg", "table_caption": ["Table 1: Perplexity (PPL) on WikiText-103 under Word Swap contamination. Elliptical achieves top PPL in clean data and second best in contaminated. Best result in bold and second best underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/731450e7631c2972493e5f10d33bae1605e4e4f1cf61d7846a339ced7c802c00.jpg", "table_caption": ["Table 2: Top-1 and Top-5 Test accuracy on ImageNet under adversarial attacks PGD, FGSM, and SPSA with perturbation budget 1/255. Best result shown in bold and second best shown underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results. Table 1 shows our Elliptical Transformer (Elliptical) achieves top test perplexity in clean data while also achieving second top test perplexity under data contamination by Word Swap [47], illustrating that the Elliptical Attention is highly robust and offers substantial advantages on clean data as well. ", "page_idx": 7}, {"type": "text", "text": "4.2 Image Classification under Adversarial Attack ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental setup. We adopt the experimental setup in [23]. We train and evaluate Elliptical on ImageNet-1K against standard vision transformers, including DeiT [78] and Distill [78], as well as the FourierFormer [54]. We also compare Elliptical with robust vision transformers, including DeiT-KDE [23], DeiT-MoM [23], RVT [44], and FAN [89]. The DeiT backbone is the tiny configuration of 5.7M parameters. We train all models on clean ImageNet-1K for 300 epochs before evaluating their top-1 and top-5 accuracy on the test dataset under fast gradient sign method (FGSM) [22], projected gradient descent (PGD) [42], and simultaneous perturbation stochastic approximation (SPSA) [81]. We also present results for performance against Auto Attack [11], which is an ensemble of auto PGD-Cross Entropy (APGD-CE), auto PGD-targeted (APGD-T), fast adaptive boundary-targeted (FAB-T), and Square. We display results for attacks individually and in default sequential mode. ", "page_idx": 7}, {"type": "text", "text": "Results. Table 2 shows Elliptical attains top robustness in PGD and SPSA and second top in FGSM while achieving highly competitive clean accuracy. DeiT-Elliptical is particularly impressive under black box attack SPSA, improving over the next best model, RVT, [44], by $10\\%$ . Table 4 shows results on Auto Attack [11], where we see DeiT-Elliptical substantially outperforms standard DeiT in each attack individually and sequentially. We again see strong performance against black box attack Square with an $8.5\\%$ improvement. When combining with SOTA robust transformer, $F A N$ [89], Elliptical Attention improves robustness to sequential Auto Attack and all individual attacks except FAB-T, for which it still remains highly competitive. This shows Elliptical Attention can further boost robustness when combined with SOTA robust models. ", "page_idx": 7}, {"type": "text", "text": "4.3 Long Sequence Modelling on the LRA Benchmark ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental setup. We adopt the setup in [7]. For each of the 5 tasks, equation calculation (ListOps) [50], review classification (Text) [41], document retrieval (Retrieval) [61], image classification (Image) [32], and image spatial dependencies (Pathfinder) [37], we compare Elliptical with standard Transformer [82], Linformer [26], Reformer [28], Performer [9], and Longformer [3]. ", "page_idx": 7}, {"type": "text", "text": "Results. Elliptical Attention achieves top or second top test accuracy in every task and top overall performance. This shows Elliptical Attention learns superior representations across a wide range of modalities in long-range contexts. ", "page_idx": 7}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/3813b0b0d45d3b2f0a88e817606e053d2f00e14cc67d90f9e47b8168c7deda04.jpg", "table_caption": ["Table 3: Test accuracy on long range tasks: ListOps, Text, Retrieval, Image, and Pathfinder. Best result in bold and second best underlined. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/4a86c516f1a0aab897f01316d6862ce028a8e357dc664009fc19ff4144a83c9e.jpg", "table_caption": ["Table 4: Top-1 and Top-5 Test accuracy on ImageNet under Auto Attack applied both individually and sequentially with perturbation budget 1/255. Best result is shown in bold. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/e469afffac6317b7ceef8e963241debb390b58f16aba8341650ec5bf41789cec.jpg", "table_caption": ["Table 5: Switch Transformer Language Modeling "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/75d459a1ad0dab35db975bdf641ef9338e8ad6e9668be99cd16bd72c93d95c02.jpg", "table_caption": ["Table 6: GLaM Language Modeling "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Image Segmentation on ADE20K ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experimental setup. We adopt the setup in [71]. The encoder is pretrained on ImageNet-1K following the same specification described in 4.2. In particular, the encoder is a DeiT-tiny backbone of 5.7M parameters pretrained for 300 epochs. After pretraining, we then attach a decoder that contains 2-layer masked transformer and finetune the full encoder-decoder model for 64 epochs on the ADE20K [88] image segmentation dataset. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 7 reports pixel accuracy, mean accuracy, and mean intersection over union (IOU). Elliptical Attention boosts performance across all metrics, with intersection over union, in particular, improving by a substantive $4.7\\%$ . ", "page_idx": 8}, {"type": "text", "text": "4.5 Further Clean Data Language Modelling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experimental setup. For experiments using Switch Transformer [18] and GLaM [17] backbones, we adopt the setup in [60]. In particular, we integrate Elliptical into small (70M parameters) and medium (220M parameters) GlaM backbones and train the models on WikiText-103 for 80 and 120 epochs, respectively. We consider Switch backbones at medium (220M parameters) and large (388M parameters) configurations, both trained for 80 epochs. All models use top-2 expert routing. For the standard transformer experiments, we continue with the setup of [54] and additionally present results for Elliptical in a medium configuration with 90M parameters trained for 100 epochs. ", "page_idx": 8}, {"type": "text", "text": "Results. We present in Tables 5 and 6 the performance of Elliptical in MoE backbones. We see moving from smaller to larger configurations, Elliptical maintains strong, consistent improvements in test PPL. We note particularly substantive improvements with scale in the GLaM backbone, where at the small configuration Elliptical attains a $2.7\\%$ improvement, but at the medium configuration this performance improvement almost doubles to $5.0\\%$ . Table 8 further shows that in the standard transformer backbone, Elliptical maintains its substantive $6.8\\%$ improvement when scaling up to a larger configuration. These results show that Elliptical Attention scales well with model size. ", "page_idx": 8}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/c8551393e407952b19cb541a0c713c53b6a9fe6fbb752b2abee2d6a50d133646.jpg", "table_caption": ["Table 7: Image Segmentation Results "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/0248bc3843e6614e694ea1362a4fe73f67b0517174c146a1e0c37ef2bf756a49.jpg", "table_caption": ["Table 8: Wikitext-103 Results "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Theoretical Frameworks for Attention. Attention mechanisms have been studied from a range of perspectives. [80] shows that self-attention can be derived from kernel similarity functions, and [77] points out that self-attention projects its query vectors onto the principal component axes of its key matrix in a feature space. [56] formulates self-attention as the support vector expansion derived from a support vector regression problem, while [73] explains attention through nonlinear singular value decomposition of asymmetric kernels. Attention has also been explained through ordinary/partial differential equations, Gaussian mixture models, and graph-structured learning [40, 68, 51, 72, 20, 52, 31, 86]. [54, 23] show that self-attention performs Nadaraya-Watson regression with Gaussian isotropic kernels. This paper leverages this viewpoint and proposes modifying the Gaussian isotropic kernels to include a Mahalanobis metric which can be interpreted as stretching the hyper-spherical neighborhoods of the kernel to hyper-ellipsoids. ", "page_idx": 9}, {"type": "text", "text": "Robust Transformers. In vision, [43] proposes an ensemble defense strategy to white-box attacks while [44] proposes position-aware attention scaling and patch-wise augmentation. Recently, [89] proposes a fully-attentional network to attain state-of-the-art accuracy on corrupted image data. In language, [85] proposes structurally aware table-text encoding, [38] proposes a robust end-to-end transformer for crisis detection, and [33] proposes duration-based hard attention. [6, 4] integrate a Gaussian process into attention for out-of-distribution detection, and [79] develops equivariant neural functional networks for transformers. These methodologies are motivated by their respective domain and tend to have limited generalizability to differing domains. Our approach, by contrast, proposes a general framework that makes no assumption on the downstream task and requires no additional parameters and negligible computational overhead. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "Ejg4d4FVrs/tmp/48e488632a09abeab259c809c70622fdb8e5b7596a8efbc7f1f847e88c73c551.jpg", "img_caption": ["Figure 4: ImageNet Efficiency: Comparison of throughput and max memory allocated for DeiT, Elliptical, RVT, RKDE, MoM on Tiny, Small, and Base sizes. Elliptical is the most efficient robust model. Numerical analysis in Table 12 of Appendix F. ", "AverageComputationSpeed(iteration/second) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Mahalanobis Metrics. Mahalanobis metrics have been used predominantly in classical machine learning algorithms. In nearest-neighbor (NN) classification and regression, [84, 49] learn the metric through backpropagation. In NN KL divergence estimation, [58] learns a Mahalanobis metric from density approximation. In kernel regression, [57] takes eigenvalues of the estimated Jacobian while [29, 30] estimate coordinate-wise variability of the true function. Our model similarly uses coordinate-wise variability of the unknown function to form the Mahalanobis transformation but instead uses a more efficient estimator that does not require materializing the prediction function and accommodates the self-attention setting. In general, our method is among the early work in incorporating Mahalanobis metrics into the self-attention mechanism. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present Elliptical Attention, a novel variant of attention that computes a Mahalanobis transformation to stretch the underlying feature space in directions of high contextual relevance. This transformation can be interpreted as modifying the hyper-spherical neighborhoods around queries to hyper-ellipsoids which upweight the attention paid to keys lying in important directions, enabling the transformer to learn better and more robust representations. This approach makes no assumptions on the downstream task, requires no learnable parameters, and can be applied to any transformer to boost clean and robust performance. A limitation of our work is that we use the values over layers to estimate the average direction-wise gradient of the true self-attention function, which makes the estimate prone to noise. For ongoing work, we are exploring more precise estimation methods with provable convergence guarantees that do not compromise efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research / project is supported by the National Research Foundation Singapore under the AI Singapore Programme (AISG Award No: AISG2-TC-2023-012-SGIL). This research / project is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2023) (A-8002040-00-00, A-8002039-00-00). This research / project is also supported by the NUS Presidential Young Professorship Award (A-0009807-01-00). ", "page_idx": 10}, {"type": "text", "text": "Thanks to our anonymous reviewers, who provided valuable feedback which improved the paper substantially. Thanks also to Thai Ha for the many illuminating conversations. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 3159\u20133166, 2019.   \n[2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. [3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.   \n[4] Long Minh Bui, Tho Tran Huu, Duy Dinh, Tan Minh Nguyen, and Trong Nghia Hoang. Revisiting kernel attention with correlated gaussian process representation. In The 40th Conference on Uncertainty in Artificial Intelligence, 2024. [5] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.   \n[6] Wenlong Chen and Yingzhen Li. Calibrating transformers via sparse gaussian processes. In The Eleventh International Conference on Learning Representations, 2023.   \n[7] Yingyi Chen, Qinghua Tao, Francesco Tonin, and Johan Suykens. Primal-attention: Selfattention through asymmetric kernel svd in primal representation. Advances in Neural Information Processing Systems, 36, 2024. [8] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder\u2013 decoder for statistical machine translation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar, October 2014. Association for Computational Linguistics. [9] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021.   \n[10] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1310\u20131320. PMLR, 09\u201315 Jun 2019.   \n[11] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pages 2206\u20132216. PMLR, 2020.   \n[12] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978\u20132988, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[13] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019.   \n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.   \n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[17] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547\u20135569. PMLR, 2022.   \n[18] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1\u201339, 2022.   \n[19] Chris D Frost and Simon G Thompson. Correcting for regression dilution bias: comparison of methods for a single predictor variable. Journal of the Royal Statistical Society: Series A (Statistics in Society), 163, 2000.   \n[20] Prasad Gabbur, Manjot Bilkhu, and Javier Movellan. Probabilistic attention for interactive segmentation. Advances in Neural Information Processing Systems, 34:4448\u20134460, 2021.   \n[21] Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. The emergence of clusters in self-attention dynamics. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[22] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.   \n[23] Xing Han, Tongzheng Ren, Tan Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Designing robust transformers using robust kernel density estimation. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Hang Xu Han Shi, JIAHUI GAO, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, and James Kwok. Revisiting over-smoothing in BERT from the perspective of graph. In International Conference on Learning Representations, 2022.   \n[25] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273\u2013 1286, 2021.   \n[26] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020.   \n[27] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1\u201341, 2022.   \n[28] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.   \n[29] Samory Kpotufe and Abdeslam Boularias. Gradient weights help nonparametric regressors. In Advances in Neural Information Processing Systems, volume 25, 2012.   \n[30] Samory Kpotufe, Abdeslam Boularias, Thomas Schultz, and Kyoungok Kim. Gradients weights improve regression and classification. Journal of Machine Learning Research, 17(22):1\u201334, 2016.   \n[31] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L\u00e9tourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:21618\u201321629, 2021.   \n[32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[33] Naihan Li, Yanqing Liu, Yu Wu, Shujie Liu, Sheng Zhao, and Ming Liu. Robutrans: A robust transformer-based text-to-speech model. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8228\u20138235, 2020.   \n[34] Hua Liang, Wolfgang H\u00e4rdle, and Raymond J. Carroll. Estimation in a semiparametric partially linear errors-in-variables model. Annals of Statistics, 27(5):1519\u20131535, Oct 1999.   \n[35] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI open, 3:111\u2013132, 2022.   \n[36] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. In International Conference on Learning Representations, 2017.   \n[37] Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. Advances in neural information processing systems, 31, 2018.   \n[38] Junhua Liu, Trisha Singhal, Lucienne TM Blessing, Kristin L Wood, and Kwan Hui Lim. Crisisbert: a robust transformer for crisis classification and contextual crisis embedding. In Proceedings of the 32nd ACM conference on hypertext and social media, pages 133\u2013141, 2021.   \n[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[40] Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie yan Liu. Understanding and improving transformer from a multi-particle dynamic system point of view. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations, 2019.   \n[41] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew $\\textrm{Y N g}$ , and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142\u2013150, 2011.   \n[42] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[43] Kaleel Mahmood, Rigel Mahmood, and Marten Van Dijk. On the robustness of vision transformers to adversarial examples. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7838\u20137847, 2021.   \n[44] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision transformer. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 12042\u201312051, 2022.   \n[45] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017.   \n[46] Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higher-order certification for randomized smoothing. In Advances in Neural Information Processing Systems, 2020.   \n[47] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119\u2013126, Online, October 2020. Association for Computational Linguistics.   \n[48] Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):141\u2013142, 1964.   \n[49] Youssef Nader, Leon Sixt, and Tim Landgraf. Dnnr: Differential nearest neighbors regression. In International Conference on Machine Learning, pages 16296\u201316317. PMLR, 2022.   \n[50] Nikita Nangia and Samuel Bowman. ListOps: A diagnostic dataset for latent tree learning. In Silvio Ricardo Cordeiro, Shereen Oraby, Umashanthi Pavalanathan, and Kyeongmin Rim, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 92\u201399, New Orleans, Louisiana, USA, June 2018. Association for Computational Linguistics.   \n[51] Tam Nguyen, Tan Nguyen, and Richard Baraniuk. Mitigating over-smoothing in transformers via regularized nonlocal functionals. Advances in Neural Information Processing Systems, 36:80233\u201380256, 2023.   \n[52] Tam Minh Nguyen, Tan Minh Nguyen, Dung DD Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard Baraniuk, Nhat Ho, and Stanley Osher. Improving transformers with probabilistic attention keys. In International Conference on Machine Learning, pages 16595\u201316621. PMLR, 2022.   \n[53] Tam Minh Nguyen, Cesar A Uribe, Tan Minh Nguyen, and Richard Baraniuk. PIDformer: Transformer meets control theory. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 37776\u2013 37797. PMLR, 21\u201327 Jul 2024.   \n[54] Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho. Fourierformer: Transformer meets generalized fourier integral theorem. Advances in Neural Information Processing Systems, 35:29319\u201329335, 2022.   \n[55] Tan Minh Nguyen, Tam Minh Nguyen, Hai Ngoc Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Nguyen Duy Khuong, Nhat Ho, and Stanley Osher. Improving transformer with an admixture of attention heads. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[56] Tan Minh Nguyen, Tam Minh Nguyen, Nhat Ho, Andrea L. Bertozzi, Richard Baraniuk, and Stanley Osher. A primal-dual framework for transformers and neural networks. In The Eleventh International Conference on Learning Representations, 2023.   \n[57] Yung-Kyun Noh, Masashi Sugiyama, Kee-Eung Kim, Frank Park, and Daniel D Lee. Generative local metric learning for kernel regression. Advances in neural information processing systems, 30, 2017.   \n[58] Yung-Kyun Noh, Masashi Sugiyama, Song Liu, Marthinus C Plessis, Frank Chongwoo Park, and Daniel D Lee. Bias reduction and metric learning for nearest-neighbor estimation of kullback-leibler divergence. In Artificial Intelligence and Statistics, pages 669\u2013677. PMLR, 2014.   \n[59] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249\u20132255, Austin, Texas, November 2016. Association for Computational Linguistics.   \n[60] Quang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, et al. Competesmoe\u2013effective training of sparse mixture of experts via competition. arXiv preprint arXiv:2402.02526, 2024.   \n[61] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 47:919\u2013944, 2013.   \n[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[63] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \n[66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821\u20138831. Pmlr, 2021.   \n[67] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \n[68] Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr\u00e9. Sinkformers: Transformers with doubly stochastic attention. In International Conference on Artificial Intelligence and Statistics, pages 3515\u20133530. PMLR, 2022.   \n[69] J. H. Sepanski, R. Knickerbocker, and R. J. Carroll. A semiparametric correction for attenuation. Journal of the American Statistical Association, 89(428):1366\u20131373, 1994.   \n[70] Charles J Stone. Optimal global rates of convergence for nonparametric regression. The annals of statistics, pages 1040\u20131053, 1982.   \n[71] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7262\u20137272, 2021.   \n[72] Binh Tang and David S Matteson. Probabilistic transformer for time series analysis. Advances in Neural Information Processing Systems, 34:23592\u201323608, 2021.   \n[73] Qinghua Tao, Francesco Tonin, Panagiotis Patrinos, and Johan A. K. Suykens. Nonlinear svd with asymmetric kernels: feature learning and asymmetric nystr\u00f6m method. CoRR, abs/2306.07040, 2023.   \n[74] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021.   \n[75] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1\u201328, 2022.   \n[76] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u20134601, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[77] Rachel Teo and Tan Nguyen. Unveiling the hidden structure of self-attention via kernel principal component analysis. Advances in Neural Information Processing Systems, 2024.   \n[78] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.   \n[79] Viet-Hoang Tran, Thieu N Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, and Tan Minh Nguyen. Equivariant neural functional networks for transformers. arXiv preprint arXiv:2410.04209, 2024.   \n[80] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer\u2019s attention via the lens of kernel. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4344\u20134353, Hong Kong, China, November 2019. Association for Computational Linguistics.   \n[81] Jonathan Uesato, Brendan O\u2019donoghue, Pushmeet Kohli, and Aaron Oord. Adversarial risk and the dangers of evaluating against weak attacks. In International Conference on Machine Learning, pages 5025\u20135034. PMLR, 2018.   \n[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[83] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language model. In Tal Linzen, Grzegorz Chrupa\u0142a, Yonatan Belinkov, and Dieuwke Hupkes, editors, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63\u201376, Florence, Italy, August 2019. Association for Computational Linguistics.   \n[84] Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neighbor classification. Journal of machine learning research, 10(2), 2009.   \n[85] Jingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, and Shachi Paul. Tableformer: Robust transformer modeling for table-text encoding. arXiv preprint arXiv:2203.00274, 2022.   \n[86] Shaolei Zhang and Yang Feng. Modeling concentrated cross-attention for neural machine translation with Gaussian mixture model. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1401\u20131411, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[87] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633\u2013641, 2017.   \n[88] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2019.   \n[89] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar, Jiashi Feng, and Jose M Alvarez. Understanding the robustness in vision transformers. In International Conference on Machine Learning, pages 27378\u201327394. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Supplement to \u201cElliptical Attention\u201d ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Full Derivation of Self-Attention as Non-Parametric Regression 17 ", "page_idx": 16}, {"type": "text", "text": "B Technical Proofs 18 ", "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Lemma 1 . 18   \nB.2 Proof of Proposition 1 . 19   \nB.3 Proof of Proposition 2 . . . 21   \nB.4 Edge-preservation Perspective on Representation Collapse 22   \nB.5 Proof of Proposition 3 . . 23   \nB.6 Lipschitz smoothness in $(\\mathcal{X},d)$ . 25 ", "page_idx": 16}, {"type": "text", "text": "C Additional Theorems 26 ", "page_idx": 16}, {"type": "text", "text": "D A Consistent Estimator 27 ", "page_idx": 16}, {"type": "text", "text": "E Implementation Procedure and Computational Efficiency 28 ", "page_idx": 16}, {"type": "text", "text": "F Experimental Details and Additional Experiments 28 ", "page_idx": 16}, {"type": "text", "text": "F.1 Out-of-Distribution Robustness and Data Corruption on ImageNet-A,R,C 28   \nF.2 Representation Collapse . . . . 28   \nF.3 Head Redundancy . . . . 29   \nF.4 Efficiency Results . . . 29   \nF.5 Elliptical Attention in Mixture of Expert Architectures . . 29   \nF.6 Additional Adversarial Attack Results on DeiT-Small Configuration . . . 30   \nF.7 Wikitext-103 Language Modelling and Word Swap Attack . . 30   \nF.8 ImageNet Image Classification and Adversarial Attack . . . 31   \nF.9 LRA Long Sequence Classification. . . . . 31   \nF.10 ADE20K Image Segmentation . . . . 32   \nF.11 Ablation Studies . . . 33   \nF.12 Pseudocode . . 33 ", "page_idx": 16}, {"type": "text", "text": "G Broader Impacts 34 ", "page_idx": 16}, {"type": "text", "text": "A Full Derivation of Self-Attention as Non-Parametric Regression ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall NW estimator is a non-parametric estimator of the unknown $f$ at any given query $\\pmb q$ described by ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\pmb{k})=\\mathbb{E}[\\pmb{v}|\\pmb{k}]=\\int_{\\mathbb{R}^{D}}\\pmb{v}\\cdot\\pmb{p}(\\pmb{v}|\\pmb{k})d\\pmb{v}=\\int_{\\mathbb{R}^{D}}\\frac{\\pmb{v}\\cdot\\pmb{p}(\\pmb{v},\\pmb{k})}{p(\\pmb{k})}d\\pmb{v},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first equality comes from the noise being zero mean, the second equality comes from the definition of conditional expectation and the final equality comes from the definition of conditional density. Eqn. 3 implies that if we can just obtain good estimates of the joint density $p(v,k)$ and marginal density $p(k)$ then we can estimate the required $f({\\pmb q})$ . The Gaussian isotropic kernels with ", "page_idx": 16}, {"type": "text", "text": "bandwidth $\\sigma$ are given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{p}_{\\sigma}(v,k)=\\frac{1}{N}\\sum_{j\\in[N]}\\varphi_{\\sigma}(v-v_{j})\\varphi_{\\sigma}(k-k_{j}),\\;\\;\\hat{p}_{\\sigma}(k)=\\frac{1}{N}\\sum_{j\\in[N]}\\varphi_{\\sigma}(k-k_{j}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\varphi_{\\sigma}$ is the multivariate Gaussian density function with diagonal covariance matrix ${\\sigma^{2}}\\pmb{I}_{D}$ . Given the kernel density estimators in Eqn. 14, the unknown function can be estimated as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{f}_{\\sigma}(\\pmb{k})=\\int_{\\mathbb{R}^{D}}\\frac{\\pmb{v}\\cdot\\hat{p}_{\\sigma}(\\pmb{v},\\pmb{k})}{\\hat{p}_{\\sigma}(\\pmb{k})}\\,d\\pmb{v}=\\int_{\\mathbb{R}^{D}}\\frac{\\pmb{v}\\cdot\\sum_{j\\in[N]}\\varphi_{\\sigma}(\\pmb{v}-\\pmb{v}_{j})\\varphi_{\\sigma}(\\pmb{k}-\\pmb{k}_{j})}{\\sum_{j\\in[N]}\\varphi_{\\sigma}(\\pmb{k}-\\pmb{k}_{j})}\\,d\\pmb{v}}\\\\ {=\\frac{\\sum_{j\\in[N]}\\varphi_{\\sigma}(\\pmb{k}-\\pmb{k}_{j})\\int\\pmb{v}\\cdot\\varphi_{\\sigma}(\\pmb{v}-\\pmb{v}_{j})d\\pmb{v}}{\\sum_{j\\in[N]}\\varphi_{\\sigma}(\\pmb{k}-\\pmb{k}_{j})}=\\frac{\\sum_{j\\in[N]}\\pmb{v}_{j}\\varphi_{\\sigma}(\\pmb{k}-\\pmb{k}_{j})}{\\sum_{j\\in[N]}\\varphi_{\\sigma}(\\pmb{k}-\\pmb{k}_{j})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, using the definition of the Gaussian isotropic kernel and evaluating the estimated function at $\\pmb q_{i}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(\\pmb{q}_{i})=\\frac{\\sum_{j}^{N}v_{j}\\exp\\left(-\\|\\pmb{q}_{i}-\\pmb{k}_{j}\\|^{2}/2\\sigma^{2}\\right)}{\\sum_{j}^{N}\\exp\\left(-\\|\\pmb{q}_{i}-\\pmb{k}_{j}\\|^{2}/2\\sigma^{2}\\right)}}\\\\ &{\\quad\\quad=\\frac{\\sum_{j}^{N}v_{j}\\exp\\big[-(\\|\\pmb{q}_{i}\\|^{2}+\\|\\pmb{k}_{j}\\|^{2})/2\\sigma^{2}\\big]\\exp(\\pmb{q}_{i}^{\\top}\\pmb{k}_{j}/\\sigma^{2})}{\\sum_{j}^{N}\\exp\\big[-(\\|\\pmb{q}_{i}\\|^{2}+\\|\\pmb{k}_{j}\\|^{2})/2\\sigma^{2}\\big]\\exp(\\pmb{q}_{i}^{\\top}\\pmb{k}_{j}/\\sigma^{2})}}\\\\ &{\\quad\\quad=\\frac{\\sum_{j}^{N}v_{j}\\exp(\\pmb{q}_{i}^{\\top}\\pmb{k}_{j}/\\sigma^{2})}{\\sum_{j}^{N}\\exp(\\pmb{q}_{i}^{\\top}\\pmb{k}_{j}/\\sigma^{2})}=\\sum_{j\\in[N]}\\mathrm{softmax}(\\pmb{q}_{i}^{\\top}\\pmb{k}_{j}/\\sigma^{2})v_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Remark 6 Note that relaxing the assumption of normalized keys, the standard unnormalized selfattention score can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\exp(q_{i}^{\\top}k_{j}/\\sigma^{2})=\\exp\\left(-\\|q_{i}-k_{j}\\|^{2}/2\\sigma^{2}\\right)\\exp\\left((\\|q_{i}\\|^{2}+\\|k_{j}\\|^{2})/2\\sigma^{2}\\right)}\\\\ {\\propto\\exp\\left(-\\|q_{i}-k_{j}\\|^{2}/2\\sigma^{2}\\right),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which shows that the dot-product self-attention scores are proportional to the NW kernel value with Euclidean distance. Hence the assumption of key normalization is sufficient to recover exactly the correspondence between self-attention and NW kernel regression, but not necessary. Analogously, the unnormalized Elliptical Attention score takes the following form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp(q_{i}^{\\top}M k_{j}/\\sigma^{2})=\\exp\\left(-d(q_{i},k_{j})^{2}/2\\sigma^{2}\\right)\\exp\\left((\\|q_{i}\\|_{M}^{2}+\\|k_{j}\\|_{M}^{2})/2\\sigma^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\propto\\exp\\left(-d(q_{i},k_{j})^{2}/2\\sigma^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d(\\cdot,\\cdot)$ is the Mahalanobis distance used in Eqn. 7 and $\\|\\cdot\\|_{M}$ is the norm in the transformed space with metric $d$ . This observation justifies the use of the transformed dot product instead of the full Mahalanobis distance metric in Eqn. 12 as it preserves the proportionality relationship between the attention computation and the corresponding nonparametric regression estimator with chosen distance metric. ", "page_idx": 17}, {"type": "text", "text": "B Technical Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present the omitted theorem statements and technical proofs in the main body of the paper. ", "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\mathcal{M}:\\mathbb{R}^{D}\\to\\mathbb{R}^{N}$ be the transformed softmax operator as defined in Lemma 1. We wish to find its Jacobian matrix given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{\\mathcal{M}}(\\pmb{q})=\\left[\\begin{array}{c c c c}{\\frac{\\partial\\mathcal{M}_{1}(\\pmb{q})}{\\partial q^{1}}}&{\\frac{\\partial\\mathcal{M}_{1}(\\pmb{q})}{\\partial q^{2}}}&{...}&{\\frac{\\partial\\mathcal{M}_{1}(\\pmb{q})}{\\partial q^{D}}}\\\\ {\\frac{\\partial\\mathcal{M}_{2}(\\pmb{q})}{\\partial q^{1}}}&{\\frac{\\partial\\mathcal{M}_{2}(\\pmb{q})}{\\partial q^{2}}}&{...}&{\\frac{\\partial\\mathcal{M}_{2}(\\pmb{q})}{\\partial q^{D}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{\\partial\\mathcal{M}_{N}(\\pmb{q})}{\\partial q^{1}}}&{\\frac{\\partial\\mathcal{M}_{N}(\\pmb{q})}{\\partial q^{2}}}&{...}&{\\frac{\\partial\\mathcal{M}_{N}(\\pmb{q})}{\\partial q^{D}}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "to measure the sensitivity of each output dimension to a change in each input dimension. Let $\\mathcal{M}_{j}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$ denote the $j^{t h}$ component of the output vector for $j\\,\\in\\,[N]$ , that is, for a vector $\\pmb q\\in\\mathbb{R}^{D}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{M}_{j}(\\pmb{q})=\\frac{\\exp(\\pmb{q}^{\\top}M\\pmb{k}_{j})}{\\sum_{s\\in[N]}\\exp(\\pmb{q}^{\\top}M\\pmb{k}_{s})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $q^{i}$ and $k_{j}^{i}$ denote the $i^{t h}$ coordinates of vectors $\\pmb q$ and $k_{j}$ , respectively. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial q^{i}}\\ln({M_{j}(q)})=\\frac{\\partial}{\\partial q^{i}}\\left({q^{\\top}M k_{j}-\\ln\\left(\\displaystyle\\sum_{s\\in[N]}\\exp(q^{\\top}M k_{s})\\right)}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=m_{i}k_{j}^{i}-\\displaystyle\\sum_{s\\in[N]}\\frac{\\partial}{\\partial q^{i}}\\exp(q^{\\top}M k_{s})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=m_{i}k_{j}^{i}-m_{i}\\displaystyle\\sum_{s\\in[N]}\\frac{k_{s}^{i}\\exp(q^{\\top}M k_{s})}{\\sum_{s^{i}\\in[N]}\\exp(q^{\\top}M k_{s^{i}})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=m_{i}\\left(\\displaystyle k_{j}^{i}-\\sum_{s\\in[N]}k_{s}^{i}{M_{s}}(q)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since the output of Eqn. 16 consists of only positive components, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial}{\\partial q^{i}}\\mathcal{M}_{j}(\\pmb{q})=\\frac{\\partial}{\\partial q^{i}}\\ln(\\mathcal{M}_{j}(\\pmb{q}))\\cdot\\mathcal{M}_{j}(\\pmb{q})}}\\\\ &{=m_{i}\\left(k_{j}^{i}-\\displaystyle\\sum_{s\\in[N]}k_{s}^{i}\\mathcal{M}_{s}(\\pmb{q})\\right)\\mathcal{M}_{j}(\\pmb{q}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the triangle inequality gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left|\\frac{\\partial}{\\partial q^{i}}\\mathcal{M}_{j}(q)\\right|=\\left|m_{i}\\left(k_{j}^{i}-\\sum_{s\\in[N]}k_{s}^{i}\\mathcal{M}_{s}(q)\\right)\\mathcal{M}_{j}(q)\\right|}}\\\\ &{}&{\\leq m_{i}\\left(|k_{j}^{i}(1-\\mathcal{M}_{j}(q))\\mathcal{M}_{j}(q)|+\\sum_{s\\in[N]\\setminus\\{j\\}}|k_{s}^{i}\\mathcal{M}_{s}(q)\\mathcal{M}_{j}(q)|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now bound each term individually. Consider the terms $j\\neq s$ first. Since $0\\le\\mathcal{M}_{s}(\\pmb{q})\\le1$ , we can bound them as ", "page_idx": 18}, {"type": "equation", "text": "$$\n|k_{s}^{i}\\mathcal{M}_{s}(\\pmb{q})\\mathcal{M}_{j}(\\pmb{q})|\\leq|k_{s}^{i}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now recall that the inequality $a b\\leq(a+b)^{2}/4$ holds for any real numbers $a$ and $b$ with equality holding at $a=b$ . Therefore, for the first term, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n|k_{j}^{i}(1-\\mathcal{M}_{j}(\\pmb{q}))\\mathcal{M}_{j}(\\pmb{q})|\\leq|k_{j}^{i}|\\frac{(1-\\mathcal{M}_{j}(\\pmb{q})+\\mathcal{M}_{j}(\\pmb{q}))^{2}}{4}=\\frac{|k_{j}^{i}|}{4}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining inequalities 17, 18 and 19, we finally arrive at ", "page_idx": 18}, {"type": "equation", "text": "$$\n|J_{\\mathcal{M}}(q)_{j i}|=\\left|\\frac{\\partial}{\\partial q^{i}}\\mathcal{M}_{j}(\\pmb{q})\\right|\\leq m_{i}\\left(\\frac{|k_{j}^{i}|}{4}+\\sum_{s\\in[N]\\backslash\\{j\\}}|k_{s}^{i}|\\right)=\\kappa_{i j}m_{i}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $i\\in[D]$ and $j\\in[N]$ , where $\\kappa_{i j}\\geq0$ denotes the coefficient in the bracket. \u25a1 ", "page_idx": 18}, {"type": "text", "text": "B.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let us estimate the distance between two output vectors of Elliptical attention mechanism corresponding to clean and contaminated query inputs, namely: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h=\\displaystyle\\sum_{j\\in[N]}\\mathrm{softmax}(q^{\\top}M k_{j}/\\sigma^{2})v_{j}=\\displaystyle\\sum_{j\\in[N]}\\mathcal{M}_{j}(q)v_{j}}\\\\ &{h_{\\epsilon}=\\displaystyle\\sum_{j\\in[N]}\\mathrm{softmax}((q+\\epsilon)^{\\top}M k_{j}/\\sigma^{2})v_{j}=\\displaystyle\\sum_{j\\in[N]}\\mathcal{M}_{j}\\left(q+\\epsilon\\right)v_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathcal{M}$ is defined as in Lemma 1. We omit the keys and scaling parameter for convenience since they do not affect the analysis. Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert h-h_{\\cdot}\\right\\rVert=\\left\\lVert\\sum_{j\\in[N]}\\left(M_{j}(q)-M_{j}(q+\\epsilon)\\right)v_{j}\\right\\rVert}&{}\\\\ &{\\leq\\frac{\\displaystyle\\sum_{j\\in[N]}\\left|M_{j}(q)-M_{j}(q+\\epsilon)\\right|\\left\\lVert\\nabla_{j}\\right\\rVert}{\\displaystyle\\sum_{j\\in[N]}\\left\\lVert\\nabla_{\\xi}\\right\\rVert\\left\\lVert\\nabla_{j}\\right\\rVert}}\\\\ &{\\stackrel{\\mathrm{()}}{=}\\frac{\\displaystyle\\sum_{j\\in[N]}\\left\\lVert\\nabla_{\\xi}M_{j}(q)\\right\\rVert\\left\\lVert\\epsilon\\right\\rVert\\left\\lVert v_{j}\\right\\rVert}{\\displaystyle\\sum_{j\\in[N]}\\left\\langle\\sum_{t}(M_{j}(q))^{2}\\right\\rVert v_{j}\\right\\rVert\\left\\lVert\\epsilon\\right\\rVert}}\\\\ &{\\stackrel{\\mathrm{()}}{\\leq}\\frac{\\displaystyle\\sum_{j\\in[N]}\\left\\lVert\\sum_{i\\in[N]}\\nu_{i j}^{2}m_{i}^{2}\\right\\rVert\\left\\lVert\\epsilon\\right\\rVert}{\\displaystyle\\sum_{j\\in[N]}\\left\\langle\\sum_{t}\\nu_{i j}^{2}m_{i}^{2}\\right\\rVert\\left\\lVert\\epsilon\\right\\rVert}}\\\\ &{=\\frac{\\displaystyle\\sum_{j\\in[N]}\\left\\langle\\sqrt{\\pi}\\left(\\boldsymbol{K}_{j}^{2}M^{2}\\right)\\right\\rVert}{\\displaystyle\\sum_{j\\in[N]}\\left\\langle\\sum_{t}(\\boldsymbol{K}_{j}^{2}M^{2})\\right\\rVert}\\left\\lVert\\epsilon\\right\\rVert,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\pmb{K}_{j}:=\\mathrm{diag}(\\kappa_{1j},\\kappa_{2j},.~.~.~,\\kappa_{\\underline{{{D}}}j})$ and $\\kappa_{i j}$ is defined as in Eqn. 20. Note that 21 follows from mean value theorem for some $\\beta\\in[0,1]$ and $\\dot{\\pmb q}:=\\pmb q+\\beta\\pmb\\epsilon$ while 22 follows from Lemma 1. \u25a1 ", "page_idx": 19}, {"type": "text", "text": "It should be noted that Proposition 1 addresses the impact of noise exclusively on the query vectors. However, the resulting bound can be extended to account for noise in all tokens by employing the same technique utilized in the proof. For completeness, we also provide the extension. Let $\\mathcal{M}:\\mathbb{R}^{D}\\times\\mathbb{R}^{D}\\stackrel{\\cdot}{\\times}\\cdot\\cdot\\cdot\\times\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{N}$ be the Elliptical Softmax function defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{M}(q,k_{1},\\ldots,k_{N})=\\frac{1}{\\sum_{j\\in[N]}\\exp(q^{\\top}M k_{j})}\\left[\\!\\!\\begin{array}{c}{\\exp(q^{\\top}M k_{1})}\\\\ {\\vdots}\\\\ {\\exp(q^{\\top}M k_{N})}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Again, take the difference between output vectors calculated from clean and noisy tokens as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle h_{\\epsilon}=\\sum_{j\\in[N]}\\mathcal{M}_{j}({\\boldsymbol{q}}+\\epsilon_{\\boldsymbol{q}},\\boldsymbol{k}_{1}+\\epsilon_{\\boldsymbol{k}},\\ldots,\\boldsymbol{k}_{N}+\\epsilon_{\\boldsymbol{k}})(\\boldsymbol{v}_{j}+\\epsilon_{v}),}\\\\ &{\\displaystyle h=\\sum_{j\\in[N]}\\mathcal{M}_{j}({\\boldsymbol{q}},\\boldsymbol{k}_{1},\\ldots,\\boldsymbol{k}_{N})\\boldsymbol{v}_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\lVert\\bar{\\boldsymbol{\\epsilon}}\\rVert:=\\operatorname*{max}\\{\\lVert\\boldsymbol{\\epsilon}_{q}\\rVert,\\lVert\\boldsymbol{\\epsilon}_{k}\\rVert,\\lVert\\boldsymbol{\\epsilon}_{v}\\rVert\\}$ denote the noise with the largest norm among query, key and value noises. Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|h_{\\epsilon}-h\\|\\leq\\displaystyle\\sum_{j\\in[N]}\\left|\\mathcal{M}_{j}({\\boldsymbol q}+\\epsilon_{\\boldsymbol q},\\boldsymbol k_{1}+\\epsilon_{\\boldsymbol k},\\dots,\\boldsymbol k_{N}+\\epsilon_{\\boldsymbol k})-\\mathcal{M}_{j}({\\boldsymbol q},\\boldsymbol k_{1},\\dots,\\boldsymbol k_{N})\\right|\\|\\boldsymbol v_{j}\\|}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\sum_{j\\in[N]}\\mathcal{M}_{j}({\\boldsymbol q}+\\epsilon_{\\boldsymbol q},\\boldsymbol k_{1}+\\epsilon_{\\boldsymbol k},\\dots,\\boldsymbol k_{N}+\\epsilon_{\\boldsymbol k})\\|\\boldsymbol\\epsilon_{\\boldsymbol v}\\|}\\\\ &{\\quad\\quad\\leq\\displaystyle\\sum_{j\\in[N]}\\left(\\|\\nabla_{\\boldsymbol q}\\mathcal{M}_{j}(\\bar{\\boldsymbol q})\\|+\\displaystyle\\sum_{s\\in[N]}\\left\\|\\nabla_{\\boldsymbol k_{s}}\\mathcal{M}_{j}(\\bar{\\boldsymbol k}_{s})\\right\\|\\right)\\|\\bar{\\|\\boldsymbol\\epsilon}\\|\\|\\boldsymbol v_{j}\\|+\\|\\bar{\\boldsymbol\\epsilon}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following the same steps as Lemma 1, one can derive the bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial}{\\partial k_{s}^{i}}\\mathcal{M}_{j}\\right|\\leq m_{i}\\cdot|q^{i}|\\left(1-\\frac{3\\delta_{s j}}{4}\\right)\\propto m_{i}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $s,j\\in[N]$ . Therefore, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|h_{\\epsilon}-h\\|\\leq\\left(1+\\sum_{j\\in[N]}\\sum_{s\\in[N+1]}\\sqrt{\\mathrm{tr}(C_{s,j}^{2}M^{2})}\\|v_{j}\\|\\right)\\|\\bar{\\epsilon}\\|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{s,j}$ are the diagonal matrices whose elements are the proportionality coefficients in the derived upper bounds. ", "page_idx": 19}, {"type": "text", "text": "B.3 Proof of Proposition 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "There are two avenues through which to see resistance to representation collapse. In this section, we provide a proof based on noise propagation through layers, which decreases representation capacity as representations in deeper layers are increasingly composed of uninformative noise. We refer the reader to Appendix B.4 for an additional lens on representation collapse, where we show that Elliptical Attention is more sensitive to the variation and local features of the underlying function. ", "page_idx": 20}, {"type": "text", "text": "Let the output at layer $\\ell$ be denoted as $h^{\\ell}$ , the standard self-attention estimator and Elliptical estimator fitted at layer $\\ell$ be denoted $\\hat{f}^{\\ell}$ and $\\hat{f}_{d}^{\\ell}$ respectively, where $d$ is the Mahalanobis metric described in Eqn. 7, and $f$ be the true underlying function described in Eqn. 3. By assumption, $\\hat{f}$ is a higher variance estimator than $\\hat{f}_{d}$ for any layer. The output for either estimator at layer $\\ell$ can be decomposed into ground truth and noise as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\pmb h^{\\ell}=\\hat{f}^{\\ell}(\\pmb q^{\\ell})=f(\\pmb q^{\\ell})+\\pmb\\epsilon^{\\ell}}}\\\\ {{\\pmb h_{d}^{\\ell}=\\hat{f}_{d}^{\\ell}(\\pmb q^{\\ell})=f(\\pmb q^{\\ell})+\\pmb\\eta^{\\ell},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\eta^{\\ell}\\sim\\gamma(\\mathbf{0},V_{\\eta}),\\epsilon^{\\ell}\\sim\\gamma(\\mathbf{0},V_{\\epsilon})$ are the noise components of the estimate at $\\pmb q^{\\ell}$ and $f(\\pmb q^{\\ell})$ is the ground truth. By assumption of $\\hat{f}_{d}$ being lower variance, $V_{\\epsilon}-V_{\\eta}$ is a positive semi-definite matrix. We first require the following Assumption 1, which is described as: ", "page_idx": 20}, {"type": "text", "text": "Assumption 1 (Random Input Noise Causes Estimator Attenuation) . Let $\\hat{f}$ be any estimator of true function $f$ and let the input $x\\sim\\mu$ drawn from marginal $\\mu$ be randomly corrupted by random noise $\\epsilon\\sim(0,V)$ of some unknown distribution and variance matrix $V$ . Let c be some constant. Then, random input noise attenuates the estimator as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{{\\pmb x}\\sim{\\pmb\\mu}}\\|\\hat{f}({\\pmb x}+{\\pmb\\epsilon})-{\\pmb c}\\|\\leq\\mathbb{E}_{{\\pmb x}\\sim{\\pmb\\mu}}\\|\\hat{f}({\\pmb x})-{\\pmb c}\\|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assumption 1 is a well-studied phenomenon in parametric regression, often referred to as attenuation bias [69], regression dilution [19], or errors-in-variables [34]. In parametric regression, it can be shown to have an exact form where the estimated gradients of the model are attenuated towards 0 proportional to the variance of the noise $\\epsilon$ . In non-parametric regression, addition of input noise is often referred to as random smoothing or random input smoothing [46, 10], and is well known to be used as regularization technique to introduce bias into the model. In non-parametric models, no exact closed forms exist to express the attenuation bias, but for our purposes we only note the attenuation exists and provide a general form of it in Assumption 1. ", "page_idx": 20}, {"type": "text", "text": "The outputs of 30 and 31 then become the inputs to the following layer after being self-added, normalized, projected, and linearly transformed. For notational simplicity and because these operations do not change the analysis, we denote the input at the next layer as the previous layer output $\\pmb q^{\\ell+1}=\\pmb h^{\\ell}$ We therefore have the following process: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h^{\\ell+1}=\\hat{f}^{\\ell+1}(\\pmb{q}^{\\ell+1})=\\hat{f}^{\\ell+1}(h^{\\ell})=\\hat{f}^{\\ell+1}(\\underbrace{f(\\pmb{q}^{\\ell})+\\epsilon^{\\ell}}_{z^{\\ell}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we see the output $\\pmb{h}^{\\ell+1}$ is obtained by ftiting $\\hat{f}^{\\ell}$ to input $z^{\\ell}$ which is composed of ground truth $f(\\pmb q^{\\ell})$ and noise $\\epsilon^{\\ell}$ passed through from the previous layer. ", "page_idx": 20}, {"type": "text", "text": "The result then follows directly from the fact that in any given layer, the standard self-attention estimator produces noisier estimates, where that noise is then passed into the subsequent layer as input noise. This is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|h^{\\ell+1}-c\\|=\\mathbb{E}\\|\\hat{f}^{\\ell+1}(\\pmb{q}^{\\ell+1})-c\\|=\\mathbb{E}\\|\\hat{f}^{\\ell+1}(f(\\pmb{q}^{\\ell})+\\epsilon^{\\ell})-c\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\|\\hat{f}^{\\ell+1}(f(\\pmb{q}^{\\ell})+\\eta^{\\ell})-c\\|}\\\\ &{\\qquad\\qquad\\qquad\\approx\\mathbb{E}\\|\\hat{f}_{d}^{\\ell+1}(f(\\pmb{q}^{\\ell})+\\eta^{\\ell})-c\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\|\\hat{f}_{d}^{\\ell+1}(f(\\pmb{q}^{\\ell+1})-c\\|=\\mathbb{E}\\|h_{d}^{\\ell+1}-c\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where line 35 follows from combining the fact that $\\eta^{\\ell}$ is lower variance with Assumption 1 and line 36 follows from the fact that $\\mathbb{E}\\|X\\|\\approx\\mathbb{E}\\|Y\\|$ when $X,Y$ have the same mean and roughly similar distribution. ", "page_idx": 20}, {"type": "text", "text": "Therefore we obtain at any layer $\\ell$ the following ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|h^{\\ell+1}-c\\|\\leq\\mathbb{E}\\|h_{d}^{\\ell+1}-c\\|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as required. \u25a1 ", "page_idx": 21}, {"type": "text", "text": "B.4 Edge-preservation Perspective on Representation Collapse ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further substantiate our findings on the mitigation of representation collapse in transformers, we now present an additional proposition that examines this phenomenon from a different perspective. In Proposition 4, we show that Elliptical attention reduces representation collapse by retaining the important local features (bumps etc.) better than the standard self-attention in the case of sparse piece-wise constant functions. ", "page_idx": 21}, {"type": "text", "text": "Proposition 4 (Representation Collapse) Let $f:A\\rightarrow\\mathbb{R}^{D}$ for $A\\subseteq\\mathbb{R}^{D}$ be a piece-wise constant function with $f|_{A_{i}}\\bar{(\\mathbf{q})}=\\mathbf{f}_{i}\\in\\mathbb{R}^{D}$ where $\\textstyle A=\\bigcup_{i\\in I}A_{i}$ for some (possibly infinite) index $I$ . Let $\\pmb q_{1}$ and $\\pmb q_{2}$ be the queries lying in any of the adjac ent domain pieces with distant function values. Then, the Elliptical estimates at these queries retain the distance better than the standard self-attention estimates which is formulated as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|\\hat{f}_{d}(\\pmb{q}_{2})-\\hat{f}_{d}(\\pmb{q}_{1})\\|\\ge\\mathbb{E}\\|\\hat{f}(\\pmb{q}_{2})-\\hat{f}(\\pmb{q}_{1})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Assume all output vectors are normalized. Then, the Euclidean distance between two vectors is determined by their dot product since ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|a-b\\|^{2}=\\|a\\|^{2}+\\|b\\|^{2}-2a^{\\top}b.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Without loss of generality, we take $A_{1}$ and $A_{2}$ to be the two adjacent pieces so that $f(\\pmb q_{i})=f_{i}$ for $i=1,2$ . Denote $\\hat{f}(\\pmb q_{i})=h_{i}$ and $\\hat{f}_{d}(\\pmb{q}_{i})=h_{i d}$ . Then, Eqn. 39 is equivalent to proving ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}}[h_{1d}^{\\top}h_{2d}]\\leq\\mathbb{E}_{\\mathcal{D}}[h_{1}^{\\top}h_{2}],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the expectation is taken over the whole sampling distribution $\\mathcal{D}$ but the points $\\pmb q_{1}\\in A_{1}$ and $\\pmb q_{2}\\in A_{2}$ are fixed as described in the definition. We drop the subscript $\\mathcal{D}$ as this will be the default distribution for computing expectation unless specified otherwise. Let $r_{S}=\\mathrm{cossim}(\\pmb{f}_{1},\\pmb{f}_{2})=\\pmb{f}_{1}^{\\top}\\pmb{f}_{2}$ be the cosine similarity of the two piece-wise values. By definition of $\\pmb q_{1}$ and $\\pmb q_{2}$ and since the estimates work by averaging the output vectors with a small amount of noise, we have $r_{S}~\\leq$ $\\operatorname*{min}\\left\\{\\mathbb{E}[h_{1d}^{\\top}h_{2d}],\\mathbb{E}[h_{1}^{\\top}h_{2}]\\right\\}.$ . We now decompose $\\pmb{h}_{1d}$ and $h_{2d}$ in terms of components along and orthogonal to $\\boldsymbol{f}_{1}$ and $\\boldsymbol{f}_{2}$ , respectively: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{h}_{1d}=(\\pmb{h}_{1d}^{\\top}\\pmb{f}_{1})\\pmb{f}_{1}+\\pmb{f}_{1}^{\\bot},\\quad\\pmb{h}_{2d}=(\\pmb{h}_{2d}^{\\top}\\pmb{f}_{2})\\pmb{f}_{2}+\\pmb{f}_{2}^{\\bot},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\pmb{f}_{i}^{\\top}\\pmb{f}_{i}^{\\bot}=0$ . Then, for their dot product, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{1d}^{\\top}h_{2d}=\\left[(h_{1d}^{\\top}f_{1})f_{1}+f_{1}^{\\bot}\\right]^{\\top}\\left[(h_{2d}^{\\top}f_{2})f_{2}+f_{2}^{\\bot}\\right]}\\\\ &{\\qquad\\quad=(h_{1d}^{\\top}f_{1})(h_{2d}^{\\top}f_{2})f_{1}^{\\top}f_{2}+(h_{1d}^{\\top}f_{1})f_{1}^{\\top}f_{2}^{\\bot}}\\\\ &{\\qquad\\quad\\qquad+\\,(h_{2d}^{\\top}f_{2})f_{2}^{\\top}f_{1}^{\\bot}+(f_{1}^{\\bot})^{\\top}f_{2}^{\\bot}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The analogous decomposition of $h_{1}^{\\top}h_{2}$ can be obtained. By Theorem 2 we have that the Elliptical estimator is lower variance and so we have $\\mathbb{E}\\|{\\pmb{f}}_{i}-{\\pmb{h}}_{i d}\\|^{2}\\stackrel{.}{\\leq}\\mathbb{E}\\|{\\pmb{f}}_{i}-{\\pmb{h}}_{i}\\|^{2}$ . This has the following implications: ", "page_idx": 21}, {"type": "text", "text": "1. $1\\geq\\mathbb{E}[\\pmb{h}_{i d}^{\\top}\\pmb{f}_{i}]\\geq\\mathbb{E}[\\pmb{h}_{i}^{\\top}\\pmb{f}_{i}]$ i.e. the component of $\\boldsymbol{h}_{i d}$ along $\\boldsymbol{\\mathscr{f}}_{i}$ is larger than that of $\\boldsymbol{h}_{i}$ , and hence $h_{i d}^{\\top}f_{i}$ is closer to 1.   \n2. Due to the first implication above, the orthogonal component $\\pmb{f}_{i}^{\\bot}$ becomes smaller in terms of magnitude so that $\\pmb{f}_{j}^{\\top}\\pmb{f}_{i}^{\\bot}$ and $(\\pmb{f}_{i}^{\\bot})^{\\top}\\pmb{f}_{j}^{\\bot}$ are closer to 0 for Elliptical compared to the standard self-attention. ", "page_idx": 21}, {"type": "text", "text": "These two arguments, combined with Eqn. 43, imply that in expectation $h_{1d}^{\\top}h_{2d}$ is closer to $1\\cdot$ $(\\pmb{f}_{1}^{\\top}\\pmb{f}_{2})+0\\stackrel{\\top}{=}\\pmb{f}_{1}^{\\top}\\pmb{f}_{2}=r_{S}$ which, by definition, is the smallest dot product over $S$ , and hence, $r_{S}\\leq\\mathbb{E}[h_{1d}^{\\top}h_{2d}]\\leq\\mathbb{E}[h_{1}^{\\top}h_{2}]$ as desired. \u25a1 ", "page_idx": 21}, {"type": "text", "text": "B.5 Proof of Proposition 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The lemma below encapsulates the necessary calculations that will then be used in the following proofs. ", "page_idx": 22}, {"type": "text", "text": "Lemma 2 Given a normally distributed zero mean random variable $\\xi\\sim\\mathcal{N}(0,\\sigma^{2})$ , the expectation of a random variable obtained by its absolute value is $\\mathbb{E}|\\xi|=\\sqrt{2\\sigma^{2}/\\pi}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Since $\\xi\\sim\\mathcal{N}(0,\\sigma^{2})$ , by definition of expectation, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}|\\xi|=\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\frac{|x|}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\displaystyle\\frac{x^{2}}{2\\sigma^{2}}\\right)d x}\\\\ &{\\quad=\\displaystyle\\int_{-\\infty}^{0}\\displaystyle\\frac{-x}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\displaystyle\\frac{x^{2}}{2\\sigma^{2}}\\right)d x+\\displaystyle\\int_{0}^{\\infty}\\displaystyle\\frac{x}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\displaystyle\\frac{x^{2}}{2\\sigma^{2}}\\right)d x}\\\\ &{\\quad=\\displaystyle\\frac{2}{\\sqrt{2\\pi\\sigma^{2}}}\\int_{0}^{\\infty}x\\exp\\left(-\\displaystyle\\frac{x^{2}}{2\\sigma^{2}}\\right)d x}\\\\ &{\\quad=\\displaystyle\\sqrt{\\frac{2}{\\pi\\sigma^{2}}}\\left[-\\sigma^{2}\\exp\\left(-\\displaystyle\\frac{x^{2}}{2\\sigma^{2}}\\right)\\right]\\displaystyle\\bigg|_{0}^{\\infty}}\\\\ &{\\quad=\\displaystyle\\sqrt{\\frac{2\\sigma^{2}}{\\pi}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used the variable change $x\\leftarrow(-x)$ in the first integral of 44 to obtain 45. \u25a1 ", "page_idx": 22}, {"type": "text", "text": "We derive the bounds for the impact of noise in 3, with respect to its variance, on our estimator 10 in Lemma 3. Henceforth, we omit the factor $\\delta$ in Eqn. 10 since it does not affect the further analysis. ", "page_idx": 22}, {"type": "text", "text": "Lemma 3 Given that the noise term in $^3$ follows a normal distribution with zero mean and variance $\\sigma^{2}$ , the following inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|m_{i}-\\mathbb{E}|f_{i}(\\pmb{k}^{\\ell+1})-f_{i}(\\pmb{k}^{\\ell})|\\right|\\leq\\frac{2}{\\sqrt{\\pi}}\\sigma\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Since all value vectors are taken from the data generating process 3, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{i}=\\underset{(\\pmb{v}^{\\ell},\\pmb{v}^{\\ell+1})\\in\\mathcal{X}_{v}^{\\ell,\\ell+1}}{\\mathbb{E}}|\\pmb{v}_{i}^{\\bar{\\ell}+1}-\\pmb{v}_{i}^{\\ell}|^{-}}\\\\ &{\\quad=\\mathbb{E}|f_{i}(\\pmb{k}^{\\ell+1})-f_{i}(\\pmb{k}^{\\ell})+\\epsilon_{i}^{\\ell+1}-\\epsilon_{i}^{\\ell}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\epsilon_{i}^{\\ell}$ and $\\epsilon_{i}^{\\ell+1}$ denote the $i^{t h}$ components of the noise terms $\\epsilon^{\\ell}$ and $\\epsilon^{\\ell+1}$ , respectively. Note that for real numbers $a$ and $b$ , we have by triangle inequality that $|a+b|\\leq|a|+|b|$ and $\\left|a+b\\right|=$ $|a-(-b)|\\geq||a|-|-b||\\geq|a|-|b|$ . Applying these and the linearity of expectation to 47, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}|f_{i}(k^{\\ell+1})-f_{i}(k^{\\ell})|-\\mathbb{E}|\\epsilon_{i}^{\\ell+1}-\\epsilon_{i}^{\\ell}|\\,\\le\\,m_{i}\\,\\le\\,\\mathbb{E}|f_{i}(k^{\\ell+1})-f_{i}(k^{\\ell})|+\\mathbb{E}|\\epsilon_{i}^{\\ell+1}-\\epsilon_{i}^{\\ell}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Recall that $\\epsilon_{i}^{\\ell}\\sim\\mathcal{N}(0,\\sigma^{2})$ and independent. Now we have that $\\epsilon_{i}^{\\ell+1}-\\epsilon_{i}^{\\ell}\\sim\\mathcal{N}(0,2\\sigma^{2})$ as the mean value does not change while variance accumulates when subtracting two zero-mean normal variables. Therefore, the Lemma 2 gives that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}|\\epsilon_{i}^{\\ell+1}-\\epsilon_{i}^{\\ell}|=\\frac{2}{\\sqrt{\\pi}}\\sigma.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging this back into the inequalities 48, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}|f_{i}(k^{\\ell+1})-f_{i}(\\dot{k}^{\\ell})|-\\displaystyle\\frac{2}{\\sqrt{\\pi}}\\sigma\\,\\le^{\\!\\alpha}m_{i}\\,\\le\\,\\mathbb{E}|f_{i}(k^{\\ell+1})-f_{i}(k^{\\ell})|+\\displaystyle\\frac{2}{\\sqrt{\\pi}}\\sigma,}\\\\ &{\\mathrm{ivalent~to~46~as~desired.~}\\!\\!\\!\\bigcup}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is equ ", "page_idx": 22}, {"type": "text", "text": "Remark 7 Note that in Lemma $^3$ , we may also take into account the possible noise in the value vectors. Let $\\epsilon_{i,v}^{\\ell}\\sim\\mathcal{N}(0,\\sigma_{v}^{2})$ be the noise in the values vectors as $m_{i}^{\\epsilon}=\\mathbb{E}|\\pmb{v}_{i}^{\\ell+1}-\\pmb{v}_{i}^{\\ell}+\\epsilon_{i,v}^{\\ell+1}-\\epsilon_{i,v}^{\\ell}|$ . Then, applying the triangle inequality, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathbb{E}|v^{\\ell+1}-\\bar{v}^{\\ell}|-\\bar{\\mathbb{E}}|\\epsilon_{i,v}^{\\ell+1}-\\epsilon_{i,v}^{\\ell}|\\leq m_{i}^{\\epsilon}\\leq\\mathbb{E}|v^{\\ell+1}-v^{\\ell}|+\\mathbb{E}|\\epsilon_{i,v}^{\\ell+1}-\\epsilon_{i,v}^{\\ell}|.}\\end{array}$ ", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now applying Lemma 2 and Lemma $^3$ , we arrive at ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|m_{i}^{\\epsilon}-\\mathbb{E}|f(\\pmb{k}^{\\ell+1})-f(\\pmb{k}^{\\ell})|\\right|\\leq\\frac{2}{\\sqrt{\\pi}}(\\sigma+\\sigma_{v}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "image", "img_path": "Ejg4d4FVrs/tmp/e4fac74eb7d7c8023651c5fa7b1e61ab581c0aa8a5794c09d0af20d75426aec4.jpg", "img_caption": ["Figure 5: Left: Evolution of mean values of key perturbations over successive layers. Right: Mean key perturbations at different layers after 300 epochs. The figures show that as the number of layers increases, mean key perturbations over layers stabilize around a constant value. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Proof of Proposition 3. We shall first make the following assumptions on the data generating process 3: ", "page_idx": 23}, {"type": "text", "text": "Assumption 2 The underlying coordinate system in the feature space $\\scriptstyle{\\mathcal{X}}_{k}$ is independent, implying that the function $f:\\mathbb{R}^{D}\\overset{}{\\rightarrow}\\mathbb{R}^{\\check{D}}$ in Eqn. 3 can be separated as $f(\\pmb{k})=(f_{1}(k_{1}),\\dots f_{D}(k_{D}))^{\\top}$ ", "page_idx": 23}, {"type": "text", "text": "Assumption 3 The noise term in Eqn. 3 has independent components with each component $\\epsilon_{j}^{\\ell}$ following a normal distribution ${\\mathcal{N}}(0,\\sigma^{2})$ for small $\\sigma$ , for all $j\\in[D]$ and $\\ell\\in\\mathbb{N}$ ", "page_idx": 23}, {"type": "text", "text": "Assumption 4 The magnitude of each component of key perturbations across consecutive layers, defined as $|k_{i}^{\\ell+1}-k_{i}^{\\ell}|,$ follows a distribution with small, layer-independent mean (\u03b4) and variance $(\\nu)$ ", "page_idx": 23}, {"type": "text", "text": "Remark 8 The assumption of layer-independence in Assumption 4, especially for deeper layers, is supported well empirically, as shown in Figure 5. Given the over-smoothing observed in transformers [24], where token representations stabilize after initial few layers, it is also practical to assume that key perturbations across layers have relatively small mean and variance when modelled as a random process. ", "page_idx": 23}, {"type": "text", "text": "Proof. Under the Assumptions 2, 3, 4, we show that $\\|f_{i}^{\\prime}\\|_{1,\\mu}\\geq\\|f_{j}^{\\prime}\\|_{1,\\mu}$ implies $m_{i}\\geq m_{j}$ with high probability where $m_{i}$ is defined as in (10). ", "page_idx": 23}, {"type": "text", "text": "Directly from the Lemma 3, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|m_{i}-\\mathbb{E}|f_{i}(\\pmb{k}^{\\ell+1})-f_{i}(\\pmb{k}^{\\ell})|\\right|\\leq\\frac{2}{\\sqrt{\\pi}}\\sigma.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Letting $\\sigma\\rightarrow0$ in this inequality, which is feasible under the Assumption 3, one can get with a small error that ", "page_idx": 23}, {"type": "equation", "text": "$$\nm_{i}\\approx\\mathbb{E}|f_{i}(\\pmb{k}^{\\ell+1})-f_{i}(\\pmb{k}^{\\ell})|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which in turn implies that the impact of the noise in (10) is negligible and the error of ignoring them can be controlled by the bounds given by (46). Now according to the theorem statement, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|f_{i}^{\\prime}\\|_{1,\\mu}\\geq\\|f_{j}^{\\prime}\\|_{1,\\mu}\\iff\\mathbb{E}\\|J_{f}(k)e_{i}\\|_{1}\\geq\\mathbb{E}\\|J_{f}(k)e_{j}\\|_{1}}&{}\\\\ {\\iff\\mathbb{E}\\left[\\displaystyle\\sum_{s\\in[D]}\\left|\\frac{\\partial f_{s}(k)}{\\partial k_{i}}\\right|\\right]\\geq\\mathbb{E}\\left[\\displaystyle\\sum_{s\\in[D]}\\left|\\frac{\\partial f_{s}(k)}{\\partial k_{j}}\\right|\\right]}&{}\\\\ {\\iff\\mathbb{E}\\left|f_{i}^{\\prime}(k_{i})\\right|\\geq\\mathbb{E}\\left|f_{j}^{\\prime}(k_{j})\\right|}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used the separability of $f$ as given in Assumption 2 which simplifies the Jacobian matrix as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{f}(k)=\\left[\\begin{array}{c c c c c}{\\frac{\\partial f_{1}(k)}{\\partial k_{1}}}&{\\frac{\\partial f_{1}(k)}{\\partial k_{2}}}&{\\ldots}&{\\frac{\\partial f_{1}(k)}{\\partial k_{2}}}\\\\ {\\frac{\\partial f_{2}(k)}{\\partial k_{1}}}&{\\frac{\\partial f_{2}(k)}{\\partial k_{2}}}&{\\ldots}&{\\frac{\\partial f_{2}(k)}{\\partial k_{D}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{\\partial f_{D}(k)}{\\partial k_{1}}}&{\\frac{\\partial f_{D}(k)}{\\partial k_{2}}}&{\\ldots}&{\\frac{\\partial f_{D}(k)}{\\partial k_{D}}}\\end{array}\\right]=\\left[\\begin{array}{c c c c c}{\\frac{\\partial f_{1}(k_{1})}{\\partial k_{1}}}&{\\frac{\\partial f_{1}(k_{1})}{\\partial k_{2}}}&{\\ldots}&{\\frac{\\partial f_{1}(k_{1})}{\\partial k_{D}}}\\\\ {\\frac{\\partial f_{2}(k_{2})}{\\partial k_{1}}}&{\\frac{\\partial f_{2}(k_{2})}{\\partial k_{2}}}&{\\ldots}&{\\frac{\\partial f_{2}(k_{2})}{\\partial k_{D}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{\\partial f_{D}(k_{D})}{\\partial k_{1}}}&{\\frac{\\partial f_{D}(k_{D})}{\\partial k_{2}}}&{\\ldots}&{\\frac{\\partial f_{D}(k_{D})}{\\partial k_{D}}}\\end{array}\\right]}\\\\ &{=\\left[\\begin{array}{c c c c c}{f_{1}^{\\prime}(k_{1})}&{0}&{\\ldots}&{0}\\\\ {0}&{f_{2}^{\\prime}(k_{2})}&{\\ldots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\ldots}&{f_{D}^{\\prime}(k_{D})}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so that $[J_{f}(\\pmb{k})]_{i i}=f_{i}^{\\prime}(k_{i})$ . Using the definition of derivative, the inequality 50 is equivalent to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left|\\operatorname*{lim}_{\\tau\\rightarrow0}\\frac{f^{i}(k_{i}^{\\ell}+\\bar{\\tau_{i}})-f^{i}(k_{i}^{\\ell})}{\\tau}\\right|\\geq\\mathbb{E}\\left|\\operatorname*{lim}_{\\tau\\rightarrow0}\\frac{f^{j}(k_{j}^{\\ell}+\\tau)-f^{j}(k_{j}^{\\ell})}{\\tau}\\right|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we note that for a small $\\delta$ , the limits in (51) can be approximated with $\\frac{\\dot{f}^{s}(k_{s}^{\\ell}\\!+\\!\\delta)\\!-\\!f^{s}(k_{s}^{\\ell})}{\\delta}$ for $s\\in\\{i,j\\}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}|f^{i}(k_{i}^{\\ell}+\\delta)-f^{i}(k_{i}^{\\ell})|}{\\delta}\\geq\\frac{\\mathbb{E}|f^{j}(k_{j}^{\\ell}+\\delta)-f^{j}(k_{j}^{\\ell})|}{\\delta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let us choose $\\delta=\\mathbb{E}|k_{i}^{\\ell+1}-k_{i}^{\\ell}|$ . Then, by Chebyshev\u2019s inequality, we have for any $\\varepsilon>0$ that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\displaystyle\\frac{\\nu^{2}}{\\varepsilon^{2}}\\leq\\mathbb{P}\\left(\\left||k_{i}^{\\ell+1}-k_{i}^{\\ell}|-\\delta\\right|\\leq\\varepsilon\\right)}\\\\ &{\\qquad\\qquad=\\mathbb{P}\\left(\\delta-\\varepsilon\\leq|k_{i}^{\\ell+1}-k_{i}^{\\ell}|\\leq\\delta+\\varepsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given that the variance $\\nu$ is sufficiently small as in the Assumption 4, the inequality (53) implies that $k_{i}^{\\ell+1}\\approx k_{i}^{\\ell}\\pm\\delta$ with high probability. Therefore, it follows from (52) with high probability that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\stackrel{\\cdot}{\\mathbb{E}}|f^{i}(k_{i}^{\\ell+1})-f^{i}(k_{i}^{\\ell})|}{\\delta}\\geq\\frac{\\mathbb{E}|f^{j}(k_{j}^{\\ell+1})-f^{j}(k_{j}^{\\ell})|}{\\delta},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which, due to 49, is equivalent to $m_{i}\\geq m_{j}$ as desired. \u25a1 ", "page_idx": 24}, {"type": "text", "text": "B.6 Lipschitz smoothness in $(\\mathcal{X},d)$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Below we show how Lipschitz smoothness of $f$ changes when moving from Euclidean to the Mahalanobis transformed space. We shall follow similar steps to [29] and [30] but for a more general class of functions. ", "page_idx": 24}, {"type": "text", "text": "Proposition 5 (Change in Lipschitz smoothness for $f$ ) Suppose there exists a positive constant $G_{i}$ such that $\\|\\nabla f_{i}(\\pmb{k})\\|\\leq G_{i}$ for any $k\\in\\mathcal{X}_{k}$ and $m_{i}>0$ for all $i\\in[D]$ . Then for any $\\pmb{q},\\pmb{k}\\in\\mathcal{X}_{k}$ , the following inequality holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|f(\\pmb{q})-f(\\pmb{k})\\|\\leq\\left(\\sum_{i\\in[D]}\\frac{G_{i}}{\\sqrt{m_{i}}}\\right)d(\\pmb{q},\\pmb{k}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\omega:=\\frac{q-k}{\\|q-k\\|}}\\end{array}$ denote the unit vector pointing from $\\pmb{k}$ to $\\pmb q$ . The fundamental theorem of calculus implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(q)-f(k)=\\int_{0}^{\\|q-k\\|}\\frac{d}{d t}f(k+t\\omega)\\,d t=\\int_{0}^{\\|q-k\\|}\\omega^{\\top}J_{f}(k+t\\omega)\\,d t,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $J_{f}$ is the Jacobian matrix of $f$ as usual. Starting with the distance between outputs $f({\\pmb q})$ and $f(k)$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|f(\\pmb{q})-f(\\pmb{k})\\|=\\left\\|\\int_{0}^{\\|q-k\\|}\\omega^{\\top}\\pmb{J}_{f}(\\pmb{k}+t\\omega)\\,d t\\right\\|\\leq\\int_{0}^{\\|q-k\\|}\\left\\|\\displaystyle\\sum_{i\\in[D]}\\omega_{i}\\nabla f_{i}(\\pmb{k}+t\\omega)\\right\\|d t}}\\\\ &{\\leq\\displaystyle\\int_{0}^{\\|q-k\\|}\\sum_{i\\in[D]}\\,|\\omega_{i}|\\,\\|\\nabla f_{i}(\\pmb{k}+t\\omega)\\|\\,d t\\leq\\displaystyle\\sum_{i\\in[D]}G_{i}|\\omega_{i}|\\int_{0}^{\\|q-k\\|}d t}\\\\ &{=\\displaystyle\\sum_{i\\in[D]}G_{i}|q_{i}-k_{i}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where, as for all other vectors, $q_{i}$ denotes the $i^{t h}$ component of vector $\\pmb q$ . Now note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n|q_{i}-k_{i}|\\leq\\sqrt{(q_{i}-k_{i})^{2}+\\sum_{j\\neq i}\\frac{m_{j}}{m_{i}}(q_{j}-k_{j})^{2}}=\\sqrt{\\frac{(q-k)^{\\top}M(q-k)}{m_{i}}}=\\frac{d(q,k)}{\\sqrt{m_{i}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining 54 and 55, we finally attain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|f(\\pmb{q})-f(\\pmb{k})\\|\\leq\\sum_{i\\in[D]}\\frac{G_{i}}{\\sqrt{m_{i}}}d(\\pmb{q},\\pmb{k}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which completes the proof. \u25a1 ", "page_idx": 25}, {"type": "text", "text": "C Additional Theorems ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The following Theorem 1 is a classic result from [70]. We refer the reader to their work for details. ", "page_idx": 25}, {"type": "text", "text": "Theorem 1 (Minimax rate for functions of bounded variability [70]) Let $F_{\\lambda}$ denote the class of distributions $P_{X,Y}$ on $\\mathcal{X}\\times[0,1]$ such that $\\forall i\\in[d]$ , the directional derivates of $f(x):=\\mathbb{E}[Y|X=x]$ satisfy $\\begin{array}{r}{|f_{i}^{\\prime}|_{\\operatorname*{sup}}:=\\operatorname*{sup}_{\\pmb{q}\\in\\mathcal{X}_{k}}\\|\\nabla f_{i}(\\pmb{q})\\|_{\\operatorname*{sup}}\\leq\\lambda.}\\end{array}$ . Then for any $f\\in F_{\\lambda}$ , estimator $\\hat{f}$ , sample size $n\\in\\mathbb N$ , there exists a $\\tilde{c}\\leq1$ independent of $n$ satisfying ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f_{n}}\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\lambda}}\\mathbb{E}_{\\lambda\\,,Y^{n}}\\|\\hat{f}-f\\|^{2}\\geq2\\tilde{c}^{2/(2+d)}(d\\lambda)^{2d/(2+d)}n^{-2/(2+d)}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Theorem 2 (Improvement in MSE for approximately sparse functions [30]) Let the norm of the largest gradient be $\\begin{array}{r}{\\lambda:=\\operatorname*{sup}_{i\\in[D]}\\|\\nabla f_{i}(\\pmb{q})\\|_{\\operatorname*{sup}}}\\end{array}$ and $\\hat{f}_{d}$ be an estimator in metric space $(\\mathcal{X}_{q},d)$ where $d$ is defined as Eqn. 7. Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\hat{f}_{d}-f\\|_{2}^{2}<\\operatorname*{inf}_{\\tilde{f}}\\operatorname*{sup}_{\\lambda}\\mathbb{E}\\|\\tilde{f}-f\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We provide an abridged proof for completeness. We refer the reader to [30] for the full details. First, the full bound is described as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\hat{f}_{d}-f\\|_{2}^{2}\\leq2C_{\\kappa_{R}}^{2/2+r}(C D\\lambda_{d}d(\\mathcal{X}))^{2r/2+r}n^{-2/2+r}<\\operatorname*{inf}_{\\tilde{f}}\\operatorname*{sup}_{\\mathcal{F}_{\\lambda}}\\mathbb{E}\\|\\tilde{f}-f\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $d(\\boldsymbol{\\mathcal{X}})$ is the $\\mathrm{d}$ -diameter of $\\mathcal{X}$ defined as $\\begin{array}{r}{\\operatorname*{sup}_{x,x^{\\prime}\\in\\mathcal{X}}d(x,x^{\\prime}),R\\subset[D],1\\leq C_{\\kappa_{R}}\\leq C^{\\prime}(4\\kappa_{R})^{|R|}}\\end{array}$ , $C$ and $C_{1}$ are universal constants and $\\lambda_{d}\\geq\\operatorname*{sup}_{i}\\|f_{i}^{\\prime}\\|_{\\operatorname*{sup}}/\\sqrt{m_{i}}$ . Let ", "page_idx": 25}, {"type": "equation", "text": "$$\nr(\\epsilon)\\leq\\left\\{\\stackrel{|R|}{D}-(D-|R|)\\frac{\\log(d(X)/\\epsilon_{R})}{\\log(1/\\epsilon)}\\quad\\mathrm{if~}\\epsilon<\\epsilon_{R}/d(X)\\;.\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For bandwidth $\\epsilon_{n}$ , $r\\,=\\,r(\\epsilon_{n})$ and let $|R|\\,\\le\\,r\\,\\le\\,D$ . Let $\\epsilon\\,>\\,0$ , $\\tilde{c}$ be defined as the same $\\tilde{c}$ in Theorem 1, and $n\\,\\in\\,\\mathbb{N}$ , define the function $\\psi_{n,d}\\,=\\,C\\epsilon^{-r(\\epsilon)}/n$ and $\\psi_{n,\\notin}(\\epsilon)\\,=\\,C_{1}^{\\prime}\\epsilon^{-D}/n$ where $C_{1}=\\tilde{c}\\,\\big(\\lambda/C\\lambda_{d}d(\\lambda)\\big)^{D}$ . Also define $\\phi(\\epsilon)=C^{2}D^{2}\\lambda_{d}^{2}d(\\mathcal{X})^{2}\\cdot\\epsilon^{2}$ . ", "page_idx": 25}, {"type": "text", "text": "For any fixed $n$ , let $\\epsilon_{n,\\not\\ d}$ be a solution to $\\psi_{n,\\hat{d}}(\\epsilon)=\\phi(\\epsilon)$ . Solving for $\\epsilon_{n,\\not\\mathcal{d}}$ obtains the following lower bound on the minmax rate of ", "page_idx": 25}, {"type": "equation", "text": "$$\n2\\phi(\\epsilon_{n,\\not\\mu})=2\\tilde{c}^{2/(2+D)}(D\\lambda)^{2d/(2+d)}n^{-2/(2+d)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For any $n\\in\\mathbb N$ there exists a solution $\\epsilon_{n,d}$ to the equation $\\psi_{n,d}(\\epsilon)=\\phi(\\epsilon)$ since $r(\\epsilon)$ is nondecreasing. Therefore it is possible to obtain the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X^{n},Y^{n}}\\|f_{n,\\epsilon,d}-f\\|_{2}^{2}\\leq2\\phi(\\epsilon_{n,d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\phi$ is independent of $n$ , and both $\\psi_{n,d}$ and $\\boldsymbol{\\psi}_{n,\\not\\mu}$ are strictly decreasing functions of $n$ , we have that $\\epsilon_{n,d}$ and $\\epsilon_{n,\\not\\ d}$ both tend to 0 as $n\\to\\infty$ . Theref ore we can define $n_{0}$ such that, for all $n\\geq n_{0}$ , both $\\epsilon_{n,d}$ and $\\epsilon_{n,\\not\\ d}$ are less than $\\epsilon_{\\!\\scriptscriptstyle R}/d(\\mathcal{X})$ . ", "page_idx": 25}, {"type": "text", "text": "Thus, $\\forall n\\:\\geq\\:n_{0}$ , we have $\\epsilon_{n,d}\\,<\\,\\epsilon_{n,\\not\\sf d}$ if, for all $0\\,<\\,\\epsilon\\,<\\,\\epsilon_{\\!\\scriptscriptstyle K}/d(\\lambda),\\,\\psi_{n,d}(\\epsilon)\\,<\\,\\psi_{n,\\!\\scriptscriptstyle f}(\\epsilon)$ , which completes the proof $\\sqsubset$ . ", "page_idx": 25}, {"type": "text", "text": "D A Consistent Estimator ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we present a consistent centered difference-based quotient estimator of the coordinatewise variability obtained by perturbing the estimated function in the $i^{t h}$ direction and measuring the $L_{1}$ norm of the difference. Similarly, this estimator requires no learnable parameters or gradients. The estimator is described in the following proposition. ", "page_idx": 26}, {"type": "text", "text": "Proposition 6 (Consistent Estimator) Given a function $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D_{v}}$ with ith directional variation $\\|f_{i}^{\\prime}\\|_{1,\\mu},i\\in[D],$ , the directional variation can be estimated by the quantity ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{m}_{i}:=\\mathbb{E}_{n}\\left[\\frac{\\|\\bar{f}(\\pmb{k}+t\\pmb{e}_{i})-\\bar{f}(\\pmb{k}-t\\pmb{e}_{i})\\|_{1}}{2t}\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $t$ is a hyperparameter controlling the degree of locality of the estimator and $\\mathbb{E}_{n}$ denotes the empirical expectation for $n$ samples. ", "page_idx": 26}, {"type": "text", "text": "Despite $\\widehat{m}_{i}$ in proposition 6\u2019s simple formulation, it is nonetheless a consistent estimator of the coordinate-wise variation in the underlying function. We utilize a simplified version of a theorem from [30], adapted to suit our specific needs, as the original formulation is more detailed than necessary for our purposes. ", "page_idx": 26}, {"type": "text", "text": "Theorem 3 (Consistency of Centered Difference-based Estimator for Scalar Function [30]) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let $\\varphi:\\mathbb{R}^{D}\\to\\mathbb{R}$ be a smooth scalar function and $\\|\\varphi_{i}^{\\prime}\\|_{1,\\mu}:=\\mathbb{E}_{\\mathbf{x}\\sim\\mu}|e_{i}^{\\top}\\nabla\\varphi|$ be the coordinate-wise variability for that scalar function. Then, for any direction $i$ and any $0<\\dot{\\delta}<1/2$ , the following bound holds with probability of at least $1-2\\delta$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{n}\\frac{|\\bar{\\varphi}(\\pmb{x}+t e_{i})-\\bar{\\varphi}(\\pmb{x}-t e_{i})|}{2t}-\\|\\varphi_{i}^{\\prime}\\|_{1,\\mu}\\right|\\leq\\mathcal{O}(n^{-1/2}t^{-1}\\ln(2D/\\delta)^{1/2}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that the Theorem 3 is different from our setting by studying a scalar function as opposed to a vector valued function. However, we show that the result can be generalized to the latter case in Corollary 1 below via the estimator 62. ", "page_idx": 26}, {"type": "text", "text": "Corollary 1 (Consistency of the Estimator (62) for Vector-valued Function) Let $\\textbf{\\textit{f}}:\\;\\mathbb{R}^{D}\\;\\;\\to$ $\\mathbb{R}^{D_{v}}$ be a vector valued function and $\\|f_{i}^{\\prime}\\|_{1,\\mu}$ be defined as in Definition 1. Then, for any direction $i$ and any $0<\\delta<1/2$ , the following bound holds with probability of at least $1-2\\delta$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\widehat{m}_{i}-\\|f_{i}^{\\prime}\\|_{1,\\mu}|\\leq\\mathcal{O}(n^{-1/2}t^{-1}\\ln(2D/\\delta)^{1/2}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We first derive the relation between the left hand side of 64 and its coordinate-wise differences as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\widehat{m}_{i}-\\|f_{i}^{\\prime}\\|_{1,\\mu}\\Big\\vert=\\Bigg|\\mathbb{E}_{\\mu}\\left[\\frac{\\|\\widehat{f}(k+t_{\\alpha})-\\widehat{f}(k-t_{\\alpha})\\|_{1}}{2t}\\right]-\\mathbb{E}_{k\\sim\\mu}\\left[\\left\\|f_{j}(k)\\epsilon_{i}\\right\\|_{1}\\right]\\Bigg|}&{}\\\\ {=\\Bigg|\\mathbb{E}_{\\mu}\\left[\\sum_{\\widehat{\\xi}\\in[D]}\\frac{\\big|\\widehat{f}_{j}(k+t_{\\alpha})-\\widehat{f}_{j}(k-t_{\\alpha})\\big|}{2t}\\right]-\\mathbb{E}_{k\\sim\\mu}\\left[\\sum_{\\widehat{\\xi}\\in[D]}\\left|e_{i}^{-\\frac{1}{\\xi}}\\nabla f_{j}\\right|\\right]\\Bigg|}\\\\ {=\\Bigg|\\underset{\\mu\\in[D]}{\\sum}\\mathbb{E}_{\\mu}\\frac{\\big|\\widehat{f}_{j}(k+t_{\\alpha})-\\widehat{f}_{j}(k-t_{\\alpha})\\big|}{2t}-\\sum_{\\widehat{\\xi}\\in[D]}\\mathbb{E}_{k\\sim\\mu}\\big|e_{i}^{\\frac{1}{\\xi}}\\nabla f_{j}\\big|}\\\\ {=\\Bigg|\\underset{\\mu\\in[D]}{\\sum}\\left(m_{i}^{(j)}-\\|f_{i}^{\\prime}\\|_{1,\\mu}^{(j)}\\right)\\Bigg|\\quad(\\mathrm{definition~of~}m_{i}\\mathrm{~and~}\\|f_{i}^{\\prime}\\|_{1,\\mu}\\mathrm{~for~compon.}}\\\\ {\\leq\\sum_{\\widehat{\\xi}\\in[D]}\\left|m_{i}^{(j)}-\\|f_{i}^{\\prime}\\|_{1,\\mu}^{(j)}\\right)\\quad\\mathrm{(triang~close~mality)}}\\\\ {\\leq Q(n-1)^{2}t^{-1}\\ln(2P/\\delta)^{(1/2)},\\quad\\mathrm{~floserwise~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where line 65 follows from the definition of the $\\ell_{1}$ norm, line 66 follows from the linearity of expectation, the superscript $j$ indicates that the case is reduced to the scalar function case for each $j^{t h}$ summand individually. Note that the probability of the last bound is at least $(1-2\\delta/D)^{D}$ since each component-wise bound holds with probability at least $1-2\\delta/D$ . However, since we can choose $\\delta$ small enough such that $2\\delta<1$ , by Bernoulli\u2019s inequality $(1-\\dot{2}\\delta/D)^{D}\\ge1-2D\\delta/D=1-2\\delta$ . \u25a1 ", "page_idx": 26}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/fef0c771dd9c40f9564c28735bfe67034b383cdc3b35e43e1b76a6c01f3f1270.jpg", "table_caption": ["Table 9: Evaluation of the performance of our model and DeiT across multiple robustness benchmarks, using appropriate evaluation metrics for each. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Remark 9 Despite the proven consistency of this estimator, we opt for the efficient estimator presented in our main body described in Eqn 10. This is because the consistent estimator requires materialising the prediction function \u2013 that is, computing a forward pass of the self-attention mechanism \u2013 twice per dimension. This makes the consistent estimator unusable in most problem settings. We present results for the consistent estimator in Appendix F.11. ", "page_idx": 27}, {"type": "text", "text": "E Implementation Procedure and Computational Efficiency ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Training and Inference. Given Elliptical Attention requires keys and values from the previous layer in order to compute the required transformation, we can only implement Elliptical Attention from the second layer on. We incorporate our Elliptical Attention into both training and inference stages. This is because, firstly, Elliptical Attention is designed to offer improvements to both clean and contaminated data, and so even in the presence of completely clean train and test data, it is advantageous to incorporate Elliptical Attention into both stages. Secondly, it is commonplace to encounter data contamination in test data and indeed also highly possible to encounter it in train data as well. Therefore, in the interest of robustness as well, we also incorporate Elliptical Attention into both stages. ", "page_idx": 27}, {"type": "text", "text": "Computational Efficiency. Computing the required transformation requires no learnable parameters and is obtained simply by averaging absolute differences in values over layers. These operations are therefore just of the order $\\mathcal{O}(b h\\bar{n}D)=\\mathcal{O}(n)$ for batch size $b$ , head number $h$ , key/value length $n$ , and dimension $D$ . Hence upper-bound time complexity of the overall Transformer is unaffected. We provide efficiency analysis in terms of computation speed and max GPU memory allocated (calculated by CUDA max_memory_allocated in Figure 4, which shows that compared with baseline robust models, Elliptical is the fastest and most memory efficient. Elliptical exhibits no perceptible slowdown versus DeiT of the same configuration and only a $0.99\\%$ increase in max memory allocated, which is why Elliptical and DeiT are shown as the same data point in the Figure 4. ", "page_idx": 27}, {"type": "text", "text": "F Experimental Details and Additional Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "F.1 Out-of-Distribution Robustness and Data Corruption on ImageNet-A,R,C ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "ImageNet-A,R,C are benchmarks capturing a range of out-of-distribution and corrupted samples. ImageNet-A contains real world adversarially filtered images that fool current ImageNet classifiers. ImageNet-R contains various artistic renditions of object classes from the original ImageNet. ImageNet-C consists of 15 types of algorithmically generated corruptions with 5 levels of severity (e.g blurring, pixelation, speckle noise etc). Given that Elliptical Attention learns attention weights dependant on the transformation $_M$ , which is itself dependant on the train data distribution, our proposed model is not designed for situations in which the test distribution is substantially different from the train distribution. This then includes OOD robustness and robustness to heavy corruption to the point where the underlying data distribution is fundamentally different. We nonetheless evaluate Elliptical Attention on ImageNet-A,R,C to assess these important forms of robustness as well. Table 9 shows that Elliptical Attention is still able to offer improvements over baseline $D e i T$ in terms of OOD robustness, while maintaining approximately the same performance as the baseline for ImageNet-C. Figure 7 shows for $F o g$ and Pixelate corruptions how Elliptical compares with DeiT over the 5 severity levels, where we see that at low severity levels Elliptical improves over DeiT, however as the severity level gets too high Elliptical falls behind. This agrees with our expectation that as the severity level grows, the distribution is further shifted relative to the train distribution and so Elliptical Attention is unable to improve performance. ", "page_idx": 27}, {"type": "text", "text": "F.2 Representation Collapse ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We provide in Figure 6 additional representation collapse results for ImageNet and ADE20K, showing that across modalities Elliptical Attention resists representation collapse. ", "page_idx": 27}, {"type": "image", "img_path": "Ejg4d4FVrs/tmp/bc59d15c16830be44c6c5a3150e20c6dc777c421a1ee261ac51735bbd67a2982.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 6: Additional Representation Collapse Results on ADE20K, WikiText-103 and ImageNet. Elliptical reduces token similarity over layers across a range of modalities ", "page_idx": 28}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/8e8fd047051a6a8618cccf9226b92688e824aadad6ed6b37f6827f898609062e.jpg", "table_caption": ["Table 10: Additional Results on Imagenet Increasing Heads But Maintaining Overall Embedding Dimension "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.3 Head Redundancy ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We present in Table 18 head redundancy results on the two large-scale tasks, WikiText-103 language modelling and ImageNet-1K object classification. Mean $\\mathcal{L}_{2}$ distance between vectorized attention heads, with the mean taken over a batch of size 1000 and averaged layer-wise. We see that Elliptical improves head redundancy on WikiText-103 versus the baseline transformer while performing approxiamtely equally to the DeiT baseline on ImageNet. ", "page_idx": 28}, {"type": "image", "img_path": "Ejg4d4FVrs/tmp/00b4448787761322b3952100675a792d46780fe9e5741a192bcd4711065a6647.jpg", "img_caption": ["Figure 7: Comparison of DeiT versus Deit-Elliptical accuracies on two types of ImageNet-C corruptions, namely, Fog (left) and Pixelate (right). The figures show two out of many cases where DeiT-Elliptical outperforms its counterpart while vanilla $D e i T$ manages to exceed only at higher severity levels. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.4 Efficiency Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We present here the comparative efficiency results for DeiT and DeiT-Elliptical in a side-by-side comparison at tiny, small, and base sizes, along with DeiT-Elliptical compared with other robust baselines. ", "page_idx": 28}, {"type": "text", "text": "F.5 Elliptical Attention in Mixture of Expert Architectures ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We additionally evaluate Elliptical Attention within Mixture of Expert architectures. Specifically, we show in Tables 13, 14, and 15 the performance of Elliptical Attention within the Switch Transformer [18] and Generalized Language Model (GLaM) backbones [17]. ", "page_idx": 28}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/2b346e1e8aa46e0269e26d682a8107b461265414fc15801a2a6b6a3c7313cd4e.jpg", "table_caption": ["Table 11: Side-by-side Efficiency comparison of DeiT and DeiT-Elliptical "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/f6d4fbab88a331b4cada4a5ce315c52f02a439cfba3d08be532b75bf4a94f2c4.jpg", "table_caption": ["Table 12: Efficiency Comparison between Elliptical and baseline robust models "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "F.6 Additional Adversarial Attack Results on DeiT-Small Configuration ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We present here additional results for DeiT and DeiT-Elliptical at the Small configuration [78] (22.1M parameters) under adversarial attack. Table 16 shows the result of Elliptical against PGD, FGSM, and SPSA. Table 17 shows the results of Elliptical against Auto Attack. Given the larger model size, we attack with a perturbation budget of $3/255$ . ", "page_idx": 29}, {"type": "text", "text": "F.7 Wikitext-103 Language Modelling and Word Swap Attack ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Dataset. The WikiText- $103^{2}$ dataset contains around 268K words and its training set consists of about 28K articles with 103M tokens. This corresponds to text blocks of about 3600 words. The validation set and test sets consist of 60 articles with 218K and 246K tokens respectively. ", "page_idx": 29}, {"type": "text", "text": "Corruption. Word Swap Text Attack3 corrupts the data by substituting random words with a generic token \u2019AAA\u2019. We follow the setup of [23] and assess models by training them on clean data before attacking only the evaluation set using a substitution rate of $2.5\\%$ . ", "page_idx": 29}, {"type": "text", "text": "Model, Optimizer & Train Specification. We adopt the training regime of [54]. To this end, the small backbone uses 16 layers, 8 heads of dimension 16, a feedforward layer of size 2048 and an embedding dimension of 128. We use a dropout rate of 0.1. We trained with Adam using a starting learning rate of 0.00025 and cosine scheduling under default PyTorch settings. We used a batch size of 96 and trained for 120 epochs and 2000 warmup steps. The train and evaluation target lengths were set to 256. ", "page_idx": 29}, {"type": "text", "text": "The medium backbone uses 16 layers, 8 heads of dimension 32, a feedforward layer of size 2048 and embedding dimension of 256. We use a dropout rate of 0.1. We trained with Adam using a starting learning rate 0.00025 and cosine scheduling under default PyTorch settings. We used a batch size of 56 and trained for 100 epochs and 2000 warmup steps. The train and evaluation target lengths were set to 384. ", "page_idx": 29}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/655e6a25152b19a058b5112189d27f66d8b509e57f9e8817a018c16c8c694c27.jpg", "table_caption": ["Table 13: Elliptical Switch Transformers Pretrained on WikiText-103 and Finetuned on Stanford Sentiment Treebank 2 (SST-2) "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/c8247f2b17d5c364fe199963be0aa4697575be66593173b85f0aeb6d634a9694.jpg", "table_caption": ["Table 14: Elliptical Switch Transformers Pretrained on EnWik8 and Finetuned on Stanford Sentiment Treebank 2 (SST-2) "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "For Elliptical Attention, we use an Elliptical layer on all possible layers 2 through 16. We use a constant delta of 1. ", "page_idx": 30}, {"type": "text", "text": "Compute Resources. All models are trained and evaluated on two NVIDIA A100 SXM4 40GB GPUs. ", "page_idx": 30}, {"type": "text", "text": "F.8 ImageNet Image Classification and Adversarial Attack ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Dataset. We use the full ImageNet dataset that contains 1.28M training images and 50K validation images. The model learns to predict the class of the input image among 1000 categories. We report the top-1 and top-5 accuracy on all experiments. ", "page_idx": 30}, {"type": "text", "text": "Corruption. We use attacks FGSM [22], PGD [42], and Auto Attack [11] with perturbation budget 1/255 while SPSA [81] uses a perturbation budget 0.1. All attacks perturb under $l_{\\infty}$ norm. PGD attack uses 20 steps with step size of 0.15. ", "page_idx": 30}, {"type": "text", "text": "Model, Optimizer & Train Specification. The configuration follows the default DeiT tiny configuration [78]. In particular, we follow the experimental setup of [23, 54]. To this end, the DeiT backbone uses 12 layers, 3 heads of dimension 64, patch size 16, feedforward layer of size 768 and embedding dimension of 192. We train using Adam with a starting learning rate of 0.0005 using cosine scheduling under default PyTorch settings, momentum of 0.9, batch size of 256, 5 warmup epochs starting from 0.000001 and 10 cooldown epochs, for an overall train run of 300 epochs. The input size is 224 and we follow the default AutoAugment policy and color jitter 0.4. ", "page_idx": 30}, {"type": "text", "text": "For Elliptical Attention, we use an Elliptical layer on all possible layers 2 through 12. We use a constant delta of 1. ", "page_idx": 30}, {"type": "text", "text": "Compute Resources. We train and evaluate all models on four NVIDIA A100 SXM4 40GB GPUs, with the exception of the robustness experiments on ImageNet-C which are conducted using four NVIDIA Tesla V100 SXM2 32GB GPUs. ", "page_idx": 30}, {"type": "text", "text": "F.9 LRA Long Sequence Classification. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Dataset. The LRA benchmark consists 5 tasks involving long range contexts of up to 4000 in sequence length. These tasks consist of equation calculation (ListOps) [50], review classification (Text) [41], document retrieval (Retrieval) [61], image classification (Image) [32] and image spatial dependencies (Pathfinder) [37]. ", "page_idx": 30}, {"type": "text", "text": "Model, Optimizer & Train Specification. We adopt the same experimental setup as [7]. To that end, the Transformer backbone is set with 2 layers, hidden dimension of 128, 2 attention heads of dimension 32, and embedding dimension of 64. We use a dropout rate of 0.1. Built on top of the standard transformer backbone, Reformer uses 2 hashes, Performer has 256 random feature dimensions and Linformer uses a projection dimension of 256. We train with Adam using a learning rate of 0.0001 with linear decay. We use a batch size of 32 for ListOps, Retrieval, and Text and 256 for Image and Pathfinder32. We use 1000, 175, 312, 800, and 1000 warmup steps for ListOps, Image, Pathfinder32, Retrieval, and Text respectively. ", "page_idx": 30}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/73adce37fb4b0eda9851198e9ea9e3476273b56754cead88fb5191328d52163c.jpg", "table_caption": ["Table 15: Test Perplexity of Elliptical GLaM on WikiText-103 Modeling "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/8d6749a5e52933993d1b4a8949bfc3df34636d2d7c8f9827e39809c19c07a0a1.jpg", "table_caption": ["Table 16: DeiT and DeiT-Elliptical Accuracy on ImageNet Under Adversarial Attacks PGD, FGSM, and SPSA with Small Backbone Configuration "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/04445e79b3817745702a29c0b9d22b3ebba7b86fd76f1169ef8ce9d81d036ee2.jpg", "table_caption": ["Table 17: DeiT and DeiT-Elliptical Accuracy on ImageNet under Auto Attack with Small Backbone Configuration "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/c3ae272b0b296a2f37d01bfed65be7c1c5fc66b37dcde50c35a2d805baa3284a.jpg", "table_caption": ["Table 18: Head Redundancy Results "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Elliptical places the Elliptical Attention layer on the final layer (as the only one possible) and uses delta equal to 1. ", "page_idx": 31}, {"type": "text", "text": "Compute Resources. All models are trained and evaluated on a single NVIDIA A100 SXM4 40GB GPU. ", "page_idx": 31}, {"type": "text", "text": "F.10 ADE20K Image Segmentation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Dataset. ADE20K [88] contains challenging scenes with fine-grained labels and is one of the most challenging semantic segmentation datasets. The training set contains 20,210 images with 150 semantic classes. The validation and test set contain 2,000 and 3,352 images respectively. ", "page_idx": 31}, {"type": "text", "text": "Model, Optimizer & Train Specification. We follow the experimental setup as in [71]. The encoder is pretrained on ImageNet following the specification described in F.8 using the setup in [78, 54]. That is, the encoder is a DeiT backbone using 12 layers, 3 heads of dimension 64, patch size 16, feedforward layer of size 768 and embedding dimension of 192. We train using Adam with a starting learning rate of 0.0005 using cosine scheduling under default PyTorch settings, momentum of 0.9, batch size of 256, 5 warmup epochs starting from 0.000001 and 10 cooldown epochs, for an overall train run of 300 epochs. The input size is 224 and we follow the default AutoAugment policy and color jitter 0.4. After pretraining the encoder, we then attach as decoder a masked transformer consisting of 2 layers. Each layer contains 3 heads of dimension 64, embedding dimension of 192 and feedforward dimension of 768. The decoder uses a dropout rate of 0.1. The full segmenter (encoder and decoder) is then finetuned using SGD with starting learning rate 0.001 and polynomial scheduling. The batch size is set to 8. ", "page_idx": 31}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/a527b59be6f2bd87f77d34a0f46dfa36f634b63aa71c6e3f97410f1c0e9b5a15.jpg", "table_caption": ["Table 19: Perplexity (PPL) of Elliptical and baselines on WikiText-103 under Word Swap data contamination. Best results are in bold. Our Elliptical method achieve substantially better robust PPL without compromising performance on clean data. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Compute Resources. All models are trained and evaluated on a single NVIDIA A100 SXM4 40GB GPU. ", "page_idx": 32}, {"type": "text", "text": "F.11 Ablation Studies ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Ablation Models. We consider the following models in our ablation studies: ", "page_idx": 32}, {"type": "text", "text": "\u2022 Random Ablation. To validate the efficacy of our proposed estimator given in Eqn. 10, we consider an alternate model in which $M$ is populated by weights uniformly drawn from the [0, 1] interval followed by the same maxscaling as in Elliptical.   \n\u2022 Elliptical-Meanscale. We ablate the effect of maxscaling by considering meanscaling of the estimates $m_{i}$ . That is, each $m_{i}\\leftarrow m_{i}/\\bar{m}$ is scaled by the mean variability estimate $\\bar{m}=\\mathbb{E}_{D}[m_{i}]$ .   \n\u2022 Elliptical-Consistent. We consider also the performance of Elliptical when using the consistent estimator of $\\|f_{i}^{\\prime}\\|_{1,\\mu}$ described by Equation 62. ", "page_idx": 32}, {"type": "text", "text": "Language Modelling. Results are shown in Table 19. Amazingly, the random ablation model performs extremely well on contaminated data. In general, this most likely suggests that training a model with randomness injected into the attention matrix can generate some robustness benefits, which is intuitive. It does, less surprisingly, come at the cost of clean data performance, where Random Ablation performs almost $10\\%$ worse than baseline transformer. ", "page_idx": 32}, {"type": "text", "text": "F.12 Pseudocode ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Algorithm 1 presents a pseudocode for implementing Elliptical Attention as given by Eqn. 13 on top of conventional self-attention. ", "page_idx": 32}, {"type": "text", "text": "ImageNet Classification and Attack. Table 21 shows the ablation model\u2019s performance on both clean ImageNet and under Auto Attack. The ablation model shows a slight improvement over the DeiT baseline in Top 1 accuracy, however Top 5 accuracy is substantially lower. Reasonable performance again Auto Attack is overall unsurprising given that the random Random Ablation model is essentially employing random defence. Nonetheless, it still does not surpass the performance of Elliptical. ", "page_idx": 32}, {"type": "text", "text": "Algorithm 1 Computation of Elliptical Attention ", "page_idx": 33}, {"type": "text", "text": "Require:   \n1: Tensor Q \u2208RN\u00d7D \u25b7current layer queries   \n2: Tensor K \u2208RN\u00d7D \u25b7current layer keys   \n3: Tensor V \u2208RN\u00d7D \u25b7current layer values   \n4: Tensor V prev \u2208RN\u00d7D \u25b7previous layer values   \n5: float $\\delta\\in\\mathbb{R}_{+}$ \u25b7step size   \n6: integer $D\\in\\mathbb{N}$ \u25b7head dimension   \n7: function ELLIPTICAL_ATTENTION(Q, K, V , V prev, \u03b4, D)   \n8: $M\\gets$ ELLIPTICAL_WEIGHT $\\mathrm{\\boldmath~\\xi~}_{\\mathrm{3}}(V,V^{\\mathrm{prev}},\\delta)$ \u25b7compute weight matrix M   \n9: logits $\\begin{array}{r}{\\leftarrow Q\\times M\\times K^{\\top}\\times\\frac{1}{\\sqrt{D}}}\\end{array}$ \u25b7modify the dot-product computation   \n10: attenti $\\mathsf{o n}\\gets\\mathrm{soFTMAX}(\\mathtt{l o g i t s})$   \n11: output $\\leftarrow$ attention $\\times\\,V$   \n12: return output   \n13: end function   \n14: function ELLIPTICAL_WEIGHTS $(V,V^{\\mathrm{prev}},\\delta)$   \n15: with torch.no_grad() do   \n16: $N\\leftarrow V$ .size(0) \u25b7sequence length   \n17: value_diff $\\leftarrow(V-V^{\\mathrm{prev}})/\\delta$   \n18: $\\begin{array}{r}{M\\gets\\frac{1}{N}\\times\\mathrm{NORM}(\\mathtt{v a l u e\\_d i f f},p=1,\\mathtt{d i m}=0)\\,\\mathtt{v a l u e\\_d i f f},}\\end{array}$ column-wise average of ${\\mathcal{L}}_{1}$ norms   \n19: M \u2190DIAG_EMBED(M) $\\triangleright$ embed the vector into a diagonal matrix   \n20: return M   \n21: end function ", "page_idx": 33}, {"type": "text", "text": "Table 20: Evaluation of the performance of our model and DeiT across multiple robustness benchmarks, using appropriate evaluation metrics for each. ", "page_idx": 33}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/f1934a8ecbb8a98a8c60fdb083d1d17a094da7681a54813abd792fd913dbcc15.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Table 21: Auto Attack Ablation Study: Top 1 and Top 5 test accuracies on clean ImageNet and under Auto Attack. The ablation model fails to fit the clean data well and is highly prone to adversarial attack. ", "page_idx": 33}, {"type": "table", "img_path": "Ejg4d4FVrs/tmp/8264647c0e6958837f85aaae3e7edaa0253bc50627dda5ebbb6172e3a46e4729.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Our research offers benefits to both clean data and robust performance. We in particular show improved results in domains with wide social applicability. These include image segmentation, with benefits to self-driving cars, and language modeling, with benefits to AI chatbot assistants. We in particular show strong improvements against contamination by adversarial attack, which we hope can protect vital AI systems from malicious actors, and competitive performance in contaminated language modeling, which we hope can improve language models evaluated on imperfect data as is often the case in the real world. There is always possibility of misuse of AI systems, however our research shows substantive improvements in fundamental architectures and theory which we hope can spur further socially beneficial outcomes. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our paper claims that using a Mahalanobis metric inside of the self-attention mechanism to produce hyper-ellipsoidal neighborhoods around queries improves both robustness and representation collapse. We provide a theoretical framework for this with proofs and a large amount of empirical validation. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We mention in our future work (section 6) that our estimator is noisy. In Appendix D we provide and prove a consistent estimator but note it is too computationally expensive, hence we need to opt for our noisier but more efficient estimator. As a result we do not prove the consistency of our estimator, but just the weaker requirement which is that it accurately estimates the relative magnitudes of the direction-wise variability. For this we assume approximate separability of the true function. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 35}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All theoretical results are numbered and have a referenced section in the Appendix where all required lemmas and assumptions are stated and proven. All proofs make each step as clear as possible. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We include in Appendix F all hyperparameters, model configuration, training and inference specifications, and dataset details. For corruption, we additionally include all perturbation budgets, steps, step sizes, norm specification, and additional details. We provide the exact equation for our coordinate-wise variability estimator. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide all code used, packaged into folders for each set of experiments (e.g Wikitext-103). Each folder then contains a bash to run the required result (e.g run_elliptical.sh or run_baseline.sh). Where possible, we also include bashes to download the data. Folders also contain readme files providing information on version and package dependencies. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: In Appendix F we provide all dataset details, train and test splits, compute resources, and other experimental configuration information. We also include citations to other authors who we compare with for which we adopt the same experimental setting. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: In our head redundancy experiments we report error bars, in particular showing the difference in performance between DeiT and Elliptical is not statistically significant. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide an efficiency analysis figure (Figure 4) containing computation time and memory resources. We also provide in Appenxix F the exact GPU resources used. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our research involves no human subjects or privacy concerns. Our research also has no clear links to discriminatory, unsafe, or harmful outcomes. Rather, as it is in large part concerned with robustness, particularly to adversarial attack, we hope our research might be usable to defend vital AI systems from ill-intentioned actors. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We discuss broader impacts in Appendix G. We see our improved accuracy and robustness as offering societal beneftis, for example with self-driving cars by our improved image segmentation results or with our adversarially robust vision models to defend against ill-intentioned actors. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper contains no such models with high risk of misuse such as pretrained language models, image generators, or scraped datasets. We use widely accepted, standard benchmark datasets and propose a fundamental and general improvement to a core architecture. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 39}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: In places where code has been borrowed from public repositories or baseline models have been implemented, we have duly credited. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We include details about training and implementation as well as limitations for our novel class of Elliptical Attention mechanisms. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do not use any crowdsourcing nor research with human subjects. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We do not use any crowdsourcing nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]