[{"figure_path": "Ejg4d4FVrs/tables/tables_7_1.jpg", "caption": "Table 1: Perplexity (PPL) on WikiText-103 under Word Swap contamination. Elliptical achieves top PPL in clean data and second best in contaminated. Best result in bold and second best underlined.", "description": "This table presents the results of the WikiText-103 language modeling experiment under word swap attack.  It compares the perplexity (PPL) scores of different transformer models, including the proposed Elliptical Attention model, on both clean and contaminated test sets. Lower perplexity indicates better performance.", "section": "4 Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_7_2.jpg", "caption": "Table 2: Top-1 and Top-5 Test accuracy on ImageNet under adversarial attacks PGD, FGSM, and SPSA with perturbation budget 1/255. Best result shown in bold and second best shown underlined.", "description": "This table presents the top-1 and top-5 test accuracy results on the ImageNet dataset under three different adversarial attacks (PGD, FGSM, and SPSA) with a perturbation budget of 1/255.  It compares the performance of the Elliptical Attention method against several other state-of-the-art methods (DeiT, Distill, FourierFormer, RVT, DeiT-KDE, DeiT-MoM). The best accuracy for each attack and metric is highlighted in bold, and the second-best is underlined.  The results demonstrate the robustness of the Elliptical Attention method in handling adversarial attacks.", "section": "4.2 Image Classification under Adversarial Attack"}, {"figure_path": "Ejg4d4FVrs/tables/tables_8_1.jpg", "caption": "Table 3: Test accuracy on long range tasks: ListOps, Text, Retrieval, Image, and Pathfinder. Best result in bold and second best underlined.", "description": "This table presents the test accuracy results of different transformer models on five long-range tasks: ListOps, Text, Retrieval, Image, and Pathfinder.  The sequence lengths for these tasks vary.  The models compared are the standard Transformer, Linformer, Reformer, Performer, Longformer, and the proposed Elliptical Attention model. The best and second-best performing models for each task are highlighted in bold and underlined, respectively. The final row shows the average accuracy across all five tasks.  This provides a comparison of the model's performance on a variety of long-range sequence tasks, showcasing Elliptical Attention's competitive performance in terms of accuracy.", "section": "4 Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_8_2.jpg", "caption": "Table 4: Top-1 and Top-5 Test accuracy on ImageNet under adversarial attacks PGD, FGSM, and SPSA with perturbation budget 1/255. Best result shown in bold and second best shown underlined.", "description": "This table presents the results of an ImageNet classification experiment under various adversarial attacks (PGD, FGSM, and SPSA).  It compares the top-1 and top-5 accuracies of different models, including the proposed Elliptical Attention model, against standard DeiT and robust vision transformer models. The best and second-best accuracies are highlighted.", "section": "4 Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_8_3.jpg", "caption": "Table 1: Perplexity (PPL) on WikiText-103 under Word Swap contamination. Elliptical achieves top PPL in clean data and second best in contaminated. Best result in bold and second best underlined.", "description": "This table shows the results of the WikiText-103 language modeling experiment under Word Swap attack.  The performance is measured by perplexity (PPL), a lower score indicating better performance.  The table compares the performance of the proposed Elliptical Attention method against several baseline transformer models, both in a clean setting and under the Word Swap data contamination.  Elliptical achieves the best perplexity on clean data and is second best on contaminated data, indicating robustness.", "section": "4.1 Robust Language Modelling"}, {"figure_path": "Ejg4d4FVrs/tables/tables_8_4.jpg", "caption": "Table 15: Test Perplexity of Elliptical GLaM on WikiText-103 Modeling", "description": "This table presents the test perplexity results for the GLaM language model with and without the proposed Elliptical Attention mechanism.  Two different sizes of the GLaM model are evaluated: small and medium. Lower perplexity indicates better performance.", "section": "4.5 Further Clean Data Language Modelling"}, {"figure_path": "Ejg4d4FVrs/tables/tables_9_1.jpg", "caption": "Table 7: Image Segmentation Results", "description": "This table presents the results of image segmentation on the ADE20K dataset.  It compares the performance of the DeiT model and the Elliptical Attention model, reporting Pixel Accuracy, Average Accuracy, and Average Intersection over Union (IoU).  The Elliptical model shows improvements across all three metrics, demonstrating enhanced performance in image segmentation tasks.", "section": "4 Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_9_2.jpg", "caption": "Table 1: Perplexity (PPL) on WikiText-103 under Word Swap contamination. Elliptical achieves top PPL in clean data and second best in contaminated. Best result in bold and second best underlined.", "description": "This table compares the perplexity (PPL) scores achieved by different language models on the WikiText-103 benchmark under both clean and Word Swap contaminated conditions.  The results show that the Elliptical model achieves the lowest PPL score in the clean setting and the second lowest score under contamination.", "section": "Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_27_1.jpg", "caption": "Table 9: Evaluation of the performance of our model and DeiT across multiple robustness benchmarks, using appropriate evaluation metrics for each.", "description": "This table compares the performance of DeiT and DeiT-Elliptical on ImageNet robustness benchmarks (ImageNet-R, ImageNet-A, ImageNet-C, and ImageNet-C (Extra)).  It shows top-1 accuracy for ImageNet-R and ImageNet-A, and mean corruption error (mCE) for ImageNet-C and ImageNet-C (Extra). The results indicate how well each model performs under different types of image corruption and adversarial attacks.  The mCE values show lower is better.", "section": "4 Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_28_1.jpg", "caption": "Table 10: Additional Results on Imagenet Increasing Heads But Maintaining Overall Embedding Dimension", "description": "This table shows the impact of increasing the number of attention heads while keeping the total number of parameters constant in the DeiT and Elliptical models.  It demonstrates that Elliptical Attention consistently achieves higher top-1 and top-5 accuracy compared to the baseline DeiT model across different head configurations.", "section": "4.2 Image Classification under Adversarial Attack"}, {"figure_path": "Ejg4d4FVrs/tables/tables_29_1.jpg", "caption": "Table 11: Side-by-side Efficiency comparison of DeiT and DeiT-Elliptical", "description": "This table compares the computation speed, maximum memory usage, FLOPs per sample, and number of parameters for DeiT and DeiT-Elliptical models at three different scales: Tiny, Small, and Base.  It shows the percentage change in each metric between the two models at each scale. The results indicate that Elliptical Attention offers efficiency improvements with only a slight increase in memory usage.", "section": "F.4 Efficiency Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_29_2.jpg", "caption": "Table 12: Efficiency Comparison between Elliptical and baseline robust models", "description": "This table compares the computational efficiency (compute speed in iterations per second), memory usage (max memory in kilobytes), floating point operations per sample, and the number of parameters (in millions) for different models.  The models compared include DeiT-MoM, DeiT-RKDE, DeiT-SPKDE, DeiT-RVT, and DeiT-Elliptical (the proposed model). It highlights that DeiT-Elliptical achieves good performance with reasonable efficiency and memory usage.", "section": "Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_30_1.jpg", "caption": "Table 13: Elliptical Switch Transformers Pretrained on WikiText-103 and Finetuned on Stanford Sentiment Treebank 2 (SST-2)", "description": "This table compares the performance of standard Switch Transformers and Switch Transformers with Elliptical Attention on two different sizes of models (medium and large) after pretraining on WikiText-103 and fine-tuning on Stanford Sentiment Treebank 2 (SST-2).  It shows the test perplexity (PPL) and finetune test accuracy. Elliptical Attention consistently improves both metrics over the baseline Switch Transformers for both model sizes.", "section": "4.1 Robust Language Modelling"}, {"figure_path": "Ejg4d4FVrs/tables/tables_30_2.jpg", "caption": "Table 14: Elliptical Switch Transformers Pretrained on EnWik8 and Finetuned on Stanford Sentiment Treebank 2 (SST-2)", "description": "This table presents the results of experiments using Switch Transformers, comparing the standard Switch Transformer with the proposed Elliptical Switch Transformer.  The models were pretrained on the EnWik8 dataset and then finetuned on the Stanford Sentiment Treebank 2 (SST-2) dataset. The table shows the test bits-per-character (BPC) and the finetune test accuracy for both models.  The results demonstrate the improved performance of the Elliptical Switch Transformer compared to the standard Switch Transformer after finetuning.", "section": "4.1 Robust Language Modelling"}, {"figure_path": "Ejg4d4FVrs/tables/tables_31_1.jpg", "caption": "Table 15: Test Perplexity of Elliptical GLaM on WikiText-103 Modeling", "description": "This table presents the results of the WikiText-103 language modeling experiments using the GLAM (Generalist Language Model) architecture. It compares the performance of the standard GLAM model with the Elliptical Attention-enhanced GLAM model (GLAM-Elliptical) across two different sizes: small and medium.  The results are expressed in terms of Test Perplexity (PPL), a lower score indicating better performance.  The table shows the improvements achieved by incorporating Elliptical Attention into the GLAM model for both sizes.", "section": "4 Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_31_2.jpg", "caption": "Table 2: Top-1 and Top-5 Test accuracy on ImageNet under adversarial attacks PGD, FGSM, and SPSA with perturbation budget 1/255. Best result shown in bold and second best shown underlined.", "description": "This table shows the top-1 and top-5 accuracy of DeiT and DeiT with Elliptical Attention on ImageNet under three different adversarial attacks (PGD, FGSM, and SPSA) with a perturbation budget of 1/255.  The results are presented for both clean data and data subjected to adversarial attacks.  The best and second-best results in each attack category are highlighted in bold and underlined, respectively, to demonstrate the performance improvement achieved by incorporating Elliptical Attention.", "section": "4.2 Image Classification under Adversarial Attack"}, {"figure_path": "Ejg4d4FVrs/tables/tables_31_3.jpg", "caption": "Table 2: Top-1 and Top-5 Test accuracy on ImageNet under adversarial attacks PGD, FGSM, and SPSA with perturbation budget 1/255. Best result shown in bold and second best shown underlined.", "description": "This table presents the top-1 and top-5 test accuracy results on the ImageNet dataset under three different adversarial attacks (PGD, FGSM, and SPSA) with a perturbation budget of 1/255.  The results are compared across several different models. The best performance for each metric is highlighted in bold, and the second-best is underlined. This demonstrates the models' robustness to adversarial attacks.", "section": "4.2 Image Classification under Adversarial Attack"}, {"figure_path": "Ejg4d4FVrs/tables/tables_31_4.jpg", "caption": "Table 18: Head Redundancy Results", "description": "This table presents the results of measuring head redundancy for both the baseline transformer and the Elliptical Attention model on two large-scale tasks: WikiText-103 language modeling and ImageNet-1K object classification.  For each task and model, the table shows the number of attention heads, the dimension of each head, and the mean L2 distance between the vectorized attention heads (averaged across layers and batches).  The L2 distance is a measure of head redundancy; higher values indicate greater redundancy.", "section": "4.1 Robust Language Modelling"}, {"figure_path": "Ejg4d4FVrs/tables/tables_32_1.jpg", "caption": "Table 1: Perplexity (PPL) on WikiText-103 under Word Swap contamination. Elliptical achieves top PPL in clean data and second best in contaminated. Best result in bold and second best underlined.", "description": "The table compares the performance of various language models, including the proposed Elliptical Attention model, on the WikiText-103 benchmark under clean and Word Swap contamination conditions.  The perplexity (PPL) score, a measure of how well a model predicts a sequence of words, is shown for each model under both clean and contaminated scenarios. Elliptical achieves the best PPL on clean data and is second-best on contaminated data, demonstrating its robustness.", "section": "4 Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_33_1.jpg", "caption": "Table 9: Evaluation of the performance of our model and DeiT across multiple robustness benchmarks, using appropriate evaluation metrics for each.", "description": "This table presents a comparison of the performance of the proposed Elliptical Attention model and the DeiT model across various ImageNet robustness benchmarks.  It shows the Top-1 accuracy for ImageNet-R (real-world adversarial examples) and ImageNet-A (artistic renditions), and the mean Corruption Error (mCE) for ImageNet-C (algorithmically generated corruptions) and ImageNet-C (Extra).  The results highlight the relative performance of each model under different types of image perturbations.", "section": "Experimental Results"}, {"figure_path": "Ejg4d4FVrs/tables/tables_33_2.jpg", "caption": "Table 2: Top-1 and Top-5 Test accuracy on ImageNet under adversarial attacks PGD, FGSM, and SPSA with perturbation budget 1/255. Best result shown in bold and second best shown underlined.", "description": "This table presents the results of the ImageNet classification experiment under three different adversarial attacks: PGD, FGSM, and SPSA.  The performance of DeiT and DeiT-Elliptical is compared with other baseline models. The \"Top 1\" and \"Top 5\" columns represent the top-1 and top-5 accuracy, respectively.  A perturbation budget of 1/255 was used for all attacks.  The best results are bolded, and the second best are underlined. The table demonstrates the robustness of DeiT-Elliptical against adversarial attacks.", "section": "4.2 Image Classification under Adversarial Attack"}]