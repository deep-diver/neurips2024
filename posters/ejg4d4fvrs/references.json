{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation for the self-attention mechanisms analyzed and extended in the current work."}, {"fullname_first_author": "Krzysztof Marcin Choromanski", "paper_title": "Rethinking attention with performers", "publication_date": "2021-01-01", "reason": "This paper presents the Performer, a state-of-the-art attention mechanism that is compared against in the current work's experiments and analysis."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "publication_date": "2020-04-05", "reason": "This paper introduces the Longformer, which addresses the limitations of standard Transformers when processing long sequences; a key area where elliptical attention demonstrates advantages."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper details a vision Transformer that is used as a baseline model for image classification tasks in the empirical evaluation, providing a strong comparison for elliptical attention."}, {"fullname_first_author": "Yi Tay", "paper_title": "Long range arena : A benchmark for efficient transformers", "publication_date": "2021-01-01", "reason": "This paper presents a benchmark for efficient Transformers used to evaluate the efficiency and performance of the proposed elliptical attention mechanism against other efficient Transformers."}]}