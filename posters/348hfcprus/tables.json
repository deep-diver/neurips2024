[{"figure_path": "348hfcprUs/tables/tables_7_1.jpg", "caption": "Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. \"WR\" refers to win-rate, and \"LC-WR\" refers to length-controlled win-rate.", "description": "This table presents the win-rate and length-controlled win-rate results for different language models (Mistral-7B, Llama-3-8B, Llama-3-8B-Instruct) evaluated using the reward model ArmoRM-Llama-3-8B and scored by GPT-4-Turbo.  The results show the performance of Best-of-N (BoN) with varying N values and the proposed Speculative Rejection method, providing a comparison of their accuracy in generating high-quality responses.", "section": "5.2 Win-rate Evaluation"}, {"figure_path": "348hfcprUs/tables/tables_8_1.jpg", "caption": "Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. \"WR\" refers to win-rate, and \"LC-WR\" refers to length-controlled win-rate.", "description": "This table presents the win-rate (WR) and length-controlled win-rate (LC-WR) results for different language models (Mistral-7B, Llama-3-8B, Llama-3-8B-Instruct) using various settings.  The win-rates are calculated against a baseline Best-of-N (BoN) method with different values of N (120, 240, 480, 960, 1920, 3840) and are compared to the proposed SPECULATIVE REJECTION method with a rejection rate (\u03b1) of 0.5.  The reward model used for scoring is ArmoRM-Llama-3-8B, and GPT-4-Turbo is used for evaluation.  The results demonstrate the relative performance of the proposed method against the baseline BoN.", "section": "5.2 Win-rate Evaluation"}, {"figure_path": "348hfcprUs/tables/tables_9_1.jpg", "caption": "Table 1: Win-rate results across various settings for the Mistral-7B, Llama-3-8B, and Llama-3-8B-Instruct models, scored by the reward model ArmoRM-Llama-3-8B and evaluated using GPT-4-Turbo. \"WR\" refers to win-rate, and \"LC-WR\" refers to length-controlled win-rate.", "description": "This table presents the win-rate and length-controlled win-rate results for different language models (Mistral-7B, Llama-3-8B, Llama-3-8B-Instruct) evaluated using GPT-4-Turbo. The models' performance is compared against the Best-of-N baseline (BoN) with varying values of N (120, 240, 480, 960, 1920, 3840) and Speculative Rejection (Ours) with a rejection rate of 0.5. Win-rate signifies the percentage of times a model's generated response is preferred over the Best-of-N response, while length-controlled win-rate accounts for response length differences.", "section": "5.2 Win-rate Evaluation"}]