{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational model for the research, providing the core vision-language model that TripletCLIP builds upon and improves."}, {"fullname_first_author": "Soravit Changpinyo", "paper_title": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "publication_date": "2021-06-01", "reason": "This paper introduces the CC12M dataset, a primary dataset used for training and evaluating TripletCLIP, demonstrating its significance as a core data resource."}, {"fullname_first_author": "Mert Yuksekgonul", "paper_title": "When and why vision-language models behave like bags-of-words, and what to do about it?", "publication_date": "2022-01-01", "reason": "This paper directly addresses the problem of compositional reasoning in vision-language models, which TripletCLIP aims to solve, making it highly relevant foundational research."}, {"fullname_first_author": "Christoph Schuhmann", "paper_title": "LAION-5B: An open large-scale dataset for training next generation image-text models", "publication_date": "2022-01-01", "reason": "This paper introduces the LAION-5B dataset, a large-scale dataset that informs the creation of TripletData, highlighting its contribution to the data resources utilized."}, {"fullname_first_author": "Cheng-Yu Hsieh", "paper_title": "SugarCrepe: Fixing hackable benchmarks for vision-language compositionality", "publication_date": "2024-01-01", "reason": "This paper introduces the SugarCrepe benchmark, a crucial evaluation metric used to assess the improved compositional reasoning capabilities of TripletCLIP, showcasing its importance in model evaluation."}]}