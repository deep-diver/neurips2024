[{"type": "text", "text": "TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maitreya Patel\u2662\u2217 Abhiram Kusumba\u2662\u2020 Sheng Cheng\u2662\u2020 Changhoon Kim\u2662 Tejas Gokhale\u2661 Chitta Baral\u2662 Yezhou Yang\u2662 \u2662Arizona State University \u2661University of Maryland, Baltimore County tripletclip.github.io ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between textual and visual modalities to learn representations. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show that generating \u201chard\u201d negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators offers a solution. We introduce a novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. We demonstrate that our method, named TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances the compositional capabilities of CLIP, resulting in an absolute improvement of over $9\\%$ on the SugarCrepe benchmark on an equal computational budget, as well as improvements in zero-shot image classification and image retrieval. Our code, models, and data are available at: tripletclip.github.io. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale vision-language models, such as CLIP [41], have significantly advanced multi-modal learning by employing contrastive learning to acquire shared semantic representations from paired datasets. This approach has resulted in improved performance in vision-language tasks as well as zero-shot image classification [51] and segmentation [23, 62]. Beyond vision-language tasks, the individual components of these models, such as the vision encoder and the language encoder, are integral to several multimodal architectures and generative models such as multimodal large language models (MLLMs) [30, 26] and text-to-image (T2I) diffusion models [45, 37, 38]. Yet, compositional reasoning remains challenging and multimodal models continue to exhibit na\u00efve \u201cbag of words\u201d behavior, frequently failing to distinguish between expressions like \u201cbulb in the grass\u201d and \u201cgrass in the bulb\u201d [58, 53]. Addressing this challenge remains critical for enhancing vision-language models and their downstream applications. ", "page_idx": 0}, {"type": "text", "text": "Contrastive learning of representations benefits from \u201chard negative samples\u201d (i.e., points that are difficult to distinguish from an anchor point) [43]. However, at each optimization step for training CLIP, image-text pairs are randomly sampled from the training dataset \u2013 this random sampling seldom exposes the model to highly similar negative pairs. We hypothesize that the limited compositional understanding of CLIP may stem from such issues in the optimization objective and sampling from training datasets. A straightforward solution could involve iteratively identifying hard negative pairs for each training iteration. However, due to the noisy captions and the scarcity of such pairs in existing datasets, prior work generates hard negative captions as a form of augmentation using rule-based strategies [58, 61]. For instance, given an image-text pair labeled \u201ca brown horse\u201d, an additional negative caption \u201ca blue horse\u201d might be introduced. However in prior work, image data is not subjected to similar hard negative semantic augmentation during training; this is mainly because of the difficulty of making semantic perturbations at the pixel levels compared to sentence perturbation. While the text-only augmentation strategies have improved the models\u2019 compositional understanding to a certain extent, it raises an intriguing question: could incorporating hard negative augmentation for both text and image modality further enhance the compositional reasoning capabilities of vision-language models? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Motivated by this, in this paper, we introduce a novel, simple, and yet highly effective strategy for integrating hard negative images as well as hard negative text to enhance the compositional understanding of vision-language models. Recent developments in text-to-image diffusion models have opened up possibilities for performing semantic perturbations within images [21]. Existing works have evaluated the impact of creating synthetic data for text-to-image generative models [3, 6]. However, it remains less explored how these generative models can benefit the CLIP-like models. To tackle this challenge, our approach leverages the in-context learning capabilities of LLMs to produce realistic, linguistically accurate negative captions [55]. We then employ a pre-trained textto-image diffusion model to create images corresponding to these captions, thereby enriching any given image-text dataset with valuable hard negatives that foster improved reasoning. This resulting TripletData comprises 13M image-text pairs to complement the CC3M and CC12M datasets [5]. ", "page_idx": 1}, {"type": "text", "text": "We developed TripletCLIP, which incorporates hard negative image-text pairs effectively by using them to optimize a novel triplet contrastive loss function. Extensive experiments on the CC3M and CC12M datasets and various downstream tasks with an equal compute budget demonstrate that TripletCLIP significantly enhances compositional reasoning. Notably, TripletCLIP results in more than $9\\%$ and $6\\%$ absolute improvement on the SugarCrepe benchmark compared to LaCLIP and NegCLIP, respectively. TripletCLIP also improves zero-shot classification and image-text retrieval performance with similar training-time concept diversity. An investigation into the effects of increasing training-time concept diversity revealed that baseline models consistently under-performed in compositional tasks despite an increase in integrated knowledge, while TripletCLIP demonstrated significant improvements. In summary, our key contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a novel CLIP pre-training strategy that employs hard negative images in conjunction with triplet contrastive learning to enhance compositionality.   \n\u2022 TripletCLIP consistently improves across downstream tasks, demonstrating the effectiveness of synthesizing hard negative image-text pairs.   \n\u2022 Our extensive ablations on the choice of the loss function, modality-specific pre-training, the increase in concept diversity, and flitering high-quality TripletData provide deeper insights into the utility of hard negative image-text pairs for CLIP pre-training.   \n\u2022 Ultimately, we present a promising avenue where synthetic contrastive datasets significantly improve reasoning capabilities, leading to the creation and release of the TripletData \u2014 a 13M contrastive image-text dataset. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Vision-Language Models. Recent advancements, including ALIGN [22] and CLIP [41], have gained significant interest due to their capability to learn transferable semantic representations across multiple modalities through contrastive learning. These models facilitate downstream tasks such as zero-shot classification [51], image-text retrieval [60, 2], visual grounding/reasoning [31], textto-image generation [52, 37, 38, 24], semantic segmentation [62, 23], and various evaluations [19, 47]. Subsequent research has sought to enhance various aspects of these models, including data efficiency [15], hierarchical representation learning [8], and the quantization of latent spaces for more stable pre-training [7]. LiT [59] employs a pre-trained frozen CLIP vision encoder to fine-tune a BERT-like text encoder [9], achieving notable improvements in zero-shot transfer performance. Similarly, BLIP-2 [27] combines contrastive pre-training with the next-token prediction for image captioning during training. However, these approaches generally presume the availability of highquality data. In contrast, TripletCLIP focuses on leveraging the proposed hard negative contrastive dataset and incorporating triplet contrastive pre-training for compositional data. This approach is orthogonal to prior works. ", "page_idx": 1}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/a75c09001dc770dfa056671596f5de8a979c6761a63853112b798a5323694a79.jpg", "img_caption": ["Figure 1: Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. $(x,y)$ represents the positive a image-text pair, and $(x^{\\prime},y^{\\prime})$ represents the corresponding negative image-text pair. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Data for Contrastive Pre-training. The effectiveness of maximizing mutual information between modalities heavily relies on the quality of extensive, web-scraped datasets that ideally encompass all possible concepts and knowledge. For instance, despite its noise, the LAION dataset [48, 16], which includes more than 5 billion internet images paired with alt-text captions, is a primary resource. Studies show that over 1 billion data points are necessary to match the performance of the original CLIP model [16, 56]. Recent works like DataComp [16] and MetaCLIP [56] have focused on creating smaller, high-quality datasets by applying stringent filters and ensuring wordnet [33] synset-level concept diversity. Nevertheless, the inherently noisy nature of internet-scraped datasets can degrade model performance. Studies such as SynthCLIP [18] demonstrate that tripling the volume of fully synthetic data is required to equal the efficacy of real data. Other efforts like VeCLIP [25] and LaCLIP [14] enhance dataset quality by using generative language models to re-caption existing images, significantly boosting performance. ", "page_idx": 2}, {"type": "text", "text": "Compositionality for vision-language. Despite the increased emphasis on data quality and modeling techniques, mastering compositionality remains a significant challenge for vision-language models. Benchmarks like ARO [58], VALSE [35], and CREPE [32] have been developed to assess models\u2019 abilities to handle compositional data. SugarCrepe [20], in particular, offers a large-scale, systematic framework for such evaluations. Previous methods primarily focused on identifying hard negatives within existing datasets or generating synthetic negative captions [58, 61, 12, 11, 57, 49]. However, these rule-based generated captions are often unrealistic and linguistically flawed, leading to suboptimal model performance on complex datasets like SugarCrepe. A handful of works focus on finding negative images. [54] propose utilizing the video data. [44] focuses on object-centric image-editing to synthesize the negative images. [39] utilizes the simulation-based data negative data. ", "page_idx": 2}, {"type": "text", "text": "Contrary to prior approaches that predominantly add unrealistic negative captions or very constrained negative images that are either very synthetic or object-focused, this work introduces TripletCLIP, which centers on generating naturally occurring hard negative image-text pairs. We propose a novel triplet contrastive learning strategy that effectively utilizes these challenging data pairs. Additionally, while our method is distinct, integrating advancements that refine contrastive learning could potentially boost TripletCLIP\u2019s efficacy further. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section begins with an overview of the contrastive learning algorithm used by CLIP and NegCLIP. We then describe the synthetic data generation pipeline for generating hard negatives using LLMs and T2I models and introduce triplet contrastive learning which forms the basis of TripletCLIP. A high-level comparison between prior work and TripletCLIP can be found in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal for self-supervised contrastive learning [13], when dealing with inputs from a single modality, is to use a feature extractor $(F)$ to encode inputs and their augmentations and minimize ", "page_idx": 2}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/743df888f4cf5e5375c6be9c8f2a6a8174f137f725f3482206554e847ec108e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Examples image-text pairs from TripletData. In each block, a positive pair from CC3M is on the left and corresponding negatives from TripletDataare shown on the right. ", "page_idx": 3}, {"type": "text", "text": "the InfoNCE loss [34] between the two encodings. CLIP is designed for multimodal settings (for example, vision and language inputs) \u2013 this entails using two encoders (one for each modality). ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ represent two modalities and $\\boldsymbol{\\mathcal{D}}=\\{(x_{i},y_{i})\\}_{i=1,...,M}$ , be the training dataset, where $x_{i}\\in\\mathcal{X}$ and $y_{i}\\in\\mathcal{V}$ . The goal is to train two modality-specific encoders, $F_{X}$ and $F_{3}$ , by minimizing the InfoNCE loss between the normalized features extracted from the encoders. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{X}\\rightarrow\\mathcal{Y}}^{C L}=\\frac{-1}{N}\\sum_{i=1}^{N}\\log\\frac{\\exp(\\langle F_{\\mathcal{X}}(x_{i}),F_{\\mathcal{Y}}(y_{i})\\rangle/\\tau)}{\\sum_{k=1}^{N}\\exp(\\langle F_{\\mathcal{X}}(x_{i}),F_{\\mathcal{Y}}(y_{k})\\rangle/\\tau)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\langle\\cdot\\rangle$ represents cosine similarity and $\\tau$ is the trainable temperature parameter. For simplicity, we do not show feature normalization in the InfoNCE loss. Similarly, we can define the $\\dot{Z}_{\\mathcal{Y}\\rightarrow\\mathcal{X}}^{C L}$ training loss. The combined CLIP total training objective is given as, $\\mathcal{L}_{C L I P}=\\mathcal{L}_{\\mathcal{X}\\rightarrow\\mathcal{Y}}^{C L}+\\mathcal{L}_{\\mathcal{Y}\\rightarrow\\mathcal{X}}^{\\breve{C}L}$ By minimizing this training loss, both encoders learn representations that maximize the mutual information between two modalities. ", "page_idx": 3}, {"type": "text", "text": "NegCLIP introduces synthetic augmentations to generate \u201chard\u201d negative captions $(y_{i}^{\\prime}\\in\\mathcal{V}^{\\prime})$ by performing semantic inverting perturbations to the reference captions $(y_{i}\\in\\mathcal{V})$ . Therefore, the single modality-specific hard negative augmentation-based training loss can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\boldsymbol{x}\\rightarrow\\boldsymbol{y};\\boldsymbol{y}^{\\prime}}^{N e g C L}=\\frac{-1}{N}\\sum_{i=1}^{N}\\log\\frac{\\exp(\\langle\\boldsymbol{F}_{\\boldsymbol{X}}(\\boldsymbol{x}_{i}),\\boldsymbol{F}_{\\boldsymbol{y}}(\\boldsymbol{y}_{i})\\rangle/\\tau)}{\\sum_{k=1}^{N}\\exp(\\langle\\boldsymbol{F}_{\\boldsymbol{X}}(\\boldsymbol{x}_{i}),\\boldsymbol{F}_{\\boldsymbol{y}}(\\boldsymbol{y}_{k})\\rangle/\\tau)+\\sum_{m=1}^{N}\\exp(\\langle\\boldsymbol{F}_{\\boldsymbol{X}}(\\boldsymbol{x}_{i}),\\boldsymbol{F}_{\\boldsymbol{y}}(\\boldsymbol{y}_{m}^{\\prime})\\rangle/\\tau)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The total loss for NegCLIP for image modality $(\\mathcal{X})$ and text modality $(\\mathcal{V})$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{N e g C L I P}(\\mathcal{X},\\mathcal{Y},\\mathcal{Y^{\\prime}})=\\mathcal{L}_{y\\rightarrow\\mathcal{X}}^{C L}+\\mathcal{L}_{\\mathcal{X}\\rightarrow\\mathcal{Y};\\mathcal{Y^{\\prime}}}^{N e g C L}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Eq. 2, the negative samples are generated only for language modality as it is easy to make semanticlevel perturbations. Existing methods have not explored performing semantic perturbations in the image modality to create hard negatives. In this work, we demonstrate how hard negatives can be created in the image modality by leveraging the semantic language grounding and photorealism of text-to-image diffusion models. Our novel hard negative generation pipeline and refined training objective seeks to bridge the significant gap identified in literature. ", "page_idx": 3}, {"type": "text", "text": "3.2 TripletData: Image-text hard negative data augmentations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To generate high-quality hard negative image-text pairs, we follow a two-step procedure. The first stage is to generate hard negative captions from the ground truth positive caption. Second, to generate images corresponding to the hard negative captions as negative images. The AltText captions from the existing web-scrapped datasets are very noisy, leading to the noisy and unreliable generation of hard negatives. Therefore, we build upon the existing work LaCLIP, which first rewrites the captions using LLM from the existing data that are linguistically accurate. Figure 2 illustrates several examples of positive and corresponding negative image-text pairs. ", "page_idx": 4}, {"type": "text", "text": "Generating hard negative captions. Existing works perform random swapping, replacing, and adding actions between the nouns, attributes, and relations of the positive caption [58, 61]. This method results in nonsensical and grammatically incorrect artifacts, such as \u201ca person riding on four slope,\u201d which impedes the generation of negative images, ultimately leading to diminishing performance on harder benchmarks [20]. Therefore, we utilize the in-context learning ability of LLMs to generate negative captions. The choice of LLM is a trivial task as long as they provide hard negative captions. We find that Mistral-7B-Instruct- $\\cdot\\mathrm{v}0.2^{2}$ performs reasonably better on our goal, and the output is easy to parse. We generate the negative captions in batches to speed up the generation process. Generating the 13M negative captions takes only 3 days on 8xRTX A6000. Instead of generating multiple hard negative captions, we find that a single high-quality hard negative caption is enough to improve the performance compared to the traditional NegCLIP style caption generation (see $\\mathrm{NegCLIP++}$ results in Table 4). We provide examples of various types of negative captions in the appendix. Specifically, we provide the following prompt to LLM: ", "page_idx": 4}, {"type": "text", "text": "You are given the description of an image. You should provide a mostly similar description, changing the original one slightly but introducing enough significant differences such that the two descriptions could not possibly be for the same image. Keep the description length the same. Finally, only a few things (such as counting, objects, attributes, and relationships) can be modified to change the image structure significantly. Provide just the updated description. Examples: Input: A dog to the left of the cat. Output: A dog to the right of the cat. Input: A person wearing a red helmet drives a motorbike on a dirt road. Output: A person in a blue helmet rides a motorbike on a gravel path. Now, do the same for the following captions: Input: {} Output: ", "page_idx": 4}, {"type": "text", "text": "Generating hard negative images. Typically, semantic perturbations within images require tools like image editing, which are resource-intensive and cannot be scaled. Remember that we want to provide additional ground truth references for the negative caption. Therefore, we propose to utilize the negative captions from the previous stage to generate the respective reference images directly for pre-training. As the previous stage generates negative captions that are linguistically correct, it becomes easier for image-generative models to synthesize the respective images precisely. We utilize pre-trained text-to-image diffusion models to generate the corresponding images. Specifically, we select SDXL-turbo [46] due to its relatively faster generation speed. After applying various inference time optimizations, we can generate 13M negative images within 2 days using 30 v100 GPUs. We provide various examples of the hard negative image-text pairs in the appendix. ", "page_idx": 4}, {"type": "text", "text": "Analyzing difficulty of the hard TripletData. Let\u2019s assume we have positive and negative image-text pairs from the TripletData , $(x_{i},y_{i})$ and $(x_{i}^{\\prime},\\bar{y}_{i}^{\\prime})$ , respectively. If the data is truly hard negative, existing pre-trained models should struggle to find the correct imagetext pairs (i.e., $\\overbar{c o s}(x_{i},y_{i})>c o s(x_{i},y_{i}^{\\prime}))$ . Following winoground, we measure the text-score, image-score, and group-score to evaluate the popular pretrained CLIP models. Table 1 shows that even CLIP models trained on billions of data struggle to get near human performance on TripletData, which is less difficult than winoground. Importantly, the goal of generating hard negative samples isn\u2019t to add more diversity ", "page_idx": 4}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/797743d383f36f81975e2106ef727cbe28c50308f9d7b4c18a54683deb3dc011.jpg", "table_caption": ["Table 1: Winoground-style evaluation of pretrained CLIP models on TripletData. "], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/652ed89fe5ceee6edd02ab09860e44c4679d30abaf49cbb325a3da006a4d95c9.jpg", "table_caption": ["Table 2: Wordnet synset analysis of captions from CC3M and TripletData. "], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/ad49cde2f9d7b55e1862e005b868ef3c4a0a638f10a905c8929eff954ad396ee.jpg", "table_caption": ["Table 3: Importance of image-text hard negatives. We measure the importance of various modalityspecific hard negatives on SugarCrepe, image-text retrieval, and ImageNet1k. We find that TripletCLIP results into the most optimal solution. Bold number indicates the best performance. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "in terms of unique concepts during the training but to add diversity in semantic meanings. Therefore, we measure the unique wordnet synsets in CC3M vs. TripletData. From Table 2, it can be observed that TripletData does not add any new concepts but uses existing concepts to provide negative samples that are semantically different. To summarize, TripletData contains the relatively hard negative image-text pairs that current models find difficult to differentiate. ", "page_idx": 5}, {"type": "text", "text": "3.3 TripletCLIP ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prior works have demonstrated the value of hard negative captions for enhancing the compositionality of CLIP models via $\\mathcal{L}_{N e g C L I P}$ as the key training objective (Eq. 2) [58, 61]. However, it remains elusive if negative images alone can benefti or not. We conduct modality-specific ablations, reporting the average performance across the diverse set of benchmarks in Table 3 (we provide more details about experiments in Section 4). Our findings indicate that both \u201chard\u201d negative captions and images individually boost performance when compared to LaCLIP. However, this initial empirical experiments to train the CLIP model on hard negative images (i.e., NegImage) by minimizing $\\mathcal{L}_{N e g C L I P}(\\mathcal{V},\\mathcal{X},\\mathcal{X}^{\\prime})$ reveal that negative images alone cannot improve the compositionality significantly (see Table 3). We hypothesize that images contain low-level information, making it difficult to train the model using images as negative examples. Aligning with our initial motivation and building upon this crucial insight, we propose to utilize the negative images to regularize the effect of negative captions and to stabilize the pre-training. Therefore, to utilize these hard negative image-text pairs from the previous stage more effectively, we propose to focus on two triplets $(\\mathcal{X},\\mathcal{Y},\\bar{\\mathcal{Y}^{\\prime}})$ and $\\bar{(\\mathcal{X}^{\\prime},\\mathcal{Y}^{\\prime},\\mathcal{Y})}$ , hence, the final triplet contrastive learning training objective is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{T C L}=\\mathcal{L}_{N e g C L I P}(\\mathcal{X},\\mathcal{Y},\\mathcal{Y^{\\prime}})+\\mathcal{L}_{N e g C L I P}(\\mathcal{X}^{\\prime},\\mathcal{Y^{\\prime}},\\mathcal{Y}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Intuitively, the second term introduces the additional form of supervision that hard negative images are closer to the corresponding negative captions than positive captions. This allows the system to understand that if the positive image does not represent the negative caption \u201cblue horse,\u201d then what does this caption entail? Through this strategic alternation of hard negative image-text pairs for the TripletCLIP, we improve compositionality and image-text understanding of the vision-language model (see Table 3). We provide the pseudo-code in the appendix and the code in supplementary materials. This simple yet effective strategy elevates the training of the CLIP, offering a scalable framework to improve overall performance. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments & Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Pretraining Datasets. We utilize the CC3M and CC12M datasets, which comprise 2.6M and 8.6M image-text pairs, respectively. Following the approach demonstrated by LaCLIP, we use LLMrewritten captions to replace noisy original captions. For NegCLIP, we introduce four negative captions per positive image-text pair, focusing on semantic inverting perturbations across four categories: attribute, relation, object, and action [61]. This generates approximately 10.4M and 34.4M text-only augmentations for CC3M and CC12M, respectively. To train the TripletCLIP, we create augmentations (TripletData) for both datasets to integrate hard negatives effectively. We produce one augmentation per image-text pair, adding 2.6M and 8.6M image-text augmented pairs for CC3M and CC12M, respectively. Finally, we perform all the ablations on the CC3M dataset. ", "page_idx": 5}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/ae7b5ea8a0d3485bc0bc4f97dd626172da23e9e7ab6ed5ab10c25b2fb068fd6d.jpg", "table_caption": ["Table 4: Composition evaluations of the methods on SugarCrepe benchmark. Bold number indicates the best performance and underlined number denotes the second-best performance. \u2020 represents the results taken from SugarCrepe benchmark. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/3963707b1dbdc192d0ed6d7c43ce1e1ac93ae5a4d0f03666fc3002a649ea9706.jpg", "table_caption": ["Table 5: Zero-shot image-text retrieval and classification results. Bold number indicates the best performance and underlined number denotes the second-best performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Baselines. We train LaCLIP, LaCLIP with real hard negatives $\\mathrm{(LaCLIP+HN)}$ ), and $\\mathrm{NegCLIP}$ from scratch to ensure consistency and fairness in our comparisons. As NegCLIP\u2019s rule-based augmentations closely resemble some compositional benchmarks, so we introduce $\\mathrm{NegCLIP++}$ as an improved baseline. $\\mathrm{NegCLIP++}$ incorporates hard negative captions generated using LLM from TripletData, enhancing the language comprehension compared to standard NegCLIP. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. Our experiments employ the ViT-B/32 [10] model architecture. To guarantee fair comparisons, we retrain all baseline models using identical hyperparameters. Since the overall training data for NegCLIP and TripletData is more than the baseline datasets, we align the number of iterations across all models to equalize the number of image-text pairs seen during training, similar to the strategy used in DataComp. The batch size is fixed to 1024 with the AdamW optimizer at a maximum learning rate of 0.0005, employing cosine decay. Training durations are set at approximately 100k iterations for CC3M and $200\\mathbf{k}$ iterations for CC12M. All models are trained on a single A100 (80GB) GPU using bf16 precision. The final training-related experiments and ablations will cost about 1200 A100 GPU hours. We leave the experiments on increasing the data and model size as future works for the community, as scaling further is not viable in the academic budget. ", "page_idx": 6}, {"type": "text", "text": "Downstream Datasets. The primary objective of this study is to enhance the compositional capabilities of CLIP models. We mainly evaluate TripletCLIP and the baseline models using the challenging SugarCrepe composition benchmark, with additional performance assessments provided in the appendix for older benchmarks. Models are also tested on image-text retrieval tasks for broader evaluation using the Flickr30k [40] and MSCOCO [29] datasets. Zero-shot classification performance is assessed across approximately 18 different datasets. Evaluations adhere to the methodologies outlined in the CLIP-Benchmark3 or the official benchmark implementations. ", "page_idx": 6}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/0b7c47d86c2ca28f4ca83978630620d690eb9f1dd76eb39d2da710136bdc1ebd.jpg", "table_caption": ["Table 6: Ablation on flitering high-quality image-text pairs from TripletData. We evaluate the TripletCLIP after applying the filters to ensure the quality similar to DataComp and compare the baselines on three benchmarks. We find that TripletCLIP results in the most optimal solution. Bold number indicates the best performance. $\\dagger$ represents that results are borrowed from DataComp. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Compositional reasoning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We comprehensively analyze the compositional understanding of models on the SugarCrepe benchmark, as detailed in Table 4. Notably, TripletCLIP consistently outperforms all baseline models across all sub-categories of SugarCrepe on both the CC12M/CC3M training datasets. Specifically, TripletCLIP surpasses LaCLIP and NegCLIP by $10.91\\%/9.4\\%$ and $6.45\\,\\%/6.31\\,\\%$ on the CC12M and CC3M datasets, respectively. Our enhanced baseline, $\\mathrm{NegCLIP++}$ , also shows improvement over standard NegCLIP, highlighting the benefits of LLM-generated negatives. Nevertheless, TripletCLIP further advances performance, underscoring the critical role of hard negative image-text pairs, not just text. Additional comparisons on older composition benchmarks (Valse [35], Cola [42], and Winoground [53]) in the appendix reveal TripletCLIP\u2019s consistent performance. Table 4 also contrasts TripletCLIP with models trained using the DataComp approach, which involves more parameters and training data, demonstrating that TripletCLIP achieves comparable performance to a ViT-B/16 model trained on 1 billion image-text pairs. ", "page_idx": 7}, {"type": "text", "text": "4.3 Zero-shot evaluations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Image-Text Retrieval. In Table 5, we summarize the performance of models on text-to-image (T2I) and image-to-text (I2T) retrieval tasks on MSCOCO and Flickr30k datasets, where we report $\\mathbf{R}\\!\\stackrel{\\rightharpoonup}{\\omega}\\!5$ scores. Remarkably, TripletCLIP significantly outperforms baseline models by an average of $8\\%/10\\%$ and $8\\%/12.\\dot{5}\\%$ on I2T and T2I tasks, respectively, on the CC3M and CC12M datasets. Intriguingly, while $\\mathsf{L a C L I P+H N}$ performs better than NegCLIP, TripletCLIP outstrips both. ", "page_idx": 7}, {"type": "text", "text": "Zero-shot Classification. Table 5 also presents the average zero-shot classification performance on 18 standard datasets, including ImageNet1k. TripletCLIP consistently enhances top-1 accuracy by an average of ${}_{3\\%}$ and top-5 accuracy by $5.7\\%$ compared to LaCLIP. Like the retrieval performance, $\\mathrm{LaCLIP+HN}$ exceeds NegCLIP, yet TripletCLIP maintains the highest performance. Datasetspecific results are in the appendix. ", "page_idx": 7}, {"type": "text", "text": "4.4 Finetuning performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this paper, we focus on pretraining-based experiments as they allow greater flexibility in learning better representations. To complement this, we also performed additional fine-tuning experiments using hyperparameters similar to the baselines (without LoRA) and compared them against various publicly available baselines [44, 50, 12, 11]. As reported in Table 7, TripletCLIP improves compositionality and outperforms nearly all baselines. Furthermore, the observed drop in retrieval and zero-shot classification performance (Table 16) is attributed to limitations in the vision encoder, highlighting the challenges of existing pre-trained vision encoders in capturing semantic representations. This is further demonstrated in Table 8. ", "page_idx": 7}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/7757f44a1da9d6cf2b494ecd61680dac05e3501a0e5cdc4644f39f12ff6e837f.jpg", "table_caption": ["Table 7: Finetuning-based composition evaluations of the methods on SugarCrepe benchmark. Bold number indicates the best performance and underlined number denotes the second-best performance. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/04949a5779f0d41f5b35e4743d30be37ac6167d498decfac4ef901328421298b.jpg", "table_caption": ["Table 8: Frozen encoder ablation. LiT style fine-tuning ablations on SugarCrepe, image-text retrieval, and ImageNet1k. Bold number indicates the best performance. ", "4.5 Ablations "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Can a high-quality filtered dataset improve the performance? Given that negative images in TripletData are generated using SDXL-turbo, these may not always be precise. Inspired by DataComp, we employ a pre-trained CLIP-L/14 to fliter the image-text pairs, selecting the highest average similarity pairs (positive and negative) individually (i.e., score $=\\bar{(s(x_{i},y_{i})+s(x_{i}^{\\prime},y_{i}^{\\prime}))}/2)$ . The top 1.4M positive image-text pairs and their corresponding negatives from TripletData are selected. Table 6 details this comparison against DataComp pre-trained models. Remarkably, TripletCLIP already surpasses baselines without filtered data; however, with the filtered dataset, despite being trained on $50\\%$ smaller dataset, $\\mathtt{T r i p l e t C L I P++}$ shows further performance improvements. This underlines the significant beneftis of carefully selected TripletData in enhancing the performance. ", "page_idx": 8}, {"type": "text", "text": "Which modality-specific encoder plays the key role in improving compositionality? To address this open question, we designed an ablation study similar to LiT, freezing either the pre-trained CLIP vision or text encoder while training the opposite modality-specific encoder from scratch. We observe the performance of LaCLIP and TripletCLIP on CC3M, as shown in Table 8. Freezing the vision model results in no performance gain on the SugarCrepe for TripletCLIP. However, significant improvements are noted when the vision encoder is actively trained, suggesting that the vision modality may be the bottleneck in compositionality. Notably, TripletCLIP outperforms LaCLIP in all settings, further demonstrating its robustness to different pre-training approaches. ", "page_idx": 8}, {"type": "text", "text": "Concept coverage analysis. Improving performance on zero-shot transfer learning tasks such as retrieval involves two key components: adding more concept diversity during training and enhancing image-text alignment/compositionality. We create subsets of CC12M data with increasing concept diversity based on unique WordNet synsets. Specifically, we select 3M, 4M, 5M, and 6M subsets for training LaCLIP, while TripletCLIP training involves only half of these training data as positive pairs, and the rest are corresponding augmentations. Evaluations across SugarCrepe, retrieval tasks, and ImageNet1k (see Figure 3) indicate that TripletCLIP not only enhances SugarCrepe performance even at lower concept coverage levels but also significantly outperforms similar concept coverage in retrieval tasks, matching LaCLIP\u2019s performance on zero-shot classification tasks that do not require compositionality at all. This bolsters our argument that incorporating hard negatives from both modalities markedly improves compositional understanding in CLIP, while baseline struggles to do so even with more concept diversity. ", "page_idx": 8}, {"type": "text", "text": "What if TripletData is used for large-scale compositional evaluations? We evaluated the CC12M pre-trained models on a 50,000 random subset of the CC3M dataset using a Winoground-style approach [53]. As shown in Table 9, TripletCLIP significantly improves performance compared to the baselines. However, we partially attribute this improvement to spurious correlations learned from the data. At the same time, we note that the models have not fully converged, suggesting minimal risk of overfitting to these spurious correlations. ", "page_idx": 8}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/6a93bc0130b06e735717758cbc08a7c0b6077b245bcec5484c9975284b1b511d.jpg", "img_caption": ["Figure 3: Average Results of LaCLIP and TripletCLIP for SugarCrepe Compositions, Image-Text Retrieval, and ImageNet1k over increasing concept diversity. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/7f9a406a9da469b3c9b3e7d1f4131343a5124e6f73c842120feb0f84aebad9d6.jpg", "table_caption": ["Table 9: TripletData as large-scale composition evaluation dataset after [53]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce TripletCLIP, a novel approach to enhancing compositional reasoning in vision-language models through the strategic incorporation of hard negative image-text pairs. Our comprehensive experiments across a suite of benchmarks demonstrate that TripletCLIP significantly outperforms existing methodologies such as LaCLIP and NegCLIP, achieving notable gains not only in compositionality but also in zero-shot classification and retrieval tasks as well. Further, our ablation studies highlight the critical role of modality-specific training and the careful curation of training data, underscoring the importance of both hard negative image and text components in the learning process. TripletCLIP\u2019s effectiveness with a smaller, refined dataset suggests a promising direction for future research\u2014maximizing performance without the need for extensive data collection, thereby reducing computational costs and enhancing model efficiency. To this end, we provide an intriguing application of synthetic datasets via hard negative image-text pairs for vision-language tasks that could be easily extended to improve Multimodal Large Language Models and Text-to-Image generative models. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Due to constraints inherent in academic settings and limited computational resources, we were unable to scale TripletCLIP to handle hundreds of millions of image-text pairs or employ larger models within the scope of this study. Nevertheless, our results indicate a promising direction for future research within a consistent experimental framework, and we encourage subsequent work to explore scaling both the TripletData and TripletCLIP. Our experimental focus was primarily on the CLIP and LiT methodologies. With additional resources, however, extending our methodologies to more advanced contrastive learning techniques, such as SigLIP, would be feasible. In conclusion, our work introduces a compelling strategy for integrating open-ended hard negatives (both text and image) during the pre-training phase, providing a methodology and large-scale data that could benefti a variety of research domains. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by NSF RI grants #1750082, #2132724, and CPS grant #2038666. We thank the Research Computing (RC) at Arizona State University (ASU) for providing computing resources. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the funding agencies and employers. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a jointembedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619\u201315629, 2023.   \n[2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Composed image retrieval using contrastive learning and task-oriented clip-based features. ACM Transactions on Multimedia Computing, Communications and Applications, 20(3):1\u201324, 2023.   \n[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.   \n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual $12\\mathrm{m}$ : Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3558\u20133568, June 2021.   \n[6] Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, et al. Getting it right: Improving spatial consistency in text-to-image models. In European Conference on Computer Vision, pages 204\u2013222. Springer, 2024.   \n[7] Yuxiao Chen, Jianbo Yuan, Yu Tian, Shijie Geng, Xinyu Li, Ding Zhou, Dimitris N Metaxas, and Hongxia Yang. Revisiting multimodal representation in contrastive learning: from patch and token embeddings to finite discrete tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15095\u201315104, 2023.   \n[8] Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ramakrishna Vedantam. Hyperbolic image-text representations. In International Conference on Machine Learning, pages 7694\u20137731. PMLR, 2023.   \n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019.   \n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[11] Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola CascanteBonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, et al. Dense and aligned captions (dac) promote compositional reasoning in vl models. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, and Leonid Karlinsky. Teaching structured vision & language concepts to vision & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2657\u20132668, 2023.   \n[13] Linus Ericsson, Henry Gouk, Chen Change Loy, and Timothy M Hospedales. Self-supervised representation learning: Introduction, advances, and challenges. IEEE Signal Processing Magazine, 39(3):42\u201362, 2022.   \n[14] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36, 2024.   \n[15] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T Toshev, and Vaishaal Shankar. Data flitering networks. In The Twelfth International Conference on Learning Representations, 2023.   \n[16] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.   \n[17] Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi, Vishwa Vinay, and Aditya Grover. Cyclip: Cyclic contrastive language-image pretraining. Advances in Neural Information Processing Systems, 35:6704\u20136719, 2022.   \n[18] Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, and Bernard Ghanem. Synthclip: Are we ready for a fully synthetic clip training? arXiv preprint arXiv:2402.01832, 2024.   \n[19] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514\u20137528, 2021.   \n[20] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: A survey. arXiv preprint arXiv:2402.17525, 2024.   \n[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n[23] Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, and Humphrey Shi. Learning mask-aware clip representations for zero-shot segmentation. Advances in Neural Information Processing Systems, 36:35631\u201335653, 2023.   \n[24] Changhoon Kim, Kyle Min, and Yezhou Yang. Race: Robust adversarial concept erasure for secure text-to-image diffusion model. arXiv preprint arXiv:2405.16341, 2024.   \n[25] Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, et al. From scarcity to efficiency: Improving clip training via visual-enriched captions. arXiv preprint arXiv:2310.07699, 2023.   \n[26] Hugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024.   \n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[28] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023.   \n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 34892\u201334916. Curran Associates, Inc., 2023.   \n[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[32] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10910\u201310921, 2023.   \n[33] George A. Miller. Wordnet: a lexical database for english. Commun. ACM, 38(11):39\u201341, nov 1995.   \n[34] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[35] Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8253\u20138280, 2022.   \n[36] Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. Conceptbed: Evaluating concept learning abilities of text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14554\u201314562, 2024.   \n[37] Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: A resource-efficient text-to-image prior for image generations. arXiv preprint arXiv:2312.04655, 2023.   \n[38] Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: A resource-efficient text-to-image prior for image generations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9069\u20139078, 2024.   \n[39] Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. Synthesize diagnose and optimize: Towards fine-grained vision-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13279\u201313288, 2024.   \n[40] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.   \n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[42] Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan Plummer, Ranjay Krishna, and Kate Saenko. cola: A benchmark for compositional text-to-image retrieval. Advances in Neural Information Processing Systems, 36, 2024.   \n[43] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. arXiv preprint arXiv:2010.04592, 2020.   \n[44] Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and Volker Tresp. Enhancing multimodal compositional reasoning of visual language models with generative negative mining. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5563\u20135573, 2024.   \n[45] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024.   \n[46] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023.   \n[47] Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, and William Yang Wang. Who evaluates the evaluations? objectively scoring text-to-image prompt coherence metrics with t2iscorescore (ts2). arXiv preprint arXiv:2404.04251, 2024.   \n[48] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[49] Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, and Yu Chen. Coarse-to-fine contrastive learning in image-text-graph space for improved visionlanguage compositionality. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 869\u2013893, 2023.   \n[50] Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, and Aparna Bharati. Learn\" no\" to say\" yes\" better: Improving vision-language models via negations. arXiv preprint arXiv:2403.20312, 2024.   \n[51] Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. Clip models are few-shot learners: Empirical studies on vqa and visual entailment. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6088\u20136100, 2022.   \n[52] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clips for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14214\u201314223, 2023.   \n[53] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238\u20135248, 2022.   \n[54] Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Equivariant similarity for vision-language foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11998\u201312008, October 2023.   \n[55] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on $1600+$ nlp tasks. In 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, 2022.   \n[56] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023.   \n[57] Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. Alip: Adaptive language-image pre-training with synthetic caption. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2922\u20132931, 2023.   \n[58] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2022.   \n[59] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18123\u201318133, 2022.   \n[60] Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, 2024.   \n[61] Le Zhang, Rabiul Awal, and Aishwarya Agrawal. Contrasting intra-modal and ranking crossmodal hard negatives to enhance visio-linguistic fine-grained understanding. arXiv preprint arXiv:2306.08832, 2023.   \n[62] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11175\u201311185, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this study, we have demonstrated the potential of utilizing high-quality positive and negative pairs to enhance the compositional understanding of vision-language models like CLIP through the introduction of TripletCLIP. While our findings are specific to TripletCLIP, the underlying techniques hold promise for broader applications, including enhancing visual understanding in Multimodal Large Language Models (MLLMs) and Text-to-Image diffusion models. Although our approach yields significant performance improvements, it does require resources to generate largescale synthetic datasets. We encourage future research to explore the utility of this pre-training strategy within the latent space, which could reduce dependence on large generative models. Initiatives like JEPA [1] have already demonstrated the efficacy of focusing on latent space models, which suggests a promising avenue for reducing computational overhead. Importantly, our experiments reveal that significant enhancements in model performance are achievable even with substantially smaller data scales. This finding suggests that, when scaled appropriately, our methodology could substantially diminish resource dependencies and enhance the efficiency of pre-training processes for CLIP-like models. ", "page_idx": 15}, {"type": "text", "text": "B Pseudocode of TripletCLIP ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/873bd9f0ccda1936a4c7eb5f5c89ee611b52780d99ce275f58e1c7347dd73919.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 10: Detailed pre-training hyper-parameters for CLIP training across various experiments and ablations. ", "page_idx": 15}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/dbe8d177b091d05b5cf868a2774345f8e2734859fa4d6a384b686b2fd79972f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 10 provides a comprehensive overview of the pre-training hyperparameters employed across all baseline models and TripletCLIP. To ensure fair comparisons, we standardized the hyperparameters across all methodologies. Although larger batch sizes are typically associated with improved performance in contrastive learning, computational constraints necessitated fixing the batch size at 1024 for all experiments. To accommodate this batch size on a single A100 GPU, we employed bf16 precision. In terms of computational resources, experiments using the CC3M dataset required approximately 16 GPU hours, while those involving the CC12M dataset utilized up to 56 GPU hours per experiment. ", "page_idx": 15}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/b713c124d3693a60980a656270957293f65131cc71cbd6c3653d77a8d3769f35.jpg", "table_caption": ["Table 11: Composition evaluations of the methods on various benchmarks. Bold number indicates the best performance and underlined number denotes the second-best performance. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/607ae9b47a01a205669f0341fdbe5d298e8c0adbefcdb3bc6bb32a87039b7f82.jpg", "table_caption": ["Table 12: Dataset-specific zero-shot classification results. Bold number indicates the best performance and underlined number denotes the second-best performance. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "D Detailed Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Compositional reasoning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Previously, we reported the results on SugarCrepe, the most challenging dataset, noted for its absence of language biases. However, evaluations were also conducted on other benchmarks, such as Valse, Cola, and Winoground. As indicated in Table 11, TripletCLIP achieves overall improvements of $2.3\\%$ compared to LaCLIP and NegCLIP. The Valse benchmark, which contains text prompts that heavily favor the perturbations made for NegCLIP, shows a strong performance from NegCLIP, while $\\mathrm{NegCLIP++}$ encounters difficulties. Interestingly, TripletCLIP faces challenges in maintaining performance on Winoground, and baseline LaCLIP maintains the SOTA, which is counterintuitive to other benchmarks. Nonetheless, TripletCLIP still manages to outperform NegCLIP significantly. These results affirm that TripletCLIP sets a new standard for state-of-the-art compositional reasoning across diverse benchmarks. ", "page_idx": 16}, {"type": "text", "text": "D.2 Dataset-specific zero-shot classification ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 12 provides fine-grained results for the 18 zero-shot classification datasets. It can be observed that TripletCLIP consistently outperforms the baselines, achieving the best average results across these challenging datasets. Although the improvements are marginal, they are in line with expectations. As discussed in Figure 3, TripletCLIP does not introduce new concepts into the training data but focuses on augmentations that enhance representation without increasing concept diversity. These enhanced representations from TripletCLIP lead to average improvements of $1.3\\%$ depending on the scenario. ", "page_idx": 16}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/e135baa3f03a54c68d537a339149d8a820e45e51678f319fd2c887b7596eaf5f.jpg", "table_caption": ["Table 13: Composition evaluations of the methods on various benchmarks. Bold number indicates the best performance and underlined number denotes the second-best performance. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/c0b16e5b25810bb8cd8ce41d02b430f7da7869a228ed170b6a063a81d01baf12.jpg", "table_caption": ["Table 14: Ablation on choice pre-trained LLM. We train $\\mathrm{NegCLIP++}$ (ViT-B/32) on negative captions generated from various LLMs and report SugarCrepe, Flickr30k Retrieval $(\\mathbf{R}@5)$ , and ImageNet-top5 performances. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/8a4ee64cca8444b42c5243caad7e735dfa50f5b70b0799d4e09c7cbfac2b24d3.jpg", "table_caption": ["Table 15: Ablation on CyCLIP with TripletLoss. To evaluate the compatibility of TripletLoss with the CyCLIP, we train the ViT-B/32 models from search on CC3M with 512 batch size and report SugarCrepe, Flickr30k Retrieval $(\\mathbf{R}@5)$ , and ImageNet-top5 performances. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D.3 Detailed image-text retrieval performance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 13 presents detailed T2I and I2T retrieval results for the MSCOCO and Flickr30k datasets. We report results at different recall thresholds: $\\mathbf{R}\\@1$ , $\\mathbf{R}@5$ , and $\\mathbf{R}\\@10$ . The data shows that TripletCLIP significantly outperforms the baselines across all recall rates. On average, TripletCLIP achieves a performance gain of $7.11\\%$ over LaCLIP. Additionally, TripletCLIP improves performance by $3{\\bf-6}\\%$ compared to previous state-of-the-art baselines. ", "page_idx": 17}, {"type": "text", "text": "D.4 Additional ablations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Choice of Pre-trained LLM. We provide further details regarding the selection of LLMs for generating hard negative captions. $\\mathrm{NegCLIP++}$ was trained on 3 million generated negative captions for CC3M using three different LLMs, and the results are reported in Table 14. We find that Phi-3 achieves the best average performance, while Gemma-2b unexpectedly has a notable negative impact on compositionality. However, we opted to use Mistral-7b-instruct-v0.2, as Phi-3 was released after our experiments were completed, preventing its earlier evaluation. ", "page_idx": 17}, {"type": "text", "text": "Evaluating the Orthogonality of TripletLoss. As discussed in Section 2, TripletCLIP can be integrated with various previously proposed methodologies. To evaluate this, we applied TripletLoss in conjunction with CyCLIP [17] and reported the results in Table 15. The most significant performance improvement is observed with the TripletLoss alone. However, TripletLoss also enhances the performance over the base CyCLIP, demonstrating its adaptability and orthogonality with respect to other approaches. ", "page_idx": 17}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/cdd9ccc054c293896aa8a58e38a5e51231439dfca31067e6e01e748766853e69.jpg", "table_caption": ["Table 16: Finetuning-based evaluations of the methods on Retrieval and ImageNet-1k benchmarks. Bold number indicates the best performance and underlined number denotes the second-best performance. "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/fb3c30cda872a018f8a6b437c40bdbbcefbc64aa9aba7948a4fab0b729731e77.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 4: Positive vs. Negative modality-specific pair-based similarity distribution of pre-trained CLIP ViT-B/32 model w.r.t. the vision and text-only encoders. The left plot is the vision embedding similarities between positive and negative images. The right plot is the text embedding similarities between positive and negative captions. In the ideal scenario, the distribution should be skewed towards 0.0, which indicates that the model can correctly distinguish between the positive and negative data. ", "page_idx": 18}, {"type": "text", "text": "E Encoder Representation Distribution Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Remember, this study aims to learn the representations that can distinguish between two data points that are very similar but semantically different. Firstly, we take LaCLIP and TripletCLIP models trained on CC12M. We also sampled 50000 positive+negative pairs from CC3M. Then, we measure the vision and text modality-specific cosine similarities between positive and negative pairs and plot the distribution (see Figure 5). It can be observed that vision representations from TripletCLIP are more skewed towards 0.0, suggesting that the vision encoder can distinguish between hard negative samples better than the baseline LaCLIP. However, in the case of the text modality, both methods perform similarly. This aligns with our findings from Table 8 that the vision encoder plays a crucial role in improving the compositionality, and to achieve this, our TripletData is necessary. ", "page_idx": 18}, {"type": "text", "text": "F TripletData Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section provides qualitative examples of the TripletData and discusses various data analyses. ", "page_idx": 18}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/30fa210aeb6a497e7b6c4204eee3a13e378d10f63eca21523700bb9d7e6e74bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: Positive vs. Negative modality-specific pair-based similarity distribution of baseline LaCLIP and TripletCLIP. The left plot is the vision embedding similarities between positive and negative images. The right plot is the text embedding similarities between positive and negative captions. ", "page_idx": 19}, {"type": "table", "img_path": "ZfRGRK5Kxl/tmp/c8e84606d9c9b457f630b2bdf57eb346ef554a52f10c36b8af2b9ba307a9d446.jpg", "table_caption": ["Table 17: Question Generation. Examples of LLM-generated existence-related questions from captions to evaluate the generated images. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Difficulty of the data. We further add one more analysis to investigate how difficult our dataset is. First, we take state-of-the-art language-only (GTE [28]) and vision-only (DINO [4]) embedding models and pretrained CLIP ViT-B/32. Later, we measure the modality-specific similarity between positive and negative vision and language data pairs. Figure 4 shows that the similarity distribution of the pretrained CLIP model between positive and negative text pairs follows the distribution of the text-only GTE model. Interestingly, vision distribution is drastically different. DINO can distinguish the positive and negative pairs correctly with high confidence. However, despite the visuals being so different, the pretrained CLIP model struggles to distinguish the different images. This further highlights that TripletData is indeed challenging for the vision-language models, even the ones trained on large-scale datasets. ", "page_idx": 19}, {"type": "text", "text": "Evaluations of Generated Images. Even though T2I diffusion models are widely evaluated on various tasks. We perform additional evaluations to measure how accurately generated images follow the text prompt. To do this, taking inspiration from [36], we first use LLM to generate binary \"yes/no\" style questions from the given caption. As shown in Table 17, we create five questions per hard negative caption. Later, we utilized the ViLT model to answer visual questions. Upon this investigation, we find that SDXL-turbo archives on an average of $76\\%$ accuracy. In other words, the T2I model can correctly generate an image that follows around 3/4th of the text. Additionally, we hypothesize that using an improved T2I model or image editing models to generate \u201chard\u201d negative examples can further improve composition reasoning. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Qualitative Examples. In Figure 6, we provide additional qualitative examples of the contrastive positive and \u201chard\u201d negative pairs from the TripletData. Additionally, in Figure 7, we illustrated several examples where the T2I model could not precisely generate images corresponding to the caption. However, we may notice that in most cases, it maintains some of the important aspects. Because of this, despite not being $100\\%$ accurate all the time, it can help TripletCLIP improve performance across the evaluation benchmarks. ", "page_idx": 20}, {"type": "text", "text": "Hard negative caption only examples: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Raw Caption: dog looking out from a window . Language Rewrite: A dog looking through the window at his owner. Negative Caption (NegCLIP): (a) A window looking through the dog at his owner. (b) A dog looking through the window at his dog. (c) A dog screams through the window at his owner. TripletData Negative Caption: A cat observing its owner from the window.   \n2. Raw Caption: person attends the premiere of film Language Rewrite: A person attends the premiere of film Negative Caption (NegCLIP): (a) A premiere attends the person of film (b) A person attends the festival of film (c) A person watches the premiere of film TripletData Negative Caption: A person waits in line for film tickets.   \n3. Raw Caption: white crocus spring flowers in the forest. Language Rewrite: white crocus flowers in the forest Negative Caption (NegCLIP): (a) white crocus flowers in the sky TripletData Negative Caption: red orchid flowers in the meadow.   \n4. Raw Caption: flag with industry in the background Language Rewrite: A flag is holding in the background an industrial site. Negative Caption (NegCLIP): (a) A background is holding in the flag an industrial site. (b) A flag is holding in the background an earthquake site. (c) A drone is holding in the background an industrial site. (d) A flag is visible in the background an industrial site. TripletData Negative Caption: A flag is flapping in the foreground of a pastoral scene.   \n5. Raw Caption: portrait of businessman with cardboard on his head carrying a briefcase and using an umbrella while standing by.   \nLanguage Rewrite: A portrait of a businessman standing by with a briefcase and cardboard on   \nhis head carrying an umbrella while looking at a blue sky and parked cars on the street   \nNegative Caption (NegCLIP):   \n(a) A businessman of a portrait standing by with a briefcase and cardboard on his head carrying an umbrella while looking at a blue sky and parked cars on the street   \n(b) A portrait of a businessman standing by with a briefcase and cardboard on his head carrying an umbrella while looking at a grey sky and parked cars on the street   \n(c) A portrait of a businessman standing by with a briefcase and cardboard on his head carrying an umbrella while looking at a blue truck and parked cars on the street   \n(d) A portrait of a businessman standing by with a briefcase and cardboard on his head carrying an umbrella while pointing at a blue sky and parked cars on the street ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "TripletData Negative Caption: A portrait of a businesswoman seated on a bench with a tote bag and newspaper on her lap holding an umbrella while looking at a red sunset over row houses. ", "page_idx": 20}, {"type": "text", "text": "6. Raw Caption: 158834 is the portion of the bound train ", "page_idx": 21}, {"type": "text", "text": "Language Rewrite: Huge locomotives sit on the tracks in front of a building. Negative Caption (NegCLIP): ", "page_idx": 21}, {"type": "text", "text": "(a) Huge tracks sit on the locomotives in front of a building.   \n(b) Three locomotives sit on the tracks in front of a building.   \n(c) Huge locomotives sit on the tracks in anticipation of a building.   \n(d) Huge locomotives mounted on the tracks in front of a building.   \nTripletData Negative Caption: Huge locomotives sit on the tracks in front of a bridge. ", "page_idx": 21}, {"type": "text", "text": "7. Raw Caption: biological subfamily eating fish on a seaweed covered shore Language Rewrite: Two blue whales are eating salmon on a beach surrounded by seaweed Negative Caption (NegCLIP): ", "page_idx": 21}, {"type": "text", "text": "(a) Two blue salmon are eating whales on a beach surrounded by seaweed (b) Two stranded whales are eating salmon on a beach surrounded by seaweed (c) Two blue whales are eating salmon on a farm surrounded by seaweed (d) Two blue whales are eating salmon on a beach covered by seaweed TripletData Negative Caption: Two blue whales are feeding on herring in a bay surrounded by kelp. ", "page_idx": 21}, {"type": "text", "text": "8. Raw Caption: image of an original oil painting on canvas ", "page_idx": 21}, {"type": "text", "text": "Language Rewrite: A young lady holding a painting to your face so you can see the detail of the painting ", "page_idx": 21}, {"type": "text", "text": "Negative Caption (NegCLIP): ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(a) A young painting holding a lady to your face so you can see the detail of the lady (b) A bearded lady holding a painting to your face so you can see the detail of the painting (c) A young lady holding a pencil to your face so you can see the detail of the painting (d) A young lady holding a painting to your face so you can enjoy the detail of the painting TripletData Negative Caption: A young lady holding a painting away from her face to show its beauty to the audience. ", "page_idx": 21}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/a3ba3812f9092b68a2f75ebdc2dea9c2792da4308132ccb3e32953417bf5e691.jpg", "img_caption": ["Figure 6: Qualitative examples of positive and hard negative image-text pairs from TripletData. In each block, left image-text pairs are positive images from CC3M, and right pairs are corresponding negatives from TripletData. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ZfRGRK5Kxl/tmp/26ea83a479e6ca488f8e749afd4615c6fdfe93aea88e564a35f64a6622f4fe99.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 7: Examples of T2I failures. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide our novelty in abstract and introduction and perform experiments accordingly. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide limitations in the appendix ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No theoretical results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide all hyperparameters and plan to release the codebase. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We plan to release the data. However, due to the large scale of the data, we have not released it for review; instead, we have provided the randomly sampled data. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See the section 4. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Due to the nature of large-scale pertaining, we cannot repeat each training multiple times to calculate the error bars. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See section 3 and 4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Read and Agree. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide broader impacts in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We plan to release the data for academic purposes with whomever agrees with terms and conditions. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide credits to all works utilized in this study. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: N/A Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]