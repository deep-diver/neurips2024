[{"figure_path": "L3RYBqzRmF/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the Counter-Current Learning Framework: (a) At initialization, the counter-current learning framework establishes a dual network structure, with a forward network that maps the input to the target output, and a complementary feedback network that mirrors the forward network's architecture but propagates information in the opposite direction. The framework leverages the data processing inequality (DPI) from information theory, which states that information content cannot be increased through signal processing. Consequently, in both networks, information content decreases from the lower to the upper layers. (b) During training, the losses are computed in a layer-wise manner, i.e., by calculating the difference of activations from corresponding layer pairs between the forward and feedback networks, allowing the networks to learn from each other's complementary information. Notably, the dependency of the gradient on earlier layer parameters is interrupted using the gradient detachment operator.", "description": "This figure illustrates the Counter-Current Learning (CCL) framework.  Panel (a) shows the initial setup of two anti-parallel networks, a feedforward network processing inputs and a feedback network processing targets.  The networks' architectures mirror each other.  Panel (b) depicts the training process. Layer-wise losses are calculated by comparing activations between corresponding layers in the two networks. A \"stop gradient operator\" prevents gradient flow from affecting earlier layers, promoting localized learning.", "section": "3 Counter-Current Learning Framework"}, {"figure_path": "L3RYBqzRmF/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the Counter-Current Learning Framework: (a) At initialization, the counter-current learning framework establishes a dual network structure, with a forward network that maps the input to the target output, and a complementary feedback network that mirrors the forward network's architecture but propagates information in the opposite direction. The framework leverages the data processing inequality (DPI) from information theory, which states that information content cannot be increased through signal processing. Consequently, in both networks, information content decreases from the lower to the upper layers. (b) During training, the losses are computed in a layer-wise manner, i.e., by calculating the difference of activations from corresponding layer pairs between the forward and feedback networks, allowing the networks to learn from each other's complementary information. Notably, the dependency of the gradient on earlier layer parameters is interrupted using the gradient detachment operator.", "description": "This figure illustrates the Counter-Current Learning (CCL) framework.  Panel (a) shows the dual network architecture at initialization, with a feedforward network processing input and a feedback network processing target data.  The networks process information in anti-parallel, enhancing each other. Panel (b) depicts the training process, where layer-wise losses are calculated using the difference in activations between corresponding layers in the two networks. Gradient detachment prevents the dependency of gradient on earlier layers, promoting local learning.", "section": "3 Counter-Current Learning Framework"}, {"figure_path": "L3RYBqzRmF/figures/figures_4_1.jpg", "caption": "Figure 3: Dynamic Feature Alignment Between Forward and Backward Models During Counter-Current Learning. This series of t-SNE plots demonstrates the evolution of feature space alignment over different stages of training. Circular dots represent features from the forward network processing MNIST images, while squares depict features from the feedback network handling one-hot encoded labels. Each color represents a distinct class, with every subplot providing an independent t-SNE visualization. This emphasizes how distinct classes increasingly converge within and across the forward and backward models as training progresses, highlighting the dynamic and reciprocal nature of learning within the counter-current framework.", "description": "This figure shows how features from the forward and backward networks align during training using t-SNE.  Distinct classes, represented by different colors, gradually converge in the feature space, indicating the effectiveness of the counter-current learning in aligning the representations of both networks.", "section": "4.1 Counter Current Learning Facilitates Dual Network Feature Alignment"}, {"figure_path": "L3RYBqzRmF/figures/figures_6_1.jpg", "caption": "Figure 4: Visualization of the First Layer Convolutional Kernels of the Forward Model Trained with Error Backpropagation (BP) and Counter-Current Learning (CCL). Kernels from models trained with BP have more high-frequency components, as manifested as neighboring white (e.g., weight with high values) and black pixels (e.g., weight with low values). In comparison, those with CCL have more low-frequency components. We posit this might be because the error signal can contain more high-frequency information than the ideal target signal.", "description": "This figure visualizes the convolutional kernels of a forward model's first layer, trained using both backpropagation (BP) and counter-current learning (CCL).  The BP-trained kernels show high-frequency components (alternating white and black pixels representing high and low weights), while CCL-trained kernels exhibit more low-frequency components (smoother transitions in weights).  The authors hypothesize that this difference stems from error signals in BP carrying higher-frequency information than ideal target signals in CCL.", "section": "4.3 Auto-Encoder Performance"}, {"figure_path": "L3RYBqzRmF/figures/figures_6_2.jpg", "caption": "Figure 5: Qualitative Comparison of an Eight-Layered Convolutional Autoencoder Trained Using Error Backpropagation (BP) and Counter-Current Learning (CCL). The network structure does not contain skip connections. Testing set reconstruction results highlight CCL's comparable reconstruction as BP while achieving biological plausibility.", "description": "This figure shows a qualitative comparison of reconstruction results from an eight-layered convolutional autoencoder trained using both error backpropagation (BP) and counter-current learning (CCL).  The models were tested on the STL-10 dataset. Each row displays the ground truth target, the reconstruction using BP, and the reconstruction using CCL for ten different test images.  The results demonstrate that counter-current learning achieves comparable reconstruction quality to BP while exhibiting better biological plausibility.", "section": "4.3 Auto-Encoder Performance"}, {"figure_path": "L3RYBqzRmF/figures/figures_8_1.jpg", "caption": "Figure 6: Counter-Current Signal Propagation Enables Learning Through Reciprocal Representation Alignment. (Top) Centered Kernel Alignment (CKA) between the forward and feedback networks during training. At the initial training step (t = 0), cross-network CKA is minimal, suggesting a low similarity between networks. As training progresses, CKA significantly increases, especially in the top layers of both networks, indicating high similarity in learned high-level representations. (Bottom) Changes in CKA between consecutive training steps (i.e., from step t to step t + 1) reveal significant increases in the top layers of both networks, consistent with our counter-current learning insights. Notably, increases are concentrated in the a4, a5, a6 columns of the forward network and in the b0, b1, b2 rows of the feedback network, as highlighted by the green dotted box. These changes align with the expected reciprocal and complementary learning dynamics.", "description": "This figure visualizes the alignment of features between the forward and feedback networks in a counter-current learning framework during training. The top part shows the Centered Kernel Alignment (CKA) between layers at different stages of training.  It reveals that initially, the networks show little similarity. As training proceeds, the alignment increases, particularly at higher layers, suggesting a reciprocal interaction. The bottom part shows changes in CKA between consecutive steps, highlighting the increase at higher layers. This supports the counter-current mechanism: higher layers in one network learn from lower layers in the other, creating a reciprocal and complementary learning dynamic.", "section": "4.4 Empirical Analysis of Learning Dynamics for Counter-Current Learning"}, {"figure_path": "L3RYBqzRmF/figures/figures_8_2.jpg", "caption": "Figure 3: Dynamic Feature Alignment Between Forward and Backward Models During Counter-Current Learning. This series of t-SNE plots demonstrates the evolution of feature space alignment over different stages of training. Circular dots represent features from the forward network processing MNIST images, while squares depict features from the feedback network handling one-hot encoded labels. Each color represents a distinct class, with every subplot providing an independent t-SNE visualization. This emphasizes how distinct classes increasingly converge within and across the forward and backward models as training progresses, highlighting the dynamic and reciprocal nature of learning within the counter-current framework.", "description": "This figure visualizes the alignment of features between the forward and backward networks in the counter-current learning framework over different training stages using t-SNE plots.  It shows how features from different classes converge during training, indicating dynamic interaction and reciprocal learning between the two networks.", "section": "4.1 Counter Current Learning Facilitates Dual Network Feature Alignment"}, {"figure_path": "L3RYBqzRmF/figures/figures_13_1.jpg", "caption": "Figure 8: Feature Alignments Across Layers. We show that a similar feature alignment organization are observed in the third and fifth layer. While feature alignment in the first layer is inapparent.", "description": "This figure visualizes the feature alignment across different layers (1, 3, and 5) of a five-layered CNN model trained on CIFAR-10 using the Counter-Current Learning (CCL) method.  The t-SNE plots show the evolution of feature space alignment over different training stages (0, 10, 100, 1000, and 3000 CCL updates).  Circles represent features from the forward network, while squares represent features from the feedback network.  The consistent alignment of features across layers and training stages suggests the effectiveness of CCL in aligning representations between the forward and feedback networks.", "section": "6.3 Visualization of Representations in Other Layers"}, {"figure_path": "L3RYBqzRmF/figures/figures_14_1.jpg", "caption": "Figure 9: Weight Alignment Between the Forward and the Backward Network Trained With CCL. We find that the weight alignment of the first and the last quickly achieve plateau, while that of the intermediate layers increase gradually.", "description": "This figure shows the cosine similarity between the forward and backward network weights during training.  The x-axis represents the training epoch, and the y-axis represents the cosine similarity.  Each line represents a different layer in the network. The figure demonstrates that the first and last layers reach a plateau in cosine similarity relatively quickly.  However, the intermediate layers show a gradual increase in cosine similarity over the course of training, indicating a slower convergence of weight alignment.", "section": "6.4 Experiments on Weight Alignment"}]