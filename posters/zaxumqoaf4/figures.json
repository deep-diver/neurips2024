[{"figure_path": "zaXuMqOAF4/figures/figures_1_1.jpg", "caption": "Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.", "description": "This figure illustrates the architecture of Mesa-Extrapolation, a method designed for enhanced extrapolation in LLMs.  The left side shows a chunk-based triangular attention matrix, where the input sequence is divided into chunks to improve memory efficiency. The attention mechanism operates within and between these chunks. The right side demonstrates the concept of Position Encoding (PE) and Stair PE (a novel weave PE method).  Standard PE uses linear relative positions within a chunk, while Stair PE introduces non-linearity, particularly useful for weaving together information from multiple chunks during extrapolation.  The Stair PE helps extend the model's ability to handle longer sequences.", "section": "3 Model Extrapolation: NoPE vs. Weave PE"}, {"figure_path": "zaXuMqOAF4/figures/figures_4_1.jpg", "caption": "Figure 2: Thresholds for hidden states observed at specific dimensions on LLaMA2-7B-Chat, allowing for extrapolative judgments based on these thresholds. The vertical black dashed line indicates the position of maximum training length of the model. In this case, it is 4k for LLaMA2-7B-Chat model. The hidden state value at this position is designated as the observed threshold and marked with a horizontal red dashed line. When the hidden state value exceeds the red dashed line as the position changes, it signifies that the hidden state value has surpassed the threshold, suggesting a failure in extrapolation after that position.", "description": "This figure shows the thresholds for hidden states observed at specific dimensions in the LLaMA2-7B-Chat model.  The maximum training length of the model is indicated by a vertical dashed line.  A horizontal dashed line marks the observed threshold for the hidden state value at the maximum training length.  The plots show how the hidden state values change as the position changes.  If the hidden state values exceed the threshold, extrapolation failure is indicated.", "section": "3.4 Validating Extrapolation Using Observed Thresholds"}, {"figure_path": "zaXuMqOAF4/figures/figures_7_1.jpg", "caption": "Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.", "description": "This figure shows the accuracy of different LLMs in retrieving a password from a sequence of varying lengths.  The x-axis is the sequence length and the y-axis is the accuracy.  The figure demonstrates that methods using weave positional encoding (Mesa-Extrapolation, ReRoPE, Leaky-ReROPE) show better extrapolation capabilities beyond the maximum training length compared to other methods. The early stopping seen in some methods is attributed to hardware limitations.", "section": "5.1 Evaluation on Passkey Retrieval Tasks"}, {"figure_path": "zaXuMqOAF4/figures/figures_7_2.jpg", "caption": "Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.", "description": "This figure compares the performance of different position encoding methods (Origin, ReRoPE, Leaky-ReROPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation) on passkey retrieval tasks across various LLMs.  The x-axis shows the input token length, and the y-axis shows the accuracy.  The shaded regions indicate variance. The black dashed lines indicate the maximum training lengths of the models. The results show that weave PE-based methods like Mesa-Extrapolation maintain high accuracy even when exceeding the maximum training lengths, unlike the other methods which show significant performance degradation. The \"early stopping\" behavior seen in some methods is explained as a result of GPU memory limitations.", "section": "5.1 Evaluation on Passkey Retrieval Tasks"}, {"figure_path": "zaXuMqOAF4/figures/figures_8_1.jpg", "caption": "Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.", "description": "This figure shows the accuracy of different LLMs in retrieving a password hidden within a text sequence of varying lengths.  The x-axis shows the input sequence length, and the y-axis represents the accuracy (percentage of correct answers). The different colored areas represent the variance in accuracy.  The black dashed line indicates the maximum training length of the model.  The key takeaway is that the methods which weave position encoding (ReROPE, Leaky-ReROPE, and Mesa-Extrapolation) perform consistently well even when the input length exceeds the model's maximum training length, while other methods' performance quickly degrades.", "section": "5.1 Evaluation on Passkey Retrieval Tasks"}, {"figure_path": "zaXuMqOAF4/figures/figures_17_1.jpg", "caption": "Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.", "description": "This figure displays the performance of different LLMs (LLaMA-3B, MPT-7B, LLaMA2-7B, Vicuna-13B, Pythia-6.9B, Pythia-12B) on a password retrieval task using various methods (Origin, ReRoPE, Leaky-ReROPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation). The x-axis shows the input token length, and the y-axis shows the accuracy of the models in retrieving the password. The figure shows that weave PE-based methods (ReRoPE, Leaky-ReROPE, Mesa-Extrapolation) maintain high accuracy even when the input length exceeds the maximum training length.  The \"early stopping\" observed in some methods is attributed to limitations of the hardware resources used.", "section": "5.1 Evaluation on Passkey Retrieval Tasks"}, {"figure_path": "zaXuMqOAF4/figures/figures_18_1.jpg", "caption": "Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.", "description": "This figure compares the performance of different methods for extrapolating LLMs on a password retrieval task.  The x-axis shows the input sequence length, and the y-axis shows the accuracy of retrieving the password. The results show that methods using weave position encoding (Mesa-Extrapolation, ReRoPE, and Leaky-ReROPE) maintain higher accuracy even when input length exceeds the maximum training length, whereas other methods experience a significant drop in accuracy.", "section": "5.1 Evaluation on Passkey Retrieval Tasks"}, {"figure_path": "zaXuMqOAF4/figures/figures_20_1.jpg", "caption": "Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.", "description": "This figure shows the results of passkey retrieval tasks on various LLMs using different methods.  The x-axis represents the input token length, and the y-axis represents the accuracy of retrieving a hidden password within the input sequence. Different colored areas show the variance in accuracy across multiple runs. The black dashed line indicates the maximum training length of the LLMs. The figure highlights that methods using weave positional encoding (ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation) maintain consistently high accuracy even beyond the maximum training length, while other methods show significant drops in accuracy. The \"early stopping\" phenomenon observed in some methods is attributed to GPU memory limitations.", "section": "5.1 Evaluation on Passkey Retrieval Tasks"}, {"figure_path": "zaXuMqOAF4/figures/figures_20_2.jpg", "caption": "Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on 100 samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that \"early stopping\" phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.", "description": "This figure compares the accuracy of different extrapolation methods (Origin, ReRoPE, Leaky-ReROPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation) on various LLMs (LLaMA-3B, MPT-7B, LLaMA2-7B-Chat, Vicuna-13B, Pythia-6.9B, and Pythia-12B) for passkey retrieval tasks. The x-axis represents the input token length, and the y-axis represents the accuracy.  The shaded areas represent variance. The black dashed line indicates the maximum training length of each LLM. The results show that weave PE-based methods (ReRoPE, Leaky-ReROPE, and Mesa-Extrapolation) generally maintain accuracy even beyond the maximum training length, suggesting improved extrapolation capabilities compared to other methods.  The \"early stopping\" seen in some methods is attributed to GPU memory limitations.", "section": "5 Experiments"}, {"figure_path": "zaXuMqOAF4/figures/figures_28_1.jpg", "caption": "Figure 10: NIAH Task on Phi-3-mini-128k-instruct model using Origin and Mesa-Extrapolation.", "description": "This figure shows the result of applying the original model and the Mesa-Extrapolation method on the Phi-3-mini-128k-instruct model for the NIAH task in the Ruler dataset. The x-axis represents the input token length, while the y-axis represents the accuracy. The original model's performance drops sharply after reaching the 128k token limit. In contrast, the Mesa-Extrapolation method successfully extrapolates up to 192k tokens, demonstrating its ability to extend the effective input window length.", "section": "C.9 Evaluation of Phi-3-mini-128k-instruct Model on Ruler Datasets"}, {"figure_path": "zaXuMqOAF4/figures/figures_37_1.jpg", "caption": "Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.", "description": "This figure illustrates the core components of the Mesa-Extrapolation method. The left panel shows a chunk-based triangular attention matrix, a memory-efficient approach for handling long sequences.  The sequence is divided into chunks, with each chunk processed independently, reducing computational cost. The right panel demonstrates the difference between standard positional encoding (PE) and the novel Stair PE used in Mesa-Extrapolation. Stair PE is designed to effectively weave positional information into the last chunk of the sequence, enabling the model to extrapolate effectively beyond its typical effective range.", "section": "3.3 Theoretical Analysis"}, {"figure_path": "zaXuMqOAF4/figures/figures_38_1.jpg", "caption": "Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.", "description": "This figure illustrates the architecture of Mesa-Extrapolation, showing how the input sequence is divided into chunks, each with its own triangular attention matrix.  It highlights the use of positional encoding (PE) and Stair PE, a novel method introduced in the paper, within the attention mechanism.  Stair PE is specifically applied to the final chunk to manage the relative positional information and extend extrapolation capabilities beyond the model's effective range. The left part displays the chunk-based triangular attention matrix, and the right depicts an example of the PE and Stair PE mechanisms.", "section": "3.3 Theoretical Analysis"}, {"figure_path": "zaXuMqOAF4/figures/figures_38_2.jpg", "caption": "Figure 2: Thresholds for hidden states observed at specific dimensions on LLaMA2-7B-Chat, allowing for extrapolative judgments based on these thresholds. The vertical black dashed line indicates the position of maximum training length of the model. In this case, it is 4k for LLaMA2-7B-Chat model. The hidden state value at this position is designated as the observed threshold and marked with a horizontal red dashed line. When the hidden state value exceeds the red dashed line as the position changes, it signifies that the hidden state value has surpassed the threshold, suggesting a failure in extrapolation after that position.", "description": "This figure shows the observed thresholds for hidden state values in different dimensions (1 and 6) and layers (1 and 2) of the LLaMA2-7B-Chat model.  The vertical dashed line represents the maximum training length (4k tokens). The horizontal dashed line represents the threshold for each dimension and layer. The plot shows that when the hidden state value surpasses the threshold (red dashed line), extrapolation fails.  This visualization supports the paper's theoretical analysis of extrapolation failure in LLMs.", "section": "3.4 Validating Extrapolation Using Observed Thresholds"}, {"figure_path": "zaXuMqOAF4/figures/figures_39_1.jpg", "caption": "Figure 2: Thresholds for hidden states observed at specific dimensions on LLaMA2-7B-Chat, allowing for extrapolative judgments based on these thresholds. The vertical black dashed line indicate the position of maximum training length of the model. In this case, it is 4k for LLaMA2-7B-Chat model. The hidden state value at this position is designated as the observed threshold and marked with a horizontal red dashed line. When the hidden state value exceeds the red dashed line as the position changes, it signifies that the hidden state value has surpassed the threshold, suggesting a failure in extrapolation after that position.", "description": "This figure shows the hidden state values at specific dimensions (1 and 6) on layers 1 and 2 of the LLaMA2-7B-Chat model, highlighting the concept of a threshold for successful extrapolation.  The threshold (red dashed line) is determined by the hidden state value at the maximum training length (black dashed line; 4k). The plot shows that when the hidden state values exceed this threshold, extrapolation fails.  Two different methods (Origin and ReROPE) are compared, with ReROPE maintaining a stable performance beyond the threshold while Origin fails.", "section": "3.4 Validating Extrapolation Using Observed Thresholds"}, {"figure_path": "zaXuMqOAF4/figures/figures_41_1.jpg", "caption": "Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.", "description": "This figure illustrates the architecture of Mesa-Extrapolation, a novel method for enhanced extrapolation in LLMs. The left panel shows a chunk-based triangular attention matrix, which improves memory efficiency by processing the input sequence in smaller chunks. The right panel demonstrates the difference between conventional positional encoding (PE) and the proposed Stair PE. Stair PE uses a chunk-based weave to carefully arrange relative positions, enabling better extrapolation beyond the model's effective window length.", "section": "3.3 Theoretical Analysis"}, {"figure_path": "zaXuMqOAF4/figures/figures_41_2.jpg", "caption": "Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.", "description": "This figure illustrates the core components of the Mesa-Extrapolation method. The left panel depicts a chunk-based triangular attention matrix which is memory-efficient.  The right panel shows a comparison between standard Positional Encoding (PE) and the novel Stair PE, highlighting how Stair PE weaves relative positions to enhance extrapolation capabilities.", "section": "3.3 Theoretical Analysis"}, {"figure_path": "zaXuMqOAF4/figures/figures_41_3.jpg", "caption": "Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.", "description": "This figure illustrates the architecture of Mesa-Extrapolation, highlighting its chunk-based triangular attention mechanism and the novel Stair PE method. The left panel depicts how the input sequence is divided into chunks for processing within the triangular matrix, aiming for memory efficiency. The right panel contrasts the traditional linear PE with the proposed Stair PE, which adaptively weaves the relative positions to enhance extrapolation beyond the model's effective range.", "section": "3.3 Theoretical Analysis"}, {"figure_path": "zaXuMqOAF4/figures/figures_41_4.jpg", "caption": "Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.", "description": "This figure illustrates the architecture of Mesa-Extrapolation, a novel method for enhancing extrapolation in LLMs. The left panel depicts a chunk-based triangular attention matrix, which efficiently manages memory consumption by processing the input sequence in smaller chunks.  The right panel demonstrates the concept of Positional Encoding (PE) and Stair PE, showing how Stair PE weaves relative positions to achieve improved extrapolation performance.  Stair PE is specifically designed for the final chunk, combining information from all previous chunks.", "section": "3.3 Theoretical Analysis"}]