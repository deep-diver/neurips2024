[{"figure_path": "zaXuMqOAF4/tables/tables_8_1.jpg", "caption": "Table 1: ROUGE metric on LLaMA2-7B-Chat, averaged on 8 samples within each interval using the GovReport dataset. Each cell contains ROUGE-1/ROUGE-2/ROUGE-L. The best values are marked in bold. Some observations: (1) Dynamic-NTK shows slightly better performance within 11k. (2) Other methods showcase the ability to achieve scores of varying degrees.", "description": "This table presents the ROUGE-1, ROUGE-2, and ROUGE-L scores for different input lengths on the LLaMA2-7B-Chat model using the GovReport dataset.  It compares the performance of Origin, ReROPE, Leaky-ReROPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation.  The table highlights that the Origin and Streaming-LLM models' performance rapidly declines beyond the effective input window length, while the other methods exhibit varying degrees of performance, with Dynamic-NTK slightly better up to 11k tokens.", "section": "5.3 Evaluation on Summary Tasks"}, {"figure_path": "zaXuMqOAF4/tables/tables_17_1.jpg", "caption": "Table 2: BLEU metric for mean and standard variance on LLaMA2-7B-Chat, averaged on 8 samples within each interval using the GovReport dataset. The best values are marked in bold. Some observations: (1) Dynamic-NTK shows slightly better performance within 11k. (2) Weave PE-based methods showcase the ability to achieve scores of varying degrees.", "description": "This table presents the BLEU scores (mean and standard deviation) obtained using different extrapolation methods on the LLaMA2-7B-Chat model for the GovReport summarization task.  The results are broken down by input sequence length (from 1k to 11k tokens) and method (Origin, ReROPE, Leaky-ReROPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, Mesa-Extrapolation).  The best-performing method for each input length is highlighted in bold. The observations highlight that Dynamic-NTK performs relatively well up to an input length of 11k, while the weave PE methods generally maintain consistent performance across the range of input lengths. ", "section": "C.1 BLEU Results on GovReport"}, {"figure_path": "zaXuMqOAF4/tables/tables_19_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results of different LLMs on various tasks from the LongBench benchmark.  The LLMs tested include Origin, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation.  The tasks are categorized into five groups: Single-Document QA, Multi-Document QA, Few-shot Learning, Synthesis Tasks, and Code Completion. The table shows accuracy for each LLM on each task category and for two different input lengths (4-8k tokens and 8k+ tokens).  The results indicate that Mesa-Extrapolation generally outperforms the other methods, especially on the Code Completion task.", "section": "C.4 Evaluation on LongBench"}, {"figure_path": "zaXuMqOAF4/tables/tables_20_1.jpg", "caption": "Table 4: Theoretical Memory Usage Based on Attention Matrix for Different Methods. Observations: (1) Origin and Dynamic-NTK exhibit identical quadratic memory consumption. (2) ReRoPE and Leaky-ReROPE demonstrate 2\u00d7 the memory consumption of Origin. (3) LM-Infinite, Streaming-LLM, and Mesa-Extrapolation showcase linear memory consumption.", "description": "This table presents a theoretical analysis of the memory usage of different methods for handling long sequences in LLMs, categorized by their scaling behavior.  Origin and Dynamic-NTK show quadratic scaling (O(n\u00b2)),  while ReRoPE and Leaky-ReROPE exhibit even higher quadratic scaling (2 \u00d7 O(n\u00b2)).  In contrast, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation demonstrate linear scaling (O(n)), with Mesa-Extrapolation showing a slightly higher linear scaling of O((2+\u221a2)n). This analysis highlights the computational efficiency advantage of the linear scaling methods compared to the quadratic ones, particularly as input sequence length (n) increases.", "section": "C.7 Theoritical Speed & Memory"}, {"figure_path": "zaXuMqOAF4/tables/tables_21_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results of different LLMs on various tasks from the LongBench dataset.  The LLMs tested include Origin, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation. The tasks are categorized into Single-Document QA, Multi-Document QA, Few-shot Learning, Synthesis Tasks, and Code Completion.  The table shows that Mesa-Extrapolation generally outperforms other methods, particularly on tasks beyond simple question answering. Dynamic-NTK shows relatively good performance, especially on Code Completion, while LM-Infinite and Streaming-LLM have lower accuracy.", "section": "C.4 Evaluation on LongBench"}, {"figure_path": "zaXuMqOAF4/tables/tables_22_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results of different LLMs on various tasks from the LongBench benchmark.  The models compared include Origin (baseline), Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation. The tasks are categorized into five groups: Single-Document QA, Multi-Document QA, Few-shot Learning, Synthesis Tasks, and Code Completion.  The table shows accuracy scores for each model across different input lengths (4-8k and 8k+) for each task.  Observations highlight that Dynamic-NTK performs relatively well, especially on code completion, while LM-Infinite shows slightly lower performance. Mesa-Extrapolation generally outperforms other methods across most tasks.", "section": "C.4 Evaluation on LongBench"}, {"figure_path": "zaXuMqOAF4/tables/tables_23_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results of different LLMs on various tasks from the LongBench benchmark.  The tasks cover several categories including question answering (single and multi-document), few-shot learning, text synthesis, and code completion. Three different LLMs are tested: Origin (baseline), Dynamic-NTK, and Mesa-Extrapolation.  The table shows that Mesa-Extrapolation generally outperforms the other models, although Dynamic-NTK shows better performance on code completion tasks. LM-Infinite shows comparatively weaker performance.", "section": "C.4 Evaluation on LongBench"}, {"figure_path": "zaXuMqOAF4/tables/tables_24_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results on LongBench benchmark across five different tasks (Single-Document QA, Multi-Document QA, Few-shot Learning, Synthesis Tasks, Code Completion) using three different methods: Origin, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation.  The results are categorized by input token length (4-8k and 8k+).  The observations highlight the relatively strong performance of Mesa-Extrapolation, particularly in comparison to the Origin and Streaming-LLM methods, which show limited extrapolation capabilities beyond the original training length.", "section": "C.4 Evaluation on LongBench"}, {"figure_path": "zaXuMqOAF4/tables/tables_25_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results of different LLMs on various tasks from the LongBench benchmark.  The LLMs evaluated include Origin, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation. The tasks are categorized into five groups: Single-Document QA, Multi-Document QA, Few-shot Learning, Synthesis Tasks, and Code Completion.  The table shows the accuracy for each model on each task, broken down by input token length (4-8k and 8k+). The observations highlight that Dynamic-NTK performs well on Code Completion, while LM-Infinite shows slightly lower accuracy, and Mesa-Extrapolation generally outperforms the other models across different tasks.", "section": "C.4 Evaluation on LongBench"}, {"figure_path": "zaXuMqOAF4/tables/tables_26_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results of different LLMs on five tasks from the LongBench benchmark.  The tasks assess various capabilities including question answering, summarization, and code generation.  The results show that Mesa-Extrapolation performs better overall, while Dynamic-NTK excels in code completion tasks.", "section": "C.4 Evaluation on LongBench"}, {"figure_path": "zaXuMqOAF4/tables/tables_27_1.jpg", "caption": "Table 3: Accuracy on LongBench across multiple tasks using LLaMA2-7B-Chat. Some observations: (1) Dynamic-NTK shows good performance, especially for Code Completion. (2) LM-Infinite shows slightly weaker performance. (3) Mesa-Extrapolation shows better performance on most tasks.", "description": "This table presents the accuracy results of different LLMs on various tasks from the LongBench benchmark.  The LLMs tested include Origin, Dynamic-NTK, LM-Infinite, Streaming-LLM, and Mesa-Extrapolation.  The tasks cover several categories: Single-Document QA, Multi-Document QA, Few-shot Learning, Synthesis Tasks, and Code Completion.  The table shows that Mesa-Extrapolation generally outperforms other methods, especially on tasks such as Code Completion. Dynamic-NTK shows good performance, particularly on Code Completion.  LM-Infinite shows slightly weaker performance than other methods.", "section": "C.4 Evaluation on LongBench"}]