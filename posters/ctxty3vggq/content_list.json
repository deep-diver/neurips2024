[{"type": "text", "text": "Online Weighted Paging with Unknown Weights ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Orin Levy\u2217 Tel-Aviv University orinlevy@mail.tau.ac.il ", "page_idx": 0}, {"type": "text", "text": "Noam Touitou Amazon Science noamtwx@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Aviv Rosenberg\u2020 Google Research avivros007@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online paging is a fundamental problem in the field of online algorithms, in which one maintains a cache of $k$ slots as requests for fetching pages arrive online. In the weighted variant of this problem, each page has its own fetching cost; a substantial line of work on this problem culminated in an (optimal) $O(\\log k)$ -competitive randomized algorithm, due to Bansal, Buchbinder and Naor $(\\mathrm{FOCS}^{\\prime}07)$ ). ", "page_idx": 0}, {"type": "text", "text": "Existing work for weighted paging assumes that page weights are known in advance, which is not always the case in practice. For example, in multi-level caching architectures, the expected cost of fetching a memory block is a function of its probability of being in a mid-level cache rather than the main memory. This complex property cannot be predicted in advance; over time, however, one may glean information about page weights through sampling their fetching cost multiple times. We present the first algorithm for online weighted paging that does not know page weights in advance, but rather learns from weight samples. In terms of techniques, this requires providing (integral) samples to a fractional solver, requiring a delicate interface between this solver and the randomized rounding scheme; we believe that our work can inspire online algorithms to other problems that involve cost sampling. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online weighted paging. In the online weighted paging problem, or OWP, one is given a cache of $k$ slots, and requests for pages arrive online. Upon each requested page, the algorithm must ensure that the page is in the cache, possibly evicting existing pages in the process. Each page $p$ also has a weight $w_{p}$ , which represents the cost of fetching the page into the cache; the goal of the algorithm is to minimize the total cost of fetching pages. Assuming that the page weights are known, this problem admits an $O(\\log k)$ -competitive randomized online algorithm, due to Bansal, Buchbinder, and Naor [2010, 2012]; This is optimal, as there exists an $\\Omega(\\log k)$ -competitiveness lower bound for randomized algorithms due to Fiat et al. [1991] (that holds even for the unweighted case). ", "page_idx": 0}, {"type": "text", "text": "However, all previous work on paging assumes that the page weights are known in advance. This assumption is not always justified; for example, the following scenario, reminiscent of real-world architectures, naturally gives rise to unknown page weights. Consider a multi-core architecture, in which data can be stored in one of the following: a local \u201cL1\u201d cache, unique to each core; a global $\\mathbf{\\mathcal{L}}2^{\\bullet\\bullet}$ cache, shared between the cores; and the (large but slow) main memory. As a specific core requests memory blocks, managing its L1 cache can be seen as an OWP instance. Suppose the costs of fetching a block from the main memory and from the L2 cache are 1 and $\\epsilon\\ll1$ , respectively. Then, when a core demands a memory block, the expected cost of fetching this block (i.e., its weight) is a convex combination of 1 and $\\epsilon$ , weighted by the probability that the block is in the L2 cache; this probability can be interpreted as the demand for this block by the various cores. When managing the L1 cache of a core, we would prefer to evict blocks with low expected fetching cost, as they are more likely to be available in the L2 cache. But, this expected cost is a complicated property of the computation run by the cores, and estimating it in advance is infeasible; however, when a block is fetched in the above example, we observe a stochastic cost of either 1 or $\\epsilon$ . As we sample a given block multiple times, we can gain insight into its weight. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Multi-armed bandit. The above example, in which we learn about various options through sampling, is reminiscent of the multi-armed bandit problem, or MAB. In the cost-minimization version of this problem, one is given $n$ options (or arms), each with its own cost in [0, 1]. At each time step, the algorithm must choose an option and pay the corresponding cost; when choosing an option $p$ , rather than learning its cost $w_{p}$ , the algorithm is only revealed a sample from some distribution whose expectation is $w_{p}$ . In this problem, the goal is to minimize the regret, which is the difference between the algorithm\u2019s total cost and the optimal cost (which is to always c\u221ahoose the cheapest option). Over $T$ time steps, the best known regret bound for this problem is $\\tilde{O}(\\sqrt{n T})$ , achieved through multiple techniques. (See, e.g., Slivkins et al. [2019], Lattimore and Szepesv\u00e1ri [2020]). ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We make the first consideration of OWP where page weights are not known in advance, and show that the optimal competitive ratio of $O(\\log{k})$ can still be obtained. Specifically, we present the problem of OWP-UW (Online Weighted Paging with Unknown Weights), that combines OWP with bandit-like feedback. In OWP-UW, every page $p$ has an arbitrary distribution, whose expectation is its weight $0<w_{p}\\leq1$ . Upon fetching a page, the algorithm observes a random, independent sample from the distribution of the page. We present the following theorem for OWP-UW. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1. There exists a randomized algorithm ON for OWP-UW such that, for every input $Q$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\mathrm{ON}}(Q)]\\leq O(\\log k)\\cdot{\\mathrm{OPT}}(Q)+{\\tilde{O}}({\\sqrt{n T}}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where ${\\mathrm{ON}}(Q)$ is the cost of ON on $Q$ , ${\\mathrm{OPT}}({\\mathcal{Q}})$ is the cost of the optimal solution to $Q$ , and the expectation is taken over both the randomness in ON and the samples from the distributions of pages. ", "page_idx": 1}, {"type": "text", "text": "Note that the bound i\u221an Theorem 1.1 combines a competitive ratio of $O(\\log k)$ with a regret (i.e., additive) term of $\\tilde{O}(\\sqrt{n T})$ . To motivate this type of bound, we observe that OWP-UW does not admit sublinear regret without a competitive ratio. Consider the lower bound of $\\Omega(\\log k)$ for the competitive ratio of paging; stated simply, one of $k+1$ pages of weight 1 is requested at random. Over a sequence of $T$ requests, the expected cost of any online algorithm is $\\Omega(T/k)$ ; meanwhile, the expected cost of the optimal solution is at most $O(T/(k\\log k))$ . (The optimal solution would be to wait for a maximal phase of requests containing at most $k$ pages, whose expected length is $\\Theta(k\\log k)$ , then change state at constant cost.) Without a competitive ratio term, the difference between the online and offilne solutions is $\\Omega(T/k)$ , i.e., linear regret. We note that this kind of bound appears in several previous works such as Basu et al. [2019], Foussoul et al. [2023]. As OWP-UW generalizes both standard OWP and MAB, both the competitive ratio and regret terms are asymptotically tight: a competitiveness lower bound of $\\Omega(\\log k)$ is known for rand\u221aomized algorithms for online (weighted) paging [Fiat et al., 1991], and a regret lower bound of $\\tilde{\\Omega}(\\sqrt{n T})$ is known for MAB [Lattimore and Szepesv\u00e1ri, 2020]. ", "page_idx": 1}, {"type": "text", "text": "1.2 Our Techniques ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Interface between fractional solution and rounding scheme. Randomized online algorithms are often built of the following components: ", "page_idx": 1}, {"type": "text", "text": "1. A deterministic, $\\alpha$ -competitive online algorithm for a fractional relaxation of the problem. 2. An online randomized rounding scheme that encapsulates any online fractional algorithm, and has expected cost $_\\beta$ times the fractional cost. ", "page_idx": 1}, {"type": "text", "text": "Combining these components yields an $\\alpha\\beta$ -competitive randomized online (integral) algorithm. ", "page_idx": 1}, {"type": "text", "text": "For our problem, it is easy to see where this common scheme fails. The fractional algorithm cannot be competitive without sampling pages; but, pages are sampled by the rounding scheme! Thus, the competitiveness of the fractional algorithm is not independent of the randomized rounding, which must provide samples. One could think of addressing this by feeding any samples obtained by the rounding procedure into the fractional algorithm. However, as the rounding is randomized, this would result in a non-deterministic fractional algorithm. As described later in the paper, this is problematic: the rounding scheme demands a globally accepted fractional solution against which probabilities of cache states are balanced. ", "page_idx": 2}, {"type": "text", "text": "Instead, we outline a sampling interface between the fractional solver and the rounding scheme. Once the total fractional eviction of a page reaches an integer, the fractional algorithm will pop a sample of the page from a designated sampling queue, and process that sample. On the other side of the interface, the rounding scheme fills the sampling queue and ensures that when the fractional algorithm demands a sample, the queue will be non-empty with probability 1. ", "page_idx": 2}, {"type": "text", "text": "Optimistic fractional algorithm, pessimistic rounding scheme. When learning from samples, one must balance the exploration of unfamiliar options and the exploitation of familiar options that are known to be good. A well-known paradigm for achieving this balance in multi-armed bandit problems is optimism under uncertainty. Using this paradigm to minimize total cost, one maintains a lower confidence bound (LCB) for the cost of an option, which holds with high probability, and tightens upon receiving samples; then, the option with the lowest LCB is chosen. As a result, one of the following two cases holds: either the option was good (high exploitation); or, the option was bad, which means that the LCB was not tight, and henceforth sampling greatly improves it (high exploration). ", "page_idx": 2}, {"type": "text", "text": "Our fractional algorithm for weighted paging employs this method. It optimistically assumes that the price of moving a page is cheap, i.e., is equal to some lower confidence bound (LCB) for that page. It then uses multiplicative updates to allocate servers according to these LCB costs. The optimism under uncertainty paradigm then implies that the fractional algorithm learns the weights over time. ", "page_idx": 2}, {"type": "text", "text": "However, the rounding scheme behaves very differently. Unlike the fractional algorithm, the (randomized) rounding scheme is not allowed to use samples to update the confidence bounds; otherwise, our fractional solution would behave non-deterministically. Instead, the rounding scheme takes a pessimistic view: it uses an upper confidence bound (UCB) as the cost of a page, thus assuming that the page is expensive. Such pessimistic approaches are common in scenarios where obtaining additional samples is not possible (e.g., offline reinforcement learning [Levine et al., 2020]), but rarely appear as a component of an online algorithm as we suggest in this paper. ", "page_idx": 2}, {"type": "text", "text": "1.3 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The online paging problem is a fundamental problem in the field of online algorithms. In the unweighted setting, the optimal competitive ratio for a deterministic algorithm is $k$ , due to Sleator and Tarjan [1985]. Allowing randomization improves the best possible competitive ratio to $\\Theta(\\log{k})$ [Fiat et al., 1991]. As part of a line of work on weighted paging and its variants (e.g., Young [1994], Manasse et al. [1990], Albers [2003], Irani [2002], Fiat and Mendel [2000], Bansal et al. [2008], Irani [1997]), the best competitive ratios for weighted paging were settled, and were seen to match the unweighted setting: $k$ -competitiveness for deterministic algorithms, due to Chrobak et al. [1991]; and $\\Theta(\\log k)$ -competitiveness for randomized algorithms, due to Bansal et al. [2012]. ", "page_idx": 2}, {"type": "text", "text": "Online (weighted) paging is a special case of the $k$ -server problem, in which $k$ servers exist in a general metric space, and must be moved to address requests on various points in this space; the cache slots in (weighted) paging can be seen as servers, moving in a (weighted) uniform metric space. The $\\Theta(k)$ bound on optimal competitiveness in the deterministic for paging also extends to general $k$ -server [Manasse et al., 1990, Koutsoupias and Papadimitriou, 1995]. However, allowing randomization, a recent breakthrough result by Bubeck et al. [2023] was a lower bound of $\\Omega(\\log^{2}k)$ - competitiveness for $k$ -server, diverging from the $O(\\log{k})$ -competitiveness possible for paging. ", "page_idx": 2}, {"type": "text", "text": "Multi-Armed Bandit (MAB) is one of the most fundamental problems in online sequential decision making, often used to describe a trade-off between exploration and exploitation. It was extensively studied in the past few decades, giving rise to several algorithmic approaches that guarantee optimal regret. The most popular methods include Optimism Under Uncertainty (e.g., the UCB algorithm [Lai and Robbins, 1985, Auer et al., 2002a]), Action Elimination [Even-Dar et al., 2006], Thompson ", "page_idx": 2}, {"type": "text", "text": "Sampling [Thompson, 1933, Agrawal and Goyal, 2012] and Exponential Weights (e.g., the EXP3 algorithm [Auer et al., 2002b]). For a comprehensive review of the MAB literature, see Slivkins et al. [2019], Lattimore and Szepesv\u00e1ri [2020]. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In OWP-UW, we are given a memory cache of $k$ slots. A sequence of $T$ page requests then arrives in an online fashion; we denote the set of requested pages by $P$ , define $n:=|P|$ , and assume that $n>k$ . Each page $p$ has a corresponding weight $0<w_{p}\\,\\leq\\,1$ ; the weights are not known to the algorithm. Moreover, every page $p$ has a distribution $\\mathcal{D}_{p}$ supported in $(0,1]$ , such that $\\mathbb{E}_{x\\sim\\mathcal{D}_{p}}[x]=w_{p}$ . ", "page_idx": 3}, {"type": "text", "text": "The online scenario proceeds in $T$ rounds.3 In each round $t\\in\\{1,2,\\ldots,T\\}$ : ", "page_idx": 3}, {"type": "text", "text": "\u2022 A page $p_{t}\\in P$ is requested.   \n\u2022 If the requested page is already in the cache, then it is immediately served.   \n\u2022 Otherwise, we experience a cache miss, and we must fetch $p_{t}$ into the cache; if the cache is full, the algorithm must evict some page from the cache to make room for $p_{t}$ .   \n\u2022 Upon evicting any page $p$ from the cache, the algorithm receives an independent sample from $\\mathcal{D}_{p}$ . ", "page_idx": 3}, {"type": "text", "text": "The algorithm incurs cost when evicting pages from the cache: when evicting a page $p$ , the algorithm incurs a cost of ${w_{p}}^{4}$ . Our goal is to minimize the algorithm\u2019s total cost of evicting pages, denoted by ON, and we measure our performance by comparison to the total cost of the optimal algorithm, denoted by OPT. We say that our algorithm is $\\alpha$ -competitive with $\\mathcal{R}$ regret if $\\mathbb{E}[{\\mathrm{ON}}]\\leq\\alpha\\cdot{\\mathrm{OPT}}+\\mathcal{R}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Algorithmic Framework and Analysis Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present an overview of the concepts and algorithmic components we use to address OWP-UW. We would like to follow the paradigm of solving a fractional problem online, and then randomly rounding the resulting solution; however, as discussed in the introduction, employing this paradigm for OWP-UW requires a well-defined interface between the fractional solver and the rounding procedure. Thus, we present a fractional version of OWP-UW that captures this interface. ", "page_idx": 3}, {"type": "text", "text": "Fractional OWP-UW. In fractional OWP-UW, one is allowed to move fractions of servers, and a request for a page is satisfied if the total server fraction at that point sums to 1. More formally, for every page $p\\in P$ we maintain an amount $y_{p}\\in[0,1]$ which is the fraction of $p$ missing from the cache; we call $y_{p}$ the fractional anti-server at $p$ . (The term anti-server comes from the related $k$ -server problem.) The feasibility constraints are: ", "page_idx": 3}, {"type": "text", "text": "1. At any point in the algorithm, it holds that $\\begin{array}{r}{\\sum_{p\\in P}y_{p}\\ge n-k}\\end{array}$ . (I.e., the total number of pages in the cache is at most $k$ .)   \n2. After a page $p$ is requested, it holds that $\\mathrm{\\Delta}{y_{p}}=0$ . (I.e., there exists a total server fraction of 1 at $p$ .) ", "page_idx": 3}, {"type": "text", "text": "Evicting an $\\epsilon$ server fraction from $p$ (i.e., increasing $y_{p}$ by $\\epsilon$ ) costs $\\epsilon\\cdot w_{p}$ . ", "page_idx": 3}, {"type": "text", "text": "Sampling. The fractional algorithm must receive samples of pages over time in order to learn about their weights. An algorithm for fractional OWP-UW receives a sample of a page $p$ whenever the total fraction of $p$ evicted by the algorithm reaches an integer. In particular, the algorithm obtains the first sample of $p$ (corresponding to 0 eviction) when $p$ is first requested in the online input. ", "page_idx": 3}, {"type": "text", "text": "Algorithmic components. We present the fractional algorithm and randomized rounding scheme. ", "page_idx": 4}, {"type": "text", "text": "Fractional algorithm. In Section 4, we present an algorithm ONF for fractional OWP-UW. Fixing the random samples from the pages\u2019 weight distributions, the fractional algorithm ONF is deterministic. For every page $p\\in P$ , the fractional algorithm maintains an upper confidence bound $\\mathrm{UCB}_{p}$ and a lower confidence bound ${\\mathrm{LCB}}_{p}$ . These confidence bounds depend on the samples provided for that page; we define the good event $\\varepsilon$ to be the event that at every time and for every page $p\\in P$ , it holds that $\\mathrm{LCB}_{p}\\,\\leq\\,w_{p}\\,\\leq\\,\\mathrm{UCB}_{p}$ . We later show that $\\varepsilon$ happens with high probability, and analyze the complementary event separately5. Thus, we henceforth focus on the good event. ", "page_idx": 4}, {"type": "text", "text": "The following lemma bounds the cost of ONF subject to the good event. In fact, it states a stronger bound, that applies also when the cost of evicting page $p$ is the upper confidence bound $\\mathtt{U C B}_{p}\\ge w_{p}$ . $Q$ ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. Fixing any input for fractional OWP-UW, and assuming the good event, it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{ONF}(Q)\\leq\\overline{{\\mathrm{ONF}}}(Q)\\leq O(\\log k)\\cdot\\mathrm{OPT}(Q)+\\tilde{O}(\\sqrt{n T})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\overline{{\\mathrm{ONF}}}$ is the cost of the algorithm on the input where the cost of evicting a page $p$ is $\\mathtt{U C B}_{p}\\ge w_{p}$ . ", "page_idx": 4}, {"type": "text", "text": "Randomized rounding. In Section 5 we present the randomized algorithm ON for (integral) OWP-UW. It maintains a probability distribution over integral cache states by holding an instance of ONF, to which it feeds the online input. For the online input to constitute a valid fractional input, the randomized algorithm ensures that samples are provided to ONF when required. In addition, the randomized algorithm makes use of ONF\u2019s exploration of page weights; specifically, it uses the UCBs calculated by ONF. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2. Fixing any input $Q$ for (integral) OWP-UW, assuming the good event $\\varepsilon$ , it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\bf O N}(Q)]\\le{O O}(1)\\cdot\\overline{{{\\bf O N F}}}(Q)+n\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\overline{{\\mathrm{ONF}}}(Q)$ is the cost of the algorithm on $Q$ such that the cost of evicting a page $p$ is $\\mathtt{U C B}_{p}\\ge w_{p}$ . ", "page_idx": 4}, {"type": "text", "text": "Figure 1 provides a step-by-step visualization of the interface between the fractional algorithm and the rounding scheme over the handling of a page request. ", "page_idx": 4}, {"type": "text", "text": "4 Algorithm for Fractional OWP-UW ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now describe our algorithm for the fractional relaxation of OWP-UW, proving Lemma 3.1. Our fractional algorithm, presented in Algorithm 1 below, uses samples provided by the rounding scheme to learn the weights. A new sample for page $p$ is provided and processed whenever the sum of fractional movements (in absolute value) $m_{p}$ hits a natural number. (At this point the number of samples $n_{p}$ is incremented.) The algorithm calculates non-increasing UCBs and non-decreasing LCBs that will be specified later in Section C and guarantee with high probability, for every page $p\\in P$ and time step $t\\in[1,T],\\mathrm{LCB}_{p}\\leq w_{p}\\leq\\mathrm{UCB}_{p}$ . ", "page_idx": 4}, {"type": "text", "text": "At each time step $t$ , upon a new page request $p_{t}$ , the algorithm updates its feasible fractional cache solution $\\{y_{p}\\}_{p\\in P}$ . The fractions are computed using optimistic estimates of the weights, i.e., the LCBs, in order to induce exploration and allow the true weights to be learned over time. After serving page $p_{t}$ (that is, setting ${\\mathrm{\\Delta}y_{p_{t}}}\\mathrm{\\Delta}=0{\\mathrm{\\Delta}}$ ), the algorithm continuously increases the anti-servers of all the other pages in the cache until feasibility is reached (that is, until $\\begin{array}{r}{\\sum_{p\\in P}y_{p}=n-k)}\\end{array}$ . The fraction $y_{p}$ for some page $p$ in the cache is increased proportionally to $\\frac{y_{p}+\\eta}{\\mathrm{LCB}_{p}}$ , which is our adaption of the algorithmic approach of Bansal et al. [2010] to the unknown-weights scenario. Finally, to fulfil its end in the interface, the fractional algorithm passes its feasible fractional solution to the rounding scheme together with pessimistic estimates of the weights, i.e, the UCBs. ", "page_idx": 4}, {"type": "text", "text": "4.1 Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this analysis section, our goal is to bound the amount $\\overline{{\\mathrm{ONF}}}$ with respect to the UCBs and LCBs calculated by the algorithm; i.e., to prove Lemma 4.1. Lemma C.1 and Lemma C.2 from Appendix C then make the choice of confidence bounds concrete, such that combining it with Lemma 4.1 yields the final bound for the fractional algorithm, i.e., Lemma 3.1. ", "page_idx": 4}, {"type": "image", "img_path": "ctxtY3VGGq/tmp/4efa885ceb2dcbed5744acc4d233d4fd2a8faef4adc42781382dfcd1029ad85f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "This figure visualizes the running of the algorithm over a request in the input. (a) shows the state prior to the arrival of the request. The integral algorithm maintains an instance of the fractional algorithm ONF, as well as a pdirsotpriebrtuyt iaonn do tvheer  siuntbesgetr apl rcoapcehrtey )s;t antoetse t thhaat t utphhesoel dpsr soopemrtei epsr oarpee rat ifeus ncwt.ir.ot.n  oOf NbFo t(hs ptheec iafnictai-llsye,r tvhere  sctoatnes $\\left\\{\\boldsymbol{y}_{p}\\right\\}_{p\\in P}^{\\bullet}$ and the upper-confidence bounds $\\left\\{{\\mathrm{UCB}}_{p}\\right\\}_{p\\in P}$ in ONF. The integral algorithm also maintains a set of page samples, to be demanded by ONF at a later time. In (b), a page is requested (in red); thus, the fractional algorithm must fetch it into the cache, i.e., set its anti-server to 0. To maintain feasibility, the fractional algorithm will increase the anti-server at other pages in which some server fraction exists (in green). These changes in anti-server are also fed into the integral algorithm, which modifies its distribution to maintain consistency and the subset property w.r.t. ONF. In (c), ONF reaches integral total eviction of a page $p$ , and demands a sample from the integral algorithm. (We show that such a sample always exists when demanded.) The bound $\\mathrm{UCB}_{p}$ is updated in ONF, and is then fed to the integral algorithm to maintain the desired properties. After this sample, continuous increasing of anti-server continues until feasibility is reached in (d). Then, the integral algorithm ensures that a sample exists for the requested page $p_{t}$ , sampling the page if needed. (As $p_{t}$ now exists in $\\mu$ with probability 1, sampling is done through evicting and re-fetching $p_{t}$ .) ", "page_idx": 5}, {"type": "text", "text": "Figure 1: Visualization of the interface between the fractional and integral algorithms ", "page_idx": 5}, {"type": "image", "img_path": "ctxtY3VGGq/tmp/8fb5d0837fbfa550f57d510b10f0dc2f680c9e37e376fa3d468877591b0c9fbe.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Lemma 4.1. Fixing any input $Q$ for fractional OWP-UW, and assuming the good event, it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{ONF}}}(Q)\\leq O(\\log k)\\cdot\\mathrm{OPT}(Q)+\\sum_{p\\in P}\\sum_{i=1}^{n_{p}}\\bigl(\\mathrm{UCB}_{p,i}-\\mathrm{LCB}_{p,i}\\bigr)+2\\log(1+1/\\eta)\\sum_{p\\in P}\\mathrm{LCB}_{p}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where (a) $\\overline{{\\mathrm{ONF}}}$ is the cost of the algorithm on the input such that the cost of evicting a page $p$ is $\\operatorname{UCB}_{p}\\;\\geq\\;w_{p}$ , and $\\mathbf{\\mu}(b)$ $\\mathrm{UCB}_{p,i},\\mathrm{LCB}_{p,i}$ are the values of $\\mathrm{UCB}_{p}$ and ${\\mathrm{LCB}}_{p}$ calculated by the procedure UPDATECONFBOUNDS (found in Appendix $C$ ) immediately after processing the \ud835\udc56\u2019th sample of $p$ , and (c) ${\\mathrm{LCB}}_{p}$ is the value after the last sample of page $p$ was processed. ", "page_idx": 5}, {"type": "text", "text": "Proof sketch. We prove the lemma using a potential analysis. In Bansal et al. [2010], a potential function was introduced that encodes the discrepancy between the state of the optimal solution and the state of the algorithm. In our case, we require an additional term which can be viewed as a fractional exploration budget. This budget is \u201crecharged\u201d upon receiving a sample; the cost of this recharging goes into a regret term. \u25a1 ", "page_idx": 5}, {"type": "image", "img_path": "ctxtY3VGGq/tmp/83f8340f772e4cd4e1192843709fce654f7d8458f9c098552ac8d985a8f7a177.jpg", "img_caption": ["This figure visualizes a single step in REBALANCESUBSETS. Subfigure (a) shows the distribution of anti-cache states prior to this step; specifically, the $x$ axis is the probability measure, and the $y$ -axis is the number of pages of class $i$ and above in the anti-cache, i.e., $m:=|S\\cap P_{\\geq i}|$ . The red line is $Y_{i}$ , which through consistency, is the expectation of $m$ ; the blue dotted lines are thus the allowed values for $m$ , which are $|[Y_{i}],\\lfloor{Y_{i}}\\rfloor|$ . The total striped area in the figure is the imbalance measure, formally defined in Definition B.2. Subfigure (b) shows a single rebalancing step; we choose the imbalanced anti-cache $S$ that maximizes $|m-Y_{i}|$ ; in our case, $m>Y_{i}$ , and thus we match its measure with an identical measure of anti-cache states that are below the upper blue line, i.e., can receive a page without increasing imbalance. Then, a page of class $i$ is handed from $S$ to every matched state $S^{\\prime}$ ; note that every matched state might get a different page from \ud835\udc46, but some such page in $S\\setminus S^{\\prime}$ is proven to exist. Finally, Subfigure (c) shows the state after the page transfer; note the decrease in imbalance that results. The REBALANCESUBSETS procedure performs such steps until there is no imbalance; then, the procedure would advance to class $i-1$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Example of a rebalancing step ", "page_idx": 6}, {"type": "text", "text": "5 Randomized Rounding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section describes a randomized algorithm for (integral) OWP-UW, which uses Algorithm 1 for fractional OWP-UW to maintain a probability distribution over valid integral cache states, while obtaining and providing page weight samples to Algorithm 1. The method in which the randomized algorithm encapsulates and tracks the fractional solution is inspired by Bansal et al. [2012], which maintains a balanced property over weight classes of pages. However, as the weights are unknown in our case, the classes are instead defined using the probabilistic bounds maintained by the fractional solution (i.e., the UCBs). But, these bounds are dynamic, and change over the course of the algorithm; the imbalance caused by these discrete changes increases exponentially during rebalancing, and thus requires a more robust rebalancing procedure. ", "page_idx": 6}, {"type": "text", "text": "Following the notation in the previous sections, we identify each cache state with the set of pages not in the cache. Observing the state of the randomized algorithm at some point in time, let $\\mu(S)$ be the probability that $S\\subseteq P$ is the set of pages missing from the cache, also called the anti-cache. For the algorithm to be a valid algorithm for OWP-UW, the cache can never contain more than $k$ pages; this is formalized in the following property. ", "page_idx": 6}, {"type": "text", "text": "Definition 5.1 (valid distribution). A probability distribution $\\mu$ is valid, if for any set $S\\subseteq P$ with $\\mu(S)>0$ it holds that $\\left|S\\right|\\geq n-k$ . ", "page_idx": 6}, {"type": "text", "text": "Instead of maintaining the distribution\u2019s validity, we will maintain a stronger property that implies validity. This property is the balanced property, involving the UCBs calculated by the fractional algorithm. ", "page_idx": 6}, {"type": "text", "text": "For every page $p$ , we define the $i^{\\,\\bullet}$ th UCB class to be $P_{i}:=\\left\\{p\\in P:6^{i}\\leq\\mathrm{UCB}_{p}<6^{i+1}\\right\\}$ . (Note that $\\mathrm{UCB}_{p}\\in(0,1]$ .) We also define $P_{\\geq j}:=\\bigcup_{i\\geq j}P_{j}$ , the set of all pages that their UCB is at least $6^{j}$ . Let $\\{\\boldsymbol{y}_{p}\\}_{p\\in P}$ be the fractional solution. The balanced property requires that, for every set $S$ such that $\\mu(S)>0$ and every index $j$ , the number of pages in $S$ of class at least $j$ is the same as in the fractional solution, up to rounding. Formally, we define the balanced property as follows. ", "page_idx": 6}, {"type": "text", "text": "Definition 5.2 (balanced distribution). A probability distribution $\\mu$ has the balanced subsets property with respect to $y$ , if for any set $S\\subseteq P$ with $\\mu(S)>0$ , the following holds for all $j$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\,{\\biggl|}\\sum_{i\\geq j}\\sum_{p\\in P_{i}}y_{p}{\\biggr|}\\leq\\sum_{i\\geq j}\\sum_{p\\in P_{i}}\\mathbb{I}[p\\in S]\\leq{\\biggl|}\\sum_{i\\geq j}\\sum_{p\\in P_{i}}y_{p}{\\biggr|}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "1 Initialization   \n2 Let ONF be an instance of Algorithm 1, that maintains a fractional anti-server allocation $\\left\\{y_{p}\\right\\}_{p\\in P}$ .   \n3 Define $\\mu$ to be a distribution over cache states, initially containing the empty cache state with   \nprobability 1.   \nFor every $p\\in P$ , let $s_{p}\\gets\\mathrm{NULL}$ .   \n5 Event Function UPONREQUEST $(p)$ // called upon a request for page \ud835\udc5d   \npass the request for $p$ to ONF.   \n7 while ONF is handling the request for $p$ do // loop of Line 4 in Alg. 1   \n8 if ONF increases $y_{p^{\\prime}}$ by $\\epsilon_{:}$ , for some $p^{\\prime}\\in P$ then   \n9 add $p^{\\prime}$ to the anti-cache in an $\\epsilon$ -measure of states without $p^{\\prime}$ .   \n10 call REBALANCESUBSETS.   \n11 if ONF decreases $y_{p^{\\prime}}$ by $\\epsilon,$ , for some $p^{\\prime}\\in P$ then   \n12 remove $p^{\\prime}$ from the anti-cache in an $\\epsilon$ -measure of states with $p^{\\prime}$ .   \n13 call REBALANCESUBSETS.   \n14 if ONF samples a page $p^{\\prime}\\in P$ then // sample due to Line 6 of Alg. 1   \n15 provide $s_{p^{\\prime}}$ as a sample to ONF, and set $s_{p^{\\prime}}\\gets\\mathrm{NULL}$ .   \n16 call REBALANCESUBSETS. // rebalance due to change in $\\mathrm{UCB}_{p^{\\prime}}$ .   \n17 if $s_{p}=\\mathrm{NULL}$ then evict and re-fetch $p$ to obtain weight sample $\\tilde{w}_{p}$ , and set $s_{p}\\leftarrow\\tilde{w}_{p}$ .   \n18 if ONF requests a sample of $p$ then $\\slash$ sample due to Line $_{l l}$ of Alg. 1   \n19 provide $s_{p}$ as a sample to ONF, and set $s_{p}\\gets\\mathrm{NULL}$ .   \n20 call REBALANCESUBSETS. // rebalance due to change in $\\mathrm{UCB}_{p}$ . ", "page_idx": 7}, {"type": "text", "text": "Algorithm 3: Rebalancing procedure for randomized algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1 Function REBALANCESUBSETS   \n2 let $j_{\\mathrm{max}}$ be the maximum class that is not balanced.   \n3 let $j_{\\mathrm{min}}:=\\lceil\\log(\\operatorname{UCB}_{\\mathrm{min}})\\rceil$ , where $\\begin{array}{r}{\\mathrm{UCB}_{\\mathrm{min}}:=\\operatorname*{min}_{p\\in P}\\mathrm{UCB}_{p}}\\end{array}$ .   \n4 for every class $j$ , let $P_{j}:=\\left\\{p\\in P\\middle|\\middle[\\log(\\mathrm{UCB}_{p})\\big]=j\\right\\}$ .   \n5 for $j$ from $j_{\\mathrm{max}}$ down to $j_{\\mathrm{min}}$ do   \n6 let $\\begin{array}{r}{P_{\\geq j}:=\\bigcup_{j^{\\prime}\\geq j}P_{j^{\\prime}}}\\end{array}$ .   \n7 let \ud835\udc4c\ud835\udc57:= \ud835\udc5d\u2208\ud835\udc43\u2265\ud835\udc57\ud835\udc66\ud835\udc5d.   \n8 while \u2203\ud835\udc46s.t. $\\mu(S)>0$ and $|S\\cap P_{\\geq j}|\\not\\in\\left\\{\\left[Y_{j}\\right],\\left\\lfloor Y_{j}\\right\\rfloor\\right\\}$ do // iteratively eliminate imbalanced states   \n9 choose such $S$ that maximizes $\\left|m-Y_{j}\\right|$ , where $\\overline{{m}}:=\\left|S\\cap P_{\\geq j}\\right|$ .   \n10 if $m\\geq\\left\\lceil Y_{j}\\right\\rceil+1$ then   \n11 Match the $\\mu(S)$ measure of $S$ with an identical measure of anti-cache states with at most   \n$\\left\\lceil Y_{j}\\right\\rceil-1$ pages from $P_{\\geq j}$ .   \n12 foreach anti-cache state $S^{\\prime}$ matched with $S$ at measure $x\\leq\\mu(S)$ do   \n13 identify a page $p\\in P_{j}$ such that $p\\in S\\setminus S^{\\prime}$ .   \n14 remove $p$ from the anti-cache in the $x$ measure of $S$ , and insert it into the anti-cache in   \nthe $x$ measure of $S^{\\prime}$ .   \n15 if $m\\leq\\left\\lfloor Y_{j}\\right\\rfloor-1$ then   \n16 Match the $\\mu(S)$ measure of $S$ with an identical measure of anti-cache states with at least   \n$\\left\\lfloor Y_{j}\\right\\rfloor+1$ pages from $P_{\\geq j}$ .   \n17 foreach anti-cache state $S^{\\prime}$ matched with $S$ at measure $x_{S^{\\prime}}\\leq\\mu(S)$ do   \n18 identify a page $p\\in P_{j}$ such that $p\\in S^{\\prime}\\setminus S$ .   \n19 remove $p$ from the anti-cache in the $x$ measure of $S^{\\prime}$ , and insert it into the anti-cache   \nin the $x$ measure of $S$ . ", "page_idx": 7}, {"type": "text", "text": "Choosing the minimum UCB class in Definition 5.2, and noting that $\\begin{array}{r}{\\sum_{p\\in P}y_{p}\\;\\ge\\;n-k}\\end{array}$ through feasibility, immediately yields the following remark. ", "page_idx": 8}, {"type": "text", "text": "Remark 5.3. Every balanced probability distribution is also a valid distribution. ", "page_idx": 8}, {"type": "text", "text": "To follow the fractional solution, we also demand that the distribution $\\mu$ is consistent with the fractional solution, meaning, the marginal probability in $\\mu$ that any page $p$ is missing from the cache must be equal to $y_{p}$ . ", "page_idx": 8}, {"type": "text", "text": "Definition 5.4 (consistent distribution). A probability distribution $\\mu$ on subsets $S\\subseteq P$ is consistent with respect to a fractional solution $\\left\\{{\\boldsymbol{y}}_{p}\\right\\}_{p\\in P}$ if for every page $p$ it holds that $\\begin{array}{r}{\\sum_{S\\subseteq P|p\\in S}\\mu(S)=y_{p}}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "In the following we describe the online maintenance of the distribution $\\mu$ , that yields a distribution satisfying all of the above. ", "page_idx": 8}, {"type": "text", "text": "Algorithm overview. The randomized algorithm for OWP-UW is given in Algorithm 2. The algorithm encapsulates an instance of Algorithm 1 for fractional OWP-UW, called ONF. Upon a new page request $p_{t}$ at round $t$ , the algorithm forwards this requests to ONF. As ONF makes changes to its fractional solution, the algorithm modifies its probability distribution accordingly to remain consistent and balanced (and hence also valid). ", "page_idx": 8}, {"type": "text", "text": "Upon any (infinitesimally small) change to a fractional variable, the algorithm first changes its distribution to maintain consistency: when the fractional algorithm ONF increases the variable $y_{p^{\\prime}}$ of any page $p^{\\prime}$ by an $\\epsilon$ -measure, the algorithm identifies an $\\epsilon$ -measure of cache states $S\\subseteq P$ in which there is no anti-server at $p^{\\prime}$ and adds anti-server at $p^{\\prime}$ . The case in which the fractional algorithm decreases a variable is analogous. ", "page_idx": 8}, {"type": "text", "text": "However, this procedure may invalidate the balanced property. Specifically, for some class $j$ , letting $Y_{j}$ be the total anti-server fraction of pages of at least class $j$ in ONF, there might now be states with $\\left\\lceil Y_{j}\\right\\rceil+1$ or $\\left\\lfloor Y_{j}\\right\\rfloor-1$ such pages in the anti-cache. Thus, the algorithm makes a call to REBALANCESUBSETS, which restores the balanced property class-by-class, in a descending order. For every class $j$ , the procedure repeatedly identifies a violating state $S$ where the number of pages of $\\mathrm{class}\\geq j$ in the anti-cache is not in $\\{\\big[Y_{j}\\big],\\\\ \\big[Y_{j}\\big]\\}$ ; suppose it identifies such a state with more than $\\left\\lceil Y_{j}\\right\\rceil$ such pages (the case of less than $\\left\\lfloor{Y_{j}}\\right\\rfloor$ pages is analogous). The procedure seeks to move a page of class $j$ from this state to another state in a way that does not increase the \u201cimbalance\u201d in class $j$ . Thus, the procedure identifies a matching measure of anti-cache states that contain at most $\\left\\lceil Y_{j}\\right\\rceil-1$ such pages, and moves a page of class $j$ from $S$ to $S^{\\prime}$ , for every $S^{\\prime}$ in the matched measure; a visualization of the procedure is given in Figure 2. (The existence of this matching measure, as well as a page to move, are shown in the analysis.) In particular, note that the probability of every page being in the anti-cache remains the same, and thus REBALANCESUBSETS does not impact consistency. ", "page_idx": 8}, {"type": "text", "text": "Regarding samples, the algorithm can maintain a sample $s_{p}$ for every page $p$ . A sample for $p$ is obtained upon a request for $p$ after $p$ is fetched with probability 1 into the cache, if no such sample already exists (i.e., $s_{p}=\\mathrm{{NULL})}$ . Whenever ONF requests a sample for a page $p$ , the randomized algorithm provides the sample $s_{p}$ , and sets the variable $s_{p}$ to be NULL (we show that $s_{p}$ is never NULL when ONF samples $p$ ). A fine point is the sampling of a new page in Line 11 of Algorithm 1; this happens after Line 17 of Algorithm 2. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we presented the first algorithm for online weighted paging in which page weights are not known in advance, but are instead sampled stochastically. In this model, we were able to recreate the best possible bounds for the classic online problem, with an added regret term typical to the multi-armed bandit setting. This unknown-costs relaxation makes sense because the problem has recurring costs; that is, the cost of evicting a page $p$ can be incurred multiple times across the lifetime of the algorithm, and thus benefits from sampling. ", "page_idx": 8}, {"type": "text", "text": "We believe this paper can inspire future work on this problem. For example, revisiting the motivating case of managing a core-local L1 cache, the popularity of a page among the cores can vary over time; this would correspond to the problem of non-stationary bandits (see, e.g., Auer et al. [2019b,a], Chen et al. [2019]), and it would be interesting to apply techniques from this domain to OWP-UW. ", "page_idx": 8}, {"type": "text", "text": "Finally, we hope that the techniques outlined in this paper could be extended to additional such problems. Specifically, we believe that the paradigm of using optimistic confidence bounds in lieu of actual costs could be used to adapt classical online algorithms to the unknown-costs setting. In addition, the interface between the fractional solver and rounding scheme could be used to mediate integral samples to an online fractional solver, which is a common component in many online algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation program (grant agreement No. 882396), by the Israel Science Foundation, the Yandex Initiative for Machine Learning at Tel Aviv University and a grant from the Tel Aviv University Center for AI and Data Science (TAD). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pages 39\u20131. JMLR Workshop and Conference Proceedings, 2012.   \nS. Albers. Online algorithms: a survey. Math. Program., 97(1-2):3\u201326, 2003. doi: 10.1007/ s10107-003-0436-0. URL https://doi.org/10.1007/s10107-003-0436-0.   \nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235\u2013256, 2002a.   \nP. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002b.   \nP. Auer, Y. Chen, P. Gajane, C. Lee, H. Luo, R. Ortner, and C. Wei. Achieving optimal dynamic regret for non-stationary bandits without prior information. In A. Beygelzimer and D. Hsu, editors, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings of Machine Learning Research, pages 159\u2013163. PMLR, 2019a. URL http://proceedings.mlr.press/v99/auer19b.html.   \nP. Auer, P. Gajane, and R. Ortner. Adaptively tracking the best bandit arm with an unknown number of distribution changes. In A. Beygelzimer and D. Hsu, editors, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings of Machine Learning Research, pages 138\u2013158. PMLR, 2019b. URL http://proceedings.mlr.press/ v99/auer19a.html.   \nN. Bansal, N. Buchbinder, and J. Naor. Randomized competitive algorithms for generalized caching. In C. Dwork, editor, Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008, pages 235\u2013244. ACM, 2008. doi: 10.1145/ 1374376.1374412. URL https://doi.org/10.1145/1374376.1374412.   \nN. Bansal, N. Buchbinder, and J. S. Naor. A simple analysis for randomized online weighted paging. Unpublished Manuscript, 2010.   \nN. Bansal, N. Buchbinder, and J. Naor. A primal-dual randomized algorithm for weighted paging. J. ACM, 59(4):19:1\u201319:24, 2012. doi: 10.1145/2339123.2339126. URL https://doi.org/10. 1145/2339123.2339126.   \nS. Basu, R. Sen, S. Sanghavi, and S. Shakkottai. Blocking bandits. Advances in Neural Information Processing Systems, 32, 2019.   \nS. Bubeck, C. Coester, and Y. Rabani. The randomized k-server conjecture is false! In B. Saha and R. A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 581\u2013594. ACM, 2023. doi: 10.1145/3564246.3585132. URL https://doi.org/10.1145/3564246.3585132.   \nY. Chen, C. Lee, H. Luo, and C. Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free. In A. Beygelzimer and D. Hsu, editors, Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedings of Machine Learning Research, pages 696\u2013726. PMLR, 2019. URL http://proceedings.mlr.press/ v99/chen19b.html.   \nM. Chrobak, H. J. Karloff, T. H. Payne, and S. Vishwanathan. New results on server problems. SIAM J. Discret. Math., 4(2):172\u2013181, 1991. doi: 10.1137/0404017. URL https://doi.org/ 10.1137/0404017.   \nE. Even-Dar, S. Mannor, Y. Mansour, and S. Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006.   \nA. Fiat and M. Mendel. Better algorithms for unfair metrical task systems and applications. In F. F. Yao and E. M. Luks, editors, Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, May 21-23, 2000, Portland, OR, USA, pages 725\u2013734. ACM, 2000. doi: 10.1145/335305.335408. URL https://doi.org/10.1145/335305.335408.   \nA. Fiat, R. M. Karp, M. Luby, L. A. McGeoch, D. D. Sleator, and N. E. Young. Competitive paging algorithms. J. Algorithms, 12(4):685\u2013699, 1991. doi: 10.1016/0196-6774(91)90041-V. URL https://doi.org/10.1016/0196-6774(91)90041-V.   \nA. Foussoul, V. Goyal, O. Papadigenopoulos, and A. Zeevi. Last switch dependent bandits with monotone payoff functions. In International Conference on Machine Learning, pages 10265\u201310284. PMLR, 2023.   \nS. Irani. Page replacement with multi-size pages and applications to web caching. In F. T. Leighton and P. W. Shor, editors, Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing, El Paso, Texas, USA, May 4-6, 1997, pages 701\u2013710. ACM, 1997. doi: 10.1145/ 258533.258666. URL https://doi.org/10.1145/258533.258666.   \nS. Irani. Randomized weighted caching with two page weights. Algorithmica, 32(4):624\u2013640, 2002. doi: 10.1007/s00453-001-0095-6. URL https://doi.org/10.1007/s00453-001-0095-6.   \nE. Koutsoupias and C. H. Papadimitriou. On the k-server conjecture. J. ACM, 42(5):971\u2013983, 1995. doi: 10.1145/210118.210128. URL https://doi.org/10.1145/210118.210128.   \nT. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4\u201322, 1985.   \nT. Lattimore and C. Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \nS. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \nM. S. Manasse, L. A. McGeoch, and D. D. Sleator. Competitive algorithms for server problems. J. Algorithms, 11(2):208\u2013230, 1990. doi: 10.1016/0196-6774(90)90003-W. URL https://doi. org/10.1016/0196-6774(90)90003-W.   \nD. D. Sleator and R. E. Tarjan. Amortized efficiency of list update and paging rules. Commun. ACM, 28(2):202\u2013208, 1985. doi: 10.1145/2786.2793. URL https://doi.org/10.1145/2786.2793.   \nA. Slivkins et al. Introduction to multi-armed bandits. Foundations and Trends\u00ae in Machine Learning, 12(1-2):1\u2013286, 2019.   \nW. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.   \nN. E. Young. The k-server dual and loose competitiveness for paging. Algorithmica, 11(6):525\u2013541, 1994. doi: 10.1007/BF01189992. URL https://doi.org/10.1007/BF01189992. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Analysis of the Fractional Algorithm ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this analysis section, our goal is to bound the amount $\\overline{{\\mathrm{ONF}}}$ with respect to the UCBs and LCBs calculated by the algorithm; i.e., to prove Lemma 4.1. Lemma C.1 and Lemma C.2 from Appendix C then make the choice of confidence bounds concrete, such that combining it with Lemma 4.1 yields the final bound for the fractional algorithm, i.e., Lemma 3.1. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 4.1. For the sake of this lemma, we assume without loss of generality that the optimal solution is lazy; that is, it only evicts a (single) page in order to fetch the currently-requested page. (It is easy to see that any solution can be converted into a lazy solution of lesser or equal cost.) In the following we present a potential analysis to prove that $\\overline{{\\mathrm{ONF}}}\\leq2\\log(1+k)\\cdot\\mathrm{OPT}+\\mathcal{U}_{T}$ , where $\\mathcal{U}_{T}$ is a regret term summed over $T$ time steps that will be defined later. To that end, we show that the following equation holds for every round $t$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Delta\\overline{{\\mathrm{ONF}}}_{t}+\\Delta\\Phi_{t}\\le2\\log(1+1/\\eta)\\cdot\\Delta\\mathrm{OPT}_{t}+\\Delta\\mathcal{U}_{t},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\Delta X_{t}$ is the change in $X$ in time $t$ and $\\Phi$ is a potential function that we define next. ", "page_idx": 12}, {"type": "text", "text": "Let $C_{t}^{*}$ denote the set of pages in the offline (optimal) cache at time $t$ . The potential function we chose is an adaptation of the potential function used by Bansal et al. [2010], but modified to encode the uncertainty cost for not knowing the true weights. We define it as follows. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Phi_{t}=-2\\sum_{p\\notin C_{t}^{*}}\\mathrm{LCB}_{p}\\cdot\\log\\left(\\frac{y_{p}+\\eta}{1+\\eta}\\right)+\\sum_{p\\in P}\\big(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\big)\\cdot\\big(n_{p}-m_{p}\\big),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathtt{U C B}_{p},\\mathtt{L C B}_{p}$ are the confidence bounds in time step $t$ , $n_{p}$ is the number of samples of page $p$ collected until that point, and $m_{p}$ is the total fractional movement of that page until that point. Note that $n_{p}-m_{p}\\in[0,1]$ . We now show that Equation 1 holds in the three different cases in which the costs of the potential or the regret change. Before moving forward, we define the regret term, denoted $\\mathcal{U}$ , as follows, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{U}:=\\sum_{p\\in P}\\sum_{i=1}^{n_{p}}(\\mathrm{UCB}_{p,i}-\\mathrm{LCB}_{p,i})+2\\log(1+1/\\eta)\\sum_{p\\in P}\\mathrm{LCB}_{p}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We note that each time the algorithm gets a new sample, $n_{p}$ is incremented and $\\mathcal{U}$ increases. ", "page_idx": 12}, {"type": "text", "text": "Case 1 - the optimal algorithm moves. Note that a change in the cache of the optimal solution does not affect $\\overline{{\\mathrm{ONF}}}$ or $\\mathbf{\\nabla}\\mathcal{U}$ , and thus $\\Delta\\overline{{\\mathrm{ONF}}}\\,=\\,\\Delta{\\mathcal{U}}\\,=\\,0$ . However, $\\Delta\\mathrm{OPT}$ might be non-zero, as the optimal solution incurs a moving cost; in addition, $\\Delta\\Phi$ might be non-zero, as the change in the optimal cache might affect the summands in the first term of the potential function. ", "page_idx": 12}, {"type": "text", "text": "Thus, proving Equation (1) for this case reduces to proving $\\Delta\\Phi_{t}\\,\\leq\\,2\\log(1+1/\\eta)\\Delta\\mathrm{OPT}.$ . Assume the optimal solution moves; as we assume that the optimal solution is lazy, it must be that the requested page \ud835\udc5d\ud835\udc61 is not in \ud835\udc36\ud835\udc61\u2217\u2212 , and that the optimal solution fetches it into the cache, possibly evicting a (single) page from its cache. ", "page_idx": 12}, {"type": "text", "text": "First, consider the case in which there exists an empty slot in the cache, and no evictions take place when fetching $p_{t}$ . In this case, $\\Delta\\mathrm{OPT}=0$ ; moreover, as $C_{t-1}^{*}\\subseteq C_{t}^{*}$ , it holds that $\\Delta\\Phi\\le0$ . Thus, Equation (1) holds. ", "page_idx": 12}, {"type": "text", "text": "Otherwise, at time $t$ , page $p_{t}$ is fetched and another page $p$ is evicted. Thus, $\\Delta\\mathrm{OPT}=w_{p}$ . For the potential change, $p$ starts to contribute to $\\Phi$ . In the worst case, $y_{p}=0$ and then $\\Phi$ is increased by at most $2\\mathrm{LCB}_{p}\\,\\mathrm{log}(\\bar{1}+1/\\eta)\\leq2w_{p}\\,\\mathrm{log}(1+1/\\eta)=2\\log(1+1/\\eta)\\mathrm{\\dot{\\Delta}}\\mathrm{OPT}$ , as desired. ", "page_idx": 12}, {"type": "text", "text": "Case 2 - the fractional algorithm moves. In this case, only the potential and the cost of the fractional algorithm change, i.e., $\\Delta\\mathrm{OPT}_{t}\\;=\\;\\Delta\\mathcal{U}_{t}\\;=\\;0$ . Thus, Equation (1) reduces to proving $\\Delta\\overline{{\\mathrm{ONF}}}_{t}+\\Delta\\Phi_{t}\\le0$ in this case. ", "page_idx": 12}, {"type": "text", "text": "There are two types of movement made by the algorithm: the immediate fetching of the requested page $p_{t}$ , and the continuous eviction of other pages from the cache until feasibility is reached (i.e., there are $n-k$ anti-servers). First, consider the fetching of $p_{t}$ into the cache; as we charge for evictions, this action does not incur cost $\\langle\\Delta\\overline{{\\mathrm{ONF}}}\\,=\\,0\\rangle$ ). In addition, through OPT\u2019s feasibility, it holds that $p_{t}\\,\\in\\,C_{t}^{*}$ , and thus changing $y_{p_{t}}$ does not affect the potential function $\\Delta\\Phi=0)$ ; thus, Equation (1) holds for this sub-case. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Next, consider the continuous eviction of pages. Suppose the fractional algorithm evicts $d_{y}$ page increases units, where $y_{p}$ $d_{y}$ proportionally to is infinitesimally small. Let $\\frac{\\mathrm{y}_{p}+\\eta}{\\mathrm{LCB}_{p}}$ for each $S=\\{p_{t}\\}\\cup\\{p:\\,y_{p}<1\\}$ $p\\,\\in\\,S\\,\\backslash\\,\\{p_{t}\\}$ , it holds that, . Since the fractional algorithm $\\begin{array}{r}{\\mathrm{~}d y_{p}\\,=\\,\\frac{1}{N}\\,\\cdot\\,\\frac{y_{p}+\\eta}{\\mathrm{LCB}_{p}}d y}\\end{array}$ , for $\\begin{array}{r}{N:=\\sum_{p\\in S\\backslash\\{p_{t}\\}}\\frac{y_{p}+\\eta}{\\mathrm{LCB}_{p}}}\\end{array}$ L\ud835\udc5dC+B\ud835\udc5d. Thus, \u0394ONF can be bounded as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta\\overline{{\\mathrm{OWF}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\le\\displaystyle\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\bigcup_{\\substack{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\bigcup_{j=1}\\overline{{\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{j}\\}}}}}\\\\ {=\\displaystyle\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\displaystyle\\Big(\\mathrm{LCB}_{p}+\\big(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\big)d y_{p}}\\\\ {=\\displaystyle\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\displaystyle\\Big(\\mathrm{LCB}_{p}+\\big(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\big)\\big)\\frac{1}{N}\\frac{y_{p}+\\eta}{\\mathrm{LCB}_{p}}d y}\\\\ {=\\displaystyle\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\displaystyle\\Big(\\mathrm{UCB}_{p}+\\big(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\big)d y_{p}}\\\\ {=\\displaystyle\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}\\frac{1}{N}\\big(y_{p}+\\eta\\big)d y+\\displaystyle\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}\\Big(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\big)d y_{p}}\\\\ {=\\displaystyle\\sum_{p\\in\\mathcal{S}\\backslash\\{p_{r}\\}}\\frac{1}{N}\\big(y_{p}+\\eta\\big)d y+\\displaystyle\\sum_{p\\in\\mathcal{P}}\\Big(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\big)d y_{p}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the final equality stems from having $d y_{p}=0$ for every $p\\notin S\\setminus\\{p_{t}\\}$ . We now bound the term $(a)$ using similar arguments to those presented in Bansal et al. [2010]. We present them below for completeness. ", "page_idx": 13}, {"type": "equation", "text": "$$\n(a)=\\sum_{p\\in S\\backslash\\{p_{t}\\}}\\frac{1}{N}\\big(y_{p}+\\eta\\big)d y\\leq\\frac{(|S|-k)}{N}d y+\\frac{(|S|-1)\\eta}{N}d y\\leq\\frac{2(|S|-k)}{N}d y,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality implied since ${\\mathrm{y}}_{p}=1$ for $p\\not\\in S$ , which yields that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{p\\in S\\backslash\\{p_{t}\\}}y_{p}\\leq\\sum_{p\\in S}y_{p}=\\sum_{p\\in P}y_{p}-\\sum_{p\\notin S}y_{p}\\leq(|P|-k)-(|P|-|S|)=|S|-k,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the second inequality implied as $(x-1)\\eta\\leq x-k$ for any $x\\geq k+1$ and using that $\\left|S\\right|\\geq k+1$ . Next, we upper bound $\\Delta\\Phi_{t}$ . The rate of change in the potential with respect to $y_{p}$ is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d\\Phi_{t}}{d y_{p}}=-2\\frac{\\mathrm{LCB}_{p}}{y_{p,t}+\\eta}-\\bigl(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\bigr).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The non-trivial part in the above calculation is that $\\begin{array}{r}{\\frac{d\\left(\\operatorname{UCB}_{p}-\\operatorname{LCB}_{p}\\right)\\cdot\\left(n_{p}-m_{p}\\right)}{d y_{p}}=-\\big(\\operatorname{UCB}_{p}-\\operatorname{LCB}_{p}\\big)}\\end{array}$ This is true since $n_{p}$ is uncorrelated with the change in $y_{p}$ , however, $m_{p}$ is increasing with with respect to $y_{p}$ in 1-linear ratio. Using the above we get that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\Phi_{t}=\\displaystyle\\sum_{p\\in P}\\left(\\frac{d\\Phi_{t}}{d y_{p}}\\right)d y_{p}}\\\\ &{\\phantom{\\Delta}=-2\\displaystyle\\sum_{p\\in S\\backslash C_{t}^{*}}\\frac{\\mathrm{LCB}_{p}}{y_{p}+\\eta}\\frac{1}{N}\\frac{y_{p}+\\eta}{\\mathrm{LCB}_{p}}d y-\\displaystyle\\sum_{p\\in P}\\left(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\right)\\cdot d y_{p}}\\\\ &{\\phantom{\\Delta}=-2\\displaystyle\\sum_{p\\in S\\backslash C_{t}^{*}}\\frac{d y}{N}-\\displaystyle\\sum_{p\\in P}\\left(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\right)\\cdot d y_{p}}\\\\ &{\\phantom{\\Delta}\\leq-2\\displaystyle\\frac{|S|-k}{N}d y-\\displaystyle\\sum_{p\\in P}\\left(\\mathrm{UCB}_{p}-\\mathrm{LCB}_{p}\\right)\\cdot d y_{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, $\\Delta\\overline{{\\mathrm{ONF}}}_{t}+\\Delta\\Phi_{t}\\le0$ , as required. ", "page_idx": 13}, {"type": "text", "text": "Case 3 - the LCBs are updated (a new sample is processed). Suppose the algorithm samples a page $p$ for the $i^{\"}$ th time, and updates $\\mathrm{UCB}_{p}$ , ${\\mathrm{LCB}}_{p}$ accordingly. Note that this sample does not incur any cost for the fractional algorithm or optimal solution, and thus $\\Delta\\overline{{\\mathrm{ONF}}}=\\Delta\\mathrm{OPT}=0$ . Thus, proving Equation (1) reduces to proving $\\Delta\\Phi\\leq\\Delta\\mathcal{U}$ for this case. Recalling the definition of $\\mathcal{U}$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{U}=(\\mathrm{UCB}_{p,i}-\\mathrm{LCB}_{p,i})+2\\log\\left(1+1/\\eta\\right)(\\mathrm{LCB}_{p,i}-\\mathrm{LCB}_{p,i-1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Denote by $n_{p},m_{p}$ the variables of that name prior to sampling $p$ ; note that $m_{p}$ remains the same after sampling, while $n_{p}$ is incremented to $n_{p_{.}}^{\\prime}:=n_{p_{.}}+1$ . Moreover, note that sampling always occurs when $m_{p}=n_{p}$ . Thus, the change in potential function is bounded as follows. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\Phi=2\\log(1+1/\\eta)\\mathbb{I}[p\\notin C_{i}^{*}]\\big(\\mathrm{LCB}_{p,i}-\\mathrm{LCB}_{p,i-1}\\big)+\\mathrm{UCB}_{p,i}-\\mathrm{LCB}_{p,i}}\\\\ {\\leq2\\log(1+1/\\eta)\\big(\\mathrm{LCB}_{p,i}-\\mathrm{LCB}_{p,i-1}\\big)+\\mathrm{UCB}_{p,i}-\\mathrm{LCB}_{p,i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality is due to the LCBs being monotone non-decreasing. ", "page_idx": 14}, {"type": "text", "text": "B Proofs from Section 5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Analysis and Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this subsection, we analyze Algorithm 2 and prove Lemma 3.2. Throughout this subsection, we assume the good event $\\varepsilon$ ; that is, the UCBs and LCBs generated by ONF throughout the algorithm are valid upper and lower bounds for the weights of pages. ", "page_idx": 14}, {"type": "text", "text": "We start by proving that the algorithm is able to provide a new sample whenever the fractional algorithm requires one. ", "page_idx": 14}, {"type": "text", "text": "Proposition B.1. Algorithm 2 provides page weight samples whenever demanded by ONF. ", "page_idx": 14}, {"type": "text", "text": "Proof. We must prove that whenever a sample of page $p$ is requested by ONF, it holds that the variable $s_{p}$ in Algorithm 2 is not NULL. Note that Algorithm 2 samples $s_{p}$ at the end of a request for $p$ . Now, note that: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The first sample of $p$ is requested after the first request for $p$ is handled by ONF, and thus after $p$ is sampled.   \n\u2022 Between two subsequent requests for samples of $p$ by ONF, the eviction fraction $m_{p}$ increased by 1. But, this cannot happen without a request for $p$ in the interim; this request ensures that $s_{p}\\neq\\mathrm{NULL}$ , and thus the second request is satisfied. ", "page_idx": 14}, {"type": "text", "text": "Combining both cases, all sample requests by ONF are satisfied by Algorithm 2. ", "page_idx": 14}, {"type": "text", "text": "Next, we focus on proving that the distribution maintained by the algorithm is consistent and balanced (and hence also valid). To prove this, we first formalize and prove the guarantee provided by the REBALANCESUBSETS procedure. To this end, we define an amount quantifying the degree to which a class is imbalanced in a given distribution. ", "page_idx": 14}, {"type": "text", "text": "Definition B.2. Let $\\mu$ be a distribution over cache states, let $\\left\\{y_{p}\\right\\}_{p\\in P}$ be the current fractional solution, and let $j$ be some class. We define the imbalance of class $j$ in $\\mu$ to be ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{S\\subseteq P}\\mu(S)\\cdot\\operatorname*{max}\\left\\{\\left|S\\cap P_{\\geq j}\\right|-\\left\\lceil Y_{j}\\right\\rceil,\\left\\lfloor Y_{j}\\right\\rfloor-\\left\\lfloor S\\cap P_{\\geq j}\\right\\rfloor\\right\\};\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition B.2 quantifies the degree to which a given class is not balanced in a given distribution; one can see that if the class is balanced, this amount would be 0. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.3. Suppose REBALANCESUBSETS is called, and let $\\mu,\\mu^{\\prime}$ be the distributions before and after the call to REBALANCESUBSETS, respectively; let $j_{\\mathrm{max}}$ be the maximum non-balanced class in $\\mu$ . ", "page_idx": 14}, {"type": "text", "text": "Suppose that $({\\pmb a})~{\\boldsymbol\\mu}$ is consistent, and $\\mathbf{\\mu}(b)$ there exists $\\epsilon\\,>\\,0$ such that the imbalance of any class $j\\,\\leq\\,j_{\\mathrm{max}}\\,\\,i n\\,\\mu$ is at most $\\epsilon$ . Then, it holds that $\\mu^{\\prime}$ is both consistent and balanced. Moreover, the total cost of REBALANCESUBSETS is at most $12\\epsilon\\cdot6J\\mathrm{max}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The running of REBALANCESUBSETS consists of iterations of the For loop in Line 5; we number these iterations according to the class considered in the iteration (e.g., \"Iteration $i\"$ considers class $i$ ). We make a claim about the state of the anti-cache distribution after each iteration, and prove this claim inductively; applying this claim to the final iteration implies the lemma. Specifically, where $j_{\\mathrm{max}}$ as defined in the lemma statement, for every $i\\,\\leq\\,j_{\\mathrm{max}}$ consider the distribution immediately before Iteration $i$ , denoted $\\mu_{i}$ . We claim that (a) the $\\mu_{i}$ is consistent, (b) all classes greater than $i$ are balanced in $\\mu_{i}$ , and (c) classes at most $i$ have imbalance at most $\\epsilon\\cdot3^{j_{\\mathrm{max}}-i}$ in $\\mu_{i}$ . Where $j_{\\mathrm{min}}$ is the minimum class, note that $\\mu^{\\prime}=\\mu_{j_{\\operatorname*{min}}-1}$ , and that this claim implies that $\\mu^{\\prime}$ is consistent and balanced. (Note that the claim implies that class $j_{\\mathrm{min}}$ is balanced, which also implies that all classes smaller than $j_{\\mathrm{min}}$ are balanced.). ", "page_idx": 15}, {"type": "text", "text": "We prove this claim by descending induction on $i$ . The base case, in which $i\\,=\\,j_{\\mathrm{max}}$ , is simply a restatement of the assumptions made in the lemma, and thus holds. Now, assume that the claim holds for any class $i\\le j_{\\mathrm{max}}$ ; we now prove it for class $i-1$ . ", "page_idx": 15}, {"type": "text", "text": "Consistency. First, as we\u2019ve assumed that $\\mu_{i}$ is consistent, note that Iteration $i$ does not change the marginal probability of a given page $p$ being in the anti-cache, as pages are only moved between identical anti-cache measures. Thus, the anti-cache distribution remains consistent at any step during Iteration $i$ ; in particular, $\\mu_{i-1}$ is consistent. ", "page_idx": 15}, {"type": "text", "text": "Existence of destination measure. Next, observe that every changes in Iteration $i$ consists of identifying a measure of a violating anti-cache, and matching this measure to an identical measure of anti-cache states to which pages can be moved. To show that the procedure is legal, we claim that this measure always exists. Consider such a change that identifies violating anti-cache $S$ , and let $\\hat{\\mu}$ be the distribution at that point. Assume that $S$ is an \u201cupwards\u201d violation, i.e., $m\\ge\\lceil Y_{i}\\rceil+1$ , where $m:=|S\\cap P_{\\geq i}|$ ; the case of a \u201cdownwards\u201d violation is analogous. Note that consistency implies that $\\mathbb{E}_{S^{\\prime}\\sim\\hat{\\mu}}\\left|S^{\\prime}\\cap P_{\\geq i}\\right|=Y_{i}$ . Also note that $S$ was chosen to maximize $|m-Y_{i}|$ , i.e., the distance from the expectation. Thus, there exists a measure of at least ${\\hat{\\mu}}(S)$ of anti-caches $S^{\\prime}$ such that $|S^{\\prime}\\cap P_{\\geq i}|<Y_{i}$ (and thus $|S^{\\prime}\\cap P_{\\geq i}|\\leq\\lceil Y_{i}\\rceil-1$ , as required). ", "page_idx": 15}, {"type": "text", "text": "Existence of page to move. After matching the aforementioned $S$ to some $S^{\\prime}$ , we want to identify some page $p\\in P_{i}$ such that $p\\in S\\backslash S^{\\prime}$ , so we can move it from the measure of $S$ to the measure of $S^{\\prime}$ . Indeed, from the choice of $S$ and $S^{\\prime}$ , it holds that $|S\\cap P_{\\geq i}|\\geq\\lceil Y_{i}\\rceil+1\\geq|S^{\\prime}\\cap P_{\\geq i}|+2$ . But, from the induction hypothesis for Iteration $i$ , class $i+1$ was balanced in $\\mu_{i}$ , and thus remains balanced at every step during Iteration $i$ (as this iteration never moves pages of classes $i+1$ and above). This implies that $|S\\cap\\bar{P_{\\geq i+1}}|\\leq\\lceil Y_{i+1}\\rceil\\leq|S^{\\prime}\\cap P_{\\geq i+1}|+1$ . We can thus conclude that there exists $p\\in P_{i}\\cap(S\\,{\\bar{\\backslash}}\\,S^{\\prime})$ as required. ", "page_idx": 15}, {"type": "text", "text": "Balanced property. Next, we prove that in $\\mu_{i-1}$ after Iteration $i$ , class $i$ is balanced, and the imbalance of any class $j<i$ is at most $\\epsilon\\cdot3^{j_{\\mathrm{max}}-(i-1)}$ . Consider any step in Iteration $i$ , where a measure $x$ of a violating state is identified; then, a page is moved from a measure $x$ to another measure $x$ . The induction hypothesis for Iteration $i$ implies that the imbalance of class $i$ at $\\mu_{i}$ is at most $\\epsilon\\cdot3^{j_{\\mathrm{max}}-i}$ . Note that: ", "page_idx": 15}, {"type": "text", "text": "1. This step decreases the imbalance of class $i$ by at least $x$ , as it decreases the imbalance in the violating state, but does not increase imbalance in the matched measure.   \n2. This step can increase the imbalance of a class $j<i$ by at most $2x$ , in the worst case in which moving the page increased imbalance in both measures of $x$ . ", "page_idx": 15}, {"type": "text", "text": "As a result, we can conclude that for $\\mu_{i-1}$ , at the end of iteration $i$ , class $i$ is balanced, while the imbalance of every class $j<i$ increased by at most $2\\cdot\\epsilon\\cdot3^{j_{\\mathrm{max}}-i}$ . Combining this with the hypothesis for iteration $i$ , the imbalance of every class $j$ at $\\mu_{i-1}$ is at most $\\epsilon\\cdot3\\cdot3^{j_{\\operatorname*{max}}-i}=\\epsilon\\cdot3^{j_{\\operatorname*{max}}-(i-1)}$ , as required. ", "page_idx": 15}, {"type": "text", "text": "This concludes the inductive proof of the claim. ", "page_idx": 15}, {"type": "text", "text": "Cost analysis. As mentioned before, every step in Iteration $i$ reduces imbalance at class $i$ by (at least) $x$ , where $x$ is the measure of the chosen violating anti-cache state. The cost of this step is the cost of evicting a single page in $P_{i}$ from a measure $x$ ; as we assume that the weight of a page is at most its UCB, this cost is at most $x\\cdot6^{i+1}$ . The inductive claim above states that the imbalance of class $i$ at the beginning of Iteration $i$ is at most $\\epsilon\\cdot3^{j_{\\mathrm{max}}-i}$ ; thus, the total cost of Iteration $i$ is at most $\\epsilon\\cdot3^{j_{\\mathrm{max}}-i}\\cdot6^{i+1}\\overset{\\smile}{=}\\epsilon\\cdot6^{\\stackrel{\\smile}{j_{\\mathrm{max}}}+1}/2^{j_{\\mathrm{max}}-i}$ . Summing over iterations, the total cost of REBALANCESUBSETS ", "page_idx": 15}, {"type": "text", "text": "is at most: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=j_{\\operatorname*{min}}}^{j_{\\operatorname*{max}}}\\epsilon\\cdot6^{j_{\\operatorname*{max}}+1}/2^{j_{\\operatorname*{max}}-i}\\leq12\\epsilon\\cdot6^{j_{\\operatorname*{max}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can now prove that the distribution is consistent and balanced. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4. The distribution maintained by Algorithm 2 is both consistent and balanced. ", "page_idx": 16}, {"type": "text", "text": "Proof. First, we prove that the distribution is consistent. Indeed, note that consistency is explicitly maintained in Lines 9 and 12, and that Lemma B.3 implies that the subsequent calls to REBALANCESUBSETS does not affect this consistency. ", "page_idx": 16}, {"type": "text", "text": "As the distribution is consistent at any point in time, Lemma B.3 also implies that it is balanced immediately after every all to REBALANCESUBSETS; as the handling of every request ends with such a call, the distribution is always balanced after every request. \u25a1 ", "page_idx": 16}, {"type": "text", "text": "At this point, we\u2019ve shown that Algorithm 2 is legal: it maintains a valid distribution (through Lemma B.4 and Remark 5.3), and it provides samples to ONF when required (Proposition B.1); thus, it is a valid randomized algorithm for OWP-UW. It remains to bound the expected cost of Algorithm 2, thus proving Lemma 3.2; recall that this bound is in terms of $\\overline{{\\mathrm{ONF}}}$ , the cost of the fractional algorithm in terms of its UCBs rather than actual page weights. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3.2. We consider the eviction costs incurred by Algorithm 2, and bound their costs individually. ", "page_idx": 16}, {"type": "text", "text": "First, consider the eviction cost due to maintaining consistency Line 9 (note that Line 12 only fetches pages, and incurs no cost). An increase of $\\epsilon$ in $y_{p^{\\prime}}$ causes an eviction of $p^{\\prime}$ with $\\epsilon$ probability; the expected cost of $\\epsilon\\cdot w_{p^{\\prime}}$ can be charged to the eviction cost of $\\boldsymbol{\\epsilon}\\cdot\\mathrm{UCB}_{p^{\\prime}}$ incurred in $\\overline{{\\mathrm{ONF}}}$ , and thus the overall cost due to this line is at most $\\overline{{\\mathrm{ONF}}}$ . ", "page_idx": 16}, {"type": "text", "text": "Next, consider the cost due to eviction during sampling (Line 17). Observe a page $p\\,\\in\\,P$ that is evicted in this way; the cost of this eviction is $w_{p}$ . For the first and second samples of $p$ , we note that $w_{p}\\leq1$ ; summing over $p\\in P$ , the overall cost of those evictions is at most $2n$ . For subsequent samples of $p$ , note that for $i>2$ , the $i^{\\,\\bullet}$ th sample of $p$ is taken when $m_{p}\\in(i-2,i-1]$ . Thus, we can charge this sample to the fractional eviction that increased $m_{p}$ from $i-3$ to $i-2$ , which costs $\\mathrm{UCB}_{p}$ . Thus, the overall cost of this sampling is at most $\\overline{{\\mathrm{ONF}}}+2n$ . ", "page_idx": 16}, {"type": "text", "text": "It remains to bound the cost of the REBALANCESUBSETS procedure. First, consider the cost of REBALANCESUBSETS due to sampling (Lines 16 and 20). Consider the state prior to such a call; some page $p$ has just been sampled, possibly decreasing $\\mathrm{UCB}_{p}$ and decreasing the class of page $p$ , which could break the balanced property. Specifically, let $i,i^{\\prime}$ be the old and new classes of $p$ , where $i^{\\prime}<i$ . Then, imbalance could be created only in classes $j\\in\\{i^{\\prime}+1,\\cdot\\cdot\\cdot\\,,i\\}$ . In such class $j$ , both $Y_{j}$ could decrease, and $\\left|S\\cap P_{\\geq j}\\right|$ could decrease for any anti-cache state $S$ . However, as only one page changed class, one can note that the total imbalance in any such class $j$ is at most 1. Thus, Lemma B.3 guarantees that the total cost of REBALANCESUBSETS is at most $\\overset{.}{12}\\cdot6^{i}\\leq\\,12\\cdot\\mathrm{UCB}_{p}$ . Using the same argument as for the cost of sampling, the total cost of such calls is at most $12\\overline{{\\mathrm{ONF}}}+24n$ . ", "page_idx": 16}, {"type": "text", "text": "Now, consider a call to REBALANCESUBSETS in Line 10; A page $p^{\\prime}$ was evicted for fraction $\\epsilon$ in ONF, and an $\\epsilon$ measure of $p^{\\prime}$ was evicted in the distribution. Let $j$ be the class of $p^{\\prime}$ ; there could only be imbalance in classes at most $j$ . For any such $i\\leq j,Y_{i}$ increased by $\\epsilon$ , and $|S\\cap P_{\\geq i}|$ increased by 1 in at most $\\epsilon$ measure of states $S$ . In addition, let $Y_{i}^{-},Y_{i}^{+}:=Y_{i}^{-}$ be the old and new values of $Y_{i}$ . Consider the imbalance in class $i$ : ", "page_idx": 16}, {"type": "text", "text": "1. If $\\left\\lfloor Y_{i}^{+}\\right\\rfloor\\ =\\ \\left\\lfloor Y_{i}^{-}\\right\\rfloor\\ +\\ 1$ , then the imbalance of class $i$ can increase due to states $S$ where $|S\\cap P_{\\geq i}|=\\left\\lfloor Y_{i}^{-}\\right\\rfloor$ becoming unbalanced. But, due to consistency, the fact that $Y_{i}^{-}\\ge\\lceil Y_{i}^{-}\\rceil{-}\\epsilon$ implies that the measure of such pages is at most $\\epsilon$ ; thus, the imbalance grows by at most $\\epsilon$ . ", "page_idx": 16}, {"type": "text", "text": "2. The adding of page $p^{\\prime}$ to an $\\epsilon$ -measure of pages can add an imbalance of at most $\\epsilon$ . ", "page_idx": 16}, {"type": "text", "text": "Overall, the imbalance in classes at most $j$ prior to calling REBALANCESUBSETS is at most $2\\epsilon$ . Through Lemma B.3, the cost of REBALANCESUBSETS is thus at most $24\\epsilon\\cdot6J$ , which is at most $24\\epsilon\\cdot\\mathrm{UCB}_{p^{\\prime}}$ . But, the increase in $\\overline{{\\mathrm{ONF}}}$ due to the fractional eviction is at least $\\boldsymbol{\\epsilon}\\cdot\\mathrm{UCB}_{p^{\\prime}}$ ; thus, the overall cost of REBALANCESUBSETS called in Line 10 is at most $24\\cdot{\\overline{{\\mathrm{ONF}}}}$ . ", "page_idx": 17}, {"type": "text", "text": "Finally, consider a call to REBALANCESUBSETS in Line 13, called upon a decrease in $y_{p^{\\prime}}$ of $\\epsilon$ for some page $p^{\\prime}$ . Using an identical argument to the case of eviction, we can bound the cost of this call by 24 times the \u201cfetching cost\u201d of $\\boldsymbol\\epsilon\\boldsymbol\\cdot\\mathrm{UCB}_{p^{\\prime}}$ . Now, note that the fractional fetching of a page exceeds the fractional eviction by at most 1; thus, the total cost of such calls is at most $24{\\overline{{\\mathrm{ONF}}}}+24n$ . ", "page_idx": 17}, {"type": "text", "text": "Summing all costs, the total expected cost of the algorithm is at most $62\\overline{{\\mathrm{ONF}}}+50n$ . ", "page_idx": 17}, {"type": "text", "text": "B.2 Proof of Theorem 1.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now combine the ingredients in this paper to prove the main competitiveness bound for Algorithm 2. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 1.1. First, assume the good event $\\varepsilon$ . Combining Lemma 3.2, Lemma 3.1 and Lemma C.2, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{ON}]\\leq O(\\log k)\\cdot\\mathrm{OPT}(Q)+\\tilde{O}(\\sqrt{n T})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, assume that $\\varepsilon$ does not occur. Upper bound the cost of the algorithm by $O(n)$ per request, as in the worst case, the algorithm replaces the entire cache and samples each page in $P$ once. Thus, an upper bound for the cost of the algorithm is $O(n T)$ . But, through Lemma C.1, the probability of $\\varepsilon$ not occurring is at most $\\textstyle{\\frac{1}{n T}}$ . Thus, we can bound the expected cost of the algorithm as follows. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{ON}]\\leq\\operatorname*{Pr}[\\mathcal{E}]\\cdot\\Big(O(\\log k)\\cdot\\mathrm{OPT}(Q)+\\tilde{O}(\\sqrt{n T})\\Big)+\\operatorname*{Pr}[\\neg\\mathcal{E}]\\cdot O(n T)}\\\\ &{\\quad\\quad\\quad\\leq O(\\log k)\\cdot\\mathrm{OPT}(Q)+\\tilde{O}(\\sqrt{n T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Choosing Confidence Bounds and Bounding Regret ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we define the UCBs and LCBs, prove that they hold with high probability and bound the regret term. ", "page_idx": 17}, {"type": "text", "text": "Let $w_{p}^{i}$ be the $i$ -th sample of page $p$ . For the initial confidence bounds, we sample each page once and set LCB\ud835\udc5d,1 = $\\begin{array}{r}{\\mathrm{LCB}_{p,1}\\:=\\:\\frac{1}{2n^{2}T}\\,\\cdot\\,w_{p}^{1}}\\end{array}$ , $\\mathrm{UCB}_{p,1}\\,=\\,1$ . Once we have $i\\,>\\,1$ samples of page $p$ , we define the confidence bounds as follows. Let $\\begin{array}{r}{\\bar{w}_{p,i}\\;:=\\;\\frac{1}{i}\\;\\sum_{j=1}^{i}w_{p}^{j}}\\end{array}$ be the average observed weight, and $\\epsilon_{p,i}=\\sqrt{\\frac{\\log(4n^{3}T^{3})}{2i}}$ be the confidence radius. Then, we set $\\begin{array}{r}{\\mathrm{LCB}_{p,i}=\\operatorname*{max}\\{\\mathrm{LCB}_{p,i-1},\\bar{w}_{p,i}-\\epsilon_{p,i}\\}}\\end{array}$ and $\\mathrm{UCB}_{p,i}=\\operatorname*{min}\\{\\mathrm{UCB}_{p,i-1},\\bar{w}_{p,i}+\\epsilon_{p,i}\\}$ . The following procedure updates the confidence bounds online. ", "page_idx": 17}, {"type": "text", "text": "We show that the confidence bounds indeed bound the true weights with high probability (Lemma C.1), and then bound the regret term (Lemma C.2). ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. Let $n_{p}$ be the final number of samples collected for page $p$ . With probability at least $\\textstyle1-{\\frac{1}{n T}}$ , the following properties hold. ", "page_idx": 17}, {"type": "text", "text": "Proof. By definition, the LCBs are monotonically non-decreasing and the UCBs are monotonically non-increasing. Moreover, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\mathbf{UCB}_{p,i}-\\mathbf{LCB}_{p,i}=\\operatorname*{min}\\bigr\\{\\mathbf{UCB}_{p,i-1},\\bar{w}_{p,i}+\\epsilon_{p,i}\\bigr\\}-\\operatorname*{max}\\bigr\\{\\mathbf{LCB}_{p,i-1},\\bar{w}_{p,i}-\\epsilon_{p,i}\\bigr\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\big(\\bar{w}_{p,i}+\\epsilon_{p,i}\\big)-\\big(\\bar{w}_{p,i}-\\epsilon_{p,i}\\big)=2\\epsilon_{p,i}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "ctxtY3VGGq/tmp/401fce1a12bac48d43898fe7c17f58ff71ebc333b0e4f70516cda5e8912bf894.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Thus, it remains to show that $\\begin{array}{r}{\\frac{1}{2n^{2}T}\\,\\cdot\\,w_{p}^{1}\\,\\leq\\,w_{p}}\\end{array}$ and that $|w_{p}-\\bar{w}_{p,i}|\\,\\leq\\,\\epsilon_{p,i}$ for every page $p$ and $i\\in[1,T]$ wi th\ud835\udc5d pro\ud835\udc43bability $\\textstyle1-{\\frac{1}{n T}}$ . For the first event, by Markov inequality and a union bound over all the pages , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg[\\exists p\\in P.\\;\\frac{1}{2n^{2}T}\\cdot w_{p}^{1}>w_{p}\\bigg]=\\mathbb{P}[\\exists p\\in P.\\;w_{p}^{1}>2n^{2}T\\cdot w_{p}]\\,\\le n\\cdot\\frac{1}{2n^{2}T}=\\frac{1}{2n T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the second event, by Hoeffding inequality and a union bound over all the pages $p\\in P$ , all the time steps $t\\in[1,T]$ and all the possible number of samples for each page $i\\in[1,n T]$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big[\\exists p\\in P,i\\in[1,T].\\,|w_{p}-\\bar{w}_{p,i}|>\\epsilon_{p,i}\\big]\\le n^{2}T^{2}\\cdot2e^{-2i\\epsilon_{p,i}^{2}}=n^{2}T^{2}\\cdot\\frac{1}{2n^{3}T^{3}}=\\frac{1}{2n T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof is now finished by taking a union bound over these two events. ", "page_idx": 18}, {"type": "text", "text": "Define $\\begin{array}{r}{\\mathcal{U}:=\\sum_{p\\in P}\\sum_{i=1}^{n_{p}}\\bigl(\\mathbf{UCB}_{p,i}-\\mathbf{LCB}_{p,i}\\bigr)+2\\log\\bigl(1+1/\\eta\\bigr)\\sum_{p\\in P}\\mathbf{LCB}_{p,i}}\\end{array}$ the regret term used in Lemma 4.1. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. Under the good event of Lemma C.1, it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{U}\\leq8\\sqrt{n T}\\log(n T)=\\tilde{O}(\\sqrt{n T}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The following holds under the good event. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{U}=\\sum_{p\\in\\mathcal{P}_{r}^{\\star}=1}^{P}(\\mathrm{UCB}_{p,i}-\\mathrm{LCB}_{p,i})+2\\mathrm{log}(1+1/\\eta)\\sum_{p\\in\\mathcal{P}_{r}^{\\star}}\\mathrm{LCB}_{p,i}}}\\\\ &{\\lesssim2\\sum_{p\\in\\mathcal{P}_{r}^{\\star}=1}^{P}\\zeta_{p,i}+2\\mathrm{log}(1+1/\\eta)\\sum_{p\\in\\mathcal{P}_{r}^{\\star}}w_{p}}\\\\ &{\\stackrel{p\\in\\mathcal{P}_{r}^{\\star}=1}{=}\\sum_{p\\in\\mathcal{P}_{r}^{\\star}=1}^{P}\\sqrt{\\log(4\\pi^{\\gamma}/\\eta^{\\star})}}\\\\ &{\\lesssim2\\sum_{p\\in\\mathcal{P}_{r}^{\\star}=1}^{P}\\sqrt{\\frac{\\log(4\\pi^{\\gamma}/\\eta^{\\star})}{2}}+2\\log(1+1/\\eta)\\sum_{p\\in\\mathcal{P}_{r}^{\\star}}w_{p}}\\\\ &{\\lesssim\\sqrt{2\\log(4\\pi^{\\gamma}/\\eta^{\\star})}\\sum_{p\\in\\mathcal{P}_{r}^{\\star}=1}^{P}\\frac{1}{\\sqrt{d}}+2n\\log(1+1/\\eta)}\\\\ &{\\leq2\\sqrt{\\log(4\\pi^{\\gamma})}\\sum_{p\\in\\mathcal{P}_{r}^{\\star}=1}^{P}\\sqrt{w_{p}}+2n\\log(1+1/\\eta)}\\\\ &{\\leq2\\sqrt{2\\log(4\\pi^{\\gamma})}\\sum_{p\\in\\mathcal{P}_{r}^{\\star}=1}^{P}\\Big|_{\\mathcal{P}_{r}^{\\star}=1/\\eta}}\\\\ &{\\leq2\\sqrt{2n\\Gamma\\log(4\\pi^{\\gamma}/\\eta^{\\star})}+2n\\log(1+\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\textstyle\\sum_{i=1}^{t}{\\frac{1}{\\sqrt{i}}}\\leq2{\\sqrt{t}})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality holds by Cauchy\u221a\u2013Schwarz and our choice of $\\eta=1/k$ . Lastly, the stated upper bound follows since $k\\leq n$ and $n\\leq{\\sqrt{n T}}$ . \u25a1 ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We provide an algorithm for the problem of online weighted paging with unknown weights, and analyse it. See Section 1.1 for a summary of our results, that will be proven throughout the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This is a theory paper that present an algorithm for the online weighted paging where the weight are unknown and prove the stated bounds. We do not believe there are additional limitations to discuss. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: All assumptions are stated in the statements of the theorem and related lemmas.   \nA proof is provided for each theoretical claim. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: No additional information is required to obtain the results in this paper, which are theoretical. A full pseudocode for the algorithm appears in the paper and appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not contain experiments at all. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not contain experiments at all. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: There are no experiments in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There are no experiments in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We reviewed the code of ethics and concluded the paper conforms to it. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper describes an algorithm for the online weighted paging problem, where the weights are unknown, and analyse it. The paper is theoretical. Thus, we do not predict any societal impact due to this work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper is theoretical. No data or models are associated with it. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: This paper is theoretical. No existing assets are associated with it. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper is theoretical. No new assets are released in it. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper is theoretical. It does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper is theoretical. It does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 24}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]