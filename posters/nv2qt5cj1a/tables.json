[{"figure_path": "nv2Qt5cj1a/tables/tables_3_1.jpg", "caption": "Table 1: Overview of VL-MIA dataset: VL-MIA covers image and text modalities and can be applied for dominant open-sourced VLLMs.", "description": "This table summarizes the Vision Language Membership Inference Attack (VL-MIA) dataset.  It shows the modality (image or text), the source of the member data (data used to train popular VLLMs), the source of the non-member data (data not used in training), and which VLLMs the dataset is designed to be used with.  The goal of VL-MIA is to provide a benchmark for evaluating Membership Inference Attacks against Vision-Language Models.", "section": "4 Dataset construction"}, {"figure_path": "nv2Qt5cj1a/tables/tables_6_1.jpg", "caption": "Table 3: Text MIA. AUC results on LLaVA.", "description": "This table presents the Area Under the Curve (AUC) scores for various membership inference attack (MIA) methods on the LLaVA model.  It compares different metrics (Perplexity, Min-K% probability, Max-Prob-Gap, and ModR\u00e9nyi) across different text lengths (32 and 64 tokens).  It also differentiates between results obtained during Language Model (LLM) pre-training and during VLLM instruction tuning, indicating whether the training data was used in the base LLM's pre-training or the subsequent instruction tuning phase. This shows the effectiveness of each MIA metric and the influence of the training data's usage stage.", "section": "6.3 Text MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_7_1.jpg", "caption": "Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \"img\" indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\" is the concatenation of the instruction slice and description slice. We use an asterisk * in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up.", "description": "This table presents the results of membership inference attacks (MIAs) on image data using different metrics and VLLMs (MiniGPT-4, LLaMA Adapter v2.1, and LLaVA 1.5).  It shows the Area Under the Curve (AUC) scores for various methods, including perplexity, minimum probability, maximum probability gap, and MaxR\u00e9nyi, broken down by the type of logits slice used (image, instruction, description, or combined instruction and description).  Target-based metrics are marked with an asterisk. The best and second-best AUC scores are highlighted in bold and underlined, respectively. The results demonstrate the effectiveness of the proposed MaxR\u00e9nyi metric and cross-modal pipeline for image MIAs.", "section": "6.2 Image MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_8_1.jpg", "caption": "Table 4: Image MIA on GPT-4.", "description": "This table presents the results of membership inference attacks (MIAs) performed on the closed-source model GPT-4, using two image datasets: VL-MIA/Flickr and VL-MIA/DALL-E.  The table shows AUC scores for various MIA methods, including MaxR\u00e9nyi-K% with different \u03b1 values and K percentages, as well as baselines such as Perplexity/zlib and Max_Prob_Gap.  The results demonstrate the effectiveness of the MIAs on GPT-4, highlighting the potential risks of privacy leakage even with closed-source models. ", "section": "6.4 Image MIA on GPT-4"}, {"figure_path": "nv2Qt5cj1a/tables/tables_14_1.jpg", "caption": "Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \"img\" indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\u201d is the concatenation of the instruction slice and description slice. We use an asterisk * in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up.", "description": "This table presents the results of Membership Inference Attacks (MIAs) on images, using different metrics and focusing on various logits slices (image, instruction, description).  It compares the Area Under the Curve (AUC) scores of different MIA methods for two datasets (VL-MIA/Flickr and VL-MIA/DALL-E) and three vision-language models (LLaVA, MiniGPT-4, and LLAMA Adapter). The asterisk (*) denotes target-based metrics, bold indicates the highest AUC, and underlined values represent the second-highest AUC within each column.", "section": "6.2 Image MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_15_1.jpg", "caption": "Table 6: Different prompts we use for dataset construction.", "description": "This table shows the prompts used to construct the VL-MIA dataset.  Specifically, it details the prompts used for generating images with DALL-E 2 and text with GPT-4-vision-preview, categorized by the dataset (VL-MIA/DALL-E, VL-MIA/Text for MiniGPT-4, VL-MIA/Text for LLaVA 1.5) and model used for generation. The prompts are designed to elicit responses relevant to the training data for each corresponding VLLM.", "section": "A.3 Datasets construction prompts"}, {"figure_path": "nv2Qt5cj1a/tables/tables_16_1.jpg", "caption": "Table 7: Model details used in this work.", "description": "This table lists the versions, base models, and training datasets of the three VLLMs used in the paper's experiments: MiniGPT-4, LLaVA 1.5, and LLAMA Adapter v2.1.  It details the base large language model (LLM), the vision processor used, the datasets used for image-text pre-training, and the datasets used for instruction tuning.", "section": "Additional experimental information"}, {"figure_path": "nv2Qt5cj1a/tables/tables_16_2.jpg", "caption": "Table 8: Values for corruption parameter c in ImageNet-C code.", "description": "This table shows the values used for the corruption parameter 'c' in the ImageNet-C code.  The parameter controls the level of corruption applied to images during the experiments.  The rows represent the severity levels (Marginal, Moderate, Severe), and the columns represent different types of corruption (Brightness, Motion_Blur, Snow, JPEG). Each cell contains the specific value used for that corruption type and severity level.", "section": "A.5 Parameters for image corruption"}, {"figure_path": "nv2Qt5cj1a/tables/tables_17_1.jpg", "caption": "Table 3: Text MIA. AUC results on LLaVA.", "description": "This table presents the Area Under the Curve (AUC) scores for various membership inference attack (MIA) methods on the LLaVA model.  The methods are evaluated on two different datasets representing pre-training and instruction-tuning stages of the VLLM.  Different text lengths (32, 64, 128, and 256 tokens) are used.  The table compares various metric-based MIA methods, including perplexity-based methods, minimum probability based methods, and R\u00e9nyi entropy-based methods (MaxR\u00e9nyi).  The results show the effectiveness of each method for detecting whether a given text sequence was part of the model's training data.", "section": "6.3 Text MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_17_2.jpg", "caption": "Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \"img\" indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\" is the concatenation of the instruction slice and description slice. We use an asterisk * in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up.", "description": "This table presents the Area Under the Curve (AUC) scores for various membership inference attack (MIA) methods on two image datasets (VL-MIA/Flickr and VL-MIA/DALL-E) using three different vision-language models (VLLMs).  The AUC scores are broken down by the type of input used for the MIA (image, instruction, description, or a combination of instruction and description) and the specific MIA method employed. Target-based metrics (indicated by an asterisk) are compared against target-free metrics (MaxR\u00e9nyi and R\u00e9nyi with various \u03b1 values). The table helps to assess the effectiveness of different MIA methods on images and the influence of various \u03b1 parameters in MaxR\u00e9nyi on the performance.", "section": "6.2 Image MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_18_1.jpg", "caption": "Table 11: MIA on LLM pre-training texts. We evaluate on WikiMIA of lengths (32, 64, 128, 256) on LLaVA. We compare MaxR\u00e9nyi with MinR\u00e9nyi.", "description": "This table presents the results of Membership Inference Attacks (MIAs) on Large Language Model (LLM) pre-training texts using two different metrics: MaxR\u00e9nyi and MinR\u00e9nyi.  The WikiMIA benchmark dataset, with text sequences of varying lengths (32, 64, 128, 256), is used.  The table shows the Area Under the Curve (AUC) scores for different variations of the MaxR\u00e9nyi and MinR\u00e9nyi metrics (with K values of 0%, 10%, and 20%), and different R\u00e9nyi entropy orders (\u03b1 = 0.5, 1, 2, \u221e).  The results help to compare the performance of these two MIA methods in detecting whether a text sequence was part of the LLM's pre-training data.", "section": "B.2 Abalation on MaxR\u00e9nyi and MinR\u00e9nyi"}, {"figure_path": "nv2Qt5cj1a/tables/tables_19_1.jpg", "caption": "Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \"img\" indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\u201d is the concatenation of the instruction slice and description slice. We use an asterisk * in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up.", "description": "This table presents the Area Under the Curve (AUC) scores for various membership inference attack (MIA) methods applied to image data from the VL-MIA benchmark.  Different slices of the model's output logits (image, instruction, description, and combined instruction+description) are evaluated.  Both target-based and target-free MIA metrics are included, and the results are broken down by model (LLaVA, MiniGPT-4, LLAMA Adapter) and metric (MaxR\u00e9nyi, R\u00e9nyi with different alpha values, etc.).  The best performing methods for each scenario are highlighted. ", "section": "6.2 Image MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_19_2.jpg", "caption": "Table 13: Text MIA. AUC results on LLaVA with 1000 members and 1000 non-members. We detect the text used in VLLM instruction tuning stage for text length equals to [32, 64].", "description": "This table presents the results of membership inference attacks (MIAs) on text data using the LLaVA model.  It compares the performance of several MIA methods (Perplexity, Max-Prob-Gap, ModR\u00e9nyi, and R\u00e9nyi with different alpha values and K percentages) in detecting whether a text sequence was part of the model's instruction-tuning training data. The experiment uses an extended dataset with 1000 member and 1000 non-member data points and text sequences of lengths 32 and 64 tokens.", "section": "6.3 Text MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_20_1.jpg", "caption": "Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \"img\" indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\" is the concatenation of the instruction slice and description slice. We use an asterisk * in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up.", "description": "This table presents the Area Under the Curve (AUC) scores for various membership inference attack (MIA) methods on two image datasets (VL-MIA/Flickr and VL-MIA/DALL-E) using three different vision-language models (VLLMs).  The methods tested include several baselines and the proposed MaxR\u00e9nyi-K% metric. The table shows the AUC scores for different slices of the model's output logits (image, instruction, description, and combined instruction and description), demonstrating the performance of each method in detecting whether an image was part of the training data. Target-based methods use the next token as the target for prediction, while target-free methods use all the tokens. The results highlight the superior performance of the MaxR\u00e9nyi-K% metric, especially with a value of \u03b1 = 0.5,  across various scenarios and VLLMs.", "section": "6.2 Image MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_21_1.jpg", "caption": "Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \"img\" indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\u201d is the concatenation of the instruction slice and description slice. We use an asterisk * in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up.", "description": "This table presents the Area Under the Curve (AUC) scores for various membership inference attack (MIA) methods on two image datasets (VL-MIA/Flickr and VL-MIA/DALL-E) using three different vision-language models (VLLMs). The methods include various target-based and target-free MIA metrics. The table shows performance across different slices of VLLM output logits (image, instruction, description, and combined instruction and description) for each VLLM and metric.", "section": "6.2 Image MIA"}, {"figure_path": "nv2Qt5cj1a/tables/tables_22_1.jpg", "caption": "Table 1: Overview of VL-MIA dataset: VL-MIA covers image and text modalities and can be applied for dominant open-sourced VLLMs.", "description": "This table provides an overview of the VL-MIA dataset, highlighting its key features.  It shows that the dataset includes both image and text modalities, making it suitable for evaluating membership inference attacks (MIAs) on various open-source large vision-language models (VLLMs). The table specifies the modality (image or text), the member data source, the non-member data source, and the intended application (which VLLM the data is designed for).  This information is crucial for understanding the scope and design of the VL-MIA benchmark presented in the paper.", "section": "4 Dataset construction"}, {"figure_path": "nv2Qt5cj1a/tables/tables_23_1.jpg", "caption": "Table 2: Image MIA. AUC results on VL-MIA under our pipeline. \"img\" indicates the logits slice corresponding to image embedding, \u201cinst\u201d indicates the instruction slice, \u201cdesp\u201d the generated description slice, and \u201cinst+desp\" is the concatenation of the instruction slice and description slice. We use an asterisk * in superscript to indicate the target-based metric. Bold indicates the best AUC within each column and underline indicates the runner-up.", "description": "This table presents the Area Under the Curve (AUC) scores for various membership inference attack (MIA) methods applied to image data from the VL-MIA benchmark.  Different metrics are used, including perplexity and variations of R\u00e9nyi entropy. The results are broken down by the type of VLLM model, the portion of the model's output logits used (image, instruction, description, or a combination), and whether the MIA method is target-based or target-free. The table highlights the superior performance of the MaxR\u00e9nyi-K% metric, especially in target-free scenarios, showcasing its effectiveness across different VLLM architectures.", "section": "6.2 Image MIA"}]