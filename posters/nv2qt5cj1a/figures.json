[{"figure_path": "nv2Qt5cj1a/figures/figures_1_1.jpg", "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi-K% metric: we first get the R\u00e9nyi entropy of each token position, then select the largest k% tokens and calculate the average R\u00e9nyi entropy.", "description": "This figure illustrates the Membership Inference Attacks (MIA) against large vision-language models (VLLMs).  The top half shows the image detection pipeline, detailing the generation and inference stages.  In the generation stage, an image and instruction are fed to the VLLM to produce a description. In the inference stage, the image, instruction, and generated description are fed back into the model, and logit slices are extracted for metric calculation. The bottom half details the MaxR\u00e9nyi-K% metric used to quantify the confidence of the model's output, providing a measure for both text and image data. The metric involves calculating the R\u00e9nyi entropy for each token position, selecting the largest K% entropies, and averaging them to determine membership. A higher average suggests higher confidence and therefore a higher probability of membership in the training set.", "section": "1 Introduction"}, {"figure_path": "nv2Qt5cj1a/figures/figures_9_1.jpg", "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi-K% metric: we first get the R\u00e9nyi entropy of each token position, then select the largest k% tokens and calculate the average R\u00e9nyi entropy.", "description": "This figure illustrates the proposed membership inference attack (MIA) pipeline against large vision-language models (VLLMs).  The top half shows the image detection pipeline, consisting of a generation stage (where an image and instruction are fed to the model to generate a description) and an inference stage (where the image, instruction, and generated description are fed to the model to extract logits for metric calculation). The bottom half details the MaxR\u00e9nyi-K% metric used to quantify the confidence of the model output for both text and image data. This metric calculates the R\u00e9nyi entropy for each token position, selects the top K% of these positions with highest entropy and averages their entropy to determine membership.", "section": "Method"}, {"figure_path": "nv2Qt5cj1a/figures/figures_9_2.jpg", "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi-K% metric: we first get the R\u00e9nyi entropy of each token position, then select the largest k% tokens and calculate the average R\u00e9nyi entropy.", "description": "This figure illustrates the proposed membership inference attack (MIA) pipeline against large vision-language models (VLLMs).  The top half shows the two-stage pipeline: a generation stage where an image and instruction are fed to the VLLM to produce a description, followed by an inference stage where the image, instruction, and generated description are used to extract logits for metric calculation. The bottom half details the MaxR\u00e9nyi-K% metric used to quantify the confidence of the model's output for both text and image data, focusing on the R\u00e9nyi entropy of token positions.  This metric helps determine if a specific data point is part of the VLLM's training data.", "section": "5 Method"}, {"figure_path": "nv2Qt5cj1a/figures/figures_14_1.jpg", "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi-K% metric: we first get the R\u00e9nyi entropy of each token position, then select the largest k% tokens and calculate the average R\u00e9nyi entropy.", "description": "This figure illustrates the Membership Inference Attack (MIA) pipeline against large vision-language models (VLLMs).  The top half shows the image detection pipeline, which involves a generation stage (feeding the image and an instruction to the model to generate a description) and an inference stage (feeding the generated description, image and instruction to the model to extract logits for metric calculation).  The bottom half details the MaxR\u00e9nyi-K% metric, a novel metric for evaluating MIAs. This metric calculates the R\u00e9nyi entropy for each token position in the model's output, selects the top K% positions with the highest R\u00e9nyi entropy, and then averages these entropies to obtain a final score.", "section": "1 Introduction"}, {"figure_path": "nv2Qt5cj1a/figures/figures_15_1.jpg", "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi-K% metric: we first get the R\u00e9nyi entropy of each token position, then select the largest k% tokens and calculate the average R\u00e9nyi entropy.", "description": "This figure illustrates the Membership Inference Attacks (MIA) against large vision-language models (VLLMs). The top part shows the image detection pipeline, which consists of two stages: generation and inference.  In the generation stage, an image and instruction are fed into the VLLM to generate a description. The inference stage uses the image, instruction, and generated description to extract logits slices for metric calculations. The bottom part details the MaxR\u00e9nyi-K% metric used to assess the confidence of the model's output for both text and image data. It involves calculating the R\u00e9nyi entropy for each token position, selecting the largest k%, and then averaging the R\u00e9nyi entropy.", "section": "1 Introduction"}, {"figure_path": "nv2Qt5cj1a/figures/figures_23_1.jpg", "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi-K% metric: we first get the R\u00e9nyi entropy of each token position, then select the largest k% tokens and calculate the average R\u00e9nyi entropy.", "description": "This figure illustrates the proposed membership inference attack (MIA) pipeline against large vision-language models (VLLMs). The top half depicts the two-stage process: a generation stage where an image and instruction are fed to the VLLM to produce a description, and an inference stage where the image, instruction, and generated description are used to extract logit slices for metric calculation.  The bottom half details the MaxR\u00e9nyi-K% metric, showing how R\u00e9nyi entropy is calculated for each token position, the top k% positions are selected, and then the average R\u00e9nyi entropy is computed.", "section": "1 Introduction"}, {"figure_path": "nv2Qt5cj1a/figures/figures_23_2.jpg", "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR\u00e9nyi-K% metric: we first get the R\u00e9nyi entropy of each token position, then select the largest k% tokens and calculate the average R\u00e9nyi entropy.", "description": "This figure illustrates the proposed membership inference attack (MIA) pipeline against large vision-language models (VLLMs).  The top half shows a diagram of the image detection pipeline, detailing the generation and inference stages.  The generation stage involves inputting an image and instruction to the VLLM to generate a description.  The inference stage uses the image, instruction, and generated description to extract logit slices for metric calculation. The bottom half explains the MaxR\u00e9nyi-K% metric used to quantify the confidence of the model's output for both text and image data, focusing on R\u00e9nyi entropy calculations for token positions.", "section": "1 Introduction"}]