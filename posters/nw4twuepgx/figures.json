[{"figure_path": "nw4TWuEPGx/figures/figures_1_1.jpg", "caption": "Figure 1: Meta-learning approach to discovering plasticity rules that organize sequences (a) In zebra finch song learning, a neural representation of time (left) in HVC simplifies the sequential motor learning task of producing the correct spectral output. (b) Putative network structure of zebra finch HVC: a feed-forward, excitatory network with recurrent inhibition (left). HVC excitatory neurons (red) fire sparsely in time while interneurons (blue) fire tonically (right). (c) Strategy for learning plasticity underlying sequence organization: candidate plasticity rules, parameterized by a set of coefficients and time constants, are simulated. A loss function is evaluated on the resulting dynamics, and new candidate rules are generated. (d) Test procedure for representation of time. Networks are activated 400 times (blue bars). From the final 50 activations, 6 are chosen to train a decoder and 6 to test the representation by decoding time from neural activity. (e) The robustness of discovered plasticity rules can be tested by introducing synaptic turnover, in which synapses are stochastically removed and added, into simulations.", "description": "This figure illustrates the meta-learning approach used in the paper to discover plasticity rules that organize and maintain sequential neural activity.  Panel (a) shows the context of the zebra finch HVC circuit in song learning. Panel (b) depicts the network structure used in the simulations, including excitatory and inhibitory neurons. Panel (c) outlines the meta-learning process: simulating candidate plasticity rules, evaluating a loss function, and adjusting the rules to minimize the loss. Panel (d) shows how the time representation is tested by training and testing a decoder on neural activity. Panel (e) demonstrates how synaptic turnover (stochastic removal and addition of synapses) is introduced to test the robustness of the learned plasticity rules.", "section": "Results"}, {"figure_path": "nw4TWuEPGx/figures/figures_3_1.jpg", "caption": "Figure 2: Meta-learning discovers unsupervised local plasticity rules that organize sequential activity (a) Evolution of coefficients and time constants during meta-learning. (b) Training loss (black) and test loss (blue) during meta-learning. Inset: median decoding accuracy for 14 learned rules across 100 networks (blue points) compared to no plasticity (dashed gray line). (c) Meta-learned plasticity rules generate sequence dynamics. Network activity during the first activation (left), 200th (middle), and 400th (right), sorted by ordering of mean firing time of the final activation (425th). Note: plasticity rule is held fixed during simulation. (d) Weight matrices at activation 1 and 400, sorted based on mean firing time of the final activation. Connectivity between E cells organizes into a feedforward structure.", "description": "This figure shows the results of meta-learning plasticity rules for organizing sequential activity. Panel (a) displays the evolution of rule coefficients and time constants during the learning process. Panel (b) shows the training and testing loss curves, indicating successful learning. Panel (c) visualizes the network activity at different time points, demonstrating the emergence of sequential dynamics. Finally, panel (d) illustrates the weight matrices, revealing a feedforward structure in the network connectivity.", "section": "2 Results"}, {"figure_path": "nw4TWuEPGx/figures/figures_4_1.jpg", "caption": "Figure 3: Perturbing learned plasticity rules reveals dependence on temporally asymmetric Hebbian learning and a postsynaptic activity bound (a) Distributions of coefficients (blue) and time constants (purple) of basis terms across N=14 training instances (individual instances in black). The basis set consists of functions of pre and post-synaptic neural activity and synaptic weights. x = activity of pre-synaptic neuron; y = activity of post-synaptic neuron; w = weight of synapse; Xint = X (x filtered with 7). (b) Performance loss when individual coefficients are set to zero. (c) Difference in median loss between full learned solutions run on 100 test networks and solutions with one term dropped. Separate column denotes median loss for terms across all trials. (d) Progressive refitting of model in order of impact on median loss with and without term. Symbols indicate the term added at each refitting. Losses for 100 test networks shown (blue); medians shown in black.", "description": "This figure explores the impact of different terms in the learned plasticity rule on the performance of the model. Panel (a) shows the distribution of coefficients and time constants for each term across multiple training instances. Panel (b) shows the loss when individual coefficients are set to zero, indicating the importance of each term. Panel (c) shows the difference in median loss between the full model and models with one term removed, further highlighting the importance of specific terms. Panel (d) shows the progressive refitting of the model, adding terms one by one in order of their impact on the loss, to demonstrate how the key terms contribute to the model's performance.", "section": "Results"}, {"figure_path": "nw4TWuEPGx/figures/figures_6_1.jpg", "caption": "Figure 3: Perturbing learned plasticity rules reveals dependence on temporally asymmetric Hebbian learning and a postsynaptic activity bound (a) Distributions of coefficients (blue) and time constants (purple) of basis terms across N=14 training instances (individual instances in black). The basis set consists of functions of pre and post-synaptic neural activity and synaptic weights. x = activity of pre-synaptic neuron; y = activity of post-synaptic neuron; w = weight of synapse; Xint = X(x filtered with 7). (b) Performance loss when individual coefficients are set to zero. (c) Difference in median loss between full learned solutions run on 100 test networks and solutions with one term dropped. Separate column denotes median loss for terms across all trials. (d) Progressive refitting of model in order of impact on median loss with and without term. Symbols indicate the term added at each refitting. Losses for 100 test networks shown (blue); medians shown in black.", "description": "This figure analyzes the learned plasticity rules by systematically perturbing them.  Panel (a) shows the distribution of coefficients and time constants across different training instances. Panel (b) assesses the impact of setting individual coefficients to zero on the performance loss.  Panel (c) compares the median loss of full models with models where one term is removed. Finally, panel (d) illustrates the iterative process of adding terms back to the model based on their impact on the loss, highlighting the most crucial components of the learned rule.", "section": "2 Results"}, {"figure_path": "nw4TWuEPGx/figures/figures_7_1.jpg", "caption": "Figure 5: Learning plasticity on all synapses (a) Schematic of synapses upon which we newly allow plasticity. (b) Weight matrices on activations 1 and 400 of a network evolving under plasticity rules learned on all 3 sets of synapses. I synapses increase. (c) Comparison of performance of rules learned only on E\u2192E synapses (red) (N=5) versus all sets of synapses (blue) (N=8) when training includes synaptic turnover across varying rates of synaptic turnover.", "description": "This figure shows the results of learning plasticity rules on all three types of synapses (E\u2192E, E\u2192I, and I\u2192E) in a network. Panel (a) illustrates the network structure with the three synapse types. Panel (b) displays weight matrices at activations 1 and 400, showing that inhibitory synapse weights increase over time. Panel (c) compares the performance of rules learned with plasticity only on excitatory synapses (E\u2192E) versus rules learned with plasticity on all three synapse types. The comparison is made across various rates of synaptic turnover, revealing that the inclusion of inhibitory plasticity enhances network performance.", "section": "2.3 Including inhibitory plasticity"}, {"figure_path": "nw4TWuEPGx/figures/figures_7_2.jpg", "caption": "Figure 6: Network perturbations reveal homeostatic compensation in E\u2192E and I\u2192E synapses (a) Response of the magnitude of summed E\u2192E weights |Wexc| = |\u03a3k wi \u2192E | (top) and summed recurrent weight |Wiec| = | \u03a3\u03ba\u03c9 WE IwE (bottom) to the imposed scaling down of E\u2192 E weights to a single cell. Values for neuron with perturbed inputs shown in light purple; all others black. Each line represents average values for a single learned rule over N=20 networks. (b) Same as (a), but for imposed upscaling of E\u2192 E weights to one cell (light purple). (c) Same as (a), but frequency of stochastic inputs to one cell is greatly increased (light purple) relative to all other cells (black). (d-f) Postsynaptic response of affected neurons in 20 networks plotted in the space of normalized peak amplitude and duration for manipulations (a-c). Black dots represent pre-perturbation responses; light purple, immediately after perturbation; blue, 25 activations after perturbation. (g) Schematic of feed-forward motif maintaining homeostasis via its E input. (h) Schematic of feed-forward motif with homeostasis on E\u2192E and I\u2192E synapses. (i) Phase flow in space of output firing envelope duration and amplitude (left) for a single neuron. Black points are first responses of the neuron for various inputs; blue stars represent responses after 200 activations. Note: initial and final output durations and delays are roughly the same. Colors of trajectories indicate how the neuron's input weight changes to satisfy the plasticity rule: see legend. (j) If the forms of homeostasis on E\u2192E and I\u2192E maintain different aspects of the postsynaptic response, an attractor forms in the duration and peak amplitude of the postsynaptic response (left). The response delay is also constrained (right).", "description": "This figure displays the results of network perturbation experiments, showing how homeostatic mechanisms maintain network dynamics under different manipulations. The figure illustrates that plasticity rules on excitatory and inhibitory synapses work together to control different aspects of the postsynaptic responses, ensuring robust performance despite disruption.", "section": "2.3 Including inhibitory plasticity"}]