[{"type": "text", "text": "An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaochuan Gong Jie Hao Mingrui Liu\\* Department of Computer Science George Mason University {xgong2, jhao6, mingruil}@gmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper investigates a class of stochastic bilevel optimization problems where the upper-level function is nonconvex with potentially unbounded smoothness and the lower-level problem is strongly convex. These problems have significant applications in sequential data learning, such as text classification using recurrent neural networks. The unbounded smoothness is characterized by the smoothness constant of the upper-level function scaling linearly with the gradient norm, lacking a uniform upper bound. Existing state-of-the-art algorithms require $\\widetilde{\\cal O}(\\epsilon^{-4})$ oracle calls of stochastic gradient or Hessian/Jacobian-vector product to find an $\\epsilon$ -stationary point. However, it remains unclear if we can further improve the convergence rate when the assumptions for the function in the population level also hold for each random realization almost surely (e.g., Lipschitzness of each realization of the stochastic gradient). To address this issue, we propose a new Accelerated Bilevel Optimization algorithm named AccBO. The algorithm updates the upper-level variable by normalized stochastic gradient descent with recursive momentum and the lower-level variable by the stochastic Nesterov accelerated gradient descent algorithm with averaging. We prove that our algorithm achieves an oracle complexity of $\\widetilde{\\cal O}(\\epsilon^{-3})$ to find an $\\epsilon$ -stationary point. Our proof relies on a novel lemma characterizing the dynamics of stochastic Nesterov accelerated gradient descent algorithm under distribution drift with high probability for the lower-level variable, which is of independent interest and also plays a crucial role in analyzing the hypergradient estimation error over time. Experimental results on various tasks confirm that our proposed algorithm achieves the predicted theoretical acceleration and significantly outperforms baselines in bilevel optimization. The code is available here. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel optimization receives tremendous attention recently in the machine learning community, due to its applications in meta-learning [25, 54], hyperparameter optimization [25, 23], data hypercleaning [39], continual learning [6, 34], and reinforcement learning [42]. The bilevel optimization problem has the following formulation: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb R^{d_{x}}}\\Phi(x):=f(x,y^{*}(x)),\\ \\mathrm{{s.t.,}}\\ y^{*}(x)\\in\\arg\\operatorname*{min}_{y\\in\\mathbb R^{d_{y}}}g(x,y),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f$ and $g$ are upper-level and lower-level functions respectively. For example, in metalearning [24, 25], $x$ denotes the layers of neural networks for shared representation learning, $y$ denotes the task-specific head encoded in the last layer, and the formulation (1) aims to learn the a common representation learning encoder $x$ such that it can be quickly adapted to downstream tasks by only updating the task-specific head $y$ . In machine learning, people typically consider stochastic optimization setting such that $\\boldsymbol{f}(\\boldsymbol{x},\\boldsymbol{y})=\\mathbb{E}_{\\boldsymbol{\\xi}\\sim\\mathcal{D}_{f}}\\left[F(\\boldsymbol{x},\\boldsymbol{y};\\boldsymbol{\\xi})\\right]$ and $\\bar{g}(x,\\bar{y})=\\bar{\\mathbb{E}}_{\\zeta\\sim\\mathcal{D}_{g}}\\left[G(x,y;\\zeta)\\right]$ where $\\mathcal{D}_{f}$ and $\\mathcal{D}_{g}$ are the underlying unknown data distributions for $f$ and $g$ respectively, and one can acess noisyobserations of $f$ and $g$ based on sampling from $\\mathcal{D}_{f}$ and $\\mathcal{D}_{g}$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "There emerges a wave of studies for algorithmic design and analysis for solving the bilevel optimization problem (1) under different assumptions of $f$ and $g$ . Most theoretical work assumes the upper-level function is smooth (i.e., gradient is Lipschitz) and nonconvex, and the lower-level function is strongly convex [28, 39, 38, 31, 43]. However, as pointed out by [70, 14], certain neural networks such as recurrent neural networks [20], long-short term memory networks [37] and transformers [60] have smoothness constants that scale with gradient norm, potentially leading to unbounded smoothness constants (i.e., gradient Lipschitz constant can be infinity). Motivated by this, Hao et al. [35] designed the first bilevel optimization algorithm to handle the cases where $f$ is nonconvex with potentially unbounded smoothness and $g$ is strongly convex. The algorithm in [35] achieves $\\widetilde{\\cal O}(\\epsilon^{-4})$ oracle complexity for finding an $\\epsilon$ -stationary point (i.e., a point $x$ such that $\\|\\nabla\\Phi(x)\\|\\leq\\epsilon)$ . Gong et al. [30] proposed an single-loop algorithm under the same setting as in [35] and also achieved $\\widetilde{\\cal O}(\\epsilon^{-4})$ oracle complexity. This complexity result is worse than the $\\tilde{\\tilde{O}}(\\epsilon^{-3})$ oracle complexity under the relatively easier setting where $f$ has a Lipschitz gradient, and each realization of the stochastic oracle calls is Lipschitz with respect to its argument (e.g., almost-sure Lipschitz oracle) [66, 41, 17]. This naturally motivates us to study the following question: ", "page_idx": 1}, {"type": "text", "text": "Is it possible to improve the $\\widetilde{\\cal O}(\\epsilon^{-4})$ oracle complexity for bilevel optimization problems where the upper-level function is nonconvex with unbounded smoothness and the lower-level function is strongly convex, by assuming that the properties of the function at the population level also hold almost surely for each random realization? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we give a positive answer to this question by designing a new algorithm named AccBO with an improved oracle complexity of $\\widetilde{\\cal O}(\\epsilon^{-3})$ . Our algorithm is inspired by momentum-based variance reduction techniques used in nonconvex smooth optimization [17] under the almost-sure Lipschitz stochastic gradient oracle framework. The innovation of AccBO lies in its update rules: it employs normalized stochastic gradient descent with recursive momentum for the upper-level variable and stochastic Nesterov accelerated gradient descent with averaging for the lower-level variable. Our approach differs from existing accelerated bilevel optimization algorithms, such as those proposed by [66, 41] in two key ways: (i) while these algorithms use recursive momentum for the upperlevel variable update, AccBO utilizes normalized recursive momentum to address the unbounded smoothness of the upper-level function; (i) for the lower-level variable update, we use stochastic Nesterov accelerated gradient descent with averaging, in contrast to the recursive momentum method used by the other algorithms. The primary challenge in analyzing the convergence rate of AccBO arises from the need to simultaneously control errors from both upper-level and lower-level variables, given the unbounded smoothness, large learning rate, and recursive momentum in the upper-level problem. Our main contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We design a new algorithm named AccBO for solving bilevel optimization problems where the upper-level function is nonconvex with unbounded smoothness and the lower-level function is strongly convex. AccBO leverages normalized recursive momentum for the upperlevel variable and Nesterov momentum for the lower-level variable under the stochastic setting to achieve acceleration. To the best of our knowledge, the simultaneous usage of these two techniques in stochastic bilevel optimization is novel and has not been previously explored in the bilevel optimization literature.   \n\u00b7 We prove that the AccBO algorithm requires $\\widetilde{\\cal O}(\\epsilon^{-3})$ oracle calls for finding an $\\epsilon$ stationary point. This complexity strictly improves the state-of-the-art oracle complexity for unbounded smooth nonconvex upper-level problem and strongly-convex lower-level problem as described in [35, 30]. To achieve this result, we introduce novel proof techniques for analyzing the dynamics of stochastic Nesterov accelerated gradient descent under distribution drift with high probability for the lower-level variable, which are crucial for analyzing the hypergradient error and also of independent interest.   \n\u00b7 We empirically verify the effectiveness of our proposed algorithm on various tasks, including deep AUC maximization and data hypercleaning. Our algorithm indeed achieves ", "page_idx": 1}, {"type": "text", "text": "the predicted theoretical acceleration and significantly outperforms baselines in bilevel optimization. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Relaxed Smoothness. The concept of relaxed smoothness was initially introduced by [70], inspired by the loss landscapes observed in recurrent neural networks and long-short term memory networks. They show that techniques such as gradient clipping and normalization could improve the performance compared with gradient descent in these scenarios. It inspired further investigations that concentrate on various aspects, including improved analysis on gradient clipping and normalization [69, 40], adaptive algorithms [14, 46, 63, 22], federated algorithms [49, 15, 16], generalized assumptions [14, 13], and recursive momentum based methods with faster rates [55, 51]. The work of [35, 30] considered a relaxed smoothness condition for the upper-level problem in the bilevel optimization setting. ", "page_idx": 2}, {"type": "text", "text": "Bilevel Optimization. Bilevel optimization refers to a special kind of optimization where one problem is embedded within another. It was first introduced by [8]. Early works developed specific bilevel optimization algorithms with asymptotic convergence analysis [62, 1, 64]. Ghadimi and Wang [28] initiated the study of non-asymptotic convergence for gradient-based methods in bilevel optimization where the upper-level problem is smooth and the lower-level problem is strongly convex. This field saw further advancements with improved complexity results [38, 39, 10, 19, 11] and fully first-order algorithms [43, 47]. There is a line of work which leverages almost-sure Lipschitz oracle (e.g., stochastic gradient) to obtain improved convergence rates of bilevel optimization algorithms [66, 41]. When the lower-level function is not strongly convex, several algorithmic framework and approximation schemes were proposed [56, 58, 44, 57, 50, 9]. The setting considered in this paper is very close to [35, 30], where the upper-level function is nonconvex and unbounded smooth, and the lower-level function is strongly convex. However, the work of [35, 30] do not have an accelerated rate $\\widetilde{\\cal O}(\\epsilon^{-3})$ for finding an $\\epsilon$ -stationary point as established in this paper. ", "page_idx": 2}, {"type": "text", "text": "Nesterov Accelerated Gradient and Variants. Nesterov Accelerated Gradient (NAG) method was introduced by [53] for deterministic convex optimization problems. The stochastic version of NAG (SNAG) was extensively studied in the literature [3, 4, 61, 12]. To the best of our knowledge, none of them provide a high probability analysis for SNAG. In the online learning setting, there is a line of work focusing on the perspective of sequential stochastic/online optimization with distributional drift [5, 65, 52, 18]. While these studies provide valuable insights into adaptive techniques and performance bounds under distributional drift, they do not explore the potential integration of such methods with bilevel optimization problems, nor do they consider the application of SNAG within this framework. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Define $\\langle\\cdot,\\cdot\\rangle$ and $\\|\\cdot\\|$ as the inner product and the Euclidean norm. Throughout the paper, we use asymptotic notation $\\widetilde{\\cal O}(\\cdot),\\widetilde\\Theta(\\cdot),\\widetilde\\Omega(\\cdot)$ tohidepolylogfactors in $\\epsilon^{-1}$ and $1/\\delta$ . Denote $f\\colon\\mathbb{R}^{d_{x}}\\times\\mathbb{R}^{d_{y}}\\rightarrow$ $\\mathbb{R}$ as the upper-level function, and $g$ $\\mathbb{R}^{d_{\\boldsymbol{x}}}\\times\\mathbb{R}^{d_{\\boldsymbol{y}}}\\rightarrow\\mathbb{R}$ as the lower-level function. The hypergradient $\\nabla\\Phi(x)$ has the following form [28]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla\\Phi(x)=\\nabla_{x}f(x,y^{*}(x))-\\nabla_{x y}^{2}g(x,y^{*}(x))\\left[\\nabla_{y y}^{2}g(x,y^{*}(x))\\right]^{-1}\\nabla_{y}f(x,y^{*}(x)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To avoid the Hessian inverse computation, we typically use the following Neumann series to approximate the hypergradient [28, 41, 38]. In particular, for the stochastic setting, define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{\\nabla}f(x,y;\\bar{\\xi})=\\nabla_{x}F(x,y;\\xi)-\\frac{Q}{l_{g,1}}\\nabla_{x y}^{2}G(x,y;\\zeta^{(0)})\\prod_{i=1}^{q(Q)}\\left(I-\\frac{\\nabla_{y y}^{2}G(x,y;\\zeta^{(i)})}{l_{g,1}}\\right)\\nabla_{y}F(x,y;\\xi),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\mathfrak{q}}(Q)\\sim{\\mathrm{Uniform}}\\{0,\\ldots,Q-1\\}$ \uff0c $\\bar{\\xi}:=\\{\\xi,\\zeta^{(0)},\\dots,\\zeta^{(\\mathsf{q}(Q))}\\}$ and we use the convention that $\\textstyle\\prod_{i=1}^{j}A_{i}=I$ $j=0$ Then $\\mathbb{E}_{\\bar{\\xi}}[\\bar{\\nabla}f(x,y;\\bar{\\xi})]$ is a good approximation of $\\nabla\\Phi(x)$ $y$ and $y^{*}(x)$ are close [28]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1 $(L_{x,0},L_{x,1},L_{y,0},L_{y,1})$ -smoothness [35]). Let $z=(x,y)$ and $z^{\\prime}=(x^{\\prime},y^{\\prime}),$ there exists $L_{x,0},L_{x,1},L_{y,0},L_{y,1}\\,>\\,0$ such that for all $z,z^{\\prime},\\,i f\\,\\|z-z^{\\prime}\\|\\,\\le\\,1/\\sqrt{2(L_{x,1}^{2}+L_{y,1}^{2})}$ then $\\|\\nabla_{x}f(z)-\\nabla_{x}f(z^{\\prime})\\|\\le\\big(L_{x,0}+L_{x,1}\\|\\nabla_{x}f(z)\\|\\big)\\|z-z^{\\prime}\\|$ and $\\|\\nabla_{y}f(z)\\overset{\\cdot}{-}\\nabla_{y}f(z^{\\prime})\\|\\,\\leq\\,(L_{y,0}\\,+$ $L_{y,1}\\|\\nabla_{y}f(z)\\|)\\|z-z^{\\prime}\\|$ ", "page_idx": 3}, {"type": "text", "text": "Remark: Assumption 3.1 is introduced by [35] for describing the bilevel optimization problems with recurrent neural networks. This assumption can be regarded as a block-wise relaxed smoothness assumptionsfortwoblocks $x$ and $y$ , which is a variant of the relaxed smoothness assumption [70] and the coordinate-wise relaxed smooth assumption [14]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2. Suppose the followings hold for objective functions $f$ and $g\\colon(i)~f$ is continuously differentiable and $(L_{x,0},L_{x,1},L_{y,0},L_{y,1})$ -smooth in $(x,y)$ : (ii) For every $x$ $\\|\\nabla_{y}f(x,y)\\|\\leq l_{f,0}$ for all $y$ : (ii) For every $x$ $g(x,y)$ is $\\mu$ -strongly-convex in y for $\\mu>0$ \uff0c $(i\\nu)\\,g$ is $l_{g,1}$ -smooth jointly $i n$ $(x,y)$ (v) $g$ is twice continuously differentiable, and $\\nabla_{x y}^{2}g,\\nabla_{y y}^{2}g$ are $l_{g,2}$ -Lipschitz jointly in $(x,y)$ ", "page_idx": 3}, {"type": "text", "text": "Remark: Assumption 3.2 is standard in the bilevel optimization literature [43, 35, 28]. Assumption 3.2 (i) characterizes the unbounded smoothness of the upper-level function and is empirically observed in recurrent neural networks [35]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.3. The following stochastic estimators are unbiased and have the following properties: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{f}}[\\|\\nabla_{x}F(x,y;\\xi)-\\nabla_{x}f(x,y)\\|^{2}]\\le\\sigma_{f,1}^{2},\\quad\\mathbb{E}_{\\xi\\sim\\mathcal{D}_{f}}[\\|\\nabla_{y}F(x,y;\\xi)-\\nabla_{y}f(x,y)\\|^{2}]\\le\\sigma_{f,1}^{2},}\\\\ &{\\qquad\\qquad\\operatorname*{Pr}(\\|\\nabla_{y}G(x,y;\\xi)-\\nabla_{y}g(x,y)\\|\\ge\\lambda)\\le2\\exp(-2\\lambda^{2}/\\sigma_{g,1}^{2})\\quad\\forall\\lambda>0,}\\\\ &{\\mathbb{E}_{\\zeta\\sim\\mathcal{D}_{g}}[\\|\\nabla_{x y}^{2}G(x,y;\\zeta)-\\nabla_{x y}^{2}g(x,y)\\|^{2}]\\le\\sigma_{g,2}^{2},\\quad\\mathbb{E}_{\\zeta\\sim\\mathcal{D}_{g}}[\\|\\nabla_{y y}^{2}G(x,y;\\zeta)-\\nabla_{y y}^{2}g(x,y)\\|^{2}]\\le\\sigma_{g,2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark: Assumption 3.3 assumes the stochastic oracle for the upper-level problem has bounded variance, which is standard in nonconvex stochastic optimization [26-28]. It also assumes the stochastic oracle for the lower-level problem is light-tailed, which is common for the high probability analysis for the lower-level problem [45, 36]. Note that the same assumption is also made in [35, 30] for the bilevel problems with a unbounded smooth upper-level function. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.4. $F(x,y;\\xi)$ and $G(x,y;\\zeta)$ satisfy Assumption 3.2 for every $\\xi$ and $\\zeta$ almost surely. ", "page_idx": 3}, {"type": "text", "text": "Remark: Assumption 3.4 assumes that each random realization of the upper- and lower-level functions satisfies the same property as in the population level. Note that this condition is the key to achieve an improved $\\widetilde{\\cal O}(\\epsilon^{-3})$ oracle complexity under various settings, including both single-level nonconvex smooth problems [21, 17, 59] and bilevel problems with nonconvex smooth upper-level objectives [66, 41]. Furthermore, this assumption is shown to be necessary for achieving improved oracle complexity in single-level problems [2]. ", "page_idx": 3}, {"type": "text", "text": "4  Algorithm and Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1  Main Challenges and Algorithm Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Main Challenges. We begin by explaining why existing bilevel optimization algorithms and their corresponding analysis techniques are insufficient in our setting. First, most algorithms developed for bilevel optimization require the upper-level function is smooth (i.e., the gradient of the upperlevel function is Lipschitz) [28, 39, 38, 66, 41, 19, 43]. They characterize the estimation error of the optimal solution for the lower-level problem, utilize an approximate hypergradient descent approach and the descent lemma for $L$ -smooth functions to prove the convergence. In particular, they demonstrate that a potential function, incorporating both the function value and the bilevel error from the lower-level problem, progressively decreases in expectation. However, when the upper-level function is $(L_{x,0},L_{x,1},L_{y,0},L_{y,1})$ -smooth as illustrated in Assumption 3.1, the previous algorithms and analyses relying on $L$ -smoothness do not work. The reason is that the hypergradient bias depends on the approximation error of the lower-level variable as well as the hypergradient itself: these elements are statistically dependent and the standard potential function argument with an expectation-based analysis would not work. Second, the recent work of Hao et al. [35] and Gong et al. [30] considered that the upper-level function is unbounded smooth and addressed this issue by performing normalized stochastic gradient with momentum for the upper-level variable and periodic updates or stochastic gradient descent for the lower-level variable, but their oracle complexity is not ", "page_idx": 3}, {"type": "table", "img_path": "v7vYVvmfru/tmp/e36c7f6db74c3a66f263c773f69c1da74f9eeb977382fcb9658abc0cfbd9d063.jpg", "table_caption": ["Algorithm 1 STOCHASTIC NESTEROV ACCELERATED GRADIENT METHOD (SNAG) "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "AIgorithm2ACCELERATEDBILEVEL OPTIMIZATIONALGORITHM(ACCBO)   \n1: Input: $\\alpha^{\\mathrm{init}},\\alpha,\\beta,\\gamma,\\eta,\\tau,I,S,T_{0},T$ set $x_{0},y_{0}^{\\mathrm{init}}=0$   \n2 2 $y_{0}=\\mathsf{S N A G}(x_{0},y_{0}^{\\mathrm{init}},\\alpha^{\\mathrm{init}},T_{0})$ , and set $y_{-1}=\\hat{y}_{0}=y_{0}$ # Warm-start   \n3: for $t=0,1,\\ldots,T-1$ do   \n4: Sample ${\\mathfrak{q}}(Q)\\sim{\\mathrm{Uniform}}\\{0,\\ldots,Q-1\\}$ and $\\{\\zeta_{t,s}^{(0)},\\ldots,\\zeta_{t,s}^{({\\mathsf{q}}(Q))}\\}_{s=1}^{S}\\sim{\\mathcal{D}}_{g}$   \n5: Sample $\\{\\xi_{t,s}\\}_{s=1}^{S}\\sim\\mathcal{D}_{f}$ denote $\\bar{\\xi}_{t}:=\\cup_{s=1}^{S}\\{\\mathfrak{q}(Q),\\xi_{t,s},\\zeta_{t,s}^{(0)},\\dots,\\zeta_{t,s}^{(\\mathfrak{q}(Q))}\\}$   \n6: # Lower-Level: Stochastic Nesterov Accelerated Gradient Descent with Averaging   \n7: # Option I: from Line $8\\sim9$ (for one-dimensional quadratic lower-level function)   \n8: $z_{t}=y_{t}+\\gamma(y_{t}-y_{t-1})$   \n9: $y_{t+1}=z_{t}-\\alpha\\nabla_{y}G(x_{t},z_{t};\\pi_{t}).$ where $\\pi_{t}\\sim\\mathcal{D}_{g}$   \n10: # Option II: from Line $11\\sim20$ (for general strongly-convex lower-level function)   \n11: if $t>0$ and $t$ is a multiple of $I$ then   \n12: Set $y_{t}^{0}=y_{t}^{-1}=y_{t}$   \n13: for $j=0,1,\\ldots,N-1\\,\\,$ do   \n14: $z_{t}^{j}=y_{t}^{j}+\\gamma(y_{t}^{j}-y_{t}^{j-1})$   \n15: $y_{t}^{j+1}=z_{t}^{j}-\\alpha\\nabla_{y}G(x_{t},z_{t}^{j};\\pi_{t}^{j})$ , where $\\pi_{t}^{j}\\sim\\mathcal{D}_{g}$   \n16: end for   \n17: $y_{t+1}=y_{t}^{N+1}$   \n18: else   \n19: $y_{t+1}=y_{t}$   \n20: end if   \n21: $\\hat{y}_{t+1}=(1-\\tau)\\hat{y}_{t}+\\tau y_{t+1}$ # Averaging   \n22: # Upper-Level: Normalized Stochastic Gradient Descent with Recursive Momentum   \n23: $\\begin{array}{r l}&{m_{t}^{\\mathrm{~\\,~i~}}\\beta m_{t-1}+(1-\\beta)\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})+\\beta(\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})-\\bar{\\nabla}f(x_{t-1},\\hat{y}_{t-1};\\bar{\\xi}_{t}))}\\\\ &{m_{0}=\\bar{\\nabla}f(x_{0},\\hat{y}_{0};\\bar{\\xi}_{0})}\\\\ &{x_{t+1}=x_{t}-\\eta\\frac{m_{t}}{\\|m_{t}\\|}}\\\\ &{\\underline{{\\hat{x}}}_{\\mathrm{~\\,~s~}}^{\\mathrm{~\\,~i~}}}\\end{array}$ $t\\geq1$ else   \n24:   \n25: end for ", "page_idx": 4}, {"type": "text", "text": "better than $\\widetilde{\\cal O}(\\epsilon^{-4})$ . These facts indicate that we need new algorithm design and analysis techniques to get potential acceleration. ", "page_idx": 4}, {"type": "text", "text": "Algorithm Design. To obtain potential acceleration, our key idea is to update the upper-level variable by normalized stochastic gradient descent with recursive momentum and update the lower-level variable by the stochastic Nesterov accelerated gradient (SNAG) method. Different from [35, 30], the key innovation of our algorithm design is that we achieve acceleration for both upper-level and lower-level problems simultaneously but without affecting each other. The upper-level update rule can be regarded as a generalization of the acceleration technique (e.g., the momentum-based variance reduction technique) [17, 51] from single-level to bilevel problems. The main challenge is that we need to deal with the accumulated error of the recursive momentum over time due to the hypergradient bias, which is caused by the inaccurate estimation of the optimal lower-level variable. Therefore we require a very small tracking error between the iterate of the lower-level variable and the optimal lower-level solution defined by the upper-level variable (i.e., $y^{*}(x)$ at every iteration. This requirement is satisfied by executing SNAG method under the distribution drift for the lower-level problem, where the drift is caused by the change of the upper-level variable over time. ", "page_idx": 4}, {"type": "text", "text": "The detailed description of our algorithm is illustrated in Algorithm 2. At the very beginning, we run a certain number of iterations of SNAG for the fixed upper-level variable $x_{0}$ (line 2) as the warm-start stage, add then update the lower-level variable by SNAG (line $8\\sim20$ ) with averaging (line 21) and update the upper-level variable by normalized stochastic gradient descent with recursive momentum (line $23\\sim24$ ). Note that we have two options for implementing SNAG. In Option I (line $8\\sim9$ the algorithm simply runs SNAG under distribution drift caused by the sequence $\\left\\{x_{t}\\right\\}$ . Option I is specifically designed for a particular subset of bilevel optimization problems where the lower-level function is a one-dimensional quadratic function. Option II (line $11\\sim20)$ ) is designed for a broader range of bilvel optimization problems, accommodating general strongly-convex lower-level functions with any dimension. In Option II, we run SNAG with periodic updates: the lower-level update is performed for $N$ iterations only when the iteration number $t$ is a multiple of $I$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2  Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first introduce some useful notations. Let $\\sigma(\\cdot)$ be the $\\sigma$ -algebra generated by the random variablesin the argumentWe dene thefollowing fltrations $t\\geq1$ $\\bar{\\mathcal{F}}^{\\mathrm{init}}=\\sigma(\\tilde{\\pi_{0}},\\ldots,\\tilde{\\pi}_{T_{0}-1})$ $\\mathcal{F}_{t}=\\sigma(\\bar{\\xi}_{0},\\ldots,\\bar{\\xi}_{t-1})$ \uff0c $\\widetilde{\\mathcal{F}}_{t}^{1}=\\sigma(\\pi_{0},\\ldots,\\pi_{t-1})$ , and we also define $\\widetilde{\\mathcal{F}}_{t}^{2}=\\sigma(\\pi_{t}^{0},\\dots,\\pi_{t}^{N-1})$ (N-1) when t is a multiple of $I$ . We use $\\mathbb{E}_{t}$ \uff0c $\\mathbb{E}_{\\mathcal{F}_{t}}$ and $\\mathbb{E}$ to denote the conditional expectation $\\mathbb{E}[\\cdot\\mid\\mathcal{F}_{t}]$ , the expectation over $\\mathcal{F}_{t}$ and the total expectation over $\\mathcal{F}_{T}$ respectively. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Suppose Assumptions 3.1 to 3.4 hold. Let $\\left\\{x_{t}\\right\\}$ be the iterates produced by Algorithm 2. For any given $\\delta\\in(0,\\dot{1})$ and small enough $\\epsilon$ (see exact choice in (53)), we set parameters $\\alpha^{\\mathrm{init}},\\alpha,\\beta,\\gamma,\\eta,\\tau,I,N,S,Q,T_{0}$ (see exact choices in (54), (55), (56), (57), and (58) as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha^{\\mathrm{init}}=\\widetilde{\\Theta}(\\epsilon^{4}),\\quad\\alpha=\\widetilde{\\Theta}(\\epsilon^{2}),\\quad1-\\beta=\\widetilde{\\Theta}(\\epsilon^{2}),\\quad\\eta=\\widetilde{\\Theta}(\\epsilon^{2}),\\quad\\tau=\\widetilde{\\Theta}(\\epsilon),\\quad\\gamma=O(1),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{0}=\\tilde{O}(\\epsilon^{-2}),~~~I=\\tilde{O}(\\epsilon^{-1}),~~~N=\\tilde{O}(\\epsilon^{-1}),~~~Q=\\tilde{O}(1),~~~S=\\tilde{O}(1).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thenwithprobability at least $1\\,-\\,2\\delta$ over the randomness in $\\sigma(\\mathcal{F}^{\\mathrm{init}}\\cup\\widetilde{\\mathcal{F}}_{T}^{1})$ (for Option $I$ )or $\\sigma(\\mathcal{F}^{\\mathrm{init}}\\cup(\\cup_{t\\le T}\\widetilde{\\mathcal{F}}_{t}^{2}))$ (for Option $I I$ ), Algorithm 2 guarantees $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|\\leq20\\epsilon}\\end{array}$ within $\\begin{array}{r}{T=\\frac{4d_{0}}{\\eta\\epsilon}=\\widetilde{O}(\\epsilon^{-3})}\\end{array}$ iterations, where $d_{0}:=\\Phi(x_{0})-\\operatorname*{inf}_{x}\\Phi(x)$ and theexctatnistakenver the randomness over $\\mathcal{F}_{T}$ For Option $I,$ it requires $T_{0}+S Q T=\\widetilde{O}(\\epsilon^{-3})$ oraclecallsofstochastic gradient orHessian/Jacobianvector product.For Option $I I_{i}$ . it requires $\\begin{array}{r}{T_{0}+\\frac{N T}{I}+S Q T=\\widetilde{O}(\\epsilon^{-3})}\\end{array}$ oraclecalls of stochastic gradient or Hessian/Jacobianvector product. ", "page_idx": 5}, {"type": "text", "text": "Remark: Theorem 4.1 established an improved $\\widetilde{\\cal O}(\\epsilon^{-3})$ oracle complexity for finding an $\\epsilon$ -stationary point. This complexity result strictly improves the $\\widetilde{\\cal O}(\\epsilon^{-4})$ obtained by [35, 30] when the upper-level function is nonconvex and unbounded smooth. This complexity result also matches that in the single-level unbounded smooth setting [51] and is nearly optimal in terms of the dependency on $\\epsilon$ [2]. The full statement of Theorem 4.1 is included in Theorem E.2. ", "page_idx": 5}, {"type": "text", "text": "4.3 Proof Sketch ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide a roadmap of proving Theorem 4.1 and the main steps. The detailed proofs can be found in Appendix D and E. The key idea is to prove two things: (1) the lower-level iterate is very close to the optimal lower-level variable at every iteration; (2) two consecutive iterates of the lower-level iterates are close to each other. In particular, define ${\\boldsymbol{y}}_{t}^{*}={\\boldsymbol{y}}^{*}({\\boldsymbol{x}}_{t})$ , and we aim to prove that $\\lVert\\hat{y}_{t}-y_{t}^{*}\\rVert\\leq O(\\epsilon)$ and $\\lVert\\hat{y}_{t+1}-\\hat{y}_{t}\\rVert\\leq O(\\epsilon^{2})$ for every $t$ . These two requirements are essential to control the hypergradient estimation error (i.e., $\\|\\dot{\\boldsymbol{m}}_{t}-\\nabla\\Phi(\\boldsymbol{x}_{t})\\|)$ caused by inaccurate estimate of the lower-level problem. Lemma 4.7 provides the guarantee for the lower-level problem, and Lemma 4.8 characterizes the hypergradient estimation error. Equipped with these two lemmas, we can adapt the momentum-based variance reduction techniques [17, 51] to the upper-level problem and prove the main theorem. ", "page_idx": 5}, {"type": "text", "text": "The main technical contribution of this paper is to provide a general framework for proving the convergence of SNAG under distributional drift in Section 4.3.1, which can be leveraged as a tool to control the lower-level error in bilevel optimization and derive the Lemma 4.7, as illustrated in Section 4.3.2. In particular, we can regard the change of the upper-level variable $x$ at each iteration as the distributional drift for the lower-level problem: the drift is small due to the normalization operator of the upper-level update rule and also the Lipschitzness of $y^{*}(x)$ . Once we have the general lemma for tracking the minimizer for any fixed distributional drift over time, this lemma can be applied to our algorithm analysis and establish guarantees for the bilevel problem. ", "page_idx": 5}, {"type": "text", "text": "4.3.1 Stochastic Nesterov Accelerated Gradient Descent under Distributional Drift ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we study the sequences of stochastic optimization problems $\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}$ $\\phi_{t}(w)$ indexed by time $t\\in\\mathbb{N}$ We denote the minimizer and the minimal value of $\\phi_{t}$ as $\\boldsymbol{w}_{t}^{*}$ and $\\phi_{t}^{*}$ , and we define the minimizer drift at time $t$ to be $\\Delta_{t}:=\\lVert w_{t}^{*}-w_{t+1}^{*}\\rVert$ . With a slight abuse of notation 2, we consider the SNAG algorithm applied to the sequence $\\{\\phi_{t}\\}_{t=1}^{T}$ , where $T$ is the total number of iterations: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=w_{t}+\\gamma(w_{t}-w_{t-1})\\qquad}\\\\ {w_{t+1}=w_{t}+\\gamma(w_{t}-w_{t-1})-\\alpha g_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $g_{t}=\\nabla\\phi_{t}(z_{t};\\xi_{t})$ is the stochastic gradient evaluated at $z_{t}$ with random sample $\\xi_{t}$ . Define $\\varepsilon_{t}=g_{t}-\\nabla\\phi_{t}(z_{t})$ as the stochastic gradient noise at $t$ -th iteration. Define $\\mathcal{H}_{t}=\\sigma(\\xi_{1},.~.~.~,\\xi_{t-1})$ as the filtration, which is the $\\sigma$ -algebra generated by all random variables until $t$ -th iteration. We make the following assumption, which is the same as Assumption 3 in [18] for high probability analysis. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2. Function $\\phi_{t}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $\\mu$ -strongly convex and $L$ -smooth for constants $\\mu,L>0$ Also, there exists constants $\\Delta,\\sigma>0$ such that the drift $\\Delta_{t}^{2}$ is sub-exponential conditioned on $\\mathcal{H}_{t}$ withparameter $\\Delta^{2}$ and the noise $\\varepsilon_{t}$ is norm sub-Gaussian conditioned on $\\mathcal{H}_{t}$ with parameter $\\sigma/2$ ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3. Suppose Assumption 4.2 holds and let $\\{w_{t}\\}$ be the iterates produced by the update rule 3 wihconstaleing a\u03b1min1/2L,1/10\u00b2\u03bc,\u03bc/0\u00b2},ad et Define $\\theta_{t}=[(w_{t}-w_{t}^{*})^{\\top},(w_{t-1}-w_{t}^{*})]^{\\top}\\in\\mathbb{R}^{2d}$ and the potential function $V_{t}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n{V_{t}}=\\theta_{t}^{\\top}\\mathbf{P}\\theta_{t}+\\phi_{t}(w_{t})-\\phi_{t}(w_{t}^{*}),\\quad w h e r e\\quad\\mathbf{P}=\\frac{1}{2\\alpha}\\left[\\sqrt{\\mu\\alpha}-1\\quad\\sqrt{\\mu\\alpha}-1\\right]\\otimes\\mathbf{I}_{d}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then for any given $\\delta\\in(0,1)$ andall $t\\geq0$ the following holds with probability at least $1-\\delta$ over therandomness in $\\mathcal{H}_{t}$ (here e denotes the base of natural logarithms): ", "page_idx": 6}, {"type": "text", "text": "(i\uff09(With drift) Le $\\textstyle\\beth t\\,\\phi_{t}(w):=\\frac{\\mu}{2}\\|w-w_{t}^{*}\\|^{2}$ and $w\\in\\mathbb{R}_{;}$ then ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{t}\\leq\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}V_{0}+\\left(2\\alpha\\sigma^{2}+\\frac{80\\Delta^{2}}{\\alpha}\\right)\\ln\\frac{e T}{\\delta}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(i\uff09(Without drift) Let $\\phi_{t}(w)\\equiv\\phi(w)$ be any general functions and $w\\in\\mathbb{R}^{d}$ with $\\Delta=0$ then ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{t}\\le\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}V_{0}+2\\alpha\\sigma^{2}\\ln\\frac{e T}{\\delta}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark: When $\\{\\phi_{t}\\}_{t=1}^{T}$ is a sequence of one-dimensional quadratic functions with moving minimizers, Lemma 4.3 provides a high probability tracking guarantee for SNAG with distributional drift, which is useful to provide guarantees for Option I in Algorithm 2. Note that this guarantee strictly improves the guarantee of stochastic gradient descent with distributional drift (e.g., Theorem 6 in [18]) and therefore is of independent interest. In particular, for small $\\alpha$ , the decaying factor in the frst term is improved from 1  to 1  , adhedrftemisimrovedfrom $\\frac{\\Delta^{2}}{\\alpha^{2}}$ $\\frac{\\Delta^{2}}{\\alpha}$ To the best of our knowledge, this is the first high probability guarantee with improved rate for SNAG under distributional drift. When there is no drift, Lemma 4.3 also provides a high probability guarantee for SNAG. It holds for any smooth and strongly convex function $\\phi$ with any dimension $d$ , and it is useful to provide guarantees for Option II of Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "4.3.2 Application of Stochastic Nesterov Accelerated Gradient to Bilevel Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Inspired by [35, 30], we can regard $\\phi_{t}(\\cdot)$ as $\\phi_{t}(\\cdot):=g(x_{t},\\cdot)$ in the bilevel setting, and then we have $\\Delta_{t}=\\eta l_{g,1}/\\mu$ for every $t$ due to the upper-level update rule and the Lipschitzness of $y^{*}(x)$ . Therefore we can focus on the high probability analysis on the lower-level variable without worrying about the randomness from the upper-level. Throughout, we assume Assumption 3.1, 3.2, 3.3 and 3.4 hold. In addition, the failure probability $\\delta\\in(0,1)$ and $\\epsilon>0$ are chosen in the same way as in Theorem 4.1. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.4 (Warm-start). Let $\\{y_{t}^{\\mathrm{init}}\\}$ be the iterates produced by line 2 of Algorithm 2. Set $\\alpha^{\\mathrm{init}}=\\widetilde\\Theta(\\epsilon^{4})$ and $\\phi_{t}(y)\\equiv g(x_{0},y)$ Then $\\lVert y_{T_{0}}^{\\mathrm{init}}-y_{0}^{*}\\rVert\\leq\\sqrt{\\frac{\\mu\\alpha}{32}}\\frac{\\epsilon}{L_{0}}$ $1-\\delta$ over the randomness in $\\widetilde{\\mathcal{F}}^{\\mathrm{init}}$ (we denote this event as $\\mathscr{E}_{\\mathrm{init.}}$ )in $T_{0}=\\widetilde{O}(\\epsilon^{-2})$ iterations. ", "page_idx": 7}, {"type": "text", "text": "Remark: Lemma 4.4 shows that for fixed initialization $x_{0}$ , running SNAG for at most $T_{0}=\\widetilde{O}(\\epsilon^{-2})$ iterations canguarantee thathe Euclidean distance between the lower-level variable yinit and the optimal solution $y^{*}(x_{0})$ is at most $O(\\epsilon)$ , with high probability. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.5 (Option 1). Under event $\\ensuremath{\\mathcal{E}_{\\mathrm{init}}}$ ,let $\\{y_{t}\\}$ be the iterates produced by Option 1. Set $\\alpha=$ $\\widetilde{\\Theta}(\\epsilon^{2})$ and $\\phi_{t}(y)\\;=\\;g(x_{t},y)\\;=\\;\\textstyle{\\frac{\\mu}{2}}\\|y\\,-\\,y_{t}^{*}\\|^{2}$ with $y\\,\\in\\,\\mathbb{R}$ .Then for any $t\\ \\in\\ [T]$ ,Algorithm 2 guarantees with probability at least $1-\\delta$ over the randomness in $\\widetilde{\\mathcal{F}}_{T}^{1}$ (we denote this event as $\\mathcal{E}_{y}^{1}$ that $\\lVert y_{t}-y_{t}^{*}\\rVert\\leq\\epsilon/2L_{0}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.6 (Option I1). Under event $\\ensuremath{\\mathcal{E}_{\\mathrm{init}}}$ let $\\{y_{t}\\}$ be the iterates produced by Option Il.Set $\\alpha=\\widetilde\\Theta(\\epsilon^{2})$ \uff0c $\\bar{N}\\,=\\,\\widetilde O(\\epsilon^{-1}).$ $I=\\widetilde O(\\epsilon^{-1})$ and $\\phi_{t}(y)\\,=\\,g(x_{t},y)$ when $t$ is a multiple of $I$ (i.e.,. $x_{t}$ is fixed for each update round of Option $I$ \\$O $g$ can be general functions). Then for any $t\\,\\in\\,[T]$ \uff0c Algorithm2guaranteeswithprobabilityatleast $1-\\delta$ over the randomness in $\\sigma(\\cup_{t\\le T}\\widetilde{\\mathcal{F}}_{t}^{2})$ (we denote this event as $\\mathcal{E}_{y}^{2}$ )that $\\|y_{t}-y_{t}^{*}\\|\\leq\\epsilon/L_{0}$ ", "page_idx": 7}, {"type": "text", "text": "Remark: Lemma 4.5 and Lemma 4.6 show that, under event $\\ensuremath{\\mathcal{E}_{\\mathrm{init}}}$ and both option I and option II, the algorithm guarantees that each iterate $y_{t}$ is $O(\\epsilon)$ -close to the the optimal lower-level variable $y_{t}^{*}$ at every iteration $t$ with high probability. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.7 (Averaging). Under event $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}\\cap\\ensuremath{\\mathcal{E}}_{y}^{1}$ (Option I) or $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}\\cap\\ensuremath{\\mathcal{E}}_{y}^{2}$ (Option II), set $\\tau=\\sqrt{\\mu\\alpha}$ in the averaging step (line 21 of Algorithm 2). Then for any $t\\geq0$ wehave $\\begin{array}{r}{\\|\\hat{y}_{t}-y_{t}^{*}\\|\\leq\\frac{2\\epsilon}{L_{0}}}\\end{array}$ and $\\begin{array}{r}{\\|\\hat{y}_{t+1}-\\hat{y}_{t}\\|\\leq\\frac{\\mu\\epsilon^{2}}{24L_{0}^{2}\\sigma_{g,1}}=:\\vartheta.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Remark: Lemma 4.7 shows tha afer performing averaging oerationsover the sequence $\\{y_{t}\\}_{t=1}^{T}$ the averaged sequence enjoys stronger guarantees. First, each averaged iterate $\\hat{y}_{t}$ is still $O(\\epsilon)$ -close to the optimal lower-level variable $y_{t}^{*}$ ; Second, two consecutive averaged iterates (i.e., $\\hat{y}_{t}$ and $\\hat{y}_{t+1})$ is $O(\\epsilon^{2}\\bar{)}$ -close to each other. The stronger guarantees are crucial to control the hypergradient estimation error as described in Lemma 4.8. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.8. Under event $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}\\cap\\ensuremath{\\mathcal{E}}_{y}^{1}$ (Option $I$ or $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}\\cap\\ensuremath{\\mathcal{E}}_{y}^{2}$ (Option $I I,$ define $\\epsilon_{t}~=~m_{t}~-$ $\\mathbb{E}_{t}[\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})]$ , then we have the following averaged cumulative error bound: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\epsilon_{t}\\|\\leq\\frac{\\bar{\\sigma}}{T(1-\\beta)}+\\sqrt{1-\\beta}\\bar{\\sigma}+\\frac{\\bar{L}_{0}}{\\sqrt{1-\\beta}}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}+\\bar{L}_{1}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S(1-\\beta)}}\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $S$ denotes the batch size, and $\\bar{\\sigma},\\bar{L}_{0},\\bar{L}_{1}$ are defined in Lemmas B.4 and B.6. ", "page_idx": 7}, {"type": "text", "text": "Remark: Lemma 4.8 characterizes the upper-level hypergradient estimation error under the good event that the lower-level error can be controlled. One can choose hyperparameters appropriately such that the cumulative error (i.e., LHS) grows only sublinearly in terms of $T$ ,which is important for establishing the fast convergence of our algorithm. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Deep AUC Maximization with Recurrent Neural Networks. AUC (Area Under the ROC Curve) [33] is a critical metric in evaluating the performance of binary classification models. It measures the ability of the model to distinguish between positive and negative classes, and it is defined as the probability that the prediction score of a positive example is higher than that is a negative example [32]. Deep AUC maximization [48, 67] can be formulated as a minmax optimization $\\begin{array}{r}{\\mathrm{~problem~}[48]{\\cdot}\\operatorname*{min}_{w\\in\\mathbb{R}^{d},(a,b)\\in\\mathbb{R}^{2}}\\operatorname*{max}_{\\alpha\\in\\mathbb{R}}f(w,a,b,\\alpha)\\ :=\\mathbb{E}_{z}[F(w,a,b,\\alpha;z)]}\\end{array}$ where $\\begin{array}{r l r}{F(w,a,b,\\alpha;z)}&{{}\\!\\!=\\!\\!}&{(1\\,-\\,r)(h(w;\\pmb x)\\,-\\,a)^{2}\\mathbb{I}_{[c=1]}\\,+\\,r(h(\\pmb w;\\pmb x)\\,-\\,b)^{2}\\mathbb{I}_{[c=-1]}\\,+\\,2(1\\,+\\,\\pmb x)^{2}}\\end{array}$ $\\alpha)(r h({\\pmb w};{\\pmb x})\\mathbb{I}_{[c=-1]}-(1-r)h({\\pmb w};{\\pmb x})\\mathbb{I}_{[c=1]})-r(1-r)\\alpha^{2}$ $\\mathbf{\\nabla}w$ denotes the model parameter, ${\\boldsymbol{z}}=({\\boldsymbol{x}},{\\boldsymbol{c}})$ is the random data sample ( $\\textbf{\\em x}$ denote the feature vector and $c\\in\\{+1,-1\\}$ denotes the label), $h(w,x)$ is the score function defined by a neural network, and $r=\\mathrm{Pr}(c=1)$ denotes the ratio of positive samples in the population. This min-max formulation is an special case of the bilevel problem with $g=-f$ in (1), which can be reformulated as the following: ", "page_idx": 7}, {"type": "image", "img_path": "v7vYVvmfru/tmp/d228dc922b4c1221e4cc2be72cc5dd2d737c19d8b176aaae52bac05f3cb4e9ea.jpg", "img_caption": ["Figure 1: Results of bilevel optimization on deep AUC maximization. Figure (a), (b) are the results over epochs, and Figure (c), (d) are the results over running time. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{s\\in\\mathbb{R}^{d},(a,b)\\in\\mathbb{R}^{2}}{\\operatorname*{min}}\\mathbb{E}_{z}[F(w,a,b,\\alpha^{*}(w,a,b);z)]}&{\\mathrm{~s.t.,~}\\;\\;\\alpha^{*}(w,a,b)\\in\\arg\\underset{\\alpha\\in\\mathbb{R}}{\\operatorname*{min}}\\;-\\mathbb{E}_{z}[F(w,a,b,\\alpha;z)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $({\\pmb w},a,b)$ denotes the upper-level variable, and $\\alpha$ denotes the lower-level variable. In this case, the lower-level is a one-dimensional quadratic function in terms of $\\alpha$ and is strongly convex, and the upper-level function is non-convex function with potential unbounded smoothness when using a recurrent neural network as the predictive model. ", "page_idx": 8}, {"type": "text", "text": "We aim to perform imbalanced text classification task and maximize the AUC metric. The Deep AUC maximization experiment is performed on imbalanced Sentiment140 [29] dataset (under the license of CC BY 4.0), which is a binary text classification task. Specifically, we follow [68] to make training set imbalanced with a pre-defined imbalanced ratio $(r)$ , and leave the test set unchanged. Given $r$ , we randomly discard the positive samples (with label 1) in original training set until the portion of positive samples equals to $r$ . The imbalance ratio $r$ is set to 0.2 in our experiment, which means only $20\\%$ data is positive in the training set. We use a two-layer recurrent neural network with input dimension $=\\!300$ , hidden dimension $=\\!4096$ , and output dimension $^{=2}$ for the model prediction. ", "page_idx": 8}, {"type": "text", "text": "We compare with some bilevel optimization baselines, including StocBio [39], TTSA [38], SABA [19], MA-SOBA [11], SUSTAIN [41], VRBO [66] and BO-REP [35]. We show the training and test AUC result with 25 epochs in (a) (b) of Figure 1 and running time in (c), (d) of Figure 1. Our algorithm AccBO achieves highest AUC score among all the baselines over epochs and running time. The running time figure shows AccBO converges to a good result faster than other baselines. The detailed parameter tuning and selection are included in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Data Hypercleaning. The Data hypercleaning task tries to learn a set of weights $\\lambda$ for the corrupted training data $\\mathcal{D}_{t r}$ , such that the model trained on the weighted corrupted training set can achieve good performance on the clean validation set $\\mathcal{D}_{v a l}$ , where the corrupted training set $\\mathcal{D}_{t r}:=\\{\\pmb{x}_{i},\\Bar{y_{i}}\\}$ and the label $\\bar{y_{i}}$ is randomly flipped to one of other labels with probability $0<p<1$ . The data hyper-cleaning can be formulated as a bilevel optimization problem, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda}\\frac{1}{|{\\mathcal{D}}_{\\mathrm{val}}|}\\sum_{\\xi\\in{\\mathcal{D}}_{\\mathrm{val}}}\\mathcal{L}(w^{*}(\\lambda);\\xi),\\;\\mathrm{s.t.}\\;w^{*}(\\lambda)\\in\\arg\\operatorname*{min}_{w}\\frac{1}{|{\\mathcal{D}}_{\\mathrm{t}}|}\\sum_{\\zeta_{i}\\in{\\mathcal{D}}_{\\mathrm{v}}}\\sigma(\\lambda_{i})\\mathcal{L}(w;\\zeta_{i})+c\\|w\\|^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Where $\\pmb{w}$ is te model parameter of a eural network, and $\\begin{array}{r}{\\sigma(x)=\\frac{1}{1+e^{-x}}}\\end{array}$ is the sigmoid funtion. We perform bilevel optimization algorithms on the noisy text classification dataset Stanford Natural Language Inference (SNLI) [7] (under the license of CC BY 4.O) with a three-layer recurrent neural networkwithinput dimension $=\\!300$ , hidden dimension $=4096$ , and output dimension $_{-3}$ for the label prediction. Each of sentence-pairs manually labeled as entailment, contradiction, and neutral. Specifically, the label of each training data is randomly flipped to one of the other two labels with probability $p$ .We set $p=0.1$ and $p=0.2$ in the experiments, respectively. We compare all the baselines used in the deep AUC maximization experiment. Different from the formulation (4) for the deep AUC maximization, the lower-level function in (5) is not quadratic function of the lower-level variable. Therefore we choose Option $\\mathrm{II}$ in Algorithm 2, i.e., periodic updates for the lower-level variable. The results are presented in Figure 2 ( $p=0.1$ and $p=0.2$ 0. Our algorithm AccBO exhibits the highest classification accuracy on training and test set among all the bilevel baselines, and also ", "page_idx": 8}, {"type": "image", "img_path": "v7vYVvmfru/tmp/6030a103472aff1b69c9dade772c5418e033673af6a50f6d47f1ff7b3be16678.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2: Results of bilevel optimization on data hyper-cleaning with $p=0.1$ . Figure (a), (b), (c), (d) are the results with noise rate $p=0.1$ where (a), (b) are the results over epochs, and Figure (c), (d) are the results over running time. Figure (e), (f), (g), (h) are the results with noise rate $p=0.2$ ", "page_idx": 9}, {"type": "text", "text": "shows a high runtime efficiency. More detailed parameter tuning and selection can be found in Appendix G. All the experiments are run on the device of NVIDIA A6000 (48GB memory) GPU and AMD EPYC 7513 32-Core CPU. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a new algorithm named AccBO for solving bilevel optimization problems where the upper-level is nonconvex and unbounded smooth and the lower-level problem is strongly convex. The algorithm achieved $\\widetilde{\\cal O}(\\epsilon^{-3})$ oracle complexity for finding an $\\epsilon$ -stationary point, which matches the rate of the state-of-the-art single-level relaxed smooth optimization [51] and is nearly optimal in terms of dependency on $\\epsilon$ [2]. ", "page_idx": 9}, {"type": "text", "text": "Limitations. One limitation of our work is that the convergence analysis for the Option I of our algorithm relies on the lower-level problem being a one-dimensional quadratic function: only under this case the algorithm becomes a single-loop procedure. However, it remains unclear how to design single-loop algorithms for more general lower-level strongly convex functions in high dimension. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their helpful comments. This work has been supported by the Presidential Scholarship, the ORIEI seed funding, and the IDIA P3 fellowship from George Mason University, the Cisco Faculty Research Award, and NSF award #2436217. The Computations were run on ARGO, a research computing cluster provided by the Office of Research Computing at George Mason University (URL: https://orc.gmu.edu). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  G Anandalingam and DJ White. A solution method for the linear static stackelberg problem using penalty functions. IEEE Transactions on automatic control, 35(10):1170-1173, 1990.   \n[2] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199 (1-2):165-214,2023.   \n[3]  Mahmoud Assran and Michael Rabbat. On the convergence of nesterov's accelerated gradient method in stochastic settings. arXiv preprint arXiv:2002.12414, 2020.   \n[4]  Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. Robust accelerated gradient methods for smooth strongly convex functions. SIAM Journal on Optimization, 30(1):717-751, 2020.   \n[5] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations research, 63(5):1227-1244, 2015.   \n[6] Zalan Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. Advances in neural information processing systems, 33: 14879-14890, 2020.   \n[7] Samuel R Bowman, Gabor Angeli, Christopher Pots, and Christopher D Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.   \n[8]  Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the constraints. Operations research, 21(1):37-44, 1973. [9]  Lesi Chen, Jing Xu, and Jingzhao Zhang. On bilevel optimization without lower-level strong convexity. arXiv preprint arXiv:2301.00712, 2023.   \n[10] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. Advances in Neural Information Processing Systems, 34:25294-25307, 2021.   \n[11]  Xuxing Chen, Tesi Xiao, and Krishnakumar Balasubramanian. Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions. arXiv preprint arXiv:2306.12067, 2023.   \n[12] You-Lin Chen, Sen Na, and Mladen Kolar. Convergence analysis of accelerated stochastic gradient descent under the growth condition. Mathematics of Operations Research, 2023.   \n[13] Ziyi Chen, Yi Zhou, Yingbin Liang, and Zhaosong Lu. Generalized-smooth nonconvex optimization is as efficient as smooth nonconvex optimization. arXiv preprint arXiv:2303.02854, 2023.   \n[14] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signsgd. Advances in neural information processing systems, 2022.   \n[15] Michael Crawshaw, Yajie Bao, and Mingrui Liu. Episode: Episodic gradient clipping with periodic resampled corrections for federated learning with heterogeneous data. In The Eleventh International Conference on Learning Representations, 2023.   \n[16] Michael Crawshaw, Yajie Bao, and Mingrui Liu. Federated learning with client subsampling, data heterogeneity, and unbounded smoothness: A new algorithm and lower bounds. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[17]  Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd. Advances in neural information processing systems, 32, 2019.   \n[18] Joshua Cutler, Dmitriy Drusvyatskiy, and Zaid Harchaoui. Stochastic optimization under distributional drift. Journal of Machine Learning Research, 24(147):1-56, 2023.   \n[19] Mathieu Dagreou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. Advances in Neural Information Processing Systems, 35:26698-26710, 2022.   \n[20] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.   \n[21] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal nonconvex optimization via stochastic path-integrated differential estimator. Advances in neural information processing systems, 31, 2018.   \n[22] Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai. Beyond uniform smoothness: A stopped analysis of adaptive sgd. arXiv preprint arXiv:2302.06570, 2023.   \n[23]  Matthias Feurer and Frank Hutter.  Hyperparameter optimization.  In Automated Machine Learning, pages 3-33. Springer, Cham, 2019.   \n[24]  Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126-1135. PMLR, 2017.   \n[25] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International conference on machine learning, pages 1568-1577. PMLR, 2018.   \n[26]  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.   \n[27]  Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.   \n[28] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv: 1802.02246, 2018.   \n[29]  Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classfication using distant supervision. CS224N project report, Stanford, 1(12):2009, 2009.   \n[30] Xiaochuan Gong, Jie Hao, and Mingrui Liu. A nearly optimal single loop algorithm for stochastic bilevel optimization under unbounded smoothness. In Forty-first International Conference on Machine Learning, 2024.   \n[31]  Riccardo Grazzi, Massimiliano Pontil, and Saverio Salzo. Bilevel optimization with a lower-level contraction: Optimal sample complexity without warm-start. arXiv preprint arXiv:2202.03397, 2022.   \n[32] James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver operating characteristic (roc) curve. Radiology, 143(1):29-36, 1982.   \n[33] James A Hanley and Barbara J McNeil. A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3):839-843, 1983.   \n[34] Jie Hao, Kaiyi Ji, and Mingrui Liu. Bilevel coreset selection in continual learning: A new formulation and algorithm. Advances in Neural Information Processing Systems, 36, 2023.   \n[35] Jie Hao, Xiaochuan Gong, and Mingrui Liu. Bilevel optimization under unbounded smoothness: A new algorithm and convergence analysis. In The Twelfth International Conference on Learning Representations, 2024.   \n[36] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15(1): 2489-2512, 2014.   \n[37]  Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.   \n[38] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actorcritic. SIAM Journal on Optimization, 33(1):147-180, 2023.   \n[39]  Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In International conference on machine learning, pages 4882-4892. PMLR, 2021.   \n[40] Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. Non-convex distributionally robust optimization: Non-asymptotic analysis. Advances in Neural Information Processing Systems, 34:2771-2782, 2021.   \n[41] Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances in Neural Information Processing Systems (NeurIPS), 34:30271-30283, 2021.   \n[42]  Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999.   \n[43] Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully first-order method forstochasticbileveloptimization In Intenational Conference onMachne Learning, pages 18083-18113. PMLR, 2023.   \n[44] Jeongyeol Kwon, Dohyun Kwon, Steve Wright, and Robert Nowak. On penalty methods for nonconvex bilevel optimization and frst-order stochastic approximation. arXiv preprint arXiv:2309.01753, 2023.   \n[45]  Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2):365-397, 2012.   \n[46] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of adam under relaxed assumptions. arXiv preprint arXiv:2304.13972, 2023.   \n[47] Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu. Bome! bilevel optimization made easy: A simple first-order approach. Advances in Neural Information Processing Systems, 35:17248-17262, 2022.   \n[48] Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang. Stochastic auc maximization with deep neural networks. ICLR, 2020.   \n[49] Mingrui Liu, Zhenxun Zhuang, Yunwen Lei, and Chunyang Liao. A communication-efficient distributed gradient clipping algorithm for training deep neural networks. Advances in Neural Information Processing Systems, 35:26204-26217, 2022.   \n[50] Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton. In International Conference on Machine Learning, pages 6305-6315. PMLR, 2020.   \n[51]  Zijian Liu, Srikanth Jagabathula, and Zhengyuan Zhou. Near-optimal non-convex stochastic optimization under generalized smoothness. arXiv preprint arXiv:2302.06032, 2023.   \n[52] Liam Madden, Stephen Becker, and Emiliano Dall' Anese. Bounds for the tracking error of first-order online optimization methods. Journal of Optimization Theory and Applications, 189: 437-457, 2021.   \n[53]  Yuri Nesterov. A method of solving a convex programming problem with convergence rate 0 (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372-376, 1983.   \n[54] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems (NeurIPS), pages 113-124, 2019.   \n[55]  Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced clipping for non-convex optimization. arXiv preprint arXiv:2303.00883, 2023.   \n[56] Shoham Sabach and Shimrit Shtern. A frst ordermethod for solving convex bilevel optimization problems. SIAM Journal on Optimization, 27(2):640-660, 2017.   \n[57]  Han Shen and Tianyi Chen. On penalty-based bilevel gradient descent method. arXiv preprint arXiv:2302.05185, 2023.   \n[58]  Daouda Sow, Kaiyi Ji, Ziwei Guan, and Yingbin Liang. A constrained optimization approach to bilevel optimization with multiple inner minima. arXiv preprint arXiv:2203.01123, 2022.   \n[59]  Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920, 2019.   \n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017.   \n[61] Sharan Vaswani, Benjamin Dubois-Taine, and Reza Babanezhad. Towards noise-adaptive, problem-adaptive (accelerated) stochastic gradient descent. In International conference on machine learning, pages 22015-22059. PMLR, 2022.   \n[62] Luis Vicente, Gilles Savard, and Joaquim Judice. Descent approaches for quadratic bilevel programming. Journal of optimization theory and applications, 81(2):379-399, 1994.   \n[63] Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen. Convergence of adagrad for non-convex objectives: Simple proofs and relaxed assumptions. In The Thirty Sixth Annual Conference on Learning Theory, pages 161-190. PMLR, 2023.   \n[64]  Douglas J White and G Anandalingam. A penalty function approach for solving bi-level linear programs. Journal of Global Optimization, 3:397-419, 1993.   \n[65]  Craig Wilson, Venugopal V Veeravalli, and Angelia Nedic. Adaptive sequential stochastic optimization. IEEE Transactions on Automatic Control, 64(2):496-509, 2018.   \n[66]  Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. Advances in Neural Information Processing Systems, 34:13670-13682, 2021.   \n[67]  Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online auc maximization. In Advances in Neural Information Processing Systems, pages 451-459, 2016.   \n[68] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3040-3049, 2021.   \n[69]  Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms for non-convex optimization. Advances in Neural Information Processing Systems, 33:15511- 15521,2020.   \n[70] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. International Conference on Learning Representations, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Technical Lemmas ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we will introduce a few useful lemmas. The following technical lemma on recursive control is crucial for providing high probability guarantee of the lower-level variables $y_{t}$ and $\\hat{y}_{t}$ in Algorithm 2 at anytime. We follow a similar argument as in [18, Proposition 29] with a slight generalization. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1 (Recursive control on MGF). Consider scalar stochastic processes $(V_{t}),\\,(V_{t,1}^{\\prime}),\\,(V_{t,2}^{\\prime}),$ $(D_{t,1}),\\,(D_{t,2})$ and $\\left(X_{t}\\right)$ on a probability spacewith filtration $\\left(\\mathcal{H}_{t}\\right)$ which are linked by theinequality ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{t+1}\\leq\\alpha_{t}V_{t}+D_{t,1}\\sqrt{V_{t,1}^{\\prime}}+D_{t,2}\\sqrt{V_{t,2}^{\\prime}}+X_{t}+\\kappa_{t}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some deterministic constants $\\alpha_{t}\\in(-\\infty,1]$ and $\\kappa_{t}\\in\\mathbb{R}$ . Suppose the following properties hold. ", "page_idx": 14}, {"type": "text", "text": "\u00b7 $V_{t},V_{t,1}^{\\prime}$ and $V_{t,2}^{\\prime}$ are non-negative and $\\mathcal{H}_{t}$ measurable. $D_{t,i}$ is mean-zero sub-Gaussian conditioned on $\\mathcal{H}_{t}$ with deterministic parameter $\\sigma_{i}$ ,and $V_{t,i}^{\\prime}\\leq V_{t}$ for $i=1,2$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb E[\\exp(\\lambda D_{t,i})\\mid\\mathcal{H}_{t}]\\leq\\exp(\\lambda^{2}\\sigma_{i}^{2}/2)\\quad f o r\\,a l l\\quad\\lambda\\in\\mathbb R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$X_{t}$ is non-negative and sub-exponential conditioned on $\\mathcal{H}_{t}$ with deterministic parameter $\\nu_{t}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\exp(\\lambda X_{t})\\mid\\mathcal{H}_{t}]\\le\\exp(\\lambda\\nu_{t})\\quad f o r\\,a l l\\quad0\\le\\lambda\\le1/\\nu_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then the estimate ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\exp(\\lambda V_{t+1})]\\leq\\exp(\\lambda(\\nu_{t}+\\kappa_{t}))\\mathbb{E}[\\exp(\\lambda(1+\\alpha_{t})V_{t}/2)]}\\\\ &{s f\\!y i n g\\;0\\leq\\lambda\\leq\\operatorname*{min}\\Big\\{\\frac{1-\\alpha_{t}}{2(\\sigma_{1}^{2}+\\sigma_{2}^{2})},\\frac{1}{2\\nu_{t}}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "holds for any $\\lambda$ sati ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.1. For any index $t\\geq0$ and any scalar $\\lambda\\geq0$ , the law of total expectation implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{z}}[\\exp(\\lambda V_{t+1})]\\leq\\mathbb{E}\\left[\\exp(\\lambda(\\alpha_{t}V_{t}+D_{t,1}\\sqrt{V_{t,1}^{\\prime}}+D_{t,2}\\sqrt{V_{t,2}^{\\prime}}+X_{t}+\\kappa_{t}))\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\exp(\\lambda\\kappa_{t})\\mathbb{E}\\left[\\exp(\\lambda\\alpha_{t}V_{t})\\mathbb{E}\\left[\\exp\\left(\\lambda\\left(D_{t,1}\\sqrt{V_{t,1}^{\\prime}}+D_{t,2}\\sqrt{V_{t,2}^{\\prime}}\\right)\\right)\\exp(\\lambda X_{t})\\mid\\mathcal{H}_{t}\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Holder's inequality in turn yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp(\\lambda\\alpha_{t}V_{t})\\mathbb{E}\\left[\\exp\\left(\\lambda\\left(D_{t,1}\\sqrt{V_{t,1}^{\\prime}}+D_{t,2}\\sqrt{V_{t,2}^{\\prime}}\\right)\\right)\\exp(\\lambda X_{t})\\mid\\mathcal{H}_{t}\\right]\\right]}\\\\ &{\\qquad\\le\\sqrt{\\mathbb{E}\\left[\\exp\\left(2\\lambda\\left(D_{t,1}\\sqrt{V_{t,1}^{\\prime}}+D_{t,2}\\sqrt{V_{t,2}^{\\prime}}\\right)\\right)\\mid\\mathcal{H}_{t}\\right]\\cdot\\mathbb{E}\\left[\\exp(2\\lambda X_{t})\\mid\\mathcal{H}_{t}\\right]}}\\\\ &{\\qquad\\le\\sqrt{\\exp\\left(2\\lambda^{2}(\\sigma_{i}^{2}V_{t,1}^{\\prime}+\\sigma_{i}^{2}V_{t,2}^{\\prime})\\right)\\exp(2\\lambda\\nu_{t})}}\\\\ &{\\qquad\\le\\exp\\left(\\lambda^{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})V_{t}\\right)\\exp(\\lambda\\nu_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "provided $0\\le\\lambda\\le1/2\\nu_{t}$ , where we use $V_{t,i}^{\\prime}\\leq V_{t}$ for $i=1,2$ in the last inequality. Therefore, under the condition that ", "page_idx": 14}, {"type": "equation", "text": "$$\n0\\leq\\lambda\\leq\\operatorname*{min}\\left\\{\\frac{1-\\alpha_{t}}{2(\\sigma_{1}^{2}+\\sigma_{2}^{2})},\\frac{1}{2\\nu_{t}}\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "the following estimate hold for all $t\\geq0$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\exp(\\lambda V_{t+1})]\\leq\\exp(\\lambda\\kappa_{t})\\mathbb{E}\\left[\\exp(\\lambda\\alpha_{t}V_{t})\\exp\\left(\\lambda^{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})V_{t}\\right)\\exp(\\lambda\\nu_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\exp(\\lambda(\\nu_{t}+\\kappa_{t}))\\mathbb{E}\\left[\\exp\\left(\\lambda(\\alpha_{t}+\\lambda(\\sigma_{1}^{2}+\\sigma_{2}^{2}))V_{t}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp(\\lambda(\\nu_{t}+\\kappa_{t}))\\mathbb{E}[\\exp(\\lambda(1+\\alpha_{t})V_{t}/2)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality follows by the given range of $\\lambda$ . Thus the proof is completed. ", "page_idx": 14}, {"type": "text", "text": "Next, we introduce the following Young's inequality beyond Euclidean norm cases. This lemma serves as an important role when dealing with distributional drift for high probability SNAG analysis. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.2 (Young's inequality). For any vectors $v_{1},v_{2}\\in\\mathbb{R}^{d}$ positivesemidefinite(PsD)matrix $\\mathbf{Q}\\in\\mathbb{R}^{d\\times d}$ andscalar $c>0$ it holds that3 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|v_{1}+v_{2}\\|_{\\mathbf{Q}}^{2}\\leq(1+c)\\|v_{1}\\|_{\\mathbf{Q}}^{2}+\\left(1+\\frac{1}{c}\\right)\\|v_{2}\\|_{\\mathbf{Q}}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.2. By definition of $||\\cdot||_{\\mathbf{Q}}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|v_{1}+v_{2}\\|_{\\mathbf{Q}}^{2}=(v_{1}+v_{2})^{\\top}\\mathbf{Q}(v_{1}+v_{2})\\quad\\quad}\\\\ {=\\|v_{1}\\|_{\\mathbf{Q}}^{2}+\\|v_{2}\\|_{\\mathbf{Q}}^{2}+2v_{1}^{\\top}\\mathbf{Q}v_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\mathbf{Q}\\in\\mathbb{R}^{d\\times d}$ is PSD, let $\\mathbf{Q}=\\mathbf{U}\\mathbf{U}^{\\top}$ be the Cholesky decomposition, then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2v_{1\\ }^{\\top}\\mathbf{Q}v_{2}=2v_{1\\ }^{\\top}\\mathbf{U}\\mathbf{U}^{\\top}v_{2}=2(\\mathbf{U}^{\\top}v_{1})^{\\top}(\\mathbf{U}^{\\top}v_{2})}\\\\ {\\displaystyle\\leq c\\|\\mathbf{U}^{\\top}v_{1}\\|^{2}+\\displaystyle\\frac{1}{c}\\|\\mathbf{U}^{\\top}v_{2}\\|^{2}}\\\\ {\\displaystyle=c\\|v_{1}\\|_{\\mathbf{Q}}^{2}+\\displaystyle\\frac{1}{c}\\|v_{2}\\|_{\\mathbf{Q}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we use Young's inequality and definition of $\\|\\cdot\\|_{\\mathbf{Q}}$ for the second and third lines, respectively. Combing (6) and (7) gives the result as claimed. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B  Auxiliary Lemmas for Bilevel Optimization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide important properties of the objective function $\\Phi$ in bilevel optimization problems, as well as characterizations (such as variance and bias) for stochastic hypergradient estimator $\\bar{\\nabla}f(x,y;\\bar{\\xi})$ based on Neumann series. For readers\u2019 convenience, we only list the results here and defer the detailed proofs to Appendix F. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1 (Lipschitz property, [35, Lemma 8]). Under Assumptions 3.1 and 3.2, $y^{*}(x)$ is $(l_{g,1}/\\mu)$ Lipschitzcontinuous. ", "page_idx": 15}, {"type": "text", "text": "LemmaB.2 $\\left(L_{0},L_{1}\\right)$ -smoothness, [35, Lemma 9]). Under Assumptions 3.1 and 3.2, for any $x,x^{\\prime}$ wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\Phi(x)-\\nabla\\Phi(x^{\\prime})\\|\\leq(L_{0}+L_{1}\\|\\nabla\\Phi(x^{\\prime})\\|)\\|x-x^{\\prime}\\|\\quad i f\\quad\\|x-x^{\\prime}\\|\\leq\\frac{1}{\\sqrt{2(1+l_{g,1}^{2}/\\mu^{2})(L_{x,1}^{2}+L_{y,1}^{2})}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\left(L_{0},L_{1}\\right)$ -smoothness constant $L_{0}$ and $L_{1}$ are defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\dot{\\overline{{\\mathbf{\\xi}}}}_{0}=\\sqrt{1+\\frac{l_{g,1}^{2}}{\\mu^{2}}}\\left(L_{x,0}+L_{x,1}\\frac{l_{g,1}l_{f,0}}{\\mu}+\\frac{l_{g,1}}{\\mu}(L_{y,0}+L_{y,1}l_{f,0})+l_{f,0}\\frac{l_{g,1}l_{g,2}+l_{g,2}\\mu}{\\mu^{2}}\\right)\\quad a n d\\quad L_{1}=L_{x,1}-L_{x,1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.3 (Descent inequality, [35, Lemma 10]). Suppose Assumptions 3.1 and 3.2 and 3.2 hold. Thenforany $x,x^{\\prime}$ wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathfrak{H}(x)\\leq\\Phi(x^{\\prime})+\\langle\\nabla\\Phi(x^{\\prime}),x-x^{\\prime}\\rangle+\\frac{L_{0}+L_{1}\\|\\nabla\\Phi(x^{\\prime})\\|}{2}\\|x-x^{\\prime}\\|^{2}\\quad i f\\quad\\|x-x^{\\prime}\\|\\leq\\frac{1}{\\sqrt{2(1+l_{g,1}^{2}/\\mu^{2})(L_{1}+l_{g,1}^{2}/\\mu^{2})}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.4 ([41, Lemma B.1]). Under Assumptions 3.1 to 3.4, the bias of the stochastic hypergradient estimateof theupper-level objectivesatisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\bar{\\nabla}f(x,y)-\\mathbb{E}_{\\bar{\\xi}}[\\bar{\\nabla}f(x,y;\\bar{\\xi})]\\|\\le\\frac{l_{g,1}l_{f,0}}{\\mu}\\left(1-\\frac{\\mu}{l_{g,1}}\\right)^{Q},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $Q$ is the number of samples chosen to approximate the Hessian inverse. Moreover, we have ", "page_idx": 15}, {"type": "text", "text": "\u6b63 $\\mathbb{\\bar{z}}_{\\bar{\\xi}}[\\|\\bar{\\nabla}f(x,y;\\bar{\\xi})-\\mathbb{E}_{\\bar{\\xi}}[\\bar{\\nabla}f(x,y;\\bar{\\xi})]\\|^{2}]\\le\\sigma_{f,1}^{2}+\\frac{3}{\\mu^{2}}\\left[(\\sigma_{f,1}^{2}+l_{f,0}^{2})(\\sigma_{g,2}^{2}+2l_{g,1}^{2})+\\sigma_{f,1}^{2}l_{g,1}^{2}\\right]:=\\bar{\\sigma}^{2}.$ 'Here we define $\\|v\\|_{\\mathbf{Q}}:=\\sqrt{v^{\\top}\\mathbf{Q}v}$ for any vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ and PSD matrix $\\mathbf{Q}\\in\\mathbb{R}^{d\\times d}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma B.5. Under Assumptions 3.1 to 3.4, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\bar{\\nabla}f(x,y)-\\nabla\\Phi(x)\\|\\leq(\\bar{L}+L_{x,1}\\|\\nabla\\Phi(x)\\|)\\|y-y^{*}(x)\\|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where constant $\\bar{L}$ is defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{L}:=L_{x,0}+L_{x,1}\\frac{l_{g,1}l_{f,0}}{\\mu}+\\frac{l_{g,1}}{\\mu}(L_{y,0}+L_{y,1}l_{f,0})+l_{f,0}\\frac{\\mu l_{g,2}+l_{g,1}l_{g,2}}{\\mu^{2}}\\le L_{0}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.6. Under Assumptions 3.1 to 3.4, we have ", "page_idx": 16}, {"type": "text", "text": "(i) For any fixed $\\boldsymbol{y}\\in\\mathbb{R}^{d_{\\boldsymbol{y}}}$ and any $x,x^{\\prime}\\in\\mathbb{R}^{d_{x}}$ \uff0c ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\bar{\\xi}}\\|\\bar{\\nabla}f(x,y;\\bar{\\xi})-\\bar{\\nabla}f(x^{\\prime},y;\\bar{\\xi})\\|^{2}\\leq(\\bar{L}_{0}^{2}+\\bar{L}_{1}^{2}\\|\\nabla\\Phi(x)\\|^{2})\\|x-x^{\\prime}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(ii) For any fixed $x\\in\\mathbb{R}^{d_{x}}$ and any $y,y^{\\prime}\\in\\mathbb{R}^{d_{y}}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\bar{\\xi}}\\|\\bar{\\nabla}f(x,y;\\bar{\\xi})-\\bar{\\nabla}f(x,y^{\\prime};\\bar{\\xi})\\|^{2}\\leq(\\bar{L}_{0}^{2}+\\bar{L}_{1}^{2}\\|\\nabla\\Phi(x)\\|^{2})\\|y-y^{\\prime}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the above expressions, we define $\\bar{L}_{0}$ and $\\bar{L}_{1}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{L}_{0}=\\left\\{4\\left(L_{x,0}+L_{x,1}\\left(\\frac{l_{g,1}l_{f,0}}{\\mu}+\\left(L_{x,0}+\\frac{L_{x,1}l_{g,1}l_{f,0}}{\\mu}\\right)\\|y-y^{*}(x)\\|\\right)\\right)^{2}\\right.}\\\\ {\\displaystyle\\left.+\\frac{6Q}{2\\mu l_{g,1}-\\mu^{2}}\\left(l_{g,1}^{2}(L_{y,0}+L_{y,1}l_{f,0})^{2}+l_{f,0}^{2}l_{g,2}^{2}+\\frac{l_{f,0}^{2}l_{g,1}^{2}l_{g,2}^{2}Q^{2}}{(l_{g,1}-\\mu)^{2}}\\right)\\right\\}^{1/2},}\\\\ {\\displaystyle\\bar{L}_{1}=2L_{x,1}(1+L_{x,1}\\|y-y^{*}(x)\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that in Lemma B.6, constant $\\bar{L}_{0}$ depends on the value of $\\|y-y^{*}(x)\\|$ . When we consider this term in Algorithm 2, it turns into $\\lVert y_{t}-y_{t}^{*}\\rVert$ or $\\|\\hat{y}_{t}-y_{t}^{*}\\|$ , which are both as small as $O(\\epsilon)$ (and thus bounded) with high probability by Lemmas 4.5 to 4.7. In other words, we can treat this term as another constant for our algorithm and analysis. ", "page_idx": 16}, {"type": "text", "text": "C Proofs of Results in Section 4.3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For convenience, we will restate a few concepts included in Section 4.3.1 here. We consider the sequences of stochastic optimization problems ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}\\phi_{t}(w)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "indexed by time $t\\in\\mathbb{N}$ .We denote the minimizer and the minimal value of $\\phi_{t}$ as $\\boldsymbol{w}_{t}^{*}$ and $\\phi_{t}^{*}$ , and we define the minimizer drift at time $t$ to be $\\Delta_{t}:=\\lVert w_{t}^{*}-w_{t+1}^{*}\\rVert$ . With a slight abuse of notation, we consider the SNAG algorithm applied to the sequence $\\{\\phi_{t}\\}_{t=1}^{T}$ , where $T$ is the total number of iterations: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=w_{t}+\\gamma(w_{t}-w_{t-1})\\qquad}\\\\ {w_{t+1}=w_{t}+\\gamma(w_{t}-w_{t-1})-\\alpha g_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $g_{t}=\\nabla\\phi_{t}(z_{t};\\xi_{t})$ is the stochastic gradient evaluated at $z_{t}$ with random sample $\\xi_{t}$ . Define $\\varepsilon_{t}=g_{t}-\\nabla\\phi_{t}(z_{t})$ as the stochastic gradient noise at $t$ -th iteration. Define $\\mathcal{H}_{t}=\\sigma(\\xi_{1},.~.~.~,\\xi_{t-1})$ as the filtration, which is the $\\sigma$ -algebra generated by all random variables until $t$ -th iteration. We will make the following standard assumption, as illustrated below 4. ", "page_idx": 16}, {"type": "text", "text": "Assumption C.1. The sequences of time-varying functions satisfy that, each function $\\phi_{t}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $\\mu$ -stronglyconvexand $L$ -smoothforsomeconstants $\\mu,L>0$ ", "page_idx": 16}, {"type": "text", "text": "Assumption C.2 (Sub-Gaussian drift and noise). There exists constants $\\Delta,\\sigma>0$ suchthatthe followingholdsfor all $t\\geq0$ ", "page_idx": 16}, {"type": "text", "text": "(i) (Drift) The drift $\\Delta_{t}^{2}$ is sub-exponential conditioned on $\\mathcal{H}_{t}$ with parameter $\\Delta^{2}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\exp(\\lambda\\Delta_{t}^{2})\\mid\\mathcal{H}_{t}\\right]\\leq\\exp(\\lambda\\Delta^{2})\\quad f o r\\,a l l\\quad0\\leq\\lambda\\leq\\Delta^{-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "4Note that Assumptions C.1 and C.2 are more concrete than that in Section 4.3.1. ", "page_idx": 16}, {"type": "text", "text": "(ii)(Noise) The noise $\\varepsilon_{t}$ is norm sub-Gaussian conditioned on $\\mathcal{H}_{t}$ withparameter $\\sigma/2$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left\\{\\|\\varepsilon_{t}\\|\\geq\\varrho\\;|\\;\\mathcal{H}_{t}\\right\\}\\leq2\\exp(-2\\varrho^{2}/\\sigma^{2})\\quad f o r\\,a l l\\quad\\varrho>0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The following lemma characterize the one-step improvement for stochastic Nesterov accelerated gradient method. Although part of our analysis is similar to [12, 4], our final goal is quite different: we aim to derive a careful formulation (see (13)) such that we can apply Lemma A.1 to recursively control the moment generating function of $V_{t}$ with distributional drift, thus leading to a high probability bound for $V_{t}$ at anytime (see Lemma C.5), while [12, 4] only show the convergence in expectation without distributional drift. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.3 (Distance recursion, with drift). Suppose that Assumptions C.1 and C.2 hold. Let $\\{w_{t}\\}$ be the iterates produced by update rule (9) with constant learning rate $\\alpha\\leq1/2L$ andsetconstants $\\gamma,\\rho>0,$ andmatrix $\\mathbf{P}\\in\\mathbb{R}^{2d\\times2d}\\,.$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma=\\frac{1-\\sqrt{\\mu\\alpha}}{1+\\sqrt{\\mu\\alpha}},\\quad\\rho^{2}=1-\\sqrt{\\mu\\alpha},\\quad\\mathbf{P}=\\frac{1}{2\\alpha}\\left[\\sqrt{\\mu\\alpha}-1\\frac{\\sqrt{\\mu\\alpha}-1}{(1-\\sqrt{\\mu\\alpha})^{2}}\\right]\\otimes\\mathbf{I}_{d}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Define $\\theta_{t}=[(w_{t}-w_{t}^{*})^{\\top},(w_{t-1}-w_{t}^{*})]^{\\top}\\in\\mathbb{R}^{2d},$ also defne the potentialfunction nd $u_{t,1},u_{t,2}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{t}=\\theta_{t}^{\\top}\\mathbf{P}\\theta_{t}+\\phi_{t}(w_{t})-\\phi_{t}(w_{t}^{*}),\\qquad u_{t,1}=\\frac{w_{t}-w_{t}^{*}}{\\lVert w_{t}-w_{t}^{*}\\rVert},\\qquad u_{t,2}=\\frac{z_{t}-w_{t}}{\\lVert z_{t}-w_{t}\\rVert}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then for all $t\\geq0$ it holdsthat ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left[\\displaystyle w_{t+1}-w_{t}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t+1}-w_{t}^{*}\\right]+\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})}\\\\ {\\displaystyle\\left[w_{t}-w_{t}^{*}\\right]^{\\top}\\mathbf{P}\\left[\\displaystyle w_{t}-w_{t}^{*}\\right]+\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\rho^{2}V_{t}-\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Specifically, if $\\begin{array}{r}{^{\\cdot}\\phi_{t}(w):=\\frac{\\mu}{2}\\|w-w_{t}^{*}\\|^{2}}\\end{array}$ with $d=1$ then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{t+1}\\leq\\left(1-\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{2}\\right)V_{t}+\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left[-\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\sqrt{\\displaystyle\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\right.}}\\\\ {{\\left.\\qquad\\qquad-\\displaystyle\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\displaystyle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\displaystyle\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}\\right]+\\displaystyle\\frac{20\\mu\\Delta_{t}^{2}}{\\sqrt{\\mu\\alpha}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma C.3. We first apply Lemma A.2 with ", "page_idx": 17}, {"type": "equation", "text": "$$\nv_{1}+v_{2}=\\theta_{t+1}=\\left[\\!\\!\\begin{array}{c}{{w_{t+1}-w_{t+1}^{*}}}\\\\ {{w_{t}-w_{t+1}^{*}}}\\end{array}\\!\\!\\right],\\quad v_{1}=\\left[\\!\\!\\begin{array}{c}{{w_{t+1}-w_{t}^{*}}}\\\\ {{w_{t}-w_{t}^{*}}}\\end{array}\\!\\!\\right],\\quad v_{2}=\\left[\\!\\!\\begin{array}{c}{{w_{t}^{*}-w_{t+1}^{*}}}\\\\ {{w_{t}^{*}-w_{t+1}^{*}}}\\end{array}\\!\\!\\right],\\quad\\mathbf{Q}=\\mathbf{P}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "to obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\prime}_{t+1}=\\theta_{t+1}^{\\top}\\mathbf{P}\\theta_{t+1}+\\phi_{t+1}(w_{t})-\\phi_{t+1}(w_{t+1}^{*})}\\\\ &{\\leq\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left[w_{t+1}-w_{t}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t+1}-w_{t}^{*}\\right]+\\left(1+\\frac{4}{\\sqrt{\\mu\\alpha}}\\right)\\left[w_{t}^{*}-w_{t+1}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t}^{*}-w_{t+1}^{*}\\right]+\\phi_{t+1}(w_{t}^{*})}\\\\ &{=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left\\{\\left[w_{t}-w_{t}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t+1}-w_{t}^{*}\\right]+\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underbrace{\\cdot\\phi_{t+1}(w_{t+1})-\\phi_{t+1}(w_{t+1}^{*})-\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left(\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})\\right)}_{(B)}+\\underbrace{\\left(1+\\frac{4}{\\sqrt{\\mu\\alpha}}\\right)\\left[w_{t}^{*}-w_{t+1}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t}^{*}-w_{t+1}^{*}\\right]^{\\top}}_{(C)}=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we bound terms $(A)$ \uff0c $(B)$ and $(C)$ accordingly. ", "page_idx": 17}, {"type": "text", "text": "Bounding $(A)$ . Let us define vector $\\omega_{t}\\in\\mathbb{R}^{2d}$ and matrices $\\mathbf{A},\\mathbf{B}\\in\\mathbb{R}^{2d\\times2d}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\omega_{t}=\\left[\\nabla\\phi_{t}(z_{t})\\right],\\quad\\mathbf{A}=\\left[1+\\gamma\\quad-\\gamma\\right]\\otimes\\mathbf{I}_{d},\\quad\\mathbf{B}=\\left[\\!\\!\\begin{array}{c c}{-\\alpha}&{-\\alpha}\\\\ {0}&{0}\\end{array}\\!\\!\\right]\\otimes\\mathbf{I}_{d}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By (9) we have $\\theta_{t+1}=\\mathbf{A}\\theta_{t}+\\mathbf{B}\\omega_{t}$ . Since $\\phi_{t}$ is $\\mu$ -strongly convex, then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi_{t}(w_{t})-\\phi_{t}(z_{t})\\geq\\langle\\nabla\\phi_{t}(z_{t}),w_{t}-z_{t}\\rangle+\\frac{\\mu}{2}\\|w_{t}-z_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By $L$ -smoothness of $\\phi_{t}$ and the fact that $w_{t+1}=z_{t}-\\alpha g_{t}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{t}(z_{t})-\\phi_{t}(w_{t+1})\\geq\\langle\\nabla\\phi_{t}(z_{t}),z_{t}-w_{t+1}\\rangle-\\displaystyle\\frac{L}{2}\\|w_{t+1}-z_{t}\\|^{2}}\\\\ &{\\quad\\quad=\\alpha\\langle\\nabla\\phi_{t}(z_{t}),g_{t}\\rangle-\\displaystyle\\frac{L\\alpha^{2}}{2}\\|g_{t}\\|^{2}}\\\\ &{\\quad\\quad=\\alpha\\|\\nabla\\phi_{t}(z_{t})\\|^{2}+\\alpha\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle-\\displaystyle\\frac{L\\alpha^{2}}{2}(\\|\\nabla\\phi_{t}(z_{t})\\|^{2}+2\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle+\\|\\varepsilon_{t}\\|^{2})}\\\\ &{\\quad\\quad=\\frac{\\alpha}{2}(2-L\\alpha)\\|\\nabla\\phi_{t}(z_{t})\\|^{2}-\\displaystyle\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}+\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use $g_{t}=\\nabla\\phi_{t}(z_{t})+\\varepsilon_{t}$ in the second equality. Noting that by (9) we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{t}-z_{t}=-\\gamma(w_{t}-w_{t}^{*})+\\gamma(w_{t-1}-w_{t}^{*}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and combining (14) and (15) we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{t}(w_{t})-\\phi_{t}(w_{t+1})\\geq\\langle\\nabla\\phi_{t}(z_{t}),w_{t}-z_{t}\\rangle+\\displaystyle\\frac{\\mu}{2}\\|w_{t}-z_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\alpha}{2}(2-L\\alpha)\\|\\nabla\\phi_{t}(z_{t})\\|^{2}-\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}+\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\left[\\omega_{t}\\right]^{\\top}\\mathbf{X}_{1}\\left[\\omega_{t}\\right]-\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}+\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where matrix $\\mathbf{X}_{1}\\in\\mathbb{R}^{4d\\times4d}$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf X}_{1}=\\frac{1}{2}\\left[\\!\\!\\begin{array}{c c c c}{{\\mu\\gamma^{2}}}&{{-\\mu\\gamma^{2}}}&{{-\\gamma}}&{{0}}\\\\ {{-\\mu\\gamma^{2}}}&{{\\mu\\gamma^{2}}}&{{\\gamma}}&{{0}}\\\\ {{-\\gamma}}&{{\\gamma}}&{{\\alpha(2-L\\alpha)}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right]\\otimes{\\bf I}_{d}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then applying the strong convexity of $\\phi_{t}$ again gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi_{t}(w_{t}^{*})-\\phi_{t}(z_{t})\\geq\\langle\\nabla\\phi_{t}(z_{t}),w_{t}^{*}-z_{t}\\rangle+\\frac{\\mu}{2}\\|w_{t}^{*}-z_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Noting that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{w_{t}^{*}-z_{t}=(w_{t}^{*}-w_{t})-\\gamma(w_{t}-w_{t}^{*})+\\gamma(w_{t-1}-w_{t}^{*})}\\\\ {=-(1+\\gamma)(w_{t}-w_{t}^{*})+\\gamma(w_{t-1}-w_{t}^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and combining (15) and (17) we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{t}(w_{t}^{*})-\\phi_{t}(w_{t+1})\\ge\\langle\\nabla\\phi_{t}(z_{t}),w_{t}^{*}-z_{t}\\rangle+\\displaystyle\\frac{\\mu}{2}\\|w_{t}^{*}-z_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\alpha}{2}(2-L\\alpha)\\|\\nabla\\phi_{t}(z_{t})\\|^{2}-\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}+\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\left[\\omega_{t}\\right]^{\\top}\\mathbf{X}_{2}\\left[\\omega_{t}\\right]-\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}+\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where matrix $\\mathbf{X}_{2}\\in\\mathbb{R}^{4d\\times4d}$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf X}_{2}=\\frac{1}{2}\\left[\\!\\!\\begin{array}{c c c c}{\\mu(1+\\gamma)^{2}}&{-\\mu\\gamma(1+\\gamma)}&{-(1+\\gamma)}&{0}\\\\ {-\\mu\\gamma(1+\\gamma)}&{\\mu\\gamma^{2}}&{\\gamma}&{0}\\\\ {-(1+\\gamma)}&{\\gamma}&{\\alpha(2-L\\alpha)}&{0}\\\\ {0}&{0}&{0}&{0}\\end{array}\\!\\!\\right]\\otimes{\\bf I}_{d}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we multiply (16) by $\\rho^{2}$ and (18) by $1-\\rho^{2}$ , then sum them up to get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\rho^{2}(\\phi_{t}(w_{t})-\\phi_{t}(w_{t}^{*}))-(\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*}))}\\\\ {\\geq\\displaystyle\\left[\\omega_{t}\\right]^{\\top}(\\rho^{2}\\mathbf{X}_{1}+(1-\\rho^{2})\\mathbf{X}_{2})\\left[\\theta_{t}\\right]-\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}+\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With definition of $\\gamma,\\rho^{2}$ and $\\mathbf{P}$ given in the statement of the theorem, by direct calculation we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[\\mathbf{A}^{\\top}\\mathbf{P}\\mathbf{A}-\\rho^{2}\\mathbf{P}\\right.\\quad\\mathbf{A}^{\\top}\\mathbf{P}\\mathbf{B}\\right]-\\left(\\rho^{2}\\mathbf{X}_{1}+(1-\\rho^{2})\\mathbf{X}_{2}\\right)}\\\\ &{\\qquad\\qquad\\left.=\\left[\\begin{array}{c c c c}{-\\frac{\\mu(1-\\sqrt{\\mu\\alpha})^{3}}{2\\sqrt{\\mu\\alpha}(1+\\sqrt{\\mu\\alpha})}}&{\\frac{\\mu(1-\\sqrt{\\mu\\alpha})^{3}}{2\\sqrt{\\mu\\alpha}(1+\\sqrt{\\mu\\alpha})}}&{0}&{-\\frac{1+\\mu\\alpha}{2(1+\\sqrt{\\mu\\alpha})}}\\\\ {\\frac{\\mu(1-\\sqrt{\\mu\\alpha})^{3}}{2\\sqrt{\\mu\\alpha}(1+\\sqrt{\\mu\\alpha})}}&{-\\frac{\\mu(1-\\sqrt{\\mu\\alpha})^{3}}{2\\sqrt{\\mu\\alpha}(1+\\sqrt{\\mu\\alpha})}}&{0}&{\\frac{1-\\sqrt{\\mu\\alpha}}{2(1+\\sqrt{\\mu\\alpha})}}\\\\ {0}&{0}&{-\\frac{\\alpha(1-L\\alpha)}{2}}&{\\frac{\\alpha}{2}}\\\\ {-\\frac{1+\\mu\\alpha}{2(1+\\sqrt{\\mu\\alpha})}}&{\\frac{1-\\sqrt{\\mu\\alpha}}{2(1+\\sqrt{\\mu\\alpha})}}&{\\frac{\\alpha}{2}}&{\\frac{\\alpha}{2}}\\end{array}\\right]\\otimes\\mathbf{I}_{d}:=\\mathbf{C},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where matrix $\\mathbf{C}$ is negative semidefinite [12], i.e., $\\mathbf{C}\\preceq\\mathbf{0}$ . Then we use this fact to leverage ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[w_{t+1}-w_{t}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t+1}-w_{t}^{*}\\right]-\\rho^{2}\\theta_{t}^{\\top}\\mathbf{P}\\theta_{t}=\\left[\\theta_{t}\\right]^{\\top}\\left(\\rho^{2}\\mathbf{X}_{1}+(1-\\rho^{2})\\mathbf{X}_{2}+\\mathbf{C}\\right)\\left[\\theta_{t}\\right]}\\\\ &{\\qquad\\leq\\left[\\theta_{t}\\right]^{\\top}\\left(\\rho^{2}\\mathbf{X}_{1}+(1-\\rho^{2})\\mathbf{X}_{2}\\right)\\left[\\theta_{t}\\right]}\\\\ &{\\qquad\\leq-\\bigl(\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})\\bigr)+\\rho^{2}(\\phi_{t}(w_{t})-\\phi_{t}(w_{t}^{*}))-\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality follows by (19). Rearrange the above inequality and by definition of the potential function $V_{t}$ ,weobtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left[\\displaystyle w_{t+1}-w_{t}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t+1}-w_{t}^{*}\\right]+\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})}\\\\ {\\displaystyle\\left[w_{t}-w_{t}^{*}\\right]^{\\top}\\mathbf{P}\\left[\\displaystyle w_{t}-w_{t}^{*}\\right]+\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\rho^{2}V_{t}-\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now recallthat in (8) our objective function has the form of $\\begin{array}{r}{\\phi_{t}(w)=\\frac{\\mu}{2}\\|w-w_{t}^{*}\\|^{2}}\\end{array}$ ,hence $\\nabla\\phi_{t}(z_{t})=$ $\\mu(z_{t}-w_{t}^{*})$ . Plugging this into the above inequality gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[w_{t+1}-w_{t}^{*}\\right]^{\\top}{\\mathbf{P}}\\left[w_{t+1}-w_{t}^{*}\\right]+\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*})}\\\\ &{\\qquad\\times\\rho^{2}V_{t}-\\mu\\alpha(1-L\\alpha)\\langle z_{t}-w_{t}^{*},\\varepsilon_{t}\\rangle+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}}\\\\ &{\\qquad=\\rho^{2}V_{t}-\\mu\\alpha(1-L\\alpha)\\langle w_{t}-w_{t}^{*},\\varepsilon_{t}\\rangle-\\mu\\alpha(1-L\\alpha)\\langle z_{t}-w_{t},\\varepsilon_{t}\\rangle+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}}\\\\ &{\\qquad=\\rho^{2}V_{t}-\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\sqrt{\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|}\\\\ &{\\qquad\\qquad\\qquad-\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $u_{t,1}$ and $u_{t,2}$ are defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\nu_{t,1}=\\frac{w_{t}-w_{t}^{*}}{\\|w_{t}-w_{t}^{*}\\|},\\qquad u_{t,2}=\\frac{z_{t}-w_{t}}{\\|z_{t}-w_{t}\\|}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we conclude that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(A)\\leq\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left[\\rho^{2}V_{t}-\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\sqrt{\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.-\\displaystyle\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\displaystyle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\displaystyle\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}\\right]}\\\\ &{\\leq\\left(1-\\displaystyle\\frac{3\\sqrt{\\mu\\alpha}}{4}\\right)V_{t}+\\left[-\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\sqrt{\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.-\\displaystyle\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\displaystyle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\displaystyle\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from the definition of $\\rho$ and simple calculation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\rho^{2}=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left(1-\\sqrt{\\mu\\alpha}\\right)\\leq1-\\frac{3\\sqrt{\\mu\\alpha}}{4}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Bounding $(B)$ .Again, recall that under distributional drift, our objective function in (8) has the form of $\\begin{array}{r}{\\phi_{t}(\\dot{w})=\\frac{\\mu}{2}\\|w-w_{t}^{*}\\|^{2}}\\end{array}$ ,thenwe have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left(B\\right)=\\phi_{t+1}(w_{t+1})-\\phi_{t+1}(w_{t+1}^{*})-\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)(\\phi_{t}(w_{t+1})-\\phi_{t}(w_{t}^{*}))}}\\\\ &{\\leq\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)(\\phi_{t+1}(w_{t+1})-\\phi_{t+1}(w_{t+1}^{*})-\\phi_{t}(w_{t+1})+\\phi_{t}(w_{t}^{*}))}\\\\ &{=\\frac{\\mu}{2}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)(\\|w_{t+1}-w_{t+1}^{*}\\|^{2}-\\|w_{t+1}-w_{t}^{*}\\|^{2})}\\\\ &{=\\frac{\\mu}{2}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\|w_{t}^{*}-w_{t+1}^{*}\\|\\|w_{t+1}-w_{t}^{*}+w_{t+1}-w_{t+1}^{*}\\|}\\\\ &{\\leq\\frac{\\mu}{2}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\Delta_{t}(2\\|w_{t+1}-w_{t+1}^{*}\\|+\\|w_{t+1}^{*}-w_{t}^{*}\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\phi_{t+1}$ is $\\mu$ -strongly convex and matrix $\\mathbf{P}$ is PSD, then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t+1}=\\theta_{t+1}^{\\top}\\mathbf{P}\\theta_{t+1}+\\phi_{t+1}(w_{t+1})-\\phi_{t}(w_{t+1}^{*})\\geq\\phi_{t+1}(w_{t+1})-\\phi_{t}(w_{t+1}^{*})}\\\\ &{\\qquad\\geq\\frac{\\mu}{2}\\|w_{t+1}-w_{t+1}^{*}\\|^{2}\\qquad\\qquad\\Longrightarrow\\qquad\\qquad\\|w_{t+1}-w_{t+1}^{*}\\|\\leq\\sqrt{\\frac{2}{\\mu}}\\sqrt{V_{t+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging the above fact back into the upper bound for $(B)$ gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{(B)\\leq\\frac{\\mu}{2}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\Delta_{t}\\left(2\\sqrt{\\frac{2}{\\mu}}\\sqrt{V_{t+1}}+\\Delta_{t}\\right)}}}\\\\ {{\\displaystyle{\\qquad=\\sqrt{2\\mu}\\Delta_{t}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\sqrt{V_{t+1}}+\\frac{\\mu}{2}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\Delta_{t}^{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Bounding $(C)$ .For this part, we mainly handle the distributional drift. Recall that under this condition we consider one-dimensional case, that is, we let $d=1$ in (8) such that $w\\in\\mathbb{R}$ .Thenwe use $\\mathbf{P}\\in\\mathbb{R}^{2\\times2}$ to obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(C\\right)=\\left(1+\\frac{4}{\\sqrt{\\mu\\alpha}}\\right)\\left[w_{t}^{*}-w_{t+1}^{*}\\right]^{\\top}\\mathbf{P}\\left[w_{t}^{*}-w_{t+1}^{*}\\right]=\\frac{\\mu}{2}\\left(1+\\frac{4}{\\sqrt{\\mu\\alpha}}\\right)\\Delta_{t}^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the last equality we use the definition of $\\mathbf{P}$ and the fact that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{P}_{11}+\\mathbf{P}_{12}+\\mathbf{P}_{21}+\\mathbf{P}_{22}=\\frac{\\mu}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Final Bound for $V_{t+1}$ . Now we are ready to derive the upper bound for $V_{t+1}$ . Combining (22), (23) and (24) together yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{V_{t+1}\\leq(A)+(B)+(C)}\\\\ {\\leq\\left(1-\\displaystyle\\frac{3\\sqrt{\\mu\\alpha}}{4}\\right)V_{t}+\\left[-\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\sqrt{\\displaystyle\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\right.}\\\\ {\\quad\\left.-\\displaystyle\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\displaystyle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\displaystyle\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}\\right]}\\\\ {\\quad+\\sqrt{2\\mu}\\Delta_{t}\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\sqrt{V_{t+1}}+\\mu\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{8}+\\displaystyle\\frac{2}{\\sqrt{\\mu\\alpha}}\\right)\\Delta_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For simplicity, we define $D$ as the following ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\supset=\\left(1-\\frac{3\\sqrt{\\mu\\alpha}}{4}\\right)V_{t}+\\left[-\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\sqrt{\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\right.}\\\\ {\\displaystyle\\left.-\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}\\right]+\\mu\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{8}+\\frac{2}{\\sqrt{\\mu\\alpha}}\\right)L_{\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence (26) turns into ", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{t+1}-\\sqrt{2\\mu}\\Delta_{t}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\sqrt{V_{t+1}}-D\\le0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Solving the above inequality we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{V_{t+1}}\\leq\\frac{1}{2}\\left[\\sqrt{2\\mu}\\Delta_{t}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)+\\sqrt{\\left(\\sqrt{2\\mu}\\Delta_{t}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\right)^{2}+4D}\\right]}\\\\ &{\\qquad\\leq\\sqrt{2\\mu}\\Delta_{t}\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)+\\sqrt{D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then an application of Young's inequality reveals ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t+1}\\leq\\left(1+\\frac{\\sqrt{\\mu^{6}}}{4}\\right)D+\\left(1+\\frac{4}{\\sqrt{\\mu^{6}}}\\right)\\left(\\sqrt{2\\mu}\\Delta_{t}\\left(1+\\frac{\\sqrt{\\mu^{6}}}{4}\\right)\\right)^{2}}\\\\ &{\\qquad=\\left(1+\\frac{\\sqrt{6\\pi}}{4}\\right)\\left\\{\\left(1-\\frac{3\\sqrt{\\mu^{6}}}{4}\\right)V_{t}+\\left[-\\sqrt{2\\mu}\\alpha_{1}(1-L\\alpha)\\langle\\alpha_{1},\\varepsilon_{1}\\rangle\\sqrt{\\frac{\\mu}{2}}\\right]|w_{t}-w_{t}^{*}|\\right.}\\\\ &{\\qquad\\qquad-\\frac{2\\mu\\sqrt{2\\pi}}{1+\\sqrt{2\\mu}\\alpha}(1-L\\alpha)\\langle u_{1},2,\\varepsilon_{1}\\rangle\\frac{1+\\sqrt{\\mu^{6}}}{2\\sqrt{2\\alpha}}|z_{t}-w_{t}|\\right\\}+\\frac{L\\alpha^{2}}{2}|z_{t}|^{2}\\Bigg\\}+\\mu\\left(1+\\frac{\\sqrt{\\mu^{6}}}{8}+\\frac{\\sqrt{\\mu^{6}}}{8}\\right)}\\\\ &{\\qquad\\qquad+\\left(1+\\frac{4}{\\sqrt{\\mu^{6}}}\\right)2\\mu\\Delta_{t}^{2}\\Bigg(1+\\frac{\\sqrt{\\mu^{6}}}{4}\\Bigg)^{2}}\\\\ &{\\qquad\\leq\\left(1-\\frac{\\sqrt{\\mu^{6}\\pi}}{2}\\right)V_{t}+\\left(1+\\frac{\\sqrt{\\mu^{6}}}{4}\\right)\\left[-\\sqrt{2\\mu}\\alpha_{1}(1-L\\alpha)\\langle u_{1},\\varepsilon_{1}\\rangle\\sqrt{\\frac{\\mu}{2}}\\right]|w_{t}-w_{t}^{*}|\\right.}\\\\ &{\\qquad\\qquad-\\frac{2\\mu\\sqrt{2\\pi}\\alpha}{1+\\sqrt{\\mu^{6}\\pi}}(1-L\\alpha)\\langle u_{1},2,\\varepsilon_{1}\\rangle\\frac{1+\\sqrt{\\mu^{6}}}{2\\sqrt{2\\alpha}}|z_{t}-w_{t}|\\Big[\\frac{L\\alpha}{2}\\Big]|\\varepsilon_{t}|^{2}\\Bigg\\}}\\\\ &{\\qquad\\qquad+\\mu\\left(1+\\frac{\\sqrt{\\mu^{6}\\pi}}{8}\\right)\\left(1+\\sqrt{\\frac{\\mu\\alpha}{8}}+\\frac{\\sqrt{\\mu\\alpha}}{2\\sqrt{\\mu}}\\right)\\Delta_{t}^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we plug in the definition of $D$ for the first equality. Since the learning rate $\\alpha\\leq1/L$ and thus $\\mu\\alpha\\leq1$ , then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{8}+\\frac{2}{\\sqrt{\\mu\\alpha}}\\right)\\leq\\frac{125}{32\\sqrt{\\mu\\alpha}},\\qquad2\\left(1+\\frac{4}{\\sqrt{\\mu\\alpha}}\\right)\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{2}\\leq\\frac{125}{8\\sqrt{\\mu\\alpha}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we finally conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{t+1}\\leq\\left(1-\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{2}\\right)V_{t}+\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left[-\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\sqrt{\\displaystyle\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\right.}}\\\\ {{\\left.\\qquad\\qquad-\\displaystyle\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\displaystyle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\displaystyle\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}\\right]+\\displaystyle\\frac{20\\mu\\Delta_{t}^{2}}{\\sqrt{\\mu\\alpha}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is as claimed in (13). ", "page_idx": 22}, {"type": "text", "text": "When there is no drift, the following lemma holds for any general functions $\\phi$ and input dimension $d$ Lemma C.4 (Distance recursion, without drift). Under the same settings as in Lemma C.3 with $\\phi_{t}(w)\\equiv\\phi(w)$ $w\\in\\mathbb{R}^{d}$ ,and $w_{t}^{*}\\equiv w^{*}$ where $\\phi(w)$ canbe any generalfunctions and $d$ can be any dimension.Weredefine $u_{t,1},u_{t,2}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\nu_{t,1}=\\frac{\\nabla\\phi(w_{t})-\\nabla\\phi(w^{*})}{\\|\\nabla\\phi(w_{t})-\\nabla\\phi(w^{*})\\|},\\qquad u_{t,2}=\\frac{\\nabla\\phi(z_{t})-\\nabla\\phi(w_{t})}{\\|\\nabla\\phi(z_{t})-\\nabla\\phi(w_{t})\\|}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then for all $t\\geq0$ it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{t+1}\\leq(1-\\sqrt{\\mu\\alpha})V_{t}-\\sqrt{\\displaystyle\\frac{2}{\\mu}}L\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\displaystyle\\frac{1}{L}\\sqrt{\\displaystyle\\frac{\\mu}{2}}\\|\\nabla\\phi_{t}(w_{t})-\\nabla\\phi_{t}(w^{*})\\|}}\\\\ {{\\displaystyle~~~~~~~~~~~~~-\\frac{2L\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\displaystyle\\frac{1+\\sqrt{\\mu\\alpha}}{2L\\sqrt{2\\alpha}}\\|\\nabla\\phi_{t}(z_{t})-\\nabla\\phi_{t}(w_{t})\\|+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma C.4. By (12) in Lemma C.3 with $\\phi_{t}(w)\\equiv\\phi(w)$ and $w_{t}^{*}\\equiv w^{*}$ ,wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\l_{t+1}\\leq\\rho^{2}V_{t}-\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t}),\\varepsilon_{t}\\rangle+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}}\\\\ {\\displaystyle\\qquad=\\rho^{2}V_{t}-\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(w_{t})-\\nabla\\phi_{t}(w^{*}),\\varepsilon_{t}\\rangle-\\alpha(1-L\\alpha)\\langle\\nabla\\phi_{t}(z_{t})-\\nabla\\phi_{t}(w_{t}),\\varepsilon_{t}\\rangle+\\frac{L\\alpha^{2}}{2}}\\\\ {\\displaystyle\\qquad=(1-\\sqrt{\\mu\\alpha})V_{t}-\\sqrt{\\frac{2}{\\mu}}L\\alpha(1-L\\alpha)\\langle u_{t,1},\\varepsilon_{t}\\rangle\\frac{1}{L}\\sqrt{\\frac{\\mu}{2}}\\|\\nabla\\phi_{t}(w_{t})-\\nabla\\phi_{t}(w^{*})\\|}\\\\ {\\displaystyle\\qquad-\\,\\frac{2L\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},\\varepsilon_{t}\\rangle\\frac{1+\\sqrt{\\mu\\alpha}}{2L\\sqrt{2\\alpha}}\\|\\nabla\\phi_{t}(z_{t})-\\nabla\\phi_{t}(w_{t})\\|+\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence the proof is completed. ", "page_idx": 22}, {"type": "text", "text": "The following result shows the first part of Lemma 4.3. To the best of our knowledge, this is the first high probability guarantee with improved rate for SNAG under distributional drift. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5 (High-probability distance tracking, with drift). Under the same setting as in Lemma C.3 with $\\alpha\\leq\\operatorname*{min}\\lbrace\\bar{1}/2\\bar{L},\\mu/2L^{2};1/10^{3}\\mu\\rbrace$ for any given $\\delta\\in(0,1)$ and all $t\\in[T],$ the following holds withprobabilityatleast $1-\\delta$ over the randomness in $\\mathcal{H}_{t}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nV_{t}\\leq\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}V_{0}+\\left(2\\alpha\\sigma^{2}+\\frac{80\\Delta^{2}}{\\alpha}\\right)\\ln\\frac{e T}{\\delta}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma C.5. We will invoke Lemma A.1 to show the results. To apply Lemma A.1, we first need to show the following two facts: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Fact}\\left(\\mathrm{I}\\right):\\quad\\sqrt{\\frac{\\mu}{2}}\\left\\|\\boldsymbol{w}_{t}-\\boldsymbol{w}_{t}^{*}\\right\\|\\leq\\sqrt{V_{t}}\\qquad\\&\\qquad\\mathrm{Fact}\\left(\\mathrm{II}\\right):\\quad\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{w}_{t}\\right\\|\\leq\\sqrt{V_{t}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Fact (I) verification. Since $\\phi_{t}$ is $\\mu$ -strongly convex and matrix $\\mathbf{P}$ is PSD, then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{V_{t}=\\theta_{t}^{\\top}\\mathbf{P}\\theta_{t}+\\phi_{t}(w_{t})-\\phi_{t}(w_{t}^{*})\\geq\\phi_{t}(w_{t})-\\phi_{t}(w_{t}^{*})}\\\\ {\\quad\\geq\\displaystyle\\frac{\\mu}{2}\\|w_{t}-w_{t}^{*}\\|^{2}\\qquad\\implies\\qquad\\quad\\sqrt{\\displaystyle\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\leq\\sqrt{V_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Fact $(\\mathbf{II})$ verification. By definition of matrix $\\mathbf{P}$ and simple calculation we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{V_{t}}\\geq\\sqrt{\\theta_{t}^{\\top}\\mathbf{P}\\theta_{t}}=\\sqrt{\\frac{1}{2\\alpha}}\\|(w_{t}-w_{t}^{*})+(\\sqrt{\\mu\\alpha}-1)(w_{t-1}-w_{t}^{*})\\|}\\\\ &{\\qquad=\\sqrt{\\frac{1}{2\\alpha}}\\|(1-\\sqrt{\\mu\\alpha})(w_{t}-w_{t-1})+\\sqrt{\\mu\\alpha}(w_{t}-w_{t}^{*})\\|}\\\\ &{\\qquad\\geq\\sqrt{\\frac{1}{2\\alpha}}(1-\\sqrt{\\mu\\alpha})\\|w_{t}-w_{t-1}\\|-\\sqrt{\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|}\\\\ &{\\qquad\\geq\\sqrt{\\frac{1}{2\\alpha}}(1-\\sqrt{\\mu\\alpha})\\|w_{t}-w_{t-1}\\|-\\sqrt{V_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearrange the above inequality, and recall the update rule of stochastic Nesterov accelerated gradient method,we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|z_{t}-w_{t}\\|=\\gamma\\|w_{t}-w_{t-1}\\|\\leq\\frac{2\\gamma\\sqrt{2\\alpha}}{1-\\sqrt{\\mu\\alpha}}\\sqrt{V_{t}}=\\frac{2\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\sqrt{V_{t}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where for the last equality we use the definition of $\\gamma$ as in (10). Rearrange it gives (30). ", "page_idx": 23}, {"type": "text", "text": "By Lemma C.3 we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{t+1}\\leq\\left(1-\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{2}\\right)V_{t}+\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\left[\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},-\\varepsilon_{t}\\rangle\\sqrt{\\frac{\\mu}{2}}\\|w_{t}-w_{t}^{*}\\|\\right.}}\\\\ {{\\left.\\qquad\\qquad+\\displaystyle\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},-\\varepsilon_{t}\\rangle\\displaystyle\\frac{1+\\sqrt{\\mu\\alpha}}{2\\sqrt{2\\alpha}}\\|z_{t}-w_{t}\\|+\\displaystyle\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}\\right]+\\displaystyle\\frac{20\\mu\\Delta_{t}^{2}}{\\sqrt{\\mu\\alpha}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that under Assumption C.2, there exists an absolute constant $c\\geq1$ such that for all $t\\geq0$ $\\|\\varepsilon_{t}\\|^{2}$ is sub-exponential conditioned on $\\mathcal{H}_{t}$ with parameter $c\\sigma^{2}$ , and $\\varepsilon_{t}$ is mean-zero sub-Gaussian conditioned on $\\mathcal{H}_{t}$ with parameter $c\\sigma$ [18, Theorem 30]. For convenience we simply let $c\\,=\\,1$ here. Thus $\\langle u_{t,1},-\\varepsilon_{t}\\rangle$ is mean-zero sub-Gaussian conditioned on $\\mathcal{H}_{t}$ with parameter $\\sigma$ , and $\\Delta_{t}^{2}$ is sub-exponential conditioned on $\\mathcal{H}_{t}$ with parameter $\\Delta^{2}$ by assumption. Hence, in light of (31), we apply Lemma A.1 with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t}=\\mathcal{H}_{t},\\quad V_{t}=V_{t},\\quad V_{t,1}^{\\prime}=\\frac{\\mu}{2}\\|w_{t}-w_{t}^{*}\\|^{2},\\quad V_{t,2}^{\\prime}=\\frac{(1+\\sqrt{\\mu\\alpha})^{2}}{8\\alpha}\\|z_{t}-w_{t}\\|^{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{\\xi}_{l,1}=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\langle u_{t,1},-\\varepsilon_{t}\\rangle,\\quad D_{t,2}=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\langle u_{t,2},-\\varepsilon_{t}\\rangle,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{t}=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\frac{L\\alpha^{2}}{2}\\|\\varepsilon_{t}\\|^{2}+\\frac{20\\mu\\Delta_{t}^{2}}{\\sqrt{\\mu\\alpha}},\\quad\\alpha_{t}=1-\\frac{\\sqrt{\\mu\\alpha}}{2},\\quad\\kappa_{t}=0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma_{1}=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\sqrt{2\\mu}\\alpha(1-L\\alpha)\\sigma,\\quad\\sigma_{2}=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\frac{2\\mu\\sqrt{2\\alpha}}{1+\\sqrt{\\mu\\alpha}}\\alpha(1-L\\alpha)\\sigma,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nu_{t}=\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\frac{L\\alpha^{2}}{2}\\sigma^{2}+\\frac{20\\mu\\Delta^{2}}{\\sqrt{\\mu\\alpha}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "yielding the following recursion ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\exp(\\lambda V_{t+1})]\\leq\\exp\\left(\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\frac{L\\alpha^{2}}{2}\\sigma^{2}+\\frac{20\\mu\\Delta^{2}}{\\sqrt{\\mu\\alpha}}\\right)\\mathbb{E}\\left[\\exp\\left(\\lambda\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)V_{t}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $\\lambda$ satisfying ", "page_idx": 23}, {"type": "equation", "text": "$$\n0\\leq\\lambda\\leq\\operatorname*{min}\\left\\lbrace\\frac{2}{125\\alpha\\sqrt{\\mu\\alpha}\\sigma^{2}},\\frac{1}{5\\alpha\\sigma^{2}/4+40\\mu\\Delta^{2}/\\sqrt{\\mu\\alpha}}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We then apply (32) recursively to deduce ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{z}}[\\exp(\\lambda V_{t+1})]\\leq\\exp\\left[\\lambda\\left(1-\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}V_{0}+\\lambda\\left(\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\displaystyle\\frac{L\\alpha^{2}}{2}\\sigma^{2}+\\displaystyle\\frac{20\\mu\\Delta^{2}}{\\sqrt{\\mu\\alpha}}\\right)\\displaystyle\\sum_{i=0}^{t-1}\\left(1-\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\displaystyle\\sum_{i=1}^{t}\\sigma^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\exp\\left\\{\\lambda\\left[\\left(1-\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}V_{0}+2\\left(1+\\displaystyle\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\displaystyle\\frac{L\\alpha^{2}}{\\sqrt{\\mu\\alpha}}\\sigma^{2}+\\displaystyle\\frac{80\\Delta^{2}}{\\alpha}\\right]\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $\\lambda$ satisfying ", "page_idx": 24}, {"type": "equation", "text": "$$\n0\\leq\\lambda\\leq\\operatorname*{min}\\left\\lbrace\\frac{2}{125\\alpha\\sqrt{\\mu\\alpha}\\sigma^{2}},\\frac{1}{5\\alpha\\sigma^{2}/4+40\\mu\\Delta^{2}/\\sqrt{\\mu\\alpha}}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, setting ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nu:=2\\alpha\\sigma^{2}+\\frac{80\\Delta^{2}}{\\alpha}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and taking into account $\\alpha\\leq\\operatorname*{min}\\{1/2L,\\mu/2L^{2},1/10^{3}\\mu\\}$ , then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n2\\left(1+\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)\\frac{L\\alpha^{2}}{\\sqrt{\\mu\\alpha}}\\sigma^{2}+\\frac{80\\Delta^{2}}{\\alpha}\\leq\\nu\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\frac{1}{\\nu}}={\\frac{1}{2\\alpha\\sigma^{2}+80\\Delta^{2}/\\alpha}}\\leq\\operatorname*{min}\\left\\{{\\frac{2}{125\\alpha{\\sqrt{\\mu\\alpha}}\\sigma^{2}}},{\\frac{1}{5\\alpha\\sigma^{2}/4+40\\mu\\Delta^{2}/{\\sqrt{\\mu\\alpha}}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\lambda\\left(V_{t}-\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}V_{0}\\right)\\right)\\right]\\leq\\exp(\\lambda\\nu)\\quad{\\mathrm{for~all}}\\quad0\\leq\\lambda\\leq1/\\nu.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking $\\lambda=1/\\nu$ and applying Markov's inequality and union bound completes the proof. ", "page_idx": 24}, {"type": "text", "text": "The following result shows the second part of Lemma 4.3. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.6 (High-probability distance tracking, without drift). Under the same setting as in Lemma $C.4$ with $\\bar{\\alpha}\\leq\\operatorname*{min}\\{1/2\\bar{L},\\mu/10^{3}L^{2}\\}$ for any given $\\delta\\in(0,1)$ and all $t\\in[T]$ the following holdswithprobabilityatleast $1-\\delta$ overtherandomnessin $\\mathcal{H}_{t}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\nV_{t}\\le\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}V_{0}+2\\alpha\\sigma^{2}\\ln\\frac{e T}{\\delta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma C.6. First it is easy to verify that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{L}\\sqrt{\\frac{\\mu}{2}}\\|\\nabla\\phi_{t}(w_{t})-\\nabla\\phi_{t}(w^{*})\\|\\leq\\sqrt{V_{t}}\\qquad\\mathrm{and}\\qquad\\frac{1+\\sqrt{\\mu\\alpha}}{2L\\sqrt{2\\alpha}}\\|\\nabla\\phi_{t}(z_{t})-\\nabla\\phi_{t}(w_{t})\\|\\leq\\sqrt{V_{t}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we apply Lemma A.1 to obtain the final result. We omit the detailed proof here since it follows the same procedure as in proof of Lemma C.5. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "DProofs of Results in Section 4.3.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We first present the following algebraic fact under suitable choice of parameters. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.1 (Parameter choice, informal). For any given $\\delta\\in(0,1)$ andanysmall $\\epsilon$ satisfying ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\epsilon\\leq\\left(\\frac{164\\cdot32e d_{0}L_{0}^{2}\\sigma_{g,1}^{2}}{\\delta\\mu^{2}}\\operatorname*{max}\\left\\{\\frac{l_{g,1}}{\\sigma_{g,1}},\\frac{\\bar{\\sigma}}{d_{0}}\\right\\}\\right)^{1/3},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "if we set parameters $\\alpha,\\beta,\\eta,T$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\n1-\\beta=\\frac{\\mu^{2}\\epsilon^{2}}{164\\cdot64L_{0}^{2}\\sigma_{g,1}^{2}\\ln(P)},\\quad\\eta=\\operatorname*{min}\\left\\{\\frac{\\sigma_{g,1}}{l_{g,1}},\\frac{d_{0}}{\\bar{\\sigma}}\\right\\}(1-\\beta),\\quad\\alpha=\\frac{1}{\\mu}(1-\\beta),\\quad T=\\frac{4d_{0}}{\\eta\\epsilon},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\bar{\\sigma}$ is defined in Lemma B.4, and $d_{0}$ and $P$ are defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\nd_{0}=\\Phi(x_{0})-\\operatorname*{inf}_{x\\in\\mathbb{R}^{d_{x}}}\\Phi(x),\\quad P=\\left(\\frac{164\\cdot64e d_{0}L_{0}^{2}\\sigma_{g,1}^{2}}{\\delta\\mu^{2}\\epsilon^{3}}\\operatorname*{max}\\left\\{\\frac{l_{g,1}}{\\sigma_{g,1}},\\frac{\\bar{\\sigma}}{d_{0}}\\right\\}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then the following holds for all $t\\in[T]$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\frac{4\\alpha\\sigma_{g,1}^{2}}{\\mu}+\\frac{160\\eta^{2}l_{g,1}^{2}}{\\mu^{3}\\alpha}\\right)\\ln\\frac{e T}{\\delta}\\leq\\frac{\\epsilon^{2}}{64L_{0}^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma D.1. By Lemma B.1, we have $\\begin{array}{r}{\\Delta_{t}=\\|y_{t}^{*}-y_{t+1}^{*}\\|\\leq\\frac{l_{g,1}}{\\mu}\\|x_{t}-x_{t+1}\\|=\\eta l_{g,1}/\\mu}\\end{array}$ Thus in our bilevel setting, we choose $\\Delta=\\eta l_{g,1}/\\mu$ where $\\Delta$ is defined in Section 4.3.1. By choice of $\\alpha,\\eta,T$ as in (35), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\frac{4\\alpha\\sigma_{g,1}^{2}}{\\mu}+\\frac{160\\eta^{2}l_{g,1}^{2}}{\\mu^{3}\\alpha}\\right)\\ln\\frac{e T}{\\delta}=\\left(\\frac{4(1-\\beta)\\sigma_{g,1}^{2}}{\\mu^{2}}+\\frac{160\\eta^{2}l_{g,1}^{2}}{\\mu^{2}(1-\\beta)}\\right)\\ln\\left(\\frac{4e d_{0}}{\\delta\\eta\\epsilon}\\right)\\quad\\quad}\\\\ {\\leq\\frac{164(1-\\beta)\\sigma_{g,1}^{2}}{\\mu^{2}}\\ln\\left(\\frac{4e d_{0}}{\\delta\\epsilon(1-\\beta)}\\operatorname*{max}\\left\\{\\frac{l_{g,1}}{\\sigma_{g,1}},\\frac{\\bar{\\sigma}}{4d_{0}}\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we choose $\\beta$ to be ", "page_idx": 25}, {"type": "equation", "text": "$$\n1-\\beta=\\frac{\\mu^{2}\\epsilon^{2}}{164\\cdot64L_{0}^{2}\\sigma_{g,1}^{2}\\ln(P)},\\quad\\mathrm{where}\\quad P=\\left(\\frac{164\\cdot64e d_{0}L_{0}^{2}\\sigma_{g,1}^{2}}{\\delta\\mu^{2}\\epsilon^{3}}\\operatorname*{max}\\left\\{\\frac{l_{g,1}}{\\sigma_{g,1}},\\frac{\\bar{\\sigma}}{d_{0}}\\right\\}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{164(1-\\beta)\\sigma_{g,1}^{2}}{\\mu^{2}}\\ln{\\left(\\frac{4e d_{0}l_{g,1}}{\\delta\\epsilon\\sigma_{g,1}(1-\\beta)}\\right)}=\\frac{\\epsilon^{2}}{64L_{0}^{2}\\ln(P)}\\ln{\\left(\\sqrt{P}\\ln(P)\\right)}\\le\\frac{\\epsilon^{2}}{64L_{0}^{2}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we use the fact that $\\ln(\\sqrt{P}\\ln(P))\\leq\\ln(P)\\leq\\ln^{2}(P)$ for any $P\\geq4$ by choice of $\\epsilon$ as in (34). \u53e3 ", "page_idx": 25}, {"type": "text", "text": "In the rest of this section, we assume Assumptions 3.1 to 3.4 hold. In addition, the failure probability $\\delta\\in(0,1)$ and $\\epsilon>0$ are chosen in the same way as in Theorem 4.1. ", "page_idx": 25}, {"type": "text", "text": "D.1 Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma D.2 (Warm-start, Restatement of Lemma 4.4). Let $\\{y_{t}^{\\mathrm{init}}\\}$ be the iterates produced by line 2 of Algorithm 2. Set $\\alpha^{\\mathrm{init}}\\,=\\,\\mu\\alpha^{2}\\,=\\,\\widetilde\\Theta(\\epsilon^{4})$ with $\\alpha$ defined in (35) and $\\phi_{t}(y)\\,\\equiv\\,g(x_{0},y)$ .Then $\\lVert y_{T_{0}}^{\\mathrm{init}}-y_{0}^{*}\\rVert\\leq\\sqrt{\\frac{\\mu\\alpha}{32}}\\frac{\\epsilon}{L_{0}}$ $1-\\delta$ over therandonesin $\\widetilde{\\mathcal{F}}^{\\mathrm{init}}$ (we denote this event as $\\mathscr{E}_{\\mathrm{init.}}$ )in $\\tilde{T_{0}}=\\tilde{O}(\\epsilon^{-2})$ iterations, where ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{0}=\\ln\\left(\\frac{\\mu^{3}\\alpha^{3}\\epsilon^{2}}{256L_{0}^{2}\\|y_{0}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}}\\right)\\Big/\\ln\\left(1-\\frac{\\mu\\alpha}{4}\\right)=\\widetilde{O}(\\epsilon^{-2}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma $D.2$ .By Lemmas C.6 and D.1 and $\\mu$ -strong convexity of $g$ in $y$ , we have with probability at least $1-\\delta$ over the randomness in ${\\mathcal{F}}^{\\mathrm{init}}$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y_{T_{0}}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}\\leq\\displaystyle\\frac{2}{\\mu}\\left(1-\\frac{\\sqrt{\\mu^{2}\\alpha^{2}}}{4}\\right)^{T_{0}}U_{0}^{\\mathrm{init}}+\\frac{4\\mu\\alpha^{2}\\sigma_{g,1}^{2}}{\\mu}\\ln\\frac{e T_{0}}{\\delta}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{2}{\\mu}\\left(1-\\frac{\\mu\\alpha}{4}\\right)^{T_{0}}U_{0}^{\\mathrm{init}}+\\frac{\\mu\\alpha\\epsilon^{2}}{64L_{0}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By $l_{g,1}$ -smoothness of $g$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{0}^{\\mathrm{init}}\\leq\\frac{2-\\sqrt{\\mu^{2}\\alpha^{2}}+\\mu^{2}\\alpha^{2}}{2\\mu\\alpha^{2}}\\|y_{0}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}+g(x_{0},y_{0}^{\\mathrm{init}})-g(x_{0},y_{0}^{*})}\\\\ &{\\qquad\\leq\\frac{3}{2\\mu\\alpha^{2}}\\|y_{0}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}+\\frac{l_{g,1}}{2}\\|y_{0}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}\\leq\\frac{2}{\\mu\\alpha^{2}}\\|y_{0}^{\\mathrm{init}}-y_{0}^{*}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality uses $\\alpha\\leq1/l_{g,1}$ . Now we set ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{2}{\\mu}\\left(1-\\frac{\\mu\\alpha}{4}\\right)^{T_{0}}\\frac{2}{\\mu\\alpha^{2}}\\|y_{0}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}+\\frac{\\mu\\alpha\\epsilon^{2}}{64L_{0}^{2}}\\leq\\frac{\\mu\\alpha\\epsilon^{2}}{32L_{0}^{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which gives ", "page_idx": 26}, {"type": "equation", "text": "$$\nT_{0}\\ge\\ln\\left(\\frac{\\mu^{3}\\alpha^{3}\\epsilon^{2}}{256L_{0}^{2}\\|y_{0}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}}\\right)\\Big/\\ln\\left(1-\\frac{\\mu\\alpha}{4}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By choice of $\\alpha$ as in (35) and simple calculation we obtain $T_{0}=\\widetilde{O}(\\epsilon^{-2})$ when $\\epsilon$ is small. ", "page_idx": 26}, {"type": "text", "text": "D.2 Proof of Lemma 4.5 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma D.3 (Option I, Restatement of Lemma 4.5). Under event $\\ensuremath{\\mathcal{E}_{\\mathrm{init}}}$ let $\\{y_{t}\\}$ be the iterates produced by Option $I.$ Set $\\alpha={\\widetilde\\Theta}(\\epsilon^{2})$ as in (35) and $\\begin{array}{r}{\\phi_{t}(y)=g(x_{t},y)=\\frac{\\mu}{2}\\|y-y_{t}^{*}\\|^{2}}\\end{array}$ with $y\\in\\mathbb R$ Then for any $t\\in[T]$ Algorithm 2 guarantees with probability at least $1-\\delta$ over the randomness in $\\widetilde{\\mathcal{F}}_{T}^{1}$ (we denote this event as $\\mathcal{E}_{y}^{1}$ )that $\\lVert y_{t}-y_{t}^{*}\\rVert\\leq\\epsilon/2L_{0}$ ", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma $D.3$ .By Lemmas C.5 and D.2 we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y_{t}-y_{t}^{*}\\|^{2}\\leq\\frac{2}{\\mu}\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}U_{0}+\\left(\\frac{4\\alpha\\sigma_{g,1}^{2}}{\\mu}+\\frac{160\\eta^{2}l_{g,1}^{2}}{\\mu^{3}\\alpha}\\right)\\ln\\frac{e T}{\\delta}}\\\\ &{\\qquad\\qquad\\leq\\frac{2}{\\mu}\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{t}\\frac{2}{\\alpha}\\|y_{T_{0}}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}+\\left(\\frac{4\\alpha\\sigma_{g,1}^{2}}{\\mu}+\\frac{160\\eta^{2}l_{g,1}^{2}}{\\mu^{3}\\alpha}\\right)\\ln\\frac{e T}{\\delta}}\\\\ &{\\qquad\\qquad\\leq\\frac{4}{\\mu\\alpha}\\|y_{T_{0}}^{\\mathrm{init}}-y_{0}^{*}\\|^{2}+\\frac{\\epsilon^{2}}{64L_{0}^{2}}\\leq\\frac{\\epsilon^{2}}{4L_{0}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus we conclude that for all $t\\in[T]$ , we have $\\lVert y_{t}-y_{t}^{*}\\rVert\\leq\\epsilon/2L_{0}$ ", "page_idx": 26}, {"type": "text", "text": "D.3 Proof of Lemma 4.6 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma D.4 (Option I, Restatement of Lemma 4.6). Under event $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}$ let $\\{y_{t}\\}$ be theiterates produced by Option Il. Set $\\alpha=\\widetilde\\Theta(\\epsilon^{2})$ as in (35) and run SNAG in each update round for ", "page_idx": 26}, {"type": "equation", "text": "$$\nN=\\ln\\left(\\frac{\\mu\\alpha}{128}\\right)\\Big/\\ln\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)=\\widetilde O(\\epsilon^{-1})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "steps in every I = 2(1-3)L0g $\\begin{array}{r}{I=\\frac{\\mu\\epsilon}{2(1-\\beta)L_{0}\\sigma_{g,1}}=\\widetilde{O}(\\epsilon^{-1})}\\end{array}$ iterations, set $\\phi_{t}(y)=g(x_{t},y)$ when $t$ is a multiple of $I$ (i.e., $x_{t}$ is fixed for each update round of Option $I I$ $g$ can be general functions). Then for any $t\\in[T]$ ,Algorithm 2 guarantees with probability at least $1-\\delta$ over the randomness in $\\sigma(\\cup_{t\\le T}\\widetilde{\\mathcal{F}}_{t}^{2})$ (we denote this event as $\\mathcal{E}_{y\\,\\cdot\\,}^{2}$ that $\\|y_{t}-y_{t}^{*}\\|\\leq\\epsilon/L_{0}$ ", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma $D.4.$ .At the beginning of the first round, by Lemmas D.2 and D.3 we have $\\lVert y_{0}-$ $y_{0}^{*}\\|\\le\\epsilon/2L_{0}$ , then we do not update the lower-level variable until $t=I$ -th iteration, then for $t=I$ wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|y_{I}-y_{I}^{*}\\|=\\|y_{0}-y_{I}^{*}\\|\\le\\|y_{0}-y_{0}^{*}\\|+\\sum_{i=1}^{I}\\|y_{i}^{*}-y_{i-1}^{*}\\|}}\\\\ &{\\le\\displaystyle\\frac{\\epsilon}{2L_{0}}+\\frac{\\eta l_{g,1}}{\\mu}I=\\frac{\\epsilon}{L_{0}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last equality we plug in the definition of $\\eta$ and $I$ By $l_{g,1}$ -smoothness of $g$ wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{U_{I}}=\\displaystyle\\frac{2-2\\sqrt{\\mu\\alpha}+\\mu\\alpha}{2\\alpha}\\|y_{I}-y_{0}^{*}\\|^{2}+{g(x_{I},y_{I})}-{g(x_{I},y_{I}^{*})}}\\\\ {\\le\\displaystyle\\frac{3}{2\\alpha}\\|y_{I}-y_{I}^{*}\\|^{2}+\\displaystyle\\frac{l_{g,1}}{2}\\|y_{I}-y_{I}^{*}\\|^{2}\\le\\displaystyle\\frac{2\\epsilon^{2}}{\\alpha L_{0}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then for $N$ steps update in the inner loops of $t=I$ -th iteration, we set ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{2}{\\mu}\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{N}\\frac{2\\epsilon^{2}}{\\alpha L_{0}^{2}}+\\frac{\\epsilon^{2}}{64L_{0}^{2}}\\leq\\frac{\\epsilon^{2}}{16L_{0}^{2}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which gives ", "page_idx": 27}, {"type": "equation", "text": "$$\nN\\geq\\ln\\left({\\frac{\\mu\\alpha}{128}}\\right){\\Big/}\\ln\\left(1-{\\frac{\\sqrt{\\mu\\alpha}}{4}}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By choice of $\\alpha$ as in (35) and simple calculation we obtain $N=\\widetilde{O}(\\epsilon^{-1})$ when $\\epsilon$ is small. Now we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|y_{I+1}-y_{I}^{*}\\|^{2}=\\|y_{I}^{N}-y_{I}^{*}\\|^{2}\\leq\\frac{2}{\\mu}\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right)^{N}\\frac{2\\epsilon^{2}}{\\alpha L_{0}^{2}}+\\frac{\\epsilon^{2}}{64L_{0}^{2}}\\leq\\frac{\\epsilon^{2}}{16L_{0}^{2}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|y_{I+1}-y_{I+1}^{*}\\|\\leq\\|y_{I+1}-y_{I}^{*}\\|+\\|y_{I}^{*}-y_{I+1}^{*}\\|\\leq\\frac{\\epsilon}{4L_{0}}+\\frac{\\eta l_{g,1}}{\\mu}\\leq\\frac{\\epsilon}{2L_{0}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "wherewechoose $1-\\beta$ to be small (see (54) for details) such that $\\eta$ is small enough to make above inequality holds. Repeating the same process yields the result. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "D.4Proof of Lemma 4.7 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma D.5 (Averaging, Restatement of Lemma 4.7). Under Assumptions 3.1 to 3.4 and event $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}\\cap\\ensuremath{\\mathcal{E}}_{y}^{1}$ (Option $I,$ or $\\bar{\\mathcal{E}}_{\\mathrm{init}}\\cap\\mathcal{E}_{y}^{2}$ (Option $I I$ ), we further set $\\tau=\\sqrt{\\mu\\alpha}$ in the averaging step (line 21 ofAlgorithm 2).Then for any $t\\overset{\\cdot}{\\underset{\\cdot}{\\geq}}0$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\hat{y}_{t}-y_{t}^{*}\\|\\leq\\frac{2\\epsilon}{L_{0}}\\quad a n d\\quad\\|\\hat{y}_{t+1}-\\hat{y}_{t}\\|\\leq\\frac{\\mu\\epsilon^{2}}{24L_{0}^{2}\\sigma_{g,1}}=:\\vartheta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma D.5. We will first show the following result by induction, i.e., for any $t\\geq0$ ,the averaged sequence $\\{\\hat{y}_{t}\\}$ satisies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\hat{y}_{t}-y_{t}^{*}\\|\\leq\\frac{(1-\\tau)\\eta l_{g,1}}{\\tau\\mu}+\\frac{\\epsilon}{L_{0}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $t=0$ , by Lemma D.2 we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\hat{y}_{0}-y_{0}^{*}\\|=\\|y_{T_{0}}^{\\mathrm{init}}-y_{0}^{*}\\|\\le\\sqrt{\\frac{\\mu\\alpha}{32}}\\frac{\\epsilon}{L_{0}}=\\sqrt{\\frac{1-\\beta}{32}}\\frac{\\epsilon}{L_{0}}\\le\\frac{\\epsilon}{L_{0}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "thus the base case holds. Now suppose (38) holds for some $t\\geq0$ , then for time step $t+1$ wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{y}_{t+1}-y_{t+1}^{*}\\|=\\|(1-\\tau)(\\hat{y}_{t}-y_{t+1}^{*})+\\tau(y_{t+1}-y_{t+1}^{*})\\|}\\\\ &{\\qquad\\qquad=\\|(1-\\tau)(\\hat{y}_{t}-y_{t}^{*})+(1-\\tau)(y_{t}^{*}-y_{t+1}^{*})+\\tau(y_{t+1}-y_{t+1}^{*})\\|}\\\\ &{\\qquad\\qquad\\leq(1-\\tau)\\|\\hat{y}_{t}-y_{t}^{*}\\|+(1-\\tau)\\|y_{t}^{*}-y_{t+1}^{*}\\|+\\tau\\|y_{t+1}-y_{t+1}^{*}\\|}\\\\ &{\\qquad\\qquad\\leq(1-\\tau)\\left(\\frac{(1-\\tau)\\eta l_{g,1}}{\\tau\\mu}+\\frac{\\epsilon}{L_{0}}\\right)+\\frac{(1-\\tau)\\eta l_{g,1}}{\\mu}+\\frac{\\tau\\epsilon}{L_{0}}}\\\\ &{\\qquad\\qquad\\leq\\frac{(1-\\tau)\\eta l_{g,1}}{\\tau\\mu}+\\frac{\\epsilon}{L_{0}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use induction hypothesis in the second inequality. Therefore, we have that (38) holds for any $t\\geq0$ . Also, as a consequence, for any $t\\geq0$ wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert\\hat{y}_{t+1}-\\hat{y}_{t}\\Vert=\\Vert\\tau(y_{t+1}-\\hat{y}_{t})\\Vert}&{}\\\\ {\\leq\\tau\\Vert y_{t+1}-y_{t+1}^{*}\\Vert+\\tau\\Vert y_{t+1}^{*}-y_{t}^{*}\\Vert+\\tau\\Vert y_{t}^{*}-\\hat{y}_{t}\\Vert}&{}\\\\ {\\leq\\tau\\left(\\displaystyle\\frac{\\epsilon}{L_{0}}+\\displaystyle\\frac{\\eta l_{g,1}}{\\mu}+\\displaystyle\\frac{(1-\\tau)\\eta l_{g,1}}{\\tau\\mu}+\\displaystyle\\frac{\\epsilon}{L_{0}}\\right)}&{}\\\\ {=\\tau\\left(\\displaystyle\\frac{\\eta l_{g,1}}{\\tau\\mu}+\\displaystyle\\frac{2\\epsilon}{L_{0}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now we plug in the definition of $\\alpha,\\beta,\\tau,\\eta$ as in (35) to obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\hat{y}_{t}-y_{t}^{*}\\|\\leq\\frac{(1-\\tau)\\eta l_{g,1}}{\\tau\\mu}+\\frac{\\epsilon}{L_{0}}=\\frac{\\sigma_{g,1}}{\\mu}\\sqrt{1-\\beta}+\\frac{\\epsilon}{L_{0}}\\leq\\frac{2\\epsilon}{L_{0}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\hat{y}_{t+1}-\\hat{y}_{t}\\|\\leq\\tau\\left(\\frac{\\eta l_{g,1}}{\\tau\\mu}+\\frac{2\\epsilon}{L_{0}}\\right)\\leq\\frac{\\mu\\epsilon^{2}}{24L_{0}^{2}\\sigma_{g,1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "D.5 Proof of Lemma 4.8 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma D.6 (Restatement of Lemma 4.8). Under Assumptions 3.1 to 3.4 and event $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}\\cap\\ensuremath{\\mathcal{E}}_{y}^{1}$ (Option $I)$ or $\\ensuremath{\\mathcal{E}}_{\\mathrm{init}}\\cap\\ensuremath{\\mathcal{E}}_{y}^{2}$ (Option $I I,$ , define $\\epsilon_{t}=m_{t}-\\mathbb{E}_{t}[\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})]$ then we have the following averaged cumulative error bound: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\epsilon_{t}\\|\\leq\\frac{\\bar{\\sigma}}{T(1-\\beta)}+\\sqrt{1-\\beta}\\bar{\\sigma}+\\frac{\\bar{L}_{0}}{\\sqrt{1-\\beta}}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}+\\bar{L}_{1}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S(1-\\beta)}}\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x)\\|_{\\mathcal{X}_{t}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma $D.6$ Define $\\epsilon_{t}=m_{t}-\\mathbb{E}_{t}[\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})]$ , also define $\\tilde{\\epsilon}_{t}$ and $\\hat{\\epsilon}_{t}$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\epsilon}_{t}=\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})-{\\mathbb{E}}_{t}[\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})],}\\\\ &{\\hat{\\epsilon}_{t}=\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})-\\bar{\\nabla}f(x_{t-1},\\hat{y}_{t-1};\\bar{\\xi}_{t})-{\\mathbb{E}}_{t}[\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})]+{\\mathbb{E}}_{t}[\\bar{\\nabla}f(x_{t-1},\\hat{y}_{t-1};\\bar{\\xi}_{t})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By definition of $\\epsilon_{t},\\tilde{\\epsilon}_{t}$ and $\\hat{\\epsilon}_{t}$ , we have the following recursion for any $t\\geq0$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon_{t+1}=\\beta\\epsilon_{t}+(1-\\beta)\\hat{\\epsilon}_{t+1}+\\beta\\tilde{\\epsilon}_{t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then we apply (39) recursively to obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{t}=\\beta^{t}\\epsilon_{0}+\\beta\\sum_{i=1}^{t}\\beta^{t-i}\\tilde{\\epsilon}_{i}+\\left(1-\\beta\\right)\\sum_{i=1}^{t}\\beta^{t-i}\\hat{\\epsilon}_{i},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which by triangle inequality and total expectation gives ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert\\epsilon_{t}\\Vert=\\beta^{t}\\underbrace{\\mathbb{E}\\Vert\\epsilon_{0}\\Vert}_{E r r_{1}}+(1-\\beta)\\underbrace{\\mathbb{E}\\left\\Vert\\sum_{i=1}^{t}\\beta^{t-i}\\hat{\\epsilon}_{i}\\right\\Vert}_{E r r_{2}}+\\beta\\underbrace{\\mathbb{E}\\left\\Vert\\sum_{i=1}^{t}\\beta^{t-i}\\tilde{\\epsilon}_{i}\\right\\Vert}_{E r r_{3}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bounding $E r r_{1}$ .By definition of $\\epsilon_{0}$ and Lemma B.4, along with Jensen's inequality, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\epsilon_{0}\\|\\leq\\sqrt{\\mathbb{E}\\|\\epsilon_{0}\\|^{2}}\\leq\\bar{\\sigma}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Bounding $E r r_{2}$ .We apply Lemma B.4 and follow the similar procedure as in [35, Lemma D.9] to obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\sum_{i=1}^{t}\\beta^{t-i}\\tilde{\\epsilon}_{i}\\right\\|\\leq\\sqrt{\\mathbb{E}\\left\\|\\sum_{i=1}^{t}\\beta^{t-i}\\tilde{\\epsilon}_{i}\\right\\|^{2}}\\leq\\sqrt{\\sum_{i=1}^{t}\\beta^{2(t-i)}\\mathbb{E}\\|\\tilde{\\epsilon}_{i}\\|^{2}}\\leq\\frac{\\bar{\\sigma}}{\\sqrt{1-\\beta}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we use Jensen's inequality for the first step. ", "page_idx": 28}, {"type": "text", "text": "Bounding $E r r_{3}$ .We will first use induction to show that for $0\\leq i\\leq t+1$ , the following inequality holds: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{Z}\\left[\\left\\lVert\\sum_{j=1}^{t}\\beta^{t-j}\\hat{\\epsilon}_{j}\\right\\rVert\\right]\\leq\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}\\bar{L}_{1}\\sum_{j=t+1-i}^{t}\\beta^{t-j}\\mathbb{E}\\|\\nabla\\Phi(x_{j})\\|+\\mathbb{E}\\left[\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})\\bar{L}_{0}^{2}}{S}}\\sum_{j=1}^{i}\\beta^{2j-2}+\\left\\lVert\\sum_{j=1}^{t-i}\\beta^{2j}\\right\\rVert\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then it's easy to check that by setting $i=t+1$ we can obtain the bound. When $i=0$ , (43) holds obviously since ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{j=1}^{t}\\beta^{t-j}\\hat{\\epsilon}_{j}\\right\\rVert\\right]\\leq\\mathbb{E}\\left[\\sqrt{\\left\\lVert\\sum_{j=1}^{t}\\beta^{t-j}\\hat{\\epsilon}_{j}\\right\\rVert^{2}}\\right]=\\mathbb{E}\\left[\\left\\lVert\\sum_{j=1}^{t}\\beta^{t-j}\\hat{\\epsilon}_{j}\\right\\rVert\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence the base case stands. Now suppose (43) holds for some $i\\geq0$ , and we aim to show that (43) holds for $i+1$ . In fact, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\frac{2(\\eta^{2}+\\vartheta^{2})L\\rho^{\\frac{1}{2}}}{S}\\frac{\\vartheta^{2}\\rho^{2}-\\sigma^{\\frac{1}{2}}}{\\sum_{m}^{1-\\ldots}}\\left\\|\\sum_{n=1}^{\\lfloor\\tau-\\tau\\rfloor}\\vartheta^{-n-\\varepsilon}_{g}\\right\\|^{2}\\right]\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{r-\\tau+\\tau+\\tau}}\\left[\\mathbb{E}_{\\mathbb{R}_{+}\\to\\infty}\\left[\\left\\langle\\frac{2(\\eta^{2}+\\vartheta^{2})L\\rho^{\\frac{1}{2}}}{S}\\sum_{\\mu=1}^{\\ell}\\widehat{\\vartheta}^{2\\mu-2}+\\left\\|\\sum_{n=1}^{\\lfloor\\tau-\\tau\\rfloor}\\widehat{\\vartheta}^{-n-\\varepsilon}_{g}\\right\\|^{2}\\right]\\right]}\\\\ &{\\leq\\mathbb{E}_{\\mathcal{R}_{r-\\tau+\\tau}}\\left[\\mathbb{E}_{\\mathbb{R}_{+}\\to\\infty}\\sqrt{\\left[\\frac{2(\\eta^{2}+\\vartheta^{2})L\\rho^{\\frac{1}{2}}}{S}\\sum_{\\mu=1}^{\\ell}\\widehat{\\vartheta}^{2\\mu-2}+\\left\\|\\sum_{n=1}^{\\lfloor\\tau-\\beta}\\theta^{-n}\\widehat{\\vartheta}_{\\mu}\\right\\|^{2}\\right]}\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{R}_{r-\\tau+\\tau+\\tau}}\\left[\\mathbb{E}_{\\mathbb{R}_{+}\\to\\infty}\\sqrt{\\left[\\frac{2(\\eta^{2}+\\vartheta^{2})L\\rho^{\\frac{1}{2}}}{S}\\sum_{\\mu=1}^{\\ell}\\widehat{\\vartheta}^{2\\mu-2}+\\beta^{2\\mu}\\|\\xi_{\\mu-1}\\|^{2}+\\left\\|\\sum_{j=1}^{\\ t-\\lfloor\\tau\\rfloor}\\widehat{\\vartheta}^{-n-\\varepsilon}_{j}\\right\\|^{2}\\right]}\\right]}\\\\ &{\\leq\\mathbb{E}_{\\mathcal{R}_{r-\\tau+\\tau}}\\left[\\mathbb{E}_{\\mathbb{R}_{+}\\to\\infty}\\sqrt{\\left[\\frac{2(\\eta^{2}+\\vartheta^{2})L\\rho^{\\frac{1}{2}}}{S}\\sum_{\\mu=1}^{\\ell}\\widehat{\\vartheta}^{2\\mu-2}+\\frac{\\beta^{2\\mu}}{S}(2L_{\\theta}^{2}+L_{\\mathbb{R}}^{\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{\\mathcal{F}_{t-i-1}}\\left[\\sqrt{\\displaystyle\\frac{2\\beta^{2i}}{S}\\bar{L}_{1}^{2}(\\eta^{2}+\\vartheta^{2})\\|\\nabla\\Phi({x}_{t-i})\\|^{2}+\\displaystyle\\frac{2\\big(\\eta^{2}+\\vartheta^{2})\\bar{L}_{0}^{2}}{S}\\sum_{j=1}^{i+1}\\beta^{2j-2}+\\left\\|\\sum_{j=1}^{t-i-1}\\beta^{t-j}\\hat{\\epsilon}_{j}\\right\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{\\mathcal{F}_{t-i-1}}\\left[\\sqrt{\\displaystyle\\frac{2\\big(\\eta^{2}+\\vartheta^{2}\\big)}{S}}\\beta^{i}\\bar{L}_{1}\\|\\nabla\\Phi({x}_{t-i})\\|+\\sqrt{\\displaystyle\\frac{2\\big(\\eta^{2}+\\vartheta^{2})\\bar{L}_{0}^{2}}{S}\\sum_{j=1}^{i+1}\\beta^{2j-2}+\\left\\|\\sum_{j=1}^{t-i-1}\\beta^{t-j}\\hat{\\epsilon}_{j}\\right\\|^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where (45) follows by law of total expectation, (46) follows by Jensen's inequality, (47) uses the fact that $\\hat{\\epsilon}_{j}$ for $j<t-i$ are $\\mathcal{F}_{t-i-1}$ -measurable, and are uncorrelated with $\\hat{\\epsilon}_{t-i}$ ; for (48) we use Lemmas B.6 and D.5 to derive ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{t-i-1}[\\|\\hat{\\epsilon}_{t-i}\\|^{2}]=\\mathbb{E}_{t-i-1}\\left[\\|\\bar{\\nabla}f(x_{t-i},\\hat{y}_{t-i};\\bar{\\xi}_{t-i})-\\bar{\\nabla}f(x_{t-i-1},\\hat{y}_{t-i-1};\\bar{\\xi}_{t-i})\\right.}\\\\ &{\\left.\\left.\\mathrm-\\mathbb{E}_{t-i}[\\bar{\\nabla}f(x_{t-i},\\hat{y}_{t-i};\\hat{\\xi}_{t-i})]+\\mathbb{E}_{t-i}[\\bar{\\nabla}f(x_{t-i-1},\\hat{y}_{t-i-1};\\bar{\\xi}_{t-i})]\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}_{t-i-1}\\left[\\|\\bar{\\nabla}f(x_{t-i},\\hat{y}_{t-i};\\bar{\\xi}_{t-i})-\\bar{\\nabla}f(x_{t-i-1},\\hat{y}_{t-i-1};\\bar{\\xi}_{t-i})\\|^{2}\\right]}\\\\ &{\\le2\\mathbb{E}_{t-i-1}\\left[\\|\\bar{\\nabla}f(x_{t-i},\\hat{y}_{t-i};\\bar{\\xi}_{t-i})-\\bar{\\nabla}f(x_{t-i},\\hat{y}_{t-i-1};\\bar{\\xi}_{t-i-1})\\|^{2}\\right]}\\\\ &{\\quad+\\left.2\\mathbb{E}_{t-i-1}\\left[\\|\\bar{\\nabla}f(x_{t-i},\\hat{y}_{t-i-1};\\bar{\\xi}_{t-i})-\\bar{\\nabla}f(x_{t-i-1},\\hat{y}_{t-i-1};\\bar{\\xi}_{t-i})\\|^{2}\\right]}\\\\ &{\\le\\frac{2}{S}(\\bar{L}_{0}^{2}+\\bar{L}_{1}^{2}\\|\\nabla\\Phi(x_{t-i})\\|^{2})(\\|\\hat{y}_{t-i}-\\hat{y}_{t-i-1}\\|^{2}+\\|x_{t-i}-x_{t-i-1}\\|^{2})}\\\\ &{=\\frac{2}{S}(\\bar{L}_{0}^{2}+\\bar{L}_{1}^{2}\\|\\nabla\\Phi(x_{t-i})\\|^{2})(\\eta^{2}+\\vartheta^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "And (49) follows from the fact that $x_{t-i}$ is $\\mathcal{F}_{t-i-1}$ -measurable, (50) uses ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}}$ for $a,b\\ge0$ . Hence the induction proof is completed. We set $i=t+1$ to obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\sum_{i=1}^{t}\\beta^{t-i}\\hat{\\epsilon}_{i}\\right\\Vert\\right]\\leq\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}\\bar{L}_{1}\\displaystyle\\sum_{i=0}^{t}\\beta^{t-i}\\mathbb{E}\\|\\nabla\\Phi(x_{i})\\|+\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})\\bar{L}_{0}^{2}}{S}\\sum_{i=0}^{t}\\beta^{2i}}}\\\\ {\\leq\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}\\bar{L}_{1}\\displaystyle\\sum_{i=0}^{t}\\beta^{t-i}\\mathbb{E}\\|\\nabla\\Phi(x_{i})\\|+\\frac{\\bar{L}_{0}}{\\sqrt{1-\\beta}}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Final Bound. Combining (41), (42) and (51) yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert\\epsilon_{t}\\Vert\\leq\\beta^{t}\\bar{\\sigma}+\\sqrt{1-\\beta}\\bar{\\sigma}+\\frac{\\bar{L}_{0}}{\\sqrt{1-\\beta}}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}+\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}\\bar{L}_{1}\\sum_{i=0}^{t}\\beta^{t-i}\\mathbb{E}\\Vert\\nabla\\Phi(x_{i})\\Vert.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking summation and dividing $1/T$ on both sides gives the final result ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\epsilon_{t}\\|\\leq\\frac{\\bar{\\sigma}}{T(1-\\beta)}+\\sqrt{1-\\beta}\\bar{\\sigma}+\\frac{\\bar{L}_{0}}{\\sqrt{1-\\beta}}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}+\\bar{L}_{1}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S(1-\\beta)}}\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x)\\|_{\\mathcal{X}_{t}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "E Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Before starting the proof of main results, i.e., Theorem 4.1, we first need the following lemma. ", "page_idx": 30}, {"type": "text", "text": "Lemma E.1. Suppose that Assumptions 3.1 to 3.4 hold. For any n satisfying ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{\\sqrt{2(1+l_{g,1}^{2}/\\mu^{2})(L_{x,1}^{2}+L_{y,1}^{2})}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\displaystyle\\frac{1}{2}\\eta L_{1}-2L_{1}\\|\\hat{y}_{t}-y_{t}^{*}\\|\\right)\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(\\boldsymbol{x}_{t})\\|}\\\\ &{\\leq\\displaystyle\\frac{\\Phi(\\boldsymbol{x}_{0})-\\Phi(\\boldsymbol{x}_{T})}{T\\eta}+\\frac{2}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\boldsymbol{\\epsilon}_{t}\\|+\\frac{2l_{g,1}l_{f,0}}{\\mu}\\left(1-\\displaystyle\\frac{\\mu}{l_{g,1}}\\right)^{Q}+\\frac{2L_{0}}{T}\\displaystyle\\sum_{t=0}^{T-1}\\|\\hat{y}_{t}-y_{t}^{*}\\|+\\frac{1}{2}\\eta L_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma E.1. Define $h_{t}=m_{t}-\\nabla\\Phi(x_{t})$ . Then we apply Lemma B.3 to obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(x_{t+1})\\leq\\Phi(x_{t})+\\langle\\nabla\\Phi(x_{t}),x_{t+1}-x_{t}\\rangle+\\frac{L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|}{2}\\|x_{t+1}-x_{t}\\|^{2}}\\\\ &{\\qquad\\qquad=\\Phi(x_{t})-\\eta\\langle\\nabla\\Phi(x_{t}),\\frac{m_{t}}{\\|m_{t}\\|}\\rangle+\\frac{1}{2}\\eta^{2}(L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|)}\\\\ &{\\qquad\\qquad=\\Phi(x_{t})-\\eta\\langle m_{t}-h_{t},\\frac{m_{t}}{\\|m_{t}\\|}\\rangle+\\frac{1}{2}\\eta^{2}(L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|)}\\\\ &{\\qquad\\qquad=\\Phi(x_{t})-\\eta\\|m_{t}\\|+\\eta\\langle h_{t},\\frac{m_{t}}{\\|m_{t}\\|}\\rangle+\\frac{1}{2}\\eta^{2}(L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|)}\\\\ &{\\qquad\\qquad\\leq\\Phi(x_{t})-\\eta\\|m_{t}\\|+\\eta\\|h_{t}\\|+\\frac{1}{2}\\eta^{2}(L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|)}\\\\ &{\\qquad\\qquad\\leq\\Phi(x_{t})-\\eta\\|\\nabla\\Phi(x_{t})\\|+2\\eta\\|h_{t}\\|+\\frac{1}{2}\\eta^{2}(L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|)}\\\\ &{\\qquad\\qquad\\leq\\Phi(x_{t})-\\eta\\|\\nabla\\Phi(x_{t})\\|+2\\eta\\|h_{t}\\|+\\frac{1}{2}\\eta^{2}(L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where for the last two lines we use Cauchy-Schwarz inequality and $\\|h_{t}\\|\\,=\\,\\|\\nabla\\Phi(x_{t})+h_{t}\\|\\,\\geq$ $\\|\\nabla\\Phi(x_{t})\\|-\\|h_{t}\\|$ . Now expanding $h_{t}$ by triangle inequality, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t}\\|=\\|m_{t}-\\nabla\\Phi(x_{t})\\|}\\\\ &{\\quad\\quad\\leq\\|m_{t}-\\mathbb{E}_{t}[\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})]\\|+\\|\\mathbb{E}_{t}[\\bar{\\nabla}f(x_{t},\\hat{y}_{t};\\bar{\\xi}_{t})]-\\bar{\\nabla}f(x_{t},\\hat{y}_{t})\\|+\\|\\bar{\\nabla}f(x_{t},\\hat{y}_{t})-\\nabla\\Phi(x_{t})\\|}\\\\ &{\\quad\\quad\\leq\\|\\epsilon_{t}\\|+\\frac{l_{g,1}l_{f,0}}{\\mu}\\left(1-\\frac{\\mu}{l_{g,1}}\\right)^{Q}+(L_{0}+L_{1}\\|\\nabla\\Phi(x_{t})\\|)\\|\\hat{y}_{t}-y_{t}^{*}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we use definition of $\\epsilon_{t}$ , Lemmas B.4 and B.5 in the last inequality. Plugging the above inequality back into (52) we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(x_{t+1})\\leq\\Phi(x_{t})-\\eta\\Vert\\nabla\\Phi(x_{t})\\Vert+2\\eta\\Vert\\epsilon_{t}\\Vert+2\\eta\\frac{l_{g,1}l_{f,0}}{\\mu}\\left(1-\\frac{\\mu}{l_{g,1}}\\right)^{Q}}\\\\ &{\\phantom{\\Phi(x_{t+1})\\leq\\Phi(x_{t})-\\eta\\Vert\\nabla\\Phi(x_{t})\\Vert+}+2\\eta(L_{0}+L_{1}\\Vert\\nabla\\Phi(x_{t})\\Vert)\\Vert\\hat{y}_{t}-y_{t}^{*}\\Vert+\\frac{1}{2}\\eta^{2}(L_{0}+L_{1}\\Vert\\nabla\\Phi(x_{t})\\Vert)}\\\\ &{\\phantom{\\Phi(x_{t+1})\\leq\\Phi(x_{t})-\\eta\\Vert}=\\Phi(x_{t})-\\frac{1}{2}\\eta^{2}L_{1}-2\\eta L_{1}\\Vert\\hat{y}_{t}-y_{t}^{*}\\Vert\\right)\\Vert\\nabla\\Phi(x_{t})\\Vert+2\\eta\\Vert\\epsilon_{t}\\Vert}\\\\ &{\\phantom{\\Phi(x_{t})\\leq\\Phi(x_{t})-\\eta\\Vert}+2\\eta\\frac{l_{g,1}l_{f,0}}{\\mu}\\left(1-\\frac{\\mu}{l_{g,1}}\\right)^{Q}+2\\eta L_{0}\\Vert\\hat{y}_{t}-y_{t}^{*}\\Vert+\\frac{1}{2}\\eta^{2}L_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Dividing $1/T\\eta$ on both sides, then taking telescope sum and total expectation, and rearranging it finally yields ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(1-\\frac{1}{2}\\eta L_{1}-2L_{1}\\|\\hat{y}_{t}-y_{t}^{*}\\|\\right)\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi({x}_{t})\\|}\\\\ {\\displaystyle\\leq\\frac{\\Phi(x_{0})-\\Phi(x_{T})}{T\\eta}+\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\epsilon_{t}\\|+\\frac{2l_{g,1}l_{f,0}}{\\mu}\\left(1-\\frac{\\mu}{l_{g,1}}\\right)^{Q}+\\frac{2L_{0}}{T}\\sum_{t=0}^{T-1}\\|\\hat{y}_{t}-y_{t}^{*}\\|+\\frac{1}{2}\\eta L_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Theorem E.2 (Restatement of Theorem 4.1). Suppose Assumptions 3.1 to 3.4 hold. Let $\\left\\{x_{t}\\right\\}$ bethe iterates produced by Algorithm 2. For any given $\\delta\\in(0,1)$ andanysmall $\\epsilon>0$ satisfying ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\leq\\operatorname*{min}\\left\\{\\frac{L_{0}}{32L_{1}},\\frac{l_{g,1}L_{0}}{\\mu\\bar{L}_{1}},\\frac{L_{0}}{8\\bar{L}_{1}},\\frac{L_{0}l_{g,1}\\sigma_{g,1}}{\\mu^{2}},\\frac{L_{0}}{\\mu}\\sqrt{\\frac{l_{g,1}\\sigma_{g,1}}{L_{1}}},\\left(\\frac{164\\cdot32e d_{0}L_{0}^{2}\\sigma_{g,1}^{2}}{\\delta\\mu^{2}}\\operatorname*{max}\\left\\{\\frac{l_{g,1}}{\\sigma_{g,1}},\\frac{\\bar{\\sigma}}{d_{0}}\\right\\}\\right)^{1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "if we set parameters $\\alpha,\\alpha^{\\prime},\\alpha^{\\mathrm{init}},\\beta,\\gamma,\\eta,\\tau,I,N,S,Q,T_{0}$ as ", "page_idx": 31}, {"type": "equation", "text": "$$\n1-\\beta=\\operatorname*{min}\\left\\{\\frac{\\mu^{2}\\epsilon^{2}}{164\\cdot16L_{0}^{2}\\sigma_{g,1}^{2}\\ln(P)},\\frac{l_{g,1}}{4\\sigma_{g,1}L_{1}},\\frac{\\epsilon^{2}}{4\\sigma^{2}}\\right\\},\\quad\\eta=\\operatorname*{min}\\left\\{\\frac{\\sigma_{g,1}}{l_{g,1}},\\frac{d_{0}}{\\bar{\\sigma}}\\right\\}(1-\\beta),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\alpha^{\\mathrm{init}}=\\frac{1-\\beta}{\\mu+l_{g,1}}\\quad\\alpha=\\frac{1-\\beta}{\\mu},\\quad\\gamma=\\frac{1-\\sqrt{\\mu\\alpha}}{1+\\sqrt{\\mu\\alpha}},\\quad\\tau=1-\\sqrt{\\mu\\alpha},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\nT_{0}=\\ln\\left(\\frac{\\mu^{3}\\alpha^{3}\\epsilon^{2}}{256L_{0}^{2}\\Vert y_{0}^{\\mathrm{init}}-y_{0}^{*}\\Vert^{2}}\\right)\\Big/\\ln\\left(1-\\frac{\\mu\\alpha}{4}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\nI=\\frac{\\mu\\epsilon}{2(1-\\beta)L_{0}\\sigma_{g,1}},\\quad N=\\ln\\left(\\frac{\\mu\\alpha}{128}\\right)\\Big/\\ln\\left(1-\\frac{\\sqrt{\\mu\\alpha}}{4}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\nS=\\operatorname*{max}\\left\\{128\\ln(P),\\frac{128\\bar{L}_{0}^{2}}{L_{0}^{2}}\\ln(P),\\frac{\\mu^{2}\\bar{L}_{0}^{2}}{l_{g,1}^{2}L_{0}^{2}}\\right\\},\\quad Q=\\ln\\left(1-\\frac{\\mu}{l_{g,1}}\\right)\\Big/\\ln\\left(\\frac{\\mu\\epsilon}{l_{g,1}l_{f,0}}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $d_{0}$ and $P$ are defined in (36). Then with probability at least $1-2\\delta$ over the randomness $\\sigma(\\mathcal{F}^{\\mathrm{init}}\\cup\\widetilde{\\mathcal{F}}_{T}^{1})$ (for Option $I$ )or $\\sigma(\\mathcal{F}^{\\mathrm{init}}\\cup(\\cup_{t\\le T}\\widetilde{\\mathcal{F}}_{t}^{2}))$ (for Option $I I$ ), Algorithm 2 guarantees $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|\\leq20\\epsilon}\\end{array}$ within $\\begin{array}{r}{T=\\frac{4d_{0}}{\\eta\\epsilon}=\\widetilde{O}(1/\\epsilon^{3})}\\end{array}$ iterations, where the expectation is taken over the randomness in $\\mathcal{F}_{T}$ . For Option $I,$ it requires $T_{0}+S Q T=\\widetilde{O}(1/\\epsilon^{3})$ oracle calls of stochastic gradiet $I I_{:}$ it requires $\\begin{array}{r}{T_{0}+\\frac{N T}{I}+S Q T=\\widetilde{O}(1/\\epsilon^{3})}\\end{array}$ oracle calls of stochastic gradient or Hessian/Jacobianvector product. ", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem $E.2$ .By Lemmas D.6 and E.1, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\displaystyle\\frac{1}{2}\\eta L_{1}-2L_{1}\\|\\hat{y}_{t}-y_{t}^{*}\\|\\right)\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|}\\\\ &{\\leq\\frac{\\Phi(x_{0})-\\Phi(x_{T})}{T\\eta}+\\frac{2}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\epsilon_{t}\\|+\\frac{2l_{g,1}l_{f,0}}{\\mu}\\left(1-\\displaystyle\\frac{\\mu}{l_{g,1}}\\right)^{Q}+\\frac{2L_{0}}{T}\\sum_{t=0}^{T-1}\\|\\hat{y}_{t}-y_{t}^{*}\\|+\\frac{1}{2}\\eta L_{0}}\\\\ &{\\leq\\frac{d_{0}}{T\\eta}+\\frac{2l_{g,1}l_{f,0}}{\\mu}\\left(1-\\displaystyle\\frac{\\mu}{l_{g,1}}\\right)^{Q}+\\frac{2L_{0}}{T}\\sum_{t=0}^{T-1}\\|\\hat{y}_{t}-y_{t}^{*}\\|+\\frac{1}{2}\\eta L_{0}}\\\\ &{\\quad+\\frac{2\\bar{\\sigma}}{T(1-\\beta)}+2\\sqrt{1-\\beta}\\bar{\\sigma}+\\frac{2\\bar{L}_{0}}{\\sqrt{1-\\beta}}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S}}+2\\bar{L}_{1}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S(1-\\beta)}}\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Rearranging the above inequality gives ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\stackrel{!}{2}\\eta L_{1}-2L_{1}\\|\\dot{y}_{t}-y_{t}^{*}\\|-2\\bar{L}_{1}\\sqrt{\\frac{2(\\eta^{2}+\\vartheta^{2})}{S(1-\\beta)}}\\right)\\frac{1}{T}\\underset{t=0}{\\overset{T-1}{\\sum}}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|}{(\\mathrm{LHS})}}\\\\ &{\\frac{+\\frac{2l_{g,1}l_{f,0}}{\\mu}\\left(1-\\frac{\\mu}{l_{g,1}}\\right)^{Q}+\\frac{2L_{0}}{T}\\underset{t=0}{\\overset{T-1}{\\sum}}\\|\\dot{y}_{t}-y_{t}^{*}\\|+\\frac{1}{2}\\eta L_{0}+\\frac{2\\bar{\\sigma}}{T(1-\\beta)}+2\\sqrt{1-\\beta}\\bar{\\sigma}+\\frac{2\\bar{L}_{0}}{\\sqrt{1-\\beta}}\\sqrt{\\frac{2(\\eta^{2}+\\bar{\\sigma})}{\\mu}}+}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Bounding (LHS).  By Lemma D.5, we have ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(LHS)}\\geq1-\\displaystyle\\frac{\\sigma_{g,1}L_{1}}{2l_{g,1}}(1-\\beta)-2L_{1}\\displaystyle\\frac{2\\epsilon}{L_{0}}-2\\bar{L}_{1}\\sqrt{\\frac{2\\big(\\eta^{2}+\\vartheta^{2}\\big)}{S(1-\\beta)}}}\\\\ &{\\quad\\qquad\\geq1-\\displaystyle\\frac{1}{8}-\\displaystyle\\frac{1}{8}-\\displaystyle\\frac{1}{4}=\\displaystyle\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Bounding (RHS). By choice of parameters, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{(RHS)}\\leq\\frac{1}{4}\\epsilon+2\\epsilon+4\\epsilon+\\epsilon+\\frac{1}{2}\\epsilon+\\epsilon+\\epsilon\\leq10\\epsilon.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combining (59) and (60) finally yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\nabla\\Phi(x_{t})\\|\\leq20\\epsilon.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "F  Omitted Proofs in Appendix B ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "F.1 Proof of Lemma B.5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma F.1 (Restatement of Lemma B.5). Under Assumptions 3.1 to 3.4, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\bar{\\nabla}f(x,y)-\\nabla\\Phi(x)\\|\\leq(\\bar{L}+L_{x,1}\\|\\nabla\\Phi(x)\\|)\\|y-y^{*}(x)\\|,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where constant $\\bar{L}$ is defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{L}:=L_{x,0}+L_{x,1}\\frac{l_{g,1}l_{f,0}}{\\mu}+\\frac{l_{g,1}}{\\mu}(L_{y,0}+L_{y,1}l_{f,0})+l_{f,0}\\frac{\\mu l_{g,2}+l_{g,1}l_{g,2}}{\\mu^{2}}\\le L_{0}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma B.5. Recall that the exact expressions of $\\bar{\\nabla}f(x,y)$ and $\\nabla\\Phi({\\boldsymbol{x}})$ are ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{\\nabla}f(x,y)=\\nabla_{x}f(x,y)-\\nabla_{x y}^{2}g(x,y)[\\nabla_{y y}^{2}g(x,y)]^{-1}\\nabla_{y}f(x,y)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nabla\\Phi(x)=\\nabla_{x}f(x,y^{*}(x))-\\nabla_{x y}^{2}g(x,y^{*}(x))[\\nabla_{y y}^{2}g(x,y^{*}(x))]^{-1}\\nabla_{y}f(x,y^{*}(x)).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then by Assumption 3.2 we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla f(x,y)-\\nabla\\Phi(x)\\|\\leq\\|\\nabla_{x}f(x,y)-\\nabla_{x}f(x,y^{*}(x))\\|}\\\\ &{\\quad+\\|\\nabla_{x y}^{2}g(x,y)\\big[\\nabla_{y y}^{2}g(x,y)\\big]^{-1}\\nabla_{y}f(x,y)-\\nabla_{x y}^{2}g(x,y^{*}(x))\\big[\\nabla_{y y}^{2}g(x,y^{*}(x))\\big]^{-1}\\nabla_{y}f(x,y^{*}(x))\\|}\\\\ &{\\leq\\big(L_{x,0}+L_{x,1}\\big\\|\\nabla_{x}f(x,y^{*}(x))\\big\\|\\big\\|\\big\\|y-y^{*}(x)\\big\\|\\big\\|}\\\\ &{\\quad+\\|\\nabla_{x y}^{2}g(x,y)\\big[\\nabla_{y y}^{2}g(x,y)\\big]^{-1}\\nabla_{y}f(x,y)-\\nabla_{x y}^{2}g(x,y^{*}(x))\\big[\\nabla_{y y}^{2}g(x,y)\\big]^{-1}\\nabla_{y}f(x,y)\\big\\|}\\\\ &{\\quad+\\|\\nabla_{x y}^{2}g(x,y^{*}(x))\\big[\\nabla_{y y}^{2}g(x,y)\\big]^{-1}\\nabla_{y}f(x,y)-\\nabla_{x y}^{2}g(x,y^{*}(x))\\big[\\nabla_{y y}^{2}g(x,y^{*}(x))\\big]^{-1}\\nabla_{y}f(x,y)\\|}\\\\ &{\\quad+\\|\\nabla_{x y}^{2}g(x,y^{*}(x))\\big[\\nabla_{y y}^{2}g(x,y^{*}(x))\\big]^{-1}\\nabla_{y}f(x,y)-\\nabla_{x y}^{2}g(x,y^{*}(x))\\big[\\nabla_{y y}^{2}g(x,y^{*}(x))\\big]^{-1}\\nabla_{y}f(x,y)}\\\\ &{\\leq\\Big(L_{x,0}^{2}+L_{x,1}\\Big(\\frac{L_{y,1}f_{x,0}}{\\mu}+\\|\\nabla_{x y}^{4}(x)\\|\\Big)\\Big)\\|\\ y-y^{*}(x)\\|}\\\\ &{\\quad+\\frac{L_{y}\\rho_{1}}{\\mu}l_{2}\\|y-y^{*}(x)\\|+\\frac{l_{f_{0}}\\rho_{1}}{\\mu}l_{2}\\|y-y^{*}(x)\\|+\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By definition of $\\bar{L}$ we conclude the proof. ", "page_idx": 33}, {"type": "text", "text": "F.2Proof of Lemma B.6 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma F.2 (Restatement of Lemma B.6). Under Assumptions 3.1 to 3.4, we have ", "page_idx": 33}, {"type": "text", "text": "(i) For any fixed $\\boldsymbol{y}\\in\\mathbb{R}^{d_{\\boldsymbol{y}}}$ and any $x_{1},x_{2}\\in\\mathbb{R}^{d_{x}}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\bar{\\xi}}\\|\\bar{\\nabla}f(x_{1},y;\\bar{\\xi})-\\bar{\\nabla}f(x_{2},y;\\bar{\\xi})\\|^{2}\\leq(\\bar{L}_{0}^{2}+\\bar{L}_{1}^{2}\\|\\nabla\\Phi(x_{1})\\|^{2})\\|x_{1}-x_{2}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "(ii\uff09 For any fixed $x\\in\\mathbb{R}^{d_{x}}$ and any $y_{1},y_{2}\\in\\mathbb{R}^{d_{y}}$ \uff0c", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\bar{\\xi}}\\|\\bar{\\nabla}f(x,y_{1};\\bar{\\xi})-\\bar{\\nabla}f(x,y_{2};\\bar{\\xi})\\|^{2}\\leq(\\bar{L}_{0}^{2}+\\bar{L}_{1}^{2}\\|\\nabla\\Phi(x_{1})\\|^{2})\\|x_{1}-x_{2}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the above expressions, we define $\\bar{L}_{0}$ and $\\bar{L}_{1}$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{L}_{0}=\\left\\{4\\left(L_{x,0}+L_{x,1}\\left(\\frac{l_{g,1}l_{f,0}}{\\mu}+\\left(L_{x,0}+\\frac{L_{x,1}l_{g,1}l_{f,0}}{\\mu}\\right)\\|y_{1}-y_{1}^{*}\\|\\right)\\right)^{2}\\right.}\\\\ {\\displaystyle\\left.+\\frac{6Q}{2\\mu l_{g,1}-\\mu^{2}}\\left(l_{g,1}^{2}(L_{y,0}+L_{y,1}l_{f,0})^{2}+l_{f,0}^{2}l_{g,2}^{2}+\\frac{l_{f,0}^{2}l_{g,1}^{2}l_{g,2}^{2}K^{2}}{(l_{g,1}-\\mu)^{2}}\\right)\\right\\}^{1/2},}\\\\ {\\displaystyle\\bar{L}_{1}=2L_{x,1}(1+L_{x,1}\\|y_{1}-y_{1}^{*}\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma B.6. We show statement $(i)$ of the lemma, and $(i i)$ follows by similar arguments. For any fixed $\\boldsymbol{y}\\in\\mathbb{R}^{d_{y}}$ and any $x_{1},x_{2}\\in\\mathbb{R}^{d_{x}}$ , by definition of $\\bar{\\nabla}f(x,y;\\bar{\\xi})$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\overline{{\\nabla}}f(x_{1},y;\\bar{\\xi})-\\bar{\\nabla}f(x_{2},y;\\bar{\\xi})\\|^{2}}\\\\ {\\displaystyle\\leq2\\|\\nabla_{x}F(x_{1},y;\\xi)-\\nabla_{x}F(x_{2},y;\\xi)\\|^{2}+2\\left\\|\\nabla_{x y}^{2}G(x_{1},y;\\zeta^{(0)})\\left[\\displaystyle\\frac{Q}{l_{g,1}}\\displaystyle\\prod_{i=1}^{\\ell}\\left(I-\\displaystyle\\frac{1}{l_{g,1}}\\nabla_{y y}^{2}G(x_{1},y;\\zeta^{(i)})\\right)\\right]\\nabla_{y}\\right.}\\\\ {\\displaystyle\\qquad\\qquad\\qquad-\\nabla_{x y}^{2}G(x_{2},y;\\zeta^{(0)})\\left[\\displaystyle\\frac{Q}{l_{g,1}}\\displaystyle\\prod_{i=1}^{\\ell}\\left(I-\\displaystyle\\frac{1}{l_{g,1}}\\nabla_{y y}^{2}G(x_{2},y;\\zeta^{(i)})\\right)\\right]\\nabla_{y}F(x_{2},y;\\xi)\\right\\|^{2}}\\\\ {\\displaystyle\\leq2(L_{x,0}+L_{x,1}\\|\\nabla_{x}f(x_{1},y)\\|)^{2}\\|x_{1}-x_{2}\\|^{2}+2\\left\\|\\nabla_{x y}^{2}G(x_{1},y;\\zeta^{(0)})\\left[\\displaystyle\\frac{Q}{l_{g,1}}\\displaystyle\\prod_{i=1}^{\\ell}\\left(I-\\displaystyle\\frac{1}{l_{g,1}}\\nabla_{y y}^{2}G(x_{1},y;\\zeta^{(i)})\\right)\\right.}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\left.-\\nabla_{x y}^{2}G(x_{2},y;\\zeta^{(0)})\\left[\\displaystyle\\frac{Q}{l_{g,1}}\\displaystyle\\prod_{i=1}^{\\ell}\\left(I-\\displaystyle\\frac{1}{l_{g,1}}\\nabla_{y y}^{2}G(x_{2},y;\\zeta^{(i)})\\right)\\right]\\nabla_{y}F(x_{2},y;\\xi)\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the second term above, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Bigg|\\nabla_{x}^{2}\\sigma(x_{1},y;\\zeta^{(0)})\\Bigg[\\frac{Q}{I_{1}}\\displaystyle\\prod_{i=1}^{1}\\Bigg(I-\\frac{1}{I_{2,i}}\\nabla_{y}^{2}G(x_{1},y;\\zeta^{(i)})\\Bigg)\\Bigg]\\nabla_{y}V(x_{1},y;\\zeta^{(i)})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\nabla_{x_{0}^{2}}^{2}G(z_{2},y;\\zeta^{(i)})\\left[\\frac{Q}{I_{2}}\\displaystyle\\prod_{i=1}^{1}\\left(I-\\frac{1}{I_{2,i}}\\nabla_{y}^{2}G(x_{2},y;\\zeta^{(i)})\\right)\\right]\\nabla_{y}V(x_{2},y;\\zeta^{(i)})\\Bigg|^{2}}\\\\ &{\\le3\\mu_{\\sigma_{1}\\sigma_{2}}^{2}\\,\\Bigg(1-\\frac{Q}{I_{3,i}}\\Bigg)^{2}\\,\\Bigg|\\nabla_{y}V(x_{1},y;\\zeta^{(i)},\\nabla_{y}V(x_{1},y;\\zeta^{(i)})-\\nabla_{y}V(x_{2},y;\\zeta^{(i)})\\Bigg|^{2}}\\\\ &{\\qquad\\qquad\\qquad+37\\rho_{\\sigma_{1}\\sigma_{2}}^{2}\\,\\Bigg(1-\\frac{\\mu}{I_{2,i}}\\bigg)^{2}\\,\\Bigg|\\nabla_{x_{0}^{2}}^{2}G(x_{1},y;\\zeta^{(i)})-\\nabla_{x y}^{2}G(x_{2},y;\\zeta^{(i)})\\Bigg|^{2}}\\\\ &{\\qquad\\qquad\\qquad+37\\rho_{\\sigma_{1}\\sigma_{2}}^{2}\\,\\Bigg|\\frac{Q}{I_{1,1}}\\displaystyle\\prod_{i=1}^{1}\\Bigg(I-\\frac{1}{I_{2,i}}\\nabla_{y}^{2}G(x_{1},y;\\zeta^{(i)})\\Bigg)-\\frac{Q}{I_{1,1}}\\displaystyle\\prod_{i=1}^{1}\\left(I-\\frac{1}{I_{2,i}}\\nabla_{y}^{2}G(x_{2},y;\\zeta^{(i)})\\right)\\Bigg|^{2}}\\\\ &{\\le3\\sigma^{2}\\left(1-\\frac{\\mu}{I_{2,i}}\\right)^{2}\\,\\Bigg(I_{1,0}+E_{x_{1}}\\\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then we take expectation with respect to $\\mathfrak{q}$ and obtain ", "text_level": 1, "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta}\\left\\lVert\\nabla_{y}^{2}\\theta(x,y;\\theta^{*})\\right\\rVert\\left[\\frac{Q}{1+\\epsilon}\\right]\\prod_{s=1}^{3}\\Bigg(I-\\frac{1}{L_{\\theta}}\\nabla_{y}^{2}G(x,y;\\theta^{*})\\Bigg)\\Bigg[\\nabla_{y}F(x,y;\\theta^{*})}\\\\ &{\\qquad\\qquad\\qquad\\quad-\\nabla_{x}^{2}G(x,y;\\theta^{*})\\left[\\frac{Q}{1+\\epsilon}\\right]\\left[\\left(I-\\frac{1}{L_{\\theta}}\\nabla_{y}^{2}G(x,y;\\theta^{*})\\right)\\right]\\nabla_{y}F(x,y;\\theta^{*})\\Bigg]}\\\\ &{\\le\\Bigg(M^{2}(I-\\frac{\\mu}{L_{\\theta}}\\nabla_{y}J(x,y))\\sqrt{2}\\left[I-\\frac{\\mu}{L_{\\theta}}\\right]+2\\frac{\\sqrt{\\mu}_{0}^{2}}{\\mu_{0}\\mu_{0}^{2}}\\mu_{0}^{2}\\left[\\alpha_{1}-\\frac{\\mu}{L_{\\theta}}\\right]^{2}\\Bigg)\\mathbb{E}_{\\theta}\\Bigg[\\left(I-\\frac{\\mu}{L_{\\theta}}\\right)^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\quad+3\\eta_{0}^{2}Q\\mathbb{E}_{\\theta}\\left[\\frac{1}{L_{\\theta}}\\Bigg(I-\\frac{1}{L_{\\theta}}\\nabla_{y}^{2}G(x,y;\\theta^{*})-\\frac{1}{L_{\\theta}}\\Bigg[I-\\frac{1}{L_{\\theta}}\\nabla_{y}^{2}G(x,y;\\theta^{*})\\Bigg]\\right)^{2}\\Bigg]}\\\\ &{\\le\\Bigg(M^{2}(I\\nu_{x}+L_{\\theta})\\nabla_{y}J(x,y))\\Bigg[\\frac{Q}{1+\\mu}[-z_{0}]^{2}(\\frac{\\mu}{L_{\\theta}}\\nabla_{y}^{2}G(x,y;\\theta^{*}))-z_{0}^{2}\\Bigg]+\\Bigg(I-\\frac{1}{L_{\\theta}}\\nabla_{y}^{2}G(x,y;\\theta^{*})}\\\\ &{\\qquad\\qquad\\quad+3\\eta_{0}^{2}Q^{2}\\cdot\\frac{L_{\\theta}^{2}\\,Q}{16\\mu_{0}^{2}}\\frac{\\mu_{0}^{2}}{\\mu_{0}\\mu_{0}^{2}}\\Bigg)\\Bigg[\\pi_ \n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally, taking expectation on both sides yields ", "text_level": 1, "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{c}}_{\\bar{\\xi}}\\|\\bar{\\nabla}f(x_{1},y;\\bar{\\xi})-\\bar{\\nabla}f(x_{2},y;\\bar{\\xi})\\|^{2}\\leq2(L_{x,0}+L_{x,1}\\|\\nabla_{x}f(x_{1},y)\\|)^{2}\\|x_{1}-x_{2}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{6Q}{2\\mu l_{g,1}-\\mu^{2}}\\left(l_{g,1}^{2}(L_{y,0}+L_{y,1}l_{f,0})^{2}+l_{f,0}^{2}l_{g,2}^{2}+\\displaystyle\\frac{l_{f,0}^{2}l_{g,1}^{2}l_{g,2}^{2}Q^{2}}{(l_{g,1}-\\mu)^{2}}\\right)\\|x_{1}-x_{2}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since for any $\\boldsymbol{y}\\in\\mathbb{R}^{d_{y}}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{x}f(x_{1},y)-\\nabla_{x}f(x_{1},y_{1}^{*})\\|\\leq(L_{x,0}+L_{x,1}\\|\\nabla_{x}f(x_{1},y_{1}^{*})\\|)\\|y_{1}-y_{1}^{*}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left(L_{x,0}+L_{x,1}\\left(\\frac{l_{g,1}l_{f,0}}{\\mu}+\\|\\nabla\\Phi(x_{1})\\|\\right)\\right)\\|y_{1}-y_{1}^{*}\\|}\\\\ &{\\qquad\\qquad\\qquad=\\left(L_{x,0}+\\frac{L_{x,1}l_{g,1}l_{f,0}}{\\mu}+L_{x,1}\\|\\nabla\\Phi(x_{1})\\|\\right)\\|y_{1}-y_{1}^{*}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which yields ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}f(x_{1},y)\\|\\leq\\|\\nabla_{x}f(x_{1},y_{1}^{*})\\|+\\left(L_{x,0}+\\frac{L_{x,1}l_{g,1}l_{f,0}}{\\mu}+L_{x,1}\\|\\nabla\\Phi(x_{1})\\|\\right)\\|y_{1}-y_{1}^{*}\\|}\\\\ &{\\qquad\\qquad\\leq\\frac{l_{g,1}l_{f,0}}{\\mu}+\\|\\nabla\\Phi(x_{1})\\|+\\left(L_{x,0}+\\frac{L_{x,1}l_{g,1}l_{f,0}}{\\mu}+L_{x,1}\\|\\nabla\\Phi(x_{1})\\|\\right)\\|y_{1}-y_{1}^{*}\\|}\\\\ &{\\qquad\\qquad=\\left(\\frac{l_{g,1}l_{f,0}}{\\mu}+\\left(L_{x,0}+\\frac{L_{x,1}l_{g,1}l_{f,0}}{\\mu}\\right)\\|y_{1}-y_{1}^{*}\\|\\right)+\\left(1+L_{x,1}\\|y_{1}-y_{1}^{*}\\|\\right)\\|\\nabla\\Phi(x_{1})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, we conclude that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{\\mu}}_{\\bar{t}}\\|\\nabla f(x_{1},y;\\xi)-\\nabla f(x_{2},y;\\xi)\\|^{2}\\leq2(L_{x,0}+L_{x,1}\\|\\nabla_{x}f(x_{1},y)\\|)^{2}\\|x_{1}-x_{2}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{6Q}_{2\\mu\\bar{t},1}\\left(\\frac{1}{\\mu_{1}}(L_{x,0}+L_{x,1}I_{f,0})^{2}+I_{f,0}^{2}\\mu_{2,2}^{2}+\\frac{I_{f,0}^{2}I_{g,1}^{2}\\rho_{2,2}^{2}}{(\\mu_{1}-\\mu)^{2}}\\right)\\|x_{1}-x_{2}\\|^{2}}\\\\ &{\\leq2\\left(L_{x,0}+L_{x,1}\\left(\\frac{I_{g,1}I_{f,0}}{\\mu}+\\left(L_{x,0}+\\frac{L_{x,1}I_{f,0}}{\\mu}\\right)\\|y_{1}-y_{1}\\|\\right)+L_{x,1}(1+L_{x,1}\\|y_{1}-y_{1}^{*}\\|)\\|\\nabla\\phi\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\underbrace{6Q}_{2\\mu\\bar{t},1}\\left(\\frac{\\bar{L}_{x,0}}{\\mu_{1}-\\mu^{2}}+\\frac{L_{x,1}I_{f,0}}{\\mu_{1}-\\mu}\\right)^{2}+I_{f,0}^{2}\\mu_{2,2}^{2}+\\frac{I_{f,0}^{2}I_{g,1}^{2}\\rho_{2,2}^{2}}{(\\mu_{1}-\\mu)^{2}}\\right)\\|x_{1}-x_{2}\\|^{2}}\\\\ &{\\leq4\\left(L_{x,0}+L_{x,1}\\left(\\frac{I_{g,1}I_{f,0}}{\\mu}+\\left(L_{x,0}+\\frac{L_{x,1}I_{f,0}}{\\mu}\\right)\\|y_{1}-y_{1}\\|\\right)\\right)^{2}\\|x_{1}-x_{2}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\frac{4L_{x,1}^{2}}{2}(1+L_{x,1}\\|y_{1}-y_{1}^{*}\\|)^{2}\\|\\nabla\\Phi(x_{1})\\|^{2 \n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we use the definition of $\\bar{L}_{0}$ and ${\\bar{L}}_{1}$ in the last equality. ", "page_idx": 35}, {"type": "text", "text": "G   Additional Experimental Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Hyerparameter setting. We tune the best hyperparameters for each algorithm, including upper/lower-level step size, the number of inner loops, momentum parameters, etc. The upper-level learning rate $\\eta_{u p}$ and lower-level learing rate $\\eta_{l o w}$ are tuned in the range of [0.001, 0.1] for all the baselines on experiments of AUC maximization and data hyper-cleaning, the best $(\\eta_{u p},\\eta_{l o w})$ on AUC maximization are summarized as follows: StocBio: (0.01, 0.001), TTSA: (0.005, 0.01), SABA: (0.01, 0.005), MA-SOBA: (0.01,0.005), SUSTAIN: (0.03,0.01), VRBO: (0.05,0.01), BO-REP: (0.001, 0.001), AccBO: (0.005, 0.005). The best learning rate on the experiment of data hyper-cleaning are summarized as follows: Stocbio: (0.01, 0.002), TTSA: (0.001, 0.01), SABA: (0.05, 0.02), MA-SOBA: (0.01, 0.01), SUSTAIN: (0.05, 0.05), VRBO: (0.1,0.05), BOREP: (0.02, 0.01), AccBO: (0.1, 0.1). Note that SUSTAIN decays its upper-/lower-level step size with epoch $(t)$ by $\\eta_{u p}=\\eta_{u p}/(t\\!+\\!2)^{1/3},\\eta_{l o w}=\\eta_{u p}/(t\\!+\\!2)^{1/3}$ , while other algorithms use a constant learning rate. The number for neumann series estimation in StocBiO and VRBO is fixed to 3, while it is uniformly sampled from $\\{1,2,3\\}$ in TTSA, SUSTAIN, and AccBO. In AUC maximization, AccBO uses Option I (Option $\\mathrm{II}$ in data hyper-cleaning) to update the lower-level variable, and sets the Nestrov momentum parameter $\\gamma=0.5$ , the averaging parameter $\\tau=0.5$ $(\\gamma=0.1$ and $\\tau=0.5$ in data hyper-cleaning). In AUC maximization, the batch size is set to be 32 for all algorithms except VRBO, which uses larger batch size of 64 (tuned in the range of $\\{32,64,128,256,512\\}$ ) at the checkpoint step and 32 otherwise. In data hyper-cleaning, the batch size is set to be 128 for all algorithms except VRBO, which uses larger batch size of 256 (tuned in the range of $\\{63,128,256,512,1024\\}$ ) at the checkpoint step and 128 otherwise. AccBO uses Option $\\mathrm{II}$ in data hyper-cleaning, and the periodical update for low-level variable sets the iterations $N=3$ and update interval $I=2$ . Other hyperparameters setting keep the same in AUC maximization and data hyper-cleaning: The momentum parameter $\\beta$ is fixed to 0.9 in AccBO, MA-SOBA, BO-REP. The warm start steps for lower-level variable in AccBO is set to 3. The number of inner loops for StocBio is set to 3. BO-REP uses the periodical update for low-level variable, and set the iterations $N=3$ and the update interval $I=2$ ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Every claim made in the abstract is specified a section of the paper, including algorithm design and analysis in Section 4 and experiments in Section 5. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We discussed the limitations of our work in Section 6. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide both assumptions and proofs in Appendices C to E. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The experimental details are fully specified in Section 5 and Appendix G. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The code and data are attached as a supplement with instructions for reproducibility. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The experimental details are included in Section 5 and Appendix G. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: We only run once due to limited computational budget. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The hardware specification is described in Section 5. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have read and conformed to the NeurIPS Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning from algorithmic and theoretical aspects. We do not see any direct paths to negative societal impacts. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our paper does not involve the release of any data or models ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Our paper uses existing text classification datasets and are cited in Section 5 and their licenses are mentioned. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]