[{"figure_path": "XlpipUGygX/figures/figures_1_1.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure details the data annotation process, dataset creation, and policy training process for the ChessBench dataset.  The top section shows how board states and actions are annotated using Stockfish 16. The bottom left shows how the dataset is created, including the split into training and test sets of various sizes. The bottom right details the three types of predictors (state-value, action-value, and behavioral cloning) trained, and how these are used to create chess policies. The figure highlights the use of both game data from Lichess and curated chess puzzles to create the comprehensive dataset.", "section": "2 Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_6_1.jpg", "caption": "Figure 2: Puzzle solving comparison for our 270M transformer, Stockfish 16 (50ms per move), Leela Chess Zero, AlphaZero, and GPT-3.5-turbo-instruct on 10K Lichess puzzles (curated following [9]).", "description": "This figure compares the puzzle-solving performance of the 270M parameter transformer model against several other strong chess engines, including Stockfish 16 (with a 50ms time limit per move), Leela Chess Zero, AlphaZero, and GPT-3.5-turbo-instruct. The comparison is performed on a set of 10,000 curated Lichess puzzles, grouped by their Elo difficulty ratings. The figure shows the accuracy of each model in solving puzzles within different Elo rating ranges.  The results demonstrate the remarkable performance of the transformer model, even without using explicit search during testing, especially in comparison to GPT-3.5-turbo-instruct.", "section": "3.2 Puzzles"}, {"figure_path": "XlpipUGygX/figures/figures_6_2.jpg", "caption": "Figure 3: Puzzle accuracy for different training set sizes (stated above panels) and model sizes (color-coded), evaluated on our small set of 1K puzzles. Generally, larger models trained on larger datasets lead to higher accuracy (which strongly correlates with test set performance and general chess playing strength), highlighting the importance of scale for strong chess play. This effect cannot be explained by memorization since <1.41% of the initial puzzle board states appear in our training set. If the model is too large in relation to the training set it overfits (left panel; loss curves in Figure A3).", "description": "The figure shows the puzzle accuracy for different training set sizes and model sizes.  It demonstrates that larger models trained on more data achieve higher accuracy.  This is not due to memorization as only a small percentage of the test puzzles were in the training data.  Overfitting is observed with the largest models on smaller datasets.", "section": "3.3 Scaling Analysis"}, {"figure_path": "XlpipUGygX/figures/figures_8_1.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure illustrates the process of creating the ChessBench dataset and training three different types of predictors (state-value, action-value, and behavioral cloning).  The top section shows the annotation process using Stockfish, while the bottom left shows the dataset creation with training and test sets of varying sizes.  The bottom right describes the three policy training methods with their respective loss functions.", "section": "2 Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_14_1.jpg", "caption": "Figure A1: Win percentage and move distributions for our action-value dataset generated from 1000 games (cf. Table A1). We use 50 buckets to generate the histograms. The win percentage distribution is skewed towards 0 as we consider all legal moves per board and most actions are not advantageous for the player.", "description": "This figure shows the distribution of win percentages and move frequencies in a subset of the ChessBench dataset.  The win percentage histogram demonstrates a skew towards 0%, reflecting that many moves in chess are not advantageous for the player. The move frequency plot provides insights into common moves at different stages of a game. Both distributions help to characterize the data used for training the transformer models.", "section": "A Experimental Setup"}, {"figure_path": "XlpipUGygX/figures/figures_14_2.jpg", "caption": "Figure A1: Win percentage and move distributions for our action-value dataset generated from 1000 games (cf. Table A1). We use 50 buckets to generate the histograms. The win percentage distribution is skewed towards 0 as we consider all legal moves per board and most actions are not advantageous for the player.", "description": "This figure shows two histograms. The first one visualizes the win percentage distribution of the dataset, which is heavily skewed towards 0%. This is because the dataset includes all legal moves, and most moves are not advantageous for the player. The second histogram displays the frequency of different moves in the dataset. The x-axis represents the move index, and the y-axis represents the count of the move. The distribution shows that some moves appear significantly more often than others.", "section": "A Experimental Setup"}, {"figure_path": "XlpipUGygX/figures/figures_17_1.jpg", "caption": "Figure A2: Train and test loss curves and puzzle accuracy over time for the models from Section 3.1. We observe no overfitting, which justifies always using the fully trained model in our evaluations.", "description": "This figure displays the training and testing loss curves, as well as the puzzle accuracy over time, for the three transformer models (9M, 136M, and 270M parameters) described in Section 3.1 of the paper.  The consistent decrease in both training and testing loss across all models indicates a lack of overfitting. The steady improvement in puzzle accuracy further reinforces the effectiveness of the training process.  The absence of overfitting supports the authors' decision to use the fully trained models in their evaluations, which is a common practice in machine learning to avoid bias from incomplete training.", "section": "A.2 Main Setup"}, {"figure_path": "XlpipUGygX/figures/figures_17_2.jpg", "caption": "Figure A3: Loss curves when scaling model size and training set size.", "description": "This figure shows the training and test loss curves for different model sizes (400K, 1M, 2M, 7M, 9M, 34M parameters) trained on different training set sizes (10K, 100K, 1M games). It demonstrates the impact of model and dataset size on model performance and the presence of overfitting for larger models trained on smaller datasets. The results show that larger models generally perform better, but only when there is enough training data to prevent overfitting. ", "section": "B.4 Loss Curves"}, {"figure_path": "XlpipUGygX/figures/figures_18_1.jpg", "caption": "Figure A5: Comparison of the three different prediction targets (action-value, state-value, and behavioral cloning) trained on exactly the same number of data points (40M). The superiority of action-value learning over state-value learning disappears (or even reverses to some degree), except when measuring the action-ranking correlation (Kendall\u2019s T) which the action-value policy is indirectly trained to perform well on.", "description": "This figure shows the result of an ablation study on the effect of different prediction targets (action-value, state-value, and behavioral cloning) when the training data size is controlled.  The results show that when the amount of training data is the same for all three targets, the superiority of the action-value approach diminishes.  The action-value predictor still outperforms others in terms of Kendall's Tau, which measures rank correlation, suggesting that the action-value target is still better at ranking actions.", "section": "B.5 Predictor-Target Comparison"}, {"figure_path": "XlpipUGygX/figures/figures_19_1.jpg", "caption": "Figure A5: Comparison of the three different prediction targets (action-value, state-value, and behavioral cloning) trained on exactly the same number of data points (40M). The superiority of action-value learning over state-value learning disappears (or even reverses to some degree), except when measuring the action-ranking correlation (Kendall\u2019s \u03c4) which the action-value policy is indirectly trained to perform well on.", "description": "This figure compares the performance of three different prediction targets (action-value, state-value, and behavioral cloning) when trained on the same amount of data.  It shows that the action-value and state-value predictors perform similarly when given equal data, unlike what was observed when the datasets for each model were of differing sizes (Figure A4).  The behavioral cloning method underperforms consistently.", "section": "B.5 Predictor-Target Comparison"}, {"figure_path": "XlpipUGygX/figures/figures_20_1.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure shows the process of creating the ChessBench dataset and training three different predictors: state-value, action-value, and behavioral cloning.  The dataset is created by extracting unique board states from Lichess games and annotating them with state values and action values from Stockfish. Three different predictors are trained using this dataset, each with a different prediction target and loss function.  The figure also shows the datasets used, including training and testing sets, and the policies generated for each predictor.", "section": "2 Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_2.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure illustrates the data annotation process, dataset creation, and policy training methods used in the paper.  The top section shows how board states and actions are annotated using Stockfish, a strong chess engine. The bottom left shows the creation of training and test datasets of various sizes, highlighting the inclusion of chess puzzles as a test set.  The bottom right illustrates the three different supervised learning approaches used to train the neural network predictors: predicting state values, action values, and behavioral cloning, and shows how the output of each predictor is used to create a chess playing policy.", "section": "2 Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_3.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure describes the data annotation, dataset creation, and training policies used in the ChessBench study.  The top section illustrates how board states and their values (state-value and action-value) are extracted from Lichess games using Stockfish as an oracle. The bottom left shows the creation of training and testing datasets of varying sizes from this data, highlighting the overlap between training and test sets. The bottom right details the three different prediction targets (state-value, action-value, and behavioral cloning) used to train the transformer models and the resulting policies.", "section": "Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_4.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure illustrates the process of creating the ChessBench dataset and training three different types of predictors.  The top section shows how board states and their corresponding values (state-value, action-value, and best action) are extracted from Lichess games using Stockfish.  The bottom left section details the creation of the training and testing datasets of varying sizes, highlighting the inclusion of chess puzzles. The bottom right section explains the three different prediction targets used for training the models (state-value, action-value, and behavioral cloning), along with the policy learning methods.", "section": "2 Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_5.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure shows the overall workflow of the paper. The top part illustrates the data annotation process using Stockfish 16 to extract board states, compute state values, and action values.  The bottom-left shows how the dataset is constructed, including training and testing sets with different sizes and compositions.  The bottom-right displays the three different policies (state-value, action-value, and behavioral cloning) used in the paper, which are trained on different targets and used for policy prediction. The different predictors are trained on various sizes of datasets, which include datasets based on games from Lichess and chess puzzles from Lichess.", "section": "Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_6.jpg", "caption": "Figure 4: Two options to win the game in 3 or 5 moves, respectively (more options exist). Since they both map into the highest-value bin our bot ignores Nh6+, the fastest way to win (in 3), and instead plays Nd6+ (mate-in-5). Unfortunately, a state-based predictor without explicit search cannot guarantee that it will continue playing the Nd6+ strategy and thus might randomly alternate between different strategies. Overall this increases the risk of drawing the game or losing due to a subsequent (low-probability) mistake, such as a bad softmax sample. Board from a game between our 9M Transformer (white) and a human (blitz Elo of 2145).", "description": "This figure shows two possible move sequences for winning a chess game, both leading to the same predicted win probability.  The model, lacking search capabilities, fails to select the optimal (faster) winning sequence and instead chooses a longer one, demonstrating the limitations of searchless approaches to complex planning problems like chess.", "section": "4 Discussion"}, {"figure_path": "XlpipUGygX/figures/figures_20_7.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure illustrates the data annotation process, dataset creation, and policy training methods used in the study.  The top section details how board states and their corresponding values and best moves are extracted from Lichess games using Stockfish. The bottom left shows the construction of training and test datasets from various sources with sizes indicated.  Finally, the bottom right explains the three training approaches: state-value, action-value, and behavioral cloning.", "section": "Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_8.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure details the process of data annotation, dataset creation, and policy training for the ChessBench project.  The top section shows how board states and actions are annotated using Stockfish, a strong chess engine.  The bottom left shows how the resulting data is split into training and testing sets, including a puzzle test set. The bottom right shows the three different types of predictors used (state-value, action-value, and behavioral cloning) and their corresponding policy creation methods.", "section": "Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_9.jpg", "caption": "Figure 4: Two options to win the game in 3 or 5 moves, respectively (more options exist). Since they both map into the highest-value bin our bot ignores Nh6+, the fastest way to win (in 3), and instead plays Nd6+ (mate-in-5). Unfortunately, a state-based predictor without explicit search cannot guarantee that it will continue playing the Nd6+ strategy and thus might randomly alternate between different strategies. Overall this increases the risk of drawing the game or losing due to a subsequent (low-probability) mistake, such as a bad softmax sample. Board from a game between our 9M Transformer (white) and a human (blitz Elo of 2145).", "description": "This figure shows two possible move sequences for winning a chess game, both leading to the highest predicted value. The model chooses the longer sequence (mate in 5 moves), highlighting the limitations of searchless approaches in handling certain situations, where the model may not consistently follow the optimal strategy due to the random nature of its decisions and risk of making suboptimal choices.", "section": "4 Discussion"}, {"figure_path": "XlpipUGygX/figures/figures_20_10.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure details the data annotation, dataset creation, and policy training process used in the study. The top section illustrates how board states and actions are annotated using Stockfish 16, a state-of-the-art chess engine. The bottom left describes the creation of training and testing datasets from Lichess games and puzzles.  Finally, the bottom right illustrates the three prediction targets and policy learning methods.", "section": "2 Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_11.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure illustrates the process of creating the ChessBench dataset and training three different types of predictors for chess. The top part shows the process of annotating chess game boards with values and best moves using Stockfish. The bottom left part explains how training and testing datasets were created. The bottom right part details the three different policies that were trained for the chess AI and how the output was created.", "section": "2 Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_12.jpg", "caption": "Figure 1: Top (Data annotation): We extract all boards from N randomly drawn games from Lichess, discard duplicate board states, and compute the state-value for each board as the win-probability via Stockfish. We compute action-values and the best action for all legal moves of a board state in the same way. Bottom left (Dataset creation): We construct training and test sets of various sizes (see Table A1). Our largest training set has 15.3B action-values. Drawing games i.i.d. from the game database for our test set leads to 14.7% of test boards appearing in the largest training set (mostly very early game states). We also use a test set of 10K chess puzzles that come with a correct sequence of moves. Bottom right (Policies): We train predictors on three targets (state- or action-values, or oracle actions), each of which can be used for a chess policy. Our value predictors are discrete discriminators (classifiers) that predict into which bin zi \u2208 {\u22481,..., ZK } the oracle value falls.", "description": "This figure illustrates the data annotation, dataset creation, and policy training process for the ChessBench dataset.  The top part shows how board states and action values are extracted from Lichess games using Stockfish. The bottom left shows how training and testing datasets of varying sizes are created. The bottom right details three different policy training approaches based on state values, action values, and behavioral cloning.", "section": "Methodology"}, {"figure_path": "XlpipUGygX/figures/figures_20_13.jpg", "caption": "Figure A6: Example of the learned tactics for our 270M transformer vs. a human player with a blitz Elo of 2145 (the game progresses from left to right and top to bottom). Our model decides to sacrifice two pawns since the white bishop will not be able to prevent it from promoting one of the pawns.", "description": "This figure shows several steps of a chess game between a 270M parameter transformer model and a human player.  The model, despite material disadvantage, strategically sacrifices pawns to force a checkmate, demonstrating an understanding of long-term strategic planning in chess.  The figure highlights the model's ability to find complex tactical solutions, even if it involves material sacrifice. Each step is accompanied by an analysis from Stockfish, a strong chess engine, confirming the optimality of the moves played by the model. ", "section": "B.7 Tactics"}, {"figure_path": "XlpipUGygX/figures/figures_21_1.jpg", "caption": "Figure A7: Examples of our 270M transformer's playing style against online human opponents.", "description": "This figure visualizes seven examples of the 270M transformer's playing style against human opponents in online chess games.  Each example showcases a specific tactical or strategic decision made by the AI, highlighting its aggressive, enterprising style.  The annotations describe the characteristics of the AI's play, such as king safety, material sacrifices for long-term advantage, and the pursuit of moves that create difficult choices for human opponents, even if they are not objectively optimal according to traditional chess evaluation metrics.  The figure provides visual evidence supporting the qualitative assessment of the AI's playing style as discussed in the paper.", "section": "B.8 Playing Style"}]