[{"figure_path": "luQiVmnviX/figures/figures_1_1.jpg", "caption": "Figure 1: illustrates the prompt brittleness of ICL and the effectiveness of our method in mitigating this issue. Experiments are conducted in one-shot setting, using SST2 [Socher et al., 2013] dataset for experiments on example selection and prompt formatting and AGnews [Zhang et al., 2015] dataset for example order experiment due to more diverse combination of orders.", "description": "The figure shows the impact of prompt brittleness on the accuracy of In-context Learning (ICL) and how the proposed UniBias method improves the robustness of ICL across different design settings (example selection, prompt formatting, and example order).  The box plots visually represent the accuracy distributions for both ICL and UniBias under various conditions, showcasing UniBias's improved performance and reduced sensitivity to prompt variations.  The datasets used are SST2 for example selection and prompt formatting, and AGNews for example order.", "section": "1 Introduction"}, {"figure_path": "luQiVmnviX/figures/figures_3_1.jpg", "caption": "Figure 2: Unveiling vanilla label bias by un-contextual accumulated FFN logits.", "description": "This figure shows the accumulated uncontextual FFN logits for different label names in a sentiment analysis task, along with their corresponding zero-shot prediction frequencies on the SST-2 dataset.  It demonstrates how the uncontextual accumulated FFN logits, reflecting the inherent bias of the LLM toward predicting certain label names without input context, correlate with prediction frequencies.  Labels with higher accumulated logits tend to have higher prediction frequencies.", "section": "2 Internal Mechanisms Causing the Bias of LLMs"}, {"figure_path": "luQiVmnviX/figures/figures_3_2.jpg", "caption": "Figure 3: The internal mechanism of the recency bias.", "description": "This figure shows the internal mechanism of recency bias in LLMs. It compares the behavior of a biased attention head (layer 16, head 29) and an unbiased attention head (layer 16, head 19) in terms of the attention weights assigned to examples at different positions and the label logits of the corresponding attention head's output. The biased attention head consistently assigns larger weights to the last example regardless of its label, demonstrating the recency bias. In contrast, the unbiased attention head assigns similar weights to both examples.", "section": "2.2 Internal Mechanisms of Bias Factors"}, {"figure_path": "luQiVmnviX/figures/figures_4_1.jpg", "caption": "Figure 4: The internal mechanism of the selection bias.", "description": "This figure shows the internal mechanism of selection bias in LLMs. The leftmost subplot displays the accumulated FFN label logits, revealing an inherent bias favoring option A. The middle and rightmost subplots illustrate the attention weights assigned by a biased attention head (layer 24, head 29) to different option positions, both in the original and reversed option sequences.  The biased attention head consistently assigns higher weights to the first option, regardless of the ground truth label, thus revealing the positional bias which contributes to the selection bias.", "section": "2 Internal Mechanisms Causing the Bias of LLMs"}, {"figure_path": "luQiVmnviX/figures/figures_7_1.jpg", "caption": "Figure 5: The performance comparison under different numbers of ICL shots using Llama-2-7b.", "description": "This figure compares the performance of different methods (ICL, CC, DC, PC, and UniBias) on three datasets (COPA, SST-2, and MMLU) under varying numbers of in-context learning (ICL) shots (0-shot, 1-shot, 2-shot, and 4-shot).  It visually demonstrates the effectiveness of the UniBias method in improving the accuracy and robustness of ICL across different shot settings and datasets. The results show that UniBias consistently outperforms other methods across all datasets and shot settings, highlighting its effectiveness in mitigating LLM bias.", "section": "4.2 Main Experiments"}, {"figure_path": "luQiVmnviX/figures/figures_14_1.jpg", "caption": "Figure 7: Performance comparison of our UniBias method against baseline methods using GPT-J and GPT2-XL models.", "description": "This figure compares the performance of the UniBias method against several baseline methods (ICL, CC, DC, and PC) across six different datasets (SST-2, WiC, COPA, MR, RTE, and the average performance across all datasets) using two different language models: GPT-J and GPT2-XL.  It shows the accuracy achieved by each method on each dataset, providing a visual representation of the relative effectiveness of UniBias compared to the baselines. The results clearly demonstrate UniBias' superiority over other methods across most datasets and language models.", "section": "4 Experiments"}, {"figure_path": "luQiVmnviX/figures/figures_15_1.jpg", "caption": "Figure 8: Performance of Unibias under different support set.", "description": "This figure shows the impact of support set size on the performance of the UniBias method. Two datasets, SST-2 and COPA, are used for evaluation.  The x-axis represents the support set size (number of samples per class), and the y-axis represents the accuracy.  The results indicate that performance generally improves with larger support sets but stabilizes beyond a certain point (around 20-30 samples per class).  In both datasets, UniBias consistently outperforms the standard ICL (In-context learning) method across all support set sizes.", "section": "E Impact of Support Set Size"}, {"figure_path": "luQiVmnviX/figures/figures_15_2.jpg", "caption": "Figure 9: Performance of Unibias using unlabeled samples as support set. It is compared against standard ICL and the original Unibias.", "description": "This figure shows the performance comparison of the UniBias method using unlabeled samples for the support set against the standard ICL and the original UniBias method. The x-axis represents the number of unlabeled samples multiplied by the number of classes, while the y-axis shows the accuracy. Two datasets, SST-2 and COPA, are included. The results suggest that using unlabeled samples in UniBias achieves similar performance compared to using labeled samples, which is beneficial when labeled data is scarce.", "section": "F Using Unlabeled Samples for Support Set"}]