[{"figure_path": "luQiVmnviX/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of one-shot ICL performance for different methods across datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation are reported for five repetitions with different ICL examples.", "description": "This table presents a comparison of the performance of different methods for one-shot in-context learning (ICL) across twelve different datasets, using two different sized language models (Llama-2 7b and Llama-2 13b).  The performance metric is accuracy, and the results are averaged over five repetitions with different ICL examples to provide a measure of variability. The methods compared include standard ICL, Contextual Calibration (CC), Domain-Context Calibration (DC), Prototypical Calibration (PC), and the proposed UniBias method.", "section": "4.2 Main Experiments"}, {"figure_path": "luQiVmnviX/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of one-shot ICL performance for different methods across datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation are reported for five repetitions with different ICL examples.", "description": "This table presents a comparison of the performance of different methods on various NLP tasks using two different sizes of the Llama-2 language model.  The methods compared are standard In-Context Learning (ICL), Contextual Calibration (CC), Domain-Context Calibration (DC), Prototypical Calibration (PC), and the proposed UniBias method.  The performance is measured as accuracy, with mean and standard deviation calculated over five repetitions using different example sets for each task.", "section": "4.2 Main Experiments"}, {"figure_path": "luQiVmnviX/tables/tables_9_1.jpg", "caption": "Table 1: Comparison of one-shot ICL performance for different methods across datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation are reported for five repetitions with different ICL examples.", "description": "This table presents a comparison of the performance of different methods on twelve datasets using two different Llama models (7B and 13B parameters).  The methods compared are the standard In-context Learning (ICL), Contextual Calibration (CC), Domain-Context Calibration (DC), Prototypical Calibration (PC), and the proposed UniBias method. The table shows the mean and standard deviation of accuracy across five repetitions, each with different in-context learning examples.  This allows for an assessment of the robustness and effectiveness of each method across various datasets and model sizes.", "section": "4.2 Main Experiments"}, {"figure_path": "luQiVmnviX/tables/tables_13_1.jpg", "caption": "Table 4: Detailed Dataset information", "description": "This table lists the twelve datasets used in the UniBias experiments, categorized by task (sentiment classification, topic classification, natural language inference, reasoning, and word disambiguation).  For each dataset, the number of classes and the size of the test set are provided.  The datasets represent a diverse range of natural language processing tasks and are used to demonstrate the effectiveness of UniBias across various scenarios.", "section": "A Experimental Details"}, {"figure_path": "luQiVmnviX/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of one-shot ICL performance for different methods across datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation are reported for five repetitions with different ICL examples.", "description": "This table presents a comparison of the performance of different methods on twelve datasets using two different Llama models. The performance metric is one-shot in-context learning (ICL) accuracy.  The methods compared are standard ICL, Contextual Calibration (CC), Domain-Context Calibration (DC), Prototypical Calibration (PC), and the proposed UniBias method.  Results are averaged over five runs with different example sets and show the mean and standard deviation of the accuracy for each method on each dataset.", "section": "4.2 Main Experiments"}, {"figure_path": "luQiVmnviX/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of one-shot ICL performance for different methods across datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation are reported for five repetitions with different ICL examples.", "description": "This table presents a comparison of the performance of different methods (ICL, CC, DC, PC, UniBias) on 12 different NLP datasets using two different Llama language models (7B and 13B parameters).  The results are averaged over five repetitions with different in-context learning (ICL) examples, showing the mean accuracy and standard deviation for each method and dataset. This allows for a comprehensive comparison of the effectiveness of each method across various tasks and model sizes.", "section": "4.2 Main Experiments"}, {"figure_path": "luQiVmnviX/tables/tables_17_2.jpg", "caption": "Table 1: Comparison of one-shot ICL performance for different methods across datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation are reported for five repetitions with different ICL examples.", "description": "This table presents a comparison of the performance of one-shot in-context learning (ICL) across various datasets using two different Llama models (Llama-2 7b and Llama-2 13b).  Multiple methods are compared, including standard ICL and several debiasing/calibration techniques. The results are averaged over five repetitions to account for variance introduced by selecting different in-context learning examples. Mean accuracy and standard deviation are reported for each method/dataset combination.", "section": "4.2 Main Experiments"}]