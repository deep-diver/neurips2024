[{"type": "text", "text": "A Concept-Based Explainability Framework for Large Multimodal Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jayneel Parekh1 Pegah Khayatan1 Mustafa Shukor1 Alasdair Newson1 Matthieu Cord1,2 ", "page_idx": 0}, {"type": "text", "text": "1ISIR, Sorbonne Universit\u00e9, Paris, France 2Valeo.ai, Paris, France {jayneel.parekh, pegah.khayatan}@sorbonne-universite.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as \u201cmultimodal concepts\u201d. We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. Our implementation is publicly available.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the exceptional capacity of deep neural networks (DNNs) to address complex learning problems, one aspect that hinders their deployment is the lack of human-comprehensible understanding of their internal computations. This directly calls into question their reliability and trustworthiness [5, 30]. Consequently, this has boosted research efforts in interpretability/explainability of these models i.e. devising methods to gain human-understandable insights about their decision processes. The growth in ability of DNNs has been accompanied by a similar increase in their design complexity and computational intensiveness. This is epitomized by the rise of vision transformers [11] and large-language models (LLMs) [8, 44] which can deploy up to tens of billions of parameters. The effectiveness of these models for unimodal processing tasks has spurred their use in addressing multimodal tasks. In particular, visual encoders and LLMs are frequently combined to address tasks such as image captioning and VQA [2, 25, 29, 43, 45]. This recent class of models are referred to as large multimodal models (LMMs). ", "page_idx": 0}, {"type": "text", "text": "For interpretability research, LMMs have largely remained unexplored. Most prior works on interpreting models that process visual data, focus on convolutional neural network (CNN) based systems and classification as the underlying task. Multimodal tasks and transformer-based architectures have both been relatively less studied. LMMs operate at the intersection of both domains. Thus, despite their rapidly growing popularity, there have been very few prior attempts at understanding representations inside an LMM [34, 35, 41, 42]. ", "page_idx": 0}, {"type": "text", "text": "This paper aims to bridge some of these differences and study in greater detail the intermediate representations of LMMs. To this end, motivated by the concept activation vector (CAV) based approaches for CNNs [14, 15, 17, 22], we propose a novel dictionary-learning based Concept eXplainability method designed for application to LMMs, titled CoX-LMM. Our method is used to learn a concept dictionary to understand the representations of a pretrained LMM for a given word/token of interest (Eg. \u2018Dog\u2019). For this token, we build a matrix containing the LMM\u2019s internal representation of the token. We then linearly decompose this matrix using dictionary learning. The dictionary elements of our decomposition represent our concepts. The most interesting consequence of our method is that the learnt concepts exhibit a semantic structure that can be meaningfully grounded in both visual and textual domains. They are visually grounded by extracting the images which maximally activate these concepts. They can simultaneously be grounded in the textual domain by decoding the concept through the language model of the LMM and extracting the words/tokens they are most associated to. We refer to such concept representations as multimodal concepts. Our key contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel concept-based explainability framework CoX-LMM, that can be used to understand internal representations of large multimodal models. To the best of our knowledge, this is the first effort targeting multimodal models at this scale.   \n\u2022 Our dictionary learning based concept extraction approach is used to extract a multimodal concept dictionary wherein each concept can be semantically grounded simultaneously in both text and vision. We also extend the previous concept dictionary-learning strategies using a Semi-NMF based optimization.   \n\u2022 We experimentally validate the notion of multimodal concepts through both, qualitative visualizations and quantitative evaluation. Our learnt concept dictionary is shown to possess a meaningful multimodal grounding covering diverse concepts, and is useful to locally interpret representations of test samples LMMs. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Large Multimodal Models (LMMs) Large language models (LLMs) [8, 21, 33, 44] have emerged as the cornerstone of contemporary multimodal models. Typical large multimodal models (LMMs) [1, 4, 25, 26] comprise three components: LLMs, visual encoders, and light-weight connector modules to glue the two models. Remarkably, recent works have demonstrated that by keeping all pretrained models frozen and training only a few million parameters in the connector (e.g., a linear layer), LLMs can be adapted to understand images, videos, and audios [12, 23, 31, 43, 45], thus paving the way for solving multi-modal tasks. However, there is still a lack of effort aimed at understanding why such frozen LLMs can generalize to multimodal inputs. In this study, we try to decode the internal representation of LLMs when exposed to multimodal inputs. ", "page_idx": 1}, {"type": "text", "text": "Concept activation vector based approaches Concept based interpretability aim to extract the semantic content relevant for a model [9]. For post-hoc interpretation of pretrained models, concept activation vector (CAV) based approaches [15, 17, 22, 46\u201348] have been most widely used. The idea of CAV was first proposed by Kim et al. [22]. They define a concept as a set of user-specified examples. The concept is represented in the activation space of deep layer of a CNN by a hyperplane that separates these examples from a set of random examples. This direction in the activation space is referred to as the concept activation vector. Built upon CAV, ACE [17] automate the concept extraction process. CRAFT [15] proposed to learn a set of concepts for a class by decomposing activations of image crops via non-negative matrix factorization (NMF). Recently, Fel et al. [14] proposed a unified view of CAV-based approaches as variants of a dictionary learning problem. However, these methods have only been applied for interpretation of CNNs on classification tasks. LMMs on the contrary exhibit a different architecture. We propose a dictionary learning based concept extraction method, designed for LMMs. We also propose a Semi-NMF variant of the dictionary learning problem, which has not been previously considered for concept extraction. ", "page_idx": 1}, {"type": "text", "text": "Understanding VLM/LMM representations There has been an increasing interest in understanding internal representations of visual-language models (VLM) through the lens of multimodality. Shukor and Cord [42] analyse multimodal tokens and shows that despite being different, visual and perceptual tokens are implicitly aligned inside LLMs. Goh et al. [18] discover neurons termed multimodal, that activate for certain conceptual information given images as input. Recently proposed TEXTSPAN [16] and SpLiCE [7], aim to understand representations in CLIP [38] by decomposing its visual representations on textual representations. For LMMs, Palit et al. [34] extend the causal tracing used for LLMs to analyze information across different layers in an LMM. Schwettmann et al. [41] first proposed the notion of multimodal neurons existing within the LLM part of an LMM. They term the neurons \u201cmultimodal\u201d as they translate high-level visual information to corresponding information in text modality. The neurons are discovered by ranking them by a gradient based attribution score. Pan et al. [35] proposed a more refined algorithm to identify such neurons based on a different neuron importance measure that leverages architectural information of transformer MLP blocks. Instead, we propose to discover a concept structure in the token representations by learning a small dictionary of multimodally grounded concepts. Limiting the analysis to a specific token of interest allows our method to discover fine details about the token in the learnt concepts. ", "page_idx": 1}, {"type": "image", "img_path": "MvjLRFntW6/tmp/4c0a79a8bd839b7f99d6f7a3ff7d9db423db710cbf5dcc9df146c52412203d9c.jpg", "img_caption": ["Figure 1: Overview of multimodal concept extraction and grounding in CoX-LMM. Given a pretrained LMM for captioning and a target token (for eg. \u2018Dog\u2019), our method extracts internal representations of $f$ about $t$ , across many images. These representations are collated into a matrix Z. We linearly decompose $\\mathbf{Z}$ to learn a concept dictionary $\\mathbf{U}$ and its coefficients/activations $\\mathbf{V}$ . Each concept $u_{k}\\,\\in\\,\\mathbf{U}$ , is multimodally grounded in both visual and textual domains. For text grounding, we compute the set of most probable words ${\\bf T}_{k}$ by decoding $u_{k}$ through the unembedding matrix $W_{U}$ . Visual grounding $\\mathbf{X}_{k,M A S}$ is obtained via $v_{k}$ as the set of most activating samples. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Background for Large Multimodal Models (LMMs) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Model architecture. We consider a general model architecture for a large multimodal model $f$ , ltcahoyante srcisso.t innWsiges  toasfs  soifum: maae g s s ndaeenrd $f_{V}$ ,  ecaia rt prataisiosnnoaicbnilgaet  tecadso knc anwpeitctithoo nra $C$ a.y LadnaMdt $f_{L M}$ $N_{L}$ $f$ $\\boldsymbol{S}=\\{(\\boldsymbol{X}_{i},\\bar{\\boldsymbol{y}_{i}})\\}_{i=1}^{N}$ $X_{i}\\in\\mathcal{X}$ $y_{i}\\subset\\mathcal{Y}$ $\\mathcal{X}$ $\\boldsymbol{\\wp}$   \nimages and set of text tokens respectively. Note that caption $y_{i}$ can be viewed as a subset of all tokens. The input to the language model $f_{L M}$ is denoted by the sequence of tokens $h^{1},h^{2},...,h^{p}$ and the output as $\\hat{y}$ . The internal representation of any token at some layer $l$ and position $p$ inside $f_{L M}$ is denoted as $h_{(l)}^{p}$ , with $h_{(0)}^{p}=h^{p}$ . Note that $h_{(l)}^{p}$ is same as the residual stream representation in LLM transformers [13] at position $p$ and layer $l$ . For the multimodal model, the input sequence of tokens for $f_{L M}$ consists of the concatenation of: (1) $N_{V}$ visual tokens provided by the visual encoder $f_{V}$ operating on an image $X$ , followed by the connector $C$ , and (2) linearly embedded textual tokens previously predicted by $f_{L M}$ . For $p>N_{V}$ , this can be expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{y}^{p}=f_{L M}(h^{1},h^{2},\\ldots,h^{N_{V}},\\ldots,h^{p}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $h^{1},\\ldots,h^{N_{V}}\\,=\\,C(f_{V}(X))$ , and $h^{p}\\,=\\,\\operatorname{Emb}(\\hat{y}^{p-1})$ for $p\\,>\\,N_{V}$ , where Emb denotes the token embedding function. To start the prediction, $h^{N_{V}+1}$ is defined as the beginning of sentence token. The output token $\\hat{y}^{p}$ is obtained by normalizing $h_{(N_{L})}^{p}$ , followed by an unembedding layer that applies a matrix $W_{U}$ followed by a softmax. The predicted caption $\\hat{y}$ consists of the predicted tokens $\\hat{y}=\\{\\hat{y}^{p}\\}_{p>N_{V}}$ until the end of sentence token. ", "page_idx": 3}, {"type": "text", "text": "Training The model is trained with next token prediction objective, to generate text conditioned on images in an auto-regressive fashion. In this work we focus on models trained to \"translate\" images into text, or image captioning models. These models keep the visual encoder $f_{V}$ frozen and only train the connector $C$ . Recent models also finetune the LLM to improve performance. Our approach can be applied in either case, and in our experiments we consider both type of LMMs. However, we find the generalization of LLMs to multimodal inputs is an interesting phenomenon to understand, thus we focus more on the setup where the LLM is kept frozen. ", "page_idx": 3}, {"type": "text", "text": "Transformer representations view Central to many previous approaches interpreting decoderonly LLM/transformer architectures, is the \u201cresidual stream view\u201d of internal representations, first proposed in [13]. Herein, the network is seen as a composition of various computational blocks that \u201cread\u201d information from the residual stream of token representations h(pl), perform their computation, and add or \u201cwrite\u201d their output back in the residual stream. This view can be summarized as:. ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{(l+1)}^{p}=h_{(l)}^{p}+a_{(l)}^{p}+m_{(l)}^{p}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$a_{(l)}^{p}$ denotes the information computed by attention function at layer $l$ and position $p$ . It has a causal structure and computes its output using $h_{(l)}^{1},...,h_{(l)}^{p}$ . $m_{(l)}^{p}$ denotes the information computed by the MLP block. It is a feedforward network (FFN) with two fully-connected layers and an intermediate activation function $\\sigma$ , that operates on $h_{(l)}^{p}+a_{(l)}^{p}$ . The output of $\\sigma(.)$ is referred to as FFN activations. ", "page_idx": 3}, {"type": "text", "text": "3.2 Method overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Fig. 1 provides a visual summary of the whole CoX-LMM pipeline. Given a pretrained LMM $f$ and a token of interest $t\\in\\mathcal{V}$ , our method consists of three key parts: ", "page_idx": 3}, {"type": "text", "text": "1. Selecting a subset of images $\\mathbf{X}$ from dataset $\\boldsymbol{S}$ , relevant for target token $t$ . We extract representations by processing samples in $\\mathbf{X}$ through $f$ . The extracted representations of dimension $B$ are collected in a matrix $\\mathbf{Z}\\in\\mathbb{R}^{B\\times M}$ , where $M$ is number of samples in $\\mathbf{X}$ .   \n2. Linearly decomposing $\\mathbf{Z}\\approx\\mathbf{U}\\mathbf{V}$ into its constituents, that includes a dictionary of learnt concepts $\\mathbf{U}\\in\\mathbb{R}^{\\mathbf{\\star}}\\!B\\!\\times\\!K$ of size $K$ and coefficient/activation matrix $\\mathbf{V}\\in\\mathbb{R}^{K\\times M}$ .   \n3. Semantically grounding the learnt \u201cmultimodal concepts\u201d, contained in dictionary $\\mathbf{U}$ in both visual and textual modalities. ", "page_idx": 3}, {"type": "text", "text": "We emphasize at this point that our main objective in employing dictionary learning based concept extraction is to understand internal representations of an LMM. Thus, our focus is on validating the use of the learnt dictionary for this goal, and not to interpret the output of the model, which can be readily accomplished by combining this pipeline with some concept importance estimation method [14]. The rest of the section is devoted to elaborate on each of the above three steps. ", "page_idx": 3}, {"type": "text", "text": "3.3 Representation extraction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To extract relevant representations from the LMM about $t$ that encode meaningful semantic information, we first determine a set of samples $\\mathbf{X}$ from dataset ${\\cal{S}}=\\{(X_{i},y_{i})\\}_{i=1}^{N}$ for extraction. We consider the set of samples where $t$ is predicted as part of the predicted caption $\\hat{y}$ . This allows us to further investigate the model\u2019s internal representations of $t$ . To enhance visual interpretability for the extracted concept dictionary, we additionally limit this set of samples to those that contain $t$ in the ground-truth caption. Thus, $\\mathbf{X}$ is computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}=\\{X_{i}\\mid t\\in f(X_{i}){\\mathrm{~and~}}t\\in y_{i}{\\mathrm{~and~}}(X_{i},y_{i})\\in S\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given any $X\\in\\mathbf{X}$ , we propose to extract the residual stream representation $h_{(L)}^{p}$ from a deep layer $L$ , at the first position in the predicted caption $p>N_{V}$ , such that $\\hat{y}^{p}=t$ . The representation $z_{j}\\in\\mathbb{R}^{B}$ of each sample $X_{j}\\in\\mathbf{X}$ is then stacked as columns of the matrix $\\mathbf{Z}=[z_{1},...,z_{M}]\\in\\mathbb{R}^{B\\times M}$ . Note that the representations of text tokens in $f_{L M}$ can possess a meaningful multimodal structure as they combine information from visual token representations $h_{(l)}^{p},p\\leq\\bar{N}_{V}$ . In contrast to $a_{(l)}^{p}$ l) and m(pl), that represent residual information at layer $l$ , $h_{(L)}^{p}$ contains the aggregated information computed by the LMM till layer $L$ , providing a holistic view of its computation across all previous layers. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.4 Decomposing the representations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The representation matrix $\\mathbf{Z}\\,\\approx\\,\\mathbf{U}\\mathbf{V}$ , is decomposed as product of two low-rank matrices ${\\textbf{U}}\\in$ $\\mathbb{R}^{B\\times K}$ , $\\textbf{V}\\in\\,\\mathbb{R}^{K\\times M}$ of rank $K\\;<<\\;\\operatorname*{min}(B,M)$ , where $K$ denotes the number of dictionary elements. The columns of $\\mathbf{U}=[u_{1},...,u_{K}]$ are the basis vectors which we refer to as conceptvectors/concepts. The rows of $\\mathbf{V}$ or columns of $\\mathbf{V}^{T}=[v_{1},...,v_{K}]$ , $v_{i}\\in\\mathbb{R}^{M}$ denote the activations of $u_{i}$ for each sample. This decomposition, as previously studied in [14], can be optimized with various constraints on $\\mathbf{U},\\mathbf{V}$ , each leading to a different dictionary. The most common ones include PCA (constraint: $\\mathbf{U}^{T}\\mathbf{U}=\\mathbf{I},$ ), K-Means (constraint: columns of $\\mathbf{V}$ correspond to columns of identity matrix) and NMF (constraint: U $;\\mathbf{V}\\geq0;$ ). NMF is considered to yield most interpretable results [14]. However, for our use case, NMF is not useful as representation matrix $\\mathbf{Z}$ is not non-negative. Instead, we propose to employ a relaxed version of NMF, Semi-NMF [10], which allows the decomposition matrix $\\mathbf{Z}$ and basis vectors $\\mathbf{U}$ to contain mixed values, and only forces the activations $\\mathbf{V}$ to be non-negative. Note that given its relations to clustering algorithms [10], enforcing non-negative combinations of decompositions is still valued from an interpretability perspective. Since we expect only a small number of concepts to be present in any given sample, we also encourage sparsity in activations $\\mathbf{V}$ . The optimization problem to decompose $\\mathbf{Z}$ via Semi-NMF can be summarized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{U}^{*},\\mathbf{V}^{*}=\\operatorname*{arg\\,min}_{\\mathbf{U},\\mathbf{V}}\\;||\\mathbf{Z}-\\mathbf{U}\\mathbf{V}||_{F}^{2}+\\lambda||\\mathbf{V}||_{1}\\quad s.t.\\;\\mathbf{V}\\geq0,\\;\\mathrm{and}\\;||u_{k}||_{2}\\leq1\\,\\forall k\\in\\{1,...,K\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given any image $X$ where token $t$ is predicted by $f$ , we can now define the process of computing activations of concept dictionary $\\mathbf{U}^{*}$ for given $X$ , denoted as $\\boldsymbol{v}(\\boldsymbol{X})\\,\\in\\,\\mathbb{R}^{\\dot{K}}$ . To do so, we first extract the token representation for $X,z_{X}\\in\\mathbb{R}^{B}$ with the process described in Sec. 3.3. Then, $z_{X}$ is projected on $\\mathbf{U}^{*}$ to compute $v(X)$ . In the case of Semi-NMF, this corresponds to $\\ensuremath{\\boldsymbol{v}}(\\ensuremath{\\boldsymbol{X}})=$ arg $\\mathrm{min}_{v\\geq0}\\,||z_{X}-\\mathbf{U}^{*}v||_{2}^{2}+\\bar{\\lambda||v||_{1}}$ . The activation of $u_{k}\\in\\mathbf{U}^{*}$ is denoted as $v_{k}(X)\\in\\mathbb{R}$ . ", "page_idx": 4}, {"type": "text", "text": "3.5 Using the concept dictionary for interpretation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Multimodal grounding of concepts. Given the learnt dictionary $\\mathbf{U}^{*}$ and corresponding activations $\\mathbf{V}^{*}$ , the key objective remaining is to ground the understanding of any given concept vector $u_{k},k\\in$ $\\{1,...,K\\}$ in the visual and textual domains. Specifically, for visual grounding, we use prototyping [3, 22] to select input images (among the decomposed samples), that maximally activate $u_{k}$ . Given the number of samples extracted for visualization $N_{M A S}$ , the set of maximum activating samples (MAS) for component $u_{k}$ , denoted as $\\mathbf{X}_{k,M A S}$ can be specified as follows $(|.|$ is absolute value): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{X}_{k,M A S}=\\underset{\\hat{X}\\subset\\mathbf{X},|\\hat{X}|=N_{M A S}}{\\mathrm{argmax}}\\sum_{X\\in\\hat{X}}|v_{k}(X)|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For grounding in textual domain, we note that the concept vectors are defined in the token representation space of $f_{L M}$ . Thus we leverage the insights from \u201cLens\u201d based methods [6, 24, 32, 40] that attempt to understand LLM representations. In particular, following [32], we use the unembedding layer to map $u_{k}$ to the token vocabulary space $\\boldsymbol{\\wp}$ , and extract the most probable tokens. That is, we extract the tokens with highest probability in $W_{U}u_{k}$ . The decoded tokens with highest probabilities are then flitered for being an english, non-stop-word with at least 3 characters, to eliminate unnecessary tokens. The final set of words is referred to as grounded words for concept $u_{k}$ and denoted as ${\\bf T}_{k}$ . Fig. 2 illustrates an example of grounding of a concept extracted for token \u201cDog\u201d in vision (5 most activating samples) and text (top 5 decoded words). ", "page_idx": 4}, {"type": "text", "text": "Most activating concepts for images. To understand the LMM\u2019s representation of a given image $X$ , we now define the most activating concepts. Firstly, we extract the representaions $z_{X}$ of the image with the same process as described previously. We then project $z_{X}$ on $\\mathbf{U}^{*}$ to obtain $\\boldsymbol{v}(\\boldsymbol{X})\\in\\mathbb{R}^{\\boldsymbol{K}}$ . We define the most activating concepts, which we denote $\\tilde{u}(X)$ , as the set of $r$ concept vectors (in $\\mathbf{U}^{*},$ ) whose activations $v_{k}(X)$ have the largest magnitude. One can then visualize the multimodal grounding of $\\tilde{u}(X)$ . This step could be further combined with concept importance estimation techniques [14] to interpret the model\u2019s prediction for token $t$ , however, the focus of this paper is to simply understand the internal representation of the model, for which the current pipeline suffices. ", "page_idx": 4}, {"type": "image", "img_path": "MvjLRFntW6/tmp/636c51d0f91972ce0d3ebc0b394679e1ca072121a12f910c0b505a11d308d674.jpg", "img_caption": ["Figure 2: Example of multimodal concept grounding in vision and text. Five most activating samples (among decomposed in $\\mathbf{Z}$ ) and five most probable decoded words are shown. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Models and dictionary learning. In the main paper, we focus on experiments with the DePALM model [45] that is trained for captioning task on COCO dataset [27]. The model consists of a frozen ViT-L/14 CLIP [39] encoder as the visual encoder $f_{V}$ . It is followed by a transformer connector to compress the encoding into $N_{V}=10$ visual tokens. The language model $f_{L M}$ is a frozen OPT-6.7B [49] and consists of 32 layers. Additional experiments with LLaVA [28] are in Appendix A. For uniformity and fairness all the results in the main paper are reported with number of concepts $K=20$ and for token representations from $L=31$ , the final layer before unembedding layer. For Semi-NMF, we set $\\lambda=1$ throughout. We consider the 5 most activating samples in $\\mathbf{X}_{k,M A S}$ for visual grounding for any $u_{k}$ . For text grounding, we consider top-15 tokens for ${\\bf T}_{k}$ before applying the filtering described in $S{\\in}c\\,3.5$ . ", "page_idx": 5}, {"type": "text", "text": "The complete dataset consists of around 120,000 images for training, and 5000 each for validation and testing with 5 captions per image following the Karpathy split. We conduct our analysis separately for various common objects in the dataset: \u201cDog\u201d, \u201cBus\u201d, \u201cTrain\u201d, \u201cCat\u201d, \u201cBear\u201d, \u201cBaby\u201d, \u201cCar\u201d, \u201cCake\u201d. The extension to other classes/tokens remains straightforward and is discussed in Appendix D. The precise details about number of samples for learning the dictionary, or testing, is available in Appendix C. The implementation of our method is publicly available on GitHub 2 ", "page_idx": 5}, {"type": "text", "text": "4.1 Evaluation setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate the quality of learnt concept dictionary $\\mathbf{U}^{*}$ on three axes: (i) Its use during inference to interpret representations of LMMs for test samples, (ii) The overlap/entanglement between grounded words of concepts in the dictionary and (iii) the quality of visual and text grounding of concepts (used to understand a concept itself). We discuss concrete details about each axis below: ", "page_idx": 5}, {"type": "text", "text": "Concept extraction during inference: To evaluate the use of $\\mathbf{U}^{*}$ in understanding any test sample $X$ , we first estimate the top $r$ most activating concepts activations, $\\tilde{u}(X)$ (Sec. 3.5). We then estimate the correspondence between the test image $X$ and the grounded words ${\\bf T}_{k}$ of $u_{k}\\,\\in\\,\\tilde{u}(X)$ . This correspondence is estimated via two different metrics. The primary metric is the average CLIPScore [20] between $X$ and ${\\bf T}_{k}$ . This directly estimates correspondence between the test image embedding with the grounded words of the top concepts. The secondary metric is the average BERTScore (F1) [50] between the ground-truth captions $y$ associated with $X$ and the words ${\\bf T}_{k}$ . These metrics help validate the multimodal nature of the concept dictionaries. Their use is inspired from [41]. Details for their implementation is in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Overlap/entanglement of learnt concepts: Ideally, we would like each concept in $\\mathbf{U}^{*}$ to encode distinct information about the token of interest $t$ . Thus two different concepts $u_{k},u_{l},k\\neq l$ should be associated to different sets of words. To quantify the entanglement of learnt concepts, we compute the overlap between the grounded words $\\mathbf{T}_{k},\\mathbf{T}_{l}$ . The overlap for a concept $u_{k}$ is defined as an average of its fraction of common words with other concepts. The overlap/entanglement metric for a dictionary ", "page_idx": 5}, {"type": "table", "img_path": "MvjLRFntW6/tmp/30908b734205c87d764c0ca8c382f293dc01a5818bb104a6de872f639f8a3e6a.jpg", "table_caption": [], "table_footnote": ["Table 1: Test data mean CLIPScore and BERTScore for top-1 activating concept for all baselines on five tokens. CLIPScore denoted as CS, and BERTScore as BS. Statistical significance is in Appendix D. Our CoX-LMM framework is evaluated with Semi-NMF as underlying dictionary learning method. Higher scores are better. Best score in bold, second best is underlined. "], "page_idx": 6}, {"type": "text", "text": "$\\mathbf{U}^{*}$ is defined as the average of overlap of each concept. ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{Overlap}(\\mathbf{U}^{*})={\\frac{1}{K}}\\sum_{k}\\operatorname{Overlap}(u_{k}),\\quad\\operatorname{Overlap}(u_{k})={\\frac{1}{(K-1)}}\\sum_{l=1,l\\neq k}^{K}{\\frac{|\\mathbf{T}_{l}\\cap\\mathbf{T}_{k}|}{|\\mathbf{T}_{k}|}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Multimodal grounding of concepts: To evaluate the quality of visual/text grounding of concepts $(\\mathbf{X}_{k,M A S},\\mathbf{T}_{k})$ , we measure the correspondence between visual and text grounding of a given concept $u_{k}$ , i.e. the set of maximum activating samples $\\mathbf{X}_{k,M A S}$ and words ${\\bf T}_{k}$ , using CLIPScore and BERTScore as described above. ", "page_idx": 6}, {"type": "text", "text": "Baselines: One set of methods for evaluation are the variants of CoX-LMM where we employ different dictionary learning strategies: PCA, KMeans and Semi-NMF. For evaluating concept extraction on test data with CLIPScore/BERTScore we compare against the following baselines: ", "page_idx": 6}, {"type": "text", "text": "- Rnd-Words: This baseline considers Semi-NMF as the underlying learning method. However, for each component $u_{k}$ , we replace its grounded words ${\\bf T}_{k}$ by a set of random words $\\mathbf{R}_{k}$ such that $\\left\\lvert\\mathbf{R}_{k}\\right\\rvert=\\left\\lvert\\mathbf{T}_{k}\\right\\rvert$ and the random words also satisfy the same filtering conditions as grounded words i.e. they are non-stopwords from english corpus with more than two characters. We do this by decoding a randomly sampled token representation and adding the top decoded words if they satisfy the conditions. ", "page_idx": 6}, {"type": "text", "text": "- Noise-Imgs: This baseline uses random noise as images and then proceeds with exactly same learning procedure as Semi-NMF including extracting activations from the same positions, and same parameters for dictionary learning. Combined with the Rnd-Words baseline, they ablate two parts of the concept extraction pipeline. ", "page_idx": 6}, {"type": "text", "text": "- Simple: This baseline considers a simple technique to build the dictionary $\\mathbf{U}^{*}$ and projecting test samples. It builds $\\mathbf{U}^{*}$ by selecting token representations in $\\mathbf{Z}$ with the largest norm. The projections are performed by mapping the test sample representation to the closest element in $\\mathbf{U}^{*}$ . For deeper layers, this provides a strong baseline in terms of extracted grounded words ${\\bf T}_{k}$ which are related to token of interest $t$ , as they are obtained by directly decoding token representations of $t$ . ", "page_idx": 6}, {"type": "text", "text": "We also report score using ground-truth captions (GT captions) instead of grounded words ${\\bf T}_{k}$ , to get the best possible correspondence score. The overlap/entanglement in concept dictionary is compared between the non-random baselines: Simple, PCA, K-Means and Semi-NMF. For evaluating the visual/text grounding we compare against the random words keeping the underlying set of MAS, $\\mathbf{X}_{k,M A S}$ , same for both. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results and discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Quantitative results Tab. 1 reports the test top-1 CLIPScore/BERTScore for all baselines and Semi-NMF on different target tokens. Appendix D contains detailed results for other tokens as well as for the PCA and K-Means variants. We report the results for only the top-1 activating concept, as the KMeans and Simple baselines map a given representation to a single cluster/element. ", "page_idx": 6}, {"type": "text", "text": "Notably, Semi-NMF generally outperforms the other baselines although the Simple baseline performs competitively. More generally, Semi-NMF, K-Means, and Simple tend to clearly outperform RndWords, Noise-Imgs and PCA on these metrics, indicating that these systems project representations of test images to concepts whose associated grounded words correspond well with the visual content. ", "page_idx": 7}, {"type": "text", "text": "Tab. 2 reports the overlap between concepts for Simple baseline and PCA, K-Means and Semi-NMF variants of $C o X.$ - LMM. Interestingly, K-Means and Simple baseline perform significantly worse than Semi-NMF/PCA with a high overlap between grounded words, often exceeding $40\\%$ . PCA outperforms other methods with almost no overlap while Semi-NMF shows some overlap. Overall, Semi-NMF strikes the best balance among all the methods, in terms of learning a concept dictionary useful for understanding test image representations, but which also learns diverse and disentangled concepts. Thus, for further $C o X-L M M$ experiments, we consider Semi-NMF as the underlying dictionary learning method. ", "page_idx": 7}, {"type": "table", "img_path": "MvjLRFntW6/tmp/eddaad9c5207052654331a26bc923a6a97a177d95abd1185b420422e9e56a12b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Overlap between learnt concepts. Lower is better. Best score in bold, second best underlined. ", "page_idx": 7}, {"type": "text", "text": "Fig. 3 shows an evaluation of visual/text grounding of concepts learnt by Semi-NMF. Each point on the figure denotes the CLIPScore (left) or BERTScore (right) for correspondence between samples $\\mathbf{X}_{k,M A S}$ and words ${\\bf T}_{k}$ for concept $u_{k}$ against random words baseline. We see that for both metrics, vast majority of concepts lie above the $y\\,=\\,x$ line, indicating that grounded words correspond much better to content of maximum activating samples than random words. ", "page_idx": 7}, {"type": "image", "img_path": "MvjLRFntW6/tmp/0d7647888e2255912ec2e0996b382c84d8bb45bae9a6ee1f13a6faf59a56d19e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Evaluating visual/text grounding (CLIPScore/BERTScore). Each point denotes score for grounded words of a concept (Semi-NMF) vs Rnd-Words w.r.t the same visual grounding. ", "page_idx": 7}, {"type": "text", "text": "Qualitative results Fig. 4 shows visual and textual grounding of concepts extracted for token \u2018dog\u2019. For brevity, we select 8 out of 20 concepts for illustration. 2. Grounding for all concepts extracted for \u2018dog\u2019 and other tokens are in Appendix E. The concept visualizations/grounding for \u2018Dog\u2019 reveal interesting insights about the global structure of the LMM\u2019s representation. Extracted concepts capture information about different aspects of a \u2018dog\u2019. The LMM separates representation of animal \u2018Dog\u2019 with a \u2018hot dog\u2019 (Concept 1). Specifically for \u2018Dog\u2019, Concepts (2), (3) capture information about color: \u2019black\u2019, \u2019brown\u2019. Concept (6) encodes information about \u2018long hair\u2019 of a dog, while concept (5) activates for \u2018small/puppy-like\u2019 dogs. Beyond concepts activating for specific characteristics of a \u2018dog\u2019, we also discover concepts describing their state of actions (Concept (4) \u2018playing/running\u2019), common scenes they can occur in (Concept (8), \u2019herd\u2019), and correlated objects they can occur with (Concept (7), \u2018cat and dog\u2019). We observe such diverse nature of extracted concepts even for other tokens (Appendix E). The information about concepts can be inferred via both the visual images and the associated grounded words, highlighting their coherent multimodal grounding. Notably, compared to solely visual grounding as for CAVs for CNNs, the multimodal grounding eases the process to understand a concept. ", "page_idx": 7}, {"type": "image", "img_path": "MvjLRFntW6/tmp/55f2350989b1cc26145e5feb92206b5961f217ae4b91b1fe7df4ed68276409c5.jpg", "img_caption": ["Figure 4: Visual/textual grounding for 8 out of 20 concepts for \u2019Dog\u2019 token (layer 31). For each concept we illustrate the set of 5 most activating samples and 5 most probable decoded words. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "MvjLRFntW6/tmp/e22c4ed5875c9bfa17f1995cf3fbfdf49055f89796e650786f07479b679aeda5.jpg", "img_caption": ["Figure 5: Local interpretations for test samples for different tokens (\u2018Dog\u2019, \u2018Cat\u2019, \u2018Bus\u2019) with SemiNMF (layer 31). Visual/text grounding for three highest concept activations (normalized) is shown. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Fig. 5 illustrates the use of concept dictionaries (learnt via Semi-NMF) to understand test sample representations for tokens $\\mathbf{\\nabla}\\mathbf{D}\\mathbf{og}^{\\star}$ , \u2018Cat\u2019 and \u2018Bus\u2019. For each sample we show the normalized activations of the three most activating concepts, and their respective multimodal grounding. Most activating concepts often capture meaningful and diverse features of a given sample. For instance, for first sample containing a $\\mathbf{\\nabla}^{*}\\mathbf{D}\\mathbf{0}\\mathbf{g}^{}\\,$ , the concepts for \u201clong hair\u201d, \u201csmall/tiny/puppy\u201d, and \u201cblack/white color\u201d all simultaneously activate. The grounding for first two concepts was also illustrated in Fig. 4. Additional visualizations for test samples are shown in Appendix E, wherein we qualitatively compare interpretations of Semi-NMF to K-Means, PCA variants and Simple baseline. ", "page_idx": 8}, {"type": "text", "text": "Layer ablation We analyze the quality of multimodal grounding of concepts across different layers $L$ . The CLIPScore between $(\\mathbf{X}_{k,M A S},\\mathbf{T}_{k})$ , averaged over all concepts $u_{k}$ is shown in Fig. 6, for \u2018Dog\u2019 and \u2018Cat\u2019 for all layers in $f_{L M}$ . For early layers the multimodal grounding is no better than Rnd-Words. Interestingly, there is a noticeable increase around $L=20$ to $L=25$ ), indicating that the multimodal structure of internal token representations starts to appear at this point. This also validates our choice that deeper layers are better suited for multimodal concepts. ", "page_idx": 8}, {"type": "image", "img_path": "MvjLRFntW6/tmp/7efb2cd0e54c4c66570fbd42eace49fd263904a50192e135f8793dfdaa406843.jpg", "img_caption": ["Figure 6: Mean CLIPScore between visual/text grounding $\\mathbf{X}_{k,M A S},\\mathbf{T}_{k}$ for all concepts (Semi-NMF), across different layers $L$ . Results are for tokens \u2018Dog\u2019 and \u2018Cat\u2019. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Additional experiments and discussion. We conduct a preliminary study to analyze the polysemanticity/superposition in concept dictionaries in Appendix B. A qualitative analysis for grounding of extracted concepts for different layers is available in Appendix F. CoX-LMM can be also be applied to understand the processing of visual/perceptual tokens inside the LMM which also exhibit this multimodal structure. The experiment for the same can be found in Appendix G. Limitations of our method are discussed in Appendix $\\mathrm{H}$ , and the broader societal impacts are discussed in Appendix I. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, we have presented a novel dictionary learning based concept extraction framework, useful to understand internal representations of a large multimodal model. The approach relies on decomposing representations of a token inside a pretrained LMM. To this end, we also propose a Semi-NMF variant of the concept dictionary learning problem. The elements of the learnt concept dictionary are grounded in the both text and visual domains, leading to a novel notion of multimodal concepts in the context of interpretability. We quantitatively and qualitatively show that (i) the multimodal grounding of concepts is meaningful, and (ii) the learnt concepts are useful to understand representations of test samples. We hope that our method inspires future work from research community towards designing concept based explainability methods to understand LMMs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Thomas Fel for his valuable comments on the paper. This work has been partially supported by ANR grant VISA DEEP (ANR-20-CHIA-0022), and HPC resources of IDRIS under the file number AD011014947, allocated by GENCI. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS), 35:23716\u201323736, 2022.   \n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716\u201323736, 2022.   \n[3] David Alvarez-Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 7775\u20137784, 2018.   \n[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.   \n[5] Val\u00e9rie Beaudouin, Isabelle Bloch, David Bounie, St\u00e9phan Cl\u00e9men\u00e7on, Florence d\u2019Alch\u00e9- Buc, James Eagan, Winston Maxwell, Pavlo Mozharovskyi, and Jayneel Parekh. Flexible and context-specific AI explainability: A multidisciplinary approach. CoRR, abs/2003.07703, 2020.   \n[6] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.   \n[7] Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P Calmon, and Himabindu Lakkaraju. Interpreting clip with sparse linear concept embeddings (splice). arXiv preprint arXiv:2402.10376, 2024.   \n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33: 1877\u20131901, 2020.   \n[9] Julien Colin, Thomas Fel, R\u00e9mi Cad\u00e8ne, and Thomas Serre. What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods. Advances in neural information processing systems, 35:2832\u20132845, 2022.   \n[10] Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. IEEE transactions on pattern analysis and machine intelligence, 32(1):45\u201355, 2008.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[12] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. Magma\u2013multimodal augmentation of generative models through adapter-based finetuning. arXiv preprint arXiv:2112.05253, 2021.   \n[13] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021.   \n[14] Thomas Fel, Victor Boutin, Louis B\u00e9thune, R\u00e9mi Cad\u00e8ne, Mazda Moayeri, L\u00e9o And\u00e9ol, Mathieu Chalvidal, and Thomas Serre. A holistic approach to unifying automatic concept extraction and concept importance estimation. Advances in Neural Information Processing Systems, 36, 2023.   \n[15] Thomas Fel, Agustin Picard, Louis Bethune, Thibaut Boissin, David Vigouroux, Julien Colin, R\u00e9mi Cad\u00e8ne, and Thomas Serre. Craft: Concept recursive activation factorization for explainability. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2711\u20132721, 2023.   \n[16] Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. Interpreting clip\u2019s image representation via text-based decomposition. In International Conference on Learning Representations, 2024.   \n[17] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic conceptbased explanations. In Advances in Neural Information Processing Systems (NeurIPS), pages 9277\u20139286, 2019.   \n[18] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 6(3): e30, 2021.   \n[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021.   \n[21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[22] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pages 2668\u20132677. PMLR, 2018.   \n[23] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. arXiv preprint arXiv:2301.13823, 2023.   \n[24] Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem H. Zuidema, and Jaap Jumelet. Decoderlens: Layerwise interpretation of encoder-decoder transformers. CoRR, abs/2310.03686, 2023.   \n[25] Hugo Lauren\u00e7on, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: An open web-scale flitered dataset of interleaved image-text documents. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id $\\cdot$ SKN2hflBIZ.   \n[26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740\u2013755. Springer, 2014.   \n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023.   \n[29] Oscar Ma\u00f1as, Pau Rodriguez, Saba Ahmadi, Aida Nematzadeh, Yash Goyal, and Aishwarya Agrawal. Mapl: Parameter-efficient adaptation of unimodal pre-trained models for visionlanguage few-shot prompting. arXiv preprint arXiv:2210.07179, 2022.   \n[30] Aniek F Markus, Jan A Kors, and Peter R Rijnbeek. The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies. Journal of biomedical informatics, 113:103655, 2021.   \n[31] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text space. arXiv preprint arXiv:2209.15162, 2022.   \n[32] Nostalgebraist. Interpreting gpt: The logit lens. https://www.lesswrong.com/posts/ AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens, 2020. Accessed: [date of access].   \n[33] OpenAI. Gpt-4 technical report. arXiv, 2023.   \n[34] Vedant Palit, Rohan Pandey, Aryaman Arora, and Paul Pu Liang. Towards vision-language mechanistic interpretability: A causal tracing tool for blip. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2856\u20132861, 2023.   \n[35] Haowen Pan, Yixin Cao, Xiaozhi Wang, and Xun Yang. Finding and editing multi-modal neurons in pre-trained transformer. arXiv preprint arXiv:2311.07470, 2023.   \n[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026\u20138037, 2019.   \n[37] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. the Journal of machine Learning research, 12:2825\u20132830, 2011.   \n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 8748\u20138763. PMLR, 2021.   \n[40] Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, Andr\u00e9 Bauer, Kyle Chard, and Ian T. Foster. Attention lens: A tool for mechanistically interpreting the attention head information retrieval mechanism. CoRR, abs/2310.16270, 2023.   \n[41] Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, and Antonio Torralba. Multimodal neurons in pretrained text-only transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2862\u20132867, 2023.   \n[42] Mustafa Shukor and Matthieu Cord. Implicit multimodal alignment: On the generalization of frozen llms to multimodal inputs. arXiv preprint arXiv:2405.16700, 2024.   \n[43] Mustafa Shukor, Corentin Dancette, and Matthieu Cord. ep-alm: Efficient perceptual augmentation of language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22056\u201322069, October 2023.   \n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[45] Th\u00e9ophane Vallaeys, Mustafa Shukor, Matthieu Cord, and Jakob Verbeek. Improved baselines for data-efficient perceptual augmentation of llms. arXiv preprint arXiv:2403.13499, 2024.   \n[46] Johanna Vielhaben, Stefan Bluecher, and Nils Strodthoff. Multi-dimensional concept discovery (mcd): A unifying framework with completeness guarantees. Transactions on Machine Learning Research, 2023.   \n[47] Chih-Kuan Yeh, Been Kim, Sercan O Arik, Chun-Liang Li, Pradeep Ravikumar, and Tomas Pfister. On concept-based explanations in deep neural networks. arXiv preprint arXiv:1910.07969, 2019.   \n[48] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A Ehinger, and Benjamin IP Rubinstein. Invertible concept-based explanations for cnn models with non-negative concept activation vectors. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11682\u201311690, 2021.   \n[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[50] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Experiments on other LMMs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section covers our experiments on other types of multimodal models. First, we test our approach on LLaVA to demonstrate that our approach generalizes to more recent networks that also fine-tune $f_{L M}$ on multimodal data. We also test our method on other variants of DePALM with non-CLIP visual encoders to observe their effect on CLIPScore. ", "page_idx": 13}, {"type": "text", "text": "A.1 Experiments with LLaVA ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We conduct further experiments on LLaVA [28], a popular open-source LMM to demonstrate the generality of our method. The model uses a CLIP-ViT-L-336px visual encoder $(f_{V})$ , a 2-layer linear connector $(C)$ that outputs $N_{V}=576$ visual tokens, and a Vicuna-7B language model $\\boldsymbol{f}_{L M}$ , 32 layers). We use identical hyperparameters as for DePALM $(K=20,\\lambda=1,L=31)$ . We report the test CLIPScore for top-1 activating concept, for Rnd-Words, Noise-Imgs, Simple and Semi-NMF, GT-captions in Tab. 3, and Overlap score for non-random baselines in Tab. 4. Quantitatively, we obtain consistent results to those observed for DePALM. Semi-NMF extracts most balanced concept dictionaries with high multimodal correspondence and low overlap. Qualitatively too, the method functions consistently and is able to extract concepts with meaningful multimodal grounding. ", "page_idx": 13}, {"type": "table", "img_path": "MvjLRFntW6/tmp/937504efb88b7e476527a61ffcc85b0318615b4074579fbebf1f83e064b83fac.jpg", "table_caption": [], "table_footnote": ["Table 3: Concept extraction on LLaVA-v1.5: Test data mean CLIPScore reported for top-1 activating concept for same baselines and tokens as in main paper table 1. Higher scores are better. Best score in bold, second best is underlined. "], "page_idx": 13}, {"type": "table", "img_path": "MvjLRFntW6/tmp/bff9fb938b2402f3b40e0f3e2a0dd9ba20f71d05d363e77d261ebce346af451c.jpg", "table_caption": ["Table 4: Overlap evaluation (LLaVA). Lower is better. Best score in bold, second best underlined. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Qualitative results and Saliency maps We also show qualitative examples of concepts extracted for token \u2018Dog\u2019 in Fig. 7. More examples for other \u2018Cat\u2019 and \u2018Train\u2019 tokens are given in Fig. 9 and 10. Interestingly, since LLaVA uses a connector $C$ that contains two linear layers, the visual tokens as processed inside $f_{L M}$ preserve the notion of specific image patch representations, i.e. $N_{V}=576$ tokens denoting representations for 576 $(24\\times24)$ input patches. This allows us to further explore a simple and computationally cheap strategy of generating saliency maps to highlight which regions a concept vector activates on. To do this one can simply compute the inner product of any given concept vector $u_{k}$ with all visual token representations from corresponding layer $L$ , i.e. $u_{k}^{\\bar{T}}[\\bar{h_{(L)}^{1}},...,h_{(L)}^{\\bar{N_{V}^{\\bar{V}}}}]$ . This can be upscaled to the input image size to visualize the saliency map. We illustrate some qualitative outputs on concepts from \u2018Dog\u2019 in Fig. 8. . ", "page_idx": 13}, {"type": "image", "img_path": "MvjLRFntW6/tmp/564b9317db9c67e8440e64318dd2251ed3becefd8334ce43902821ce63d66b42.jpg", "img_caption": ["Figure 8: Examples of generating visual concept saliency maps for two \u2018Dog\u2019 concepts for LLaVA. Red denotes high activations, blue denotes low activation (bottom row) "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 DePALM with ViT visual encoders ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We further test LMMs which do not contain a CLIP visual encoder to confirm that the high CLIPScore is not due to use of CLIP visual encoders. To test this, we experiment on two different DePALM models with frozen visual encoders different from CLIP, a frozen ViT-L encoder trained on ImageNet [11] and another frozen ViT-L trained as a masked autoencoder (MAE) [19]. Both LMMs use the same pretrained OPT-6.7B language model. Collectively, the three encoders (including CLIP) are pretrained for three different types of objectives. We use Semi-NMF to extract concept dictionaries, with all hyperparameters identical. The results are reported in tables below. \u2019Rnd-Words\u2019 and \u2019GT-captions\u2019 references are reported for each LMM separately, although they are very close to the ones in main paper. The \"ViT-L (CLIP)\" baseline denotes our system from the main paper that uses CLIP encoder. Importantly, we still obtain similar test CLIPScores as with CLIP visual encoder. The concept dictionaries still possess meaningful multimodal grounding. Many concepts also tend to be similar as for CLIP visual encoder, further indicating that processing inside language model plays a major role in the discovery of multimodally grounded concepts. ", "page_idx": 14}, {"type": "table", "img_path": "MvjLRFntW6/tmp/33f26e3da9e9c9dd2f886f76b0100072ab830e2aec5ad156e725beadbfeb14a7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 5: Test CLIPScore evaluation for DePALM with ViT-L frozen image encoder trained on ImageNet: Scores reported for top-1 activating concept of Semi-NMF for Rnd-Words, GT-captions and ViT-L (CLIP) which denotes the system in main text. ", "page_idx": 14}, {"type": "image", "img_path": "MvjLRFntW6/tmp/ad9a2497a756d4a64ab9043150e76eac575edc779d069ed520b31c3c6fbdd343.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "MvjLRFntW6/tmp/e958253820f1ff764d7053122f9f2502d876cfde1ad3cc3cc78e7a4174fa65ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 6: Test CLIPScore evaluation for DePALM with ViT-L frozen image encoder trained as masked autoencoder (MAE): Scores reported for top-1 activating concept of Semi-NMF for Rnd-Words, GT-captions and ViT-L (CLIP) which denotes the system in main text. ", "page_idx": 15}, {"type": "text", "text": "B Analyzing polysemanticity in the learnt concepts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conducted a preliminary qualitative study on some concept vectors in the dictionary learnt for token \"Dog\" (DePALM model), to analyze if these concept vectors tend to activate strongly for a specific semantic concept (monosemantic) or multiple semantic concepts (polysemantic). In particular, we first manually annotated the 160 test samples for \"Dog\" for four semantic concepts, for which we knew we had concept vectors in our dictionary, namely \"Hot dog\" (Concept 2, row 1, column 2 in Fig. 7), \"Black dog\" (Concept 20, row 10, column 2 in Fig. 7), \"Brown/orange dog\" (Concept 6, row 3, column 2 in Fig. 7), and \"Bull dog\" (Concept 15, row 8, column 1 in Fig. 7). For a given semantic concept, we call this set $C_{t r u e}$ . Then, for its corresponding concept vector $u_{k}$ we find the set of test samples for which $u_{k}$ activates greater than a threshold $\\tau$ . This threshold was set to half of its maximum activation over test samples. We call this set of samples $C_{t o p}$ . To estimate specificity of the concept vector we compute how many samples in $C_{t o p}$ lie in the ground-truth set, i.e. $|C_{t o p}|\\cap|C_{t r u e}|/|\\bar{C_{t o p}}|$ . ", "page_idx": 15}, {"type": "image", "img_path": "MvjLRFntW6/tmp/b3b998811b226941f4330b297508e154d858b0492af40eb6d6574263fed675c4.jpg", "img_caption": ["Figure 10: Multimodal grounding for example concepts for \u2019Train\u2019 token (layer 31) on LLaVA. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "We found Concept 2 (\"Hot dog\") to be most monosemantic with $100\\%$ specificity. For Concept 20 (\"Black dog\") too, we found high specificity of $93.3\\%$ . For concept 15 (\"Bull dog\") we observed the lowest specificity of $50\\%$ . This concept also activated for test images with toy/stuffed dogs. Interestingly, the multimodal grounding of concept 15 already indicates this superposition with maximum activating samples also containing images of \u2019toy dogs\u2019. Concept 6 (\"Brown/orange dog\") is somewhere in between, with $76\\%$ specificity. This concept vector also activated sometimes for dark colored dogs, which wasn\u2019t apparent from its multimodal grounding. ", "page_idx": 16}, {"type": "text", "text": "Prominent or distinct semantic concepts seem to be captured by more specific/monosemantic concept vectors, while more sparsely present concepts seem at risk to be superposed resulting in a more polysemantic concept vector capturing them. It is also worth noting that the multimodal grounding can be a useful tool in some cases to identify polysemanticity in advance. ", "page_idx": 16}, {"type": "table", "img_path": "MvjLRFntW6/tmp/9bf2b656eb425fd6ce0d2f49671b16c2d216c7a69e55927534f8443dae7af5b9.jpg", "table_caption": ["Table 7: Number of samples training/testing samples for each token for DePALM "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Further implementation details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Dictionary learning details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The details about the number of samples used for training the concept dictionary of each token, and the number of samples for testing is given in Tab. 7. The token representations are of dimension $B=4096$ . ", "page_idx": 17}, {"type": "text", "text": "The hyperparameters for the dictionary learning methods are already discussed in the main paper. All the dictionary learning methods (PCA, KMeans, Semi-NMF) are implemented using scikitlearn [37]. For PCA and KMeans we rely on the default optimization strategies. Semi-NMF is implemented through the DictionaryLearning() class, by forcing a positive code. It utilizes the coordinate descent algorithm for optimization during both the learning of ${\\bf U}^{*},{\\bf V}^{*}$ and the projection of test representations $v(X)$ . ", "page_idx": 17}, {"type": "text", "text": "C.2 CLIPScore/BERTScore evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For a given image $X$ and set of words ${\\bf T}_{k}$ associated to concept $u_{k}$ , CLIPScore is calculated between CLIP-image embedding of $X$ and CLIP-text embedding of comma-separated words in ${\\bf T}_{k}$ . We consider a maximum of 10 most probable words in each ${\\bf T}_{k}$ , filtering out non-English and stop words. The computation of the metric from embeddings adheres to the standard procedure described in [20]. Our adapted implementation is based on the CLIPScore official repository, which utilizes the ViT-B/32 CLIP model to generate embeddings. ", "page_idx": 17}, {"type": "text", "text": "We found that computing BERTScores from comma-separated words and captions is unreliable. Instead, we adopted a method using the LLaMA-3-8B instruct model to construct coherent phrases from a set of grounded words, ${\\bf T}_{k}$ . Specifically, we provide the LLaMA model with instructions to describe a scene using a designated set of words, for which we also supply potential answers. This instruction is similarly applied to another set of words, but without providing answers. The responses generated by LLaMA are then compared to the captions $y$ using BERTScore. The instruction phrase and an example of the output are detailed in 8. The highest matching score between the generated phrases and the captions of a test sample determines the score assigned to the concept $u_{k}$ . This approach ensures that the evaluation accurately reflects coherent and contextually integrated language use. The metric calculation from embeddings follows the established guidelines outlined in [50]. Our adapted implementation is based on BERTScore official repository, and we use the default Roberta-large model to generate embeddings. ", "page_idx": 17}, {"type": "text", "text": "C.3 Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "DePALM experiments compute Each experiment to analyze a token with a selected dictionary learning method is conduced on a single RTX5000 (24GB)/ RTX6000 (48GB)/ TITAN-RTX (24GB) GPU. Within dictionary learning, generating visualizations and projecting test data, the majority of time is spent in loading the data/models and extracting the representations. For analysis of a single token with $\\approx3000$ training samples, it takes around 10-15 mins for this complete process. Evaluation for CLIPScore/BERTScore are also conducted using the same resources. Evaluating CLIPScore for 500 (image, grounded-words) pairs takes around 5 mins. The BERTScore evaluation is in contrast more expensive, consuming around 150 mins for 500 pairs. ", "page_idx": 17}, {"type": "text", "text": "LLaVA experiments compute Each experiment to extract a concept dictionary for LLaVA was conducted on a single A100 (80GB) GPU. Representation extraction process for LLaVA is more expensive compared to DePALM consuming around $90\\;\\mathrm{{mins}}$ for $\\approx3000$ samples. The remaining aspects of dictionary learning, multimodal grounding, representation projection etc. remains relatively cheap. The CLIPScore/BERTScore evaluations are completed with same resources as before. ", "page_idx": 17}, {"type": "table", "img_path": "MvjLRFntW6/tmp/65c66a3beca73a4d846719a44f31fad8eed16438461ccb01ef9703934f4e7f38.jpg", "table_caption": ["Table 8: Generating contextually and grammatically coherent phrases Using the LLaMA Model for BERTScore Evaluation "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Licenses of assets The part of the code for representation extraction from LMM is implemented using PyTorch [36]. For our analyses, we also employ the OPT-6.7B model [49] from Meta AI, released under the MIT license, and the CLIP model [39] from OpenAI, available under a custom usage license. Additionally, the COCO dataset [27] used for validation is accessible under the Creative Commons Attribution 4.0 License. We also use CLIPScore [20] and BERTScore [50] for evaluating our method, both publicly released under MIT license. All utilized resources comply with their respective licenses, ensuring ethical usage and reproducibility of our findings. ", "page_idx": 18}, {"type": "text", "text": "C.4 Choice for number of concepts $K$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our choice of using $K=20$ concepts for all tokens was driven by the behaviour of reconstruction error of Semi-NMF on the training samples with different values of $K$ , i.e. $||\\mathbf{Z}-\\mathbf{U}\\mathbf{V}||_{2}^{2}$ . We validate this behaviour on the four target tokens from main text in Fig. 11. We generally found $K=20$ as the minimal number of concepts where the reconstruction error drops by at least $50\\%$ from $K=0$ . ", "page_idx": 18}, {"type": "text", "text": "We also conducted an ablation study to test how our method behaves with different values of number of concepts $K$ . Fig. 12 presents the variation of test CLIPScore and Overlap score for $K\\,\\in\\,\\{10,20,30,\\bar{5}0\\}$ for two target tokens, \u2018Dog\u2019 and \u2018Cat\u2019. Our method can learn meaningful concepts for different values of $K$ , evident by the consistently high CLIPScore. The Overlap score on the other hand tends to drop more consistently as the number of concepts increase but behaves stably for different choices. Nonetheless they indicate that our method can accommodate dictionaries of larger sizes without compromising the quality of learnt concepts, provided $K<<M$ (number of decomposed samples) and at the expense of greater user overhead. ", "page_idx": 18}, {"type": "text", "text": "D Evaluation and extension to more tokens ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide test data mean CLIPScore and BERTScore for top-1 activating concept for all baselines and more tokens: Baby, Car, Cake, and Bear in Tab. 9 (results in the main paper are reported for tokens Dog, Bus, Train, Cat in Tab. 1). Additionally, we also report the macro average over a set of 30 additional COCO-nouns apart from the 8 tokens with individually reported results, denoted as \u2018Extra- $.30^{\\circ}$ . These nouns are single-token words with at least 40 predicted test samples. We put the filter of single-token words to keep consistency with the presented framework. Extension to multi-token words is straightforward but discussed separately in D.1. The lower bound criterion on test samples is to ensure average test CLIPScore is reliable for each target token. We only report CLIPScore for \u2018Extra- $.30^{\\circ}$ tokens as BERTScore evaluation was more expensive to conduct on large number of dictionaries. ", "page_idx": 18}, {"type": "image", "img_path": "MvjLRFntW6/tmp/bf0c5ee1653ba9f29ec5323d521c93d22383c04d488fad9fe1b10c5ac8ed9f52.jpg", "img_caption": ["Figure 11: Variation of reconstruction error with number of concepts $K$ for decompositions on different target tokens. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "MvjLRFntW6/tmp/dc341b340bbf5872e9c6fc268f4ca5db963ab5f54b711bf3bad35d6911430c07.jpg", "img_caption": ["Figure 12: Test CLIPScore and Overlap score ablation with number of concepts $K$ . CLIPScore remains consistently high and drops slightly only for very small $K$ . Overlap score generally improves with higher $K$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We observe that we consistently obtain higher scores across for Semi-NMF and K-Means. We also report the overlap score between grounded words in Tab. 10 to illustrate the superiority of our method over the simple baseline. As previously noted, we observe a high overlap between grounded words with KMeans and Simple baselines compared to Semi-NMF/PCA. A low overlap should be encouraged, as it indicates the discovery of diverse and disentangled concepts in the dictionary. Among all the methods, Semi-NMF provides the most balanced concept dictionaries which are both meaningful (high CLIPScore/BERTScore) and diverse (low overlap). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "MvjLRFntW6/tmp/58e5e136d1d51fe8afb2c190cb7931dc870fd5136b22f7c0f7694893c689a1d7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "MvjLRFntW6/tmp/ca30f079a48328a20c0af048019291f2c4edd1a9194cca0a8127678ef67b7730.jpg", "table_caption": ["Table 9: Test data mean CLIPScore and BERTScore for top-1 activating concept CLIPScore denoted as CS, BERTScore denoted as BS for all concept extraction baselines considered. Analysis for layer $L=31$ . Best score indicated in bold and second best is underlined. \u2018Extra- $.30^{\\circ}$ denotes a set of 30 additional single-token COCO nouns (apart from previous 8) with at least 40 predicted test samples by $f_{L M}$ . For \u2018Extra-30\u2019 tokens we report the macro average and standard deviation of mean test data CLIPScore, taken over the set of 30 tokens. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 10: Overlap/entanglement between grounded words of learnt concepts for different dictionary learning methods. Results are for additional tokens. Lower is better. Best score indicated in bold and second best is underlined. For \u2018Extra-30\u2019 tokens (additional 30 single-token COCO nouns) we report the macro average of Overlap score, taken over the set. ", "page_idx": 20}, {"type": "text", "text": "Statistical significance The statistical significance of Semi-NMF w.r.t all other baselines and variants, for CLIPScore/BERTScore evaluation to understand representations of test samples is given in Tab/ 11 (for all tokens separately). We report the $p$ -values for an independent two sided T-test with null hypothesis that mean performance is the same between Semi-NMF and the respective system. The results for Semi-NMF are almost always significant compared to Rnd-Words, Noise-Imgs, PCA. However for these metrics, Simple baseline, K-Means and Semi-NMF all perform competitively and better than other systems. Within these three systems the significance depends on the target token, but are often not significant in many cases. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "MvjLRFntW6/tmp/c7b31f786ec046cc6c3c50df20be95171ffcea550cd22d85ddd92c131ef2baa3.jpg", "table_caption": ["Table 11: Statistical significance of Semi-NMF w.r.t other baselines for test data CLIPScore/BERTScore. $p$ -values for two sided T-test are reported. Significant values ( $\\acute{p}$ -value $<0.05)$ are indicated in bold. The values do not indicate which system has better mean score but just that if the difference is significant. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.1 Extending to multi-token words ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The presentation of our approach assumes that our token of interest $t$ is a single token. This poses no issues for words which are represented as single tokens but can raise some questions when we wish to extract concept representations for multi-token words. Our approach however, can be easily adapted to this setting. In particular, we extract representation of last token from first prediction of the multi-token sequence. Note that when filtering the training data for samples where ground-truth caption contains the token of interest, we now search for the complete multi-token sequence. The other aspects of the method remain unchanged. While there can also be other viable strategies, the rationale behind this adaptation is that the last token of our sequence of interest can also combines representations from previous tokens in the sequence. We add below results for such examples in Tab. 12 and 13. We observe behaviour consistent with the previous results with Semi-NMF extracting concept dictionaries with high CLIPScore and low overlap. ", "page_idx": 21}, {"type": "table", "img_path": "MvjLRFntW6/tmp/01a5716f1ffd518b6b9fd5477ca6d7732698ff6ba7807d65c89201ff12ebf757.jpg", "table_caption": [], "table_footnote": ["Table 12: Test mean CLIPScore (\u2191) reported for top-1 activating concept for multi-token words. Higher scores are better. Best score in bold, second best is underlined. "], "page_idx": 21}, {"type": "table", "img_path": "MvjLRFntW6/tmp/dd5d88da1b6d7f99fd764a592ef3fe84c0da6633fb104dd2b530ba7fc761326c.jpg", "table_caption": ["Table 13: Overlap score (\u2193) reported for top-1 activating concept for multi-token words. Higher scores are better. Best score in bold, second best is underlined. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E Additional visualizations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Concept grounding ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The visual/textual grounding for all tokens in Tab. 1 are given in Figs. 13 (\u2018Dog\u2019), 14 (\u2018Cat\u2019), 15 (\u2018Bus\u2019), 16 (\u2018Train\u2019). All the results extract $K=20$ concepts from layer $L=31$ . Similar to our analysis for token \u2018Dog\u2019 in main paper, for a variety of target tokens our method extracts diverse and multimodally coherent concepts encoding various aspects related to the token. ", "page_idx": 22}, {"type": "text", "text": "E.2 Local interpretations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we qualitatively analyze the local interpretations of various decomposition methods, including PCA, k-means, semi-NMF, and the simple baseline strategy. We select these four as they produce coherent grounding compared to Rnd-Words and Noise-Img baselines. We decompose test sample representations on our learnt dictionary and visualize the top three activating components. Note that in the case of KMeans and Simple baseline, the projection maps a given test representation to a single element of the concept dictionary, the one closest to it. However, for uniformity we show the three most closest concept vectors for both. Figs. 17, 18, 19, 20, 21 are dedicated to interpretations of a single sample each, for all four concept extraction methods. ", "page_idx": 22}, {"type": "text", "text": "The inferences drawn about the behaviour of the four baselines from quantitative metrics can also be observed qualitatively. Semi-NMF, K-Means and \u2018Simple\u2019 baseline, are all effective at extracting grounded words can be associated to a given image. However, both K-Means and \u2018Simple\u2019 display similar behaviour in terms of highly overlapping grounded words across concepts. This behaviour likely arises due to both the baselines mapping a given representation to a single concept/cluster. This limits their capacity to capture the full complexity of data distributions. In contrast, Semi-NMF and PCA utilize the full dictionary to decompose a given representation and thus recover significantly more diverse concepts. PCA in particular demonstrates almost no overlap, likely due to concept vectors being orthogonal. However, the grounded words for it tend to be less coherent with the images. As noted previously, Semi-NMF excels as the most effective method, balancing both aspects by extracting meaningful and diverse concepts. ", "page_idx": 22}, {"type": "text", "text": "F Qualitative analysis for different layers ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide a qualitative comparison of multimodal grounding for the token \u2019dog\u2019 across different layers in Fig. 22. As observed in Fig. 6 (main paper), the multimodal nature of token representations for two tokens \u2018Dog\u2019 and \u2018Cat\u2019 starts to appear around layers $L=20$ to $L=25$ . It is interesting to note that the representations of images still tend to be separated well, as evident by the most activating samples of different concepts. However, until the deeper layers the grounded words often do not correspond well to the visual grounding. This behaviour only appears strongly in deeper layers. ", "page_idx": 22}, {"type": "text", "text": "G Analysis for visual tokens ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our analysis in main paper was limited to decomposing representations of text tokens in various layers of an LLM, h(pl), p > NV . This was particularly because these were the predicted tokens of the multimodal model. Nevertheless, the same method can also be used to understand the information stored in the visual/perceptual tokens representations as processed in $f_{L M},\\,h_{(l)}^{p},p\\;\\leq\\;N_{V}$ . An interesting aspect worth highlighting is that while the text token representations in $f_{L M}$ can combine information from the visual token representations (via attention function), the reverse is not true. The causal processing structure of $f_{L M}$ prevents the visual token representations to attend to any information in the text token representations. Given a token of interest $t$ , for any sample $X\\in\\mathbf{X}_{t}$ we now only search for first position $p\\in\\{1,...,N_{V}\\}$ , s.t. $t=$ arg max Unembed (h(NL)). Only the samples for which such a $p$ exists are considered for decomposition. The rest of the method to learn ${\\bf U}^{*},{\\bf V}^{*}$ proceeds exactly as before. ", "page_idx": 22}, {"type": "image", "img_path": "MvjLRFntW6/tmp/7495d08d361089ec46db46d74edc343cf5a02cc03440a9976740b9778e5ff3f4.jpg", "img_caption": ["Figure 13: multimodal concept grounding in vision and text for the token \u2019Dog\u2019. The five most activating samples and the five most probable decoded words for each component $u_{k}$ , $k\\in\\{1,...,20\\}$ are shown. The token representations are extracted from $L{=}31$ of the LLM section of our LMM. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "We conduct a small experiment to qualitatively analyze concepts extracted for visual token representations for $\\mathbf{\\nabla}\\mathbf{D}\\mathbf{og}^{*}$ . We extract $K\\,=\\,20$ concepts from $L\\,=\\,31$ . The dictionary is learnt with representations from $M=1752$ samples, less than $M=3693$ samples for textual tokens. As a brief illustration, 12 out of 20 extracted concepts are shown in Fig. 23. Interestingly, even the visual token representations in deep layers of $f_{L M}$ , without ever attending to any text tokens, demonstrate a multimodal semantic structure. It is also worth noting that there are multiple similar concepts that appear for both visual and textual tokens. Concepts 3, 7, 10, 12, 17, 19 are all similar visually and textually to certain concepts discovered for text tokens. This indicates to a possibility that these concepts are discovered by $f_{L M}$ in processing of the visual tokens and this information gets propagated to predicted text token representations. ", "page_idx": 23}, {"type": "image", "img_path": "MvjLRFntW6/tmp/11985a7b67540b7ce2e9d30fba0d711e5af1b56c04ca741b1e273280cd1bd2d2.jpg", "img_caption": ["Figure 14: multimodal concept grounding in vision and text for the token \u2019Cat\u2019. The five most activating samples and the five most probable decoded words for each component $u_{k}$ , $k\\in\\{1,...,20\\}$ are shown. The token representations are extracted from $L{=}31$ of the LLM section of our LMM. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "H Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We list below some limitations of our proposed method: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The concept dictionaries extracted currently are token-specific. It can be interesting to explore learning concept dictionaries that can encode shared concepts for different tokens. \u2022 The current study is conducted mainly on visual captioning models. While we expect the key ideas to generalize to many other types of large multimodal models, this application of our approach to other types of LMMs remains to be explored/confirmed. \u2022 We select the most simple and straightforward concept grounding techniques. Both visual and textual grounding could potentially be enhanced. The visual grounding can be improved by enhancing localization of concept activation for any MAS or test sample. Text grounding could be enhanced by employing more sophisticated approaches such as tuned lens [6]. ", "page_idx": 24}, {"type": "image", "img_path": "MvjLRFntW6/tmp/d08bc1dd3496732d448ef4630536ff659302f87776bdde3da7171e96d033c0bb.jpg", "img_caption": ["Figure 15: multimodal concept grounding in vision and text for the token \u2019Bus\u2019. The five most activating samples and the five most probable decoded words for each component $u_{k}$ , $k\\in\\{1,...,20\\}$ are shown. The token representations are extracted from $L{=}31$ of the LLM section of our LMM. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "\u2022 While the proposed CLIPScore/BERTScore metrics are useful to validate this aspect, they are not perfect metrics and affected by imperfections and limitations of the underlying models extracting the image/text embeddings. The current research for metrics useful for interpretability remains an interesting open question, even more so in the context of LLMs/LMMs. ", "page_idx": 25}, {"type": "text", "text": "I Broader societal impact ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The popularity of large multimodal models and the applications they are being employed is growing at an extremely rapid pace. The current understanding of these models and their representations is limited, given the limited number of prior methods developed to understand LMMs. Since interpretability is generally regarded as an important trait for machine learning/AI models deployed in real world, we expect our method to have a positive overall impact. This includes its usage for understanding LMMs, as well as encouraging further research in this domain. ", "page_idx": 25}, {"type": "image", "img_path": "MvjLRFntW6/tmp/a36abb3fcf2c4cd2b30a5b5185c1a5fb7e62b94b2ff8d0fb844fdcfd15d325a3.jpg", "img_caption": ["Figure 16: multimodal concept grounding in vision and text for the token \u2019Train\u2019. The five most activating samples and the five most probable decoded words for each component $u_{k}$ , $k\\in\\{1,...,20\\}$ are shown. The token representations are extracted from $L{=}31$ of the LLM section of our LMM. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We propose a novel dictionary learning based concept extraction framework for understanding Large Multimodal Model (LMM) representations (Sec. 3). We extend previous dictionary learning-based concept extraction methods by proposing a Semi-NMF based optimization (Sec. 3). The learnt concept dictionary has multimodal semantic structure which we qualitatively and quantitatively validate with our experiments (Sec. 4). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. ", "page_idx": 26}, {"type": "image", "img_path": "MvjLRFntW6/tmp/c6897413050aee8578f32e2d0a5ae51ba72037cf5c0975f3720168e15ded5c6b.jpg", "img_caption": ["Figure 17: Local interpretations for test sample 9 of token \u2018Dog\u2019 with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown. SemiNMF baseline provides the most visually and textually consistent results, while other baselines provide components that are not well disentangled (Simple and KMeans baseline), or the text grounding is not closely related to the test image. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "MvjLRFntW6/tmp/4f94363fc60c169b9616160739bcc4860c0e721a6f369598610d4234c4e743d4.jpg", "img_caption": ["Figure 18: Local interpretations for test sample 37 of token \u2018Dog\u2019 with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The limitations are discussed separately in Appendix H Guidelines: ", "page_idx": 27}, {"type": "image", "img_path": "MvjLRFntW6/tmp/6d915af7035911db96de2e6d16c68ae48e87b0ef103aa5c310b2ff57f824d3ec.jpg", "img_caption": ["Figure 19: Local interpretations for test sample 43 of token \u2018Cat\u2019 with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "MvjLRFntW6/tmp/7020a4b1a0865c2aa623b2086caea7e3b5ca36f749c56b1a36816429b819af18.jpg", "img_caption": ["Figure 20: Local interpretations for test sample 6 of token \u2018Bus\u2019 with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution ", "page_idx": 28}, {"type": "image", "img_path": "MvjLRFntW6/tmp/41b0a55ca319e2f674df23365a52f1c4e084a9443e8de88457602510b0f4eb8d.jpg", "img_caption": ["Figure 21: Local interpretations for test sample 12 of token \u2018Bus\u2019 with SemiNMF, KMeans, PCA, and Simple baselines (layer 31). Visual/text grounding for the three highest concept activations (normalized) is shown. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "MvjLRFntW6/tmp/7065c86afaf2b91ef9d49e07ca9812da1d9802c4ec02c870b85f099c4ff17af7.jpg", "img_caption": ["Figure 22: Examples of multimodal grounding across different layers for concepts with similar visual grounding (target token \u2018Dog\u2019). The grounded words from early layers do not correspond well to the most activating samples of a concept. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover ", "page_idx": 29}, {"type": "image", "img_path": "MvjLRFntW6/tmp/42ee57018c56028eb2fb5c7a613b2560cf0b348f8e761d57ad4eb1f495665af6.jpg", "img_caption": ["Figure 23: Example concepts extracted for \u2018Dog\u2019 from visual token representations in layer 31. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not posit any theoretical results. The paper primarily presents empirical evidence demonstrating the effectiveness of the method rather than focusing on theoretical results. our approach highlights the practical applicability and robust performance of the method through a series of experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have disclosed complete information about our pretrained model, extraction of representations, parameters, and implementation to train the dictionary, evaluation setup, and resource usage for the experiments. Details are in Sec. 4 and Appendix C. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Our code is available at https://github.com/mshukor/xl-vlms Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Major details are in Sec. 4 and all additional details in Appendix C. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We show standard deviation of the key metrics in Tab. 1 and Tab. 9 and discuss the statistical significance comparing baselines and our method in Appendix D. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] Justification: Details are in Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have reviewed them and adhere to them. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The societal impacts are discussed separately in Appendix I. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We work with models and data already publicly available. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Details available in Appendix C. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We publicly release our code and documentation is available alongside it. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]