[{"figure_path": "UGKgoAZuuL/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of BGE's performance improvement on CIFAR100. CIFAR, CP, and CPI are different external dataset compositions. Performance was evaluated by linear evaluation accuracy of the final network. We equally divided classes into 4 tasks and 10 tasks. BGE consistently improves base methods across different external dataset compositions. As for Joint training, ED represents adding equivalent external data, which does not improve the performance.", "description": "This table presents the performance improvement achieved by the proposed method, BGE, on the CIFAR-100 dataset under different settings.  It compares the linear evaluation accuracy of several continual self-supervised learning methods (Fine-tune, CaSSLe, PFR) with and without the integration of BGE.  Three different compositions of external datasets (CIFAR, CP, and CPI) are used to test the effectiveness of BGE.  The results are shown for both 4 and 10 task settings. For comparison, the performance of joint training with and without equivalent external data is also provided.  The numbers in parentheses indicate the performance improvement due to the addition of BGE.", "section": "4.2 Results"}, {"figure_path": "UGKgoAZuuL/tables/tables_6_2.jpg", "caption": "Table 2: Performance improvement yielded by BGE on ImageNet100. IN, INP, and IND are different external dataset compositions. ED represents adding equivalent external data in joint training.", "description": "This table presents the performance improvement achieved by the proposed method, BGE, on the ImageNet100 dataset.  It compares the baseline methods (Fine-Tune, CaSSLe, and PFR) with and without BGE across three different external dataset compositions (IN, INP, and IND). The results show the improvement in accuracy after incorporating external data using BGE, also showing the performance of joint training with and without equivalent external data (ED) added.", "section": "4.2 Results"}, {"figure_path": "UGKgoAZuuL/tables/tables_7_1.jpg", "caption": "Table 3: Accuracy on CIFAR100 and ImageNet100 with different sampling algorithms. Bold indicates better performance.", "description": "This table compares the performance of using random sampling versus the One-Propose-One (OPO) sampling algorithm for the Fine-Tune (FT) and Proximal Feature Replay (PFR) methods on two datasets: CIFAR100 and ImageNet100.  Three external dataset compositions (CIFAR, CP, CPI for CIFAR100 and IN, INP, IND for ImageNet100) are used, representing different levels of similarity and out-of-distribution data. The results show that OPO generally outperforms random sampling in most cases, indicating its effectiveness in selecting high-quality external data for continual contrastive self-supervised learning.  The bold numbers indicate where OPO achieves better accuracy than random sampling. ", "section": "4.3 Ablation study"}, {"figure_path": "UGKgoAZuuL/tables/tables_8_1.jpg", "caption": "Table 4: Comparison of additional positive and negative pairs' effects.", "description": "This table presents the results of an ablation study investigating the impact of additional positive and negative pairs introduced by the BGE method on the performance of a contrastive learning framework (SimCLR). It shows that both positive and negative pairs contribute to performance improvement, with negative pairs yielding a more significant improvement, highlighting the importance of addressing the lack of inter-task comparisons in continual learning.", "section": "4.2 Results"}, {"figure_path": "UGKgoAZuuL/tables/tables_8_2.jpg", "caption": "Table 5: Effectiveness of BGE when external data are totally OOD.", "description": "This table presents the results of experiments conducted to evaluate the effectiveness of the BGE method when only out-of-distribution (OOD) data is used as external data. It compares the performance of the PFR method with and without BGE, and also includes results from joint training with and without equivalent external data (ED). The results show that BGE consistently improves the performance of the PFR method, even when the external data are entirely OOD, demonstrating its robustness and generalizability.", "section": "4.2 Results"}, {"figure_path": "UGKgoAZuuL/tables/tables_8_3.jpg", "caption": "Table 6: Performance of BGE when choosing more types of datasets.", "description": "This table presents the results of experiments using different types of external datasets with the BGE method. It shows the accuracy achieved by BGE when using GenImage, CC3M, and CUB200 as external datasets, in comparison to the baseline result (N/A) without external datasets. The results demonstrate the effectiveness of BGE across various types of external datasets, including generated images (GenImage), real-world internet data (CC3M), and a fine-grained bird dataset (CUB200).", "section": "4.2 Results"}, {"figure_path": "UGKgoAZuuL/tables/tables_13_1.jpg", "caption": "Table 7: Comparison of intra-task confusion and inter-task confusion. \u2193 means the value is the lower the better.", "description": "This table compares the intra-task and inter-task confusion rates for different continual self-supervised learning methods, both with and without the proposed BGE method.  Lower values indicate better performance.  Intra-task confusion is when the model misclassifies an example as belonging to a different class within the same task; inter-task confusion is when it's misclassified to a different task entirely.  The table shows that BGE consistently reduces inter-task confusion, while having minimal impact on intra-task confusion, indicating an improvement in discriminating between tasks.", "section": "A.2.1 BGE's improvement to inter-task confusion"}, {"figure_path": "UGKgoAZuuL/tables/tables_13_2.jpg", "caption": "Table 8: Performance improvement yielded by BGE in BYOL.", "description": "This table shows the performance improvement achieved by incorporating the BGE method into the BYOL framework.  It demonstrates that the positive effects of BGE extend beyond frameworks that utilize negative samples in contrastive learning, as BYOL does not require negative samples for training. The table presents results for the CIFAR and CP datasets with both 4 and 10 tasks.", "section": "A.2.2 Experiments on the method without negative samples"}, {"figure_path": "UGKgoAZuuL/tables/tables_14_1.jpg", "caption": "Table 9: Statistics on how many of the k-nearest neighbors of a sample belong to the same class as this sample in self-supervised and supervised networks.", "description": "This table compares the average number of k-nearest neighbors belonging to the same class as a given sample for both self-supervised and supervised networks.  The results show that supervised networks exhibit a greater number of same-class neighbors, demonstrating the influence of class labels in their clustering. In contrast, the self-supervised networks are less affected by labels.  This characteristic of self-supervised methods is important because it means that similar features can be found even in samples from different classes, enabling the use of external datasets in continual learning.", "section": "A.2.4 Self-supervised learning feature characteristics"}, {"figure_path": "UGKgoAZuuL/tables/tables_15_1.jpg", "caption": "Table 10: The class name and average number of the top 5 classes with the highest number of the top 100 neighbors of the \"willow tree\" class.", "description": "This table presents a comparison of the top 100 nearest neighbors of the \"willow tree\" class in CIFAR100 dataset, as identified by both supervised and self-supervised learning.  It shows the number of times each class appears among the nearest neighbors, highlighting the difference in how class labels influence nearest neighbor selection between the two learning methods. Self-supervised learning exhibits less class-label influence in neighbor selection, and nearest neighbors of other classes show greater similarity to the \"willow tree\" class compared to those from supervised learning.", "section": "A.2.4 Self-supervised learning feature characteristics"}, {"figure_path": "UGKgoAZuuL/tables/tables_15_2.jpg", "caption": "Table 1: Comparison of BGE's performance improvement on CIFAR100. CIFAR, CP, and CPI are different external dataset compositions. Performance was evaluated by linear evaluation accuracy of the final network. We equally divided classes into 4 tasks and 10 tasks. BGE consistently improves base methods across different external dataset compositions. As for Joint training, ED represents adding equivalent external data, which does not improve the performance.", "description": "This table presents the results of experiments conducted on the CIFAR100 dataset to evaluate the performance improvement achieved by the proposed method, BGE (Bridging the inter-task Gap of CCSSL using External Data), compared to baseline methods.  Different external dataset compositions (CIFAR, CP, CPI) were tested, along with varying numbers of tasks (4 and 10). The results show that BGE consistently improves the performance of baseline methods across all datasets and task settings.", "section": "4.2 Results"}, {"figure_path": "UGKgoAZuuL/tables/tables_15_3.jpg", "caption": "Table 12: Results with multiple runs.", "description": "This table presents the mean and standard deviation of three random trials for the primary experiments. It shows the performance of BGE on three baseline methods (FT, CaSSLe, and PFR) when using CIFAR and CPI as external dataset compositions under the CIFAR100 4 tasks and 10 tasks settings.", "section": "4.3 Ablation study"}, {"figure_path": "UGKgoAZuuL/tables/tables_15_4.jpg", "caption": "Table 13: Results with multiple runs.", "description": "This table presents the results of experiments conducted with multiple runs, focusing on the impact of different sampling algorithms (random vs. OPO) on the performance of three continual self-supervised learning base methods (FT, CaSSLe, and PFR) under two task settings (4 tasks and 10 tasks).  The CPI dataset composition was used for the external data.  The mean and standard deviation across three random trials are reported for each method and task setting.", "section": "4.3 Ablation study"}, {"figure_path": "UGKgoAZuuL/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of BGE's performance improvement on CIFAR100. CIFAR, CP, and CPI are different external dataset compositions. Performance was evaluated by linear evaluation accuracy of the final network. We equally divided classes into 4 tasks and 10 tasks. BGE consistently improves base methods across different external dataset compositions. As for Joint training, ED represents adding equivalent external data, which does not improve the performance.", "description": "This table presents the performance improvements achieved by the proposed method, BGE, on the CIFAR100 dataset across different experimental settings.  It compares the linear evaluation accuracy (a measure of model performance) of several continual learning methods (Fine-tune, CaSSLe, and PFR) both with and without BGE. The comparison is made across varying compositions of external datasets (CIFAR, CP, and CPI).  The results demonstrate that BGE consistently improves the performance of the base continual learning methods, highlighting its effectiveness in bridging the inter-task gap in continual self-supervised learning. The 'Joint' row shows performance if all tasks are trained simultaneously, demonstrating that BGE significantly closes the gap between continual and joint learning.", "section": "4.2 Results"}]