[{"figure_path": "vUrOuc6NR3/tables/tables_6_1.jpg", "caption": "Table 1: Downstream policy performance on frozen visual representation on four simulated benchmarks - Franka Kitchen, Blocking Pushing, Push-T, and LIBERO Goal. We observe that DynaMo matches or significantly outperforms prior work on all simulated tasks.", "description": "This table presents the results of downstream policy performance using different visual representations on four simulated benchmark tasks: Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal.  It compares DynaMo's performance against various pretrained representations (Random, ImageNet, R3M, VC-1, MVP) and self-supervised methods (BYOL, BYOL-T, MoCo-v3, RPT, TCN-MV, TCN-SV, MAE). The results are presented as the average number of successful task completions, normalized by the maximum possible number of completions for each task.  The table shows that DynaMo achieves either the highest or very competitive performance compared to the baselines across all the benchmarks.", "section": "4 Experiments"}, {"figure_path": "vUrOuc6NR3/tables/tables_6_2.jpg", "caption": "Table 2: We evaluate DynaMo on eight tasks across two real-world environments: Allegro Manipulation, and xArm Kitchen. Results are presented as (successes/total). We observe that DynaMo significantly outperforms prior representation learning methods on real tasks.", "description": "This table presents the results of experiments conducted on two real-world robotic manipulation tasks using DynaMo, BYOL, BYOL-T, and MoCo-v3.  The tasks involve using robot hands to perform actions such as picking up objects or placing them in specific locations.  The success rate for each task is presented as a fraction (successes/total attempts). DynaMo significantly outperforms the other methods across all tasks, demonstrating its superior performance in real-world scenarios.", "section": "4.2 Does DynaMo improve downstream policy performance?"}, {"figure_path": "vUrOuc6NR3/tables/tables_7_1.jpg", "caption": "Table 1: Downstream policy performance on frozen visual representation on four simulated benchmarks - Franka Kitchen, Blocking Pushing, Push-T, and LIBERO Goal. We observe that DynaMo matches or significantly outperforms prior work on all simulated tasks.", "description": "This table presents the results of downstream policy performance tests on four simulated benchmark environments using different visual representation learning methods. The methods include pretrained representations (Random, ImageNet, R3M, VC-1, MVP), self-supervised methods (BYOL, BYOL-T, MoCo v3, RPT, TCN-SV, MAE), and the proposed method, DynaMo. The performance is measured by the average success rate for each task (Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal). DynaMo significantly outperforms all other methods across all four benchmarks, showcasing its effectiveness in improving downstream policy performance.", "section": "4.2 Does DynaMo improve downstream policy performance?"}, {"figure_path": "vUrOuc6NR3/tables/tables_7_2.jpg", "caption": "Table 1: Downstream policy performance on frozen visual representation on four simulated benchmarks - Franka Kitchen, Blocking Pushing, Push-T, and LIBERO Goal. We observe that DynaMo matches or significantly outperforms prior work on all simulated tasks.", "description": "This table presents a comparison of the downstream policy performance using different visual representation methods on four simulated benchmark environments: Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal.  The results show the average number of tasks completed for each environment, normalized to the maximum possible value (e.g., ./4 for Franka Kitchen, where the maximum number of tasks is 4). The table compares DynaMo's performance against various pretrained and self-supervised visual representation methods, demonstrating DynaMo's significant improvement in downstream policy performance. ", "section": "4 Experiments"}, {"figure_path": "vUrOuc6NR3/tables/tables_7_3.jpg", "caption": "Table 2: We evaluate DynaMo on eight tasks across two real-world environments: Allegro Manipulation, and xArm Kitchen. Results are presented as (successes/total). We observe that DynaMo significantly outperforms prior representation learning methods on real tasks.", "description": "This table presents the results of applying DynaMo to eight real-world robotic manipulation tasks.  Two environments are used: Allegro Manipulation (three tasks) and xArm Kitchen (five tasks). The performance of DynaMo is compared to BYOL, BYOL-T, and MoCo-v3, showing a significant improvement in success rates for DynaMo.", "section": "4.2 Does DynaMo improve downstream policy performance?"}, {"figure_path": "vUrOuc6NR3/tables/tables_8_1.jpg", "caption": "Table 6: Ablation analysis of downstream performance relative to the full architecture (100%)", "description": "This table presents the results of an ablation study on the DynaMo model. It shows the impact of removing different components of the model (no forward, no inverse, no bottleneck, no covariance regularization, no stop gradient, short context) on the downstream policy performance across four simulated environments: Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal.  The performance is reported as a percentage relative to the full DynaMo model (100%).  The table helps to understand the relative importance of each component in the model's overall effectiveness.", "section": "4 Experiments"}, {"figure_path": "vUrOuc6NR3/tables/tables_8_2.jpg", "caption": "Table 7: Variants with ground truth actions, downstream performance relative to the base model (100%)", "description": "This table presents the results of ablative studies where ground truth action labels were provided during the training of the visual encoder.  Two variants are compared against the base DynaMo model: one using only inverse dynamics with ground truth actions and another including the full DynaMo model with the addition of ground truth action labels.  The downstream policy performance (relative to the base DynaMo model's performance) is shown for each variant and across four different simulated robotics environments.", "section": "4.7 Variants with access to ground truth actions"}, {"figure_path": "vUrOuc6NR3/tables/tables_18_1.jpg", "caption": "Table 8: Environment-dependent hyperparameters for DynaMo pretraining, random init", "description": "This table lists the hyperparameters used for pretraining the DynaMo model with random initialization.  It shows how different hyperparameters were set for each of the five environments: Franka Kitchen, Block Pushing, Push-T, LIBERO Goal, and xArm Kitchen.  The hyperparameters listed are: the observation context (number of frames considered), the EMA beta value (used for exponential moving average in the encoder), forward dynamics dropout rate, and the transition latent dimensionality.  The table illustrates that the hyperparameter settings were customized for each environment to optimize performance.", "section": "B.1 Visual encoder training"}, {"figure_path": "vUrOuc6NR3/tables/tables_18_2.jpg", "caption": "Table 1: Downstream policy performance on frozen visual representation on four simulated benchmarks - Franka Kitchen, Blocking Pushing, Push-T, and LIBERO Goal. We observe that DynaMo matches or significantly outperforms prior work on all simulated tasks.", "description": "This table compares the performance of different methods for training downstream visual imitation policies. The methods are compared based on their performance on four simulated benchmark tasks: Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal. The table shows that DynaMo significantly outperforms other methods, especially on more challenging closed-loop tasks. ", "section": "4.2 Does DynaMo improve downstream policy performance?"}, {"figure_path": "vUrOuc6NR3/tables/tables_18_3.jpg", "caption": "Table 8: Environment-dependent hyperparameters for DynaMo pretraining, random init", "description": "This table presents the hyperparameters used for pretraining the DynaMo model with random initialization.  It shows the observation context, EMA beta value, and transition latent dimension used for each of the five environments: Franka Kitchen, Block Pushing, Push-T, LIBERO Goal, and xArm Kitchen.  The hyperparameters were chosen based on empirical results and are specific to each environment due to differences in task complexity and data characteristics.", "section": "B.1 Visual encoder training"}, {"figure_path": "vUrOuc6NR3/tables/tables_18_4.jpg", "caption": "Table 11: Shared hyperparameters for DynaMo fine-tuning", "description": "This table shows the hyperparameters used for fine-tuning the DynaMo model from ImageNet weights.  It includes settings for the optimizer (AdamW), learning rate, dropout rates, weight decay, betas, gradient clipping, covariance regularization, number of epochs, and batch size.", "section": "4 Experiments"}, {"figure_path": "vUrOuc6NR3/tables/tables_19_1.jpg", "caption": "Table 1: Downstream policy performance on frozen visual representation on four simulated benchmarks - Franka Kitchen, Blocking Pushing, Push-T, and LIBERO Goal. We observe that DynaMo matches or significantly outperforms prior work on all simulated tasks.", "description": "This table presents a comparison of the downstream policy performance using different visual representation methods on four simulated robotic manipulation tasks.  The performance is measured using several metrics for each task.  The results show that DynaMo generally outperforms existing methods, particularly on more complex tasks.", "section": "4.2 Does DynaMo improve downstream policy performance?"}, {"figure_path": "vUrOuc6NR3/tables/tables_20_1.jpg", "caption": "Table 13: Hyperparameters for VQ-BeT training", "description": "This table shows the hyperparameters used for training the Vector-Quantized Behavior Transformer (VQ-BeT) policy on four different simulated benchmark environments: Franka Kitchen, Block Pushing, Push-T, and LIBERO Goal.  The hyperparameters listed include batch size, number of epochs, window size, prediction window size, learning rate, and weight decay.  These settings were specific to each environment and reflect the choices made to achieve optimal performance on each task.", "section": "4.4 Is DynaMo compatible with different policy classes?"}, {"figure_path": "vUrOuc6NR3/tables/tables_20_2.jpg", "caption": "Table 14: Hyperparameters for Diffusion Policy Training", "description": "This table shows the hyperparameters used for training the Diffusion Policy model on the Push-T simulated environment.  The hyperparameters include the batch size, number of epochs, learning rate, weight decay, observation horizon, prediction horizon, and action horizon.", "section": "4.4 Is DynaMo compatible with different policy classes?"}, {"figure_path": "vUrOuc6NR3/tables/tables_20_3.jpg", "caption": "Table 15: Hyperparameters for MLP Training", "description": "This table presents the hyperparameters used for training Multilayer Perceptron (MLP) policies in the Push-T environment.  It lists the batch size, number of epochs, learning rate, weight decay, hidden layer dimensions, hidden layer depth, observation context, and prediction context.  These settings were used for the downstream policy training experiment to demonstrate the effectiveness of the proposed DynaMo method.", "section": "4.4 Is DynaMo compatible with different policy classes?"}]