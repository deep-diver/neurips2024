{"references": [{"fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-07-01", "reason": "This paper is foundational for text-to-image diffusion models, providing the basis for many subsequent works including this one."}, {"fullname_first_author": "Chitwan Saharia", "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding", "publication_date": "2022-12-01", "reason": "This work significantly advanced the state-of-the-art in text-to-image generation, achieving high photorealism and demonstrating strong language understanding capabilities."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "This paper introduced a crucial technique for generating high-resolution images using latent diffusion models, a key element in many modern T2I systems."}, {"fullname_first_author": "Nataniel Ruiz", "paper_title": "DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation", "publication_date": "2023-06-01", "reason": "DreamBooth is a highly influential method for personalizing T2I models, enabling the generation of images featuring specific subjects from only a few examples, directly addressing a key challenge in this field and serving as a baseline for this paper."}, {"fullname_first_author": "Kihyuk Sohn", "paper_title": "Styledrop: Text-to-image generation in any style", "publication_date": "2023-06-01", "reason": "This paper introduces a method for effective style transfer in text-to-image generation, a closely related and important area within the field, providing another relevant baseline for comparison."}]}