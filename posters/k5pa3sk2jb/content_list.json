[{"type": "text", "text": "ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kiyohiro Nakayama1 Mikaela Angelina Uy1,2 Yang You1 Ke Li3 Leonida J. Guibas1 1 Stanford University 2 Nvidia 3 Simon Fraser University w4756677@stanford.edu keli@sfu.ca {mikacuy, yangyou, guibas}@cs.stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural radiance fields (NeRFs) have gained popularity with multiple works showing promising results across various applications. However, to the best of our knowledge, existing works do not explicitly model the distribution of training camera poses, or consequently the triangulation quality, a key factor affecting reconstruction quality dating back to classical vision literature. We close this gap with ProvNeRF, an approach that models the provenance for each point \u2013 i.e., the locations where it is likely visible \u2013 of NeRFs as a stochastic field. We achieve this by extending implicit maximum likelihood estimation (IMLE) to functional space with an optimizable objective. We show that modeling per-point provenance during the NeRF optimization enriches the model with information on triangulation leading to improvements in novel view synthesis and uncertainty estimation under the challenging sparse, unconstrained view setting against competitive baselines1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural radiance fields (NeRFs) [42], allowing for learning 3D scenes given only 2D images, have grown in popularity in recent years. It has shown promise in many different applications such as novel view synthesis [4, 6], depth estimation [14], robotics [22, 1], localization [36, 38], etc. Existing literature [11, 15, 45] show that the quality of NeRF reconstruction is correlated with the selection of training camera poses. Similar correlations are observed in the classical literature too, triangulation is highly dependent on camera poses [47, 44, 3], which greatly influences the reconstruction quality. One common and important setting in computer vision literature [2, 37, 40, 16] is the sparse view [18] setting in unconstrained [53] environments, and triangulation is even more critical, affecting the reconstruction quality as limited input views make the system more sensitive to noise. ", "page_idx": 0}, {"type": "text", "text": "Despite the correlation between triangulation and reconstruction quality, to the best of our knowledge, existing works do not explicitly model the former when optimizing the latter. In this work, we address this gap in the literature by modeling for each point the locations where it is likely visible. We dub this as the provenances of a point. Modeling and learning per-point provenance can help NeRF understand how the training cameras are distributed in space, which inherently links it to triangulation and reconstruction quality. ", "page_idx": 0}, {"type": "text", "text": "However, determining the provenances of a point $\\textbf{\\em x}$ without the underlying geometry is not straightforward as many factors influence the visibility of each point in the reconstructed geometry. For example, the literature on stereo matching [44, 47] has extensively studied the influences of camera locations on 3D reconstruction. One such well-known challenge arises when selecting the baseline of a pair of cameras in a stereo system. As shown in Fig. 2, points\u2019 visibility can suffer from different sets of errors when the length of the camera pair\u2019s baseline changes. For NeRFs, the dependence becomes more complex as multiple cameras\u2019 visibility needs to be estimated. To overcome this challenge, we propose to model the provenance as the samples from a probability distribution, where a location $\\textit{\\textbf{y}}$ is assigned with a large likelihood if and only if $\\textbf{\\em x}$ is likely to be visible from $\\textit{\\textbf{y}}$ . ", "page_idx": 0}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/f6d9ea2b1b2acc9adb84921f537f8c3be386c990ae50e71a14f6c1b40870bbbe.jpg", "img_caption": ["Figure 1: (Left) ProvNeRF models a provenance field that outputs provenances for each 3D point as likely samples (arrows). For 3D points (brown triangle and blue circle), the corresponding provenances (illustrated by the arrows), are locations that likely observe them. (Right) ProvNeRF enables better novel view synthesis and estimating the uncertainty of the capturing process because it models the locations of likely observations that is critical for NeRF\u2019s optimization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To handle the potential complexity of this distribution, we represent the provenance of $\\textbf{\\em x}$ as a set of location samples, generated from a learned probability distribution. This is distinct from the existing \u201cattribute\" prediction extensions of NeRFs [69, 30, 8] since provenance is a distribution for every 3D point in space. Thus, this amounts to modeling an infinite collection of distributions (per-point\u2019s provenance) over all 3D points, which is mathematically, a stochastic field over $\\mathbb{R}^{3}$ . In our work, we extend implicit maximum likelihood estimation (IMLE) [33], a sample-based generative model, to model stochastic fields by adapting the objective to functional space. Furthermore, we derive an equivalent pointwise objective that can be efficiently optimized with gradient descent and use it to model the provenance field. ", "page_idx": 1}, {"type": "text", "text": "We dub our method ProvNeRF which models per-point provenance during the training stage of NeRF (Fig. 1). This enriches the model with information on triangulation quality when the model parameters are optimized. Once the provenance stochastic field is trained, we show that we can use it to improve novel view synthesis (Sec. 5.1) and estimate triangulation uncertainty in the capturing process (Sec. 5.2) under the challenging sparse, unconstrained view setting. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "NeRFs and their Extensions. Neural radiance fields (NeRFs) [42] have revolutionized the field of 3D reconstruction [20] and novel view synthesis [50, 32] with its powerful representation of a scene using weights of an MLP that is rendered by volume rendering [41, 58]. Follow-ups on NeRF further tackle novel view synthesis under more difficult scenarios such as unconstrained photo collections [39], unbounded [5], dynamic [35] and deformable [46] scenes, and reflective objects [59, 7]. Going beyond novel view synthesis, the NeRF representation has also shown great promise in different applications such as autonomous driving [56, 62], robotics [1, 22] and editing [67, 63]. Recent works have also extended NeRFs to model other fields in addition to color and opacity such as semantics [69, 68], normals [66], CLIP embeddings [28], image features [30] and scene flow [34]. Most of these works learn an additional function that predicts an auxiliary deterministic output at each point that is either a scalar or a vector, trained with extra supervision using volume rendering. All of the above works use a deterministic field to output the additional information. However, because each point\u2019s provenance is a probabilistic distribution, we need to model a stochastic field instead of a deterministic field for provenance. ", "page_idx": 1}, {"type": "text", "text": "Sparse View Novel View Synthesis. NeRFs with rendering supervision alone struggle with sparse view input due to insufficient constraints in volume rendering. Several approaches have been proposed to train NeRFs under the sparse-view regime with regularization losses [43, 64], semantic consistency [23], and image [60] or cost volume [12, 61, 10] feature constraints. Other works also constrain the optimization using priors from data [24, 65] or depth [49, 57, 54]. Despite addressing the setting with limited number of input views, many works are not specifically designed to tackle our desired sparse, unconstrained views setting as they either focus on object-level [24, 65, 43, 23, 21], limited camera baselines [12, 61], or forward-facing scenes [60] scenes. Recent works [49, 57, 54] have looked into improving the NeRF quality on a more difficult setting of sparse, unconstrained (outward-facing) input views by incorporating depth priors. However, none of these works consider the locations and orientations of training cameras in their optimization process despite it being one of the major factors influencing the NeRF\u2019s optimization in a sparse setup. ", "page_idx": 1}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/c67d8132c03fb929b03946e9e2a75d14aa7ec4d33bdd9347312f2750fb2b8671.jpg", "img_caption": ["Figure 2: Complex influence of camera baseline distance on the 3D reconstruction. Right: With a wide baseline, the reconstruction is more robust against 2D measurement noises. However, it is more likely to omit hidden surfaces because the invisible region is larger than a small baseline camera pair. Left: With a small baseline, the 3D reconstruction is less likely to suffer from occlusions as the invisible region between cameras is small. However, the reconstruction can be noisy due to large stereo range errors (large deviation in depth with a small amount of noise in the 2D measurement). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Uncertainty Modeling in Neural Radiance Fields. The current literature separates NeRF\u2019s uncertainty into aleatoric \u2013 e.g., transient objects or changes in lighting \u2013 and epistemic \u2013 data limitation due to weak texture or limited camera views \u2013 uncertainties. Some works [39, 26] model aleatoric uncertainty by directly predicting the uncertainty values through a neural network. However, their approach requires training on large-scale data and is not suited for estimating the uncertainty of a specific scene. On the other hand, several works explore epistemic uncertainty estimation in NeRFs through variational inference [51, 52, 48], ensemble learning [55, 31], and Bayesian inference [19, 25, 45]. While these works estimate epistemic uncertainty, they still entangle different sources of uncertainty such as texture, camera poses, and model bias, resulting in unclear and inconsistent definitions of the uncertainty quantified. In our work, we specifically model the uncertainty caused by the capturing process that is useful in various downstream tasks [45, 26, 48, 31]. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Neural Radiance Fields (NeRF) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A neural radiance field (NeRF) is a coordinate-based neural network that learns a field in 3D space, where each point $\\pmb{x}\\in\\mathbb{R}^{3}$ is of certain opacity and color. Mathematically, a NeRF is parameterized by two functions representing the two fields $\\pmb{F}_{\\phi,\\psi}=(\\pmb{\\sigma}_{\\psi}(\\pmb{x}),\\pmb{c}_{\\phi}(\\pmb{x},\\pmb{d}))$ , one for opacity $\\pmb{\\sigma}_{\\psi}:\\mathbb{R}^{3}\\rightarrow\\mathbb{R}_{+}^{\\bullet}$ and one for color $\\pmb{c}_{\\phi}:\\mathbb{R}^{3}\\times\\mathbb{S}^{2}\\rightarrow[0,1]^{3}$ , where $\\pmb{d}\\in\\mathbb{S}^{2}$ is the direction from where $\\textbf{\\em x}$ is viewed from. One of the key underpinnings of NeRFs is volume rendering allowing for end-to-end differentiable learning with only training images. Concretely, given a set of $M$ images $I_{1},I_{2},...,I_{M}$ and their corresponding camera poses $P_{1},P_{2},...,P_{M}$ , the rendered color of a pixel $x$ is the expected color along a camera ray $\\pmb{r}_{i,x}(t)=\\pmb{o}_{i}+t\\pmb{d}_{i,x}$ , where $\\pmb{o}_{i}$ is the camera origin and $d_{i,x}$ is the ray direction for pixel $x$ that can be computed from the corresponding camera pose $P_{i}$ . The pixel value for 2D coordinate $x$ is then given by the line integral: ", "page_idx": 2}, {"type": "equation", "text": "$$\nC_{\\phi,\\psi}\\left(r_{i,x}\\right)=\\int_{t_{n}}^{t_{f}}\\sigma_{\\psi}\\left(r_{i,x}(t)\\right)T\\left(r_{i,x}(t)\\right)c_{\\phi}\\left(r_{i,x}(t)\\right)\\,d t,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t_{n},t_{f}$ defines the near and far plane, and ", "page_idx": 3}, {"type": "equation", "text": "$$\nT\\left(r_{i,x}(t)\\right)=\\exp\\left[-\\int_{t_{n}}^{t}\\sigma\\left(r_{i,x}(s)\\right)\\,d s\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the transmittance of the point ${\\mathbf{}}r_{i,x}(t)$ along the direction $d_{i,x}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Implicit Maximum Likelihood Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One choice of probabilistic model is implicit maximum likelihood estimation (IMLE) [33] that represents a distribution as a set of samples and is designed to handle possibly multimodal distributions. As an implicit probabilistic model, IMLE learns a parameterized transformation $H_{\\theta}(\\cdot)$ of a latent random variable, e.g. a Gaussian $z\\sim\\mathcal{N}(0,\\mathbf{I})$ , where $H_{\\theta}(\\cdot)$ often takes the form of a neural network that output samples $\\pmb{w}_{j}=\\pmb{H}_{\\theta}(\\pmb{z}_{j})$ with $\\pmb{w}_{j}\\sim\\mathbb{P}_{\\pmb{\\theta}}(\\pmb{w})$ . Here, $\\mathbb{P}_{\\theta}$ is a probability measure obtained by transforming the standard Gaussian distribution measure via $H_{\\theta}$ . Given a set of data samples $\\{\\hat{w}_{1},...,\\hat{w}_{N}\\}$ , the IMLE objective optimizes the model parameters $\\theta$ with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\theta}=\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{z_{1},\\dots,z_{K}}\\left[\\sum_{i=1}^{N}\\operatorname*{min}_{j}\\left\\|H_{\\theta}(z_{j})-\\hat{w}_{i}\\right\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is shown that the above objective to be equivalent to maximizing the likelihood. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the following sections, we formally define the provenance at all points as a stochastic field (Sec. 4.1), extend IMLE to model the provenance field (Sec. 4.2), and derive an equivalent pointwise loss for gradient descent (Sec. 4.3). ", "page_idx": 3}, {"type": "text", "text": "Notations. We denote a stochastic field with a calligraphic font $(\\mathcal{D}_{\\theta})$ and samples from the stochastic field using the same letter but bolded $(D_{\\theta})$ . Concretely $D_{\\theta}$ is a function sample, a function defined over all points $\\pmb{x}\\in\\mathbb{R}^{3}$ , that is sampled from the stochastic field $\\mathcal{D}_{\\theta}$ , i.e. $D_{\\theta}\\sim\\mathcal{D}_{\\theta}$ . $D_{\\theta}$ maps each point to one possible sample in its provenances. We also denote the distribution of the provenances at point $\\textbf{\\em x}$ as $\\mathcal{D}_{\\theta}(\\pmb{x})$ . Moreover, a provenance sample $D_{\\theta}(x)$ from $\\mathcal{D}_{\\theta}(\\pmb{x})$ is equivalent to evaluating the function $D_{\\theta}\\sim\\mathcal{D}_{\\theta}$ at $\\textbf{\\em x}$ . Finally, we let a hat (\u02c6\u00b7) denote the empirical samples/distributions. ", "page_idx": 3}, {"type": "text", "text": "4.1 Provenance as a Stochastic Field ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The provenance of a point is defined as the locations where it is likely visible from, and as a point can be visible from multiple locations, it can be represented as samples from a distribution. Specifically, provenances of a point $\\textbf{\\em x}$ can be represented as samples from its provenance distribution $\\mathcal{D}_{\\theta}(x)$ . That means that the likelihood of sampling a location $\\pmb{y}\\in\\mathbb{R}^{3}$ from $\\mathcal{D}_{\\theta}(x)$ determines how likely $\\textbf{\\em x}$ is visible from location $\\textit{\\textbf{y}}$ . Because such distributions are defined for each 3D point $\\pmb{x}\\in\\mathbb{R}^{3}$ , the collection of per-point provenances forms a stochastic field $\\mathcal{D}_{\\theta}$ indexed by coordinates $\\pmb{x}\\in\\mathbb{R}^{3}$ . ", "page_idx": 3}, {"type": "text", "text": "Empirically, given sparse training camera views $P_{1},\\L\\dotsc,P_{M}$ , if $\\textbf{\\em x}$ is inside the camera frustum $\\Pi_{i}$ for view $P_{i}$ and is not occluded, an empirical sample from the provenances of $\\textbf{\\em x}$ can be parameterized as a distance-direction tuple $\\hat{D}_{i}(\\pmb{x})=(\\hat{t}_{i,\\pmb{x}},\\hat{d}_{i,\\pmb{x}})\\in\\mathbb{R}_{+}\\times\\mathbb{D}^{3}\\ ^{2}$ . Considering all $M$ training views, the empirical distribution of provenances at point $\\textbf{\\em x}$ is defined by the following density function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p_{e m p}\\left(t,d\\right)=\\frac{1}{M}\\sum_{i=1}^{M}\\delta[(t,d)=\\left(t_{i,x},d_{i,x}\\right)]}}\\\\ {{\\mathrm{~}\\qquad\\qquad\\mathrm{~where~}\\left(t_{i,x},d_{i,x}\\right)=\\left(v_{i,x}\\left\\Vert x-o_{i}\\right\\Vert,v_{i,x}\\frac{x-o_{i}}{\\left\\Vert x-o_{i}\\right\\Vert}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\delta$ is the Dirac delta function; $v_{i,{\\pmb x}}\\in[0,1]$ determines the length of $^d$ to handle occlusions. We modeled it as the transmittance from the NeRF model. To recover the location that observes $\\textbf{\\em x}$ , we can write $\\pmb{y}_{i,x}=\\pmb{x}-t_{i,x}\\pmb{d}_{i,x}$ . ", "page_idx": 3}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/e54083d6adfceab6d890292a89ad19118037104a34ded677751114e169fe06f8.jpg", "img_caption": ["Figure 3: Training pipeline for ProvNeRF. For each point $\\textbf{\\em x}$ seen from provenance tuple $(\\hat{t},\\hat{d})$ , with direction $^d$ at distance $t$ , we first sample $K$ latent random functions $\\{\\bar{Z}_{j}\\}$ from distribution $\\mathcal{Z}$ . The learned transformation $H_{\\theta}$ transforms each $Z_{j}(x)$ to a provenance sample $D_{\\theta}^{(j)}(x)$ . Finally $H_{\\theta}$ is trained with $\\mathcal{L}_{\\mathbf{ProvNeRF}}$ as defined in Eq. 9. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "While the above empirical distribution of provenances is given by the training cameras, the actual distribution of provenances, i.e. for each point the locations that point is likely visible from, can have a more complex dependence on both the underlying geometry and the cameras. To capture this complexity, we model $\\mathcal D_{\\boldsymbol\\theta}\\left(\\boldsymbol x\\right)$ as a learnable network, a probabilistic model that can model potentially complex distribution, and one choice of such a model is implicit maximum likelihood estimate (IMLE) [33]. Similar to the empirical distribution, we also represent provenance samples from $\\mathcal{D}_{\\theta}(\\pmb{x})$ as a distance-direction tuple $(t,d)$ as defined in $\\operatorname{Eq}5$ . We optimized our network with the empirical distribution D\u02c6 as training signals. ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{D}_{\\theta}(\\pmb{x})$ defines a distribution for all point $\\pmb{x}\\in\\mathbb{R}^{3}$ . Treating $\\mathbb{R}^{3}$ as the index set, $\\smash{\\mathcal{D}_{\\theta}=\\{\\mathcal{D}_{\\theta}\\left(\\boldsymbol{x}\\right)\\}_{\\boldsymbol{x}\\in\\mathbb{R}^{3}}}$ defines a stochastic field on $\\mathbb{R}^{3}$ as a collection of distributions $\\mathcal{D}_{\\theta}\\left(x\\right)$ for all $\\pmb{x}\\in\\mathbb{R}^{3}$ . Because a stochastic field is composed of infinitely many random variables over $\\mathrm{\\dot{R}^{3}}$ , existing methods cannot be applied out of the box as they only model finite-dimensional distributions. In the following sections, we extend IMLE [33] to model this stochastic field. ", "page_idx": 4}, {"type": "text", "text": "4.2 ProvNeRF ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "ProvNeRF models provenances of a NeRF as a stochastic field by extending IMLE [33] to functional space. IMLE learns a mapping that transforms a latent distribution to the data distribution, where each data sample is either a scalar or a vector (Sec. 3.2). However, in our context, since samples from the stochastic field $\\mathcal{D}_{\\theta}$ are functions mapping 3D locations to provenances, we need to extend IMLE to learn a neural network mapping $H_{\\theta}$ that transforms a pre-defined latent stochastic field $\\mathcal{Z}$ to the provenance distribution $\\mathcal{D}_{\\theta}$ (See Fig. 3). ", "page_idx": 4}, {"type": "text", "text": "Let $\\mathcal{Z}$ be the stochastic field where each sample $Z\\sim{\\mathcal{Z}}$ is a function $Z:\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{b}$ . To transform $\\mathcal{Z}$ to $\\mathcal{D}_{\\theta}$ , fIMLE learns a deterministic mapping $H_{\\theta}$ that maps each latent function $Z\\sim{\\mathcal{Z}}$ to a function $D_{\\theta}\\sim\\mathcal{D}_{\\theta}$ via composition: $D_{\\theta}=H_{\\theta}\\circ Z$ . $H_{\\theta}$ here is represented as a neural network to handle complex transformations from $\\mathcal{Z}$ to $\\mathcal{D}_{\\theta}$ . ", "page_idx": 4}, {"type": "text", "text": "We define a latent function sample $Z\\sim{\\mathcal{Z}}$ to be the concatenation of a random linear transformation of $\\textbf{\\em x}$ and $\\textbf{\\em x}$ itself. Mathematically, each latent function $Z\\sim{\\mathcal{Z}}$ is a block matrix of size $(\\bar{b+4})\\times3$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nZ\\left(x\\right)=\\left[\\frac{z}{I}\\right]x,\\mathrm{~where~}\\,z\\sim\\mathcal{N}\\left(\\mathbf{0},\\lambda^{2}I\\right),x\\in\\mathbb{R}^{3}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Although $Z$ can be designed to have non-linear dependence on the input location $\\textbf{\\em x}$ , we experimentally show that this simple design choice works well across different downstream applications. ", "page_idx": 4}, {"type": "text", "text": "To train $H_{\\theta}$ , we maximize the likelihood of the training provenances (Eq. 4) under $\\mathcal{D}_{\\theta}$ for each $\\textbf{\\em x}$ using the IMLE objective [33] extended to functional space. We term this extension as functional Implicit Maximum Likelihood Estimation (fIMLE). Because a direct extension to fIMLE leads to an intractable objective, we derive an efficient pointwise loss between the training provenances and model predictions equivalent to the fIMLE objective in the following section. ", "page_idx": 4}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/a4555a59c14d20b7b88f23b887540d690e972c99e0580fa23d43ab7da20b6edf.jpg", "img_caption": ["Figure 4: Visual Effect of LProvNVS in Eq. 10. Compared to pre-trained SCADE model, our method can remove additional floaters in the scene (see the boxed region). "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "K5PA3SK2jB/tmp/bdd7e47948afca25922939d6e8c438cec38c8349479f56cea568b53c8c78e880.jpg", "table_caption": [], "table_footnote": ["Table 1: Novel View Synthesis Results. Our method outperforms baselines in novel view synthesis on both Scannet and Tanks and Temple Datasets. This is because our novel NeRF regularizer in Eq. 10 can remove additional floaters in the scene as shown in Fig. 4. See Sec. 5.1 for details. "], "page_idx": 5}, {"type": "text", "text": "4.3 Functional Implicit Maximum Likelihood Estimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We construct an IMLE objective for stochastic fields to maximize the likelihood of training provenances under $\\mathcal{D}_{\\theta}$ . Similar to Eq. 3, if we have i.i.d. empirical samples $\\hat{D}_{1},\\hdots,\\hat{D}_{M}$ from the empirical stochastic field D\u02c6 (defined in Sec. 4.1), and model samples D(\u03b81 ), . . . , D(\u03b8K)from the parameter stochastic field $\\mathcal{D}_{\\theta}$ , we define the fIMLE objective as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\theta}=\\arg\\operatorname*{min}_{\\theta}\\mathbb{E}_{D_{\\theta}^{(1)},\\dots,D_{\\theta}^{(K)}}\\left[\\sum_{i=1}^{n}\\operatorname*{min}_{j}\\left\\|\\hat{D}_{i}-D_{\\theta}^{(j)}\\right\\|_{L^{2}}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Unlike the original IMLE objective (Eq. 3) that can be directly optimized, the fIMLE objective in Eq. 7 requires the computation of a $L^{2}$ integral norm \u2013 a functional analogy to the $L^{2}$ vector norm \u2013 which, in general, is not analytically in closed form. Furthermore, approximations of this integral are very expensive since each point query to $D_{\\theta}$ needs a forward pass through $H_{\\theta}$ . ", "page_idx": 5}, {"type": "text", "text": "To get around this, we use the calculus of variations to reformulate Eq. 7 to minimize the pointwise difference between the empirical samples and model predictions 3. This allows us to write the fIMLE objective as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{fIMLE}}=\\mathbb{E}_{\\mathbf{D}_{\\theta}^{(1)},\\ldots,\\mathbf{D}_{\\theta}^{(K)}\\sim\\mathcal{D}_{\\theta}}\\left[\\sum_{i=1}^{n}\\operatorname*{min}_{j}\\mathbb{E}_{x\\sim\\mathcal{U}(\\Omega)}\\left\\lVert\\hat{D}_{i}\\left(x\\right)-D_{\\theta}^{(j)}\\left(x\\right)\\right\\rVert_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{U}(\\Omega)$ is a uniform distribution over the scene bound $\\Omega$ . Eq. 8 only requires computing the pointwise difference between samples from $\\hat{\\mathcal{D}}({\\pmb x})$ and $\\mathcal{D}_{\\theta}(\\pmb{x})$ , making it efficiently optimizable with ", "page_idx": 5}, {"type": "text", "text": "gradient descent. Ultimately, ProvNeRF jointly updates the underlying NeRF\u2019s parameters and $\\mathcal{D}_{\\theta}$ by minimizing ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ProvNeRF}}=\\mathcal{L}_{\\mathrm{NeRF}}+\\mathcal{L}_{\\mathrm{flMLE}}=\\mathcal{L}_{\\mathrm{NeRF}}+\\mathbb{E}_{D_{\\theta}^{(1)},\\dots,D_{\\theta}^{(K)},x}\\left[\\operatorname*{min}_{j}\\mathbb{E}_{\\left(i,\\hat{d}\\right)}\\left\\|(\\hat{t},\\hat{d})-(t_{j,x},d_{j,x})\\right\\|_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $(t_{j,\\mathbf{x}},d_{j,\\mathbf{x}})\\;=\\;D_{\\theta}^{(j)}\\,(\\mathbf{x}),\\;(\\hat{t},$ $(\\hat{t},\\hat{d})$ are i.i.d. samples from $\\hat{\\mathcal{D}}({\\pmb x})$ , and $\\mathcal{L}_{\\mathrm{NeRF}}$ is the original objective of the NeRF model, e.g. photometric loss and depth loss. We provide implementation and architectural details in the supplementary material. See Figure 3 for the training pipeline illustration. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our ProvNeRF learns per-point provenance field $\\mathcal{D}_{\\theta}$ by optimizing $\\mathcal{L}_{\\mathrm{ProvNeRF}}$ on a NeRF-based model. To validate ProvNeRF, we demonstrate that jointly optimizing the provenance distribution and NeRF representation can result in better scene reconstruction as shown in the task of novel view synthesis (Sec. 5.1). Moreover, we also show that the learned provenance distribution enables other downstream tasks such as estimating the uncertainty of the capturing field (Sec. 5.2). We provide an ablation study on fIMLE against other probabilistic methods in Sec. 5.3. Lastly, in Sec. 5.4, we show a preliminary extension of ProvNeRF to 3DGS [27]. ", "page_idx": 6}, {"type": "text", "text": "Stochastic Provenance Field Visualization Fig. 7 visualizes the provenance stochastic field by sampling 16 provenances on a test view of the Scannet 758 scene. The directions of the samples are the negative of the predicted provenance directions for better illustration. Each sample is colored based on its predicted visibility. Notice that fIMLE allows ProvNeRF to predict multimodal provenance distributions at different scene locations. ", "page_idx": 6}, {"type": "text", "text": "5.1 Novel View Synthesis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We show modeling per-point provenance improves sparse, unconstrained novel view synthesis. As a point\u2019s provenances are sample locations from where the point is likely visible, the region between the provenance location samples and the query point should likely be empty. We design our provenance loss for NVS with this intuition. ", "page_idx": 6}, {"type": "text", "text": "Concretely, starting from a given NeRF model, we first sample points $\\pmb{x}_{1},...,\\pmb{x}_{N}$ for a training camera ray parameterized as $\\hat{\\pmb{r}}_{x}(t)$ . Here we denote point $\\mathbf{\\deltax}_{i}\\;\\;=\\;\\;\\hat{r}_{x}(\\hat{t}_{i})$ . We only take points $\\pmb{x}_{i}$ with transmittance greater than a selected threshold $\\lambda=0.9$ . For each visible point $\\pmb{x}_{i}$ , we sample provenances $(t_{1}^{(i)},\\mathbf{\\dot{\\alpha}}_{d_{1}}^{*}),\\dots,(t_{K}^{(i)},d_{K}^{(i)})$ from $\\mathcal{D}_{\\theta}(\\pmb{x}_{i})$ with $\\lVert\\pmb{d}_{1}^{(i)}\\rVert_{2}\\geq0.7_{,}$ (.i )The(ni) each distance", "page_idx": 6}, {"type": "table", "img_path": "K5PA3SK2jB/tmp/ea0030cdaec6bf4587c7d05962cb975772ee3d030c73fa0464942b87cde16ebc.jpg", "table_caption": ["Table 2: NVS Ablation Results on Scannet. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "directio1n  tu2 ple $(t_{j}^{(i)},\\pmb{d}_{j}^{(i)})$ gives a location $\\pmb{y}_{j}^{(i)}=\\pmb{x}_{i}-t_{j}^{(i)}\\pmb{d}_{j}^{(i)}$ from which $\\pmb{x}_{i}$ is observed. This in turn means $\\textbf{\\em x}$ should be equally visible when rendered from ray parameterized as $\\pmb{r}_{x}^{(i)}(t)=\\pmb{y}_{j}^{(i)}+t\\pmb{d}_{j}^{(i)},\\forall j$ . With this, we define our provenance loss for novel view synthesis as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ProvNVS}}=\\sum_{i=1}^{N}\\sum_{j=1}^{K}\\left[\\alpha\\ +\\ T({r}_{x}^{(i)}(t_{j}^{(i)}))-T(\\hat{r}_{x}(\\hat{t}_{i}))\\right]_{+},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $[\\cdot\\cdot\\cdot]_{+}$ denotes the hinge loss and $\\alpha=0.05$ is a constant margin. $\\mathcal{L}_{\\mathrm{ProvNVS}}$ encourages the transmittance at $\\pmb{x}_{i}$ along training camera rays to be at least the visibility predicted by the sampled provenances from the provenance field with margin $\\alpha$ . By matching transmittances between the provenance directions and the training rays, $\\mathcal{L}_{\\mathrm{ProvNVS}}$ can be used together with $\\mathcal{L}_{\\mathbf{ProvNeRF}}$ to optimize the NeRF representation and the provenance field, resulting in an improved scene geometry. We apply ProvNeRF to SCADE [57] for the task of novel view synthesis. See the supplement for details on the dataset, metrics, baselines, and implementation details. ", "page_idx": 6}, {"type": "text", "text": "Results. Table 1 shows our approach outperforms the state-of-the-art baselines in NVS on scenes from both the Scannet [13] and Tanks and Temples [29] dataset. Qualitative comparisons are shown in Fig. 4. We see that compared to the baseline SCADE, whose geometry is already relatively crisp, our $\\mathcal{L}_{\\mathrm{ProvNVS}}$ can further improve its NVS quality by removing additional cloud artifacts, as shown in the encircled regions. Note that this improvement does not require any additional priors and is only based on the provenance of the scene. ", "page_idx": 6}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/577bf819ac92590898f2c57bf84537ee485944540c5be9ec5dd2d239bec30592.jpg", "img_caption": ["Figure 5: Qualitative Results for Uncertainty Modeling. We visualize our uncertainty maps obtained using the method described in Sec. 5.2. The uncertainty and depth error maps are shown with color bars specified. Uncertainty values and depth errors are normalized per test image for the result to be comparable. As shown in the boxed regions, our method predicts uncertainty regions with more correlation with the predicted depth errors. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "K5PA3SK2jB/tmp/558425429a1e3aea8bcce0695d4e3d74eeaa8b6d44850d482ce20b7b828f6a41.jpg", "table_caption": ["Table 3: NLL Results for Triangulation Uncertainty. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We also compare our performance with deterministic baselines: Deterministic Field regresses one provenance for each 3D location using a neural network and Frustum Check calculates the training provenance defined in Eq. 4 by back-projecting the sampled points to one of the training camera and use that as the regularization information. Table 2 shows that our provenance field outperforms these baselines on the novel view synthesis task because the deterministic field cannot model complex provenance distribution and the frustum check baseline lacks generalization ability as it cannot be optimized to adapt the output provenance based on the current NeRF\u2019s geometry. ", "page_idx": 7}, {"type": "text", "text": "5.2 Modeling Uncertainty in the Capturing Process ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Provenances allow for estimating the uncertainty in triangulation, i.e., the capturing process. In classical multiview geometry [20], the angle between the rays is a good rule of thumb that determines the accuracy of reconstruction. Fig. 6 illustrates this rule as the region of uncertainty changes depending on the setup of the cameras. Formally, for a 3D point $\\textbf{\\em x}$ , we sample provenances $\\{(t_{j},\\pmb{d}_{j})\\}_{j=1}^{K}$ from $\\mathcal{D}_{\\theta}(\\pmb{x})$ . Treating $d_{j}$ as the principal axes and $t_{j}$ as the distances from $\\textbf{\\em x}$ to the camera origin, each provenance sample defines a pseudo camera $P_{j}$ that observes $\\textbf{\\em x}$ at pixel location $x_{j}=\\mathrm{Proj}_{j}(x)$ . Following chapter 12.6 of [20], we define the triangulation uncertainty of $\\textbf{\\em x}$ as the probability of $\\textbf{\\em x}$ given its noisy 2D pseudo observations: $\\mathbb{P}\\left(\\pmb{x}|x_{1},\\ldots,x_{K}\\right)\\propto\\mathbb{P}\\left(\\dot{x_{1}},\\ldots,\\dot{\\b{x}_{K}}|\\pmb{x}\\right)\\mathbb{P}\\left(\\pmb{x}\\right)=$ $\\begin{array}{r}{\\prod_{j=1}^{K}\\mathbb{P}\\left(x_{j}|\\pmb{x}\\right)\\mathbb{P}\\left(\\pmb{x}\\right)}\\end{array}$ . ", "page_idx": 7}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/0c23dae2f8ba08012ebd1b716eae1d9b98073283429bab49e6e7ffd2f0635b73.jpg", "img_caption": ["Figure 6: Triangulation Uncertainty [20]. The figure shows that $\\pmb{x}^{\\prime}$ is more uncertain compared to $\\textbf{\\em x}$ because the predicted provenances for $\\pmb{x}^{\\prime}$ give a narrower baseline than the baseline given by provenances of $\\textbf{\\em x}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/a639b955d3298d26283e485361ae6db4bb527f1a3f664b93e8ab6b66e81b99ca.jpg", "img_caption": ["Figure 7: Visualization of Provenance Field. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "K5PA3SK2jB/tmp/8e730a48803abd4ad40d9828df01b818c3e6340a6dae5e46a03e12f283e8c382.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The last two equalities are derived by assuming independence of the 2D observations and each $\\mathbb{P}(x_{j}|\\pmb{x})$ follows a Gaussian distribution $\\mathcal{N}(\\mathrm{Proj}_{j}(\\breve{\\pmb{x}}),\\sigma^{2}\\)$ . This assumption is equivalent to corrupting each 2D observation $\\mathrm{Proj}_{j}(x)$ by a zero-mean Gaussian noise with $\\sigma^{2}$ variance, accounting for measurement noises in the capturing process. Assuming a uniform prior of $\\mathbb{P}(x)$ over the scene bound, the exact likelihood can be efficiently computed with importance sampling. This quantifies a point\u2019s triangulation quality given the sampled provenances, which becomes a measurement of the uncertainty of the capturing process. We apply our provenance field to ProvNeRF with different NeRF backbones [57, 49] and compute the likelihood. See supplementary for details on the dataset, metrics, baselines, and implementations. ", "page_idx": 8}, {"type": "text", "text": "Results. Tab. 3 shows the quantitative results on Scannet [13] and Matterport3D [9]. We follow [51, 55] to measure the negative log-likelihood (NLL) of the ground-truth surface under each model\u2019s uncertainty prediction. Since our ProvNeRF can be applied to any pre-trained NeRF module, we use pre-trained SCADE [57] for Scannet and DDP [49] for Matterport3D, both of which are stateof-the-art approaches in each dataset. Our approach achieves the best NLL across all scenes in both datasets by a margin because we compute a more fundamentally grounded uncertainty from classical multiview geometry [20] based on triangulation errors, while both CF-NeRF and Bayes\u2019 Rays require an approximation of the true posterior likelihood. Fig. 5 shows qualitative comparisons between baselines\u2019 and our method\u2019s uncertainty estimation. We expect a general correlation between uncertain regions with high-depth errors. An ideal uncertainty map should mark high-depth error regions with high uncertainty and vice versa. As shown in the boxed regions in the figure, our method\u2019s uncertainty map shows better correlation with the depth error maps. We also quantitatively evaluate uncertainty maps using negative log-likelihood following prior works Notice that in both examples, our method\u2019s certain (blue) regions mostly have low-depth errors (e.g., encircled parts in Fig. 5) because our formulation only assigns a region to be certain if it is well triangulated (Fig. 6). On the other hand, baselines struggle in these regions because they either use an empirical approximation from data or a Gaussian approximation of the ground truth posterior likelihood. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We validate the choice of fIMLE as our probabilistic model by measuring the average precision (AP) [17] and area under the curve (AUC) [17] of predicted provenances $(t,d)$ against ground truth provenances $(\\hat{t},\\hat{d})$ for a set of densely sampled points in the scene bound. See supplementary for metric and ablation implementation details. ", "page_idx": 8}, {"type": "text", "text": "Deterministic v.s. Stochastic Field. We validate the importance of modeling per-point provenance as a stochastic field rather than a deterministic field. We model $\\mathcal{D}_{\\theta}$ with a deterministic field parameterized by a neural network. Table 4 shows the importance of modeling per-point provenance as a stochastic field. Since the provenances of a point are inherently multimodal, a deterministic field that only maps each $\\textbf{\\em x}$ to a single provenance cannot capture this multimodality. ", "page_idx": 8}, {"type": "text", "text": "Choice of Probabilistic Model. We validate our choice of fIMLE [33] as our probabilistic model. We first compare with explicit probabilistic models that model the provenance field as a mixture of $C$ Gaussian processes and a VAE-based model. Table 4 shows results for the Gaussian Mixture field with $C=2,5$ and the VAE-based process. Although the performances for the Gaussian-based models improve as we increase $C$ , they still suffer from expressivity because of their explicit density ", "page_idx": 8}, {"type": "image", "img_path": "K5PA3SK2jB/tmp/e5c48d0dface1f39f2b1102814a20116315064eff5ae768739ba9b6649a66a4e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 8: Uncertainty Estimation Comparison with 3DGS. Compared with FishRF, our method is able to estimate uncertainties that correlate more with the depth error as shown by the encircled regions. The right shows a quantitative comparison of uncertainty in negative log-likelihood. ", "page_idx": 9}, {"type": "text", "text": "assumption. Similarly, the VAE-based model suffers from mode-collapse while our fIMLE enables capturing a more complex distribution with a learned transformation $H_{\\theta}$ . ", "page_idx": 9}, {"type": "text", "text": "Choice of Random Function $\\mathcal{Z}$ . Lastly, we validate our latent stochastic field $\\mathcal{Z}$ . We ablate our choice of $\\mathcal{Z}$ with instead using a spatially invariant latent stochastic field $\\mathcal{Z}^{\\star}$ with $\\mathcal{Z}^{\\star}\\left(\\pmb{x}\\right)=\\left[\\varepsilon,\\pmb{x}\\right]\\forall x$ . Here, $\\varepsilon$ is a Gaussian noise vector in $\\mathbb{R}^{d}$ . Table 4 shows the comparison between $\\mathcal{D}_{\\theta}$ obtained by transforming $\\mathcal{Z}$ (Ours) and transforming $\\mathcal{Z}^{\\star}$ (Spatial Inv. $\\mathcal{Z}$ ). We see that using a spatially varying latent stochastic field further increases the expressivity of our model. ", "page_idx": 9}, {"type": "text", "text": "5.4 Preliminary Extension to 3D Gaussian Splatting ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Because ProvNeRF is a post-hoc method that can model the provenance information for arbitrary novel view synthesis representations, we conduct a preliminary experiment that extends our provenance field modeling to 3D Gaussian Splatting [27]. Specifically, given a pre-trained Gaussian representation $\\mathcal{G}$ , we model a provenance distribution for each splat using IMLE with a shared 6-layer MLP for $H_{\\theta}$ . The post-training takes around 30 minutes on a single A6000 Nvidia GPU. To show the usefulness of ProvNeRF applied to 3DGS, we use the methodology in Sec. 5.2 to estimate uncertainty maps and compare them with the predicted depth errors. Fig. 8 shows a qualitative comparison of our uncertainty map w.r.t. FishRF [25], a recent 3DGS uncertainty estimation baseline. Compared to their uncertainty map, ours shows more correlation to the depth error as highlighted by the boxed regions. Quantitatively we evaluate NLL on the three Scannet scenes shown on the right side of the same figure and show substantial improvements over FishRF. This improvement over existing literature suggests applying ProvNeRF to other representations such as 3DGS is promising. We leave further exploration of the method and applications as future works. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion, Limitation, & Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present ProvNeRF, a model that enhances the traditional NeRF representation by modeling provenance through an extension of IMLE for stochastic processes. ProvNeRF can be easily applied to any NeRF model to enrich its representation. We showcase the advantages of modeling per-point provenance in various downstream applications such as improving novel view synthesis and modeling the uncertainty of the capturing process. ", "page_idx": 9}, {"type": "text", "text": "We note that our work is not without limitations. Our ProvNeRF requires post-hoc optimization, which takes around 8 hours on SCADE, limiting its current usability for real-time or on-demand applications. However, the idea presented in our work is not specific to the model design and can be adapted to other representations. See Sec. 5.4 for preliminary adaption of ProvNeRF to 3DGS. ", "page_idx": 9}, {"type": "text", "text": "We also note that the hyperparameters to incorporate ProvNeRF are chosen for better performance, e.g. for the uncertainty and novel view synthesis applications, and in the future, it will be beneficial to explore a more adaptive approach in integrating provenance to different downstream applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement This work is supported by a Vannevar Bush Faculty Fellowship, ARL grant W911NF-21-2-0104, an Apple Scholars in AI/ML PhD Fellowship, and the Natural Sciences and Engineering Research Council of Canada (NSERC). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, and Mac Schwager. Vision-only robot navigation in a neural radiance world. IEEE Robotics and Automation Letters, 7(2):4606\u20134613, 2022.   \n[2] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz, and Richard Szeliski. Building rome in a day. In ICCV, 2009.   \n[3] Kalin Atanassov, Vikas Ramachandra, Sergio R. Goma, and Milivoje Aleksic. 3d image processing architecture for camera phones. In J. Angelo Beraldin, Geraldine S. Cheok, Michael B. McCarthy, Ulrich Neuschaefer-Rube, Ian E. McDowall, Margaret Dolinsky, and Atilla M. Baskurt, editors, ThreeDimensional Imaging, Interaction, and Measurement, volume 7864, page 786414. International Society for Optics and Photonics, SPIE, 2011.   \n[4] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021.   \n[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022.   \n[6] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. ICCV, 2023.   \n[7] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick HoldGeoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reflectance fields for appearance acquisition. arXiv preprint arXiv:2008.03824, 2020.   \n[8] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. In NeurIPS, 2023.   \n[9] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. 3DV, 2017.   \n[10] Di Chang, Alja\u017e Bo\u017ei\u02c7c, Tong Zhang, Qingsong Yan, Yingcong Chen, Sabine S\u00fcsstrunk, and Matthias Nie\u00dfner. Rc-mvsnet: Unsupervised multi-view stereo with neural rendering. In ECCV, 2022.   \n[11] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In arXiv, 2023.   \n[12] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In CVPR, 2021.   \n[13] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017.   \n[14] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In CVPR, 2022.   \n[15] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline stereo pairs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.   \n[16] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(3):611\u2013625, 2018.   \n[17] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303\u2013338, 2010.   \n[18] Yasutaka Furukawa and Carlos Hern\u00e1ndez. Multi-View Stereo: A Tutorial. Now Foundations and Trends, 2015.   \n[19] Lily Goli, Cody Reading, Silvia Sell\u00e1n, Alec Jacobson, and Andrea Tagliasacchi. Bayes\u2019 rays: Uncertainty quantification in neural radiance fields. arXiv, 2023.   \n[20] Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2 edition, 2004.   \n[21] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparsecontrolled gaussian splatting for editable dynamic scenes. arXiv preprint arXiv:2312.14937, 2023.   \n[22] Jeffrey Ichnowski\\*, Yahav Avigal\\*, Justin Kerr, and Ken Goldberg. Dex-nerf: Using a neural radiance field to grasp transparent objects. In Conference on Robot Learning (CoRL), 2020.   \n[23] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5885\u20135894, October 2021.   \n[24] Wonbong Jang and Lourdes Agapito. Codenerf: Disentangled neural radiance fields for object categories. In ICCV, 2021.   \n[25] Wen Jiang, Boshu Lei, and Kostas Daniilidis. Fisherrf: Active view selection and uncertainty quantification for radiance fields using fisher information, 2023.   \n[26] Liren Jin, Xieyuanli Chen, Julius R\u00fcckin, and Marija Popovi\u00b4c. Neu-nbv: Next best view planning using uncertainty estimation in image-based neural rendering. In IROS, 2023.   \n[27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023.   \n[28] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In ICCV, 2023.   \n[29] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017.   \n[30] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. In NeurIPS, 2022.   \n[31] Georgios Kopanas and George Drettakis. Improving nerf quality by progressive camera placement for unrestricted navigation in complex environments. In Vision, Modeling, and Visualization, 2023.   \n[32] K.N. Kutulakos and S.M. Seitz. A theory of shape by space carving. In ICCV, 1999.   \n[33] Ke Li and Jitendra Malik. Implicit maximum likelihood estimation, 2018.   \n[34] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021.   \n[35] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In CVPR, 2023.   \n[36] Jianlin Liu, Qiang Nie, Yong Liu, and Chengjie Wang. Nerf-loc: Visual localization with conditional neural radiance field. In ICRA, 2023.   \n[37] Sheng Liu, Xiaohan Nie, and Raffay Hamid. Depth-guided sparse structure-from-motion for movies and tv shows. In CVPR, 2022.   \n[38] Dominic Maggio, Marcus Abate, Jingnan Shi, Courtney Mario, and Luca Carlone. Loc-nerf: Monte carlo localization using neural radiance fields, 2022.   \n[39] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In CVPR, 2021.   \n[40] Hidenobu Matsuki, Raluca Scona, Jan Czarnowski, and Andrew J. Davison. Codemapping: Real-time dense mapping for sparse slam using compact scene representations. IEEE Robotics and Automation Letters, 2021.   \n[41] Nelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics, 1(2):99\u2013108, 1995.   \n[42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.   \n[43] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In CVPR, 2022.   \n[44] M. Okutomi and T. Kanade. A multiple-baseline stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(4):353\u2013363, 1993.   \n[45] Xuran Pan, Zihang Lai, Shiji Song, and Gao Huang. Activenerf: Learning where to see with uncertainty estimation. In ECCV, 2022.   \n[46] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021.   \n[47] Satyarth Praveen. Efficient depth estimation using sparse stereo-vision with other perception techniques. In Sudhakar Radhakrishnan and Muhammad Sarfraz, editors, Coding Theory, chapter 7. IntechOpen, Rijeka, 2019.   \n[48] Yunlong Ran, Jing Zeng, Shibo He, Jiming Chen, Lincheng Li, Yingke Chen, Gim Hee Lee, and Qi Ye. Neurar: Neural uncertainty for autonomous 3d reconstruction with implicit neural representations. IEEE Robotics and Automation Letters, 2022.   \n[49] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan, and Matthias Nie\u00dfner. Dense depth priors for neural radiance fields from sparse input views. In CVPR, 2022.   \n[50] S.M. Seitz and C.R. Dyer. Photorealistic scene reconstruction by voxel coloring. In CVPR, 1997.   \n[51] J. Shen, A. Ruiz, A. Agudo, and F. Moreno-Noguer. Stochastic neural radiance fields: Quantifying uncertainty in implicit 3d representations. In 3DV, 2021.   \n[52] Jianxiong Shen, Antonio Agudo, Francesc Moreno-Noguer, and Adria Ruiz. Conditional-flow nerf: Accurate 3d modelling with reliable uncertainty quantification. In ECCV, 2022.   \n[53] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo tourism: Exploring photo collections in 3d. ACM Trans. Graph., 2006.   \n[54] Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin Cho, and Seungryong Kim. D\u00e4rf: Boosting radiance fields from sparse inputs with monocular depth adaptation. In NeurIPS, 2023.   \n[55] Niko S\u00fcnderhauf, Jad Abou-Chakra, and Dimity Miller. Density-aware nerf ensembles: Quantifying predictive uncertainty in neural radiance fields. In ICRA, 2022.   \n[56] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In CVPR, 2022.   \n[57] Mikaela Angelina Uy, Ricardo Martin-Brualla, Leonidas Guibas, and Ke Li. Scade: Nerfs from space carving with ambiguity-aware depth estimates. In CVPR, 2023.   \n[58] Mikaela Angelina Uy, George Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas, and Ke Li. Nerf revisited: Fixing quadrature instability in volume rendering. In NeurIPS, 2023.   \n[59] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In CVPR, 2022.   \n[60] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In CVPR, 2021.   \n[61] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. In ICCV, 2021.   \n[62] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In ECCV, 2022.   \n[63] Tianhan Xu and Tatsuya Harada. Deforming radiance fields with cages. In ECCV, 2022.   \n[64] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[65] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021.   \n[66] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. In NeurIPS, 2022.   \n[67] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. Nerf-editing: Geometry editing of neural radiance fields. In CVPR, 2022.   \n[68] Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser, Leonidas Guibas, Hao Su, and Kyle Genova. Nerflets: Local radiance fields for efficient structure-aware 3d scene representation from 2d supervision. In CVPR, 2023.   \n[69] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew Davison. In-place scene labelling and understanding with implicit scene representation. In ICCV, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We discuss the limitations of our method in the supplementary ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: we provide the full derivation of our fIMLE objective in the supplementary. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We provide the training details, hyperparameters, network architectures in the supplementary. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We will provide reproducible code upon the paper\u2019s acceptance. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "page_idx": 14}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 14}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] Guidelines: ", "page_idx": 14}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We address the potential societal impact on climate change in the supplementary materials ", "page_idx": 15}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}]