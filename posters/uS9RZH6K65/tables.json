[{"figure_path": "uS9RZH6K65/tables/tables_6_1.jpg", "caption": "Table 1: Comparisons between Various Competitors. Using ActionCLIP [49] as \u03a6OVAR while evaluating on UCF101, we compare with statistical text spell-checkers (PySpellChecker [15]), neural based ones (Bert from NeuSpell) [13], and GPT 3.5 [1]. Our method remarkably outperforms others in terms of Top-1 classification accuracy, and semantic similarity of recovered text descriptions.", "description": "This table compares the performance of the proposed DENOISER method against several baseline methods for noisy open-vocabulary action recognition on the UCF101 dataset using ActionCLIP as the underlying OVAR model. The baselines include statistical spell-checkers (PySpellChecker), neural-based spell-checkers (Bert from NeuSpell), and the GPT 3.5 language model.  The comparison is made across different noise rates (0%, 5%, and 10%), evaluating Top-1 accuracy, label accuracy, and semantic similarity of corrected text descriptions. The results demonstrate that DENOISER significantly outperforms the baseline methods.", "section": "4 Experiments"}, {"figure_path": "uS9RZH6K65/tables/tables_7_1.jpg", "caption": "Table 2: Comparison Across Datasets and Models. On three standard datasets, facing multiple noise types (real or simulated), and under various noise rates, our DENOISER consistently improves the performance for noisy OVAR, regardless of underlying OVAR methods \u03a6OVAR.", "description": "This table presents a comparison of the performance of the proposed DENOISER method against existing methods on three different datasets (UCF101, HMDB51, and K700).  The comparison is done under different noise conditions: real-world noise and simulated noise at various noise rates (5% and 10%). The results are shown for two different baseline open-vocabulary action recognition (OVAR) models: ActionCLIP and XCLIP.  The table demonstrates the robustness and generalizability of DENOISER across various datasets and baseline models, showing consistent improvements in performance even with noisy input.", "section": "4 Experiments"}, {"figure_path": "uS9RZH6K65/tables/tables_8_1.jpg", "caption": "Table 3: Ablations for Inter-modal Weighting \nInter, Intra-modal Weighting Inter, Schedule of\nTemperature \u03bb. \u03a6Inter alone outperforms Intra. Both contribute to correcting class texts, and give\nthe best results when combined. Linear schedule of balancing factor A outperforms the constant one,\nmeaning that it helps to rely more on Intra at first, and then gradually switch to Inter.", "description": "This table presents the ablation study results on three key components of the DENOISER framework: inter-modal weighting, intra-modal weighting, and the schedule of the temperature parameter (\u03bb).  It shows the impact of each component individually and in combination on Top-1 accuracy, label accuracy, and semantic similarity. The results demonstrate the effectiveness of both weighting schemes and the benefit of a linear temperature schedule.", "section": "4.3 Ablation Study"}, {"figure_path": "uS9RZH6K65/tables/tables_16_1.jpg", "caption": "Table 1: Comparisons between Various Competitors. Using ActionCLIP [49] as \u03a6OVAR while evaluating on UCF101, we compare with statistical text spell-checkers (PySpellChecker [15]), neural based ones (Bert from NeuSpell) [13], and GPT 3.5 [1]. Our method remarkably outperforms others in terms of Top-1 classification accuracy, and semantic similarity of recovered text descriptions.", "description": "This table compares the performance of the proposed DENOISER method against several other methods for correcting noisy text descriptions in the context of open-vocabulary action recognition.  The comparison is performed using the ActionCLIP model on the UCF101 dataset.  Competitors include statistical spell-checkers (PySpellChecker), neural-based spell-checkers (Bert from NeuSpell), and the GPT 3.5 language model. The evaluation metrics are Top-1 accuracy (Top-1 Acc), label accuracy (Label Acc), and semantic similarity. The results demonstrate that the DENOISER method significantly outperforms the other methods across all three metrics, highlighting its effectiveness in handling noisy text inputs.", "section": "4 Experiments"}]