{"references": [{"fullname_first_author": "Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that is used extensively in open-vocabulary action recognition and is directly relevant to the current work."}, {"fullname_first_author": "Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "publication_date": "2021-07-01", "reason": "This paper addresses the issue of noisy text descriptions which is a core focus of the current paper, making it highly relevant."}, {"fullname_first_author": "Wang", "paper_title": "ActionCLIP: A new paradigm for video action recognition", "publication_date": "2021-09-01", "reason": "This paper introduces ActionCLIP, one of the main baseline models used for comparison in the current work, establishing its importance."}, {"fullname_first_author": "Zhou", "paper_title": "Non-contrastive learning meets language-image pre-training", "publication_date": "2023-06-01", "reason": "This paper introduces XCLIP, another significant baseline model for comparison, highlighting its relevance to the current research."}, {"fullname_first_author": "Kuehne", "paper_title": "HMDB: A large video database for human motion recognition", "publication_date": "2011-01-01", "reason": "This paper introduces the HMDB-51 dataset, a key dataset used in the experimental evaluation of the current work, demonstrating its importance."}]}