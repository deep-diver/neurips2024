[{"figure_path": "MzTdZhMjeC/figures/figures_1_1.jpg", "caption": "Figure 1: An Example of Multi-object Demand-driven Navigation. A user plans to host a party at his new house and outlines some basic demands (highlighted in orange), along with specific preferences for different individuals (highlighted in red). The agent parses these demands and locates multiple objects in various locations in the scene to fulfill them. Despite not meeting the preferred \"ice cream\" demand, the agent successfully addresses basic demands, such as organizing lunch.", "description": "This figure illustrates an example of a multi-object demand-driven navigation scenario. A user is planning a party and has several demands, including basic needs (e.g., food, seating) and individual preferences (e.g., type of food).  The agent must locate multiple objects to fulfill these demands.  The visualization shows the house layout, object locations, user demands, and the agent's trajectory as it collects the required items. The example highlights that while the agent may not satisfy all preferences, it can still successfully address the fundamental demands of the situation.", "section": "1 Introduction"}, {"figure_path": "MzTdZhMjeC/figures/figures_4_1.jpg", "caption": "Figure 2: Attribute Model. This figure shows the architecture of the attribute model. Instructions and objects share the same model architecture. Instructions and items share the same model architecture. For parameters, they share only the parameters of the shared codebook, while the parameters of the MLP Encoder and Decoder are independent. Only the red with flames modules in the figure will be trained while the blue with snowflakes CLIP model parameters will be frozen.", "description": "This figure presents the architecture of the attribute model used in the MO-DDN system.  The model processes both instructions and object information through a shared pathway to generate attribute features.  It uses a CLIP model (frozen) for initial feature extraction, then an MLP encoder to generate the attribute features. These features are then quantized using a codebook (the parameters of which are shared between the instruction and object processing branches). An MLP decoder reconstructs the features, and several loss functions compare the original features with the reconstructed and quantized features, as well as matching instructions with objects. The diagram highlights that only specific parts of the model (those in red) are trained during the process.", "section": "4.1 Attribute Model Training"}, {"figure_path": "MzTdZhMjeC/figures/figures_6_1.jpg", "caption": "Figure 3: Navigation Policy. The agent continuously switches between a coarse exploration phase and a fine exploration phase until the Find count limit nfind is reached or the total number of steps Nstep is reached. See Sec. 4.2.1 and Sec. 4.2.2 for details about the two phases. In each timestep, the GLEE model is used to identify and label objects in the RGB and project them to the point cloud.", "description": "This figure illustrates the navigation policy of the proposed agent, showing how it alternates between coarse and fine exploration phases. The coarse phase involves segmenting the environment into blocks and using attribute features to select promising waypoints. The fine phase then utilizes an end-to-end module for precise object identification. The process continues until either a solution is found or a predefined number of steps or Find actions are reached.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_6_2.jpg", "caption": "Figure 4: Coarse Exploration. This figure presents the process of building and labeling the point clouds, segmenting the blocks, and calculating the scores for each block.", "description": "This figure shows the process of the coarse exploration phase in the MO-DDN navigation task.  It starts with the RGB-D image and depth map as inputs, and these are used to generate a point cloud representation of the environment. The point cloud is then segmented into blocks. For each block, the similarity between object features and instruction attributes is calculated to generate a block score. These block scores are used to select the next waypoint for the agent. The LLM branch and the MLP branch are described in the figure to show two different approaches for generating instruction attribute features. Blocks with higher scores are indicated by darker shades of red in the image.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_7_1.jpg", "caption": "Figure 5: Fine Exploration. We employ imitation learning to train an end-to-end module in this phase. This module loads the Ins MLP Encoder and Obj MLP Encoder's parameters as initialization from attribute training, along with a Transformer Encoder to integrate features. The output feature corresponding to the CLS token is combined with GPS+Compass features and a previous action embedding and passed through an LSTM to generate actions by an actor.", "description": "This figure shows the architecture of the fine exploration phase in the MO-DDN model.  It takes several inputs including RGB images, depth information, GPS/compass data, previous actions, and instruction attributes.  These are processed by various encoders (CNN, CLIP Visual/Text, Obj MLP, Ins MLP), combined in a transformer, and passed through an LSTM to generate actions for the navigation agent. This fine-tuning phase uses imitation learning and relies on the attribute model trained earlier in the process.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_18_1.jpg", "caption": "Figure 3: Navigation Policy. The agent continuously switches between a coarse exploration phase and a fine exploration phase until the Find count limit nfind is reached or the total number of steps Nstep is reached. See Sec. 4.2.1 and Sec. 4.2.2 for details about the two phases. In each timestep, the GLEE model is used to identify and label objects in the RGB and project them to the point cloud.", "description": "This figure illustrates the overall navigation policy used by the agent. The agent alternates between a coarse exploration phase and a fine exploration phase. The coarse phase involves segmenting the environment into blocks and selecting a waypoint using attribute features, and the fine phase uses an end-to-end model to locate the target object within the chosen block. The process continues until a solution is found or the limits on the number of Find operations or steps are reached.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_23_1.jpg", "caption": "Figure 3: Navigation Policy. The agent continuously switches between a coarse exploration phase and a fine exploration phase until the Find count limit nfind is reached or the total number of steps Nstep is reached. See Sec. 4.2.1 and Sec. 4.2.2 for details about the two phases. In each timestep, the GLEE model is used to identify and label objects in the RGB and project them to the point cloud.", "description": "This figure illustrates the navigation policy used in the MO-DDN task. The agent alternates between a coarse exploration phase, where it segments the environment into blocks and scores them based on attributes, and a fine exploration phase, where an end-to-end module refines object identification.  This cycle continues until a solution is found or time constraints are met.  The GLEE model plays a key role in object detection and labeling during both phases.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_25_1.jpg", "caption": "Figure 6: Block Visualizations. Instruction: I need to take quick notes during a meeting, preferably with a device that saves them digitally. Solution: Computer, Armchair, Table.", "description": "This figure visualizes the point cloud of a scene segmented into blocks.  The instruction is to find a place to take quick notes during a meeting, ideally with a digital note-taking device. The solution shown consists of the Computer, Armchair, and Table, which suggests these items are located within a high-scoring block based on their attributes and relevance to the given instruction. The red region highlights the area where the selected objects, relevant to the given instruction, are located.", "section": "A.3.1 Design Details for Coarse Exploration Module"}, {"figure_path": "MzTdZhMjeC/figures/figures_26_1.jpg", "caption": "Figure 7: Block Visualizations. Instruction: I need to store a collection of fine china, preferably in a way that displays them elegantly. Solution: Wall shelf, Cabinet.", "description": "This figure visualizes the results of the coarse exploration phase of the MO-DDN agent.  The agent is given the instruction to find a place to store fine china, preferably in an elegant way. The figure shows the point cloud representation of the environment, segmented into blocks. The color intensity of each block indicates its score, calculated based on the similarity between the block's object attributes and the instruction attributes.  Darker blocks have higher scores and higher probabilities of containing the target object(s). In this case, the blocks containing the wall shelf and cabinet are highlighted, indicating that these objects are predicted as the most suitable locations to store the fine china.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_27_1.jpg", "caption": "Figure 8: Block Visualizations. Instruction: I need to quickly dry a batch of laundry, but I prefer an fast and energy-efficient method. Solution: Dryer.", "description": "This figure shows the point cloud representation of a scene with objects detected and labeled.  The instruction given to the agent was to find a way to quickly dry laundry, and the agent identified the dryer as the most suitable solution. The red highlighted area indicates the location of the dryer within the scene.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_28_1.jpg", "caption": "Figure 9: Block Visualizations. Instruction: I need to find a comfortable place to read for my study group, preferable with good lighting. Solution: Sofa, Side table, Ceiling lamp.", "description": "This figure shows a point cloud representation of a scene in a house.  The red highlighted areas indicate areas with high scores based on an attribute-based exploration method used in the MO-DDN (Multi-object Demand-driven Navigation) task. The task given to the agent was to find a comfortable place to read with good lighting. The agent successfully identified a sofa, side table, and ceiling lamp as suitable objects that meet this demand. The darker the red color, the higher the score for the corresponding block.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_29_1.jpg", "caption": "Figure 10: Block Visualizations. Instruction: I need to organize a small evening gathering but want to avoid any accidents with real candles. Solution: Dining table, Straight chair.", "description": "This figure shows a point cloud representation of a scene, segmented into blocks.  The darker red blocks indicate a higher probability of containing objects relevant to the user's request: to organize a small evening gathering without real candles.  The caption highlights the specific instruction and the solution identified by the model (dining table and straight chair).", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_30_1.jpg", "caption": "Figure 11: The Effect of r<sub>b</sub> and r<sub>p</sub>. This is an example used only to demonstrate scoring blocks. The darker the color in the figure, the higher the block score. When the task is \u201cFind a place to sit, prefering a comfortable and soft place\u201d, lowering r<sub>p</sub> (left) ignores preferences, so ArmChair, Sofa and Stool can get a high score (dark red); raising r<sub>p</sub> (right) focuses on preferences, so Sofa and Bed can get a high score.", "description": "This figure shows how adjusting the weights (r<sub>b</sub> and r<sub>p</sub>) for basic and preferred demand similarity scores impacts block selection in the coarse exploration phase.  The darker the red shading of a block, the higher its score. The left image shows a scenario where the basic demand weight (r<sub>b</sub>) is prioritized, resulting in multiple seating options being highlighted. The right image shows a scenario where the preferred demand weight (r<sub>p</sub>) is prioritized, resulting in only the most comfortable seating options (Sofa and Bed) being strongly highlighted.", "section": "4.2 Coarse-to-fine Exploration Agent"}, {"figure_path": "MzTdZhMjeC/figures/figures_31_1.jpg", "caption": "Figure 3: Navigation Policy. The agent continuously switches between a coarse exploration phase and a fine exploration phase until the Find count limit  n<sub>find</sub> is reached or the total number of steps N<sub>step</sub> is reached. See Sec. 4.2.1 and Sec. 4.2.2 for details about the two phases. In each timestep, the GLEE model is used to identify and label objects in the RGB and project them to the point cloud.", "description": "This figure illustrates the navigation policy employed by the agent.  It shows a continuous switching between a coarse exploration phase (where the agent explores larger areas of the environment, selecting waypoints based on attribute similarity) and a fine exploration phase (where the agent explores a smaller region around a selected waypoint, using an end-to-end attribute exploration module to identify the target objects). This process continues until either a predefined number of object searches ('Find' actions) is reached, or a maximum number of navigation steps is exceeded.  The figure highlights the key components involved in each phase: point cloud generation and segmentation, object detection (GLEE model), and waypoint selection and path planning. ", "section": "4.2 Coarse-to-fine Exploration Agent"}]