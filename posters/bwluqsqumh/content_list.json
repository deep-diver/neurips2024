[{"type": "text", "text": "A Non-parametric Direct Learning Approach to Heterogeneous Treatment Effect Estimation under Unmeasured Confounding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinhai Zhang\u2217 Xingye Qiao\u2217 xzhan222@binghamton.edu xqiao@binghamton.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many social, behavioral, and biomedical sciences, treatment effect estimation is a crucial step in understanding the impact of an intervention, policy, or treatment. In recent years, an increasing emphasis has been placed on heterogeneity in treatment effects, leading to the development of various methods for estimating Conditional Average Treatment Effects (CATE). These approaches hinge on a crucial identifying condition of no unmeasured confounding, an assumption that is not always guaranteed in observational studies or randomized control trials with non-compliance. In this paper, we proposed a general framework for estimating CATE with a possible unmeasured confounder using Instrumental Variables. We also construct estimators that exhibit greater efficiency and robustness against various scenarios of model misspecification. The efficacy of the proposed framework is demonstrated through simulation studies and a real data example. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In various domains, different subjects may exhibit different responses to the same set of treatments. The exploration of this heterogeneity in the effects resulting from exposure has gained substantial interest in recent years. For instance, inferring the heterogeneous effect of a medical treatment on clinical outcome can contribute to the development of personalized treatment (Cai et al., 2011). A similar concept has found application in personalized marketing as well (Chandra et al., 2022). The heterogeneity among subjects can be measured by the disparity in conditional mean outcomes given other covariates, typically referred to as the Conditional Average Treatment Effect (CATE). Another problem closely related to the heterogeneity in treatment effects is the optimal Individualized Treatment Regime (ITR), which is a decision rule that selects treatments for individuals to maximize the expected outcome. ", "page_idx": 0}, {"type": "text", "text": "There has been significant development in the literature regarding the estimation of CATE and the optimal ITR in the case of no unmeasured confounding. For example, Q-learning (Qian and Murphy, 2011) models the conditional mean outcome under each treatment separately and the estimated CATE is constructed using the difference between the estimated conditional mean outcomes. The success of this method relies on the correct specification of the outcome models. To address this issue, direct learning (DL) (Tian et al., 2014; Qi and Liu, 2018) and robust direct learning (RD) (Meng and Qiao, 2022) models the conditional contrast between treatments directly, which has been shown to be more robust to model misspecification. Another strand of work approaches with tree-based or forest-based methods. Hill (2011) and Green and Kern (2012) extended the Bayesian Additive Regression Tree (BART) method of Chipman et al. (2010) for estimating heterogeneous treatment effect. Athey and Imbens (2016) proposed Causal Trees with an \u201chonest\u201d splitting approach, wherein the partitioning is constructed in one sample, and the treatment effects within each node are estimated using another sample. This methodology is subsequently adopted in Causal Forest (Wager and Athey, 2018), which extends the random forest algorithm to estimate heterogeneous treatment effects. On the other hand, optimal ITR estimation aims to determine the optimal decision rule for treatment assignment based on subjects\u2019 covariates to maximize the mean outcome. A significant line of work in the field involves transforming ITR estimation into a classification problem through the use of Inverse Probability Weighting (IPW). Notable contributions include Outcome Weighted Learning (OWL) (Zhang et al., 2012; Zhao et al., 2012) and Residual Weighted Learning (RWL) (Zhou et al., 2017). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The aforementioned methods all rely on the key assumption of no unmeasured confounding to identify the heterogeneous treatment effect or the optimal ITR. However, this assumption is in most cases unverifiable (if not untrue) in observational studies or randomized controlled trials (RCT) with noncompliance. A well-known approach that takes into account the unmeasured confounding is the use of an instrumental variable (IV). A proper IV is usually a pre-treatment variable that is independent of any possible unmeasured confounder while correlated with the treatment. For example, in RCT with non-compliance, the random treatment assignment can be considered as an IV while the treatment received is considered the treatment variable. Here these two are clearly correlated since a subject will not receive the treatment if they are not assigned one, though the strength of the correlation may depend on other characteristics such as the education level of the subject. ", "page_idx": 1}, {"type": "text", "text": "There is a growing literature on estimating heterogeneous treatment effects or optimal ITR under unmeasured confounding using IV. Imbens and Angrist (1994) identified and estimated the socalled Local Average Treatment Effect (LATE), restricted to the subgroup of the always-compliant population, with the help of an IV. More recently, machine learning methods like Doubly Robust IV (Syrgkanis et al., 2019) and Generalized Random Forest (Athey et al., 2019) have shown their applicability and effectiveness in various settings including unmeasured confounding, particularly when used in conjunction with an IV. Wang and Tchetgen Tchetgen (2018) introduced two alternative assumptions on the unobserved confounders and the IV, which enable the identification of the Average Treatment Effect (ATE). They proposed an estimator that has the so-called multiply robustness property, which guarantees consistent estimate under three observed data models. These findings were incorporated into Cui and Tchetgen Tchetgen (2021) to obtain an optimal ITR estimation while accounting for unmeasured confounding. On the other hand, Frauen and Feuerriegel (2022) utilized these findings for CATE estimation. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a new framework for estimating CATE using IV when there exist unmeasured confounders. This framework can be viewed as an extension of the Direct Learning method under unconfoundedness to the case that allows the existence of unmeasured confounding. We call the proposed method Direct Learning using Instrumental Variables (IV-DL). The proposed framework is easy to implement under many flexible learning methods. Additionally, we introduce several efficient and robust estimators by residualizing the outcome. These estimators have been demonstrated to be robust to multiple model misspecification scenarios. ", "page_idx": 1}, {"type": "text", "text": "The rest of this paper is organized as follows. The notations and some related preliminaries are introduced in Section 2. The proposed framework IV-DL is formally introduced in Section 3. In Section 4 and 5, we proposed efficient and robust estimators. In Section 6, we conduct simulation studies and compare the performance with existing methods in the literature. A real data example is included in Section 7. Section 8 concludes the paper with a discussion on possible future work. Proofs and additional simulations are provided in the Appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Notations and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Denote $A\\in{\\mathcal{A}}=\\{+1,-1\\}$ as the binary treatment, and $X\\in{\\mathcal{X}}\\subseteq\\mathbb{R}^{p}$ the pre-treatment covariates. We adapt the potential outcome framework (Rubin, 1974) in causal inference and denote by $Y(a)\\in\\mathbb{R}$ the potential outcome that the subject would have obtained if the received treatment was $a\\in{\\mathcal{A}}$ . The observed outcome is then given by $\\ell=Y(A)=Y(1)\\mathbb{1}[A=1]+Y(-1)\\mathbb{1}[A=-1]$ . Denote by $U$ the unobserved confounder of the effect of $A$ on $Y$ . Suppose we have access to a pre-treatment binary IV denoted by $Z\\in{\\mathcal{Z}}=\\{+1,-1\\}$ . Then the complete data consists of independent and identically distributed copies of $(Y,X,A,U,Z)$ , even though only copies of $(Y,X,A,Z)$ are observed. ", "page_idx": 1}, {"type": "text", "text": "Our goal is to estimate the Conditional Average Treatment Effect (CATE), defined as $\\Delta(x)$ \u225c $\\mathbb{E}[Y(\\bar{1})-Y(-1)|X=x]$ . As mentioned in Section 1, most of the prior works are based on the core assumption of no unmeasured confounding: ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Unconfoundedness). $Y(a)\\perp A|X$ for $a=\\pm1$ ", "page_idx": 2}, {"type": "text", "text": "This assumption essentially implies that the observed covariates $X$ would suffice to account for the confounding of the effect of $A$ on $Y$ , thereby excluding the presence of $U$ . Under the above assumption of unconfoundedness, it can be easily verified that CATE is identified by $\\Delta(x)\\;=\\;$ $\\mathbb{E}[Y|A\\,=\\,1,X\\,=\\,x]\\,-\\,\\mathbb{E}[Y|A\\,=\\,-1,X\\,=\\,x]$ . Q-learning (Qian and Murphy, 2011) models the two conditional mean outcomes separately and estimates the CATE by taking the difference between these estimates. Consequently, its effectiveness depends on correctly specifying the models for the conditional mean outcomes. Denote the propensity score for the treatment as $\\pi_{A}(a,x)=$ $\\operatorname{P}[A\\,=\\,a|X\\,=\\,x]$ for $a\\,=\\,\\pm1$ . Direct Learning (Qi and Liu, 2018; Tian et al., 2014) propose to directly model for the heterogeneous treatment effect, based on the observation that $\\Delta(x)\\,=$ $\\mathbb{E}[A Y/\\pi_{A}(A,X)|X=x]$ . In other words, one can obtain an estimate of CATE by regressing the modified outcome $A Y/\\pi_{A}(A,X)$ on $X$ . Robust Direct Learning (RD) Meng and Qiao (2022) further extends this framework by residualizing the outcome using an estimate of the main effect, which is the average of the two conditional mean outcomes. This method demonstrates double robustness in the sense that it yields consistent estimation of CATE if either the propensity score or the main effect is correctly specified. Despite the success in RCT or observational studies, all the methods mentioned above rely on the unconfoundedness Assumption 1. In the next section, we will introduce a general framework that directly models CATE using an IV approach when there exists unmeasured confounding. ", "page_idx": 2}, {"type": "text", "text": "3 Direct Learning with Instrumental Variable Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we look beyond Assumption 1, and consider the existence of an unmeasured confounder $U$ . To establish the identification of CATE in this setting, we approach with the use of a proper IV. We will start with the following assumptions seen in Cui and Tchetgen Tchetgen (2021). ", "page_idx": 2}, {"type": "text", "text": "Assumption 2. This assumption consists of five parts as follows: ", "page_idx": 2}, {"type": "text", "text": "a. $Y(z,a)\\perp(Z,A)|X,U\\,f\\!o r\\,z,a=\\pm1.$   \nb $\\cdot\\ {\\cal Z}\\ \\not\\perp{\\cal A}|{\\cal X}$ .   \nc. $Z\\perp{\\boldsymbol{U}}|X$ .   \nd. $Y(z,a)=Y(z^{\\prime},a)$ for $z,z^{\\prime},a=\\pm1.$ .   \ne. $0<\\pi_{Z}(1,X)<1$ almost surely, where $\\pi_{Z}(z,x)=\\operatorname{P}[Z=z|X=x]\\,f o r\\,z=\\pm1.$ ", "page_idx": 2}, {"type": "text", "text": "Here, $Y(z,a)$ represents the potential outcome that would be observed if a subject were exposed to treatment $a\\in A$ , and the IV takes a value of $z\\in{\\mathcal{Z}}$ . Assumption 2.a rules out the existence of any other confounder, except for $X$ and $U$ , for the joint effect of $Z$ and $A$ on the outcome $Y$ . However, this unconfoundedness is hidden from the data collected, since $U$ is never observed. Assumptions 2.b-2.e provides us with a well-defined IV. Assumption 2.b requires a correlation between the IV and the treatment given observed covariates. In many applications, a strong correlation is often necessary to ensure accurate inference in the estimation process. Assumption 2.c guarantees that the causal effect of $Z$ on $Y$ is not confounded given $X$ ; otherwise $Z$ suffers the same issue as $A$ . Additionally, required by Assumption 2.d, the causal effect of $Z$ on $Y$ can only be mediated by the treatment $A$ . In light of this assumption, we omit the argument $z$ in the potential outcome and denote the common value as $Y(a)$ . Assumption 2.e implies that each subject has a positive chance of having either value of the IV. An example of the relationships between variables that satisfy Assumption 2 is presented in a directed acyclic graph in Figure 1. In order to identify the CATE, we also need the following assumption on the unmeasured confounder. ", "page_idx": 2}, {"type": "text", "text": "Assumption 3. At least one of the following is true: ", "page_idx": 2}, {"type": "equation", "text": "$$\na.~\\mathbb{E}[A|Z=1,X,U]-\\mathbb{E}[A|Z=-1,X,U]=\\mathbb{E}[A|Z=1,X]-\\mathbb{E}[A|Z=-1,X]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\nb.\\ \\mathbb{E}[Y(1)-Y(-1)|X,U]=\\mathbb{E}[Y(1)-Y(-1)|X]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "bwlUQsQumh/tmp/2e057d8f99d5f1b5e0a6012a3f55305fa522b54050e1001b0885194c8eb1c95a.jpg", "img_caption": ["Figure 1: A directed acyclic graph with unmeasured confounding and an IV "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Assumption 3 states that, conditional on the measured covariates, either the additive effect of $Z$ on $A$ is independent of $U$ , or the additive effect of $A$ on $Y$ is independent of $U$ . Now, we finally have identification of the CATE. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Under Assumptions 2\u20133, the CATE can be identified by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(x)=\\cfrac{\\mathbb{E}[Y|Z=1,X=x]-\\mathbb{E}[Y|Z=-1,X=x]}{\\mathbb{P}[A=1|Z=1,X=x]-\\mathbb{P}[A=1|Z=-1,X=x]}}\\\\ &{\\qquad=\\mathbb{E}\\left[\\cfrac{Z Y}{\\delta(x)\\pi_{Z}(Z,x)}\\bigg\\vert X=x\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pi_{Z}(z,x)\\,=\\,\\operatorname{P}[Z\\,=\\,z|X\\,=\\,x]$ and $\\delta(x)\\,=\\,\\mathrm{P}[A\\,=\\,1|Z\\,=\\,1,X\\,=\\,x]\\,-\\,\\mathrm{P}[A\\,=\\,1|Z\\,=$ $-1,X=x]$ for any $x\\in\\mathscr{X}$ . ", "page_idx": 3}, {"type": "text", "text": "The first equality (1) was shown in Wang and Tchetgen Tchetgen (2018), which means that the CATE is identified by the conditional Wald estimand. Equation (2) reveals an interesting observation that we do not need the realized treatment $A$ as long as we have $\\delta(x)$ , which can be viewed as the conditional effect of the IV on the treatment given observed covariates. Hereafter, we denote the conditional means of $Y$ and $A$ by $\\mu_{z}^{Y}(x)=\\mathbb{E}[\\bar{Y}|Z=z,X=x]$ and $\\mu_{z}^{A}(x)=\\mathbb{E}[A|Z=z,X=x]$ respectively, for any $z\\in\\{-1,+1\\}$ and $x\\in\\mathscr{X}$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Conditional Average Treatment Effect Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will introduce the IV-DL framework. Motivated by Equation (2), the next lemma offers a way to estimate $\\Delta(x)$ using inverse propensity score of IV as weight. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. Under Assumptions 2\u20133, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta\\in\\mathop{\\mathrm{argmin}}_{f}\\mathbb{E}\\left[{\\frac{1}{\\pi_{Z}(Z,X)}}\\biggl({\\frac{2Z Y}{\\delta(X)}}-f(X)\\biggr)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Based on Lemma 1, we can adopt many existing regression methods to obtain an estimate on CATE by regressing the modified outcome on the covariates, weighted by the propensity score for $Z$ . Specifically, given the data $\\{y_{i},x_{i},a_{i},z_{i}\\}_{i=1}^{n}$ , an estimator $\\hat{\\pi}_{Z}$ of the propensity score function and an estimator $\\hat{\\delta}$ of the effect of $Z$ on $A$ \u201e the IV-DL estimate for $\\Delta$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{f}(x)=\\operatorname*{argmin}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\hat{\\pi}_{Z}(z_{i},x_{i})}\\left(\\frac{2z_{i}y_{i}}{\\hat{\\delta}(x_{i})}-f(x_{i})\\right)^{2}+\\lambda\\|f\\|_{\\mathcal{F}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{F}$ is a function space with norm $\\Vert\\cdot\\Vert_{\\mathcal{F}}$ , and $\\lambda\\geq0$ is the tuning parameter for the regularization term $\\|f\\|_{\\mathcal{F}}$ . To obtain $\\hat{\\pi}_{Z}$ , we can fti a logistic regression of $Z$ on $X$ or a non-parametric model such as random forest. Since $\\delta$ is the treatment effect of $Z$ on $A$ , it is noteworthy that, under Assumption 3, estimation of $\\delta$ can be viewed as a CATE estimation problem with unconfoundedness. In this case, $A$ may be viewed as a binary \u201coutcome\u201d and $Z$ a binary \u201ctreatment\u201d. Thus, we can adopt many existing CATE estimation methods such as Q-learning, DL, and Causal Forest. ", "page_idx": 3}, {"type": "text", "text": "The proposed framework allows a variety of learning methods to model the treatment effect $\\Delta(x)$ . For example, under the linear model, we may model $\\ {\\boldsymbol{\\mathsf{f}}}\\left({\\boldsymbol{x}}\\right)={\\boldsymbol{\\tilde{x}}}^{T}{\\boldsymbol{\\beta}}$ where the regression coefficients are $\\beta$ and $\\widetilde{x}_{i}\\triangleq(1,x_{i}^{T})^{T}$ . Then IV-DL estimator for $\\beta$ is ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\hat{\\boldsymbol{\\beta}}}=\\operatorname{argmin}_{\\boldsymbol{\\beta}\\in\\mathbb{R}^{p+1}}{\\frac{1}{n}}\\sum_{i=1}^{n}{\\frac{1}{{\\hat{\\pi}}_{Z}(z_{i},x_{i})}}\\left({\\frac{2z_{i}y_{i}}{{\\hat{\\delta}}(x_{i})}}-{\\tilde{x}}_{i}^{T}{\\boldsymbol{\\beta}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the CATE $\\Delta(x)$ is estimated by ${\\hat{f}}(x)={\\tilde{x}}^{T}{\\hat{\\beta}}$ . ", "page_idx": 4}, {"type": "text", "text": "In high dimensional setting where $p$ is large, sparse regularization can be easily applied here because the optimization is essentially a weighted least square problem. For example, we can use Least Absolute Shrinkage and Selection Operator (LASSO) and the estimator of $\\beta$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol\\beta}^{l a s s o}=\\underset{\\beta\\in\\mathbb{R}^{p+1}}{\\operatorname{argmin}}\\,\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\hat{\\pi}_{Z}(z_{i},x_{i})}\\bigg(\\frac{2z_{i}y_{i}}{\\hat{\\delta}(x_{i})}-\\tilde{x}_{i}^{T}\\beta\\bigg)^{2}+\\lambda\\|\\boldsymbol\\beta\\|_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda>0$ is the tuning parameter for the $l_{1}$ penalty. ", "page_idx": 4}, {"type": "text", "text": "In practice, there is no guarantee that the true treatment effect follows a linear model. For a more complex model, we can adopt nonlinear methods such as Kernel Ridge Regression (KRR) and solve ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\beta\\in\\mathbb{R}^{n},\\beta_{0}\\in\\mathbb{R}}{\\mathrm{argmin}}\\,\\,\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\hat{\\pi}_{Z}(z_{i},x_{i})}\\bigg(\\frac{2z_{i}y_{i}}{\\hat{\\delta}(x_{i})}-(\\boldsymbol K_{i}^{T}\\beta+\\beta_{0})\\bigg)^{2}+\\lambda\\beta^{T}{\\bf K}\\beta,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\kappa_{i}$ is the $i$ -th column of the kernel matrix $\\mathbf{K}\\,=\\,(K(x_{i},x_{j}))_{n\\times n}$ and $K(\\cdot,\\cdot)$ is a kernel function. KRR might be computationally expensive when dealing with large datasets. In such cases, other machine learning methods capable of solving a weighted least squares problem can be considered. Examples include local regression, regression trees, random forests, and neural networks. ", "page_idx": 4}, {"type": "text", "text": "3.2 Optimal Individualized Treatment Regime Estimation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In some domains, the optimal Individualized Treatment Regime (ITR) can be of interest. The goal here is to find a mapping $d:\\mathcal{X}\\rightarrow\\mathcal{A}$ from a specific class $\\mathcal{D}$ to maximizes the expected outcome: $d^{*}\\triangleq\\operatorname{argmax}_{d\\in{\\mathcal{D}}}\\mathbb{E}[Y(d(X))]$ , where $Y(d(X))$ is the potential outcome that the subject $X$ obtained after receiving treatment $d(X)$ , and $\\mathbb{E}[Y(d(X))]$ is also known as the Value of the regime $d$ . ", "page_idx": 4}, {"type": "text", "text": "ITR and CATE are closely related. For example, in the binary treatment setting, the CATE $\\Delta$ is the difference between two conditional mean outcomes. Assuming greater values of outcome is preferred, then the sign of $\\Delta$ will determine which treatment is optimal. It can be verified that $\\,\\!\\!\\!\\!\\!d^{*}(x)\\,=\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!d^{*}(x)\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!d^{*}(x)\\,$ $\\mathrm{sign}(\\Delta(x))$ . Therefore, we define the estimated optimal ITR using IV-DL as ${\\hat{d}}(x)=\\operatorname{sign}({\\hat{f}}(x))$ , where ${\\hat{f}}(x)$ may be any CATE estimator introduced in the last subsection. ", "page_idx": 4}, {"type": "text", "text": "4 Efficient Estimators by Residualization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the literature, considerable advancements have been made to enhance the efficiency and robustness of the CATE and optimal ITR estimation. To this end, residualization and augmentation are two common strategies. For example, in the IPW framework for optimal ITR estimation, Zhou et al. (2017) and Zhou and Kosorok (2017) proposed to replace the outcome by its residual $Y-{\\hat{g}}(x)$ in estimation of the optimal regime, where ${\\hat{g}}(x)$ is an estimate of the weighted average of the conditional mean outcomes. For the estimation of CATE, Meng and Qiao (2022) residualized the outcome by an estimate of the average of conditional mean outcomes. Frauen and Feuerriegel (2022) proposed augmenting a preliminary estimate of CATE to enhance the robustness of the estimator. ", "page_idx": 4}, {"type": "text", "text": "In this section, we present the Robust Direct Learning using IV approach (IV-RDL), which involves residualizing the outcome in IV-DL to enhance both efficiency and robustness. We propose two ways of residualization, referred to as IV-RDL1 and IV-RDL2, respectively. They are shown to reduce the variance when estimating CATE. In Section 5, we show that they have robustness properties when confronted with model misspecification for nuisance variables. ", "page_idx": 4}, {"type": "text", "text": "4.1 Residualization using a Function of Covariates ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first consider residualizing the outcome by a function of the observed covariates only. Ideally, we would like to find a function $g:\\mathcal{X}\\to\\mathbb{R}$ that can improve the efficiency of the estimation on CATE, ", "page_idx": 4}, {"type": "text", "text": "while keeping it consistent. As shown in the following lemma, the consistency of the estimator is in fact preserved under a shift of $Y$ by any function of the observed covariates. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. For any measurable $g:\\mathcal{X}\\to\\mathbb{R}$ and any probability distribution for $(Y,X,A,Z)$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\in\\underset{f}{\\operatorname{argmin}}\\,\\mathbb{E}\\left[\\frac{1}{\\pi_{Z}(Z,X)}\\left(\\frac{2(Y-g(X))Z}{\\delta(X)}-f(X)\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Asymptotically, the variance of the estimator is related to the variance of the derivative of $[\\pi_{Z}^{'}(\\dot{Z_{,}}X)]^{-1}\\bar{[2(Y-g(X))Z)}Z\\delta^{-1}(X)-f(X)]^{2}$ , the weighted loss for each individual. Hence, it is natural to choose $g$ that minimize the variance of $[\\pi_{Z}(Z,\\bar{X^{\\prime}})]^{-1}[2(Y-g(X))Z\\delta^{-1}(X)-f(X)]$ . See Appendix $\\mathbf{B}$ for a more detailed discussion using the linear model as an example. The following theorem gives us the minimizer. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Among all measurable $g:\\mathcal{X}\\to\\mathbb{R}_{}$ , the following function minimize the variance of $\\begin{array}{r}{\\frac{1}{\\pi_{Z}(Z,X)}\\left(\\frac{2(Y-g(X))Z}{\\delta(X)}-f(X)\\right)}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\ng^{*}(x)\\triangleq\\frac{1}{2}\\mathbb{E}\\left[\\frac{Y}{\\pi_{Z}(Z,X)}\\bigg\\vert X=x\\right]=\\frac{\\mu_{1}^{Y}(x)+\\mu_{-1}^{Y}(x)}{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "There is an interesting interpretation of the optimal function $g^{*}$ , which equals the average of $\\mu_{1}^{Y}(x)$ and $\\mu_{-1}^{Y}(x)$ . Recall that Eq. (1) states that CATE under unmeasured confounding is identified by the ratio of two contrasts, where the numerator happens to be $\\mu_{1}^{Y}(x)-\\mu_{-1}^{Y}(x)$ . The residualization strategy amounts to shifting the outcome $Y$ , and hence $\\mu_{1}^{Y}(x)$ and $\\mu_{-1}^{Y}(x)$ as well. Naturally, shifting both by their average will not affect their difference, but it will reduce the variance. A similar residualization was incorporated in RD under unconfoundedness (Meng and Qiao, 2022), where the goal was to learn the contrast between conditional mean outcomes given the two treatments. ", "page_idx": 5}, {"type": "text", "text": "In practice, $g^{*}$ needs to be estimated before we can estimate the CATE. There are several approaches to obtain the estimate of $g^{*}$ , denoted by $\\hat{g}^{*}$ . For example, we can take the average of estimated conditional mean outcomes, i.e., $\\hat{g}^{*}(x)=\\bar{(\\hat{\\mu}_{1}^{Y}(x)\\!+\\!\\hat{\\mu}_{-1}^{Y}(\\bar{x}))}/2$ . One can also regress $\\bar{Y}/(2\\pi_{Z}(Z,X))$ on $X$ , inspired by Eq. (3). Given ${\\hat{g}}^{*}(x)$ , the IV-RDL1 estimator for $\\Delta$ is obtained by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{f}_{g}(x_{i})=\\operatorname*{argmin}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\hat{\\pi}_{Z}(z_{i},x_{i})}\\left(\\frac{2(y_{i}-\\hat{g}^{*}(x_{i}))z_{i}}{\\hat{\\delta}(x_{i})}-f(x_{i})\\right)^{2}+\\lambda\\|f\\|_{\\mathcal{F}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Section 5, we will show that this estimator is robust against misspecification of either $g^{\\ast}$ or $\\pi_{Z}$ , given that $\\delta$ is correctly specified. ", "page_idx": 5}, {"type": "text", "text": "4.2 Residualization using Covariates, Treatment, and IV ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this paper, we also consider an alternative way of residualizing the outcome by a function $h:(\\mathcal{X},\\mathcal{A},\\mathcal{Z})\\rightarrow\\mathbb{R}$ . Like IV-RDL1, the optimal choice is the function that minimizes the variance while maintaining the consistency of CATE estimation. Among all functions that still convey consistent CATE estimation, the following three equivalent functions minimize the variance of $[\\pi_{Z}(Z,X)]^{-1}[2(Y-h(X,A,Z))Z\\delta^{-1}(\\bar{X_{}})-f(X)]$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{h_{1}^{*}(x,a,z)=\\mu_{1}^{Y}(x)+\\Delta(x)\\big(a-\\mu_{1}^{A}(x)-z\\delta(x)\\big)/2}}\\\\ {{h_{2}^{*}(x,a,z)=\\mu_{-1}^{Y}(x)+\\Delta(x)\\big(a-\\mu_{-1}^{A}(x)-z\\delta(x)\\big)/2}}\\\\ {{h_{3}^{*}(x,a,z)=m^{Y}(x)+\\Delta(x)\\big(a-m^{A}(x)-z\\delta(x)\\big)/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $m^{Y}(x)\\triangleq(\\mu_{1}^{Y}(x)+\\mu_{-1}^{Y}(x))/2$ and $m^{A}(x)\\triangleq(\\mu_{1}^{A}(x)+\\mu_{-1}^{A}(x))/2$ . The technical details are provided in the Appendix C. In practice, all these conditional means $(\\mu_{-1}^{Y},\\mu_{1}^{Y},\\mu_{-1}^{A}$ and $\\mu_{1}^{A}$ ) need to be estimated, together with estimations of $\\pi_{Z}$ and $\\delta$ . Additionally, we need to obtain a preliminary estimate of CATE. The IV-RDL2 estimator is constructed by, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{f}_{h}(x_{i})=\\underset{f\\in\\mathcal{F}}{\\mathrm{argmin}}\\,\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{\\hat{\\pi}_{Z}(z_{i},x_{i})}\\left(\\frac{2(y_{i}-\\hat{h}^{*}(x_{i},a_{i},z_{i}))z_{i}}{\\hat{\\delta}(x_{i})}-f(x_{i})\\right)^{2}+\\lambda\\|f\\|_{\\mathcal{F}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{h}^{*}$ is an estimator for one of $h_{1}^{*},h_{2}^{*}$ and $h_{3}^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "5 Robustness Properties ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we investigate the robustness properties of IV-RDL1 and IV-RDL2. We start with the following theorem to demonstrate the double robustness property of the IV-RDL1 that residualizes the outcome by using $g(x)$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Suppose Assumption 2\u20133 holds, and we have a consistent estimator of $\\delta$ , denoted by $\\hat{\\delta}$ . Let $\\tilde{\\pi}_{Z}$ be a working model for $\\pi_{Z}$ , and $\\tilde{g}$ be a working model for $g^{\\ast}$ . Then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta\\in\\operatorname*{argmin}_{f\\in\\{\\mathcal{X}\\rightarrow\\mathbb{R}\\}}\\mathbb{E}\\left[\\frac{1}{\\tilde{\\pi}_{Z}(Z,X)}\\left(\\frac{(Y-\\tilde{g}(X))Z}{\\hat{\\delta}(X)}-f(X)\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "if either $\\tilde{\\pi}_{Z}(z,x)=\\pi_{Z}(z,x)$ or $\\tilde{g}(x)=g_{1}^{*}(x)$ almost surely. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 indicates that we will have a doubly robust estimator for $\\Delta$ if either $\\pi_{Z}$ or $g^{\\ast}$ is correctly specified when $\\delta$ is known or correctly specified. However, the requirement of a consistent estimate of $\\delta$ would not pose a significant issue in practical application, since it is essentially a CATE estimation problem under no unmeasured confounding. A consistent estimator for $\\delta$ can be found by implementing any state-of-the-art CATE estimation method in the literature. ", "page_idx": 6}, {"type": "text", "text": "For the IV-RDL2, there are more nuisance variables that need to be estimated. The next theorem shows that IV-RDL2 is robust to various scenarios of misspecified nuisance variables. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Suppose Assumption 2\u20133 holds. Let $\\tilde{\\pi}_{Z}$ , $\\tilde{\\delta}$ , $\\tilde{\\mu}_{1}^{Y}$ , $\\tilde{\\mu}_{-1}^{Y}$ , $\\tilde{\\mu}_{Z}^{A}$ , $\\tilde{\\mu}_{-1}^{A}$ , $\\tilde{m}^{Y}$ , $\\tilde{m}^{A}$ and $\\tilde{\\Delta}$ be working models for $\\pi_{Z}$ , $\\delta_{:}$ , $\\mu_{1}^{Y}$ , $\\mu_{-1}^{Y}$ , $\\mu_{Z}^{A}$ , $\\mu_{-1}^{A}$ , $m^{Y}$ , $m^{A}$ and $\\Delta$ , respectively. Denote $\\tilde{h}_{1},\\,\\tilde{h}_{2}$ and $\\tilde{h}_{3}$ as chosen augmentation formulated according to $h_{1}^{*}$ , $h_{2}^{*}$ and $h_{3}^{*}$ using working estimates. Then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta\\in\\underset{f\\in\\{\\mathcal{X}\\}}{\\mathrm{argmin}}\\mathbb{E}\\left[\\frac{1}{\\tilde{\\pi}_{Z}(Z,X)}\\left(\\frac{(Y-\\tilde{h}(X,A,Z))Z}{\\hat{\\delta}(X)}-f(X)\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "if any one of the following condition is satisfied: $(l)\\,\\tilde{\\pi}_{Z}=\\pi_{Z}$ and $\\tilde{\\Delta}=\\Delta$ almost surely, and $\\tilde{h}$ can be any one of $\\tilde{h}_{1},\\,\\tilde{h}_{2}$ and $\\tilde{h}_{3}$ . (2) $\\tilde{\\pi}_{Z}=\\pi_{Z}$ and $\\tilde{\\delta}=\\delta$ almost surely, and $\\tilde{h}$ can be any one of $\\tilde{h}_{1},\\,\\tilde{h}_{2}$ and $\\tilde{h}_{3}$ . (3) $\\tilde{\\mu}_{1}^{Y}=\\mu_{1}^{Y}$ , $\\tilde{\\mu}_{1}^{a}=\\mu_{1}^{A}$ and $\\tilde{\\Delta}=\\Delta$ almost surely, and $\\tilde{h}=\\tilde{h}_{1}$ . (4) $\\tilde{\\mu}_{-1}^{Y}=\\mu_{-1}^{Y}$ , $\\tilde{\\mu}_{-1}^{a}=\\mu_{-1}^{A}$ and $\\tilde{\\Delta}=\\Delta$ almost surely, and $\\tilde{h}=\\tilde{h}_{2}$ . (5) $\\tilde{m}^{Y}=m^{Y}$ , $m^{A}=m^{A}$ and $\\tilde{\\Delta}=\\Delta$ almost surely, and $\\tilde{h}=\\tilde{h}_{3}$ . (6) $\\tilde{m}^{Y}=m^{Y}$ , $m^{A}=m^{A}$ and $\\tilde{\\delta}=\\delta$ almost surely, and $\\tilde{h}=\\tilde{h}_{3}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 summarizes in total six cases of the minimal combination of correctly specified nuisance variables in order to have a consistent estimate of CATE. The three choices of residualization functions possess robustness against different scenarios. In the first two scenarios, obtaining a consistent estimate of CATE is guaranteed as long as we correctly specify $\\pi_{Z}$ and either $\\Delta$ or $\\delta$ . This consistency holds irrespective of the choice of the three $\\tilde{h}$ functions. In practice, the second scenario may be particularly accessible, especially when $\\pi_{Z}$ is known. The other scenarios are less likely to be verified in practice and therefore requires more domain knowledge of the data structure. Specifically, scenarios (3)-(5) requires the corresponding set of conditional means to be correctly specified as well as the preliminary $\\Delta$ . Lastly, in scenario (6), when $\\delta$ and the averages of conditional means are correctly specified, the IV-RDL2 will also provide a consistent estimate of CATE. ", "page_idx": 6}, {"type": "text", "text": "While working on this paper, we encountered unpublished work by Frauen and Feuerriegel (2022) that is similar to our IV-RDL2 estimator. Inspired by Wang and Tchetgen Tchetgen (2018), Frauen and Feuerriegel introduced the MRIV framework, which is a two-step process. First, a preliminary estimator of CATE and nuisance estimators of $\\delta$ , $\\pi_{Z}$ , $\\mu_{-1}^{Y}$ and $\\mu_{-1}^{A}$ are obtained. Then, a pseudooutcome is created by augmenting the preliminary CATE with the nuisance estimates, and the final CATE estimator is obtained by regressing the pseudo-outcome on the covariates. As shown in Wang and Tchetgen Tchetgen (2018), this estimator is robust against model misspecification of the nuisance variables in three of the six scenarios in Theorem 3 (scenarios (1), (2), and (4)). Our numerical studies have shown that our proposed IV-DL framework performs better than the MRIV method. ", "page_idx": 6}, {"type": "text", "text": "6 Simulation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present the results of the simulation study conducted to assess the performance of the proposed IV-DL framework. We compared the proposed method with Bayesian additive regression trees (BART; Chipman et al., 2010), robust direct learning (RD; Meng and Qiao, 2022), causal forest with IV approach (CF; Athey et al., 2019), MRIV method (Frauen and Feuerriegel, 2022), and weighted learning with IV approach (IPW-MR; Cui and Tchetgen Tchetgen, 2021). ", "page_idx": 7}, {"type": "text", "text": "6.1 Simulation Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We begin by introducing the data-generating mechanism. The covariates, denoted as $X\\ =$ $\\left(X_{1},X_{2},X_{3},X_{4},X_{5}\\right)$ , were generated from uniform distribution with $X_{i}~\\sim~U n i f(-1,1)$ for $i\\,=\\,1,\\ldots,5$ . We followed Cui and Tchetgen Tchetgen (2021) and generated the treatment $A$ under logistic model with probability for success: $\\mathbb{P}(A=1|X,Z,U)=\\mathrm{expit}\\{2X_{1}+2.5Z-0.5U\\}$ , where the instrumental variable $Z$ was a Bernoulli random variable with probability $1/2$ and $U$ was the unobserved confounder that followed Bridge distribution with parameter $\\phi=1/2$ . By the results from Wang and Louis (2003), the above usage of Bridge distribution will guarantee that the marginal distribution $f(A|X,Z)$ can be modeled directly by logistic regression. In other words, there exists some vector $_{\\alpha}$ such that logit $\\{\\mathbb{P}(A=1|X,Z)\\}=\\pmb{\\alpha}^{\\vee}(1,X,\\breve{Z})$ . ", "page_idx": 7}, {"type": "text", "text": "The outcome $Y$ was generated in two different settings corresponding to linear and non-linear models of the true CATE: ", "page_idx": 7}, {"type": "equation", "text": "$.\\,\\,\\,Y=h(X)+q(X)A+0.5U+\\epsilon$ ", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n2.\\ Y=h(X)+\\{\\exp(q(X))-1\\}A+U+\\epsilon\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the error term $\\epsilon$ follows $N(0,1)$ . Functions $h(X)$ and $q(X)$ are defined as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{h(X)=0.5+0.5X_{1}+0.8X_{2}+0.3X_{3}-0.5X_{4}+0.7X_{5}}}\\\\ {{q(X)=0.2-0.6X_{1}-0.8X_{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Setting 1, the true CATE is $2q(x)$ , which is linear in $x$ . In Setting 2, the true CATE is $2(e x p(q(x))-$ 1), which is nonlinear. The sample size for each setting was 500 and the simulation was repeated 100 times. An independent sample of size 5000 was used to evaluate the performance of different methods. The proposed methods were implemented according to Sections 3 and 4 with ${\\hat{\\delta}}(X)$ estimated by causal forest (\u201cgrf\u201d package) and the other nuisance variables estimated by random forest. For methods that require to estimate the same nuisance variable, they shared the same copies of nuisance estimates. ", "page_idx": 7}, {"type": "text", "text": "6.2 Numerical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compared all methods based on three performance metrics in the testing sample: the correct classification rate by the estimated ITR (AR); the value function evaluated at the estimated ITR (Value); the mean squared error of the estimated CATE (MSE). Table 1 reports the mean and standard error of these three evaluation metrics over 100 replications for different methods in the two settings. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Simulation results: mean $\\times10^{-2}(\\mathrm{SE}{\\times}10^{-2})$ . IPW-MR: the multiply robust weighted learning; BART: Bayesian additive regression trees; RD: robust direct learning; CF: causal forest. The empirical maximum value is 0.998 for setting 1 and 1.01 for setting 2. ", "page_idx": 7}, {"type": "table", "img_path": "bwlUQsQumh/tmp/e06c8c78aa8d4b8da06f211fab663d3219335c1f05f866954aa7ea8f87907c6b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Among the methods implemented, BART and RD rely on the unconfoundedness assumption and therefore fail to identify CATE when there is unobserved confounding. Both IPW-MR and CF make use of the IV to take unmeasured confounding into account. IPW-MR had fine performances on estimating ITR and maximizing the value. However, it was not designed to estimate CATE. CF performs slightly worse than IPW-MR in terms of AR and value in Setting 1, despite offering a CATE estimation. Its performance is more competitive compared to IPW-MR in Setting 2. Our proposed methods showed superior performances on all the metrics. In particular, IV-RDL1, which residualized the outcome using averages of the estimated conditional means, outperformed all the methods in both settings. IV-RDL2 had a more complicated residualization, and achieved the secondbest performance (but still fairly close to IV-RDL1). Even the unresidualized IV-DL performed better than other methods in most of the metrics. Additional simulation results on testing the robustness of the proposed framework is reported in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "7 Data Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, following Angrist and Evans (1998), we study the causal effect of child-rearing on a mother\u2019s labor-force participation, using a sample of married mothers with two or more children from the U.S. 1980 census data (80PUMS). Assuming the sex of children is random, \u201cfirst two children mixed sex or not\u201d becomes a suitable instrumental variable for the causal effect of having a third child on a mother\u2019s labor force participation. Angrist and Evans showed that having a third child reduces women\u2019s labor force participation on average. Our goal is to investigate heterogeneity among families, offering personalized insights on the decision to have a third child and its impact on employment opportunities. We used a dataset of 478,005 subjects with at least two children. The outcome, $Y$ , represents whether the mother was employed in the year preceding the census. The treatment, $A$ , indicates whether the mother had three or more children at the census time, and the instrumental variable, $Z$ , indicates whether the first two children were of the same sex. We considered five covariates, $X$ : mother\u2019s age at first birth, age at census time, years of education, race, and the father\u2019s income. ", "page_idx": 8}, {"type": "image", "img_path": "bwlUQsQumh/tmp/419c0a72828a11b694e89b9fcd6ad23163c56ec7619ec7b55ccbf3cafc731838.jpg", "img_caption": ["Figure 2: Tree splitting of estimated CATE on covariates. The five leaf nodes shall be numbered 1\u20135. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "bwlUQsQumh/tmp/e7641836bc6c3d7601fc2f9bf580b805731aed5b26e97163371bc05e40f5b8a5.jpg", "img_caption": ["Figure 3: Histograms of estimated CATE in three majority subgroups "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We used the random forest algorithm for both the implementation of the proposed method and the estimation of the nuisance variables $\\mu_{z}^{Y}$ , $\\mu_{z}^{A}$ , $\\delta$ , and $\\pi_{Z}$ . The preliminary CATE estimator was formulated according to Eq. 1 with plug-in estimates on the conditional means. To identify subgroups with distinct treatment effects, we used the estimated CATE as the response to construct a regression tree, shown in Figure 2. The splits occurred at the mother\u2019s age at census (33), age at first birth (23), and father\u2019s income $(\\mathbb{S}2.1\\mathrm{k}/$ year and $\\mathbb{S}26\\mathrm{k}.$ /year). By investigating the five subgroups $32\\%$ , $4\\%$ , $3\\%$ , $15\\%$ , and $47\\%$ of the sample), labeled as groups 1\u20135, we have made the following observations. First, older mothers are more likely to work after having a third child (subgroups 4 and 5 show a larger estimated treatment effect). Second, younger mothers with very low-income fathers (subgroup 3) tend to stay in the labor force after the third child. Lastly, younger mothers are more likely to stop working if their husband\u2019s income is between $\\mathbb{S}2.1\\mathbf{k}$ and $\\mathbb{S}26\\mathrm{k}/\\mathrm{}$ year (subgroups 1-3). Figure 3 displays the histogram of estimated CATE for the three majority groups (1, 4, and 5). The estimated CATE for group 1 is overall smaller than for groups 4 and 5. We also constructed 3-dimensional scatter ", "page_idx": 8}, {"type": "text", "text": "8 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed a new framework to estimate CATE under unmeasured confounding by using an instrumental variable. Under the proposed framework, the estimation procedure boils down to solving a weighted least square problem, which can be tackled with any modern statistical or machine learning method. We also constructed two robust estimators by residualizing the outcome, which are shown to be more efficient and robust to model misspecification on nuisance variables. Numerical studies have shown very competitive performance for our proposed methods. ", "page_idx": 9}, {"type": "text", "text": "A potential extension of our work involves using IV to estimate treatment effects for multi-arm and continuous treatments, with the challenge lying in the generalization of Assumption 3. Another avenue is to incorporate deep neural networks to make use of their rich expressiveness for data distribution. However, the empirical performance and theoretical properties need to be formally studied. One notable limitation is the issue of extreme weights, which can arise during the estimation process and potentially lead to instability and biased results. Addressing this limitation is crucial for improving the reliability and accuracy of our method. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Angrist, J. D. and Evans, W. N. (1998), \u201cChildren and Their Parents\u2019 Labor Supply: Evidence from Exogenous Variation in Family Size,\u201d The American Economic Review, 88, 450\u2013477.   \nAthey, S. and Imbens, G. (2016), \u201cRecursive partitioning for heterogeneous causal effects,\u201d Proceedings of the National Academy of Sciences, 113, 7353\u20137360.   \nAthey, S., Tibshirani, J., and Wager, S. (2019), \u201cGeneralized Random Forests,\u201d The Annals of Statistics, 47, 1148\u20131178.   \nCai, T., Tian, L., Wong, P. H., and Wei, L. (2011), \u201cAnalysis of randomized comparative clinical trial data for personalized treatment selections,\u201d Biostatistics, 12, 270\u2013282.   \nChandra, S., Verma, S., Lim, W. M., Kumar, S., and Donthu, N. (2022), \u201cPersonalization in personalized marketing: Trends and ways forward,\u201d Psychology & Marketing, 39, 1529\u20131562.   \nChipman, H. A., George, E. I., and McCulloch, R. E. (2010), \u201cBART: Bayesian additive regression trees,\u201d The Annals of Applied Statistics, 4, 266 \u2013 298.   \nCui, Y. and Tchetgen Tchetgen, E. (2021), \u201cA semiparametric instrumental variable approach to optimal treatment regimes under endogeneity,\u201d Journal of the American Statistical Association, 116, 162\u2013173.   \nFrauen, D. and Feuerriegel, S. (2022), \u201cEstimating individual treatment effects under unobserved confounding using binary instruments,\u201d arXiv preprint arXiv:2208.08544.   \nGreen, D. P. and Kern, H. L. (2012), \u201cModeling heterogeneous treatment effects in survey experiments with Bayesian additive regression trees,\u201d Public opinion quarterly, 76, 491\u2013511.   \nHill, J. L. (2011), \u201cBayesian nonparametric modeling for causal inference,\u201d Journal of Computational and Graphical Statistics, 20, 217\u2013240.   \nImbens, G. W. and Angrist, J. D. (1994), \u201cIdentification and Estimation of Local Average Treatment Effects,\u201d Econometrica, 62, 467\u2013475.   \nMeng, H. and Qiao, X. (2022), \u201cAugmented direct learning for conditional average treatment effect estimation with double robustness,\u201d Electronic Journal of Statistics, 16, 3523\u20133560.   \nQi, Z. and Liu, Y. (2018), \u201cD-learning to estimate optimal individual treatment rules,\u201d Electronic Journal of Statistics, 12, 3601\u20133638.   \nQian, M. and Murphy, S. A. (2011), \u201cPerformance guarantees for individualized treatment rules,\u201d Annals of statistics, 39, 1180.   \nRubin, D. B. (1974), \u201cEstimating causal effects of treatments in randomized and nonrandomized studies.\u201d Journal of educational Psychology, 66, 688.   \nSyrgkanis, V., Lei, V., Oprescu, M., Hei, M., Battocchi, K., and Lewis, G. (2019), \u201cMachine learning estimation of heterogeneous treatment effects with instruments,\u201d Advances in Neural Information Processing Systems, 32.   \nTian, L., Alizadeh, A. A., Gentles, A. J., and Tibshirani, R. (2014), \u201cA simple method for estimating interactions between a treatment and a large number of covariates,\u201d Journal of the American Statistical Association, 109, 1517\u20131532.   \nWager, S. and Athey, S. (2018), \u201cEstimation and inference of heterogeneous treatment effects using random forests,\u201d Journal of the American Statistical Association, 113, 1228\u20131242.   \nWang, L. and Tchetgen Tchetgen, E. (2018), \u201cBounded, efficient and multiply robust estimation of average treatment effects using instrumental variables,\u201d Journal of the Royal Statistical Society Series B: Statistical Methodology, 80, 531\u2013550.   \nWang, Z. and Louis, T. A. (2003), \u201cMatching conditional and marginal shapes in binary random intercept models using a bridge distribution function,\u201d Biometrika, 90, 765\u2013775.   \nZhang, B., Tsiatis, A. A., Davidian, M., Zhang, M., and Laber, E. (2012), \u201cEstimating optimal treatment regimes from a classification perspective,\u201d Stat, 1, 103\u2013114.   \nZhao, Y., Zeng, D., Rush, A. J., and Kosorok, M. R. (2012), \u201cEstimating individualized treatment rules using outcome weighted learning,\u201d Journal of the American Statistical Association, 107, 1106\u20131118.   \nZhou, X. and Kosorok, M. R. (2017), \u201cAugmented outcome-weighted learning for optimal treatment regimes,\u201d arXiv preprint arXiv:1711.10654.   \nZhou, X., Mayer-Hamblett, N., Khan, U., and Kosorok, M. R. (2017), \u201cResidual weighted learning for estimating individualized treatment rules,\u201d Journal of the American Statistical Association, 112, 169\u2013187. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . For any $z\\in\\{-1,+1\\}$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{E}[2Y|Z=z,X]\n$$", "text_format": "latex", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\;\\mathbb{E}_{U}\\big(\\mathbb{E}[2Y|Z=z,X,U]\\big)}\\\\ &{=\\;\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(1+A)|Z=z,X,U]\\big)+\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(1-A)|Z=z,X,U]\\big)}\\\\ &{=\\;\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(1)(1+A)|Z=z,X,U]\\big)+\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(-1)(1-A)|Z=z,X,U]\\big)}\\\\ &{=\\;\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(1)+Y(-1)|Z=z,X,U]\\big)+\\mathbb{E}_{U}\\big(\\mathbb{E}[A Y(1)-A Y(-1)|Z=z,X,U]\\big)}\\\\ &{=\\;\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(1)+Y(-1)|X,U]\\big)+\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(1)-Y(-1)|X,U]\\mathbb{E}[A|Z=z,X,U]\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Evaluate the above equality at $z=1$ and $z=-1$ , and take the difference. Then we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n2\\mathbb{E}[Y|Z=1,X]-2\\mathbb{E}[Y|Z=-1,X]\n$$", "text_format": "latex", "page_idx": 11}, {"type": "equation", "text": "$$\n=\\;\\mathbb{E}_{U}\\left[\\mathbb{E}[Y(1)-Y(-1)|X,U]\\big(\\mathbb{E}[A|Z=1,X,U]-\\mathbb{E}[A|Z=-1,X,U]\\big)\\right]\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Based on Assumption 3, we have ", "page_idx": 11}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}[A|Z=1,X,U]-\\mathbb{E}[A|Z=-1,X,U]=\\mathbb{E}[A|Z=1,X]-\\mathbb{E}[A|Z=-1,X].}\\end{array}$ Combining the above two, we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y|Z=1,X]-\\mathbb{E}[Y|Z=-1,X]\n$$", "text_format": "latex", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\;\\frac{1}{2}\\mathbb{E}_{U}\\big(\\mathbb{E}[Y(1)-Y(-1)|X,U]\\big)\\big(\\mathbb{E}[A|Z=1,X]-\\mathbb{E}[A|Z=-1,X]\\big)}\\\\ &{=\\;\\mathbb{E}[Y(1)-Y(-1)|X]\\big(\\mathrm{P}[A|Z=1,X]-\\mathrm{P}[A|Z=-1,X]\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The above equality is equivalent to Equation (1). On the other hand, for any $x\\in\\mathscr{X}$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\bigg[\\frac{Z Y}{\\pi_{Z}(Z,X)}\\bigg|X=x\\bigg]}\\\\ {\\,=\\,\\pi_{Z}(1,x)\\mathbb{E}\\bigg[\\frac{Y}{\\pi_{Z}(1,X)}\\bigg|Z=1,X=x\\bigg]+\\pi_{Z}(-1,x)\\mathbb{E}\\bigg[\\frac{-Y}{\\pi_{Z}(-1,X)}\\bigg|Z=-1,X=x\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Dividing both sides of the above equality by $\\delta(x)=\\mathrm{P}[A|Z=1,X=x]-\\mathrm{P}[A|Z=-1,X=x]$ yields Equation (2). \u5382 ", "page_idx": 11}, {"type": "text", "text": "Lemma 3. Let $\\ell(X,f)=\\mathbb{E}[Q(X,f)|X]$ and $L(f)=\\mathbb{E}\\ell(X,f)$ . Denote $f^{*}\\in\\operatorname{argmin}_{f}\\ell(X,f)$ .   \nThen $f^{*}\\in\\operatorname{argmin}_{f}L(f)$ . ", "page_idx": 11}, {"type": "text", "text": "Proof. Denote $f^{+}\\in\\operatorname{argmin}_{f}L(f)$ . Then by definition, we have the following two inequalities: ", "page_idx": 11}, {"type": "equation", "text": "$$\nL(f^{+})\\leq L(f^{*})=\\mathbb{E}\\ell(X,f^{*})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "equation", "text": "$$\nL(f^{*})=\\mathbb{E}\\ell(X,f^{*})\\leq\\mathbb{E}\\ell(X,f^{+})=L(f^{+})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Therefore, $L(f^{*})=L(f^{+})$ and $f^{*}\\in\\operatorname{argmin}_{f}L(f)$ . ", "page_idx": 11}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Let $\\begin{array}{r}{\\ell(X,f)=\\mathbb{E}\\left[\\frac{1}{\\pi z(Z,X)}\\left(\\frac{2Y Z}{\\delta(X)}-f(X)\\right)^{2}\\bigg|X\\right]}\\end{array}$ . By Lemma 3, it suffices to show $\\Delta\\in\\operatorname{argmin}_{f}\\ell(X,f)$ . ", "page_idx": 11}, {"type": "text", "text": "The gradient of $\\ell(X,f)$ with respect to $f$ is given by ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial}{\\partial f}\\ell(X,f)=\\ \\mathbb{E}\\biggl[\\frac{\\partial}{\\partial f}\\frac{1}{\\pi_{Z}(Z,X)}\\biggl(\\frac{2Y Z}{\\delta(X)}-f(X)\\biggr)^{2}\\biggl|X\\biggr]}}\\\\ {=}&{-\\,2\\mathbb{E}\\biggl[\\frac{1}{\\pi_{Z}(Z,X)}\\biggl(\\frac{2Y Z}{\\delta(x)}-f(X)\\biggr)\\biggl|X\\biggr]}\\\\ &{=\\ 2\\mathbb{E}\\biggl[\\frac{f(X)}{\\pi_{Z}(Z,X)}\\biggl|X\\biggr]-2\\mathbb{E}\\biggl[\\frac{2Y Z}{\\delta(X)\\pi_{Z}(Z,X)}\\biggl|X\\biggr]}\\\\ &{=\\ 4f(X)-4\\Delta(X)}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Since $\\ell(X,f)$ is convex, we have $\\Delta\\in\\operatorname{argmin}_{f}\\ell(X,f)$ . ", "page_idx": 11}, {"type": "text", "text": "Proof of Lemma 2. Let $\\begin{array}{r}{\\ell_{g}(f)=\\mathbb{E}\\left[\\frac{1}{\\pi_{Z}(Z,X)}\\left(\\frac{2(Y-g(X))Z}{\\delta(X)}-f(X)\\right)^{2}\\bigg|X\\right],}\\end{array}$ 2(Y \u03b4\u2212(gX()X))Z\u2212f(X) 2    X . By Lemma 3, it suffices to show that for any $g,\\operatorname{argmin}_{f}\\ell_{g}{\\bar{(X,f)}}=\\operatorname{argmin}_{f}\\ell(X,f)$ . For any $g(\\bar{x})$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{g}(X,f)=\\mathbb{E}\\Bigg[\\frac{1}{\\pi_{Z}(Z,X)}\\Bigg(\\frac{2Y Z}{\\delta(X)}-f(X)-\\frac{2g(X)Z}{\\delta(X)}\\Bigg)^{2}\\Bigg|x\\Bigg]}\\\\ &{\\quad\\quad=\\ell(X,f)+2\\mathbb{E}\\Bigg[\\frac{1}{\\pi_{Z}(Z,X)}\\Bigg(\\frac{2Y Z}{\\delta(X)}-f(X)\\Bigg)\\Bigg(\\frac{2g(X)Z}{\\delta(X)}\\Bigg)\\Bigg|x\\Bigg]}\\\\ &{\\quad\\quad\\quad+\\mathbb{E}\\Bigg[\\frac{1}{\\pi_{Z}(Z,X)}\\Bigg(\\frac{2g(X)Z}{\\delta(X)}\\Bigg)^{2}\\Bigg|x\\Bigg]}\\\\ &{\\quad\\quad=\\ell(X,f)+8\\frac{g(X)}{(\\delta(X))^{2}}\\mathbb{E}\\Bigg[\\frac{Y}{\\pi_{Z}(Z,X)}\\Bigg|X\\Bigg]-4\\frac{g(X)f(X)}{\\delta(X)}\\mathbb{E}\\Bigg[\\frac{Z}{\\pi_{Z}(Z,X)}\\Bigg|X\\Bigg]}\\\\ &{\\quad\\quad\\quad\\quad+4\\left[\\frac{g(X)}{\\delta(X)}\\right]^{2}\\mathbb{E}\\Bigg[\\frac{1}{\\pi_{Z}(Z,X)}\\Bigg|X\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here the second term and the fourth term don\u2019t depend on $f$ , and the third term is 0 because $\\begin{array}{r}{\\mathbb{E}\\left[\\frac{Z}{\\pi_{Z}\\left(Z,X\\right)}\\Big|X=x\\right]=0}\\end{array}$ . Therefore, $\\operatorname{argmin}_{f}L_{g}(f)=\\operatorname{argmin}_{f}L(f)$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . For any $g(x)$ , the variance of the derivative of the weighted loss at $f=\\Delta$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Var}\\left(\\cfrac{1}{\\pi_{Z}(Z,X)}\\left(\\cfrac{2\\left(Y-g(X)\\right)Z}{\\delta(X)}-\\Delta(X)\\right)\\right)}\\\\ &{=\\;\\mathbb{E}\\bigg(\\mathbb{E}\\bigg[\\cfrac{1}{(\\pi_{Z}(Z,X))^{2}}\\bigg(\\cfrac{2\\left(Y-g(X)\\right)}{\\delta(X)}-Z\\Delta(X)\\bigg)^{2}\\bigg|X\\bigg]\\bigg)}\\\\ &{\\triangleq\\;\\mathbb{E}[S(X,g)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Set the gradient of $S$ with respect to $g$ equal to 0. Then for any $x\\in\\mathscr{X}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{0={\\mathbb E}\\bigg[-\\frac{4}{\\delta(x)(\\pi_{Z}(Z,x))^{2}}\\bigg(\\frac{2(Y-g(x))}{\\delta(x)}-Z\\Delta(x)\\bigg)\\bigg\\vert X=x\\bigg]}}\\\\ {{2g(x){\\mathbb E}\\bigg[\\frac{1}{(\\pi_{Z}(Z,x))^{2}}\\bigg\\vert X=x\\bigg]=2{\\mathbb E}\\bigg[\\frac{Y}{(\\pi_{Z}(Z,x))^{2}}\\bigg\\vert X=x\\bigg]-{\\mathbb E}\\bigg[\\frac{Z\\delta(x)\\Delta(x)}{(\\pi_{Z}(Z,x))^{2}}\\bigg\\vert X=x\\bigg]}}\\\\ {{2g(x)\\big(\\pi_{Z}^{-1}(1,x)+\\pi_{Z}^{-1}(-1,x)\\big)=\\frac{2\\mu_{X}^{1}(x)}{\\pi_{Z}(1,x)}+\\frac{2\\mu_{X}^{-1}(x)}{\\pi_{Z}(-1,x)}}}\\\\ {{-\\left(\\mu_{Y}^{1}(x)-\\mu_{Y}^{-1}(x)\\right)\\big(\\pi_{Z}^{-1}(1,x)-\\pi_{Z}^{-1}(-1,x)\\big)}}\\\\ {{2g(x)\\big(\\pi_{Z}^{-1}(1,x)+\\pi_{Z}^{-1}(-1,x)\\big)=\\big(\\mu_{Y}^{\\gamma}(x)+\\mu_{X}^{\\gamma}(x)\\big)\\big(\\pi_{Z}^{-1}(1,x)+\\pi_{Z}^{-1}(-1,x)\\big)}}\\\\ {{g(x)=\\frac{1}{2}\\big(\\mu_{Y}^{\\gamma}(x)+\\mu_{X}^{\\gamma}(x)\\big)}}\\\\ {{=\\frac{1}{2}{\\mathbb E}\\bigg[\\frac{Y}{\\pi_{Z}(Z,X)}\\bigg\\vert X=x\\bigg]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Additionally, $S$ is convex since $\\begin{array}{r l r}{\\frac{\\partial^{2}S}{\\partial g^{2}}}&{{}=}&{\\frac{8}{(\\delta(x))^{2}\\pi_{Z}(1,x)\\pi_{Z}(-1,x)}}\\end{array}$ . By Lemma 3, $\\boldsymbol{g}^{*}\\quad\\in$ $\\mathrm{argmin}_{g}\\,\\mathbb{E}[S(X,g)]$ . \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 2. Let $\\begin{array}{r}{\\tilde{\\ell}_{g}(X,f)\\,=\\,\\mathbb{E}\\Bigl[\\frac{1}{\\tilde{\\pi}_{Z}(Z,X)}\\left(\\frac{2(Y-\\tilde{g}(X))Z}{\\hat{\\delta}(X)}-f(X)\\right)^{2}\\Big|X\\Bigr]}\\end{array}$ 2(Y \u03b4\u2212\u02c6(g\u02dcX()X))Z\u2212f(X) 2   X . By Lemma 3, it suffices to show $\\Delta\\in\\mathrm{argmin}_{f}\\,\\tilde{\\ell}_{g}(X,f)$ , if either $\\tilde{\\pi}_{Z}=\\pi_{Z}$ almost surely or ${\\tilde{g}}=g$ almost surely. ", "page_idx": 12}, {"type": "text", "text": "The gradient of $\\tilde{\\ell}_{g}(x,f)$ with respect to $f$ is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ensuremath{\\frac{\\partial\\tilde{\\ell}_{g}}{\\partial f}}(x,f)=2\\ensuremath{\\mathbb{E}\\left[\\frac{1}{\\tilde{\\pi}_{Z}(Z,X)}f(X)\\Bigg\\vert X=x\\right]}-2\\ensuremath{\\mathbb{E}\\left[\\frac{1}{\\tilde{\\pi}_{Z}(Z,X)}\\frac{2(Y-\\tilde{g}(X))Z}{\\hat{\\delta}(X)}\\Bigg\\vert X=x\\right]}}\\\\ &{\\qquad\\qquad=2f(x)\\bigg(\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\bigg)}\\\\ &{\\qquad\\qquad-\\ensuremath{\\frac{4}{\\hat{\\delta}(x)}}\\bigg[\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}\\bigg(\\mu_{1}^{Y}(x)-\\tilde{g}(x)\\bigg)-\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\bigg(\\mu_{-1}^{Y}(x)-\\tilde{g}(x)\\bigg)\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If $\\tilde{\\pi}_{Z}=\\pi_{Z}$ almost surely, then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial\\tilde{\\ell}_{g}(x,f)}{\\partial f}=4f(x)-\\frac{4}{\\hat{\\delta}(x)}\\Big(\\mu_{1}^{Y}(x)-\\mu_{-1}^{Y}(x)\\Big)=4\\Big(f(x)-\\Delta(X)\\Big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If ${\\tilde{g}}=g$ almost surely, then $\\tilde{g}(x)=[\\mu_{1}^{Y}(x)+\\mu_{-1}^{Y}(x)]/2$ . Thus, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\tilde{\\ell}_{g}(x,f)}{\\partial f}=2f(x)\\left(\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\right)}\\\\ &{\\qquad\\qquad\\qquad-\\frac{2}{\\hat{\\delta}(x)}\\bigg(\\frac{\\pi_{Z}(1,X)}{\\tilde{\\pi}_{Z}(1,x)}\\big(\\mu_{1}^{Y}(x)-\\mu_{-1}^{Y}(x)\\big)\\bigg)}\\\\ &{\\qquad\\qquad\\qquad-\\frac{2}{\\hat{\\delta}(x)}\\bigg(\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\big(\\mu_{1}^{Y}(x)-\\mu_{-1}^{Y}(x)\\big)\\bigg)}\\\\ &{\\qquad\\qquad\\qquad=2\\left(f(x)-\\Delta(x)\\right)\\bigg(\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Check that $\\begin{array}{r}{\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\,>\\,0}\\end{array}$ . By convexity of $\\tilde{\\ell}_{g}(x,f),\\,\\Delta\\,\\in\\,\\mathrm{argmin}_{f}\\,\\tilde{\\ell}_{g}(X,f)$ in both cases. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3. Let \u2113\u02dch(X, f) = \u03c0\u02dcZ(1Z,X) $\\begin{array}{r}{\\tilde{\\ell}_{h}(X,f)=\\left[\\frac{1}{\\tilde{\\pi}_{Z}(Z,X)}\\left(\\frac{2(Y-\\tilde{h}(X,A,Z))Z}{\\tilde{\\delta}(X)}-f(X)\\right)^{2}\\Big|X\\right]}\\end{array}$ $\\tilde{\\ell}_{h}$ and Lemma 3, it suffices to show that the gradient of $\\tilde{\\ell}_{h}(x,f)$ with respect to $f$ is 0 at $f=\\Delta$ in all cases. The gradient is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\tilde{\\ell}_{h}(x,f)}{\\partial f}=2\\ensuremath{\\mathbb{E}\\left[\\frac{1}{\\tilde{\\pi}_{Z}(Z,X)}f(X)\\right]}X=x\\Biggr]-2\\ensuremath{\\mathbb{E}\\left[\\frac{2Z}{\\tilde{\\pi}_{Z}(Z,X)}\\frac{Y-\\tilde{h}(X,A,Z)}{\\tilde{\\delta}(X)}\\bigg\\lvert X=x\\right]}}\\\\ &{\\phantom{2p c{2p c{2p c}}}=2f(x)\\left(\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\right)}\\\\ &{\\phantom{2p c{2p c}}-\\frac{4}{\\tilde{\\delta}(x)}\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}\\left(\\mu_{1}^{Y}(x)-{\\mathbb{E}\\left[\\tilde{h}(X,A,Z)\\right\\rvert}Z=1,X=x\\right]\\right)}\\\\ &{\\phantom{2p c{2p c}}+\\frac{4}{\\tilde{\\delta}(x)}\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\left(\\mu_{-1}^{Y}(x)-{\\mathbb{E}\\left[\\tilde{h}(X,A,Z)\\right\\rvert}Z=-1,X=x\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 If $\\tilde{\\pi}_{Z}\\,=\\,\\pi_{Z}$ almost surely, then we have the unweighted difference $\\mathbb{E}[\\tilde{h}(X,A,Z)|Z=$ $1,X=x]-\\mathbb{E}[\\Tilde{h}(X,A,Z)|Z=-1,X=x]=0$ by Equation (4). The resulting gradient is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial\\tilde{\\ell}_{h}(x,f)}{\\partial f}=4f(x)-\\frac{2\\big(\\mu_{1}^{A}(x)-\\mu_{-1}^{A}(x)\\big)}{\\tilde{\\delta}(x)}\\Big(\\Delta(x)-\\tilde{\\Delta}(x)\\Big)-4\\tilde{\\Delta}(x)}}\\\\ {{=4\\bigg[f(x)-\\tilde{\\Delta}(x)-\\frac{\\delta(x)}{\\tilde{\\delta}(x)}\\Big(\\Delta(x)-\\tilde{\\Delta}(x)\\Big)\\bigg]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It will yield $4(f(x)-\\Delta(x))$ if either $\\tilde{\\Delta}=\\Delta$ or $\\tilde{\\delta}=\\delta$ almost surely. ", "page_idx": 13}, {"type": "text", "text": "\u2022 If $\\tilde{\\mu}_{1}^{Y}=\\mu_{1}^{Y},\\tilde{\\mu}_{1}^{A}=\\mu_{1}^{A}$ and $\\tilde{\\Delta}=\\Delta$ almost surely, and the choice of residualization function is $\\tilde{h}_{1}$ , then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\tilde{\\ell}_{h}(x,f)}{\\partial f}}\\\\ &{=2f(x)\\left(\\frac{\\pi_{Z}(1,x)}{\\bar{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\right)-\\frac{\\pi_{Z}(1,x)}{\\bar{\\pi}_{Z}(1,x)}2\\tilde{\\Delta}(x)}\\\\ &{\\quad+\\frac{4}{\\bar{\\delta}(x)}\\frac{\\pi_{Z}(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\left(\\Delta(x)\\delta(x)-[\\partial\\delta(x)-\\tilde{\\delta}(x)]\\tilde{\\Delta}(x)/2\\right)}\\\\ &{=2(f(x)-\\tilde{\\Delta}(x))\\left(\\frac{\\pi_{Z}(1,x)}{\\bar{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\right)+\\frac{\\pi_{Z}(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\frac{\\delta(x)}{\\bar{\\delta}(x)}4(\\Delta(x)-\\tilde{\\Delta}(x))}\\\\ &{=2(f(x)-\\tilde{\\Delta}(x))\\left(\\frac{\\pi_{Z}(1,x)}{\\bar{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 If $\\tilde{\\mu}_{-1}^{Y}=\\mu_{-1}^{Y}$ , $\\tilde{\\mu}_{-1}^{A}\\,=\\,\\mu_{-1}^{A}$ and $\\tilde{\\Delta}=\\Delta$ almost surely, and the choice of residualization function is $\\tilde{h}_{2}$ , then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\hat{\\delta}_{t}(x,f)}{\\partial f}}\\\\ &{=2f(x)\\left(\\frac{\\pi z(1,x)}{\\bar{\\pi}_{Z}(1,x)}+\\frac{\\pi z(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\right)}\\\\ &{\\quad-\\frac{4}{\\hat{\\delta}(x)}\\frac{\\pi z(1,x)}{\\bar{\\pi}_{Z}(1,x)}\\left(\\Delta(x)\\delta(x)-[2\\delta(x)-\\bar{\\delta}(x)]\\bar{\\Delta}(x)/2\\right)}\\\\ &{\\quad+\\frac{\\pi z(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}2\\bar{\\Delta}(x)}\\\\ &{=2(f(x)-\\bar{\\Delta}(x))\\left(\\frac{\\pi z(1,x)}{\\bar{\\pi}_{Z}(1,x)}+\\frac{\\pi z(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\right)-\\frac{\\pi z(1,x)}{\\bar{\\pi}_{Z}(1,x)}\\frac{\\delta(x)}{\\bar{\\delta}(x)}4(\\Delta(x)-\\bar{\\Delta}(x))}\\\\ &{=2(f(x)-\\bar{\\Delta}(x))\\left(\\frac{\\pi z(1,x)}{\\bar{\\pi}_{Z}(1,x)}+\\frac{\\pi z(-1,x)}{\\bar{\\pi}_{Z}(-1,x)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 If $(\\tilde{\\mu}_{1}^{Y}+\\tilde{\\mu}_{-1}^{Y})/2=(\\mu_{1}^{Y}+\\mu_{-1}^{Y})/2$ and $(\\tilde{\\mu}_{1}^{A}+\\tilde{\\mu}_{-1}^{A})/2=(\\mu_{1}^{A}+\\mu_{-1}^{A})/2$ almost surely, and the choice of residualization function is $\\tilde{h}_{3}$ , then the gradient is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\tilde{\\ell}_{h}(x,f)}{\\partial f}=2f(x)\\left(\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\frac{4}{\\tilde{\\delta}(x)}\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}\\bigg(\\frac{\\Delta(x)\\delta(x)}{2}-(\\delta(x)-\\tilde{\\delta}(x))\\tilde{\\Delta}(x)/2\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{4}{\\tilde{\\delta}(x)}\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\bigg(-\\frac{\\Delta(x)\\delta(x)}{2}-(-\\delta(x)+\\tilde{\\delta}(x))\\tilde{\\Delta}(x)/2\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=2\\bigg(f(x)-\\tilde{\\Delta}(x)-\\frac{\\delta(x)}{\\tilde{\\delta}(x)}(\\Delta(x)-\\tilde{\\Delta}(x))\\bigg)\\left(\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It will yield $\\begin{array}{r}{2\\Big(f(x)-\\Delta(x))\\Big)\\left(\\frac{\\pi_{Z}(1,x)}{\\tilde{\\pi}_{Z}(1,x)}+\\frac{\\pi_{Z}(-1,x)}{\\tilde{\\pi}_{Z}(-1,x)}\\right)}\\end{array}$ if either $\\tilde{\\Delta}=\\Delta$ or $\\tilde{\\delta}\\,=\\,\\delta$ almost surely. ", "page_idx": 14}, {"type": "text", "text": "B Optimal residualization (linear model example) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider linear model for $\\Delta(x)$ with coefficients denoted by $\\beta$ . The objective function with outcome residualized by a function $g(x)$ is defined as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{g}(y,z,x,\\beta)=\\frac{1}{\\pi_{Z}(z,x)}\\bigg(\\frac{2(y-g(x))z}{\\delta(x)}-x^{T}\\beta\\bigg)^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\beta^{*}$ be the unique minimizer of $Q(\\beta)\\,\\triangleq\\,\\mathbb{E}[L_{g}(Y,Z,X,\\beta)]$ . Let $\\ell_{g}(y,z,x,\\beta)$ be the derivative of $L_{g}(y,z,x,\\beta)$ with respect to $\\beta$ . Denote by $\\hat{\\beta}$ the root of the estimating equation $\\begin{array}{r}{n^{-1}\\sum_{i=1}^{n}\\ell_{g}(Y_{i},X_{i},\\beta)=0}\\end{array}$ . By Bahadur representation, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\beta}-\\beta^{*}=n^{-1}H^{-1}\\sum_{i=1}^{n}\\ell_{g}(Y_{i},X_{i},\\beta^{*})+o_{P}(n^{-1})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $H$ is the second derivative of $Q(\\beta)$ with respect to $\\beta$ at $\\beta\\,=\\,\\beta^{*}$ . Therefore, selecting the optimal $g$ is equivalent to minimizing the variance of $\\ell_{g}(Y_{i},X_{i},\\beta^{*})$ . ", "page_idx": 15}, {"type": "text", "text": "C Technical details for IV-RDL2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Unlike IV-RDL1, we need additional constraints on $h(x,a,z)$ to make sure the estimation for CATE remains consistent after the residualization. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4. For any measurable $h:(\\mathcal{X},\\mathcal{A},\\mathcal{Z})\\rightarrow\\mathbb{R}$ satisfying ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{Z h(X,A,Z)}{\\pi_{Z}(Z,X)}\\bigg|X=x\\right]=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "or equivalently, $\\begin{array}{r}{\\mathbb{E}[h(X,A,Z)|Z=1,X=x]=\\mathbb{E}[h(X,A,Z)|Z=-1,X=x]}\\end{array}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta\\in\\ \\underset{f}{\\operatorname{argmin}}\\,\\mathbb{E}\\left[\\frac{1}{\\pi_{Z}(Z,X)}\\left(\\frac{2(Y-h(X,A,Z))Z}{\\delta(X)}-f(X)\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 4. Let $\\begin{array}{r}{\\ell_{h}(X,f)=\\mathbb{E}\\left[\\frac{1}{\\pi_{Z}(Z,X)}\\left(\\frac{2(Y-h(X,A,Z))Z}{\\delta(X)}-f(X)\\right)^{2}\\bigg|X\\right]\\!.}\\end{array}$ . By Lemma 3, it suffices to show argmi $\\iota_{f}\\,\\ell_{h}(X,f)=\\mathsf{\\bar{a r g m i n}}_{f}\\,\\ell(X,f)$ . For any $h(x,a,z)$ satisfying Equation (4), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{h}(X,f)=\\mathbb{E}\\left[\\frac{1}{\\pi z\\left(Z,X\\right)}\\left(\\frac{2Y Z}{\\delta(X)}-f(X)-\\frac{2h(X,A,Z)Z}{\\delta(X)}\\right)^{2}\\bigg|X\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\ell(X,f)+2\\mathbb{E}\\left[\\frac{1}{\\pi z\\left(Z,X\\right)}\\left(\\frac{2Y Z}{\\delta(X)}-f(X)\\right)\\left(\\frac{2h(X,A,Z)Z}{\\delta(X)}\\right)\\bigg|X\\right]}\\\\ &{\\phantom{\\quad\\quad}+\\mathbb{E}\\left[\\frac{1}{\\pi z\\left(Z,X\\right)}\\left(\\frac{2h(X,A,Z)Z}{\\delta(X)}\\right)^{2}\\bigg|X\\right]}\\\\ &{\\phantom{\\quad\\quad}=\\ell(X,f)+\\frac{8}{(\\delta(X))^{2}}\\mathbb{E}\\left[\\frac{Y h(X,A,Z)}{\\pi z\\left(Z,X\\right)}\\bigg|X\\right]-\\frac{4f(X)}{\\delta(X)}\\mathbb{E}\\left[\\frac{Z h(X,A,Z)}{\\pi{z}\\left(Z,X\\right)}\\bigg|X\\right]}\\\\ &{\\phantom{\\quad\\quad}+\\frac{4}{(\\delta(X))^{2}}\\mathbb{E}\\left[\\frac{(h(X,A,Z))^{2}}{\\pi z\\left(Z,X\\right)}\\bigg|X\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here the second term and the fourth term don\u2019t depend on $f$ , and the third term is 0. Therefore, $\\mathrm{argmin}_{f}\\,\\ell_{h}(X,f)=\\mathrm{argmin}_{f}\\,\\ell(X,f)$ \u53e3 ", "page_idx": 15}, {"type": "text", "text": "As shown in Lemma 4, the minimizer is invariant of a shift on outcome by a function $h$ that satisfies Eq. (4). Similar to the way of finding $\\hat{g}^{*}$ , we would like to find the function $h$ with the smallest variance of $[\\pi_{Z}(Z,X)]^{-1}[\\bar{2}(Y-h(\\bar{X,A},Z))Z\\delta^{-1}(X)-f(X)]$ among all $h$ that satisfies Eq. (4). ", "page_idx": 15}, {"type": "text", "text": "Theorem 4. Among all measurable $h:(\\mathcal{X},A,Z)\\to\\mathbb{R}$ satisfying Eq. (4), the following function minimizes $\\begin{array}{r}{\\mathrm{Var}\\left[\\frac{1}{\\pi_{Z}(Z,X)}\\left(\\frac{2(Y-h(X,A,Z))Z}{\\delta(X)}-f(X)\\right)\\right]}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nh^{\\ast}(x,a,z)=\\mu^{Y}(x)+\\frac{\\Delta(x)}{2}\\big(a-\\mu^{A}(x)-z\\delta(x)\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "if the conditional means $\\mu^{Y}(x)$ and $\\mu^{A}(x)$ is one of these three pairs: $\\left(\\boldsymbol{l}\\right)\\mu_{1}^{Y}(\\boldsymbol{x})$ and $\\mu_{1}^{A}(x)$ ; (2) $\\mu_{-1}^{Y}(x)$ and $\\mu_{-1}^{A}(x);(3)\\,m^{Y}(x)\\triangleq(\\mu_{1}^{Y}(x)+\\mu_{-1}^{Y}(x))/2$ and $m^{A}(x)\\triangleq(\\mu_{1}^{A}(x)+\\mu_{-1}^{A}(x))/2$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4. For any $h(x,a,z)$ satisfying Equation (4), the variance of the derivative of the weighted loss $L_{h}(f)$ at $f=\\Delta$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Var}\\left(\\frac{1}{\\pi_{Z}(Z,X)}\\bigg(\\frac{2\\big(Y-h(X,A,Z)\\big)Z}{\\delta(X)}-\\Delta(X)\\bigg)\\right)}\\\\ &{=\\,\\mathbb{E}\\bigg(\\mathbb{E}\\bigg[\\frac{1}{(\\pi_{Z}(Z,X))^{2}}\\bigg(\\frac{2\\big(Y-h(X,A,Z)\\big)}{\\delta(X)}-Z\\Delta(X)\\bigg)^{2}\\bigg|Z,X\\bigg]\\bigg)}\\\\ &{\\triangleq\\,\\mathbb{E}[S(X,Z,h)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\nS(x,z,h)=\\mathbb{E}\\bigg[\\frac{1}{(\\pi_{Z}(Z,X))^{2}}\\bigg(\\frac{2\\big(Y-h(X,A,Z)\\big)}{\\delta(X)}-Z\\Delta(X)\\bigg)^{2}\\bigg|Z=z,X=x\\bigg]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we seek to minimize $\\mathbb{E}[S(X,Z,h)]$ . By convexity of $S(X,Z,h)$ and Lemma 3, it suffices to show that the gradient of $S(X,Z,h)$ with respect to $h$ is 0 at $f=\\Delta$ , if $h$ is one of the three equivalent forms. To this end, set the gradient of $S$ with respect to $h$ to be 0. Then for any $(x,z)\\in(\\bar{x_{,}}\\bar{z})$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[-\\frac{4}{\\delta(X)}\\frac{1}{(\\pi_{Z}(Z,X))^{2}}\\left(\\frac{2(Y-h(X,A,Z))}{\\delta(X)}-Z\\Delta(X)\\right)\\bigg|Z=z,X=x\\right]=0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which leads to the following condition on the optimal $h$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[h(X,A,Z)|Z=z,X=x]=\\mathbb{E}[Y|Z=z,X=x]-z\\Delta(x)\\delta(x)/2\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\delta(x)\\Delta(x)=\\mu_{1}^{Y}(x)-\\mu_{-1}^{Y}(x)$ , it can be verified that Equation (5) implies $\\mathbb{E}[h(X,A,Z)|Z=$ $1,X=x]=\\mathbb{E}[h(X,A,Z)|Z=-1,X=x],$ , which is equivalent to Equation (4). We will then verify that the following three equivalent functions satisfy Equation (5). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{h_{1}^{*}(x,a,z)=\\mu_{1}^{Y}(x)+\\displaystyle\\frac{\\Delta(x)}{2}\\big(a-\\mu_{1}^{A}(x)-z\\delta(x)\\big)}}\\\\ {{h_{2}^{*}(x,a,z)=\\mu_{-1}^{Y}(x)+\\displaystyle\\frac{\\Delta(x)}{2}\\big(a-\\mu_{-1}^{A}(x)-z\\delta(x)\\big)}}\\\\ {{h_{3}^{*}(x,a,z)=\\displaystyle\\frac{\\mu_{1}^{Y}(x)+\\mu_{-1}^{Y}(x)}{2}+\\displaystyle\\frac{\\Delta(x)}{2}\\bigg(a-\\displaystyle\\frac{\\mu_{1}^{A}(x)+\\mu_{-1}^{A}(x)}{2}-z\\delta(x)\\bigg)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To see their equivalence, notice that $\\Delta(x)=2[\\mu_{1}^{Y}(x)-\\mu_{-1}^{Y}(x)]/[\\mu_{1}^{A}(x)-\\mu_{-1}^{A}(x)]$ . Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{-1}^{Y}(x)-\\mu_{-1}^{A}(x)\\Delta(x)/2=\\frac{\\mu_{-1}^{Y}(x)\\mu_{1}^{A}(x)-\\mu_{1}^{Y}(x)\\mu_{-1}^{A}(x)}{\\mu_{1}^{A}(x)-\\mu_{-1}^{A}(x)}=\\mu_{1}^{Y}(x)-\\mu_{1}^{A}(x)\\Delta(x)/2,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $h_{3}^{*}$ is simply the average of $h_{1}^{*}$ and $h_{2}^{*}$ . It suffices to show $h_{1}^{*}$ satisfies (5). We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}[h_{1}^{*}(X,A,Z)\\vert Z=1,X=x]=\\mu_{1}^{Y}(x)-\\displaystyle\\frac{\\Delta(x)}{2}\\delta(x)}\\\\ &{\\mathbb{E}[h_{1}^{*}(X,A,Z)\\vert Z=-1,X=x]=\\mu_{1}^{Y}(x)+\\displaystyle\\frac{\\Delta(x)}{2}\\big(\\mu_{-1}^{A}(x)-\\mu_{1}^{A}(x)+\\delta(x)\\big)}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof. ", "page_idx": 16}, {"type": "text", "text": "D Additional Simulations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we conducted simulations that evaluate the performance of the proposed framework against model mispecification on the nuisance variables. The data is generated by the same model as Setting 1 in Section 6, except that $\\pi_{Z}(1,X)=\\operatorname{expit}\\{2X_{1}\\}$ . Based on the true model, the conditional mean outcome is non-linear on $X$ . However, in Setting 3, we will use its OLS estimate as a case of misspecification. In Setting 4, we deliberately used a wrong propensity $\\hat{\\pi}_{Z}(1,x)=1/2$ . We keep all the other procedures the same as Setting 1. The results are summarized in Table D. We can observe that the residualized version have superior performance, and have significant lower MSE compared to the original version. ", "page_idx": 17}, {"type": "text", "text": "Table 2: Simulation results: mean $\\times10^{-2}(\\mathrm{SE}{\\times}10^{-2})$ . IPW-MR: the multiply robust weighted learning; BART: Bayesian additive regression trees; RD: robust direct learning; CF: causal forest. The empirical maximum value is 0.967 for setting 3 and 0.979 for setting 4. ", "page_idx": 17}, {"type": "table", "img_path": "bwlUQsQumh/tmp/6bb51f01bff6d8a635636d4c384a85cdef9630ffd1b6884df7baa9b45ca3e185.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E 3D plots for the data analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the data analysis, we construct a 3-dimensional plot for the estimated CATE based on the three splitting variables (age of mom at census, age of mom at first birth, and income of father). The plot is presented in two rotations in Figure 4. The points in the plots are color-coded by the estimated CATE with red indicating more likely to work and blue indicating more likely to not work. We can see that, overall, blue points are at the bottom of the plots, with a majority of them below $\\mathbb{S}25\\mathrm{k}$ /year. Subgroup 3 of young mothers with extremely low fathers\u2019 income only accounts for $3\\%$ of the data and hence is hard to see here. ", "page_idx": 17}, {"type": "image", "img_path": "bwlUQsQumh/tmp/92f629b89bf06442e95a2e35c5e885f2a118bedd8a2a0666105110c4dc018d02.jpg", "img_caption": ["Figure 4: 3D scatter plots of three covariates colored by estimated CATE for 3000 randomly selected subjects. Both plots reflect different rotations. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: They provide a summary of the research objectives, methodologies, and findings, which are consistently supported by the detailed content of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We included discussion on the accessibility of the assumptions in practice. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The proof for each theoretical result can be found in Appendix Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The code provided in the supplemental materials can be used to reproduce results in both simulation study and real data analysis. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The code is included in the supplementary materials, with instructions to reproduce the results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All steps for data generation, model fitting, evaluation, etc., are discussed in the paper. Full details provided with the code. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: For each evaluation metric, we provided standard error as a measure of the error bar. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Each experiment takes a few hours on an old Macbook. Detailed information on the computer resources and an estimate of time needed is included in the code instructions. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research conform with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper proposed an estimation framework and technical details within a specific field, without offering practical applications or solutions that could be implemented in broader societal contexts. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our data analysis used public data. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]