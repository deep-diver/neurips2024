[{"figure_path": "oUXiNX5KRm/tables/tables_8_1.jpg", "caption": "Table 1: Required time to simulate a full trajectory rollout. UPT and GINO are orders of magnitude faster than traditional finite volume solvers. The latent rollout is additionally more than 5x faster than an autoregressive rollout via the physics domain. Neural surrogate models are also faster on CPUs as traditional solvers require extremely small timescales to remain stable (\u2206t < 0.05 vs. At = 1).", "description": "This table compares the time required to simulate a full trajectory rollout using different methods: pisoFoam (a traditional finite volume solver), GINO-68M (autoregressive), UPT-68M (autoregressive), and UPT-68M (latent).  It highlights the significant speedup achieved by UPTs, especially when using the latent rollout technique.  The speedup is shown for both CPU and GPU computation, demonstrating the efficiency gains offered by the neural surrogate models compared to the traditional approach.", "section": "4.2 Transient flows"}, {"figure_path": "oUXiNX5KRm/tables/tables_23_1.jpg", "caption": "Table 2: Comparison on a regular grid Navier-Stokes dataset [31]. (a) UPT outperforms competitor methods that train only small models by a large margin. (b) UPT also performs well on larger model sizes, outperforming competitors even if they train much larger models or pre-train (PT) on more data followed by fine-tuning (FT) on the Navier-Stokes dataset.", "description": "This table presents a comparison of the performance of Universal Physics Transformers (UPTs) against other state-of-the-art models on a regular grid Navier-Stokes dataset. The comparison is done for both small and large model sizes, highlighting the scalability and performance advantages of UPTs.", "section": "D.3.1 NavierStokes-2D dataset"}, {"figure_path": "oUXiNX5KRm/tables/tables_24_1.jpg", "caption": "Table 3: Comparison on the regular gridded small-scale ShallowWater-2D climate modeling dataset. UPT outperforms models that are specifically designed for regular grid data.", "description": "This table compares the performance of UPT-S against several other models on the regular gridded small-scale Shallow Water-2D dataset. The table shows that UPT-S achieves a lower relative L2 error than the other models, demonstrating its effectiveness even on datasets for which other models were specifically designed.", "section": "D.3.2 Shallow Water-2D dataset"}, {"figure_path": "oUXiNX5KRm/tables/tables_25_1.jpg", "caption": "Table 4: Normalized test MSE for ShapeNet-Car pressure prediction. Memory is the amount of memory required to train on a single sample. UPTs can model the dynamics with a fraction of latent tokens compared to other models. SDF additionally uses the signed distance function from each gridpoint to the geometry as input features. To include SDF features into UPT, we encode the SDF features with resolution 323, 483 or 643 into 83 tokens using a shallow ConvNeXt V2 [113] and concatenate these tokens to the tokens coming from the UPT encoder. To balance the number of SDF tokens with the number of latent tokens, we increase the number of latent tokens to 1024 in the settings where we use SDF features for UPT. Runtime is measured on an A100 GPU.", "description": "This table presents the results of ShapeNet-Car pressure prediction experiments.  It compares different models (U-Net, FNO, GINO, and UPT) in terms of their Mean Squared Error (MSE), memory usage, and runtime per epoch.  The table also shows results with and without using Signed Distance Function (SDF) features as input to the UPT model.", "section": "4.1 Steady state flows"}, {"figure_path": "oUXiNX5KRm/tables/tables_33_1.jpg", "caption": "Table 5: Extending the right table of Fig. 2 with theoretical asymptotic complexities. Complexity includes number of mesh points M, and maximum degree of the graph D. Grid-based methods project the mesh to G grid points. UPTs instead use a small amount of supernodes S as discretization, where G is typically much larger than S. The UPT training procedure separates responsibilities between components, allowing us to forward propage dynamics purely within the latent space.", "description": "This table extends Figure 2 by adding a column for theoretical complexity analysis of different neural operator models (GNN, CNN, Transformer, Linear Transformer, GNO, FNO, GINO, and UPT).  It also includes columns indicating whether each model uses a regular grid, produces discretization-convergent results, learns the underlying field, and performs latent rollout for efficient scaling. The complexity analysis considers the number of mesh points (M), graph degree (D), number of grid points (G), and number of supernodes (S).", "section": "D.7 Qualitative study of scaling limits details"}]