{"references": [{"fullname_first_author": "Marc G. Bellemare", "paper_title": "Distributional Reinforcement Learning", "publication_date": "2023-05-01", "reason": "This paper is foundational to the distributional reinforcement learning methods used in the current paper, providing the theoretical basis for the distributional approach."}, {"fullname_first_author": "L. Baird", "paper_title": "Reinforcement Learning Through Gradient Descent", "publication_date": "1999-05-01", "reason": "This PhD thesis introduces the advantage function and advantage-based learning which are central to addressing the challenges of high-frequency decision-making in reinforcement learning, providing a key theoretical foundation for this work."}, {"fullname_first_author": "Corentin Tallec", "paper_title": "Making Deep Q-learning methods robust to time discretization", "publication_date": "2019-01-01", "reason": "This paper directly addresses the sensitivity of deep Q-learning to decision frequency, which is a core concern of the current paper, proposing the Deep Advantage Updating algorithm that improves the robustness to high-frequency updates."}, {"fullname_first_author": "Will Dabney", "paper_title": "Distributional Reinforcement Learning with Quantile Regression", "publication_date": "2017-01-01", "reason": "This paper introduces quantile regression methods for distributional reinforcement learning, which are used in the algorithms developed in the current paper for improved performance and robustness in the high-frequency setting."}, {"fullname_first_author": "Yanwei Jia", "paper_title": "q-learning in continuous time", "publication_date": "2023-01-01", "reason": "This paper provides theoretical foundation for continuous-time reinforcement learning which is the basis for the analysis in this paper on the impact of high decision frequency on distributional reinforcement learning agents."}]}