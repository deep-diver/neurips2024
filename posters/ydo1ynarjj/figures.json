[{"figure_path": "yDo1ynArjj/figures/figures_1_1.jpg", "caption": "Figure 1: Diffusion Forcing capabilities. Today, different applications such as language modeling [6], planning [36], or video generation [31, 69] rely on either auto-regressive next-token prediction or full-sequence diffusion, according to their respective unique capabilities. The proposed Diffusion Forcing is a novel sequence generative model that enjoys key strengths of both model types.", "description": "This figure illustrates the capabilities of Diffusion Forcing (DF) in comparison to traditional methods for sequence generation: teacher forcing and full-sequence diffusion.  It highlights DF's advantages in handling guidance, tree search, compositionality, causal uncertainty, and flexible horizons.  Teacher forcing excels in autoregressive generation of variable-length sequences but struggles with long-range guidance. Full-sequence diffusion excels at long-range guidance but lacks flexibility in sequence length and struggles with compositional generation and causal uncertainty. DF is positioned as a model combining strengths of both paradigms.", "section": "1 Introduction"}, {"figure_path": "yDo1ynArjj/figures/figures_2_1.jpg", "caption": "Figure 2: Method Overview. Diffusion Forcing trains causal sequence neural networks (such as an RNN or a masked transformer) to denoise flexible-length sequences where each frame of the sequence can have a different noise level. In contrast, next-token prediction models, common in language modeling, are trained to predict a single next token from a ground-truth sequence (teacher forcing [64]), and full-sequence diffusion, common in video generation, train non-causal architectures to denoise all frames in a sequence at once with the same noise level. Diffusion Forcing thus interleaves the time axis of the sequence and the noise axis of diffusion, unifying strengths of both alternatives and enabling completely new capabilities (see Secs. 3.2, 3.4).", "description": "This figure compares three different sequence modeling approaches: Diffusion Forcing, Teacher Forcing, and Full-Sequence Diffusion.  It illustrates how each method handles noise and the prediction process during both training and sampling phases. Diffusion Forcing stands out by allowing different noise levels for each token, enabling flexible-length sequence generation. Teacher Forcing uses a ground-truth sequence for prediction, while Full-Sequence Diffusion applies the same noise level to all frames. The figure visually highlights the unique strengths of Diffusion Forcing in terms of noise handling and flexible-length sequence modeling.", "section": "3 Method"}, {"figure_path": "yDo1ynArjj/figures/figures_5_1.jpg", "caption": "Figure 1: Diffusion Forcing capabilities. Today, different applications such as language modeling [6], planning [36], or video generation [31, 69] rely on either auto-regressive next-token prediction or full-sequence diffusion, according to their respective unique capabilities. The proposed Diffusion Forcing is a novel sequence generative model that enjoys key strengths of both model types.", "description": "This figure illustrates the capabilities of Diffusion Forcing (DF), a novel sequence generative model. It compares DF to traditional methods like teacher forcing and full-sequence diffusion, highlighting DF's advantages in various applications such as language modeling, planning, and video generation.  Specifically, it shows how DF combines the strengths of both autoregressive next-token prediction models (variable-length generation) and full-sequence diffusion models (guidance during sampling).", "section": "1 Introduction"}, {"figure_path": "yDo1ynArjj/figures/figures_6_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares the video generation capabilities of three different methods: Teacher Forcing, Causal Full-Sequence Diffusion, and Diffusion Forcing.  The figure shows that Diffusion Forcing produces videos that are temporally consistent and do not diverge, even when generating sequences much longer than those seen during training. In contrast, the other two methods produce videos that are either inconsistent or diverge over longer sequences. The red boxes highlight the input frames and the columns show generations of different lengths (496, 500, 996, 1000 frames).", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_7_1.jpg", "caption": "Figure 1: Diffusion Forcing capabilities. Today, different applications such as language modeling [6], planning [36], or video generation [31, 69] rely on either auto-regressive next-token prediction or full-sequence diffusion, according to their respective unique capabilities. The proposed Diffusion Forcing is a novel sequence generative model that enjoys key strengths of both model types.", "description": "This figure illustrates the capabilities of Diffusion Forcing (DF) by comparing it to traditional methods like teacher forcing and full-sequence diffusion.  Teacher forcing, common in autoregressive models, sequentially predicts the next token based on previous tokens, while full-sequence diffusion models the entire sequence jointly. DF combines the strengths of both approaches: variable-length generation and the ability to guide sampling towards desirable trajectories, as indicated by the checkmarks in the diagram.  The figure highlights DF's additional capabilities in guidance, tree search, compositionality, flexible horizons, and causal uncertainty.", "section": "1 Introduction"}, {"figure_path": "yDo1ynArjj/figures/figures_8_1.jpg", "caption": "Figure 4: In our real robot task, a robot arm is asked to swap the slots of two fruits using a third slot. Since the fruits are input in random slots at the beginning, one cannot determine the next steps from a single observation without knowledge of the initial placement of the fruits. As illustrated in (a) and (b), the upper observation is the same but the desired outcome illustrated below can vary\u2014the task thus requires remembering the initial configuration. In addition, as shown in (c), the same model that generates actions also synthesizes realistic video from just a single frame.", "description": "This figure demonstrates a real-world robotic task where a robot arm needs to swap the positions of two fruits using a third slot. The initial positions of the fruits are randomized, making it impossible to determine the next steps without remembering the initial configuration. The figure showcases two different scenarios (A and B) with the same initial observation but different desired outcomes, highlighting the need for memory.  Additionally, it shows that the model not only generates the actions required for the task but also synthesizes realistic video from a single frame.", "section": "4.4 Robotics: Long horizon imitation learning and robust visuomotor control"}, {"figure_path": "yDo1ynArjj/figures/figures_26_1.jpg", "caption": "Figure 1: Diffusion Forcing capabilities. Today, different applications such as language modeling [6], planning [36], or video generation [31, 69] rely on either auto-regressive next-token prediction or full-sequence diffusion, according to their respective unique capabilities. The proposed Diffusion Forcing is a novel sequence generative model that enjoys key strengths of both model types.", "description": "This figure illustrates the capabilities of Diffusion Forcing (DF) by comparing it to traditional teacher forcing and full-sequence diffusion methods.  Teacher forcing, commonly used in next-token prediction models, is limited in its ability to guide sampling to desirable trajectories and struggles with generating long sequences, especially continuous data like video.  Full-sequence diffusion models excel at guiding sampling but lack the flexibility of variable-length generation and compositional generalization. DF combines the strengths of both approaches, offering guidance, variable-length generation, flexible horizons, and compositional generalization, as depicted in the diagram.", "section": "1 Introduction"}, {"figure_path": "yDo1ynArjj/figures/figures_28_1.jpg", "caption": "Figure 1: Diffusion Forcing capabilities. Today, different applications such as language modeling [6], planning [36], or video generation [31, 69] rely on either auto-regressive next-token prediction or full-sequence diffusion, according to their respective unique capabilities. The proposed Diffusion Forcing is a novel sequence generative model that enjoys key strengths of both model types.", "description": "This figure illustrates the capabilities of Diffusion Forcing (DF) by comparing it to existing methods for sequence generation: teacher forcing and full-sequence diffusion.  Teacher forcing, commonly used in next-token prediction, has limitations in guiding sampling and handling continuous data. Full-sequence diffusion, while capable of guidance, is restricted by its non-causal architecture and fixed sequence length.  DF combines the strengths of both approaches, enabling variable-length generation, guidance, and handling of continuous data.  The diagram visually represents the unique characteristics of each method.", "section": "1 Introduction"}, {"figure_path": "yDo1ynArjj/figures/figures_30_1.jpg", "caption": "Figure 7: Given a dataset of trajectories (a), Diffusion Forcing models the joint distribution of all subsequences of arbitrary length. At sampling time, we can sample from the trajectory distribution by sampling Diffusion Forcing with full horizon (b) or recover Markovian dynamics by disregarding previous states (c).", "description": "This figure demonstrates the ability of Diffusion Forcing to model the joint distribution of subsequences of trajectories.  Panel (a) shows a dataset of trajectories, panel (b) shows samples from the full trajectory distribution using Diffusion Forcing with full memory, and panel (c) shows samples that recover Markovian dynamics when previous states are disregarded.", "section": "Additional results in compositional generation"}, {"figure_path": "yDo1ynArjj/figures/figures_31_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares the video generation results of three different methods: Teacher Forcing, Causal Full-Seq Diffusion, and Diffusion Forcing.  It shows that Diffusion Forcing produces temporally consistent videos that do not diverge even when generating sequences much longer than those seen during training. In contrast, the other two methods show instability and divergence, especially in longer sequences.  The figure highlights the superior performance of Diffusion Forcing in generating long, coherent video sequences.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_32_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares video generation results from three different methods: Teacher Forcing, Causal Full-Sequence Diffusion, and Diffusion Forcing.  The results show that Diffusion Forcing produces temporally consistent videos that do not diverge even when generating sequences significantly longer than those seen during training. In contrast, the other methods struggle to generate coherent and temporally consistent videos beyond their training horizon, suggesting that Diffusion Forcing is significantly more robust and capable of generating longer sequences.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_33_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares the video generation results of three different methods: Teacher Forcing, Causal Full-Seq. Diffusion, and Diffusion Forcing.  Two example sequences are shown for each method.  The results demonstrate that Diffusion Forcing produces temporally consistent videos that do not diverge even when generating sequences much longer than those seen during training, unlike the other two methods. The figure encourages readers to visit the project website to view the video results.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_34_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares the video generation results of three different methods: Teacher Forcing, Causal Full-Sequence Diffusion, and Diffusion Forcing.  The results show that Diffusion Forcing produces videos that are temporally consistent and do not diverge, even when generating sequences much longer than those seen during training.  In contrast, the other methods produce inconsistent or diverging videos.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_35_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure shows the results of video generation experiments comparing Diffusion Forcing against teacher forcing and full-sequence diffusion.  The top row shows input sequences.  The subsequent rows show the video generations produced by each method for two different sequences. Diffusion Forcing's generations are temporally consistent even when generating video far beyond the length of the training sequences. Teacher Forcing and Full-Sequence Diffusion methods diverge in these long-horizon prediction tasks.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_36_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares the video generation capabilities of three different methods: Teacher Forcing, Causal Full-Sequence Diffusion, and Diffusion Forcing.  The figure shows that only Diffusion Forcing produces temporally consistent and non-divergent video generations, even when generating sequences much longer than those seen during training. The other methods either diverge or generate inconsistent results.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_37_1.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares the video generation results of three different methods: Teacher Forcing, Causal Full-Sequence Diffusion, and Diffusion Forcing.  The results show that Diffusion Forcing produces videos that are temporally consistent and do not diverge, even when generating sequences that are much longer than those seen during training. In contrast, the other two methods produce videos that are less temporally coherent and tend to diverge as the sequence length increases.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_37_2.jpg", "caption": "Figure 3: Video Generation. Among tested methods, Diffusion Forcing generations are uniquely temporally consistent and do not diverge even when rolling out well past the training horizon. Please see the project website for video results.", "description": "This figure compares the video generation results of three different methods: Teacher Forcing, Causal Full-Seq Diffusion, and Diffusion Forcing.  The figure shows that Diffusion Forcing produces videos that are temporally consistent and do not diverge even when the generated sequence is much longer than the training sequences. In contrast, the other two methods fail to produce consistent and temporally coherent video when generating sequences longer than the training sequences.", "section": "4.1 Video Prediction: Consistent, Stable Sequence Generation and Infinite Rollout"}, {"figure_path": "yDo1ynArjj/figures/figures_38_1.jpg", "caption": "Figure 4: In our real robot task, a robot arm is asked to swap the slots of two fruits using a third slot. Since the fruits are input in random slots at the beginning, one cannot determine the next steps from a single observation without knowledge of the initial placement of the fruits. As illustrated in (a) and (b), the upper observation is the same but the desired outcome illustrated below can vary\u2014the task thus requires remembering the initial configuration. In addition, as shown in (c), the same model that generates actions also synthesizes realistic video from just a single frame.", "description": "This figure shows a real-world robotic manipulation task where a robot arm needs to swap the positions of two fruits (apple and orange) using a third slot.  The initial positions of the fruits are randomized. The figure highlights that the same model used for generating the robot's actions can also synthesize realistic videos from just a single frame. Subfigures (a) and (b) illustrate that identical initial observations can lead to different desired outcomes, depending on the initial placement of the fruits, emphasizing the need for memory in the robot's planning process.", "section": "4.4 Robotics: Long horizon imitation learning and robust visuomotor control"}]