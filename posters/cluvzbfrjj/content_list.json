[{"type": "text", "text": "From Instance Training to Instruction Learning: Task Adapters Generation from Instructions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Huanxuan Liao1,2, Shizhu $\\mathbf{H}\\mathbf{e}^{1,2*}$ \u2217, Yao $\\mathbf{X}\\mathbf{u}^{1,2}$ , Yuanzhe Zhang1,2, Yanchao $\\mathbf{Hao}^{3}$ , Shengping $\\mathbf{Liu^{4}}$ , Kang ${\\bf L i u^{1,2}}$ , Jun Zhao1,2 ", "page_idx": 0}, {"type": "text", "text": "1 The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China ", "page_idx": 0}, {"type": "text", "text": "2 School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China 3 Platform and Content Group, Tencent, Beijing, China 4 Unisound, Beijing, China liaohuanxuan $2023\\,\\@$ ia.ac.cn {shizhu.he, yao.xu, kliu, jzhao}@nlpr.ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT). However, IFT still relies heavily on instance training of extensive task data, which greatly limits the adaptability of LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines. This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization. Within this context, we introduce Task Adapters Generation from Instructions (TAGI), which automatically constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. Specifically, we utilize knowledge distillation to enhance the consistency between TAGI developed through Learning with Instruction and task-specific models developed through Training with Instance, by aligning the labels, output logits, and adapter parameters between them. TAGI is endowed with cross-task generalization capabilities through a two-stage training process that includes hypernetwork pretraining and finetuning. We evaluate TAGI on the Super-Natural Instructions and P3 datasets. The experimental results demonstrate that TAGI can match or even outperform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements. Our code will be available at https://github.com/Xnhyacinth/TAGI. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT), which describes different tasks in the same natural language format [3; 6; 23]. However, IFT still relies heavily on instance training of extensive task data {(Description, [Demostrations], Source, Target)} [37; 39], which faces significant limitations in adapting LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. ", "page_idx": 0}, {"type": "text", "text": "Therefore, for better cross-task generalization, the \"zero-shot\" learning ability of LLMs is crucial for real-world applications: models learned with instructions can achieve non-trivial performance on unseen tasks with just a single instruction that provides a comprehensive description of the task (e.g., \"You will be given sentences in which your task is to recognize the name of a person.\"). Traditionally, achieving this capability involves meta-training the model by associating each input with specific task instructions [21; 37]. For example, GPT-3 [25] has demonstrated strong \"zero-shot\" capabilities through meta-training. However, these methods heavily depend on the foundation model\u2019s abilities and are inefficient for various unseen tasks [22; 44], as they require reprocessing extensive task instructions and some supplementary task data (e.g., examples from few-shot instances) for each input (see the top of Figure 1). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In recent years, researchers have begun to explore meta-learning to enhance the cross-task generalization capabilities of LLMs, aiming to construct flexible, reusable and robust taskspecific models [1; 34]. For example, taskspecific models such as Adapter [11], LoRA [12], and Prefix [17] have been constructed by a hypernetwork [8]. This approach significantly enhances task generalization by processing instructions efficiently, reducing redundant computations [27]. However, these methods heavily depend on a substantial corpus of training instances, which can hinder their capacity to efficiently learn and construct task-specific models based on provided instructions [13]. ", "page_idx": 1}, {"type": "image", "img_path": "CluvZBfrjj/tmp/174898b83e67a6c711a1454eaeb8445ef2a293d6e6600c82b2095108f5248592.jpg", "img_caption": ["Figure 1: Comparison of the typical Training with Instance and the proposed Learning with Instruction: The former involves training the model at the instance level with parameter updates, while the latter generates a task-specific adapter at the task level with parameter generation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In fact, contrary to LLMs, humans acquire skills and complete tasks not only through repeated practice but also by understanding and following instructional guidelines [15]. For example, a tourist with basic knowledge of riding vehicles can easily learn to use new ones abroad for the first time with the help of travel guides. This paper aims to mimic the way humans learn skills by understanding instructions. This shift represents a modest evolution in task model construction, transitioning from traditional instance training models to a contemporary approach focused on instruction learning. By providing task instructions, the novel paradigm offers an automated solution for generating task-specific adapters and seamlessly integrating them into the base model. This approach aims to streamline the development of task-specific models while enhancing their ability to generalize across diverse tasks with instructions. ", "page_idx": 1}, {"type": "text", "text": "Guided by this goal, we introduce Task Adapters Generation from Instructions (TAGI), which converts instructions to task-specific adapters using a hypernetwork. Under the knowledge distillation framework [10; 36], we enable models to the \"Learning with Instruction\" paradigm in a manner analogous to the \"Training with Instance\" paradigm. TAGI will enhance the alignment between the task-specific model $\\theta_{k}$ (acting as the teacher) and the vanilla LLM $\\theta_{0}$ combined with the generated task adapters $\\Delta_{k}$ (acting as the student) (see the bottom of Figure 1). This alignment is achieved not only through instance training but also by incorporating parameter learning for task-specific models based on instructions. Specifically, we align the student under two distinct paradigms, encompassing not just the targets and logits, but also the adapters\u2019 parameters by an L2 regularization within instruction, which represents the enhancement of the understanding of instructions and the ability to generate more efficient task-specific adapters. Moreover, TAGI endows the model with task generalization capabilities through a two-stage training process: hypernetwork pretraining on standard text pretraining data (e.g., C4 [29]), followed by finetuning on meta-training tasks. This allows it to generalize effectively across unseen tasks without sacrificing performance. ", "page_idx": 1}, {"type": "text", "text": "We evaluate TAGI on the Super-Natural Instructions (SNI) [37] and P3 [30] datasets. Experimental results demonstrate its ability to effectively generate adapters for unseen tasks, surpassing metatrained models by $2\\%$ in SNI and $5\\%$ in P3, while significantly reducing computational demands by $60\\%$ , and outperforming other hypernetwork models by $7\\%$ . Notably, our method does not require additional parameter updating or gradient back-propagation, and it avoids the inefficiency of repeatedly encoding instructions during inference. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel model construction paradigm by imitating human learning abilities, Learning with Instruction, for the cross-task generalization of the LLMs. To the best of our knowledge, it is the first time that a task-specific model has been generated based on instruction learning, and its capabilities and parameters are distilled from a teacher model trained on instance learning. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We used a knowledge distillation framework to develop task-specific models within the instruction learning paradigm. By aligning model parameters comprehensively, the TAGI method improves the model\u2019s ability to understand instructions and solve unseen tasks more accurately and efficiently. \u2022 Comprehensive quantitative and qualitative assessments have highlighted the effectiveness of TAGI on two publicly available large-scale instruction datasets, with lower inference costs. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "TAGI draws inspiration from previous research on instruction following, hypernetworks and knowledge distillation. In this section, we will delve into the pioneering work in these areas. ", "page_idx": 2}, {"type": "text", "text": "Instruction Following is often used to evaluate the cross-task generalization of LLMs, and it is dedicated to handling any task described in natural language. Recent findings suggest that additional finetuning of LLMs with instructions substantially improves their zero-shot capabilities [6; 38; 39]. Moreover, large-scale multi-task meta-training has been shown to equip models with the ability to address new tasks in zero- or few-shot scenarios, facilitated by standard task formats and prompts [30; 44] alongside providing concise task instructions and select examples [24; 37]. However, the instructions and examples can significantly escalate the computational burden compared to taskspecific models. Existing works attempt to mitigate this issue involved creating adapters to separately process instructions and examples [13; 42] with reduced performance. To overcome these limitations, we introduce a new paradigm that draws on instruction-based learning, simulating instance training to enhance the perception and processing capabilities of LLMs for handling unseen tasks. ", "page_idx": 2}, {"type": "text", "text": "Hypernetworks [8; 31] are neural networks that generate weights for other neural networks [4], which are designed to use fewer parameters to dynamically build task-specific models [9; 33]. Notable works such as HyperTuning [27], HINT [13], and Hypter [42] have all adopted hypernetworks to convert task instructions and demonstrations into adapters for LLMs. And MEND [5] utilizes hypernetworks to compress demonstrations for distilled vectors. Although they all avoided processing lengthy instructions repeatedly and utilized adapters to make training and testing more cost-effective [19], they still have a performance loss compared to meta-training [7]. The proposed method TAGI incorporates the utilization of hypernetworks, which are instrumental in generating task-specific adapters that are seamlessly integrated into LLMs. Compared to existing models based on hypernetworks, TAGI not only trains at the instance level but also incorporates knowledge distillation to supervise the adapters generated by hypernetworks, thereby achieving both efficiency and effectiveness. ", "page_idx": 2}, {"type": "text", "text": "Knowledge Distillation is a technique where a smaller model (student) learns to mimic the predictions of a larger model (teacher), aiming to retain performance while reducing computational resources [10]. Indeed, the application of knowledge distillation is the essential difference between the proposed method in this paper and other hypernetwork-based methods such as HINT [13] and Hypter [42]. Recently, some works [32] utilize knowledge distillation to finetune small language models such as T5 [29], enabling them to act as LLMs with pre-prompting without any given prompts. Compared with the typical knowledge distillation methods of LLMs, the proposed method TAGI in this paper further utilizes model parameter alignment and aims to mimic another learning paradigm of human skill learning. We not only calculate the Kullback\u2013Leibler (KL) divergence [14] between teacher and student models [10], but also compute the L2 regularization between the generated adapter by instruction learning and task-specific models by instance training. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Cross-task Generalization: Given a set of tasks $\\mathcal{T}=\\{\\mathcal{T}_{1},...,\\mathcal{T}_{|\\mathcal{T}|}\\}$ , where each task $\\mathcal{T}_{i}$ contains a set of (source, target) samples $\\mathcal{D}_{i}=\\{(s_{1},t_{1}),...,(s_{n},t_{n})\\}$ . We categorize these tasks into three distinct non-overlapping groups for validating out-of-distribution generalization: meta-train $(\\tau_{t r a i n})$ , metavalid $(\\mathcal{T}_{v a l i d})$ , and meta-test $(T_{t e s t})$ , assuming all tasks adhere to a text-to-text format. For example, $\\tau_{t r a i n}$ comprises tasks like translation and question answering, the $\\tau_{v a l i d}$ and $\\mathcal{T}_{t e s t}$ encompass tasks such as paraphrasing and natural language inference respectively. Within the $\\tau_{t r a i n}$ , the goal is to utilize the data for training and transfer knowledge to facilitate learning to resolve the test tasks. For all methods discussed, aside from the original unsupervised pretraining of the language model backbone on separate corpora, the model learning primarily takes place through multi-task training on the $\\tau_{t r a i n}$ . ", "page_idx": 2}, {"type": "image", "img_path": "CluvZBfrjj/tmp/48c75cd4010259dcb551bec30d7f05e1693f964e06e280f964060b083de9a5cc.jpg", "img_caption": ["Figure 2: Overview of TAGI. The hypernetwork takes instruction as input and generates adapters subsequently integrated into the vanilla LLM, and constructed the task-specific model as student. After training the task models through instances on multiple basic tasks as a teacher, TAGI constructs task-specific models by aligning the labels, output logits, and adapter parameters between teacher and student models. To improve compliance with task instructions and the efficacy of weight generation, TAGI undergoes a two-stage hypernetwork training process: hypernetwork pretraining and finetuning. a-c are random divisions of the sampled sentences from pretraining datasets. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Task Adapters Generation from Instructions (TAGI) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will introduce the detailed method of TAGI. For each (unseen) task, TAGI consists of two core components: a hypernetwork $\\S\\ 3.2.1$ which receives task instructions and generates parameter-efficient adapters, and a task-specific model which combines the vanilla LLM and the generated adapters from hypernetwork. ", "page_idx": 3}, {"type": "text", "text": "Unlike traditional meta-training methods, we transition from training with instance to learning with instruction, which not only addresses efficiency issues at the instance level but also incorporates parameter alignment for the task-specific model parameters at the instruction level. Specifically, the complete process is shown in Figure 2, we initially train the LoRA modules $\\S\\ 3.2.2$ on various upstream tasks (seen tasks) with task datasets of meta-train $(\\mathcal{T}_{t r a i n})$ . Specifically, for $N$ distinct upstream tasks, we independently train $N$ LoRA modules, with each module denoted as $\\Delta_{i}$ for task $\\bar{T_{i}}\\in\\mathcal{T}$ , presumed to represent the optimal model for its respective task. Subsequently, TAGI is committed to building proprietary models for downstream tasks (unseen tasks). Its training process is bifurcated into two primary phases: hypernetwork pretraining $\\S\\ 3.2.3$ and hypernetwork finetuning $\\S\\ 3.2.4$ which encompasses distillation and alignment. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Hypernetwork for Converting Instructions into LoRA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A pivotal element of our model is the hypernetwork that converts task instructions (descriptions and demonstrations) into a parameter-efficient module. Our hypernetwork comprises two crucial components: the encoder, derived from the vanilla $\\mathrm{LLM}^{2}$ , is designed to minimize encoding biases by converting task instructions into a continuous contextual representation. This representation is then fused with LLM input and concated with encoded input for the decoder. Additionally, the adapter generator, utilizing a basic MLP design [18], is both lightweight and efficient, effectively converting encoded instructions into parameter-efficient modules. ", "page_idx": 3}, {"type": "text", "text": "Encoder: Prior studies simply concatenated encoded instructions with inputs, overlooking the interactions between them. To address this, we integrated a hierarchical cross-attention layer into the encoder of the LLM to refine the input representation with embedded instruction details. Specifically, for an input $x$ and its corresponding task instruction $i_{x}$ , we initially employ the encoder within the hypernetwork to encode the instruction into representations $\\mathbf{I}_{x}\\in\\mathit{\\dot{\\mathbb{R}}}^{s\\times\\mathit{\\dot{d}}}$ . Then, we feed the $x$ into the model and obtain the output representation $\\mathbf{S}_{l}$ from the self-attention sublayer in the $l.$ -th layer. Ultimately, $\\mathbf{S}_{l}$ is processed through the $l.$ -th cross-attention layer, resulting in a text representation that is enriched with instruction information: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{F}_{l}=\\mathrm{CrossAttentionLayer}_{l}(\\mathbf{S}_{l},\\mathbf{I}_{x})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where CrossAttentionLayerl conducts multi-head attention on the query, key, and value matrices, followed by residual connection and layer normalization. The final input to the decoder is the concatenation of the encoded instruction and the encoded fusion input, i.e., $\\left(\\mathbf{I}_{x};\\mathbf{F}_{l}\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "Adapter Generator: Considering the efficiency and effectiveness, we utilize a two-layer multi-layer perceptron (MLP) to generate parameter-efficient modules (e.g., LoRA) for the encoded instruction. To differentiate between the query $\\mathcal{Q}$ and value $\\mathcal{V}$ matrices as well as the layers, we introduce layer ids $\\mathrm{id}\\mathrm{x}_{l}^{\\{\\mathcal{Q},\\mathcal{V}\\}}\\in\\{0,\\dots,2\\times\\dot{\\#}\\mathrm{blocks}\\}$ as positional information. We use a unique network for each layer and share it between $\\mathcal{Q}$ and $\\mathcal{V}$ (i.e., one network is used for a certain layer LoRA generation). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{LoRA}_{l}^{\\{\\mathcal{Q},\\mathcal{V}\\}}=\\mathbf{MLP}_{l}(\\mathbf{I}_{x_{k}};\\mathrm{idx}_{l}^{\\{\\mathcal{Q},\\mathcal{V}\\}}\\mid\\mathrm{idx}_{l}^{\\mathcal{Q}}=2l,\\mathrm{idx}_{l}^{\\mathcal{V}}=2l+1)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{LoRA}_{l}^{\\mathcal{Q}}$ and $\\mathrm{LoRA}_{l}^{\\mathcal{V}}$ are the $l$ -th LoRA of $\\mathcal{Q}$ and $\\mathcal{V}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 LoRA Tuning for Task-specific Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "LoRA [12] efficiently reduces the number of trainable parameters by decomposing the update of the LLM\u2019s attention weight matrix (denoted as $\\mathbf{W}_{0}\\in\\dot{\\mathbb{R}}^{d\\times k})$ into low-rank matrices. Specifically, LoRA updates the weight matrix as $\\mathbf{W}_{0}+\\delta\\mathbf{W}=\\mathbf{W}_{0}+\\mathbf{A}\\mathbf{B}$ , with $\\mathbf{A}\\in\\mathbb{R}^{d\\times r}$ and $\\dot{\\mathbf{B}}\\in\\mathbb{R}^{r\\times\\dot{k}}$ being trainable low-rank matrices of rank $r$ , significantly smaller in dimensions than $d$ and $k$ . We finetune a robust baseline to derive the LoRA parameters $\\Delta_{i}$ for task-specific models for $i$ -th task, facilitating LLM instruction learning and parameter alignment. SNI is categorized into 60 types based on task types, while P3 encompasses 36 categories, corresponding to 60 and 36 parameter modules, respectively. ", "page_idx": 4}, {"type": "text", "text": "3.2.3 Hypernetwork Pretraining for Preliminary Generalization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Previous research [5; 27] has demonstrated that pretraining hypernetworks can substantially improve the model\u2019s cross-task generalization capabilities. Adhering to the HINT [13], we pretrain the hypernetwork on C4 [29] before finetuning it on a diverse multi-task prompt dataset. As illustrated in the right segment of Figure 2, given an input sequence, we partition it into randomly sized segments $a,\\,b$ , and $c$ , where $a$ is fed into the hypernetwork, $b$ into the LLM, and $c$ is the segment to predict. During this stage, training is conducted by minimizing the cross-entropy loss $\\mathcal{L}_{\\mathrm{pred}}$ , aiming to ensure that the hypernetwork learns to recognize instructions to enhance generalization ability. ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{pred}}=\\log P_{\\mathrm{(LLM+Hypernetwork}(a))}{\\big(}c\\mid b{\\big)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2.4 Hypernetwork Finetuning for Instruction Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "At this stage, TAGI is finetuned on a multi-task prompt dataset, enabling it to learn the generation of optimal parameters from task instructions, thereby ensuring effective generalization to future unseen tasks. Similar to the pretraining phase, task instructions (alongside some few-shot samples) replace $a$ , the main input replaces $b$ , and the target replaces $c$ . In each iteration, the hypernetwork generates LoRA parameters and encodes the instructions. LoRA is a parameter-efficient module (i.e., inserting into the model), and the encoded instructions are integrated with the encoder\u2019s embeddings for information fusion and concatenated with the fused encoding input during decoding. Beyond the standard $\\mathcal{L}_{\\mathrm{pred}}$ , we employ knowledge distillation for instruction learning: a strong baseline combining complete task instructions and input, serves as the teacher, while the model incorporating generated LoRA parameters with the input, acts as the student. The KL divergence $\\mathcal{L}_{\\mathrm{kl}}$ measures the discrepancy in word probability distributions between the two models as an implicit learning outcome, and the MSE loss ${\\mathcal{L}}_{\\mathrm{ins}}$ calculates the difference between the generated parameters and those of task-specific parameter-efficient modules as an explicit learning intermediate result. The formulation of finetuning is as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{ins}}=\\mathrm{MSE}(\\Delta_{i},\\mathrm{Hypernetwork}(a))\\qquad\\qquad}\\\\ {\\mathcal{L}_{\\mathrm{kl}}=\\mathrm{KL}(P_{\\mathrm{(LLM+}\\Delta_{i})}(x\\mid(a;b))\\mid\\mid P_{\\mathrm{(LLM+Hypernetwork}(a))}(x\\mid b))}\\\\ {\\mathcal{L}_{\\mathrm{finetune}}=\\mathcal{L}_{\\mathrm{pred}}+\\lambda_{1}\\mathcal{L}_{\\mathrm{kl}}+\\lambda_{2}\\mathcal{L}_{\\mathrm{ins}}\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $a\\in T_{i}$ , $\\Delta_{i}$ is the optimal LoRA modules of the $i$ -th task, $\\lambda_{1}$ and $\\lambda_{2}$ are the hyper-parameter to control the importance of distillation in finetuning. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first present the datasets $(\\S\\,4.1)$ and baselines (\u00a7 4.2) used in our evaluation and then discuss three research questions (RQs): ", "page_idx": 5}, {"type": "text", "text": "RQ1: Can the proposed instruction learning paradigm effectively learn the ability of instance training? Can it support cross-task generalization of LLMs? (\u00a7 4.4) ", "page_idx": 5}, {"type": "text", "text": "RQ2: How many foundation tasks does TAGI need to learn to achieve better results? $(\\S\\ 4.5)$ ", "page_idx": 5}, {"type": "text", "text": "RQ3: What is the impact of different modules and learning stages on TAGI? (\u00a7 4.7) ", "page_idx": 5}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To demonstrate the generality of our method, we evaluate our approach on two popular multi-task instruction datasets3: Super-Natural Instructions (SNI) [37] and T0 split of P3 (P3) [30]. ", "page_idx": 5}, {"type": "text", "text": "SNI comprising over 1,600 task datasets, each dataset includes a task definition and a set of fixed positive and negative demonstrations. We follow the previous research [13; 27] and examine two settings: only using the task definition as the input to the hypernetwork $({\\bf\\nabla}^{\\leftarrow}{\\bf D}{\\bf e}{\\bf f^{\\circ}})$ , and using the definition along with two few-shot positive examples $\\mathbf{^{\\circ}D e f}+2\\mathbf{\\^{p}o s}^{\\circ})$ . We only use the English tasks in the dataset and the model\u2019s generation is evaluated on a set of 119 unseen tasks using ROUGE-L. ", "page_idx": 5}, {"type": "text", "text": "P3 composed of 62 task datasets, the T0 model is trained with these tasks divided into meta-training and meta-test sets. The format of the prompts takes into consideration 0-shot reasoning and typically includes instructions or possible answer options. We follow the precedent work [41] by using the T0 training subset 36 tasks to train our model. The evaluation is conducted based on the accuracy scores of multiple-choice questions for unseen 11 tasks in the meta-test set (MTest11). ", "page_idx": 5}, {"type": "text", "text": "4.2 Baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We compare the characteristics of TAGI against eight primary groups of baselines (as shown in Table 1): 1) ", "page_idx": 5}, {"type": "text", "text": "No FT: models without finetuning. 2) HyperTuning [27]: models that use hypernetwork to convert demonstrations into adapters without instruction fusion. 3) Hypter [42]: models based on hypernetwork do not use pretraining. 4) HINT [13]: models pretrain hypernetwork and concat instruction. 5) T0 and Tk-Instruct: strong base", "page_idx": 5}, {"type": "table", "img_path": "CluvZBfrjj/tmp/ae4f14c37ea0bf6f97337626e62b1699ad23bb55bebdf3a03f41373b62f83c38.jpg", "table_caption": ["Table 1: Compare the characteristics of all comparison methods and the proposed TAGI. More comparisons can be seen in C.1. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "lines fully finetuned on P3 and SNI respectively with instruction concatenated. 6) Full FT: models fineuned on target tasks. 7) Decoder-only model: decoder-only models fully finetuned like GPT-2 [28] and OPT [43]. 8) FiD-ICL [41]: ICL method use encoder intermediate fusion. ", "page_idx": 5}, {"type": "table", "img_path": "CluvZBfrjj/tmp/ab77388903a5551f8ee349e9ff914383621e7bc35d8f21c7c56caec3eadf8cff.jpg", "table_caption": ["Table 2: RougeL results on Super-Natural Instructions. The best results are in bold, while the second-best are underlined. $^{*,\\,\\dag}$ means that those results are from HINT [13] and [27] respectively, \"-\" means not reported. $\\ddagger$ indicates that there is no parameter alignment loss in the hypernetwork finetuning because the model is too large, leading to a significant amount of time required for LoRA tuning for each task. The Average Relative FLOPs cost is calculated relative to Tk-Instruct. We use the number of FLOPs required by each model to process one task (containing 100 examples). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Implementations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We limit our scope to encoder-decoder models for our experiments4. We use T5-LM-Adapt5 and T0 [30] as initializations in our experiments. The two model groups have the same architectural framework but differ in weight; T0 uses T5-LM-Adapt for initialization and undergoes multi-task training on the P3 meta-training set. For SNI, only T5-LM-Adapt is considered, and three different sizes are tested: Base (250M), XL (3B), and XXL (11B), with the teacher model being TK-Instruct [37]. For P3, we experimented with two sets of models of three different sizes: Base (250M), Large (800M), and XL (3B) with the only template as input, while the teacher model being FiD-ICL [41] with 16-shot examples. The A.4 contains more implementation details and experimental settings. ", "page_idx": 6}, {"type": "text", "text": "4.4 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Super-Natural Instructions. We report the performance and inference costs of TAGI models and baselines in Table 2. Our analysis and findings yield several key insights: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Firstly, methods lacking finetuning exhibit subpar performance. As shown in the first row of the table, the performance of No FT is significantly lower than other baseline methods by approximately 30 points (except for Hypter), which underscores the critical role of inductive bias, introduced during meta-training, in enhancing the model\u2019s instructional adherence and cross-task generalization. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Secondly, TAGI demonstrates notable improvements over other hypernetwork-based baselines, with only a marginal increase in inference overhead (see Table 2 last column). We find that TAGI still outperforms the advanced method HINT $(\\ge2$ points) while achieving similar computational savings. This highlights the efficacy of instruction learning with knowledge distillation. The underperformance of HINT and Hypertuning may stem from their sole reliance on cross-entropy with the target during meta-training, lacking explicit supervision of intermediate task-specific module parameters and implicit supervision of the teacher outcome. This deficiency impedes their ability to fully leverage instruction tasks for generating superior adapter parameters during meta-test. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Thirdly, TAGI consistently matches or even surpasses robust baselines in both zero- and few-shot settings. Comparing TAGI with multi-task finetuning approaches such as Full FT and TK-Instruct, we observe that TAGI achieves comparable performance $(0-2.3\\$ points) except for 11B while utilizing approximately $2.5~\\times$ fewer FLOPs. TAGI\u2019s performance on the 11B model is somewhat lacking, potentially attributable to either insufficient training due to resource limitations or a decrement in performance stemming from the omission of parameter alignment constraints due to time constraints6. In alignment with prior research, TAGI significantly surpasses GPT-2 and OPT-13B in comparative analyses with decoder-only models ( $\\geq10$ points in GPT2 and $\\geq7$ points in OPT-13B), affirming the superiority of encoder-decoder models within similar meta-learning frameworks. Overall, TAGI fulfills its objective by enhancing cross-task generalization capabilities through instruction learning and striking an optimal balance between performance and efficiency. ", "page_idx": 6}, {"type": "table", "img_path": "CluvZBfrjj/tmp/ba7ccaf5a679b786ee672aaa29819b7d140bd136ffc3ee5d47090d0315ea6f43.jpg", "table_caption": ["Table 3: Average accuracy results over T0 evaluation tasks after training on the ${\\mathrm{T}}0\\,{\\mathrm{P}}3$ train set. $\\alpha$ means results are from [41]. \u2661trained by us followed the Tk-Instruct (meta-training) [37]. Our method uses only template inputs without demonstrations yet achieves competitive performance with ICL-based methods using 16 shots, with much-reduced inference overhead. The Average Relative Inference Time is calculated relative to the Metatrain. We use the inference time required by each model to process all 11 test tasks with batch_size of 1. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "P3. We report results on the T0 evaluation set in Table 3, with full results in C.2. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Firstly, examining the ICL-based methods presented in the middle section, it is evident that all three ICL aggregation strategies achieve superior performance. This underscores the utility of instructions and demonstrations in aiding LLMs. However, these methods require concatenating extensive demonstrations during both training and inference, which significantly increases computational demands and reduces efficiency $(\\times2-\\times13.2\\$ inference time). In contrast, TAGI by leveraging solely task instructions one time, attains comparable or superior accuracy levels while significantly curtailing computational burdens $\\mathbf{\\left(\\times0.88\\right)}$ . TAGI demonstrates a slight disadvantage (merely 1.2 points) to FiD-ICL [41] on T5-LM, yet it outperforms other methods $\\geq1$ point). For T0, it is only 1.5 points lower than Full FT and exceeds all ICL-based methods. Notably, TAGI does not require the 16 examples like the ICL-based method, nor does it necessitate repeated processing of instructions like the baselines, significantly reducing inference overhead. ", "page_idx": 7}, {"type": "text", "text": "\u2022 A comparison of the first three lines of results indicates that for large or XL models, initializing with T5-LM outperforms T0. We hypothesize that the process of training T5-LM to transition into T0 might result in the dilution of world knowledge or the diminishment of certain specific capabilities, thereby attenuating the beneftis derived from meta-training. Conversely, for models of base size, T0 serves as a more effective initialization point. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Furthermore, TAGI outperforms competing hypernetwork models7. By comparing the last two columns, it is evident that the performance in MTest11 surpasses HINT and Hypertuning by 0.5 and 4.6 points respectively. Additionally, in the HyperT5 evaluation, the performance exceeds Hypertuning by 1 point. This aligns with prior findings, suggesting that instruction learning augments the hypernetwork\u2019s task comprehension and its capacity to generate task-specific adapters. ", "page_idx": 7}, {"type": "text", "text": "4.5 Varying Number of Meta-Training Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "A fundamental component of our methodology is incorporating parameter alignment in instruction learning. Consequently, it is imperative to examine the effect of varying the number of tasks on which parameter alignment is applied on outcomes and its influence on the generalization capabilities of LLMs. To this end, we conduct a comprehensive experimental analysis to compare the efficacy of instruction learning with parameter alignment across a spectrum of task quantities against instruction learning devoid of parameter alignment. Tasks are organized in descending order based on the number of datasets encompassed within each. Subsequently, a predetermined number of tasks are sequentially selected for meta-training purposes. This approach allows us to systematically evaluate the impact of parameter alignment on learning and generalization as the number of tasks varied. ", "page_idx": 7}, {"type": "image", "img_path": "CluvZBfrjj/tmp/88c2bb8683db23677ddc159cb03f4b8496dc8d19a369f9c76e0571b20a0ffb32.jpg", "img_caption": ["Figure 3: The performance of different numbers of meta-training tasks. The backbone model is T5-LM-Base, all trained for 20,000 steps. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "CluvZBfrjj/tmp/5a36737b20bd3f36d35509633aae9ad61832582bc3ab7c59fa6fc9a1808317a6.jpg", "img_caption": ["Figure 4: The percentage of generated parameters $(\\%)$ against performance (RougeL). The backbone model is T5-LM-Base, all trained for 20,000 steps. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "From Figure 3, we find that, firstly, an increase in the number of tasks correlates with improved performance across all methods, suggesting that meta-training across a broader array of tasks enhances the model\u2019s instruction-following capabilities. However, the practical limitations of sourcing a sufficient quantity of tasks for meta-training must be acknowledged. Secondly, it was observed that the TAGI model exhibits lower overall performance in the absence of parameter alignment for instruction learning, yet it demonstrates a smaller relative standard deviation and less variability in performance in response to the number of tasks. This pattern aligns with the expected outcomes of instruction learning, highlighting the efficacy of our approach in bolstering the model\u2019s ability to adhere to task instructions and generate task-specific adapters. ", "page_idx": 8}, {"type": "text", "text": "4.6 Parameter Size against Performance ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyzed the proportion of generated parameter sizes relative to the total parameter size during the generation of various ranks, and compared this to the performance of the full meta-training fine-tuning method, as demonstrated in Figure 4 and Table 7. We can find that TAGI requires only about $10\\%$ of the parameters to outperform full meta-training fine-tuning which indicates that the limited parameters generated by the Hypernetwork serve as an optimal solution for task completion. The ability to adaptively construct models tailored to specific tasks removes the necessity for additional fine-tuning, underscoring TAGI\u2019s effectiveness and efficiency. ", "page_idx": 8}, {"type": "text", "text": "4.7 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To evaluate the significance of each component within the TAGI model, we conducted a series of experiments across two meta-task datasets utilizing the T5-LM-XL (3B) model. The results as depicted in the Table 4, highlight that the instructions fusion plays a pivotal role in enhancing model performance. This process facilitates dynamic interaction between the input and the instructions, enriching the model\u2019s input with additional contextual information, reminiscent of the substantial benefits observed with ICL. Moreover, pretraining emerges as a critical phase, markedly improving the capabilities of models that have not undergone pretraining, thereby significantly enhancing their proficiency in interpreting and executing task instructions. Furthermore, the systematic removal of various components during the finetuning phase indicates a consistent decline in performance, underscoring the integral contribution of each component to the model\u2019s overall efficacy. ", "page_idx": 9}, {"type": "table", "img_path": "CluvZBfrjj/tmp/d259346a3afcf3e67c4f878f9afaf324ef763579495db91d5346815524e81ca7.jpg", "table_caption": ["Table 4: Ablation study of TAGI model. All models utilized are T5-LM-XL (3B) and training for 20,000 steps. The P3 dataset was selected by the HyperT5 evaluation. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Compared to meta-learning methods such as LoRA fine-tuning (rank $=\\!32$ ) \"Tk-Instruct-LoRA\", prefix fine-tuning (num_virtual_token $\\scriptstyle=32$ ) \"Tk-Instruct-prefix\", and full fine-tuning \"Tk-Instruct\", our TAGI method enhances task comprehension and utilization which achieved through a hypernetwork that dynamically generates adapter LoRA insertions into the LLM based on input, leads to better crosstask generalization capabilities. Notably, prefix fine-tuning excels in the Def $+\\,2\\mathrm{Pos}$ scenario, likely due to its effective integration of information from positive examples. Conversely, the Def scenario performs less satisfactorily, indicating that instructions alone are insufficient for optimal results. Comparative analysis with other hypernetwork models reveals that TAGI\u2019s ablation performance remains robust, affirming the effectiveness of each step in bolstering TAGI\u2019s operational efficiency. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce an innovative method of instruction learning designed to emulate instance training. This approach enables the model to achieve specified tasks and learn from instructions on how to address a category of problems. The proposed TAGI seamlessly integrates instruction into the input and processes the instruction simultaneously, thereby ensuring minimal inference overhead. Concurrently, we employ a knowledge distillation framework to facilitate instruction learning for distilling skills and aligning task-specific models. This allows the hypernetwork to transform task instructions into an efficient module inserted into the LLMs, thereby boosting generalization performance. Remarkably, TAGI consistently equals or surpasses the efficacy of conventional meta-training approaches while requiring fewer FLOPs and obviating the need for additional model parameters updating or gradient back-propagation. Future work will investigate more potent hypernetwork pretraining techniques and develop superior instruction fusion methods to augment the hypernetwork\u2019s expressive capability, thereby enhancing the model\u2019s ability to generalize to unseen tasks. Moreover, future work will investigate various task type classifications and the generalization effects of cross-modal tasks in instruction learning. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Key R&D Program of China (No. 2022YFF0711900) and the National Natural Science Foundation of China (No.62376270, No.62276264). This work was supported by the Youth Innovation Promotion Association CAS. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jonathan Baxter. Learning to Learn. Springer US, 1998. ", "page_idx": 9}, {"type": "text", "text": "[2] Christos Baziotis, Mikel Artetxe, James Cross, and Shruti Bhosale. Multilingual machine translation with hyper-adapters, 2022. ", "page_idx": 10}, {"type": "text", "text": "[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. [4] Vinod Kumar Chauhan, Jiandong Zhou, Ping Lu, Soheila Molaei, and David A. Clifton. A brief review of hypernetworks in deep learning, 2023.   \n[5] Tong Chen, Qirun Dai, Zhijie Deng, and Dequan Wang. Demonstration distillation for efficient in-context learning, 2024.   \n[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.   \n[7] Budhaditya Deb, Guoqing Zheng, and Ahmed Hassan Awadallah. Boosting natural language generation from instructions with meta-learning, 2022. [8] David Ha, Andrew Dai, and Quoc V. Le. Hypernetworks, 2016.   \n[9] Yun He, Huaixiu Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. Hyperprompt: Prompt-based task-conditioning of transformers, 2022.   \n[10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019.   \n[12] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[13] Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and Matthew Peters. Hint: Hypernetwork instruction tuning for efficient zero-shot generalisation. ACL, 2023.   \n[14] James M. Joyce. Kullback-Leibler Divergence, pages 720\u2013722. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.   \n[15] Sharon Kim, Mahjabeen Raza, and Edward Seidman. Improving 21st-century teaching skills: The key to effective 21st-century learners. Springer US, 2019.   \n[16] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario \u0160a\u0161ko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl\u00e9ment Delangue, Th\u00e9o Matussi\u00e8re, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran\u00e7ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Heike Adel and Shuming Shi, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175\u2013184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[17] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, Online, August 2021. Association for Computational Linguistics.   \n[18] Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, and Jun Zhao. Imagination augmented generation: Learning to imagine richer context for question answering over large language models. arXiv preprint arXiv:2403.15268, 2024.   \n[19] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning, 2022.   \n[20] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning, 2023.   \n[21] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States, July 2022. Association for Computational Linguistics.   \n[22] Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk\u2019s language, 2022.   \n[23] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In ACL, 2022.   \n[24] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions, 2022.   \n[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.   \n[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: an imperative style, high-performance deep learning library. Curran Associates Inc., Red Hook, NY, USA, 2019.   \n[27] Jason Phang, Yi Mao, Pengcheng He, and Weizhu Chen. Hypertuning: Toward adapting large language models without back-propagation, 2022.   \n[28] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   \n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[30] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[31] J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4:131\u2013139, 1992. ", "page_idx": 12}, {"type": "text", "text": "[32] Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context, 2022.   \n[33] Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid transformers: Towards a single model for multiple tasks. In International Conference on Learning Representations, 2021.   \n[34] Sebastian Thrun and Lorien Y. Pratt. Learning to learn: Introduction and overview. In Learning to Learn, 1998.   \n[35] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot generalization?, 2022.   \n[36] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776\u20135788, 2020.   \n[37] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on $1600+$ NLP tasks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.   \n[38] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.   \n[39] Orion Weller, Nicholas Lourie, Matt Gardner, and Matthew E. Peters. Learning from task descriptions. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1361\u20131375, Online, November 2020. Association for Computational Linguistics.   \n[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020. Association for Computational Linguistics.   \n[41] Qinyuan Ye, Iz Beltagy, Matthew Peters, Xiang Ren, and Hannaneh Hajishirzi. FiD-ICL: A fusion-in-decoder approach for efficient in-context learning. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8158\u20138185, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[42] Qinyuan Ye and Xiang Ren. Learning to generate task-specific adapters from task description, 2021.   \n[43] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. ", "page_idx": 12}, {"type": "text", "text": "[44] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections, 2021. ", "page_idx": 13}, {"type": "text", "text": "[45] Ahmet \u00dcst\u00fcn, Arianna Bisazza, Gosse Bouma, Gertjan van Noord, and Sebastian Ruder. Hyperx: A unified hypernetwork for multi-task multilingual transfer, 2022. ", "page_idx": 13}, {"type": "text", "text": "A Experimantal Settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Problem Setting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Meta-Training and Inference: Our methodology rigorously adheres to the protocol outlined in MetaICL [21]. In the meta-train phase, we commence by selecting a task $\\tau$ from $\\tau_{t r a i n}$ , followed by the sampling of $k$ support examples $\\{(x_{i}^{(s)},y_{i}^{(s)})\\}$ and $m$ query examples $\\{(x_{i}^{(q)},y_{i}^{(q)})\\}$ from the chosen task. The proposed hypernetwork is then adjusted to minimize the overall loss, focusing on generating a task model that can accurately predict the target sequences (e.g., answer) for source sequences (e.g. question). During the meta-test/inference phase, for each novel task in $\\mathcal{T}_{t e s t}$ , we employ instructions to create the task-specific adapter, to optimize the model\u2019s performance across all query examples $\\{(x_{i}^{(q)},y_{i}^{(q)})\\}$ . ", "page_idx": 13}, {"type": "table", "img_path": "CluvZBfrjj/tmp/a5f95703e8a915854ffcc3883fe05789e60dd8d950089d8604dd1cb464f00c83.jpg", "table_caption": ["Table 5: Number of samples in given splits for each dataset. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "During the pretraining phase, we utilized the C4 dataset [29], truncating each sequence to 1024 tokens. For the training phase, we employed Super-Natural Instructions (SNI) [37] and P3 datasets [30] for meta-training and meta-test. For SNI, we adhered to the default settings [13; 37], which include 100 examples per task for both the training and test splits. For P3, we used the data and prompts provided by T0. All prompts related to the meta-training tasks were included in the meta-training process, while the meta-test phase utilized evaluation prompts specified by T0 [30]. We treated ANLI R1, R2, and R3 as three distinct tasks, resulting in 11 tasks for the original meta-test in P3 (Meta-Test-11). Due to resource constraints, we deviated from the sampling procedures of prior work, opting to sample 1000 examples per task for each prompt template. This approach yielded a smaller dataset size, as detailed in Table 5. For further information on P3 refer to [30]. Additionally, to facilitate comparison with the Hypertuning method, we excluded the StoryCloze task from the evaluation since it was not included in the datasets for the HyperT5 evaluation. ", "page_idx": 13}, {"type": "text", "text": "A.3 Split Sizes for Varying Number of Meta-Training Tasks ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As shown in Table 11 and Table 12, we present a comprehensive list of the two datasets, including the number of tasks or templates contained in each task and the task divisions from $\\S\\,4.5$ experiments. The divisions in the table are cumulative; thus, the second division includes both the first and the second divisions. For SNI, tasks were sorted in descending order based on the number of tasks they contained and then divided into specified sizes (6, 15, 30, 60). For P3, we selected a specified number of tasks (5, 10, 20, 36) based on the task classification in the original paper, which includes categories such as Multiple-Choice QA, Closed-Book QA, Summarization, Structure-To-Text, Paraphrase Identification, Sentiment, Topic Classification, and Extractive QA. ", "page_idx": 13}, {"type": "text", "text": "We obtain all our data from huggingface datasets [16]. In the following, we provide the dataset links: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Super-Natural Instructions: https://github.com/allenai/ natural-instructions ", "page_idx": 13}, {"type": "text", "text": "\u2022 P3: https://huggingface.co/datasets/bigscience/P3 ", "page_idx": 13}, {"type": "text", "text": "Additionally, the Super-Natural Instructions dataset (previously known as Natural Instructions-v2) has undergone some changes over time. In our experiments, we use the v2.6 version. ", "page_idx": 14}, {"type": "text", "text": "A.4 Implementations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our implementations are based on huggingface transformers v4.23.1 [40] using PyTorch v1.13.1 [26] and deepspeed8 v0.10.0. All experiments were conducted on 4 A100 NVIDIA GPUs, each equipped with 80GB of memory, and eight A6000 NVIDIA GPUs with 48GB of memory. Unless otherwise specified, the rank of LoRA generated by the hypernetwork is 32, and we use the Adamw optimizer with a learning rate of 5e-5 and a linear warmup rate of 0.02. We pre-train all models for 50,000 steps using C4 [29] with a batch size of 8 samples and sequences of length 1024. ", "page_idx": 14}, {"type": "text", "text": "A.5 T0-Base/Large/3B ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "T0 [30] provides model checkpoints only in sizes 3B and 11B. Additionally, HINT [13] and FiD-ICL [41] re-pretrained T0 and found that the model was not sufficiently trained, achieving better results after reproduction. Therefore, we used the T0 model 9 reproduced by FiD-ICL to conduct a series of experiments. ", "page_idx": 14}, {"type": "table", "img_path": "CluvZBfrjj/tmp/4cbb74461f3ff68ce158faa5f7a7066577fe29b9700be15f24735c8ebebb3001.jpg", "table_caption": ["Table 6: Hyperparameters for Training TAGI Models and LoRA Tuning. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.6 Hyperparameter ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The complete stable hyperparameter set used for training runs can be found in Table 6. ", "page_idx": 14}, {"type": "text", "text": "B Additional Experiments and Findings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Why we choose Enc-Dec Models? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Previous work has suggested that models with an encoder-decoder (enc-dec) structure have advantages over decoder-only (dec-only) models in terms of task generalization and instruction-following capabilities [20; 35; 41]. Therefore, in our experiments, we only considered models with an enc-dec structure (T5-LM and T0). Our experimental results demonstrated that enc-dec models indeed have an advantage when compared, although dec-only models might have higher computational efficiency due to their ability to cache KV and have fewer layers. However, our method, TAGI, significantly improves performance in various aspects with only a slight increase in computational overhead. We encode the task instructions only once based on the original computation. ", "page_idx": 14}, {"type": "text", "text": "B.2 T5-LM-XXL Training Trend ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we detail how the performance of the T5-LM-XXL (11B) model surpasses the hypernetwork models but falls short of the meta-trained strong baseline Tk-Instruct by 1-4 points, as mentioned earlier in $\\S\\ 4.4$ . The primary reason is insufficient training; when replicating the Tk-Instruct experiment, our results were significantly lower than reported when finetuning for only ", "page_idx": 14}, {"type": "image", "img_path": "CluvZBfrjj/tmp/ceb99a927c3823afbe809bbb13e151f4edb84928328c14fea9552d112d5689a8.jpg", "img_caption": ["Figure 5: Analysis of T5-LM-XXL (11B). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "20,000 steps. Consequently, we analyzed the performance of our TAGI model at different finetuning steps. As shown on the left side of Figure 5, performance steadily improves with more steps with substantial growth. Thus, we reasonably predict that increasing the steps to 50,000 or more could surpass Tk-Instruct. Another possible reason is the lack of parameter alignment for the 11B model due to limited resources. Our previous analysis has shown that parameter alignment is crucial, with larger models beneftiing more. Therefore, we analyzed performance with a small number of tasks for parameter alignment. As shown on the right side of Figure 5, performance with parameter alignment for 6 and 15 tasks is better than without alignment. Based on these trends, it can be inferred that performance with full task parameter alignment could surpass Tk-Instruct. ", "page_idx": 15}, {"type": "text", "text": "B.3 Analysis on Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To explore the optimal hyperparameter settings for our experiments, we conducted a series of tests and error analyses using the T5-LM-Base (800M) model. The findings presented in Table 7 reveal that variations in hyperparameters can lead to performance fluctuations, particularly with higher learning rates or reduced finetuning steps. Given the varying pre-training conditions of models of different sizes, a size-specific analysis is essential; however, details on larger models are omitted here due to resource limitations. ", "page_idx": 15}, {"type": "text", "text": "We observed that different settings of LoRA minimally affect performance, leading us to select a balanced size of 32. Similarly, the impact of the warmup ratio is negligible; thus, based on our experience, we chose a warmup ratio of one percent of the maximum finetuning steps. While more finetuning steps generally correlate with improved performance, excessive finetuning can result in overfitting on meta-training tasks, thereby diminishing generalizability. Moreover, increased finetuning steps require greater computational resources. Consequently, we determined that the optimal number of finetuning steps is 20,000 based on our experimental outcomes. ", "page_idx": 15}, {"type": "text", "text": "B.4 How $\\lambda_{1}$ and $\\lambda_{2}$ are tuned? ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the experiment, we set $\\lambda_{1}$ and $\\lambda_{2}$ to two different values: $\\lambda_{1}=5$ and $\\lambda_{2}={\\mathrm{sigmoid}}({\\mathcal{L}}_{\\mathrm{ins}})$ . The effects of these different $\\lambda$ values on the results are illustrated in Figure 6 and Table 8. We maintained all other conditions constant and only varied $\\lambda$ to perform an ablation experiment at $\\mathrm{Def}{+}2\\mathrm{Pos}$ . scenario. ", "page_idx": 15}, {"type": "text", "text": "B.5 Inference Cost ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To analyze the computational efficiency of the TAGI model compared to the standard instruction training model (full fine-tuning), let\u2019s consider a scenario where we have to process $n$ samples, each of length $i$ , along with a task instruction of length $t$ . We assume the output sequence length is negligible and thus ignore it in our computations. ", "page_idx": 15}, {"type": "text", "text": "In a typical full fine-tuning setup, such as Tk-Instruct, each input is concatenated with the task instruction, requiring the model to process the combined input sequence. If we denote the number of FLOPs required to process a single token with an encoder-decoder model as $N$ , where $N$ is the total number of model parameters, then the total computation cost for all samples can be estimated as: $\\mathrm{FLOPs}_{\\mathrm{standard}}=N\\stackrel{\\cdot}{\\cdot}n(t+i)$ Here, each of the $n$ samples includes both the instruction and the sample input, leading to $n(t+i)$ tokens being processed. ", "page_idx": 15}, {"type": "table", "img_path": "CluvZBfrjj/tmp/552cabc8041bb6fcca6a41a360f06f0a3cdcf2e22e51625a9278b70f778d07c4.jpg", "table_caption": ["Table 7: Performance variation due to different hyperparameters. The base model is T5-LM-Base, and all experiments follow the previous hyperparameter settings, changing only the target hyperparameter, where underlines indicate experimental defaults. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "CluvZBfrjj/tmp/676e2a79ecbcfc283582a0b602ffc29a279b09db3856d00354f75c8d7df07f26.jpg", "img_caption": ["Figure 6: Ablation study on $\\lambda$ hyperparameters. The backbone model is T5-Base. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Our TAGI model, on the other hand, processes the task instruction only once, regardless of the number of samples. This unique feature significantly reduces the computation required, especially as the number of samples or the length of the instruction increases. The total computation cost in this model is given by: $\\mathrm{{FLOPs}}_{\\mathrm{{TAGI}}}=N\\cdot(t+n i)$ In this case, the instruction length $t$ is processed only once, and each sample is processed separately, resulting in a total of $(t+n i)$ tokens being processed. ", "page_idx": 16}, {"type": "text", "text": "C Extended Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Characteristics Comparison of the Proposed TAGI and Other Baselines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we report a full comparison of methods and the proposed TAGI in Table 9, also visualized in Table 1. In this report, we compare various methods across eight dimensions. Finetuning on target tasks yields good performance; however, it necessitates retuning when applied to unseen tasks and fails to address these effectively. Strong baseline meta-training methods excel at handling unseen tasks by enabling models to solve problems based on task-specific instructions. Nevertheless, these methods are limited to instance-level operations and entail repetitive processing of concatenated instructions and comprehensive finetuning, resulting in significant parameter updates and high inference costs. ", "page_idx": 16}, {"type": "text", "text": "Table 8: Ablation study on $\\lambda$ hyperparameters. The backbone model is T5-Base. ", "page_idx": 17}, {"type": "table", "img_path": "CluvZBfrjj/tmp/6f50af1ad923d1a69b896a2cc5cae4ca39c30ff29db2d13b66ae25f2f4d55baf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Hypter [42] initially introduced the approach of considering tasks at the task level, treating identical tasks as a unified entity, and employing a hypernetwork to generate adapters that represent specific task models from instructions. Building on this, Hypertuning [27] uses demonstrations to generate adapters and pretrains the hypernetwork to boost its expressive capabilities. Both strategies avoid the direct input of instructions and rely on hypernetwork, which reduces parameter updates and lowers computational demands during inference. However, they suffer from notable performance degradation due to the lack of instructional information in the input. ", "page_idx": 17}, {"type": "text", "text": "HINT [13] addresses this issue by appending instructions post-encoder, thus eliminating redundant computations. Although these methods facilitate learning at the task level, they do not engage in instruction-based learning, i.e., they do not explicitly supervise the hypernetwork\u2019s generation process to aid in understanding instructions and generating parameters. ", "page_idx": 17}, {"type": "text", "text": "The proposed TAGI rectifies these deficiencies by integrating cross-attention for enhanced information fusion and supervised learning of adapter weights within HINT. This innovation aids in generalizing to unseen tasks without increasing the computational burden. ", "page_idx": 17}, {"type": "table", "img_path": "CluvZBfrjj/tmp/21db99ac8fd98c25614b939f812d6a2935ae210fe36661fb3e673a9ffde80e97.jpg", "table_caption": ["Table 9: Compare the characteristics of all comparison methods and the proposed TAGI. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 P3 Full Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 10 reports the per-task performance and average accuracy on P3 reported in Table 3. ", "page_idx": 17}, {"type": "text", "text": "D Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Large Language Models. Due to computational constraints, most of our experiments were conducted using models with $\\leq3B$ parameters. Given the complexity of our research, we restricted our focus to encoder-decoder models, which have demonstrated superior performance in cross-task generalization [35], which we explore further in B.1. Consequently, it remains uncertain whether instruction learning can be effectively scaled to larger models $\\zeta\\ge7B$ parameters) or commonly used decoder-only models. However, since our method preserves the original model parameters without compromising performance, we anticipate its applicability to broader research in the future. ", "page_idx": 17}, {"type": "text", "text": "Training Costs. Although TAGI is computationally efficient during inference, its training cost is significantly higher. This is due to the additional requirements beyond the foundation laid by previous work, including the introduction of knowledge distillation, running a hypernetwork to generate adapters for each batch, and pre-training some downstream task-specific models. Consequently, while ", "page_idx": 17}, {"type": "table", "img_path": "CluvZBfrjj/tmp/8acfbe92e56ee8f5c2d1c38b216a44008ea1b5dd0d413dbbec11715d154df333.jpg", "table_caption": ["Table 10: Main Full P3 Results. \"-\" means not reported. $\\dagger$ and $\\ddagger$ mean the results are from FiD-ICL [41] and Hypertuning [27] respectively. $\\diamondsuit$ Computed as the average of R1/R2/R3 (except for HyperT5 rows where the numbers are quoted). More ICL-based results and details can be seen FiD-ICL [41]. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "TAGI may be highly efficient for inference and suitable for users with limited resources, training a unique TAGI model presents considerable challenges. ", "page_idx": 18}, {"type": "text", "text": "Datasets. In the SNI study, our investigation was limited to tasks in English, leaving the generalization capabilities in a multilingual context unexplored. However, given the proven effectiveness of hypernetwork methods in achieving multilingual generalization [2; 45], we are optimistic about the potential directions for our future research in this domain. Furthermore, in P3, we adopted the methodologies of T0 [30] and FiD-ICL [41], concentrating primarily on natural language processing (NLP) tasks amenable to ranking classification. This focus included tasks related to classification and multiple-choice questions but excluded other types of generative tasks. Looking ahead, we aim to develop new research resources and broaden our experimental scope and evaluations to encompass a more diverse array of categories. ", "page_idx": 18}, {"type": "table", "img_path": "CluvZBfrjj/tmp/d118b91d947fd2fced6aa66c6d2ce83ee96a196eadf3280ecdfe9749e0bbefd4.jpg", "table_caption": ["Table 11: Meta-Train dataset of Super-NaturalTable 12: P3 dataset tasks. $\\dagger$ means evaluation Instructions. without story_cloze. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We can find the limitations in D. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We can reproduce the main experimental results following our settings in A and 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We\u2019ll open source the code to an anonymous site https://anonymous.   \n4open.science/r/TAGI and put it on github after review. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We can find the experimental settings (hyperparameters and datasets) in 4.3 and A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We examined the effect of different hyperparameters on results in B.3. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We can find it in 4.3 and A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All of our studies follow the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We follow their open-source protocols in all our uses. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]