{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to instruction finetuning and demonstrates the impressive few-shot capabilities of large language models, a key concept that the current paper builds on."}, {"fullname_first_author": "Hyung Won Chung", "paper_title": "Scaling instruction-finetuned language models", "publication_date": "2022-12-01", "reason": "This paper extensively investigates instruction finetuning and its scaling properties, which the current paper directly addresses by proposing a more efficient approach."}, {"fullname_first_author": "Yizhong Wang", "paper_title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks", "publication_date": "2022-12-01", "reason": "This paper introduces the Super-Natural Instructions (SNI) dataset, a benchmark dataset used in the current paper to evaluate the effectiveness of the proposed TAGI method."}, {"fullname_first_author": "Hamish Ivison", "paper_title": "HINT: Hypernetwork instruction tuning for efficient zero-shot generalisation", "publication_date": "2023-07-01", "reason": "This paper proposes the HINT method, which is closely related to the TAGI method, making it a crucial comparative study for the current paper."}, {"fullname_first_author": "Jason Phang", "paper_title": "Hypertuning: Toward adapting large language models without back-propagation", "publication_date": "2022-12-01", "reason": "This paper presents the HyperTuning method, a relevant hypernetwork-based approach that the current paper compares against, highlighting the contributions and differences in their methodologies."}]}