[{"figure_path": "WftaVkL6G2/figures/figures_4_1.jpg", "caption": "Figure 1: Results for synthetic objective and CIFAR-10. Left: Amplified SCAFFOLD and SCAFFOLD both converge to the global minimum, but Amplified SCAFFOLD converges significantly faster. Right: Amplified SCAFFOLD converges to the best solution by a significant margin. Note that in both cases, the curves for FedAvg and FedProx are nearly overlapping.", "description": "This figure shows the comparison of the performance of different federated learning algorithms on synthetic data and CIFAR-10 dataset.  The left panel shows that both Amplified SCAFFOLD and SCAFFOLD reach the global minimum, but Amplified SCAFFOLD is significantly faster. The right panel shows that Amplified SCAFFOLD achieves the best results compared to other methods, while FedAvg and FedProx perform similarly and poorly.", "section": "5.2 Main Results"}, {"figure_path": "WftaVkL6G2/figures/figures_8_1.jpg", "caption": "Figure 1: Results for synthetic objective and CIFAR-10. Left: Amplified SCAFFOLD and SCAFFOLD both converge to the global minimum, but Amplified SCAFFOLD converges significantly faster. Right: Amplified SCAFFOLD converges to the best solution by a significant margin. Note that in both cases, the curves for FedAvg and FedProx are nearly overlapping.", "description": "This figure shows the experimental results for a synthetic objective function and the CIFAR-10 dataset.  The left panel compares the convergence speed of five different federated learning algorithms: FedAvg, FedProx, SCAFFOLD, Amplified FedAvg, and Amplified SCAFFOLD. It demonstrates that Amplified SCAFFOLD converges to the global minimum significantly faster than the other algorithms, especially SCAFFOLD, while FedAvg and FedProx show very similar performance. The right panel focuses on the CIFAR-10 dataset and highlights the superior performance of Amplified SCAFFOLD in achieving the best solution compared to the other algorithms.", "section": "5.2 Main Results"}, {"figure_path": "WftaVkL6G2/figures/figures_9_1.jpg", "caption": "Figure 2: Results for Fashion MNIST and ablation study. Left: Amplified SCAFFOLD reaches the best solution, but SCAFFOLD is competitive. Other baselines are much slower. Right: Amplified SCAFFOLD is robust to changes in data heterogeneity, number of participating clients, and number of client groups.", "description": "The left panel shows the training and testing performance of five algorithms on the Fashion-MNIST dataset.  Amplified SCAFFOLD converges faster than other algorithms and achieves the lowest loss and highest accuracy. The right panel shows ablation studies where data heterogeneity, number of participating clients per round, and number of client groups are varied to test the robustness of the five algorithms.  Amplified SCAFFOLD consistently achieves the best performance across all settings.", "section": "5.2 Main Results"}, {"figure_path": "WftaVkL6G2/figures/figures_50_1.jpg", "caption": "Figure 2: Results for Fashion MNIST and ablation study. Left: Amplified SCAFFOLD reaches the best solution, but SCAFFOLD is competitive. Other baselines are much slower. Right: Amplified SCAFFOLD is robust to changes in data heterogeneity, number of participating clients, and number of client groups.", "description": "The left plot shows the training loss and test accuracy for different federated learning algorithms on the Fashion-MNIST dataset.  Amplified SCAFFOLD demonstrates superior performance compared to baselines (FedAvg, FedProx, SCAFFOLD, Amplified FedAvg). The right plot illustrates the robustness of Amplified SCAFFOLD to variations in data heterogeneity (data similarity), number of participating clients per round, and number of client groups in a cyclic participation pattern.  Amplified SCAFFOLD consistently achieves the best solution across these variations.", "section": "5.2 Main Results"}, {"figure_path": "WftaVkL6G2/figures/figures_51_1.jpg", "caption": "Figure 4: CIFAR-10 with additional baselines. FedAdam is competitive, but Amplified SCAFFOLD maintains superiority.", "description": "This figure shows the results of training a CNN on CIFAR-10 with nine different federated learning algorithms.  The algorithms include five algorithms from the main paper (FedAvg, FedProx, SCAFFOLD, Amplified FedAvg, Amplified SCAFFOLD), and four additional baselines (FedAdam, FedYogi, FedAvg-M, and Amplified FedProx). Amplified SCAFFOLD consistently achieves the best performance (lowest training loss and highest test accuracy). FedAdam shows competitive performance, while the other algorithms show significantly worse performance. The results highlight the superior performance of Amplified SCAFFOLD compared to existing baselines in terms of convergence speed and final accuracy.", "section": "5.2 Main Results"}, {"figure_path": "WftaVkL6G2/figures/figures_52_1.jpg", "caption": "Figure 5: CIFAR-10 under SCA (stochastic cyclic availability). Amplified SCAFFOLD converges fastest.", "description": "This figure shows the training loss and test accuracy for different federated learning algorithms on the CIFAR-10 dataset under a stochastic cyclic availability client participation pattern.  The x-axis represents the number of communication rounds, and the y-axis shows the training loss (left panel) and test accuracy (right panel).  Nine different algorithms are compared, including FedAvg, FedProx, SCAFFOLD, Amplified FedAvg, Amplified SCAFFOLD, Amplified FedProx, FedAdam, FedYogi, and FedAvg-M. The results demonstrate that Amplified SCAFFOLD consistently achieves the lowest training loss and highest test accuracy, converging significantly faster than other algorithms.", "section": "5.2 Main Results"}]