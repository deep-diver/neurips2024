[{"type": "text", "text": "The Implicit Bias of Adam on Separable Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chenyang Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Difan Zou ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics and Actuarial Science School of Computing and Data Science The University of Hong Kong chyzhang@connect.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science School of Computing and Data Science & Institute of Data Science The University of Hong Kong dzou@cs.hku.hk ", "page_idx": 0}, {"type": "text", "text": "Yuan Cao Department of Statistics and Actuarial Science School of Computing and Data Science & Department of Mathematics The University of Hong Kong yuancao@hku.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adam has become one of the most favored optimizers in deep learning problems. Despite its success in practice, numerous mysteries persist regarding its theoretical understanding. In this paper, we study the implicit bias of Adam in linear logistic regression. Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum $\\ell_{\\infty}$ -margin in direction. Notably, for a general class of diminishing learning rates, this convergence occurs within polynomial time. Our result shed light on the difference between Adam and (stochastic) gradient descent from a theoretical perspective. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adam [25] is one of the most widely used optimization algorithms in deep learning. By entry-wisely adjusting the learning rate based on the magnitude of historical gradients, Adam has proven to be highly efficient in solving optimization tasks in machine learning. However, despite the remarkable empirical success of Adam, current theoretical understandings of Adam cannot fully explain its fundamental difference compared with other optimization algorithms. ", "page_idx": 0}, {"type": "text", "text": "It has been recently pointed out that the implicit bias [32, 40, 21] of an optimization algorithm is essential in understanding the performance of the algorithm in machine learning. In over-parameterized learning tasks where the training objective function may have infinitely many solutions, the implicit bias of an optimization algorithm characterizes how the algorithm prioritizes converging towards a specific optimum with particular structures and properties. Several recent works studied the implicit bias of Adam and other adaptive gradient methods. Specifically,  [35] studied the implicit bias of AdaGrad, and showed that AdaGrad converges to a direction that can be characterized as the solution of a quadratic optimization problem related to the limit of preconditioners. However, their results cannot be extended to Adam. [45] showed that gradient descent with momentum (GDM) and its adaptive variants have the same implicit bias with gradient descent. This result is extended to the setting of training homogeneous models in [44]. However, the results in [45, 44] reply on a nonnegligible stability constant - when the gradient entries are minimized below the stability constant (which is by default $\\mathrm{i0^{-8}}$ in Adam), adaptive gradient methods will essentially behave like gradient descent. Therefore, it remains an open question how Adam will behave under the more practical regime where stability constant is negligible. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We note that there exist several recent works studying variants of Adam without the stability constant. First of all, sign gradient descent, which can be regarded as Adam without momentum or stability constant, is usually considered a proxy of Adam in theoretical studies due to its ease of analysis. [14] studied the implicit bias of steepest descent with respect to different potentials and norms covering a variant of sign gradient descent, and demonstrated that sign gradient descent converges to the maximum $\\ell_{\\infty}$ -margin solution. However, this result for sign gradient descent cannot cover Adam, as the momentum terms in the update of Adam are crucial. Besides, a more recent work [50] studied the implicit bias of Adam W without considering the stability constant. They showed that, if the iterates of Adam W converge, then the limiting point must be a KKT point of an optimization problem With $\\ell_{\\infty}$ constraints. However, the analysis of Adam W in [50] relies on a non-zero regularization parameter, and therefore cannot be extended to the study of Adam. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we investigate the implicit bias of Adam. Specifically, let $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}\\subset\\mathbb{R}^{d}\\times\\{\\pm1\\}$ be a training data set of a binary classification problem. We consider using Adam to train a linear model to minimize the empirical logistic loss (or exponential loss). Then our main results can be summarized as the following informal theorem: ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Simplified version of Theorem 4.5). Let $\\{\\eta_{t}\\}_{t=0}^{\\infty}$ \uff0c $\\{\\mathbf{w}_{t}\\}_{t=0}^{\\infty}$ be the sequence of learning rates and iterates of Adam respectively. Suppose that the data set $\\{(\\dot{\\mathbf{x}}_{i},\\dot{y}_{i})\\}_{i=1}^{n}$ is linearly separable, and that $\\mathrm{lim}_{t\\rightarrow}\\,\\eta_{t}=0$ $\\textstyle\\sum_{t=0}^{\\infty}{\\dot{\\eta}}_{t}=\\infty$ . Then under certain conditions, it holds that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\bigg|\\underset{i\\in[n]}{\\operatorname*{min}}\\frac{\\langle\\mathbf w_{t},y_{i}\\cdot\\mathbf x_{i}\\rangle}{\\|\\mathbf w_{t}\\|_{\\infty}}-\\gamma\\bigg|\\leq O\\bigg(\\frac{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=0}^{\\tau-1}\\eta_{\\tau^{\\prime}}}}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}}\\bigg),}&{\\mathrm{(for~logistic~loss)}}\\\\ &{\\bigg|\\underset{i\\in[n]}{\\operatorname*{min}}\\frac{\\langle\\mathbf w_{t},y_{i}\\cdot\\mathbf x_{i}\\rangle}{\\|\\mathbf w_{t}\\|_{\\infty}}-\\gamma\\bigg|\\leq O\\bigg(\\frac{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}}\\bigg),}&{\\mathrm{(for~exponential~loze)}}\\end{array}\n$$Ss) ", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma:=\\operatorname*{max}_{\\|\\mathbf{w}\\|_{\\infty}\\leq1}\\operatorname*{min}_{i\\in[n]}\\langle\\mathbf{w},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}\\end{array}$ is the maximum $\\ell_{\\infty}$ -margin on the training data set. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 shows that, for a general class of learning rate schedules, Adam will eventually achieve the maximum $\\ell_{\\infty}$ -margin on the training data set. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We demonstrate the implicit bias of Adam for solving linear logistic regression with linearly separable data. Specifically, we prove that Adam has an implicit bias towards a maximum $\\ell_{\\infty}$ margin solution when solving linear classification problems. Our result distinguishes Adam from (stochastic) gradient descent with/without momentum, whose implicit bias is towards the maximum $\\ell_{2}$ -margin solution. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Our analysis of Adam covers a broad range of diminishing learning rate schedules. For $\\eta_{t}=\\Theta(t^{-a})$ With $a\\in(0,1]$ , our result demonstrates the following convergence rate: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}\\|_{\\infty}}-\\gamma\\right|\\leq\\left\\{\\begin{array}{l l}{O\\big(t^{-a/2}\\big),}&{\\mathrm{if~}a<2/3;}\\\\ {O\\big(t^{-1/3}\\log t\\big),}&{\\mathrm{if~}a=2/3;}\\\\ {O\\big(t^{1-a}\\big),}&{\\mathrm{if~}2/3<a<1;}\\\\ {O\\big(1/\\log t\\big),}&{\\mathrm{if~}a=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Notably,when $a<1$ , the above rates indicate that the convergence towards the maximum $\\ell_{\\infty}$ margin occurs in polynomial time. This further differentiates Adam from (stochastic) gradient descent with/without momentum in terms of the convergence speed. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Our result focuses on a particularly challenging setting where we ignore the \u201cstability constant $\\epsilon^{\\mathnormal{\\\"}}$ in the Adam algorithm. In practice, the stability constant is by default set as $\\epsilon=10^{\\dot{-}8}$ ,which is almost negligible throughout the optimization process. Therefore, by covering the setting without the stability constant, our theory matches the practical setting better. We demonstrate by simulation that our theory can also correctly characterize the implicit bias of Adam with the stability constant. ", "page_idx": 1}, {"type": "text", "text": "Notation. Given two sequences $\\{x_{n}\\}$ and $\\{y_{n}\\}$ , we denote $x_{n}=O(y_{n})$ if there exist some absolute constant $C_{1}>0$ and $N>0$ such that $|x_{n}|\\leq C_{1}|y_{n}|$ for all $n\\geq N$ . Similarly, we denote $x_{n}=\\Omega(y_{n})$ ", "page_idx": 1}, {"type": "text", "text": "if there exist $C_{2}>0$ and $N>0$ such that $|x_{n}|\\geq C_{2}|y_{n}|$ for all $n>N$ .We say $x_{n}=\\Theta(y_{n})$ if $x_{n}=O(y_{n})$ and $x_{n}=\\Omega(y_{n})$ both holds. We use $\\widetilde{\\cal O}(\\cdot),\\widetilde{\\Omega}(\\cdot)$ , and $\\widetilde{\\Theta}(\\cdot)$ to hide logarithmic factors in these notations respectively. Moreover, we denote $x_{n}\\,=\\,\\mathrm{poly}(y_{n})$ if $x_{n}\\,=\\,{\\bar{O}}(y_{n}^{D})$ for some positive constant $D$ , and $x_{n}\\,=\\,\\mathrm{polylog}(y_{n})$ if $x_{n}\\,=\\,\\mathrm{poly}(\\log(y_{n}))$ . For two scalars $a$ and $b$ ,we denote $a\\vee b=\\operatorname*{max}\\{a,b\\}$ and $a\\wedge b=\\operatorname*{min}\\{a,b\\}$ . For any $n\\in\\mathbb{N}_{+}$ , we use $[n]$ to denote the set $\\{1,2,\\cdots\\,,n\\}$ . For any scalar $c\\in\\mathbb{R}$ \uff0c $\\lceil c\\rceil$ denotes the smallest integer larger or equal to $c$ and $\\lfloor c\\rfloor$ denotes the largest integer smaller or equal to $c$ . For a vector $\\mathbf{a}\\in\\mathbb{R}^{d},\\mathbf{a}[k]$ denote its $k$ -th entry. Finally, $\\mathbf{e}_{i}\\in\\mathbb{R}^{n}$ denotes the $i$ -th basis vector in $\\mathbb{R}^{n}$ ", "page_idx": 2}, {"type": "text", "text": "2 Additional Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Theoretical analyses of Adam and its variants. There has been a line of works studying the properties of Adam and its variants from different aspects. [37] pointed out that there exists simple convex objective functions which Adam may fail to minimize, and proposed a new variant of Adam, the AMSGrad algorithm, which enjoys convergence guarantees in convex optimization. [54, 10, 17, 33, 53, 18] established optimization guarantees of Adam and its variants in non-convex optimization. [29, 19] implemented variance reduction techniques in Adam and proposed new variants of Adam accordingly. [47, 52, 55, 56] studied the generalization performance of Adam and compared it with GD under different learning tasks. [27, 4, 6, 3, 5] tried to explain the performance of Adam by studying the connections between Adam and sign gradient descent. [51] explored the optimization trajectories of Adam from the $\\ell_{\\infty}$ geometry. ", "page_idx": 2}, {"type": "text", "text": "Implicit bias. Classic results [40, 21] demonstrated the iterates of GD will converge to the maximum $\\ell_{2}$ -margin solution in direction on linear logistic regression with linear separable datasets. [31] extended this result under stochastic settings. [14] explored the implicit bias of a general class of optimization methods, containing mirror descent and steepest descent. [23] proposed a primal-dual analysis and derived a faster convergence rate with a larger learning rate compared to [40, 21]. [48] explored the implicit bias of gradient descent at the 'edge of stability\u2019 regime, where the learning rate can be an arbitrarily large constant. [30, 22] showed that $q$ -homogeneous neural network trained by GD will converge to a KKT point of maximum $\\ell_{2}$ -margin optimization problem. [8] established an implicit bias type result for the Lion [9] algorithm in its continuous-time form. There also exist numerous works studying the implicit bias for different problem setting, including matrix factorization models [16, 28, 2, 36], squared loss models [38, 1, 24], weight normalization and batch normalization [49, 7], deep linear neural networks [15, 20], two-layer neural networks [11, 34, 13, 42, 43, 26]. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Settings ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider binary linear classification problems. Specifically, given $n$ training data points $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ where $\\mathbf{x}_{i}\\,\\in\\,\\mathbb{R}^{d}$ and $y_{i}\\;\\in\\;\\{+1,-1\\}$ , we aim to find a coefficient vector w which minimizes the following empirical loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^{n}\\ell\\big(\\langle\\mathbf{w},y_{i}\\cdot\\mathbf{x}_{i}\\rangle\\big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell(\\left\\langle\\mathbf{w},y_{i}\\cdot\\mathbf{x}_{i}\\right\\rangle)$ is the loss function value on the data point $\\left({{\\bf{x}}_{i}},{y_{i}}\\right)$ . In this paper, we consider $\\ell\\in\\{\\ell_{\\mathrm{log}},\\ell_{\\mathrm{exp}}\\}$ ,where $\\ell_{\\log}(z)=\\log(1+e^{-z})$ is the logistic loss function and $\\ell_{\\exp}(z)=e^{-z}$ is the exponential loss function. We consider using Adam to minimize (3.1). Denoting $\\mathbf{m}_{-1}=\\mathbf{v}_{-1}=\\mathbf{0}\\in$ $\\mathbb{R}^{\\dot{d}}$ and starting with initialization $\\mathbf{w}_{0}$ , Adam applies the following iterative formulas: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{m}_{t}=\\beta_{1}\\mathbf{m}_{t-1}+(1-\\beta_{1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t}),}\\\\ &{\\mathbf{v}_{t}=\\beta_{2}\\mathbf{v}_{t-1}+(1-\\beta_{2})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})^{2},}\\\\ &{\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\eta_{t}\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\beta_{1},\\beta_{2}\\in[0,1)$ are the hyperparameters of Adam, and the square $(\\cdot)^{2}$ , square root $({\\sqrt{\\cdot}})$ and division $(\\div)$ above all denote entry-wise calculations. ", "page_idx": 2}, {"type": "text", "text": "Note that in praetice is comon to considerthe variant $\\begin{array}{r}{\\mathbf{w}_{t+1}=\\mathbf{w}_{t}-\\eta_{t}\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}+\\epsilon}}\\end{array}$ , where an aditional term $\\epsilon\\approx10^{-8}$ is added in (3.4) to improve stability. However, in our analysis, we do not consider such a term $\\epsilon$ . This is because in practice, one seldom run Adam until $\\mathbf{v}_{t}$ is around the same level as e. However, by the nature of implicit bias, the result needs to cover infinitely many iterations, and the additional term $\\epsilon$ will eventually significantly affect the result. In fact, a recent work [45] showed that when one considers such an additional $\\epsilon$ term, Adam will be asymptotically equivalent to gradient descent. In comparison, in this paper, we will show that when ignoring $\\epsilon$ Adam has a unique implicit bias that is different from gradient descent. ", "page_idx": 3}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our main result on the implicit bias of Adam in linear classification problems. We first introduce several assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.1. There exists $\\mathbf{w}\\in\\mathbb{R}^{d}$ such that $\\langle\\mathbf{w},y_{i}\\cdot\\mathbf{x}_{i}\\rangle>0$ for all $i\\in[n]$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.1 is a standard assumption in the study of implicit bias of linear models [40, 14, 31, 21, 23, 45, 48]. It can be easily satisfied in the over-parameterized setting where $d\\geq n$ With the linear separability assumption, we can further define the maximum $\\ell_{\\infty}$ -margin: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{max}_{\\|\\mathbf{w}\\|_{\\infty}\\leq1}\\operatorname*{min}_{i\\in[n]}\\langle\\mathbf{w},y_{i}\\cdot\\mathbf{x}_{i}\\rangle.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We also make the following assumption on the initialization $\\mathbf{w}_{\\mathrm{0}}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.2. The initialization $\\mathbf{w}_{0}$ of Adam satisfies that for all $k\\in[d],\\nabla\\mathcal{R}(\\mathbf{w}_{0})[k]^{2}\\geq\\rho.$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.2 ensures that at every finite iteration, the entries of $\\mathbf{v}_{t}$ are strictly positive. We remark that this is a mild assumption: if $\\mathbf{x}_{i}$ \uff0c $i\\in[n]$ are generated from a continuous, non-degenerate distribution, then regardless of the choice of ${\\bf w}_{0}$ $\\zeta\\mathcal{R}(\\mathbf{w}_{0})[k]\\neq0$ with probability 1. Moreover, $\\rho$ will only appear in our results in the form of $\\log(1/\\rho)$ , and therefore, even if $\\rho$ is small, it will not significantly hurt the convergence rates. A similar assumption has also been considered in [50]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.3. $\\{\\eta_{t}\\}_{t=1}^{\\infty}$ are decreasing in $t$ and satisfy $\\textstyle\\sum_{t=0}^{\\infty}\\eta_{t}=\\infty$ $\\operatorname*{lim}_{t\\to\\infty}\\eta_{t}=0$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.3 is a mild and standard assumption of the learning rates $\\{\\eta_{t}\\}_{t=0}^{\\infty}$ that iscommonly considered in the general optimization literature. It has also been considered in recent studies of Adam and its variants [12, 19, 50]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4.4. For all $\\beta\\in(0,1)$ and $c_{1}>0$ there exist $t_{1}\\in\\mathbb{N}_{+}$ and $c_{2}>0$ that only depend on $\\beta,c_{1}$ , such that $\\begin{array}{r}{\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\left(e^{c_{1}\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\right)\\le c_{2}\\eta_{t}}\\end{array}$ for all $t\\geq t_{1}$ ", "page_idx": 3}, {"type": "text", "text": "Although Assumption 4.4 seems non-trivial, we claim it is a fairly mild assumption. In fact, for both small fixed learning rate $\\eta_{t}\\,=\\,\\eta$ and decaylearningrate $\\dot{\\eta_{t}}\\,=\\,(t+2)^{-\\bar{a}}$ With $a\\,\\in\\,(0,1]$ Assumption 4.4 always hold. We formally prove this result in Lemma C.1 in the appendix. ", "page_idx": 3}, {"type": "text", "text": "Now, we state our main theorem about the implicit bias about Adam as follows. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.5. Let $\\{\\mathbf{w}_{t}\\}_{t=0}^{\\infty}$ be the iterates of Adam in (3.2)-(3.4) with $\\beta_{1}\\leq\\beta_{2}$ . In addition, let $\\gamma$ be defined in (4.1) and $B:=\\operatorname*{max}_{i\\in[n]}\\,\\|\\mathbf{x}_{i}\\|_{1}$ . Then under Assumptions 4.1, 4.2, 4.3 and 4.4, there exists $t_{0}=t_{0}(n,d,\\beta_{1},\\beta_{2},\\gamma,B,\\rho,\\dot{\\mathbf{w}_{0}})$ such that ", "page_idx": 3}, {"type": "text", "text": "\u00b7If $\\ell=\\ell_{\\mathrm{exp}}$ , then for all $t\\geq t_{0}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}_{t})\\leq\\frac{\\log2}{n}\\cdot e^{-\\frac{\\gamma}{2}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}},\\;\\mathrm{and}\\;\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}\\|_{\\infty}}-\\gamma\\right|\\leq O\\left(\\frac{\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+d\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u00b7If $\\ell=\\ell_{\\mathrm{log}}$ , then for all $t\\geq t_{0}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}_{t})\\leq\\frac{\\log2}{n}\\cdot e^{-\\frac{\\gamma}{4}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf w_{t},y_{i}\\cdot\\mathbf x_{i}\\rangle}{\\|\\mathbf w_{t}\\|_{\\infty}}-\\gamma\\right|\\leq O\\left(\\frac{\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+d\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau^{\\prime}}}}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we use $O(\\cdot)$ to omit factors that only depend on $\\beta_{1},\\beta_{2},\\gamma,B$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.5 implies that Adam can minimize the loss function to zero, and that the normalized $\\ell_{\\infty}$ -margin achieved by Adam will eventually converge to the maximum $\\ell_{\\infty}$ -margin of the training data set. To address general learning rate schedules, we do not specify a particular convergence rate for either the loss or the margin, nor do we provide an exact formula for $t_{0}$ . However, it can be easily verified that $\\mathcal{R}(\\mathbf{w}_{t})\\leq\\bar{O}\\big(e^{-\\gamma t^{1-a}/4(1-a)}\\big)$ when $\\eta_{t}=(t+2)^{-a}$ with $a<1$ . This loss convergence rate of Adam is much faster than that of (stochastic) gradient descent (with momentum) given a fixed small learning rate, which is of order $O(1/t)$ [40, 31, 45]. In addition, we have $\\overline{{t}}_{0}=\\mathrm{poly}[n,d,(1-\\beta_{1})^{-1}$ $\\left(1-\\beta_{2}\\right)^{-1},\\gamma^{-1},B,\\log(1/\\rho),\\dot{\\mathcal{R}}(\\mathbf{w}_{0})\\right]$ when $\\eta_{t}=(t+2)^{-a}$ with $a<1$ and we defer the derivation details to Appendix B.2. Regarding margin convergence, we will give a set of detailed convergence rate results for different learning rate schedules in Corollary 4.7. ", "page_idx": 4}, {"type": "text", "text": "According to Theorem 4.5, the nature of Adam is vastly different from (stochastic) gradient descent from the perspective of implicit bias: Adam maximizes the $\\ell_{\\infty}$ -margin, while existing works have demonstrated that (stochastic) gradient descent maximizes the $\\ell_{2}$ -margin [40, 31, 21]. Compared with existing works on the implicit bias of adaptive gradient methods [35, 45, 50], our result demonstrates a novel type of implicit bias with accurate convergence rates, which can not been covered in the bywthAawilaly ngeadiised converge to the maximum $\\ell_{2}$ -margin solution. However, the analysis in [45] relies on a positive $\\epsilon$ their proof is based the fact that after a large number of iterations, the entries of $\\mathbf{v}_{t}$ will eventually be much smaller than $\\epsilon$ , and the update of Adam will be similar to gradient descent with momentum. In our analysis, we are able to cover the setting where $\\epsilon=0$ , and our result demonstrates that studying the setting without $\\epsilon$ is essential, as the implicit bias is completely different. In Section 5, we will demonstrate by experiments that our setting matches the practical observations better. ", "page_idx": 4}, {"type": "text", "text": "As we have discussed, Theorem 4.5 implies the convergence of the normalized $\\ell_{\\infty}$ -margin of Adam iterates towards the maximum $\\ell_{\\infty}$ -margin. Since the results cover very general learning rates, the convergence rates are presented in rather complicated formats. However, based on the assumption that $\\textstyle\\sum_{t=0}^{\\infty}\\eta_{t}=\\infty$ $\\operatorname*{lim}_{t\\to\\infty}\\eta_{t}=0$ wecanimmediatlyconcludethefollwing smlifed reult by the Stolz-Cesaro theorem (see Theorem C.8 in the appendix). ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.6. Under the same conditions in Theorem 4.5, it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}\\|_{\\infty}}=\\operatorname*{max}_{\\|\\mathbf{w}\\|_{\\infty}\\leq1}\\operatorname*{min}_{i\\in[n]}\\langle\\mathbf{w},y_{i}\\cdot\\mathbf{x}_{i}\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If there exists aunique maximum $\\ell_{\\infty}$ margin solution $\\begin{array}{r}{\\mathbf{w}^{*}=\\operatorname*{argmax}_{\\lVert\\mathbf{w}\\rVert_{\\infty}\\leq1}\\operatorname*{min}_{i\\in[n]}\\langle\\mathbf{w},\\mathbf{x}_{i}\\rangle}\\end{array}$ , then we have limt\u2192oo Iwl ", "page_idx": 4}, {"type": "text", "text": "We can also investigate the convergence rates of the $\\ell_{\\infty}$ -margin with specific learning rates. The results are summarized in the following Corollary. ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.7. Consider $\\eta_{t}\\,=\\,(t+2)^{-a}$ with $a\\,\\in\\,(0,1]$ . Denote by $\\mathbf{w}_{t}^{\\mathrm{exp}}$ and $\\mathbf{w}_{t}^{\\mathrm{log}}$ the iterates of Adam for $\\ell=\\ell_{\\mathrm{exp}}$ and $\\ell=\\ell_{\\mathrm{log}}$ respectively.Suppose that $\\beta_{1}\\,\\leq\\,\\beta_{2}$ and Adam starts with initialization ${\\bf w}_{0}$ . Let $B:=\\,\\operatorname*{max}_{i\\in[n]}\\,\\|\\mathbf{x}_{i}\\|_{1}$ . Then under Assumptions 4.1 and 4.2, there exists $t_{0}=t_{0}(n,d,\\beta_{1},\\beta_{2},\\gamma,B,\\rho,{\\mathbf w}_{0})$ such that for all $t\\geq t_{0}$ , the following results hold: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{exp}},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{exp}}\\|_{\\infty}}-\\gamma\\right|,\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{log}},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{log}}\\|_{\\infty}}-\\gamma\\right|\\leq O\\left(\\frac{d}{t^{a/2}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u00b7If $\\begin{array}{r}{a=\\frac{2}{3}}\\end{array}$ \uff0c", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{i\\in[n]}{\\operatorname*{min}}\\,\\frac{\\left\\langle\\mathbf{w}_{t}^{\\mathrm{exp}},y_{i}\\cdot\\mathbf{x}_{i}\\right\\rangle}{\\left\\|\\mathbf{w}_{t}^{\\mathrm{exp}}\\right\\|_{\\infty}}-\\gamma\\right|\\le O\\left(\\frac{d\\cdot\\log t+\\log n+\\log\\mathcal{R}(\\mathbf{w}_{0})+\\left[\\log(1/\\rho)\\right]^{1/3}}{t^{1/3}}\\right),}\\\\ &{\\left|\\underset{i\\in[n]}{\\operatorname*{min}}\\,\\frac{\\left\\langle\\mathbf{w}_{t}^{\\mathrm{log}},y_{i}\\cdot\\mathbf{x}_{i}\\right\\rangle}{\\left\\|\\mathbf{w}_{t}^{\\mathrm{log}}\\right\\|_{\\infty}}-\\gamma\\right|\\le O\\left(\\frac{d\\cdot\\log t+n d+n\\mathcal{R}(\\mathbf{w}_{0})+\\left[\\log(1/\\rho)\\right]^{1/3}}{t^{1/3}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u00b7 If ${\\frac{2}{3}}<a<1$ \uff0c", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf w_{t}^{\\mathrm{exp}},y_{i}\\cdot\\mathbf x_{i}\\rangle}{\\|\\mathbf w_{t}^{\\mathrm{exp}}\\|_{\\infty}}-\\gamma\\right\\rvert\\leq O\\left(\\frac{d+\\log n+\\log\\mathcal{R}(\\mathbf w_{0})+\\left[\\log(1/\\rho)\\right]^{1-a}}{t^{1-a}}\\right),}\\\\ &{\\displaystyle\\left\\lvert\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf w_{t}^{\\mathrm{log}},y_{i}\\cdot\\mathbf x_{i}\\rangle}{\\|\\mathbf w_{t}^{\\mathrm{log}}\\|_{\\infty}}-\\gamma\\right\\rvert\\leq O\\left(\\frac{d+n d^{\\frac{2(1-a)}{a}}+n\\mathcal{R}(\\mathbf w_{0})+\\left[\\log(1/\\rho)\\right]^{1-a}}{t^{1-a}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\u00b7If $a=1$ \uff0c", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{exp}},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{exp}}\\|_{\\infty}}-\\gamma\\right\\rvert\\leq O\\left(\\frac{d+\\log n+\\log\\mathcal{R}(\\mathbf{w}_{0})+\\log\\log(1/\\rho)}{\\log t}\\right),}\\\\ &{\\displaystyle\\left\\lvert\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{log}},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{log}}\\|_{\\infty}}-\\gamma\\right\\rvert\\leq O\\left(\\frac{d+n\\log d+n\\mathcal{R}(\\mathbf{w}_{0})+\\log\\log(1/\\rho)}{\\log t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Corollary 4.7 comprehensively presents the convergence rate of the $\\ell_{\\infty}$ -margin for different learning rates. It also indicates that the margin convergence rates for $\\ell_{\\mathrm{exp}}$ and $\\ell_{\\mathrm{log}}$ are of the same order of $t$ . Notably, for $a<1$ , the normalized $\\ell_{\\infty}$ -margin converges in polynomial time. This clearly distinguishes Adam from (stochastic) gradient descent with/without momentum, for which the normalized $\\ell_{2}$ -margin converges at a speed $O(1/\\log t)$ [40, 20, 45]. We note that a recent work [46] proposed a novel algorithm named progressive rescaling gradient descent that can maximize the margin at an exponential rate. Here our focus is different from [46]: our purpose is not to propose new algorithms to achieve better convergence rates, but is to theoretically study the properties of the classic Adam algorithm. We would also like to remark that, although Corollary 4.7 seemingly indicates that $\\eta_{t}=(t+2)^{-2/3}$ is the learning rate schedule with the fastest convergence rate, it does not mean that $\\eta_{t}=(t+2)^{-2/3}$ always converge faster than the other learning rate schedules in all learning tasks. The bounds in Corollary 4.7 are derived under the worst cases, and in practice, we can frequently observe that the margins all converge faster than the bounds in the corollary. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct numerical experiments to verify our theoretical conclusions. We set the samplesize $n=50$ , and dimension $d=50$ .Then the data set $\\{(\\mathbf{x}_{i},y_{i})\\}$ are generated as follows: ", "page_idx": 5}, {"type": "text", "text": "1. $\\mathbf{x}_{i}$ \uff0c $i\\in[n]$ are independently generated from $N(\\mathbf{0},\\mathbf{I})$   \n2. $y_{i},i\\in[n]$ are independently generated from as $+1$ or $-1$ with equal probability. ", "page_idx": 5}, {"type": "text", "text": "Note that for data sets generated following the procedure above, Assumption 4.1 almost surely holds. We can also apply standard convex optimization to calculate the maximum $\\ell_{\\infty}$ -margin $\\gamma$ . In order to make a clearer comparison between Adam and GD, we generate 10 independent sets of data, and we select the dataset with the most significant difference in the directions of the maximum $\\ell_{2}$ -margin solution and maximum $\\ell_{\\infty}$ -margin solution. We then run the experiments on this selected data set. Throughout our experiments, for gradient descent with momentum, we set the momentum parameter as $\\beta_{1}\\,=\\,0.9$ , and for Adam, we set $\\beta_{1}\\,=\\,0.9$ $\\beta_{2}\\,=\\,0.99$ . All these hyper-parameter setups are common in practice. All optimization algorithms are initialized with standard Gaussian distribution, and are run for $10^{6}$ iterations. ", "page_idx": 5}, {"type": "text", "text": "We first run GD, GDM, Adam without the stability constant, and Adam with stability constant $\\epsilon=10^{-8}$ to train a linear model minimizing the logistic loss, and compare their normalized $\\ell_{\\infty}$ margin and normalized $\\ell_{2}$ -margin. The results are given in Figure 1. We can see that the normalized $\\ell_{\\infty}$ -margins of Adam, both with and without $\\epsilon$ , converge to the maximum $\\ell_{\\infty}$ -margin, whereas the normalized $\\ell_{\\infty}$ -margins of GD and GDM do not. In contrast, the normalized $\\ell_{2}$ -margins of GD and GDM converge to the maximum $\\ell_{2}$ -margin, while the $\\ell_{2}$ -margins of Adam, both with and without $\\epsilon$ , do not. By comparing the curves of Adam with and without $\\epsilon$ , we find that they behave similarly and their convergence remains highly stable. This justifies our theoretical setting where we ignore the stability constant in Adam, and demonstrate that our maximum $\\ell_{\\infty}$ -margin implicit bias result derived without $\\epsilon$ characterizes the practical behaviour of Adam more accurately compared with the maximum $\\ell_{2}$ -margin result for Adam with $\\epsilon$ in [45]. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We also run a set of experiments to demonstrate the polynomial time convergence rate of the $\\ell_{\\infty}$ margin. We run experiments on Adam with learning rates $\\eta_{t}=\\Theta(t^{-a})$ for $a\\in\\{0.3,0.5,0.7,1\\}$ and report the log-log plots in Figure 2, where we perform the experiments for Adam with/without the stability constant separately. In the log-log plot, we observe that after a certain number of iterations, curves for $a<1$ almost appear as straight lines, suggesting that the normalized $\\ell_{\\infty}$ -margin converges in polynomial time for $a<1$ , while the curve for $a=1$ exhibits logarithmic behavior, indicating the normalized $\\ell_{\\infty}$ -margin converges logarithmically in $t$ for $a\\,=\\,1$ .Similarly to the previous observations, there is still no significant distinction between Adam with and without $\\epsilon$ further demonstrating that our theoretical setting, which disregards $\\epsilon$ , is reasonable. We also note that in Figure 2, the margin achieved by Adam with $\\bar{\\eta}_{t}=\\Theta(t^{-0.3})$ converges the fastest. However, as we have commented in Section 4, different learning rate schedules may perform differently on different data sets, and it is not necessarily true that $\\eta_{t}=\\Theta(t^{-0.3})$ is always the best learning rate schedule. ", "page_idx": 6}, {"type": "image", "img_path": "xRQxan3WkM/tmp/47a4736263437add0919759b5a4c5a09f0dc9a7597044213bcb68e7aec5ff71c.jpg", "img_caption": ["(a) normalized $\\ell_{\\infty}$ -margin "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "xRQxan3WkM/tmp/c0aed5093d9aa9c1a244e9cb3225a8c840a342fd5b6c984921b26c3ba5651d09.jpg", "img_caption": ["(b) normalized $\\ell_{2}$ -margin "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: Normalized $\\ell_{\\infty}$ -margins and $\\ell_{2}$ -margins achieved by GD, GDM, and Adam with/without the stability constant $\\epsilon$ during training. (a) gives the results of normalized $\\ell_{\\infty}$ -margins, while (b) shows the results of normalized $\\ell_{2}$ -margins. ", "page_idx": 6}, {"type": "image", "img_path": "xRQxan3WkM/tmp/9ab28c472681178b9b095a5c5c0405226ab5b3389c98134e29717c72f9803916.jpg", "img_caption": ["Figure 2: Log-log plots of the normalized $\\ell_{\\infty}$ -margin gaps $\\begin{array}{r}{|\\operatorname*{min}_{i\\in[n]}\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle/\\|\\mathbf{w}_{t}\\|_{\\infty}-\\gamma|}\\end{array}$ versus training iterations. (a) presents the results for Adam with the stability constant $\\epsilon$ , and (b) presents the results for Adam without the stability constant $\\epsilon$ ", "(a) normalized $\\ell_{\\infty}$ -margin gap with $\\epsilon$ "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "xRQxan3WkM/tmp/e238143a684f0a6d8ac4f4d35c89834621798ecf07a8cea3a3002f075e996a66.jpg", "img_caption": ["(b) normalized $\\ell_{\\infty}$ -margin gap without $\\epsilon$ "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "6 Proof Sketch for Theorem 4.5 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we explain how we establish the convergence of the $\\ell_{\\infty}$ -margin of linear models trained by Adam, and provide the sketch proof of Theorem 4.5. For simplicity, here we focus on the case $\\ell=\\ell_{\\mathrm{exp}}$ .Theproof for $\\ell=\\ell_{\\mathrm{log}}$ is almost the same. ", "page_idx": 6}, {"type": "text", "text": "We first introduce several notations. Define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{G}(\\mathbf{w})=-\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime}(\\langle\\mathbf{w},y_{i}\\cdot\\mathbf{x}_{i}\\rangle).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then for $\\ell\\in\\{\\ell_{\\mathrm{exp}},\\ell_{\\mathrm{log}}\\}$ , it is clear that $\\mathcal{G}(\\mathbf{w})>0$ for all $\\mathbf{w}\\in\\mathbb{R}^{d}$ . In the following, we will show that $\\mathcal{G}(\\mathbf{w})$ plays a key role in the convergence and implicit bias analysis. ", "page_idx": 7}, {"type": "text", "text": "Step 1. Accurate characterizations of the first and second moments. Adam algorithm is defined based on the first and second moments ${\\mathbf{m}}_{t}$ and $\\mathbf{v}_{t}$ , which are calculated as exponential moving averages of the historical gradients and squared gradients respectively. A key challenge in studying Adam is to accurately characterize each entry of ${\\mathbf{m}}_{t}$ and $\\mathbf{v}_{t}$ throughout training. We present the following lemma. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6.1. Under the same condition in Theorem 4.5, there exists $t_{1}=t_{1}(\\beta_{1},\\beta_{2},B)$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbf{m}_{t}[k]-(1-\\beta_{1}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\right|\\leq c_{m}\\eta_{t}\\mathcal{G}(\\mathbf{w}_{t}),}\\\\ &{\\left|\\sqrt{\\mathbf{v}_{t}[k]}-\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\left|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\right|\\right|\\leq c_{v}\\sqrt{\\eta_{t}}\\mathcal{G}(\\mathbf{w}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for all $t>t_{1}$ and $k\\in[d]$ , where $c_{m}$ and $c_{v}$ are constants that only depend on $\\beta_{1},\\beta_{2}$ and $B$ ", "page_idx": 7}, {"type": "text", "text": "Since $\\eta_{t}$ \uff0c $\\beta_{1}^{t+1}$ and $\\beta_{2}^{t+1}$ all decrease to zero as $t$ increases, Lemma 6.1 implies that afer a suffcient number of iterations, the entries of ${\\mathbf{m}}_{t}$ and $\\mathbf{v}_{t}$ will be close to the corresponding entries of $\\nabla\\mathcal{R}(\\mathbf{w}_{t})$ and $\\mathbf{\\nabla}|\\nabla\\mathcal{R}(\\mathbf{w}_{t})|$ respectively. Notably, the term $\\mathcal{G}(\\mathbf{w}_{t})$ also appears in the bounds. In fact, deriving such bounds with the factor $\\mathcal{G}(\\mathbf{w}_{t})$ is essential to enable our implicit bias analysis: when the algorithm converges, by definition, $\\mathscr{G}(\\mathbf{w}_{t})$ will also decrease to zero, which implies that the bounds with the factor $\\mathscr{G}(\\mathbf{w}_{t})$ are strictly tighter than the bounds without $\\mathcal{G}(\\mathbf{w}_{t})$ . Lemma 6.1 is one of our key technical contributions. ", "page_idx": 7}, {"type": "text", "text": "Step2. ${\\mathcal{R}}(\\mathbf{w}_{t})$ starts to decrease after a fixed number of iterations. Based on Lemma 6.1, we can analyze the convergence of $\\mathcal{R}(\\mathbf{w}_{t})$ . Specifically, we can show that, after a fixed number of iterations, the training loss function will start to decrease. This result is summarized in the following lemma. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6.2. Under the same condition in Theorem 4.5, there exist $t_{1}=t_{1}(\\beta_{1},\\beta_{2},B)$ such that for all $t>t_{1}$ , it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\mathbf{w}_{t+1})\\leq\\mathcal{R}(\\mathbf{w}_{t})-\\eta_{t}\\gamma\\cdot\\Big(1-C_{1}\\beta_{1}^{t/2}-C_{2}d\\cdot\\big(\\eta_{t}^{\\frac{1}{2}}+\\eta_{t}\\big)\\Big)\\cdot\\mathcal{G}(\\mathbf{w}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $C_{1},C_{2}$ only depend on $\\beta_{1},\\beta_{2},B$ ", "page_idx": 7}, {"type": "text", "text": "Note that by definition, $\\mathcal{G}(\\mathbf{w})>0$ for all $\\mathbf{w}\\in\\mathbb{R}^{d}$ . Therefore, Lemma 6.2 implies that $\\mathcal{R}(\\mathbf{w}_{t})$ starts to decrease after a fixed number of iterations, and gives a bound on the decreasing speed. We remark that the proof of Lemma 6.2 is highly non-trivial. Although we have related ${\\bf m}_{t}$ and $\\mathbf{v}_{t}$ to the loss gradient $\\nabla\\mathcal{R}(\\mathbf{w}_{t})$ in Lemma 6.1, the fact that $\\mathbf{w}_{t+1}$ is updated according to the entry-wise ratio $\\mathbf{m}_{t}/\\sqrt{\\mathbf{v}_{t}}$ still introduces challenges: under our problem setting, it is entirely possible that at a certain iteration, a certain entry of $\\nabla\\mathcal{R}\\bar{(}{\\mathbf{w}_{t}})$ will exactly equal zero. In this case, the results in Lemma 6.1 can not directly lead to any conclusions about the ratio $\\mathbf{m}_{t}/\\sqrt{\\mathbf{v}_{t}}$ . In our proof, we implement a careful inequality that also takes the historical values of $\\nabla\\mathcal{R}(\\mathbf{w}_{t})$ into consideration. ", "page_idx": 7}, {"type": "text", "text": "Step 3. Lower bound for un-normalized margin. The proof of the implicit bias towards maximum $\\ell_{\\infty}$ -margin also relies on a tight analysis on the un-normalized margin $\\operatorname*{min}_{i\\in[n]}\\left\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\right\\rangle$ during training. We have the following lemma providing a lower bound on the un-normalized margin. ", "page_idx": 7}, {"type": "text", "text": "$t_{0}$ such that $\\begin{array}{r}{\\mathcal{R}(w_{t})\\leq\\frac{\\log2}{n}}\\end{array}$ for $t\\geq t_{0}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[n]}\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle\\ge\\gamma\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}\\cdot\\frac{\\mathcal{G}(\\mathbf{w}_{\\tau})}{{\\mathcal R}(\\mathbf{w}_{\\tau})}-C_{3}d\\Bigg(\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{2}\\Bigg)-C_{4}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for all $t\\geq t_{0}$ ,where $C_{3},C_{4}$ only depend on $\\beta_{1},\\beta_{2},B$ ", "page_idx": 7}, {"type": "text", "text": "Note that thislower bound contains a ngaive termn $\\begin{array}{r}{-C_{3}d\\big(\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{3/2}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{2}\\big)}\\end{array}$ Under our (mild) asumptions on the learning rates,itis entirely possible that $\\begin{array}{r l r}{\\sum_{\\tau=t_{0}}^{\\infty}\\eta_{\\tau}^{3/2},\\sum_{\\tau=t_{0}}^{\\infty}\\eta_{\\tau}^{2}=}&{{}}&{}\\end{array}$ $+\\infty$ and thus the negative term in the lower bound may go to $-\\infty$ .However, we can show that $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathcal{G}(\\mathbf{w}_{t})/\\mathcal{R}(\\bar{\\mathbf{w}_{t}})\\,=\\,1}\\end{array}$ (in fact, for exponential loss, it is obvious that $\\mathcal{G}(\\mathbf{w}_{t})/\\mathcal{R}(\\mathbf{w}_{t})\\,=\\,1)$ Therefore, after a fixed number of iterations, the positive term in the lower bound will dominate, and Lemma 6.3 gives a non-trivial bound. The strength of this lemma lies in its applicability to very general learning rates $\\{\\eta_{t}\\}_{t=1}^{\\infty}$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Step 4. An upper bound of $\\|\\mathbf{w}_{t}\\|_{\\infty}$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Lemma 6.3, we have obtained a lower bound of the un-normalized margin. However, to show the convergenceof the $\\ell_{\\infty}$ -normalized margin, we also need to establish a tight upper bound of $\\|\\mathbf{w}_{t}\\|_{\\infty}$ We present this result in the following lemma, which is inspired by Lemma 4.2 in [50]. ", "page_idx": 8}, {"type": "text", "text": "Lemma 6.4. Suppose that the same conditions in Theorem 4.5 hold. There exist $C_{5},C_{6}$ that only depend on $\\beta_{1},\\beta_{2},B$ , such that the following result hold: if there exists $t_{0}>\\log(1/\\rho)$ such that $\\begin{array}{r}{\\mathcal{R}(w_{t})\\leq\\frac{1}{\\sqrt{B^{2}+C_{5}\\eta_{0}}}}\\end{array}$ for all $t\\geq t_{0}$ , then $\\begin{array}{r}{\\|\\mathbf{w}_{t}\\|_{\\infty}\\leq\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}}\\end{array}$ for all $t>t_{0}$ ", "page_idx": 8}, {"type": "text", "text": "Lemma 6.4 gives an upper bound of $\\|\\mathbf{w}_{t}\\|_{\\infty}$ which mainly depends on =to lr. Note that Lemma 6.3 also gives a lower bound of the un-normalized margin which mainly depends on $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}\\mathcal{G}(\\mathbf{w}_{\\tau})\\big/\\bar{\\mathcal{R}}(\\mathbf{w}_{\\tau})}\\end{array}$ . These two lemmas will be combined to derive the convergence of the normalized margin. ", "page_idx": 8}, {"type": "text", "text": "Step 5. Finalizing the proof. Finally, based on the lemmas established in the previous steps, we can prove Theorem 4.5. We also need the following utility lemma provided by [56]. ", "page_idx": 8}, {"type": "text", "text": "Lemma 6.5 (Lemma A.2 in [56]). For Adam iterations defined in (3.2)-(3.4) with $\\beta_{1}\\leq\\beta_{2}$ and let $\\begin{array}{r}{\\alpha=\\sqrt{\\frac{\\beta_{2}(1-\\beta_{1})^{2}}{(1-\\beta_{2})(\\beta_{2}-\\beta_{1}^{2})}}}\\end{array}$ then $\\mathbf{m}_{t}[k]\\leq\\alpha\\cdot\\sqrt{\\mathbf{v}_{t}[k]}$ for all $k\\in[d]$ ", "page_idx": 8}, {"type": "text", "text": "We are now ready to prove Theorem 4.5 for the case $\\ell=\\ell_{\\mathrm{exp}}$ ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 4.5. By Lemma 6.2, there exists $t_{2}=t_{2}(d,\\beta_{1},\\beta_{2},\\gamma,B)$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}_{t+1})\\leq\\mathcal{R}(\\mathbf{w}_{t})-\\frac{\\gamma\\eta_{t}}{2}\\mathcal{G}(\\mathbf{w}_{t})\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for all $t\\geq t_{2}$ . Note that for $\\ell=\\ell_{\\mathrm{exp}}$ , by definition we have $\\begin{array}{r}{\\mathcal{G}(\\mathbf{w}_{t})=\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle)=}\\end{array}$ ${\\mathcal{R}}(\\mathbf{w}_{t})$ . Therefore, for all $t>t_{2}$ , (6.1) can be re-written as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}_{t+1})\\leq\\left(1-\\frac{\\gamma\\eta_{t}}{2}\\right)\\cdot\\mathcal{R}(\\mathbf{w}_{t})\\leq\\mathcal{R}(\\mathbf{w}_{t})\\cdot e^{-\\frac{\\gamma\\eta_{t}}{2}}\\leq\\mathcal{R}(\\mathbf{w}_{t_{2}})\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{2}}^{t}\\eta_{\\tau}}{2}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Although lexp is not Lipschitz continuous over R, we have R(wta) \u2264 R(wo) eaB\u22652= mby Lemma 6.5 and triangle inequality. Letting Ro=min o B\u00b2+Cm} and $\\begin{array}{r c l}{t_{0}}&{=}&{t_{0}(n,d,\\beta_{1},\\beta_{2},\\gamma,B,\\rho,{\\bf w}_{0})}\\end{array}$ be the frst time such that $\\begin{array}{r l r}{\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}}&{{}\\ge}&{\\frac{2\\alpha B}{\\gamma}\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}\\;+}\\end{array}$ 2logR(wo)-2logRo and to \u2265 - log p. By such definition of to, we can derive that forall t\u2265 to, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\mathbf{w}_{t})\\leq\\mathcal{R}(\\mathbf{w}_{t_{2}})\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}}{2}}\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}}{2}}\\leq\\mathcal{R}_{0}\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which proves the bound on $\\mathcal{R}(\\mathbf{w}_{t})$ Since $t_{0}$ satisfies all the conditions in Lemmas 6.3 and 6.4, by Lemmas 6.3, 6.4 and the fact that $\\mathscr{G}(\\mathbf{w}_{\\tau})=\\mathscr{R}(\\mathbf{w}_{\\tau})$ for exponential loss, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle}{\\|\\mathbf{w}_{t}\\|_{\\infty}}\\geq\\frac{\\gamma\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}-C_{3}d\\big(\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{2}\\big)-C_{4}}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for all $i\\;\\in\\;[n]$ ,where $C_{3},C_{4}$ and $C_{6}$ are constants solely depending on $\\beta_{1},\\beta_{2}$ and $B$ . Now by definition, we have $\\begin{array}{r}{\\gamma\\geq\\operatorname*{min}_{i\\in[n]}\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\rangle/\\|\\mathbf{w}_{t}\\|_{\\infty}.}\\end{array}$ Therefore, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\underset{i\\in[n]}{\\operatorname*{min}}\\,\\frac{\\big\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\big\\rangle}{\\big\\|\\mathbf{w}_{t}\\big\\|_{\\infty}}-\\gamma\\bigg|\\leq\\frac{\\gamma C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+C_{3}d\\Big(\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{2}\\Big)+C_{4}}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq O\\Biggl(\\frac{\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+d\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}}\\Biggr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the second inequality follows by the assumption that $\\eta_{t}\\to0$ . This finishes the proof. ", "page_idx": 8}, {"type": "text", "text": "7  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the implicit bias of Adam under a challenging but insightful setting where the \"stability constant $\\epsilon\"$ is negligible and set to zero. We demonstrate that Adam has an implicit bias converging towards the maximum $\\ell_{\\infty}$ -margin solution, and such convergence occurs in polynomials of time for a general class of learning rates. This result further helps to understand the distinctions between Adam and (stochastic) gradient descent with/without momentum, whose iterates will eventually converge to the maximum $\\ell_{2}$ -margin solution with an $O(1/\\log t)$ convergence rate. This finding aligns with the implicit bias of Adam observed in experiments, for both cases the stability constant $\\epsilon$ is zero and $10^{-{\\bar{8}}}$ . We predict that similar result can be extended to homogeneous neural networks, and we believe that this is a good future work direction. Moreover, since this paper focuses on full-batch Adam, another feasible future work is to investigate the implicit bias of stochastic Adam based on our results. In addition, establishing matching lower bounds for the loss and margin convergence rates for Adam is also an interesting future work direction. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers and area chairs for their helpful comments. DZ is supported in part by NSFC 62306252, Guangdong NSF 2024A1515012444, Hong Kong ECS award 27309624, and the central fund from HKU IDS. YC is supported in part by NSFC 12301657 and Hong Kong ECS award 27308624. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] ALI, A., DOBRIBAN, E. and TIBSHIRANI, R. (2O20). The implicit regularization of stochastic gradient flow for least squares. In 8th International Conference on Learning Representations, ICLR 2020.PMLR.   \n[2] ARORA, S., COHEN, N., HU, W. and LUO, Y. (2019). Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems 32.   \n[3] BALLES, L. and HENNIG, P. (2018). Dissecting adam: The sign, magnitude and variance of stochastic gradients. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018 (J. G. Dy and A. Krause, eds.), vol. 80 of Proceedings of Machine Learning Research. PMLR.   \n[4] BALLES, L., PEDREGOSA, F. and ROUX, N. L. (2020). The geometry of sign gradient descent. CoRR abs/2002.08056.   \n[5] BERNSTEIN, J., WANG, Y., AZIZZADENESHELI, K. and ANANDKUMAR, A. (2O18). SIGNSGD: compressed optimisation for non-convex problems. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, vol. 80 of Proceedings of Machine Learning Research. PMLR.   \n[6] BERNSTEIN, J., ZHAO, J., AZIZZADENESHELI, K. and ANANDKUMAR, A. (2019). signsgd with majority vote is communication efficient and fault tolerant. In 7th International Conference on Learning Representations, ICLR 2019. OpenReview.net.   \n[7] CA0, Y., ZoU, D., L1, Y. and GU, Q. (2023). The implicit bias of batch normalization in linear models and two-layer linear convolutional neural networks. In The Thirty Sixth Annual Conference on Learning Theory. PMLR.   \n[8] CHEN, L., LIU, B., LIANG, K. ET AL. (2023). Lion secretly solves a constrained optimization: As lyapunov predicts. In The Twelfth International Conference on Learning Representations.   \n[9] CHEN, X., LIANG, C., HUANG, D., REAL, E., WANG, K., PHAM, H., DONG, X., LUONG, T., HSIEH, C.-J., LU, Y. ET AL. (2024). Symbolic discovery of optimization algorithms. Advances in Neural InformationProcessing Systems 36.   \n[10] CHEN, X., LIU, S., SUN, R. and HoNG, M. (2019). On the convergence of a class of adamtype algorithms for non-convex optimization. In 7th International Conference on Learning Representations, ICLR 2019.   \n[11]  CHIZAT, L. and BACH, F. (2020). Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on learning theory. PMLR.   \n[12] DE, S., MUKHERJEE, A. and ULLAH, E. (2018). Convergence guarantees for rmsprop and adam in non-convex optimization and an empirical comparison to nesterov acceleration. arXiv preprint arXiv:1807.06766 .   \n[13] FREI, S., VARDI, G., BARTLETT, P., SREBRO, N. and HU, W. (2022). Implicit bias in leaky relu networks trained on high-dimensional data. In The Eleventh International Conference on Learning Representations.   \n[14] GUNASEKAR, S., LEE, J., SOUDRY, D. and SREBRO, N. (2018). Characterizing implicit bias in terms of optimization geometry. In Proceedings of the 35th International Conference on Machine Learning, vol. 80 of Proceedings of Machine Learning Research. PMLR.   \n[15] GUNASEKAR, S., LEE, J. D., SOUDRY, D. and SREBRO, N. (2018). Implicit bias of gradient descent on linear convolutional networks. Advances in neural information processing systems 31.   \n[16] GUNASEKAR, S., WOODWORTH, B. E., BHOJANAPALLI, S., NEYSHABUR, B. and SREBRO, N. (2017). Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems.   \n[17] GUO, Z., XU, Y., YIN, W., JIN, R. and YANG, T. (2021). A novel convergence analysis for algorithms of the adam family. arXiv preprint arXiv:2112.03459 .   \n[18] HoNG, Y. and LIN, J. (2024). On convergence of adam for stochastic optimization under relaxed assumptions. arXiv preprint arXiv:2402.03982 .   \n[19] HUANG, F., LI, J. and HUANG, H. (2021). Super-adam: faster and universal framework of adaptive gradients. Advances in Neural Information Processing Systems 34 9074-9085.   \n[20] J1, Z. and TELGARSKY, M. (2019). Gradient descent aligns the layers of deep linear networks. In 7th International Conference on Learning Representations, ICLR 2019.   \n[21] J1, Z. and TELGARSKY, M. (2019). The implicit bias of gradient descent on nonseparable data. In Proceedings of the Thirty-Second Conference on Learning Theory, vol. 99 of Proceedings of Machine Learning Research. PMLR.   \n[22] JI1, Z. and TELGARKY, M. (2020). Directional convergence and alignment in deep learning. Advances in Neural Information Processing Systems 33 17176-17186.   \n[23]  J1, Z. and TELGARSKY, M. (2021). Characterizing the implicit bias via a primal-dual analysis. In Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide, vol. 132 of Proceedings of Machine Learning Research. PMLR.   \n[24] JIN, H. and MONTUFAR, G. (2023). Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks. Journal of Machine Learning Research 24 1-97.   \n[25] KINGMA, D. P. and BA, J. (2015). Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015.   \n[26]  KoU, Y., CHEN, Z. and GU, Q. (2024). Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data. Advances in Neural Information Processing Systems 36.   \n[27] KUNSTNER, F., CHEN, J., LAVINGTON, J. W. and SCHMIDT, M. (2023). Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be. In The Eleventh International Conference on Learning Representations, ICLR 2023. OpenReview.net.   \n[28] L1, Y, MA, T. and ZHANG, H. (2018). Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory.   \n[29] LIU, M., ZHANG, W., ORABONA, F. and YANG, T. (2020). Adam+: A stochastic method with adaptive variance reduction. arXiv preprint arXiv:2011.11985 .   \n[30] LYU, K. and L1, J. (2019). Gradient descent maximizes the margin of homogeneous neural networks. In 7th International Conference on Learning Representations, ICLR 2019.   \n[31] NACSON, M. S., SREBRO, N. and SoUDRY, D. (2019). Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. In The 22nd International Conference on ArtifcialItligenceand Statistic AISTATS2019,vol 89ofProcedings ofMachineLeaning Research. PMLR.   \n[32] NEYSHABUR, B., TOMIOKA, R. and SREBRO, N. (2014). In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv: 1412.6614 .   \n[33] PAN, Y. and L1, Y. (2022). Toward understanding why adam converges faster than sgd for transformers. In OPT 2022: Optimization for Machine Learning (NeuriPS 2022 Workshop).   \n[34]  PHUONG, M. and LAMPERT, C. H. (2020). The inductive bias of relu networks on orthogonally separable data. In 8th International Conference on Learning Representations, ICLR 2020.   \n[35] QIAN, Q. and Q1AN, X. (2019). The implicit bias of adagrad on separable data. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019.   \n[36] RAZIN, N. and CoHEN, N. (2020). Implicit regularization in deep learning may not be explainable by norms. Advances in neural information processing systems 33 21174-21187.   \n[37] REDDI, S. J., KALE, S. and KUMAR, S. (2018). On the convergence of adam and beyond. In 6th International Conference on Learning Representations, ICLR 2018.   \n[38] RosAsco, L. and VILLA, S. (2015). Learning with incremental iterative regularization. Advances in neural information processing systems 28.   \n[39] SHALEV-SHWARTZ, S. and SINGER, Y. (2010). On the equivalence of weak learnability and linear separability: new relaxations and efficient boosting algorithms. Mach. Learn. 80 141-163.   \n[40] SOUDRY, D., HOFFER, E., NACSON, M. S., GUNASEKAR, S. and SREBRO, N. (2018). The implicit bias of gradient descent on separable data. J. Mach. Learn. Res. 19 70:1-70:57.   \n[41] TELGARSKY, M. (2013). Margins, shrinkage, and boosting. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, vol. 28 of JMLR Workshop and Conference Procedings. JMLR.org.   \n[42] VARDI, G., SHAMIR, O. and SREBRO, N. (2022). On margin maximization in linear and relu networks. Advances in Neural Information Processing Systems 35 37024-37036.   \n[43] VARDI, G., YEHUDAI, G. and SHAMIR, O. (2022). Gradient methods provably converge to non-robust networks. Advances in Neural Information Processing Systems 35 20921-20932.   \n[44] WANG, B., MENG, Q., CHEN, W. and LIU, T.-Y. (2021). The implicit bias for adaptive optimization algorithms on homogeneous neural networks. In International Conference on Machine Learning. PMLR.   \n[45] WANG, B., MENG, Q., ZHANG, H., SUN, R., CHEN, W., MA, Z.-M. and LIU, T.-Y. (2022). Does momentum change the implicit regularization on separable data? In Advances in Neural Information Processing Systems, vol. 35. Curran Associates, Inc.   \n[46] WANG, M., MIN, Z. and WU, L. (2024). Achieving margin maximization exponentially fast via progressive norm rescaling. In Forty-first International Conference on Machine Learning.   \n[47] WILSON, A. C., ROELOFS, R., STERN, M., SREBRO, N. and RECHT, B. (2017). The marginal value of adaptive gradient methods in machine learning. Advances in neural information processing systems 30.   \n[48] WU, J., BRAVERMAN, V. and LEE, J. D. (2023). Implicit bias of gradient descent for logistic regression at the edge of stability. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023.   \n[49] WU, X., DOBRIBAN, E., REN, T., WU, S., LI, Z., GUNASEKAR, S., WARD, R. and LIU, Q. (2020). Implicit regularization and convergence for weight normalization. Advances in Neural Information Processing Systems 33 2835-2847.   \n[50] XIE, S. and L1, Z. (2024). Implicit bias of adamw: $\\ell_{\\infty}$ -norm constrained optimization. In Forty-first International Conference on Machine Learning.   \n[51] XIE, S., MOHAMADI, M. A. and LI, Z. (2024). Adam exploits $\\ell_{\\infty}$ -geometry of loss landscape via coordinate-wise adaptivity. arXiv preprint arXiv:2410.08198 .   \n[52] ZHANG, J., KARIMIREDDY, S. P., VEIT, A., KIM, S., REDDI, S., KUMAR, S. and SRA, S. (2020). Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems 33 15383-15393.   \n[53] ZHANG, Y., CHEN, C., SHI, N., SUN, R. and LUO, Z.-Q. (2022). Adam can converge without any modification on update rules. Advances in neural information processing systems 35 28386-28399.   \n[54] ZHOU, D., CHEN, J., CAO, Y., TANG, Y., YANG, Z. and GU, Q. (2024). On the convergence of adaptive gradient methods for nonconvex optimization. Transactions on Machine Learning Research.   \n[55] ZHOU, P., FENG, J., MA, C., XIONG, C., HO1, S. C. H. ET AL. (2020). Towards theoretically understanding why sgd generalizes better than adam in deep learning. Advances in Neural Information Processing Systems 33 21285-21296.   \n[56] ZoU, D., CA0, Y., L1, Y. and GU, Q. (2023). Understanding the generalization of adam in learning neural networks with proper regularization. In The Eleventh International Conference on Learning Representations, ICLR 2023. OpenReview.net. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "AProof in Section 6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Preliminary Lemma ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "It can be figured out that only the product $y_{i}\\cdot\\mathbf{x}_{i}$ is concerned in (3.1). Therefore, we define $\\mathbf{z}_{i}=y_{i}\\cdot\\mathbf{x}_{i}$ then (3.1) could be re-written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We also define $\\mathbf{Z}\\ =\\ [\\mathbf{z}_{1},\\mathbf{z}_{2},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{z}_{n}]^{\\top}\\ \\ \\in\\ \\ \\mathbb{R}^{n\\times d}$ to denote our sample with $i\\cdot$ -th row is $\\mathbf{z}_{i}^{\\top}$ , and we will use $\\textbf{Z}\\in\\ \\mathbb{R}^{n\\times d}$ to denote the data sample instead of $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ in the following paragraphs. Then $\\begin{array}{r l r}{\\mathcal{G}(\\mathbf{w})}&{=}&{\\frac{1}{n}\\sum_{i=1}^{n}-\\ell^{\\prime}(\\left<\\mathbf{w},\\mathbf{z}_{i}\\right>)}\\end{array}$ , and we introduce $L^{\\prime}(\\mathbf{w})\\;\\;=$ $\\begin{array}{r}{[-\\frac{1}{n}\\ell^{\\prime}(\\langle\\mathbf{w},\\mathbf{z}_{1}\\rangle),-\\frac{1}{n}\\ell^{\\prime}(\\langle\\mathbf{w},\\mathbf{z}_{2}\\rangle),\\cdots,-\\frac{1}{n}\\ell^{\\prime}(\\langle\\mathbf{\\dot{w}},\\mathbf{z}_{n}\\rangle)]^{\\top}}\\end{array}$ , which means $\\mathcal{G}(\\mathbf{w})=\\|L^{\\prime}(\\mathbf{w})\\|_{1}$ . The following lemma reveals the relationship between the maximum margin $\\gamma$ and $\\nabla\\mathcal{R}(\\mathbf{w}_{t})$ by duality. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. For data sample $\\mathbf{Z}$ under Assumption 4.1 and maximum $\\ell_{\\infty}$ -margin $\\gamma$ as defined in (4.1), then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\gamma\\leq\\operatorname*{min}_{r\\in\\Delta_{n}}\\|\\mathbf{Z}^{\\top}r\\|_{1},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta_{n}=\\{r|r\\in\\mathbb{R}^{n},\\sum_{i=1}^{n}r_{i}=1,r_{i}\\geq0\\}}\\end{array}$ is the $n$ dimensional simplex. ", "page_idx": 13}, {"type": "text", "text": "Remark A.2. Since $\\frac{L^{\\prime}(\\mathbf{w}_{t})}{\\mathcal{G}(\\mathbf{w}_{t})}\\;\\in\\;\\Delta_{n}$ ,and $\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\ =\\ \\mathbf{Z}^{\\top}L^{\\prime}(\\mathbf{w}_{t})$ wealwayshave $\\gamma\\mathcal{G}(\\mathbf{w}_{t})~\\leq$ $\\|\\nabla\\mathcal{R}({\\bf w}_{t})\\|_{1}$ , which is the essence for proving the convergence direction of gradient-based algorithms. This result was also proposed in [39, 41, 14, 21, 23]. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma A.1. Firstly, we introduce a definition of indicator function $\\iota(\\cdot)$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\iota_{E}(z)=\\left\\{\\!\\!\\begin{array}{l l}{0,\\mathrm{if}\\;z\\in E;}\\\\ {+\\infty,\\mathrm{if}\\;z\\notin E,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $E$ is any set. Let $f(\\pmb{r})\\,=\\,\\iota_{\\Delta_{n}}(\\pmb{r})$ and $g(z)\\,=\\,\\|z\\|_{1}$ , then we could derive that $f^{*}(\\pmb{r}^{*})=$ $\\operatorname*{max}_{i\\in[n]}\\langle\\mathbf{e}_{i},\\mathbf{\\boldsymbol{r}}^{*}\\rangle$ is the dual function of $f(r)$ and $g^{*}(z^{*})=\\iota_{\\|z^{*}\\|\\infty}{\\le}1$ is the dual function of $g(z)$ Then by Fenchel- Young inequality, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r\\in\\Delta_{n}}{\\operatorname*{min}}\\|\\mathbf{Z}^{\\top}r\\|_{1}=\\underset{r\\in\\mathbb{R}^{n}}{\\operatorname*{min}}\\left[f(r)+g(\\mathbf{Z}^{\\top}r)\\right]\\geq\\underset{\\mathbf{w}\\in\\mathbb{R}^{d}}{\\operatorname*{max}}\\Big[-f^{*}(\\mathbf{Z}\\mathbf{w})-g^{*}(-\\mathbf{w})\\Big]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\mathbf{w}\\in\\mathbb{R}^{d}}{\\operatorname*{max}}\\Big[-\\underset{i\\in[n]}{\\operatorname*{max}}\\,\\mathbf{e}_{i}^{\\top}\\mathbf{Z}\\mathbf{w}-\\iota_{\\|\\mathbf{w}\\|_{\\infty}\\leq1}\\Big]=\\underset{\\|\\mathbf{w}\\|_{\\infty}\\leq1}{\\operatorname*{max}}\\underset{i\\in[n]}{\\operatorname*{min}}\\,\\mathbf{e}_{i}^{\\top}\\mathbf{Z}\\mathbf{w}=\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Lemma 6.5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first introduce the proof of Lemma 6.5 since it will be used for further proof of other lemmas. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 6.5. By Cauchy-Schwartz inequality, we could derive an upper bound for $\\mathbf{m}_{t}[k]$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbf{m}_{t}[k]|=\\big|\\beta_{1}\\mathbf{m}_{t-1}[k]+(1-\\beta_{1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|\\leq\\displaystyle\\sum_{\\tau=0}^{t}\\beta_{1}^{\\tau}(1-\\beta_{1})\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]\\big|}\\\\ &{\\qquad=\\displaystyle\\sum_{\\tau=0}^{t}\\sqrt{\\beta_{2}^{\\tau}(1-\\beta_{2})}\\frac{\\beta_{1}^{\\tau}(1-\\beta_{1})}{\\sqrt{\\beta_{2}^{\\tau}(1-\\beta_{2})}}\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]\\big|}\\\\ &{\\qquad\\leq\\Big(\\displaystyle\\sum_{\\tau=0}^{t}\\beta_{2}^{\\tau}(1-\\beta_{2})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]^{2}\\Big)^{\\frac{1}{2}}\\Big(\\displaystyle\\sum_{\\tau=0}^{t}\\frac{\\beta_{1}^{2\\tau}(1-\\beta_{1})^{2}}{\\beta_{2}^{\\tau}(1-\\beta_{2})}\\Big)^{\\frac{1}{2}}}\\\\ &{\\qquad\\leq\\alpha\\sqrt{\\mathbf{v}_{t}[k]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The last inequality is from ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}[k]=\\sum_{\\tau=0}^{t}\\beta_{2}^{\\tau}(1-\\beta_{2})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t}\\frac{\\beta_{1}^{2\\tau}(1-\\beta_{1})^{2}}{\\beta_{2}^{\\tau}(1-\\beta_{2})}\\leq\\frac{(1-\\beta_{1})^{2}}{1-\\beta_{2}}\\sum_{\\tau=0}^{\\infty}\\Big(\\frac{\\beta_{1}^{2}}{\\beta_{2}}\\Big)^{\\tau}=\\frac{\\beta_{2}(1-\\beta_{1})^{2}}{(1-\\beta_{2})(\\beta_{2}-\\beta_{1}^{2})}=\\alpha^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 14}, {"type": "text", "text": "A.3Proof of Lemma 6.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 6.1. Let \u03b1 = \u221a (-Ba)(B2-\u03b2) be defined in Lemma 6.5. For $\\mathbf{m}_{t}[k]$ , it could be rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{m}_{t}[k]=\\displaystyle\\sum_{\\tau=0}^{t}\\beta_{1}^{\\tau}(1-\\beta_{1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]}\\\\ &{\\quad\\quad=(1-\\beta_{1}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]+\\displaystyle\\sum_{\\tau=0}^{t}(1-\\beta_{1})\\beta_{1}^{\\tau}\\cdot\\Big(\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]-\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore the difference between $\\mathbf{m}_{t}[k]$ and $(1-\\beta_{1}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]$ can be bounded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{m}_{t}[k]-(1-\\beta_{1}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\Big|=\\Bigg|\\displaystyle\\sum_{\\tau=0}^{t}(1-\\beta_{1})\\beta_{1}^{\\tau}\\cdot\\Big(\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]-\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\Big)\\Bigg|}&{}\\\\ {=\\Bigg|\\displaystyle\\sum_{\\tau=0}^{t}(1-\\beta_{1})\\beta_{1}^{\\tau}\\Big(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big[\\ell^{\\prime}(\\langle\\mathbf{w}_{t-\\tau},\\mathbf{z}_{i}\\rangle)-\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)\\big]\\cdot\\mathbf{z}_{i}[k]}\\\\ {\\leq\\displaystyle\\sum_{\\tau=0}^{t}(1-\\beta_{1})\\beta_{1}^{\\tau}\\Big(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big|\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)\\big|\\left|\\frac{\\ell^{\\prime}(\\langle\\mathbf{w}_{t-\\tau},\\mathbf{z}_{i}\\rangle)}{\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)}-1\\right|\\Big|\\mathbf{z}_{i}[k]}\\\\ {\\leq(1-\\beta_{1})B\\sum_{\\tau=0}^{t}\\beta_{1}^{\\tau}\\Big(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big|\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)\\big|\\Big)\\left(e^{\\alpha B\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\right)}\\\\ {\\leq(1-\\beta_{1})B c_{2}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t})=c_{m}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second inequality holds since $|\\mathbf{z}_{i}[k]|\\leq\\|\\mathbf{z}_{i}\\|_{1}\\leq B$ , and for both $\\ell\\in\\{\\ell_{\\mathrm{exp}},\\ell_{\\mathrm{log}}\\}$ ,we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\frac{\\ell^{\\prime}\\left(\\left\\langle\\mathbf w_{t-\\tau},\\mathbf z_{i}\\right\\rangle\\right)}{\\ell^{\\prime}\\left(\\left\\langle\\mathbf w_{t},\\mathbf z_{i}\\right\\rangle\\right)}-1\\Big|\\leq e^{\\vert\\langle\\mathbf w_{t}-\\mathbf w_{t-\\tau},\\mathbf z_{i}\\rangle\\vert}-1\\leq e^{\\Vert\\mathbf w_{t}-\\mathbf w_{t-\\tau}\\Vert_{\\infty}\\Vert\\mathbf z_{i}\\Vert_{1}}-1\\leq e^{\\alpha B\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "by Lemma 6.5 and LemmaC5.The last inequalty holds since $\\begin{array}{r}{\\sum_{\\tau=0}^{t}\\beta_{1}^{\\tau}\\left(e^{\\alpha B\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\right)\\le}\\end{array}$ $c_{2}\\eta_{t}$ by our Assumption 4.4. Similarly for $\\mathbf{v}_{t}[k]$ , we also have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{v}_{t}[k]-(1-\\beta_{2}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]^{2}\\Big|}\\\\ {\\displaystyle=\\bigg|\\sum_{\\tau=0}^{t}(1-\\beta_{2})\\beta_{2}^{\\tau}\\cdot\\Big(\\nabla\\mathcal{R}(\\mathbf{w}_{t-\\tau})[k]^{2}-\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]^{2}\\Big)\\bigg|}\\\\ {\\displaystyle=\\bigg|\\sum_{\\tau=0}^{t}(1-\\beta_{2})\\beta_{2}^{\\tau}\\Big(\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}\\big[\\ell^{\\prime}(\\langle\\mathbf{w}_{t-\\tau},\\mathbf{z}_{i}\\rangle)\\ell^{\\prime}(\\langle\\mathbf{w}_{t-\\tau},\\mathbf{z}_{j}\\rangle)-\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{j}\\rangle)\\big]\\cdot\\mathbf{z}_{i}[k]\\mathbf{z}_{j}[k]\\Big)}\\\\ {\\displaystyle\\leq\\bigg|\\sum_{\\tau=0}^{t}(1-\\beta_{2})\\beta_{2}^{\\tau}\\Big(\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}\\big|\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)\\big|\\Big|\\ell^{\\prime}(\\langle\\mathbf{w}_{t-\\tau},\\mathbf{z}_{j}\\rangle)\\Big|\\Big|\\frac{\\ell^{\\prime}\\big(\\langle\\mathbf{w}_{t-\\tau},\\mathbf{z}_{i}\\rangle\\big)\\ell^{\\prime}\\big(\\langle\\mathbf{w}_{t-\\tau},\\mathbf{z}_{j}\\rangle\\big)}{\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)\\ell^{\\prime}\\big(\\langle\\mathbf{w}_{t},\\mathbf{z}_{j}\\rangle\\big)}-1\\Big|\\mathbf{z}_{i}[k]\\big|\\mathbf{z}_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq3(1-\\beta_{2})B^{2}\\displaystyle\\sum_{\\tau=0}^{t}\\beta_{2}^{\\tau}\\Big(\\frac{1}{n^{2}}\\displaystyle\\sum_{i,j=1}^{n}\\big|\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle)\\big|\\big|\\ell^{\\prime}(\\langle\\mathbf{w}_{t},\\mathbf{z}_{j}\\rangle)\\big|\\Big)\\Big(e^{2\\alpha B\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\Big)}\\\\ &{\\leq3(1-\\beta_{2})B^{2}c_{2}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t})^{2}=c_{v}^{2}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, the second inequality holds since $|\\mathbf{z}_{i}[k]||\\mathbf{z}_{j}[k]|\\,\\le\\,\\|\\mathbf{z}_{i}\\|_{1}\\,\\|\\mathbf{z}_{j}\\|_{1}\\,\\le\\,B^{2}$ , and for both $\\ell\\in$ $\\{\\ell_{\\mathrm{exp}},\\ell_{\\mathrm{log}}\\}$ ,we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\frac{\\ell^{\\prime}\\big(\\big\\langle\\mathbf w_{t-\\tau},\\mathbf z_{i}\\big\\rangle\\big)\\ell^{\\prime}\\big(\\big\\langle\\mathbf w_{t-\\tau},\\mathbf z_{j}\\big\\rangle\\big)}{\\ell^{\\prime}\\big(\\big\\langle\\mathbf w_{t},\\mathbf z_{i}\\big\\rangle\\big)\\ell^{\\prime}\\big(\\big\\langle\\mathbf w_{t},\\mathbf z_{j}\\big\\rangle\\big)}-1\\Big|}\\\\ &{\\leq\\Big(e^{|\\langle\\mathbf w_{t}-\\mathbf w_{t-\\tau},\\mathbf z_{i}\\rangle|}-1\\Big)+\\Big(e^{|\\langle\\mathbf w_{t}-\\mathbf w_{t-\\tau},\\mathbf z_{j}\\rangle|}-1\\Big)+\\Big(e^{|\\langle\\mathbf w_{t}-\\mathbf w_{t-\\tau},\\mathbf z_{i}+\\mathbf z_{j}\\rangle|}-1\\Big)}\\\\ &{\\leq\\Big(e^{\\|\\mathbf w_{t}-\\mathbf w_{t-\\tau}\\|_{\\infty}\\|\\mathbf z_{i}\\|_{1}}-1\\Big)+\\Big(e^{|\\|\\mathbf w_{t}-\\mathbf w_{t-\\tau}\\|_{\\infty}\\|\\mathbf z_{j}\\|_{1}}-1\\Big)+\\Big(e^{|\\|\\mathbf w_{t}-\\mathbf w_{t-\\tau}\\|_{\\infty}\\|\\mathbf z_{i}+\\mathbf z_{j}\\|_{1}}-1\\Big)}\\\\ &{\\leq3\\Big(e^{2\\alpha B\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by Lemma 6.5 and Lemma C.6. The last inequality holds since $\\begin{array}{r}{\\sum_{\\tau=0}^{t}\\beta_{2}^{\\tau}\\left(e^{2\\alpha B\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\right)\\le}\\end{array}$ $c_{2}\\eta_{t}$ by our Assumption 4.4. Now, it remains to show the upper bound for $|\\sqrt{\\mathbf{v}_{t}[k]}-\\sqrt{1-\\beta_{2}^{t+1}}$ $\\left|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\right|$ . Notice that both $\\mathbf{v}_{t}[k]$ and $(1-\\beta_{2}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]^{2}$ are positive and for two positive numbers $a$ and $b$ $|a^{2}-b^{2}|=|a-b||a+b|\\geq|a-b|^{2}$ , therefore we finally conclude that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\sqrt{\\mathbf v_{t}[k]}-\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\left|\\nabla\\mathcal{R}(\\mathbf w_{t})[k]\\right|\\right|\\leq c_{v}\\sqrt{\\eta_{t}}\\cdot\\mathcal G(\\mathbf w_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 15}, {"type": "text", "text": "A.4  Proof of Lemma 6.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before we prove Lemma 6.2, we first introduce and prove Lemma A.3, which will be used for proving Lemma6.2. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. Under the same condition in Theorem 4.5, there exists $t_{1}=t_{1}(\\beta_{1},\\beta_{2},\\gamma)$ , such that when $t>t_{1}$ ,wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\left\\langle\\nabla\\mathcal{R}(\\mathbf{w}_{t}),\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}\\right\\rangle-\\left\\|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\right\\|_{1}\\right\\vert\\leq4\\sqrt{\\frac{\\beta_{1}^{t+1}}{1-\\beta_{2}^{t+1}}}\\cdot\\left\\|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\right\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{d}{\\sqrt{1-\\beta_{2}}}\\Big(\\frac{6c_{v}}{\\sqrt{1-\\beta_{2}^{t+1}}}\\sqrt{\\eta_{t}}+3c_{m}\\eta_{t}\\Big)\\cdot\\mathcal{G}(\\mathbf{w}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $c_{m}$ and $c_{v}$ are both constants which only depend on $\\beta_{1},\\beta_{2}$ and $B$ ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.3. By Lemma 6.1, we could re-write $\\mathbf{m}_{t}[k]$ and $\\sqrt{\\mathbf{v}_{t}[k]}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{m}_{t}[k]=(1-\\beta_{1}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]+c_{m}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\epsilon_{t,m,k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\mathbf v_{t}[k]}=\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\left|\\nabla\\mathcal R(\\mathbf w_{t})[k]\\right|+c_{v}\\sqrt{\\eta_{t}}\\cdot\\mathcal G(\\mathbf w_{t})\\cdot\\epsilon_{t,v,k}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\epsilon_{t,m,k}$ and $\\epsilon_{t,v,k}$ are just some error terms with $|\\epsilon_{t,m,k}|,|\\epsilon_{t,v,k}|\\leq1$ Then we can calculate the inner-product $\\begin{array}{r}{\\langle\\nabla\\mathcal{R}(\\mathbf{w}_{t}),\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}\\rangle}\\end{array}$ for each iteration as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla\\mathcal{R}(\\mathbf{w}_{t}),\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}\\right\\rangle=\\left\\|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\right\\|_{1}+\\sum_{k=1}^{d}\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\cdot\\left(\\frac{\\mathbf{m}_{t}[k]}{\\sqrt{\\mathbf{v}_{t}[k]}}-\\frac{\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]}{|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]|}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, we let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi_{t,k}=\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\cdot\\left(\\frac{\\mathbf{m}_{t}[k]}{\\sqrt{\\mathbf{v}_{t}[k]}}-\\frac{\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]}{|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]|}\\right)}\\\\ &{\\qquad=\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\cdot\\left(\\frac{(1-\\beta_{1}^{t+1})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]+c_{m}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\epsilon_{t,m,k}}{\\sqrt{1-\\beta_{2}^{t+1}}\\cdot|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]|+c_{v}\\sqrt{\\eta_{t}}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\epsilon_{t,v,k}}-\\frac{\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]}{|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]|}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and consider to separate $\\xi_{t,k}$ into two complementary parts. The first part is $\\xi_{t,k}\\mathbb{1}_{A_{t,k}}$ , where $\\begin{array}{r l}&{A_{t,k}=\\Big\\{\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|\\ge2c_{v}\\sqrt{\\eta_{t}}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\big|\\epsilon_{t,v,k}\\big|\\Big\\}.\\mathrm{~While~anot}}\\\\ &{\\mathrm{where~}A_{t,k}^{c}=\\Big\\{\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|<2c_{v}\\sqrt{\\eta_{t}}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\big|\\epsilon_{t,v,k}\\big|\\Big\\}.\\mathrm{~By~}}\\end{array}$ her pat is $\\xi_{t,k}\\mathbb{1}_{A_{t,k}^{c}}$ \uff0c such separation, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\left(*\\right)\\right|=\\left|\\sum_{k=1}^{d}\\xi_{t,k}\\mathbb{1}_{A_{t,k}}+\\xi_{t,k}\\mathbb{1}_{A_{t,k}^{c}}\\right|\\leq\\left|\\sum_{k=1}^{d}\\xi_{t,k}\\mathbb{1}_{A_{t,k}}\\right|+\\left|\\sum_{k=1}^{d}\\xi_{t,k}\\mathbb{1}_{A_{t,k}^{c}}\\right|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We calculate it part by part. For the first part $|\\textstyle\\sum_{k=1}^{d}\\xi_{t,k}\\mathbb{1}_{A_{t,k}}|$ wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{k=1}^{d}\\xi_{t,k}\\mathbb{I}_{A_{t,k}}\\bigg|\\leq\\sum_{k=1}^{d}\\frac{\\displaystyle\\bigg|1-\\beta^{t+1}-\\sqrt{1-\\beta_{2}^{t+1}}\\bigg|\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|^{3}+\\left(c_{m}\\eta_{t}+c_{v}\\sqrt{\\eta_{t}}\\right)\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\cdot\\big|\\mathcal{R}(\\mathbf{w}_{t})\\cdot\\big|^{3}}{\\left(\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|+c_{v}\\sqrt{\\eta_{t}}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\epsilon_{t,v,k}\\right)\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|}}}\\\\ &{\\leq\\sum_{k=1}^{d}\\frac{\\displaystyle\\bigg|1-\\beta^{t+1}-\\sqrt{1-\\beta_{2}^{t+1}}\\bigg|\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|^{3}+\\left(c_{m}\\eta_{t}+c_{v}\\sqrt{\\eta_{t}}\\right)\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\cdot\\big|^{3}}{\\displaystyle\\frac{1}{2}\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|^{2}}}\\\\ &{\\leq4\\sqrt{\\frac{\\beta_{1}^{t+1}}{1-\\beta_{2}^{t+1}}}\\cdot\\Big\\|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\Big\\|_{1}+\\frac{2d}{\\sqrt{1-\\beta_{2}^{t+1}}}\\Big(c_{m}\\eta_{t}+c_{v}\\sqrt{\\eta_{t}}\\Big)\\cdot\\mathcal{G}(\\mathbf{w}_{t}).\\qquad(\\mathrm{A.2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first inequality is derived by triangle inequality and $|\\epsilon_{t,m,k}|,|\\epsilon_{t,v,k}|\\leq1.$ The second inequality holds since $\\begin{array}{r}{\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\left|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\right|+c_{v}\\sqrt{\\eta_{t}}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\cdot\\epsilon_{t,v,k}>\\frac{1}{2}\\sqrt{1-\\beta_{2}^{t+1}}\\cdot\\left|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\right|}\\end{array}$ when $\\mathbb{1}_{A_{t,k}}=1$ . And the last inequality is simply due to an arithmetic result that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|1-\\beta_{1}^{t+1}-\\sqrt{1-\\beta_{2}^{t+1}}\\right|\\leq\\left|1-\\sqrt{1-\\beta_{2}^{t+1}}\\right|+\\beta_{1}^{t+1}\\leq\\sqrt{1-1+\\beta_{2}^{t+1}}+\\beta_{1}^{t+1}\\leq2\\beta_{1}^{\\frac{t+1}{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then for another part $|\\sum_{k=1}^{d}\\xi_{t,k}\\mathbb{1}_{A_{t,k}^{c}}|$ we use the property $\\sqrt{\\mathbf{v}_{t}[k]}\\geq\\sqrt{1-\\beta_{2}}\\cdot\\left|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\right|$ 10 derive an upper bound as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\displaystyle\\sum_{k=1}^{d}\\xi_{t,k}\\mathbb{1}_{A_{t,k}^{c}}\\bigg|\\le\\displaystyle\\sum_{k=1}^{d}\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|\\cdot\\bigg(\\frac{\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|+c_{m}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t})}{\\sqrt{1-\\beta_{2}}\\cdot\\big|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\big|}+1\\bigg)\\mathbb{1}_{A_{t,k}^{c}}}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{d}{\\sqrt{1-\\beta_{2}}}\\Big(\\frac{4c_{v}}{\\sqrt{1-\\beta_{2}^{t+1}}}\\sqrt{\\eta_{t}}+c_{m}\\eta_{t}\\Big)\\cdot\\mathcal{R}(\\mathbf{w}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first inequality is derived by triangle inequality and $|\\epsilon_{t,m,k}|\\leq1$ . The second inequality holds since $\\begin{array}{r}{\\left|\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]\\right|\\leq\\frac{2c_{v}}{\\sqrt{1-\\beta_{2}^{t+1}}}\\sqrt{\\eta_{t}}\\cdot\\mathcal{G}(\\mathbf{w}_{t})}\\end{array}$ m(w) when 1A, 1. Combin the rsults of(2),(A.3) and Lemma C.2, we finally prove finish the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Now, we are ready to prove Lemma 6.2. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 6.2. We upper bound $\\mathcal{R}(\\mathbf{w}_{t+1})$ for $t>t_{1}$ by second-order Taylor expansion as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{\\Omega}(\\mathbf{w}_{t+1})=\\mathcal{R}(\\mathbf{w}_{t})+\\left\\langle\\nabla\\mathcal{R}(\\mathbf{w}_{t}),\\mathbf{w}_{t+1}-\\mathbf{w}_{t}\\right\\rangle+\\frac{1}{2}\\big(\\mathbf{w}_{t+1}-\\mathbf{w}_{t}\\big)^{\\top}\\nabla^{2}\\mathcal{R}\\big(\\mathbf{w}_{t}+\\zeta(\\mathbf{w}_{t+1}-\\mathbf{w}_{t})\\big)\\big(\\mathbf{w}_{t+1}-\\mathbf{w}_{t}\\big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{={\\mathcal{R}}(\\mathbf{w}_{t})-\\left\\langle\\nabla{\\mathcal{R}}(\\mathbf{w}_{t}),\\eta_{t}\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{V}_{t}}}\\right\\rangle+\\frac{1}{2}\\Big(\\eta_{t}\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{V}_{t}}}\\Big)^{\\top}\\nabla^{2}{\\mathcal{R}}\\big(\\mathbf{w}_{t}+\\zeta(\\mathbf{w}_{t+1}-\\mathbf{w}_{t})\\big)\\Big(\\eta_{t}\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{V}_{t}}}\\Big)}\\\\ &{\\le{\\mathcal{R}}(\\mathbf{w}_{t})-\\eta_{t}\\left(1-4\\sqrt{\\frac{\\beta_{t}^{\\dagger+1}}{1-\\beta_{t}^{2}+1}}\\right)\\cdot\\Big\\|\\nabla{\\mathcal{R}}(\\mathbf{w}_{t})\\Big\\|_{1}}\\\\ &{\\quad+\\eta_{t}\\frac{d}{\\sqrt{1-\\beta_{t}^{2}}}\\Big(\\frac{6c_{v}}{\\sqrt{1-\\beta_{t}^{2}+1}}\\sqrt{\\eta_{t}+3c_{m}\\eta_{t}}\\Big)\\cdot{\\mathcal{Q}}(\\mathbf{w}_{t})+\\frac{\\eta_{t}^{2}\\alpha^{2}B^{2}}{2}\\cdot\\operatorname*{max}\\left\\{{\\mathcal{G}}(\\mathbf{w}_{t}),{\\mathcal{G}}(\\mathbf{w}_{t+1})\\right\\}}\\\\ &{\\le{\\mathcal{R}}(\\mathbf{w}_{t})-\\eta_{t}\\gamma\\left(1-4\\sqrt{\\frac{\\beta_{t}^{\\dagger+1}}{1-\\beta_{t}^{2}+1}}\\right)\\cdot{\\mathcal{Q}}(\\mathbf{w}_{t})+\\eta_{t}^{\\frac{3}{2}}\\frac{6c_{v}d}{\\sqrt{(1-\\beta_{2})(1-\\beta_{2}^{t+1})}}\\cdot{\\mathcal{Q}}(\\mathbf{w}_{t})}\\\\ &{\\quad+\\eta_{t}^{2}\\Big(\\frac{\\alpha^{2}B^{2}e^{\\alpha B\\eta_{t}}}{2}+\\frac{3c_{m}d}{\\sqrt{1-\\beta_{2}}}\\Big)\\cdot{\\mathcal{Q}}(\\mathbf{w}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The frst inequality is from Lemma A.3, and for the vector $\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Big(\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}\\Big)^{\\top}\\nabla^{2}\\mathcal{R}(\\mathbf{w})\\Big(\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}\\Big)\\le\\frac{1}{n}\\sum_{i=1}^{n}\\ell^{\\prime\\prime}\\big(\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle\\big)\\Big\\|\\frac{\\mathbf{m}_{t}}{\\sqrt{\\mathbf{v}_{t}}}\\Big\\|_{\\infty}^{2}\\Big\\|\\mathbf{z}_{i}\\Big\\|_{1}^{2}\\le\\alpha^{2}B^{2}\\cdot\\mathcal{G}(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by Lemma C.2 and Lemma 6.5, and $\\mathcal{G}\\big(\\mathbf{w}_{t}+\\zeta(\\mathbf{w}_{t+1}-\\mathbf{w}_{t})\\big)\\,\\leq\\,\\operatorname*{max}\\left\\{\\mathcal{G}(\\mathbf{w}_{t}),\\mathcal{G}(\\mathbf{w}_{t+1})\\right\\}$ from convexity of $\\mathcal{G}(\\mathbf{w})$ . The last inequality is from $\\begin{array}{r}{\\frac{\\mathcal{G}(\\mathbf{w}_{t+1})}{\\mathcal{G}(\\mathbf{w}_{t})}\\,\\le\\,e^{\\alpha B\\eta_{t}}\\,\\le\\,e^{\\alpha B\\eta_{0}}}\\end{array}$ by Lemma C.5 and $\\gamma\\mathcal{G}(\\mathbf{w}_{t})\\leq\\|\\nabla\\mathcal{R}(\\mathbf{w}_{t})\\|_{1}$ by Lemma A.1. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "A.5 Proof of Lemma 6.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 6.3. By Lemma 6.2 and Lemma C.2 , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(\\mathbf{w}_{t+1})\\leq\\mathcal{R}(\\mathbf{w}_{t})\\cdot\\Bigg(1-\\gamma\\eta_{t}\\cdot\\frac{\\mathcal{G}(\\mathbf{w}_{t})}{\\mathcal{R}(\\mathbf{w}_{t})}+\\left(C_{1}\\gamma\\eta_{t}\\beta_{1}^{t/2}+\\eta_{t}^{\\frac{3}{2}}C_{2}d+\\eta_{t}^{2}C_{2}d\\right)\\cdot\\frac{\\mathcal{G}(\\mathbf{w}_{t})}{\\mathcal{R}(\\mathbf{w}_{t})}\\Bigg)}\\\\ &{\\qquad\\qquad\\leq\\mathcal{R}(\\mathbf{w}_{t})\\cdot\\exp\\left(-\\gamma\\eta_{t}\\cdot\\frac{\\mathcal{G}(\\mathbf{w}_{t})}{\\mathcal{R}(\\mathbf{w}_{t})}+C_{1}\\gamma\\eta_{t}\\beta_{1}^{t/2}+C_{2}d\\cdot(\\eta_{t}^{\\frac{3}{2}}+\\eta_{t}^{2})\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{\\log2}{n}\\cdot\\exp\\left(-\\gamma\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}\\cdot\\frac{\\mathcal{G}(\\mathbf{w}_{\\tau})}{\\mathcal{R}(\\mathbf{w}_{\\tau})}+C_{2}d\\Big(\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}^{\\frac{3}{2}}+\\displaystyle\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}^{2}\\Big)+\\frac{C_{1}\\gamma\\eta_{t_{0}}\\beta_{1}^{\\frac{t_{0}+1}{2}}}{1-\\sqrt{\\beta_{1}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $t\\geq t_{0}$ . By Lemma C.4, we can derive that $\\left\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\right\\rangle\\,\\geq0$ for all $i\\,\\in\\,[n]$ and $t\\,\\geq t_{0}$ . Then we have $\\begin{array}{r}{e^{-\\operatorname*{min}_{i\\in[n]}\\left\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\right\\rangle}\\leq\\frac{1}{\\log2}\\operatorname*{max}_{i\\in[n]}\\ell\\big(\\langle\\mathbf{w}_{t},\\mathbf{z}_{i}\\rangle\\big)\\leq\\frac{n}{\\log2}\\mathcal{R}(\\mathbf{w}_{t})}\\end{array}$ by LemaC.3.Plugingtbis result into (A.4) and taking log on both sides, we finish the proof for Lemma 6.3. ", "page_idx": 17}, {"type": "text", "text": "A.6Proof of Lemma 6.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Before we prove Lemma 6.4, we first present and prove Lemma A.4 which will be used for proving Lemma6.4. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. For Adam iterations defined in (3.2)-(3.4) with $\\beta_{1}\\leq\\beta_{2}$ for any $t_{0}\\in\\mathbb{N}_{+}$ $t>t_{0}$ ,and all $k\\in[d]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf w_{t}[k]-\\mathbf w_{t_{0}}[k]\\,{\\le}\\,\\Big(\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}\\Big)\\cdot}\\\\ &{\\overset{\\quad,}{\\underset{\\mathrm{\\footnotesize(}}{\\mathrm{\\it~1}}+\\frac{\\beta_{2}-\\beta_{1}}{1-\\beta_{2}}\\frac{\\sum_{\\tau=t_{0}}^{t-1}\\beta_{1}^{\\tau-t_{0}}\\eta_{\\tau}}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}}+\\frac{(\\beta_{2}-\\beta_{1})(1-\\beta_{1})}{1-\\beta_{2}}\\sum_{\\tau=t_{0}}^{t-1}\\frac{\\eta_{\\tau}\\frac{1-\\beta_{1}^{\\tau-t_{0}}}{1-\\beta_{1}}-\\sum_{\\tau^{\\prime}=1}^{t-\\tau-1}\\eta_{\\tau+\\tau^{\\prime}}\\beta_{1}^{\\tau^{\\prime}-1}}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}}\\log(\\mathbf v_{\\tau}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma A.4. If we consider implementing the Cauchy-Schwartz inequality on the sum of the iterations, we can get, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\mathbf{w}_{t}[k]-\\mathbf{w}_{t_{0}}[k]\\right|=\\left|\\displaystyle\\sum_{r=t_{0}}^{t-1}\\eta_{r}\\frac{\\mathbf{m}_{r}[k]}{\\sqrt{\\mathbf{v}_{r}[k]}}\\right|}\\\\ &{=\\left|\\displaystyle\\sum_{r=t_{0}}^{t-1}\\frac{\\eta_{r}}{\\sqrt{\\mathbf{v}_{r}[k]}}\\right|\\left(\\beta_{1}^{r-t_{0}+1}\\mathbf{m}_{t_{0}-1}[k]+\\displaystyle\\sum_{r=0}^{r-t_{0}}\\beta_{1}^{r}(1-\\beta_{1})\\cdot\\nabla R(\\mathbf{w}_{r-r})[k]\\right)\\right|}\\\\ &{\\le\\left[\\displaystyle\\sum_{r=t_{0}}^{t-1}\\frac{\\eta_{r}}{\\nabla r}\\Big|\\!\\left(\\beta_{1}^{r-t_{0}+1}\\mathbf{m}_{t_{0}-1}[k]^{2}+\\displaystyle\\sum_{r=0}^{r-t_{0}}\\beta_{1}^{r^{\\prime}}(1-\\beta_{1})\\cdot\\nabla R(\\mathbf{w}_{r-r})[k]^{2}\\right)\\right]^{\\frac{1}{2}}}\\\\ &{\\quad\\cdot\\left[\\displaystyle\\sum_{r=t_{0}}^{t-1}\\eta_{r}\\left(\\beta_{1}^{r-t_{0}+1}+\\displaystyle\\sum_{r=0}^{r-t_{0}}\\beta_{1}^{r^{\\prime}}(1-\\beta_{1})\\right)\\right]^{\\frac{1}{2}}}\\\\ &{=\\left[\\displaystyle\\sum_{r=t_{0}}^{t-1}\\left(\\frac{\\eta_{r}}{\\nabla r}\\beta_{1}^{r-t_{0}+1}\\mathbf{m}_{t_{0}-1}[k]^{2}+\\displaystyle\\sum_{r=0}^{r-t_{0}}\\eta_{r}\\beta_{1}^{r^{\\prime}}\\frac{1-\\beta_{1}}{1-\\beta_{2}}\\frac{\\mathbf{v}_{r-r}[k]-\\beta_{2}\\mathbf{v}_{r-r}[k]}{\\mathbf{v}_{r}[k]}\\right)\\right]^{\\frac{1}{2}}\\left(\\displaystyle\\sum_{r=t_{0}}^{t-1}\\eta_{r}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The inequality is by Cauchy-Schwartz inequality and the second equality is from ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\beta_{2})\\cdot\\nabla\\mathcal{R}(\\mathbf{w}_{\\tau-\\tau^{\\prime}})[k]^{2}=\\mathbf{v}_{\\tau-\\tau^{\\prime}}[k]-\\beta_{2}\\mathbf{v}_{\\tau-\\tau^{\\prime}-1}[k],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}\\Big(\\beta_{1}^{\\tau-t_{0}+1}+\\sum_{\\tau^{\\prime}=0}^{\\tau-t_{0}}\\beta_{1}^{\\tau^{\\prime}}(1-\\beta_{1})\\Big)=\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}\\Big(\\beta_{1}^{\\tau-t_{0}+1}+1-\\beta_{1}^{\\tau-t_{0}+1}\\Big)=\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the first part $(*)$ defined in (A.6), we could re-arrange it as, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma_{1}}&{\\leq-\\frac{\\widehat{\\gamma}_{2}^{\\star\\star}(1,\\overline{{S}})}{n_{1}\\varepsilon_{1}^{3}}\\frac{\\gamma_{2}^{\\star\\star\\star\\star}(n_{1},\\overline{{S}})}{\\gamma_{2}}\\Bigg[\\phantom{\\frac{b}{b}}}\\\\ &{\\quad+\\sum_{i=1}^{n}\\sigma_{i}(1-\\delta_{i})\\frac{\\gamma_{1}^{i}}{\\gamma_{3}}(1-\\delta_{i})\\frac{\\gamma_{1}^{i}}{(n_{1}-\\delta_{i})}\\bigg[\\phantom{\\frac{b}{b}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}+\\frac{\\delta}{1-\\delta}\\sum_{j=0}^{k}\\left(\\int_{\\frac{\\pi}{\\sqrt{n}}}^{\\pi/2}\\mathrm{e}^{\\mathrm{i}\\left(1-\\delta\\right)\\cdot\\frac{\\delta}{j-k_{j}}}\\mathrm{e}^{-\\mathrm{i}\\left(1-\\delta\\right)\\cdot\\frac{\\delta}{j-k_{j}}}\\mathrm{e}^{-\\mathrm{i}\\left(\\frac{\\pi}{\\sqrt{n}}\\right)\\cdot\\frac{1}{\\sqrt{n}}}\\right)}\\\\ &{=\\sum_{i=1}^{n}+\\frac{\\delta}{1-\\delta}\\sum_{j=0}^{k}\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{k}\\mathrm{e}^{-\\mathrm{i}\\left(\\frac{\\pi}{\\sqrt{n}}-\\delta\\right)\\cdot\\left(1-\\delta\\right)\\cdot\\frac{1}{j-k_{j}}}\\sum_{i=1}^{k}\\frac{\\sum_{j=0}^{k}\\delta_{i}-1}{\\sigma_{j-n,i}}\\mathrm{e}^{-\\mathrm{i}\\left(1-\\delta\\right)\\cdot\\frac{\\delta}{j-k_{j}}}\\mathrm{e}^{-\\mathrm{i}\\left(2\\delta\\right)\\cdot\\delta}}\\\\ &{=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\delta_{i}-\\frac{\\delta}{1-\\delta}\\sum_{j=0}^{k}\\mathrm{e}^{-\\mathrm{i}\\left(1-\\delta\\right)\\cdot\\delta}}\\\\ &{+\\frac{\\delta}{1-\\delta}\\sum_{i=1}^{n}\\delta_{i}\\biggr(\\sum_{j=0}^{k}\\frac{1}{\\sqrt{n}}-\\delta\\frac{1-\\delta}{1-\\delta}\\sum_{i=0}^{k}\\mathrm{e}^{\\mathrm{i}\\left(1-\\delta\\right)\\cdot\\delta}-\\sum_{n=1}^{k}\\frac{\\sum_{j=0}^{k}\\delta_{i}-1}{\\sigma_{j-n,i}}\\mathrm{e}^{-\\mathrm{i}\\left(1-\\delta\\right)\\cdot\\delta}}\\\\ &{\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\delta_{i}-\\frac{\\delta}{1-\\delta}\\sum_{j=0}^{k}\\left(\\sum_{n=1}^{k}\\frac{1}{\\sqrt{n}}-\\delta\\frac{1-\\delta^{2}-\\delta}{1-\\delta}\\mathrm{e}^{\\mathrm{i}\\left(1-\\delta\\right)\\cdot\\delta}\\right)}\\\\ &{+\\frac{\\\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging (A.7) into (A.6), then we derive the result of (A.5). ", "page_idx": 19}, {"type": "text", "text": "Now we are ready to prove Lemma 6.4. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 6.4. Considering the last two terms on the RHS of (A.7), for the second term, we can upper-bound it as, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\beta_{2}-\\beta_{1}}{1-\\beta_{2}}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}\\beta_{1}^{\\tau-t_{0}}\\le\\frac{\\eta_{t_{0}}(\\beta_{2}-\\beta_{1})}{(1-\\beta_{1})(1-\\beta_{2})},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\eta_{t}$ is decreasing. Let $C_{5}\\ =\\ c_{v}^{2}$ in statement of Lemma A.4. Then for the third term, by Lemma 6.1 and our condition $\\begin{array}{r}{\\bar{\\mathcal{R}}(\\mathbf{w}_{t})\\overset{\\cdot}{\\leq}\\frac{1}{\\sqrt{B^{2}+C_{5}\\eta_{0}}}}\\end{array}$ ,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}[k]\\leq\\nabla\\mathcal{R}(\\mathbf{w}_{t})[k]^{2}+c_{v}^{2}\\eta_{t}\\cdot\\mathcal{G}(\\mathbf{w}_{t})^{2}\\leq(B^{2}+C_{5}\\eta_{0})\\cdot\\mathcal{R}(\\mathbf{w}_{t})^{2}\\leq1\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $k\\in[d]$ , which implies that $\\log(\\mathbf{v}_{t}[k])<0$ for all $t>t_{0}$ . On the other hand, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(\\mathbf{v}_{t}[k])\\geq\\log(\\beta_{2}^{t}(1-\\beta_{2})\\nabla\\mathcal{R}(\\mathbf{w}_{0})[k]^{2})\\geq t\\log\\beta_{2}+\\log(1-\\beta_{2})+\\log\\rho,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta_{\\tau}\\frac{1-\\beta_{1}^{\\tau-t_{0}}}{1-\\beta_{1}}-\\sum_{\\tau^{\\prime}=1}^{t-\\tau-1}\\eta_{\\tau+\\tau^{\\prime}}\\beta_{1}^{\\tau^{\\prime}-1}\\geq\\eta_{\\tau}\\Big(\\frac{1-\\beta_{1}^{\\tau-t_{0}}}{1-\\beta_{1}}-\\sum_{\\tau^{\\prime}=1}^{t-\\tau-1}\\beta_{1}^{\\tau^{\\prime}-1}\\Big)\\geq-\\eta_{\\tau}\\frac{\\beta_{1}^{\\tau-t_{0}}}{1-\\beta_{1}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining these results, we can upper-bound the third term as, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{(\\beta_{2}-\\beta_{1})(1-\\beta_{1})}{1-\\beta_{2}}\\sum_{\\tau=t_{0}}^{t-1}\\left(\\eta_{\\tau}\\frac{1-\\beta_{1}^{\\tau-t_{0}}}{1-\\beta_{1}}-\\sum_{\\tau^{\\prime}=1}^{t-\\tau-1}\\eta_{\\tau+\\tau^{\\prime}}\\beta_{1}^{\\tau^{\\prime}-1}\\right)\\log(\\mathbf{v}_{\\tau}[k])\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\frac{\\beta_{2}-\\beta_{1}}{1-\\beta_{2}}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}\\beta_{1}^{\\tau-t_{0}}\\big(-\\tau\\log\\beta_{2}-\\log(1-\\beta_{2})-\\log\\rho\\big)}\\\\ &{\\le\\displaystyle\\frac{\\eta_{t_{0}}(\\beta_{2}-\\beta_{1})}{(1-\\beta_{1})(1-\\beta_{2})}\\bigg[\\Big(t_{0}+\\frac{1}{1-\\beta_{1}}\\Big)(-\\log\\beta_{2})-\\log(1-\\beta_{2})-\\log\\rho\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging these results into (A.5) with Bernoulli inequality, we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbf{w}_{t}[k]\\vert\\leq\\vert\\mathbf{w}_{t_{0}}[k]\\vert+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+\\frac{\\eta_{t_{0}}(\\beta_{2}-\\beta_{1})}{2(1-\\beta_{1})(1-\\beta_{2})}\\Bigg[\\Big(t_{0}+\\frac{1}{1-\\beta_{1}}\\Big)(-\\log\\beta_{2})-\\log(1-\\beta_{2})-\\log\\rho_{t_{0}}}}\\\\ {{\\displaystyle\\leq\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+\\alpha\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+\\frac{\\eta_{t_{0}}(\\beta_{2}-\\beta_{1})}{2(1-\\beta_{1})(1-\\beta_{2})}\\Bigg[\\Big(t_{0}+\\frac{1}{1-\\beta_{1}}\\Big)(-\\log\\beta_{2})-\\log(1-\\beta_{2})-\\log\\beta_{2}}}\\\\ {{\\displaystyle\\leq\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+\\alpha\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+C_{6}^{\\prime}\\eta_{0}t_{0}\\leq\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $C_{6}$ and $C_{6}^{\\prime}$ are constants only depending on $\\beta_{1},\\beta_{2}$ and $B$ . The second inequality is from triangle inequality of $|\\mathbf{w}_{t_{0}}[k]|$ and Lemma 6.5. The third inequality is from our condition $t_{0}>-\\log\\rho$ and the last inequality is because $\\eta_{t}$ is decreasing. Since the preceding result holds for all $k\\in[d]$ ,it also holds for $\\|{\\bf w}_{t}\\|\\infty$ , which finishes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B Complete Proof for Theorem 4.5 and Calculation Details for Corollary 4.7 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1  Complete Proof for Theorem 4.5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 4.5. For $C_{1},C_{2}$ defined in Lemma 6.2, it's trivial that when $t$ is large we have the folowin iequalies hodic) $\\begin{array}{r c l}{\\beta_{1}^{t/2}\\ \\leq\\ \\frac{1}{6C_{1}}}\\end{array}$ $\\begin{array}{r l r}{\\eta_{t}\\!\\!}&{{}\\le}&{\\!\\!\\operatorname*{min}\\left\\{\\frac{\\gamma^{2}}{36C_{2}^{2}d^{2}},\\frac{\\gamma}{6C_{2}d}\\right\\}}\\end{array}$ oweuse $t_{2}~=~t_{2}(d,\\beta_{1},\\beta_{2},\\gamma,B)$ to denote the first time that all the preceding inequalities hold after $t_{1}\\,=\\,t_{1}(\\beta_{1},\\beta_{2},B)$ in Assumption 4.4. Plugging all aforementioned inequality conditions into Lemma 6.2, we can derive that for all $t\\geq t_{2}$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}_{t+1})\\leq\\mathcal{R}(\\mathbf{w}_{t})-\\frac{\\eta_{t}\\gamma}{2}\\mathcal{G}(\\mathbf{w}_{t}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore we prove that $\\mathcal{R}(\\mathbf{w}_{t+1})<\\mathcal{R}(\\mathbf{w}_{t})$ for all $t\\geq t_{2}$ . For further proof, we separately consider $\\ell=\\ell_{\\mathrm{exp}}$ and $\\ell=\\ell_{\\mathrm{log}}$ ", "page_idx": 20}, {"type": "text", "text": "When $\\ell=\\ell_{\\mathrm{exp}}$ , by definition we have $\\mathcal{G}(\\mathbf{w}_{t})\\,=\\,\\mathcal{R}(\\mathbf{w}_{t})$ . Therefore, for all $t\\geq t_{2}$ , (B.1) can be re-written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}_{t+1})\\leq\\left(1-\\frac{\\gamma\\eta_{t}}{2}\\right)\\cdot\\mathcal{R}(\\mathbf{w}_{t})\\leq\\mathcal{R}(\\mathbf{w}_{t})\\cdot e^{-\\frac{\\gamma\\eta_{t}}{2}}\\leq\\mathcal{R}(\\mathbf{w}_{t_{2}})\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{2}}^{t}\\eta_{\\tau}}{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Although $\\ell_{\\mathrm{exp}}$ is not Lipschitz continuous over $\\mathbb{R}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\mathbf{w}_{t_{2}})\\leq\\mathcal{R}(\\mathbf{w}_{0})\\cdot\\exp\\Big(\\frac{1}{n}\\sum_{i=1}^{n}\\|\\mathbf{x}_{i}\\|_{1}\\|\\mathbf{w}_{t_{2}}-\\mathbf{w}_{0}\\|_{\\infty}\\Big)\\leq\\mathcal{R}(\\mathbf{w}_{0})\\cdot\\exp\\Big(\\alpha B\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}\\Big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r l r}{\\mathcal{R}_{0}}&{=}&{\\operatorname*{min}\\{\\frac{\\log2}{n},\\frac{1}{\\sqrt{B^{2}+c_{v}^{2}\\eta_{0}}}\\}}\\end{array}$ and $t_{0}~~=$ $t_{0}(n,d,\\beta_{1},\\beta_{2},\\gamma,B,t_{1},{\\bf w}_{0})$ $\\begin{array}{r}{\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}\\ge\\frac{2\\alpha B}{\\gamma}\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}+}\\end{array}$ 2lgR(wo)logRe and to \u2265 -log pBy such denitionf to, we can derive that for alt\u2265to, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(\\mathbf{w}_{t})\\leq\\mathcal{R}(\\mathbf{w}_{t_{2}})\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}}{2}}\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}}{2}}\\leq\\mathcal{R}_{0}\\cdot e^{-\\frac{\\gamma\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $t_{0}$ satisfies all the requirements in Lemma 6.3 and Lemma 6.4, we can finally derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf w_{t},y_{i}\\cdot\\mathbf x_{i}\\rangle}{\\|\\mathbf w_{t}\\|_{\\infty}}-\\gamma\\right|\\leq\\frac{\\gamma C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+C_{3}d\\Big(\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{2}\\Big)+C_{4}}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\leq O\\Bigg(\\frac{\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+d\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}}\\Bigg),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since the decay learning rate $\\eta_{t}\\to0$ by Assumption 4.3, and $C_{3},C_{4}$ and $C_{6}$ are constants solely depending on $\\beta_{1},\\beta_{2}$ and $B$ ", "page_idx": 21}, {"type": "text", "text": "When $\\ell=\\ell_{\\mathrm{log}}$ by taking tlescoping sum on thereslt o B.1), we obtain $\\begin{array}{r}{\\gamma\\sum_{\\tau=t_{2}}^{t}\\eta_{\\tau}\\mathcal{G}(\\mathbf{w}_{t})\\leq}\\end{array}$ $2\\mathcal{R}(\\mathbf{w}_{t_{2}})$ $t\\geq t_{2}$ $\\ell_{\\mathrm{log}}$ $\\mathcal{R}(\\mathbf{w}_{t_{1}})\\leq\\mathcal{R}(\\mathbf{w}_{0})+$ $\\alpha B\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}$ $\\begin{array}{r}{\\mathcal{R}_{0}=\\operatorname*{min}\\{\\frac{\\log2}{n},\\frac{1}{\\sqrt{B^{2}+c_{v}^{2}\\eta_{0}}}\\}}\\end{array}$ $t_{0}=t_{0}(n,d,\\beta_{1},\\beta_{2},\\gamma,B,t_{1},{\\bf w}_{0})$ frs timesuch that $\\begin{array}{r}{\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}\\ge\\frac{4\\mathcal{R}(\\mathbf{w}_{0})+4\\alpha B\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}}{\\gamma\\mathcal{R}_{0}}}\\end{array}$ and $t_{0}\\geq-\\log\\rho$ , then we can conclude that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\in[t_{2},t_{0}]}\\mathcal{G}(\\mathbf{w}_{\\tau})\\leq\\frac{2\\mathcal{R}(\\mathbf{w}_{t_{2}})}{\\gamma\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}}\\leq\\frac{2\\mathcal{R}(\\mathbf{w}_{0})+2\\alpha B\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}}{\\gamma\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}}\\leq\\frac{\\mathcal{R}_{0}}{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\begin{array}{r}{\\tau^{\\prime}=\\arg\\!\\operatorname*{min}_{\\tau\\in[t_{2},t_{0}]}\\mathcal{G}(\\mathbf{w}_{\\tau})}\\end{array}$ , then we obtain that $\\left\\langle\\mathbf{z}_{i},\\mathbf{w}_{\\tau^{\\prime}}\\right\\rangle\\geq0$ for all $i\\in[n]$ by Lemma C.4. Moreover, by Lemma C.3 and the monotonicity of $\\mathcal{R}({\\bf w}_{t})$ derived in (B.1), we can conclude that $\\mathcal{R}(\\mathbf{w}_{t})<\\mathcal{R}(\\mathbf{w}_{\\tau^{\\prime}})\\leq2\\mathcal{G}(\\mathbf{w}_{\\tau^{\\prime}})\\leq\\mathcal{R}_{0}$ for all $t>t_{0}$ . Similarly, the inequality $\\mathcal{R}(\\mathbf{w}_{t})\\,<\\,\\mathcal{R}_{0}$ also implies $\\left\\langle\\mathbf{z}_{i},\\mathbf{w}_{t}\\right\\rangle\\,\\geq\\,0$ for all $t\\,>\\,t_{0}$ by Lemma C.4, and correspondingly $\\mathscr{R}(\\mathbf{w}_{t})\\,\\leq\\,2\\mathcal{G}(\\mathbf{w}_{t})$ by Lemma C.3. Then for all $t\\geq t_{0}$ , we can re-write (B.1) as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(\\mathbf{w}_{t})\\leq\\mathcal{R}(\\mathbf{w}_{t-1})-\\frac{\\gamma\\eta_{t-1}}{2}\\cdot\\mathcal{G}(\\mathbf{w}_{t})\\leq\\left(1-\\frac{\\gamma\\eta_{t-1}}{4}\\right)\\cdot\\mathcal{R}(\\mathbf{w}_{t-1})}\\\\ &{\\qquad\\qquad\\leq\\mathcal{R}(\\mathbf{w}_{t-1})\\cdot e^{-\\frac{\\gamma\\eta_{t-1}}{4}}\\leq\\mathcal{R}(\\mathbf{w}_{t_{0}})\\cdot e^{-\\frac{\\gamma}{4}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}}\\leq\\mathcal{R}_{0}\\cdot e^{-\\frac{\\gamma}{4}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By this result and Lemma C.7, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{G}(\\mathbf{w}_{t})}{\\mathcal{R}(\\mathbf{w}_{t})}\\geq1-\\frac{n\\mathcal{R}(\\mathbf{w}_{t})}{2}\\geq1-\\frac{n\\mathcal{R}_{0}\\cdot e^{-\\frac{\\gamma}{4}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}}}{2}\\geq1-e^{-\\frac{\\gamma}{4}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $t_{0}$ satisfies all the requirements in Lemma 6.3 and Lemma 6.4, we can combine Lemma 6.3, Lemma 6.4 and (B.2) and finally derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[n]}{\\operatorname*{min}}\\frac{\\left\\langle\\mathbf{w}_{t},y_{i}\\cdot\\mathbf{x}_{i}\\right\\rangle}{\\left\\Vert\\mathbf{w}_{t}\\right\\Vert\\left\\Vert\\infty}-\\gamma\\right\\vert\\leq\\frac{\\gamma C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+C_{3}d\\left(\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{2}\\right)+C_{4}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}e^{-\\frac{\\tau}{4}\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}}}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}+C_{6}\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq O\\left(\\frac{\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}+d\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}+\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}e^{-\\frac{\\tau}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau}^{\\prime}}}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since the decay learning rate $\\eta_{t}\\to0$ by Assumption 4.3, and $C_{3},C_{4}$ and $C_{6}$ are constants solely depending on $\\beta_{1},\\beta_{2}$ and $B$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "B.2Calculation Details for Corollary 4.7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we use the notation $C_{1},C_{2},C_{3},...$ to denote constants solely depending on $\\beta_{1},\\beta_{2},\\gamma$ and $B$ . While it may seem an abuse of notation as these symbols could be different from Section 6 or denote distinct constants across different formulas, we assert that their exact values are immaterial for our analysis. Therefore, we opt for this shorthand notation for the sake of brevity and clarity, without concern for the precise numerical values of these constants in each instance. ", "page_idx": 21}, {"type": "text", "text": "For given $\\eta_{t}=(t+2)^{-a}$ with $a\\in(0,1]$ , recall the definition of $t_{2}$ in Appendix B.1 to be the first time such that (i). $\\begin{array}{r}{\\beta_{1}^{t/2}\\le\\frac{1}{6C_{1}}}\\end{array}$ (i) $\\begin{array}{r}{\\eta_{t}\\le\\operatorname*{min}\\left\\{\\frac{\\gamma^{2}}{36C_{2}^{2}d^{2}},\\frac{\\gamma}{6C_{2}d}\\right\\}}\\end{array}$ .Ww can deriv that ", "page_idx": 21}, {"type": "equation", "text": "$$\nt_{2}=\\operatorname*{max}\\Bigg\\{-\\frac{2\\log6+2\\log C_{1}}{\\log\\beta_{1}},\\Big(\\frac{36C_{2}^{2}d^{2}}{\\gamma^{2}}\\Big)^{\\frac1a},\\Big(\\frac{6C_{2}d}{\\gamma}\\Big)^{\\frac1a}\\Bigg\\}=C_{3}d^{\\frac2a}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We consider the following four cases, ", "page_idx": 21}, {"type": "text", "text": "\u00b7If $\\ell=\\ell_{\\mathrm{exp}}$ and $a\\,\\in\\,(0,1)$ , recall in Appendix B.1, the definition for $t_{0}$ when $\\ell=\\ell_{\\mathrm{exp}}$ is the frst time such tha t=t# \u2265 $\\begin{array}{r}{\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}\\ge\\frac{2\\alpha B}{\\gamma}\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}+\\frac{2\\log\\mathcal{R}(\\mathbf{w}_{0})-2\\log\\mathcal{R}_{0}}{\\gamma}}\\end{array}$ and $t_{0}\\geq-\\log\\rho$ bysimple approximation from integral of $t^{-a}$ , we can derive that, ", "page_idx": 22}, {"type": "equation", "text": "$$\nt_{0}\\leq C_{1}d^{\\frac{2}{a}}+C_{2}[\\log n]^{\\frac{1}{1-a}}+C_{3}[\\log\\mathcal{R}(\\mathbf{w}_{0})]^{\\frac{1}{1-a}}+C_{4}\\log(1/\\rho).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, we can also derive ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}\\leq C_{1}t_{0}^{1-a}\\leq C_{2}d^{\\frac{2(1-a)}{a}}+C_{3}\\log n+C_{4}\\log\\mathcal{R}(\\mathbf{w}_{0})+C_{5}\\lceil\\log(1/\\rho)\\rceil^{1-a}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $a\\,>\\,{\\frac{2}{3}}$ $\\scriptstyle\\sum_{\\tau=t_{0}}^{\\infty}\\eta_{\\tau}^{\\frac{3}{2}}$ is boundby some costnt, $\\begin{array}{r}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}\\,=\\,O(t^{1-a})}\\end{array}$ and $\\frac{2(1-a)}{a}\\,<\\,1\\,$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{exp}},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{exp}}\\|_{\\infty}}-\\gamma\\right|\\leq O\\left(\\frac{d+\\log n+\\log\\mathcal{R}(\\mathbf{w}_{0})+[\\log(1/\\rho)]^{1-a}}{t^{1-a}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $\\begin{array}{r}{a=\\frac{2}{3}}\\end{array}$ $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}=O(\\log t)}\\end{array}$ $\\begin{array}{r}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}=O(t^{1/3})}\\end{array}$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bigg|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{exp}},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{exp}}\\|_{\\infty}}-\\gamma\\bigg|\\leq O\\bigg(\\frac{d\\cdot\\log t+\\log n+\\log\\mathcal{R}(\\mathbf{w}_{0})+[\\log(1/\\rho)]^{1/3}}{t^{1/3}}\\bigg).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$a\\,<\\,{\\frac{2}{3}}$ $\\textstyle\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}=O(t^{1-\\frac{3a}{2}})$ $\\textstyle\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}=O(t^{1-a})$ $t_{0}^{\\prime}\\ge t_{0}$ $t_{0}$ also have $d\\cdot t_{0}^{\\prime1-3a/2}>\\operatorname*{max}\\{d^{\\frac{2(1-a)}{a}},\\log n,\\log\\mathcal{R}(\\mathbf{w}_{0}),[\\log(1-\\rho)]^{1-a}\\}.$ Letting this new $t_{0}^{\\prime}$ .0 be the $t_{0}$ in our statement of Corollary 4.7, then we conclude that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{exp}},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{exp}}\\|_{\\infty}}-\\gamma\\right|\\leq O\\left(\\frac{d\\cdot t^{1-\\frac{3a}{2}}+d^{\\frac{2(1-a)}{a}}+\\log n+\\log\\mathcal{R}(\\mathbf{w}_{0})+[\\log(1/\\rho)]^{1-a}}{t^{1-a}}\\right)\\leq O\\left(\\frac{d}{t^{a}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u00b7If $\\ell=\\ell_{\\mathrm{exp}}$ and $a=1$ , then by the definition of $t_{0}$ and integral of $t^{-1}$ , we obtain that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log t_{0}\\leq C_{1}\\log d+C_{2}\\log n+C_{3}\\log\\mathcal{R}(\\mathbf{w}_{0})+C_{4}\\log\\log(1/\\rho).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, we can also derive ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}\\leq C_{1}\\log d+C_{2}\\log n+C_{3}\\log\\mathcal{R}(\\mathbf{w}_{0})+C_{4}\\log\\log(1/\\rho).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}=O(\\log t)}\\end{array}$ and $\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}$ is bounded, we obtain ha, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bigg|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{exp}},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{exp}}\\|_{\\infty}}-\\gamma\\bigg|\\leq O\\bigg(\\frac{d+\\log n+\\log\\mathcal{R}(\\mathbf{w}_{0})+\\log\\log(1/\\rho)}{\\log t}\\bigg).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u00b7If $\\ell=\\ell_{\\mathrm{log}}$ and $a\\in(0,1)$ frstlyweuppr bound the ast tem $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau^{\\prime}}}}\\end{array}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau^{\\prime}}}\\leq e^{\\frac{\\gamma(t_{0}+1)^{1-a}}{4(1-a)}}\\sum_{\\tau=t_{0}}^{\\infty}\\frac{1}{(\\tau+2)^{a}}e^{-\\frac{\\gamma(\\tau+1)^{1-a}}{4(1-a)}}\\leq\\frac{4}{\\gamma}e^{\\frac{\\gamma}{4(1-a)}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau^{\\prime}}}}\\\\ {.}\\end{array}$ isalays boundedbya constant.Theonlydifrence etwen $\\ell_{\\mathrm{log}}$ and $\\ell_{\\mathrm{exp}}$ ishowtodeterminethevalueof $t_{0}$ . For $\\ell_{\\mathrm{log}}$ , the formula for $t_{0}$ is the firs time such $\\begin{array}{r}{\\sum_{\\tau=t_{2}}^{t_{0}-1}\\eta_{\\tau}\\ge\\frac{4\\mathcal{R}(\\mathbf{w}_{0})+4\\alpha B\\sum_{\\tau=0}^{t_{2}-1}\\eta_{\\tau}}{\\gamma\\mathcal{R}_{0}}}\\end{array}$ (2= nr and t \u2265 log(1/ p). Similar to the preceding process, we couldderive that ", "page_idx": 22}, {"type": "equation", "text": "$$\nt_{0}\\leq C_{1}n^{\\frac{1}{1-a}}d^{\\frac{2}{a}}+C_{2}n^{\\frac{1}{1-a}}[\\mathscr{R}(\\mathbf{w}_{0})]^{\\frac{1}{1-a}}+C_{3}\\log(1/\\rho).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t_{0}-1}\\eta_{\\tau}\\leq C_{1}n d^{\\frac{2(1-a)}{a}}+C_{2}n\\mathscr{R}(\\mathbf{w}_{0})+C_{3}[\\log(1/\\rho)]^{1-a}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When $\\begin{array}{r}{a\\,>\\,\\frac{2}{3},\\,\\sum_{\\tau=t_{0}}^{\\infty}\\eta_{\\tau}^{\\frac{3}{2}}}\\end{array}$ is bounded by some constant, $\\textstyle\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}\\,=\\,O(t^{1-a})$ and $\\frac{2(1\\!-\\!a)}{a}\\,\\leq\\,1\\,$ \uff0c therefore we conclude that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\log},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\log}\\|_{\\infty}}-\\gamma\\right|\\leq O\\left(\\frac{d+n d^{\\frac{2(1-a)}{a}}+n\\mathcal{R}(\\mathbf{w}_{0})+[\\log(1/\\rho)]^{1-a}}{t^{1-a}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When $\\begin{array}{r}{a=\\frac{2}{3}}\\end{array}$ $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}=O(\\log t)}\\end{array}$ $\\begin{array}{r}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}=O(t^{1/3})}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{log}},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{log}}\\|_{\\infty}}-\\gamma\\right|\\leq O\\left(\\frac{d\\cdot\\log t+n d+n\\mathcal{R}(\\mathbf{w}_{0})+[\\log(1/\\rho)]^{1/3}}{t^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When $a\\,<\\,\\frac{2}{3}$ $\\textstyle\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}=O(t^{1-\\frac{3a}{2}})$ = O(t-\u53f7\uff09 andn=O(t1-a).Under this setting, we could always find a new $t_{0}^{\\prime}\\ge t_{0}$ such that besides the preceding condition for $t_{0}$ we alsghave $d\\cdot t_{0}^{\\prime1-3a/2}>\\operatorname*{max}\\{n d^{\\frac{2(1-a)}{a}},n\\log\\mathcal{R}(\\mathbf{w}_{0}),[\\log(1-\\rho)]^{1-a}\\}$ Letingthis new $t_{0}^{\\prime}$ to be the $t_{0}$ in our statement of Corollary 4.7, then we conclude that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{log}},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{log}}\\|_{\\infty}}-\\gamma\\right|\\leq\\!O\\!\\left(\\frac{d\\cdot t^{1-\\frac{3a}{2}}+n d^{\\frac{2(1-a)}{a}}+n\\mathcal{R}(\\mathbf{w}_{0})+[\\log(1/\\rho)]^{1-a}}{t^{1-a}}\\right)\\leq\\!O\\!\\left(\\frac{d}{t^{a/2}}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "\u00b7If $\\ell=\\ell_{\\mathrm{log}}$ and $a=1$ rstlyweuppeboud thlst $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau^{\\prime}}}}\\end{array}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau^{\\prime}}}\\leq(t_{0}+1)^{\\gamma/4}\\sum_{\\tau=t_{0}}^{\\infty}\\frac{1}{(\\tau+1)^{1+\\gamma/4}}\\leq\\frac{\\gamma}{4}2^{\\gamma/4},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Wwhich imples $\\begin{array}{r}{\\sum_{\\tau=t_{0}}^{t}\\eta_{\\tau}e^{-\\frac{\\gamma}{4}\\sum_{\\tau^{\\prime}=t_{0}}^{\\tau-1}\\eta_{\\tau^{\\prime}}}}\\end{array}$ $t_{0}$ and integral of $t^{-1}$ , we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\log t_{0}\\leq C_{1}n\\log d+C_{2}n\\mathcal{R}(\\mathbf{w}_{0})+C_{3}\\log\\log(1/\\rho).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and similarly ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t_{0}-1}\\leq C_{1}n\\log d+C_{2}n\\mathcal{R}(\\mathbf{w}_{0})+C_{3}\\log\\log(1/\\rho).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{\\tau=0}^{t-1}\\eta_{\\tau}=O(\\log t)}\\end{array}$ and $\\sum_{\\tau=t_{0}}^{t-1}\\eta_{\\tau}^{\\frac{3}{2}}$ is bounded, we obtain that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bigg|\\operatorname*{min}_{i\\in[n]}\\frac{\\langle\\mathbf{w}_{t}^{\\mathrm{log}},\\mathbf{z}_{i}\\rangle}{\\|\\mathbf{w}_{t}^{\\mathrm{log}}\\|_{\\infty}}-\\gamma\\bigg|\\leq O\\bigg(\\frac{d+n\\log d+n\\mathcal{R}(\\mathbf{w}_{0})+\\log\\log(1/\\rho)}{\\log t}\\bigg).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C Technical Lemmas ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1  Lemma for Assumption 4.4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma C.1. Asumption44holds frboth smallfed learing rat $\\begin{array}{r}{\\eta_{t}=\\eta\\le\\frac{1-\\beta}{2c_{1}}}\\end{array}$ andecay learning rate $\\eta_{t}=(t+2)^{-a}$ with $a\\in(0,1]$ ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma C.1. Firstly, we prove it for learning rate $\\begin{array}{r}{\\eta_{t}\\,=\\,\\eta\\,\\le\\,\\frac{1-\\beta}{2c_{1}}}\\end{array}$ , which is a small fixed constant, then we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\left(e^{c_{1}\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\right)=\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\left(e^{c_{1}\\eta\\tau}-1\\right)=\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\sum_{k=1}^{\\infty}\\frac{(c_{1}\\eta\\tau)^{k}}{k!}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{\\tau=0}^{\\infty}\\beta^{\\tau}\\sum_{k=1}^{\\infty}\\frac{(c_{1}\\eta\\tau)^{k}}{k!}=\\sum_{k=1}^{\\infty}\\frac{(c_{1}\\eta)^{k}}{k!}\\sum_{\\tau=0}^{\\infty}\\beta^{\\tau}\\tau^{k}}\\\\ {\\displaystyle\\leq\\frac{1}{1-\\beta}\\sum_{k=1}^{\\infty}\\left(\\frac{c_{1}\\eta}{1-\\beta}\\right)^{k}\\leq\\frac{2c_{1}}{(1-\\beta)^{2}}\\eta}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The penultimate inequalityholds because $\\begin{array}{r}{\\sum_{\\tau=0}^{\\infty}\\beta^{\\tau}\\tau^{k}\\le\\int_{0}^{\\infty}\\beta^{\\tau}\\tau^{k}\\mathrm{d}\\tau=\\frac{k!}{[-\\log(\\beta)]^{k+1}}\\le\\frac{k!}{(1-\\beta)^{k+1}}}\\end{array}$ Therefore, we prove that Assumption 4.4 holds for t E N when nt = 7\u2264 - . Next, we consider the case where decay learning rate $\\begin{array}{r}{\\eta_{t}=\\frac{1}{(t+2)^{a}}}\\end{array}$ With $a\\in(0,1)$ and similarly we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}=\\sum_{\\tau^{\\prime}=1}^{\\tau}\\displaystyle\\frac{1}{(t-\\tau^{\\prime}+2)^{a}}\\le\\displaystyle\\int_{1}^{\\tau+1}\\displaystyle\\frac{1}{(t-\\tau^{\\prime}+2)^{a}}\\mathrm{d}\\tau^{\\prime}}\\\\ {\\displaystyle=\\displaystyle\\frac{1}{1-a}\\Big((t+1)^{1-a}-(t-\\tau+1)^{1-a}\\Big)}\\\\ {\\displaystyle=\\displaystyle\\frac{1}{1-a}\\displaystyle\\frac{\\tau+(t+1)^{1-a}(t-\\tau+1)^{a}-(t+1)^{a}(t-\\tau+1)^{1-a}}{(t+1)^{a}+(t-\\tau+1)^{a}}}\\\\ {\\displaystyle\\le\\displaystyle\\frac{2}{(1-a)}\\displaystyle\\frac{\\tau}{(t+1)^{a}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By this result, we can similarly obtain that for $\\begin{array}{r}{t\\geq\\left(\\frac{4c_{1}}{(1-\\beta)^{2}(1-a)}\\right)^{\\frac{1}{a}}}\\end{array}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=0}^{t}\\beta^{\\tau}\\bigl(e^{c_{1}\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\bigr)\\leq\\displaystyle\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\bigl(e^{\\frac{2c_{1}\\tau}{(1-a)(t+1)^{\\alpha}}}-1\\bigr)=\\displaystyle\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\displaystyle\\sum_{k=1}^{\\infty}\\Big(\\frac{2c_{1}\\tau}{(1-a)(t+1)^{\\alpha}}\\Big)^{k}\\displaystyle\\frac{1}{k!}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{\\tau=0}^{\\infty}\\beta^{\\tau}\\displaystyle\\sum_{k=1}^{\\infty}\\Big(\\frac{2c_{1}\\tau}{(1-a)(t+1)^{a}}\\Big)^{k}\\displaystyle\\frac{1}{k!}=\\displaystyle\\sum_{k=1}^{\\infty}\\Big(\\frac{2c_{1}}{(1-a)(t+1)^{a}}\\Big)^{k}\\displaystyle\\frac{1}{k!}\\displaystyle\\sum_{\\tau=0}^{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{1-\\beta}\\displaystyle\\sum_{k=1}^{\\infty}\\Big(\\frac{2c_{1}}{(1-\\beta)(1-a)(t+1)^{a}}\\Big)^{k}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{4c_{1}}{(1-\\beta)^{2}(1-a)}\\cdot\\displaystyle\\frac{1}{(t+1)^{a}}<\\displaystyle\\frac{8c_{1}}{(1-\\beta)^{2}(1-a)}\\cdot\\eta_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we prove that Assumption 4.4 holds for $\\begin{array}{r}{t\\geq\\left(\\frac{4c_{1}}{(1-\\beta)^{2}(1-a)}\\right)^{\\frac{1}{a}}}\\end{array}$ when $\\begin{array}{r}{\\eta_{t}=\\frac{1}{(t+2)^{a}}}\\end{array}$ (t+2)a. Finally we consider the case where the decay learning rate $\\begin{array}{r}{\\eta_{t}=\\frac{1}{t+2}}\\end{array}$ , and similaly we have $\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}=$ $\\begin{array}{r}{\\sum_{\\tau^{\\prime}=1}^{\\tau}\\frac{1}{t-\\tau^{\\prime}+2}\\leq\\int_{1}^{\\tau+1}\\frac{1}{t-\\tau^{\\prime}+2}\\mathrm{d}\\tau^{\\prime}=\\log(1+\\frac{\\tau}{t-\\tau+1})}\\end{array}$ . By this result, we obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\bigl(e^{c_{1}\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\bigr)\\leq\\displaystyle\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\biggl(\\Bigl(1+\\frac{\\tau}{t-\\tau+1}\\Bigr)^{\\lceil c_{1}\\rceil}-1\\biggr)}\\\\ &{}&{\\displaystyle=\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\displaystyle\\sum_{k=1}^{\\lceil c_{1}\\rceil}\\left(\\!\\!\\begin{array}{c}{\\lceil c_{1}\\rceil}\\\\ {k}\\end{array}\\!\\!\\right)\\Bigl(\\frac{\\tau}{t-\\tau+1}\\Bigr)^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Because $\\left\\lceil c_{1}\\right\\rceil$ and $\\binom{\\lceil c_{1}\\rceil}{k}$ are both absolute constant, i sufces to show that $\\begin{array}{r}{\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\Bigl(\\frac{\\tau}{t-\\tau+1}\\Bigr)^{k}\\leq}\\end{array}$ $\\scriptstyle{\\frac{c_{2}}{t+2}}$ for all $t>t_{1}$ and $k\\geq1$ where $t_{1},c_{2}$ are both constants. Actually, we could split the summation of $\\tau$ into two parts as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\Bigl(\\frac{\\tau}{t-\\tau+1}\\Bigr)^{k}=\\sum_{\\tau=0}^{\\lfloor\\frac{t}{2}\\rfloor}\\beta^{\\tau}\\Bigl(\\frac{\\tau}{t-\\tau+1}\\Bigr)^{k}+\\sum_{\\tau=\\lfloor\\frac{t}{2}\\rfloor+1}^{t}\\beta^{\\tau}\\Bigl(\\frac{\\tau}{t-\\tau+1}\\Bigr)^{k}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the first part, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\tau=0}^{\\lfloor\\frac{t}{2}\\rfloor}\\beta^{\\tau}\\Big(\\frac{\\tau}{t-\\tau+1}\\Big)^{k}\\le\\Big(\\frac{2}{t}\\Big)^{k}\\sum_{\\tau=0}^{\\lfloor\\frac{t}{2}\\rfloor}\\beta^{\\tau}\\tau^{k}\\le\\frac{2^{k}k!}{(1-\\beta)^{k+1}}\\cdot\\frac{1}{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the send part, we couldd astat $t_{1}=t_{1}(c_{1},\\beta)$ such that $\\begin{array}{r}{t^{\\left\\lceil c_{1}\\right\\rceil+2}\\le\\left(\\frac{1}{\\beta}\\right)^{\\frac{t}{2}}}\\end{array}$ for all $t\\geq t_{1}$ since $\\textstyle{\\frac{1}{\\beta}}>1$ . Then for all $t\\geq t_{1}$ , we can derive that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\tau=\\lfloor\\frac{t}{2}\\rfloor+1}^{t}\\beta^{\\tau}\\Big(\\frac{\\tau}{t-\\tau+1}\\Big)^{k}\\leq\\beta^{\\frac{t}{2}}\\sum_{\\tau=\\lfloor\\frac{t}{2}\\rfloor+1}^{t}\\tau^{k}\\leq\\beta^{\\frac{t}{2}}t^{k+1}\\leq\\frac{1}{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The last inequality is by our condition $t\\,\\geq\\,t_{1}$ and $k\\,\\leq\\,\\lceil c_{1}\\rceil$ . Combining these two results and plugging it into (C.1), we finally get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\tau=0}^{t}\\beta^{\\tau}\\bigl(e^{c_{1}\\sum_{\\tau^{\\prime}=1}^{\\tau}\\eta_{t-\\tau^{\\prime}}}-1\\bigr)\\leq\\lceil c_{1}\\rceil\\cdot\\underset{k\\in[\\lceil c_{1}\\rceil]}{\\operatorname*{max}}\\left(\\overset{\\lceil c_{1}\\rceil}{k}\\right)\\cdot\\biggl(\\frac{2^{\\lceil c_{1}\\rceil}\\lceil c_{1}\\rceil!}{(1-\\beta)^{\\lceil c_{1}\\rceil+1}}+1\\biggr)\\cdot\\frac{1}{t}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le2\\lceil c_{1}\\rceil\\cdot\\underset{k\\in[\\lceil c_{1}\\rceil]}{\\operatorname*{max}}\\left(\\overset{\\lceil c_{1}\\rceil}{k}\\right)\\cdot\\biggl(\\frac{2^{\\lceil c_{1}\\rceil}\\lceil c_{1}\\rceil!}{(1-\\beta)^{\\lceil c_{1}\\rceil+1}}+1\\biggr)\\cdot\\eta_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all $t\\geq t_{1}$ , which completes the proof. ", "page_idx": 25}, {"type": "text", "text": "C.2 Properties for Logistic and Exponential Loss Function ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma C.2. For $\\ell\\;\\in\\;\\{\\ell_{\\mathrm{exp}},\\ell_{\\mathrm{log}}\\}$ and any $z~\\in~\\mathbb{R}$ $\\ell^{\\prime\\prime}(z)\\;\\leq\\;|\\ell^{\\prime}(z)|\\;\\leq\\;\\ell(z)$ . For any $z\\ \\geq\\ 0$ $\\ell_{\\log}(z)\\le2|\\ell_{\\log}^{\\prime}(z)|$ ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma C.2. For $\\ell\\,=\\,\\ell_{\\mathrm{exp}}$ \uff0c $|\\ell_{\\mathrm{exp}}^{\\prime}(z)|\\;=\\;\\ell_{\\mathrm{exp}}^{\\prime\\prime}(z)\\;=\\;\\ell_{\\mathrm{exp}}(z)\\;=\\;e^{-z}$ . For $\\ell\\,=\\,\\ell_{\\mathrm{log}}$ we caleulate the drivatives as $\\begin{array}{r}{\\ell_{\\log}^{\\prime}(z)=-\\frac{1}{1+e^{z}}}\\end{array}$ and $\\begin{array}{r}{\\ell_{\\mathrm{log}}^{\\prime\\prime}(z)=\\frac{e^{z}}{(1+e^{z})^{2}}}\\end{array}$ Noticethat ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{z\\to+\\infty}\\ell_{\\log}(z)=\\operatorname*{lim}_{z\\to+\\infty}|\\ell_{\\log}^{\\prime}(z)|=0,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\ell_{\\log}^{\\prime}(z)=-{\\frac{1}{1+e^{z}}}\\leq-{\\frac{1}{2+e^{z}+e^{-z}}}=-\\ell_{\\log}^{\\prime\\prime}(z)={\\bigl(}|\\ell_{\\log}^{\\prime}(z)|{\\bigr)}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore we derive that $\\ell_{\\log}^{\\prime\\prime}(z)\\leq|\\ell_{\\log}^{\\prime}(z)|\\leq\\ell_{\\log}(z)$ ", "page_idx": 25}, {"type": "text", "text": "Lemma C.3. For any $z\\geq0$ $\\frac{|\\ell_{\\mathrm{log}}^{\\prime}(z)|}{\\ell_{\\mathrm{log}}(z)}$ \uff0c $\\begin{array}{r}{\\frac{|\\ell_{\\mathrm{log}}^{\\prime}(z)|}{\\ell_{\\mathrm{exp}}(z)}\\geq\\frac{1}{2}}\\end{array}$ and ax(a) \u2265 log 2. ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma C.3. For $z\\geq0$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\ell_{\\log}(z)\\leq\\ell_{\\exp}(z)=\\frac{2}{2e^{z}}\\leq\\frac{2}{1+e^{z}}=2|\\ell_{\\log}^{\\prime}(z)|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The second result holds because $\\begin{array}{r}{\\frac{\\ell_{\\mathrm{log}}(z)}{\\ell_{\\mathrm{exp}}(z)}=\\frac{\\log(1+e^{-z})}{e^{-z}}}\\end{array}$ log(I+) is an decreasing function for e\\~ and e\\~ $(0,1]$ for $z\\geq0$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "aC.4.For $\\ell\\in\\{\\ell_{\\mathrm{exp}},\\ell_{\\mathrm{log}}\\}$ either $\\begin{array}{r}{\\mathcal{G}(\\mathbf{w})\\leq\\frac{1}{2n}}\\end{array}$ $\\begin{array}{r}{\\mathcal{R}(\\mathbf{w})\\leq\\frac{\\log2}{n}}\\end{array}$ impies $\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle\\geq0$ foral $i\\in[n]$ ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma C.4. If $\\begin{array}{r}{\\mathcal{G}(\\mathbf{w})\\leq\\frac{1}{2n}}\\end{array}$ , we have $\\begin{array}{r}{\\left|\\ell^{\\prime}(\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle)\\right|\\le n\\mathcal{G}(\\mathbf{w})\\le\\frac{1}{2}}\\end{array}$ . Then by monotonicity of $|\\ell^{\\prime}(\\cdot)|$ we have $\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle\\geq0$ . Similarly if $\\begin{array}{r}{\\mathcal{R}(\\mathbf{w})\\leq\\frac{\\log2}{n}}\\end{array}$ , we also have $\\ell(\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle)\\leq n\\mathcal{R}(\\mathbf{w})\\leq\\log2$ Then by monotonicity of $\\ell(\\cdot)$ we have $\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle\\geq0$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma C.5. For $\\ell\\in\\{\\ell_{\\mathrm{exp}},\\ell_{\\mathrm{log}}\\}$ and any $z_{1},z_{2}\\in\\mathbb{R}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\ell^{\\prime}(z_{1})}{\\ell^{\\prime}(z_{2})}}-1\\right|\\leq e^{|z_{1}-z_{2}|}-1\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma C.5. For $\\ell=\\ell_{\\mathrm{exp}}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{\\ell_{\\mathrm{exp}}^{\\prime}(z_{1})}{\\ell_{\\mathrm{exp}}^{\\prime}(z_{2})}-1\\right|=\\left|e^{z_{2}-z_{1}}-1\\right|\\leq e^{|z_{2}-z_{1}|}-1\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the inequality is from $|e^{x}-1|\\leq e^{|x|}-1$ . For $\\ell=\\ell_{\\mathrm{log}}$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{\\ell_{\\mathrm{log}}^{\\prime}(z_{1})}{\\ell_{\\mathrm{log}}^{\\prime}(z_{2})}-1\\right|=\\left|\\frac{1+e^{z_{2}}}{1+e^{z_{1}}}-1\\right|=\\left|\\frac{e^{z_{2}}-e^{z_{1}}}{1+e^{z_{1}}}\\right|\\leq\\left|\\frac{e^{z_{2}}-e^{z_{1}}}{e^{z_{1}}}\\right|\\leq e^{|z_{2}-z_{1}|}-1\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma C.6. For $\\ell\\in\\{\\ell_{\\mathrm{exp}},\\ell_{\\mathrm{log}}\\}$ and any $z_{1},z_{2},z_{3},z_{4}\\in\\mathbb{R}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{\\ell^{\\prime}(z_{1})\\ell^{\\prime}(z_{3})}{\\ell^{\\prime}(z_{2})\\ell^{\\prime}(z_{4})}-1\\right|\\le\\left(e^{|z_{1}-z_{2}|}-1\\right)+\\left(e^{|z_{3}-z_{4}|}-1\\right)+\\left(e^{|z_{1}+z_{3}-z_{2}-z_{4}|}-1\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma C.6. For $\\ell=\\ell_{\\mathrm{exp}}$ \uff0c", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\frac{\\ell_{\\mathrm{exp}}^{\\prime}(z_{1})\\ell_{\\mathrm{exp}}^{\\prime}(z_{3})}{\\ell_{\\mathrm{exp}}^{\\prime}(z_{2})\\ell_{\\mathrm{exp}}^{\\prime}(z_{4})}-1\\right|=\\left|e^{z_{2}+z_{4}-z_{1}-z_{3}}-1\\right|\\le\\left(e^{|z_{1}+z_{3}-z_{2}-z_{4}|}-1\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the inequality is from $|e^{x}-1|\\leq e^{|x|}-1$ For $\\ell=\\ell_{\\mathrm{log}}$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\ell_{\\mathrm{log}}^{\\prime}\\left(z_{1}\\right)\\ell_{\\mathrm{log}}^{\\prime}\\left(z_{3}\\right)}{\\ell_{\\mathrm{log}}^{\\prime}\\left(z_{2}\\right)\\ell_{\\mathrm{log}}^{\\prime}\\left(z_{4}\\right)}-1\\bigg|=\\bigg|\\frac{\\left(1+e^{z_{2}}\\right)\\left(1+e^{z_{4}}\\right)}{\\left(1+e^{z_{1}}\\right)\\left(1+e^{z_{3}}\\right)}-1\\bigg|=\\bigg|\\frac{e^{z_{2}}+e^{z_{4}}+e^{z_{2}+z_{4}}-e^{z_{1}}-e^{z_{3}}-e^{z_{1}+z_{3}}}{1+e^{z_{1}}+e^{z_{3}}+e^{z_{1}+z_{3}}}\\bigg|}\\\\ {\\leq\\bigg|\\frac{e^{z_{2}}-e^{z_{1}}}{e^{z_{1}}}\\bigg|+\\bigg|\\frac{e^{z_{4}}-e^{z_{3}}}{e^{z_{3}}}\\bigg|+\\bigg|\\frac{e^{z_{2}+z_{4}}-e^{z_{1}+z_{3}}}{e^{z_{1}+z_{3}}}\\bigg|}\\\\ {\\leq\\left(e^{|z_{1}-z_{2}|}-1\\right)+\\left(e^{|z_{3}-z_{4}|}-1\\right)+\\left(e^{|z_{1}+z_{3}-z_{2}-z_{4}|}-1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma C.7. For $\\ell=\\ell_{\\mathrm{log}}$ , and any $\\mathbf{w}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{G}(\\mathbf{w}_{t})}{\\mathcal{R}(\\mathbf{w})}\\geq1-\\frac{n\\mathcal{R}(\\mathbf{w})}{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma C.7. Let $r_{i}\\;=\\;\\ell_{\\log}(\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle)\\;=\\;\\log(1+{e^{-\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle}})$ and $f(z)\\;=\\;1\\,-\\,e^{-z}$ , then $\\begin{array}{r}{|\\ell_{\\log}^{\\prime}(\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle)|=\\frac{e^{-\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle}}{1+e^{-\\langle\\mathbf{w},\\mathbf{z}_{i}\\rangle}}=\\frac{e^{r_{i}}-1}{e^{r_{i}}}=f(z_{i})}\\end{array}$ = f(za). Therefore for any given R(w),fnding ming(wt) equals to the following optimization problem, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}{\\frac{1}{n}}\\sum_{i=1}^{n}f{\\big(}r_{i}{\\big)}\\quad{\\mathrm{s.t.}}\\quad\\sum_{i=1}^{n}r_{i}=n\\mathcal{R}(\\mathbf{w}),\\;r_{i}\\geq0\\;{\\mathrm{for~all~}}i\\in[n]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $f(z)$ is an increasing function and the increasing rate would be slow as $z$ increase since $f^{\\prime\\prime}(z)<0$ , we can easily derive that the aforementioned optimization problem will take the minimum at $r_{i}=n\\mathcal{R}(\\mathbf{w})$ for some $i\\in[n]$ and $r_{j}=0$ for all $j\\neq i$ . Therefore, we can derive that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{G}(\\mathbf{w}_{t})}{\\mathcal{R}(\\mathbf{w})}\\geq\\frac{1-e^{-n\\mathcal{R}(\\mathbf{w})}}{n\\mathcal{R}(\\mathbf{w})}\\geq1-\\frac{n\\mathcal{R}(\\mathbf{w})}{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "by Taylor's expansion. ", "page_idx": 26}, {"type": "text", "text": "C.3  Auxiliary Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The following result is the classic Stolz-Cesaro theorem. ", "page_idx": 26}, {"type": "text", "text": "Theorem C.8 (Stolz-Cesaro theorem). Let $\\{a_{n}\\}_{n\\ge1}$ $\\{b_{n}\\}_{n\\ge1}$ be two sequences of real numbers. Assume that $\\{b_{n}\\}_{n\\ge1}$ is a strictly monotone and divergent sequence (i.e. strictly increasing and approaching $+\\infty$ , or strictly decreasing and approaching $-\\infty)$ and the following limit exists: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}{\\frac{a_{n+1}-a_{n}}{b_{n+1}-b_{n}}}=l.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}{\\frac{a_{n}}{b_{n}}}=l.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The abstract and introduction clearly outline the primary results and contributions of the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have discussed the limitation of this work in the conclusion and future worksection. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In Section 4, we have clearly stated all the assumptions that are necessary for our theoremresults. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the complete configuration of the experiments. Since the experiments are all on synthetic data, the configuration is simple to explain. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper focuses on theoretical analysis of the standard optimization algorithm Adam. While we include some simulation results, they are very simple and are irrelevant to the key theoretical contributions of this paper. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have provided all the experiment details. The experiments are on synthetic data and the setup is simple. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: This paper studies optimization problems and there is no statistical significance results to report. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: We only present very simple simulation results and computational resources are not the focus. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The research results align with the ethical principles outlined in the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper is a theoretical work that has no direct social impact. We would like to emphasize that, although the topic is related to \u201cimplicit bias\", it is a pure theoretical terminology and has nothing to do with social bias. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the asshts? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}]