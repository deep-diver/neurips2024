[{"type": "text", "text": "Autobidder\u2019s Dilemma: Why More Sophisticated Autobidders Lead to Worse Auction Efficiency ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuan Deng Jieming Mao Vahab Mirrokni Google Research Google Research Google Research dengyuan@google.com maojm@google.com mirrokni@google.com ", "page_idx": 0}, {"type": "text", "text": "Hanrui Zhang Chinese University of Hong Kong hanrui@cse.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Song Zuo Google Research szuo@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent increasing adoption of autobidding has inspired the growing interest in analyzing the performance of classic mechanism with value-maximizing autobidders both theoretically and empirically. It is known that optimal welfare can be obtained in first-price auctions if autobidders are restricted to uniform bid-scaling and the price of anarchy is 2 when non-uniform bid-scaling strategies are allowed. In this paper, we provide a fine-grained price of anarchy analysis for non-uniform bid-scaling strategies in first-price auctions, demonstrating the reason why more powerful (individual) non-uniform bid-scaling strategies may lead to worse (aggregated) performance in social welfare. Our theoretical results match recent empirical findings that a higher level of non-uniform bid-scaling leads to lower welfare performance in first-price auctions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The online advertising market has witnessed increasing adoption of autobidding in recent years, which shifts the bidding behavior model of advertisers from the classic utility-maximizing bidding to value-maximizing bidding [Aggarwal et al., 2019, Balseiro et al., 2021b]. Unlike the classic utility maximizers who maximize their quasi-linear utility given by the difference between value and payment, value maximizers maximize the total value subject to a return-on-spend (RoS) constraint [Balseiro et al., 2021b]. ", "page_idx": 0}, {"type": "text", "text": "The shift in the bidding behavior model has motivated a growing body of literature on auction design with value-maximizing autobidders. Notably, it has been shown that the price of anarchy (PoA) in second-price auctions [Aggarwal et al., 2019] and first-price auctions [Deng et al., 2022, Liaw et al., 2023] are both 2. However, (1) these PoA results measure the welfare performance in the worst-case scenario (i.e., the worst-case equilibrium of the worst-case instance); moreover, (2) these PoA results assume the bidding agents can find the optimal bidding strategies in response to other bidders so that their bidding profile forms an equilibrium, while computing optimal bidding strategies and/or finding equilibria could be computationally infeasible [Aggarwal et al., 2023, Chen et al., 2021, Li and Tang, 2024, Paes Leme et al., 2024]. On the other hand, Balseiro et al. [2021a] demonstrate that optimal welfare can be obtained in first-price auctions if autobidders are restricted to the simple uniform bid-scaling strategy (i.e., always bid $\\theta v$ when the bidder\u2019s value is $v$ with a universal bid multiplier \u03b8). But, uniform bid-scaling is not always an optimal strategy for autobidders in first-price auctions as non-uniform bidding scaling, in which autobidder may apply different bid multipliers in different auctions, may result in better bidding performance. ", "page_idx": 0}, {"type": "text", "text": "To circumvent the above mentioned limitations of PoA results and bridge the gap between uniform bid-scaling and non-uniform bid-scaling, Deng et al. [2024] propose a hierarchical structure over ad auction instances in which the auctions are partitioned to different categories following a multi-layer laminar structure. Autobidders are required to adopt the same bid multipliers for auctions within the same slice but can use different bid multipliers across different slices. Such a multi-layer structure enables the comparison between non-uniform bid-scaling strategies of different degrees of freedom and the empirical results from [Deng et al., 2024] show that a higher level of non-uniform bid-scaling may lead to lower aggregated welfare performance in first-price auctions. In this paper, we take a theoretical approach to non-uniform bid-scaling in first-price marketplaces, aiming to (1) provide a formal understanding of the phenomenon observed by Deng et al. [2024], and (2) establish principles that can guide the design of marketplaces where non-uniform bid-scaling is involved. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our main technical result is a fine-grained analysis of the price of anarchy of automated first-price marketplaces when autobidders are capable of non-uniform bidding. We adopt the high-level model introduced by Deng et al. [2024]: Roughly speaking, the entire market is divided into a number of slices, and autobidders may choose one bid multiplier independently for each slice. Here, intuitively, the granularity of the slices measures autobidders\u2019 capability of non-uniform bidding. Through a refined analysis, we present a parameterized price of anarchy bound, which connects the efficiency of an automated marketplace to the power of autobidders measured by their capability of non-uniform bidding, as well as the \u201cbalancedness\u201d of slices. Here, the balancedness of a slice roughly measures the smallest \u201cmarket share\u201d across all bidders. Qualitatively, our results suggest: ", "page_idx": 1}, {"type": "text", "text": "\u2022 First-price markets are more efficient when autobidders are less powerful. This is reminiscent of the prisoner\u2019s dilemma, where both prisoners end up in a strictly worse situation, when each of them chooses an action that is superior to the alternative, regardless of the other prisoner\u2019s choice. In automated marketplaces, fixing others\u2019 bids, a more powerful autobidder always achieves a (weakly) better payoff. And yet, the interplay of such \u201cbetter\u201d autobidders could lead to worse auction efficiency. This provides a theoretical explanation for the phenomenon observed by Deng et al. [2024]. ", "page_idx": 1}, {"type": "text", "text": "\u2022 First-price markets are more efficient with more balanced slices. In particular, when the granularity of non-uniform bidding is fixed, one can improve the efficiency of the market by creating slices that are more balanced. Intuitively, this introduces more intense competition within each slice, resulting in better auction efficiency. Such an insight can also be applied to marketplaces with the multi-channel setting [Deng et al., 2023a, Susan et al., 2023], where advertisers can procure ad impressions simultaneously on multiple channels with different bidding strategies. In the multi-channel setting, more balanced channels could lead to better auction efficiency across all channels. ", "page_idx": 1}, {"type": "text", "text": "1.2 Further Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The price of anarchy in first-price auctions with quasi-linear utility maximizers has been extensively studied [Roughgarden et al., 2017]. For Bayesian settings, the price of anarchy is at least $1/2$ for subadditive valuations [Feldman et al., 2013] and the price of anarchy is at least $1-1/e$ for submodular valuations [Syrgkanis and Tardos, 2013]. When values are independently distributed, Hoy et al. [2018] improved the price of anarchy to $\\approx0.743$ . Recently, Jin and Lu [2022] resolve the long-standing open problem and show that the price of anarchy in first-price auctions with independently distributed values is exactly $1-1/e^{2}$ . ", "page_idx": 1}, {"type": "text", "text": "Our work is also closely related to the recent growing body of research [Aggarwal et al., 2019, Deng et al., 2021, Balseiro et al., 2021a, Mehta, 2022, Deng et al., 2023b, Liaw et al., 2024] which study the price of anarchy (PoA) with value-maximizing bidders in several classic auction mechanisms like second-price auctions, (randomized) first-price auctions, and generalized second-price auctions. In particular, it has been shown the price of anarchy of first-price auctions is exactly $1/2$ [Liaw et al., 2023, Deng et al., 2022]. In the multi-channel setting, Deng et al. [2023a] study the problem of multi-channel bidding where an advertiser aims to maximize their total value across and analyze the effectiveness of levers of return-on-spend and budget. Susan et al. [2023] develop multi-channel bidding algorithms when channels adopt auction rules that may or may not be incentive-compatible under the presence of budget constraints. More recently, Feng et al. [2023] investigate the PoA of running first-price auctions with strategic budget-constrained autobidders, in which advertisers may manipulate their reported budget constraints for the autobidders. For a more comprehensive overview of auctions and autobidding, see, e.g., [Aggarwal et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The multi-auction model. Following prior works [Aggarwal et al., 2019, Balseiro et al., 2021b], we consider the following model where multiple bidders participate in multiple auctions simultaneously. There are $n$ bidders (generally indexed by $i$ ) and $m$ auctions (generally indexed by $j)$ . In each auction $j$ , each bidder $i$ has a value $v_{i,j}$ , and places a bid $b_{i,j}$ . In this paper, we assume these bids are placed through autobidders using non-uniform bidding, in which the bids are subject to certain restrictions to be discussed below. For brevity, we let $\\pmb{v}_{i}\\;=\\;(v_{i,1},\\ldots,v_{i,m})$ , $\\pmb{v}~\\dot{=}~(\\pmb{v}_{1},\\ldots,\\pmb{v}_{n})$ , $\\mathbf{\\vec{v}}_{-i}~=~\\left(\\boldsymbol{v}_{1},\\ldots,\\boldsymbol{v}_{i-1},\\boldsymbol{v}_{i+1},\\ldots,\\boldsymbol{v}_{n}\\right)$ , $\\pmb{v}_{j}~=~(v_{1,j},\\ldots,v_{n,j})$ , and $\\pmb{v}_{-i,j}~=$ $(v_{1,j},\\dotsc,v_{i-1,j},v_{i+1,j},\\dotsc,v_{n,j})$ . We use $^{b}$ in a similar way. ", "page_idx": 2}, {"type": "text", "text": "Each bidder $i$ \u2019s allocation $x_{i,j}$ and payment $p_{i,j}$ in each auction $j$ is determined by the auction rule and all bidders\u2019 bids $b_{j}$ in auction $j$ . We focus on first-price auctions: In each auction $j$ , the bidder with the highest bid1 wins, receives the whole item and pays the bid. All other bidders receive nothing and pay 0. Formally, for each auction $j$ , let $i^{*}(j)=\\mathrm{argmax}_{i}\\,b_{i,j}$ . The allocation $x_{i,j}$ and payment $p_{i,j}$ of each bidder $i$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{i,j}=\\left\\{\\!\\!\\begin{array}{l l}{1,}&{\\mathrm{if}\\;i=i^{*}(j)\\!}\\\\ {0,}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.,\\qquad\\mathrm{and}\\qquad p_{i,j}=\\left\\{\\!\\!\\begin{array}{l l}{b_{i,j},}&{\\mathrm{if}\\;i=i^{*}(j)}\\\\ {0,}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right..\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We will omit the dependency of $x_{i,j}$ and $p_{i,j}$ on $^{b}$ for simplicity. For each auction $j$ , we also identify a \u201crightful winner\u201d $\\mathsf{r w}(j)=\\mathrm{argmax}_{i\\in[n]}\\,\\bar{v_{i,j}}$ , who has the highest value and therefore should win in auction $j$ in the socially optimal allocation.2 ", "page_idx": 2}, {"type": "text", "text": "Slices and partially non-uniform bidding. We capture the power of autobidders beyond uniform bidding with a fine-grained slice-based model [Deng et al., 2024]. Intuitively, we assume that the $m$ auctions are partitioned into slices. Within each slice, the autobidder must bid uniformly, while across slices, it can use different bidding multipliers. Formally, there are $s$ slices (generally indexed by $k$ ), which together form a partition of the $m$ auctions. Each slice $k$ is described by the set $S_{k}\\subseteq[m]$ of (indices of) auctions it contains. We always have $S_{k}\\cap S_{k^{\\prime}}=\\emptyset$ for all $k\\neq k^{\\prime}$ , and $\\textstyle\\bigcup_{k}S_{k}=[m]$ . For each auction $j\\in S_{k}$ , we let ${\\mathsf{s l i c e}}(j)=k$ . The bids $b_{i}$ of each bidder $i$ is generate d in the following way: For each slice $k$ , the autobidder chooses a bid multiplier $\\theta_{i,k}$ such that for each auction $j\\in S_{k}$ , $b_{i,j}=v_{i,j}\\cdot\\theta_{i,k}$ . We use $\\pmb{\\theta}$ in a similar way to $\\pmb{v}$ and $^{b}$ . ", "page_idx": 2}, {"type": "text", "text": "Constrained value maximizers. We adopt the value maximization model prevalent in the autobidding literature [Aggarwal et al., 2019, Balseiro et al., 2021b]. The objective of a value maximizer is to maximize the total value that they win in all auctions, subject to the constraint that the overall return on spend (RoS) is larger than a given target, by choosing the optimal bid multipliers for all $s$ slices conditioned on other bidders\u2019 bids. In addition, we make the following regularity assumption about bidders\u2019 strategies: On each slice $k$ , each bidder $i$ never underbids (i.e., never chooses a multiplier $\\theta_{i,k}<1\\rangle$ ) unless $i$ wins in all auctions $j\\in S_{k}$ where $i$ is the rightful winner (i.e., where $\\mathsf{r w}(j)\\bar{=}\\,i)$ . The assumption rules out brittle and pathological equilibrium behavior, which allows us to focus on the relation between the power of autobidders and auction efficiency. Intuitively, if a bidder $i$ is forced to underbid on a slice $k$ , then $i$ must be overbidding on some other slices (because the overall RoS constraint must be binding \u2014 see below for the formal definition). In such cases, the assumption says that autobidders are conservative, in the sense that bidder $i$ would try to secure $i$ \u2019s share in the socially optimal allocation on each slice first, before bidding more aggressively and trying to acquire other bidders\u2019 share on other slices. We also remark that such regularity assumptions are used in prior work (e.g., [Deng et al., 2021]) to derive meaningful efficiency bounds for other auction formats. ", "page_idx": 2}, {"type": "text", "text": "Formally, let $\\mathsf{v a l}_{i,j}=x_{i,j}\\cdot v_{i,j}$ be $i$ \u2019s value in auction $j$ . Let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{v a l}_{i}=\\sum_{j\\in[m]}\\mathsf{v a l}_{i,j}\\quad\\mathrm{and}\\quad p_{i}=\\sum_{j\\in[m]}p_{i,j}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, for each bidder $i$ and slice $k$ , fixing $\\theta_{-i,k}$ , let $\\theta_{i,k}(\\pmb{\\theta}_{-i,k})=\\operatorname*{min}\\{1,\\underline{{{\\theta}}}\\}$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underline{{\\theta}}=\\operatorname*{min}\\{\\theta_{i,k}\\mid x_{i,j}=1,\\forall j\\in S_{k}{\\mathrm{~where~}}r\\warrow(j)=i\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta_{i}(\\pmb\\theta_{-i})=[\\underline{{\\theta}}_{i,1}(\\pmb\\theta_{-i,1}),\\infty)\\times\\cdots\\times[\\underline{{\\theta}}_{i,s}(\\pmb\\theta_{-i,s}),\\infty).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, fixing $\\theta_{-i}$ (and therefore $\\pmb{b}_{-i}$ ), each bidder $i$ solves the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\pmb{\\theta}_{i}\\in\\Theta_{i}(\\pmb{\\theta}_{-i})}{\\operatorname*{max}}}&{\\forall\\pmb{\\mathrm{a}}\\|_{i}}\\\\ {\\mathrm{s.t.}}&{\\forall\\pmb{\\mathrm{a}}\\|_{i}\\geq p_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We say $\\pmb{\\theta}_{i}\\in\\mathsf{B R}_{i}(\\pmb{\\theta}_{-i})$ if $\\theta_{i}$ achieves the maximum of the above optimization problem given $\\theta_{-i}$ . Note that here, we are assuming that bidder $i$ \u2019s target RoS is 1. This is without loss of generality, because scaling each bidder\u2019s values by their target RoS preserves the auction outcomes and the liquid welfare, the latter being the standard measure of efficiency with constrained value maximizers. ", "page_idx": 3}, {"type": "text", "text": "Equilibria and price of anarchy. We consider stable auction outcomes based on Nash equilibria, where all bidders are best responding to each other. Formally, we say a strategy profile $\\pmb{\\theta}$ is an equilibrium, iff for each bidder $i$ , $\\bar{\\pmb{\\theta_{i}}}\\ \\in\\ \\mathsf{B R}_{i}(\\pmb{\\theta_{-i}})$ . We measure efficiency by considering the standard notion of the Price of Anarchy (PoA), which is the ratio between the liquid welfare at the worst equilibrium and the optimal liquid welfare. Formally, the PoA of a particular instance $(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ in the multi-auction model is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{P o A}(n,m,s,\\{S_{k}\\},v)=\\operatorname*{inf}_{\\theta\\mathrm{~is~an~equilibrium}}\\frac{\\sum_{i\\in[n]}\\mathsf{v a l}_{i}}{\\sum_{j\\in[m]}\\operatorname*{max}_{i\\in[n]}v_{i,j}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the above definition of PoA is with respect to a single market instance. In later sections, we will adopt a specific parametrization of non-uniformity, and consider the worst-case PoA for any fixed non-uniformity parameters. This allows us to clearly depict the relation between non-uniformity and efficiency of auction outcomes. We also remark that this approach refines the standard PoA analysis, which normally establishes a single worst-case PoA bound over all instances. ", "page_idx": 3}, {"type": "text", "text": "3 Fine-Grained PoA of First-Price Auctions with Non-Uniform autobidders ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present and prove the main result of this paper: a fine-grained PoA bound capturing the effect of non-uniform bidding on auction efficiency. As we will discuss later, our result implies that more sophisticated (i.e., more non-uniform) autobidders generally lead to worse auction efficiency. ", "page_idx": 3}, {"type": "text", "text": "3.1 The Balancedness Parametrization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first introduce our parametrization of non-uniformity. Fix a market instance $(n,m,s,\\{S_{k}\\},\\boldsymbol{v})$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Market share). The market share of a bidder $i$ in a set of auctions $S\\subseteq[m]$ is $i$ \u2019s contribution to the optimal liquid welfare in $S$ , as a fraction of the optimal liquid welfare. Formally, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{s h a r e}_{i,S}=\\frac{\\sum_{j\\in S:r w(j)=i}v_{i,j}}{\\sum_{j\\in[m]}\\operatorname*{max}_{i^{\\prime}\\in[n]}v_{i^{\\prime},j}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Balancedness). The balancedness of a set of auctions $S\\,\\subseteq\\,[m]$ is the minimum market share of all bidders in $S$ , as a fraction of the sum of all bidders\u2019 market shares in $S$ (i.e., the contribution of $S$ to the optimal liquid welfare). Formally, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{b a l}_{S}=\\frac{\\operatorname*{min}_{i\\in[n]}\\mathsf{s h a r e}_{i,S}}{\\sum_{i\\in[n]}\\mathsf{s h a r e}_{i,S}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Without loss of generality, we assume (unless otherwise specified) that slices are numbered such that $\\mathsf{b a l}_{k}$ is weakly decreasing in $k$ , i.e., for each $k\\in[s-\\bar{1}]$ , $\\mathsf{b a l}_{k+1}\\leq\\mathsf{b a l}_{k}$ . Higher balancedness generally means that the contribution to the optimal welfare is more equally distributed among bidders. We will pay special attention to the market share in, and the balancedness of, a whole slice, where $S=S_{k}$ for some slice $k$ . Abusing notation, we let $\\mathsf{s h a r e}_{i,k}=\\mathsf{s h a r e}_{i,S_{k}}$ for each bidder $i$ and slice $k$ , and sh $\\begin{array}{r}{\\mathsf{r e}_{k}=\\sum_{i\\in[n]}\\mathsf{s h a r e}_{i,k}}\\end{array}$ and $\\mathsf{b a l}_{k}=\\mathsf{b a l}_{S_{k}}$ for each slice $k$ . ", "page_idx": 4}, {"type": "text", "text": "We discuss the intuition behind these definitions below. Generally speaking, inefficiency in first-price auctions with value maximizers often manifests in the following way: ", "page_idx": 4}, {"type": "text", "text": "\u2022 A bidder wins in a large fraction of auctions where that bidder is the rightful winner, without much competition. This means the bidder wins by bidding far below their true value in these auctions, which gives the bidder a significant amount of buyer surplus. \u2022 While the bidder (being a value maximizer) does not intrinsically care about buyer surplus, it allows the bidder to bid more aggressively (i.e., far above their true value) in other auctions, and win extra auctions in which they are not the rightful winners, which makes the auction outcome less efficient. ", "page_idx": 4}, {"type": "text", "text": "For autobidders using slice-based non-uniform bidding strategies, since each bidder must bid uniformly (i.e., equally aggressively) within each slice, intuitively, more balanced slices make it harder for the above phenomenon to happen. In particular, in the first bullet point, on a balanced slice, all bidders are bidding seriously because they want to defend their market share, which leads to higher competition and lower buyer surplus. In particular, this means they are more likely to use higher bid multipliers. In the second bullet point, by bidding aggressively above their true values, the bidder is also paying much more than they \u201cshould\u201d in auctions where they are the rightful winner, which depletes their buyer surplus without hurting the efficiency (because they are the rightful winner in these auctions). We will prove below that this is in fact what happens. ", "page_idx": 4}, {"type": "text", "text": "In order to measure the balancedness of a market instance, we will aggregate the balancedness across slices via the balancedness quantile function defined as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Balancedness quantile function). Fix any market instance $(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ . The balancedness quantile function $F$ of the market instance is a function such that for any given $t\\in[0,1]$ , $F(t)$ is the infimum value $b$ such that at least a $t$ fraction of the entire market has balancedness at most $b$ . That is, ", "page_idx": 4}, {"type": "equation", "text": "$$\nF(t)=\\operatorname*{inf}\\left\\{b\\left|\\sum_{k\\in[s]:\\mathbf{b}\\mathbf{a}\\mathbf{l}_{k}\\leq b}\\mathsf{s h a r e}_{k}\\geq t\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The balancedness quantile function can also be viewed as the inverse of the \u201ccumulative distribution function\u201d of the unbalancedness in the entire market. Note that $F(t)\\in[0,1/2]$ since $\\mathsf{b a l}_{k}\\in[0,1/2]$ whenever $n\\geq2$ . Our actual bound will depend on the following parameter related to $F(t)$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 4 (Unbalancedness of market instance). Fix any market instance $(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ . Let $\\alpha$ be the function such that for any $b\\in[0,1/2]$ and $u\\in\\mathbb R$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha(b,u)=1-2b+\\sqrt{b\\cdot(1-b)}\\cdot u.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, let $\\beta$ be the function such that for any $w\\in[0,1],\\beta(w)$ is the unique number $u$ such that, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{0}^{w}\\alpha(F(t),u)\\,\\mathrm{d}t=1-w.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The unbalancedness unba $|(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ of the market instance is defined such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{b a l}(n,m,s,\\{S_{k}\\},v)=}\\\\ &{\\displaystyle\\operatorname*{max}_{w\\in[0,1]}\\int_{0}^{w}\\left(1+\\alpha(F(t),\\beta(w))-\\sqrt{(1-\\alpha(F(t),\\beta(w)))^{2}+4F(t)\\cdot\\alpha(F(t),\\beta(w))}\\right)\\,\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We establish the following intuitive properties of the unbalancedness, which will help make the conceptual messages of our main result (to be discussed in the next subsection) clearer. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. The unbalancedness parameter has the following properties: ", "page_idx": 4}, {"type": "image", "img_path": "hQJksiskaa/tmp/c8a91cc5ac2dedd493fcdca1b897dd7679e6b5eac0b05353ede90c5ea48af9b2.jpg", "img_caption": ["Figure 1: A graphical example of the balancedness quantile function. Each grey rectangle corresponds to a slice $k$ , where the height is $\\mathsf{b a l}_{k}$ and the width sharek. Observe that the height of each rectangle is at most $\\mathtt{b a l}_{k}\\leq1/2$ , and the widths of all rectangles sum to $\\begin{array}{r}{\\sum_{k\\in[s]}\\mathsf{s h a r e}_{k}=1}\\end{array}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "\u2022 unba $(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ weakly increases when $\\mathsf{b a l}_{k}$ decreases for some slice $k\\in[s]$ . \u2022 unba $|(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ weakly increases when a slice is subdivided, i.e., $\\mathtt{u n b a l}(n,m,s,\\{S_{k}\\},v)\\le\\mathtt{u n b a l}(n,m,s+1,\\{S_{k}^{\\prime}\\},v),$ where there exists $k^{*}\\in[s]$ , such that $S_{k}^{\\prime}=S_{k}$ for all $k\\in[s]\\setminus\\{k^{*}\\}$ and $S_{s+1}^{\\prime}\\cup S_{k^{*}}^{\\prime}=S_{k^{*}}$ .3 \u2022 unba $(n,m,s,\\{S_{k}\\},\\mathbf{v})\\in[2/5,1]$ for each market instance $(n,m,s,\\{S_{k}\\},\\boldsymbol{v})$ where $n\\geq2$ and $s\\geq2$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. Note that the unbalancedness depends only on the balancedness quantile function. We first argue that the unbalancedness weakly increases when the balancedness quantile function pointwise weakly decreases. Observe that unba $|(\\cdot)$ can be equivalently defined in the following way: For each $w\\in[0,1]$ , let ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma(w)=\\operatorname*{max}_{\\lambda\\in\\Lambda(w)}\\int_{0}^{w}\\Big(1+\\lambda(t)-\\sqrt{(1-\\lambda(t))^{2}+4F(t)\\cdot\\lambda(t)}\\Big)\\,\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Lambda(w)$ is the set of mappings from $[0,w]$ to $[0,\\infty)$ , satisfying for each $\\lambda\\in\\Lambda(w)$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\int_{0}^{w}\\lambda(t)\\,\\mathrm{d}t=1-w.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that this maximum is guaranteed to exist.4Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathsf{u n b a l}}(n,m,s,\\{S_{k}\\},\\pmb{v})=\\operatorname*{max}_{w\\in[0,1]}\\gamma(w).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This alternative definition is equivalent to Definition 4 because for a fixed $w$ , the maximizing $\\lambda$ of $\\gamma(\\lambda)$ must satisfy: For each $\\bar{t_{1},t_{2}}\\in[0,w]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial(1+\\lambda(t_{1})-\\sqrt{(1-\\lambda(t_{1}))^{2}+4F(t_{1})\\lambda(t_{1})})}{\\partial(\\lambda(t_{1}))}=\\frac{\\partial(1+\\lambda(t_{2})-\\sqrt{(1-\\lambda(t_{2}))^{2}+4F(t_{2})\\lambda(t_{2})})}{\\partial(\\lambda(t_{2}))}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3Here we do not assume slices are ordered such that the balancedness is weakly increasing. 4To quickly see why this is the case, observe that without loss of generality $\\lambda$ is piecewise constant with at most $s+1$ pieces, because $F$ is piecewise constant with at most $s$ pieces. ", "page_idx": 5}, {"type": "text", "text": "This must be true because otherwise one can adjust $\\lambda$ locally and achieve a larger value of the integral. Given the above, one can show (see the proof of Lemma 1 in Appendix A for a detailed argument) that the maximizing $\\lambda$ must satisfy: There exists some $u$ such that for each $t\\in[0,w]$ , $\\lambda(t)=\\dot{\\alpha}(F(t),u)$ , where $\\alpha$ is defined in Definition 4. The unique choice of $u$ that satisfies the constraint on $\\lambda$ is then $\\beta(w)$ , and the alternative definition then reduces to Definition 4. ", "page_idx": 6}, {"type": "text", "text": "Now to argue that unbal weakly increases when $F$ pointwise weakly decreases, we only need to show that for each $w\\in[0,1]$ , $\\gamma(w)$ weakly increases when $F$ pointwise weakly decreases. The latter further reduces to: Fixing any $\\lambda$ satisfying the conditions above, the integral ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int_{0}^{w}\\Big(1+\\lambda(t)-\\sqrt{(1-\\lambda(t))^{2}+4F(t)\\cdot\\lambda(t)}\\Big)\\,\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "weakly increases when $F$ pointwise weakly decreases. This is true because the integrand increases when $F(t)$ decreases. ", "page_idx": 6}, {"type": "text", "text": "Now we come back to the properties to be proved. The first property becomes almost obvious, because when $\\mathsf{b a l}_{k}$ weakly decreases for some slice $k$ , $F$ must pointwise weakly decrease, and therefore unbal must weakly increase. As for the third property, we consider extreme cases of $F$ : When $F(t)=0$ for each $t\\in[0,1]$ , in Definition 4, $\\bar{\\alpha}(\\bar{F}(t)\\bar{,}u)$ is always 1, and unba $|(n,m,s,\\{S_{k}\\},\\boldsymbol{v})=\\dot{1}$ . This is the largest unbalancedness possible. When $F(t)\\,=\\,1/2$ for each $t\\in[0,1]$ , $\\dot{\\alpha}(\\dot{F}(t),u)\\,=\\,u/2$ , $\\beta(w)=2/w-2$ , and one can show unba $|(n,m,s,\\{S_{k}\\},\\mathbf{v})=2/5$ (achieved when $w=3/5$ ). This is the smallest unbalancedness possible. ", "page_idx": 6}, {"type": "text", "text": "The second property is a bit trickier. To see why this is true, consider the parameters $w$ and $\\lambda$ that achieve unba $|(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ in the alternative definition. Moreover, without loss of generality (recall that $\\lambda$ is without loss of generality piecewise constant), suppose $\\lambda$ is weakly decreasing on $\\begin{array}{r}{\\left(\\sum_{k\\in[k^{*}-1]}\\mathsf{s h a r e}_{k},\\sum_{k\\in[k^{*}]}\\mathsf{s h a r e}_{k}\\right)}\\end{array}$ . Let share\u2032k and $\\mathsf{b a l}_{k}^{\\prime}$ be the market share and balancedness of each slice $k$ in the new market instance. Without loss of generality, suppose ${\\sf b a l}_{k^{*}}^{\\prime}\\,\\leq\\,{\\sf b a l}_{s+1}^{\\prime}$ Define $G$ to be the \u201cpartially unordered\u201d balancedness quantile function of the new market instance $(n,m,s+1,\\{S_{k}^{\\prime}\\},\\bar{v})$ that preserves the ordering of the slices before subdivision, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G(t)=\\left\\{\\!\\!\\begin{array}{l l}{F(t),}&{\\mathrm{if~}t\\leq\\sum_{k\\in[k^*-1]}\\mathsf{s h a r e}_{k}\\mathrm{~or~}t>\\sum_{k\\in[k^*]}\\mathsf{s h a r e}_{k}}\\\\ {\\mathsf{b a l}_{k^{*}}^{\\prime},}&{\\mathrm{if~}\\sum_{k\\in[k^*-1]}\\mathsf{s h a r e}_{k}<t\\leq\\sum_{k\\in[k^*]}\\mathsf{s h a r e}_{k}^{\\prime}}\\\\ {\\mathsf{b a l}_{s+1}^{\\prime},}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Intuitively, $G$ is obtained by splitting slice $k^{*}$ \u201cin place\u201d from $F$ . One can show that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{unbal}(n,m,s+1,\\{S_{k}^{\\prime}\\},v)\\geq\\int_{0}^{w}\\left(1+\\lambda(t)-\\sqrt{(1-\\lambda(t)^{2}+4G(t)\\cdot\\lambda(t)}\\right)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This is essentially because the alternative definition is oblivious to \u201cordering\u201d of the balancedness quantile function. So, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\mathrm{unbal}(n,m,s+1,\\{S_{k}^{\\prime}\\},v)-\\mathrm{unbal}(n,m,s,\\{S_{k}\\},v)}\\\\ &{\\ge\\,\\int_{\\sum_{k\\in[k^{*}-1]}\\mathrm{share}_{k}}^{\\sum_{k\\in[k^{*}]}\\mathrm{share}_{k}}\\left(\\sqrt{(1-\\lambda(t))^{2}+4F(t)\\cdot\\lambda(t)}-\\sqrt{(1-\\lambda(t))^{2}+4G(t)\\cdot\\lambda(t)}\\right)\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We only need to argue that the right hand side is at least 0. Since $\\lambda$ is weakly decreasing, $F$ is constant, and $G$ is weakly increasing (because ${\\sf b a l}_{k^{*}}^{\\prime}\\leq{\\sf b a l}_{s+1}^{\\prime})$ , in the worst case, $\\lambda$ is constant on the interval of interest here (say $\\lambda(t)\\overline{{=}}\\,\\ell>0)$ ). Then, the right hand side becomes ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{s h a r e}_{k^{*}}\\cdot\\sqrt{(1-\\ell)^{2}+4\\mathsf{b a l}_{k^{*}}\\cdot\\ell}}\\\\ &{\\qquad-\\mathsf{s h a r e}_{k^{*}}^{\\prime}\\cdot\\sqrt{(1-\\ell)^{2}+4\\mathsf{b a l}_{k^{*}}^{\\prime}\\cdot\\ell}-\\mathsf{s h a r e}_{s+1}^{\\prime}\\cdot\\sqrt{(1-\\ell)^{2}+4\\mathsf{b a l}_{s+1}^{\\prime}\\cdot\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here we have ", "page_idx": 6}, {"type": "text", "text": "$\\mathsf{s h a r e}_{k^{*}}^{\\prime}+\\mathsf{s h a r e}_{s+1}^{\\prime}=\\mathsf{s h a r e}_{k^{*}}^{\\prime}\\quad\\mathrm{and}\\quad\\mathsf{b a l}_{k^{*}}^{\\prime}\\cdot\\mathsf{s h a r e}_{k^{*}}^{\\prime}+\\mathsf{b a l}_{s+1}^{\\prime}\\cdot\\mathsf{s h a r e}_{s+1}^{\\prime}\\leq\\mathsf{b a l}_{k^{*}}\\cdot\\mathsf{s h a r e}_{k^{*}}\\,,$ because the two new slices are obtained by subdividing the old slice $k^{*}$ . For brevity, let ", "page_idx": 6}, {"type": "equation", "text": "$$\na=\\mathsf{s h a r e}_{k^{*}}^{\\prime},\\ b=\\mathsf{s h a r e}_{s+1}^{\\prime},\\ c=\\mathsf{s h a r e}_{k^{*}},\\ x=\\mathsf{b a l}_{k^{*}}^{\\prime},\\ y=\\mathsf{b a l}_{s+1}^{\\prime},\\ z=\\mathsf{b a l}_{k^{*}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We only need to show ", "page_idx": 7}, {"type": "equation", "text": "$$\nc{\\sqrt{(1-\\ell)^{2}+4z\\ell}}\\geq a{\\sqrt{(1-\\ell)^{2}+4x\\ell}}+b{\\sqrt{(1-\\ell)^{2}+4y\\ell}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\na+b=c\\quad{\\mathrm{and}}\\quad a x+b y\\leq c z.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Applying Cauchy-Schwarz, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a\\sqrt{(1-\\ell)^{2}+4x\\ell}+b\\sqrt{(1-\\ell)^{2}+4y\\ell}\\leq\\sqrt{(a+b)\\cdot(a((1-\\ell)^{2}+4x l)+b((1-\\ell)^{2}+4y\\ell))}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\sqrt{(a+b)^{2}(1-\\ell)^{2}+(a+b)(4a x+4b y)\\ell}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{c^{2}(1-\\ell)^{2}+4c^{2}z\\ell}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=c\\sqrt{(1-\\ell)^{2}+4z\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 7}, {"type": "text", "text": "3.2 Efficiency under the Balancedness Parametrization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now we are ready to prove our main result, which links auction efficiency with unbalancedness when autobidders perform partially non-uniform bidding. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 (PoA of FPA with partially non-uniform bidding). For any $t\\in[2/5,1]$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(n,m,s,\\{S_{k}\\},v)\\in\\mathbb{Z}_{t}}\\mathsf{P o A}(n,m,s,\\{S_{k}\\},v)=1-\\frac{1}{2}t,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{Z}_{t}=\\{(n,m,s,\\{S_{k}\\},\\pmb{v})~|~{\\mathsf{u n b a l}}(n,m,s,\\{S_{k}\\},\\pmb{v})\\leq t\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The theorem is a corollary of Lemma 1 and Lemma 2, which are stated and proved in Appendix A.   \nBelow we discuss the conceptual implications of Theorem 1. ", "page_idx": 7}, {"type": "text", "text": "Better autobidders lead to worse efficiency. Theorem 1 and Proposition 1 together establish a possibly counterintuitive relation between the capability of autobidders and the efficiency of the auction outcome: Better optimized autobidders generally are capable of implementing more sophisticated non-uniform bidding strategies, which correspond to establishing finer partitions of the entire market into slices. However, by the second bullet of Proposition 1, any refinement of the slices can only lead to higher unbalancedness, which, by Theorem 1, leads to worse auction efficiency. In the extreme case where autobidders are capable of bidding optimally in each individual auction (which, in our model, corresponds to the case in which each auction forms its own slice with balancedness 0), the unbalancedness of the market is 1, and therefore, Theorem 1 states that the PoA of such a market is $1/2$ , matching the bound established in prior work [Liaw et al., 2023, Deng et al., 2022]. In fact, this is precisely the phenomenon observed empirically by Deng et al. [2024]. Our results therefore provide a theoretical explanation for their empirical findings. Together with empirical results from [Deng et al., 2024], our theoretical results further suggest that it is unlikely to achieve better auction efficiency (especially in first-price marketplaces) by using autobidders that are (more) capable of non-uniform bidding in practice. ", "page_idx": 7}, {"type": "text", "text": "The importance of balanced channels / slices. In situations where there are naturally multiple channels or slices (e.g., different ad exchanges operated by different companies, or by different organizations within a single company) [Deng et al., 2023a, Susan et al., 2023], our results also highlight the importance of keeping each channel / slice balanced. In particular, the first bullet of Proposition 1 suggests that if the balancedness of one slice can be improved without hurting the balancedness of the other slices, then the unbalancedness of the entire market decreases, and Theorem 1 guarantees better efficiency overall. In practice, our results suggest that it is better off for different channels / slices to coordinate to achieve higher overall efficiency. ", "page_idx": 7}, {"type": "text", "text": "Even perfectly balanced slices introduce inefficiency. Proposition 1 states that whenever there are at least 2 slices in the market instance, the unbalancedness of the market instance is at least $2/5$ , even if all slices are perfectly balanced. Theorem 1 then implies that the efficiency of such market instances is at most $4/5$ , which suggests that efficiency loss is inevitable whenever the market consists of multiple slices. In contrast, when autobidders perform uniform bidding over the entire market (i.e., when there is only 1 slice), it is known that first-price marketplaces achieve full efficiency [Balseiro et al., 2021a]. In other words, this indicates that one may expect to see a phase transition in market efficiency when moving away from uniform bidding to (even highly restrictive) non-uniform bidding. We also remark that for second-price marketplaces, the PoA is 2 even with perfectly balanced slices, or a single slice. This suggests that first-price marketplaces generally achieve higher efficiency compared to second-price ones. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We thank anonymous reviewers for their helpful feedback. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Gagan Aggarwal, Ashwinkumar Badanidiyuru, and Aranyak Mehta. Autobidding with constraints. In Web and Internet Economics: 15th International Conference, WINE 2019, New York, NY, USA, December 10\u201312, 2019, Proceedings 15, pages 17\u201330. Springer, 2019.   \nGagan Aggarwal, Andres Perlroth, and Junyao Zhao. Multi-channel auction design in the autobidding world. In Proceedings of the 24th ACM Conference on Economics and Computation, EC \u201923, page 21, New York, NY, USA, 2023. Association for Computing Machinery.   \nGagan Aggarwal, Ashwinkumar Badanidiyuru, Santiago R Balseiro, Kshipra Bhawalkar, Yuan Deng, Zhe Feng, Gagan Goel, Christopher Liaw, Haihao Lu, Mohammad Mahdian, et al. Auto-bidding and auctions in online advertising: A survey. ACM SIGecom Exchanges, 22(1):159\u2013183, 2024.   \nSantiago Balseiro, Yuan Deng, Jieming Mao, Vahab Mirrokni, and Song Zuo. Robust auction design in the auto-bidding world. Advances in Neural Information Processing Systems, 34:17777\u201317788, 2021a.   \nSantiago R Balseiro, Yuan Deng, Jieming Mao, Vahab S Mirrokni, and Song Zuo. The landscape of auto-bidding auctions: Value versus utility maximization. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 132\u2013133, 2021b.   \nXi Chen, Christian Kroer, and Rachitesh Kumar. The complexity of pacing for second-price auctions. In EC, 2021. URL https://arxiv.org/abs/2103.13969.   \nYuan Deng, Jieming Mao, Vahab Mirrokni, and Song Zuo. Towards efficient auctions in an auto-bidding world. In Proceedings of the Web Conference 2021, pages 3965\u20133973, 2021.   \nYuan Deng, Jieming Mao, Vahab Mirrokni, Hanrui Zhang, and Song Zuo. Efficiency of the first-price auction in the autobidding world. arXiv preprint arXiv:2208.10650, 2022.   \nYuan Deng, Negin Golrezaei, Patrick Jaillet, Jason Cheuk Nam Liang, and Vahab Mirrokni. Multi-channel autobidding with budget and ROI constraints. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 7617\u20137644. PMLR, 23\u201329 Jul 2023a.   \nYuan Deng, Mohammad Mahdian, Jieming Mao, Vahab Mirrokni, Hanrui Zhang, and Song Zuo. Efficiency of the generalized second-price auction for value maximizers. arXiv preprint arXiv:2310.03105, 2023b.   \nYuan Deng, Jieming Mao, Vahab Mirrokni, Yifeng Teng, and Song Zuo. Non-uniform bid-scaling and equilibria for different auctions: An empirical study. In Proceedings of the ACM on Web Conference 2024, page 256\u2013266, 2024.   \nMichal Feldman, Hu Fu, Nick Gravin, and Brendan Lucier. Simultaneous auctions are (almost) efficient. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 201\u2013210, 2013.   \nYiding Feng, Brendan Lucier, and Aleksandrs Slivkins. Strategic budget selection in a competitive autobidding world, 2023.   \nDarrell Hoy, Samuel Taggart, and Zihe Wang. A tighter welfare guarantee for first-price auctions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 132\u2013137, 2018.   \nYaonan Jin and Pinyan Lu. First price auction is $1-1/e^{2}$ efficient. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 179\u2013187. IEEE Computer Society, 2022.   \nJuncheng Li and Pingzhong Tang. Vulnerabilities of single-round incentive compatibility in auto-bidding: Theory and evidence from roi-constrained online advertising markets. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. International Joint Conferences on Artificial Intelligence Organization, 2024. Main Track.   \nChristopher Liaw, Aranyak Mehta, and Andres Perlroth. Efficiency of non-truthful auctions in auto-bidding: The power of randomization. In Proceedings of the ACM Web Conference 2023, WWW \u201923, page 3561\u20133571, New York, NY, USA, 2023. Association for Computing Machinery.   \nChristopher Liaw, Aranyak Mehta, and Wennan Zhu. Efficiency of non-truthful auctions in auto-bidding with budget constraints. In Proceedings of the ACM Web Conference 2024, WWW \u201924, 2024.   \nAranyak Mehta. Auction design in an auto-bidding setting: Randomization improves efficiency beyond vcg. In Proceedings of the ACM Web Conference 2022, pages 173\u2013181, 2022.   \nRenato Paes Leme, Georgios Piliouras, Jon Schneider, Kelly Spendlove, and Song Zuo. Complex dynamics in autobidding systems. In Proceedings of the 25th ACM Conference on Economics and Computation, 2024.   \nTim Roughgarden, Vasilis Syrgkanis, and Eva Tardos. The price of anarchy in auctions. Journal of Artificial Intelligence Research, 59:59\u2013101, 2017.   \nFransisca Susan, Negin Golrezaei, and Okke Schrijvers. Multi-platform budget management in ad markets with non-ic auctions. arXiv preprint arXiv:2306.07352, 2023.   \nVasilis Syrgkanis and Eva Tardos. Composable and efficient mechanisms. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 211\u2013220, 2013. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Lemma 1. For each market instance $(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ , ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\mathsf{P o A}(n,m,s,\\{S_{k}\\},v)\\geq1-\\frac{1}{2}\\mathsf{u n b a l}(n,m,s,\\{S_{k}\\},v).\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Proof. Fix a market instance $(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ , together with an equilibrium given by bid multipliers $\\pmb{\\theta}$ . Without loss of generality, suppose the optimal liquid welfare is 1, i.e., ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\sum_{j\\in[m]}\\operatorname*{max}_{i\\in[n]}v_{i,j}=1.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "For each bidder $i\\in[n]$ and auction $j\\in[m]$ , let the \u201closs of efficiency\u201d $\\mathsf{l o s s}_{i,j}$ caused by $i$ in $j$ be: ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\mathsf{l o s s}_{i,j}=x_{i,j}\\cdot(v_{\\mathsf{r w}(j),j}-v_{i,j}).\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Moreover, abusing notation, for each bidder $i\\in[n]$ and each slice $k\\in[s]$ , let ", "page_idx": 10}, {"type": "equation", "text": "$$\n|\\mathsf{o s s}_{i,k}=\\sum_{j\\in S_{k}}|\\mathsf{o s s}_{i,j}\\quad\\mathsf{a n d}\\quad|\\mathsf{o s s}_{k}=\\sum_{i\\in[n]}|\\mathsf{o s s}_{i,k}.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "We only need to show ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n],j\\in[m]}v_{i,j}\\cdot x_{i,j}\\geq1-\\frac{1}{2}\\mathsf{u n b a l}(n,m,s,\\{S_{k}\\},v),\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "which is equivalent to ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\sum_{k\\in[s]}\\mathsf{l o s s}_{k}\\leq\\frac{1}{2}\\mathsf{u n b a l}(n,m,s,\\{S_{k}\\},v).\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "To upper bound the total loss of efficiency, we first establish a tradeoff between the total loss on slice $k$ and the total \u201coverpayment\u201d on the same slice. This will involve upper bounding the loss and lower bounding the overpayment. Fix a slice $k$ and partition auctions on $k$ into 3 sets: ", "page_idx": 10}, {"type": "text", "text": "\u2022 $A_{k}$ : auctions where the rightful winner wins with a bid multiplier smaller than 1, i.e., ", "page_idx": 10}, {"type": "equation", "text": "$$\nA_{k}=\\{j\\in S_{k}\\mid x_{\\mathsf{r w}(j),j}=1\\;\\mathrm{and}\\;\\theta_{\\mathsf{r w}(j),k}<1\\};\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "\u2022 $B_{k}$ : auctions where the rightful winner wins with a bid multiplier larger than or equal to 1, ", "page_idx": 10}, {"type": "equation", "text": "$$\nB_{k}=\\{j\\in S_{k}\\mid x_{\\mathsf{r w}(j),j}=1\\mathrm{~and~}\\theta_{\\mathsf{r w}(j),k}\\geq1\\};\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "\u2022 $C_{k}$ : auctions where the rightful winner does not win, i.e., ", "page_idx": 10}, {"type": "equation", "text": "$$\nC_{k}=\\{j\\in S_{k}\\mid x_{\\sf r w(j),j}=0\\}.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "For brevity, we define the \u201creserved\u201d share $\\mathsf{r e s}_{k}$ to be the market share of set $A_{k}$ , i.e., $\\mathsf{r e s}_{k}=$ shar $\\mathtt{a}_{A_{k}}\\le\\mathtt{s h a r e}_{k}$ . ", "page_idx": 10}, {"type": "text", "text": "Consider any bidder $i$ . First observe that for each auction $j\\in S_{k}$ , $\\mathsf{l o s s}_{i,j}>0$ only if the following happen simultaneously: ", "page_idx": 10}, {"type": "text", "text": "\u2022 Bidder $i$ must outbid the rightful winner in auction $j$ , i.e., $p_{i,j}=b_{i,j}\\geq b_{\\mathsf{r w}(j),j}$ . ", "page_idx": 10}, {"type": "text", "text": "\u2022 Since the rightful winner $\\mathsf{r w}(j)$ is not winning in $j$ , we must have $\\theta_{\\mathsf{r w}(j),k}\\,\\geq\\,1$ , which implies brw(j),j \u2265vrw(j),j. ", "page_idx": 10}, {"type": "text", "text": "Given the above properties, for each auction $j~\\in~S_{k}$ where $\\mathsf{l o s s}_{i,j}\\,>\\,0$ , we have $x_{i,j}\\,=\\,1\\,$ and $\\theta_{i,k}\\cdot v_{i,j}=b_{i,j}\\geq b_{\\mathsf{r w}(j),j}\\geq v_{\\mathsf{r w}(j),j}$ . As a result, ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\mathsf{l o s s}_{i,j}=v_{\\mathsf{r w}(j),j}-v_{i,j}\\le v_{\\mathsf{r w}(j),j}-v_{\\mathsf{r w}(j),j}/\\theta_{i,k}=\\frac{\\theta_{i,k}-1}{\\theta_{i,k}}\\cdot v_{\\mathsf{r w}(j),j}\\le\\frac{\\theta_{i^{*},k}-1}{\\theta_{i^{*},k}}\\cdot v_{r w(j),j},\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "where $i^{*}\\in[n]$ is the bidder $i^{*}$ who has the largest bid multiplier on slice $k$ . Recall that $C_{k}$ is the set of auctions in $S_{k}$ where $\\sum_{i}\\vert0\\mathtt{S S}_{i,j}>0$ . Since for any auction $j\\in S_{k}$ where $\\mathsf{r w}(j)=i^{*}$ , $x_{i^{*},j}=1$ , we must have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathtt{s h a r e}_{C_{k}}\\leq\\mathtt{s h a r e}_{k}-\\mathtt{r e s}_{k}-\\mathtt{s h a r e}_{i^{*},k}\\leq(1-\\mathtt{b a l}_{k})\\cdot\\mathtt{s h a r e}_{k}-\\mathtt{r e s}_{k}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Summing the upper bound for $\\mathsf{l o s s}_{i,j}$ over $i\\in[n]$ and $j\\in S_{k}$ , and plugging in the above inequality involving share $C_{k}$ , we get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\cos s_{k}=\\sum_{i\\in[n],j\\in S_{k}:\\log_{k,j}>0}\\log s_{k,j}}}\\\\ &{\\leq\\sum_{j\\in C_{k}}\\frac{\\theta_{i^{*},k}-1}{\\theta_{i^{*},k}}\\cdot v_{r w(j),j}}\\\\ &{=\\frac{\\theta_{i^{*},k}-1}{\\theta_{i^{*},k}}\\cdot\\mathsf{s h a r e}_{C_{k}}}\\\\ &{\\leq\\frac{\\theta_{i^{*},k}-1}{\\theta_{i^{*},k}}\\cdot((1-\\mathsf{b a l}_{k})\\cdot\\mathsf{s h a r e}_{k}-r\\mathsf{e s}_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "This is our upper bound on the total loss, which will be useful later. ", "page_idx": 11}, {"type": "text", "text": "Now we lower bound the total overpayment. Note that $x_{i,j}=1$ implies $b_{i,j}=p_{i,j}$ . So whenever $\\mathsf{l o s s}_{i,j}>0$ , based on the observations in the two bullet points above, we must have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{l o s s}_{i,j}\\le x_{i,j}\\cdot(b_{\\mathsf{r w}(j),j}-v_{i,j})\\le x_{i,j}\\cdot(b_{i,j}-v_{i,j})=x_{i,j}\\cdot(p_{i,j}-v_{i,j})=p_{i,j}-x_{i,j}\\cdot v_{i,j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "In words, $i$ can only cause as much loss of efficiency in $j$ as the amount $i$ overpays in $j$ . To this end, let ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathsf{s u r}_{i,j}=x_{i,j}\\cdot v_{i,j}-p_{i,j}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "be the surplus $i$ gets from $j$ . We have: for each $i\\in[n]$ and $j\\in[m]$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathsf{l o s s}_{i,j}\\leq\\operatorname*{max}\\{0,-\\mathsf{s u r}_{i,j}\\}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Summing over $i\\in[n]$ and $j\\in C_{k}$ , we get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n],j\\in L_{k}}\\operatorname*{max}\\{0,-\\mathsf{s u r}_{i,j}\\}\\geq\\sum_{i\\in[n],j\\in L_{k}}\\mathsf{l o s s}_{i,j}=\\mathsf{l o s s}_{k}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "This is one of the two inequalities that we will use to lower bound the total overpayment. ", "page_idx": 11}, {"type": "text", "text": "On the other hand, consider the bidder $i^{*}$ who has the largest bid multiplier on slice $k$ . Recall that for any auction $j\\in S_{k}$ where $\\mathsf{r w}(j)=i^{*}$ , $x_{i^{*},j}=1$ . This means $i^{*}$ overpays in auctions where $i^{*}$ is the rightful winner without causing any loss of efficiency. In fact, the amount of overpayment here can be bounded as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j\\in S_{k}:\\mathsf{r w}(j)=i^{*}}-\\mathsf{s u r}_{i^{*},j}=\\displaystyle\\sum_{j\\in S_{k}:\\mathsf{r w}(j)=i^{*}}(\\theta_{i^{*},k}-1)\\cdot v_{i^{*},j}}&{}\\\\ {\\displaystyle=(\\theta_{i^{*},k}-1)\\cdot\\mathsf{s h a r e}_{i^{*},k}}&{}\\\\ {\\displaystyle\\geq(\\theta_{i^{*},k}-1)\\cdot\\mathsf{s h a r e}_{k}\\cdot\\mathsf{b a l}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "So, combining this inequality with the one above, we get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n],j\\in S_{k}}\\operatorname*{max}\\{0,-\\mathsf{s u r}_{i,j}\\}\\geq(\\theta_{i^{*},k}-1)\\cdot\\mathsf{s h a r e}_{k}\\cdot\\mathsf{b a l}_{k}+\\mathsf{l o s s}_{k}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "This is the desired lower bound on the total overpayment. ", "page_idx": 11}, {"type": "text", "text": "Now let us put the bounds for the loss and that for the overpayment together into a tradeoff between the two quantities. For brevity, let ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathsf{s u r}_{k}^{-}=\\sum_{i\\in[n],j\\in S_{k}}\\operatorname*{max}\\{0,-\\mathsf{s u r}_{i,j}\\}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Recall the upper bound we have on the total loss: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\displaystyle\\lvert\\mathsf{o s s}_{k}\\leq\\frac{\\theta_{i^{*},k}-1}{\\theta_{i^{*},k}}\\cdot((1-\\mathsf{b a}\\mathsf{l}_{k})\\cdot\\mathsf{s h a r e}_{k}-\\mathsf{r e s}_{k}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This gives us ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{i^{*},k}\\geq\\frac{(1-{\\mathsf{b a l}}_{k})\\cdot{\\mathsf{s h a r e}}_{k}-{\\mathsf{r e s}}_{k}}{(1-{\\mathsf{b a l}}_{k})\\cdot{\\mathsf{s h a r e}}_{k}-{\\mathsf{r e s}}_{k}-{\\mathsf{l o s s}}_{k}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Plugging this into the lower bound we have on the total overpayment, we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\operatorname{sur}_{k}^{-}\\geq\\frac{\\left|\\operatorname{oss}_{k}\\right.}{\\left(1-\\mathsf{b a l}_{k}\\right)\\cdot\\mathsf{s h a r e}_{k}-\\mathsf{r e s}_{k}-\\mathsf{l o s s}_{k}}\\cdot\\mathsf{s h a r e}_{k}\\cdot\\mathsf{b a l}_{k}+\\mathsf{l o s s}_{k}}\\\\ {\\Longrightarrow\\ l o s s_{k}^{2}-(\\mathsf{s h a r e}_{k}-\\mathsf{r e s}_{k}+\\mathsf{s u r}_{k}^{-})\\cdot\\mathsf{l o s s}_{k}+((1-\\mathsf{b a l}_{k})\\cdot\\mathsf{s h a r e}_{k}-\\mathsf{r e s}_{k})\\cdot\\mathsf{s u r}_{k}^{-}\\geq0.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The latter is a quadratic inequality in $\\mathtt{l o s s}_{k}$ , which in our case implies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathsf{l o s s}_{k}\\leq\\frac{1}{2}\\left(\\mathsf{s h a r e}_{k}-\\mathsf{r e s}_{k}+\\mathsf{s u r}_{k}^{-}-\\sqrt{(\\mathsf{s h a r e}_{k}-\\mathsf{r e s}_{k}-\\mathsf{s u r}_{k}^{-})^{2}+4\\mathsf{s h a r e}_{k}\\cdot\\mathsf{b a l}_{k}\\cdot\\mathsf{s u r}_{k}^{-}}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The above tradeoff is useful because together with an additional constraint on $\\{\\mathsf{r e s}_{k}\\}$ and $\\{\\mathsf{s u r}_{k}^{-}\\}$ , it gives an upper bound on $\\sum_{k}\\vert0\\mathsf{s s}_{k}$ , which is the quantity we want to bound. The additional constraint is from bidders\u2019 RoS constraints. For each bidder $i$ , the RoS constraint requires that $\\begin{array}{r}{\\sum_{j\\in[m]}\\mathsf{s u r}_{i,j}\\ge0}\\end{array}$ , which implies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n],j\\in[m]}\\mathsf{s u r}_{i,j}\\geq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "On the other hand, for each auction $j$ $,\\mathsf{s u r}_{i,j}>0$ only if $j\\in A_{\\mathsf{s l i c e}(j)}$ (i.e., $\\theta_{\\mathsf{r w}(j),\\mathsf{s i i c e}(j)}<1)$ . This means ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n],j\\in[m]}\\operatorname*{max}\\{0,\\mathsf{s u r}_{i,j}\\}\\leq\\sum_{k\\in[s]}\\mathsf{s h a r e}_{A_{k}}=\\sum_{k\\in[s]}\\mathsf{r e s}_{k}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "As a result, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{k\\in[s]}\\mathsf{s u r}_{k}^{-}\\leq\\sum_{i\\in[n],j\\in[m]}\\operatorname*{max}\\{0,\\mathsf{s u r}_{i,j}\\}\\leq\\sum_{k\\in[s]}\\mathsf{r e s}_{k}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This is the additional constraint we need. ", "page_idx": 12}, {"type": "text", "text": "Now the problem of upper bounding $\\sum_{k\\in[s]}{\\left|\\cos\\mathsf{s}_{k}\\right|}$ reduces to solving the following optimization problem (replacing all variables with shorthands: $x_{k}$ for $\\mathsf{r e s}_{k}$ , $y_{k}$ for $\\mathsf{s u r}_{k}^{-}$ , and $z_{k}$ for $\\mathtt{l o s s}_{k}$ ): ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k\\in[s]}z_{k}}\\\\ &{\\forall k\\in[s]:~z_{k}\\leq\\frac{1}{2}\\left(\\mathsf{s h a r e}_{k}-x_{k}+y_{k}-\\sqrt{(\\mathsf{s h a r e}_{k}-x_{k}-y_{k})^{2}+4\\mathsf{s h a r e}_{k}\\cdot\\mathsf{b a l}_{k}\\cdot y_{k}}\\right)}\\\\ &{\\displaystyle\\sum_{k\\in[s]}y_{k}\\leq\\sum_{k\\in s}x_{k}}\\\\ &{\\forall k\\in[s]:~0\\leq x_{k}\\leq\\mathsf{s h a r e}_{k}}\\\\ &{\\forall k\\in[s]:~y_{k}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The rest of the proof is devoted to solving this optimization problem. ", "page_idx": 12}, {"type": "text", "text": "First we make some structural observations. Let $f_{k}(x_{k},y_{k})$ be the right hand side of the upper bound on $z_{k}$ . When optimality is achieved: ", "page_idx": 12}, {"type": "text", "text": "\u2022 $z_{k}=f_{k}(x_{k},y_{k})$ for all $k\\in[s]$ , and $\\begin{array}{r}{\\sum_{k\\in[s]}y_{k}=\\sum_{k\\in[s]}s_{k}.}\\end{array}$ . The latter is simply because $f_{k}(x_{k},y_{k})$ is strictly increasing in $y_{k}$ .   \n\u2022 For each $k\\in[s]$ , if $x_{k}\\geq\\left(1-{\\mathsf{b a l}}_{k}\\right)\\cdot{\\mathsf{s h a r e}}_{k}$ , then $y_{k}=0$ and $x_{k}={\\mathsf{s h a r e}}_{k}$ . This is because in such cases, $f_{k}(x_{k},y_{k})\\le0$ , and $f_{k}(x_{k},y_{k})=0$ when $y_{k}=0$ . So we would save our budget for $\\{y_{k^{\\prime}}\\}$ on slice $k$ and spend it elsewhere. For similar reasons, we would max out $x_{k}$ to create more budget for $\\{y_{k^{\\prime}}\\}$ . In particular, this property implies that $x_{k}={\\mathsf{s h a r e}}_{k}$ and $y_{k}>0$ cannot happen simultanously. ", "page_idx": 12}, {"type": "text", "text": "With the above observations enforced as constraints (which does not change the optimal objective value), we further relax the problem. In particular, for each slice $k$ , we replace the constraint that $z_{k}\\leq f_{k}(x_{k},y_{k})$ with $z_{k}\\leq g_{k}(x_{k},y_{k})$ , where ", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{k}(x_{k},y_{k})={\\frac{1}{2}}\\left(\\mathsf{s h a r e}_{k}-x_{k}+y_{k}-{\\sqrt{(\\mathsf{s h a r e}_{k}-x_{k}-y_{k})^{2}+4(\\mathsf{s h a r e}_{k}-x_{k})\\cdot\\mathsf{b a l}_{k}\\cdot y_{k}}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This relaxation essentially means we imagine slices are arbitrarily divisible, and we can divide each slice into two with the same balancedness, whose market shares sum to that of the one before division. As we will see, the relaxation has no cost under our parametrization, because the hard market instances are precisely the ones that divide slices in the way that achieves the maximum objective value in the relaxed problem. We consider this relaxed problem with the additional constraints (in the bullet points) from now on. ", "page_idx": 13}, {"type": "text", "text": "For this relaxed problem, we make more structural observations when optimality is achieved: ", "page_idx": 13}, {"type": "text", "text": "\u2022 There is at most one slice $k$ where $0<x_{k}<\\mathsf{s h a r e}_{k}$ . This is because fixing $\\textstyle\\sum_{k\\in[s]}x_{k}$ , the most efficient way to distribute this sum into slices is greedy. That is, we first find the last slice $s$ which has the minimum balancedness ${\\sf b a l}_{s}$ and increase $x_{s}$ . If we reach the limit, i.e., $x_{s}=\\mathsf{s h a r e}_{s}$ , then we move on to the next slice $s-1$ among the remaining ones with the minimum balancedness and repeat this, etc., until we reach the desired sum. One may check that this procedure in fact induces the sub-optimization problem over $\\{y_{k}\\}$ and $\\scriptstyle\\{z_{k}\\}$ with the largest optimal objective value given k\u2208[s] xk. ", "page_idx": 13}, {"type": "text", "text": "\u2022 For any $k,k^{\\prime}\\in[s]$ where $\\operatorname*{min}\\{y_{k},y_{k^{\\prime}}\\}>0$ , we must have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial g_{k}(x_{k},y_{k})}{\\partial y_{k}}=\\frac{\\partial g_{k^{\\prime}}(x_{k^{\\prime}},y_{k^{\\prime}})}{\\partial y_{k^{\\prime}}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "because otherwise we can locally adjust $y_{k}$ and $y_{k^{\\prime}}$ (while keeping the sum unchanged) and get a strictly larger objective value. ", "page_idx": 13}, {"type": "text", "text": "Putting the above observations together, we conclude that the optimal solution to the optimization problem must have the following structure: There is an integer $s^{\\prime}\\in[s]$ , a positive real number $C\\in[0,1]$ , and \u201ceffective\u201d market shares $\\{\\mathsf{s h a r e}_{k}^{\\prime}\\}$ , such that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{k\\in[s^{\\prime}]}\\mathsf{s h a r e}_{k}^{\\prime}=C.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 $\\boldsymbol{x}_{k}=\\mathsf{s h a r e}_{k}-\\mathsf{s h a r e}_{k}^{\\prime}$ for each $k\\in[s]$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 There exists a real number $D>0$ such that $\\begin{array}{r}{\\frac{\\mathrm{d}h_{k}\\left(y_{k}\\right)}{\\mathrm{d}y_{k}}=D}\\end{array}$ for each $k\\in[s^{\\prime}]$ , where ", "page_idx": 13}, {"type": "equation", "text": "$$\nh_{k}(y_{k})=\\frac{1}{2}\\left(\\mathsf{s h a r e}_{k}^{\\prime}+y_{k}-\\sqrt{(\\mathsf{s h a r e}_{k}^{\\prime}-y_{k})^{2}+4\\mathsf{s h a r e}_{k}^{\\prime}\\cdot\\mathsf{b a l}_{k}\\cdot y_{k}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In particular, fixing $C$ $,y_{k}/\\mathsf{s h a r e}_{k}^{\\prime}$ should only depend on $\\mathsf{b a l}_{k}$ ", "page_idx": 13}, {"type": "text", "text": "\u2022 For each $k>s^{\\prime}$ , $y_{k}=0$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{k\\in[s^{\\prime}]}y_{k}=1-C.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Below we solve the relaxed problem given all simplifying observations above. The key step is to pin down the dependency of $y_{k}\\bar{/}\\mathsf{s h a r e}_{k}^{\\prime}$ on $\\mathsf{b a l}_{k}$ given $C$ . In fact, for each $k\\in[s^{\\prime}]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\exists D,\\,\\forall k\\in[s^{\\prime}],\\,\\frac{\\mathrm{d}h_{k}(y_{k})}{\\mathrm{d}y_{k}}=D}\\\\ &{\\implies\\exists D,\\,\\forall k\\in[s^{\\prime}],\\,\\frac{(y_{k}-\\mathrm{e}\\mathrm{h}a r_{k}^{\\prime})+2\\mathrm{sh}a r_{k}^{\\prime}}{\\sqrt{(y_{k}-\\mathrm{sh}a r_{k}^{\\prime})^{2}+4\\mathrm{sh}a r_{k}^{\\prime}}\\cdot\\mathrm{ba}h_{k}\\cdot\\mathrm{~sh}_{k}}=D}\\\\ &{\\implies\\exists D,\\,\\forall k\\in[s^{\\prime}],\\,\\frac{(y_{k}-\\mathrm{e}\\mathrm{h}a r_{k}^{\\prime})^{2}+4\\mathrm{sh}a r_{k}^{\\prime}\\cdot\\mathrm{ba}h_{k}\\cdot(y_{k}-\\mathrm{sh}a r_{k}^{\\prime})+4\\mathrm{sh}a r_{k}^{\\prime}^{\\prime}\\cdot\\mathrm{ba}h_{k}^{\\prime}}{(y_{k}-\\mathrm{sh}a r_{k}^{\\prime})^{2}+4\\mathrm{sh}a r_{k}^{\\prime}\\cdot\\mathrm{ba}h_{k}^{\\prime}\\cdot\\mathrm{~sh}_{k}}=D}\\\\ &{\\implies\\exists D,\\,\\forall k\\in[s^{\\prime}],\\,\\frac{\\mathrm{e}\\mathrm{h}a r_{k}^{\\prime}\\cdot\\mathrm{e}\\mathrm{h}a\\ r_{k}^{\\prime}}{(y_{k}-\\mathrm{e}\\mathrm{h}a r_{k}^{\\prime})^{2}+4\\mathrm{sh}a r_{k}^{\\prime}\\cdot\\mathrm{ba}h_{k}\\cdot\\mathrm{~sh}_{k}}=D}\\\\ &{\\implies\\exists D,\\,\\forall k\\in[s^{\\prime}],\\,\\frac{\\mathrm{sh}a r_{k}^{\\prime}\\cdot\\mathrm{e}\\mathrm{h}a\\cdot(b^{2}-\\mathrm{ba}h_{k}\\cdot(1-\\mathrm{ba}h_{k})}{(y_{k}-\\mathrm{sh}a r_{k}^{\\prime})^{2}+4\\mathrm{sh}a r_{k}^{\\prime}\\cdot\\mathrm{ba}h_{k}\\cdot\\mathrm{~sh}_{k}}=D}\\\\ &{\\implies\\exists D,\\,\\forall k\\in[s^{\\prime}],\\,\\frac{(y_{k}-\\mathrm{e}\\mathrm{h}a r_{k}^{\\prime})^{2}+4\\mathrm{sh}a r_{k}^{\\prime}\\cdot\\mathrm{ba}h_{k}\\cdot\\mathrm{~sh}_{k}}{\\sqrt{(y_{k\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, observe that the sign in the above expression must be the same for all $k\\in[s^{\\prime}]$ in order for the derivatives to be the same, which means ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exists D\\in\\mathbb{R},\\,\\forall k\\in[s^{\\prime}],\\,y_{k}={\\mathsf{s h a r e}}_{k}^{\\prime}\\cdot\\left((1-2{\\mathsf{b a l}}_{k})+{\\sqrt{{\\mathsf{b a l}}_{k}\\cdot(1-{\\mathsf{b a l}}_{k})}}\\cdot D\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Abusing notation, let ", "page_idx": 14}, {"type": "equation", "text": "$$\ny_{k}(D)=\\mathsf{s h a r e}_{k}^{\\prime}\\cdot\\left((1-2\\mathsf{b a l}_{k})+\\sqrt{\\mathsf{b a l}_{k}\\cdot(1-\\mathsf{b a l}_{k})}\\cdot D\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that here $D$ can be either positive or negative. ", "page_idx": 14}, {"type": "text", "text": "Given the above observation, when optimality is achieved, the solution is essentially parametrized by the parameter $C$ . In particular, $s^{\\prime}$ and $\\{{\\mathsf{s h a r e}}^{\\prime}\\}$ are uniquely determined by $C$ , and the parameter $D$ is unique given $C$ .5 In fact, since $y_{k}(D)$ is monotone for each $k\\in[s^{\\prime}]$ , there is a unique $D$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{k\\in[s^{\\prime}]}y_{k}(D)=1-C.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So we may alternatively write $y_{k}(C)=y_{k}(D(C))$ for the unique choice of $y_{k}$ given $C$ . Below we link the maximum value of the objective to the unbalancedness of the market instance and conclude the proof. ", "page_idx": 14}, {"type": "text", "text": "Recall that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha(b,u)=1-2b+\\sqrt{b\\cdot(1-b)}\\cdot u.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So ", "page_idx": 14}, {"type": "equation", "text": "$$\ny_{k}(C)={\\mathsf{s h a r e}}_{k}^{\\prime}\\cdot\\alpha({\\mathsf{b a l}}_{k},D(C)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that here shar $\\boldsymbol{\\mathrm{e}}_{k}^{\\prime}$ depends on $C$ , and we omit this dependency for brevity. Given the choice of $D(C)$ , since the slices are indexed such that $\\mathsf{b a l}_{k}$ is weakly increasing, we must have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{C}\\alpha(F(t),D(C))\\,\\mathrm{d}t=\\sum_{k\\in[s^{\\prime}]}\\alpha(\\mathsf{b a l}_{k},D(C))\\cdot\\mathsf{s h a r e}_{k}^{\\prime}=\\sum_{k\\in[s^{\\prime}]}y_{k}(C)=1-C,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $F(t)$ is the balancedness quantile function of the market instance. In other words, the choice of $D$ given $C$ is precisely $D(C)={\\bar{\\beta}}(C)$ as defined in Definition 4. Then the objective can be bounded ", "page_idx": 14}, {"type": "text", "text": "as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k\\in\\mathbb{N}_{F}^{1}}{\\sum}h_{k}(y_{k})}\\\\ &{=\\underset{k\\in\\mathbb{N}_{F}^{1}}{\\sum}\\left(\\operatorname{share}_{i}^{t}+y_{k}-\\sqrt{(\\operatorname{share}_{k}^{t}-y_{k})^{2}+4\\operatorname{share}_{k}^{t}\\cdot\\operatorname{bal}_{k}\\cdot y_{k}}\\right)}\\\\ &{=\\underset{k\\in\\mathbb{N}_{F}^{1}}{\\sum}\\frac{1}{2}\\mathrm{exin}e_{k}^{t}\\cdot\\Big(1+y_{k}/\\mathrm{exin}e_{k}^{t}-\\sqrt{(1-y_{k}/\\operatorname{share}_{k}^{t})^{2}+4\\operatorname{shal}_{k}\\cdot y_{k}/\\operatorname{share}_{k}^{t}}\\Big)}\\\\ &{=\\underset{k\\in\\mathbb{N}_{F}^{1}}{\\sum}\\frac{1}{2}\\mathrm{exin}e_{k}^{t}\\cdot\\Big(1+\\alpha\\mathsf{b a r e}_{k}^{t}(\\Gamma)-\\sqrt{(1-\\alpha(\\ensuremath{\\mathbf{bare}}_{k},\\beta(\\Gamma))^{2}+4\\ensuremath{\\mathbf{bal}}_{k}\\cdot\\ensuremath{\\mathbf{\\beta}}_{0}(\\ensuremath{\\mathbf{bare}}_{k}^{t}))}}\\\\ &{=\\underset{k\\in\\mathbb{N}_{F}^{1}}{\\sum}\\frac{1}{2}\\mathrm{exin}e_{k}^{t}\\cdot\\Big(1+\\alpha(\\ensuremath{\\mathbf{bal}}_{k},\\beta(\\Gamma))-\\sqrt{(1-\\alpha(\\ensuremath{\\mathbf{bare}}_{k},\\beta(\\Gamma))^{2}+4\\ensuremath{\\mathbf{bal}}_{k}\\cdot\\ensuremath{\\mathbf{\\beta}}_{0}(\\ensuremath{\\mathbf{bal}}_{k},\\beta(\\Gamma))})}\\\\ &{=\\frac{1}{2}\\int_{0}^{t}\\Big(1+\\alpha(F(t),\\beta(\\Gamma))-\\sqrt{(1-\\alpha(F(t),\\beta(\\Gamma))^{2}+4F(t)\\cdot\\alpha(F(t),\\beta(\\Gamma))}\\Big)\\,d t}\\\\ &{\\leq\\frac{1}{2}\\underset{k\\in\\mathbb{N}_{F}^{1}}{\\sum}\\Big(\\int_{0}^{\\infty}\\Big(1+\\alpha(F(t),\\beta(w))-\\sqrt{(1-\\alpha(F(t),\\beta(w))^{2}+4F(t)\\cdot\\alpha(F(t) \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This concludes the proof of the lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. For each $t\\in[2/5,1]$ , there exists a market instance $(n,m,s,\\{S_{k}\\},{\\boldsymbol{v}})$ which satisfies unba $(n,m,s,\\{S_{k}\\},\\pmb{v})=\\dot{\\pmb{t}}$ and $\\mathsf{P o A}(n,m,s,\\{S_{k}\\},\\boldsymbol{v})=1-t/2$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Recall that the proof of Proposition 1 establishes that unbal $(F)$ (abusing notation here since unbal depends only on $F$ ) weakly increases whenever $F$ pointwise weakly decreases. Moreover, unba $(F)$ is clearly continuous in $F$ (where we use any natural norm for the space where $F$ resides, e.g., $L_{1}$ ). These properties together ensures that for any $t\\in[2/5,1]$ , there exists some $F$ such that unbal $(F)=t$ \u2014 in fact, there exists a constant function $F$ such that unba $(F)=t$ , i.e., there exists $f\\,\\in\\,[0,1/2]$ such that the function $F$ where $F(x)\\,=\\,f$ for all $x\\in[0,1]$ satisfies unba $(F)\\,=\\,t$ . So, we only need to show that given any constant function $F$ where (1) $\\stackrel{\\cdot}{F}(x)$ is constantly $f$ for some $f\\,\\in\\,[0,1/2]$ and (2) unbal $(F)=t$ , there exists a market instance $(n,\\dot{m},s,\\{S_{k}\\},{\\pmb v})$ whose balancedness quantile function is $F$ , and $\\mathsf{P o A}(n,m,s,\\{S_{k}\\},\\boldsymbol{v})\\leq1-t/2$ . ", "page_idx": 15}, {"type": "text", "text": "We construct such a market instance with $n=3$ bidders, $m=6$ auctions, and $s=3$ slices where each slice contains precisely 2 auctions. Let $S_{1}=\\{1,2\\}$ , $S_{2}=\\{3,4\\}$ , and $S_{3}=\\{5,6\\}$ . Intuitively, we will construct valuations together with an equilibrium, where (1) on $S_{1}$ , bidder 1 steals from bidder 3, (2) on $S_{2}$ , bidder 2 steals from bidder 3, and (3) on $S_{3}$ , both bidder 1 and bidder 2 win their market share without any competition. Below we choose the exact valuations that implement this plan. ", "page_idx": 15}, {"type": "text", "text": "Let $w\\ \\in\\ [0,1]$ be a parameter to be optimized later. The valuations we choose depend on $f$ and $w$ . Below we will only specify the non-zero part of the valuations. Consider $S_{3}$ first: We let $v_{1,5}=f\\!\\cdot\\!(1\\!-\\!w)$ and $v_{2,6}=\\bar{(}1-\\dot{f})\\cdot(1-w)$ . On slice $S_{1}$ , we let $v_{1,1}=f^{2}\\cdot w,v_{3,2}=f\\cdot(1-f)\\cdot w$ , and $v_{1,2}$ be such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\frac{v_{3,2}}{v_{1,2}}-1\\right)\\cdot(v_{1,1}+v_{1,2})=f\\cdot(1-w).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, the choice of $v_{1,2}$ guarantees that when bidder 1 pays precisely the minimum amount to \u201csteal\u201d the entire market share of bidder 3 on $S_{1}$ , bidder 1\u2019s overall buyer surplus is 0. Similarly, on slice $S_{2}$ , we let $v_{2,3}=f\\cdot(1-f)\\cdot w,v_{3,4}=(1-f)^{2}\\cdot w,$ , and $v_{2,4}$ be such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\frac{v_{3,4}}{v_{2,4}}-1\\right)\\cdot(v_{2,3}+v_{2,4})=(1-f)\\cdot(1-w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, the choice of $v_{2,4}$ guarantees that when bidder 2 pays precisely the minimum amount to \u201csteal\u201d the entire market share of bidder 3 on $S_{2}$ , bidder 2\u2019s overall buyer surplus is $0$ . ", "page_idx": 15}, {"type": "text", "text": "Now observe that the following bidding strategies form an equilibrium: $\\theta_{1,1}=v_{3,2}/v_{1,2},\\theta_{2,2}=$ $v_{3,4}/v_{2,4}$ , and all other bid multipliers are 1.6 This means ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{P o A}(n,m,s,\\{S_{k}\\},v)\\le\\frac{v_{1,1}+v_{1,2}+v_{2,3}+v_{2,4}+v_{1,5}+v_{2,6}}{\\mathsf{s h a r e}_{1}+\\mathsf{s h a r e}_{2}+\\mathsf{s h a r e}_{3}}\\qquad\\qquad\\qquad}\\\\ {=\\mathsf{s h a r e}_{1}-h_{1}(f\\cdot(1-w))+\\mathsf{s h a r e}_{2}-h_{2}((1-f)\\cdot(1-w))+\\mathsf{s h a r e}_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall that we still have the freedom to choose $w\\in[0,1]$ . So we only need to show ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w}(h_{1}(f\\cdot(1-w))+h_{2}((1-f)\\cdot(1-w)))=\\frac{1}{2}{\\mathbf u}{\\mathbf n}{\\mathbf b}{\\mathbf a}|(n,m,s,\\{S_{k}\\},v).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In fact, we show that for each $w\\in[0,1]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ensuremath{\\varepsilon}_{1}(f\\cdot(1-w))+h_{2}((1-f)\\cdot(1-w))=\\operatorname*{max}_{\\lambda\\in\\Lambda(w)}\\int_{0}^{w}\\left(1+\\lambda(\\tau)-\\sqrt{(1-\\lambda(\\tau))^{2}+4f\\cdot\\lambda(\\tau)}\\right)\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, $h_{1}$ and $h_{2}$ are as defined in the proof of Lemma 1. This implies the above equation given the alternative definition of unbal established in the proof of Proposition 1. To see why this equation holds, observe that the right hand side is equal to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in[0,1]}h_{1}(x\\cdot(1-w))+h_{2}((1-x)\\cdot(1-w)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, since $\\mathsf{b a l}_{1}=\\mathsf{b a l}_{2}=f$ , the optimizer must satisfy $x/{\\mathsf{s h a r e}}_{1}=(1-x)/{\\mathsf{s h a r e}}_{2}$ , which means $x=f$ . This concludes the proof of the lemma. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The abstract and introduction discuss our theoretical claims and their implications. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We discuss scenarios where our results do not apply, and also present negative results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We state the full model in Preliminaries and prove all the claims made in the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: No experimental results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA]   \nJustification: No data or code. Guidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA]   \nJustification: No experimental results. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: No experimental results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: No experimental results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In particular, we have done our best to preserve anonymity. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We present only theoretical results that do not have immediate societal impacts. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No data or models released. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No assets used. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No new assets introduced. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No crowdsourcing or human subjects involved. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No human subjects involved. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]