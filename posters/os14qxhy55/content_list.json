[{"type": "text", "text": "OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuhang Lu ShanghaiTech University luyh2@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Xinge Zhu The Chinese University of Hong Kong zhuxinge123@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Tai Wang\u2217 Shanghai AI Laboratory taiwang.me@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Yuexin Ma\u2217 ShanghaiTech University mayuexin@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Occupancy prediction has increasingly garnered attention in recent years for its fine-grained understanding of 3D scenes. Traditional approaches typically rely on dense, regular grid representations, which often leads to excessive computational demands and a loss of spatial details for small objects. This paper introduces OctreeOcc, an innovative 3D occupancy prediction framework that leverages the octree representation to adaptively capture valuable information in 3D, offering variable granularity to accommodate object shapes and semantic regions of varying sizes and complexities. In particular, we incorporate image semantic information to improve the accuracy of initial octree structures and design an effective rectification mechanism to refine the octree structure iteratively. Our extensive evaluations show that OctreeOcc not only surpasses state-of-the-art methods in occupancy prediction, but also achieves a $15\\%-\\bar{2}4\\%$ reduction in computational overhead compared to dense-grid-based methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Holistic 3D scene understanding is a pivotal aspect of a stable and reliable visual perception system, especially for real-world applications such as autonomous driving. Occupancy, as a classical representation, has been renascent recently with more datasets support and exploration in learning-based approaches. Such occupancy prediction tasks aim at partitioning the 3D scene into grid cells and predicting semantic labels for each voxel. It is particularly an essential solution for recognizing irregularly shaped objects and also enables the open-set understanding (1), further beneftiing downstream tasks, like prediction and planning. ", "page_idx": 0}, {"type": "text", "text": "Existing occupancy prediction methods (2; 3; 4; 5; 6) typically construct dense and regular grid representations, same as the ground truth. While such approach is intuitive and direct, it overlooks the statistical and geometric properties of 3D environments. In fact, the 3D scene is composed of foreground objects and background regions with various shapes and sizes. For example, the space occupied by larger objects, such as buses, are considerably more extensive than that taken up by smaller items like traffic cones (Fig. 1a). Consequently, employing a uniform voxel resolution to depict the scene proves to be inefficient, leading to computational waste for larger objects and a lack of geometry details for smaller ones. Considering the large computation cost of aforementioned works, some recent works attempted to mitigate the heavy memory footprint by utilizing other representations, such as 2D planes in TPVFormer (7) and coarse voxels in PanoOcc (8), or modeling non-empty regions by depth estimation (9). However, these methods suffer from the loss of spatial information because of too coarse representations or accumulated estimated depth errors. ", "page_idx": 0}, {"type": "image", "img_path": "os14qXhy55/tmp/a76957cba8e69e3da29ad2c6d9bbbf811bfb5f0f0ef74cfa4daa2fe83d709c70.jpg", "img_caption": ["Figure 1: Scale difference of various categories and octree representation. (a) compares the average space occupied by different object types, indicating varying granularities needed for different semantic regions. (b) demonstrates the advantage of octree representations, enabling specific granularities for different objects and even parts of objects, reducing computational overhead while retaining spatial information. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To reduce the computational overhead and meanwhile improve the prediction accuracy, we propose to use octree (10) representation for the occupancy prediction, which can flexibly adapt to objects and semantic regions with various shapes and sizes. As a tree-based data structure, it recursively divides the 3D space into eight octants, thus allowing coarse spatial partition for large regions and fine-grained processing for small objects or complex details (Fig. 1b). Incorporating octree representation, we propose OctreeOcc, an efficient and multi-granularity method for occupancy prediction. It constructs octree queries by predicting the octree structure from the stored features of leaf nodes at each level of the tree. However, directly predicting 3D structure from 2D images is challenging due to the lack of depth and occlusion issues. To address this problem, we first propose Semantic-guided Octree Initialization that incorporates image semantic information to produce more accurate initial structures. And then, we devise an Iterative Structure Rectification module that predicts new octree structure from the encoded query to rectify the low-confidence region of the original prediction, further improving the prediction precision. ", "page_idx": 1}, {"type": "text", "text": "Our extensive evaluations against state-of-the-art occupancy prediction methods show that OctreeOcc outperforms others on nuScenes and SemanticKITTI datasets, reducing computational overhead by $15\\bar{\\%}-24\\%$ for dense-grid-based methods. Ablation studies further validate the effectiveness of each module within our method. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce OctreeOcc, a 3D occupancy prediction approach based on multi-granularity octree queries. This method facilitates spatial sparsification, significantly decreasing the number of voxels needed to accurately depict a scene, yet retains critical spatial details. \u2022 We propose a semantic-guided octree initialization module and an iterative structure rectification module, which empower the network with a robust initial setup and the ability to dynamically refine the octree, leading to a more efficient and effective representation. \u2022 Comprehensive experiments demonstrate that OctreeOcc achieves state-of-the-art performance and reduces computational overhead, highlighting the feasibility and potential of octree structures in 3D occupancy prediction. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Camera-based 3D Perception ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Camera-based 3D perception has gained significant traction in recent years due to its ease of deployment, cost-effectiveness, and the preservation of intricate visual attributes. According to the view transformation paradigm, these methods can be categorized into three distinct types. LSS-based approaches(11; 2; 12; 13; 14; 15) explicitly lift multi-view image features into 3D space through depth prediction. Another category of works(16; 17; 18) implicitly derives depth information by querying from 3D to 2D. Notably, projection-free methods(19; 20; 21; 22) have recently demonstrated exceptional performance. While commendable progress has been made in detection, this approach compromises the comprehensive representation of the overall scene in 3D space and proves less effective in recognizing irregularly shaped objects. Consequently, there is a burgeoning interest in methodologies aimed at acquiring a dense voxel representation through the camera, facilitating a more comprehensive understanding of 3D space. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 3D Occupancy Prediction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3D occupancy prediction involves the prediction of both occupancy and semantic attributes for all voxels encompassed within a three-dimensional scene, particularly valuable for autonomous vehicular navigation. Recently, some valuable datasets (23; 24; 25) have been proposed, boosting more and more research works (26; 1; 27; 28; 6; 4; 5; 7; 8; 9; 29; 30; 31; 32; 33; 34) in this field. Most of the research focuses on dense voxel modeling. MonoScene(27) pioneers a camera-based approach using a 3D UNet architecture. OccDepth(28) improves 2D-to-3D geometric projection using stereo-depth information. OccFormer(6) decomposes the 3D processing into the local and global transformer pathways along the horizontal plane. SurroundOcc(4) achieves fine-grained results with multiscale supervision. Symphonies(5) introduces instance queries to enhance scene representation. ", "page_idx": 2}, {"type": "text", "text": "Nevertheless, owing to the high resolution of regular voxel representation and sparse context distribution in 3D scenes, these methods encounter substantial computational overhead and efficiency issues. Some approaches recognize this problem and attempt to address it by reducing the number of modeled voxels. For instance, TPVFormer(7) models the three-view 2D planes and subsequently recovering 3D spatial information from them. However, its performance degrades due to the lack of 3D information. PanoOcc(8) initially represents scenes at the coarse-grained level and then upsamples them to the fine-grained level, but the lack of information from coarse-grained modeling cannot be adequately addressed by the up-sampling process. VoxFormer(9) mitigates computational complexity by initially identifying non-empty regions through depth estimation and modeling only those specific areas. However, the effectiveness of this process is heavily contingent on the accuracy of depth estimation. In contrast, our approach provides different granularity of modeling for different regions by predicting the octree structure, which reduces the number of voxels to be modeled while preserving the spatial information, thereby reducing the computational overhead and maintaining the accuracy. ", "page_idx": 2}, {"type": "text", "text": "2.3 Octree-Based 3D Representation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The octree structure(10) finds widespread use in computer graphics for rendering or reconstruction(35; 36; 37; 38), owing to its spatial efficiency and GPU compatibility. Researchers have extended its utility to efficient point cloud learning and related tasks(39; 40; 41; 42). OctFormer(43) and OcTr (44) utilize multi-granularity features of octree to capture a comprehensive global context, thereby enhancing the efficiency of understanding point clouds at the scene level. Furthermore, certain studies (45; 46) adopt octree representation for effectively compressing point cloud data. These works have highlighted the versatility and effectiveness of octree-based methodologies in point cloud analysis and processing applications. However, unlike constructing an octree from 3D point clouds, we are the first to predict the octree structure of a 3D scene from images, which is more challenging owing to the absence of explicit spatial information inherent in 2D images. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce details of our efficient and multi-granularity occupancy prediction method OctreeOcc. After defining the problem and providing an overview of our method in Sec. 3.1 and 3.2, we introduce key components of our method in order. In Sec. 3.3, we outline how we define octree queries and transform dense queries into octree queries. Next, we utilize image semantic priors to construct a high-quality initialized octree structure, as detailed in Sec. 3.4. Once the initialized octree query is obtained, we encode it and refine the octree structure in Sec. 3.5. Finally, Sec. 3.6 describe how to supervise the network. ", "page_idx": 2}, {"type": "image", "img_path": "os14qXhy55/tmp/042bc1de53c0532d4769b01aba0d83184b1c6b64556fa51b2273b279b9558546.jpg", "img_caption": ["Figure 2: Overall framework of OctreeOcc. From multi-view images, we extract multi-scale features using an image backbone. The initial octree structure is derived from image segmentation priors, transforming dense queries into octree queries. The octree encoder refines these queries and rectifies the octree structure. Finally, we decode the octree queries to obtain occupancy predictions. The diagram of the Iterative Structure Rectification module shows the octree query and mask in 2D (quadtree) form for better visualization. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Camera-based occupancy prediction aims to predict the present occupancy state and semantics of each voxel grid within the scene using input from multi-view camera images. Specifically, we consider a set of $N$ multi-view images $I\\stackrel{\\triangledown}{=}\\{I_{i}\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}\\}_{i=1}^{N}$ , together with camera intrinsics K = {Ki \u2208R3\u00d73}iN=1 and extrinsics $T=\\{T_{i}\\in\\mathbb{R}^{4\\times4}\\}_{i=1}^{N}$ as input, and the objective of the model is to predict the 3D semantic voxel volume $O\\in\\{w_{0},\\bar{w_{1}},...,w_{C}\\}^{X\\times Y\\times Z}$ , where $H,\\,W$ indicate the resolution of input image and $X,Y,Z$ denote the volume resolution (e.g. $200\\!\\times\\!200\\!\\times\\!16)$ . The primary focus lies in accurately distinguishing the empty class $\\left(w_{0}\\right)$ and other semantic classes $(w_{1}\\sim w_{C})$ for every position in the 3D space, which entails the network learning both the geometric and semantic information inherent in the data. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a set of multi-view images $I\\,=\\,\\{I_{i}\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}\\}_{i=1}^{N}$ , we extract multi-view image features F = {Fi \u2208RH\u00d7W \u00d7C}iN=1 . Simultaneously, we randomly initialize the dense voxel query $Q_{d e n s e}\\,\\in\\,\\mathbb{R}^{X\\times Y\\times Z\\times C}$ . To enhance computational efficiency, we transform $Q_{d e n s e}$ into sparse representation $Q_{o c t r e e}\\in\\mathbb{R}^{N\\times C}$ , leveraging octree structure information (i.e. octree mask) derived from segmentation priors. During encoding, we utilize $Q_{o c t r e e}$ to gather information, including temporal fusion and sampling from image features $\\mathbb{F}$ , while also rectifying the octree structure. Upon encoding $Q_{o c t r e e}$ , to conform to the output format, we convert it back to $Q_{d e n s e}$ and apply a Multi-Layer Perceptron (MLP) to obtain the final occupancy prediction $O\\in\\mathbb{R}^{X\\times Y\\times Z\\times K}$ . Here, $H$ , $W$ indicate the resolution of input image and $X$ , $Y$ , $Z$ denote the volume resolution. $N$ means the number of octree query , $C$ denotes the feature dimension and $K$ indicates the number of classes. ", "page_idx": 3}, {"type": "text", "text": "3.3 Octree Query ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given that objects within 3D scenes exhibit diverse granularities, employing dense queries $(6;4)$ overlooks this variation and leads to inefficiency. To address this, we propose sparse and multigranularity octree queries, leveraging the octree structure. This approach creates adaptable voxel representations tailored to semantic regions at different scales. ", "page_idx": 3}, {"type": "text", "text": "Octree Mask. To effectively construct the octree query, it\u2019s essential to understand its underlying structure. An octree partitions each node into eight child nodes within 3D space, each representing equal subdivisions of the parent node. This recursive process begins with the initial level and proceeds with gradual splitting. At every level, a voxel query serves either as a leaf query if it remains unsplit or as a parent query if it undergoes division. We obtain this geometric information by maintaining a learnable octree mask, denoting as $M_{o}=\\{M_{o}^{l}\\in\\mathbb{R}^{\\frac{X}{2^{l}},\\frac{Y}{2^{l}},\\frac{Z}{2^{l}}}\\}_{l=1}^{L-1}$ , where $X,Y,Z$ denote the ground truth\u2019s volume resolution. The denotes the depth of the octree, representing the existence of different resolutions of queries, with $L-1$ splits being performed from the top to the bottom of the octree. The value in the octree mask represents the probability that a voxel at that level requires a split, which is initialized through segmentation priors (Sec. 3.4), rectified during query encoding (Sec. 3.5), and supervised by octree ground truth (Sec. 3.6). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Transformation between octree query and dense voxel query. During the query encoding process, we prioritize efficiency by leveraging octree queries. This involves transforming the initial dense queries $Q_{d e n s e}$ into octree queries $Q_{o c t r e e}$ using learned structural information. To determine the octree structure, we need to binarise the learned octree mask. Since most of the voxels in the scene at various resolutions do not necessitate splitting, neural network-based prediction binarization is susceptible to pattern collapse, tending to predict all as non-split, leading to a decrease in performance. To mitigate this issue, we introduce a manually defined query selection ratio, where a subset of voxels with the highest probability of splitting is selected for division through the top- $\\cdot\\mathbf{k}$ mechanism. ", "page_idx": 4}, {"type": "text", "text": "The transformation from $Q_{d e n s e}$ to $Q_{o c t r e e}$ begins at the finest granularity, we downsample $Q_{d e n s e}$ to each level through average pooling and retain queries that do not require splitting (leaf queries) with the assistance of the binarized octree mask. This process iterates until reaching the top of the octree. By retaining all leaf queries, we establish $\\stackrel{\\triangledown}{Q_{o c t r e e}}\\in\\mathbb{R}^{N\\times C}$ , where $N=N_{1}+N_{2}+\\ldots+N_{L}$ represents the total count of leaf queries, $\\mathrm{L}$ indicates the depth of octree. Conversely, applying the inverse operation of this process allows the conversion of $Q_{o c t r e e}$ back into $Q_{d e n s e}$ for the final output. Further details are provided in Appendix. ", "page_idx": 4}, {"type": "text", "text": "3.4 Semantic-Guided Octree Initialization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Predicting octree structure from an initialised query via neural network can yield sub-optimal results due to the inherent lack of meaningful information in the query. To overcome this limitation, we employ the semantic priors inherent in images as crucial references. Specifically, we adopt UNet(47) tWo es etghemne ngte tnheer aitnep suta mmpullitni-gv ipeowi nitms $I$ , mweintth  meaacph $I_{s e g}=\\overline{{\\{I_{s e g}^{i}\\in\\mathring{\\mathbb{R}}^{H\\times W}\\}_{i=1}^{N}}}$ $p\\,=\\,\\{p_{i}\\,\\in\\,\\mathbb{R}^{3}\\}_{i=1}^{X\\times Y\\times Z}$   \ncenter coordinates of dense voxel queries. Subsequently, we project these points onto various image views. The projection from sampling point $p_{i}=(x_{i},y_{i},z_{i})$ to its corresponding 2D reference point $(u_{i j},v_{i j})$ on the $j$ -th image view is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{j}(p_{i})=(u_{i j},v_{i j}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi_{j}(p_{i})$ denotes the projection of the $i$ -th sampling point at location $p_{i}$ on the $j$ -th camera view. We project the point $p_{i}$ onto the acquired semantic segmentation map $I_{s e g}$ through the described projection process. To ensure the prioritization of crucial areas such as foreground objects and buildings in the initial structure, we adopt an unbalanced weight assignment method. Here, the highest weight is allocated to sampling points projecting onto foreground areas, with decreasing weights assigned to points projecting onto buildings or vegetation, and the lowest weight designated for points projecting onto roads, among others. Subsequently, the voxel\u2019s weight is determined as the average of the weights of all its sampling points. During this process, we determine the weights of each voxel at the finest granularity, denoted as $W\\in\\mathbb{R}^{\\breve{X}\\times Y\\times\\dot{Z}}$ . Subsequently, we employ average pooling to downsample $W$ to each level of the octree, resulting in an initial octree mask $M_{o}\\,=\\,\\bar{\\{{M_{o}^{l}}\\in\\bar{\\mathbb{R}}^{\\frac{X}{2^{l}},\\frac{Y}{2^{l}},\\frac{Z}{2^{l}}}\\}}_{l=1}^{L-\\bar{1}}$ 2Yl , 2Zl }lL=\u221211 . Here, X, Y , and Z represent the resolution of the ground truth volume, while $L$ denotes the depth of the octree. Further details are provided in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "3.5 Octree Encoder ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given octree queries $Q_{o c t r e e}$ and extracted image features $\\mathbb{F}$ , the octree encoder updates both the octree query features and the octree structure. Referring to the querying paradigm in dense querybased methods(8; 25), we adopt efficient deformable attention(48) for temporal self-attention(TSA) and image cross-attention(ICA). ", "page_idx": 4}, {"type": "image", "img_path": "os14qXhy55/tmp/3b455ddac180347bf1917551bb43aa320dd42d3842378fdd92ac974a00387c3d.jpg", "img_caption": ["Figure 3: Illustration of octree structure rectification. The left figure shows the initially predicted octree structure, while the right figure displays the structure after rectification. It\u2019s evident that the rectification module improves the consistency of the octree structure with the object\u2019s shape. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In accurately representing the driving scene, temporal information plays a crucial role. By leveraging historical octree queries $Q_{t-1}$ , we align it to the current octree queries by the ego-vehicle motion transformation. Given historal octree queries $Q_{t-1}\\,\\in\\,\\mathbb{R}^{N,C}$ , a current octree query $q$ located at $\\boldsymbol{p}=\\left({x,y,z}\\right)$ , the TSA is represented by: ", "page_idx": 5}, {"type": "equation", "text": "$$\nT S A(q,Q_{t-1})=\\sum_{m=1}^{M_{1}}{D e f o r m A t t n(q,p,Q_{t-1})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $M_{1}$ indicates the number of sampling points. Implementing it within the voxel-based selfattention ensures that each octree query interacts exclusively with local voxels of interest, keeping the computational cost manageable. ", "page_idx": 5}, {"type": "text", "text": "Image cross-attention is devised to enhance the interaction between multi-scale image features and octree queries. Specifically, for an octree query $q$ , we can obtain its centre\u2019s 3D coordinate $\\left(x,y,z\\right)$ as reference point $R e f_{x,y,z}$ . Then we project the 3D point to images like formula 1 and perform deformable attention: ", "page_idx": 5}, {"type": "equation", "text": "$$\nI C A(q,\\mathbf{F})=\\frac{1}{N}\\sum_{n\\in N}\\sum_{m=1}^{M_{2}}D e f o r m A t t n(q,\\pi_{n}(R e f_{x,y,z}),\\mathbf{F}_{n}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $N$ denotes the camera view, $m$ indexes the reference points , and $M_{2}$ is the total number of sampling points for each query. $\\mathbf{F}_{n}$ is the image features of the $n$ -th camera view. ", "page_idx": 5}, {"type": "text", "text": "Iterative Structure Rectification Module. The initial octree structure, derived from image segmentation priors, may not precisely match the scene due to the potential segmentation and projection errors. Nonetheless, the encoded octree query captures crucial spatial information. Thus, the predicted octree structure based on this query complements and rectifies the initial structure prediction, allowing us to mitigate limitations arising from segmentation and projection issues. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we partition the octree structure into two parts: the high-confidence regions and the low-confidence regions, as Fig. 2 shows. By sorting the octree split probability values stored in the octree mask in descending order and selecting the top ${\\mathrm{k}}\\%$ of regions at each level, we can identify the locations of high-confidence regions. For these regions, predictions are relatively more accurate and no additional adjustments are required in this iteration. For regions where confidence remains low, we first extract the query features corresponding to those areas by utilizing the index of low-confidence rqeugeiroienss ,i nd elnevoteel .  asW $\\overset{\\cdot}{Q}_{l c r}=\\{Q_{l c r}^{l}\\in\\bar{\\mathbb{R}}^{N_{l}\\times C}\\}_{l=1}^{L-1}$ c, t wohcterree $N_{l}$ lriet pprreosbeanbtisl itthiee sn furommb l.o Swu-bcsoenqfuideenntlcye, $l$ $Q_{l c r}$ we apply a weighted sum with the previous split probability predictions of low-confidence regions to obtain rectified predictions. These refined predictions are concatenated with the preserved probability predictions of high-confidence regions, culminating in the generation of the final rectified octree mask. It is worth noting that, due to the iterative nature of structure updates, regions initially considered high confidence may not necessarily remain unchanged, as they might be partitioned into low-confidence regions in the next iteration. More details are shown in Appendix. ", "page_idx": 5}, {"type": "text", "text": "3.6 Loss Function ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To train the model, we use focal loss $\\boldsymbol{L}_{f o c a l}$ , lovasz-softmax loss $L_{l s}$ , dice loss $L_{d i c e}$ , affinity loss $L_{s c a l}^{g e o}$ and $L_{s c a l}^{s e m}$ from MonoScene(27). In addition, we also use focgaelo loss to supervise the octree prediction. The overall loss function $L=L_{f o c a l}+L_{l s}+L_{d i c e}+L_{s c a l}^{g e o}+L_{s c a l}^{s e m}+L_{o c t r e e}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first introduce the datasets (Sec. 4.1), evaluation metrics (Sec. 4.2), and implementation details (Sec. 4.3). Subsequently, we evaluate our method on 3D occupancy prediction and semantic scene completion tasks (Sec. 4.4) to demonstrate its effectiveness. Additionally, we conduct extensive ablation studies and provide more analysis (Sec. 4.5) of our method. ", "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Occ3D-nuScenes(23) re-annotates the nuScenes dataset(49) with precise occupancy labels derived from LiDAR scans and human annotations. It includes 700 training instances and 150 validation instances, with occupancy spanning $-40\\mathrm{m}$ to $40\\mathrm{m}$ in X and Y axes, and $-1\\mathrm{m}$ to $5.4\\mathrm{m}$ in the $Z_{\\cdot}$ -axis. Labels are divided into 17 classes, with each class representing a volumetric space of 0.4 meters in each dimension, plus an 18th \u201cfree\u201d category for empty regions. The dataset also provides visibility masks for LiDAR and camera modalities. ", "page_idx": 6}, {"type": "text", "text": "SemanticKITTI(50) comprises 22 distinct outdoor driving scenarios, with a focus on areas located in the forward trajectory of the vehicle. Each sample in this dataset covers a spatial extent ranging from [0.0m, -25.6m, $-2.0\\mathrm{m}$ , $51.2\\mathrm{m}$ , 25.6m, $4.4\\mathrm{m}]$ , with a voxel granularity set at $[0.2\\mathrm{m},0.2\\mathrm{m},0.\\dot{2}\\mathrm{m}]$ . The dataset consists of volumetric representations, specifically in the form of $256{\\times}256{\\times}32$ voxel grids. These grids undergo meticulous annotation with 21 distinct semantic classes. The voxel data is derived through a rigorous post-processing procedure applied to Lidar scans. ", "page_idx": 6}, {"type": "text", "text": "4.2 Evaluation metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Both 3D occupancy prediction and semantic scene completion utilize intersection-over-union (mIoU) over all classes as evaluation metrics, calculated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{\\mIoU}=\\frac{1}{\\mathrm{C}}\\sum_{\\mathrm{c}=1}^{\\mathrm{C}}\\frac{\\mathrm{TP_{c}}}{\\mathrm{TP_{c}}+\\mathrm{FP_{c}}+\\mathrm{FN_{c}}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathrm{TP_{c},F P_{c}}$ , and $\\mathrm{FN_{c}}$ correspond to the number of true positive, false positive, and false negative predictions for class $\\mathrm{c_{i}}$ , and $\\mathrm{C}$ is the number of classes. ", "page_idx": 6}, {"type": "text", "text": "4.3 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Based on previous research, we set the input image size to $900\\!\\times\\!1600$ and employ ResNet101- DCN(51) as the image backbone. Multi-scale features are extracted from the Feature Pyramid Network(52) with downsampling sizes of 1/8, 1/16, 1/32, and 1/64. The feature dimension $C$ is set to 256. The octree depth is 3, and the initial query resolution is $50\\!\\times\\!50\\!\\times\\!4$ . We choose query selection ratios of $20\\%$ and $60\\%$ for the two divisions. The octree encoder comprises three layers, each composed of TSA, ICA, and Iterative Structure Rectification (ISR) modules. Both $M_{1}$ and $M_{2}$ are set to 4. In TSA, we fuse four temporal frames. In ISR, the top $10\\%$ predictions are considered high-confidence in level 1, and $30\\%$ in level 2. The loss weights are uniformly set to 1.0. For optimization, we employ Adam(53) optimizer with a learning rate of 2e-4 and weight decay of 0.01. The batch size is 8, and the model is trained for 24 epochs, consuming around 3 days on 8 NVIDIA A100 GPUs. ", "page_idx": 6}, {"type": "text", "text": "4.4 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3D Occupancy Prediction. In Tab. 1, we compare our method with other SOTA occupancy prediction methods on Occ3d-nus validation set. The performance of FBOCC(3) relies on open-source code, which we evaluate after ensuring consistency in details (utilizing the same backbone, image resolution, and excluding CBGS(54)) for a fair comparison. Performance for other methods are reported in a series of works(26; 8; 23). Our approach demonstrates superior performance on mIoU compared to them, particularly excelling in foreground classes such as barriers, cars, and buses, as well as in scene structure classes like manmade and vegetation. This highlights that processing scenes using multiple granularities aligns better with the scene characteristics, enhancing overall expressiveness. ", "page_idx": 6}, {"type": "table", "img_path": "os14qXhy55/tmp/1b0193cb0273c20367fa20d4bb3694a1116644d683f861229f9a0a967f9951dd.jpg", "table_caption": ["Table 1: 3D Occupancy prediction performance on Occ3D-nuScenes dataset. \u201c $\\star^{:}$ \u201d denotes training with the camera mask. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "os14qXhy55/tmp/dbce3a43e1bacd4df6737378318f2f20f8a6f6d4983e15945bee71f886001f06.jpg", "table_caption": ["Table 2: 3D Semantic Scene Completion performance on SemanticKITTI dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Moreover, we evaluate the efficiency of our approach by comparing it to alternative methods utilizing diverse query forms, as depicted in Tab 3. The results indicate that our approach not only surpasses these methods in terms of performance but also demonstrates significantly reduced memory usage and lower latency compared to dense queries, approaching the levels observed with 2D queries. Clearly, sparse octree queries effectively mitigate computational overhead while ensuring robust performance. Fig. 4 displays qualitative results obtained by our methods and some other methods, illustrating that our approach comprehensively understands the structure of the scene, showcasing superior performance in scene understanding. ", "page_idx": 7}, {"type": "text", "text": "3D Semantic Scene Completion. To better evaluate the effectiveness of our approach, we conduct comparative experiments for the Semantic Scene Completion (SSC) task. As demonstrated in Tab 2, we compare our results with those of other SSC methods on the SemanticKITTI validation set. Our model demonstrates a more accurate perception of space due to octree construction and correction, outperforming others in IoU metric for geometry reconstruction. Additionally, for specific semantic classes such as car and vegetation, we achieve superior results, attributed to the enhanced treatment of objects facilitated by multi-granularity modeling. Additionally, Tab 3 also indicates that our method consumes fewer computational resources than other dense query-based methods. ", "page_idx": 7}, {"type": "table", "img_path": "os14qXhy55/tmp/072134082af57323e08b46bd0ca2381792525f823c51a78e0f256b1eafaddbab.jpg", "table_caption": ["Table 3: Comparison of query form and efficiency with SOTA methods on the Occ3D-nuScenes (left table) and SemanticKITTI (right table) datasset. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "os14qXhy55/tmp/eb041eae25e44ced87e0b3e6f3b89e89e60c00a7a2756de9637c3b4c817fb303.jpg", "table_caption": ["Table 4: Ablation experiments of Modules on Occ3d-nuScenes val set. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "os14qXhy55/tmp/fb3b0637395f0044dd3eefecb60c6d9ea1e08b79ef2576ae8e00385884fa8426.jpg", "table_caption": ["Table 5: Comparison of octree structure quality at different stages. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "os14qXhy55/tmp/9f57baff6c0d37ccec1c34caffe6d30977a354a641f2619d113430812b414789.jpg", "table_caption": ["Table 6: Ablation for different octree depth on Occ3dnuScenes val set. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "os14qXhy55/tmp/08c53268073e63ee0caa4319a3ada11cfa9fd9cca836ff34eda89aa4722e86f3.jpg", "table_caption": ["Table 7: Ablation for the choice of query selection ratio on Occ3dnuScenes val set. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study and More Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we perform ablation studies and analysis experiments on the Occ3d-nus validation set to assess the effectiveness of our proposed modules. All the experiments are conducted on the NVIDIA A40 GPU with reducing the input image size to $0.3\\mathrm{x}$ . ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Octree Queries. To validate the superiority of octree queries, we maintained consistent TSA and ICA settings while removing the proposed octree structure initialization and rectification modules. This facilitated a comparison with the baseline employing dense queries of size $100\\!\\times\\!100\\!\\times\\!16$ . As illustrated in Tab. 4 (b), experimental results consistently demonstrate our outperformance, with a notable $0.8\\ \\mathrm{mIoU}$ advantage, despite achieving a memory saving of approximately 9G. This underscores the adeptness of octree prediction in allocating queries with varying granularities to diverse semantic regions. Moreover, the constructed octree queries exhibit adaptability to various object shapes, thereby optimizing the utilization of computational resources. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Semantic-guided Structure Initialization module.To highlight the significance of the initial octree structure, we replaced the Semantic-guided Octree Initialization module with an MLP predicting the octree structure from randomly initialized queries. This results in a $1.7\\;\\mathrm{mIoU}$ performance decrease, highlighting the inaccuracy of structurally predicted information from the initialized query due to the absence of valid information coding. Incorporating semantic priors proves crucial as they enhance the quality of the initialized octree, thereby improving overall model performance. Meanwhile, Tab. 5(a) evaluates the effectiveness of the initial octree structure, which shows that assigning different initial weights to voxels based on their semantic regions improves the octree structure by focusing on the scene\u2019s effective areas. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Iteritive Structure Rectification module. We perform an ablation study on the Iterative Structure Rectification module, as shown in Tab. 4(d). The incorporation of this module has led to noticeable improvements in performance. Meanwhile, Tab. 5(b) shows this rectification gradually rectifies areas where structural predictions are incorrect. Consequently, this refinement contributes to the efficiency of octree query expression, positively impacting overall performance. ", "page_idx": 8}, {"type": "text", "text": "Discussion on the depth of octree. Tab. 6 presents experiments on octree depth variations. (a)-(d) show performance with varying 3D query sizes, while (e)-(h) depict octree query performance with different depths and initial resolutions. Comparing (e) to (c) and (f) to (d) reveals our approach achieves comparable performance to dense queries, significantly reducing resource consumption. (g) shows that the predetermined octree depth should not be excessive. While reducing memory usage, the imperfect predictions of octree splitting result in accumulated errors, leading to performance degradation as the depth increases. ", "page_idx": 8}, {"type": "text", "text": "Discussion on query selection ratio of each level in the octree. In Tab. 7, we present results for different query ratios at various octree levels. Results shows that inadequate queries result in an imperfect scene representation, especially for detailed regions (a,b vs e). Conversely, excessive queries impact computational efficiency, particularly for coarse-grained regions with empty spaces (c vs d and c,d vs e). Optimizing query numbers based on scene object granularity distribution ensure effective processing of semantic regions of different sizes. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "os14qXhy55/tmp/4c97fa3bbd2704171ed603cdd9306430d2128a2018b40365cf93d5af2940c407.jpg", "img_caption": ["Figure 4: Qualitative results on Occ3D-nuScenes val set, where the resolution of the voxel predictions is $200\\!\\times\\!200\\!\\times\\!16$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, our paper introduces OctreeOcc, a novel 3D occupancy prediction framework that addresses the limitations of dense-grid representations in understanding 3D scenes. OctreeOcc\u2019s adaptive utilization of octree representations enables the capture of valuable information with variable granularity, catering to objects of diverse sizes and complexities. Our extensive experimental results affirm OctreeOcc\u2019s capability to attain state-of-the-art performance in 3D occupancy prediction while concurrently reducing computational overhead. ", "page_idx": 9}, {"type": "text", "text": "Limitation. The quality of the octree ground truth depends on the accuracy of the occupancy ground truth. Current occupancy ground truth comes from sparse lidar point clouds and surface reconstruction, leading to low-quality results for some frames, which affects the octree construction. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Z. Tan, Z. Dong, C. Zhang, W. Zhang, H. Ji, and H. Li, \u201cOvo: Open-vocabulary occupancy,\u201d arXiv preprint arXiv:2305.16133, 2023.   \n[2] J. Huang, G. Huang, Z. Zhu, Y. Yun, and D. Du, \u201cBevdet: High-performance multi-camera 3d object detection in bird-eye-view,\u201d arXiv preprint arXiv:2112.11790, 2021.   \n[3] Z. Li, Z. Yu, W. Wang, A. Anandkumar, T. Lu, and J. M. Alvarez, \u201cFb-bev: Bev representation from forward-backward view transformations,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6919\u20136928, 2023.   \n[4] Y. Wei, L. Zhao, W. Zheng, Z. Zhu, J. Zhou, and J. Lu, \u201cSurroundocc: Multi-camera 3d occupancy prediction for autonomous driving,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 21729\u201321740, October 2023.   \n[5] H. Jiang, T. Cheng, N. Gao, H. Zhang, W. Liu, and X. Wang, \u201cSymphonize 3d semantic scene completion with contextual instance queries,\u201d arXiv preprint arXiv:2306.15670, 2023.   \n[6] Y. Zhang, Z. Zhu, and D. Du, \u201cOccformer: Dual-path transformer for vision-based 3d semantic occupancy prediction,\u201d arXiv preprint arXiv:2304.05316, 2023.   \n[7] Y. Huang, W. Zheng, Y. Zhang, J. Zhou, and J. Lu, \u201cTri-perspective view for vision-based 3d semantic occupancy prediction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9223\u20139232, 2023.   \n[8] Y. Wang, Y. Chen, X. Liao, L. Fan, and Z. Zhang, \u201cPanoocc: Unified occupancy representation for camera-based 3d panoptic segmentation,\u201d arXiv preprint arXiv:2306.10013, 2023.   \n[9] Y. Li, Z. Yu, C. Choy, C. Xiao, J. M. Alvarez, S. Fidler, C. Feng, and A. Anandkumar, \u201cVoxformer: Sparse voxel transformer for camera-based 3d semantic scene completion,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9087\u20139098, June 2023.   \n[10] D. Meagher, \u201cGeometric modeling using octree encoding,\u201d Computer graphics and image processing, vol. 19, no. 2, pp. 129\u2013147, 1982.   \n[11] J. Philion and S. Fidler, \u201cLift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d,\u201d in Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIV 16, pp. 194\u2013210, Springer, 2020.   \n[12] Y. Li, H. Bao, Z. Ge, J. Yang, J. Sun, and Z. Li, \u201cBevstereo: Enhancing depth estimation in multi-view 3d object detection with dynamic temporal stereo,\u201d 2022.   \n[13] Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li, \u201cBevdepth: Acquisition of reliable depth for multi-view 3d object detection,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, pp. 1477\u20131485, 2023.   \n[14] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. L. Rus, and S. Han, \u201cBevfusion: Multi-task multi-sensor fusion with unified bird\u2019s-eye view representation,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 2774\u20132781, IEEE, 2023.   \n[15] J. Park, C. Xu, S. Yang, K. Keutzer, K. Kitani, M. Tomizuka, and W. Zhan, \u201cTime will tell: New outlooks and a baseline for temporal multi-view 3d object detection,\u201d arXiv preprint arXiv:2210.02443, 2022.   \n[16] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai, \u201cBevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers,\u201d in European conference on computer vision, pp. 1\u201318, Springer, 2022.   \n[17] C. Yang, Y. Chen, H. Tian, C. Tao, X. Zhu, Z. Zhang, G. Huang, H. Li, Y. Qiao, L. Lu, et al., \u201cBevformer v2: Adapting modern image backbones to bird\u2019s-eye-view recognition via perspective supervision,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17830\u201317839, 2023.   \n[18] Y. Jiang, L. Zhang, Z. Miao, X. Zhu, J. Gao, W. Hu, and Y.-G. Jiang, \u201cPolarformer: Multicamera 3d object detection with polar transformer,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, pp. 1042\u20131050, 2023.   \n[19] Y. Liu, T. Wang, X. Zhang, and J. Sun, \u201cPetr: Position embedding transformation for multi-view 3d object detection,\u201d in European Conference on Computer Vision, pp. 531\u2013548, Springer, 2022.   \n[20] Y. Liu, J. Yan, F. Jia, S. Li, A. Gao, T. Wang, and X. Zhang, \u201cPetrv2: A unified framework for 3d perception from multi-camera images,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3262\u20133272, 2023.   \n[21] S. Wang, Y. Liu, T. Wang, Y. Li, and X. Zhang, \u201cExploring object-centric temporal modeling for efficient multi-view 3d object detection,\u201d arXiv preprint arXiv:2303.11926, 2023.   \n[22] X. Lin, T. Lin, Z. Pei, L. Huang, and Z. Su, \u201cSparse4d: Multi-view 3d object detection with sparse spatial-temporal fusion,\u201d arXiv preprint arXiv:2211.10581, 2022.   \n[23] X. Tian, T. Jiang, L. Yun, Y. Wang, Y. Wang, and H. Zhao, \u201cOcc3d: A large-scale 3d occupancy prediction benchmark for autonomous driving,\u201d arXiv preprint arXiv:2304.14365, 2023.   \n[24] X. Wang, Z. Zhu, W. Xu, Y. Zhang, Y. Wei, X. Chi, Y. Ye, D. Du, J. Lu, and X. Wang, \u201cOpenoccupancy: A large scale benchmark for surrounding semantic occupancy perception,\u201d arXiv preprint arXiv:2303.03991, 2023.   \n[25] C. Sima, W. Tong, T. Wang, L. Chen, S. Wu, H. Deng, Y. Gu, L. Lu, P. Luo, D. Lin, and H. Li, \u201cScene as occupancy,\u201d 2023.   \n[26] M. Pan, J. Liu, R. Zhang, P. Huang, X. Li, L. Liu, and S. Zhang, \u201cRenderocc: Vision-centric 3d occupancy prediction with 2d rendering supervision,\u201d 2023.   \n[27] A.-Q. Cao and R. de Charette, \u201cMonoscene: Monocular 3d semantic scene completion,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3991\u20134001, June 2022.   \n[28] R. Miao, W. Liu, M. Chen, Z. Gong, W. Xu, C. Hu, and S. Zhou, \u201cOccdepth: A depth-aware method for 3d semantic scene completion,\u201d arXiv preprint arXiv:2302.13540, 2023.   \n[29] Y. Huang, W. Zheng, B. Zhang, J. Zhou, and J. Lu, \u201cSelfocc: Self-supervised vision-based 3d occupancy prediction,\u201d 2023.   \n[30] Q. Ma, X. Tan, Y. Qu, L. Ma, Z. Zhang, and Y. Xie, \u201cCotr: Compact occupancy transformer for vision-based 3d occupancy prediction,\u201d arXiv preprint arXiv:2312.01919, 2023.   \n[31] Z. Yu, C. Shu, J. Deng, K. Lu, Z. Liu, J. Yu, D. Yang, H. Li, and Y. Chen, \u201cFlashocc: Fast and memory-efficient occupancy prediction via channel-to-height plugin,\u201d arXiv preprint arXiv:2311.12058, 2023.   \n[32] Z. Ming, J. S. Berrio, M. Shan, and S. Worrall, \u201cInversematrixvt3d: An efficient projection matrix-based approach for 3d occupancy prediction,\u201d arXiv preprint arXiv:2401.12422, 2024.   \n[33] H. Zhang, X. Yan, D. Bai, J. Gao, P. Wang, B. Liu, S. Cui, and Z. Li, \u201cRadocc: Learning cross-modality occupancy knowledge through rendering assisted distillation,\u201d arXiv preprint arXiv:2312.11829, 2023.   \n[34] S. Silva, S. B. Wannigama, R. Ragel, and G. Jayatilaka, \u201cS2tpvformer: Spatio-temporal triperspective view for temporally coherent 3d semantic occupancy prediction,\u201d arXiv preprint arXiv:2401.13785, 2024.   \n[35] C. H\u00e4ne, S. Tulsiani, and J. Malik, \u201cHierarchical surface prediction for 3d object reconstruction,\u201d in 2017 International Conference on 3D Vision (3DV), pp. 412\u2013420, IEEE, 2017.   \n[36] C. H. Koneputugodage, Y. Ben-Shabat, and S. Gould, \u201cOctree guided unoriented surface reconstruction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16717\u201316726, 2023.   \n[37] J.-H. Tang, W. Chen, J. Yang, B. Wang, S. Liu, B. Yang, and L. Gao, \u201cOctfield: Hierarchical implicit functions for 3d modeling,\u201d arXiv preprint arXiv:2111.01067, 2021.   \n[38] P.-S. Wang, Y. Liu, and X. Tong, \u201cDual octree graph networks for learning adaptive volumetric shape representations,\u201d ACM Transactions on Graphics (TOG), vol. 41, no. 4, pp. 1\u201315, 2022.   \n[39] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, \u201cO-cnn: Octree-based convolutional neural networks for 3d shape analysis,\u201d ACM Transactions On Graphics (TOG), vol. 36, no. 4, pp. 1\u201311, 2017.   \n[40] P.-S. Wang, C.-Y. Sun, Y. Liu, and X. Tong, \u201cAdaptive o-cnn: A patch-based deep representation of 3d shapes,\u201d ACM Transactions on Graphics (TOG), vol. 37, no. 6, pp. 1\u201311, 2018.   \n[41] H. Lei, N. Akhtar, and A. Mian, \u201cOctree guided cnn with spherical kernels for 3d point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9631\u20139640, 2019.   \n[42] M. Tatarchenko, A. Dosovitskiy, and T. Brox, \u201cOctree generating networks: Efficient convolutional architectures for high-resolution 3d outputs,\u201d in Proceedings of the IEEE international conference on computer vision, pp. 2088\u20132096, 2017.   \n[43] P.-S. Wang, \u201cOctformer: Octree-based transformers for 3d point clouds,\u201d arXiv preprint arXiv:2305.03045, 2023.   \n[44] C. Zhou, Y. Zhang, J. Chen, and D. Huang, \u201cOctr: Octree-based transformer for 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5166\u20135175, 2023.   \n[45] C. Fu, G. Li, R. Song, W. Gao, and S. Liu, \u201cOctattention: Octree-based large-scale contexts model for point cloud compression,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 625\u2013633, 2022.   \n[46] Z. Que, G. Lu, and D. Xu, \u201cVoxelcontext-net: An octree based framework for point cloud compression,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6042\u20136051, 2021.   \n[47] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234\u2013241, Springer, 2015.   \n[48] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable detr: Deformable transformers for end-to-end object detection,\u201d arXiv preprint arXiv:2010.04159, 2020.   \n[49] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11621\u201311631, 2020.   \n[50] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall, \u201cSemantickitti: A dataset for semantic scene understanding of lidar sequences,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9297\u20139307, 2019.   \n[51] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, \u201cDeformable convolutional networks,\u201d in 2017 IEEE International Conference on Computer Vision (ICCV), pp. 764\u2013773, 2017.   \n[52] T.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie, \u201cFeature pyramid networks for object detection,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117\u20132125, 2017.   \n[53] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d 2017.   \n[54] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu, \u201cClass-balanced grouping and sampling for point cloud 3d object detection,\u201d arXiv preprint arXiv:1908.09492, 2019.   \n[55] J. Huang and G. Huang, \u201cBevdet4d: Exploit temporal cues in multi-camera 3d object detection,\u201d arXiv preprint arXiv:2203.17054, 2022.   \n[56] L. Roldao, R. de Charette, and A. Verroust-Blondet, \u201cLmscnet: Lightweight multiscale 3d semantic completion,\u201d in 2020 International Conference on 3D Vision (3DV), pp. 111\u2013119, IEEE, 2020.   \n[57] J. Li, K. Han, P. Wang, Y. Liu, and X. Yuan, \u201cAnisotropic convolutional networks for 3d semantic scene completion,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3351\u20133359, 2020.   \n[58] X. Chen, K.-Y. Lin, C. Qian, G. Zeng, and H. Li, \u201c3d sketch-aware semantic scene completion via semi-supervised structure prior,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4193\u20134202, 2020.   \n[59] X. Yan, J. Gao, J. Li, R. Zhang, Z. Li, R. Huang, and S. Cui, \u201cSparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, pp. 3101\u20133109, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A More Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide detailed explanations of our proposed modules. ", "page_idx": 13}, {"type": "text", "text": "For the Semantic-Guided Octree Initialization, our approach commences with acquiring semantic segmentation labels for images by projecting occupancy labels onto the surround-view images. Subsequently, a UNet is trained using these labels. The initialization process entails randomly initializing dense queries, where each query\u2019s center point serves as a reference point projected onto the range-view images. If a reference point is projected onto a ground pixel (i.e., driveable surface, other flat, or sidewalk), the probability increases by 0.1. Conversely, if projected onto a background pixel (excluding ground classes), the probability increases by 0.5. Projection onto a foreground pixel increases the probability of requiring a split at that position by 1.0. This process assigns a split probability to each query, and the octree mask is constructed through average pooling, capturing split probabilities at different query levels. After obtaining the octree mask, we designate the top $20\\%$ confidence queries as parent queries in level 1, while the remaining queries become Leaf queries and remain unsplit. Moving to level 2, after splitting the parent queries into octants, the top $60\\%$ confidence positions are selected as new parent queries, and the remainder as leaf queries. By storing leaf queries at each level, we construct a sparse and multi-granularity octree structure for queries. ", "page_idx": 13}, {"type": "text", "text": "In Iterative Structure Rectification, at level 1, we retain predictions for the top $10\\%$ of positions with confidence. For the remaining positions, a 2-layer MLP is utilized to predict probabilities. These new probabilities are blended with the existing probabilities, with a weight distribution of $60\\%$ for the new probabilities and $40\\%$ for the old ones. The top $10\\%$ of positions with the new probability values are identified as the required splits, shaping the structure of the new level 1. Similarly, at level 2, predictions for the top $30\\%$ of positions with confidence are preserved. For positions not in the top $30\\%$ , probabilities are predicted using a 2-layer MLP. The new probabilities are computed by merging them with the original probabilities, with an even weight distribution of $50\\%$ for each. The top $30\\%$ of the new probability values are then selected as the positions necessitating splitting, delineating the structure of the new level 2. ", "page_idx": 13}, {"type": "text", "text": "B Octree node index calculation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The hierarchical structure of the octree, particularly the assignment of queries to respective levels, is determined based on the octree mask $M_{o}$ and the query selection ratio denoted as $\\alpha=\\{\\alpha^{1},\\alpha^{2},\\ldots,\\alpha^{L-1}\\}$ . These ratios govern the number of subdivisions at each level, thereby defining the hierarchical organization of the octree. The procedure is as follows. For level l, queries with $\\bar{M_{o}^{l}}$ values within the top $\\alpha^{l}$ percentile are identified as candidates for octant subdivision. Subsequently, within octants that have undergone one previous subdivision at the next level, queries are once again selected based on their values falling within the top $\\alpha^{2}$ percentile, initiating another round of subdivision. This process continues iteratively until reaching the final level of the octree. ", "page_idx": 13}, {"type": "text", "text": "Simultaneously, exploiting the octree structure facilitates the direct conversion of sparse octree queries into dense queries to align with the desired output shape. For a query $q_{o c t r e e}$ at level $l$ with the index $(a,b,c)$ , the indexes of its corresponding $\\mathring{\\mathsf{B}}^{L-l}$ children nodes in level $L$ are determined by $(a\\times2^{L-l}+a_{o f f s e t},b\\times2^{L-l}+b_{o f f s e t},c\\times2^{L-l}+c_{o f f s e t})$ , where $a_{o f f s e t}$ , $b_{o f f s e t}$ , and $c_{o f f s e t}$ are independent, ranging from 0 to $2^{L-l}$ . Here, $L$ denotes the depth of the octree. During this process, we allocate the feature of $q_{o c t r e e}$ to all of these positions. By iteratively applying this procedure to all queries at each level, we effectively transform $Q_{o c t r e e}$ into $Q_{d e n s e}$ . ", "page_idx": 13}, {"type": "text", "text": "C Octree Ground Truth Generation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We derive the octree ground truth from the semantic occupancy ground truth. Specifically, for a voxel at level $l$ in the octree, we identify its corresponding $8^{L-\\dot{l}}$ voxels in the semantic occupancy ground truth. If these voxels share the same labels, we deem the voxel at level $l$ unnecessary to split (assigned a value of 0); otherwise, it necessitates division (assigned a value of 1), as the current resolution is insufficient to represent it adequately. Through this process, each voxel at each level is assigned a binary value of 0 or 1. Then we obtain the octree ground truth $G_{o c t r e e}=\\{G_{o c t r e e}^{l}\\in\\mathbb{R}^{\\frac{X}{2^{l}},\\frac{Y}{2^{l}},\\frac{Z}{2^{l}}}\\}_{l=1}^{L-1}$ Here, $L$ represents the depth of the octree, while $X,Y$ , and $Z$ denote the volume resolution of the semantic occupancy ground truth. $G_{o c t r e e}$ is employed to supervise the octree mask using focal loss, facilitating the network in learning the octree structure information. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "D More discussion of octree initialization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given that the FloSP method outlined in MonoScene(27) incorporates a 3D to 2D projection operation, similar to our initialization approach, we additionally adapted this method for comparison. Specifically, we employed FloSP to extract 3D voxel features from 2D image features. Subsequently, we applied a Multi-Layer Perceptron (MLP) to predict the splitting probability of each voxel, replacing the randomly initialized queries used in the original ablation experiments. The results indicate that, although this operation outperforms predictions from randomly initialized queries, it is still constrained by insufficient information, resulting in a decline in overall performance. ", "page_idx": 14}, {"type": "table", "img_path": "os14qXhy55/tmp/301d57f023c721604f0acd2bc97df1dd00a6bf210c04bf32f76e04ba8ca4d44e.jpg", "table_caption": ["Table 8: More ablation of octree initialization "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "E Analysis of Various Usage of Octree. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As a classic technique, octree is employed in various tasks (42; 35; 36; 37; 38). Despite differences in addressed problems, we compare our method with OGN(42), which proposes an octree-based upsampling approach. We keep the similar setup in Tab. 4 (b) but substitute the deconvolution decoder with OGN\u2019s octree decoder. Results in Tab. 9 indicate that employing octree solely in the decoder fails to mitigate excessive computational costs and yields sub-optimal performance, mainly due to the high query count during encoding. ", "page_idx": 14}, {"type": "table", "img_path": "os14qXhy55/tmp/bebe79d8d085ca9f640351aac8c86f083e5580b529cf18047ba2ee42ed742ad5.jpg", "table_caption": ["Table 9: Comparison with another octree method. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "F More Visualization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Fig. 5 shows additional visualizations of proposed OctreeOcc. Evidently, our approach, leveraging the multi-granularity octree modeling, demonstrates superior performance particularly in the categories of truck, bus, and manmade objects. ", "page_idx": 14}, {"type": "text", "text": "Fig. 6 illustrates the results of occupancy prediction alongside the corresponding octree structure. For clarity in visualization, we employ distinct colors to represent voxels at various levels of the octree prediction, based on their occupancy status. For improved visualization, only a portion correctly corresponding to the occupancy prediction is displayed, rather than the entire octree structure, ensuring clarity and focus on the relevant information. Level 3 (voxel size: $0.4\\mathrm{m}\\times0.4\\mathrm{m}\\times0.4\\mathrm{m})$ is depicted in light gray, level 2 (voxel size: $0.8\\mathrm{m}\\,\\times\\,0.8\\mathrm{m}\\,\\times\\,0.8\\mathrm{m})$ in medium gray, and level 1 (voxel size: $1.6\\mathrm{m}\\times1.6\\mathrm{m}\\times1.6\\mathrm{m})$ in dark gray. It\u2019s worth noting that level 1 voxels, predominantly situated in free space and within objects, might be less intuitively discernible. Nonetheless, this image underscores the efficacy of octree modeling, which tailors voxel sizes to different semantic regions, enhancing representation accuracy. ", "page_idx": 14}, {"type": "image", "img_path": "os14qXhy55/tmp/d5a274e2850fc0dddafcf9ec4442604e970fd78069d15fca86ad3b0a1ab15599.jpg", "img_caption": ["Figure 5: More visualization on Occ3D-nuScenes validation set. The first row displays input multi-view images, while the second row showcases the occupancy prediction results of PanoOcc(8), FBOCC(3), our methods, and the ground truth. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "os14qXhy55/tmp/48e58c970edf8a321dad67f6e592dcf748be6265e10dc9f83e1ecabd6ac36b2b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: Visulization of octree structure. The first row displays input multi-view images, while the second and third rows showcase the occupancy prediction results and the corresponding octree structure prediction results. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The abstract has mentioned the motivation of the paper to reduce the computational overhead using octree technique, the details of the methodology and the experimental results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The conclusion section describes the limitations of the methodology ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Justification: Our paper does not cover theoretical results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We include all parameter settings, optimizer details, training resources, etc., for the model in the paper. Additionally, we outline how each experiment was conducted. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We will make the code publicly available upon acceptance of the paper to advance the field. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Section 4.3 shows all of the model details. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The field of occupancy prediction that we study does not usually require error bars, confidence intervals, or statistical significance tests, and we have done extensive comparative and ablation experiments to demonstrate the validity of the method. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The resources, time, etc. required for model training are provided in the Implementation Details. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our research complies with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our approach will facilitate the advancement of self-driving perception algorithms with less computational resource requirements that are practical in real-world deployments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our approach does not involve such data with pre-trained models Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: For the datasets used with the methods of comparison, we have cited them. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have not submitted new assets. If the paper is accepted, we will make the code public soon. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]