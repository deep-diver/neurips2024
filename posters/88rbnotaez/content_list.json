[{"type": "text", "text": "Make-it-Real: Unleashing Large Multimodal Model for Painting 3D Objects with Realistic Materials ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ye Fang\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fudan University Shanghai AI Laboratory yefang23@m.fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Zeyi Sun\u2217 Shanghai Jiao Tong University Shanghai AI Laboratory szy2023@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Tong Wu\u2020 The Chinese University of Hong Kong Stanford University wutong16@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Jiaqi Wang Shanghai AI Laboratory wangjiaqi@pjlab.org.cn ", "page_idx": 0}, {"type": "text", "text": "Ziwei Liu S-Lab, NTU ziwei.liu@ntu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Gordon Wetzstein Stanford University gordon.wetzstein@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Dahua Lin\u2020 The Chinese University of Hong Kong Shanghai AI Laboratory CPII under InnoHK dhlin@ie.cuhk.edu.hk ", "page_idx": 0}, {"type": "image", "img_path": "88rbNOtAez/tmp/6dcf33844f7f7bbaf0c0c540bb90283479b8b9dc3d374337245f0211d7ef0561.jpg", "img_caption": ["Figure 1: Usage of Make-it-Real. Our method can refine a wide range of albedo-map-only 3D objects from both CAD design and generative models. Our method enhances the realism of objects, enables part-specific material assignment to objects and generate PBR maps that are compatible with downstream engines. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions. However, existing 3D assets and generative models often lack authentic material properties. Manual assignment of materials using graphic software is a tedious and time-consuming task. In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) ", "page_idx": 0}, {"type": "text", "text": "We demonstrate that GPT-4V can effectively recognize and describe materials. 2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects. 3) The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original albedo map, significantly enhancing their visual authenticity. Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets. Our project website is at: https://SunzeY.github.io/Make-it-Real/. ", "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "High-quality materials are important for the nuanced inclusion of view-dependent and lightingdependent effects in 3D assets for traditional graphics pipelines, critical for achieving realism in gaming, online product showcasing, and virtual/augmented reality. However, many existing assets and generated objects often lack realistic material properties, limiting their application in downstream tasks. Furthermore, creating hand-designed realistic textures necessitates specialized graphic software and involves a laborious and time-consuming process, compounded by significant creative challenges [32]. ", "page_idx": 1}, {"type": "text", "text": "Traditional computer graphics methods have been either manually creating materials or reconstructing them from physical measurements. Emerging text-to-3D generative models [7, 34, 55, 10, 73, 50, 19, 51] and image-to-3D generative models [69, 58, 25, 63, 22, 62] are successful in creating complex geometries and detailed appearances, but they struggle to generate physically realistic materials, hampering their practical applicability. Recent studies have also explored advanced aspects of appearance generation [11, 75, 70, 8, 40]. However, they often rely on simplified material models. For instance, [40] lacks the ability to produce metallic maps. All these approaches do not generate corresponding displacement and height maps, which restricts the diversity and realism of the generated materials, especially regarding depth and tactile qualities. Furthermore, these methods typically require relatively long training and inference time. Considering the abundance of high-quality 3D assets online [16, 17] that lack material attributes, and the maturity of 3D generative models in geometry and albedo modeling, we aim to recover materials from high-quality geometry and base-colored 3D meshes. ", "page_idx": 1}, {"type": "text", "text": "However, extracting and recovering material representations for 3D meshes is challenging. Unlike previous material recognition methods [40, 4, 20], this difficulty is heightened when identifying and separating different material regions within 3D objects in constricted albedo textures. These maps only reflect the base color and can be distorted, as shown by the globe in the upper right corner of Figure 1. Additionally, shadows and lighting can affect judgment. Thus, the model must have strong material recognition capabilities and prior knowledge of object types and materials. ", "page_idx": 1}, {"type": "text", "text": "The emergence of Multimodal Large Language Models (MLLMs) [5, 46, 12, 2, 23, 59] provides novel approaches to problem-solving. These models have powerful visual understanding and recognition capabilities, along with a vast repository of prior knowledge, which covers the task of material estimation. Specifically, we are using GPT-4V(ision) for matching of materials. We first create highly detailed descriptions for materials to build a comprehensive library. Next, we use GPT-4V to retrieve materials for each segmented part of the object, utilizing visual prompts [72] and hierarchical text prompts. Finally, we meticulously designed algorithms to generate SVBRDF maps with consistent albedo, achieving realistic visual quality. ", "page_idx": 1}, {"type": "text", "text": "Notably, our work differs from the aforementioned studies by leveraging prior knowledge from foundation models like GPT-4V to extract and infer materials in albedo-only constrained scenarios. Additionally, we utilize existing material libraries as references to generate corresponding SVBRDF maps on a designed region-to-pixel algorithm. As illustrated in Figure 1, our approach features: 1) Enhanced 3D mesh realism: Leveraging GPT-4V\u2019s visual perception and external knowledge, our method improves the realism, depth, and visual quality of a wide range of mature 3D content generation models. 2) Part-specific material matching: Ensuring material consistency with a segmentation network and refinement process, enabling precise material property retrieval for each segment. 3) Rendering engine compatibility: Generating six comprehensive material maps(roughness, metallic, specular, normal, displacement, height), which are compatible with downstream rendering engines. Developers only need to paint albedo textures; material properties are then automatically generated, saving extensive time on detailed ambient occlusion masks and material map creation. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present the first exploration of leveraging multimodal large language models (MLLMs), e.g.,GPT-4V for material recognition and unleashing their potential in applying real-world materials to extensive 3D objects with albedo-only.   \n\u2022 We create a material library containing thousands of materials with highly detailed descriptions readily for MLLMs to look up and assign.   \n\u2022 We develope an effective pipeline for texture segmentation, material matching and SVBRDF maps generation, enabling the high-quality application of materials to 3D assets. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3D Object Generation. The generation of 3D models using deep learning methods has experienced rapid development in recent times. The mainstream research can be primarily divided into two categories. The first category relies on techniques that optimize a Neural Radiance Field (NeRF) [43] or 3D Gaussian [30] guided by 2D diffusion model through score-distillation-sampling(SDS) loss [48, 42, 39, 8, 64, 53, 56, 67]. The second category aims at obtaining 3D representation via direct inference model, e.g.\u201e Point-E [44], Shap-E [28], and LRM [25], proven fast with high quality through large-scale pretraining [24, 22, 57, 35, 55, 71, 58, 63, 65]. Although the capabilities of these methods are continuously improving, they still lack a high degree of realism in textures. More importantly, since the textures of 3D objects obtained by these methods are shaded, they cannot directly respond to lighting changes under different lighting conditions, leading to less realism. Although some works attempt to generate PBR textures, their results are considerably limited to generate physically realistic materials due to the low robustness of SDS [75, 8]. Our work is the first to introduce the prior knowledge of MLLMs into the texture synthesis process. We verifies that our method can be seamlessly and effectively applied to generated 3D objects, facilitating their downstream applications under different lighting condition. ", "page_idx": 2}, {"type": "text", "text": "Material Capture and Generation. Recent studies such as Material Palette [40], MatSim [20], TwoshotBRDF [4] have made progress in the recognition and extraction of 3D object materials, allowing for the retrieval of SVBRDF information from images of real materials [40] and combining object shape and illumination [4], but they fail to extract and infer the materials of 3D objects with only albedo. On the other hand, works like Paint-it [75], Matlaber [70], Collaborative [60], Fantasia3D [8], and TANGO [11] focus on generating text-controllable 3D meshes with physically-based rendering material properties. However, these methods require either extensive training time on BRDF datasets or long inference time for generating materials. Additionally, they are limited to synthesizing only a subset of PBR textures and cannot generate the full range of material properties, such as height and displacement, which are essential for the fine tactile perception of object surfaces. ", "page_idx": 2}, {"type": "text", "text": "Multimodal Large Language Models. In the wake of the advancements achieved by large language models (LLMs) [5, 46, 12, 2, 23, 59], domain of research has increasingly turned its attention towards multimodal large language models (MLLMs). Recent advances in this field focus on the integration of vision understanding capabilities with LLMs [76, 1, 37, 36, 26, 21, 3, 18, 54, 49, 6].The advent of GPT-4V [45] has marked a significant milestone in the evolution of MLLMs, demonstrating groundbreaking 2D comprehending capabilities and open-world knowledge. Although GPT-4V cannot directly process 3D data, a pioneering work , GPTEval3D [68], first exploited GPT-4V\u2019s ability in evaluating the quality of generated 3D objects, and found that GPT-4V\u2019s judgement was in line with human evaluation. In this work, we delve into a new application of GPT-4V for material assignment of 3D objects. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Physically Based Rendering (PBR) materials are a compact representation of the bi-directional reflectance distribution function (BRDF), which describes how light is reflected from a surface. PBR material maps primarily encompass seven attributes: Albedo, Roughness, Metallic, Normal, Specular, Height, and Displacement. Based on the rendering equation [29], given a location $x$ and the surface normal $n$ , the incident light intensity at this point is denoted as $L_{i}(\\omega_{i};x)$ along the direction $\\omega_{i}$ ; BRDF $f_{r}(\\omega_{o},\\omega_{i};\\boldsymbol{x})$ denotes the reflectance coefficient of the material viewing from direction $\\omega_{o}$ . The observed light intensity $L_{o}(\\omega_{o};x)$ is calculated over the hemisphere $\\Omega=\\{\\omega_{i}:\\omega_{i}\\cdot n>0\\}$ : ", "page_idx": 2}, {"type": "image", "img_path": "88rbNOtAez/tmp/0d98891c6a799a7b57da903cc1c3468e7606cfc6bc8bb9b550c6659131680bb4.jpg", "img_caption": ["Figure 2: Overall pipeline. This pipeline of Make-it-Real is composed of image rendering and material segmentation, MLLM-based material retrieval, and SVBRDF Maps Generation. We finally use blender engine to conduct physically-based rendering. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{o}(\\omega_{o};x)=\\int_{\\Omega}L_{i}(\\omega_{i};x)f_{r}(\\omega_{o},\\omega_{i};x)(\\omega_{i}\\cdot n)d\\omega_{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given the advancements in generating high-quality 3D shapes with albedo maps, the restoration of realistic material properties remains a challenge. We highlight a novel problem: given a 3D mesh $(\\tilde{O})$ and known Albedo $(\\tilde{A})$ map, which reflect the object\u2019s intrinsic appearance, the goal is to extract and restore all other SVBRDF attributes of the object, i.e. $M a k e\\mathrm{-}i t\\mathrm{-}\\bar{R e a l}(\\tilde{O},\\tilde{A})=\\left\\{R,M,N,S,H,D\\right\\}$ . Our setup supports the popular Cook-Torrance analytical BRDF model [13]. In this parameterization, the BRDF includes components for albedo $b_{a}\\in\\dot{\\mathbb{R}}^{3}$ , metallic $b_{m}\\in\\mathbb{R}$ , and roughness $b_{r}\\in\\mathbb{R}$ . For more complex surface simulations, such as displacement and height modeling, we use the Blender rendering engine to simulate the BRDF function $f_{r}(\\omega_{o},\\omega_{i};\\boldsymbol{x})$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Make-it-Real: A Framework for Material Matching and Generataion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we outline our material matching and generation pipeline, illustrated in Figure 2, which encompasses three stages: rendering and segmenting 3D meshes, retrieving matching materials using MLLM, and generating spatially varying BRDF maps from coarse to fine. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Rendering and Material Segmentation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To accurately segment different material regions on 3D meshes with albedo maps, we propose an innovative segmentation strategy based on 2D image rendering in Figure 2 (a). Initially, we use rasterization to render the input albedo mesh from various viewpoints to obtain a series of images: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{Z}(\\boldsymbol{x},\\boldsymbol{y})=\\mathcal{R}\\left(\\mathrm{UV}_{\\mathrm{map}}\\left(\\mathrm{rasterize}(\\tilde{O},\\boldsymbol{v}_{t},\\boldsymbol{x},\\boldsymbol{y})\\right),\\mathcal{T}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ensuremath{\\mathcal{T}}(x,y)$ is the pixel value at image coordinates $(x,y)$ , rasterize $(\\tilde{O},v_{t},\\cdot)$ maps the 3D mesh $\\tilde{O}$ from viewpoint $v_{t}$ to 2D screen coordinates, $\\mathrm{{UV}_{m a p}(\\cdot)}$ converts rasterized coordinates to UV coordinates, $\\tau$ is the albedo map, and $\\mathcal{R}(\\cdot,\\tau)$ samples color from the $\\tau$ using the UV coordinates. ", "page_idx": 3}, {"type": "text", "text": "For the rendered images, we employ the Semantic-SAM [33] to perform preliminary semantic segmentation. Empirically, we select the main viewpoint with the largest projected area of the mesh, as it is more likely to contain more details. To address potential over-segmentation, drawing inspiration from [74], we extract non-overlapping segments from the masks to form distinct patches, as detailed in our approach in Figure 4 (a). These patches are then merged based on similar colors to obtain the final material grouping. We incorporate Set-of-Mark [72] method to annotate each material segment with a unique identifier, sorted by area size from largest to smallest. This annotation acts as a visual cue, enhancing the visual comprehension capabilities of large multimodal language models. ", "page_idx": 3}, {"type": "image", "img_path": "88rbNOtAez/tmp/1a0ebea2f074b69410fc3fef58e7169592049f14d90e3375bda036cc2d04c524.jpg", "img_caption": ["Figure 3: The process of MLLM retrieving materials from the Material Library. Utilizing GPT-4V model, we develop a material library, meticulously generating and cataloging comprehensive descriptions for each material. This structured repository facilitates hierarchical querying for material allocation in subsequent looking up processes. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2.2 MLLM-based Material Retrieval ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Material library with fine-grained annotations. To enable large multimodal language models to accurately retrieve and match materials, we construct a finely annotated material library, as shown in Figure 3. It is composed of three main components: comprehensive PBR texture maps, highlydetailed records, and a category tree. It comprises 1,400 unique, tileable materials spanning 13 primary categories and 80 subtypes. The data primarily derives from the [61], which offers comprehensive PBR material textures under a CC0 license with 4K resolution. Each material is represented by seven maps: albedo, roughness, metallic, specular, normal, displacement and height. Accompanying each material are highly-detailed annotations by GPT-4V, offering thorough descriptions of the material\u2019s visual characteristics and rich semantic information for the subsequent retrieval process. Created by crawling material sphere images and constructing prompts, these annotations capture subtle differences between materials, facilitating precise retrieval by GPT-4V, as detailed in Appendix B.4. ", "page_idx": 4}, {"type": "text", "text": "Hierarchical prompting for material retrieval. Due to the vast size of our material library, feeding all prompts to GPT-4V simultaneously proves inefficient and challenging for memory retention. To ensure efficient and accurate material allocation in segmented areas of 3D meshes, we adopt a hierarchical text prompting approach. The schematic of the designed prompt is shown on the left side of Figure 3, and a complete querying process unfolds in Appendix B.4. This method starts by identifying the primary material types corresponding to each labeled region. Subsequently, hierarchical prompts guide GPT-4V to distinguish specific subclasses within the main material categories. We retrieve all descriptions under these subclasses to ascertain the most ftiting descriptions for the segmented blocks. This hierarchical processing enables a more granular search of our material library, identifying the optimal description for each material segment. This approach aids in assigning the most suitable materials to each region and reduces memory and time consumption. ", "page_idx": 4}, {"type": "image", "img_path": "88rbNOtAez/tmp/50f4e70aeec27481b26f6635ac595969278df891afea302d63549132a69ed2c3.jpg", "img_caption": ["Figure 4: Illustrations of mask refinement in 2D image space and UV texture space. (a) We effectively cluster concise material-aware masks compared to original segmented parts from [33]. (b) We fix missing parts on the uv texture space to get a complete texture partition map. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.2.3 SVBRDF Maps Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We propose a method to generate SVBRDF maps on a region-to-pixel scale in Figure 2 (c). Initially, we segment texture map in uv space based on queried material regions on 2D image space. We then estimate BRDF values in pixel space using the object\u2019s original albedo map for reference, ensuring consistency with the albedo map. This approach effectively enhances the realism of rendered surfaces. ", "page_idx": 5}, {"type": "text", "text": "Region-level texture map partitioning. Upon acquiring segmentation masks for material regions within the 2D rendered space of a 3D mesh, our objective transitions to transposing these segmentations from the 2D image space to the corresponding UV space. As described in Section 3.2.1,we extract 2D image features from 3D mesh points via rasterization, and then we apply the material masks to these features, facilitating the accurate transfer of segmentation to UV space. To project the image feature $I_{t}$ back to the texture atlas $\\mathcal{T}_{t}$ with segmented image mask $m_{t}$ at the viewpoint $v_{t}$ , we apply gradient-based optimization for $L_{t}$ over the values of $\\mathcal{T}_{t}$ when rendered through the differential renderer $\\mathcal{R}$ , as presented in Equation (3). In Equation (4), we then compute the difference between $\\mathcal{T}_{t}^{m a s k}$ and the initialized $\\mathcal{T}_{t}$ to transfer the mask image feature $m_{t}$ into the texture space, represented by $m_{u v}$ . The term $\\sigma$ denotes the difference coefficient. Due to the limited perspectives available in rendering, we avoid using a naive median-filling approach to ensure that there are no missing areas on the texture map. Instead, we employ a block-centric clustering based on albedo, as illustrated in Figure 4 (b), to obtain cohesive and refined region masks. The process is shown in Appendix B.1 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{T_{t}}L_{t}=(\\mathcal{R}(\\mathrm{mesh},\\mathcal{T}_{t},v_{t})-I_{t})\\odot m_{t}\\frac{\\partial\\mathcal{R}}{\\partial T_{t}}.}\\\\ &{\\quad m_{u v}=\\mathbf{1}\\left(\\displaystyle\\sum_{i=1}^{3}|\\mathcal{T}_{t}^{m a s k}-\\mathcal{T}_{t}|>\\sigma\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Pixel-level albedo-referenced estimation. To achieve precise estimation of spatially varying BRDF (SVBRDF) at the pixel level, we draw inspiration from techniques commonly utilized by artists in creating texture maps. Artists typically use albedo maps as a reference for constructing ambient occlusion masks and further generating SVBRDF maps for material properties, which occupies a significant time portion of appearance modeling. Our method involves using the albedo map of the original object as a reference to refine the retrieved materials. We enhance the querying process using a KD-Tree algorithm, which searches for the nearest neighbor pixel index in the key albedo(retrieved map) for each RGB value of the queried albedo(input map) pixel, detailed in Appendix B.2. This process ensures that areas with similar colors exhibit similar BRDF values, avoiding abrupt changes in material properties; For regions with greater color differences, the distribution of differences aligns consistently with the input albedo, to simulate variations such as embossments or scratches. We retrieves SVBRDF values at pixel level, maintaining texture consistency with the albedo and producing appropriate concave or smooth surface effects. We further analyze the effects in Section 4.2. ", "page_idx": 5}, {"type": "image", "img_path": "88rbNOtAez/tmp/4ddf009f4b26cc67c0004e540f004176beb808e43fe6f4913f12f1aa8e15255f.jpg", "img_caption": ["Figure 5: Qualitative results of Make-it-Real refining 3D asserts without PBR maps. Objects are selected from Objaverse [15] with albedo maps only. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "88rbNOtAez/tmp/1e4db7caf91f849dc7aa28837ac9ceb774c2195271302cfb800e09dbcc00a43f.jpg", "img_caption": ["Figure 6: Visualization of generated texture maps. We visualize some SVBRDF maps, where the material maps are well aligned with the albedo maps. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "88rbNOtAez/tmp/6747f49590484e6934ce9d0ea1286deff68565a54a0ddd1d6863099b4a145907.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: Qualitative comparisons between Make-it-Real refining results and 3D objects generated by edge-cutting 3D content creation models. The upper row depicts image-to-3D models (InstantMesh and TripoSR), and the lower row shows results of text-to-3D models. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Experiment settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To verify the effectiveness of Make-it-Real, we conduct refinement experiments mainly on two types of objects. The first type is artificial 3D assets, with the primary model from Objaverse [15] flitered by [55]; The second type is objects generated by state-of-the-art 3D generation methods. For existing 3D assets, we pick 200 objects with diverse textures from Objaverse by human experts. For 3D generative models (InstantMesh [69], TripoSR [58], MVDream [53], Instant3D [34] and Fantasia3D [8]), we also generate 200 objects for each methods use prompts designed in GPTEval3D [68]. We use Make-it-Real to refine the objects and compare the texture quality before and after refinement. We perform both GPT-4V [45] based evaluation and user study on the above objects (Detailed guidence and prompts for evaluation are available in Appendix B.5). ", "page_idx": 7}, {"type": "text", "text": "In addition to evaluation details, we also provide information about the rendering procedure in Appendix B.3 for reproducibility. This includes the rendering tools used in the experiments, hyper parameters related to back-projection, and the basic performance metrics of the model during the experiments, such as time, memory, and number of queries. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experiment results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Texture refinement for existing 3D assets. As shown in Figure 5, assets processed through Makeit-Real demonstrate the capability to accurately segment objects, assign various suitable materials, and synthesize high-fidelity, photorealistic textures. Some materials exhibit notable highlights, such as a marble bathtub and a globe made of gold. The interaction of these different materials with light varies significantly, leading to diverse reflective effects under the same environmental lighting, thus presenting a range of textures. Additionally, material properties vary across different regions; for instance, the globe\u2019s landmass and handle exhibit gold characteristics, while other parts are identified as plastic. Furthermore, the texture and appearance of materials also vary, such as the subtle wrinkles on the red box in the second column and the more pronounced color contrast at the base of the blue box, which enhances realism and reflects signs of use. Due to the albedo-referenced algorithm design in Section 3.2.3, regions with similar colors have similar BRDF values, avoiding abrupt changes in material properties, such as the continuous gold surface of the globe and the silver body of the kettle. Regions with significant color differences display consistent differential distributions with the key map, such as the embossed textures on the lower left corner of the water bottle in Figure 5 and the subtle particle variations on the surface of the red oil drum. Additionally, visualization of texture maps demonstrates reasonably consistent textures with albedo, shown in Figure 6. Quantitive results are shown in Table 1, and more qualitative results can be found in Appendix E. ", "page_idx": 7}, {"type": "table", "img_path": "88rbNOtAez/tmp/070fdf7fa44751dfeffbd38a837cd38a82275ec44d2edb60e705dbdea7c8bdec.jpg", "table_caption": ["Table 1: GPT evaluation and user preference. GPT\u2019s and user\u2019s preference comparison on Make-itReal refined objects sourced from existing 3D assets and state-of-the-art 3D generation methods. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "88rbNOtAez/tmp/22fb001ef176f3a0c6cd26b7725a00f9ac034150c152c8f3fbaa123fbf429319.jpg", "img_caption": ["Figure 8: Ablation study of material segmentation refinement. Compared to direct usage of SemanticSAM [33], Our post-process tailored for material segmentation on 3D object can produce more consistent results. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "88rbNOtAez/tmp/f6433d6381e0e9eb4a19c0445782d86f45b9502072896602a01f783af636d4cc.jpg", "img_caption": ["Figure 9: Ablation study of missing part refinement. Our method on the bottom row produces consistent texture maps and avoids the missing parts of material texture. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Texture refinement for generated 3D objects. Figure 7 displays the qualitative results. By leveraging our model for enhancement, we observe that Make-it-Real successfully generates appropriate material maps for both image-to-3D and text-to-3D models. ", "page_idx": 8}, {"type": "text", "text": "Similarly, our Make-it-Real successfully paint these models with materials. As reported in Table 1, human experts consistently favor the objects post-refinement across all evaluated 3D content creation methods. This preference aligns with the evaluations performed by GPT-4V, indicating a general consensus on the enhancement in quality achieved through Make-it-Real refinement process. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Effects of mask refinement module. In Section 3.2.1, we performed additional material postprocessing on the segmentation outcomes, as depicted in Figure 8. The refined results in the last row indicate that our module achieves precise material segmentation for most standard objects, thereby enabling more accurate queries by GPT-4V. In Section 3.2.3, we addressed the completion of missing regions in the texture map within the UV space at the regional level, as illustrated in Figure 9. This method not only increases texture coverage but also enhances the visual quality and consistency of the refined 3D model. More ablation studies can be found in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "Effects of different texture maps. We validate the impact of various texture maps generated by Make-it-Real on the appearance of 3D objects, as illustrated in Figure 16 of Appendix D.1. We provide a detailed qualitative analysis of how different maps enhance the visual texture of materials. For example, increasing metallic results in dampening of the base albedo and increasing in the shine on the surface, while reducing roughness gives the surface a smoother appearance and enhances highlights. Meanwhile, displacement and height maps contribute to the fine-grained bump details on the object\u2019s surface. ", "page_idx": 9}, {"type": "text", "text": "Effects of different UV mappings. Since UV mapping is a crucial step in the 2D-3D alignment technique of our method, we conduct experiments to assess how its quality impacts model performance. The results, shown in Appendix D.2 and in the second column of Figure 17, indicate that UV mappings with excessive fragmentation and color entanglement can cause issues. However, our method still performs well with artist-created UV mappings and Blender\u2019s built-in mapping techniques. This indicates our method still demonstrates good robustness with many mapping techniques. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a novel framework leveraging MLLMs prior of the world to build a material library and proposing an automatic pipeline to refine and synthesize new PBR maps for initial 3D models, achieving highly photo-realistic PBR textures maps synthesis. Experimental results confirm that our approach can automatically refine both generated and CAD models to achieve photo-realism under dynamic lighting conditions. We believe Make-it-Real is a new and promising solution in the last few procedures of AI based 3D content creation pipeline with the development of MLLMs like GPT-4V [45] as well as the roaring field of deep learning based 3D generation from scratch. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project is funded in part by Shanghai Artificial lntelligence Laboratory, the National Key R&D Program of China (2022ZD0160201), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)\u2019s InnoHK. Dahua Lin is a PI of CPII under the InnoHK. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022. URL https: //api.semanticscholar.org/CorpusID:248476411. 3 ", "page_idx": 9}, {"type": "text", "text": "[2] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Cr\u00e9py, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C. D\u2019iaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier Garc\u00eda, Sebastian Gehrmann, Lucas ", "page_idx": 9}, {"type": "text", "text": "Gonz\u00e1lez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, ZhongZhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. ArXiv, abs/2305.10403, 2023. URL https://api.semanticscholar.org/CorpusID:258740735. 2, 3 ", "page_idx": 10}, {"type": "text", "text": "[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. ArXiv, abs/2308.01390, 2023. URL https://api.semanticscholar.org/CorpusID:261043320. 3 ", "page_idx": 10}, {"type": "text", "text": "[4] Mark Boss, Varun Jampani, Kihwan Kim, Hendrik Lensch, and Jan Kautz. Two-shot spatiallyvarying brdf and shape estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3982\u20133991, 2020. 2, 3   \n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. URL https://api.semanticscholar.org/CorpusID:218971783. 2, 3   \n[6] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 3   \n[7] Rui Chen, Y. Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 22189\u201322199, 2023. URL https://api.semanticscholar. org/CorpusID:257757213. 2   \n[8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023. 2, 3, 8, 9   \n[9] Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, and Xiaowei Zhou. Intrinsicanything: Learning diffusion priors for inverse rendering under unknown illumination. arXiv preprint arXiv:2404.11593, 2024. 25   \n[10] Xu Chen, Tianjian Jiang, Jie Song, Jinlong Yang, Michael J Black, Andreas Geiger, and Otmar Hilliges. gdna: Towards generative detailed neural avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20427\u201320437, 2022. 2   \n[11] Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. Tango: Text-driven photorealistic and robust 3d stylization via lighting decomposition. Advances in Neural Information Processing Systems, 35:30923\u201330936, 2022. 2, 3   \n[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1\u2013240:113, 2022. URL https://api.semanticscholar.org/CorpusID:247951931. 2, 3 ", "page_idx": 11}, {"type": "text", "text": "[13] Robert L Cook and Kenneth E. Torrance. A reflectance model for computer graphics. ACM Transactions on Graphics (ToG), 1(1):7\u201324, 1982. 4 ", "page_idx": 11}, {"type": "text", "text": "[14] Sri Aditya Deevi, Connor Lee, Lu Gan, Sushruth Nagesh, Gaurav Pandey, and Soon-Jo Chung. Rgb- $\\mathbf{\\nabla}\\cdot\\mathbf{X}$ object detection via scene-specific fusion modules. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 7366\u20137375, 2024. 25 ", "page_idx": 11}, {"type": "text", "text": "[15] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects, 2022. 7, 8, 9 ", "page_idx": 11}, {"type": "text", "text": "[16] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. 2, 25, 27, 28 ", "page_idx": 11}, {"type": "text", "text": "[17] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: A universe of $10\\mathrm{m}{+3}\\mathrm{d}$ objects. arXiv preprint arXiv:2307.05663, 2023. 2 ", "page_idx": 11}, {"type": "text", "text": "[18] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model, 2024. 3 ", "page_idx": 11}, {"type": "text", "text": "[19] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Otmar Hilliges, and Andreas Geiger. Ag3d: Learning to generate 3d avatars from 2d image collections. arXiv preprint arXiv:2305.02312, 2023. 2 ", "page_idx": 11}, {"type": "text", "text": "[20] Manuel S Drehwald, Sagi Eppel, Jolina Li, Han Hao, and Alan Aspuru-Guzik. One-shot recognition of any material anywhere using contrastive learning with physics-based rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23524\u2013 23533, 2023. 2, 3 ", "page_idx": 11}, {"type": "text", "text": "[21] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:257364842. 3 ", "page_idx": 11}, {"type": "text", "text": "[22] Zexin He and Tengfei Wang. Openlrm: Open-source large reconstruction models. https: //github.com/3DTopia/OpenLRM, 2023. 2, 3 ", "page_idx": 11}, {"type": "text", "text": "[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https: //api.semanticscholar.org/CorpusID:247778764. 2, 3 ", "page_idx": 11}, {"type": "text", "text": "[24] Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu. 3dtopia: Large text-to-3d generation model with hybrid diffusion priors, 2024. 3   \n[25] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d, 2023. 2, 3   \n[26] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023. URL https: //api.semanticscholar.org/CorpusID:257219775. 3   \n[27] Krishna Murthy Jatavallabhula, Edward Smith, Jean-Francois Lafleche, Clement Fuji Tsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, Rev Lebaredian, and Sanja Fidler. Kaolin: A pytorch library for accelerating 3d deep learning research, 2019. 19   \n[28] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. 3   \n[29] James T Kajiya. The rendering equation. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques, pages 143\u2013150, 1986. 3   \n[30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. 3   \n[31] Peter Kocsis, Vincent Sitzmann, and Matthias Nie\u00dfner. Intrinsic image diffusion for indoor single-view material estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5198\u20135208, 2024. 25   \n[32] Matthias Labsch\u00fctz, Katharina Kr\u00f6sl, Mariebeth Aquino, Florian Grash\u00e4ftl, and Stephanie Kohl. Content creation for a 3d game with maya and unity 3d. Institute of Computer Graphics and Algorithms, Vienna University of Technology, 6:124, 2011. 2   \n[33] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity, 2023. 4, 6, 9   \n[34] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. https://arxiv.org/abs/2311.06214, 2023. 2, 8   \n[35] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model, 2023. 3, 9   \n[36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, 2022. URL https://api.semanticscholar.org/ CorpusID:246411402. 3   \n[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023. URL https://api.semanticscholar.org/CorpusID:256390509. 3   \n[38] Zeyu Li, Ruitong Gan, Chuanchen Luo, Yuxi Wang, Jiaheng Liu, Ziwei Zhu Man Zhang, Qing Li, Xucheng Yin, Zhaoxiang Zhang, and Junran Peng. Materialseg3d: Segmenting dense materials from 2d priors for 3d assets. arXiv preprint arXiv:2404.13923, 2024. 26 [39] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 300\u2013309, June 2023. 3 [40] Ivan Lopes, Fabio Pizzati, and Raoul de Charette. Material palette: Extraction of materials from a single image. arXiv preprint arXiv:2311.17060, 2023. 2, 3, 22 [41] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36, 2024. 26 [42] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022. 3 [43] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 3 [44] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751,   \n2022. 3 [45] OpenAI. Gpt-4v(ision) system card. OpenAI, 2023. URL https://api.semanticscholar. org/CorpusID:263218031. 3, 8, 10, 32, 33, 34 [46] R OpenAI. Gpt-4 technical report. arXiv, pages 2303\u201308774, 2023. 2, 3 [47] Keunhong Park, Konstantinos Rematas, Ali Farhadi, and Steven M Seitz. Photoshape: Photorealistic materials for large-scale shape collections. arXiv preprint arXiv:1809.09761, 2018.   \n22 [48] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using   \n2d diffusion. arXiv, 2022. 3 [49] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. Gpt4point: A unified framework for point-language understanding and generation. arXiv preprint arXiv:2312.02980, 2023. 3 [50] Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. Tailor3d: Customized 3d assets editing and generation with dual-side images. arXiv preprint arXiv:2407.06191, 2024. 2 [51] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023. 2, 18, 19 [52] Prafull Sharma, Julien Philip, Micha\u00ebl Gharbi, Bill Freeman, Fredo Durand, and Valentin Deschaintre. Materialistic: Selecting similar materials in images. ACM Transactions on Graphics (TOG), 42(4):1\u201314, 2023. 26 [53] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multiview diffusion for 3d generation. arXiv:2308.16512, 2023. 3, 8, 9 [54] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-clip: A clip model focusing on wherever you want, 2023. 3 [55] Zeyi Sun, Tong Wu, Pan Zhang, Yuhang Zang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Bootstrap3d: Improving 3d content creation with synthetic data. arXiv preprint arXiv:2406.00093, 2024. 2, 3, 8 [56] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. ArXiv, abs/2309.16653, 2023. URL https://api.semanticscholar.org/CorpusID:263131552. 3   \n[57] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024. 3   \n[58] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from a single image, 2024. 2, 3, 8, 9   \n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023. URL https://api.semanticscholar. org/CorpusID:257219404. 2, 3   \n[60] Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, and Simon Donn\u00e9. Collaborative control for geometry-conditioned pbr image generation. arXiv preprint arXiv:2402.05919, 2024. 3   \n[61] Giuseppe Vecchio and Valentin Deschaintre. Matsynth: A modern pbr materials dataset. arXiv preprint arXiv:2401.06056, 2024. 5, 35   \n[62] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation, 2023. 2   \n[63] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction, 2023. 2, 3   \n[64] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 3   \n[65] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality mesh, 2024. 3   \n[66] Felix Wimbauer, Shangzhe Wu, and Christian Rupprecht. De-rendering 3d objects in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18490\u201318499, 2022. 25   \n[67] Tong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xinggang Pan, Jiaqi Wang, Dahua Lin, and Ziwei Liu. Hyperdreamer: Hyper-realistic 3d content generation and editing from a single image, 2023. 3   \n[68] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v (ision) is a human-aligned evaluator for text-to-3d generation. arXiv preprint arXiv:2401.04092, 2024. 3, 8   \n[69] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2, 8, 9   \n[70] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. Matlaber: Material-aware text-to-3d via latent brdf auto-encoder. arXiv preprint arXiv:2308.09278, 2023. 2, 3   \n[71] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model, 2023. 3   \n[72] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, 2023. 2, 5   \n[73] Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Yixuan Li, Gordon Wetzstein, Ziwei Liu, and Dahua Lin. Layerpano3d: Layered 3d panorama for hyper-immersive scene generation. arXiv preprint arXiv:2408.13252, 2024. 2   \n[74] Haiyang Ying, Yixuan Yin, Jinzhi Zhang, Fan Wang, Tao Yu, Ruqi Huang, and Lu Fang. Omniseg3d: Omniversal 3d segmentation via hierarchical contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20612\u201320622, 2024. 4   \n[75] Kim Youwang, Tae-Hyun Oh, and Gerard Pons-Moll. Paint-it: Text-to-texture synthesis via deep convolutional texture map optimization and physically-based rendering. arXiv preprint arXiv:2312.11360, 2023. 2, 3   \n[76] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition, 2023. 3   \n[77] Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. Mapa: Text-driven photorealistic material painting for 3d shapes. In ACM SIGGRAPH 2024 Conference Papers, pages 1\u201312, 2024. 22 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Appendix Overview ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this appendix, we provide additional details and results that are not included in the main paper due to the space limit. The attached video includes intuitive and interesting qualitative results of Make-it-Real. ", "page_idx": 16}, {"type": "text", "text": "B Details of Make-it-Real ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we detail the pipeline that briefly outlined in the main paper. We commence by elaborating on the generation of SVBRDF Maps, incorporating illustrative figures to detail the process involving operations in computer graphics. Appendix B.1 is dedicated to explaining the acquisition of region-level texture map partition. Following that, in Appendix B.2 we discuss the method of pixel-level albedo-referenced estimation. Then, in Appendix B.3 we declare some details of rendering procedure. Appendix B.4 details the prompt design for material captioning and matching. Appendix B.5 reports the details of the GPT-4V based evaluation. ", "page_idx": 16}, {"type": "text", "text": "B.1 Texture Partition Module Design ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As illustrated in Figure 10, our process initiates with the rendering of a 3D object incorporating the original albedo (i.e. query albedo) from multiple perspectives. Following this rendering phase, we employ GPT-4V alongside a segmentor to derive segmented masks for the materials associated with each viewpoint. The subsequent step involves the extraction of regions masked in these images and their back-projection onto the mesh of the object. By examining the object from all acquired viewpoints and applying UV unwrapping techniques, we achieve preliminary segmentation of all materials. Subsequently, each material\u2019s segment is refined using a albedo-based mask refinement operation. Ultimately, by combining the segments of all materials, we obtain a region-level texture partition map, which serves to guide our subsequent, more detailed operations. ", "page_idx": 16}, {"type": "text", "text": "B.2 Albedo-referenced Module Design ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As illustrated in Figure 11, we developed a pixel-level albedo-referenced estimation module, building upon the foundations laid out in Appendix B.1. This module is inspired by a technique frequently employed by 3D artists, who often utilize albedo maps as a reference to generate images of other material properties. Accordingly, we designate the known albedo map as the query albedo, and the albedo corresponding to the region of interest in the material as the key albedo. ", "page_idx": 16}, {"type": "image", "img_path": "88rbNOtAez/tmp/bd7874deb3a704be4adca28cbc3708877aae1b0e178ca8670966fa7aaa31c571.jpg", "img_caption": ["Figure 10: Region-level texture partition module. This module extracts and back-projects localized rendered images on to a 3D mesh, using UV unwrapping for texture segmentation, thereby resulting in precise partition map of different materials. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "88rbNOtAez/tmp/9c226391620abf79c53392fcee0d0130dcc4f3495a0de9c7540c4aa68736765b.jpg", "img_caption": ["Figure 11: Pixel-level albedo-referenced estimation module. We generate spatially variant BRDF maps by referencing albedo maps, employing KD-Tree algorithm for efficient nearest neighbor searches, and normalizing colors via histogram equalization. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "The process to precisely obtain the final SVBRDF maps is divided into four steps: 1) Initially, to address potential gaps in color intensity between the two albedo maps, histogram equalization is employed to achieve a more uniform color distribution across the image. 2) Subsequently, for each pixel on the query albedo\u2014termed a reference pixel\u2014we seek the most similar neighboring color index on the key albedo. Given the high dimensionality of both maps (typically 1024x1024 pixels), a brute-force approach to this search would be computationally prohibitive. To this end, we accelerate the pixel query process using a KD-Tree algorithm, which organizes the RGB values of the key map\u2019s albedo into a KD-Tree for efficient nearest neighbor searches, reducing the computation time to under ten seconds. 3) The third step involves using the obtained indices to obtain corresponding values from the rest of the material maps. 4) Finally, by aggregating the query results for all material segments, we are able to generate the comprehensive spatially variant SVBRDF maps. ", "page_idx": 17}, {"type": "text", "text": "B.3 Rendering Procedure Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the context of computer graphics, \u201calbedo\u201d typically denotes the primary color of a material, a concept analogous to \u201cbase color\u201d and \u201cdiffuse\u201d in Physically-Based Rendering (PBR) paradigms, both representing the inherent color of the material under uniformly scattered illumination. ", "page_idx": 17}, {"type": "text", "text": "In accordance with workflow requirements, the inclusion of height maps, displacement maps, specular maps, and combinations of additional maps is optional. For objects demanding high surface details, such as the oil barrel in Figure 5 and the stone horse in Figure 16, displacement or height maps are incorporated and rendered using the mature 3D computer graphics software Blender engine. Regarding 2D-3D alignment techniques, including rasterization rendering, back-projection, and UV unwrapping detailed in Section 3.2.1, we adhere to the methodologies outlined in [51]. Specifically, ", "page_idx": 17}, {"type": "image", "img_path": "88rbNOtAez/tmp/74d50a8eff51bd4b3294ddd32ee6bed0ba1802149294ce0ec6791440a102b59d.jpg", "img_caption": ["You are showed with a group of spherical PBR materials(all made of tiles, which is a kind ofCeramic, in total 5), can you generate a caption for each(about 20-30 words), try to distinguishtheir difference. Only describe the appearance features (must including color\" and detailedmaterial'(such as patterns, roughness, metalness, concave and convex patterns, condition))and don't give too much other information. Do not describe whether it is reflective or not. Do notdescribe the shape of overall object. Please use a dictionary to represent the output result {1:Description 1, 2: Description 2, 3: Description 3, 4: Description 4, 5: Description 5) "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "{1: \"A gradient of pastel purple to blue tiles with a smooth, matte ceramic finish and slight bevel edges.\",   \n2: \"Soft teal ceramic tiles with a satin finish, arranged in a fish scale pattern with a slightly raised   \ntexture.\".   \n3: \"Creamy beige ceramic tiles with a matte finish, laid in a staggered brick pattern with flatsurface and sharp edges.\",   \n4: \"Pure white ceramic tiles with a glossy finish, aligned in a straight grid pattern with a slightconvex curvature.\",   \n5: \"Bright white ceramic tiles with a matte finish, arranged in an offset pattern with pronounced grout lines and a smooth texture.} ", "page_idx": 18}, {"type": "text", "text": "Figure 12: Material captioning. We demonstrate how to construct detailed material descriptions, registering material information in textual form, thus providing a convenient bridge when querying with multi-modal large language models. ", "page_idx": 18}, {"type": "text", "text": "we utilize the Kaolin package (Kaolin [27]) and back-projection with masks [51], with the model learning rate set to 0.02 in Equation (3) and the difference coefficient $\\sigma$ set to 0.1 in Equation (4). ", "page_idx": 18}, {"type": "text", "text": "This approach facilitates the systematic generation of various material texture maps in the presence of only an albedo map, thereby ensuring consistency and realism within the Physically Based Rendering (PBR) workflow. Moreover, the flexible application of different map types and 2D-3D alignment techniques significantly enhances the detail and realism of rendered objects, effectively meeting the demands of diverse rendering scenarios. ", "page_idx": 18}, {"type": "text", "text": "We evaluate the basic performance of the model by testing on a subset of 70 objects. Specifically, we measure the average runtime, the number of material parts per object, the number of GPT-4 queries per object, and GPU memory usage, as shown in Table 2. Additionally, we provide a detailed time consumption analysis for each step, presented in Table 3. ", "page_idx": 18}, {"type": "table", "img_path": "88rbNOtAez/tmp/017658af2a7782bd85ab7b1296687fcfa754f36bba8276e5df308b253b8b8255.jpg", "table_caption": ["Table 2: Model performance metrics. The table reports the model\u2019s runtime, average number of material parts per object, queries per object, and GPU memory usage. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.4 Prompt Details for Multi-modal Large Language Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Prompts design has emerged as a pivotal factor for eliciting desired outcomes from MLLMs. This section delineates the intricacies of our prompt design for both material captioning and matching. ", "page_idx": 18}, {"type": "image", "img_path": "88rbNOtAez/tmp/2c6b6d14b0a51ec9d0251337f8add2487fb05f44bcb1d7a8ac5d4a32c8062c7c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This is a(or a group of) 3D rendered image(s) of an object using a pure texture map without $\\bar{\\mathsf{P}}\\mathsf{B R}$ material information. Please use its visual characteristics (including color, pattern, object type) along with your existing knowledge to INFER and IMAGINE the possible materials of different parts. ", "page_idx": 19}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "...By choosing a yellow metal for the continents and a matte white plastic for the ocean areas, the globe maintains a clean, minimalist aesthetic while the contrasting materials bring out the details and contours of the Earth's surface. The combination of the shiny and matte finishes would make the globe an attractive decorative item, adding a touch of modern sophistication to its setting. ", "page_idx": 19}, {"type": "text", "text": "", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Identify the material of each part(marked with Arabic numerals), presented in the exact form of {number_id: \"material\"}. Don't output other information. (optional list of material is [Ceramic, Concrete, Fabric, Ground, Leather, Marble, Metal, Plaster, Plastic, Stone, Terracotta, Wood, Misc], The 'Misc' category is output when nothing else matches.) ", "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/4c97a3fe78726fef4444fefae98d1c9a571c88ccbc74b01e77ff2690f111f616.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "{1: 'Metal', 2: 'Plastic'} ", "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/a4187041988dcbd9293d24d58397ceeab173338f93bc208dba070d6b16e74f3e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Select the most similar Metal material type of number 1 part of the image, according to the analysis of corresponding part material(including color, pattern, roughness, age and so on...). If you find it difficult to subdivide, just output {}. Don't output other information. Only a single word representing the category from optional list needs to be output. (optional list of material is {Chainmail, Chip, ChristmasTreeOrnament, CorrugatedSteel, DiamondPlate, Foil, Metal, MetalPlates, MetalWalkway, PaintedMetal, Rust, SheetMetal, SolarPanel}) ", "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/6414710bed3d4f893e87a0360fbb1742e50883a90dbc72c13d1244463158d5ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Metal ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/d5f684ab42ba8cdbcff8beb2cfae3bfd9b56530a834f2ab1af105fdd35206cf9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Look at the material carefully of number 1 part of the image, here are some descriptions about metal materials, can you tell me which two are the best description match the part 1 in the image? Just tell me the final result in dict format with material name and descrption. Don't output other information. ", "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/4839c68329b827a3f4a6e6380b9842899a00e88518151dcf2bc7594c97a8c427.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "\"Metal_Metal001\": \"A silver-colored metal with a high-gloss finish and visible smudges and scratches.\", \"Metal_Metal002\": \"A slightly tarnished silver metal with marks and a few rust spots indicating weathering.\", \"Metal_Metal003\": \"A clean silver metal surface with minimal weathering and a few small scratches and dents.\" ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": ".. material library look up... ", "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/932dcd339edb423c412db93f6ea0bdd57977d70a5e0b5a2318f4cbc18a9d9f30.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "{'Metal_Metal007': 'A golden color material with a polished surface showing some tarnish and subtle wear marks.',   \n'Metal_Metal035': 'A burnished copper color with a weathered patina and faint swirling material striations.'} ", "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/940d5ed539bf2a706ac6b700393e863affee0b558e58f6e7f24e84898dbef555.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 13: Detailed prompts of GPT-4V based material matching. Prompts in blue changes according to the current assigning part and GPT-4V\u2019s results. ", "page_idx": 19}, {"type": "image", "img_path": "88rbNOtAez/tmp/6877db17200e64e932a87e61b5a6f695fe084d9fa7f22911cc95d5142b42f745.jpg", "img_caption": ["Assuming you are an expert in determining whether the textures of 3D models meet Photorealistic standards (mainly focus on the quality of texture), we have generated some 3D objects using two different methods. These objects have the same shape but different textures. Above are the four-view images of a model generated by the two methods (the upper half is the four views of the object generated by method one, and the lower half is the four views of the object generated by method two). Please help me evaluate which of the two objects is more Photorealistic in terms of texture. Respond with 1 if you think the upper half object is more realistic. Respond with 2 if you think the lower half object is more realistic. Also tell me the reason in a new paragraph. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Based on the provided images, my evaluation would favor object 2 (the lower half object) as being more photorealistic in terms of texture. ", "page_idx": 20}, {"type": "text", "text": "The textures on object 2 have a more complex and nuanced appearance. The material of the uniform exhibits subtle variations in reflectivity and surface detail that suggest a more realistic fabric texture.  Additionally, the skin texture appears more lifelike with variations in tone and reflectivity that are characteristic of human skin. The metal police badges on object 2 has a more nuanced reflection, which suggests a more realistic interaction with environmental light. In contrast, the textures on object 1, while still detailed, lack the same level of subtlety and variation. The uniform looks somewhat flat and less fabric-like, and the overall surface details are less convincing as natural variations. ", "page_idx": 20}, {"type": "text", "text": "Figure 14: GPT-4V based evaluation prompts. We define a prompt for GPT-4V to generate humanaligned comparison over objects before and after our texture refinement. ", "page_idx": 20}, {"type": "text", "text": "Material captioning. Due to limit context length for image tokens, it is currently impossible for MLLMs to directly memories thousands of images of material balls. To address this, we use GPT4V to generate detailed captions for each material ball. As demonstrated in Figure 12, we input concatenated images of material ball from same subcategory with prompt specifically tailored to highlight texture properties. This strategy guides GPT-4V to generate detailed caption for each material ball, distinguishing the subtle differences between them. These detailed captions are then registered into material library. ", "page_idx": 20}, {"type": "text", "text": "Material matching. The MLLM-based material query process is exemplified through a simplified case, as illustrated in Figure 13. Initially, GPT-4V is queried to identify the basic material for matching. Following this preliminary matching, GPT-4V is queried for more specific materials using the names of subcategories as fliters, which narrows down the selection to a few candidates within the same subcategory. For final material selection, GPT-4V is prompted with detailed captions from the library, directing it to allocate the most suitable material. This is accomplished in conjunction with a meticulously engineered segmentation process in UV space, ultimately facilitating MLLM-based material matching. It is noteworthy that prompting GPT-4V to navigate through a three-level tree structure has been proven to enhance both the efficiency and accuracy of the matching process, as opposed to directly selecting from thousands of materials directly. ", "page_idx": 20}, {"type": "text", "text": "B.5 Evaluation Details of Quantitative Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Evaluation details. For the purpose of evaluating objects pre- and post-refinement, we render four view images of each object and concatenate them vertically to facilitate a comprehensive assessment. We craft a specific prompt to guide GPT-4V in conducting an impartial comparison of the texture quality between two objects. As demonstrated in Figure 14. GPT-4V\u2019s advanced capabilities enable it to differentiate between the two objects, providing comparison results that closely align with assessments made by human experts. In the scenario evaluated, GPT-4V successfully identifies all enhancements (metal badges, human skin, and fabric textures). ", "page_idx": 20}, {"type": "image", "img_path": "88rbNOtAez/tmp/5c4f942f99297c43624a65ae9f6e8abacbca523697673aeb5bec7d51315ca58c.jpg", "img_caption": ["Figure 15: Comparison between previous method and Make-it-Real. We demonstrate the distinctions between Material Palette[40] and our method in terms of material identification and extraction. Our overall pipeline presents a more challenging task, where the input is a rendered image with only albedo information, and the output consists of textures for the entire object. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "For human evaluators, we engage 16 volunteers with strong backgrounds in computer science, specifically in 3D modeling and generation. To ensure unbiased evaluations, we maintain gender balance through random selection. The assessment focuses on determining object material photorealism, as shown in Figure 14. Volunteers train on 40 Objaverse cases, with and without materials, to enhance their realism judgment for accurate test data evaluation. ", "page_idx": 21}, {"type": "text", "text": "C Additional Related Works ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the context of extracting real-world materials from single images, the methodologies most closely related to our work are Material Palette [40] and Photoshape [47]. Material Palette enables the extraction of materials at the region level from a single image, generating tilable texture maps for corresponding areas. Photoshape, on the other hand, automates the assignment of real materials to different parts of a 3D shape by training a material classification network. However, this approach needs the training of a material classifier, which is constrained to a limited set of object categories (e.g., chairs) and material types. Besides, both methods require rendered images of objects with real materials as input. ", "page_idx": 21}, {"type": "text", "text": "Our problem setting poses a more challenging task, as illustrated in Figure 15, which involves identifying and recovering the original material properties from images of objects that only have a albedo map, in addition to generating different material maps for the entire object. Humans have the capacity to intuitively infer the underlying material properties of 3D objects from single images containing only shape and basic colors. This capability is attributed to our robust material recognition abilities and comprehensive prior knowledge of object categories, colors, and material attributes. Building on this concept, our approach harnesses the potent image recognition capabilities and prior knowledge inherent in large-scale multimodal language models to efficiently execute region-level material extraction and identification, which is further enhanced by subsequent 2D-3D alignment and albedo-referenced techniques to generate and apply material texture maps for physically realistic rendering of 3D objects. Furthermore, [40] extracts the material takes 3 to 4 minutes, and process an image containing three materials takes more than 10 minutes. In contrast, our method takes only about 1 to 2 minutes to match and generate all the materials. ", "page_idx": 21}, {"type": "text", "text": "In more recent work, both MaPa [77] and our paper aim to enhance 3D objects with materials, but the key difference lies in material retrieval methods. MaPa [77] relies on CLIP, while we use MLLM, a distinction that we believe significantly boosts retrieval effectiveness for these reasons: 1) Enhanced Global Context: CLIP focuses on one area at a time, requiring other parts to be masked, which limits global semantic information. MLLM, however, retains this context by simply highlighting the relevant area. 2) Hierarchical Material Dictionary: We introduce a hierarchical dictionary in MLLM, providing descriptions and images of material spheres, enabling access to a more extensive library of 1,394 materials compared to MaPa\u2019s 118. 3) Advanced Explainability: With MLLM\u2019s rapid development, transitioning from CLIP to a multi-step hierarchical inference process with LLM offers improved performance and clearer decision-making. ", "page_idx": 21}, {"type": "image", "img_path": "88rbNOtAez/tmp/1893338273663c3c4ffb113fd3cc093308db1d2f2d4a20555069702f53669af5.jpg", "img_caption": ["Figure 16: Effects of different texture maps. We evaluate the effects of metalness, roughness, and displacement/height maps on the appearance of 3D objects. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Effects of Different Texture Maps ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We validate the impact of various texture maps generated by Make-it-Real on the appearance of 3D objects, as illustrated in Figure 16. For each example, the first column lacks the corresponding map enhanced by Make-it-Real, while the second column includes the same conditions with the specific material map. Initially, we compare the effect of metalness on object appearance. We observe that the objects on the right, with a higher metalness map value, exhibit higher reflectivity, and the reflected light\u2019s color is similar to the material\u2019s own color, closely resembling real-world objects and appearing more aesthetically pleasing. In contrast, the objects on the left without the metalness map have lower reflectivity, and the reflected light tends to be white. ", "page_idx": 22}, {"type": "text", "text": "Next, we examine the impact of the roughness map, which controls the smoothness of the material\u2019s surface. We observe that the silver chess piece on the right, with a low roughness texture map, becomes smooth, and together with metalness, produces a mirror-like reflection effect. On the other hand, the box on the left with a roughness map exhibits changes in light dispersion on its surface, with some areas showing highlights, while also adding and enriching scratch texture details on the surface. ", "page_idx": 22}, {"type": "text", "text": "Furthermore, we compare the effects of displacement and height on objects, both of which are usually optional and can also impact object appearance. Height maps typically use grayscale values to represent the surface\u2019s relative height, simulating a relief effect through changes in lighting and shadows. As shown in the last row on the right, the object with a height map has an uneven surface, enhancing the sense of depth. Displacement mapping is a more powerful technique that changes the vertex positions of the geometry based on the map\u2019s values, creating realistic relief effects. As shown on the left, the stone sculpture with displacement mapping exhibits very realistic details and a sense of relief. We acknowledge that the generated PBR maps are not true representations of physical maps, but our algorithm generates PBR maps that closely approximate the visual characteristics of true PBR properties, which significantly enhances the realism of 3D objects. ", "page_idx": 22}, {"type": "image", "img_path": "88rbNOtAez/tmp/58ca01fa8b7f56983a5d1693807df00bc68c1b3bd46a97ff9da4be28ab8b05df.jpg", "img_caption": ["Figure 17: Effects of different UV mappings of input mesh. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "88rbNOtAez/tmp/c3c5b387abb0db7a543ebddb918efed44fef4de4ca3486d5ad4f1a1c57ee7a93.jpg", "img_caption": ["Figure 18: More Comparisons between Make-it-Real and Artist-Created Materials. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "D.2 Effects of Different UV Mappings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We conduct experiments to evaluate the impact of UV mapping on our method. As shown in the second column of Figure 17, UV mappings with excessive fragmentation and color entanglement can cause the 2D segmented images to mix with other regions when reprojected into the UV space, leading to material blending issues. However, our method shows good results with original artist-created UV mappings and Blender\u2019s built-in mapping methods in Figure 17, such as smart, sphere, and unwrap. This indicates that our method still demonstrates good robustness with many mapping techniques. ", "page_idx": 23}, {"type": "text", "text": "In practice, we observe that most objects in Objaverse have UV mappings with good properties, meaning they are not excessively fragmented into small pieces. Additionally, we can control the UV mapping process: For some low-quality UV maps and generative objects that originally lack UV maps, we can re-unwrap them to achieve higher quality. ", "page_idx": 23}, {"type": "image", "img_path": "88rbNOtAez/tmp/b8bc591f88a490a31f6f5422778df779cc07e48402f7a07ab50b8360e95841b2.jpg", "img_caption": ["Figure 19: Addressing Shading Issues with IntrinsicAnything [9]. The first row shows that the derendered albedo images from Objaverse are consistent with the original one. The last two rows demonstrate the successful derendering results for generative models, effectively reducing shading. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "E More Qualitive Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Make-it-Real for Existing 3D Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we show more qualitative results of Make-it-Real for existing 3D assets from [16]. We show more qualitative results of Make-it-Real for existing 3D assets from [16]. Results are shown in Figures 20 and 21. The first row presents the original 3D object with only a albedo map, while the second row showcases the object enhanced by our method with various material maps. ", "page_idx": 24}, {"type": "text", "text": "Comparisions with Artist-Created Materials. We conduct a user study comparing materials generated by our Make-it-Real with those created by artists, visualizing some results in Figure 18 for close examination. For objects like pistol, wooden chair, speakers, and saxophone, Make-it-Real showed strong similarity to artist-made materials, maintaining consistent metallicity, roughness, highlights, and coloration. In cases such as shield, boots, landline phone, and oil drum, our method produced materials that, while different, are realistic and sometimes visually superior. Our pipeline approaches the level of refinement seen in some artist-created materials, which often require significant time and effort, and clearly outperforms more basic and crude artist-generated materials. This highlights its effectiveness and potential for practical applications. ", "page_idx": 24}, {"type": "text", "text": "E.2 Visualization of generated texture maps. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we show visualization of generated texture maps in Figures 22 and 23. Our approach, guided by the query albedo reference, produces material maps with distinct material partitions and maintains a distribution consistent with the albedo map. As a result, the enhanced object exhibits both realism and texture consistency. ", "page_idx": 24}, {"type": "text", "text": "F Limitations and future work ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "While promising results have been achieved by MLLMs for texture assignment through Make-it-Real.   \nOur work still faces several challenges. ", "page_idx": 24}, {"type": "text", "text": "Lighting Effect and Addressed Solutions. First, although our method can achieve appropriate texture assignment for albedo-only model, it does not support reverse transform from shaded texture map to albedo map for generated 3D objects. This causes problem of assigning different materials to the dark shadow and highlight area when generated object with mesh already shaded in different lighting conditions. ", "page_idx": 24}, {"type": "text", "text": "This issue is less pronounced in artist-created models, where albedo maps have less shadows and lighting effects. Our tests show that the albedo obtained from inverse rendering methods [9] closely matches the original artist-created albedo maps from the Objaverse dataset, as illustrated in the first row of Figure 19. For generative models, baked-in shading effects are more pronounced. Our method addresses this by integrating mature derendering algorithms [9, 14, 31, 66], specifically the Intrinsic Anything [9], into our pipeline with minimal complexity. This integration derenders from four viewpoints and back-projects to obtain a better albedo map, then applies our Make-it-Real method for material painting. As shown in the last row, this approach reduces lighting noise and supports a wider range of inputs. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Impact of Model Quality and Segmentation. Second, we find base model quality is essential for MLLM to assign correct materials. When base 3D object is of low quality(e.g., uneven surfaces or mixed colors across different parts), it is difficult for MLLMs to identify object properties when ground truth text prompt describing the object is not available. Additionally, there has been limited progress in material segmentation in 2D and 3D demoain [52, 38]. The material segmentation algorithm included in our proposed pipeline can serve as a useful baseline and provide inspiration for future work. We also explore the integration of the pipeline with 3D segmentation networks, which shows promising results. For future work, we consider methods mitigating these challenges as well as adding user friendly control into our fully-automatic pipeline. ", "page_idx": 25}, {"type": "text", "text": "G Data Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The datasets utilized in our work, specifically Objaverse, is composed entirely of inanimate objects. Importantly, the Objaverse dataset we used have undergone rigorous ethical filtering as part of the Cap3D [41]. Cap3D\u2019s ethical filtering process removed all potentially problematic data, including any identifiable human elements and NSFW content (detailed in $\\sec3.2$ of Cap3D). This process included the removal of objects with licenses that do not permit commercial usage, the exclusion of objects lacking sufficient camera information for rendering, and the application of face detection and NSFW classifiers with high thresholds to ensure thorough flitering. The final dataset used in our work, therefore, does not contain any human-derived data or data related to human subjects. ", "page_idx": 25}, {"type": "text", "text": "We recognize the potential risk of misuse associated with this technology, particularly in the creation of realistic fake human representations. However, since our dataset fully excludes human data, this risk is substantially mitigated. ", "page_idx": 25}, {"type": "text", "text": "H Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Potential positive societal impacts. The proposed method facilitates more realistic and accurate representations of materials in 3D models, benefiting industries such as gaming, virtual reality, and film, leading to more immersive and engaging experiences. By automating the material assignment process, Make-it-Real significantly reduces the time and effort required for 3D content creators, allowing for more efficient workflows and enabling creators to focus on more creative aspects of their work. This approach can democratize high-quality 3D content creation by making advanced material application techniques accessible to a broader range of users, including those without specialized skills in graphic software. ", "page_idx": 25}, {"type": "text", "text": "Potential negative societal impacts. The improved realism in 3D assets could be exploited for creating highly convincing fake visuals or deepfakes, which might be used in disinformation campaigns or to mislead audiences. There is a risk that the materials generated could inadvertently perpetuate biases or stereotypes if the training data for GPT-4V includes biased representations of certain materials or objects. As the method involves processing and recognizing visual data, there could be concerns regarding the privacy of any real-world images used as inputs, particularly if they contain sensitive or personal information. ", "page_idx": 25}, {"type": "image", "img_path": "88rbNOtAez/tmp/c90f8d824fc12fe48bcc66c08c1e43b2eb5a98f79d13dfa7954289014d55c10c.jpg", "img_caption": ["Figure 20: More qualitative results of Make-it-Real refining existing 3D assets without material. Objects are selected from Objaverse[16] with albedo only. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "88rbNOtAez/tmp/1507234f12a862e53a23e91cc32fc83cc12fa7689cec8c36aa3513e9a59ca2e7.jpg", "img_caption": ["Figure 21: More qualitative results of Make-it-Real refining existing 3D assets without material. Objects are selected from Objaverse[16] with albedo only. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "88rbNOtAez/tmp/a3d2c0f5b0103d08653cf8e6679e1c24b4298b694168e42cf64263816eddba50.jpg", "img_caption": ["Figure 22: Visualization of generated texture maps. The first column represents the original query albedo map of 3D objects, while the subsequent columns showcase the corresponding material maps generated by Make-it-Real. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "88rbNOtAez/tmp/0cbe043da59b5e3df76b1a92857f1bc7ada23be3b0c1acb3225562e877a8cf64.jpg", "img_caption": ["Figure 23: Visualization of generated texture maps. We show part of the SVBRDF material maps generated by Make-it-Real and the final rendering results. We displayed the texture maps and the corresponding 3D rendering effects. The albedo is the input, and the following four columns show the material effects in the UV space and on the 3D object. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All claims of the improvements are supported with extensive quantitative experiments with a great amount of qualitative cases. The proposed method is tested on many objects both sourced from Objaverse and generated by many edge-cutting 3D generative models and proven to be effective and robust. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed discussion of limitations in Sec. F ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: For main results, we detail the full test settings in Sec. 4. For GPT-4V [45] based preference study, we provide detailed test prompts and test settings in Sup. B.5. Readers can easily follow the same setting and reproduce all of our experiment results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Full experiment details is available in Sec. 4 ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: The full test with is time consuming mainly due to the rate limit of GPT4V [45] (each conversation takes around 1 minute). We improve the experiment confidence by increasing the amount of test to hundreds of objects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper mainly reliance on API access to GPT-4V [45] and Blender. The segmentation model can be deployed on any computing resources with most mainstream GPUs with CUDA and $\\geq8\\mathrm{G}$ memory. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The authors have read the NeurIPS Code of Ethics carefully and find no conflicts with it. Potential societal impacts are answered in the next question. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Detailed discussions available in Sup. H ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Existing assets we use is a quality library of selectable real materials\u2014we build upon the MatSynth [61] dataset, which contains over 4,000 high resolution tileable materials under CC0 license. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]