[{"figure_path": "88rbNOtAez/tables/tables_8_1.jpg", "caption": "Table 1: GPT evaluation and user preference. GPT's and user's preference comparison on Make-it-Real refined objects sourced from existing 3D assets and state-of-the-art 3D generation methods.", "description": "This table presents a quantitative comparison of the results obtained using the Make-it-Real method against baseline results.  The evaluation is performed from two perspectives: GPT evaluation and user preference. For each category (3D assets, Image-to-3D, and Text-to-3D), the table shows the percentage of successful refinement for both the baseline and Make-it-Real methods, as assessed by both GPT-4V and human users. This allows for a direct comparison of the effectiveness of the Make-it-Real method across different object types and generation techniques.", "section": "4 Experiments"}, {"figure_path": "88rbNOtAez/tables/tables_18_1.jpg", "caption": "Table 2: Model performance metrics. The table reports the model's runtime, average number of material parts per object, queries per object, and GPU memory usage.", "description": "This table presents the performance metrics of the Make-it-Real model.  It shows the average runtime required to process a single 3D object, the average number of distinct material parts identified within each object, the average number of queries made to the GPT-4V model during material retrieval, and the average GPU memory consumption during model execution.", "section": "B.3 Rendering Procedure Details"}]