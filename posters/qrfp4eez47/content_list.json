[{"type": "text", "text": "FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jitesh Joshi1, Sos S. Agaian2, and Youngjun Cho1 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computer Science, University College London, UK 2Department of Computer Science, College of Staten Island, City University of New York, USA {jitesh.joshi.20, youngjun.cho}@ucl.ac.uk, sos.agaian@csi.cuny.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Remote photoplethysmography (rPPG) enables non-invasive extraction of blood volume pulse signals through imaging, transforming spatial-temporal data into time series signals. Advances in end-to-end rPPG approaches have focused on this transformation where attention mechanisms are crucial for feature extraction. However, existing methods compute attention disjointly across spatial, temporal, and channel dimensions. Here, we propose the Factorized Self-Attention Module (FSAM), which jointly computes multidimensional attention from voxel embeddings using nonnegative matrix factorization. To demonstrate FSAM\u2019s effectiveness, we developed FactorizePhys, an end-to-end 3D-CNN architecture for estimating blood volume pulse signals from raw video frames. Our approach adeptly factorizes voxel embeddings to achieve comprehensive spatial, temporal, and channel attention, enhancing performance of generic signal extraction tasks. Furthermore, we deploy FSAM within an existing 2D-CNN-based rPPG architecture to illustrate its versatility. FSAM and FactorizePhys are thoroughly evaluated against state-of-theart rPPG methods, each representing different types of architecture and attention mechanism. We perform ablation studies to investigate the architectural decisions and hyperparameters of FSAM. Experiments on four publicly available datasets and intuitive visualization of learned spatial-temporal features substantiate the effectiveness of FSAM and enhanced cross-dataset generalization in estimating rPPG signals, suggesting its broader potential as a multidimensional attention mechanism. The code is accessible at https://github.com/PhysiologicAILab/FactorizePhys. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Attention mechanisms in computer vision are inspired by the human ability to identify salient regions in complex scenes. Such mechanisms can be interpreted as a dynamic weight adjustment process that selects useful features and disregards irrelevant ones in a multidimensional feature space. Recent surveys [20, 23] provide a comprehensive overview of attention mechanisms and distinctly categorize existing attention mechanisms. Amidst a spectrum of research from convolution block attention [66] to computationally intensive multi-head attention [58], an effective, yet computation and memory efficient, attention mechanism has remained desirable for real-world applications. Matrix decomposition [12, 19, 31], a dimensionality reduction technique, has captured the interest of researchers and has been explored in deep learning research for different objectives [57, 60, 17, 18]. This work investigates nonnegative matrix factorization (NMF), a matrix decomposition technique, for its potential to efficiently perform multidimensional attention and evaluates its effectiveness in the spatial-temporal context of estimating rPPG signal from video frames. ", "page_idx": 0}, {"type": "text", "text": "Verkruysse [59]\u2019s pioneering investigation on extracting photoplethysmography (PPG) or blood volume pulse (BVP) signals from RGB cameras in a contactless manner led to an exciting research field of imaging-based physiological sensing. There exist several potential applications and contexts of noninvasive and contactless measurement techniques, such as stress and mental workload recognition [8, 9, 7], driver drowsiness monitoring [71] and social biofeedback interaction [43]. The seminal works on unsupervised rPPG methods [59, 48, 11] either used video frames acquired under stationary conditions or performed skin segmentation [62] or region of interest (RoI) tracking as a preprocessing step. This preprocessing step can be considered as a basic form of attention mechanism that enables the unsupervised models to process only the relevant regions. Some of the supervised rPPG methods, including HR-CNN [54], RhythmNet [45], NAS-HR [40], PulseGAN [51], and Dual-GAN [41] also relied on extracting spatial-temporal features from the tracked RoIs as a preprocessing step. ", "page_idx": 1}, {"type": "text", "text": "As end-to-end rPPG methods, such as DeepPhys [6], and MTTS-CAN [36] among several others, take whole facial frames as input, they rely on attention mechanisms that enable models to emphasize the relevant spatial-temporal features. Estimating BVP signal from raw facial video frames in an end-to-end manner is therefore an interesting downstream task to investigate the attention mechanism in multidimensional feature space. This requires networks to learn to pick the spatial features having the desired temporal signature, while discarding the variance related to head-motion, illumination, and skin-tones, thus representing one of the challenging spatial-temporal tasks. Few other notable end-to-end rPPG methods include PhysNet [83], 3DCNN [4], SAM-rPPGNet [26], RTrPPG [3], and transformer-network-based methods such as PhysFormer [77], PhysFormer+ $^{\\cdot+}$ [76], EfficientPhys [37], JAMSNet [79], and GLISNet [80]. A recent survey article on visual contactless physiological monitoring in clinical settings [27] highlights susceptibility to disturbance, such as head movement, as one of the key challenges, among others. Some of the recent end-to-end rPPG methods [79, 80] further highlight the need for multidimensional attention, as squeezing features in selective dimensions for deriving attention reduces the feature space to a single dimension and is therefore not well suited for the task of signal extraction. ", "page_idx": 1}, {"type": "text", "text": "To address this, our work draws inspiration from the seminal work on NMF [31] which a recent work formulated as an approach to design the global information block, referred to as Hamburger [18]. Hamburger [18] implements NMF to derive low-rank embeddings, which serve as a global context block. Despite the low computational complexity of $O(n)$ , Hamburger [18] outperformed various attention modules in the semantic segmentation [68] and image generation tasks. In addition, researchers have combined matrix factorization with deep architectures in several ways for different applications such as layer-wise learning of dictionary for classification and clustering [57], adaptive learning of dictionary for image denoising [81], multi-attention model for recommendation systems [60], and linearly scalable approach to context modeling for medical image segmentation [1] among several others. Drawing inspiration from these studies, especially those that use matrix factorization to model global context [18, 1] in vision tasks, we investigate the application of NMF as a multidimensional attention block. Although matrix factorization in deep learning has remained a topic of significant interest, it has not been investigated in the realm of rPPG, which stands to gain from joint spatial, temporal, and channel attention. ", "page_idx": 1}, {"type": "text", "text": "We introduce the Factorized Self-Attention Module (FSAM), which implements NMF to jointly compute spatial-temporal attention and describe an appropriate formulation for the low-rank recovery problem. To investigate the relevance and effectiveness of FSAM in computing multidimensional attention, we build a 3D-CNN architecture FactorizePhys that implements FSAM. We further adapt FSAM for EfficientPhys [37], an end-to-end rPPG architecture that builds on the Temporal Shift Module (TSM) [34], to uniquely learn spatial-temporal features using 2D-CNN layers. Evaluation of FactorizePhys and EfficientPhys [37] with FSAM, against existing SOTA rPPG methods, demonstrates the versatility of FSAM as multidimensional attention along with its effectiveness for the downstream task of estimating time series from spatial-temporal data. In summary, we make the following contributions. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Factorized Self-Attention Module (FSAM): NMF [31]-based novel approach that jointly computes multidimensional attention within voxel embeddings.   \n\u2022 FactorizePhys: an end-to-end 3D-CNN architecture that integrates FSAM for robust estimation of rPPG from spatial-temporal facial video frames.   \n\u2022 Thorough assessment of FactorizePhys and FSAM with multiple evaluation metrics and the corresponding measure of standard errors to compare cross-dataset generalization performance with SOTA rPPG methods, using four benchmarking rPPG datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Attention Mechanisms in Vision ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Varied forms of attention mechanisms have been successful in different visual tasks such as image classification [70, 25, 66], object detection [5, 82], semantic segmentation [78, 16, 18, 28], video understanding [63, 15, 33, 21], 3D vision [69, 24], and multimodal tasks [73, 56] among others [20, 23]. The most widely used attention mechanisms are channel attention [35, 75], spatial attention [66, 63], temporal attention [72, 74], self-attention or transformer-based approaches [58, 14], multimodal attention [56, 73], graph-based approaches [32], as well as different combinations of these types [66, 16, 53]. In addition, researchers have proposed attention mechanisms for video understanding [63, 15, 33, 21] as well as 3D vision [69, 24]. Despite notable advances in different forms of attention mechanisms, some of the existing challenges include the requirement for high computational costs, large training data, the overall efficiency of the model, and a cost-benefit analysis of performance improvement [23]. Additionally, for rPPG research, the impact of attention mechanisms on an ability of models to generalize on unseen datasets is not systematically studied, which we address in this work. ", "page_idx": 2}, {"type": "text", "text": "2.2 Attention Mechanisms in rPPG Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "End-to-end rPPG methods can be categorized into convolution neural networks (CNN) architectures such as PhysNet [83], EfficientPhys-C [37], 3DCNN [4], SAM-rPPGNet [26], and RTrPPG [3], and transformer-network-based architectures such as PhysFormer [77], PhysFormer+ $^+$ [76], and EfficientPhys-T [37]. Among end-to-end rPPG methods, DeepPhys [6] first implemented a novel convolutional attention mechanism in an architecture that comprised separate motion and appearance branches, with the latter intended to compute attention for the main motion branch. Inspired by the CBAM attention mechanism [66], originally validated for classification and detection tasks, STAttention [46] was proposed to fliter salient information from spatial-temporal maps, thus improving remote HR estimation. The similar dual attention mechanism was also found to be effective in the SMP-Net framework [13], which jointly learned the features of RGB and infrared spatial-temporal maps to estimate multiple physiological signals. ", "page_idx": 2}, {"type": "text", "text": "Recently, EfficientPhys [37], an end-to-end network, presented an efficient single-branch approach with a gated attention mechanism. The Swin-Transformer [39] based version of EfficientPhys [37] insightfully added the TSM [34] module to the Swin transformer [39], enabling the architecture to perform efficient spatial-temporal modeling and compute attention by combining shifting window partitions spatially and shifting frames temporally. It should be noted that the convolution-based version of EfficientPhys [37], which combined the TSM [34] module and a convolutional attention mechanism [6] showed superior accuracy along with significantly low latency, making it highly suitable for deployment on mobile devices. Recently, there has been an upsurge in transformer-based rPPG architectures, some of which include PhysFormer [77], PhysFormer+ $^{\\cdot+}$ [76], TransPhys [61], and RADIANT [22]. ", "page_idx": 2}, {"type": "text", "text": "Unlike other transformer-based architectures that rely on spatial-temporal maps as input, PhysFormer [77] and PhysFormer+ $^{\\cdot+}$ [76] are end-to-end video transformer-based architectures, which adaptively aggregate both local and global spatial-temporal features. PhysFormer $^{++}$ [76] extends PhysFormer [77] by better exploiting temporal contextual and periodic rPPG clues, as it extracts and fuses attentional features from slow and fast pathways. In addition, both architectures [77, 76] are trained using label distribution learning and a curriculum learning-inspired dynamic constraint in the frequency domain, which helps to alleviate overfitting. Although unlike convolution-based EfficientPhys [37], transformer architectures require significantly higher computational resources. ", "page_idx": 2}, {"type": "text", "text": "Most of the light-weight convolutional attention mechanisms require attention to be separately derived in spatial, temporal, and channel dimensions, which is later merged [46, 13]. Although 3D-CNN architectures such as PhysNet [83] and iBVPNet [29] have shown promising performances, they have not explored attention mechanisms that can potentially enhance performance in unseen datasets. JAMSNet [79] and GLISNet [80] are recent 3D-CNN architectures that benefit significantly from channel-temporal joint attention (CTJA) and spatial-temporal joint attention (STJA). However, unlike CTJA and STJA [79, 80], we jointly derive attention in temporal, spatial, and channel dimensions, without squeezing any dimension of multidimensional features. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Primer: Nonnegative Matrix Factorization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Nonnegative matrix factorization is a dimensionality reduction paradigm that decomposes $M\\times N$ matrix $\\mathbf{\\bar{{V}}}=[v_{1},v_{2},...,v_{N}]\\in\\mathbb{R}_{\\ge0}^{M\\times N}$ into nonnegative $M\\times L$ basis matrix $W=[w1,w2,...,w L]\\in$ $\\mathbb{R}_{\\geq0}^{M\\times L}$ and nonnegative $L\\times N$ coefficient matrix $H=[h1,h2,...,h N]\\in\\mathbb{R}_{\\ge0}^{L\\times N}$ RL\u22650\u00d7N, as depicted in fig. 1 and expressed as: ", "page_idx": 3}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/e58f8bbd99d1507ada3b5ce4e3c46daecd8ad7bed11e6aef94dc856b1f09584d.jpg", "img_caption": ["Figure 1: Formulation of Nonnegative Matrix Factorization (NMF) "], "img_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\nV=W H+E=\\hat{V}+E\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{V}=[\\hat{v_{1}},\\hat{v_{2}},...,\\hat{v_{N}}]\\in\\mathbb{R}_{\\ge0}^{M\\times N}$ is reconstructed low-rank matrix and $E\\in\\mathbb{R}_{\\geq0}^{M\\times N}$ is an error matrix, which is discarded. $\\mathbb{R}_{\\geq0}^{M\\times N}$ stands for the set of $M\\times N$ element-wise nonnegative matrices. Equivalent vector formulation for this approximation can be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{j}\\approx\\hat{v_{j}}=\\sum_{i}^{L}w_{i}H_{i j}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The objective to represent high-dimensional matrix with fewer basis can be achieved only when $L$ is chosen such that $L\\ll m i n(M,N)$ , while when $L$ is larger than $M$ , it results in over-complete basis. An optimization in $W$ and $H$ to achieve the optimal approximation effectively results in the discovery of inherent correlations between the basis vectors in $W$ and the corresponding coefficients in $H$ [31, 64]. The optimization objective is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nm i n_{W,H}\\|V-W H\\|_{F}^{2}\\ni W_{m l}\\geq0,H_{l n}\\geq0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Further, the imposed non-negativity constraints on $W$ and $H$ enable parts-based representations, where activation of one or many of the coefficients in $H$ together with the basis vectors in $W$ can reconstruct different interpretable parts of $V$ . For a detailed primer on NMF, we refer the reader to the seminal work [31] and a survey article [64] that summarizes different NMF models and algorithms. ", "page_idx": 3}, {"type": "text", "text": "The factorization of deeper layer embeddings can be elucidated as the squeeze of information without reducing the dimensions of the embeddings, unlike the existing attention mechanisms [25]. Therefore, exciting or multiplying with the resultant low-rank (information squeezed) embeddings can potentially serve as an attention mechanism. While factorization is formulated for two-dimensional matrix, high-dimensional embeddings can be mapped to two-dimensional matrix. In PyTorch [47], this is achieved with the \u2018view\u2019 operation. ", "page_idx": 3}, {"type": "text", "text": "3.2 Factorized Self-Attention Module (FSAM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For the downstream task of estimating rPPG from video frames, spatial-temporal input data can be expressed as $\\mathcal{T}\\in\\mathbb{R}^{T\\times C\\times H\\times W}$ , where $T,C,H,a n d\\,W$ represents total frames (temporal dimension), channels in a frame (e.g., for RGB frames, $C=3$ ), height and width of pixels in a frame, respectively. $\\mathcal{T}$ is passed through a feature extractor that generates voxel embeddings $\\varepsilon\\ \\in\\ \\mathbb{R}^{\\tau\\times\\kappa\\times\\alpha^{\\star}\\beta}$ , with temporal $(\\tau)$ , channel $(\\kappa)$ and spatial $(\\alpha,\\beta)$ dimensions. ", "page_idx": 3}, {"type": "text", "text": "The goal is to jointly derive the attention in the multidimensional space of $\\varepsilon$ , without squeezing individual dimensions. For this, we deploy NMF-based matrix factorization to compute low-rank $\\hat{\\varepsilon}$ by reconstructing it from the factorized basis matrix $W$ and a coefficient matrix $H$ . It is essential to factorize $\\varepsilon$ in a way that $\\hat{\\varepsilon}$ approximated through the computed basis and coefficient matrices serves as an effective self-attention. Among several parameters that govern factorization, here we delve into the ones most relevant for the time series estimation task. These include: i) the transformation of the voxel embeddings that maps $\\varepsilon\\in\\mathbb{R}^{\\tau\\times\\kappa\\times\\alpha\\times\\beta}$ to the factorization matrix $V^{s t}\\in\\mathbb{R}^{M\\times N}$ and ii) the rank of the factorization. ", "page_idx": 3}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/d2862d7e979ce4d8babcb18625d5535a296ff16976b351c1e3fe6d02fed2f54c.jpg", "img_caption": ["Figure 2: Factorized Self-Attention Module (FSAM) illustrated for a 3D-CNN architecture for rPPG estimation. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "For a 2D-CNN architecture with $\\kappa$ channels and $\\alpha\\!\\times\\!\\beta$ spatial features, the transformation $(\\Gamma^{\\kappa\\alpha\\beta\\mapsto M N})$ ) implemented in the Hamburger module [18] is expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{s}\\in\\mathbb{R}^{M\\times N}=\\Gamma^{\\kappa\\alpha\\beta\\mapsto M N}(\\varepsilon\\in\\mathbb{R}^{\\kappa\\times\\alpha\\times\\beta})\\ni\\kappa\\mapsto M,\\alpha\\times\\beta\\mapsto N\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\kappa$ channels are mapped to $M$ and $\\alpha\\times\\beta$ spatial features are mapped to $N$ , with an underlying assumption that spatial features are inherently correlated due to learnt CNN kernels. However, for 3D-CNN architectures, as $\\varepsilon\\,\\in\\,\\mathbb{R}^{\\tau\\times\\kappa\\times\\alpha\\times\\beta}$ encodes temporal, channel, and spatial features, it is required to revisit these mappings. While it can be argued that similar to 2D-CNN architectures, as 3D-CNN architectures have 3D kernels, the spatial-temporal features are inherently correlated. However, it should be noted that the scales of spatial and temporal dimensions are very distinct, owing to which the spatial-temporal patterns to be learned may not be uniformly captured through typical convolutional kernels (e.g. $3\\times3\\times3)$ ). Adjusting the spatial-temporal kernel sizes can be heuristic task, and does not guarantee the extraction of desired features, while drastically increasing the model complexity (since for time series estimation, $\\tau\\,>>\\,\\alpha,\\,\\,\\beta)$ . Also, $\\kappa$ channels are not inherently correlated, and therefore it is crucial to devise a multidimensional attention that jointly computes the spatial-temporal and channel attention. To address this, we first consider negative Pearson correlation, a loss function that is commonly deployed to optimize end-to-end rPPG methods, expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{p}=1-\\frac{\\sum_{i}^{T}(r_{i}^{p p g}-\\overline{{r^{p p g}}})(g_{i}^{p p g}-\\overline{{g^{p p g}}})}{\\sqrt{\\sum_{i}^{T}(r_{i}^{p p g}-\\overline{{r^{p p g}}})^{2}}\\sqrt{\\sum_{i}^{T}(g_{i}^{p p g}-\\overline{{g^{p p g}}})^{2}}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, $r^{p p g}\\in\\mathbb{R}^{1\\times T}$ is an estimated rPPG signal and $g^{p p g}\\in\\mathbb{R}^{1\\times T}$ corresponds to the ground-truth BVP signal. The optimization of end-to-end model to estimate a vector in temporal dimension $(r^{p p g}\\in\\mathbf{\\breve{R}}^{1\\times T})$ ) can be leveraged by establishing the correlation of features in spatial and channel dimensions with the features in temporal dimension. Factorization of a matrix that consists of vectors in temporal domain and spatial and channel dimension as the features of the vectors, uniquely offers an opportunity to design the requisite attention. Prior to transforming $\\varepsilon$ to $V^{s t}\\,\\in\\,\\mathbb{R}^{\\dot{M}\\times\\dot{N}}$ , it is pre-processed through a convolution layer (with $1\\times1\\times1$ kernels), and a ReLU activation to ensure non-negativity of the embeddings. Following this preprocessing, the temporal features of $\\varepsilon$ are mapped to the vector dimension $(M)$ in $V^{s t}$ , while spatial and channel dimensions are mapped to the feature dimension $(N)$ of $V^{s t}$ . This transformation of $\\varepsilon$ as depicted in fig. 2, can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{s t}\\in\\mathbb{R}^{M\\times N}=\\Gamma^{\\tau\\kappa\\alpha\\beta\\mapsto M N}\\big(\\xi_{p r e}\\big(\\varepsilon\\in\\mathbb{R}^{\\tau\\times\\kappa\\times\\alpha\\times\\beta}\\big)\\big)\\ni\\tau\\mapsto M,\\kappa\\times\\alpha\\times\\beta\\mapsto N\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, $\\xi_{p r e}$ represents preprocessing operation. Factorization of thus formed matrix $V^{s t}$ with temporal vectors shall result in a low-rank matrix $\\hat{V}^{s t}$ which is approximated based on the latent structure that establishes correlation of temporal features with spatial and channel features. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{V}^{s t}=\\phi(V^{s t})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi$ represents factorization operation. $\\hat{V}^{s t}$ is transformed back to the embedding space, resulting in an approximated voxel embeddings $\\hat{\\varepsilon}$ that selectively retains the spatial and channel features that contribute towards the recovery of salient temporal features in $\\varepsilon$ . The resultant $\\hat{\\varepsilon}$ can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\varepsilon}=\\Gamma^{M N\\mapsto\\tau\\kappa\\alpha\\beta}(\\hat{V^{s t}}\\in\\mathbb{R}^{M\\times N})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Gamma^{M N\\mapsto\\tau\\kappa\\alpha\\beta}$ represents matrix transformation operations. We use the one-step gradient optimization based approach [18] to factorize $V^{s t}$ . This approach is a linear approximation of the conventional back-propagation through time algorithm (for time $t\\to\\infty$ ) [65], as proposed with the Hamburger module [18]. Approximated low-rank matrix V \u02c6st is transformed back to the embedding space through \u0393MN \u2192\u03c4\u03ba\u03b1\u03b2, resulting in $\\hat{\\varepsilon}$ that can potentially serve as the requisite attention. $\\hat{\\varepsilon}$ is post-processed with a convolution layer (with $1\\times1\\times1$ kernels), and a ReLU activation, followed by element-wise multiplication with $\\varepsilon$ . This multiplication operation serves as an excitation operation, which can be distinctly effective as $\\hat{\\varepsilon}$ retains the dimension of $\\varepsilon$ while computing the attention. The product is instance-normalized, and added with $\\varepsilon$ that serves as residual connection as depicted in fig. 2. It is to be noted that for each single forward pass through the model, approximation of $\\hat{V}^{s t}$ requires 4-8 steps, however, FSAM implements NMF within \u201cno_grad\u201d block, that does not require back-propagation through the NMF for model optimization. Representing the network head as $\\omega$ , $\\xi_{p o s t}$ as post-processing operation and $\\mathcal{T N}$ as instance normalization, the estimated $r^{p p g}$ signal can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nr^{p p g}=\\omega(\\varepsilon+\\mathcal{T}\\mathcal{N}(\\varepsilon\\odot\\xi_{p o s t}(\\hat{\\varepsilon})))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, we look at the rank of the factorization that affects the approximation of $V^{s t}$ . The primary consideration for the rank ${\\cal L}\\,\\ll\\,m i n(M,N)$ as mentioned in $\\S3.1$ ensures that $\\hat{V}^{s t}$ is of low rank. Although the choice of $L$ is generally governed by the downstream task, it is often derived empirically. In the context of $r^{P P\\widetilde{G}}$ estimation, we revisit the formulation of factorization matrix through $\\Gamma^{\\prime\\kappa\\alpha\\beta\\mapsto M N}$ that maps temporal features along the $M$ dimension. As we expect only a single signal underlying source of BVP signal across all facial regions, single vector estimation $\\hat{v_{0}^{s t}}$ corresponding to rank-1 (i.e., $L=1$ ) shall be sufficient to capture the spatial, temporal, and channel features that contribute to the $r^{P P G}$ estimation. Experimentation with rank-1 and higher rank factorization (appendix A.3) shows that for the higher ranks, the performance remains at par with that of the network without the FSAM, indicating that for rPPG estimation task, rank-1 factorization offers the optimal multidimensional attention, confirming our understanding. ", "page_idx": 5}, {"type": "text", "text": "3.3 Deployment of FSAM in 3D-CNN and 2D-CNN Architectures ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We deploy FSAM in our proposed 3D-CNN model, FactorizePhys and integrate it in an existing 2D-CNN architecture, EfficientPhys [37] to assess its versatility. ", "page_idx": 5}, {"type": "text", "text": "FactorizePhys Architecture: FactorizePhys, as depicted in fig. 3[A], is an end-to-end 3D-CNN architecture for estimating rPPG signal from raw video frames. Skin reflection models [62, 6] discuss the presence of several unrelated stationary and time-varying temporal components, and a relatively weaker pulsatile component of interest. To eliminate stationary components, FactorizePhys implements a Diff as first layer, inspired by existing rPPG architectures [6, 36, 37]. The resultant Diff frames are normalized with $\\mathcal{T N}$ , unlike existing architectures that use BatchNorm. The size of the kernel and the strides of each convolution layer are depicted in fig. 3[A]. For each layer, we use TanH activation followed by $\\mathcal{T N}$ . Spatial features are gradually aggregated by not padding the features, while we deploy spatial convolution strides only on the $\\dot{3}^{r d}$ and $\\mathbf{\\breve{6}}^{t h}$ layers. For temporal features, same padding retains the input temporal dimension throughout the network, as depicted in fig. 3[A]. Downsizing of temporal features may result in high-amplitude unrelated time-varying components to outweigh the weaker rPPG related pulsatile component. To provide a clearer overview of the architecture of FactorizePhys, only the spatial and temporal dimensions of the features at multiple layers are shown in fig. 3[A], while the channel dimension is skipped. FSAM, as elaborated in $\\S3.2$ , is deployed to jointly compute multidimensional attention, at the layer where the spatial dimension is reduced to $7\\times7$ , as reported as the optimal spatial dimension in a recent work [3]. ", "page_idx": 5}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/e53b89a609deb3a6fa9de703ef16dc10faaecf018fc14d5fbb344cbe5644893f.jpg", "img_caption": ["Figure 3: (A) Proposed FactorizePhys with FSAM; (B) FSAM Adapted for EfficientPhys [37] "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Adaptation of FSAM for 2D-CNN Architecture: Several SOTA rPPG methods [36, 37] leverage the TSM [34] that efficiently models spatial-temporal features using 2D-CNN architectures. The parameter called \u2018Frame Depth\u2019 $(\\psi\\;\\ni\\;\\psi\\;\\ll\\;\\kappa$ channels) controls the number of channels that are shifted along the temporal dimension for modeling temporal features. We investigated the effectiveness of the proposed FSAM with a more recent TSM-based SOTA rPPG architecture, EfficientPhys [37], which also deploys the Self-Attention Shifted Network (SASN) as the attention module. As $\\psi$ controls the amount of temporal information that is learned, we use this to formulate the mapping for the factorization matrix. Equation (6) can be adapted for TSM based architectures to appropriately transform the embeddings to factorization matrix as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV^{t s m}\\in\\mathbb{R}^{M\\times N}=\\Gamma^{\\kappa\\alpha\\beta\\mapsto M N}(\\varepsilon^{t s m}\\in\\mathbb{R}^{\\kappa\\times\\alpha\\times\\beta})\\ni\\psi\\mapsto M,\\kappa\\times\\alpha\\times\\beta\\mapsto N\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Figure 3[B] shows modified EfficientPhys [37] architecture, in which we drop SASN blocks [37] and add a single FSAM. Unlike SASN [37], FSAM derives attention without squeezing any individual dimension, which in turn can strengthen the correlation between temporal, channel and spatial features. This adaption critically evaluates the proposed FSAM against its counterpart in the SOTA architecture, addressing the recommendations of a recent survey article [23] on visual attention methods. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We perform an evaluation with carefully selected end-to-end SOTA rPPG methods that include PhysNet [83], a 3D-CNN architecture without the attention mechanism, EfficientPhys [37], a 2D-CNN architecture with self-attention, and PhysFormer [77], a transformer-based 3D-CNN architecture with multi-head self-attention. Comparison of FactorizePhys and PhysNet [83] can indicate the importance of the attention mechanism in 3D-CNN rPPG architectures, while comparison of EfficientPhys with SASN [37] and EfficientPhys [37] with FSAM allows evaluating the effectiveness of the proposed FSAM in 2D-CNN architectures, and thus allows assessing the versatility of FSAM. Similarly, the comparison of FactorizePhys and PhysFormer [77] offers a thorough evaluation of the proposed FSAM against multi-head self-attention in 3D-CNN architectures. ", "page_idx": 6}, {"type": "text", "text": "Each model was trained on one of the four existing datasets that include iBVP [29], PURE [55], UBFC-rPPG [2] and SCAMPS [42] and evaluated on the other three. Appendix A.1 provides our detailed description of these datasets. Our code is based on the rPPG-Toolbox [38], with specific adaptations described in appendix A.2. We train all models uniformly with 10 epochs [79] on iBVP [29], PURE [55], and UBFC-rPPG [2] datasets, and with one epoch on SCAMPS [42] dataset. For fair evaluation, all model-specific hyperparameters were maintained as provided by the respective SOTA rPPG methods, while the training pipeline related hyperparameters, which include preprocessing steps for images and labels, batch size, number of epochs, learning rate, scheduler, and optimizer were kept consistent for training all the models. ", "page_idx": 6}, {"type": "text", "text": "5 Results and Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Ablation Study: First, we train FactorizePhys on UBFC-rPPG [2] dataset and test on PURE [55] and iBVP [29] datasets, to compare different transformations of $\\bar{\\varepsilon}\\,\\in\\,\\mathbb{R}^{\\tau\\times\\kappa\\times\\alpha\\times\\beta}$ with temporal, channel and spatial features to factorization matrix $V^{s t}\\in\\mathbb{R}^{M\\times N}$ as tabulated in table 1. Superior cross-dataset generalization can be observed when the temporal dimension, $\\tau$ is mapped to $M$ , as described in $\\S3.2$ . We assess contribution of FSAM over base FactorizePhys model and observe consistent performance gains with FSAM, as reported in table 3 in appendix A. We then investigate residual connection in table 3, and observe it to contribute positively. We also observed that the base FactorizePhys model trained with FSAM retains the performance gains in-spite when FSAM is skipped during the inference. As this eliminates the computational overhead during inference, we report our main results of FactorizePhys trained with FSAM, by running inference without the FSAM. On contrary, in case of TSM [34] based EfficientPhys [37] model trained with FSAM, we observed performance drop when FSAM was skipped during inference, and therefore for EfficientPhys with FSAM, we do not drop FSAM during inference. Evaluation for factorization ranks and optimization steps to solve NMF shows consistent superiority of rank-1 factorization in table 4 in appendix A. ", "page_idx": 6}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/e1673f5d23048cd17bc8d6318a9fb27d59d5ca0b013ed4370a78eb7451cfb035.jpg", "table_caption": ["Table 1: Ablation Study for Different Mapping of Voxel Embeddings to Factorization Matrix "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "FactorizePhys vs. State-of-the-Art: We use heart rate (HR) [67] along with BVP metrics that include signal-to-noise ratio (SNR) and maximum amplitude of cross-correlation (MACC) [29, 10] for evaluation. SNR and MACC are direct measures to compare estimated rPPG signals with ground-truth BVP signals. The HR metrics reported are the mean absolute error (MAE), the square root of the mean square error (RMSE), the mean absolute percentage error (MAPE), and Pearson\u2019s correlation coefficient (Corr) [67] of the estimated HR. Uncertainty estimates quantifying the variability associated with signal estimation have been shown to be strongly correlated with the absolute error of the estimated HR [52]. In addition, for each metrics, we report the standard error to estimate the variability of each model. As most SOTA end-to-end models show robust withindataset performance, we present cross-dataset performance in table 2, while reporting within-dataset performance in table 6 in appendix A. ", "page_idx": 7}, {"type": "text", "text": "First, we observe that for all the evaluation metrics reported, the proposed FactorizePhys with FSAM outperforms the SOTA methods on PURE [55] and iBVP [29] datasets, across all training datasets. This suggests a consistent and superior generalization achieved by the proposed method. Crossdataset evaluation on the UBFC-rPPG [2] dataset further highlights the performance gains of the proposed FactorizePhys model when trained with the iBVP [29] and the SCAMPS [42] datasets, and at-par performance when trained with the PURE dataset. When models are trained with SCAMPS [42] (synthesized dataset), FactorizePhys uniquely outperforms the SOTA methods on all testing datasets further indicating the superior cross-dataset generalization. The performance of EfficientPhys [37] with FSAM exceeds in most cases and remains at par in the rest, compared to the EfficientPhys model with SASN [37], suggesting the versatility of FSAM as an attention module. As 3D CNN kernels in FactorizePhys can learn spatial-temporal patterns better than the TSM [34] based 2D-CNN model (EfficientPhys [37]), FactorizePhys with FSAM outperforms EfficientPhys [37] with FSAM across all datasets. Lastly, the proposed method consistently achieves superior SNR and MACC for the estimated rPPG signals, highlighting the enhanced reliability of the extracted signals. ", "page_idx": 7}, {"type": "text", "text": "Computation Cost and Latency: We compare computational complexity and latency for all the models in fig. 4[A], and provide further details in table 9 in appendix A. The cumulative MAE is computed by averaging cross-dataset performance for respective models across all combinations of training and testing datasets reported in table 2. The proposed FactorizePhys with FSAM not only shows the best performance, it has significantly less number of model parameters and performs at par in terms of latency as the 2D-CNN SOTA rPPG method, EfficientPhys [37]. Specifically, dropping FSAM during inference does not result in loss of performance for FactorizePhys, while reducing latency considerably, making it highly suitable for real-time and resource-constrained deployment. In contrast, when FSAM was dropped after training EfficientPhys [37] with FSAM, it did not retain the performance (results not shown). We interpret that while FSAM effectively influences the 3D convolutional kernels in FactorizePhys to increase the saliency of relevant spatial-temporal features, 2D convolutional kernels cannot benefti adequately due to the limited ability to model spatial-temporal features. It should also be noted that the higher latency of FactorizePhys compared to EfficientPhys [37], although it has fewer model parameters, can be attributed to the difference in floating-point operations (FLOPS) between the 3D-CNN and 2D-CNN architectures. ", "page_idx": 7}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/e6b1c7330a076db1d0f33707b38a9489be4a2547e5f5d2eb56ec541aadd30e4f.jpg", "table_caption": ["Table 2: Cross-dataset Performance Evaluation for rPPG Estimation "], "table_footnote": ["TD-MHSA\\*: Temporal Difference Multi-Head Self-Attention [77]; SASN: Self-Attention Shifted Network [37]; FSAM: Proposed Factorized Self-Attention Module. $\\dag$ All metrics are shown with two decimal places, except those correlation measures that range between 0.99 and 1.0 "], "page_idx": 8}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/737c94cc7a072438b564b502185cc1062afb03415b92f25d127f62d4c67c32e9.jpg", "img_caption": ["Figure 4: (A) Cumulative cross-dataset performance (MAE) v/s latency\u2020 plot. The size of the sphere corresponds to the number of model parameters; (B) Visualization of learned spatial-temporal features from the base 3D-CNN model trained without and with FSAM; $\\dagger$ System specs: Ubuntu $22.04\\:\\mathrm{OS}$ , NVIDIA GeForce RTX 3070 Laptop GPU, Inte $\\textsuperscript{\\textregistered}$ Core\u2122i7-10870H CPU $@$ 2.20GHz, 16 GB RAM. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Visualization of Learned Attention: We compute absolute cosine similarity between the temporal dimension of 4D embeddings (with temporal, spatial, and channel dimensions) and the ground-truth signal to visualize the learned attention for FactorizePhys trained without and with FSAM in fig. 4[B], where each tile represents a channel of the embedding layer. A higher cosine similarity score between the temporal dimension of the embeddings and the ground-truth PPG signal, which is observed for FactorizePhys trained with FSAM, indicates a higher saliency of temporal features. The spatial spread of high cosine similarity scores in different channels for FactorizePhys trained with FSAM, highlights selectivity of the learned attention, providing clearer evidence that the FactorizePhys model trained with FSAM can effectively pick the spatial features having the strong presence of the rPPG signal (i.e., facial regions with visible skin surface). Figure 4[B] not only suggests the effectiveness of the joint computation of multidimensional attention, but also offers more intuitive visualization of learned spatial-temporal features than existing visualization approaches [79, 37]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present FactorizePhys, a 3D-CNN model utilizing the Factorized Self-Attention Module, FSAM, to concurrently extract multidimensional (spatial, temporal, and channel) attention for the downstream task of rPPG estimation from video frames. The assessment performed utilizing various rPPG datasets demonstrates that our proposed method possesses superior generalization capabilities across different datasets, compared to current state-of-the-art methods. Moreover, when adjusted to the 2D-CNN architecture, FSAM achieves performance on par with the established SASN [37] attention, underscoring its adaptability across diverse network architectures. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts and Limitations: The superior performance of FactorizePhys equipped with FSAM to estimate rPPG indicates its potential utility in various healthcare applications that require the estimation of physiological signals through noncontact imaging. Although FSAM has shown efficacy as a multidimensional attention mechanism specifically for the extraction of rPPG signals, more research is needed to determine the efficacy of the proposed method in extracting heart rate variability metrics as well as other physiological signals. Despite the state-of-the-art performance of the proposed rPPG method, signal peaks can still be susceptible to challenging real-world scenarios, such as active head movements, occlusions, and dynamic changes in ambient lighting conditions, an issue that is qualitatively illustrated in the waveforms depicted in appendix A.11. Moreover, it is imperative to conduct additional research to evaluate the effectiveness of FSAM across other spatialtemporal domains, including video understanding, video object tracking, and video segmentation, along with several other downstream tasks that depend on multi-dimensional input data. In the context of signal estimation tasks, the utilization of NMF variants that integrate temporal or frequency constraints on time series vectors may offer enhanced attention capabilities. These constraints are congruent with the characteristics of the ground truth and present avenues for future investigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The author JJ was fully supported by the UCL CS PhD Studentship (GDI - Physiological Computing and Artificial Intelligence) which Prof. Cho has secured. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Pooya Ashtari, Diana M. Sima, Lieven De Lathauwer, Dominique Sappey-Marinier, Frederik Maes, and Sabine Van Huffel. Factorizer: A scalable interpretable approach to context modeling for medical image segmentation. Medical Image Analysis, 84:102706, 2023.   \n[2] Serge Bobbia, Richard Macwan, Yannick Benezeth, Alamin Mansouri, and Julien Dubois. Unsupervised skin tissue segmentation for remote photoplethysmography. Pattern Recognition Letters, 124:82\u201390, 2019.   \n[3] Deivid Botina-Monsalve, Yannick Benezeth, and Johel Miteran. Rtrppg: An ultra light 3dcnn for real-time remote photoplethysmography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 2146\u20132154, June 2022.   \n[4] Fr\u00e9d\u00e9ric Bousefsaf, Alain Pruski, and Choubeila Maaoui. 3d convolutional neural networks for remote pulse rate measurement and mapping from facial video. Applied Sciences, 9(20), 2019.   \n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.   \n[6] Weixuan Chen and Daniel McDuff. Deepphys: Video-based physiological measurement using convolutional attention networks. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.   \n[7] Youngjun Cho. Rethinking eye-blink: Assessing task difficulty through physiological representation of spontaneous blinking. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI \u201921, New York, NY, USA, 2021. Association for Computing Machinery.   \n[8] Youngjun Cho, Nadia Bianchi-Berthouze, and Simon J. Julier. Deepbreath: Deep learning of breathing patterns for automatic stress recognition using low-cost thermal imaging in unconstrained settings. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII), pages 456\u2013463, 2017.   \n[9] Youngjun Cho, Simon J Julier, and Nadia Bianchi-Berthouze. Instant stress: detection of perceived mental stress through smartphone photoplethysmography and thermal imaging. JMIR mental health, 6(4):e10140, 2019.   \n[10] Youngjun Cho, Simon J. Julier, Nicolai Marquardt, and Nadia Bianchi-Berthouze. Robust tracking of respiratory rate in high-dynamic range scenes using mobile thermal imaging. Biomed. Opt. Express, 8(10):4480\u20134503, Oct 2017.   \n[11] Gerard de Haan and Vincent Jeanne. Robust pulse rate from chrominance-based rppg. IEEE Transactions on Biomedical Engineering, 60(10):2878\u20132886, 2013.   \n[12] Inderjit S Dhillon and Dharmendra S Modha. Concept decompositions for large sparse text data using clustering. Machine learning, 42:143\u2013175, 2001.   \n[13] Shuai Ding, Zhen Ke, Zijie Yue, Cheng Song, and Lu Lu. Noncontact multiphysiological signals estimation via visible and infrared facial features fusion. IEEE Transactions on Instrumentation and Measurement, 71:1\u201313, 2022.   \n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[15] Wenbin Du, Yali Wang, and Yu Qiao. Recurrent spatial-temporal attention network for action recognition in videos. IEEE Transactions on Image Processing, 27(3):1347\u20131360, 2018.   \n[16] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.   \n[17] Xiao Fu, Kejun Huang, Nicholas D. Sidiropoulos, and Wing-Kin Ma. Nonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications. IEEE Signal Processing Magazine, 36(2):59\u201380, 2019.   \n[18] Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke Wei, and Zhouchen Lin. Is attention better than matrix decomposition? In International Conference on Learning Representations, 2021.   \n[19] R.M. Gray and D.L. Neuhoff. Quantization. IEEE Transactions on Information Theory, 44(6):2325\u20132383, 1998.   \n[20] Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer vision: A survey. Computational visual media, 8(3):331\u2013368, 2022.   \n[21] Xudong Guo, Xun Guo, and Yan Lu. Ssan: Separable self-attention network for video representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12618\u201312627, June 2021.   \n[22] Anup Kumar Gupta, Rupesh Kumar, Lokendra Birla, and Puneet Gupta. Radiant: Better rppg estimation using signal embeddings and transformer. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 4976\u20134986, January 2023.   \n[23] Mohammed Hassanin, Saeed Anwar, Ibrahim Radwan, Fahad Shahbaz Khan, and Ajmal Mian. Visual attention methods in deep learning: An in-depth survey. Information Fusion, 108:102417, 2024.   \n[24] Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang. Voxel set transformer: A set-to-set approach to 3d object detection from point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8417\u20138427, June 2022.   \n[25] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n[26] Min Hu, Fei Qian, Xiaohua Wang, Lei He, Dong Guo, and Fuji Ren. Robust heart rate estimation with spatial\u2013temporal attention network from facial videos. IEEE Transactions on Cognitive and Developmental Systems, 14(2):639\u2013647, 2022.   \n[27] Bin Huang, Shen Hu, Zimeng Liu, Chun-Liang Lin, Junfeng Su, Changchen Zhao, Li Wang, and Wenjin Wang. Challenges and prospects of visual contactless physiological monitoring in clinical study. NPJ Digital Medicine, 6(1):231, 2023.   \n[28] Jitesh Joshi, Nadia Berthouze, and Youngjun Cho. Self-adversarial multi-scale contrastive learning for semantic segmentation of thermal facial images. In 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022. BMVA Press, 2022.   \n[29] Jitesh Joshi and Youngjun Cho. iBVP Dataset: RGB-Thermal rPPG Dataset with High Resolution Signal Quality Labels. Electronics, 13(7), 2024.   \n[30] Jitesh Joshi, Katherine Wang, and Youngjun Cho. PhysioKit: An Open-Source, Low-Cost Physiological Computing Toolkit for Single-and Multi-User Studies. Sensors, 23(19), 2023.   \n[31] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization. nature, 401(6755):788\u2013791, 1999.   \n[32] John Boaz Lee, Ryan A. Rossi, Sungchul Kim, Nesreen K. Ahmed, and Eunyee Koh. Attention models in graphs: A survey. ACM Trans. Knowl. Discov. Data, 13(6), nov 2019.   \n[33] Dong Li, Ting Yao, Ling-Yu Duan, Tao Mei, and Yong Rui. Unified spatio-temporal attention networks for action recognition in videos. IEEE Transactions on Multimedia, 21(2):416\u2013428, 2019.   \n[34] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.   \n[35] Tonglai Liu, Ronghai Luo, Longqin Xu, Dachun Feng, Liang Cao, Shuangyin Liu, and Jianjun Guo. Spatial channel attention for deep convolutional neural networks. Mathematics, 10(10):1750, 2022.   \n[36] Xin Liu, Josh Fromm, Shwetak Patel, and Daniel McDuff. Multi-task temporal shift attention networks for on-device contactless vitals measurement. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19400\u201319411. Curran Associates, Inc., 2020.   \n[37] Xin Liu, Brian Hill, Ziheng Jiang, Shwetak Patel, and Daniel McDuff. Efficientphys: Enabling simple, fast and accurate camera-based cardiac measurement. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 5008\u20135017, January 2023.   \n[38] Xin Liu, Girish Narayanswamy, Akshay Paruchuri, Xiaoyu Zhang, Jiankai Tang, Yuzhe Zhang, Roni Sengupta, Shwetak Patel, Yuntao Wang, and Daniel McDuff. rppg-toolbox: Deep remote ppg toolbox. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 68485\u201368510. Curran Associates, Inc., 2023.   \n[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10012\u201310022, October 2021.   \n[40] Hao Lu and Hu Han. Nas-hr: Neural architecture search for heart rate estimation from face videos. Virtual Reality & Intelligent Hardware, 3(1):33\u201342, 2021. Emotion recognition for human-computer interaction.   \n[41] Hao Lu, Hu Han, and S. Kevin Zhou. Dual-gan: Joint bvp and noise modeling for remote physiological measurement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12404\u201312413, June 2021.   \n[42] Daniel McDuff, Miah Wander, Xin Liu, Brian Hill, Javier Hernandez, Jonathan Lester, and Tadas Baltrusaitis. Scamps: Synthetics for camera measurement of physiological signals. Advances in Neural Information Processing Systems, 35:3744\u20133757, 2022.   \n[43] Clara Moge, Katherine Wang, and Youngjun Cho. Shared user interfaces of physiological data: Systematic review of social biofeedback systems and contexts in hci. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1\u201316, 2022.   \n[44] Girish Narayanswamy, Yujia Liu, Yuzhe Yang, Chengqian Ma, Xin Liu, Daniel McDuff, and Shwetak Patel. Bigsmall: Efficient multi-task learning for disparate spatial and temporal physiological measurements. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 7914\u20137924, 2024.   \n[45] Xuesong Niu, Shiguang Shan, Hu Han, and Xilin Chen. Rhythmnet: End-to-end heart rate estimation from face via spatial-temporal representation. IEEE Transactions on Image Processing, 29:2409\u20132423, 2020.   \n[46] Xuesong Niu, Xingyuan Zhao, Hu Han, Abhijit Das, Antitza Dantcheva, Shiguang Shan, and Xilin Chen. Robust remote heart rate estimation from face utilizing spatial-temporal attention. In 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019), pages 1\u20138, 2019.   \n[47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[48] Ming-Zher Poh, Daniel J. McDuff, and Rosalind W. Picard. Non-contact, automated cardiac pulse measurements using video imaging and blind source separation. Opt. Express, 18(10):10762\u201310774, May 2010.   \n[49] Delong Qi, Weijun Tan, Qi Yao, and Jingfeng Liu. Yolo5face: Why reinventing a face detector. In European Conference on Computer Vision, pages 228\u2013244. Springer, 2022.   \n[50] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages 369\u2013386. SPIE, 2019.   \n[51] Rencheng Song, Huan Chen, Juan Cheng, Chang Li, Yu Liu, and Xun Chen. Pulsegan: Learning to generate realistic pulse waveforms in remote photoplethysmography. IEEE Journal of Biomedical and Health Informatics, 25(5):1373\u20131384, 2021.   \n[52] Rencheng Song, Han Wang, Haojie Xia, Juan Cheng, Chang Li, and Xun Chen. Uncertainty quantification for deep learning-based remote photoplethysmography. IEEE Transactions on Instrumentation and Measurement, 2023.   \n[53] Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and Jiaying Liu. An end-to-end spatio-temporal attention model for human action recognition from skeleton data. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1), Feb. 2017.   \n[54] Radim \u0160petl\u00edk, Vojtech Franc, and Jir\u00ed Matas. Visual heart rate estimation with convolutional neural network. In Proceedings of the british machine vision conference, Newcastle, UK, pages 3\u20136, 2018.   \n[55] Ronny Stricker, Steffen M\u00fcller, and Horst-Michael Gross. Non-contact video-based pulse rate measurement on a mobile service robot. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication, pages 1056\u20131062. IEEE, 2014.   \n[56] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. In International Conference on Learning Representations, 2020.   \n[57] Snigdha Tariyal, Angshul Majumdar, Richa Singh, and Mayank Vatsa. Deep dictionary learning. IEEE Access, 4:10096\u201310109, 2016.   \n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[59] Wim Verkruysse, Lars O Svaasand, and J Stuart Nelson. Remote plethysmographic imaging using ambient light. Opt. Express, 16(26):21434\u201321445, Dec 2008.   \n[60] Jing Wang and Lei Liu. A multi-attention deep neural network model base on embedding and matrix factorization for recommendation. International Journal of Cognitive Computing in Engineering, 1:70\u201377, 2020.   \n[61] Rui-Xuan Wang, Hong-Mei Sun, Rong-Rong Hao, Ang Pan, and Rui-Sheng Jia. Transphys: Transformerbased unsupervised contrastive learning for remote heart rate measurement. Biomedical Signal Processing and Control, 86:105058, 2023.   \n[62] Wenjin Wang, Albertus C. den Brinker, Sander Stuijk, and Gerard de Haan. Algorithmic principles of remote ppg. IEEE Transactions on Biomedical Engineering, 64(7):1479\u20131491, 2017.   \n[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794\u20137803, 2018.   \n[64] Yu-Xiong Wang and Yu-Jin Zhang. Nonnegative matrix factorization: A comprehensive review. IEEE Transactions on Knowledge and Data Engineering, 25(6):1336\u20131353, 2013.   \n[65] P.J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990.   \n[66] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.   \n[67] Hanguang Xiao, Tianqi Liu, Yisha Sun, Yulin Li, Shiyi Zhao, and Alberto Avolio. Remote photoplethysmography for heart rate measurement: A review. Biomedical Signal Processing and Control, 88:105608, 2024.   \n[68] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 12077\u201312090. Curran Associates, Inc., 2021.   \n[69] Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, and Jun Wang. Mlcvnet: Multilevel context votenet for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[70] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2048\u20132057, Lille, France, 07\u201309 Jul 2015. PMLR.   \n[71] Ming Xu, Guang Zeng, Yongjun Song, Yue Cao, Zeyi Liu, and Xiao He. Ivrr-ppg: An illumination variation robust remote-ppg algorithm for monitoring heart rate of drivers. IEEE Transactions on Instrumentation and Measurement, 72:1\u201310, 2023.   \n[72] Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, and Pan Zhou. Jointly attentive spatialtemporal pooling networks for video-based person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 4733\u20134742, 2017.   \n[73] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.   \n[74] Chenggang Yan, Yunbin Tu, Xingzheng Wang, Yongbing Zhang, Xinhong Hao, Yongdong Zhang, and Qionghai Dai. Stat: Spatial-temporal attention mechanism for video captioning. IEEE Transactions on Multimedia, 22(1):229\u2013241, 2020.   \n[75] Zongxin Yang, Linchao Zhu, Yu Wu, and Yi Yang. Gated channel transformation for visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[76] Zitong Yu, Yuming Shen, Jingang Shi, Hengshuang Zhao, Yawen Cui, Jiehua Zhang, Philip Torr, and Guoying Zhao. Physformer $^{++}$ : Facial video-based physiological measurement with slowfast temporal difference transformer. International Journal of Computer Vision, 131(6):1307\u20131330, February 2023.   \n[77] Zitong Yu, Yuming Shen, Jingang Shi, Hengshuang Zhao, Philip H.S. Torr, and Guoying Zhao. Physformer: Facial video-based physiological measurement with temporal difference transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4186\u20134196, June 2022.   \n[78] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Ocnet: Object context for semantic segmentation. International Journal of Computer Vision, 129(8):2375\u20132398, 2021.   \n[79] Changchen Zhao, Hongsheng Wang, Huiling Chen, Weiwei Shi, and Yuanjing Feng. Jamsnet: A remote pulse extraction network based on joint attention and multi-scale fusion. IEEE Transactions on Circuits and Systems for Video Technology, 33(6):2783\u20132797, 2023.   \n[80] Changchen Zhao, Menghao Zhou, Zheng Zhao, Bin Huang, and Bing Rao. Learning spatio-temporal pulse representation with global-local interaction and supervision for remote prediction of heart rate. IEEE Journal of Biomedical and Health Informatics, 28(2):609\u2013620, 2024.   \n[81] Hongyi Zheng, Hongwei Yong, and Lei Zhang. Deep convolutional dictionary learning for image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 630\u2013641, June 2021.   \n[82] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}: Deformable transformers for end-to-end object detection. In International Conference on Learning Representations, 2021.   \n[83] Yu Zitong, Li Xiaobai, and Guoying Zhao. Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks. In 30th British Machine Vision Conference (BMVC), 9th-12th September 2019, Cardiff, UK, September 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix / Supplemental Material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All datasets provide video recordings with a resolution of $640\\times480$ , and frame rate of 30 FPS. Below we provide data-specific details. ", "page_idx": 15}, {"type": "text", "text": "iBVP [29]: The iBVP dataset consists of 124 synchronized RGB and thermal infrared videos from 31 subjects, acquired under controlled conditions. Each video is 3 minutes in duration, and the ground truth BVP signals were acquired from the ear using PhysioKit [30]. Data were acquired under 4 different conditions that include controlled breathing, math tasks, and head movements. BVP signals are marked with the signal quality, enabling the use of the video frames only where the quality of ground-truth BVP signal is high. In this work, we use only RGB frames to train the models. ", "page_idx": 15}, {"type": "text", "text": "PURE [55]: This data set comprises video recordings from 10 subjects, with the ground-truth BVP and SpO2 signals acquired from the subject\u2019s finger. For each participant, six recordings are acquired under varied motion conditions, offering a range of data reflecting different physical states. ", "page_idx": 15}, {"type": "text", "text": "UBFC-rPPG [2]: This data set contains video recordings of 43 subjects acquired under indoor conditions with a combination of natural sunlight and artificial illumination. ", "page_idx": 15}, {"type": "text", "text": "SCAMPS [42]: This dataset comprises 2800 videos of synthetic avatars that were generated through high-fidelity, quasi-photorealistic renderings. Although the videos introduce various conditions such as head motions, facial expressions, and changes in ambient illumination, they are often used as a training set rather than a validation or test set. ", "page_idx": 15}, {"type": "text", "text": "A.2 Implementation Overview ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The preprocessing steps for video frames include face detection using the YOLO5Face [49] face detector at an interval of 30 frames and using the detected facial bounding box to crop 30 subsequent frames, prior to performing the next face detection. The cropped facial frames are resized to a resolution of $72\\times72$ , which has been shown to be sufficient to estimate the rPPG. Additionally, to ensure uniform input data for all models, we add Diff layer to the PhysNet [83] and PhysFormer [77] architectures, as implemented by EfficientPhys [37] and the proposed FactorizePhys models, and train all the models from scratch using uniformly preprocessed video frames. ", "page_idx": 15}, {"type": "text", "text": "The number of frames in a video chunk is maintained as 161, which after the Diff layer becomes 160, making the spatial-temporal input data size $160\\times72\\times72$ . Ground-truth BVP signals are also uniformly standardized for training all models. This is different from some of the recent work [37] that applies Diff in addition to standardization. We empirically found that all models perform significantly better when trained with the standardized BVP signals, although when the Diff is applied to the video frames. ", "page_idx": 15}, {"type": "text", "text": "All models were trained with 10 epochs on, following a recent work [79], as a higher number of epochs, e.g. 30 epochs as used in rPPG-Toolbox [38] resulted in poor generalization for all models. However, we used only one epoch for all models to train on the SCAMPS [42] dataset, since this dataset is a synthesized dataset with generated BVP signals that are easier for models to learn, unlike real-world datasets. Training beyond one epoch resulted in poorer cross-dataset performance for all the models. The batch size of 4 was used consistently throughout the training and the maximum learning rate was set to $1~\\times~10^{-3}$ with 1 cycle learning rate scheduler [50] for all CNN models. ", "page_idx": 15}, {"type": "text", "text": "In addition, CNN models were optimized using negative Pearson correlation as a loss function. The learning rate for PhysFormer [77] was set to $\\bar{1}\\times\\bar{1}0^{-4}$ and it was optimized using a dynamic loss composed of several hyperparameters, a negative Pearson loss, a frequency cross-entropy loss and a label distribution loss as used by the authors and implemented in the rPPG-Toolbox [38]. Before computing HR for performance evaluation, both ground truth and estimated BVP signals were flitered using a bandpass filter (low cutoff $=0.60\\ \\mathrm{Hz}$ , high cutoff $=3.30\\ \\mathrm{Hz}$ ) to accommodate HR ranges of 36 to 198 BPM. HR was then computed using the FFT-peaks-based approach as implemented in rPPG-Toolbox [38]. ", "page_idx": 15}, {"type": "text", "text": "A.3 Ablation Studies for FactorizePhys ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conduct ablation studies to evaluate optimal architectural choices and hyperparameters for the proposed FactorizePhys and FSAM. In table 3, we compare base FactorizePhys without FSAM and with FSAM and observe consistent performance gains with FSAM. Evaluation with and without residual connection indicates performance gains when residual connection around FSAM is implemented. ", "page_idx": 16}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/a3e7e89e76e6add647fc7c667a1a414cbea3b2764e4a0730b07af3f902dd6e21.jpg", "table_caption": ["Table 3: Ablation study to assess residual connection to FSAM Module, and to compare the models trained with FSAM, for their inferences without FSAM "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Retention of performance gains despite FSAM being skipped during inference, for FactorizePhys trained with FSAM offers insight into the mechanics of how FSAM functions. This can be interpreted as follows: Optimization of a network having FSAM implemented as an attention mechanism influences the network to increase the saliency of the most relevant features, so that a factorized approximation of embeddings retains these features, while discarding the less important features. Due to the increased saliency of relevant features and the presence of residual connection, FSAM can be skipped during inference, significantly reducing computational overhead. ", "page_idx": 16}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/a7fd1eddb7123c3b19052c87076973e73c7e1358045bb42041604689eed40817.jpg", "table_caption": ["Table 4: Performance Evaluation of Models on PURE Dataset [55], Trained with UBFC-rPPG Dataset [2], using Different Ranks and Optimization Steps for Factorization "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "In table 4, we present results to compare the performance obtained for different ranks $L$ , as well as the optimization steps used to solve factorization. For all experiments, FactorizePhys is trained with the UBFC-rPPG dataset [2] and the performance is presented for the PURE dataset [55]. We can observe that the best performance was achieved for rank $L=1$ for the different steps used to solve the factorization. For higher ranks, performance remains on par with that of the network without the FSAM, indicating that for the rPPG estimation task, the rank-1 factorization offers the optimal spatial-temporal attention. These results align with the expected single source of the underlying BVP signals in different facial regions. ", "page_idx": 17}, {"type": "text", "text": "A.4 Statistical Significance of the Main Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We performed repeated experiments with 10 different random seed values between 1 and 1000 to compare the proposed FactorizePhys trained with FSAM with the best performing SOTA rPPG method. For the cross-dataset generalization results reported in table 2, EfficientPhys with SASN [37] was found to perform the best among the existing SOTA methods. ", "page_idx": 17}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/bde09e22fdfeaa94fbeb2e6068adc96332aea069b70b27b3430ab693ad564829.jpg", "table_caption": ["Table 5: Performance Evaluation of Models on PURE Dataset, Trained with UBFC-rPPG Dataset, using Different Random Seed Values "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "For each random seed value, we trained the proposed FactorizePhys with FSAM and EfficientPhys with SASN [37] on the UBFC-rPPG [2] dataset and evaluated them on the PURE dataset [55]. Paired T tests for each reported evaluation metrics suggest that the performance gains achieved with the proposed method are statistically significant compared against the best performing SOTA rPPG method, highlighting its effectiveness and thereby highlighting contributions of this work in the research field of end-to-end rPPG estimation from video frames. ", "page_idx": 17}, {"type": "text", "text": "A.5 Within Dataset Performance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this work, we primarily focus on comparing rPPG methods for their cross-dataset generalization, which offers more critical evaluation and reliable estimates of how models perform on unseen or out-of-distribution data. Within-dataset performance signifies an representation ability of model to fit the data, derived from the same distribution, serving as an essential criteria. Therefore, for completeness, in table 6, we report within-dataset evaluation on iBVP [29], [55], and UBFC-rPPG [2] datasets, where we observe at-par performance of FactorizePhys as compared with the SOTA rPPG methods. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/a35665943ae545e7e61618637a6ff70a2500bea0135bf97615e6a6d2edff4bb9.jpg", "table_caption": ["Table 6: Within Dataset Performance Evaluation "], "table_footnote": ["TD-MHSA\\*: Temporal Difference Multi-Head Self-Attention [77]; ", "SASN: Self-Attention Shifted Network [37]; FSAM: Proposed Factorized Self-Attention Module "], "page_idx": 18}, {"type": "text", "text": "A.6 Scalability Assessment of FSAM ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We further investigate FSAM for its scalability to higher spatial-temporal resolution. For this, we perform within-dataset evaluation on the UBFC-rPPG dataset [2], which is pre-processed with the regular input dimension of $160\\times72\\times72$ as well as with a higher spatial and temporal dimension of $240\\times128\\times128.$ . Repeatable experiments are conducted with 10 different random seeds between 1 and 1000 to compare the performance of FactorizePhys with FSAM for each spatial-temporal input dimension. ", "page_idx": 18}, {"type": "text", "text": "Comparable performance, as observed in table 7, for both spatial-temporal input dimensions, suggests that FSAM can be easily deployed for different spatial-temporal scales. It should also be noted that the higher spatial dimension of video frames (i.e., $128\\times128)$ ) does not produce improved performance, indicating that the spatial dimension of $72\\times72$ is sufficient to extract rPPG signals with end-to-end methods. ", "page_idx": 18}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/c45409290dd7237faa547a61b2e1ab878c53ba875fbda955691113afd429df93.jpg", "table_caption": ["Table 7: Scalability Assessment of FSAM for Higher Spatial and Temporal Dimensions "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.7 Multimodal rPPG Extraction ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As iBVP dataset offers synchronized RGB and thermal infrared video frames, we conducted a brief experiment using FactorizePhys with FSAM to investigate whether combining both modalities can result in performance gains for the estimation of rPPG. For this, we also individually trained FactorizePhys on RGB and thermal frames keeping the identical data split of $70\\%{-30\\%}$ . Results in table 8 suggest weaker presence of rPPG signal in thermal infrared frames, leading to poorer performance when FactorizePhys is trained only on thermal frames, while not showing significant performance gains when jointly trained with RGB and thermal frames. ", "page_idx": 19}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/7c759848e4d68482c875e8c966f755c3a983dc8db1f301a682ee1a77e8a146ea.jpg", "table_caption": ["Table 8: Performance Evaluation on iBVP Dataset, Subject-wise Split: Train $(70\\%)$ , Test $(30\\%)$ "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "A.8 Visual Overview of Cross-Dataset Generalization and Latency ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 5 offers a quick visual summary of the cross-dataset generalization performance on different evaluation metrics, their respective standard error, and latency for the proposed and existing SOTA ", "page_idx": 19}, {"type": "text", "text": "methods. The performance reported on Y-axis of each plot is cumulative cross-dataset performance ", "page_idx": 20}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/efd8f2ec9c946c9190a37fb79cec322352f83034b2e9bf494bddd2ae1f12da12.jpg", "img_caption": ["Figure 5: Cross-dataset performance comparison between SOTA and the proposed method reported with cumulative evaluation metrics, their standard error (SE) and latency "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "for respective models, averaged over different training and testing datasets. The proposed method outperforms existing state-of-the-art methods in all evaluation metrics by a significant margin, while achieving at-par latency. ", "page_idx": 20}, {"type": "text", "text": "A.9 Computational Cost and Latency ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 9 compares model parameters, latency on GPU and CPU, and model size of the proposed FactorizePhys with FSAM with that of the existing SOTA rPPG methods. Considering the identical inference time performance of the base FactorizePhys, when trained using the proposed FSAM, the proposed method uses an order of magnitude fewer parameters and achieves a par latency on both CPU and GPU systems. Relatively higher latency compared to the EfficientPhys [37] model, despite the fewer model parameters, is due to the difference in the number of floating point operations (FLOPS). FactorizePhys, being a 3D-CNN architecture, requires more FLOPS to compute 3D features at each layer compared to the fewer FLOPS for EfficientPhys [37] which implements the 2D-CNN architecture. It should be noted that the FLOPS are also dependent on the input dimension, which is kept consistent for all the models. For resource critical deployment, FLOPS can be significantly reduced by decreasing the spatial dimension of input from $72\\times72$ to $8\\times8$ as found optimal for RTrPPG [3] or to $9\\times9$ as used in the small branch of the Bigsmall model [44] for rPPG estimation. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "qrfp4eeZ47/tmp/b7daffe713c03f91946e93f72bfaa44decda4ccff3a0c5de1e9c8efa8f36730f.jpg", "table_caption": ["Table 9: Comparison of FactorizePhys based on Model Parameters, Latency and Model Size "], "table_footnote": ["\u2020CPU Specs: Intel\u00ae Core\u2122i7-10870H CPU $\\textcircled{\\omega}2.20\\mathrm{GHz}\\times16$ GB RAM. $\\ddagger\\mathrm{GPU}$ Specs: NVIDIA GeForce RTX 3070 Laptop GPU (CUDA cores $=5120\\$ . "], "page_idx": 21}, {"type": "text", "text": "A.10 Visualization of Learned Attention ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In fig. 6, we present additional samples of learned spatial-temporal features. For FactorizePhys trained with FSAM, we can observe superior cosine similarity and more relevant spatial distribution specifically under challenging scenarios with occlusions such as arising from hairs, eye-glasses and beard. ", "page_idx": 21}, {"type": "text", "text": "A.11 Qualitative Comparison with Estimated rPPG Signals ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Qualitative comparison of the estimated rPPG signals between the proposed method and the best performing SOTA method (i.e., EfficientPhys [37] is presented for different test datasets - iBVP [29] (fig. 7) , PURE [55] (fig. 8), and UBFC-rPPG [2] (fig. 9). ", "page_idx": 21}, {"type": "text", "text": "A.12 Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We intend to release our rPPG estimation code only for academic purposes, with Responsible AI license (RAIL). Research areas that will benefit directly from this work include human-computer interaction and contactless health tracking or vital signs monitoring. Although the methods presented in this work may potentially benefit certain clinical scenarios, thorough validation studies, with appropriate ethics approval, are required to critically assess performance in such settings. ", "page_idx": 21}, {"type": "text", "text": "In addition, in some recent work, rPPG methods have been indicated as effective in detecting deepfake videos. In this context, we would like to caution such a use, considering the main results presented for the models trained using the SCAMPS [42] dataset, consisting of synthesized avatars. We argue that the rPPG signal can be embedded in the synthesized (or deep-fake) videos, with a similar approach as used for generating the SCAMPS [42] dataset. In such scenarios, in spite of high accuracy in estimating rPPG signals, such methods can be fooled by the synthesized videos that embed BVP signals. Therefore, we highlight that it is necessary to use the rPPG signal estimation methods in this context with great caution. ", "page_idx": 21}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/9041fb63ff21577fc649d342d20e9a9f67f6a359b4360856a4915135cb0c9c1b.jpg", "img_caption": ["Figure 6: Visualization of Learned Spatial-Temporal Features "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/eeda850f431aad6a110123bc58c09eb82fbd61d3b5f53d3634cb04d251087b0a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 7: Comparison of Estimated rPPG Signals on iBVP Dataset for Models Trained with PURE, SCAMPS and UBFC-rPPG Datasets ", "page_idx": 23}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/7f409e4782220c76a064a98d04edea7d5febe7914ef266c5ec0139f2339b01e4.jpg", "img_caption": ["Figure 8: Comparison of Estimated rPPG Signals on PURE Dataset for Models Trained with iBVP, SCAMPS and UBFC-rPPG Datasets "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "qrfp4eeZ47/tmp/939c2a15082724ff759500664647f2b00fc2865011c7a5edd8010e101d5a4b79.jpg", "img_caption": ["Figure 9: Comparison of Estimated rPPG Signals on UBFC-rPPG Dataset for Models Trained with iBVP, PURE and SCAMPS Datasets "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "B NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We make thorough evaluation of the claims and results support the claims, which are appropriately highlighted in the abstract. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: In the conclusion section, we have dedicated a paragraph to mention the broader impact and discuss the limitations and future directions that can be investigated. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This work does not propose a new theory, as it focuses on adapting established foundational algorithm for low-rank recovery using nonnegative matrix factorization into deep learning architecture to jointly compute spatial-temporal attention. We present empirical results of thoroughly conducted experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In sections $\\S4$ , appendix A.1, and appendix A.2, we provide a detailed description necessary to reproduce the main experimental results. In addition, we make our code available at https://github.com/PhysiologicAILab/FactorizePhys. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 27}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Although this paper does not present new data, we provide access to our code at https://github.com/PhysiologicAILab/FactorizePhys, accompanied by comprehensive usage instructions. Our code extends the rPPG-Toolbox [38], a widely embraced tool by the rPPG research community, and includes exhaustive guidelines to allow replication of our main results. Furthermore, we urge researchers who encounter any difficulties in reproducing the results to report them through issue tracker on our repository. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Through $\\S4$ and appendix A.2, we specify data-splits as well as hyperparameters. We provide results from the ablation study in $\\S5$ and appendix A.3 for key hyperparameters that we use in this work. In addition, the code released at https://github.com/PhysiologicAILab/FactorizePhys includes config files used to train and test all models. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We conduct a rigorous assessment of the proposed models by means of an fair comparison with SOTA rPPG methods, and report standard error for all evaluation metrics. This examination is further supplemented with a statistical significance analysis of the principal results, as detailed in table 5 in appendix A. Our evaluation encompasses the most effective SOTA rPPG method and includes a paired T-test result derived from 10 experiments that use distinct random seeds. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide these details in $\\S5$ . ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work is in accordance with the NeurIPS code of ethics. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Although we briefly highlight the potential positive impacts in $\\S6$ , to prevent any negative societal impact, we have safeguarded our code with Responsible AI License, specifying the appropriate restrictions. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have mentioned safeguard measures in appendix A.12. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We cite all the code, data and models used in this work. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have prepared ReadMe for the code repository, as well as we have added code comments where applicable to provide the needed documentation. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This work does not involve direct collection of data from human subjects, as it uses existing datasets, which have been cited. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Although this work did not require working with human subjects, we have institutional ethics covered for this research, which is approved by the Ethics Committee of the University College London Interaction Center (ID Number: UCLIC/1920/006/Staff/Cho, Approval date: 20 May 2020). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]