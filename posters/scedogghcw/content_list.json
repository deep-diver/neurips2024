[{"type": "text", "text": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adam Karvonen\u2217 Benjamin Wright\u2217 Can Rager Rico Angell Independent MIT Independent UMass, Amherst Jannik Brinkmann Logan Smith Claudio Mayrink Verdun University of Mannheim Independent Harvard University ", "page_idx": 0}, {"type": "text", "text": "Samuel Marks Northeastern University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features that we expect good SAEs to recover. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on chess and Othello transcripts. These settings carry natural collections of interpretable features\u2014for example, \u201cthere is a knight on F3\u201d\u2014 which we leverage into supervised metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, $p$ -annealing, which improves performance on prior unsupervised metrics as well as our new metrics.2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mechanistic interpretability aims to reverse engineer neural networks into human-understandable components. What, however, should these components be? Recent work has applied Sparse Autoencoders (SAEs) [9, 16], a scalable unsupervised learning method inspired by sparse dictionary learning to find a disentangled representation of language model (LM) internals. However, measuring progress in training SAEs is challenging because we do not know what a gold-standard dictionary would look like, as it is difficult to anticipate which ground-truth features underlie model cognition. Prior work has either attempted to measure SAE quality in toy synthetic settings [57] or relied on various proxies such as sparsity, fidelity of the reconstruction, and LM-assisted autointerpretability [6]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we explore a setting that lies between toy synthetic data (where all ground-truth features are known; cf. Elhage et al. [24]) and natural language: LMs trained on board game transcripts. This setting allows us to formally specify natural categories of interpretable features, e.g., \u201cthere is a knight on e3\u201d or \u201cthe bishop on f5 is pinned.\u201d We leverage this to introduce two novel metrics for how much of a model\u2019s knowledge an SAEs has captured: ", "page_idx": 0}, {"type": "text", "text": "\u2022 Board reconstruction. Can we reconstruct the state of the game board by interpreting each feature as a classifier for some board configuration? ", "page_idx": 1}, {"type": "text", "text": "\u2022 Coverage. Out of a catalog of researcher-specified candidate features, how many of these candidate features actually appear in the SAE? ", "page_idx": 1}, {"type": "text", "text": "These metrics carry the limitation that they are sensitive to researcher preconceptions. Nevertheless, we show that they provide a useful new signal of SAE quality. ", "page_idx": 1}, {"type": "text", "text": "Additionally, we introduce $p$ -annealing, a novel technique for training SAEs. When training an SAE with $p$ -annealing, we use an $L_{p}$ -norm-based sparsity penalty with $p$ ranging from $p=1$ at the beginning of training (corresponding to a convex minimization problem) to some $p<1$ (a non-convex objective) by the end of training. We demonstrate that p-annealing improves over prior methods, giving performance on par with the more compute-intensive Gated SAEs from Rajamanoharan et al. [54], as measured both by prior metrics and our new metrics. ", "page_idx": 1}, {"type": "text", "text": "Overall, our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We train and open-source over 500 SAEs trained on chess and Othello models each.   \n2. We introduce two new metrics for measuring the quality of SAEs.   \n3. We introduce $p$ -annealing, a novel technique for training SAEs that improves on prior techniques. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Language models for Othello and chess ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we make use of LMs trained to autoregressively predict transcripts of chess and Othello games. We emphasize that these transcripts only give lists of moves in a standard notation and do not directly expose the board state. Based on behavioral evidence (the high accuracy of the LMs for predicting legal moves) and prior studies of LM representations [36, 47, 33] we infer that the LMs internally model the board state, making them a good testbed setting for studying LM representations. ", "page_idx": 1}, {"type": "text", "text": "Othello. Othello is a two-player strategy board game played on an 8x8 grid, with players using black and white discs. Players take turns placing discs on the board, capturing their opponent\u2019s discs by bracketing them between their own, causing the captured discs to turn their color. The goal is to have more discs turned to display your color at the end of the game. The game ends if every square on the board is covered or either player cannot make a move. ", "page_idx": 1}, {"type": "text", "text": "In our experiments, we use an 8-layer GPT model with 8 attention heads and a $n=512$ dimensional hidden space, as provided by Li et al. [36]. This model had no prior knowledge of the game or its rules and was trained from scratch on 20 million game transcripts, where each token in the corpus represents a tile on which players place their discs. The game transcripts were synthetically generated by uniformly sampling from the Othello game tree. Thus, the data distribution captures valid move sequences rather than strategic depth. For this model, Li et al. [36] demonstrated the emergence of a world model\u2014an internal representation of the correct board state allowing it to predict the next move\u2014that can be extracted from the model activations using a non-linear probe. Nanda et al. [47] extended this finding, showing that a similar internal representation could be extracted using linear probes, supporting the linear representation hypothesis [46]. ", "page_idx": 1}, {"type": "text", "text": "Chess. Othello makes a natural testbed for studying emergent internal representations since the game tree is far too large to memorize. However, the rules and state are not particularly complex. Therefore, we also consider a language model trained on chess game transcripts with identical architecture, provided by Karvonen [33]. The model again had no prior knowledge of chess and was trained from scratch on 16 million human games from the Lichess chess game database [38]. The input to the model is a string in the Portable Game Notation (PGN) format (e.g., \u201c1. e4 e5 2. Nf3 ...\u201d). The model predicts a legal move in $99.8\\;\\%$ of cases and, similar to Othello, it has an internal representation of the board state that can be extracted from the internal activations using a linear probe [33]. ", "page_idx": 1}, {"type": "image", "img_path": "SCEdoGghcw/tmp/a03ea24edf3f0cda23b8c72dfe56da0f2774dfb42346ae490f99c89c1600710c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: We find SAE features that detect interpretable board state properties (BSP) with high precision (i.e., above 0.95). This figure illustrates three distinct chessboard states, each an example of a BSP associated with a high activation of a particular SAE feature. Left: A board state detector identifies a knight on square f3, owned by the player to move. Middle: A rook threat detector indicates an immediate threat posed by a rook to a queen regardless of location and piece threatened. Right: A pin detector recognizes moves that resolve a check on a diagonal by creating a pin, again, regardless of location and piece pinned. ", "page_idx": 2}, {"type": "text", "text": "2.2 Sparse autoencoders ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a dataset $\\mathcal{D}$ of vectors $\\mathbf{x}\\in\\mathbb{R}^{d}$ , a sparse autoencoder (SAE) is trained to produce an approximation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}\\approx\\sum_{i}^{d_{\\mathrm{SAE}}}f_{i}(\\mathbf{x})\\mathbf{d}_{i}+\\mathbf{b}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "as a sparse linear combination of features. Here, the feature vectors $\\mathbf{d}_{i}\\in\\mathbb{R}^{d}$ are unit vectors, the feature activations $f_{i}(\\mathbf{x})\\geq0$ are a sparse set of coefficients, and $\\mathbf{b}\\in\\mathbb{R}^{d}$ is a bias term. Concretely, an SAE is a neural network with an encoder-decoder architecture, where the encoder maps $\\mathbf{x}$ to the vector ${\\bf f}=\\left[f_{1}({\\bf x})\\right]$ . . . $f_{d_{\\mathrm{SAE}}}(\\mathbf{x})]$ of feature activations, and the decoder maps f to an approximate reconstruction of $\\mathbf{x}$ . ", "page_idx": 2}, {"type": "text", "text": "In this paper, we train SAEs on datasets consisting of activations extracted from the residual stream after the sixth layer for both the chess and Othello models. At these layers, linear probes trained with logistic regression were accurate for classifying a variety of properties of the game board [33, 47]. For training SAEs, we employ a variety of SAE architectures and training algorithms, as detailed in Section 4. ", "page_idx": 2}, {"type": "text", "text": "3 Measuring autoencoder quality for chess and Othello models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Many of the features learned by our SAEs reflect uninteresting, surface-level properties of the input, such as the presence of certain tokens. However, upon inspection, we additionally find many SAE features which seem to reflect a latent model of the board state, e.g., features that reflect the presence of certain pieces on particular squares, squares that are legal to play on, and strategy-relevant properties like the presence of a pin in chess (Figures 1 and 6). ", "page_idx": 2}, {"type": "text", "text": "Fortunately, in the setting of board games, we can formally specify certain classes of these interesting features, allowing us to more rigorously detect them and use them to understand our SAEs. In Section 3.1, we specify certain classes of interesting game board properties. Then, in Section 3.2, we leverage these classes into two metrics of SAE quality. ", "page_idx": 2}, {"type": "text", "text": "3.1 Board state properties in chess and Othello models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We formalize a board state property (BSP) to be a function $g:\\{{\\mathrm{game~board}}\\}\\rightarrow\\{0,1\\}$ . In this work, we will consider the following interpretable classes of BSPs: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Gboard state contains BSPs which classify the presence of a piece at a specific board square, where the board consists of $8\\times8$ squares in both games. For chess, we consider the full board for the twelve distinct piece types (white king, white queen, ..., black king), giving a total of $8\\times8\\times12$ BSPs. For Othello, we consider the full board for the two distinct piece types (black and white), yielding $8\\times8\\times2$ BSPs. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "\u2022 $\\mathcal{G}_{\\mathrm{strategy}}$ consists of BSPs relevant for predicting legal moves and playing strategically in chess, such as a pin detector. They were selected by the authors based on domain knowledge and prior interpretability work on the chess model AlphaZero [45]. We provide a full list of strategy BSPs in Table 3 in the Appendix. Because our Othello model was trained to play random legal moves, we do not consider strategy BSPs for Othello. ", "page_idx": 3}, {"type": "text", "text": "3.2 Measuring SAE quality with board state properties ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce two metrics of SAE quality: coverage and board reconstruction. ", "page_idx": 3}, {"type": "text", "text": "Coverage. Given a collection $\\mathcal{G}$ of BSPs, our coverage metric quantifies the extent to which an SAE has identified features that coincide with the BSPs in $\\mathcal{G}$ . In more detail, suppose that $f_{i}$ is an SAE feature and $t\\in[0,1]$ is a threshold, we define the function ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{f_{i},t}(\\mathbf{x})=\\mathbb{I}\\left[f_{i}(\\mathbf{x})>t\\cdot f_{i}^{\\operatorname*{max}}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{i}^{\\mathrm{max}}$ is (an empirical estimate of) $\\operatorname*{max}_{\\mathbf{x}\\sim\\mathcal{D}}{f_{i}(\\mathbf{x})}$ , the maximum value that $f_{i}$ takes over the dataset $\\mathcal{D}$ of activations extracted from our model, and $\\mathbb{I}$ is the indicator function. We interpret $\\phi_{f_{i},t}$ as a binary classifier; intuitively, it corresponds to binarizing the activations of $f_{i}$ into \u201con\u201d vs. \u201coff\u201d at some fraction $t$ of the maximum value of $f_{i}$ on $\\mathcal{D}$ . Given some BSP $g\\in{\\mathcal{G}}$ , let $F_{1}(\\phi_{f_{i},t};g)\\in[0,1]$ denote the F1-score for $\\phi_{f_{i},t}$ classifying $g$ . Then we define the coverage of an SAE with features $\\{f_{i}\\}$ relative to a set of BSPs $\\mathcal{G}$ to be ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Cov}(\\{f_{i}\\},\\mathcal{G}):=\\frac{1}{|\\mathcal{G}|}\\sum_{g\\in\\mathcal{G}}\\operatorname*{max}_{t}\\operatorname*{max}_{f_{i}}F_{1}(\\phi_{f_{i},t};g).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, we take, for each $g\\in{\\mathcal{G}}$ , the $F_{1}$ -score of the feature that best serves as a classifier for $g$ , and then take the mean of these maximal $F_{1}$ -scores. An SAE receives a coverage score of 1 if, for each BSP $g\\in{\\mathcal{G}}$ , it has some feature that is a perfect classifier for $g$ . Since Cov depends on the choice of threshold $t$ , we sweep over $t\\in\\{0,0.1,\\bar{0}.2,\\ldots,0.9\\}$ and take the best coverage score; typically this best $t$ is in $\\{0,0.1,0.2\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Board reconstruction. Again, let $\\mathcal{G}$ be a set of BSPs. Intuitively, the idea of our board reconstruction metric is that, for a sufficiently good SAE, there should be a simple, human-interpretable way to recover the state of the board from the proflie of feature activations $\\{f_{i}(\\mathbf{x})\\}$ on an activation $\\mathbf{x}\\in\\mathbb{R}^{d}$ . Here, the activation $\\mathbf{x}$ was extracted after the post-MLP residual connection in layer 6. ", "page_idx": 3}, {"type": "text", "text": "We will base our board reconstruction metric around the following human-interpretable way of recovering a board state from a feature activation profile; we emphasize that different ways of recovering boards from feature activations may lead to qualitatively different results. This recovery rule is based on the assumption that interpretable SAE features tend to be high precision for some subset of BSPs, in line with Templeton et al. [58]. For example, features that classify common configurations of pieces are high precision (but not necessarily high recall) for multiple BSPs. We use a consistent dataset of 1000 games as our training set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ for identifying high-precision features across all Board State Properties (BSPs). An additional, separate set of 1000 games serves as our test set $\\mathcal{D}_{\\mathrm{test}}$ . Using the training set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ , we identify, for each SAE feature $f_{i}$ , all of the BSPs $g\\in{\\mathcal{G}}$ for which $\\phi_{f_{i},t}$ is a high precision (of at least 0.95) classifier. Then for each $g\\in{\\mathcal{G}}$ our prediction rule is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{P}_{g}(\\{f_{i}(\\mathbf{x})\\})=\\left\\{\\begin{array}{l l}{1,}&{\\mathrm{if}~\\phi_{f_{i},t}(\\mathbf{x})=1~\\mathrm{for}~\\mathrm{any}~f_{i}~\\mathrm{which}}\\\\ &{~~\\mathrm{is~high}~\\mathrm{precision}~\\mathrm{for}~g~\\mathrm{on}~\\mathcal{D}_{\\mathrm{train}}}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $F_{1}(\\mathbf{\\mathcal{P}}(\\{f_{i}(\\mathbf{x})\\});\\mathbf{b})$ denote the $F_{1}$ -score for a given board state $\\mathbf{b}$ , where ${\\mathcal P}(\\{f_{i}({\\bf x})\\})\\;=$ $\\{\\mathcal{P}_{g}(\\{f_{i}(\\mathbf{x})\\})\\}_{g\\in\\mathcal{G}}$ represents the full predicted board (containing predictions for all 64 squares) obtained from the SAE activations.3 ", "page_idx": 3}, {"type": "text", "text": "Then, the average $F_{1}$ -score over all board states in the test dataset $\\mathcal{D}_{\\mathrm{test}}$ can be calculated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{Rec}(\\{x_{i}\\},\\mathcal{D}_{\\mathrm{test}})=\\frac{1}{|\\mathcal{D}_{\\mathrm{test}}|}\\sum_{\\mathbf{x}\\in\\mathcal{D}_{\\mathrm{test}}}\\operatorname*{max}_{t}F_{1}(\\mathbf{\\mathcal{P}}(\\{f_{i}(\\mathbf{x})\\});\\mathbf{b}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4 Training methodologies for SAEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In our experiments, we investigate four methods for training SAEs, as explained in this section. These are given by two autoencoder architectures and two training methodologies\u2014one with $p$ -annealing and one without $p$ -annealing\u2014for each architecture. Our SAEs are available at https://huggingface.co/adamkarvonen/chess_saes/tree/main (chess) and https:// huggingface.co/adamkarvonen/othello_saes/tree/main (Othello). ", "page_idx": 4}, {"type": "text", "text": "4.1 Standard SAEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $n$ be the dimension of the model\u2019s residual stream activations that are input to the autoencoder, $m$ the autoencoder hidden dimension, and $s$ the dataset size. Our baseline \u201cstandard\u201d SAE architecture, as introduced in Bricken et al. [9] is defined by encoder weights $W_{e}\\,\\in\\,\\mathbb{R}^{m\\times n}$ , decoder weights $W_{d}\\in\\mathbb{R}^{n\\times m}$ with columns constrained to have a $L_{2}$ -norm of 1, and biases $b_{e}\\in\\mathbb{R}^{m}$ , $b_{d}\\in\\mathbb{R}^{n}$ . Given an input $\\mathbf{x}\\in\\mathbb{R}^{n}$ , the SAE computes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\bf f}({\\bf x})=\\mathrm{ReLU}(W_{e}({\\bf x}-{\\bf b}_{d})+{\\bf b}_{e})}\\\\ {\\hat{{\\bf x}}=W_{d}\\,{\\bf f}({\\bf x})+{\\bf b}_{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{f}\\left(\\mathbf{x}\\right)$ is the vector of feature activations, and $\\hat{\\bf x}$ is the reconstruction. For a standard SAE, our baseline training method is as implemented in the open-source dictionary_learning repository [43], optimizing the loss ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{standard}}=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathrm{train}}}\\Big[\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|_{2}+\\lambda\\|\\mathbf{f}(\\mathbf{x})\\|_{1}\\Big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some hyperparameter $\\lambda>0$ controlling sparsity. ", "page_idx": 4}, {"type": "text", "text": "4.2 Gated SAEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The $L_{1}$ penalty used in the original training method encourages feature activations to be smaller than they would be for optimal reconstruction [62]. To address this, Rajamanoharan et al. [54] introduced a modification to the original SAE architecture that separates the selection of dictionary elements to use in a reconstruction and estimating the coefficients of these dictionary elements. This results in the following gated architecture: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{gate}}(\\mathbf{x}):=W_{\\mathrm{gate}}(\\mathbf{x}-\\mathbf{b}_{d})+\\mathbf{b}_{\\mathrm{gate}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{f}}(\\mathbf{x}):=\\mathbb{I}\\left[\\pi_{\\mathrm{gate}}(\\mathbf{x})>0\\right]\\odot\\mathrm{ReLU}(W_{\\mathrm{mag}}(\\mathbf{x}-\\mathbf{b}_{d})+\\mathbf{b}_{\\mathrm{mag}})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{x}(\\tilde{\\mathbf{f}}(\\mathbf{x}))=W_{d}\\tilde{\\mathbf{f}}(\\mathbf{x})+\\mathbf{b}_{d}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{I}[\\cdot>0]$ is the Heaviside step function and $\\odot$ denotes elementwise multiplication. Then, the loss function uses $\\hat{x}_{\\mathrm{frozen}}$ , a frozen copy of the decoder: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{gated}}:=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathrm{train}}}\\Big[\\|\\mathbf{x}-\\hat{x}(\\widetilde{\\mathbf{f}}(\\mathbf{x}))\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\lambda\\|\\mathrm{ReLU}(\\pi_{\\mathrm{gate}}(\\mathbf{x}))\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\|\\mathbf{x}-\\hat{x}_{\\mathrm{frozen}}(\\mathrm{ReLU}(\\pi_{\\mathrm{gate}}(\\mathbf{x})))\\right\\|_{2}^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.3 $p$ -Annealing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Fundamentally, an $L_{1}$ penalty has been used to induce sparsity in SAE features because it serves as a convex relaxation of the true sparsity measure, the $L_{0}$ -norm. The $L_{1}$ -norm is the convex hull of the $L_{0}$ -norm, making it a tractable alternative for promoting sparsity [63]. However, the proxy loss function is not the same as directly optimizing for sparsity, leading to issues such as feature shrinkage [62] and potentially less sparse learned features. Unfortunately, the $L_{0}$ -norm is non-differentiable and directly minimizing it is an NP-hard problem [48, 18], rendering it impractical for training. ", "page_idx": 4}, {"type": "image", "img_path": "SCEdoGghcw/tmp/bbf17996bb60cac4da08c4af215fa8418ecd2d888f782d2e459da490a91a698a.jpg", "img_caption": ["Figure 2: Comparison of the coverage and board reconstruction metrics for chess SAE quality on Gboard state. The coverage score reports the mean F1 scores over BSPs. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. $L_{0}$ , with the scheme color corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes. ", "(c) Board Reconstruction ", "(d) Board Reconstruction vs. $L_{0}$ "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In this work, we propose the use of nonconvex $L_{p}^{p}$ -minimization, with $p<1$ , as an alternative to the standard $L_{1}$ minimization in sparse autoencoders (SAEs). This approach has been successfully employed in compressive sensing and sparse recovery to achieve even sparser representations [11, 61, 64, 60]. To perform this optimization, we introduce a method called $p$ -annealing for training SAEs, based on the compressive sensing technique called $p$ -continuation [66]. The key idea is to start with convex $L_{1}$ -minimization through setting $p=1$ and progressively decrease the value of $p$ during training, resulting in closer approximations of the true sparsity measure, $L_{0}$ , as $p$ approaches 0. We define the sparsity penalty for each batch $x$ as a function of the current training step $s$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{sparse}}(\\mathbf{x},s)=\\lambda_{s}\\|\\mathbf{f}(\\mathbf{x})\\|_{p_{s}}^{p_{s}}=\\lambda_{s}\\sum_{i}f_{i}(\\mathbf{x})^{p_{s}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In other words, the sparsity penalty will be a scaled $L_{p}^{p}$ norm of the SAE feature activations, with $p$ decreasing over time. At $p=1$ , the $L_{p}^{p}$ norm is equal to the $L_{1}$ norm, and as $p\\rightarrow0$ , the $L_{p}^{p}$ norm limits to the $L_{0}$ -norm, as $\\begin{array}{r}{\\operatorname*{lim}_{p\\to0}\\sum_{i}\\bar{f_{i}}(\\mathbf x)^{p}=\\sum_{i}f_{i}(\\mathbf x)^{0}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The purpose of annealing $p$ from $1\\rightarrow0$ instead of starting from a fixed, low value for $p$ is that the lower the $p$ , the more concave (non-convex) the $L_{p}^{p}$ norm is, increasing the likelihood of the training process getting stuck in local optima, which we have observed in initial experiments. Therefore, we aim to first arrive at a region of an optimum using the easier-to-train $L_{1}$ penalty and then gradually shift the loss function. This manifests as keeping $p=1$ for a certain number of steps and then starting decreasing $p$ linearly down to $p_{\\mathrm{end}}>0$ at the end of training. We set $p_{\\mathrm{end}}=0.2$ . ", "page_idx": 5}, {"type": "text", "text": "Coefficient Annealing. Changing the value of $p$ changes the scale of the $L_{p}^{p}$ norm. Without also adapting the coefficient $\\lambda$ , the strength of the sparsity penalty would vary too wildly across training. Empirically, we found that keeping a constant $\\lambda$ would lead to far too weak of a sparsity penalty for the larger $p$ \u2019s at the start of training, making the process worse than simply training with a constant $p$ from the beginning. Consequently, we aim to adapt the coefficient $\\lambda$ such that the strength of the sparsity penalty is not changed significantly due to $p$ updates. Formally, the update step is: ", "page_idx": 5}, {"type": "image", "img_path": "SCEdoGghcw/tmp/68f0a3e78b111b409dd0e8b81fc27410dbf32a1c4e91cc530e8487338f9274f5.jpg", "img_caption": ["Figure 3: Comparison of the coverage and board reconstruction metrics for chess SAE quality on $\\mathcal{G}_{\\mathrm{strategy}}$ . The metrics represent the average coverage and board reconstruction obtained across all BSPs in $\\mathcal{G}_{\\mathrm{strategy}}$ . The coverage score reports the mean of maximal F1 scores over $\\mathrm{BSPs}$ . The absolute coverage scores vary significantly between strategy BSPs, as discussed in Appendix D. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. $L_{0}$ , with the color scheme corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes. ", ""], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda_{s+1}\\leftarrow\\lambda_{s}\\frac{\\sum_{j=s-q+1}^{s}\\sum_{i}f_{i}(\\mathbf{x_{j}})^{p_{s}}}{\\sum_{j=s-q+1}^{s}\\sum_{i}f_{i}(\\mathbf{x_{j}})^{p_{s+1}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We keep a queue of the most recent $q$ batches of feature activations mid-training and use them to calibrate the $\\lambda_{s}$ updates. Therefore, the strength of the sparsity penalty is kept locally constant. ", "page_idx": 6}, {"type": "text", "text": "Combining $p$ -annealing with other SAEs. Since the $p$ -annealing method only modifies the $L_{1}$ terms in the loss function without affecting the SAE architecture, it is simple to combine $p$ -annealing with other SAE modifications. This allows us to create the Gated-Annealed SAE method by combining the Gated SAE architecture and $p$ -annealing. Concretely, we modify $\\mathcal{L}_{\\mathrm{gated}}$ (Equation 12) by replacing the sparsity term $\\lambda\\|\\mathrm{ReLU}\\big(\\pi_{\\mathrm{gate}}(\\mathbf{x})\\big)\\|_{1}$ in with $\\lambda_{s}\\|\\mathrm{ReLU}\\big(\\pi_{\\mathrm{gate}}(\\mathbf{x})\\big)\\|_{p_{s}}^{p_{s}}$ . Our experiments showed that the optimum values for coefficients $\\lambda$ and $\\lambda_{s}$ differ. ", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we explore the performance of SAEs applied to language models trained on Othello and chess. Consistent with Nanda et al. [47], we find that interpretable SAE features typically track properties relative to the player whose turn it is (e.g. \u201cmy king is pinned\u201d rather than \u201cthe white king is pinned\u201d). To side-step subtleties arising from this, we only extract our activations from the token immediately preceding white\u2019s move. Specifically, we consider SAEs trained on the residual stream activations after the sixth layer using the four methods from Section 4 (see Table 2 for additional hyperparameters). In addition to our metrics introduced above, we also make use of unsupervised metrics previously appearing in the literature [9, 16, 54]: ", "page_idx": 6}, {"type": "table", "img_path": "SCEdoGghcw/tmp/5d09fdb3e837831bd7cae0ad244c3163acd5573d9a02dd760971b040d83cc826.jpg", "table_caption": [], "table_footnote": ["Table 1: Best performance obtained for different techniques across games for $\\mathcal{G}_{\\mathrm{board\\;state}}$ . As a baseline, we train an SAE on random GPT, a version of the trained GPT model with randomly initialized weights. All models were trained on activations after the post-MLP residual connection in layer 6. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u2022 $L_{0}$ measures the average number of active SAE active features (i.e., positive activation) on a given input. \u2022 Loss recovered measures the change in model performance when replacing activations with the corresponding SAE reconstruction during a forward pass. This metric is quantified as $\\left(H_{*}\\right.-$ $H_{0})/(\\bar{H}_{\\mathrm{orig}}-H_{0})$ , where $H_{\\mathrm{orig}}$ is the cross-entropy loss of the board game model for next-token prediction, $H_{*}$ is the cross-entropy loss after substituting the model activation $\\mathbf{x}$ with its SAE reconstruction during the forward pass, and $H_{0}$ is the cross-entropy loss when zero-ablating $\\mathbf{x}$ . ", "page_idx": 7}, {"type": "text", "text": "Our key takeaways are as follows. ", "page_idx": 7}, {"type": "text", "text": "SAE features can accurately reconstruct game boards. In general, we find that SAE features are effective at capturing board state information in both Othello and chess (see Table 1, Figure 2d and 4d). In contrast, SAEs trained on a model with random weights perform very poorly according to our metrics, showing that SAE performance is driven by identifying structure in the models\u2019 learned representation of game boards. Nonetheless, SAEs do not match the performance of linear probes in terms of reconstructing the board state. This performance gap suggests that SAEs do not capture all of the information encoded in the model\u2019s internal representations. ", "page_idx": 7}, {"type": "text", "text": "Standard SAEs trained with $p$ -annealing perform on par with Gated SAEs. We find that standard SAEs trained using $p$ -annealing consistently perform better than those trained with a constant $L_{1}$ penalty (Equation 8), as measured by existing proxy metrics and in terms of improvement in coverage (see Figure 2a and 4a). In fact, standard SAEs trained using $p$ -annealing show a coverage score that is comparable to Gated SAEs trained without $p$ -annealing. Further, we find that both $p$ -annealing and Gated SAEs significantly outperform Standard SAEs in addressing the shrinkage problem [62], as detailed in Appendix E. However, we find cases where our coverage metric disagrees with existing metrics. In Figure 2, for example, Gated SAEs perform achieve a higher loss recovered score than Standard SAEs trained using $p$ -annealing. We emphasize that the training and inference of Gated SAEs is more computationally expensive, requiring $50\\%$ more compute per forward pass compared to Standard SAEs [54]. ", "page_idx": 7}, {"type": "text", "text": "Coverage and board reconstruction reveal differences in SAE quality not captured by unsupervised metrics. Our metrics reveal improvements in SAE performance that traditional proxy metrics fail to capture. For example, we trained SAEs with hidden dimensions 4096 and 8192 (expansion factors of 8 and 16, respectively). We expect the SAEs with 8192 hidden dimensions to perform better since they have greater capacity. However, we observe that they perform equally well according to prior unsupervised metrics (see Figures 2 a, c and 4 a, c). In contrast, our metrics reveal that SAEs with larger hidden dimensions are better. For the Standard architecture, this is reflected by the parallel lines (of purple diamonds) in Figures $^{2\\,\\mathfrak{b}}$ , d and $^{4\\,\\mathrm{b}}$ , d. Thus, our metrics are able to capture improvements from larger expansion factors. In addition, we find that the performance of $p$ -annealing closely resembles that of Gated SAEs when evaluated using standard proxy metrics; it demonstrates clear improvements under our proposed metrics. ", "page_idx": 7}, {"type": "image", "img_path": "SCEdoGghcw/tmp/09dc4bd544168aba46b9a353b8951f9833176ab75dbadbbaf117cf8acbc036d1.jpg", "img_caption": ["(c) Board Reconstruction ", "(d) Board Reconstruction vs. $L_{0}$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Comparison of the coverage and board reconstruction metrics for Othello SAE quality on $\\mathcal{G}_{\\mathrm{board\\;state}}$ . The coverage score reports the mean of maximal F1 scores over BSPs. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. $L_{0}$ , with the color scheme corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes. ", "page_idx": 8}, {"type": "text", "text": "Coverage and board reconstruction are consistent with existing metrics. Figures 2, 4, and 3 demonstrate that both coverage and board reconstruction metrics are optimal in the elbow region of the Pareto frontier. This region, where SAEs reconstruct internal activations efficiently with minimal features, also yielded the most coherent interpretations during our manual inspections. This provides precise, empirical validation to the common wisdom that SAEs in this region of the Pareto frontier are the best. ", "page_idx": 8}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The proposed metrics for board reconstruction and coverage provide a more objective evaluation of SAE quality than previous subjective methods. Nevertheless, these metrics exhibit several limitations. Primarily, their applicability is confined to the chess and Othello domains, raising concerns about their generalizability to other domains or different models. Additionally, the set of BSPs that underpin these metrics is determined by researchers based on their domain knowledge. This approach may not encompass all pertinent features or strategic concepts, thus potentially overlooking essential aspects of model evaluation. Developing comparable objective metrics for other domains, such as natural language processing, remains a significant challenge. Moreover, our current focus is on evaluating the quality of SAEs in terms of their ability to capture internal representations of the model. However, this does not directly address how these learned features could be utilized for downstream interpretability tasks. ", "page_idx": 8}, {"type": "text", "text": "7 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sparse dictionary learning. Since the nineties, dictionary learning [22, 20], sparse regression [26], and later, sparse autoencoders [49] have been extensively studied in the machine learning and signal processing literature. The seminal work of Olshausen and Field [51] introduced the concept of sparse coding in neuroscience (see also [52], building upon the earlier concept of sparse representations [19] and matching pursuit [42]. Subsequently, a series of works established the theoretical and algorithmic foundations of sparse dictionary learning [23, 30, 21, 1, 65, 32, 59, 2, 5, 7, 10]. Notably, Gregor and LeCun [28] introduced LISTA, an unrolled version of ISTA [17] that learns the dictionary instead of having it fixed. ", "page_idx": 9}, {"type": "text", "text": "In parallel, autoencoders were introduced in machine learning to automatically learn data features and perform dimensionality reduction [29, 37]. Inspired by sparse dictionary learning, sparse autoencoders [49, 14, 13, 41, 35] were proposed as an unsupervised learning model to build deep sparse hierarchical models of data, assuming a certain degree of sparsity in the hidden layer activations. Later, Luo et al. [39] generalized sparse autoencoders (SAEs) to convolutional SAEs. Although the theory for SAEs is less developed than that of dictionary learning with a fixed dictionary, some progress has been made in quantifying whether autoencoders can, indeed, do sparse coding, e.g., Arpit et al. [4], Rangamani et al. [55], Nguyen et al. [50]. ", "page_idx": 9}, {"type": "text", "text": "Feature disentanglement using sparse autoencoders. The individual computational units of neural networks are often polysemantic, i.e., they respond to multiple seemingly unrelated inputs [3]. Elhage et al. [24] investigated this phenomenon and suggested that neural networks represent features in linear superposition, which allows them to represent more features than they have dimensions. Thus, in an internal representation of dimension $n$ , a model can encode $m\\gg n$ concepts as linear directions [53], such that only a sparse subset of concepts are active across all inputs \u2013 a concept deeply related to the coherence of vectors [26] and to frame theory in general [12]. To identify these concepts, Sharkey et al. [57] used SAEs to perform dictionary learning on a one-layer transformer, identifying a large (overcomplete) basis of features. Cunningham et al. [16] applied SAEs to language models and demonstrated that dictionary features can be used to localize and edit model behavior. Marks et al. [44] proposed a scalable method to discover sparse feature circuits, as opposed to circuits consisting of polysemantic model components, and demonstrated that a human could change the generalization of a classifier by editing its feature circuit. Recently, Kissane et al. [34] explored autoencoders for attention layer outputs. These works have benefited from a variety of open-source libraries for training SAEs for LLM interpretability [43, 8, 15]. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Most SAE research has relied on proxy metrics such as loss recovered and $L_{0}$ , or subjective manual evaluation of interpretability by examining top activations. However, proxy measures only serve as an estimate of interpretability, monosemantic nature, and comprehensiveness of the learned features, while manual evaluations depend on the researcher\u2019s domain knowledge and tend to be inconsistent. ", "page_idx": 9}, {"type": "text", "text": "Our work provides a new, more objective paradigm for evaluating the quality of an SAE methodology; coverage serves as a quantifiable measure of monosemanticity and quality of feature extraction, while board reconstruction serves as a quantifiable measure of the extent to which an SAE is exhaustively representing the information contained within the language model. Therefore, the optimal SAE methodology can be judged by whether it yields both high coverage and high board reconstruction. ", "page_idx": 9}, {"type": "text", "text": "Finally, we propose the $p$ -annealing method, a modification to the SAE training paradigm that can be combined with other SAE methodologies and results in an improvement in both coverage and board reconstruction over the Standard SAE architecture. ", "page_idx": 9}, {"type": "text", "text": "Author Contributions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A.K. built and maintained our infrastructure for working with board-game models. A.K., S.M., C.R., J.B., and L.S. designed the proposed metrics. B.W. performed initial experiments demonstrating the benefits of training SAEs with $p<1$ . B.W., C.M.V., and S.M. then proposed $p$ -annealing, with B.W. leading the implementation and developing coefficient annealing. The basic framework for our dictionary learning work was built and maintained by S.M. and C.R. The training algorithms studied were implemented by S.M., C.R., B.W., R.A., and J.B. R.A. trained the SAEs used in our experiments. A.K., C.R., and J.B. selected and implemented the BSPs. A.K. and J.B. trained the linear probes. Many of the authors (including L.S., J.B., R.A.) did experiments applying traditional dictionary learning methods and exploring both toy problems and natural language settings, which helped build valuable intuition. The manuscript was primarily drafted by A.K., B.W., C.R., R.A., J.B., C.M.V., and S.M., with extensive feedback and editing from all authors. D.B. suggested the original project idea. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "C.R. is supported by Manifund Regrants and AISST. L.R. is supported by the Long Term Future Fund. S.M. is supported by an Open Philanthropy alignment grant. ", "page_idx": 10}, {"type": "text", "text": "The work reported here was performed in part by the University of Massachusetts Amherst Center for Data Science and the Center for Intelligent Information Retrieval, and in part using high performance computing equipment obtained under a grant from the Collaborative R&D Fund managed by the Massachusetts Technology Collaborative. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):4311\u20134322, 2006.   \n[2] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In Conference on learning theory, pages 113\u2013149. PMLR, 2015.   \n[3] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6:483\u2013495, 2018. doi: 10.1162/tacl_a_00034. URL https: //aclanthology.org/Q18-1034.   \n[4] Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju. Why regularized autoencoders learn sparse representation? In International Conference on Machine Learning, pages 136\u2013144. PMLR, 2016.   \n[5] Chenglong Bao, Hui Ji, Yuhui Quan, and Zuowei Shen. Dictionary learning for sparse coding: Algorithms and convergence analysis. IEEE transactions on pattern analysis and machine intelligence, 38(7):1356\u20131369, 2015.   \n[6] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuronexplainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.   \n[7] Jaroslaw Blasiok and Jelani Nelson. An improved analysis of the er-spud dictionary learning algorithm. In 43rd International Colloquium on Automata, Languages, and Programming (ICALP 2016). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.   \n[8] Joseph Bloom and David Chanin. Sae lens. https://github.com/jbloomAus/SAELens, 2024.   \n[9] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, page 2, 2023.   \n[10] Matthew Chalk, Olivier Marre, and Ga\u0161per Tka\u02c7cik. Toward a unified theory of efficient, predictive, and sparse coding. Proceedings of the National Academy of Sciences, 115(1): 186\u2013191, 2018.   \n[11] Rick Chartrand. Exact reconstruction of sparse signals via nonconvex minimization. IEEE Signal Processing Letters, 14(10):707\u2013710, 2007.   \n[12] Ole Christensen et al. An introduction to frames and Riesz bases, volume 7. Springer, 2003.   \n[13] Adam Coates and Andrew $\\textrm{Y N g}$ . The importance of encoding versus training with sparse coding and vector quantization. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 921\u2013928, 2011.   \n[14] Adam Coates, Andrew $\\mathrm{Ng}$ , and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.   \n[15] Alan Cooney. Sparse autoencoder library. https://github.com/ai-safety-foundation/ sparse_autoencoder, 2023.   \n[16] Hoagy Cunningham, Aidan Ewart, Logan Riggs Smith, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, 2023.   \n[17] Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 57(11): 1413\u20131457, 2004.   \n[18] Geoff Davis, Stephane Mallat, and Marco Avellaneda. Adaptive greedy approximations. Constructive approximation, 13:57\u201398, 1997.   \n[19] David L Donoho. Superresolution via sparsity constraints. SIAM journal on mathematical analysis, 23(5):1309\u20131331, 1992.   \n[20] Bogdan Dumitrescu and Paul Irofti. Dictionary learning algorithms and applications. Springer, 2018.   \n[21] Julian Eggert and Edgar Korner. Sparse coding and nmf. In 2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541), volume 4, pages 2529\u20132533. IEEE, 2004.   \n[22] Michael Elad. Sparse and redundant representations: from theory to applications in signal and image processing. Springer Science & Business Media, 2010.   \n[23] Michael Elad and Alfred M Bruckstein. A generalized uncertainty principle and sparse representation in pairs of bases. IEEE Transactions on Information Theory, 48(9):2558\u20132567, 2002.   \n[24] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022.   \n[25] Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R Costa-juss\u00e0. A primer on the inner workings of transformer-based language models. arXiv preprint arXiv:2405.00208, 2024.   \n[26] Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing. Springer New York, New York, NY, 2013.   \n[27] Leo Gao, Tom Dupr\u00e9 la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024.   \n[28] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th international conference on international conference on machine learning, pages 399\u2013406, 2010.   \n[29] Geoffrey E Hinton and Richard Zemel. Autoencoders, minimum description length and helmholtz free energy. Advances in neural information processing systems, 6, 1993.   \n[30] Patrik O Hoyer. Non-negative sparse coding. In Proceedings of the 12th IEEE workshop on neural networks for signal processing, pages 557\u2013565. IEEE, 2002.   \n[31] Adam Jermyn, Adly Templeton, Joshua Batson, and Trenton Bricken. Tanh penalty in dictionary learning, 2024. URL https://transformer-circuits.pub/2024/feb-update/index. html. Accessed: 2024-05-20.   \n[32] Alexander Jung, Yonina C Eldar, and Norbert G\u00f6rtz. Performance limits of dictionary learning for sparse coding. In 2014 22nd European Signal Processing Conference (EUSIPCO), pages 765\u2013769. IEEE, 2014.   \n[33] Adam Karvonen. Emergent world models and latent variable estimation in chess-playing language models, 2024.   \n[34] Connor Kissane, Robert Krzyzanowski, Joseph Isaac Bloom, Arthur Conmy, and Neel Nanda. Interpreting attention layer outputs with sparse autoencoders. In ICML 2024 Workshop on Mechanistic Interpretability, 2024.   \n[35] Jun Li, Tong Zhang, Wei Luo, Jian Yang, Xiao-Tong Yuan, and Jian Zhang. Sparseness analysis in the pretraining of deep neural networks. IEEE transactions on neural networks and learning systems, 28(6):1425\u20131438, 2016.   \n[36] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In The Eleventh International Conference on Learning Representations, 2023.   \n[37] Pengzhi Li, Yan Pei, and Jianqiang Li. A comprehensive survey on design and application of autoencoder in deep learning. Applied Soft Computing, 138:110176, 2023.   \n[38] Lichess. lichess.org open database, 2024. URL https://database.lichess.org.   \n[39] Wei Luo, Jun Li, Jian Yang, Wei Xu, and Jian Zhang. Convolutional sparse autoencoders for image classification. IEEE transactions on neural networks and learning systems, 29(7): 3289\u20133294, 2017.   \n[40] Aleksandar Makelov, George Lange, and Neel Nanda. Towards principled evaluations of sparse autoencoders for interpretability and control. arXiv preprint arXiv:2405.08366, 2024.   \n[41] Alireza Makhzani and Brendan Frey. K-sparse autoencoders. arXiv preprint arXiv:1312.5663, 2013.   \n[42] St\u00e9phane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing, 41(12):3397\u20133415, 1993.   \n[43] Samuel Marks and Aaron Mueller. dictionary_learning. https://github.com/saprmarks/ dictionary_learning, 2024.   \n[44] Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024.   \n[45] Thomas McGrath, Andrei Kapishnikov, Nenad Toma\u0161ev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik. Acquisition of chess knowledge in alphazero. Proceedings of the National Academy of Sciences, 119(47), November 2022. ISSN 1091-6490. doi: 10.1073/pnas.2206625119. URL http://dx.doi.org/10. 1073/pnas.2206625119.   \n[46] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013.   \n[47] Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 16\u201330, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1. 2. URL https://aclanthology.org/2023.blackboxnlp-1.2.   \n[48] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on computing, 24(2):227\u2013234, 1995.   \n[49] Andrew $\\mathrm{Ng}$ et al. Sparse autoencoder. CS294A Lecture notes, 72(2011):1\u201319, 2011.   \n[50] Thanh V Nguyen, Raymond KW Wong, and Chinmay Hegde. On the dynamics of gradient descent for autoencoders. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2858\u20132867. PMLR, 2019.   \n[51] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607\u2013609, 1996.   \n[52] Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current opinion in neurobiology, 14(4):481\u2013487, 2004.   \n[53] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658, 2023.   \n[54] Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, J\u00e1nos Kram\u00e1r, Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders. arXiv preprint arXiv:2404.16014, 2024.   \n[55] Akshay Rangamani, Anirbit Mukherjee, Amitabh Basu, Ashish Arora, Tejaswini Ganapathi, Sang Chin, and Trac D Tran. Sparse coding and autoencoders. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 36\u201340. IEEE, 2018.   \n[56] Logan Riggs and Jannik Brinkmann. Improving sparse autoencoders by square-rooting l1 and removing lowest activation features, 2024. URL https://www.lesswrong.com/posts/ YiGs8qJ8aNBgwt2YN/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest. Accessed: 2024-05-20.   \n[57] Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders, 2023. URL https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/ interim-research-report-taking-features-out-of-superposition. Accessed: 2023-05-10.   \n[58] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/ scaling-monosemanticity/index.html.   \n[59] Andreas M Tillmann. On the computational intractability of exact and approximate dictionary learning. IEEE Signal Processing Letters, 22(1):45\u201349, 2014.   \n[60] Meng Wang, Weiyu Xu, and Ao Tang. On the performance of sparse recovery via $\\ell_{p}$ - minimization ( $[0\\,\\leq\\,p\\,\\leq\\,1]$ ). IEEE Transactions on Information Theory, 57(11):7255\u20137278, 2011.   \n[61] Jinming Wen, Dongfang Li, and Fumin Zhu. Stable recovery of sparse signals via lpminimization. Applied and Computational Harmonic Analysis, 38(1):161\u2013176, 2015.   \n[62] Benjamin Wright and Lee Sharkey. Addressing feature suppression in sparse autoencoders, 2024. URL https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/ addressing-feature-suppression-in-saes. Accessed: 2024-05-20.   \n[63] John Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles, computation, and applications. Cambridge University Press, 2022.   \n[64] Chengzhu Yang, Xinyue Shen, Hongbing Ma, Yuantao Gu, and Hing Cheung So. Sparse recovery conditions and performance bounds for $\\ell_{p}$ -minimization. IEEE Transactions on Signal Processing, 66(19):5014\u20135028, 2018.   \n[65] Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Linear spatial pyramid matching using sparse coding for image classification. In 2009 IEEE Conference on computer vision and pattern recognition, pages 1794\u20131801. IEEE, 2009.   \n[66] Le Zheng, Arian Maleki, Haolei Weng, Xiaodong Wang, and Teng Long. Does $\\ell_{p}$ -minimization outperform $\\ell_{1}$ -minimization? IEEE Transactions on Information Theory, 63(11):6896\u20136935, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Improving and evaluating sparse autoencoders ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Despite the success of SAEs at extracting human-interpretable features, they fail to perfectly reconstruct the activations [16]. One challenge in the training of SAEs with an $L_{1}$ penalty is shrinkage (or \u2019feature suppression\u2019); in addition to encouraging sparsity, an $L_{1}$ penalty encourages feature activations to be smaller than they would be otherwise. Wright and Sharkey [62] approached this problem by fine-tuning the sparse autoencoder without a sparsity penalty. Appendix $\\boldsymbol{\\mathrm E}$ further quantifies shrinkage across a suite of SAEs trained on chess and Othello models. Jermyn et al. [31] and Riggs and Brinkmann [56] explored alternative sparsity penalties to reduce feature suppression during training. Rajamanoharan et al. [54] introduced Gated SAEs, an architectural variation for the encoder which both addresses shrinkage and improves on the Pareto frontier of $L_{0}$ vs reconstruction error. Recently, Gao et al. [27] systematically evaluated the scaling laws with respect to sparsity, autoencoder size, and language model size. ", "page_idx": 15}, {"type": "text", "text": "The goal of dictionary learning in machine learning is to produce human-interpretable features and capture the underlying model\u2019s computations [9]. However, quantitatively measuring interpretability is difficult and often involves manual inspection. Therefore, most existing work assesses the quality of SAEs along different proxy metrics: (1) The cross-entropy loss recovered, which reflects the degree to which the original loss of the language model can be recovered when replacing activations with the autoencoder predictions. (2) The $L_{0}$ -norm of feature activations $\\mathbb{E}_{z\\sim\\mathcal{D}}\\left\\|h(z)\\right\\|_{0}$ , measuring the number of activate features given an input [25]. Makelov et al. [40] proposed to compare SAEs against supervised feature dictionaries in a natural language setting. However, this requires a significant understanding of the model\u2019s internal computations and is thus not scalable. ", "page_idx": 15}, {"type": "text", "text": "B Sparse Autoencoder Training Parameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We used a single NVIDIA A100 GPU for training SAEs and experiments. It takes much less than 24 hours to train a single SAE on 300 million tokens. Given a trained SAE, our evaluation requires less than 5 minutes of computing time. ", "page_idx": 15}, {"type": "table", "img_path": "SCEdoGghcw/tmp/ee6b2d07dae9f0387a187e7f945b1e5cbea9c093904f1d2c235d361593c7cb11.jpg", "table_caption": ["Table 2: Training parameters of our sparse autoencoders. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C List of Board State Properties ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 3 summarizes the high-level board state properties considered in $\\mathcal{G}_{\\mathrm{strategy}}$ . The selection of concepts was inspired by McGrath et al. [45]. The column indicated by $\\#$ denotes the number of individual BSPs per concept. A single BSP per concept indicates we match this condition globally for any corresponding piece. ", "page_idx": 16}, {"type": "table", "img_path": "SCEdoGghcw/tmp/eb58567eea99844e8c5739f958830d46cb5f002e7b2cb89949f65b8a9b9048d6.jpg", "table_caption": ["Table 3: List of strategic Board State Properties. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Performance of Linear Probes and SAEs on Board State Properties ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Figure 3, we present a mean coverage score over strategy board state properties $\\mathcal{G}_{\\mathrm{BSP}}$ . Properties within $\\mathcal{G}_{\\mathrm{BSP}}$ vary significantly in complexity. For example, queen detection can be inferred directly from the move history, while fork detection requires an accurate representation of the board state. Table 4 shows that linear probe F1-score is below 0.95 for 6 out of 15 properties in $\\mathcal{G}_{\\mathrm{BSP}}$ . This suggests that chess-GPT [33] does not represent these properties linearly. Additional experiments are required to determine whether the representation is present at all. ", "page_idx": 17}, {"type": "text", "text": "For the board state case, reconstruction is significantly higher than coverage. This is because there are many SAE features that are high precision classifiers for a configuration of squares, such as \"white pawn on e4, white knight of $\\mathbf{f}\\,3\"$ . In cases where coverage is higher than reconstruction (such as for can_check), it is because there are not many features that are over $95\\%$ precision for \u201cthere is a check move available\u201d from which we can recover if there is an available check move. Coverage is significantly higher because there is at least one feature that has an $F_{1}$ -score of 0.54 for can_check, which may not have a precision greater than $95\\%$ . ", "page_idx": 17}, {"type": "table", "img_path": "SCEdoGghcw/tmp/9a6b9f744ae72fb4ab1ff4bb7d0af69d29309c2229bc020e660a39c91fe83e34.jpg", "table_caption": ["Table 4: Comparison of performance of linear probes trained to predict board state properties given residual stream activation of ChessGPT after the sixth layer with SAEs evaluated using our coverage and reconstruction metrics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "SCEdoGghcw/tmp/02c8b43c6ba1eb9fa780e67a589df06cbfee1371be5597e70666d2c6e74fb3fd.jpg", "table_caption": ["Table 5: Comparison of performance of linear probes trained to predict high-level board state properties given residual stream activations with SAEs, both trained on a model with the same architecture as ChessGPT but randomly initialized. Performance on metrics can be high when the metric is correlated with move number or syntax level patterns (such as castling, which corresponds to $^{\\bullet\\bullet}0\\!-\\!0^{\\circ\\bullet},$ ). "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Relative Reconstruction Bias ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Training Standard SAEs with an $L_{1}$ penalty, as described in Section 4, causes a systematic underestimation of feature activations. Wright and Sharkey [62] term this phenomenon shrinkage. Following Rajamanoharan et al. [54], we measure the relative reconstruction bias $\\gamma$ of our SAEs, defined as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma:=\\arg\\operatorname*{min}_{\\gamma^{\\prime}}\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\|\\hat{x}_{\\mathrm{SAE}}(x)/\\gamma^{\\prime}-x\\|_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, $\\mathcal{D}$ denotes a large dataset of model internal activations. Intuitively, $\\gamma<1$ indicates shrinkage.   \nA perfectly unbiased SAE would have $\\gamma=1$ . ", "page_idx": 18}, {"type": "text", "text": "Our experiments show that p-annealing achieves similar relative reconstruction bias improvements to gated SAEs, both outperforming the baseline architecture. Figure 5 shows that improvements manifest differently across domains: Chess SAEs show a narrower range of bias $(\\gamma\\approx0.98)$ compared to Othello $(\\gamma\\approx0.80)$ . This domain-dependent variation may reflect differences in the underlying models or data distributions. We observe unstable $\\gamma$ values for SAEs with L0 near zero, which represent degenerate cases outside the typical operating range of these models. ", "page_idx": 18}, {"type": "image", "img_path": "SCEdoGghcw/tmp/5d2469c93b921956a97242c5a2cfb2a2c36c9bc065da4207b088e08f69c8569d.jpg", "img_caption": ["Figure 5: Comparison of the relative reconstruction bias metric $\\gamma$ quantifying feature activation shrinkage across a suite of SAEs. $\\gamma<1$ indicates shrinkage. A perfectly unbiased SAE would have $\\gamma=1$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Model Internal Board State Representation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Othello Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Previous research of Othello-playing language models found that the model learned a nonlinear model of the board state [36]. Further investigation found a closely related linear representation of the board when probing for \"my color\" vs. \"opponent\u2019s color\" rather than white vs. black [47]. Based on these findings, when measuring the state of the board in Othello, we represent squares as (mine, yours) rather than (white, black). ", "page_idx": 19}, {"type": "text", "text": "F.2 Chess Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Similar to Othello models, prior studies of chess-playing language models found the same property, where linear probes were only successful on the objective of the (mine, yours) representation and were unsuccessful on the (white, black) representation [33]. They measured board state at the location of every period in the Portable Game Notation (PGN) string, which indicates that it is white\u2019s turn to move and maintain the (mine, yours) objective. Some characters in the PGN string contain little board state information as measured by linear probes, and there is not a clear ground truth board state part way through a move (e.g., the \u201cf\u201d in \u201cNf3\u201d). We follow these findings and measure the board state at every period in the PGN string. ", "page_idx": 19}, {"type": "text", "text": "When measuring chess piece locations, we do not measure pieces on their initial starting location, as this correlates with position in the PGN string. An SAE trained on residual stream activations after the first layer of the chess model (which contains very little board state information as measured by linear probes) obtains a board reconstruction $F_{1}$ -score of 0.01 in this setting. If we also measure pieces on their initial starting location, the layer 1 SAE\u2019s $F_{1}$ -score increases to 0.52, as the board can be mostly reconstructed in early game positions purely from the token\u2019s location in the PGN string. Masking the initial board state and blank squares decreases the $F_{1}$ -score of the linear probe from 0.99 to 0.98. ", "page_idx": 19}, {"type": "text", "text": "G Additional examples of learned SAE features ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present two additional examples of learned SAE features that we (subjectively) match to board state properties based on their maximally activating input PGN-strings in Figure 6. ", "page_idx": 19}, {"type": "image", "img_path": "SCEdoGghcw/tmp/40b2a0260c1f6d5471bda2e323d3a240faa7dc1f4e8fee379518af60be7c03e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "1.e4 c5 2.Nc3 Nc6 3.Nf3 g6 4.d4 cxd4 5.Nxd4 Bg7 6.Be3 Nf6 7.Qd2 Ng4  \n8.Nxc6 bxc6 9.Bd4 Bxd4 10.Qxd4 0-0 11.Be2 d6 12.Bxg4 Bxg4 13.f3 Be6  \n14.h4 Qb6 15.0-0-0 Rab8 16.Qxb6 axb6 17.h5 Kg7 18.b3 b5 19.Kb2 b4  \n20.Ne2 c5 21.Nf4 Ra8 22.Ra1 Ra3 23.c4Ra7 24.a4", "page_idx": 20}, {"type": "text", "text": "(a) \"En passant available\" detector learned by an SAE. ", "page_idx": 20}, {"type": "image", "img_path": "SCEdoGghcw/tmp/48a7dd6e9a7d76109e5672d3abcc560acc716be8805f34038d9863d0e51bb86d.jpg", "img_caption": ["(b) \"Knight on e2 or e7\" detector learned by an SAE. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "SCEdoGghcw/tmp/ec2c91c43fe2050078455e0b976f698ca40ed0c4b925ed24f8fda037d25d85b4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "1.d4 d5 2.c4 Nc6 3.cxd5 Qxd5 4.Nc3 Qxd4 5.e3 Qxd1+ 6.Nxd1 Bg4 7.Be2 Bxe2   \n8.Nxe20-0-0 9.0-0 e5 10.a3 Nf6 11.b4 Ne4 12.Bb2f6 13.Ndc3 Nd2 14.Rfd1 Nc4   \n15.Rab1 Nxb2 16.Rxb2 ", "page_idx": 20}, {"type": "text", "text": "(c) Another example of the \"Knight on e2 or e7\" detector shown in subfigure (b) above. We interpret this feature as representing a mirrored perspective. It may be firing for \"opponent knight on e column, 1 square away from opponent back rank\", which is why it fires for both e2 (if black\u2019s turn to move) and e7 (if white\u2019s turn to move). ", "page_idx": 20}, {"type": "text", "text": "Figure 6: Additional examples of learned SAE features. We show the full board state of a chosen game in which the SAE latent has a high activation. The PGN-string (model input) which represents the game history is shown below the board. Tokens that activate SAE features are marked in blue, where darker shades correspond to higher feature activations. Moves that create the considered a board state are highlighted in yellow. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The main claims presented in the abstract and introduction are a faithful representation of the contributions and scope of the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss our main limitation, the sensitivity of our metrics to human preconceptions, both in the introduction and in a dedicated limitations section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We disclose all information about the sources of base models and datasets, as well as information about the sparse autoencoder achitecture and hyperparameters used during training. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the relevant code to reproduce the main experimental results of the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the training process in the main section and provide additional information in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We present results across a large sweep of training parameters to provide information about the significance of the experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss our compute resources in Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The research conducted does conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our work focuses on evaluating current methods to extract features from neural networks, and proposes a method to improve this process. We do not believe that this work has a direct impact on society. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not introduce new datasets or models that pose such risks. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The creators of original assets (models and data) are properly cited in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide a Readme as well as in-code documentations alongside the code. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]