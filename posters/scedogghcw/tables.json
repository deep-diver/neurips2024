[{"figure_path": "SCEdoGghcw/tables/tables_7_1.jpg", "caption": "Table 1: Best performance obtained for different techniques across games for Gboard state. As a baseline, we train an SAE on random GPT, a version of the trained GPT model with randomly initialized weights. All models were trained on activations after the post-MLP residual connection in layer 6.", "description": "This table presents the best performance achieved by different methods (SAE with random GPT, SAE with trained GPT, and linear probe) in reconstructing board states using Gboard properties for both chess and Othello games.  The SAE models used activations from after the post-MLP residual connection in layer 6. The random GPT SAE serves as a baseline, demonstrating the importance of training on actual game data.", "section": "5 Results"}, {"figure_path": "SCEdoGghcw/tables/tables_15_1.jpg", "caption": "Table 2: Training parameters of our sparse autoencoders.", "description": "This table lists the hyperparameters used during the training of sparse autoencoders (SAEs).  The parameters include the number of tokens processed, the optimizer used (Adam), the Adam beta values, the number of linear warmup steps, the batch size, the learning rate, the expansion factor (which determines the size of the hidden layer relative to the input layer), the annealing start point (the training step at which the p-annealing process begins), the final value of p (Pend) in the p-annealing process, and the initial value of lambda (\u03bbinit) used for the sparsity penalty.", "section": "4 Training methodologies for SAEs"}, {"figure_path": "SCEdoGghcw/tables/tables_16_1.jpg", "caption": "Table 4: Comparison of performance of linear probes trained to predict board state properties given residual stream activation of ChessGPT after the sixth layer with SAEs evaluated using our coverage and reconstruction metrics.", "description": "This table compares the performance of linear probes and sparse autoencoders (SAEs) in predicting various board state properties in chess.  It shows the F1-score for linear probes, the reconstruction score for SAEs, and the best SAE coverage score for each property.  The properties range from simple ones (like checking the king) to more complex ones (like detecting forks or pins). The table highlights the relative strengths and weaknesses of linear probes and SAEs in capturing different aspects of the game state.", "section": "D Performance of Linear Probes and SAEs on Board State Properties"}, {"figure_path": "SCEdoGghcw/tables/tables_17_1.jpg", "caption": "Table 4: Comparison of performance of linear probes trained to predict board state properties given residual stream activation of ChessGPT after the sixth layer with SAEs evaluated using our coverage and reconstruction metrics.", "description": "This table compares the performance of linear probes and Sparse Autoencoders (SAEs) in predicting various board state properties in chess.  Linear probes are simpler models that directly predict properties from the neural network activations. SAEs, on the other hand, create a lower-dimensional representation of the activations and then use that representation to predict the board state properties. The table shows the F1-score for linear probes, the reconstruction score for SAEs (how well the SAE can reconstruct the original board state), and the coverage score for SAEs (how many board state properties the SAE is able to capture). The results show that while linear probes perform better overall, SAEs are capable of capturing and reconstructing several key properties.", "section": "3. Measuring autoencoder quality for chess and Othello models"}, {"figure_path": "SCEdoGghcw/tables/tables_18_1.jpg", "caption": "Table 4: Comparison of performance of linear probes trained to predict board state properties given residual stream activation of ChessGPT after the sixth layer with SAEs evaluated using our coverage and reconstruction metrics.", "description": "This table compares the performance of linear probes and sparse autoencoders (SAEs) in predicting various board state properties.  The linear probes use the residual stream activations from ChessGPT after the 6th layer as input.  The SAEs are evaluated based on two new metrics introduced in the paper: coverage and board reconstruction. The table shows the F1-score for each linear probe and the reconstruction score and coverage score for the best-performing SAE for each property.", "section": "D Performance of Linear Probes and SAEs on Board State Properties"}]