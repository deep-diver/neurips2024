[{"figure_path": "IRXyPm9IPW/tables/tables_2_1.jpg", "caption": "Table 1: Statistics of existing preference datasets for text-to-image generative models. \u201cFine-grained\u201d denote containing preference regarding multiple aspects or not.", "description": "This table compares several existing datasets used for evaluating the performance of text-to-image generative models.  It highlights key features of each dataset, including the number of preference choices, whether the dataset is open-source, whether it provides fine-grained feedback (assessing multiple aspects of image quality), and the format of the feedback (ranking, text, or scalar scores). The table demonstrates the limitations of existing datasets and positions the new VisionPrefer dataset as an improvement in terms of scalability, fine-grained feedback, and data size.", "section": "Related Work"}, {"figure_path": "IRXyPm9IPW/tables/tables_4_1.jpg", "caption": "Table 2: Preference prediction accuracy across the test sets of ImageRewardDB, HPD v2 and Pick-a-Pic. The Aesthetic Classifier makes prediction without seeing the text prompt. The best performance is in bold, and the second-best performance is underlined.", "description": "This table presents a comparison of the preference prediction accuracy of several models, including VP-Score, across three different human preference datasets: ImageRewardDB, HPD v2, and Pick-a-Pic. The accuracy is measured by comparing the model's predictions with human annotations.  The table highlights VP-Score's competitive performance compared to existing models.  It also includes a model (Aesthetic Classifier) that makes predictions without access to the text prompt, showcasing the effect of utilizing text information in preference prediction.", "section": "4.1 Reward Modeling"}, {"figure_path": "IRXyPm9IPW/tables/tables_12_1.jpg", "caption": "Table 1: Statistics of existing preference datasets for text-to-image generative models. \u201cFine-grained\u201d denote containing preference regarding multiple aspects or not.", "description": "This table provides a comparison of different existing datasets used for evaluating human preferences in text-to-image generation.  It lists each dataset's name, the corresponding reward model used, the number of annotators, the number of prompts, the number of preference choices, whether they are open-source, whether the preferences are fine-grained (considering multiple aspects), and the feedback format used (ranking, text, scalar).  It highlights the relative sizes and features of these datasets, setting the stage for the introduction of the authors' new dataset, VisionPrefer.", "section": "Related Work"}, {"figure_path": "IRXyPm9IPW/tables/tables_12_2.jpg", "caption": "Table 4: Human evaluation study on win count and win rate of generative models optimized with different reward models, benchmarked against the Stable Diffusion v1.5 baseline. Compared to other reward models, VP-Score exhibits competitive performance. The best results are highlighted in bold, while the second-best results are underlined.", "description": "This table presents the results of a human evaluation study comparing the performance of generative models fine-tuned using different reward models.  The models were benchmarked against the Stable Diffusion v1.5 model. The table shows the number of wins (\"#Win\") and win rates (\"WinRate\") for each reward model across three different datasets (DiffusionDB, ReFL, and HPD v2). VP-Score, the reward model introduced in the paper, shows competitive performance compared to other models.", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/tables/tables_12_3.jpg", "caption": "Table 5: Human evaluation on generative models optimized with different preference datasets in DPO experiments. The best results are highlighted in bold, while the second-best results are underlined.", "description": "This table presents the results of a human evaluation study comparing the performance of generative models fine-tuned using different preference datasets in Direct Policy Optimization (DPO) experiments.  The evaluation focuses on three different test benchmarks: DiffusionDB, ImageRewardDB, and HPD v2. The table shows the number of wins (#Win) and win rate (WinRate) for each model on each benchmark.  The model fine-tuned with VisionPrefer (the authors' proposed method) is compared against models trained on three other existing human preference datasets: ImageRewardDB, HPD v2, and Pick-a-Pic. The best and second-best performing models for each dataset are highlighted in bold and underlined, respectively. This allows for a direct comparison of the effectiveness of VisionPrefer relative to existing datasets in improving alignment between text and generated images. ", "section": "4.2 Fine-tuning Text-to-Image Generative Models"}, {"figure_path": "IRXyPm9IPW/tables/tables_15_1.jpg", "caption": "Table 6: Ablation study for different reward model backbones.", "description": "This table presents the results of an ablation study comparing the performance of two different backbones, CLIP and BLIP, used in the VP-Score reward model. The study evaluates the performance on three different human preference datasets: ImageRewardDB, HPD v2, and Pick-a-Pic.  The average performance across these datasets is also shown, demonstrating a slight but consistent performance advantage for the BLIP backbone.", "section": "A Ablation Study"}, {"figure_path": "IRXyPm9IPW/tables/tables_17_1.jpg", "caption": "Table 7: Human evaluation study on the aspect of \"Prompt-Following\". The best results are highlighted in bold, while the second-best results are underlined.", "description": "This table presents the results of a human evaluation study focusing on the \"Prompt-Following\" aspect of the VisionPrefer dataset.  It compares the performance of different reward models (PickScore, HPS v2, VP-Score+, and VP-Score) in predicting human preferences for image generation quality based on how well the generated images followed the given prompt.  VP-Score+ is a variant trained without the Prompt-Following labels, allowing for an assessment of that aspect's specific contribution to performance.", "section": "B.1 Better Prompt-Following"}, {"figure_path": "IRXyPm9IPW/tables/tables_17_2.jpg", "caption": "Table 2: Preference prediction accuracy across the test sets of ImageRewardDB, HPD v2 and Pick-a-Pic. The Aesthetic Classifier makes prediction without seeing the text prompt. The best performance is in bold, and the second-best performance is underlined.", "description": "This table presents a comparison of the preference prediction accuracy of several models, including the model proposed in the paper (VP-Score), across three different human-preference datasets: ImageRewardDB, HPD v2, and Pick-a-Pic.  It shows the accuracy of each model in predicting human preferences, highlighting the best-performing models for each dataset and overall. The inclusion of an 'Aesthetic Classifier' highlights the impact of considering text prompts for accurate image evaluation.", "section": "4.1 Reward Modeling"}, {"figure_path": "IRXyPm9IPW/tables/tables_17_3.jpg", "caption": "Table 9: Human evaluation study on the aspect of \"Fidelity\". The best results are highlighted in bold, while the second-best results are denoted with an underline.", "description": "This table presents the results of a human evaluation study focusing on the \"Fidelity\" aspect of image generation.  Three different reward models (PickScore, HPS v2, and VP-Score) were evaluated using the \"Anything Prompts\" from a related work [36]. The results show the number of times each model produced a top-3 image that was rated as preferable to other models and the corresponding win rate.  VP-Score, despite removing the fidelity labels, performs competitively, indicating the model's robustness.", "section": "B.3 Reduce Image Distortion"}, {"figure_path": "IRXyPm9IPW/tables/tables_20_1.jpg", "caption": "Table 1: Statistics of existing preference datasets for text-to-image generative models. \u201cFine-grained\u201d denote containing preference regarding multiple aspects or not.", "description": "This table presents a comparison of several existing datasets used for evaluating the preferences of text-to-image generation models.  Key features compared across datasets include the number of preference choices, the number of image pairs, whether the dataset is open source, whether it includes fine-grained preferences (meaning it assesses multiple aspects of preference rather than an overall judgment), and the types of feedback collected (e.g., ranking, text, or numerical scores). The table highlights the significant increase in scale and detail offered by the VisionPrefer dataset introduced in this paper compared to previously existing datasets.", "section": "Related Work"}, {"figure_path": "IRXyPm9IPW/tables/tables_21_1.jpg", "caption": "Table 11: Image sources of VisionPrefer.", "description": "This table presents the sources of images used in the VisionPrefer dataset.  It shows that the images were generated using four different diffusion models: Stable Diffusion v1-5, Stable Diffusion 2.1, Dreamlike Photoreal 2.05, and Stable Diffusion XL. The table lists the type of each model, its resolution, and the proportion of images in the VisionPrefer dataset that were generated by each model.", "section": "3 VisionPrefer"}]