[{"type": "text", "text": "EAGLE : Efficient Adaptive Geometry-based Learning in Cross-view Understanding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thanh-Dat Truong1, Utsav Prabhu2, Dongyi Wang3 Bhiksha Raj4,5, Susan Gauch6, Jeyamkondan Subbiah7, Khoa Luu ", "page_idx": 0}, {"type": "text", "text": "1CVIU Lab, University of Arkansas, USA 2Google DeepMind, USA 3Dep. of BAEG, University of Arkansas, USA 4Carnegie Mellon University, USA 5Mohammed bin Zayed University of AI, UAE 6Dep. of EECS, University of Arkansas, USA 7Dep. of FDSC, University of Arkansas, USA {tt032, dongyiw, sgauch, jsubbiah, khoaluu}@uark.edu bhiksha@cs.cmu.edu, utsavprabhu@google.com https://uark-cviu.github.io/projects/EAGLE ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Unsupervised Domain Adaptation has been an efficient approach to transferring the semantic segmentation model across data distributions. Meanwhile, the recent Open-vocabulary Semantic Scene understanding based on large-scale vision language models is effective in open-set settings because it can learn diverse concepts and categories. However, these prior methods fail to generalize across different camera views due to the lack of cross-view geometric modeling. At present, there are limited studies analyzing cross-view learning. To address this problem, we introduce a novel Unsupervised Cross-view Adaptation Learning approach to modeling the geometric structural change across views in Semantic Scene Understanding. First, we introduce a novel Cross-view Geometric Constraint on Unpaired Data to model structural changes in images and segmentation masks across cameras. Second, we present a new Geodesic Flow-based Correlation Metric to efficiently measure the geometric structural changes across camera views. Third, we introduce a novel view-condition prompting mechanism to enhance the view-information modeling of the open-vocabulary segmentation network in cross-view adaptation learning. The experiments on different cross-view adaptation benchmarks have shown the effectiveness of our approach in cross-view modeling, demonstrating that we achieve State-of-the-Art (SOTA) performance compared to prior unsupervised domain adaptation and open-vocabulary semantic segmentation methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Modern segmentation models [3, 4, 63] have achieved remarkable results on the close-set training with a set of pre-defined categories and concepts. To work towards human-level perception where the scenes are interpreted with diverse categories and concepts, the openvocabulary (open-vocab) perception model [38, 40] based on the power of large vision-language models [30, 39] has been introduced to address the limitations of close-set training. By using the power of language as supervision, the largescale vision language model is able to learn the more powerful representations where languages offer better reasoning mechanisms and openword concept representations compared to traditional close-set training methods [3, 63, 9]. ", "page_idx": 0}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/d43f87603dcf5a32d8fdc05aaafb26aa8f5702aadbe37bfec24234b1b4b9afec.jpg", "img_caption": ["Figure 1: Our Proposed Cross-view Adaptation Learning Approach. Prior models, e.g., FreeSeg [38], DenseCLIP [40], trained on the car view do not perform well on the drone-view images. Meanwhile, our cross-view adaptation approach is able to generalize well from the car to drone view. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Recent work is inspired by the success of large vision-language models [39, 27] that are able to learn informative feature representations of both visual and textual inputs from large-scale image-text pairs. These have been adopted to further develop open-vocab semantic segmentation models [38, 40, 31, 29] that can work well in open-world environments. However, the open-vocab perception models remain unable to generalize across camera viewpoints. As shown in Fig. 1, the open-vocab model trained on car views is not able to perform well on the images captured from unmanned aerial vehicles (UAVs) or drones. While this issue can be improved by training the segmentation model on drone-view ", "page_idx": 1}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/1cc4dd69ccad4aa1ebc71622d7c06ad933de50daed991344d5829289d6a2da08.jpg", "img_caption": ["Figure 2: An Example of Illustration of CrossView Adaptation From Car View to Drone View. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "data, the annotation process of high-resolution UAV data is costly and time-consuming. At present, there exist many large-scale datasets with dense labels captured from camera views on the ground, e.g., car views (SYNTHIA [44], GTA [43], Cityscapes [11], BDD100K [68]). They have been widely adopted to develop robust perception models. Since these car view and drone view datasets have many common objects of interest, incorporating knowledge from car views with drone views beneftis the learning process by reusing large-scale annotations and saving efforts of manually labeling UAV images. Unsupervised domain adaptation (UDA) [58, 23, 1, 51, 53] is one of the potential approaches to transfer the knowledge from the car view (i.e., source domain) to the drone view (i.e., target domain). While UDA approaches have shown their effectiveness in transferring knowledge across domains, e.g., environment changes or geographical domain shifts, these methods remain limited in the cases of changing camera viewpoints. Indeed, the changes in camera positions, e.g., from the ground of cars to the high positions of drones, bring a significant difference in structures and topological layouts of scenes and objects (Fig. 2). Therefore, UDA is not a complete solution to this problem due to its lack of cross-view structural modeling. Additionally, although the open-vocab segmentation models have introduced several prompting mechanisms, e.g., context-aware prompting [40] or adaptive prompting [38] to improve context learning across various open-world concepts, they are unable to model the cross-view structure due to the lack of view-condition information in prompts and geometric modeling. To the best of our knowledge, there are limited studies that have exploited this cross-view learning. These limitations motivate us to develop a new adaptation learning paradigm, i.e., Unsupervised Cross-view Adaptation, that addresses prior methods to improve the performance of semantic segmentation models across views. ", "page_idx": 1}, {"type": "text", "text": "Contributions: This work introduces a novel Efficient Adaptive Geometry-based Learning (EAGLE) to Unsupervised Cross-view Adaptation that can adaptively learn and improve the performance of semantic segmentation models across camera viewpoints. First, by analyzing the geometric correlations across views, we introduce a novel cross-view geometric constraint on unpaired data of structural changes in images and segmentation masks. Second, to efficiently model cross-view geometric structural changes, we introduce a new Geodesic Flow-based Metric to measure the structural changes across views via their manifold structures. In addition, to further improve the prompting mechanism of the open-vocab segmentation network in cross-view adaptation learning, we introduce a new view-condition prompting. Then, our cross-view geometric constraint is also imposed on its feature representations of view-condition prompts to leverage its geometric knowledge embedded in our prompting mechanism. Our proposed method holds a promise to be an effective approach to addressing the problem of cross-view learning and contributes to improving UDA and open-vocab segmentation in cross-view learning. Thus, it increases the generalizability of the segmentation models across camera views. Finally, our experiments on three presented cross-view adaptation benchmarks, i.e., SYNTHIA $\\rightarrow$ UAVID, GTA $\\rightarrow$ UAVID, BDD $\\rightarrow$ UAVID, illustrate the effectiveness of our approach in cross-view modeling and our State-of-the-Art (SOTA) performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Unsupervised Domain Adaptation Adversarial learning [6, 58, 59] and self-supervised learning [1, 69, 23, 14] are common approaches to UDA in semantic segmentation. The adversarial learning approaches are typically simultaneously trained on source and target data [58, 7, 6]. Chen et al. [7] first introduced an adversarial framework to domain adaptation. Later, several approaches improved adversarial learning by utilizing generative models [74, 34, 21], using additional labels [28, 59], incorporating with entropy minimization [58, 65, 51, 52], or adopting the curriculum training [37]. Recently, the self-supervised approaches [1, 69, 23, 14] have achieved outstanding performance. Araslanov et al. [1] first proposed a self-supervised augmentation consistency framework for UDA. Hoyer et al. [23] utilized Transformers to improve the UDA performance. Later, this approach was further improved by utilizing multi-resolution cropped images [24] and masked image consistency strategy [25] to enhance contextual learning. Recent studies improved the self-supervised approach by aligning both output and attention levels via the cross-domain prediction consistency framework [60], using a prototypical representation [69], learning the cross-model consistency via depths [67], improving the class-relevant fairness [53, 55, 56], or exploring the relations of pseudo-labels [71]. Fashes et al. [15] introduced a prompt-based feature augmentation method to zero-shot UDA. Gong et al. [18] introduced a geodesic flow kernel to model the manifold structure between domains. Later, Simon et al. [47] designed distillation loss by the geodesic flow path. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Vision-Language and Open-Vocab Segmentation By pre-training on a large-scale vision-language dataset [39, 27], the vision-language models can learn various visual concepts and can further be transferred to other vision problems through \u201cprompting\u201d [17, 38, 31, 35], e.g., open-vocab segmentation [64, 13, 38]. Li et al. [29] first introduced the language-driving approach to semantic segmentation. Rao et al. [40] represented a context-aware prompting mechanism for dense prediction tasks. Ghiasi et al. [17] proposed an OpenSeg framework that learns the visual-semantic alignments. Qin et al. [38] presented a unified, universal, and open-vocab segmentation network based on Mask2Former [8] with an adaptive prompting mechanism. Xu et al. [64] proposed a two-stage open-vocab segmentation framework using the mask proposal generator and the pre-trained CLIP model. Ding et al. [13] decoupled the zero-shot semantic segmentation to class-agnostic segmentation and segment-level zero-shot classification. Liang et al. [31] improved the two-stage open-vocab segmentation model by further fine-tuning CLIP on masked image regions and corresponding descriptions. ", "page_idx": 2}, {"type": "text", "text": "Cross-view Learning The early studies exploited cross-view learning in geo-localization by using a polar transform across views [46, 45] or generative networks to cross-view images [41, 49]. Meanwhile, Zhu et al. [75] exploited the correlation between street- and aerial-view data via selfattention. In semantic segmentation, Coors et al. [10] first introduced a cross-view adaptation approach utilizing the depth labels and the cross-view transformation between car and truck views. However, this change of views in [10] is not as big a hurdle as the change of views in our problem, i.e., car view to drone view. Ren et al. [42] presented an adaptation approach across viewpoints using the 3D models of scenes to create pairs of cross-view images. Vidit et al. [57] modeled the geometric shift in cross FoV setting for object detection by learning position-invariant homography transform. Di Mauro et al. [12] introduced an adversarial method trained on a multi-view synthetic dataset where images are captured from different pitch and yaw angles at the same altitudes of the camera positions. Meanwhile, in our problem, the camera views could be placed at different altitudes (e.g., the car and the drone), which reveals large structural differences between the images. Truong et al. [50, 54] first introduced a simple approach to model the relation across views. CROVIA [50] measures the cross-view structural changes by measuring the distribution shift and only focuses on the cross-view adaptation setting in semantic segmentation. However, these methods [50, 54] lack a theory and a mechanism for cross-view geometric structural change modeling. To the best of our knowledge, there are limited studies exploiting cross-view adaptation in semantic segmentation. Therefore, our work presents a new approach to model the geometric correlation across views. ", "page_idx": 2}, {"type": "text", "text": "3 The Proposed EAGLE Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we consider cross-view adaptation learning as UDA where the images of the source and target domains are captured from different camera positions (Fig. 2). Formally, let $\\mathbf{x}_{s},\\mathbf{x}_{t}$ be the input images in the source and target domains, $\\mathbf{p}_{s},\\mathbf{p}_{t}$ be the the corresponding prompts, and $\\mathbf{y}_{s},\\mathbf{y}_{t}$ be the segmentation masks of $\\mathbf{x}_{s},\\mathbf{x}_{t}$ . Then, the open-vocab segmentation model $F$ maps the input $\\mathbf{x}$ and the prompt $\\mathbf{p}$ to the corresponding output $\\mathbf{y}=F(\\mathbf{x},\\mathbf{p})$ . It should be noted that in the case of traditional semantic segmentation, the prompt $\\mathbf{p}$ will be ignored, i.e., $\\mathbf{y}=F(\\mathbf{x})$ The cross-view adaptation learning can be formulated as Eqn. (1). ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\theta}\\left[\\mathbb{E}_{\\mathbf{x}_{s},\\mathbf{p}_{s},\\hat{\\mathbf{y}}_{s}}\\mathcal{L}_{M a s k}\\big(\\mathbf{y}_{s},\\hat{\\mathbf{y}}_{s}\\big)+\\mathbb{E}_{\\mathbf{x}_{t},\\mathbf{p}_{t}}\\mathcal{L}_{A d a p t}\\big(\\mathbf{y}_{t}\\big)\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta$ is the parameters of $F,\\,\\hat{\\mathbf{y}}_{s}$ is the ground truth, $\\mathcal{L}_{M a s k}$ is the supervised (open-vocab) segmentation loss with ground truths, and $\\mathcal{L}_{A d a p t}$ is unsupervised adaptation loss from the source to the target domain. In the open-vocab setting, we adopt the design of Open-Vocab Mask2Former [8, 38] to our network $F$ . Prior UDA methods defined the adaptation loss $\\mathcal{L}_{A d a p t}$ via the adversarial loss [28, 5], entropy loss [51, 58], or self-supervised loss [23, 25]. Although these prior results have illustrated their effectiveness in UDA, these losses remain limited in cross-view adaptation setup. Indeed, the adaptation setting in prior studies [58, 1, 23, 15] is typically deployed in the context of environmental changes (e.g., simulation to real [58, 59, 15], day to night [25, 15], etc) where the camera positions between domains remain similar. Meanwhile, in cross-view adaptation, the camera position of the source and target domain remains largely different (as shown in Fig. 2). This change in camera positions leads to significant differences in the geometric layout and topological structures between the source and target domains. As a result, direct adoption of prior UDA approaches to cross-view adaptation would be ineffective due to the lack of cross-view geometric correlation modeling. To effectively address cross-view adaptation, the adaptation loss $\\mathcal{L}_{A d a p t}$ should be able to model (1) the geometric correlation between two views of source and target domains and (2) the structural changes across domains. ", "page_idx": 3}, {"type": "text", "text": "3.1 Cross-View Geometric Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To efficiently address the cross-view adaptation learning task, it is essential to explicitly model cross-view geometric correlations by analyzing the relation between two camera views. Therefore, we first re-reconsider the cross-view geometric correlation. In particular, let $\\bar{\\bf x}_{t}$ be the corresponding image of $\\mathbf{x}_{s}$ captured from the target view, $\\mathbf{y}_{s}$ and $\\bar{\\mathbf{y}}_{t}$ be the semantic segmentation outputs of source image $\\mathbf{x}_{s}$ and target image $\\bar{\\bf x}_{t}$ , $\\bar{\\mathbf{p}}_{t}$ be the corresponding prompt of $\\mathbf{p}_{s}$ in target view, respectively. Formally, the images captured from the source and the target views can be modeled as Eqn. (2). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{s}=\\mathcal{R}(\\mathbf{K}_{s},[\\mathbf{R}_{s},\\mathbf{t}_{s}],\\Theta),\\quad\\bar{\\mathbf{x}}_{t}=\\mathcal{R}(\\mathbf{K}_{t},[\\mathbf{R}_{t},\\mathbf{t}_{t}],\\Theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{R}$ is the rendering function, $\\mathbf{K}_{s}$ and $\\mathbf{K}_{t}$ are the intrinsic matrices, $[\\mathbf{R}_{s},\\mathbf{t}_{s}]$ and $[\\mathbf{R}_{t},\\mathbf{t}_{t}]$ are the extrinsic matrices, and $\\Theta$ represents the capturing scene. In addition, as the camera parameters of both source and target views are represented by matrices, there should exist linear transformations of camera parameters between two views as follow, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{K}_{t}=\\mathbf{T}_{\\mathbf{K}}\\times\\mathbf{K}_{s},\\quad\\left[\\mathbf{R}_{t},\\mathbf{t}_{t}\\right]=\\mathbf{T}_{\\mathbf{R}\\mathbf{t}}\\times\\left[\\mathbf{R}_{s},\\mathbf{t}_{s}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bf T_{K}}$ and ${\\bf{T_{R t}}}$ are the transformation matrices. ", "page_idx": 3}, {"type": "text", "text": "Remark 1: The Geometric Transformation Between Camera Views. From Eqn. (2) and Eqn. (3), we argue that there should exist a geometric transformation $\\tau$ of images between two camera views as: $\\bar{\\mathbf{x}}_{t}=\\mathcal{T}(\\mathbf{x}_{s};\\mathbf{T}_{\\mathbf{K}},\\mathbf{T}_{\\mathbf{Rt}})$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 2: The Equivalent Transformation Between Image and Segmentation Output. As RGB images and segmentation maps are pixel-wised corresponding, the same geometric transformation $\\tau$ in the image space can be adopted for segmentation space as: $\\bar{\\mathbf{y}}_{t}=\\mathcal{T}(\\bar{\\mathbf{y_{s}}};\\mathbf{T_{K}},\\mathbf{T_{Rt}})$ ", "page_idx": 3}, {"type": "text", "text": "Remarks 1-2 have depicted that the geometric transformation of both image and segmentation from the source to the target view can be represented by the shared transformation $\\tau$ with the camera transformation matrices $\\mathbf{T}_{\\mathbf{K}},\\mathbf{T}_{\\mathbf{Rt}}$ . Let $\\mathcal{D}_{x}(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t})$ and $\\mathcal{D}_{y}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t})$ be the metrics the measure the cross-view structures changes of images and segmentation maps from the source to target domains. ", "page_idx": 3}, {"type": "text", "text": "We argue that the cross-view geometric correlation in the image space, i.e., $\\mathcal{D}_{x}(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t})$ , is theoretically proportional to the one in the segmentation space, i.e., $\\mathcal D_{y}(\\mathbf y_{s},\\bar{\\mathbf y}_{t})$ . Since the camera transformations between the two views are linear (Eqn. (3)) and the images $\\mathbf{x}$ and outputs $\\mathbf{y}$ are pixel-wised corresponding, we hypothesize that the cross-view geometric correlation in the image space $\\mathcal{D}_{x}\\big(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t}\\big)$ and the segmentation space $\\mathcal{D}_{y}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t})$ can be modeled by a linear relation with linear scale $\\alpha$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{x}\\big(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t}\\big)\\propto\\mathcal{D}_{y}\\big(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t}\\big)\\Leftrightarrow\\mathcal{D}_{x}\\big(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t}\\big)=\\alpha\\mathcal{D}_{y}\\big(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Cross-view Geometric Learning on Unpaired Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Eqn. (4) defines a necessary condition to explicitly model the cross-view geometric correlation. Therefore, cross-view adaptation learning in Eqn. (1) can be re-formed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\Big[\\mathbb{E}_{{\\mathbf{x}}_{s},{\\mathbf{p}}_{s},\\bar{\\mathbf{y}}_{s}}\\mathcal{L}_{M a s k}\\big({\\mathbf{y}}_{s},{\\mathbf{p}}_{s},\\hat{\\mathbf{y}}_{s}\\big)+\\mathbb{E}_{{\\mathbf{x}}_{s},{\\mathbf{p}}_{s},\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{p}}_{t}}\\big|\\big|\\mathcal{D}_{x}\\big({\\mathbf{x}}_{s},\\bar{\\mathbf{x}}_{t}\\big)-\\alpha\\mathcal{D}_{y}\\big({\\mathbf{y}}_{s},\\bar{\\mathbf{y}}_{t}\\big)\\big|\\big]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, $\\mathcal{L}_{A d a p t}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t})=||\\mathcal{D}_{x}(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t})||$ is the cross-view geoemtric adaptation loss, $\\left|\\left|\\cdot\\right|\\right|$ is the mean squared error loss. However, in practice, the pair data between source and target views are inaccessible as data from these two views are often collected independently. Thus, optimizing Eqn. (5) without cross-view pairs of data remains an ill-posed problem. To address this limitation, instead of learning Eqn. (5) on paired data, we proposed to model this correlation on unpaired data. Instead of solving the cross-view geometric constraint of Eqn. (5) on pair data, let us consider all cross-view unpaired samples $(\\mathbf{x}_{s},\\mathbf{x}_{t})$ . Formally, learning the Cross-view Geometric Constraint between unpaired samples can be formulated as in Eqn. (6). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\left[\\mathbb{E}_{\\mathbf{x}_{s},\\hat{\\mathbf{y}}_{s}}\\mathcal{L}_{M a s k}\\big(\\mathbf{y}_{s},\\mathbf{p}_{s},\\hat{\\mathbf{y}}_{s}\\big)+\\mathbb{E}_{\\mathbf{x}_{s},\\mathbf{p}_{s},\\mathbf{x}_{t},\\mathbf{p}_{t}}\\big|\\big|\\mathcal{D}_{x}\\big(\\mathbf{x}_{s},\\mathbf{x}_{t}\\big)-\\alpha\\mathcal{D}_{y}\\big(\\mathbf{y}_{s},\\mathbf{y}_{t}\\big)\\big|\\big|\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{x}_{s}$ and $\\mathbf{x}_{t}$ are unpaired data, and $\\mathcal{L}_{A d a p t}(\\mathbf{y}_{s},\\mathbf{y}_{t})=||\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})||$ is the Crossview Geometric Adaptation loss on unpaired data. Intuitively, although the cross-view pair samples are not available, the cross-view geometric constraints on paired samples between two views can be indirectly imposed by modeling the cross-view geometric structural constraint among unpaired samples. Then, by modeling the cross-view structural changes in the image and segmentation spaces, the structural change on images of unpaired data could be considered as the reference for the cross-view structural change in the segmentation space during the optimization process. This action promotes the structures of segmentation that can be effectively adapted from the source view to the target view. Importantly, the cross-view geometric constraint imposed on unpaired data can be mathematically proved as an upper bound of the cross-view constraint on paired data as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{||\\mathcal{D}_{x}(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t})||=\\mathcal{O}\\left(\\mathcal{D}_{x}(||\\mathbf{x}_{s},\\mathbf{x}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})||\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\scriptscriptstyle\\mathcal{O}$ is the Big O notation. The upper bound in Eqn. (19) can be proved by using the properties of triangle inequality and our correlation metrics $D_{x}$ and $\\mathcal{D}_{y}$ (Sec. 3.3). The detailed proof is provided in the appendix. Eqn. (19) has illustrated that by minimizing the cross-view geometric constraint on unpaired samples in Eqn. (6), the cross-view constraint on paired samples in Eqn. (5) is also maintained due to the upper bound. Therefore, our proposed Cross-view Geometric Constraint loss does NOT require the pair data between source and target views during training. Fig. 3 illustrates our cross-view adaptation learning framework. ", "page_idx": 4}, {"type": "text", "text": "3.3 Cross-view Structural Change Modeling via Geodesic Flow Path ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Modeling the correlation metrics $\\mathcal{D}_{x}$ and $\\mathcal{D}_{y}$ is an important task in our approach. Indeed, the metrics should be able to model the structure changes from the source to the target view. Intuitively, the changes from the source to the target view are essentially the geodesic flow between two subspaces on the Grassmann manifold. Then, the images (or segmentation) of two views can be projected along the geodesic flow path to capture the cross-view structural changes. Therefore, to model $\\mathcal{D}_{x}$ and $\\mathcal{D}_{y}$ , we adopt the Geodesic Flow path to measure the cross-view structural changes by modeling the geometry in the latent space. ", "page_idx": 4}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/bc4977e8e9ac9eb673504574c7ab4c350b736c5c55b136ae2cdb246003a35a5b.jpg", "img_caption": ["Figure 3: Our Cross-View Learning Framework. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Remark 3: Grassmann Manifold is the set of $N$ -dimensional linear subspaces of $\\mathbb{R}^{D}(0<N<D)$ , i.e, $\\mathcal{G}(N,D)$ . A matrix with orthonormal columns $\\mathbf{P}\\in\\mathbb{R}^{D\\times N}$ define a subspace of $\\mathcal{G}(N,D)$ , i.e., $\\mathbf{P}\\in{\\dot{\\mathcal{G}}}(N,{\\dot{D}})\\Rightarrow\\mathbf{P}^{\\top}\\mathbf{P}=\\mathbf{I}_{N}$ where ${\\mathbf{I}}_{N}$ is the $N\\times N$ identity matrix. ", "page_idx": 4}, {"type": "text", "text": "For simplicity, we present our approach to model the cross-view structural change $\\mathcal{D}_{x}$ in the image space. Formally, let ${\\bf P}_{s}$ and $\\mathbf{P}_{t}$ be the basis of the source and target domains. These bases can be obtained by the PCA algorithm. The geodesic flow between $\\mathbf{P}_{s}$ and $\\mathbf{P}_{t}$ in the manifold can be defined via the function $\\bar{\\mathbf{I}^{\\prime}}\\colon\\nu\\in[0..1]\\ \\bar{\\to}\\ \\underline{{\\Pi}}(\\nu)$ , where $\\Pi(\\nu)\\in\\mathcal{G}(N,D)$ is the subspace lying on the geodesic flow path from the source to the target view: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Pi(\\nu)=[\\mathbf{P}_{s}\\,\\,\\,\\,\\,\\mathbf{R}][\\mathbf{U}_{1}\\Gamma(\\nu)\\,\\,\\,\\,\\,-\\,\\mathbf{U}_{2}\\Sigma(\\nu)]^{\\top}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{R}\\,\\in\\,\\mathbb{R}^{D\\times(D-N)}$ is the orthogonal complement of $\\mathbf{P}_{s}$ , i.e., $\\mathbf{R}^{\\top}\\mathbf{P}_{s}\\,=\\,\\mathbf{0}$ . $\\mathbf{T}(\\nu)$ and $\\Sigma(\\nu)$ are the diagonal matrices whose diagonal element at row $i$ can be defined as $\\gamma_{i}\\,=\\,\\cos(\\nu\\omega_{i})$ and $\\sigma_{i}\\,=\\,\\sin(\\nu\\omega_{i})$ . The list of $\\omega_{i}$ is the principal angles between source and target subspaces, i.e., $0\\leq\\omega_{1}\\leq\\ldots\\leq\\omega_{N}\\leq\\frac{\\pi}{2}$ . $\\mathbf{U}_{1}$ and $\\mathbf{U}_{2}$ are the orthonormal matrices obtained by the following pair of SVDs: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{P}_{s}^{\\top}\\mathbf{P}_{T}=\\mathbf{U}_{1}\\mathbf{\\Gamma}(1)\\mathbf{V}^{\\top}\\qquad\\mathbf{R}^{\\top}\\mathbf{P}_{T}=-\\mathbf{U}_{2}\\boldsymbol{\\Sigma}(1)\\mathbf{V}^{\\top}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $\\mathbf{P}_{s}^{\\top}\\mathbf{P}_{t}$ and $\\mathbf{R}^{\\top}\\mathbf{P}_{t}$ share the same singular vectors $\\mathbf{V}$ , we adopt the generalized Singular Value Decomposition (SVD) [18, 47] to decompose the matrices. In our approach, we model the cross-view structural changes $D_{x}$ by modeling the cosine similarity between projections along the geodesic flow $\\Pi(\\nu)$ . In particular, given a subspace $\\Pi(\\nu)$ on the geodesic flow path from the source to the target view, the cross-view geometric correlation of images between the source and target views can formulated by the inner product $g_{\\mathbf{II}(\\nu)}{\\big(}\\mathbf{x}_{s},\\mathbf{x}_{t}{\\big)}$ along the geodesic flow $\\Pi(\\nu)$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\ng(\\mathbf{x},\\mathbf{x}t)=\\int_{0}^{1}g_{\\Pi({\\nu})}(\\mathbf{x}_{s},\\mathbf{x}t)d{\\nu}=\\int_{0}^{1}\\mathbf{x}_{s}^{\\top}\\Pi({\\nu})\\Pi({\\nu})^{\\top}\\mathbf{x}_{t}d{\\nu}=\\mathbf{x}_{s}^{\\top}\\left(\\int_{0}^{1}\\Pi({\\nu})\\Pi({\\nu})^{\\top}d{\\nu}\\right)\\mathbf{x}_{t}=\\mathbf{x}_{s}^{\\top}\\mathbf{Q}\\mathbf{x}_{t}d\\mathbf{x}_{s}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{Q}=\\int_{0}^{1}\\mathbf{II}(\\nu)\\mathbf{II}(\\nu)^{\\top}d\\nu}\\end{array}$ . Intuitively, the matrix $\\mathbf{Q}$ represents the manifold structure between the source to the target view. Then, Eqn. (10) measures the cross-view structural changes between the source and the target domain based on their manifold structures. The matrix $\\mathbf{Q}$ can be obtained in a closed form [18, 47] as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Q}=[\\mathbf{P}_{s}\\mathbf{U}_{1}\\quad\\mathbf{R}\\mathbf{U}_{2}]\\left[\\mathbf{A}_{2}\\quad\\mathbf{A}_{3}\\right]\\left[\\mathbf{U}_{1}^{\\top}\\mathbf{P}_{s}^{\\top}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Lambda_{1},\\Lambda_{2}$ , and $\\Lambda_{3}$ are the diagonal matrices, whose diagonal elements at row $i$ can be defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{1,i}=1+\\frac{\\sin(2\\omega_{i})}{2\\omega_{i}},\\;\\lambda_{2,i}=\\frac{\\cos(2\\omega_{i})-1}{2\\omega_{i}},\\;\\lambda_{3,i}=1-\\frac{\\sin(2\\omega_{i})}{2\\omega_{i}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In practice, we model the cross-view structural changes $\\mathcal{D}_{x}$ via the cosine similarity along the geodesic flows. Finally, the cross-view structural changes $D_{x}$ can be formulate as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})=1-\\frac{\\mathbf{x}_{s}^{\\top}\\mathbf{Q}\\mathbf{x}_{t}}{\\|\\mathbf{Q}^{1/2}\\mathbf{x}_{s}\\|\\|\\mathbf{Q}^{1/2}\\mathbf{x}_{t}\\|}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, we can model the cross-view geometric correlation of segmentation $\\mathcal{D}_{y}$ via Geodesic Flow. ", "page_idx": 5}, {"type": "text", "text": "3.4 View-Condition Prompting to Cross-View Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "View-Condition Prompting Previous efforts [40, 38, 16, 73] in open-vocab segmentation have shown that a better prompting mechanism can provide more meaningful textual and visual knowledge. Prior work in open-vocab segmentation designed the prompt via the class names [64, 13, 38], e.g., \u201cclass1, ${\\tt c l a s s}_{1}$ , ..., class $K$ \u201d. Meanwhile, other methods improve the prompting mechanism by introducing the learnable variables into the prompt [40] or adding the task information [38]. This action helps to improve the context learning of the vision-language model. In our approach, we also exploit the effectiveness of designing prompting to cross-view learning. In particular, describing the view information can further improve the visual context learning, e.g., $^{\\bullet\\bullet}\\mathtt{c l a s s}_{1}$ , ${\\tt c l a s s}_{1}$ , ..., class $K$ captured from the [domain] view\u201d, where [domain] could be car (source domain) or drone (target domain). Therefore, we introduce a view-condition prompting mechanism by introducing the view information, i.e., captured from the [domain] view\u201d, into the prompt. Our view-condition prompt offers the context specific to visual learning, thus providing better transferability in cross-view segmentation. ", "page_idx": 5}, {"type": "text", "text": "Cross-view Correlation of View-Condition Prompts We hypothesize that the correlation of the input prompts across domains also provides the cross-view geometric correlation in their deep representations. In particular, let $\\mathbf{f}_{s}^{p}$ and ${\\bf f}_{t}^{p}$ be the deep textual embeddings of view-condition prompts $\\mathbf{p}_{s}$ and $\\mathbf{p}_{t}$ , and $\\mathcal{D}_{p}$ be metric measuring the correlation between $\\mathbf{f}_{s}^{p}$ and ${\\bf f}_{t}^{p}$ . In addition, since the textual encoder has been pre-trained on large-scale vision-language data [39, 27], the visual and the textual representations have been well aligned. Then, we argue that the correlation of textual feature representations across views, i.e., $\\mathcal{D}_{p}(\\mathbf{f}_{s}^{p},\\mathbf{f}_{t}^{p})$ , also provides the cross-view geometric correlation due to the embedded view information in the deep representation of prompts aligned with visual representations. Therefore, similar to Eqn. (4), we hypothesize the cross-view correlation of segmentation masks and textual features can be modeled as a linear relation with a scale factor $\\gamma$ as: ", "page_idx": 5}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/8d4e2fb42989308b157b2e8cfd809cb0365bbc1bde4eb85e91a6d1490a420c6f.jpg", "table_caption": ["Table 1: Effectiveness of Our Cross-view Adaptation Losses and Prompting Mechanism. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{\\Phi}_{p}(\\mathbf{f}_{s}^{p},\\mathbf{f}_{t}^{p})\\propto\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})\\Leftrightarrow\\mathcal{D}_{p}(\\mathbf{f}_{s}^{p},\\mathbf{f}_{t}^{p})=\\gamma\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, learning the cross-view adaptation with view-condition prompts can be formulated as follows: $\\begin{array}{r}{\\mathbf{\\mu}^{*}=\\arg\\operatorname*{min}_{\\mathbf{\\eta}_{a}}\\left[\\mathbb{E}_{\\mathbf{x}_{s},\\mathbf{p}_{s},\\mathbf{\\eta}_{s}}\\mathcal{L}_{M a s k}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{s})+\\mathbb{E}_{\\mathbf{x}_{s},\\mathbf{x}_{s},\\mathbf{x}_{t},\\mathbf{p}_{t}}\\left(\\lambda_{I}||\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})+\\lambda_{P}||\\mathcal{D}_{p}(\\mathbf{f}_{s}^{p},\\mathbf{f}_{t}^{p})-\\gamma\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})||\\right)\\right]}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "where $\\lambda_{I}$ and $\\lambda_{P}$ are the balanced-weight of losses. Similar to metrics $D_{x}$ and $\\mathcal{D}_{y}$ , we also adopt the geodesic flow path to model the cross-view correlation metric $\\mathcal{D}_{p}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets, Benchmarks, and Implementation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To efficiently evaluate cross-view adaptation, the cross-view benchmarks are set up from the car to the drone view. Following common practices in UDA [23, 58], we choose SYNTHIA [44], GTA [43], and BDD100K [68] as the source domains while UAVID [33] is chosen as the target domain. We chose to adopt these datasets because they share a class of interests and are commonly used in UDA and segmentation benchmarks [23, 61]. ", "page_idx": 6}, {"type": "text", "text": "SYNTHIA $\\rightarrow$ UAVID Benchmark SYNTHIA and UAVID share five classes of interest, i.e., Road, Building, Car, Tree, and Person. Since the UAVID dataset annotated cars, trucks, and buses as a class of Car, we collapse these classes in SYNTHIA into a single class of Car. ", "page_idx": 6}, {"type": "text", "text": "$\\mathbf{\\vec{y}}\\mathbf{T}\\mathbf{A}\\rightarrow\\mathbf{\\vec{l}}$ UAVID Benchmark consists of five classes in the SYNTH $\\mathrm{IA}\\to\\mathrm{UAX}$ VID benchmark and includes one more class of Terrain. Therefore, the GTA $\\rightarrow$ UAVID benchmark has six classes of interest, i.e., Road, Building, Car, Tree, Terrain, and Person. ", "page_idx": 6}, {"type": "text", "text": "BDD $\\rightarrow$ UAVID Benchmark is a real-to-real cross-view adaptation setting. Similar to $\\mathrm{GTA\\rightarrow}$ UAVID benchmark, there are six classes of interest between BDD100K and UAVID. In our experiments, we adopt the mean Intersection over Union (mIoU) metric to measure the performance. ", "page_idx": 6}, {"type": "text", "text": "Implementation We adopt Mask2Former [8] (ResNet 101) with Semantic Context Interaction of FreeSeg [38] and pre-trained text encoder of CLIP [39] for our open-vocab segmentation networks. Our balanced weights of losses are set to $\\lambda_{I}=1.0$ and $\\lambda_{P}=0.5$ . Further details of our networks and hyper-parameters are provided in the appendix. ", "page_idx": 6}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Effectiveness of Cross-view Adaptation and Prompting Mechanisms Table 1 analyzes the effectiveness of prompting mechanisms, i.e., i.e., with and without Prompting, with and without Cross-view Adaptation (in Eqn. (6)), with and without View-Condition Prompting (in Eqn. (15)). For supervised results, we train two different models on UAVID with and without the ", "page_idx": 6}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/02b0c2cb1a9da46c795f697f077978c617054fad531f9ca1b73f4221eaacfc9e.jpg", "table_caption": ["Table 2: Effectiveness of Backbones and Crossview Metrics. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Terrain class on two benchmarks. As in Table 1, the cross-view adaptation loss in Eqn. (6) significantly improve the performance of segmentation models. With prompting and cross-view adaptation, the mIoU performance is further boosted, i.e., the mIoU performance achieves $48.6\\%$ and $40.1\\%$ on two benchmarks. Additionally, by further using the view-condition prompting mechanism with our cross-view loss in Eqn. (15), the mIoU results are slightly improved by $+1.1\\%$ and $+1.7\\%$ on two benchmarks compared to the one without view-condition prompting. Our results have closed the gap with the upper-bound results where the models are trained on UAVID with labels. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Cross-view Correlation Metrics and Network Backbones Table 2 studies the impact of choosing metrics and network backbones. We consider two options, i.e., Euclidean Metric and our Geodesic Flow-based Metric, for correlation metrics $D_{x}$ , $\\mathcal{D}_{y}$ , and $\\mathcal{D}_{p}$ . As shown in Table 2, our Geodesic Flow-based metrics significantly improve the performance of our cross-view adaptation. It has shown that our approach is able to measure the structural changes across views better than using the Euclidean metrics. In addition, by using the more powerful backbone (Swin), the performance of cross-view adaptation is further improved. ", "page_idx": 7}, {"type": "text", "text": "Learning Parameters Table 3 illustrates the impact of the linear scaling factors $\\alpha$ and $\\beta$ . As in Table 3, the mIoU performance has been majorly affected by the relation between images and segmentation. The best performance is gained at the optimal value of $\\alpha=1.5$ . Since the variation of RGB images is higher than the segmentation, the small value $\\alpha$ could not correctly scale the rela", "page_idx": 7}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/ba8b8afb72b9c8f85b8cc1c6fcf27c135b64b4bd568d28a2ddedfb26001fdcea.jpg", "table_caption": ["Table 3: Effectiveness of Linear Scale Factors, i.e., $\\alpha$ and $\\gamma$ , and Subspace dimension $D$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "tion between images and segmentation while the higher value of $\\alpha$ exaggerates the structural change of segmentation masks. Additionally, the change of $\\gamma$ slightly affects the mIoU performance. Since the textual features are well-aligned with the image, the performance of segmentation models when changing $\\gamma$ also behaves similarly to the changes of $\\alpha$ . However, the linear scale factor $\\alpha$ is more sensitive to mIoU results since the images play a more important role in the segmentation results due to the pixel-wise corresponding of images and segmentation. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Subspace Dimension in Geodesic Flow Table 3 reveals the importance of choosing the subspace dimension. The cross-view geometric structural change is better modeled by increasing the dimension of the subspaces. As in Table 3, the performance is improved when the dimension is increased from 96 to 256. However, beyond that point, the mIoU performance tends to be dropped. We have observed that low dimensionality cannot model the structural changes across views since it captures small variations in structural changes. Conversely, higher dimensionality includes more noise in the cross-view structural changes and increases the computational cost. We also study the impact of batch size in our appendix. ", "page_idx": 7}, {"type": "text", "text": "Qualitative Results. To further illustrate the effectiveness of our proposed, we visualize the results produced by our model. In the model without prompting, Figure 4 illustrates the results of our cross-view adaptation compared to those without adaptation. As shown in the results, our approach can effectively segment the objects in the drone view. We also compare with the prior ProDA [69] and CROVIA [50] methods. Our qualitative results remain better than the prior adaptation method. For the model with prompting, Figure 5 illustrates the effectiveness of our approach in three cases: ", "page_idx": 7}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/e9ba4bd5f96403fe49a557a4555d0d56180971170e491622a9cc7be461734fd0.jpg", "img_caption": ["Figure 4: The Qualitative Results of Cross-View Adaptation (Without Prompt). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/9443f17b474e780f5c0e71555693ab23e7c1088dca708fa920c1c6c0491f232f.jpg", "img_caption": ["Figure 5: The Qualitative Results of Cross-View Adaptation (With Prompt). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "without adaptation, with cross-view adaptation, and with view-condition prompting. As shown in the results, our cross-view adaptation can efficiently model the segmentation of the view. By using the view-condition prompting, our model can further improve the segmentation of persons and vehicles. ", "page_idx": 8}, {"type": "text", "text": "4.3 Comparisons with Prior UDA Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SYNTHIA $\\rightarrow$ UAVID As shown in Table 4, our EAGLE has achieved SOTA results and outperforms prior view transformation (i.e., Polar Transform [45]) UDA methods by a large margin. For fair comparisons, we adopt the DeepLab [3] and DAFormer [23] for the segmentation network. In particular, our mIoU results using DeepLab and DAFormer are $45.\\bar{2\\%}$ and $50.8\\%$ . In the DAFormer backbone, the ", "page_idx": 8}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/a8b0bf91ef90ac224bbd92dec02b973e3e0899f6354a43278bafd4299535e56d.jpg", "table_caption": ["Table 4: Comparisons with Domain Adaptation Approaches (Without Prompting). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "mIoU results of our approach are higher than CROVIA [50] and MIC [25] by $+4.8\\%$ and $+9.0\\%$ . The IoU result of each class also consistently outperformed the prior methods. Highlighted that although our approach does NOT use depth labels, our results still outperform the one using depths, i.e., DADA [59]. It has emphasized that our approach is able to better capture the cross-view structural changes compared to prior methods. Figure 4 illustrates our qualitative results compared to ProDA [69] and CROVIA [50]. ", "page_idx": 8}, {"type": "text", "text": "$\\mathbf{GTA}\\rightarrow\\mathbf{UAVID}$ As shown in Table 4, our effectiveness outperforms prior polar view transformation [45] and domain adaptation approaches when measured by both mIoU performance and the IoU accuracy of each class. In particular, our mIoU performance using DeepLab and DAFormer network achieves $36.7\\%$ and $40.7\\%$ , respectively. Our results have substantially closed the performance gap with the supervised results. By using the better segmentation-based network, i.e., Mask2Former with ResNet, the performance of our approach is further improved to $40.1\\%$ compared to DeepLab. ", "page_idx": 8}, {"type": "text", "text": "4.4 Comparisons with Open-vocab Segmentation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare EAGLE with the prior open-vocab segmentation methods, i.e., DenseCLIP [40] and an adaptive prompting FreeSeg [38] with four settings, i.e., Source Only, with AdvEnt [58], and with SAC [1], and our Cross-View Adaptation in Eqn. (6) (without view-condition). ", "page_idx": 8}, {"type": "text", "text": "Open-vocab Semantic Segmentation As in Table 5, the mIoU performance of our proposed approach with cross-view adaptation outperforms prior DenseCLIP by a large margin on SYNTHI $\\Delta\\rightarrow\\mathrm{U},$ AVID. By using our cross-view geometric adaptation loss, the performance of DenseCLIP and FreeSeg is further enhanced, i.e., higher than DenseCLIP and FreeSeg with SAC by $+3.7\\%$ and $+5.0\\%$ . While FreeSeg [38] with our cross-view adaptation slightly outperforms EAGLE due to its adaptive prompting, our EAGLE approach with the better view-condition prompting achieves higher mIoU performance. Similarly, our proposed cross-view loss consistently improves the performance of DenseCLIP and FreeSeg on $\\mathrm{GTA}\\rightarrow\\mathrm{UAVID}$ . The mIoU results of DenseCLIP and FreeSeg using our cross-view loss achieve $37.3\\%$ and $44.4\\%$ . By further using the view-condition prompting mechanism, our mIoU result is considerably higher than FreeSeg with our cross-view adaptation by $+1.3\\%$ . Figure 6 visualizes our qualitative results of our proposed approach. ", "page_idx": 8}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/bd2262c44d37f7c898c142173ccba1a251431df4d0a02d4dee86e26474064388.jpg", "table_caption": ["Table 5: Comparisons with Open-vocab Semantic Segmenta-Table 6: Comparisons with Open-vocab tion. Segmentation on Seen (mIoUS) and Unseen (mIoUU) Classes. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/95aa02b1bcb56393c0c0dd9abb9e4cd5516a9ee19e7c8e822f0f9414d826f5db.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Open-vocab Segmentation on Unseen Classes Table 6 illustrates the experimental results of our cross-view adaptation approach on unseen classes. In this experiment, we consider classes of Tree and Person as the unseen classes. As shown in the results, our cross-view adaptation approach with a view-condition prompting mechanism has achieved the best mIoU performance on unseen classes on both benchmarks, i.e., ${\\bar{3}9.3\\%}$ and $39.6\\%$ on two benchmarks. Our experimental results have further confirmed the effectiveness and the generalizability of our cross-view geometric modeling and view-condition prompting approach to the open-vocab segmentation across views. ", "page_idx": 9}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/228ea1cafb67d187fc8f9a0e38919063dcc3ae2b6f5d5c030a24e05d55147929.jpg", "img_caption": ["Figure 6: Results of Segmenting Cars, Trees, Persons. (A) Input, (B) FreeSeg [38], and (C) Our EAGLE. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Real-to-Real Cross-view Adaptation Setting We evaluated our approach in the real-to-real setting, i.e., $\\mathrm{BDD}\\rightarrow\\mathrm{UAVID}$ . Our approach is evaluated in two different settings, i.e., Unsupervised Domain Adaptation and Open-Vocab Semantic Segmentation. As shown in Table 7, our results have shown a significant improvement in our approach in real-to-real settings in both unsupervised domain adaptation and openvocab semantic segmentation. While the results of prior unsupervised domain adaptation, i.e., ", "page_idx": 9}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/d3a0f44a08fdf37a2b73e5a64f8613ff4c17de10dc12d0fcabafd33dcc1763a2.jpg", "table_caption": ["Table 7: Comparison with Prior Adaptation Methods and Open-Vocab Segmentation on Real-toReal Cross-View Setting. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "BiMaL [51] and DAFormer [23], gain limited performance due to their limits in cross-view learning, our method outperforms other methods these prior methods by a large margin. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper has presented a novel unsupervised cross-view adaptation approach that models the geometric correlation across views. We have introduced the Geodesic Flow-based metric to better model geometric structural changes across camera views. In addition, a new view-condition prompting mechanism has been presented to further improve the cross-view modeling. Through our theoretical analysis and SOTA performance on both unsupervised cross-view adaptation and open-vocab segmentation, our approach has shown its effectiveness in cross-view modeling and improved robustness of segmentation models across views. ", "page_idx": 9}, {"type": "text", "text": "Limitations Our study has selected a set of learning hyper-parameters to support our hypothesis and experiments. However, this work can potentially contain several limitations related to learning parameters and linear relation hypothesis in Eqn. (4). The details of the limitations are discussed in the appendix. We believe that these limitations will motivate future studies to improve our unsupervised cross-view adaptation learning approach. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment This work is partly supported by NSF Data Science, Data Analytics that are Robust and Trusted (DART), NSF SBIR Phase 2, and Arkansas Biosciences Institute (ABI) grants. We also acknowledge the Arkansas High-Performance Computing Center for providing GPUs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] N. Araslanov, , and S. Roth. Self-supervised augmentation consistency for adapting semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[2] M. Armando, S. Galaaoui, F. Baradel, T. Lucas, V. Leroy, R. Br\u00e9gier, P. Weinzaepfel, and G. Rogez. Cross-view and cross-pose completion for 3d human understanding. arXiv preprint arXiv:2311.09104, 2023.   \n[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. TPAMI, 2018.   \n[4] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.   \n[5] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Domain adaptive faster r-cnn for object detection in the wild. In CVPR, 2018.   \n[6] Y. Chen, W. Li, and L. Van Gool. Road: Reality oriented adaptation for semantic segmentation of urban scenes. In CVPR, 2018.   \n[7] Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017. [8] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1290\u20131299, 2022.   \n[9] B. Cheng, A. G. Schwing, and A. Kirillov. Per-pixel classification is not all you need for semantic segmentation. In NeurIPS, 2021.   \n[10] B. Coors, A. P. Condurache, and A. Geiger. Nova: Learning to see in novel viewpoints and domains. In 2019 International Conference on 3D Vision (3DV), pages 116\u2013125, 2019.   \n[11] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.   \n[12] D. Di Mauro, A. Furnari, G. Patan\u00e8, S. Battiato, and G. M. Farinella. Sceneadapt: Scene-based domain adaptation for semantic segmentation using adversarial learning. Pattern Recognition Letters, 136:175\u2013182, 2020.   \n[13] J. Ding, N. Xue, G.-S. Xia, and D. Dai. Decoupling zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11583\u201311592, 2022.   \n[14] S. Ettedgui, S. Abu-Hussein, and R. Giryes. Procst: Boosting semantic segmentation using progressive cyclic style-transfer, 2022.   \n[15] M. Fahes, T.-H. Vu, A. Bursuc, P. P\u00e9rez, and R. de Charette. Poda: Prompt-driven zero-shot domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18623\u201318633, 2023.   \n[16] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, pages 1\u201315, 2023.   \n[17] G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Scaling open-vocabulary image segmentation with image-level labels. In European Conference on Computer Vision, pages 540\u2013557. Springer, 2022.   \n[18] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2066\u20132073, 2012.   \n[19] G. Goyal, N. Noceti, and F. Odone. Cross-view action recognition with small-scale datasets. Image and Vision Computing, 120:104403, 2022.   \n[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[21] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In ICML, 2018.   \n[22] C.-A. Hou, Y.-R. Yeh, and Y.-C. F. Wang. An unsupervised domain adaptation approach for cross-domain visual classification. In 2015 12th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1\u20136. IEEE, 2015.   \n[23] L. Hoyer, D. Dai, and L. Van Gool. DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation. In CVPR, 2022.   \n[24] L. Hoyer, D. Dai, and L. Van Gool. HRDA: Context-aware high-resolution domain-adaptive semantic segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.   \n[25] L. Hoyer, D. Dai, H. Wang, and L. Van Gool. Mic: Masked image consistency for contextenhanced domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11721\u201311732, 2023.   \n[26] J. Huang, D. Guan, A. Xiao, and S. Lu. Cross-view regularization for domain adaptive panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10133\u201310144, 2021.   \n[27] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n[28] K.-H. Lee, G. Ros, J. Li, and A. Gaidon. SPIGAN: Privileged adversarial learning from simulation. In ICLR, 2019.   \n[29] B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl. Language-driven semantic segmentation. In International Conference on Learning Representations, 2022.   \n[30] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, K.-W. Chang, and J. Gao. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965\u201310975, 2022.   \n[31] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, and D. Marculescu. Openvocabulary semantic segmentation with mask-adapted clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7061\u20137070, 2023.   \n[32] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[33] Y. Lyu, G. Vosselman, G.-S. Xia, A. Yilmaz, and M. Y. Yang. Uavid: A semantic segmentation dataset for uav imagery. ISPRS Journal of Photogrammetry and Remote Sensing, 165:108 \u2013 119, 2020.   \n[34] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim. Image to image translation for domain adaptation. In CVPR, 2018.   \n[35] H.-Q. Nguyen, T.-D. Truong, X. B. Nguyen, A. Dowling, X. Li, and K. Luu. Insect-foundation: A foundation model and large-scale 1m dataset for visual insect understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21945\u201321955, 2024.   \n[36] B. Pan, J. Sun, H. Y. T. Leung, A. Andonian, and B. Zhou. Cross-view semantic segmentation for sensing surroundings. IEEE Robotics and Automation Letters, 5(3):4867\u20134873, 2020.   \n[37] F. Pan, I. Shin, F. Rameau, S. Lee, and I. S. Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In CVPR, 2020.   \n[38] J. Qin, J. Wu, P. Yan, M. Li, R. Yuxi, X. Xiao, Y. Wang, R. Wang, S. Wen, X. Pan, et al. Freeseg: Unified, universal and open-vocabulary image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19446\u201319455, 2023.   \n[39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u2013 8763. PMLR, 2021.   \n[40] Y. Rao, W. Zhao, G. Chen, Y. Tang, Z. Zhu, G. Huang, J. Zhou, and J. Lu. Denseclip: Languageguided dense prediction with context-aware prompting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[41] K. Regmi and M. Shah. Bridging the domain gap for ground-to-aerial image matching. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 470\u2013479, 2019.   \n[42] H. Ren, Y. Yang, H. Wang, B. Shen, Q. Fan, Y. Zheng, C. K. Liu, and L. Guibas. Adela: Automatic dense labeling with attention for viewpoint shift in semantic segmentation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8069\u20138079, 2022.   \n[43] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016.   \n[44] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, 2016.   \n[45] Y. Shi, L. Liu, X. Yu, and H. Li. Spatial-aware feature aggregation for image based cross-view geo-localization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u2019e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 10090\u201310100. Curran Associates, Inc., 2019.   \n[46] Y. Shi, X. Yu, D. Campbell, and H. Li. Where am i looking at? joint location and orientation estimation by cross-view matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[47] C. Simon, P. Koniusz, and M. Harandi. On learning the geodesic path for incremental learning. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 1591\u20131600, 2021.   \n[48] N. Sun, Q. Lu, W. Zheng, J. Liu, and G. Han. Unsupervised cross-view facial expression image generation and recognition. IEEE Transactions on Affective Computing, 2020.   \n[49] A. Toker, Q. Zhou, M. Maximov, and L. Leal-Taixe. Coming down to earth: Satellite-to-street view synthesis for geo-localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6488\u20136497, June 2021.   \n[50] T.-D. Truong, C. N. Duong, A. Dowling, S. L. Phung, J. Cothren, and K. Luu. Crovia: Seeing drone scenes from car perspective via cross-view adaptation, 2023.   \n[51] T.-D. Truong, C. N. Duong, N. Le, S. L. Phung, C. Rainwater, and K. Luu. Bimal: Bijective maximum likelihood approach to domain adaptation in semantic scene segmentation. In IEEE International Conference on Computer Vision (ICCV), pages 8548\u20138557, 2021.   \n[52] T.-D. Truong, P. Helton, A. Moustafa, J. D. Cothren, and K. Luu. Conda: Continual unsupervised domain adaptation learning in visual perception for self-driving cars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5642\u20135650, 2024.   \n[53] T.-D. Truong, N. Le, B. Raj, J. Cothren, and K. Luu. Fredom: Fairness domain adaptation approach to semantic scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[54] T.-D. Truong and K. Luu. Cross-view action recognition understanding from exocentric to egocentric perspective. arXiv preprint arXiv:2305.15699, 2023.   \n[55] T.-D. Truong, H.-Q. Nguyen, B. Raj, and K. Luu. Fairness continual learning approach to semantic scene understanding in open-world environments. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[56] T.-D. Truong, U. Prabhu, B. Raj, J. Cothren, and K. Luu. Falcon: Fairness learning via contrastive attention approach to continual semantic scene understanding in open world. arXiv preprint arXiv:2311.15965, 2023.   \n[57] V. Vidit, M. Engilberge, and M. Salzmann. Learning transformations to reduce the geometric shift in object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17441\u201317450, 2023.   \n[58] T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. P\u00e9rez. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In CVPR, 2019.   \n[59] T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. P\u00e9rez. Dada: Depth-aware domain adaptation in semantic segmentation. In ICCV, 2019.   \n[60] K. Wang, D. Kim, R. Feris, and M. Betke. Cdac: Cross-domain attention consistency in transformer for domain adaptive semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11519\u201311529, 2023.   \n[61] L. Wang, R. Li, C. Zhang, S. Fang, C. Duan, X. Meng, and P. M. Atkinson. Unetformer: A unet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery. ISPRS Journal of Photogrammetry and Remote Sensing, 190:196\u2013214, 2022.   \n[62] Q. Wang, W. Min, Q. Han, Z. Yang, X. Xiong, M. Zhu, and H. Zhao. Viewpoint adaptation learning with cross-view distance metric for robust vehicle re-identification. Information Sciences, 564:71\u201384, 2021.   \n[63] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021.   \n[64] M. Xu, Z. Zhang, F. Wei, Y. Lin, Y. Cao, H. Hu, and X. Bai. A simple baseline for openvocabulary semantic segmentation with pre-trained vision-language model. In European Conference on Computer Vision, pages 736\u2013753. Springer, 2022.   \n[65] Z. Yan, X. Yu, Y. Qin, Y. Wu, X. Han, and S. Cui. Pixel-level intra-domain adaptation for semantic segmentation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 404\u2013413, 2021.   \n[66] Y. Yao and H. S. Park. Multiview cross-supervision for semantic segmentation. arXiv preprint arXiv:1812.01738, 2018.   \n[67] Y. Yin, W. Hu, Z. Liu, G. Wang, S. Xiang, and R. Zimmermann. Crossmatch: Source-free domain adaptive semantic segmentation via cross-modal consistency training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21786\u201321796, 2023.   \n[68] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2636\u20132645, 2020.   \n[69] P. Zhang, B. Zhang, T. Zhang, D. Chen, Y. Wang, and F. Wen. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation. arXiv preprint arXiv:2101.10979, 2021.   \n[70] Q. Zhang, W. Lin, and A. B. Chan. Cross-view cross-scene multi-view crowd counting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 557\u2013567, 2021.   \n[71] D. Zhao, S. Wang, Q. Zang, D. Quan, X. Ye, R. Yang, and L. Jiao. Learning pseudo-relations for cross-domain semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19191\u201319203, 2023.   \n[72] B. Zhou and P. Kr\u00e4henb\u00fchl. Cross-view transformers for real-time map-view semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13760\u201313769, 2022.   \n[73] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[74] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.   \n[75] S. Zhu, M. Shah, and C. Chen. Transgeo: Transformer is all you need for cross-view image geo-localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1162\u20131171, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1 Proof of Eqn. (9) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As shown in our Eqn. (16), our Geodesic Flow-based metrics have the upper bound as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall\\mathbf{x}_{s},\\mathbf{x}_{t}:}&{{}\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})=1-\\frac{\\mathbf{x}_{s}^{\\top}\\mathbf{Q}\\mathbf{x}_{t}}{\\lVert\\mathbf{Q}^{1/2}\\mathbf{x}_{s}\\rVert\\lVert\\mathbf{Q}^{1/2}\\mathbf{x}_{t}\\rVert}\\le2}\\\\ {\\forall\\mathbf{y}_{s},\\mathbf{y}_{t}:}&{{}\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})=1-\\frac{\\mathbf{y}_{s}^{\\top}\\mathbf{Q}\\mathbf{y}_{t}}{\\lVert\\mathbf{Q}^{1/2}\\mathbf{y}_{s}\\rVert\\lVert\\mathbf{Q}^{1/2}\\mathbf{y}_{t}\\rVert}\\le2}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In addition, as $\\mathcal{D}_{x}$ is the distance metric, this metric should satisfy the following triangular inequality as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{x}(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t})\\leq\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})+\\mathcal{D}_{x}(\\mathbf{x}_{t},\\bar{\\mathbf{x}}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, $\\mathcal{D}_{y}$ should satisfy the following triangular inequality as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{D}_{y}(\\mathbf{y}_{t},\\bar{\\mathbf{y}}_{t})+\\mathcal{D}_{y}(\\bar{\\mathbf{y}}_{t},\\mathbf{y}_{s})\\geq\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})}\\\\ &{\\Leftrightarrow\\mathcal{D}_{y}(\\mathbf{y}_{t},\\bar{\\mathbf{y}}_{t})\\geq\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})-\\mathcal{D}_{y}(\\bar{\\mathbf{y}}_{t},\\mathbf{y}_{s})}\\\\ &{\\Leftrightarrow\\,-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{t},\\bar{\\mathbf{y}}_{t})\\leq-\\alpha\\left(\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})-\\mathcal{D}_{y}(\\bar{\\mathbf{y}}_{t},\\mathbf{y}_{s})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, from Eqn. (16) and Eqn. (17) above, we can further derive as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{D}_{x}(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t})}\\\\ &{\\ \\leq\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})+\\mathcal{D}_{x}(\\mathbf{x}_{t},\\bar{\\mathbf{x}}_{t})-\\alpha\\left(\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})-\\mathcal{D}_{y}(\\bar{\\mathbf{y}}_{t},\\mathbf{y}_{s})\\right)}\\\\ &{\\leq\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})+\\mathcal{D}_{x}(\\mathbf{x}_{t},\\bar{\\mathbf{x}}_{t})+\\alpha\\mathcal{D}_{y}(\\bar{\\mathbf{y}}_{t},\\mathbf{y}_{s})}\\\\ &{\\ \\leq\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})+\\underbrace{2(1+\\alpha)}_{C o n s t a n t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\alpha$ is the constant linear scale value, therefore, we can further derive as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Rightarrow\\left\\lvert\\lvert\\mathcal{D}_{x}(\\mathbf{x}_{s},\\bar{\\mathbf{x}}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\bar{\\mathbf{y}}_{t})\\rvert\\right\\rvert}\\\\ &{\\qquad\\qquad=\\mathcal{O}(\\lvert\\lvert\\mathcal{D}_{x}(\\mathbf{x}_{s},\\mathbf{x}_{t})-\\alpha\\mathcal{D}_{y}(\\mathbf{y}_{s},\\mathbf{y}_{t})\\rvert\\rvert)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "2 Implementation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We follow the implementation of Mask2Former [8] and FreeSeg [38] with ResNet [20] and Swin backbones [32] for our segmentation network. In particular, we adopt Mask2Former with Semantic Context Interaction of FreeSeg [38] for our open-vocab segmentation network. We use the pre-trained text encoder of CLIP [39]. The textual features $\\mathbf{f}_{s}^{p}$ and $\\mathbf{f}_{t}^{\\bar{p}}$ are obtained by the CLIP textual encoder. Following common practices [38, 31], we adopt the open-vocab segmentation loss of FreeSeg [38] to our supervised loss $\\mathcal{L}_{M a s k}$ . For experiments without prompting, we use the Mask2Former network. Following the UAV protocol of [61], the image size is set to $512\\times512$ . The linear scale factors $\\alpha$ and $\\gamma$ are set to $\\alpha=1.5$ and $\\gamma=1.0$ , respectively. For the Geodesic Flow modeling, we adopt the implementation of generalized SVD decomposition [18, 47] in the framework. The subspace dimension in our geodesic flow-based metrics is set to $D\\,=\\,256$ . The batch size and the base learning rate in our experiments are set to 16 and $2.5\\times10^{-4}$ . The balanced weights of losses in our experiments are set to $\\lambda_{I}=1.0$ and $\\lambda_{P}=0.5$ . During training, the classes in the prompts are generated similarly for both view images. ", "page_idx": 15}, {"type": "text", "text": "In our Geodesic Flow-based metrics, the subspaces of images and ground-truth segmentation of the source domain are pre-computed on the entire data. For the language space, we compute the subspaces of each view based on the textual feature representations of all possible prompts in each domain. Meanwhile, the subspaces of the segmentation on the target domain are computed based on the current batch of training. For the implementation of DenseCLIP [40] and FreeSeg [38] with AdvEnt [58], we perform the adaptation process on the mask predictions. Meanwhile, we adopt the pseudo labels and the self-supervised framework of SAC[1] for the implementation of DenseCLIP [40] and FreeSeg [38] with SAC [1]. ", "page_idx": 15}, {"type": "table", "img_path": "AXcYtHQnxt/tmp/4b169a127436657d9c23784adc20b28975142ce920b349d2a145f8aa482fe221.jpg", "table_caption": ["Table 8: Effectiveness of Batch Size. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "3 Ablation Study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Effectiveness of Batch Size Table 8 illustrates the impact of the batch size on the performance of cross-view domain adaptation. By increasing the batch size, the mIoU performance is also increased accordingly on both benchmarks. This result has illustrated that the small batch size could not have enough samples to approximate the subspace that represents geometric structures. Meanwhile, the subspace created from the large batch size will be ale to capture the geometric structure of drone-view scenes. However, due to the limitation of GPU resources, we could not evaluate the cross-view adaptation model with larger batch size. ", "page_idx": 16}, {"type": "text", "text": "Subspace Representation of Geodesic Flow-based Metrics To illustrate the ability of structural learning of our geodesic flow-based metrics, we use a subset of images of the car-view and the drone-view dataset to visualize the base structure of subspaces obtained from the PCA algorithm. Fig. 7 visualizes the mean structures of car-view and drone-view images. As shown in Fig. 7, The subspaces of car-view images represent the geometric structures of car-view data, i.e., the road in the middle, buildings, trees on two sides, etc. Meanwhile, the geometric structures of the drone view have also been illustrated in the figure with structures and topological distributions of objects (e.g., the road in the middle and trees and buildings on the sides) on the scenes. The results have illustrated the base geometric structures of the car-view and the drone-view data. Then, by modeling the geodesic flow path across two subspaces, our metric is able to measure the cross-view geometric structural changes (i.e., the change of structures and topological layouts of the scene) from the car view to the drone view. Our experimental results in other ablation studies have further confirmed our effectiveness in geometric structural modeling across views. Figure 8 illustrates the feature distributions with and without our proposed approach. As shown in Figure 8, our approach can help to improve the feature representations of classes, and the cluster of each class is more compact, especially in classes of car, tree, and person. ", "page_idx": 16}, {"type": "text", "text": "4 Discussion of Limitations and Broader Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Limitations. In our paper, we have specified a set of hyper-parameters and network designs to support our hypothesis and theoretical analysis. However, our proposed approach could potentially consist of several limitations. First, our work focuses on studying the impact of cross-view geometric adaptation loss and view-condition prompting mechanisms on the segmentation models across views. The balanced weights among weights, i.e., $\\lambda_{I}$ and $\\lambda_{P}$ , have not been fully exploited. We leave this investigation as our future experiments. Second, although the datasets and benchmarks used in our experiments have sufficiently illustrated the effectiveness of our proposed cross-view adaptation learning approach, the lack of diverse classes and categories in datasets is also a potential limitation. Third, the hypothesis of the linear relations across views of images and segmentation mask, i.e., $\\alpha$ , and textual representations and segmentation masks, i.e., $\\gamma$ , could limit the performance of the relation. The non-trivial relations across views should be deeply exploited in future research. Also, while the implementation of Mask2Former and FreeSeg is adopted to develop our approach, the experiments with other open-vocab segmentation networks should be considered in subsequent research studies. These aforementioned limitations will motivate new studies to further improve the methodology, datasets, and benchmarks of the cross-view adaptation learning paradigm. ", "page_idx": 16}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/56a8bc05396116d36154eeff34c085e287a704cad9ee8e2de75a83bee65c5468.jpg", "img_caption": ["Figure 7: The Structures of Subspaces of Car-View and Drone-View Dataset Learned From a Subset of Images. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "AXcYtHQnxt/tmp/e24b1983632f8349a03f58fbaf12caf2b21e1d0efaeb23e7b10b2c29a45d4ce6.jpg", "img_caption": ["Figure 8: The Feature Distribution of Classes in SYNTHIA $\\rightarrow$ UAVID Experiments. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Broader Impact. Our paper could bring significant potential for various applications that require learning across camera viewpoints. Our approach enables generalizability across camera views, thus enhancing the robustness of the segmentation model across views. In addition, our approach helps to reuse off-the-shelf large-scale data while reducing the effort of manually labeling data of new camera views. ", "page_idx": 17}, {"type": "text", "text": "5 Other Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "While the important and closely related work to our approach has been presented in our main paper, we also would like to review some other research studies that are related to our method as follows. In particular, Brady et al. [72] presented a cross-view transformer that learns the camera-aware positional embeddings. Although the views are captured from left and right angles, the camera positions in the approach remain at the same altitude. Similarly, Pan et al. [36] present a View Parsing Network to accumulate features across first-view observations with multiple angles. Yao et al. [66] proposed a semi-supervised learning approach to learn the segmentation model from multiple views of an image. Huang et al. [26] a cross-style regularization for domain adaptation in panoptic segmentation by imposing the consistency of the segmentation between the target images and stylized target images. Wang et al. [62] proposed a viewpoint adaptation framework for the person re-identification problem by using the generative model to generate training data across various viewpoints. Hou et al. [22] presented a matching cross-domain data approach to domain adaptation in visual classification. Sun et al. [48] proposed a cross-view facial expression adaptation framework to parallel synthesize and recognize cross-view facial expressions. Goyal et al. [19] introduced a cross-view action recognition approach to transferring the feature representations to different views. Zhang et al. [70] proposed a multi-view crowd counting approach that adaptively chooses and aggregates multi-cameras and a noise view regularization. Armando et al. [2] proposed a self-supervised pre-training approach to human understanding learned on pairs of images captured from different viewpoints. Then, the pre-trained models are later used for various downstream human-centric tasks. In summary, these prior cross-view methods could require either a pair of cross-view images [2] or images captured at the same altitude with different angles [26, 72, 22]. In addition, the cross-view geometric correlation modeling has not been exploited in these prior studies [26, 72, 22, 2] ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 18}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 18}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 18}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 18}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 18}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The claims declared in the abstract match with the contributions, experimental results, and scope of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The limitations of the paper are discussed in the last section of the paper. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The proof of formula is provided in the supplementary. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The details of datasets and implementations are presented in the experimental sections. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The code will be published may the paper be accepted. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 20}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The details of training and testing are presented in the experimental section. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Following the standard evaluation of semantic segmentation, we evaluate our model by the standard mIoU metrics instead of the statistical tests. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The computational resources used in our experiments are presented in the experimental section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The content of the paper and datasets strictly follows the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not have a negative societal impact. The broader impact is discussed in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not have a risk. The released models will be available may the paper be accepted. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper provides all the references to code, data, and models used in the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not introduce the new dataset. The code of the paper will be published may the paper be accepted. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research in this paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research in this paper does not involve crowdsourcing nor research with human subjects. Thus, there is no requirement for IRB. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]