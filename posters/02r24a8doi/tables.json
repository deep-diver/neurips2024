[{"figure_path": "02r24A8doi/tables/tables_2_1.jpg", "caption": "Table 1: Instance-dependent regret bounds for different algorithms under the linear MDP setting. Here d is the dimension of the linear function (s, \u03b1), H is the horizon length, \u0394 is the minimal suboptimality gap. All results in the table represent high probability regret bounds. The regret bound depends the number of episodes K in He et al. (2021a) and the minimum positive eigenvalue \u03bb of features mapping in Papini et al. (2021b). Misspecified MDP? indicates if the algorithm can (\u2713) handle the misspecified linear MDP or not (\u00d7).", "description": "This table compares three different reinforcement learning algorithms under the linear Markov Decision Process (MDP) setting.  It shows the instance-dependent regret bounds achieved by each algorithm, highlighting the dependence on factors like the feature dimension (d), horizon (H), and suboptimality gap (\u0394).  It also notes whether the algorithm can handle misspecified MDPs, where the transition kernel and reward function may not perfectly match linear approximations.", "section": "2 Related Work"}, {"figure_path": "02r24A8doi/tables/tables_13_1.jpg", "caption": "Table 1: Instance-dependent regret bounds for different algorithms under the linear MDP setting. Here d is the dimension of the linear function (s, \u03b1), H is the horizon length, \u2206 is the minimal suboptimality gap. All results in the table represent high probability regret bounds. The regret bound depends the number of episodes K in He et al. (2021a) and the minimum positive eigenvalue \u03bb of features mapping in Papini et al. (2021b). Misspecified MDP? indicates if the algorithm can (\u2713) handle the misspecified linear MDP or not (\u00d7).", "description": "This table compares the instance-dependent regret bounds achieved by different reinforcement learning algorithms for linear Markov Decision Processes (MDPs).  It contrasts algorithms that can handle misspecified MDPs (where the reward and transition functions are only approximately linear) with those that cannot. The table shows that the proposed algorithm, Cert-LSVI-UCB, achieves a constant regret bound with high probability, unlike other methods which exhibit logarithmic or square-root dependence on the number of episodes.", "section": "2 Related Work"}, {"figure_path": "02r24A8doi/tables/tables_37_1.jpg", "caption": "Table 1: Instance-dependent regret bounds for different algorithms under the linear MDP setting. Here d is the dimension of the linear function (s, \u03b1), H is the horizon length, \u2206 is the minimal suboptimality gap. All results in the table represent high probability regret bounds. The regret bound depends the number of episodes K in He et al. (2021a) and the minimum positive eigenvalue \u03bb of features mapping in Papini et al. (2021b). Misspecified MDP? indicates if the algorithm can (\u2713) handle the misspecified linear MDP or not (\u00d7).", "description": "The table compares instance-dependent regret bounds of three different algorithms for linear Markov Decision Processes.  It shows how the regret scales with problem parameters (dimension of feature space, horizon length, suboptimality gap) and whether the algorithm can handle model misspecification.  Key differences between the algorithms are highlighted.", "section": "2 Related Work"}]