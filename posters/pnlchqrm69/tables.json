[{"figure_path": "PnlCHQrM69/tables/tables_7_1.jpg", "caption": "Table 1: Overall performance of SEMCODER. For code generation, the numbers outside and inside parenthesis \"()\" indicate the base and plus versions of EvalPlus, respectively. All results are reported with pass@1. CXEval indicates CRUXEval, and LCB indicates LiveCodeBench.", "description": "This table presents a comprehensive comparison of SEMCODER's performance against various state-of-the-art code generation and execution reasoning models.  It shows the pass@1 scores (the percentage of tests passed) on HumanEval (HEval), MBPP (a modified version of HumanEval), LiveCodeBench-Lite (LCB-Lite), CRUXEval-I, CRUXEval-O, and LiveCodeBench-Exec (LCB-Exec) for different model sizes and variants, including both base and instruction-tuned versions.  The results highlight SEMCODER's competitive performance, particularly in execution reasoning, even with a smaller parameter count compared to other models.", "section": "6 Evaluation"}, {"figure_path": "PnlCHQrM69/tables/tables_8_1.jpg", "caption": "Table 2: Ablation study for input and output prediction with different types of execution reasoning.", "description": "This table presents the results of an ablation study comparing different methods for execution reasoning on the tasks of input and output prediction.  The methods compared include few-shot prompting, fine-tuning with scratchpad reasoning, fine-tuning with NeXT trace format, fine-tuning with concise trace format, and the proposed monologue reasoning approach.  The evaluation metrics used are CRUXEval-I, CRUXEval-O, and LCB-Exec. The table shows that the proposed monologue reasoning approach significantly outperforms the other methods across all three evaluation metrics.", "section": "6.2 Effectivenss of Monologue Reasoning"}, {"figure_path": "PnlCHQrM69/tables/tables_8_2.jpg", "caption": "Table 3: Performance of iterative debug and self-refine", "description": "This table presents the results of iterative debugging and self-refinement experiments.  It compares the performance of several Code LLMs (Magicoder-DS, Magicoder-S-DS, DeepSeekCoder-Inst, Llama-3.1-Inst, SEMCODER, SEMCODER-S) on two metrics: HumanEval and MBPP.  The results are shown for both zero-shot prompting and fine-tuning with the PYX-R dataset.  The table highlights the improvements in iterative programming capabilities achieved through the combination of model training and the use of rubber-duck debugging.", "section": "Experiments"}, {"figure_path": "PnlCHQrM69/tables/tables_14_1.jpg", "caption": "Table 3: Performance of iterative debug and self-refine", "description": "This table presents the results of iterative debugging and self-refinement experiments.  Two versions of SEMCODER (base and advanced) are compared against four state-of-the-art instruction-tuned code LLMs (Magicoder-DS, Magicoder-S-DS, DeepSeekCoder-Inst, and Llama-3.1-Inst) using two metrics: HEval and MBPP.  Both zero-shot prompting and fine-tuning with PYX-R (a debugging dataset) are assessed. The results show the performance in terms of HEval and MBPP after five iterative refinements.", "section": "6 Evaluation"}, {"figure_path": "PnlCHQrM69/tables/tables_15_1.jpg", "caption": "Table 1: Overall performance of SEMCODER. For code generation, the numbers outside and inside parenthesis \"()\" indicate the base and plus versions of EvalPlus, respectively. All results are reported with pass@1. CXEval indicates CRUXEval, and LCB indicates LiveCodeBench.", "description": "This table presents a comparison of SEMCODER's performance against various other code large language models (Code LLMs) across different code generation and execution reasoning benchmarks.  The benchmarks used are HumanEval, MBPP, LiveCodeBench-Lite (LCB-Lite), CRUXEval-I, CRUXEval-O, and LCB-Exec.  Results are shown as the percentage of tasks successfully completed (pass@1).  The table helps illustrate SEMCODER's competitive performance, particularly its effectiveness in execution reasoning, even with a smaller parameter count compared to other models.", "section": "6 Evaluation"}, {"figure_path": "PnlCHQrM69/tables/tables_15_2.jpg", "caption": "Table 1: Overall performance of SEMCODER. For code generation, the numbers outside and inside parenthesis \"()\" indicate the base and plus versions of EvalPlus, respectively. All results are reported with pass@1. CXEval indicates CRUXEval, and LCB indicates LiveCodeBench.", "description": "This table presents the overall performance comparison of the SEMCODER model against various baselines on code generation and execution reasoning tasks.  For code generation, it shows the results on HumanEval and MBPP benchmarks, differentiating between base and enhanced versions using EvalPlus.  For execution reasoning, the table includes performance on CRUXEval-I, CRUXEval-O and LiveCodeBench, demonstrating the model's capabilities in understanding and reasoning about program execution.", "section": "6 Evaluation"}, {"figure_path": "PnlCHQrM69/tables/tables_16_1.jpg", "caption": "Table 7: Top-10 error types of inexecutable Python code in OSS-Instruct[16]", "description": "This table presents the top ten most frequent error types encountered when executing Python code from the OSS-Instruct dataset.  The dataset contains 43.1k Python samples, a significant portion of which (11.6k, or 26.9%) are found to be non-executable. This table details the specific error types and their counts, providing insights into common issues during code generation, and highlighting the need for robust executable code generation processes.", "section": "E Executability Analysis of OSS-INSTRUCT"}, {"figure_path": "PnlCHQrM69/tables/tables_17_1.jpg", "caption": "Table 8: The comparison between OSS-INSTRUCT and our PYX.", "description": "This table compares the characteristics of two datasets used in the paper: OSS-INSTRUCT and PYX. It shows the number of problems in each dataset, the percentage of seeds that could be parsed and executed, and the performance on two code generation benchmarks (HumanEval and MBPP).  The results highlight that PYX, a dataset curated by the authors, has a higher quality of executable code compared to OSS-INSTRUCT.", "section": "Curating Executable Code Dataset"}]