[{"type": "text", "text": "Exploring Consistency in Graph Representations: from Graph Kernels to Graph Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuyuan Liu Yinghao Cai Qihui Yang Yujun Yan Dartmouth College {xuyuan.liu.gr, yinghao.cai, qihui.yang, yujun.yan}@dartmouth.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Networks (GNNs) have emerged as a dominant approach in graph representation learning, yet they often struggle to capture consistent similarity relationships among graphs. While graph kernel methods such as the WeisfeilerLehman subtree (WL-subtree) and Weisfeiler-Lehman optimal assignment (WLOA) kernels are effective in capturing similarity relationships, they rely heavily on predefined kernels and lack sufficient non-linearity for more complex data patterns. Our work aims to bridge the gap between neural network methods and kernel approaches by enabling GNNs to consistently capture relational structures in their learned representations. Given the analogy between the message-passing process of GNNs and WL algorithms, we thoroughly compare and analyze the properties of WL-subtree and WLOA kernels. We find that the similarities captured by WLOA at different iterations are asymptotically consistent, ensuring that similar graphs remain similar in subsequent iterations, thereby leading to superior performance over the WL-subtree kernel. Inspired by these findings, we conjecture that the consistency in the similarities of graph representations across GNN layers is crucial in capturing relational structures and enhancing graph classification performance. Thus, we propose a loss to enforce the similarity of graph representations to be consistent across different layers. Our empirical analysis verifies our conjecture and shows that our proposed consistency loss can significantly enhance graph classification performance across several GNN backbones on various datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph classification tasks are extensively applied across multiple domains, including chemistry [Liu et al., 2022, Xu et al., 2023], bioinformatics [Yan et al., 2019, Li et al., 2023a,b], and social network analysis [Ying et al., 2018, Wang et al., 2024]. Graph neural networks (GNNs) [Kipf and Welling, 2017, Xu et al., 2019, Velickovic et al., 2018, Huang et al., 2024] have emerged as the predominant approach for performing graph classification, owing to their ability to extract rich representations from various types of graph data. A typical GNN employs the message-passing mechanism [Gilmer et al., 2017], where node features are propagated and aggregated across connected nodes. This process effectively captures local tree structures, enabling the differentiation between various graphs. However, GNNs often struggle to preserve relational structures among graphs, resulting in inconsistent relative similarities across the layers. As shown in Figure 1, graphs with higher relative similarity in one layer may exhibit reduced similarity in the subsequent layer. This phenomenon arises from the limitations of cross-entropy loss, which fails to preserve relational structures, as it forces graphs within the same class into identical representations. ", "page_idx": 0}, {"type": "text", "text": "Graph kernel methods, on the other hand, are designed to capture similarities between graphs and utilize these similarities for classification tasks. For instance, subgraph-pattern approaches [Shervashidze et al., 2009, Costa and Grave, 2010, Kriege et al., 2020] compare graphs by counting the occurrences of fixed-size subgraph motifs. Other methods compare sequences of vertices or edges encountered during graph traversals [Borgwardt and Kriegel, 2005, Kashima et al., 2003, Zhang et al., 2018]. Among all graph kernels, two notable ones are the Weisfeiler-Lehman subtree (WLsubtree) kernel [Shervashidze et al., 2011] and the Weisfeiler-Lehman optimal assignment (WLOA) kernel [Kriege et al., 2016]. They are found to have comparable performance to simple GNNs [Nikolentzos et al., 2021].The WL-subtree kernel iteratively relabels graphs using the Weisfeiler-Lehman algorithm [Weisfeiler and Lehman, 1968] and constructs a kernel based on the number of occurrences of each label. The WLOA kernel uses the same relabeling scheme but computes a matching between substructures to reveal structural correspondences between the graphs. ", "page_idx": 0}, {"type": "image", "img_path": "dg0hO4M11K/tmp/c2ce87048a16a71b02189020e10d767e743837178c60eba2da1b923c6d5ae9f8.jpg", "img_caption": ["Figure 1: Cosine similarity of three molecules from the NCI1 dataset, evaluated using graph representations from three consecutive GIN layers. Common GNN models fail to preserve relational structures across the layers. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While effective in capturing relative graph similarity, kernel methods rely on predefined kernels and exhibit insufficient non-linearities, limiting their ability to capture complex patterns in highdimensional data. Additionally, kernel methods are computationally costly, making them unsuitable for handling large datasets and consequently limiting their overall applicability. ", "page_idx": 1}, {"type": "text", "text": "In this work, we aim to bridge the gap between kernel methods and GNN models. Given the iterative nature of GNNs, we study a class of kernels which are induced from graph representations obtained through an iterative process and name them iterative graph kernels (IGK). Within this framework, we define the consistency property, which ensures that similar graphs remain similar in subsequent iterations. Our analysis demonstrates that kernels with this property yield better classification performance. Furthermore, we find that this property sheds light on why the WLOA kernel outperforms the WL-subtree kernel. The WLOA kernel asymptotically demonstrates consistency as the iteration goes to infinity, whereas the WL-subtree kernel does not exhibit this behavior. Inspired by these findings and the analogy between message-passing GNNs and the WL-subtree kernel [Xu et al., 2019], we hypothesize that this principle is also applicable to GNNs. To explore this, we introduce a novel loss function designed to align the ranking of graph similarities across GNN layers. The aim is to ensure that the relational structures of the graphs are preserved and consistently reflected throughout the representation space of these layers. We validate this hypothesis by applying our loss function to different GNN backbones across various graph datasets. Extensive experiments demonstrate that our proposed model-agnostic consistency loss improves graph classification performance comprehensively. ", "page_idx": 1}, {"type": "text", "text": "In summary, the main contributions of this work are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Novel perspective: We present a novel perspective on understanding the graph classification performance of GNNs by analyzing the similarity relationships captured by different layers. \u2022 New insights: We are the first to introduce and formalize the consistency principle within both kernel-based and GNN methods for graph classification tasks. Additionally, we provide theoretical proofs explaining how this principle enhances the performance. \u2022 Simple yet effective method: Empirical results demonstrate that the proposed consistency loss universally enhances performance across a wide range of base models and datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we begin by introducing the notations and definitions used throughout the paper. Next, we provide an introduction to the fundamentals of Weisfeiler-Lehman isomorphism test, GNNs and graph kernels. ", "page_idx": 1}, {"type": "text", "text": "2.1 Notations and Definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{G}(\\boldsymbol{\\upnu},\\boldsymbol{\\mathcal{E}},\\mathbf{X})$ be an undirected and unweighted graph with $N$ nodes, where $\\mathcal{V}$ denotes the node set, $\\mathcal{E}$ denotes the edge set, and $\\mathbf{X}$ denotes the feature matrix, where each row represents the features of a corresponding node. The neighborhood of a node $v$ is defined as the set of all nodes that connected to $v\\colon\\mathcal{N}(\\bar{v})=\\{\\bar{u}|(v,u)\\in\\mathcal{E}\\}$ . In a graph dataset, each graph $\\mathcal{G}_{i}$ is associated with a label $\\mathcal{y}_{i}$ , which is sampled from a label set $\\mathcal{L}$ . In this paper, we focus on the graph classification task, where a model $\\phi$ is trained to map each graph to its label. ", "page_idx": 2}, {"type": "text", "text": "2.2 Weisfeiler-Lehman Isomorphism Test ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce the Weisfeiler-Lehman isomorphism test [Weisfeiler and Lehman, 1968], which can be used to distinguish different graphs and is closely related to the message-passing process of GNNs [Xu et al., 2019]. The WL algorithm operates by iteratively relabeling the colors of vertices in the graph. Initially, all vertices are assigned the same color $C_{0}$ . In iteration $i$ , the color of a vertex $v$ is updated based on its current color $C_{v,i-1}$ and the colors of its neighbors $\\left\\{C_{u\\in\\mathcal{N}(v),i-1}\\right\\}$ . The update is given as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nC_{v,i}=f_{c}^{i}\\left(\\left\\{C_{v,i-1},\\left\\{C_{u\\in\\mathcal{N}(v),i-1}\\right\\}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f_{c}^{i}$ is an injective coloring function that maps the multisets to different colors at iteration $i$ .This process continues for a predefined number of iterations or until the coloring stabilizes (i.e., the colors no longer change). ", "page_idx": 2}, {"type": "text", "text": "2.3 Graph Neural Network ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Most GNNs adopt the message-passing framework [Gilmer et al., 2017], which can be viewed as a derivative of the Weisfeiler-Lehman coloring mechanism. Specifically, let $h_{v}^{(k-1)}$ represent the feature vector of node $v$ at the $(k-1)$ -th iteration. A GNN computes the new feature for $v$ by aggregating the representations of itself and its neighboring nodes $\\bar{u^{\\dag}}\\in\\bar{\\mathcal{N}}(v)$ as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nh_{v}^{(k)}=\\mathtt{U P D A T E}^{(k)}\\left(h_{v}^{(k-1)},m_{v}^{(k)}\\right),\\mathrm{where~}m_{v}^{(k)}=\\mathtt{A G G R}^{(k)}\\left(\\left\\{h_{u}^{(k-1)}:u\\in\\mathcal{N}(v)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The initial node representations $h_{v}^{(0)}$ are set to the raw node features $\\mathbf{X}_{v}$ . At the $k$ -th iteration, the aggregation function $\\mathtt{A G G R}^{(k)}(\\cdot)$ computes the messages $m_{v}^{(k)}$ received from neighboring nodes. Subsequently, the update function $\\mathtt{U P D A T E}^{(k)}(\\cdot)$ computes a new representation for each node by integrating the neighborhood messages $m_{v}^{(k)}$ with its previous embedding h(vk\u22121). After T iterations, the final node representations are combined into a graph representation using a readout function: ", "page_idx": 2}, {"type": "equation", "text": "$$\nh_{G}=\\mathtt{R E A D O U T}\\left(\\left\\{h_{v}^{(T)}\\mid v\\in\\mathcal{V}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The readout function, essentially a set function learned by the neural network, commonly employs AVERAGE or MAXPOOL. ", "page_idx": 2}, {"type": "text", "text": "2.4 Graph Kernel ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A kernel is a function used to measure the similarity between pairs of objects. For a non-empty set $\\chi$ and a function $\\mathbb{K}:\\chi\\times\\chi\\to\\mathbb{R}$ , the function $\\mathbb{K}$ qualifies as a kernel on $\\chi$ if there exists a Hilbert space $\\mathcal{H}_{k}$ and a feature map function $\\phi:\\chi\\to\\mathcal{H}_{k}$ , such that $\\mathbb{K}(x,y)\\,=\\,\\langle\\phi(x),\\phi(y)\\rangle$ for any $x,y\\in\\chi$ , where $\\langle\\cdot,\\cdot\\rangle$ denotes the inner product in $\\mathcal{H}_{k}$ . Notably, such a feature map exists if and only if $\\mathbb{K}$ is a positive semi-definite function. Let $\\mathbb{K}$ be a kernel defined on $\\chi$ , and let $S=\\{x_{1},\\ldots,\\dot{x}_{n}\\}$ be a finite set of $n$ samples on $\\chi$ . The Gram matrix for $S$ is defined as $\\mathbf{G}\\in\\mathbb{R}^{n\\times n}$ , with each element $\\mathbf{G}_{i j}=\\mathbb{K}({x}_{i},{x}_{j})$ representing the kernel value between the $i$ -th and $j$ -th data points in $S$ . The Gram matrix is always positive semi-definite. ", "page_idx": 2}, {"type": "text", "text": "Graph kernel methods apply the kernel approaches to graph data, typically defined using the $\\mathcal{R}$ - convolution framework [Haussler et al., 1999, Bause and Kriege, 2022]. Consider two graphs, $\\mathcal{G}$ and $\\mathcal{G}^{\\prime}$ . The key idea is to decompose the graphs into substructures using a predefined feature map function $\\phi$ , and then compute the kernel value by taking the inner product in the feature space: $\\mathbb{K}(\\mathcal{G},\\mathcal{G}^{\\prime})\\,=\\,\\langle\\phi(\\mathcal{G}),\\phi(\\mathcal{G}^{\\prime})\\rangle$ , based on these substructures. Weisfeiler-Lehman (WL) graph kernels stand out as one of the most widely used approaches. These methods employ the Weisfeiler-Lehman coloring scheme to iteratively encode graphs, calculating kernel values based on these colorings. The final kernel values are derived through the aggregation of intermediate results. Next, we introduce the WL-subtree kernel and the WLOA kernel in more detail. Let $f^{i}$ be the coloring function at the $i$ -th iteration, mapping the colored graph from the previous iteration to a new colored graph. Define $\\psi^{i}$ as the function that captures the cumulative coloring effect up to the $i$ -th iteration: $\\stackrel{\\triangledown}{\\boldsymbol{\\psi}^{i}}=f^{i}\\circ\\cdots\\circ f^{1}$ . Specifically, the WL-subtree kernel computes the kernel value by directly using the label histogram as a feature to compute the dot product, which is expressed as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathbb K}_{w l_{-}s u b t r e e}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)=\\sum_{i=1}^{h}\\left\\langle\\phi(\\psi^{i}(\\mathcal{G})),\\phi(\\psi^{i}\\left(\\mathcal{G}^{\\prime}\\right))\\right\\rangle\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The WLOA kernel applies a histogram intersection kernel to match substructures between different graphs, which is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{K}_{W L O A}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)=\\sum_{i=1}^{h}\\mathrm{{hi}\\,s t m i n}\\left\\{\\phi(\\psi^{i}(\\mathcal{G})),\\phi(\\psi^{i}(\\mathcal{G}^{\\prime}))\\right\\}\\cdot\\omega(i)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\omega(i)$ is a nonnegative, monotonically non-decreasing weight function, and $\\omega(i)\\;=\\;1$ is commonly used in practice [Kriege et al., 2016, Siglidis et al., 2020]. The operator histmin denotes the histogram intersection kernel. It is computed by comparing and summing the smallest matching elements between two sets. For example, consider two sets $S_{1}:\\{\\mathfrak{a},\\mathfrak{a},\\mathfrak{b},\\mathfrak{b},\\mathfrak{c}\\}$ and $S_{2}:\\{\\mathfrak{a},\\mathfrak{b},\\mathfrak{b},\\mathfrak{c},\\mathfrak{c}\\}$ . The histmin $(S_{1},S_{2})$ is calculated by taking the minimum frequency of each distinct element across the two sets, yielding $\\operatorname*{min}(2,1)+\\operatorname*{min}(2,2)+\\operatorname*{min}(1,2)=4$ . ", "page_idx": 3}, {"type": "text", "text": "To ensure that the kernel values fall in the range of 0 to 1, normalization is often applied. The normalized kernel value $\\tilde{\\mathbb{K}}^{(h)}$ is then expressed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})=\\frac{\\mathbb{K}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sqrt{\\mathbb{K}^{(h)}(\\mathcal{G},\\mathcal{G})}\\sqrt{\\mathbb{K}^{(h)}(\\mathcal{G}^{\\prime},\\mathcal{G}^{\\prime})}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Consistency Principles ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To encode relational structures in GNN learning, we first examine how similarities are represented in graph kernels. In this section, we start by defining a class of graph kernels, i.e., the iterative graph kernels, which encompasses many widely used kernels. Then, we delve into a key property known as the consistency property, which may play an important role in enhancing classification performance. We support this assertion through theoretical analysis, elucidating how different kernels adhere to or deviate from this property, thereby explaining their performance differences. ", "page_idx": 3}, {"type": "text", "text": "3.1 Iterative Graph Kernels ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this paper, we are interested in a set of kernels defined as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 Given a colored graph set $\\chi_{i}$ , a feature map function $\\phi:\\chi\\to\\mathcal{H}_{k}$ (where $\\mathcal{H}_{k}$ is $a$ Hilbert space), and a set of coloring functions $\\mathcal{F}_{c}=\\{f^{0},\\bar{f^{1}},\\ldots,f^{i}\\}$ on $\\chi$ (with $f^{i}:\\chi\\to\\chi,$ , we define the set of iterative graph kernels (IGK) as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{K}_{\\mathcal{F}_{c},\\phi}(x,y,i)=\\langle\\phi(f^{i}\\circ\\dots\\cdot f^{1}(x)),\\phi(f^{i}\\circ\\dots\\cdot f^{1}(y))\\rangle=\\langle\\psi^{i}(x),\\psi^{i}(y)\\rangle\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x,y\\in\\chi$ and $\\psi^{i}(\\cdot)$ represents a composite function given by: $\\psi^{i}=f^{i}\\circ\\cdots\\circ f^{1}$ , Then the normalized kernel is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i)=\\frac{\\mathbb{K}_{\\mathcal{F}_{c},\\phi}(x,y,i)}{\\sqrt{\\mathbb{K}_{\\mathcal{F}_{c},\\phi}(x,x,i)}\\sqrt{\\mathbb{K}_{\\mathcal{F}_{c},\\phi}(y,y,i)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Based on this definition, we can see that graph kernels utilizing the Weisfeiler-Lehman framework, including the WL-subtree kernel [Shervashidze et al., 2011], WLOA [Kriege et al., 2016], and the Wasserstein Weisfeiler-Lehman (WWL) kernel [Togninalli et al., 2019], should be classified as iterative graph kernels. Conversely, the subgraph-pattern approaches, such as the graphlet kernel [Shervashidze et al., 2009] and the shortest-path kernel [Borgwardt and Kriegel, 2005], do not fall into this category. ", "page_idx": 3}, {"type": "text", "text": "3.2 Key Properties: Monotonic Decrease & Order Consistency ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To effectively capture relational structures, we design the IGKs to progressively differentiate between graphs. With each iteration, additional structural features are considered, enabling the distinction of graphs that may have been indistinguishable in earlier iterations. This implies two properties: (1) the kernel values monotonically decrease with larger iterations, as the similarity between two graphs decreases with the consideration of more features; and (2) the similarity rankings across different iterations should remain consistent, meaning that graphs deemed dissimilar in early iterations should not be considered similar in later iterations. We then formally define these two properties and demonstrate how they can lead to a non-decreasing margin (better performance) in the binary classification task. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Monotonic Decrease) The normalized iterative graph kernels $\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i)$ are said to be monotonically decreasing if and only $i f$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i)\\ge\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i+1)\\quad\\forall x,y\\in\\chi\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Definition 3.3 (Order Consistency) The normalized iterative graph kernels $\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i)$ are said to preserve order consistency if the similarity ranking remains consistent across different iterations for any pair of graphs, which is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i)>\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,z,i)\\Rightarrow\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i+1)\\geq\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,z,i+1)\\quad\\forall x,y,z\\in\\chi\\\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Next we show that these two properties can lead to a non-decreasing margin in the binary classification task, which suggests better performance. ", "page_idx": 4}, {"type": "text", "text": "Consider a binary graph classification task and assume that the graph representations obtained at any iteration have a uniform norm. This can be achieved by simply normalizing the graph representations at the end of each iteration. That is, for any graph $x$ : $\\|\\phi(\\bar{\\psi^{i}}(x))\\|=1$ . Then, for any IGK that is monotonically decreasing and preserves the order consistency, the following theorem holds: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4 Let $\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i)$ be a normalized iterative graph kernel that is monotonically decreasing and preserves order consistency. In the binary graph classification task with uniform graph representations, the margin between two classes in the representation space is non-decreasing w.r.t the iteration $i$ , where the margin at iteration i is defined as the shortest distance between two graph representations from different classes: margin= $=\\|\\psi^{\\tilde{i}}(x)-\\psi^{i}(y)\\|$ , $\\mathcal{Y}(x)\\ne\\mathcal{Y}(y)$ . ", "page_idx": 4}, {"type": "text", "text": "Proof sketch. Suppose that at iteration $i$ , the margin is defined by a pair of graphs, $x_{1}$ from class 1 and $y_{1}$ from class 2. In the next iteration, $i+1$ , the margin is determined by another pair, $x_{2}$ and $y_{2}$ . Two possibilities arise: (1) $x_{1}=x_{2}$ and $y_{1}=y_{2}$ , or (2) $x_{1}\\neq x_{2}$ or $y_{1}\\neq y_{2}$ . ", "page_idx": 4}, {"type": "text", "text": "(1) If the pair defining the margin remains unchanged, the margin at iteration $i+1$ can only increase or stay the same, as the kernel is a monotonically decreasing function, indicating a reduction in similarity.   \n(2) We prove case 2 by contradiction, assuming the margin decreases at iteration $i+1$ . This would imply that the kernel value for $x_{2}$ and $y_{2}$ increases at iteration $i+1$ , which contradicts the fact that the kernel function is monotonically decreasing. ", "page_idx": 4}, {"type": "text", "text": "Thus, in both cases, the margin does not decrease as the iterations progress. ", "page_idx": 4}, {"type": "text", "text": "We provide the detailed proof in Appendix A.1 ", "page_idx": 4}, {"type": "text", "text": "3.3 Theoretical Verification with WL-based Kernels ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed in Section 3.1, WL-based kernels can be categorized as iterative graph kernels, as they are generated by the coloring functions in an iterative refinement process. Consequently, a natural question arises regarding how various kernels adhere to these properties and whether their adherence reflects their actual performance. We thus investigate two popular WL-based Kernels: the WL-subtree kernel [Shervashidze et al., 2011] and the WLOA kernel [Kriege et al., 2016]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.5 The normalized WL-subtree kernel is neither monotonically decreasing nor does it preserve order consistency. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6 The normalized WLOA kernel is monotonically decreasing and asymptotically preserves order consistency when $\\omega(i)=1$ . ", "page_idx": 5}, {"type": "text", "text": "Proof sketch. For Theorem 3.5, we illustrate a counterexample, while for Theorem 3.6, we consider two graph pairs where the similarity condition holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{W L O A}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)\\ge\\tilde{\\mathbb{K}}_{W L O A}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The similarity at the next iteration $h+1$ is scaled by a factor dependent $\\omega(i),i=1,\\cdots,h+1$ . The unnormalized kernel increases monotonically, though with diminishing increments over iterations. Given this, when $\\omega(i)=1$ and $h\\to\\infty$ , we obtain: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)\\ge\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We include the complete proof in Appendix A.2 and A.3. ", "page_idx": 5}, {"type": "text", "text": "These findings imply that the WLOA kernel better preserves relational structures compared to the WLsubtree kernel, leading to improved classification performance, as supported by the literature [Kriege et al., 2016]. ", "page_idx": 5}, {"type": "text", "text": "4 Proposed Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given the analogy between WL-based kernels and GNNs [Shervashidze et al., 2011, Gilmer et al., 2017], and the observation that GNNs often fail to preserve relational structures, we hypothesize that the consistency principle is also beneficial to GNN learning. Thus, we aim to explore how to effectively preserve this principle within the GNN architectures. ", "page_idx": 5}, {"type": "image", "img_path": "dg0hO4M11K/tmp/bf31ac363e98d51692893ba6341b9194d7a4a95f4c78ff4af9d0a73c0fd31288.jpg", "img_caption": ["Figure 2: Computation of loss. At each layer, pairwise distance matrix $\\mathbf{D}$ is calculated using the normalized representations of graphs in a batch. After randomly selecting a reference graph $x_{k}$ , the reference probability matrix is computed using the distance matrix from previous layer, where entry $(n,m)$ represents the known probability that the graph $x_{k}$ is more similar to the graph $x_{n}$ than to the graph $x_{m}$ . For the distance matrix of current layer, we compute the predicted probability that $x_{k}$ is closer to $x_{n}$ than to $x_{m}$ and form the prediction probability matrix. Consistency loss is computed as the cross-entropy between the predicted and reference probability matrices "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Consistency Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our objective is to enhance graph representation consistency across GNN layers, which has significant potential to preserve the relational structure in the representation space. If we compare $\\phi(\\bar{\\psi^{i}}(G))$ to the graph representations obtained at the $i$ -th layer, preserving the consistency principle is equivalent to preserving the ordering of cosine similarity among the graph representations. However, due to the non-differentiable nature of ranking operations, directly minimizing the ranking differences between consecutive layers is not feasible using gradient-based optimization techniques. Therefore, we aim to optimize pairwise ordering relations instead of the entire ranking list. In this work, our proposed loss employs a probabilistic approach inspired by [Burges et al., 2005]. The entire framework is illustrated in Figure 2. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Let $\\mathbf{H}^{h}\\,\\in\\,\\mathbb{R}^{n\\times d}$ denote the graph embedding matrix for $n$ examples in a batch, each with a $d\\!.$ - dimensional feature vector. We first compute the distance matrix $\\mathbf{D}^{\\hat{h}}$ for all the graphs in a batch at the $h$ -th layer. The entries $D_{i,j}^{h}$ of this matrix represent the distance between the representations of the $i$ -th and $j$ -th graphs, calculated as $D_{i,j}^{h}=\\mathsf{D i s t}\\left(H_{x_{i}}^{h},H_{x_{j}}^{h}\\right)$ . Here, we use the cosine distance, the complement of cosine similarity in positive space, expressed as: $1-{\\frac{H_{x_{i}}^{h}\\cdot H_{x_{j}}^{h}}{\\left\\|H_{x_{i}}^{h}\\right\\|\\cdot\\left\\|H_{x_{j}}^{h}\\right\\|}}$ Considering the distance relationship to an arbitrary graph $x_{k}$ in the batch, the predicted piro babjil ity $\\hat{\\mathbb{P}}_{n,m|k}^{h}$ that $x_{k}$ is more similar to graph $x_{n}$ than to graph $x_{m}$ at layer $h$ is defined as $\\hat{\\mathbb{P}}_{n,m|k}^{h}\\left(\\hat{D}_{k,n}^{h}<\\,\\hat{D}_{k,m}^{h}\\right)$ . This probability score, which ranges from 0 to 1, is formulated using the sigmoid function as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{P}}_{n,m|k}^{h}\\left(\\hat{D}_{k,n}^{h}<\\hat{D}_{k,m}^{h}\\right)=\\frac{1}{1+\\exp\\left(\\hat{D}_{k,m}^{h}-\\hat{D}_{k,n}^{h}\\right)}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given the distance matrix from the previous layer $D^{h-1}$ , the known probability that graph $x_{k}$ is more similar to graph $x_{n}$ than to graph $x_{m}$ , denoted as $\\tilde{\\mathbb{P}}_{n,m}^{h-1}(\\hat{D}_{k,n}^{h-1},\\hat{D}_{k,m}^{h-\\bar{1}})$ , can be formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{P}}_{n,m|k}^{h-1}=\\frac{1}{2}\\left(1+\\mathrm{sign}(\\hat{D}_{k,n}^{h-1}-\\hat{D}_{k,m}^{h-1})\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We can then minimize the discrepancy between the predicted and the known probability distributions to enhance representation consistency across the layers. Here,we employ the cross-entropy loss to effectively measure the divergence between these two distributions. Specifically, for a pair $(x_{n},x_{m})$ centered on $x_{k}$ at layer $h$ , the cross-entropy loss can be expressed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cross-entrop}}\\left(\\left(\\boldsymbol{x}_{n},\\boldsymbol{x}_{m}\\right)\\mid\\boldsymbol{x}_{k},\\boldsymbol{h}\\right)=-\\tilde{\\mathbb{P}}_{n,m\\mid k}^{h-1}\\log\\hat{\\mathbb{P}}_{n,m\\mid k}^{h}-\\left(1-\\tilde{\\mathbb{P}}_{n,m\\mid k}^{h-1}\\right)\\log\\left(1-\\hat{\\mathbb{P}}_{n,m\\mid k}^{h}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, the total loss function, which quantifies the consistency of pair-wise distance relations for graph $x_{k}$ at layer $h$ , can be formulated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{consistency}}\\left(k\\right)=\\sum_{n,m}{\\mathcal{L}}((x_{n},x_{m})\\mid x_{k},h)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The overall objective function of the proposed framework can then be formulated as the weighted sum of the original loss and the consistency loss. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{total}}\\,=\\mathcal{L}_{\\mathrm{origin}}+\\lambda\\sum_{i}\\mathcal{L}_{\\mathrm{consistency}}\\left(i\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $\\lambda$ is a hyperparameter that controls the strength of the consistency constraint. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we examine the effectiveness of the proposed consistency loss for the graph classification task. Specifically, we aim to address the following questions: Q1: Does the consistency loss effectively enhance the performance of various GNN backbones in the graph classification task? Q2: How does the consistency loss influence the rank correlation of graph similarities across GNN layers? Q3: How does the consistency loss influence dataset performance across varying levels of complexity, both in structural intricacy and task difficulty? ", "page_idx": 6}, {"type": "text", "text": "5.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset We conduct extensive experiments using the TU Dataset [Morris et al., 2020], the Open Graph Benchmark (OGB) [Hu et al., 2020] and Reddit Threads(Reddit-T) dataset [Bause and Kriege, ", "page_idx": 6}, {"type": "text", "text": "2022]. The TU Dataset consists of eight graph classification datasets, categorized into three main types: (1) Chem/Bioinformatics datasets, including D&D, NCI1, NCI109, and PROTEINS; (2) Social Network datasets, including IMDB-BINARY,IMDB-MULTI and COLLAB, where a constant feature value of 1 was assigned to all vertices due to the absence of vertex features; and (3) a Computer Vision dataset, COIL-RAG. The OGB datasets include ogbg-molhiv, a molecular property prediction dataset used to determine whether a molecule inhibits HIV replication. Reddit-T dataset is used for classifying threads from Reddit as either discussions or non-discussions, where users are represented as nodes and replies between them as links. ", "page_idx": 7}, {"type": "text", "text": "For the TU dataset and Reddit-T, consistent with prior work [Xinyi and Chen, 2019, Xu et al., 2019], we utilize an 8:1:1 ratio for training, validation, and testing sets. For the OGB datasets, we use the official splits provided. Training stops at the highest validation performance, and test accuracy is taken from the corresponding epoch in each fold. Final results are reported as the mean accuracy (except ogbg-molhiv) and standard deviation over 10 folds. For ogbg-molhiv, we follow the official evaluator and use ROC-AUC as the evaluation metric. Detailed information about these datasets can be found in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Model We use three widely adopted GNN models as baselines: namely, GCN [Kipf and Welling, 2017], GIN [Xu et al., 2019], and GraphSAGE [Hamilton et al., 2017]. We also include two recent GNN models, namely GTransformer [Shi et al., 2021] and GMT [Baek et al., 2021]. To ensure a fair comparison, we maintain the same number of layers and layer sizes for both the base models and the models with our proposed consistency loss, ensuring the sharing of the same network architecture. Detailed information about hyperparameter tuning is provided in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "5.2 Effectiveness of Consistency Loss ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To answer Q1, we present the results for the TU, OGB and Reddit-T datasets in Table 1. As shown in this table, GNN models with the consistency loss yield significant performance improvements over their base models on different datasets. These findings suggest that the consistency framework is a versatile and robust approach for enhancing the predictive capabilities of GNNs in real-world datasets, irrespective of the base model and dataset domain. Notably, the GIN method demonstrates the most significant improvements, achieving enhancements of up to $4.51\\%$ on the D&D dataset, $4.32\\%$ on the COLLAB dataset, and $3.70\\%$ on the IMDB-B dataset. This improvement can be linked to our empirical observation regarding the weak ability of GIN to preserve consistency across layers. In addition, our method demonstrates satisfactory improvements on datasets with numerous classes (e.g., COIL-RAG) and large-scale datasets (e.g., ogbg-molhiv and Reddit-T), indicating that our approach is both flexible and scalable for handling complex and extensive datasets. ", "page_idx": 7}, {"type": "table", "img_path": "dg0hO4M11K/tmp/23ac272082a992889e379fbea6932ee90b8399a479c4b60bf4272a884cd6b34c.jpg", "table_caption": ["Table 1: Classification performance on the TU and OGB datasets, with and without the consistency loss. Highlighted cells indicate instances where the base GNN with the consistency loss outperforms the base GNN alone. The reported values are average accuracy for TU datasets and ROC-AUC for the ogbg-molhiv dataset, including their standard deviations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Furthermore, we analyze the complexity and scalability of our method on the TU Dataset, with additional details provided in Appendices D.1 and D.2. Additionally, we demonstrate the method\u2019s potential to enhance performance significantly, even with a marginal increase in computational cost, as illustrated in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5.3 Effect of the Consistency Loss on Rank Correlation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To answer Q2, we compare the consistency of graph representations across layers with and without the proposed consistency loss. Specifically, we use the Spearman\u2019s rank correlation coefficient, a widely accepted method for computing correlation between ranked variables, to quantitatively measure the consistency of graph similarities across layers. For a fair comparison, we construct a distance matrix $D^{h}$ for all test data at each layer $h$ , where each row $D_{x_{i}}^{h}$ ; represents the distances from graph $x_{i}$ to all other graphs. We then compute the rank correlation between $D_{x_{i},\\mathbf{\\lambda}}^{h}$ ,: and $D_{x_{i};}^{h+1}$ for each graph $x_{i}$ . ", "page_idx": 8}, {"type": "text", "text": "We average the correlation values for all graphs to obtain the overall correlation for layer $h$ . Then, we compute the mean of these values across layers, enabling a global comparison of relational ", "page_idx": 8}, {"type": "table", "img_path": "dg0hO4M11K/tmp/55e4bc3ca9c5e13b3da809d392b4bd41cfcc911a734aa624c59e370b9c3c0866.jpg", "table_caption": ["Table 2: Spearman correlation was computed for graph representations from consecutive layers on the TU datasets, both with and without consistency loss. Values with higher rank correlation are highlighted in grey. The consistency loss can enhance the rank correlation of graph similarities. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "consistency throughout the model and dataset. All results were averaged over 5 repeated experiments with same training setting. ", "page_idx": 8}, {"type": "text", "text": "We present our results on a series of datasets from the TU Dataset in Table 2. As shown in the table, it is evident that the representation space becomes more consistent with our proposed consistency loss. For example, a significant enhancement is observed for the GIN model. Another notable point is the result for the GCN model on the NCI109 dataset and for the GMT model on the IMDB-B dataset. We find that the correlation is already fairly high even without the implementation of Lconsistency, resulting in minimal correlation improvements with our method. This phenomenon provides a plausible explanation for why our method is not effective in these two cases. ", "page_idx": 8}, {"type": "text", "text": "5.4 Study on Task Complexity ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To address Q3 and further evaluate the performance of our method across different scenarios, we extended our study by conducting experiments on graph datasets with increasing task and structural complexity. ", "page_idx": 8}, {"type": "text", "text": "Increasing Task Complexity We increase the task complexity by expanding the number of classes that the model needs to classify. To assess the effect of increased class complexity on our method\u2019s performance, we sampled subsets from the REDDIT-MULTI 5K dataset [Yanardag and Vishwanathan, 2015] with a progressively greater number of classes, which originally consists of five classes. Specifically, we randomly sampled between 2 and 4 classes to construct new datasets from the original dataset and conducted classification tasks using both GCN and GCN with Lconsistency on these newly constructed datasets. We report the mean test accuracy over five experiments for each subset, as presented in Table.3. ", "page_idx": 8}, {"type": "table", "img_path": "dg0hO4M11K/tmp/5b3feed6caa45fee4bf2159778a42d7de3c491537deebd5a40e643e3cd70abdd.jpg", "table_caption": ["Table 3: Performance comparison across different subsets and the full set. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The results demonstrate that the effectiveness of our method remains robust, even as the number of classes increases. In fact, it may provide greater advantages when applied to multi-class classification tasks. This resilience likely stems from our method\u2019s focus on identifying relational structures in the intermediate representations of GNN models, rather than relying heavily on label information. This approach helps mitigate the impact of potential label noise in the original data. These findings align with the noticeable performance improvements observed in both binary and multi-class classification tasks, as shown in Table 1. ", "page_idx": 8}, {"type": "text", "text": "Increasing Structural Complexity We assessed the impact of structural complexity by partitioning the IMDB-B dataset into three subsets with progressively increasing graph densities. Graph density, denoted as N(2NM\u22121), where N is the number of nodes and M is the number of edges in graph $\\mathcal{G}$ , was used as the criterion for creating these subsets. The dataset was divided into three groups: (small) for graphs with densities below the 33rd percentile, (median) for densities between the 33rd and 67th percentiles, and (large) for graphs with densities above the 67th percentile. We applied both GCN and $\\mathrm{GCN}{+}\\mathcal{L}_{\\mathrm{c}}$ onsistency models to these subsets, and the results are summarized in Table 4. ", "page_idx": 9}, {"type": "table", "img_path": "dg0hO4M11K/tmp/e7cc9e4c49a6be1b0debca9313fb6294178580d4c381ad529d2e632e73aae8fa.jpg", "table_caption": ["Table 4: Performance comparison on IMDB-B datasets of different densities. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "The above results show that the GCN model, enhanced with the Lconsistency loss function, consistently outperforms the original version across different structural complexity groups, demonstrating the robustness and effectiveness of the proposed method. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, we evaluate the method\u2019s efficiency under various complexity scenarios, as detailed in the Appendix D.3. ", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Graph Distance and Similarity Measuring distances or similarities between graphs is a fundamental problem in graph learning. Graph kernels, which define graph similarity, have gained significant attention. Most graph kernels use the $\\mathcal{R}$ -Convolution framework [Haussler et al., 1999] to compare substructure similarities. A trailblazing kernel by [Kashima et al., 2003] used node and edge attributes to generate label sequences through a random walk. The WL-subtree kernel [Shervashidze et al., 2011] generates graph-level features by summing node representation contributions. Recent works align matching substructures between graphs. For instance, Kriege et al. [2016] proposed a discrete optimal assignment kernel based on vertex kernels from WL labels. Togninalli et al. [2019] extended this to include fractional assignments using the Wasserstein distance. Additionally, measuring graph distances is also a prevalent problem. Vayer et al. [2019] combined the Wasserstein and GromovWasserstein distances [Villani and Society, 2003, M\u00e9moli, 2011]. Chen et al. [2022] proposed a polynomial-time WL distance for labeled Markov chains, treating labeled graphs as a special case. ", "page_idx": 9}, {"type": "text", "text": "Bridging Graph Kernels and GNNs Many studies have explored the connection between graph kernels and GNNs, attempting to integrate them into a unified framework. Certain approaches focus on leveraging GNN architecture to design novel kernels. For instance, Mairal et al. [2014] presents neural network architectures that learn graph representations within the Reproducing Kernel Hilbert Space (RKHS) of graph kernels. Similarly, Du et al. [2019] proposed a graph kernel equivalent to infinitely wide GNNs, which can be trained using gradient descent. Conversely, other studies have incorporated kernel methods directly into GNNs. For example, Nikolentzos and Vazirgiannis [2020] utilize graph kernels as convolutional fliters within GNN architectures. Additionally, Lee et al. [2024] proposes a novel Kernel Convolution Network that employs the random walk kernel as the core mechanism for learning descriptive graph features. Instead of applying specific kernel patterns as mentioned in previous work, we introduce a general method for GNNs to capture consistent similarity relationships, thereby enhancing classification performance. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study a class of graph kernels and introduce the concept of consistency property in graph classification tasks. We theoretically prove that this property leads to a more structure-aware representation space for classification using kernel methods. Based on this analysis, we extend this principle to enhance GNN models. We propose a novel, model-agnostic consistency learning framework for GNNs that enables them to capture relational structures in the graph representation space. Experiments show that our proposed method universally enhances the performance of backbone networks on graph classification benchmarks, providing new insights into bridging the gap between traditional kernel methods and GNN models. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph multiset pooling. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net /forum?id $\\equiv$ JHcqXGaqiGn. ", "page_idx": 10}, {"type": "text", "text": "Franka Bause and Nils Morten Kriege. Gradual weisfeiler-leman: Slow and steady wins the race. In Bastian Rieck and Razvan Pascanu, editors, Learning on Graphs Conference, LoG 2022, 9-12 December 2022, Virtual Event, volume 198 of Proceedings of Machine Learning Research, page 20. PMLR, 2022. URL https://proceedings.mlr.press/v198/bause22a.html. ", "page_idx": 10}, {"type": "text", "text": "Karsten M. Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005), 27-30 November 2005, Houston, Texas, USA, pages 74\u201381. IEEE Computer Society, 2005. doi: 10.1109/ICDM.2005.132. URL https://doi.org/10.1109/ICDM.2005.132. ", "page_idx": 10}, {"type": "text", "text": "Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. Learning to rank using gradient descent. In Luc De Raedt and Stefan Wrobel, editors, Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August 7-11, 2005, volume 119 of ACM International Conference Proceeding Series, pages 89\u201396. ACM, 2005. doi: 10.1145/1102351.1102363. URL https://doi.org/10.1145/1102351.1102363. ", "page_idx": 10}, {"type": "text", "text": "Samantha Chen, Sunhyuk Lim, Facundo M\u00e9moli, Zhengchao Wan, and Yusu Wang. Weisfeilerlehman meets gromov-wasserstein. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 3371\u20133416. PMLR, 2022. URL https://proceedings.mlr.pres s/v162/chen22o.html. ", "page_idx": 10}, {"type": "text", "text": "Fabrizio Costa and Kurt De Grave. Fast neighborhood subgraph pairwise distance kernel. In Johannes F\u00fcrnkranz and Thorsten Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 255\u2013262. Omnipress, 2010. URL https://icml.cc/Conferences/2010/papers/347.pdf. ", "page_idx": 10}, {"type": "text", "text": "Simon S. Du, Kangcheng Hou, Ruslan Salakhutdinov, Barnab\u00e1s P\u00f3czos, Ruosong Wang, and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5724\u20135734, 2019. URL https://proceedings.neurips.cc/paper/201 9/hash/663fd3c5144fd10bd5ca6611a9a5b92d-Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1263\u20131272. PMLR, 2017. URL http://proceedings.mlr.press/v70/gilmer17a.html. ", "page_idx": 10}, {"type": "text", "text": "William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 1024\u20131034, 2017. URL https://proceedings.neurips. cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html. ", "page_idx": 10}, {"type": "text", "text": "David Haussler et al. Convolution kernels on discrete structures. Technical report, Citeseer, 1999. ", "page_idx": 10}, {"type": "text", "text": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, ", "page_idx": 10}, {"type": "text", "text": "editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/fb60d411a5c5b72b2e7d3527c fc84fd0-Abstract.html.   \nZheng Huang, Qihui Yang, Dawei Zhou, and Yujun Yan. Enhancing size generalization in graph neural networks through disentangled representation learning. In Forty-first International Conference on Machine Learning, 2024.   \nHisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs. In Tom Fawcett and Nina Mishra, editors, Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA, pages 321\u2013328. AAAI Press, 2003. URL http://www.aaai.org/Library/ICML/2003/icml03-044.php.   \nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openrevi ew.net/forum?id $\\equiv$ SJU4ayYgl.   \nNils M. Kriege, Pierre-Louis Giscard, and Richard C. Wilson. On valid optimal assignment kernels and applications to graph classification. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1615\u20131623, 2016. URL https://proceedings.neurips.cc/paper /2016/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html.   \nNils M. Kriege, Fredrik D. Johansson, and Christopher Morris. A survey on graph kernels. Appl. Netw. Sci., 5(1):6, 2020. doi: 10.1007/S41109-019-0195-3. URL https://doi.org/10.1007/ s41109-019-0195-3.   \nMeng-Chieh Lee, Lingxiao Zhao, and Leman Akoglu. Descriptive kernel convolution network with improved random walk kernel. In Tat-Seng Chua, Chong-Wah Ngo, Ravi Kumar, Hady W. Lauw, and Roy Ka-Wei Lee, editors, Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024, pages 457\u2013468. ACM, 2024. doi: 10.1145/3589334.3645405. URL https://doi.org/10.1145/3589334.3645405.   \nGaotang Li, Marlena Duda, Xiang Zhang, Danai Koutra, and Yujun Yan. Interpretable sparsification of brain graphs: Better practices and effective designs for graph neural networks. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1223\u20131234, 2023a.   \nGaotang Li, Danai Koutra, and Yujun Yan. Size generalization of graph neural networks on biological data: Insights and practices from the spectral perspective. arXiv preprint arXiv:2305.15611, 2023b.   \nLu Lin, Jinghui Chen, and Hongning Wang. Spectral augmentation for self-supervised learning on graphs. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id $=$ DjzBCrMBJ_p.   \nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pretraining molecular graph representation with 3d geometry. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id $=$ xQUe1pOKPam.   \nJulien Mairal, Piotr Koniusz, Za\u00efd Harchaoui, and Cordelia Schmid. Convolutional kernel networks. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2627\u20132635, 2014. URL https://proceedings.neurips.cc/paper/2014/hash/81ca026 2c82e712e50c580c032d99b60-Abstract.html.   \nFacundo M\u00e9moli. Gromov-wasserstein distances and the metric approach to object matching. Found. Comput. Math., 11(4):417\u2013487, 2011. doi: 10.1007/S10208-011-9093-5. URL https: //doi.org/10.1007/s10208-011-9093-5. ", "page_idx": 11}, {"type": "text", "text": "Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. CoRR, abs/2007.08663, 2020. URL https://arxiv.org/abs/2007.08663. ", "page_idx": 12}, {"type": "text", "text": "Giannis Nikolentzos and Michalis Vazirgiannis. Random walk graph neural networks. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ba95d78a7c942571185308775 a97a3a0-Abstract.html. ", "page_idx": 12}, {"type": "text", "text": "Giannis Nikolentzos, Giannis Siglidis, and Michalis Vazirgiannis. Graph kernels: A survey. J. Artif. Intell. Res., 72:943\u20131027, 2021. doi: 10.1613/JAIR.1.13225. URL https://doi.org/10.161 3/jair.1.13225. ", "page_idx": 12}, {"type": "text", "text": "Nino Shervashidze, S. V. N. Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten M. Borgwardt. Efficient graphlet kernels for large graph comparison. In David A. Van Dyk and Max Welling, editors, Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, AISTATS 2009, Clearwater Beach, Florida, USA, April 16-18, 2009, volume 5 of JMLR Proceedings, pages 488\u2013495. JMLR.org, 2009. URL http://proceedings.mlr.press/v5/s hervashidze09a.html. ", "page_idx": 12}, {"type": "text", "text": "Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12:2539\u20132561, 2011. doi: 10.5555/1953048.2078187. URL https://dl.acm.org/doi/10.5555/1953048.2078187. ", "page_idx": 12}, {"type": "text", "text": "Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 1548\u20131554. ijcai.org, 2021. doi: 10.24963/IJCAI.2021/214. URL https://doi.org/10.24963/ijcai.2021/214. ", "page_idx": 12}, {"type": "text", "text": "Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Konstantinos Skianis, and Michalis Vazirgiannis. Grakel: A graph kernel library in python. J. Mach. Learn. Res., 21: 54:1\u201354:5, 2020. URL https://www.jmlr.org/papers/v21/18-370.html. ", "page_idx": 12}, {"type": "text", "text": "Matteo Togninalli, M. Elisabetta Ghisu, Felipe Llinares-L\u00f3pez, Bastian Rieck, and Karsten M. Borgwardt. Wasserstein weisfeiler-lehman graph kernels. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 6436\u20136446, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/73fed7fd472e502d8 908794430511f4d-Abstract.html. ", "page_idx": 12}, {"type": "text", "text": "Titouan Vayer, Nicolas Courty, Romain Tavenard, Laetitia Chapel, and R\u00e9mi Flamary. Optimal transport for structured data with application on graphs. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 6275\u20136284. PMLR, 2019. URL http://proceedings.mlr.press/ v97/titouan19a.html. ", "page_idx": 12}, {"type": "text", "text": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=rJXMpikCZ. ", "page_idx": 12}, {"type": "text", "text": "C. Villani and American Mathematical Society. Topics in Optimal Transportation. Graduate studies in mathematics. American Mathematical Society, 2003. ISBN 9781470418045. URL https://books.google.com/books?id=MyPjjgEACAAJ. ", "page_idx": 12}, {"type": "text", "text": "Haohui Wang, Yuzhen Mao, Yujun Yan, Yaoqing Yang, Jianhui Sun, Kevin Choi, Balaji Veeramani, Alison Hu, Edward Bowen, Tyler Cody, et al. Evolunet: Advancing dynamic non-iid transfer learning on graphs. In Forty-first International Conference on Machine Learning, 2024. ", "page_idx": 12}, {"type": "text", "text": "Boris Weisfeiler and AA Lehman. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12\u201316, 1968. ", "page_idx": 13}, {"type": "text", "text": "Zhang Xinyi and Lihui Chen. Capsule graph neural network. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Byl8BnRcYm. ", "page_idx": 13}, {"type": "text", "text": "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= ryGs6iA5Km. ", "page_idx": 13}, {"type": "text", "text": "Minkai Xu, Alexander S. Powers, Ron O. Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 38592\u201338610. PMLR, 2023. URL https: //proceedings.mlr.press/v202/xu23n.html. ", "page_idx": 13}, {"type": "text", "text": "Yujun Yan, Jiong Zhu, Marlena Duda, Eric Solarz, Chandra Sripada, and Danai Koutra. Groupinn: Grouping-based interpretable neural network for classification of limited, noisy brain data. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 772\u2013782, 2019. ", "page_idx": 13}, {"type": "text", "text": "Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. In Longbing Cao, Chengqi Zhang, Thorsten Joachims, Geoffrey I. Webb, Dragos D. Margineantu, and Graham Williams, editors, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10-13, 2015, pages 1365\u20131374. ACM, 2015. doi: 10.1145/2783258.2783417. URL https://doi.org/10.1145/2783258.2783417. ", "page_idx": 13}, {"type": "text", "text": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Yike Guo and Faisal Farooq, editors, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018, pages 974\u2013983. ACM, 2018. doi: 10.1145/3219819.3219890. URL https://doi.org/10.1145/3219819.3219890. ", "page_idx": 13}, {"type": "text", "text": "Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/3fe230348e9a12c13120749e3f9fa4cd-Abstract.html. ", "page_idx": 13}, {"type": "text", "text": "Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and Arye Nehorai. Retgk: Graph kernels based on return probabilities of random walks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 3968\u20133978, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/7f16109f1619fd7a733da f5a84c708c1-Abstract.html. ", "page_idx": 13}, {"type": "text", "text": "Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph Contrastive Learning with Adaptive Augmentation. In Proceedings of The Web Conference 2021, WWW \u201921, pages 2069\u20132080, New York, NY, USA, April 2021. Association for Computing Machinery. ISBN 9781450370233. doi: 10.1145/3442381.3449802. URL https://doi.org/10.1145/3442381. 3449802. ", "page_idx": 13}, {"type": "text", "text": "A Theoretical Verification with WL-based Kernels ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Non-decreasing Margin ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Suppose that at iteration $i$ , graph $x_{1}$ in class 1 and graph $y_{1}$ in class 2 decide the margin and at iteration $i+1$ , graph $x_{2}$ in class 1 and graph $y_{2}$ in class 2 decide the margin. There are two scenarios: (1) $x_{1}=x_{2}$ and $y_{1}=y_{2}$ , (2) $x_{1}\\neq x_{2}$ or $y_{1}\\neq y_{2}$ . For case 1, margin at iteration $i$ is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\phi(\\psi^{i}(x_{1}))-\\phi(\\psi^{i}(y_{1}))\\|=\\sqrt{(\\phi(\\psi^{i}(x_{1}))-\\phi(\\psi^{i}(y_{1})))^{T}(\\phi(\\psi^{i}(x_{1}))-\\phi(\\psi^{i}(y_{1})))}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sqrt{2-2\\langle\\phi(\\psi^{i}(x_{1})),\\phi(\\psi^{i}(y_{1}))\\rangle}=\\sqrt{2-2\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x_{1},y_{1},i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x,y,i)$ is monotonically decreasing, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{2-2\\mathbb{K}_{\\mathcal{F}_{c},\\phi}(x_{1},y_{1},i)}\\leq\\sqrt{2-2\\mathbb{K}_{\\mathcal{F}_{c},\\phi}(x_{1},y_{1},i+1)}=\\|\\phi(\\psi^{i+1}(x_{1}))-\\phi(\\psi^{i+1}(y_{1}))\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This suggests that the margin does not decrease. ", "page_idx": 14}, {"type": "text", "text": "For case 2, we prove by contradiction and first assume that the margin decreases. Since the margin decreases, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\phi(\\psi^{i+1}(x_{2}))-\\phi(\\psi^{i+1}(y_{2}))\\|<\\|\\phi(\\psi^{i}(x_{1}))-\\phi(\\psi^{i}(y_{1}))\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "At iteration $i$ , since $x_{1}$ and $y_{1}$ decide the margin, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\phi(\\psi^{i}(x_{1}))-\\phi(\\psi^{i}(y_{1}))\\|\\leq\\|\\phi(\\psi^{i}(x_{2}))-\\phi(\\psi^{i}(y_{2}))\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining Equation 3 and Equation 4, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\phi(\\psi^{i+1}(x_{2}))-\\phi(\\psi^{i+1}(y_{2}))\\|<\\|\\phi(\\psi^{i}(x_{2}))-\\phi(\\psi^{i}(y_{2}))\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This is equivalent to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{2-2\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x_{2},y_{2},i+1)}<\\sqrt{2-2\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x_{2},y_{2},i)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "suggesting that $\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x_{2},y_{2},i)$ increases with $i$ , which conflicts with the fact that $\\tilde{\\mathbb{K}}_{\\mathcal{F}_{c},\\phi}(x_{2},y_{2},i)$ is monotonically decreasing. Therefore, the margin does not decrease. ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Following Equation 2.4, the normalized WL-subtree kernel after $h$ iterations can be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{w l_{-}s u b t r e e}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)=\\frac{\\sum_{i=1}^{h}\\left\\langle\\phi(\\psi^{i}(\\mathcal{G})),\\phi(\\psi^{i}\\left(\\mathcal{G}^{\\prime}\\right))\\right\\rangle}{\\sqrt{\\sum_{i=1}^{h}\\left\\langle\\phi(\\psi^{i}(\\mathcal{G})),\\phi(\\psi^{i}\\left(\\mathcal{G}\\right))\\right\\rangle}\\sqrt{\\sum_{i=1}^{h}\\left\\langle\\phi(\\psi^{i}(\\mathcal{G}^{\\prime})),\\phi(\\psi^{i}\\left(\\mathcal{G}^{\\prime}\\right))\\right\\rangle}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we show by counterexample that the WL-subtree kernel may not preserve relational structure. ", "page_idx": 14}, {"type": "text", "text": "For example, consider two graphs, $\\mathcal{G}$ and $\\mathcal{G}^{\\prime}$ , in the $h$ -th iteration. The color (label) set of the nodes in these graphs is denoted as $\\{C_{a},C_{b},C_{c},...\\}$ . Graph $\\mathcal{G}$ contains 200 nodes labeled $C_{a}$ and 4 nodes labeled $C_{b}$ , whereas graph $\\mathcal{G}^{\\prime}$ contains 4 nodes labeled $C_{a}$ and 200 nodes labeled $C_{b}$ . The similarity is computed using the vectors $\\phi\\left(\\psi^{i}({\\mathcal G})\\right)=[200,4]$ and $\\stackrel{\\cdot}{\\phi}\\left(\\psi^{i}\\left(\\mathcal{G}^{\\prime}\\right)\\right)=\\left[4,200\\right]$ . Consequently, the resulting similarity $\\tilde{\\mathbb{K}}_{w l\\_s u b t r e e}^{(h)}$ is 0.0400. ", "page_idx": 14}, {"type": "text", "text": "In the next iteration, for graph $\\mathcal{G}$ , assume that half of the nodes currently labeled $C_{a}$ are relabeled to $C_{c}$ , while the other half are relabeled to $C_{d}$ . Additionally, all nodes labeled $C_{b}$ are relabeled to $C_{e}$ . As a result, the updated histogram vector for $\\mathcal{G}$ becomes $\\dot{\\phi}\\left(\\psi^{i+1}(\\mathcal{G})\\right)=\\left[100,100,4\\right]$ . Similarly, in graph $\\mathcal{G}^{\\prime}$ , half of the nodes labeled $C_{a}$ are relabeled to $C_{c}$ and the remaining half to $C_{d}$ , with all nodes labeled $C_{b}$ relabeled to $C_{e}$ , resulting in a histogram vector of $\\phi\\left(\\psi^{i+1}\\check{(}\\mathcal{G}^{\\prime})\\right)=[2,\\bar{2},200]$ Here, the normalized similarity $\\tilde{\\mathbb{K}}_{w l\\_s u b t r e e}^{(h)}$ is 0.0404, which exceeds $\\tilde{\\mathbb{K}}_{w l\\_s u b t r e e}^{(h)}$ . ", "page_idx": 14}, {"type": "text", "text": "This situation violates the principle of monotonic decrease, indicating that the WL-subtree kernel does not exhibit monotonicity\u2014a phenomenon frequently observed in real-world datasets. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.3.1 Monotonic Decrease in WLOA Kernel ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The WLOA kernel after $h$ iterations can be computed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{K}_{W L O A}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)=\\sum_{i=1}^{h}\\mathrm{{hi}\\,s t m i n}\\left\\{\\phi(\\psi^{i}(\\mathcal{G})),\\phi(\\psi^{i}(\\mathcal{G}^{\\prime}))\\right\\}\\cdot\\omega(i)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\omega(i)$ is a monotonically increasing function. The normalized kernel is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})=\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sqrt{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G})}\\sqrt{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G}^{\\prime},\\mathcal{G}^{\\prime})}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "At any iteration $i$ , the expression histmin $\\left\\{\\phi(\\psi^{i}({\\mathcal{G}})),\\phi(\\psi^{i}({\\mathcal{G}}))\\right\\}\\cdot\\omega(i)$ equals to $\\omega(i)|\\gamma|$ . Consequently, for the $h$ -th iteration, this implies: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G})=\\sum_{i=1}^{h}\\omega(i)|\\mathcal{V}|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the normalized kernel value for the graph pair $(\\mathcal{G},\\mathcal{G}^{\\prime})$ can be expressed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})=\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sqrt{|\\mathcal{V}||\\mathcal{V}^{\\prime}|}\\sum_{h}^{i=1}\\omega(i)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given this, t(hhe+ 1d)ifference between the kernel value at the $h$ -th iteration, $\\tilde{\\mathbb{K}}_{W L O A}^{(h)}$ , and the $(h+1)$ -th iteration, , can be expressed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\Tilde{\\mathbb{K}}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})-\\Tilde{\\mathbb{K}}_{W L O A}^{(h+1)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\mathbb{K}_{W L O A}^{(h+1)}(\\mathcal{G},\\mathcal{G}^{\\prime})}}\\\\ &{=\\!\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sqrt{|\\mathcal{V}||\\mathcal{V}^{\\prime}|}}-\\frac{\\mathbb{K}_{W L O A}^{(h+1)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sqrt{|\\mathcal{V}||\\mathcal{V}^{\\prime}|}\\sum_{i=1}^{h+1}\\omega(i)}}\\\\ &{=\\!\\frac{1}{\\sqrt{|\\mathcal{V}||\\mathcal{V}^{\\prime}|}}\\cdot(\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h}\\omega(i)}-\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})+\\operatorname{histmin}\\{\\phi(\\psi^{h+1}(\\mathcal{G})),\\phi(\\psi^{h+1}(\\mathcal{G}^{\\prime}))\\omega(h+1)\\}}{\\sum_{i=1}^{h+1}\\omega(i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Considering the hierarchical structure of the refinement processes, one can deduce the following inequality: ", "page_idx": 15}, {"type": "text", "text": "his $\\operatorname*{min}\\{\\phi(\\psi^{i}(\\mathcal{G})),\\phi(\\psi^{i}(\\mathcal{G}^{\\prime}))\\}\\leq\\mathrm{histmin}\\{\\phi(\\psi^{i-1}(\\mathcal{G})),\\phi(\\psi^{i-1}(\\mathcal{G}^{\\prime}))\\}\\leq\\cdots\\mathrm{histmin}\\{\\phi(\\psi^{1}(\\mathcal{G})),\\phi(\\psi^{1}(\\mathcal{G}^{\\prime}))\\}$ ", "page_idx": 15}, {"type": "text", "text": "Thus, for the kernel at the $(h+1)$ -th iteration, it follows that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{istmin}\\{\\phi(\\psi^{h+1}(\\mathcal{G})),\\phi(\\psi^{h+1}(\\mathcal{G}^{\\prime}))\\}\\le\\frac{\\sum_{i=1}^{h}\\mathrm{histmin}\\{\\phi(\\psi^{i}(\\mathcal{G})),\\phi(\\psi^{i}(\\mathcal{G}^{\\prime}))\\cdot\\omega(i)\\}}{\\sum_{i=1}^{h}\\omega(i)}=\\frac{\\mathbb{E}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h}\\omega(i)}\\{\\frac{1}{\\sum_{i=1}^{h}\\omega(i)},\\psi^{i}(\\mathcal{G},\\mathcal{G}^{\\prime})\\}\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given this inequality, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\ge\\frac{1}{\\sqrt{|\\mathcal{V}||\\mathcal{V}^{\\prime}|}}\\cdot(\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h}\\omega(i)}-\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})+\\frac{\\omega(h+1)}{\\sum_{i=1}^{h}\\omega(i)}\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h+1}\\omega(i)})}\\\\ &{\\quad=\\frac{1}{\\sqrt{|\\mathcal{V}||\\mathcal{V}^{\\prime}|}}\\cdot(\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h}\\omega(i)}-\\frac{\\sum_{i=1}^{h}\\omega(i)\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h+1}\\omega(i)}+\\frac{\\omega(h+1)}{\\sum_{i=1}^{h}\\omega(i)}\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h+1}\\omega(i)})}\\\\ &{\\quad=\\frac{1}{\\sqrt{|\\mathcal{V}||\\mathcal{V}^{\\prime}|}}\\cdot\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sum_{i=1}^{h}\\omega(i)}\\cdot(1-\\frac{\\sum_{i=1}^{h}\\omega(i)+\\omega(h+1)}{\\sum_{i=1}^{h}\\omega(i)+\\omega(h+1)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we get $D~\\geq~0$ . The value of Equation 7 is non-negative, indicating that WLOA is monotonically decreasing. ", "page_idx": 16}, {"type": "text", "text": "A.3.2 Order Consistency in WLOA Kernel ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Assume two graph pairs satisfy the following relation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})\\ge\\tilde{\\mathbb{K}}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime\\prime})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The normalized WLOA kernel for a graph pair $(\\mathcal{G},\\mathcal{G}^{\\prime})$ at iteration $h+1$ can be expressed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)=\\frac{\\mathbb{K}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)}{\\sqrt{|\\mathcal{V}|\\,|\\mathcal{V}^{\\prime}|}\\sum_{i=1}^{h+1}\\omega(i)}=\\frac{\\sum_{i=1}^{h}\\omega(i)}{\\sum_{i=1}^{h+1}\\omega(i)}\\cdot\\frac{\\mathbb{K}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime}\\right)}{\\sqrt{|\\mathcal{V}|\\,|\\mathcal{V}^{\\prime}|}\\sum_{i=1}^{h}\\omega(i)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\mathbb{K}_{W L O A}^{(h)}$ i+s  hmiosntomtionn $h$ ,u sa, s wite  fhoallvoe:ws from: $\\mathbb{K}_{W L O A}^{(h+1)}=$ $\\left\\{\\phi(\\psi^{h+1}(\\mathcal{G})),\\phi(\\psi^{h+1}(\\mathcal{G}^{\\prime}))\\right\\}\\cdot\\omega(h+1)$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}(\\mathcal{G},\\mathcal{G}^{\\prime})\\ge\\frac{\\sum_{i=1}^{h}\\omega(i)}{\\sum_{i=1}^{h+1}\\omega(i)}\\cdot\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime})}{\\sqrt{|\\mathcal{V}||V^{\\prime}|}\\sum_{i=1}^{h}\\omega(i)}}\\\\ &{\\ge\\frac{\\sum_{i=1}^{h}\\omega(i)}{\\sum_{i=1}^{h+1}\\omega(i)}\\cdot\\frac{\\mathbb{K}_{W L O A}^{(h)}(\\mathcal{G},\\mathcal{G}^{\\prime\\prime})}{\\sqrt{|\\mathcal{V}||V^{\\prime\\prime}|}\\sum_{i=1}^{h}\\omega(i)}}\\\\ &{=\\frac{\\sum_{i=1}^{h}\\omega(i)}{\\sum_{i=1}^{h+1}\\omega(i)}\\cdot\\tilde{\\mathbb{K}}_{W L O A}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the monotonic decrease property of \u02dcK(Wh )LOA (G, G\u2032\u2032),we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)\\le\\tilde{\\mathbb{K}}_{W L O A}^{(h)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we can conclude: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}(\\mathcal{G},\\mathcal{G}^{\\prime})\\ge\\frac{\\sum_{i=1}^{h}\\omega\\left(i\\right)}{\\sum_{i=1}^{h+1}\\omega\\left(i\\right)}\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)}\\\\ &{\\qquad\\qquad=\\left(1-\\frac{\\omega\\left(h+1\\right)}{\\sum_{i=1}^{h+1}\\omega\\left(i\\right)}\\right)\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When \u03c9(i) = 1, limh\u2192\u221e $\\begin{array}{r}{\\operatorname*{lim}_{h\\rightarrow\\infty}{\\frac{\\omega(h+1)}{\\sum_{i=1}^{h+1}\\omega(i)}}\\,=\\,\\operatorname*{lim}_{h\\rightarrow\\infty}{\\frac{h+1}{(h+2)(h+1)/2}}\\,=\\,\\operatorname*{lim}_{h\\rightarrow\\infty}{\\frac{2}{h+2}}\\,=\\,0}\\end{array}$ . Therefore, $\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}(\\mathcal{G},\\mathcal{G}^{\\prime})\\ge\\tilde{\\mathbb{K}}_{W L O A}^{(h+1)}\\left(\\mathcal{G},\\mathcal{G}^{\\prime\\prime}\\right)$ when $h\\to\\infty$ . \u25a1 ", "page_idx": 16}, {"type": "table", "img_path": "dg0hO4M11K/tmp/729e3abe16c4bdfd83344f1b65564c8a082b950d4bff5564072880a79911b338.jpg", "table_caption": ["Table 5: Dataset statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide the statistics of the datasets used, as shown in Table 5. ", "page_idx": 17}, {"type": "text", "text": "C Detailed Set-Up ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "TU Dataset We restrict the hyperparameters and ensure the same architecture is used for both the base and enhanced models on the same dataset for a fair comparison. Specifically, we fixed the hidden size to 32, the number of layers to 3, and used a global mean pooling layer to generate the graph representations. The Adam optimizer was used for optimization. During the training process, we tuned both the base and enhanced models with the same search ranges: batch size {64, 128}, dropout rate $\\{0,0.3\\}$ and learning rate {0.0001, 0.001, 0.01}.,The only additional parameter for the enhanced model is the regularization term, which ranges from {0.1, 0.5, 1, 10}. ", "page_idx": 17}, {"type": "text", "text": "OGB Considering the complexity of the OGB dataset, we slightly expand the hyperparameter search range. Initially, we train the model with consistency loss, exploring hidden sizes {32, 64, 128, 256}, batch sizes {64, 128, 256}, and the number of layers {3, 4}, while keeping other parameters consistent with our experiments on the TU dataset. Subsequently, we fix the hidden size and the number of layers to ensure an identical network structure. We then repeat the parameter search process and employ the optimal settings for testing on the base model. All experiments on the OGB dataset are repeated 5 times to calculate the mean and standard deviation. ", "page_idx": 17}, {"type": "text", "text": "Reddit-T Given that the Reddit-T and OGB datasets are of comparable size, we employ similar experimental settings for training models on these datasets. The architecture is kept fixed, and we search for optimal hyperparameters for both the base model and the models incorporating consistency loss. Each model is trained and evaluated over 5 independent runs, with the mean and standard deviation of the results recorded for performance comparison. ", "page_idx": 17}, {"type": "text", "text": "D Complexity& Scalability ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Time Complexity ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present the time complexity analysis for our proposed consistency loss. The loss computation icnovmolpvleesx itthy e ocf $O$ pbuaattcihosni zoef $:\\frac{\\mathrm{batchsize}-1}{2}\\big)=O(\\mathrm{batchsize}^{2})$ h. s Giinv ea n btahtacth t, hreerseu latrien datasetsize obmaptcuthaetsi oinn each training epoch and that the similarities are computed between consecutive layers, the total ", "page_idx": 17}, {"type": "text", "text": "complexity is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{O(\\mathrm{\\loss\\it~})=O\\left\\{\\mathrm{\\batchsize^{2}\\times(\\mathrm{\\layernum~}-1)\\times\\frac{\\ d a t a s e t s i z e}{\\ b a t c h s i z e}}\\right\\}}\\\\ &{\\mathrm{\\qquad~\\~=~}O(\\mathrm{\\datasetsize\\timesbatchsize\\times\\mathrm{\\layernum~}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This analysis shows that the time required to compute consistency loss scales linearly with dataset size, batch size, and the number of layers. It is important to note that the training time for baseline models also scales linearly with dataset size. ", "page_idx": 18}, {"type": "text", "text": "Since batch size and the number of layers are generally small compared to dataset size, our experiments primarily focus on how dataset size affects training time. We evaluate the training time of several models\u2014GCN, GIN, GraphSAGE, GTransformer, and GMT\u2014each enhanced with our consistency loss. This evaluation is conducted on different subsets of the ogbg-molhiv dataset, with subset sizes adjusted by varying the sampling rates. The training time, measured in seconds, are presented in Figure 3 . As shown, our findings confirm that training time increases linearly with dataset size, indicating that our method maintains training efficiency comparable to baselines without adding significant time burdens. ", "page_idx": 18}, {"type": "image", "img_path": "dg0hO4M11K/tmp/c9b7a268296a7a8e66e497a98bb22b0993c0323911ef34225f3b7a20ddbfd70e.jpg", "img_caption": ["Figure 3: Training Cost Escalates Linearly with Dataset Size Increase "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Furthermore, we empirically measure the training time for both the baseline models and our proposed methods. Each model comprises three layers and is trained on the ogbg-molhiv dataset $^{40,000+}$ graphs) for 100 epochs. We calculate the average training time per epoch in seconds and present the results in Table 6, showing that while the inclusion of the consistency loss slightly increases the training time, the impact is minimal. ", "page_idx": 18}, {"type": "table", "img_path": "dg0hO4M11K/tmp/a964ef62aefbb8c40ad766d6104dfd23a4e0cbcd4608f6be542e5dc07206a8b4.jpg", "table_caption": ["Table 6: Average training time per epoch for different models on the ogbg-molhiv dataset, measured in seconds. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.2 Space Complexity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Next, we present the space complexity analysis for our consistency loss. At each iteration, the loss function requires storing two pairwise similarity matrices corresponding to two consecutive layers, which is given by: ", "page_idx": 18}, {"type": "equation", "text": "$$\nO(\\mathrm{\\loss\\it{\\Delta})=O(b a t c h s i z e^{2})}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since we use stochastic gradient descent, similarity matrices are not retained for the next iteration. The consistency loss requires significantly less space than node embeddings, making the additional space requirement minimal. Table 7 shows the peak memory usage in megabytes (MB) for different models when training on the ogbg-molhiv dataset, illustrating that the space costs are negligible. ", "page_idx": 18}, {"type": "table", "img_path": "dg0hO4M11K/tmp/8aa4bc9e531aa84440a4b0d20cb7650088a41b1e4884608f7d6a010ea011ac7a.jpg", "table_caption": ["Table 7: Peak memory usage for different models on the ogbg-molhiv dataset, measured in megabytes. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.3 Efficiency on Different Task and Structural Complexities ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Task Complexity We measured the runtime of the models on different subsets to evaluate how task complexity, in terms of the number of classes, influences the efficiency of the proposed method. The results are presented in Table 8. As demonstrated, the additional computational time remains minimal even with an increasing number of classes, suggesting that the method scales effectively with growing class complexity. ", "page_idx": 19}, {"type": "table", "img_path": "dg0hO4M11K/tmp/3249f048d2a6d287a75bbd411fa8b5cda563035ced2f552f078d68e8522c9832.jpg", "table_caption": ["Table 8: Average training time per epoch on REDDIT subsets with varying class complexity, measured in seconds "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Structure Complexity We also conducted experiments to assess the training costs on datasets with varying structural complexities when introducing the ${\\mathcal{L}}_{\\mathrm{c}}$ onsistency . The results, summarized below in Table 9, show that the additional training cost remains minimal across datasets with different structures. This demonstrates the broad applicability of the proposed method, regardless of structural complexity. ", "page_idx": 19}, {"type": "table", "img_path": "dg0hO4M11K/tmp/3b7c1f3c5919b932f1dab10e4076d2720844fe75c737792e1fe83ee91beb0a4f.jpg", "table_caption": ["Table 9: Average training time per epoch for subsets of varying structural complexity from IMDB-B, measured in seconds. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Efficient Consistent Learning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To further minimize the overhead of our proposed consistency loss, we examined a scenario where the consistency loss, denoted as $\\mathcal{L}_{F L}$ , was only applied to the first and last layers. ", "page_idx": 19}, {"type": "text", "text": "Building upon the experimental setup described in Section 5, we conducted experiments using various backbone models. The results are summarized in Table 10. The penultimate column of this table highlights the performance gains achieved by applying the consistency loss across all layers, while the final column demonstrates the improvements observed when the consistency loss is only applied to the first and last layers. ", "page_idx": 19}, {"type": "table", "img_path": "dg0hO4M11K/tmp/cf62bdfa0f389b090beb3d73a029d03110df169fe7467105cc41e5e72826152b.jpg", "table_caption": ["Table 10: Graph classification performance with improvements of $\\mathcal{L}_{A L L}$ and $\\mathcal{L}_{F L}$ over base models. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Notably, applying the consistency loss only to the first and last layers achieves performance comparable to that of applying it across all layers, with both configurations yielding substantial improvements over the original model. This finding suggests that our proposed approach can be accelerated with minimal additional computational cost while still enhancing performance, thereby validating the effectiveness of the consistent learning principle. ", "page_idx": 19}, {"type": "text", "text": "F Similarity/Difference with Contrastive learning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we discuss the similarities and differences between our method and graph contrastive learning. Graph Contrastive Learning (GCL) is a self-supervised technique for graph data that emphasizes instance discrimination [Lin et al., 2023, Zhu et al., 2021]. A typical GCL framework generates multiple graph views via augmentations and contrasts positive samples (similar instances) with negative samples (dissimilar instances). This approach facilitates effective representation learning by capturing relationships between views, ensuring positive pairs remain close in the embedding space while distinctly separating negative pairs. ", "page_idx": 20}, {"type": "text", "text": "While both GCL and our method leverage graph similarity, our approach focuses on maintaining consistency across layers, rather than solely capturing similarities as in contrastive learning. To demonstrate this, we integrated the GraphCL technique [You et al., 2020] into a GCN model $\\scriptstyle(\\mathrm{GCN+CL})$ ) and assessed its performance and layer consistency across various datasets. The results, detailed in Tables 11 and 12, use classification accuracy and Spearman rank correlation to measure performance and consistency, respectively. ", "page_idx": 20}, {"type": "table", "img_path": "dg0hO4M11K/tmp/1eaf985992e55adcf6707847cafbb52dce54bb36d76cf742133aeb7331ebd6d9.jpg", "table_caption": ["Table 11: Graph classification accuracy of GCN with contrastive learning applied across various datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "dg0hO4M11K/tmp/3aa4a461df82ddbdfb4a024c3b7922b38e585530769f0f1c61fc4980e1bcd5ff.jpg", "table_caption": ["Table 12: Spearman correlation for graph representations from consecutive layers. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "As demonstrated by the results, our method consistently outperforms $\\mathrm{GCN+CL}$ in both graph classification performance and in enhancing similarity consistency across layers. This underscores the significant differences between our approach and regular GCL methods. ", "page_idx": 20}, {"type": "text", "text": "G Boarder Impact ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This paper aims to advance the field of graph learning by proposing a model-agnostic consistency learning framework. Our framework can be plugged into and improve current methods for graph classification tasks. This has potential benefits in sectors such as chemistry, bioinformatics and social analysis, where graph classification is widely used. Additionally, we do not foresee any direct negative societal or ethical consequences stemming from our work. ", "page_idx": 20}, {"type": "text", "text": "H Limitation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "One limitation of our work is that the method involves additional computational costs, especially during large batch training processes. To extend our framework, sampling methodologies on data or layers can be applied during the consistency-preserving training process. By selectively sampling data points or specific layers, we can reduce the computational burden while still maintaining the effectiveness of the cross-layer consistency loss, making the framework more scalable and applicable to larger datasets. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The experiments and results to support the claims in the introduction and abstract are all in the main paper and the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The limitations are discussed in the main paper Sec. H Limitations. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper include theoretical result and proof involved in main paper and appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The training schedule and hyper-parameters are listed in Appendix Sec. C. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The data are public, and code is provided in the anonymous github. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have listed all the settings in the appenix for paramtere settings and training schedules. The data splits are decribed under each case study section in the main paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All of our results are listed with error bars. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 23}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The compute resources of experiments are reported in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research conducted in this paper conforms, in every respect, to the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The broader impact is discussed in the appendix G. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: They are properly cited and credited. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: the new model introduced in the paper is well documented. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]