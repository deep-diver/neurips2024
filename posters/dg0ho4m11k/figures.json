[{"figure_path": "dg0hO4M11K/figures/figures_1_1.jpg", "caption": "Figure 1: Cosine similarity of three molecules from the NCI1 dataset, evaluated using graph representations from three consecutive GIN layers. Common GNN models fail to preserve relational structures across the layers.", "description": "This figure demonstrates the inconsistency in similarity relationships captured by graph neural networks (GNNs) across different layers.  Cosine similarity is calculated between three molecules from the NCI1 dataset using graph representations obtained from three consecutive GIN layers (layer 0, 1 and 2).  The figure shows that the relative similarity between the molecules changes across layers. For example, two molecules that are very similar at layer 0 might show reduced similarity at layer 1, and then increased similarity again at layer 2. This inconsistency in similarity relationships highlights a limitation of common GNNs in preserving relational structures during the message-passing process.", "section": "1 Introduction"}, {"figure_path": "dg0hO4M11K/figures/figures_5_1.jpg", "caption": "Figure 2: Computation of loss. At each layer, pairwise distance matrix D is calculated using the normalized representations of graphs in a batch. After randomly selecting a reference graph xk, the reference probability matrix is computed using the distance matrix from previous layer, where entry (n, m) represents the known probability that the graph xk is more similar to the graph xn than to the graph xm. For the distance matrix of current layer, we compute the predicted probability that xk is closer to xn than to xm and form the prediction probability matrix. Consistency loss is computed as the cross-entropy between the predicted and reference probability matrices.", "description": "This figure illustrates the computation of the consistency loss, which is designed to align the ranking of graph similarities across GNN layers.  The process involves calculating pairwise distance matrices (D) at each layer using graph representations. A reference graph is randomly selected, and a reference probability matrix is created based on distances from the previous layer.  A prediction probability matrix is then generated using the current layer's distances. The consistency loss is finally computed as the cross-entropy between the reference and prediction matrices.", "section": "4.1 Consistency Loss"}, {"figure_path": "dg0hO4M11K/figures/figures_18_1.jpg", "caption": "Figure 2: Computation of loss. At each layer, pairwise distance matrix D is calculated using the normalized representations of graphs in a batch. After randomly selecting a reference graph xk, the reference probability matrix is computed using the distance matrix from previous layer, where entry (n, m) represents the known probability that the graph xk is more similar to the graph xn than to the graph xm. For the distance matrix of current layer, we compute the predicted probability that xk is closer to xn than to xm and form the prediction probability matrix. Consistency loss is computed as the cross-entropy between the predicted and reference probability matrices.", "description": "This figure illustrates the computation of the consistency loss, a key component of the proposed method in the paper.  It shows how pairwise distances between graph representations are used to generate probability matrices at each layer. These matrices quantify the similarity relationships between graphs, and the consistency loss measures the discrepancy between these relationships across consecutive layers.  This helps to ensure consistent similarity rankings across layers during training.", "section": "4.1 Consistency Loss"}]