[{"figure_path": "dg0hO4M11K/tables/tables_7_1.jpg", "caption": "Table 1: Classification performance on the TU and OGB datasets, with and without the consistency loss. Highlighted cells indicate instances where the base GNN with the consistency loss outperforms the base GNN alone. The reported values are average accuracy for TU datasets and ROC-AUC for the ogbg-molhiv dataset, including their standard deviations.", "description": "This table presents the classification performance of several Graph Neural Networks (GNNs) on various graph datasets, both with and without the proposed consistency loss.  It shows the average accuracy (for TU datasets) or ROC-AUC score (for OGB-HIV) for each model and dataset. Highlighted entries indicate cases where adding the consistency loss improved performance. The table helps demonstrate the effectiveness of the consistency loss across multiple GNN architectures and diverse datasets.", "section": "5.2 Effectiveness of Consistency Loss"}, {"figure_path": "dg0hO4M11K/tables/tables_8_1.jpg", "caption": "Table 2: Spearman correlation was computed for graph representations from consecutive layers on the TU datasets, both with and without consistency loss. Values with higher rank correlation are highlighted in grey. The consistency loss can enhance the rank correlation of graph similarities.", "description": "This table presents the Spearman rank correlation coefficients, measuring the consistency of graph similarity rankings across consecutive layers of various GNN models.  It compares the results with and without the proposed consistency loss applied to the models.  Higher correlation values indicate stronger consistency in the similarity rankings between layers. The results are shown for several graph datasets from the TU benchmark dataset.", "section": "5.3 Effect of the Consistency Loss on Rank Correlation"}, {"figure_path": "dg0hO4M11K/tables/tables_8_2.jpg", "caption": "Table 3: Performance comparison across different subsets and the full set.", "description": "This table presents the performance comparison of GCN and GCN enhanced with consistency loss across different subsets of Reddit dataset with varying number of classes (2,3,4,5).  The subsets are created by randomly sampling classes from the original dataset. The table shows the mean accuracy over five experiments for each subset.", "section": "5.4 Study on Task Complexity"}, {"figure_path": "dg0hO4M11K/tables/tables_9_1.jpg", "caption": "Table 4: Performance comparison on IMDB-B datasets of different densities.", "description": "This table presents the classification performance comparison between the GCN model and the GCN model enhanced with the proposed consistency loss (GCN+Lconsistency). The comparison is conducted on three subsets of the IMDB-B dataset, categorized based on their graph density: (small), (medium), and (large). The results demonstrate the effectiveness of the consistency loss in improving classification performance across different levels of graph structural complexity. ", "section": "5.4 Study on Task Complexity"}, {"figure_path": "dg0hO4M11K/tables/tables_17_1.jpg", "caption": "Table 5: Dataset statistics.", "description": "This table presents the statistics of the datasets used in the paper's experiments.  For each dataset, it shows the task it's used for, the number of classes, the number of graphs, the average number of nodes and edges per graph, and whether node labels are available.", "section": "5.1 Experiment Setup"}, {"figure_path": "dg0hO4M11K/tables/tables_18_1.jpg", "caption": "Table 6: Average training time per epoch for different models on the ogbg-molhiv dataset, measured in seconds.", "description": "This table presents the average training time per epoch for various Graph Neural Network (GNN) models, both with and without the proposed consistency loss, on the ogbg-molhiv dataset.  The models are GCN, GIN, GraphSAGE, GTransformer, and GMT. The table showcases the computational overhead introduced by the consistency loss and allows for a comparison of training efficiency across different GNN architectures.", "section": "5.2 Effectiveness of Consistency Loss"}, {"figure_path": "dg0hO4M11K/tables/tables_18_2.jpg", "caption": "Table 7: Peak memory usage for different models on the ogbg-molhiv dataset, measured in megabytes.", "description": "This table presents the peak memory usage (in MB) during training on the ogbg-molhiv dataset for various GNN models, both with and without the proposed consistency loss.  It shows the memory consumption for each model and highlights the minimal increase in memory usage when the consistency loss is added, demonstrating the scalability of the method.", "section": "5.2 Effectiveness of Consistency Loss"}, {"figure_path": "dg0hO4M11K/tables/tables_19_1.jpg", "caption": "Table 8: Average training time per epoch on REDDIT subsets with varying class complexity, measured in seconds", "description": "This table presents the average training time per epoch for different models on subsets of the REDDIT dataset with varying numbers of classes. The subsets consist of 2, 3, 4, and 5 classes, respectively.  The table compares the training time of the baseline GCN model to the GCN model with the proposed consistency loss added.  This allows the reader to assess the impact of increased task complexity and the consistency loss on training efficiency.", "section": "5.4 Study on Task Complexity"}, {"figure_path": "dg0hO4M11K/tables/tables_19_2.jpg", "caption": "Table 9: Average training time per epoch for subsets of varying structural complexity from IMDB-B, measured in seconds.", "description": "This table shows the average training time per epoch for three subsets of the IMDB-B dataset, each with a different level of structural complexity (small, medium, large).  The training time is measured for both a standard GCN model and a GCN model enhanced with the proposed consistency loss. The table demonstrates that the added computational cost of the consistency loss remains minimal across datasets with varying structural complexity.", "section": "5.4 Study on Task Complexity"}, {"figure_path": "dg0hO4M11K/tables/tables_19_3.jpg", "caption": "Table 10: Graph classification performance with improvements of LALL and LFL over base models.", "description": "This table shows the performance improvements achieved by applying the consistency loss across all layers (LALL) and when applying it only to the first and last layers (LFL). It compares the performance gains on various datasets (NCI1, NCI109, PROTEINS, DD, IMDB-B, OGB-HIV) for different GNN models (GCN, GIN, GraphSAGE, GTransformer, GMT).  The results demonstrate that applying the loss to only the first and last layers achieves comparable performance gains to applying it to all layers, indicating that the computational cost can be reduced while maintaining effectiveness.", "section": "5.2 Effectiveness of Consistency Loss"}, {"figure_path": "dg0hO4M11K/tables/tables_20_1.jpg", "caption": "Table 11: Graph classification accuracy of GCN with contrastive learning applied across various datasets.", "description": "This table presents the graph classification accuracy results achieved using a Graph Convolutional Network (GCN) model enhanced with contrastive learning (GCN+CL) and a GCN model enhanced with the proposed consistency loss (GCN+Lconsistency). The results are shown for several benchmark datasets, including NCI1, NCI109, PROTEINS, D&D, and IMDB-B.  The table highlights the comparative performance of both methods across different datasets, illustrating the impact of the proposed consistency loss on classification accuracy.", "section": "F Similarity/Difference with Contrastive learning"}, {"figure_path": "dg0hO4M11K/tables/tables_20_2.jpg", "caption": "Table 12: Spearman correlation for graph representations from consecutive layers.", "description": "This table presents the Spearman rank correlation coefficients, measuring the consistency of graph similarity rankings across consecutive layers for different graph datasets (NCI1, NCI109, PROTEINS, D&D, IMDB-B).  The results compare the consistency using Graph Contrastive Learning (GCN+CL) and the proposed consistency loss (GCN+Lconsistency). Higher correlation indicates greater consistency in similarity relationships across layers.", "section": "5.3 Effect of the Consistency Loss on Rank Correlation"}]