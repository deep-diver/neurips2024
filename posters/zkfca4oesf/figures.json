[{"figure_path": "zkfCa4oESF/figures/figures_1_1.jpg", "caption": "Figure 1: (a) In the challenging and realistic generalized zero-shot learning (GZSL) setting, our method significantly outperforms the state-of-the-art methods on both seen and unseen classes. (b-d) Finetuning CLIP will lead to the weak generalization problem [8] on unseen classes. We tackle this problem by inheriting the topology of CLIP feature space by maintaining the pairwise angles.", "description": "This figure shows a comparison of the performance of different methods on seen and unseen classes in a generalized zero-shot learning (GZSL) setting.  Panel (a) presents a radar chart comparing the harmonic mean of seen and unseen class accuracies for several methods, highlighting the superior performance of the proposed method ('Ours'). Panels (b), (c), and (d) illustrate feature space visualizations for CLIP (before finetuning), CLIP (after finetuning), and the proposed method, respectively, showing how finetuning can negatively impact generalization to unseen classes, and how the proposed method addresses this issue by preserving the original CLIP topology.", "section": "1 Introduction"}, {"figure_path": "zkfCa4oESF/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our TPR framework. The latent space directly aligns visual and linguistic features extracted from frozen VLMs. To augment latent space for fine-grained visual-textual pattern mining, we devise a novel attribute reservoir for constructing a new attribute space. The reservoir consists of both static and learnable vocabulary tokens, enabling flexible exploration and control of feature granularity for the GZSL task. Furthermore, we propose a topology-preserving objective to keep the generalization capability of VLMs, mitigating the weak generalization problem [8].", "description": "This figure shows the architecture of the Topology-Preserving Reservoir (TPR) framework, which is the core method proposed in the paper.  It uses a dual-space feature alignment module with a novel attribute reservoir to improve visual-linguistic feature representation and a topology-preserving objective to maintain the generalization capability of pre-trained Vision-Language Models (VLMs).", "section": "Methodology"}, {"figure_path": "zkfCa4oESF/figures/figures_7_1.jpg", "caption": "Figure 3: Impact of the size of the attribute vocabulary (i.e., N1) on model performance.", "description": "This figure shows the impact of the size of the attribute vocabulary on the model's performance across three datasets (AwA2, CUB, and FLO).  The x-axis represents the attribute vocabulary size, and the y-axis represents the accuracy (S, U, and H). The results indicate that increasing the attribute vocabulary size generally improves performance. The curves for each dataset show the separate performances on seen (S), unseen (U) and harmonic mean (H) data.", "section": "4 Experiments"}, {"figure_path": "zkfCa4oESF/figures/figures_7_2.jpg", "caption": "Figure 3: Impact of the size of the attribute vocabulary (i.e., N1) on model performance.", "description": "This figure shows the effect of varying the size of the attribute vocabulary on the performance of the TPR model.  The x-axis represents the size of the vocabulary, and the y-axis represents the accuracy (S, U, and H metrics) achieved on three different datasets: AwA2, CUB, and FLO.  The plots demonstrate how changing the size of the attribute vocabulary affects the model's ability to generalize to seen and unseen classes in the generalized zero-shot learning setting.  The trend shows that increasing the vocabulary size improves performance on unseen classes, while it may not always increase performance on seen classes.", "section": "4 Experiments"}, {"figure_path": "zkfCa4oESF/figures/figures_16_1.jpg", "caption": "Figure 5: Effects of multiple textual descriptions on CUB. For each class, we randomly sample n textual descriptions 10 times.", "description": "The figure shows the impact of using multiple textual descriptions (n=1 to 5) on the performance of the model for the CUB dataset. For each class, 10 random sets of n descriptions were used, and the average accuracy for seen (S), unseen (U), and harmonic mean (H) classes is plotted with error bars.  The results indicate that increasing the number of descriptions generally improves performance, especially for unseen classes, and stabilizes the results. The most noticeable gain comes from increasing from 1 description to 2.", "section": "4 Experiments"}, {"figure_path": "zkfCa4oESF/figures/figures_16_2.jpg", "caption": "Figure 6: The cosine similarity between textual description features and corresponding attribute annotation features on AwA2 (left) and CUB (right).", "description": "This figure displays bar charts illustrating the cosine similarity between automatically generated textual descriptions and ground-truth attribute annotations.  The left chart shows the distribution for the AwA2 dataset, while the right chart shows the distribution for the CUB dataset.  Each bar represents a different class, and the height of the bar corresponds to the cosine similarity between the textual and attribute feature vectors for that class. A higher bar indicates a greater similarity between the descriptions and annotations for that class. The charts aim to demonstrate the quality of the automatically generated descriptions, showing that they capture attributes similar to those in the ground truth.", "section": "4.2 Ablation Study"}, {"figure_path": "zkfCa4oESF/figures/figures_16_3.jpg", "caption": "Figure 2: Overview of our TPR framework. The latent space directly aligns visual and linguistic features extracted from frozen VLMs. To augment latent space for fine-grained visual-textual pattern mining, we devise a novel attribute reservoir for constructing a new attribute space. The reservoir consists of both static and learnable vocabulary tokens, enabling flexible exploration and control of feature granularity for the GZSL task. Furthermore, we propose a topology-preserving objective to keep the generalization capability of VLMs, mitigating the weak generalization problem [8].", "description": "This figure shows the architecture of the proposed TPR (Topology-Preserving Reservoir) framework.  It highlights the dual-space feature alignment module which uses both a latent space and an attribute space (created from a novel attribute reservoir) to effectively align visual and linguistic features for fine-grained tasks.  The topology-preserving objective is also shown, designed to maintain the generalization ability of the pre-trained Vision-Language Model (VLM) by preserving the relationships between classes.  The attribute reservoir itself is composed of static vocabulary and learnable tokens, offering flexibility in feature granularity.", "section": "3 Methodology"}, {"figure_path": "zkfCa4oESF/figures/figures_17_1.jpg", "caption": "Figure 8: t-SNE visualization of visual representations on AwA2 and CUB. Instances of the same class are marked with the same color. Best viewed in color.", "description": "This figure visualizes the distribution of visual features in the attribute space learned by TPR and CLIP for both AwA2 and CUB datasets.  The t-SNE algorithm is used to reduce the dimensionality of the features for visualization.  Each point represents a sample, and points of the same color represent instances of the same class. The visualization demonstrates how well TPR groups samples of the same class together, compared to CLIP, indicating better feature separability and class clustering by TPR.", "section": "A Appendix"}, {"figure_path": "zkfCa4oESF/figures/figures_17_2.jpg", "caption": "Figure 8: t-SNE visualization of visual representations on AwA2 and CUB. Instances of the same class are marked with the same color. Best viewed in color.", "description": "This figure visualizes the distribution of visual features extracted by both CLIP and TPR in the attribute space using t-SNE.  It shows how well TPR clusters instances of the same class together compared to CLIP, demonstrating the effectiveness of TPR in aligning visual features and preserving the semantic topology of classes.", "section": "4.2 Ablation Study"}, {"figure_path": "zkfCa4oESF/figures/figures_18_1.jpg", "caption": "Figure 2: Overview of our TPR framework. The latent space directly aligns visual and linguistic features extracted from frozen VLMs. To augment latent space for fine-grained visual-textual pattern mining, we devise a novel attribute reservoir for constructing a new attribute space. The reservoir consists of both static and learnable vocabulary tokens, enabling flexible exploration and control of feature granularity for the GZSL task. Furthermore, we propose a topology-preserving objective to keep the generalization capability of VLMs, mitigating the weak generalization problem [8].", "description": "This figure shows the architecture of the Topology-Preserving Reservoir (TPR) framework proposed in the paper.  It illustrates how visual and linguistic features are extracted from pre-trained vision-language models (VLMs), aligned in a latent space, and further enhanced by a novel attribute reservoir to improve fine-grained feature representation. The attribute reservoir uses both static and learnable tokens, allowing for flexible control over feature granularity. A topology-preserving objective ensures that the model maintains the generalization capability of the VLMs, addressing the problem of weak generalization after fine-tuning.", "section": "3 Methodology"}, {"figure_path": "zkfCa4oESF/figures/figures_18_2.jpg", "caption": "Figure 1: (a) In the challenging and realistic generalized zero-shot learning (GZSL) setting, our method significantly outperforms the state-of-the-art methods on both seen and unseen classes. (b-d) Finetuning CLIP will lead to the weak generalization problem [8] on unseen classes. We tackle this problem by inheriting the topology of CLIP feature space by maintaining the pairwise angles.", "description": "This figure shows the performance comparison of the proposed TPR method against state-of-the-art methods in the generalized zero-shot learning setting.  (a) presents a clear visual demonstration of TPR's superior performance on both seen and unseen classes, showcasing a significant improvement over existing methods.  Subfigures (b), (c), and (d) illustrate the negative impact of fine-tuning CLIP models on unseen classes and how TPR addresses this \"weak generalization\" problem by preserving the original CLIP feature space topology.", "section": "Introduction"}, {"figure_path": "zkfCa4oESF/figures/figures_18_3.jpg", "caption": "Figure 12: Textual respondence visualization. We present the response distribution to specific text within an image by using CLIP surgery [68]. The top row denotes the query text, with each subsequent row illustrating the heatmap distribution of responses from CLIP and our proposed method, respectively. Best viewed in color.", "description": "This figure visualizes the response distribution of CLIP and TPR to specific text within images using CLIP surgery. The heatmaps show the response intensity for each query word. The top row shows the query texts, and the subsequent rows show heatmaps for CLIP and TPR, respectively. The figure demonstrates that TPR better localizes the regions of interest mentioned in the texts compared to CLIP.", "section": "4.2 Ablation Study"}]