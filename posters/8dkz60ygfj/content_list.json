[{"type": "text", "text": "Adjust Pearson\u2019s $r$ to Measure Arbitrary Monotone Dependence ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinbo Ai School of Intelligent Engineering and Automation Beijing University of Posts and Telecommunications Beijing 100876, China axb@bupt.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pearson\u2019s $r$ , the most widely-used correlation coefficient, is traditionally regarded as exclusively capturing linear dependence, leading to its discouragement in contexts involving nonlinear relationships. However, recent research challenges this notion, suggesting that Pearson\u2019s $r$ should not be ruled out a priori for measuring nonlinear monotone relationships. Pearson\u2019s $r$ is essentially a scaled covariance, rooted in the renowned Cauchy-Schwarz Inequality. Our findings reveal that different scaling bounds yield coefficients with different capture ranges, and interestingly, tighter bounds actually expand these ranges. We derive a tighter inequality than Cauchy-Schwarz Inequality, leverage it to refine Pearson\u2019s $r$ , and propose a new correlation coefficient, i.e., rearrangement correlation. This coefficient is able to capture arbitrary monotone relationships, both linear and nonlinear ones. It reverts to Pearson\u2019s $r$ in linear scenarios. Simulation experiments and real-life investigations show that the rearrangement correlation is more accurate in measuring nonlinear monotone dependence than the three classical correlation coefficients, and other recently proposed dependence measures. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Proposed in the late 19th century, Pearson\u2019s $r$ (Pearson, 1896) has been one of the main tools for scientists and engineers to study bivariate dependence during the 20th century. It is remarkably unaffected by the passage of time (Lee Rodgers and Alan Nice Wander, 1988) and still goes strong in the 21st century (Puccetti, 2022). It has been, and probably still is, the most used measure for statistical associations, and generally accepted as the measure of dependence, not only in statistics, but also in most applications of natural and social sciences (Tj\u00f8stheim, Otneim, and St\u00f8ve, 2022). ", "page_idx": 0}, {"type": "text", "text": "Despite its popularity, Pearson\u2019s $r$ has a number of shortcomings, and the most serious issue might be that it can only capture linear dependence, as stated in classical textbooks (Wasserman, 2004) and contemporary literatures (Armstrong, 2019; Tj\u00f8stheim, Otneim, and St\u00f8ve, 2022). The use of Pearson\u2019s $r$ has been strongly discouraged for forms of associations other than linear ones (Speed, 2011). ", "page_idx": 0}, {"type": "text", "text": "Numerous nonlinear alternative coefficients have been proposed to address this deficiency, such as Spearman\u2019s $\\rho$ (Spearman, 1904), Kendall\u2019s $\\tau$ (Kendall, 1938), Hilbert-Schmidt Independence Criterion(HSIC) (Gretton et al., 2005), distance correlation(dCor) (Sz\u00e9kely, Rizzo, and Bakirov, 2007), Maximal Information Coefficient(MIC) (Reshef et al., 2011), and Chatterjee\u2019s $\\xi$ (Chatterjee, 2021). Their capture ranges are extending from linear dependence to monotone dependence, and then to non-monotone dependence. Without exception, all these coefficients adopt radically different approaches for nonlinear dependence, rather than following the original way of Pearson\u2019s $r$ for a breakthrough. ", "page_idx": 0}, {"type": "text", "text": "In their recent paper titled \u201cMyths About Linear and Monotonic Associations: Pearson\u2019s r, Spearman\u2019s $\\rho$ , and Kendall\u2019s $\\tau^{\\bullet}$ , van den Heuvel and Zhan (2022) challenged the widespread belief that Pearson\u2019s $r$ is only a measure for linear dependence, proving this notion to be false. Their findings indicate that Pearson\u2019s $r$ should not be ruled out a priori for measuring nonlinear monotone dependence. Although this potential has been recognized, the specific approach to using Pearson\u2019s $r$ for accurate measurement of nonlinear monotone dependence remains unresolved. ", "page_idx": 1}, {"type": "text", "text": "Pearson\u2019s $r$ is essentially a scaled covariance, with the renowned Cauchy-Schwarz Inequality as its mathematical foundation. We find that different scaling bounds yield coefficients with different capture ranges, and interestingly, tighter bounds actually expand these ranges. We derive a tighter inequality than Cauchy-Schwarz Inequality, leverage it to adjust Pearson\u2019s $r$ to measure nonlinear monotone dependence. The adjusted version of Pearson\u2019s $r$ is more accurate in measuring nonlinear monotone dependence than the three classical correlation coefficients, and other recently proposed dependence measures. ", "page_idx": 1}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Definitions and notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider real-valued random variables $X$ and $Y$ with cdf\u2019s (cumulative distribution functions) $F$ and $G$ respectively. We denote the covariance of $X$ and $Y$ as cov $(X,Y)$ ; the variance of $X$ as var $(X)$ , and the variance of $Y$ as var $(Y)$ . We assume that $0\\,<\\,\\mathrm{var}\\left(X\\right)\\,<\\,\\infty$ , $0\\,<\\,\\mathrm{var}\\left(Y\\right)\\,<$ $\\infty$ . We define $X^{\\uparrow}\\;=\\;F^{-1}\\left(U\\right)$ , $X^{\\downarrow}\\,=\\,F^{-1}\\,(1-U)$ . Here, $U$ is a random variable with the uniform distribution on $(0,1)$ , and $F^{-1}$ is the inverse cdf or quantile function defined as $F^{-1}\\left(u\\right)=$ inf $\\{x\\in\\mathbb{R}:F\\left(x\\right)\\geqslant u\\}$ , $u\\in(0,1)$ . Similarly, $Y^{\\uparrow}=G^{-1}\\left(U\\right)$ , $Y^{\\downarrow}=G^{-1}\\left(1-U\\right)$ . ", "page_idx": 1}, {"type": "text", "text": "Let $x=(x_{1},\\cdot\\cdot\\cdot,x_{n})$ and $y=(y_{1},\\cdot\\cdot\\cdot,y_{n})$ be samples of $X$ and $Y$ , each with $n$ elements. Neither $x$ nor $y$ is constant. We denote the sample covariance of $x$ and $y$ as $s_{x,y}$ ; the sample variance of $x$ as $s_{x}^{2}$ ; the sample variance of $y$ as $s_{y}^{2}$ . We define the increasing and decreasing rearrangement of $x$ as $x^{\\uparrow}=\\left(x_{(1)},x_{(2)},\\cdot\\cdot\\cdot\\,,x_{(n)}\\right)$ and ${\\bar{x}}^{\\downarrow}=\\left(x_{(n)},x_{(n-1)},\\cdot\\cdot\\cdot{\\bf\\Phi},x_{(1)}\\right)$ respectively, with $x_{(1)}\\leqslant x_{(2)}\\leqslant$ $\\cdots\\leqslant x_{(n)}$ . Similarly, we define $y^{\\uparrow}=\\left(y_{(1)},y_{(2)},\\cdot\\cdot\\cdot\\mathbf{\\Omega},y_{(n)}\\right)$ , $y^{\\downarrow}={\\big(}y_{(n)},y_{(n-1)},\\cdot\\cdot\\cdot\\,,y_{(1)}{\\big)}$ . ", "page_idx": 1}, {"type": "text", "text": "Definition 1. A subset $S$ of $\\mathbb{R}^{2}$ is non-decreasing (resp. non-increasing) if and only if for all $(x_{1},y_{1})$ , $(x_{2},y_{2})$ in $S$ , $x_{1}<x_{2}$ implies $y_{1}\\leqslant y_{2}$ (resp. $x_{1}<x_{2}$ implies $y_{1}\\geqslant y_{2},$ ). Random variables $X$ and $Y$ are called increasing (resp. decreasing) monotone dependent i $^{f}(X,Y)$ lies almost surely in $a$ non-decreasing (resp. non-increasing) subset of $\\mathbb{R}^{2}$ . Samples $x$ and $y$ are called increasing (resp. decreasing) monotone dependent if $\\{(x,y)\\}$ is a non-decreasing (resp. non-increasing) subset of $\\mathbb{R}^{2}$ . ", "page_idx": 1}, {"type": "text", "text": "Definition 1 is sourced from (Mikusinski, Sherwood, and Taylor, 1991). Clearly Definition 1 is symmetrical with respect to $X$ and $Y$ . The monotone dependence outlined here encompasses a broader scope than definitions like the one in (Kimeldorf and Sampson, 1978), where \u201ceach of $X$ and $Y$ is almost surely a monotone function of the other\u201d. This is primarily because it doesn\u2019t necessitate a one-to-one mapping. Also, linear dependence, i.e., $P\\left(Y=\\alpha X+\\beta\\right)=1$ at the population level or $y=a x+b\\,$ at the sample level, is special case of monotone dependence, and we will refer to dependence which is monotone but not linear as \u201cnonlinear monotone dependence\u201d. ", "page_idx": 1}, {"type": "text", "text": "2.2 Different bounds lead to different capture ranges ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "With Cauchy-Schwarz Inequality, the well-known covariance inequality can be directly derived as ", "page_idx": 1}, {"type": "equation", "text": "$$\n|\\mathrm{cov}\\left(X,Y\\right)|\\leqslant{\\sqrt{\\mathrm{var}\\left(X\\right)\\mathrm{var}\\left(Y\\right)}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "thus the geometric mean of var $(X)$ and var $(Y)$ , i.e., $\\sqrt{\\mathrm{var}\\left(X\\right)\\mathrm{var}\\left(Y\\right)}$ , mathematically provides a bound for covariance cov $(X,Y)$ , which ensures that Pearson\u2019s Correlation Coefficient ", "page_idx": 1}, {"type": "equation", "text": "$$\nr\\left(X,Y\\right)=\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\sqrt{\\mathrm{var}\\left(X\\right)\\mathrm{var}\\left(Y\\right)}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "always falls into the range $[-1,+1]$ . Scaled by $\\sqrt{\\mathrm{var}\\left(X\\right)\\mathrm{var}\\left(Y\\right)}$ , Pearson\u2019s $r$ turns into a normalized covariance, which is dimensionless and bounded. It possesses significant advantage over the original covariance in the sense that its value will not be affected by the change in the units of $X$ and $Y$ . ", "page_idx": 1}, {"type": "text", "text": "A crucial issue that has been neglected so far is that boundedness doesn\u2019t ensure optimum. Scaling cov $(X,Y)$ to the range $[-1,+\\bar{1}]$ is not the only thing that matters. In fact, different bounds can be utilized to scale covariance to be bounded coefficients, as reported in previous works (Lin, 1989; Zegers, 1986). ", "page_idx": 2}, {"type": "text", "text": "For example, with the Mean Inequality Series (Hardy, Littlewood, and Polya, 1952), it is immediate that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left|\\operatorname{cov}\\left(X,Y\\right)\\right|\\leqslant{\\sqrt{\\operatorname{var}\\left(X\\right)\\operatorname{var}\\left(Y\\right)}}\\leqslant{\\frac{1}{2}}\\left(\\operatorname{var}\\left(X\\right)+\\operatorname{var}\\left(Y\\right)\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in the sense that geometric mean $\\sqrt{\\operatorname{var}\\left(X\\right)\\operatorname{var}\\left(Y\\right)}$ is always less than or equal to arithmetic mean ${\\frac{1}{2}}\\left(\\operatorname{var}\\left(X\\right)+\\operatorname{var}\\left(Y\\right)\\right)$ for nonnegative values var $(X)$ and var $(Y)$ . Then we get a looser bound for covariance, i.e., ${\\frac{1}{2}}\\left(\\operatorname{var}\\left(X\\right)+\\operatorname{var}\\left(Y\\right)\\right)$ , with which another measure can be defined as follows(Zegers, 1986): ", "page_idx": 2}, {"type": "equation", "text": "$$\nr^{+}\\left(X,Y\\right)=\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\frac{1}{2}\\left(\\mathrm{var}\\left(X\\right)+\\mathrm{var}\\left(Y\\right)\\right)}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$r^{+}\\left(X,Y\\right)$ is named early as Additivity Coefficient (Zegers, 1986) and later as Standardized Covariance (Andraszewicz and Rieskamp, 2014). It is proved that the capture range of $r^{+}\\left(X,Y\\right)$ is limited to additive relationships, i.e., $Y=\\pm X+\\beta$ , which are special cases of linear relationships, i.e., $Y=\\alpha X+\\beta\\,$ , with $\\alpha$ being fixed to $\\pm1$ (Zegers, 1986). ", "page_idx": 2}, {"type": "text", "text": "Further, we can find an even looser bound for covariance, in the sense that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left(\\operatorname{var}\\left(X\\right)+\\operatorname{var}\\left(Y\\right)\\right)\\leqslant\\frac{1}{2}\\left(\\operatorname{var}\\left(X\\right)+\\operatorname{var}\\left(Y\\right)+\\left|\\bar{X}-\\bar{Y}\\right|^{2}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and define a new measure as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nr^{=}\\left(X,Y\\right)={\\frac{\\mathrm{cov}\\left(X,Y\\right)}{{\\frac{1}{2}}\\left(\\mathrm{var}\\left(X\\right)+\\mathrm{var}\\left(Y\\right)+\\left|{\\bar{X}}-{\\bar{Y}}\\right|^{2}\\right)}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$r^{=}$ is named as Concordance Correlation Coefficient (Lin, 1989), and it is designed to measure identical relationship, i.e., $Y=\\pm X$ . When $X$ and $Y$ are both positive, it can be utilized to evaluate their agreement by measuring the variation from the $45^{\\circ}$ line through the origin (Lin, 1989). ", "page_idx": 2}, {"type": "text", "text": "As for the abovementioned three measures, $r,r^{+}$ , and $r^{=}$ , they share the same numerator, cov $(X,Y)$ , the differences lie in their denominators. These denominators serve as bounds for cov $(X,Y)$ . Different bounds lead to different capture ranges. With the bounds being looser, their capture ranges are shrinking from linear $(Y=\\alpha X+\\beta)$ towards additive $(Y=\\pm X+\\beta)$ and ultimately to identical $Y=\\pm X)$ relationships. The looser the bound, the narrower the capture range. ", "page_idx": 2}, {"type": "text", "text": "Up until now, all the efforts have only led to looser bounds and measures with narrower capture ranges. Could we possibly explore breakthroughs by approaching the problem from the opposite direction, aiming to achieve a tighter bound and consequently, devise a new measure with a broader capture range? ", "page_idx": 2}, {"type": "text", "text": "The bound in Pearson\u2019s $r$ is intrinsically provided by Cauchy-Schwarz Inequality, which is one of the most widespread and useful inequalities in mathematics. Cauchy-Schwartz Inequality is so classic and reliable that one seldom tries to improve it. Both bounds in $r^{+}$ and $r^{=}$ are looser than that provided by Cauchy-Schwartz Inequality. To loosen Cauchy-Schwartz Inequality might be easy while to tighten such a classic inequality might be relatively difficult. However, we find that it is not impossible to improve the tightness of Cauchy-Schwarz Inequality. In other words, there exists sharper bound for covariance, which will be depicted in the next section. ", "page_idx": 2}, {"type": "text", "text": "2.3 New inequality tighter than Cauchy-Schwarz Inequality ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Before deriving the new inequality, we will briefly review the classic Cauchy-Schwarz inequality, which is common in textbooks. The Cauchy\u2013Schwarz inequality states that for $x$ and $y$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left|\\langle x,y\\rangle\\right|\\leqslant\\left\\|x\\right\\|\\left\\|y\\right\\|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ is the inner product. and $\\left\\Vert\\cdot\\right\\Vert$ is the norm. The equality holds if and only if $x$ and $y$ are linearly dependent, i.e., $y=a x$ for some constant $a$ . ", "page_idx": 2}, {"type": "text", "text": "After defining an inner product on the set of random variables using the expectation of their product, i.e., $\\langle X,Y\\rangle=E X Y$ , the Cauchy\u2013Schwarz inequality becomes ", "page_idx": 3}, {"type": "equation", "text": "$$\n|E X Y|\\leqslant{\\sqrt{E X^{2}E Y^{2}}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Now, we will sharpen the Cauchy\u2013Schwarz inequality. On the basis of the rearrangement theorems (Hardy, Littlewood, and Polya, 1952), we derive 6 theorems(corollaries/propositions) as follows. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. For random variables $X$ and $Y$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|E X Y|\\leqslant\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|\\leqslant\\sqrt{E X^{2}E Y^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The equality on the left holds if and only if $X$ and $Y$ are monotone dependent, and the equality on the right holds if and only if $Y\\,{\\stackrel{d}{=}}\\,\\alpha X$ , with $\\operatorname{sgn}\\left(E X Y\\right)=\\operatorname{sgn}\\left(\\alpha\\right)\\!.$ ", "page_idx": 3}, {"type": "text", "text": "Here,= denotes equality in distribution, and $E X^{\\uparrow}Y^{\\downarrow}$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nE X^{\\uparrow}Y^{\\downarrow}=\\left\\{\\begin{array}{l}{{E X^{\\uparrow}Y^{\\uparrow},i f\\ \\ E X Y\\geqslant0}}\\\\ {{E X^{\\uparrow}Y^{\\downarrow},i f\\ \\ E X Y<0}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the sake of conciseness, the proofs of Theorem 1 and theorems undermentioned are all included in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. For samples $x$ and $y$ we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|\\langle x,y\\rangle\\right|\\leqslant\\left|\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\right|\\leqslant\\left\\|x\\right\\|\\left\\|y\\right\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The equality on the left holds if and only if x and y are monotone dependent, and the equality on the right holds if and only if y is arbitrary permutation of ax, with $\\operatorname{sgn}\\left(\\langle x,y\\rangle\\right)=\\operatorname{sgn}\\left(a\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "Here, $\\langle x^{\\uparrow},y^{\\downarrow}\\rangle$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left<x^{\\uparrow},y^{\\uparrow}\\right>=\\left\\{\\begin{array}{l l}{\\left<x^{\\uparrow},y^{\\uparrow}\\right>,i f}&{\\left<x,y\\right>\\geqslant0}\\\\ {\\left<x^{\\uparrow},y^{\\downarrow}\\right>,i f}&{\\left<x,y\\right><0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Corollary 1. For random variables $X$ and $Y$ , we have covariance equality series as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\mathrm{cov}\\left(X,Y\\right)\\right|\\leqslant\\left|\\mathrm{cov}\\left(X^{\\uparrow},Y^{\\downarrow}\\right)\\right|\\leqslant\\sqrt{\\mathrm{var}\\left(X\\right)\\mathrm{var}\\left(Y\\right)}}&{}\\\\ {\\leqslant\\frac{1}{2}\\left(\\mathrm{var}\\left(X\\right)+\\mathrm{var}\\left(Y\\right)\\right)}&{}\\\\ {\\leqslant\\frac{1}{2}\\left(\\mathrm{var}\\left(X\\right)+\\mathrm{var}\\left(Y\\right)+\\left|\\bar{X}-\\bar{Y}\\right|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first equality holds if and only if $X$ and $Y$ are monotone dependent, and the second equality holds if and only i $\\c^{\\prime}Y\\,{\\overset{d}{=}}\\,\\alpha X+\\beta$ , with $\\operatorname{sgn}\\left(\\operatorname{cov}\\left(X,Y\\right)\\right)=\\operatorname{sgn}\\left(\\alpha\\right)\\!.$ . ", "page_idx": 3}, {"type": "text", "text": "Here, cov $(X^{\\uparrow},Y^{\\updownarrow})$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nc o\\nu\\left(X^{\\uparrow},Y^{\\updownarrow}\\right)=\\left\\{\\begin{array}{c}{{c o\\nu\\left(X^{\\uparrow},Y^{\\uparrow}\\right),\\ \\ i f\\ \\ c o\\nu\\left(X,Y\\right)\\geqslant0}}\\\\ {{c o\\nu\\left(X^{\\uparrow},Y^{\\downarrow}\\right)\\ \\ i f\\ \\ c o\\nu\\left(X,Y\\right)<0}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Corollary 2. For samples $x$ and $y$ , we have covariance inequality series as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert s_{x,y}\\vert\\leqslant\\left\\vert s_{x^{\\uparrow},y^{\\downarrow}}\\right\\vert\\leqslant\\sqrt{s_{x}^{2}s_{y}^{2}}}&{}\\\\ &{\\leqslant\\frac{1}{2}\\left(s_{x}^{2}+s_{y}^{2}\\right)}\\\\ &{\\leqslant\\frac{1}{2}\\left(s_{x}^{2}+s_{y}^{2}+\\vert\\bar{x}-\\bar{y}\\vert^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The first equality holds if and only if x and $y$ are monotone dependent, and the second equality holds if and only if y is arbitrary permutation of $a x+b$ , with $\\operatorname{sgn}\\left(s_{x,y}\\right)=\\operatorname{sgn}\\left(a\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "Here, $s_{x^{\\uparrow},y^{\\downarrow}}$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{x^{\\uparrow},y^{\\updownarrow}}=\\left\\{\\begin{array}{l l}{s_{x^{\\uparrow},y^{\\uparrow}},i f\\ \\ s_{x,y}\\geqslant0}\\\\ {s_{x^{\\uparrow},y^{\\downarrow}},i f\\ \\ s_{x,y}<0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.4 The proposed Rearrangement Correlation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The inequality series in Corollary 1 and Corollary 2 provides sharper bounds for covariance at the population level and the sample level respectively. We will leverage them to define the so-called Rearrangement Correlation, which is the adjusted version of Pearson\u2019s $r$ proposed here. ", "page_idx": 4}, {"type": "text", "text": "Definition 2. The Rearrangement Correlation of random variables $X$ and $Y$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nr^{\\#}\\left(X,Y\\right)=\\frac{\\mathrm{cov}\\left(X,Y\\right)}{\\left|\\mathrm{cov}\\left(X^{\\uparrow},Y^{\\updownarrow}\\right)\\right|}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Definition 3. The Rearrangement Correlation of samples $x$ and $y$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nr^{\\#}\\left(x,y\\right)=\\frac{s_{x,y}}{\\left\\vert s_{x^{\\uparrow},y^{\\updownarrow}}\\right\\vert}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The new measure is named \u201cRearrangement Correlation\u201d because its theoretical foundation is the rearrangement inequality, as shown in Theorem 1 and Theorem 2. We adopt the musical sharp symbol # to denote rearrangement correlation, signifying that this measure has sharp values because of its sharp bounds. Analogous to how $\\mathbf{C}\\#$ is pronounced as $C_{}$ -sharp, $r^{\\#}$ is pronounced as $r$ -sharp. ", "page_idx": 4}, {"type": "text", "text": "As for the relationship between $r^{\\#}\\left(x,y\\right)$ and $r^{\\#}\\left(X,Y\\right)$ , it is clear that $r^{\\#}\\left(x,y\\right)$ converges to $r^{\\#}\\left(X,Y\\right)$ when $n\\to\\infty$ according to their definitions. ", "page_idx": 4}, {"type": "text", "text": "The capture range of rearrangement correlation is no longer limited to linear dependence but monotone dependence, which is revealed by the next proposition. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. For random variables $X,\\,Y$ , and samples $x,\\,y,$ , the following hold: ", "page_idx": 4}, {"type": "text", "text": "\u2022 $\\left|r^{\\#}\\left(X,Y\\right)\\right|\\leqslant1$ and the equality holds if and only if $X$ and $Y$ are monotone dependent.   \n\u2022 $\\left|r^{\\#}\\left(x,y\\right)\\right|\\leqslant1$ and the equality holds if and only if x and y are monotone dependent. ", "page_idx": 4}, {"type": "text", "text": "An interesting question might arise in one\u2019s mind: how can the simple adjustment, replacing $\\sqrt{\\mathrm{var}\\left(X\\right)\\mathrm{var}\\left(Y\\right)}$ with $\\left|\\operatorname{cov}\\left(X^{\\uparrow},Y^{\\updownarrow}\\right)\\right|$ , leads to the capture range expanding from linear dependence to (nonlinear) monotone dependence? The capture range is inherited from covariance itself. The capture range of covariance is limited neither to identical dependence as that of Concordance Correlation Coefficient, additive dependence as that of Additivity Coefficient, nor to linear dependence as that of Pearson\u2019s $r$ . In fact, it can potentially detect and measure arbitrary monotone dependence, if scaled properly. In other words, Pearson\u2019s $r$ is also measuring nonlinear monotone dependence to some extent. The adjustment is nothing more than compensating for underestimation. ", "page_idx": 4}, {"type": "text", "text": "The relationships among the above-mentioned Concordance Correlation Coefficient $(r^{=})$ , Additivity Coefficient $(r^{+})$ , Pearson\u2019s $r$ , and the new proposed Rearrangement Correlation $(r^{\\#})$ are depicted in Figure 1. ", "page_idx": 4}, {"type": "image", "img_path": "8Dkz60yGfj/tmp/232dc41596f0d100d2617a593d0ecc3760122e7753aeb2792eb875081e05e00e.jpg", "img_caption": ["Figure 1: Covariance inequality series, correlation coefficients and their capture ranges "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The following proposition reveals the relationship between Pearson\u2019s $r$ and its adjusted version, i.e., Rearrangement Correlation: ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. For random variables $X,\\,Y$ , and samples $x,\\,y,$ , the following hold: ", "page_idx": 5}, {"type": "text", "text": "\u2022 $\\left|r^{\\#}\\left(X,Y\\right)\\right|\\ \\geqslant\\ \\left|r\\left(X,Y\\right)\\right|$ and the equality holds if and only if $Y\\,{\\overset{d}{=}}\\,\\alpha X\\,+\\,\\beta_{!}$ , with $\\operatorname{sgn}\\left(r\\left(X,Y\\right)\\right)=\\operatorname{sgn}\\left(\\alpha\\right)$ .   \n\u2022 $\\left|r^{\\#}\\left(x,y\\right)\\right|\\geqslant\\left|r\\left(x,y\\right)\\right|$ and the equality holds if and only if y is arbitrary permutation of $a x+b$ , with $\\operatorname{sgn}\\left(r\\left(x,y\\right)\\right)=\\operatorname{sgn}\\left(a\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 2 shows that $r^{\\#}\\left(X,Y\\right)$ will revert to $r\\left(X,Y\\right)$ if and only if $Y\\,{\\overset{d}{=}}\\,\\alpha X\\,+\\,\\beta$ , $\\operatorname{sgn}\\left(r\\left(X,Y\\right)\\right)\\;=\\;\\operatorname{sgn}\\left(\\alpha\\right)$ , and $r^{\\#}\\left(x,y\\right)$ to $r\\left(x,y\\right)$ if and only if $y$ is arbitrary permutation of $a x+b$ , with $\\operatorname{sgn}\\left(r\\left(x,y\\right)\\right)\\,=\\,\\operatorname{sgn}\\left(a\\right)$ . It is clear that linear dependence is special case of these conditions. Thus, $r^{\\#}$ reverts to $r$ in linear scenarios. ", "page_idx": 5}, {"type": "text", "text": "Another question to be asked is, do we need a new monotone measure given that rank-based measures such as Spearman\u2019s $\\rho$ can already measure monotone dependence? The answer is twofold: ", "page_idx": 5}, {"type": "text", "text": "On the one hand, $r^{\\#}$ has a higher resolution and is more accurate. Without exception, all measures designed for monotone dependence are utilizing the order information. However, what we utilize here is the original information, rather than the ranks. Mapping numerical values to their ranks does of course produce a certain loss of information. A small difference between two values may no longer be distinguished from a large difference. With sample size $n$ , there are totally $\\textstyle{\\frac{n^{3}-n}{6}}$ possible $\\rho$ values between $-1$ and $+1$ , whatever raw values are and however correlated patterns differ. The resolution of Spearman\u2019s $\\rho$ might be inadequate. To take a simple example, let $x=(4,3,2,1)$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ y_{1}=(5,4,3,2.00)}\\\\ &{\\bullet\\ y_{2}=(5,4,3,3.25)}\\\\ &{\\bullet\\ y_{3}=(5,4,3,3.50)}\\\\ &{\\bullet\\ y_{4}=(5,4,3,3.75)}\\\\ &{\\bullet\\ y_{5}=(5,4,3,4.50)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Obviously, $y_{1}$ and $x$ behaves exactly in the same way, with their values getting small and small step by step. The behavior of $y_{2},\\,y_{3},\\,y_{4}$ , and $y_{5}$ are becoming more and more different from that of $x$ . However, the $\\rho$ values are all the same for $y_{2},\\,y_{3}$ and $y_{4}$ . In contrast, the $r^{\\#}$ values can reveal all these differences exactly. ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\bullet\\ r^{\\#}\\left(x,y_{1}\\right)=1.00,\\rho\\left(x,y_{1}\\right)=1.00}\\\\ &{\\bullet\\ r^{\\#}\\left(x,y_{2}\\right)=0.93,\\rho\\left(x,y_{2}\\right)=0.80}\\\\ &{\\bullet\\ r^{\\#}\\left(x,y_{3}\\right)=0.85,\\rho\\left(x,y_{3}\\right)=0.80}\\\\ &{\\bullet\\ r^{\\#}\\left(x,y_{4}\\right)=0.76,\\rho\\left(x,y_{4}\\right)=0.80}\\\\ &{\\bullet\\ r^{\\#}\\left(x,y_{5}\\right)=0.38,\\rho\\left(x,y_{5}\\right)=0.40}\\end{array}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "On the other hand, $r^{\\#}$ is comparable with Pearson\u2019s $r$ , while the latter is not. For nonlinear monotone dependence, the value of Spearman\u2019s $\\rho$ might be remarkably greater than the value of Pearson\u2019s $r$ . One may attempt to search for nonlinear relationships in data by checking whether the value of $\\rho$ far exceeds that of $r$ . However, it might be meaningless and even impossible to compare their values directly. In cases, $\\rho$ can be either greater or less than $r$ , and their sign can even be different. Thus the difference $|\\rho|-|r|$ is confusing. On the contrary, the signs of $r^{\\#}$ and $r$ are always the same, and $\\left|r^{\\#}\\right|$ is always greater than or equal to $|r|$ $|.\\ |r^{\\#}|-|r|$ equals to 0 if and only if $y$ is arbitrary permutation of $a x+b$ . Its value increases with the degree of nonlinearity. ", "page_idx": 5}, {"type": "text", "text": "However, Spearman\u2019s $\\rho$ can also be superior to $r^{\\#}$ in the sense that the former is robust to outliers while the latter is not. Rearrangement correlation is a scaled covariance, and the limitation of being non-robust to outliers is inherited from covariance itself. In fact, concordance correlation coefficient, additivity coefficient, and Pearson\u2019s $r$ are also scaled covariance measures, and none of them are robust to outliers. ", "page_idx": 5}, {"type": "text", "text": "To be more robust, we can also transform the raw data into their ranks before calculating $r^{\\#}$ . Interestingly, $r^{\\#}$ becomes equivalent to Spearman\u2019s $\\rho$ when calculated on ranks. Let $P$ and $Q$ be the ranks of $x$ and $y$ respectively. Then, in the sense that $\\begin{array}{r}{s d\\left(P,Q\\right)\\,=\\,s d\\left(P^{\\uparrow},Q^{\\updownarrow}\\right)\\,=\\,\\frac{n\\left(n+1\\right)}{12}}\\end{array}$ $r^{\\#}\\left(P,Q\\right)=\\rho\\left(P,Q\\right)$ . This explains why $\\rho$ can measure nonlinear monotone relationships while $r$ only measures linear ones, despite them sharing a similar formula. The key is not just the ranking but achieving a sharp bound. Since $\\rho$ and $r^{\\#}$ are equivalent when applied to ranks, and $r^{\\#}$ can measure arbitrary monotone dependence (as proven in our manuscript), $\\rho$ can do the same. ", "page_idx": 6}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.1 Performance metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The main purpose of proposing Rearrangement Correlation is to provide a measure of dependence strength for nonlinear monotone relationships, rather than to simply serve as a test statistic for testing independence. Thus, our performance metrics focus on strength measurement. ", "page_idx": 6}, {"type": "text", "text": "The basic question to be asked when measuring any attribute is how accurate is this measurement, and there should be no exception for dependence measurement. ISO 5725 uses two terms \u201ctrueness\u201d and \u201cprecision\u201d to describe the accuracy of a measurement method. Trueness refers to the closeness of agreement between the mean or median of measured results and the true or accepted reference value. Precision refers to the closeness of agreement between measured values (ISO, 1994). The comprehensive performance of trueness and precision can be represented as the mean absolute error $M A E$ for short), and calculated as the mean of the absolute values of the difference between the measured value and the conventional true value. On the whole, a measure with lower MAE value is better. ", "page_idx": 6}, {"type": "text", "text": "We evaluate the performance of different measures in a supervised way. We employ the coefficient of determination, $R^{2}$ , which is defined as the proportion of variance for one variable explained by the other variable, as the ground truth of strength of dependence, which is common in practices (Reshef et al., 2011). Further, we take its square root, $R$ , as the conventional true value of the relationship strength. A simple evidence is that Pearson\u2019s $r$ , as the golden standard for measuring linear dependence, is equivalent to $R$ , as long as the relationship is linear. Thus, it is reasonable to adopt $R$ as the reference value. ", "page_idx": 6}, {"type": "text", "text": "3.2 Simulation procedure ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We investigate the accuracy performance of $r^{\\#}$ , along with $r^{+}$ , Pearson\u2019s $r$ , Spearman\u2019s $\\rho$ , Kendall\u2019s $\\tau$ and four other leading dependence measures, i.e., HSIC, dCor, MIC and Chatterjee\u2019s $\\xi$ in the following way: for each scenario $y=f\\left(x\\right)$ , we generated 512 pairs of $(x,y)$ from the regression model $y=f\\left(x\\right)+\\varepsilon$ , and computed the values of different measures between $x$ and $y$ at different $R$ levels. In the regression model, the $x$ sample is uniformly distributed on the unit interval $(0,1)$ , and the noise is normally distributed as $\\varepsilon\\sim N\\left(0,\\sigma\\right)$ , with $\\sigma$ controlling $R$ to a certain level. ", "page_idx": 6}, {"type": "equation", "text": "$$\nR={\\sqrt{1-{\\frac{\\sigma^{2}}{\\operatorname{var}\\left(Y\\right)}}}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the sake of robustness, the computation process is repeated 10 times for each measure at each $R$ level, and the mean value is adopted. ", "page_idx": 6}, {"type": "text", "text": "Simulation procedure is implemented in the recor $\\mathbf{R}$ package, which is available in supplemental materials. The workflow is to call accuracy_ $d b()$ firstly, accuracy_results_frm_ $d b()$ secondly and accuracy_plot_lite() finally. To reproduce the results, just keep all the parameters as default. More details are available in the package help files. ", "page_idx": 6}, {"type": "text", "text": "3.3 Performance in simulated scenarios ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The simulation is conducted in up to 50 types of different monotone scenarios, including all basic elementary functions, lots of composite functions and several special functions. To our knowledge, our research explores the most extensive and representative range of scenarios. Detailed descriptions of these scenarios can be found in Appendix A.3, and the results are shown in Figure 2. ", "page_idx": 6}, {"type": "image", "img_path": "8Dkz60yGfj/tmp/07db0b6687e2471e84e942add3cb6c9566eeb8bf677bf7c754056935becb9cf2.jpg", "img_caption": ["Figure 2: Performance of different measures in 50 simulated scenarios "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2 shows the scatter plots of conventional true value versus measured values in different scenarios. The nine investigated measures are located in nine panels respectively. In each panel, there are totally 50 transparent green lines, representing the measured value $Y$ -axis) with respect to conventional true value $X$ -axis) for 50 scenarios. The prefixes $I-$ and $H\\cdot$ - refer to Inverse and Hyperbolic respectively. For example, $I-H\\cdot$ -Tangent stands for Inverse Hyperbolic Tangent function. In all these panels, the dashed diagonal lines represent an ideal measure, the score of which is exactly the same as the conventional true value. Apparently, the closeness to this reference line reflects the performance of a measure for a certain scenario. The median values of each measure among scenarios at different $R$ levels are also calculated and denoted by the non-transparent red line. ", "page_idx": 7}, {"type": "text", "text": "We first look at the extreme values on both sides. It is expected that a measure will score nearly zero when $X$ and $Y$ are randomly generated, i.e., $R\\approx0$ , and score one when there is perfect monotone relationship, i.e., $R=1$ . Only Spearman\u2019s $\\rho$ , Kendall\u2019s $\\tau$ and the adjusted $r^{\\#}$ meet this requirement. MIC also scores $+1$ when $R=1$ , however, it tends to overestimate the strength when $R$ is near zero, as also reported in other literature before (Chatterjee, 2021). The remaining five measures, i.e., $r^{+}$ , $r$ , HSIC, dCor and $\\xi$ , underestimate the strength of nonlinear relationships, and never converge to $+1$ even when $R$ approaches to 1. ", "page_idx": 7}, {"type": "text", "text": "Now let\u2019s take a close look at the intermediate values. It can be seen in Figure 2 that the nontransparent red line of $r^{\\#}$ is the closest one to the dashed line, which means the measured values by $r^{\\#}$ possess the minimum error. To further quantify the accuracy, we add four boxplots at four representative $R$ levels (approximately, 0.25, 0.50, 0.75, and 1.00) for each measure. $r^{\\#}$ has the highest trueness in all these representative levels. As for precision, the $r^{\\#}$ also outperforms all other measures except HSIC and MIC. Although HSIC and MIC possesses the best precision, they suffer from lower trueness. HSIC tends to underestimate the strength severely, and MIC is also a biased measure, tending to overestimate the strength when the signal is weak, and underestimate it when the signal is strong, as shown in Figure 2. ", "page_idx": 7}, {"type": "text", "text": "The overall performance in terms of MAE is ordered as $r^{\\#}\\left(0.060\\right)\\succ\\rho\\left(0.102\\right)\\succ d C o r\\left(0.127\\right)\\succ$ $r\\left(0.150\\right)\\succ\\tau\\left(0.157\\right)\\succ M I C\\left(0.196\\right)\\succ\\xi\\left(0.206\\right)\\succ r^{+}\\left(0.263\\right)\\succ H S I C\\left(0.518\\right)\\cdot r^{\\#}\\left[\\xi\\left(0.196\\right)\\succ\\xi\\left(0.1,0.127\\right)\\right]$ ossesses significant accuracy advantage over all other measures. ", "page_idx": 7}, {"type": "text", "text": "3.4 Performance in real-life scenarios ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In addition to simulated scenarios, we also investigate the performance of these measures on real life scenarios provided by NIST (National Institute of Standards and Technology, 2003). There are five available monotone scenarios: Chwirut1, Hahn1, Rat43, Roszman1, and Thurber. Details for these scenarios are available in Appendix A.4. ", "page_idx": 7}, {"type": "text", "text": "The performance of nine measures in these five scenarios are depicted in Figure 3. Bar plots illustrate the measured values, the conventional true value verified by NIST is annotated on the top of each scenario group. And the differences between the measured value and the true value are mapped as error bars. ", "page_idx": 7}, {"type": "image", "img_path": "8Dkz60yGfj/tmp/52db08a6fb2c489074e9646bcad683b8b5905221ed88d89d8f0bd8ff64975d05.jpg", "img_caption": ["Figure 3: Performance of Different Measures in 5 Real-life Scenarios "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "It can be seen from Figure 3 that $r^{\\#}$ possesses minimum error, or best accuracy performance among all these five scenarios, with its MAE value as 0.00141, followed by $\\rho(0.0159)$ , MIC(0.0249), $\\mathrm{d}\\mathrm{Cor}(0.0575)$ , $\\tau(0.0779)$ , $r(0.0916)$ , $\\xi(0.166)$ , HSIC(0.891) and $r^{+}(0.9\\dot{5}6)$ . ", "page_idx": 8}, {"type": "text", "text": "3.5 Performance in non-monotone scenarios ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "It\u2019s worth noting that the aforementioned scenarios only cover monotone cases. To evaluate performance in typical non-monotone contexts, we conducted experiments in 16 scenarios, encompassing those outlined in (Reshef et al., 2011) and (Simon and Tibshirani, 2014). Details for these scenarios are available in Appendix A.5. ", "page_idx": 8}, {"type": "text", "text": "As anticipated, the MAE value of $r^{\\#}$ reaches a significant 0.418, notably inferior to those of $\\xi(0.141)$ , MIC (0.157) and dCor (0.364). In essence, $r^{\\#}$ struggles to accurately measure non-monotone dependence. This limitation stems from its reliance on covariance, which inherently fails to detect non-monotone relationships. To illustrate, consider a standard introductory text book example, i.e., cov $(X,Y)=0$ despite $Y$ being totally dependent on $X$ via $Y=X^{2}$ . Attempts to tighten its bound proves futile. ", "page_idx": 8}, {"type": "text", "text": "However, the performance of $r^{\\#}$ is also superior to those of Spearman\u2019s $\\rho(0.431)$ , Pearson\u2019s $r(0.437)$ and Kendall\u2019s $\\tau(0.461)$ . As for accuracy performance, $r^{\\#}$ outperforms the three classical correlation coefficients in not only monotone, but also non-monotone scenarios. ", "page_idx": 8}, {"type": "text", "text": "4 Conclusion and discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We proposed here an adjusted version of Pearson\u2019s $r$ , i.e., rearrangement correlation, which can be treated as counterpart of Pearson\u2019s $r$ for nonlinear monotone dependence. ", "page_idx": 8}, {"type": "text", "text": "The basic idea of rearrangement correlation is simple and straightforward. Its mathematical foundation is a sharpened version of the famous Cauchy-Schwarz Inequality. Tighter bound leads to wider capture range. With the adjustment, the capture range of Pearson\u2019s $r$ is extended from linear dependence to (nonlinear) monotone dependence. Simulated and real-life investigations demonstrate that the rearrangement correlation is more accurate in measuring nonlinear monotone dependence than the three classical correlation coefficients and other more recently proposed dependence measures. ", "page_idx": 8}, {"type": "text", "text": "We may draw the conclusion that: Pearson\u2019s $r$ is undoubtedly the gold measure for linear dependence.   \nNow, it might be the gold measure also for nonlinear monotone dependence, if adjusted. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Andraszewicz, Sandra and J\u00f6rg Rieskamp (2014). \u201cStandardized Covariance\u2014A Measure of Association, Similarity and Co-riskiness between Choice Options\u201d. In: Journal of Mathematical Psychology 61, pp. 25\u201337.   \nArmstrong, Richard A (2019). \u201cShould Pearson\u2019s Correlation Coefficient Be Avoided?\u201d In: Ophthalmic and Physiological Optics 39.5, pp. 316\u2013327.   \nChatterjee, Sourav (2021). \u201cA New Coefficient of Correlation\u201d. In: Journal of the American Statistical Association 116.536, pp. 2009\u20132022.   \nGretton, Arthur et al. (2005). \u201cMeasuring Statistical Dependence with Hilbert-Schmidt Norms\u201d. In: Algorithmic Learning Theory. Ed. by David Hutchison et al. Vol. 3734. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 63\u201377.   \nHardy, G.H., J.E. Littlewood, and G. Polya (1952). Inequalities. London: Cambridage University Press.   \nISO (1994). Accuracy (Trueness and Precision) of Measurement Methods and Results \u2014 Part 1: General Principles and Definitions. International Standard.   \nKendall, M. G. (1938). \u201cA New Measure of Rank Correlation\u201d. In: Biometrika $30.1/2$ , pp. 81\u201393.   \nKimeldorf, George and Allan R. Sampson (July 1978). \u201cMonotone Dependence\u201d. In: The Annals of Statistics 6.4. (Visited on 09/21/2022).   \nLee Rodgers, Joseph and W. Alan Nice Wander (1988). \u201cThirteen Ways to Look at the Correlation Coefficient\u201d. In: American Statistician 42.1, pp. 59\u201366.   \nLin, Lawrence I-Kuei (1989). \u201cA Concordance Correlation Coefficient to Evaluate Reproducibility\u201d. In: Biometrics 45.1, pp. 255\u2013268.   \nMikusinski, Sherwood, and Taylor (1991). \u201cThe Fr\u00e9chet Bounds Revisted\u201d. In: Real Analysis Exchange 17.2, p. 759.   \nNational Institute of Standards and Technology (2003). NIST Standard Reference Database 140 - Statistical Reference Datasets. Tech. rep.   \nPearson, Karl (1896). \u201cMathematical Contributions to the Theory of Evolution, III. Regression, Heredity, and Panmixia\u201d. In: Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 187, pp. 253\u2013318.   \nPuccetti, Giovanni (2022). \u201cMeasuring Linear Correlation between Random Vectors\u201d. In: Information Sciences 607, pp. 1328\u20131347.   \nR Core Team (2024). R: A Language and Environment for Statistical Computing. Manual. Vienna, Austria.   \nReshef, David N. et al. (2011). \u201cDetecting Novel Associations in Large Data Sets\u201d. In: Science 334.6062, pp. 1518\u20131524.   \nSimon, Noah and Robert Tibshirani (2014). Comment on \"Detecting Novel Associations In Large Data Sets\" by Reshef Et Al, Science Dec 16, 2011. arXiv: 1401.7645.   \nSpearman, C. (1904). \u201cThe Proof and Measurement of Association between Two Things\u201d. In: The American Journal of Psychology 15.1, pp. 72\u2013101.   \nSpeed, T. (2011). \u201cA Correlation for the 21st Century\u201d. In: Science 334.6062, pp. 1502\u20131503.   \nSz\u00e9kely, G\u00e1bor J., Maria L. Rizzo, and Nail K. Bakirov (2007). \u201cMeasuring and Testing Dependence by Correlation of Distances\u201d. In: Annals of Statistics 35.6, pp. 2769\u20132794.   \nTj\u00f8stheim, Dag, H\u00e5kon Otneim, and B\u00e5rd St\u00f8ve (2022). \u201cStatistical Dependence: Beyond Pearson\u2019s \u03c1\u201d. In: Statistical Science 37.1, pp. 90\u2013109.   \nvan den Heuvel, Edwin and Zhuozhao Zhan (2022). \u201cMyths About Linear and Monotonic Associations: Pearson\u2019s r, Spearman\u2019s $\\rho$ , and Kendall\u2019s \u03c4\u201d. In: The American Statistician 76.1, pp. 44\u2013 52.   \nWasserman, Larry (2004). All of Statistics: A Concise Course in Statistical Inference. New York: Springer.   \nWhitt, Ward (1976). \u201cBivariate Distributions with Given Marginals\u201d. In: The Annals of Statistics 4.6, pp. 1280\u20131289.   \nZegers, Frits E. (1986). \u201cA Family of Chance-corrected Association Coefficients for Metric Scales\u201d. In: Psychometrika 51.4, pp. 559\u2013562. ", "page_idx": 9}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A.1 Proofs of theorems, corollaries and propositions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Theorem 1. For random variables $X$ and $Y$ , we have ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|E X Y|\\leqslant\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|\\leqslant\\sqrt{E X^{2}E Y^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "The equality on the left holds if and only if $X$ and $Y$ are monotone dependent, and the equality on the right holds if and only if $Y\\,{\\stackrel{d}{=}}\\,\\alpha X$ , with $\\operatorname{sgn}\\left(E X Y\\right)=\\operatorname{sgn}\\left(\\alpha\\right)\\!.$ . ", "page_idx": 10}, {"type": "text", "text": "Here, $\\underline{{\\underline{{d}}}}$ denotes equality in distribution, and $E X^{\\uparrow}Y^{\\downarrow}$ is defined as: ", "page_idx": 10}, {"type": "equation", "text": "$$\nE X^{\\uparrow}Y^{\\downarrow}=\\left\\{\\begin{array}{l}{{E X^{\\uparrow}Y^{\\uparrow},i f\\ \\ E X Y\\geqslant0}}\\\\ {{E X^{\\uparrow}Y^{\\downarrow},i f\\ \\ E X Y<0}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Proof. The proof will be completed in two parts. ", "page_idx": 10}, {"type": "text", "text": "\u2022 The proof of $\\vert E X Y\\vert\\,\\leqslant\\,\\left\\vert E X^{\\uparrow}Y^{\\downarrow}\\right\\vert$ is mainly based on the rearrangement theorem for functions, i.e., Theorem 378 on page 278 of (Hardy, Littlewood, and Polya, 1952): Let $f^{\\uparrow}$ , $g^{\\uparrow}$ denote increasing rearrangements and $f^{\\downarrow},g^{\\downarrow}$ decreasing rearrangements of $f$ and $g$ on $[0,1]$ as defined on page 276 of (Hardy, Littlewood, and Polya, 1952). Then we have ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\int_{0}^{1}f^{\\dag}\\left(u\\right)g^{\\dag}\\left(u\\right)d u\\leqslant\\int_{0}^{1}f\\left(u\\right)g\\left(u\\right)d u\\leqslant\\int_{0}^{1}f^{\\dag}\\left(u\\right)g^{\\dag}\\left(u\\right)d u.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Let $\\Pi\\left(F,G\\right)$ be the set of all joint cdf\u2019s on $\\mathbb{R}^{2}$ having $F$ and $G$ as marginal cdf\u2019s. For arbi trary cdf $H\\in\\Pi$ there exists $(X,Y)$ : $[0,1]\\to\\mathbb{R}^{2}$ such that $[X\\left(U\\right),Y\\left(U\\right)]$ has cdf $H$ . We can let $f\\left(u\\right)\\;=\\;X\\left(u\\right)$ and $g\\left(u\\right)\\;=\\;Y\\left(u\\right)$ so that $\\begin{array}{r}{E X Y\\;=\\;\\int_{0}^{1}f\\left(u\\right)g\\left(u\\right)\\!d u}\\end{array}$ . The increasing and decreasing rearrangements of $f$ and $g$ are just $f^{\\uparrow}\\left(u\\right)\\;=\\;F^{-1}\\left(u\\right)$ , $f^{\\downarrow}\\left(u\\right)=F^{-1}\\left(1-u\\right),g^{\\uparrow}\\left(u\\right)=G^{-1}\\left(u\\right).$ , and $g^{\\downarrow}\\left(u\\right)=G^{-1}\\left(1-u\\right)$ , as stated in (Whitt, 1976). Thus, we have ", "page_idx": 10}, {"type": "equation", "text": "$$\nE X^{\\uparrow}Y^{\\downarrow}\\leqslant E X Y\\leqslant E X^{\\uparrow}Y^{\\uparrow}.\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "The right-hand (resp. left-hand) equality holds if and only if $(X,Y)\\overset{d}{=}\\left(F^{-1}\\left(U\\right),G^{-1}\\left(U\\right)\\right)$ (resp. $\\left({X,Y}\\right)\\overset{d}{=}\\left({F\\sp{-1}\\left({U}\\right),G\\sp{-1}\\left({1-U}\\right)}\\right)$ . The equality conditions can be equivalently expressed as $X$ and $Y$ are increasing (resp. decreasing) monotone dependent (Mikusinski, Sherwood, and Taylor, 1991). ", "page_idx": 10}, {"type": "text", "text": "If $E X Y\\geqslant0$ , we have $E X^{\\uparrow}Y^{\\downarrow}=E X^{\\uparrow}Y^{\\uparrow}\\geqslant E X Y\\geqslant0$ , which implies that $\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|=$ $E X^{\\uparrow}Y^{\\uparrow}\\geqslant E X Y=|E X Y|$ , and the equality holds if and only if $X$ and $Y$ are increasing monotone dependent. If $E X Y<0$ , we have $E X^{\\uparrow}Y^{\\downarrow}=E X^{\\uparrow}Y^{\\downarrow}\\leqslant E X Y<0$ , which implies that $\\left|E X^{\\uparrow}Y^{\\updownarrow}\\right|=-E X^{\\uparrow}Y^{\\downarrow}\\geqslant-E X Y=\\left|E X Y\\right|$ , and the equality holds if and only if $X$ and $Y$ are decreasing monotone dependent. Either way, we have $\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|\\geqslant$ $|E X Y|$ , and the equality holds if and only if $X$ and $Y$ are monotone dependent. ", "page_idx": 10}, {"type": "text", "text": "\u2022 The proof of $\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|\\leq\\sqrt{E X^{2}E Y^{2}}$ is mainly based on Cauchy-Schwarz Inequality: If $E X Y\\geqslant0$ , $\\left|E X^{\\uparrow}Y^{\\updownarrow}\\right|=\\left|E X^{\\uparrow}Y^{\\uparrow}\\right|\\leqslant{\\sqrt{E(X^{\\uparrow})^{2}E(Y^{\\uparrow})^{2}}}={\\sqrt{E X^{2}E Y^{2}}}$ in the sense that $X^{\\uparrow}\\,{\\overset{d}{=}}\\,X$ , and $Y^{\\uparrow}\\,{\\frac{d}{=}}\\,Y$ . And the equality holds if and only if $Y^{\\uparrow}=\\alpha X^{\\uparrow}$ , equivalently, $Y\\,{\\stackrel{d}{=}}\\,\\alpha X$ , for some constant $\\alpha\\geq0$ . Similarly, If $E\\left(X Y\\right)<0$ , $\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|=\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|\\leqslant$ $\\sqrt{E(X^{\\uparrow})^{2}E(Y^{\\downarrow})^{2}}=\\sqrt{E X^{2}E Y^{2}}$ in the sense that $X^{\\uparrow}\\,{\\overset{d}{=}}\\,X$ , and $Y^{\\downarrow}\\,{\\frac{d}{=}}\\,Y$ . And the equality holds if and only if $Y^{\\downarrow}=\\alpha X^{\\uparrow}$ , equivalently, $Y\\,{\\stackrel{d}{=}}\\,\\alpha X$ , for some constant $\\alpha<0$ . Either way, we have $\\left|E X^{\\uparrow}Y^{\\downarrow}\\right|\\leqslant{\\sqrt{E X^{2}E Y^{2}}}$ and the equality holds if and only if Y= \u03b1X, with $\\operatorname{sgn}\\left(E X Y\\right){\\dot{=}}\\\\operatorname{sgn}\\left(\\alpha\\right)$ . ", "page_idx": 10}, {"type": "text", "text": "Theorem 2. For samples $x$ and y we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left|\\langle x,y\\rangle\\right|\\leqslant\\left|\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\right|\\leqslant\\left\\|x\\right\\|\\left\\|y\\right\\|.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The equality on the left holds if and only if $x$ and y are monotone dependent, and the equality on the right holds if and only if $y$ is arbitrary permutation of ax, with $\\operatorname{sgn}\\left(\\left\\langle x,y\\right\\rangle\\right)=\\operatorname{sgn}\\left(a\\right)$ . ", "page_idx": 11}, {"type": "text", "text": "Here, $\\langle x^{\\uparrow},y^{\\downarrow}\\rangle$ is defined as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left<x^{\\uparrow},y^{\\uparrow}\\right>=\\left\\{\\begin{array}{l l}{\\left<x^{\\uparrow},y^{\\uparrow}\\right>,i f}&{\\left<x,y\\right>\\geqslant0}\\\\ {\\left<x^{\\uparrow},y^{\\downarrow}\\right>,i f}&{\\left<x,y\\right><0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. The proof will also be completed in two parts. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The proof of $|\\langle x,y\\rangle|\\leqslant\\left|\\langle x^{\\uparrow},y^{\\downarrow}\\rangle\\right|$ is mainly based on another rearrangement theorem for finite sets, i.e., Theorem 368 on page 261 of (Hardy, Littlewood, and Polya, 1952): With $x^{\\uparrow}=$ $\\left(x_{(1)},x_{(2)},\\cdots\\,,x_{(n)}\\right)$ , $y^{\\uparrow}=\\bar{\\left(y_{(1)},y_{(2)},\\cdot\\cdot\\cdot\\right.},y_{(n)}^{\\;\\;\\;})$ , and $y^{\\downarrow}={\\big(}y_{(n)},{\\dot{y}}_{(n-1)},\\cdot\\cdot\\cdot\\,,y_{(1)}{\\big)}$ , we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}x_{(i)}y_{(n-i+1)}\\leqslant\\sum_{i=1}^{n}x_{i}y_{i}\\leqslant\\sum_{i=1}^{n}x_{(i)}y_{(i)}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "That is, ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\leqslant\\left\\langle x,y\\right\\rangle\\leqslant\\left\\langle x^{\\uparrow},y^{\\uparrow}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and the right-hand (resp. left-hand) equality holds if and only if $x$ and $y$ are similarly(resp. oppositely) ordered. According to the definitions of \u201csimilarly(resp. oppositely) ordered\u201d on page 43 in (Hardy, Littlewood, and Polya, 1952), the equality conditions can be equivalently expressed as $x$ and $y$ are increasing(resp. decreasing) monotone dependent. ", "page_idx": 11}, {"type": "text", "text": "If $\\langle x,y\\rangle\\geqslant0$ , we have $\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle=\\left\\langle x^{\\uparrow},y^{\\uparrow}\\right\\rangle\\geqslant\\left\\langle x,y\\right\\rangle\\geqslant0$ , which implies $\\left|\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\right|\\;=$ $\\left\\langle x^{\\uparrow},y^{\\uparrow}\\right\\rangle\\geqslant\\left\\langle x,y\\right\\rangle=\\left|\\left\\langle x,y\\right\\rangle\\right|$ , and the equality holds if and only if $x$ and $y$ are increasing monotone dependent.If $\\langle x,y\\rangle\\,<\\,0$ , we have $\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\,=\\,\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\,\\leqslant\\,\\langle x,y\\rangle\\,<\\,0$ , which implies $\\left|\\left\\langle{x^{\\uparrow},y^{\\downarrow}}\\right\\rangle\\right|=-\\left\\langle{x^{\\uparrow},y^{\\downarrow}}\\right\\rangle\\geqslant-\\left\\langle{x,y}\\right\\rangle=|\\left\\langle{x,y}\\right\\rangle|$ , and the equality holds if and only if $x$ and $y$ are decreasing monotone dependent. Either way, we have $\\left|\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\right|\\geqslant\\left|\\left\\langle x,y\\right\\rangle\\right|$ and the equality holds if and only if $x$ and $y$ are monotone dependent. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The proof of $\\left|\\langle x^{\\uparrow},y^{\\downarrow}\\rangle\\right|\\leqslant\\|x\\|\\,\\|y\\|$ is mainly based on Cauchy-Schwarz Inequality: in the sense that norm $\\lVert\\cdot\\rVert$ is permutation invariant, we have $\\left\\|x^{\\uparrow}\\right\\|=\\left\\|x\\right\\|$ and $\\|y^{\\uparrow}\\|=\\|y^{\\downarrow}\\|=\\|y\\|$ . If $\\langle x,y\\rangle\\geqslant0$ , we have $\\left|\\left\\langle x^{\\uparrow},y^{\\uparrow}\\right\\rangle\\right|=\\left|\\left\\langle x^{\\uparrow},y^{\\uparrow}\\right\\rangle\\right|\\leqslant\\left\\|x^{\\uparrow}\\right\\|\\left\\|y^{\\uparrow}\\right\\|=\\left\\|x\\right\\|\\left\\|y\\right\\|$ , and the equality holds if and only if $y^{\\uparrow}=a x^{\\uparrow}$ , or equivalently, $y\\textsubscript{j}$ is arbitrary permutation of $a x$ for some constant $a\\geq0$ . If $\\langle x,y\\rangle<0$ , we have $\\left|\\left\\langle x^{\\uparrow},y^{\\uparrow}\\right\\rangle\\right|=\\left|\\left\\langle x^{\\uparrow},y^{\\downarrow}\\right\\rangle\\right|\\leqslant\\left\\|x^{\\uparrow}\\right\\|\\left\\|y^{\\downarrow}\\right\\|=\\left\\|x\\right\\|\\left\\|y\\right\\|$ , and the equality holds if and only if $y^{\\downarrow}=a x^{\\uparrow}$ , or equivalently, $y$ is arbitrary permutation of $a x$ for some constant $a<0$ . Either way, we have $\\left|\\langle x^{\\uparrow},y^{\\downarrow}\\rangle\\right|\\leqslant\\|x\\|\\,\\|y\\|$ , and the equality holds if and only if $y$ is arbitrary permutation of $a x$ , , with $\\operatorname{sgn}\\left(\\left\\langle x,y\\right\\rangle\\right)=\\operatorname{sgn}\\left(a\\right)$ . ", "page_idx": 11}, {"type": "text", "text": "Corollary 1. For random variables $X$ and $Y$ , we have covariance equality series as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\mathrm{cov}\\left(X,Y\\right)\\right|\\leqslant\\left|\\mathrm{cov}\\left(X^{\\uparrow},Y^{\\downarrow}\\right)\\right|\\leqslant\\sqrt{\\mathrm{var}\\left(X\\right)\\mathrm{var}\\left(Y\\right)}}&{}\\\\ {\\leqslant\\frac{1}{2}\\left(\\mathrm{var}\\left(X\\right)+\\mathrm{var}\\left(Y\\right)\\right)}&{}\\\\ {\\leqslant\\frac{1}{2}\\left(\\mathrm{var}\\left(X\\right)+\\mathrm{var}\\left(Y\\right)+\\left|\\bar{X}-\\bar{Y}\\right|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The first equality holds if and only if $X$ and $Y$ are monotone dependent, and the second equality holds if and only i $f Y\\,{\\overset{d}{=}}\\,\\alpha X+\\beta$ , with $\\operatorname{sgn}\\left(\\operatorname{cov}\\left(X,Y\\right)\\right)=\\operatorname{sgn}\\left(\\alpha\\right)\\!.$ . ", "page_idx": 11}, {"type": "text", "text": "Here, cov $(X^{\\uparrow},Y^{\\updownarrow})$ is defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\nc o\\nu\\left(X^{\\uparrow},Y^{\\updownarrow}\\right):=\\left\\{\\begin{array}{l l}{c o\\nu\\left(X^{\\uparrow},Y^{\\uparrow}\\right),\\ \\ i f\\ \\ c o\\nu\\left(X,Y\\right)\\geqslant0}\\\\ {c o\\nu\\left(X^{\\uparrow},Y^{\\downarrow}\\right)\\ \\ i f\\ \\ c o\\nu\\left(X,Y\\right)<0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. $\\left|\\operatorname{cov}\\left(X,Y\\right)\\right|\\leqslant\\left|\\operatorname{cov}\\left(X^{\\uparrow},Y^{\\updownarrow}\\right)\\right|\\leqslant{\\sqrt{\\operatorname{var}\\left(X\\right)\\operatorname{var}\\left(Y\\right)}},$ , and the equality conditions are immediate from Theorem 1. The remaining parts of the inequality series are obvious. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Corollary 2. For samples $x$ and $y$ , we have covariance inequality series as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|s_{x,y}|\\leqslant\\left|s_{x^{\\uparrow},y^{\\downarrow}}\\right|\\leqslant\\sqrt{s_{x}^{2}s_{y}^{2}}}}\\\\ &{\\leqslant\\frac{1}{2}\\left(s_{x}^{2}+s_{y}^{2}\\right)}\\\\ &{\\leqslant\\frac{1}{2}\\left(s_{x}^{2}+s_{y}^{2}+|\\bar{x}-\\bar{y}|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The first equality holds if and only if $x$ and y are monotone dependent, and the second equality holds if and only if y is arbitrary permutation of $a x+b$ , with $\\operatorname{sgn}\\left(s_{x,y}\\right)=\\operatorname{sgn}\\left(a\\right)$ . ", "page_idx": 12}, {"type": "text", "text": "Here, $s_{x^{\\uparrow},y^{\\downarrow}}$ is defined as: ", "page_idx": 12}, {"type": "equation", "text": "$$\ns_{x^{\\uparrow},y^{\\updownarrow}}=\\left\\{\\begin{array}{l l}{s_{x^{\\uparrow},y^{\\uparrow}},i f\\ \\ s_{x,y}\\geqslant0}\\\\ {s_{x^{\\uparrow},y^{\\downarrow}},i f\\ \\ s_{x,y}<0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. $|s_{x,y}|\\leqslant\\left|s_{x^{\\uparrow},y^{\\updownarrow}}\\right|\\leqslant\\sqrt{s_{x}^{2}s_{y}^{2}}$ , and the equality conditions are immediate from Theorem 2. The remaining parts of the inequality series are obvious. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Proposition 1. For random variables $X,\\,Y$ , and samples $x,\\,y_{:}$ , the following hold: ", "page_idx": 12}, {"type": "text", "text": "\u2022 $\\left|r^{\\#}\\left(X,Y\\right)\\right|\\leqslant1$ and the equality holds if and only if $X$ and $Y$ are monotone dependent.   \n\u2022 $\\left|r^{\\#}\\left(x,y\\right)\\right|\\leqslant1$ and the equality holds if and only if x and y are monotone dependent. ", "page_idx": 12}, {"type": "text", "text": "Proof. The proposition is immediate from Corollary 1 and Corollary 2. ", "page_idx": 12}, {"type": "text", "text": "Proposition 2. For random variables $X,\\,Y_{}.$ , and samples $x$ , y, the following hold: ", "page_idx": 12}, {"type": "text", "text": "\u2022 $\\left|r^{\\#}\\left(X,Y\\right)\\right|\\ \\geqslant\\ \\left|r\\left(X,Y\\right)\\right|$ and the equality holds $i f$ and only if $Y\\,{\\overset{d}{=}}\\,\\alpha X\\,+\\,\\beta,$ , with $\\operatorname{sgn}\\left(r\\left(X,Y\\right)\\right)=\\operatorname{sgn}\\left(\\alpha\\right)\\!.$   \n\u2022 $\\left|r^{\\#}\\left(x,y\\right)\\right|\\geqslant\\left|r\\left(x,y\\right)\\right|$ and the equality holds if and only if y is arbitrary permutation of $a x+b$ , with $\\operatorname{sgn}\\left(r\\left(x,y\\right)\\right)=\\operatorname{sgn}\\left(a\\right)$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. The proposition is immediate from Corollary 1 and Corollary 2. ", "page_idx": 12}, {"type": "text", "text": "A.2 Experiments settings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "All the experiments are implemented with the R language (R Core Team, 2024), along with several add-on packages. The following are lists of packages and functions for the implementation of the nine measures involved in our study: ", "page_idx": 12}, {"type": "text", "text": "\u2022 $r^{+}$ , recor::loose_pearson()   \n\u2022 r, stats::cor()   \n\u2022 $r^{\\#}$ , recor::sharp_pearson()   \n\u2022 \u03c1, stats::cor(), with the argument method set as \u201cspearman\u201d   \n\u2022 \u03c4, stats::cor(), with the argument method set as \u201ckendall\u201d   \n\u2022 HSIC, dHSIC::dhsic()   \n\u2022 dCor, energy::dcor() ", "page_idx": 12}, {"type": "text", "text": "\u2022 MIC, minerva::mine_stat() \u2022 $\\xi$ , XICOR::calculateXI() ", "page_idx": 13}, {"type": "text", "text": "For convenience, we developed an R package recor, which encapsulates all these measures as $c o r\\_X X X()$ functions. The recor package is available as recor_1.0.2.tar.gz in supplemental materials. For a latest version, please visit https://github.com/byaxb/recor. ", "page_idx": 13}, {"type": "text", "text": "Hardware environment configuration for this study was: DELL OptiPlex 7070 Tower, equipped with 8-core CPU Core i7-9700 $@$ 3.00GHz, 24G DDR4 2666MHz RAM. Under this configuration, it took about 5 days to complete all the experiments. ", "page_idx": 13}, {"type": "text", "text": "A.3 Simulated scenarios ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We carry out our experiments on 50 simulated scenarios, including all basic elementary functions, lots of composite functions and several typical special functions. ", "page_idx": 13}, {"type": "text", "text": "1. Linear: $y=2x+1,x\\in[0,1]$ 2. Quadratic [asymmetry]: $y=x^{2},x\\in[0,1]$ 3. Square Root: $y=\\sqrt{x},x\\in[0,1]$ 4. Cubic: $y=x^{3},x\\in[0,1]$ 5. Reciprocal: $\\textstyle y={\\frac{1}{x}},x\\in[0,1]$ 6. Exponential: $y=e^{x}$ , with $x\\in[0,1]$ 7. Logarithm: $y=\\ln x,x\\in[0,1]$ 8. Sine [quarter period]: $y=\\sin\\left(x\\right),x\\in\\left[0,{\\frac{\\pi}{2}}\\right]$ 9. Cosine [quarter period]: $y=\\cos\\left(x\\right),x\\in\\left[0,\\frac{\\pi}{2}\\right]$   \n10. Tangent [half period]: $y=\\tan\\left(x\\right),x\\in\\left[0,{\\frac{\\pi}{2}}\\right]$   \n11. Cotangent [half period]: $y=\\cot\\left(x\\right),x\\in\\left[0,{\\frac{\\pi}{2}}\\right]$   \n12. Inverse Sine: $y=\\arcsin\\left(x\\right),x\\in\\left[0,1\\right]$   \n13. Inverse Cosine: $y=\\operatorname{arccos}\\left(x\\right),x\\in\\left[0,1\\right]$   \n14. Inverse Tangent: $y=\\arctan\\left(x\\right),x\\in\\left[0,1\\right]$   \n15. Inverse Cotangent: $y=\\operatorname{arccot}\\left(x\\right),x\\in\\left[0,1\\right]$   \n16. Secant [quarter period]: $y=\\sec\\left(x\\right),x\\in\\left[0,{\\frac{\\pi}{2}}\\right]$   \n17. Cosecant [quarter period]: $y=\\csc\\left(x\\right),x\\in\\left[0,{\\frac{\\pi}{2}}\\right]$   \n18. Hyperbolic Sine: $\\begin{array}{r}{y=\\sinh x=\\frac{e^{x}-e^{-x}}{2},x\\in[0,1]}\\end{array}$   \n19. Hyperbolic Cosine: $\\begin{array}{r}{y=\\cosh x=\\frac{e^{x}+e^{-x}}{2},x\\in[0,1]}\\end{array}$   \n20. Hyperbolic Tangent: $\\begin{array}{r}{y=\\operatorname{tanh}x=\\frac{e^{2x}-1}{e^{2x}+1},x\\in[0,1]}\\end{array}$   \n21. Hyperbolic Cotangent: $\\begin{array}{r}{y=\\coth x=\\frac{e^{2x}+1}{e^{2x}-1},x\\in[0,1]}\\end{array}$   \n22. Hyperbolic Secant: $\\begin{array}{r}{y=\\mathrm{sech}\\left(x\\right)=\\frac{2}{e^{x}+e^{-x}},x\\in\\left[0,100\\right]}\\end{array}$   \n23. Hyperbolic Cosecant: $\\begin{array}{r}{y=c s c h\\left(x\\right)=\\frac{2}{e^{x}-e^{-x}},x\\in\\left[0,100\\right]}\\end{array}$   \n24. Inverse Hyperbolic Sine: $y=a r c s i n h\\left(x\\right)=\\ln\\left(x+{\\sqrt{x^{2}+1}}\\right),x\\in\\left[0,1\\right]$   \n25. Inverse Hyperbolic Cosine: $y=a r c c o s h\\left(x\\right)=\\ln\\left(x+{\\sqrt{x^{2}-1}}\\right),x\\in\\left[1,2\\right]$   \n26. Inverse Hyperbolic Tangent: $y=a r c t a n h\\left(x\\right)=\\textstyle{\\frac{1}{2}}\\ln\\left({\\frac{1+x}{1-x}}\\right)\\,\\,,x\\in\\left[0,1\\right]$   \n27. Inverse Hyperbolic Cotangent: $y=a r c c o t h\\left(x\\right)=\\textstyle{\\frac{1}{2}}\\ln\\left({\\frac{x+1}{x-1}}\\right),x\\in\\left[1,2\\right]$   \n28. Inverse Hyperbolic Secant: $y=a r c s e c h\\left(x\\right)=\\ln\\left({\\frac{1}{x}}+{\\sqrt{{\\frac{1}{x^{2}}}-1}}\\right),x\\in\\left[0,1\\right]$ ", "page_idx": 13}, {"type": "text", "text": "29. Inverse Hyperbolic Cosecant: $y=a r c c s c h\\left(x\\right)=\\ln\\left({\\frac{1}{x}}+{\\sqrt{{\\frac{1}{x^{2}}}+1}}\\right),x\\in\\left[0,1\\right]$   \n30. Hook: $\\begin{array}{r}{y=a x+\\frac{b}{x},a=1,b=1,x\\in[0,1]}\\end{array}$   \n31. Rational: $\\textstyle y={\\frac{x+1}{x-1}},x\\in[0,1]$   \n32. Hoerl: $y=x^{a}e^{x},a=-1,x\\in[0,1]$   \n33. Sigmoid: $\\textstyle y={\\frac{1}{1+e^{-x}}},x\\in[-0.5,0.5]$   \n34. Logit: $\\textstyle y=\\ln{\\frac{x}{1-x}},x\\in[0,1]$   \n35. Step: y = $y=\\left\\{\\begin{array}{r l}{0,i f\\ 0\\leqslant x<{\\frac{1}{2}}}\\\\ {1,i f\\ {\\frac{1}{2}}\\leqslant x\\leqslant1}\\end{array}\\right.$   \n36. Piecewise [Sigmoid]: $y=\\left\\{\\begin{array}{c}{0,\\qquad\\qquad i f\\ 0\\leqslant x\\leqslant{\\frac{49}{100}}}\\\\ {50\\left(x-{\\frac{1}{2}}\\right)+{\\frac{1}{2}},i f\\ {\\frac{49}{100}}<x<{\\frac{51}{100}}}\\\\ {1,\\qquad\\qquad i f\\ {\\frac{51}{100}}\\leqslant x\\leqslant1}\\end{array}\\right.$   \n37. Linear $^+$ Periodic, High Freq: $\\begin{array}{r}{y=\\frac{1}{10}\\sin\\left(10.6\\left(2x-1\\right)\\right)+\\frac{11}{10}\\left(2x-1\\right),x\\in\\left[0,1\\right]}\\end{array}$   \n38. Sinc Function: $\\begin{array}{r}{S_{k,h}\\left(x\\right)=\\frac{\\sin\\left(\\pi\\left(x-k h\\right)/h\\right)}{\\pi\\left(x-k h\\right)/h},k=0,h=1,x\\in\\left[0,1\\right]}\\end{array}$   \n39. Einstein Function: Einstein1 $\\begin{array}{r}{(x)=\\frac{x^{2}e^{x}}{(e^{x}-1)^{2}},x\\in[0,1]}\\end{array}$   \n40. Exponential Integral: $\\begin{array}{r}{E_{1}\\left(x\\right)=\\int_{x}^{\\infty}\\frac{e^{-t}}{t}d t,x\\in\\left[0,1\\right]}\\end{array}$   \n41. Hyperbolic Sine Integral: $\\begin{array}{r}{S h i\\left(x\\right)=\\int_{0}^{x}\\frac{\\sinh t}{t}d t,x\\in\\left[0,1\\right]}\\end{array}$   \n42. Hyperbolic Cosine Integral: $\\begin{array}{r}{C h i\\left(x\\right)\\,=\\,\\gamma+\\ln{x}+\\int_{0}^{x}\\frac{\\cosh t-1}{t}d t,x\\,\\in\\,\\left[0,1\\right]}\\end{array}$ . Here $\\gamma$ is   \nEuler\u2019s Constant   \n43. Error Function: $\\begin{array}{r}{\\begin{array}{r}{e r f\\left(x\\right)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x}e^{-t^{2}}d t,x\\in\\left[0,1\\right]}\\end{array}}\\end{array}$   \n44. Inverse Error Function: i $n v e r f\\left(x\\right)=t+{\\textstyle{\\frac{1}{3}}}t^{3}+{\\textstyle{\\frac{7}{30}}}t^{5}+\\dotsb,t={\\textstyle{\\frac{1}{2}}}\\sqrt{\\pi}x,x\\in\\left[0,1\\right]$   \n45. Gamma Function: $\\begin{array}{r}{\\Gamma\\left(x\\right)=\\int_{0}^{\\infty}t^{x-1}e^{-t}d t,x\\in\\left[0,1\\right]}\\end{array}$   \n46. Psi Function: $\\begin{array}{r}{\\psi\\left(x\\right)=\\frac{d^{k+1}}{d x^{k+1}}\\ln\\Gamma\\left(x\\right)=\\frac{\\Gamma^{\\prime}\\left(x\\right)}{\\Gamma\\left(x\\right)},x\\in\\left[0,1\\right],k=0}\\end{array}$   \n47. Riemann Zeta Function: $\\zeta\\left(x\\right)=\\sum_{n=1}^{\\infty}{\\frac{1}{n^{x}}},x\\in\\left[0,1\\right]$   \n48. Bessel Function: $\\begin{array}{r}{Y_{v}\\left(x\\right)\\,=\\,\\frac{J_{v}\\left(x\\right)\\cos\\left(v\\pi\\right)-J_{-v}\\left(x\\right)}{\\sin\\left(v\\pi\\right)},\\,J_{v}\\left(x\\right)\\,=\\,\\left(\\frac{1}{2}x\\right)^{v}\\,\\underset{k=0}{\\overset{\\infty}{\\sum}}\\,(-1)^{k}\\frac{\\left(\\frac{1}{4}x^{2}\\right)^{k}}{k!\\Gamma\\left(v+k+1\\right)},}\\end{array}$   \n$v=0,x\\in[0,1]$   \n49. Beta Function: $\\begin{array}{r}{B\\left(x,w\\right)=\\frac{\\Gamma(x)\\Gamma(w)}{\\Gamma\\left(x+w\\right)},w=1,x\\in\\left[0,1\\right]}\\end{array}$   \n50. Dirichlet Eta Function: $\\eta\\left(x\\right)=\\sum_{n=1}^{\\infty}{\\frac{-1^{n-1}}{n^{x}}},x\\in\\left[0,1\\right]$ ", "page_idx": 14}, {"type": "text", "text": "A.4 Real-life scenarios ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All the five real life scenarios are provided by NIST (National Institute of Standards and Technology, 2003) as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Chwirut1: ultrasonic calibration, with $Y$ as ultrasonic response, and $X$ as metal distance.   \n\u2022 Hahn1: thermal expansion of copper, with $Y$ as the coefficient of thermal expansion, and $X$ as temperature in degrees kelvin.   \n\u2022 Rat43: sigmoid growth, with $Y$ as dry weight of onion bulbs and tops, and $X$ as growing time.   \n\u2022 Roszman1: quantum defects in iodine atoms, with $Y$ as the number of quantum defects, and $X$ as the excited energy state. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Thurber: semiconductor electron mobility, with $Y$ as a measure of electron mobility, and $X$ as the natural log of the density. ", "page_idx": 15}, {"type": "text", "text": "Data and details about these scenarios are available publicly at: https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml ", "page_idx": 15}, {"type": "text", "text": "A.5 Non-monotone scenarios ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conducted our experiments on 16 non-monotone scenarios, comprehensively covering all the scenarios from (Reshef et al., 2011) and (Simon and Tibshirani, 2014). ", "page_idx": 15}, {"type": "text", "text": "1. Quadratic [symmetry]: $y=4x^{2},x\\in\\left[-{\\frac{1}{2}},{\\frac{1}{2}}\\right]$   \n2. Cubic 2: $y=128{\\left(x-{\\textstyle{\\frac{1}{3}}}\\right)}^{3}-48{\\left(x-{\\textstyle{\\frac{1}{3}}}\\right)}^{2}-12\\left(x-{\\textstyle{\\frac{1}{3}}}\\right),x\\in\\left[0,1\\right]$   \n3. Sine, High Freq: $y=\\sin\\left(16\\pi x\\right),x\\in\\left[0,1\\right]$   \n4. Cosine [High Freq]: $y=\\cos\\left(14\\pi x\\right),x\\in\\left[0,1\\right]$   \n\uf8f1 200x, if 0 \u2a7dx <2100   \n5. Lopsided L-shaped: $y=\\left\\{\\begin{array}{c}{-198x+\\frac{199}{100},i f\\ \\frac{1}{200}\\leqslant\\overset{=}x<\\frac{1}{100}}\\\\ {-\\frac{x}{99}+\\frac{1}{99},\\ i f\\ \\frac{1}{100}\\leqslant x\\leqslant1}\\end{array}\\right.$   \n6. Circle: $y=\\sqrt{1\\!-\\!\\left(2x-1\\right)^{2}},x\\in\\left[0,1\\right]$   \n7. Linear $^+$ Periodic, Medium Freq: $y=\\sin\\left(10\\pi x\\right)+x,x\\in[0,1]$   \n8. Cubic 3: $y=4x^{3}+x^{2}-4x,x\\in[-1.1,1.3]$   \n9. Cubic, Y-stretched: $y=41\\left(4x^{3}+x^{2}-4x\\right),x\\in[-1.1,1.3]$   \n10. Sine [Two periods]: $y=\\sin\\left(4\\pi x\\right),x\\in\\left[0,1\\right]$   \n11. Sine [Low Freq]: $y=\\sin\\left(8\\pi x\\right),x\\in\\left[0,1\\right]$   \n12. Sine, Non-Fourier Freq [Low]: $y=\\sin\\left(9\\pi x\\right),x\\in\\left[0,1\\right]$   \n13. Cosine, Non-Fourier Freq [Low]: $y=\\cos\\left(7\\pi x\\right),x\\in\\left[0,1\\right]$   \n14. Sine, Varying Freq [Medium]: $y=\\sin\\left(6\\pi x\\left(1+x\\right)\\right),x\\in\\left[0,1\\right]$   \n15. Cosine, Varying Freq [Medium]: $y=\\cos\\left(5\\pi x\\left(1+x\\right)\\right),x\\in\\left[0,1\\right]$   \n16. Linear $^+$ Periodic, High Freq 2: $\\begin{array}{r}{y=\\frac{1}{5}\\sin\\left(10.6\\left(2x-1\\right)\\right)+\\frac{11}{10}\\left(2x-1\\right),x\\in\\left[0,1\\right]}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The contributions are summarized in Abstract and the last paragraph of Introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The proposed $r^{\\#}$ measures linear and nonlinear monotone relationships accurately. However, it may fail to measure non-monotone dependence. The limitations are discussed in 3.5 Performance in non-monotone scenarios. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Full set of assumptions and complete and correct proofs are provided in 2.3 New inequality tighter than Cauchy-Schwarz Inequality, 2.4 The proposed Rearrangement Correlation and A.1 Proofs of theorems, corollaries and propositions. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We fully disclosed all the information needed to reproduce the experimental results, as depicted in 3.2 Simulation procedure. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The data and code are available as an attached zip flie, Code and data.zip. We developed an R package recor, which implemented all the experiments. The recor package is available as recor_1.0.2.tar.gz, included in the zip flie. Sufficient instructions to faithfully reproduce the experiments are available in the package help files. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The experimental settings are provided in 3 Experiments, and A.2 Experiments Settings. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We evaluate the performance of proposed method and others according to ISO 5725. Please see 3 Experiments for details. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Details about the hardware configuration and the time of execution are shown in section $^{\\bullet}A.2$ Experiments Settings\u201d. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have thoroughly reviewed the NeurIPS Code of Ethics. And we confirm that our research fully complies with all of its provisions. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: What we proposed here is an adjusted version of Pearson\u2019s $r$ . This study simply provides theoretical results for measuring dependence and does not involve societal impacts. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Neither the proposed method nor the released data has risk for misuse. The applicability range of this method has undergone comprehensive discussion in 3.5 Performance in non-monotone scenarios. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All code and data used in our paper are properly credited and explicitly mentioned, as shown in A.2 Experiments Settings. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We developed an R package to reproduce the experiments. And all the functions in this package are well documented, which can be accessed by the command \u201c?function_name\u201d after installation. For more details, see A.2 Experiments Settings. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our research does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]