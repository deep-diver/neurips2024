[{"figure_path": "GruuYVTGXV/tables/tables_7_1.jpg", "caption": "Table 1: Performance of agents on MiniGrid after 1e5 frames (27 games) and 1e6 frames (13 games) of training. We report the mean and standard error of the performance.", "description": "This table presents the performance comparison of different RL algorithms on MiniGrid environment in terms of average return after 1e5 and 1e6 frames of training.  The algorithms compared are Recurrent Actor-Critic, Asymmetric Actor-Critic, Oracle Guiding, Unbiased Asymmetric Actor-Critic, and DCRL (the proposed method).  The mean and standard error of the performance are reported for each algorithm and training duration.  This shows the relative performance of each algorithm across different training durations.", "section": "5 Experiments"}, {"figure_path": "GruuYVTGXV/tables/tables_18_1.jpg", "caption": "Table 1: Performance of agents on MiniGrid after 1e5 frames (27 games) and 1e6 frames (13 games) of training. We report the mean and standard error of the performance.", "description": "This table presents the performance comparison of different reinforcement learning algorithms on the MiniGrid environment.  Two different training durations are considered: 1e5 frames (27 games) and 1e6 frames (13 games). For each algorithm, the mean and standard error of the performance (average return) are shown for both training durations.  This allows for an assessment of the convergence and overall performance of each algorithm across different training lengths.", "section": "5 Experiments"}]