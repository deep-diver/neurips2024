[{"figure_path": "N4quRxE19p/tables/tables_4_1.jpg", "caption": "Table 1: Key differences between AVATAR and prevailing agent methods. AVATAR demonstrates the ability to: 1) self-improve on specific tasks, 2) retain memory throughout the optimization process, 3) enhance the agent's generalization capability, and 4) autonomously generate holistic, high-quality prompts for better tool usage. Please refer to Section 4 for details.", "description": "This table summarizes the key differences between AVATAR and other existing agent methods (ReAct, Self-refine, and Reflexion) across four aspects: self-improvement, memory retention, generalization ability, and holistic prompt generation for tool usage.  It highlights that AVATAR outperforms other methods in all four aspects, particularly in generating holistic prompts that effectively guide tool usage.", "section": "4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks"}, {"figure_path": "N4quRxE19p/tables/tables_7_1.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the results of the retrieval performance of different models on the STARK benchmark dataset.  The metrics used are Hit@1, Hit@5, Recall@20, and MRR (Mean Reciprocal Rank).  The models compared are DPR, QAGNN, ada-002, multi-ada-002, ReAct, Reflexion, AVATAR-C, and AVATAR.  The last row shows the percentage improvement of AVATAR over the best performing baseline for each metric across all three subsets of the STARK dataset (AMAZON, MAG, PRIME).", "section": "5.1 Textual and Relational Retrieval Tasks"}, {"figure_path": "N4quRxE19p/tables/tables_9_1.jpg", "caption": "Table 3: Performance (%) on three QA benchmarks. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the performance of different models on three question answering benchmarks: HotpotQA, ArxivQA, and ToolQA.  The performance is measured by several metrics, including exact match, and the last row shows the percentage improvement of AVATAR over the best performing baseline model for each metric on each benchmark.", "section": "5.3 Question Answering Tasks"}, {"figure_path": "N4quRxE19p/tables/tables_16_1.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the results of retrieval experiments conducted on the STARK benchmark dataset.  It compares the performance of several retrieval methods, including DPR, QAGNN, ada-002, multi-ada-002, ReAct, Reflexion, AVATAR-C, and AVATAR.  The performance metrics used are Hit@1, Hit@5, Recall@20, and MRR (Mean Reciprocal Rank), calculated across three subsets of the STARK benchmark: AMAZON, MAG, and PRIME. The last row shows the percentage improvement of AVATAR over the best-performing baseline for each metric and dataset.", "section": "5 Experiments"}, {"figure_path": "N4quRxE19p/tables/tables_16_2.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the performance of different retrieval models on the STARK benchmark dataset.  The models are compared across various metrics, including Hit@1, Hit@5, Recall@20, and MRR, for three different subsets of the dataset (AMAZON, MAG, PRIME). The last row shows the percentage improvement of AVATAR compared to the best-performing baseline model for each metric.", "section": "5.1 Textual and Relational Retrieval Tasks"}, {"figure_path": "N4quRxE19p/tables/tables_16_3.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the performance of different retrieval models on the STARK benchmark.  The models are evaluated using four metrics: Hit@1, Hit@5, Recall@20, and MRR.  The table shows the performance of several baselines (DPR, QAGNN, ada-002, multi-ada-002, ReAct, Reflexion, AVATAR-C) and the proposed model, AVATAR. The last row displays the relative improvement achieved by AVATAR compared to the best-performing baseline for each metric.  The results demonstrate the superior performance of the AVATAR model on this benchmark.", "section": "5 Experiments"}, {"figure_path": "N4quRxE19p/tables/tables_17_1.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the performance of different retrieval models on the STARK benchmark, including DPR, QAGNN, ada-002, multi-ada-002, ReAct, and Reflexion.  The metrics used are Hit@1, Hit@5, Recall@20, and MRR.  The results are broken down by dataset (AMAZON, MAG, PRIME). The final row shows the percentage improvement of AVATAR over the best performing baseline model for each metric and dataset.", "section": "5.1 Textual and Relational Retrieval Tasks"}, {"figure_path": "N4quRxE19p/tables/tables_18_1.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the results of the retrieval performance on the STARK benchmark using different methods.  The metrics used include Hit@1, Hit@5, Recall@20, and MRR (Mean Reciprocal Rank). The table compares AVATAR's performance against several baselines (DPR, QAGNN, ada-002, multi-ada-002, ReAct, and Reflexion) across three subsets of the benchmark (AMAZON, MAG, and PRIME). The last row shows the percentage improvement of AVATAR relative to the best-performing baseline for each metric and dataset.", "section": "5 Experiments"}, {"figure_path": "N4quRxE19p/tables/tables_21_1.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the performance of different retrieval models on the STARK benchmark across three datasets: AMAZON, MAG, and PRIME.  The metrics used are Hit@1, Hit@5, Recall@20, and MRR. The table compares the performance of AVATAR against several baselines including DPR, QAGNN, ada-002, multi-ada-002, ReAct, and Reflexion. The last row shows the relative improvement of AVATAR over the best performing baseline for each metric and dataset.", "section": "5 Experiments"}, {"figure_path": "N4quRxE19p/tables/tables_22_1.jpg", "caption": "Table 2: Retrieval performance (%) on STARK benchmark. Last row shows the relative improvements over the best metric value in each column.", "description": "This table presents the results of retrieval experiments conducted on the STARK benchmark, a large-scale semi-structured retrieval benchmark.  It compares the performance of several methods, including DPR, QAGNN, ada-002, multi-ada-002, ReAct, Reflexion, AVATAR-C and AVATAR, across three different subsets of the benchmark (AMAZON, MAG, PRIME).  The metrics used are Hit@1, Hit@5, Recall@20, and MRR. The last row shows the relative improvement of AVATAR over the best performing baseline for each metric and dataset.", "section": "5 Experiments"}]