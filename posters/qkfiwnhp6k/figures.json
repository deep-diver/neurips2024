[{"figure_path": "qKfiWNHp6k/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of typical region-level visual understanding architecture. (a) Learning the region recognition model by distilling image-level ViL representations from cropped regions and incorporating them into a detection model (e.g., [7]). (b) Fully fine-tuning both vision and text models with a substantial dataset of region-label pairs. (c) Our proposed approach integrates pretrained (frozen) localization and ViL models, emphasizing the learning of their representational correlation.", "description": "This figure compares three different approaches for region-level visual understanding. (a) shows a common approach where image-level representations from cropped regions are used to train a region recognition model. (b) illustrates a method that fine-tunes both vision and text models with a large dataset of region-label pairs. Finally, (c) presents the RegionSpot approach, which leverages pre-trained localization and vision-language models and focuses on learning their representational correlation to improve efficiency.", "section": "1 Introduction"}, {"figure_path": "qKfiWNHp6k/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our proposed RegionSpot. (a) We integrate position-aware tokens from a localization model, such as SAM, with image-level feature maps extracted from a ViL model like CLIP. This integration yields region-level semantic tokens, which are then subjected to region text alignment. (b) Our cross-modal feature interaction design based on the attention mechanism.", "description": "This figure illustrates the RegionSpot architecture.  Panel (a) shows the overall workflow, highlighting how position-aware tokens from a localization model (like SAM) are combined with image-level features from a vision-language model (like CLIP) to create region-level semantic tokens. These tokens are then used in a region-text alignment step. Panel (b) zooms in on the cross-modal feature interaction, which is based on a transformer's attention mechanism to effectively fuse localization and semantic information.", "section": "3 Method"}, {"figure_path": "qKfiWNHp6k/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative prediction results of GLIP-T [18] (first row) and RegionSpot (second row) on the LVIS dataset [8]. Our model recognizes the objects more accurately. Best viewed when zooming-in.", "description": "This figure shows a qualitative comparison of object detection results between GLIP-T and the proposed RegionSpot model on the LVIS dataset.  Three image examples are presented.  Each image shows the bounding boxes and predicted labels from both models. The results visually demonstrate that RegionSpot achieves higher accuracy in recognizing and labeling objects compared to GLIP-T, especially for smaller or more nuanced objects.", "section": "4.3 Visualization"}, {"figure_path": "qKfiWNHp6k/figures/figures_8_2.jpg", "caption": "Figure 4: Cross-attention maps in RegionSpot. These maps show that the position-aware token aligns effectively with the semantic feature map of the entire image. In each row, the blue and red boxes are corresponding to the left and right maps respectively.", "description": "This figure visualizes the cross-attention mechanism within the RegionSpot model.  It shows how position-aware tokens (from the localization model, SAM) interact with the image-level semantic features (from the ViL model, CLIP). The attention maps highlight the correlation between the localized regions and their semantic representations in the full image. The blue and red boxes in the image correspond to the left and right attention maps respectively, demonstrating how the model integrates positional and semantic information.", "section": "3.2 Region text alignment with frozen foundation models"}, {"figure_path": "qKfiWNHp6k/figures/figures_14_1.jpg", "caption": "Figure 3: Qualitative prediction results of GLIP-T [18] (first row) and RegionSpot (second row) on the LVIS dataset [8]. Our model recognizes the objects more accurately. Best viewed when zooming-in.", "description": "This figure shows a qualitative comparison of object recognition results between GLIP-T and the proposed RegionSpot model on the LVIS dataset.  Three image examples are presented, each showing the bounding boxes and predicted labels from both methods.  RegionSpot demonstrates superior accuracy in recognizing objects within the images, particularly in identifying smaller or more obscure objects.", "section": "4.3 Visualization"}]