{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduced CLIP, a foundational vision-language model that is heavily leveraged by the current work for its semantic capabilities."}, {"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment Anything", "publication_date": "2023-04-02", "reason": "This paper introduced SAM, a foundational model used in the current work for its class-agnostic visual localization capabilities, critical for region-level understanding."}, {"fullname_first_author": "Xiuye Gu", "paper_title": "Open-vocabulary object detection via vision and language knowledge distillation", "publication_date": "2021-04-13", "reason": "This paper is among the most important as it proposes one of the current state-of-the-art methods that the current work aims to improve on; it is a key competitor."}, {"fullname_first_author": "Liunian Harold Li", "paper_title": "Grounded language-image pre-training", "publication_date": "2022-06-01", "reason": "This paper introduced GLIP, another state-of-the-art method that the current work compares against and aims to improve upon; this is a key competitor."}, {"fullname_first_author": "Yiwu Zhong", "paper_title": "RegionCLIP: Region-based language-image pretraining", "publication_date": "2022-06-01", "reason": "This paper is highly relevant as it uses a similar strategy of combining CLIP with another model; it is a key comparative method."}]}