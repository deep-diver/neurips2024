[{"figure_path": "P5dEZeECGu/figures/figures_0_1.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "The figure shows the capabilities of FlexCap in generating region-specific descriptions with varying lengths, demonstrating control over detail.  The left side illustrates this controllable captioning, ranging from simple object labels to detailed descriptions. The right side showcases how FlexCap's localized descriptions, combined with large language models (LLMs), achieve state-of-the-art zero-shot visual question answering.", "section": "Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_1_1.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "This figure demonstrates FlexCap's capabilities. The left side showcases its ability to generate region-specific descriptions of varying lengths, from simple object labels to detailed narratives, by controlling caption length. The right side illustrates how FlexCap's localized descriptions, combined with Large Language Models (LLMs), enable zero-shot Visual Question Answering (VQA), enabling the system to answer questions about specific image regions without explicit training.", "section": "Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_2_1.jpg", "caption": "Figure 2: Architecture and Training Setup. FlexCap outputs a length-controlled caption of the object contained in the bounding box by taking (left) an image, (middle) coordinates of a bounding box and (right) the length prefix and caption, as inputs. The training loss is the standard next-word prediction loss that is used to train image captioning models.", "description": "This figure illustrates the architecture and training process of the FlexCap model.  The model takes an image, bounding box coordinates, and a length prefix as input. The image encoder processes the image, and a linear projection handles the bounding box coordinates. These are concatenated and fed to a transformer-based text decoder, which generates a caption with the specified length. The training utilizes standard next-word prediction loss to optimize the model's caption generation.", "section": "2 FlexCap"}, {"figure_path": "P5dEZeECGu/figures/figures_4_1.jpg", "caption": "Figure 3: Dataset Generation. We use OWL-ViT to generate a dataset of triplets of image, bounding box and captions from a web-scale dataset of noisy image-text pairs. Increasing levels of richness in captions is captured through different length descriptions for each box.", "description": "This figure illustrates the process of generating a large-scale dataset for training the FlexCap model.  It starts with 2 billion image-text pairs from the WebLI dataset.  These captions are then broken down into n-grams (sequences of words) of varying lengths.  OWL-ViT, an object detection model, is used to identify regions (bounding boxes) within the images that correspond to these n-grams. The result is a massive dataset of 32 billion triplets, each containing an image, a bounding box, and a caption of a specific length. This diverse dataset allows the model to learn to generate captions of varying detail and length for specific image regions.", "section": "3 Localized Captions Dataset"}, {"figure_path": "P5dEZeECGu/figures/figures_4_2.jpg", "caption": "Figure 4: Evaluating open-vocabulary outputs from FlexCap using the CLIP [39] text encoder.", "description": "This figure illustrates the process of evaluating open-vocabulary outputs generated by FlexCap using the CLIP text encoder.  FlexCap processes the input image and bounding box to generate multiple captions of varying lengths. These captions are then encoded by CLIP's text encoder, producing text embeddings. By comparing the mean of the predicted caption embeddings with the embeddings of ground-truth class names, the model obtains classification scores. This process allows for evaluating the model's ability to generate open-vocabulary outputs and provides quantitative metrics to assess its performance.", "section": "4.1 Correctness and Compliance of Generated Captions"}, {"figure_path": "P5dEZeECGu/figures/figures_5_1.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "The figure demonstrates FlexCap's ability to generate detailed captions for specific image regions. The left side shows how FlexCap generates captions of varying lengths for the same region, showcasing its control over detail. The right side illustrates how FlexCap's localized captions, combined with LLMs, enable zero-shot visual question answering.", "section": "1 Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_6_1.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "The figure demonstrates FlexCap's ability to generate localized image descriptions with varying lengths, controlled by input parameters.  The left panel shows how different lengths of captions can be generated for the same image region. The right panel illustrates how FlexCap's detailed regional descriptions, when combined with a Large Language Model (LLM), enable zero-shot Visual Question Answering (VQA).", "section": "1 Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_6_2.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "This figure demonstrates the capabilities of FlexCap, a vision-language model, in generating region-specific image descriptions with controllable detail.  The left side shows how FlexCap produces captions ranging from concise object labels to rich, detailed descriptions for the same image region, simply by adjusting the specified caption length. The right side illustrates how FlexCap's detailed localized descriptions, when used as input to a large language model (LLM), enable the system to achieve zero-shot visual question answering.", "section": "1 Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_6_3.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "The figure demonstrates FlexCap's capability to generate region-specific image descriptions of varying length, from concise object labels to detailed captions.  The left side shows this controllable detail generation. The right side demonstrates how these rich, localized descriptions, combined with LLMs, enable zero-shot visual question answering, showcasing a key application of the model.", "section": "Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_6_4.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "This figure demonstrates the capabilities of FlexCap, a vision-language model, in generating region-specific descriptions with controllable detail.  The left side shows how FlexCap can generate captions of varying length for the same image region, ranging from simple object labels to detailed descriptions. The right side illustrates how these localized, detailed descriptions can be used as input for a Large Language Model (LLM) to enable zero-shot Visual Question Answering (VQA). This highlights FlexCap's ability to bridge the gap between localized image understanding and comprehensive language modeling.", "section": "1 Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_8_1.jpg", "caption": "Figure 6: Open-Ended Object Detection on the Visual Genome dataset. We find that the localize-then-describe approach, which involves describing every detected box with FlexCap, achieves higher recall (see (a)) and produces more bounding boxes with good matching scores (see (b)) compared to describe-then-localize approach with SOTA VLMs and open-vocabulary detection. The difference between the two approaches is most stark for small and medium sized objects (see (c-e)).", "description": "This figure compares the performance of two different approaches for open-ended object detection on the Visual Genome dataset. The first approach, \"localize-then-describe,\" uses FlexCap to describe each detected bounding box individually. The second approach, \"describe-then-localize,\" first uses a state-of-the-art vision-language model (LLAVA) to generate a comprehensive description of the entire image, and then uses an open-vocabulary object detection method to localize the objects described. The figure shows that the \"localize-then-describe\" approach (using FlexCap) achieves higher recall and identifies more objects, especially small and medium-sized objects, than the \"describe-then-localize\" approach.", "section": "4.4 Open-Ended Object Detection"}, {"figure_path": "P5dEZeECGu/figures/figures_8_2.jpg", "caption": "Figure 7: Extracting properties by conditioning FlexCap with prefixes. Examples of FlexCap extracting properties of objects of different categories by using relevant prefixes. Note how we are able to retrieve a one-word answer from the model by controlling the length of the caption.", "description": "This figure demonstrates the ability of FlexCap, when provided with specific prefixes, to extract particular attributes of objects.  Two examples are shown: one where the prefix \"LENGTH-4 The color is ____\" is used, resulting in color identification (red, blue, purple, black, orange) for various items in the image; and another using the prefix \"LENGTH-5 This is made of ____\", which successfully identifies materials (concrete, wicker, metal, leather, ceramic, copper) of the objects shown.  The key takeaway is that by controlling caption length with prefixes, it is possible to guide the model toward providing specific information, in this case, attributes.", "section": "4.5 Discussion"}, {"figure_path": "P5dEZeECGu/figures/figures_8_3.jpg", "caption": "Figure 7: Extracting properties by conditioning FlexCap with prefixes. Examples of FlexCap extracting properties of objects of different categories by using relevant prefixes. Note how we are able to retrieve a one-word answer from the model by controlling the length of the caption.", "description": "This figure shows examples of how FlexCap can be used to extract specific properties of objects by conditioning the model with prefixes.  Different prefixes, such as \"The color is\" and \"This is made of\", are used to guide the model to output a single-word answer describing the color or material of an object within a given bounding box. The figure demonstrates this capability across multiple object categories, showcasing the model's versatility and ability to extract targeted information through controlled prompting.", "section": "4.5 Discussion"}, {"figure_path": "P5dEZeECGu/figures/figures_14_1.jpg", "caption": "Figure 9: Distribution of caption lengths in the Localized Captions Dataset. We show the distribution of caption length for the region-captions obtained using paired image-texts from two datasets: WebLI [9] and YFCC100M [47]", "description": "This figure shows the distribution of caption lengths for the localized captions dataset created using two different datasets: WebLI and YFCC100M.  Two bar charts are presented, one for each dataset. The x-axis represents the caption length (number of words), and the y-axis represents the fraction of captions with that length.  The charts illustrate the frequency distribution of caption lengths in each dataset, which helps to understand the diversity and balance in the data used for training the model.", "section": "Localized Captions Dataset"}, {"figure_path": "P5dEZeECGu/figures/figures_15_1.jpg", "caption": "Figure 3: Dataset Generation. We use OWL-ViT to generate a dataset of triplets of image, bounding box and captions from a web-scale dataset of noisy image-text pairs. Increasing levels of richness in captions is captured through different length descriptions for each box.", "description": "This figure illustrates the process of generating a large-scale dataset for training the FlexCap model.  It starts with web-scale image-text pairs.  N-grams (sequences of words) are extracted from the captions and used as text queries for an object detection model (OWL-ViT). This model identifies bounding boxes in the images that correspond to the n-grams. The resulting triplets of (image, bounding box, caption) form the dataset, which contains captions of varying lengths, capturing different levels of detail for each image region. This process leverages existing web data and automates the creation of a large-scale, region-specific caption dataset.", "section": "3 Localized Captions Dataset"}, {"figure_path": "P5dEZeECGu/figures/figures_16_1.jpg", "caption": "Figure 11: Objects missed by LLAVA but found by FlexCap. We show some typical examples of objects (5 per image to avoid clutter) that FlexCap can find with OWL-ViT but LLAVA does not.", "description": "This figure shows some examples where the object detection model LLAVA fails to detect certain objects while FlexCap, combined with OWL-ViT, successfully detects them. The images demonstrate that FlexCap, by using a localize-then-describe approach, is better at detecting small and medium-sized objects.  Each image contains labels from FlexCap's detections, including object class and confidence score.", "section": "Open-Ended Object Detection"}, {"figure_path": "P5dEZeECGu/figures/figures_18_1.jpg", "caption": "Figure 1: FlexCap generates controllably rich localized descriptions for any region in an image as shown on the left. It has the flexibility to produce captions in a controllable manner which allows the full spectrum of valid descriptions to be explored from short object category names to fully-detailed captions. On the right, we demonstrate that rich localized captions generated by FlexCap, when coupled with large language models (LLMs), enable zero-shot visual question answering.", "description": "This figure demonstrates FlexCap's ability to generate localized image captions with controllable detail, ranging from concise object labels to rich descriptions.  The left side shows examples of captions generated for different regions of an image, highlighting the length control aspect. The right side illustrates how these detailed localized captions, in conjunction with a large language model (LLM), facilitate zero-shot visual question answering.  This showcases FlexCap's application in various tasks beyond simple image captioning.", "section": "1 Introduction"}, {"figure_path": "P5dEZeECGu/figures/figures_18_2.jpg", "caption": "Figure 14: FlexCap for VQA with bounding box proposals and an LLM. FlexCap generates captions for different regions in a given image. To answer any open-ended questions, we prompt an LLM [15] with FlexCap's detections (box-caption pairs).", "description": "This figure illustrates the architecture of FlexCap used for Visual Question Answering (VQA).  FlexCap first uses OWL-ViTv2 to generate bounding box proposals for the input image. Then, it generates captions for each region using length-conditioned captioning.  These box-caption pairs are then fed into a large language model (LLM), along with a preamble specifying the task as answering questions based on image regions. Finally, the LLM processes the information and provides an answer to the given question.", "section": "4.2 Visual Question Answering"}]