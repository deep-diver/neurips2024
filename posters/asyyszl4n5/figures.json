[{"figure_path": "asYYSzL4N5/figures/figures_2_1.jpg", "caption": "Figure 1: The feature plots of backdoor and benign models with neuron noise using ResNet18 on CIFAR-10. The darker blue represents the target label. As noise increases, the backdoor model identifies more inputs from each class as the target label. The clean model has fewer errors, and there is no significant increase in the number of misclassifications to the target class.", "description": "This figure visualizes the effect of adversarially added neuron noise on the feature representations of clean and backdoored models using t-SNE.  As the noise (epsilon) increases from 0.0 to 0.3, the backdoored model misclassifies a growing proportion of data points from all classes as the target class, indicated by the increasing concentration of dark blue points. In contrast, the clean model shows a much smaller increase in misclassifications, maintaining a clearer separation between classes.", "section": "3 BAN Method"}, {"figure_path": "asYYSzL4N5/figures/figures_4_1.jpg", "caption": "Figure 1: The feature plots of backdoor and benign models with neuron noise using ResNet18 on CIFAR-10. The darker blue represents the target label. As noise increases, the backdoor model identifies more inputs from each class as the target label. The clean model has fewer errors, and there is no significant increase in the number of misclassifications to the target class.", "description": "This figure visualizes the feature space of backdoored and clean models under different levels of adversarial neuron noise using t-SNE.  It shows how, as the neuron noise increases, backdoored models misclassify more data points from various classes as the target class, while clean models remain relatively unaffected. This difference in behavior highlights the sensitivity of backdoored models to adversarial neuron noise, a key principle exploited by the BAN defense.", "section": "3 BAN Method"}, {"figure_path": "asYYSzL4N5/figures/figures_5_1.jpg", "caption": "Figure 1: The feature plots of backdoor and benign models with neuron noise using ResNet18 on CIFAR-10. The darker blue represents the target label. As noise increases, the backdoor model identifies more inputs from each class as the target label. The clean model has fewer errors, and there is no significant increase in the number of misclassifications to the target class.", "description": "This figure visualizes the effect of adversarial neuron noise on the feature space of backdoored and clean models using t-SNE.  As neuron noise increases, backdoored models misclassify more data points from all classes as the target class, whereas clean models show minimal change. This difference is exploited by BAN to distinguish between backdoored and clean models.", "section": "3 BAN Method"}, {"figure_path": "asYYSzL4N5/figures/figures_7_1.jpg", "caption": "Figure 1: The feature plots of backdoor and benign models with neuron noise using ResNet18 on CIFAR-10. The darker blue represents the target label. As noise increases, the backdoor model identifies more inputs from each class as the target label. The clean model has fewer errors, and there is no significant increase in the number of misclassifications to the target class.", "description": "This figure visualizes the feature space of backdoored and clean models under different levels of adversarial neuron noise using t-SNE.  It shows that as the noise increases, the backdoored model misclassifies more data points as the target class, while the clean model remains relatively unaffected. This difference in behavior highlights the effectiveness of adversarial neuron noise in detecting backdoors.", "section": "3 BAN Method"}, {"figure_path": "asYYSzL4N5/figures/figures_14_1.jpg", "caption": "Figure 5: Illustrative diagram of BAN", "description": "This figure provides a visual representation of the BAN (Backdoors Activated by Adversarial Neuron Noise) method.  It shows a simplified neural network where a clean input is processed.  Adversarial neuron noise is added to the network, particularly affecting the weights of certain neurons. The output of a subset of neurons (masked feature) is then used in the subsequent layers, along with the adversarial noise, to determine the attack success rate (ASR). The darker shaded neurons in the figure represent the targeted neurons that are manipulated by the adversarial noise.", "section": "3.3 Detection with Neuron Noise"}]