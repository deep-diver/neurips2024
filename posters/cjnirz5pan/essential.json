{"importance": "This paper is crucial for researchers in continual learning as it addresses the limitations of existing methods by proposing a novel framework, SAFE.  **SAFE significantly improves the generalizability and plasticity of pre-trained models in continual learning scenarios**, opening new avenues for research in parameter-efficient tuning and addressing catastrophic forgetting. Its effectiveness across diverse benchmark datasets highlights its potential impact on various real-world applications.", "summary": "SAFE, a novel parameter-efficient tuning framework, boosts pre-trained model performance in continual learning by balancing model stability and plasticity through slow and fast learning stages, significantly outperforming state-of-the-art methods.", "takeaways": ["The SAFE framework effectively balances stability and plasticity in continual learning.", "SAFE outperforms state-of-the-art methods on multiple benchmark datasets.", "The proposed slow and fast learner strategy offers a novel approach to parameter-efficient tuning in continual learning."], "tldr": "Continual learning (CL) aims to enable models to learn new concepts without forgetting previously learned ones.  However, existing CL methods often struggle when using pre-trained models (PTMs) as the starting point.  They either lose the general knowledge inherent in PTMs or lack the plasticity to effectively learn new concepts. This is due to the direct application of parameter-efficient tuning (PET) in the first session and parameter freezing in subsequent sessions. This paper focuses on solving the issues of applying parameter-efficient tuning directly to downstream data, and freezing parameters in incremental sessions.\nThe proposed Slow and Fast parameter-Efficient tuning (SAFE) framework addresses this by introducing a slow learner that inherits general knowledge from PTMs and a fast learner that incorporates novel concepts while mitigating catastrophic forgetting.  **SAFE employs a knowledge transfer loss to ensure the slow learner retains general knowledge, and a feature alignment loss to help the fast learner learn without forgetting.**  Experiments across seven benchmark datasets show SAFE significantly outperforms existing methods.", "affiliation": "Tencent AI Lab", "categories": {"main_category": "Machine Learning", "sub_category": "Continual Learning"}, "podcast_path": "Cjnirz5pan/podcast.wav"}