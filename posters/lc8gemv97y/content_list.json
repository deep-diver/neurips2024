[{"type": "text", "text": "Dealing with Synthetic Data Contamination in Online Continual Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maorong Wang1 Nicolas Michel1,2 Jiafeng Mao1 Toshihiko Yamasaki1 ", "page_idx": 0}, {"type": "text", "text": "1The University of Tokyo 2Univ Gustave Eiffel, CNRS, LIGM {ma_wang, yamasaki}@cvm.t.u-tokyo.ac.jp nicolas.michel@univ-eiffel.fr mao@hal.t.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image generation has shown remarkable results in generating high-fidelity realistic images, in particular with the advancement of diffusion-based models. However, the prevalence of AI-generated images may have side effects for the machine learning community that are not clearly identified. Meanwhile, the success of deep learning in computer vision is driven by the massive dataset collected on the Internet. The extensive quantity of synthetic data being added to the Internet would become an obstacle for future researchers to collect \u201cclean\u201d datasets without AI-generated content. Prior research has shown that using datasets contaminated by synthetic images may result in performance degradation when used for training. In this paper, we investigate the potential impact of contaminated datasets on Online Continual Learning (CL) research. We experimentally show that contaminated datasets might hinder the training of existing online CL methods. Also, we propose Entropy Selection with Real-synthetic similarity Maximization (ESRM), a method to alleviate the performance deterioration caused by synthetic images when training online CL models. Experiments show that our method can significantly alleviate performance deterioration, especially when the contamination is severe. For reproducibility, the source code of our work is available at https://github.com/maorong-wang/ESRM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual Learning (CL) [10, 31, 41, 12] solves the problem of learning from a sequence of everemerging machine-learning tasks without forgetting previously learned knowledge. Defined by learning manners, CL can be classified into two categories [4]: offline CL and online CL. In offline CL (i.e. conventional CL), the learners can access the training dataset on current task multiple times before proceeding to the next task. In online CL, the training data also comes in a continual data stream, and the continual learner only sees the training data once. Besides learning manners, there are also three typical CL settings [38]: Task-Incremental Learning (TIL), Domain-Incremental Learning (DIL), and Class-Incremental Learning (CIL). In this paper, we investigate the more challenging CIL setting in the online CL manner. ", "page_idx": 0}, {"type": "text", "text": "Image generation with deep generative models has shown remarkable success. Thanks to denoising diffusion models [19, 34], Internet users are capable of generating high-fidelity and realistic images within several seconds. Despite the astonishing quality of those images to human eyes, research has shown that AI-generated content may be harmful when used to train machine learning models, leading to potential performance deterioration [18, 28], bias amplification [8], loss of diversity [28], etc. ", "page_idx": 0}, {"type": "text", "text": "Recently, it has become a trend for researchers to collect datasets from the Internet, and synthetic data contamination would become a potential threat to the CL community. Moreover, the online CL is particularly affected as assessing the soundness of data in the online scenario is impractical. In this work, we first aim to investigate how this new form of dataset contamination might affect the existing Online CL methods. Then, we empirically observe the characteristics of synthetic data when used to train online CL models and form four observations of synthetic data properties in online CL, which might be of interest to the community. Moreover, we investigate synthetic data properties and exhibit specific differences in terms of entropy and representations when compared to real data. Guided by these properties, we propose Entropy Selection with Real-synthetic similarity Maximization (ESRM), a method to alleviate the performance degradation caused by the synthetic contamination. As a replay-based method, ESRM consists of two key components: Entropy Selection (ES) and Real-synthetic similarity Maximization (RM). ES selects more realistic samples in the memory buffer to alleviate catastrophic forgetting. RM is a contrastive learning based optimization strategy, that aims to alleviate the performance artifact caused by synthetic data contamination. ", "page_idx": 1}, {"type": "text", "text": "The major contribution of this paper can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We investigate the potential impact of synthetic data contamination on existing online CL methods and outline four observations regarding the properties of synthetic data in continual scenarios.   \n\u2022 We propose ESRM, a method to alleviate the performance deterioration caused by synthetic data contamination.   \n\u2022 Comprehensive experiments show that ESRM can successfully mitigate the performance deterioration caused by synthetic contamination, especially when the contamination is severe. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Synthetic data contamination. Recently, diffusion models [19, 34] have achieved high-fidelity image generation and surpassed GANs in terms of image quality and diversity. Likewise, text-toimage generation based on diffusion models can generate astonishing images that faithfully follow the users\u2019 text instructions. Furthermore, these generative models demonstrate excellent extrapolation capabilities (i.e., meaningfully combining concepts that would be nearly impossible to combine in reality), such as \u201ca photo of an astronaut riding a house\u201d. Various generative models are open-sourced to the public, and users can use these models to generate realistic images in seconds. However, while people are appreciating the new format of art and flooding the Internet with fabulous images, such synthetic images are difficult to differentiate from the real ones. Therefore, these generated images are becoming a potential source of contamination for the future datasets collected from the Internet. Research has proven that synthetic data contamination may lead to a significant performance drop when supervising machine learning models in non-continual scenarios [18, 28]. Also, training deep models with such a contaminated dataset may give rise to bias amplification [8] and loss of diversity [28]. To tackle the issue caused by the synthetic contamination, researchers have proposed different strategies to detect the synthetic data with deep learning based detectors [30, 27]. However, the challenge brought by synthetic data contamination to the CL community is exclusive, and it is even more problematic in the online scenario since it is almost impossible to assess the quality of the training data, due to the unique challenge brought by the online setting. ", "page_idx": 1}, {"type": "text", "text": "Continual Learning. The mainstream CL strategies can be classified into four categories: regularization-based, parameter-isolation-based, prompt-based, and replay-based. Regularizationbased methods [7, 26, 1, 23, 46] design and apply extra regularization terms to balance the learning and forgetting of CL learners. Parameter-isolation-based methods [14, 36, 37, 35, 3] tackle the CL problem by allocating task-specific parameters. Prompt-based methods [44, 43] take the idea of prompt learning and use prompt pools against catastrophic forgetting. Replay-based methods [33, 5, 6, 16, 17, 45] store a small portion of historical data with a memory buffer. Among all of the strategies, replay-based methods have prevailed in Online CL with better performance and simplicity. Early work [33] proposed Experience Replay (ER), suggesting using a random replay buffer to alleviate catastrophic forgetting. Dark Experience Replay $({\\bf D}{\\bf E}{\\bf R}{+}{+})$ [5] proposes to store the logits in the memory buffer and leverage the stored logits as dark knowledge to extend ER. ER-ACE [6] is a variant of ER with asymmetric cross-entropy loss. OCM [16] alleviates catastrophic forgetting by maximizing the mutual information between current and past representations. GSA [17] addresses the cross-task class discrimination problem with gradient self-adaption. OnPro [45] solves the shortcut learning problem with online prototype learning. In this paper, these methods are used as baselines to assess the impact of synthetic data contamination. ", "page_idx": 1}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/bcaef2a172ac11eae3fd5bf370008b13d2a7818d944418ad96f008de3cb371cc.jpg", "img_caption": ["Figure 1: Overview of proposed ESRM framework for online CL. The proposed ESRM framework has two main components: Entropy Selection (ES) and Real-synthetic similarity Maximization (RM). Motivated by Obs. 2 and Obs. 3, ES is a buffer management strategy designed to use entropy as a criterion to select more real samples in the memory buffer, thereby alleviating catastrophic forgetting and performance degradation caused by the contamination. RM aims to bridge the embedding gap between synthetic and real data, as noted in Obs. 4, using a contrastive learning technique. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Synthetic dataset generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To simulate a dataset contaminated with synthetic data, we employed five diffusion-based models to generate synthetic counterparts of the original datasets. These twin datasets contain the same number of images and the same classes, while all images are synthetic. The models used in generation include Stable Diffusion XL [34], Stable Diffusion v1.4, Stable Diffusion v2.1, VQDM [15], and GLIDE [29]. The synthetic data contamination was simulated across four benchmark datasets used in online CL, including CIFAR-10 [24], CIFAR-100 [24], TinyImageNet [25], and ImageNet-100 [13, 20]. ", "page_idx": 2}, {"type": "text", "text": "The generation of synthetic twin datasets is guided by the category names of the original dataset. For each class, we devised a simple yet effective prompt. For instance, for the class \u201chelicopter\u201d, we employed the prompt \u201can image of a helicopter\u201d. After the generation by the diffusion model, we adjusted the image size to match that of the original dataset. ", "page_idx": 2}, {"type": "text", "text": "In the experiments, we design two different settings: a) all the synthetic twin datasets are generated from Stable Diffusion XL, one of the state-of-the-art generative models; and b) Synthetic images are generated with the aforementioned five diffusion models, with each model contributing $20\\%$ to the synthetic dataset. ", "page_idx": 2}, {"type": "text", "text": "For setting a), we denote the generated dataset as SDXL-C10, SDXL-C100, SDXL-Tiny, and SDXL$\\mathrm{In}100$ , respectively, while for setting b), we denote the generated dataset as Mix-C10, Mix-C100, and Mix-Tiny. Some examples of generated images and more detailed information about the synthetic twin dataset generation can be found in Appendix D.2. Notably, the generation results in Fig. 10 reveal the lack of diversity for synthetic data compared with the real data. ", "page_idx": 2}, {"type": "text", "text": "3.2 Simulation of synthetic data contamination ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To simulate the contamination by the synthetic data, we substitute a portion $P$ of the original dataset with its synthetic twin, where $P$ is the contamination ratio. We designate these contaminated datasets using specific notations. For example, we denote the CIFAR-100 contaminated by SDXL-C100 as C100/SDXL. More details about the simulation of contamination are included in Appendix D.3. In the following sections, we will investigate the effect of synthetic contamination with these simulated datasets. ", "page_idx": 2}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/7215d65de301897a044b3b593247fbfe1ea217add6bf0d4667977ddad2e9cee9.jpg", "img_caption": ["Figure 2: The entropy distribution of the training dataset produced by ER and OnPro on In-100/SDXL $\\langle P=5\\bar{0}\\%\\rangle$ ) at the end of the training. ", "Figure 3: T-SNE visualization of the memory data at the end of training on In100/SDXL $(P\\,=\\,50\\%)$ ). For clarity, only the first 10 classes are visualized. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Synthetic Data Contamination in Online CL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we explore the potential impact of synthetic data contamination on the existing online CL methods and exhibit specific properties of synthetic data. ", "page_idx": 3}, {"type": "text", "text": "4.1 Contamination as a cause of performance degradation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When dealing with synthetic data contamination, it is crucial to measure the impact of such contamination on existing methods. In that sense, we train ResNet-18 models in the online continual setting on the contaminated dataset C10/SDXL, C100/SDXL, Tiny/SDXL, and In-100/SDXL with representative online CL methods. We observed that as the contamination ratio $P$ increases, the performance of all existing methods drops significantly, as shown in Table 2. Detailed information about the experiment settings can be found in Sec. 6. With such experiments, we can form the following observation: ", "page_idx": 3}, {"type": "text", "text": "Observation 1 As a source of potential contamination, synthetic data is harmful to the performance of existing online CL methods. Performance degradation increases as contamination becomes more severe. ", "page_idx": 3}, {"type": "text", "text": "4.2 Detecting synthetic data matters ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Replay-based methods are characterized by the existence of a replay buffer, which helps alleviate forgetting and implicitly improves network plasticity [42]. The quality of data in the replay buffer intuitively influences network performance. Motivated by Obs. 1, we hypothesize that the presence of synthetic data in the replay buffer might degrade performance. We trained ER on the C100/SDXL dataset, with extra information on the samples\u2019 synthetic status (i.e., whether the image is real or synthetic). We em", "page_idx": 3}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/36d80c90168f128fdbf8ff1581841bfb94255e4e0660981e125095be19b8d341.jpg", "table_caption": [], "table_footnote": ["Table 1: The performance of ER with different memory strategies on C100/SDXL dataset, with different contamination ratio $P$ . "], "page_idx": 3}, {"type": "text", "text": "ployed two memory strategies: storing only real data in the replay buffer and storing only synthetic data. The results showed in Table 1 indicate that knowing the synthetic status and storing only real data in the replay buffer can achieve performance on par with the no-contamination scenario, even at high contamination ratios. For instance, at a contamination ratio of $P=80\\%$ , ER achieves an accuracy of $38.13\\%$ on C100/SDXL when only real images are stored in the replay buffer. This result is comparable to the accuracy achieved when training on the clean CIFAR-100 dataset $(38.70\\%)$ , which contains four times more real data. These findings also demonstrate the potential of synthetic models to enhance performance through data generation and augmentation in an online continual learning scenario. ", "page_idx": 3}, {"type": "text", "text": "4.3 Synthetic data properties ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Lower entropy distribution. One noteworthy characteristic of synthetic data is its lower entropy distribution compared to real data, as observed from the perspective of continual learners. Fig. 2 shows the entropy distribution produced by a representative method ER and a state-of-the-art method OnPro when trained on the contaminated dataset. The values in the histogram are calculated at the end of the training on the whole training dataset (In-100/SDXL, $P=50\\%$ ). From the figure, we can spot a salient distribution difference between synthetic data and real data. Moreover, the synthetic samples have a peak in entropy distribution close to 0. Extra entropy histogram with other baselines can be found in Appendix C.1. Thus, we conclude with another finding: ", "page_idx": 4}, {"type": "text", "text": "Observation 3 Compared with real data, synthetic data entropy tends to be lower. ", "page_idx": 4}, {"type": "text", "text": "Feature gap in the embedding space. We find another intriguing property of synthetic data in the feature embeddings. Fig. 3 shows the t-SNE [39] visualization of the memory data produced by ER and OnPro on the In-100/SDXL dataset. For clarity, we only visualize the embeddings of the first 10 classes. With the synthetic data contamination, the features of the synthetic samples are better clustered than the real data. The pattern of the clustering of synthetic data indicates the ease of classification, which is on par with the limited diversity of synthetic data (cf. Fig. 10), and the low entropy distribution (cf. Obs. 3). Moreover, the embeddings of real data are inferior and fail to align with the superior embeddings of synthetic samples, which explains the performance degradation of inference on real test datasets. Extra visualization produced by other baselines is illustrated in Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "Observation 4 With the limited diversity of synthetic data, the synthetic data are better clustered than the real data, leading to a misalignment in the embedding space between synthetic samples and real samples. Such misalignment likely contributes to performance deterioration. ", "page_idx": 4}, {"type": "text", "text": "5 Proposed Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Fig. 1 presents the main framework of our proposed ESRM to alleviate the performance degradation caused by synthetic data contamination. In this section, we introduce the two components of ESRM: Entropy Selection (ES) and Real-synthetic similarity Maximization (RM). Then, we explain the whole ESRM framework. ", "page_idx": 4}, {"type": "text", "text": "5.1 Entropy selection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The introduced ES aims to select more representative samples in the memory buffer. For replay-based methods, having high-quality samples in the memory buffer helps alleviate forgetting and achieve better overall performance. As per Obs. 2, selecting real images into the memory buffer can provide more representative and reliable features and therefore lead to better performance. Motivated by this, we propose ES, a memory management strategy. Guided by Obs. 3 (synthetic data has lower entropy distributions), the core idea of ES is to select more real samples in the memory buffer based on the entropy distribution of the current batch. ", "page_idx": 4}, {"type": "text", "text": "At the beginning of the training, ES initializes an empty buffer. When a new batch comes, ES drops $50\\%$ of the batch samples with lower entropy, and stores the remaining samples, along with their entropy of the prediction. Once the buffer is full, as shown in Fig. 4, ES takes four steps to replace the elements in the buffer. Firstly, ES drops $50\\%$ of low entropy samples in the incoming batch. Then, ES uses Reservoir Sampling [21, 40] to decide whether to keep or discard the remaining incoming samples. If Reservoir Sampling decides to keep the incoming sample, it will nominate a buffer sample. After that, ES checks the class of the nominated sample and chooses a sample of the same class with the lowest entropy in the memory buffer. And finally, ES replaces the chosen memory sample with the incoming sample, along with its entropy. Moreover, all entropy values in the memory buffer are updated by the current model\u2019s prediction at the end of each task. The pseudo-code of ES is given in Appendix B. ", "page_idx": 4}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/e33cd2a0751b9bc94253dc27e89882a656bfa7aec70d1918506b2eb808cf758c.jpg", "img_caption": ["Figure 4: Overview of the proposed Entropy Selection strategy. The color of the samples indicates the class, and the number in the samples represents the entropy predicted by the learner. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5.2 Real-synthetic similarity maximization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prior to introducing the loss function of RM, we present the network structures of our ESRM. As shown in Fig. 1, the continual learner consists of three components: a feature extractor $f$ , a projection head $g$ , and a classifier $\\phi$ . The output dimension of the projection head $g$ is set to 128. For each sample $x$ from incoming data stream $X^{n e w}$ , the projected embedding $z$ can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz=g(f(x)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As per Obs. 4, the gap in the embedding space might be a cause for the performance degradation in the contamination setting. To tackle this issue, we propose RM. The main idea of RM is to maximize the cosine similarity between the features of real and synthetic data. Motivated by the seminal supervised contrastive loss [22], we propose our variant RM loss. ", "page_idx": 5}, {"type": "text", "text": "RM aims to maximize the similarity between two groups of data $X_{1}$ and $X_{2}$ . We define the loss to match the similarity of samples in group $X_{1}$ to group $X_{2}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{M}(X_{1},X_{2})=\\sum_{i\\in I_{1}}\\frac{-1}{|P(i)|}\\sum_{p\\in P(i)}l o g\\frac{e x p(z_{i}\\cdot z_{p}/\\tau)}{\\sum_{d\\in I_{2}}e x p(z_{i}\\cdot z_{d}/\\tau)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $I_{1}=\\{i:x_{i}\\in X_{1}\\},I_{2}=\\{d:x_{d}\\in X_{2}\\}$ are the set of the indices of $X_{1},X_{2}$ , respectively. And $P(i)=\\{p\\in X_{2}:y_{p}=y_{i}\\}$ is the set of the indices of positive samples in group $X_{2}$ , which share the same class with $x_{i}$ . $\\tau$ is the temperature hyperparameter which is set to 0.07. Since $\\mathcal{L}_{M}(X_{1},X_{2})$ only optimize samples in the group $X_{1}$ , in the optimization, it is used together with $\\mathcal{L}_{M}(X_{2},X_{1})$ . Due to RM aims to maximize the inter-group similarity between the $X_{1}$ group and the $X_{2}$ group, we do not need to perform augmentations as shown in similar work, such as SimCLR [9] and SupCon [22]. Intuitively, the way to handle the similarity matrix is illustrated in Fig.1. ", "page_idx": 5}, {"type": "text", "text": "For an incoming batch $X^{n e w}$ , we use entropy criteria to split it into two groups $X_{+}^{n e w}$ and $X_{-}^{n e w}$ of the same size. Group $X_{+}^{n e w}$ includes $50\\%$ of the samples with the highest entropy, and as per Obs. 3, tend to contain more real images. On the contrary, group $X_{-}^{n e w}$ contains low entropy samples which tend to be synthetic. To alleviate the gap in feature embeddings mentioned in Obs. 4, we maximize the inter-group similarity between $X_{+}^{n e w}$ and $X_{-}^{n e w}$ with $\\mathcal{L}_{M}(X_{+}^{n e w},X_{-}^{n e w})$ and $\\mathcal{L}_{M}(X_{-}^{n e w},X_{+}^{n e w})$ . Moreover, to align the holistic feature embeddings between the stream data $X^{n e w}$ and memory data $X^{m e m}$ , we also applied $\\mathcal{L}_{M}(X^{n e w},X^{m e m})$ and $\\mathcal{L}_{M}(X^{m e m},X^{n e w})$ in the loss function. ", "page_idx": 5}, {"type": "text", "text": "Thus, the proposed RM to alleviate the feature gap in Obs. 4 can be achieved by employing ${\\mathcal{L}}_{R M}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{R M}=\\mathcal{L}_{M}(X_{+}^{n e w},X_{-}^{n e w})+\\mathcal{L}_{M}(X_{-}^{n e w},X_{+}^{n e w})+\\mathcal{L}_{M}(X^{n e w},X^{m e m})+\\mathcal{L}_{M}(X^{m e m},X^{n e w}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5.3 Overall framework of ESRM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The overall framework of ESRM is shown in Fig. 1. Besides ES and RM, following [42], ESRM employs a self-distillation technique to alleviate the overconfidence problem of the replay-based methods. For the combined batch $X=(X^{n e w},X^{m e m})$ , we apply the self-distillation as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{S D C}=D_{K L}\\big(\\phi(f(X))/t,\\overline{{\\phi(f(a u g(X)))}}/t\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D_{K L}(\\cdot)$ is the Kullback-Leibler divergence, $t$ is another temperature hyperparameter which is set to 4, $\\overline{{\\phi(f(a u g(X)))}}$ is the fixed copy of $\\phi(f(a u g(X)))$ , without gradient propagation, and $a u g(\\cdot)$ is the data augmentation used in the training process, with detailed information in Appendix D.5. ", "page_idx": 5}, {"type": "text", "text": "Thus, the total loss of ESRM can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{E S R M}=\\mathcal{L}_{C E}+\\lambda_{1}\\mathcal{L}_{S D C}+\\lambda_{2}\\mathcal{L}_{R M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}_{C E}=C E(\\phi(f(X)),y)+C E(\\phi(f(a u g(X))),y)$ is the cross-entropy loss, and $\\lambda_{1},\\lambda_{2}$ are the balancing hyperparamteres. We set $\\lambda_{1}=1$ and $\\lambda_{2}=0.5$ after a small hyperparameter search as illustrated in the Appendix D.6. ", "page_idx": 5}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/6c5ca4d518201bfdfda852faed82f3d0910d8cfa261b8612216a70f518eface2.jpg", "table_caption": [], "table_footnote": ["Table 2: Average Accuracy ( $\\%$ ; higher is better) on four benchmark datasets with different contamination ratios $P$ . Numbers in parentheses indicate the performance degradation due to contamination compared to the clean setting. The average and deviation over five runs are reported for ImageNet-100 and 10 runs for other datasets. "], "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Experiment setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. In the experiments, we used four benchmark datasets in evaluation, including CIFAR10/100, TinyImageNet, and ImageNet-100. All of the datasets are split into tasks containing nonoverlapping classes. The details about the task split are available in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We evaluate the effectiveness of ESRM against six representative and state-of-the-art baselines, including ER [33], $\\mathrm{DER++}$ [5], ERACE [6], OCM [16], GSA [17], and OnPro [45]. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We use full-width ResNet-18 (not pre-trained) as the backbone for all experiments. For a fair comparison, we conduct a hyperparameter search on CIFAR-100 (Memory $\\mathrm{Size}=5\\mathrm{K})$ ) and apply the same hyperparameter to all settings. Stream batch size is set to 10 and memory batch size is set to 64. We do not use multiple updates trick for incoming batches as detailed in [2]. Detailed information about task allocation, hyperparameter search protocol, and data augmentation can be found in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Buffer size. For CIFAR-10 and CIFAR-100 experiments, the buffer size is set to 1,000 and 5,000, respectively. For the harder TinyImageNet experiments, the buffer size is set to 10,000. The buffer size of ImageNet-100 is set to 5,000. Appendix C.5 demonstrates more experiments with different memory buffer sizes. ", "page_idx": 6}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/2c61890d1c531047acb320ee5c7f98a6a0ebc62f74bac7f50a017b9108781200.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/7fb0f637156a7e57708339bba36823767a90f7c4413d605455bd111fba5e1d92.jpg", "table_caption": ["Table 3: Learning Accuracy ( $\\%$ ; higher is better) on In-100/SDXL with various contamination ratio $P$ . "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/366c07d4bd3a40aa23ca0944cf50ff916dc3ee8fc19f30e36de2e40273b7b697.jpg", "table_caption": ["Table 4: Relative Forgetting ( $\\%$ ; lower is better) on In-100/SDXL with various contamination ratio $P$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Results and analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Final Average Accuracy. Table 2 shows the final average accuracy of learners trained with four datasets, including C10/SDXL, C100/SDXL, Tiny/SDXL, and In-100/SDXL, with different contamination ratios $P$ . More results on C10/Mix, C100/Mix, and Tiny/Mix are given in Appendix C.3. For the six baselines, we can notice a significant performance drop when synthetic data contamination appears. Notably, when the contamination is severe (contamination ratio $P\\geq70\\%$ ), the performance degradation is significant. Also, it can be observed that ESRM is less impacted by synthetic data contamination. For most datasets and contamination ratio $P$ , the ESRM performance drop is the lowest of the compared methods. This is remarkably true for large values of $P$ . ", "page_idx": 7}, {"type": "text", "text": "Apart from the robustness against synthetic data contamination, the absolute performance of ESRM is also attractive. In most cases, ESRM outperforms the baseline methods by a large margin. More interestingly, for some datasets, such as CIFAR-100 and ImageNet-100, even with an extreme contamination ratio, ESRM can still achieve a substantial performance, while the performance of most baseline methods is unsatisfactory. ", "page_idx": 7}, {"type": "text", "text": "Plasticity and Stability Metrics. We measure the model\u2019s plasticity and stability with Learning Accuracy (LA) [32] and Relative Forgetting (RF) [42], respectively. As shown in Table 3 and 4, for baseline methods, both plasticity and stability performance are hindered by synthetic data contamination. From the model plasticity perspective, ESRM alleviates the problem with a larger plasticity. For the model stability, ESRM solves the problem of stability degradation with the presence of contamination. It is notable that with RM loss and self-distillation loss, ESRM implicitly trades off some model stability in favor of plasticity. Plasticity and stability performance on other datasets are available in Appendix C.4. ", "page_idx": 7}, {"type": "text", "text": "Domain-Incremental Learning Results. While Class-Incremental Learning (CIL) setting is the standard evaluation protocol in online CL, we also evaluated the performance of ESRM in DomainIncremental Learning (DIL) scenarios. We conducted the experiment with the 20 coarse labels of the ", "page_idx": 7}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/f41d3b2f4146630f8fefe790ebceec094c0bd90179e5cfe8a2fe1ea4760a30fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 7: Final Average Accuracy $\\%$ ; higher is better) on DIL-C20/SDXL dataset. Numbers in parentheses indicate the performance degradation due to synthetic contamination compared to the clean setting. The average and deviation over 10 runs are reported. ", "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: T-SNE visualization of memory data produced by ESRM at the end of training on the In-100/SDXL $'P=50\\%$ ) dataset. For clarity, only the first 10 classes are visualized. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/1d380072370fbf13b133c9bd0f2972a4183c73b287e5a8012d267c497d491d81.jpg", "img_caption": ["Figure 6: The percentage of synthetic data in the memory buffer throughout the training of ESRM on the In-100/SDXL dataset with different contamination ratios $(P)$ . The average value of 5 runs is plotted. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "CIFAR-100 dataset. Since the 100 classes in CIFAR-100 are grouped into 20 superclasses with 5 finegrained classes for each superclass, we split the CIFAR-100 dataset with 5 domain increment steps. For each step, we feed the model with the training data of a fine-grained class from each superclass. Because the model only classifies 20 coarse labels, we refer to this dataset as DIL-CIFAR20. Similar to the simulated CIFAR100/SDXL dataset, we replace the images in the DIL-CIFAR20 dataset with its Stable Diffusion XL generated counterpart with a contamination ratio P, as per the protocol in Sec. 3.2. ", "page_idx": 8}, {"type": "text", "text": "Table 7 shows the final average accuracy with different contamination ratios. Notably, we adapted the CIL-specifically designed components in OnPro and GSA to the DIL scenario, and the performance suffered a decent loss. We did not report ERACE results because its Asymmetric Cross Entropy (ACE) loss converges to standard cross-entropy loss in the DIL scenario, making it equivalent to vanilla ER. The experimental results show that ESRM can yield robust performance against domain shift in the DIL setting, under different synthetic contamination situations, which validates the efficiency of ESRM under DIL settings. ", "page_idx": 8}, {"type": "text", "text": "6.3 Ablation studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effect of ES. To evaluate the effect of ES, we substitute ES with three different memory strategies: random sampling, storing real data only, and storing synthetic data only. Note that storing real or synthetic data requires knowing the ground truth of an image\u2019s synthetic status, which is not practical in realistic settings. Additionally, storing only real data is an idealized case for memory management, while storing only synthetic data represents the worst-case scenario. As shown in Table 5, both ES and random reservoir sampling [21, 40] outperform the worst-case scenario by a large margin. Moreover, ES outperforms the random sampling significantly. ", "page_idx": 8}, {"type": "text", "text": "Effects of loss terms. We also conduct experiments to verify the effects of loss terms in Eq. 5. As shown in Table 6, Both $\\mathcal{L}_{S D C}$ and ${\\mathcal{L}}_{R M}$ can benefit the final average accuracy of the classification. Furthermore, the combination of the two loss terms can further improve the final accuracy, validating that both terms complement each other. ", "page_idx": 8}, {"type": "text", "text": "7 Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The alleviation of feature misalignment. As mentioned in Obs. 4, baseline methods suffer performance degradation due to the misalignment between the inferior feature embedding of real images and the superior feature embedding of synthetic samples. Fig. 5 presents the t-SNE visualization of memory data at the end of training of ESRM on the In-100/SDXL dataset. Similar to Fig. 3, only the first 10 classes are visualized for clarity. Compared to ER and OnPro, the embeddings of synthetic and real samples in ESRM are better aligned, facilitated by the RM. ", "page_idx": 9}, {"type": "text", "text": "Training dynamics of memory buffer. To intuitively demonstrate the effect of ES, we visualized the percentage of synthetic data in the memory buffer throughout the whole training process. Fig. 6 displays the curve of the percentage of synthetic data in the memory buffer when the model is trained with ESRM on In-100/SDXL with different contamination ratios $P$ . To generate this curve, we checked the memory buffer every 10 iterations. As shown in the figure, the percentage of synthetic data is close to the contamination ratio $P$ in the early stages of training. As training progresses, the amount of synthetic data decreases. This trend intuitively illustrates the effect of ES in selecting real samples. Furthermore, we take a model trained with ESRM at the end of training and use the model\u2019s entropy as the criterion to categorize the synthetic status of samples in the training dataset and generate an ROC curve, as shown in Fig. 7. We regard real data as positive and use models trained with ESRM on In-100/SDXL $\\mathrm{\\Delta}P=50\\%$ ) to categorize the samples in the training set. The AUC of the ROC curve is 0.7098, showing the effect of the entropy criterion in discriminating real and synthetic samples. ", "page_idx": 9}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/b818e72a7673cfb9819b0d07d134e5370d784a40b94e74686cc7efd56085c9e2.jpg", "img_caption": ["Figure 7: The ROC curve of the model trained with ESRM on the In-100/SDXL dataset $'P=50\\%$ ) in predicting the synthetic status of samples in the training dataset. Real samples are regarded as positives and synthetic samples as negatives. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "With the widespread availability of advanced generative models, the prevalence of AI-generated images appears inevitable, posing a potential challenge for researchers attempting to collect datasets devoid of AI-generated content from the Internet. In this paper, we examine the potential side effects of AI-powered image generation on the continual learning community. First, we experimentally demonstrate that synthetic data has become a potential source of data pollution. We spot a catastrophic performance loss when the contaminated datasets are used to train continual learning models. Based on our experiments, we identify and summarize four typical characteristics of synthetic data when involved in the training of continual learners. Additionally, we propose ESRM, a method designed to alleviate performance deterioration, maintaining satisfactory performance even with highly contaminated datasets. Lastly, we hope our work highlights the need for improved regulation and systematic control over generated data, such as watermarking AI-generated content before publication. Internet data is a valuable resource accumulated over decades. We believe ensuring the integrity of Internet data is crucial for the future soundness of AI development. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018.   \n[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In NeurIPS, 2019.   \n[3] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, 2017.   \n[4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In NeurIPS, 2019.   \n[5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In NeurIPS, 2020.   \n[6] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky. New insights on reducing abrupt representation change in online continual learning. arXiv preprint arXiv:2104.05025, 2021. [7] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018. [8] Tianwei Chen, Yusuke Hirota, Mayu Otani, Noa Garcia, and Yuta Nakashima. Would deep generative models amplify bias in future models? arXiv preprint arXiv:2404.03242, 2024. [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, pages 1597\u20131607, 2020.   \n[10] Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 12(3):1\u2013207, 2018.   \n[11] Yubei Chen Chun-Hsiao Yeh. IN100pytorch: Pytorch implementation: Training resnets on imagenet-100. https://github.com/danielchyeh/ImageNet-100-Pytorch, 2022.   \n[12] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. TPAMI, 44(7):3366\u20133385, 2021.   \n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255, 2009.   \n[14] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.   \n[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages 10696\u201310706, 2022.   \n[16] Yiduo Guo, Bing Liu, and Dongyan Zhao. Online continual learning through mutual information maximization. In ICML, pages 8109\u20138126, 2022.   \n[17] Yiduo Guo, Bing Liu, and Dongyan Zhao. Dealing with cross-task class discrimination in online continual learning. In CVPR, pages 11878\u201311887, 2023.   \n[18] Ryuichiro Hataya, Han Bao, and Hiromi Arai. Will large-scale generative models corrupt future datasets? In ICCV, pages 20555\u201320565, 2023.   \n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, volume 33, pages 6840\u20136851, 2020.   \n[20] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In CVPR, pages 831\u2013839, 2019.   \n[21] David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In AAAI, volume 32, 2018.   \n[22] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.   \n[23] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521\u20133526, 2017.   \n[24] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.   \n[25] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[26] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In NeurIPS, 2017.   \n[27] Huan Liu, Zichang Tan, Chuangchuang Tan, Yunchao Wei, Jingdong Wang, and Yao Zhao. Forgery-aware adaptive transformer for generalizable synthetic image detection. In CVPR, pages 10770\u201310780, 2024.   \n[28] Gonzalo Mart\u00ednez, Lauren Watson, Pedro Reviriego, Jos\u00e9 Alberto Hern\u00e1ndez, Marc Juarez, and Rik Sarkar. Towards understanding the interplay of generative artificial intelligence and the internet. In International Workshop on Epistemic Uncertainty in Artificial Intelligence, pages 59\u201373, 2023.   \n[29] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, volume 162, pages 16784\u201316804, 2022.   \n[30] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In CVPR, pages 24480\u201324489, 2023.   \n[31] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019.   \n[32] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018.   \n[33] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In NeurIPS, 2019.   \n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022.   \n[35] Amir Rosenfeld and John K Tsotsos. Incremental learning through deep adaptation. TPAMI, 42(3):651\u2013663, 2018.   \n[36] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.   \n[37] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, 2018.   \n[38] Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.   \n[39] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 9(11), 2008.   \n[40] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 11(1):37\u201357, 1985.   \n[41] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application. arXiv preprint arXiv:2302.00487, 2023.   \n[42] Maorong Wang, Nicolas Michel, Ling Xiao, and Toshihiko Yamasaki. Improving plasticity in online continual learning via collaborative learning. arXiv preprint arXiv:2312.00600, 2023.   \n[43] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In ECCV, pages 631\u2013648, 2022.   \n[44] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In CVPR, pages 139\u2013149, 2022.   \n[45] Yujie Wei, Jiaxin Ye, Zhizhong Huang, Junping Zhang, and Hongming Shan. Online prototype learning for online continual learning. In ICCV, pages 18764\u201318774, 2023.   \n[46] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Algorithm 1 PyTorch-like pseudo-code of ES. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "# model: continual learning model.   \n# criteria(): loss function as in Eq. 5.   \n# n_seen_so_far: count of images seen by the buffer.   \n# random(): function returns random values in (0,1).   \n# buffer_labels: 1D Tensor of size [buffer_size] storing the labels.   \n# buffer_ent: 1D Tensor of size [buffer_size] storing the entropy values.   \n# update_ent(): Update all entropy values in the buffer.   \nn_seen_so_far $=~0$   \nfor t in tasks: for img, label in dataloader: # train the network with stream data model.train() mem_img, mem_label $=$ mem_sample() c_img, c_label $=$ concat((img, mem_img), (label, mem_label)) # combined batch loss $=$ criteria(model(aug(c_img)), c_label) # Eq. 5. loss.backward() optimizer.step() # ES updates # calculate the entropy criteria of stream data model.eval() logits $=$ model(img) prob $=$ softmax(logits) entropy $=$ -torch.sum(prob $^\\ast$ torch.log(prob)) # update buffer threshold $=$ torch.quantile(entropy, 0.5) # Step 1 in ES stream_img $=$ img[entropy $>$ threshold] stream_label $=$ label[entropy $>$ threshold] stream_entropy $=$ entropy[entropy $>$ threshold] for x, y, ent in zip(stream_img, stream_label, stream_ent): nominate $=$ int(random() $^\\ast$ (n_seen_so_far $+\\ 1;$ )) # Step 2 in ES if n_seen_so_far $<$ buffer_size: # if the buffer is not full nominate $=$ n_seen_so_far replace_data(nominate, x, y, ent) n_seen_so_far $+=~1$ elif nominate $<$ buffer_size: nominate_class $=$ buffer_labels[nominate] # Step 3 in ES idx $=$ buffer_ent[buffer_labels $==$ nominate_class].argmin() replace_data(idx, x, y, ent) # Step 4 in ES n_seen_so_far $+=~1$ update_ent() ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The paper investigates the potential impact of synthetic data contamination on online CL research and proposes a method to alleviate side effects caused by the contamination. Nevertheless, our research has some limitations. Firstly, in the evaluation, we only use five generative models, including Stable Diffusion v1.4, Stable Diffusion $\\times2.1$ , Stable Diffusion XL, VQDM, and GLIDE. Other excellent commercial generative works, such as Midjourney and DALL-E, are not included in the data generation. With limited computation/resources, we could not exhaust all generative methods. ", "page_idx": 13}, {"type": "text", "text": "Secondly, the way we produce the generated dataset is simple: we use prompts like \u201can image of a <class_name>\u201d in the generation. In reality, the users\u2019 prompts are usually more diverse. Some works use LLMs to simulate more realistic prompts, where we leave a further in-depth analysis for future research. ", "page_idx": 13}, {"type": "text", "text": "B Pseudo code for ES ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The PyTorch-like pseudo code showing how the ES updates the memory buffer is shown in Alg. 1. ", "page_idx": 13}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/56f8b2dfd55394b270cacb8d1515c463d6607846e0f0d7dc2e76a5ea9c144259.jpg", "img_caption": ["Figure 8: The entropy distribution of the training set produced by all methods on In-100/SDXL $P\\bar{=}\\,50\\%$ at the end of the training. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/0acb45619bf7b9ec85a5d37ccbc2560b11cceeed8028972ac60ac379f948d14b.jpg", "img_caption": ["Figure 9: T-SNE visualization of the memory data at the end of training on In-100/SDXL $'P=50\\%$ ). For clarity, only the first 10 classes are visualized. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Extra Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Entropy distribution of other baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As mentioned in Sec. 4, we show the entropy distribution produced by other baselines, when trained on In-100/SDXL $'P=50\\%$ ). Fig. 8 illustrates the entropy distribution histogram, which is calculated at the end of the training on the whole contaminated dataset. Similar to the result in Fig. 2, the entropy distribution of the synthetic data is saliently lower than the entropy distribution of the real data. ", "page_idx": 14}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/a74ee517455c9311670da8e80b042592dc54e08c85e9c9342fdf46676395f594.jpg", "table_caption": [], "table_footnote": ["Table 8: Average Accuracy ( $\\%$ ; higher is better) on four benchmark datasets with different contamination ratios $P$ . Numbers in parentheses indicate the performance degradation due to contamination compared to the clean setting. The average and deviation over 10 runs are reported. "], "page_idx": 15}, {"type": "text", "text": "C.2 T-SNE visualization of other baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We show additional experiments of t-SNE visualization of memory data produced by other baseline methods. As shown in Fig. 9, we visualize the memory embeddings of different baselines on the In-100/SDXL $'P=50\\%$ ) dataset. For clarity, we only visualize the first 10 classes. Similar to the results in Fig. 3, the synthetic data are better clustered compared with the real data. This proves the Obs. 4. ", "page_idx": 15}, {"type": "text", "text": "C.3 Performance on C10/Mix, C100/Mix and Tiny/Mix Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As mentioned in Sec. 6, we would like to include the experiment results on C10/Mix, C100/Mix, and Tiny/Mix datasets. The final average accuracy of different methods on the dataset is included in Table 8. Similar to the results in Table 2, ESRM has less performance degradation and better performance in most cases. ", "page_idx": 15}, {"type": "text", "text": "C.4 Plasticity and stability performance on other datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As mentioned in Sec. 6.2, the plasticity and stability metrics of methods on other datasets are demonstrated in Table 9 and 10. Similar to the results in Table 3 and 4, in most settings, the plasticity metric (LA) and stability metric (RF) of baseline methods drop with an increased contamination ratio $P$ . For the model plasticity, ESRM alleviates the issue with a larger plasticity. From a stability perspective, ESRM addresses the issue of stability degradation with the presence of contamination. ", "page_idx": 15}, {"type": "text", "text": "C.5 The impact of buffer size. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Sec. 6.2, we evaluate the effectiveness of ESRM in a limited buffer size setting. To evaluate our methods\u2019 scalability against different buffer sizes $M$ and contamination ratio $P$ , we compare the accuracy of different methods on C100/SDXL with different buffer sizes $M$ , as shown in Table 11. Similar to the results in Table 2, ESRM can obtain better performance when the dataset is contaminated with synthetic data. ", "page_idx": 15}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/1580f4b1e05a471236baf09f929b22ce1a5e064e4b6ae919f3a7605bdf2444f3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/9c45e32a6f02c623f0b376f37614ed1290360f54bf16871b7370c11575c3651d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/8bd392f64a4073913c334950e3d60ce5ed07fe8cccb7a39f5cf739335b8039be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 11: Final Average Accuracy ( $\\%$ ; higher is better) on C100/SDXL dataset with different memory size $M$ and contamination ratio $P$ . Numbers in parentheses indicate the performance degradation due to contamination compared to the clean setting. The average and deviation over 10 runs are reported. ", "page_idx": 18}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/5beb5b479806f047328bf67cd8a29362222c2cd353aa4cf36dde2bb9b306ea42.jpg", "img_caption": ["Figure 10: Random sampled images from class \u201cn01558993\u201d (Robin) in SDXL-In100 and original ImageNet-100 dataset. For clarity, we have cropped some backgrounds and resized the vanilla ImageNet-100 samples. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Dataset. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As discussed in Sec. 3, we used four benchmark datasets in evaluation, including CIFAR-10/100, TinyImageNet, and ImageNet-100. In the experiments, all of the datasets are split into tasks containing non-overlapping classes. The details about the task split are as follows: ", "page_idx": 18}, {"type": "text", "text": "CIFAR-10 [24] has ten classes with 50,000 training images and 10,000 test images. The image is $32\\times32$ in size. The dataset is split into five disjoint tasks with two classes per task. ", "page_idx": 18}, {"type": "text", "text": "CIFAR-100 [24] has 100 classes with 50,000 training samples and 10,000 test samples. Image size is $32\\times32$ . It is split into 10 disjoint tasks with 10 classes per task. ", "page_idx": 18}, {"type": "text", "text": "TinyImageNet [25] has 200 classes, 100,000 training samples, and 10,000 test samples. Image size is $64\\times64$ . The dataset is split into 100 non-overlapping tasks with two classes per task. ", "page_idx": 18}, {"type": "text", "text": "ImageNet-100 [20] is a subset of ImageNet-1k [13] dataset. It consists of 100 classes. We follow [11] for the class selection. We do not perform image resizing in generating ImageNet-100 from the original ImageNet-1k dataset. The dataset is split into 10 disjoint classes with 10 classes per task. ", "page_idx": 18}, {"type": "text", "text": "D.2 Details about synthetic dataset generation. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Image size of SDXL-IN100. In the synthetic dataset generation, we manually adjust the size of generated images to match that of the original dataset. For the ImageNet-100 dataset, since the image size is not fixed, we resize the generated images to $224\\times224$ , to align with the training protocols. ", "page_idx": 18}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/6d914b38329556fa516081cab8d2744956eb3569bd020d94e19d682a94e1b522.jpg", "table_caption": ["Table 12: Hyperparameters used in image generation. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Class-wise distribution of sources for Mix-C10/C100/Tiny dataset. In the main manuscript, we mentioned that the dataset of setting b) in Sec. 3.1 is generated from five synthetic models: Stable Diffusion v1.4, Stable Diffusion v2.1, Stable Diffusion XL, VQDM, and GLIDE. Each method contributes $20\\%$ of the dataset in setting b). In our implementation, we ensure that this distribution is consistent across all classes in the dataset, so that each class has an equal number of images from each generation model. ", "page_idx": 19}, {"type": "text", "text": "Hyperparameters used in image generation. For Stable Diffusion and VQDM, we use source code and model snapshots from huggingface, as mentioned in Table 14. For Glide experiments, we use the official implementation and the released model snapshots. Following the recommendation, we use the refiner in Stable Diffusion XL and the upsampler in GLIDE. The diffusion steps and guidance scale hyperparameters we used are shown in Table 12. For other hyperparameters, we follow the recommendations from Huggingface and GLIDE\u2019s official implementation. We use the prompt \"An image of a class_name.\" as the text guidance to generate the image and interpolate the generated image to the size of the target dataset (32 for CIFAR, 64 for TinyImageNet, and 224 for ImageNet-100). ", "page_idx": 19}, {"type": "text", "text": "Samples from the generated datasets. Fig. 10 shows some samples from class \u201cn01558993\u201d (Robin) in the SDXL-In100 dataset along with the samples from the original dataset. We can notice a significant loss of diversity in the samples from SDXL-In100. ", "page_idx": 19}, {"type": "text", "text": "D.3 Details about synthetic contamination simulation. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As mentioned in Sec. 3.2, we generate synthetic twins of benchmark datasets and substitute a fixed portion $P$ of the original datasets with their synthetic counterparts. Similar to the class-wise distribution of Mix-C10/C100/Tiny, we also conduct the mixture class-wise. For datasets in setting a), we assure that the contamination ratio in each class is also $P$ . For datasets in setting b), while maintaining a consistent class-wise contamination ratio, we also ensure that each individual synthetic model contributes $20\\%$ of the contamination in each class. ", "page_idx": 19}, {"type": "text", "text": "D.4 Task sequence. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In some work, the authors use a fixed task sequence for fair comparison. However, the final performance is largely affected by the task order. For fair comparison, we randomly assign the class to tasks and shuffle the sequence of tasks with 10 fixed random seeds. This ensures the evaluation is not biased to task difficulty. ", "page_idx": 19}, {"type": "text", "text": "D.5 Data augmentation. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Data augmentation is effective in boosting the training of online continual learners. Methods may benefit differently from different augmentation intensities, and some methods may favor simpler augmentations instead of complicated ones. For a fair comparison, it is vital to ensure all methods are in their optimal performance. Thus, we introduce two different augmentation strategies: ", "page_idx": 19}, {"type": "text", "text": "1. Partial strategy. The partial augmentation is a weaker version of augmentation, consisting of random cropping with $p=0.5$ , followed by random horizontal flip with $p=0.5$ . ", "page_idx": 19}, {"type": "text", "text": "2. Full strategy. The full augmentation strategy is a stronger version of augmentation. The full augmentation strategy is a superset of its partial counterpart, which consists of random cropping, random horizontal flip, color jitter, and random grayscale. The parameters for color jitter are set to $(0.4,0.4,0.4,0.1)$ with $p=0.8$ . The probability of random grayscale is set to $p=0.2$ . ", "page_idx": 19}, {"type": "text", "text": "We define the data augmentation strategy of each method with a hyperparameter search, as detailed in Appendix D.6. ", "page_idx": 20}, {"type": "text", "text": "D.6 Hyperparameter search protocol. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For hyperparameters in all of the methods (except $\\mathrm{DER++}$ on TinyImageNet), we conduct a hyperparameter search on the clean CIFAR-100 dataset with a memory size of 5,000, and apply the same hyperparameter to all of the other settings. The exhaustive list of the hyperparameter search is shown in Table 13. ", "page_idx": 20}, {"type": "text", "text": "Special treatment for $\\mathbf{D}\\mathbf{E}\\mathbf{R}{+}{+}.$ . $\\mathrm{DER++}$ encounters a catastrophic performance defect (close to 0) when trained on the TinyImageNet dataset using an optimizer with momentum. Thus, we applied another hyperparameter search for $\\mathrm{DER++}$ on TinyImageNet and found the SGD optimizer (without Momentum) gives reasonable performance. All the experiments of $\\mathrm{DER++}$ on TinyImageNet are conducted using these new hyperparameters. ", "page_idx": 20}, {"type": "text", "text": "D.7 Hardware and computation. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "All of the experiments are conducted on NVIDIA A100 GPUs. The average training time of each method on CIFAR-100 (Memory size $=5\\mathrm{k}$ ), ImageNet-100 (Memory size $=5\\mathrm{k}$ ), and TinyImageNet (Memory size $=10\\mathbf{k}$ ) is shown in Fig. 11. The training efficiency is much faster than OCM and OnPro, while on par with the most efficient method. ", "page_idx": 20}, {"type": "text", "text": "D.8 Useful source code links. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For continual learning baselines, we use the codebase listed in Table 14 to reimplement baseline methods. For image generation methods, we use the Diffuser library from Hugging Face for Stable Diffusion and VQDM experiments, and we use the codebase in Table 14 for GLIDE. ", "page_idx": 20}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/ef821fdc156f0b755155e3d478fe214bf342b1e7b45225b88a1bac69615ff8b7.jpg", "img_caption": ["Figure 11: The average training time of each method trained on CIFAR-100 $\\scriptstyle(\\mathrm{M}=5\\mathrm{k})$ ), IN-100 $\\scriptstyle(\\mathrm{M}=5\\mathrm{k})$ ), and TinyImageNet $(\\mathbf{M}{=}10\\mathbf{k})$ ) dataset. For better readability, the values are plotted on the logarithm scale. The numbers are averaged from 10 runs. "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "Lc8gemv97Y/tmp/5b93f12c5bc01e5d78312bbac0fbfa69e25b6b0ad35f67da3d30695e16e941da.jpg", "table_caption": ["Table 14: Baselines and their source code URLs. "], "table_footnote": ["Table 13: Exhaustive list of hyperparameters searched on CIFAR-100. "], "page_idx": 21}, {"type": "image", "img_path": "Lc8gemv97Y/tmp/0df82f3503a32752c5b47da913fc7ad99554e9f0b134fd231105daa11799caa5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All the main claims made in both the abstract and introduction can accurately reflect the paper\u2019s contribution. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The limitation section is included in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The source code of our work, along with the result of the hyperparameter search are included in the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The code is included in the supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The main experimental setting is included in the paper, and all of the details are included in the source code in the supplementary material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Error bars are reported in the experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 24}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Detailed information is included in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The Code of Ethics is fully respected and obeyed in our research. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not pose such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have cited the used assets in the appendix. The licenses of existing assets are properly respected. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have a Readme file along with our source code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]