[{"type": "text", "text": "Non-parametric classification via expand-and-sparsify representation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kaushik Sinha   \nSchool of Computing   \nWichita State University   \nWichita, KS 67260   \nkaushik.sinha@wichita.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In expand-and-sparsify (EaS) representation, a data point in $S^{d-1}$ is first randomly mapped to higher dimension $\\mathbb{R}^{m}$ ,where $m\\,>\\,d$ , followed by a sparsification operation where the informative $k\\ll m$ of the $m$ coordinates are set to one and the rest are set to zero. We propose two algorithms for non-parametric classification using such EaS representation. For our first algorithm, we use winners-take-all operation for the sparsification step and show that the proposed classifier admits the form of a locally weighted average classifier and establish its consistency via Stone's Theorem. Further, assuming that the conditional probability function $P(y=1|x)=\\eta(x)$ is Holder continuous and for optimal choice of $m$ ,we show that the convergence rate of this classifier is minimax-optimal. For our second algorithm, we use empirical $k$ -thresholding operation for the sparsification step, and under the assumption that data lie on a low dimensional manifold of dimension $d_{0}\\ll d$ , we show that the convergence rate of this classifier depends only on $d_{0}$ and is again minimax-optimal. Empirical evaluations performed on real-world datasets corroborate our theoretical results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given a training set $x_{1},\\ldots,x_{n}$ of size $n$ and a test point $x$ sampled i.i.d. from a distribution $\\mu$ over some sample space $\\mathcal{X}\\,\\subset\\,\\mathbb{R}^{d}$ , the basic idea of non-parametric estimation (e.g., density estimation, regression or classification) is to construct a function $x\\mapsto f_{n}(x)=f_{n}(x,x_{1},\\ldots,x_{n})$ that approximates the true function $f$ (which could be a density/regression/classification function as appropriate) as $n\\to\\infty$ Tsybakov [2008]. For any $x\\in\\mathscr{X}$ \uff0c $f_{n}(\\bar{x})$ typically depends on a small subset of the training set lying within a small neighborhood of (thus close to) $x$ . For example, in case of $k$ -nearest neighbor, $f_{n}(x)$ depends on the $k$ points from the training set that are closest to $x$ ; in case of random forest, for each tree constituting the forest, $f_{n}(x)$ depends on the points from the training set that are routed to the same leaf node to which $x$ is also routed to; in case of kernel methods, $f_{n}(x)$ depends on the points from training set defined by an appropriate kernel. In this paper, we propose a new non-parametric classification method where, for any $x\\in S^{d-1}$ , the appropriate neighborhoods are obtained using expand-and sparsify (EaS) representation of $x$ ", "page_idx": 0}, {"type": "text", "text": "On a high level, expand-and-sparsify representation is essentially a transformation from a lowdimensional dense representation of sensory inputs to a much higher-dimensional, sparse representation. Such representation has been found, for instance, in the olfactory system of the fy (Wilson [2013]) and mouse (Stettler and Axel [2009]), the visual system of the cat (Olshausen and Field [2004]), and the electrosensory system of the electric fish (Chacron et al. [2011]). For example, in the olfactory system of Drosophila (Turner et al. [2008], Masse et al. [2009], Wilson [2013], Caron et al. [2013]), the primary sense receptors of the fly are the roughly 2,500 odor receptor neurons (also known as, ORNs), which can be clustered into 50 types, based on their odor responses, leading to a dense, 50-dimensional sensory input vector. In fact, all ORNs of a given type converge on a corresponding glomerulus in the antennal lobe; there are 50 of these in a topographically fixed configuration. This information is then relayed via projection neurons to a collection of roughly 2000 Kenyon cells (KCs) in the mushroom body, with each KC receiving signal from roughly 5-10 glomeruli (Dasgupta and Tosh [2020]. The pattern of connectivity between the glomeruli and Kenyon cells appears random (Chacron et al. [2011]). The output of the KCs is integrated by a single anterior paired lateral (APL) neuron which then provides negative feedback causing all but the $5\\%$ highestfiring KCs to be suppressed (Lin et al. [2014]). The result is a random sparse high-dimensional representation of the sensory input, that is the basis for subsequent learning. The primary motivation of this paper is to study the benefit of this type of naturally occurring representation in the context of supervised classification. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In our setting, the EaS representation is a mapping from the $d$ dimensional unit sphere $S^{d-1}$ to $\\{0,1\\}^{m},\\stackrel{\\bar{m}}{m}\\ \\gg\\ d,$ where a data point $x\\in S^{\\check{d}-1}$ is first randomly mapped to higher dimension $\\mathbb{R}^{m}$ using a random projection matrix $\\Theta$ , and is followed by a sparsification operation, where the informative $k~\\ll~m$ of the $m$ coordinates of the resulting vector $\\Theta x\\ \\in\\ \\mathbb{R}^{m}$ are set to one and the rest of the coordinates are set to zero. Rows of $\\Theta$ are typically drawn independently from some distribution $\\nu$ over $S^{d-1}$ or $\\mathbb{R}^{d}$ Let $C_{j},j~=~1,\\ldots,m$ ,be the set of all examples from the input space $\\mathcal{X}~\\subset~S^{d-1}$ whose $j^{t h}$ coordinate in respective EaS representations are set to 1. We call $C_{j}$ the response region of $\\theta_{j}$ (the $j^{t h}$ row of $\\Theta$ ", "page_idx": 1}, {"type": "text", "text": "Note that for any $x\\in\\mathscr{X}$ , there are $k$ activated response regions corresponding to the $k$ nonzero bits in the EaS representation of $x$ . These $k$ activated response regions serves as $k$ neighborhoods of $x$ for our proposed non-parametric classifier, where, for any $x\\in\\mathscr{X}$ and each activated response region $C_{j}$ ,we estimate $\\operatorname*{Pr}(y=1|C_{j})$ - expected value of $y$ when $x$ is restricted to $\\bar{C}_{j}$ - and take their average to be the estimate of $\\operatorname*{Pr}(y\\,=\\,1|x)$ . In a toy example, we visually show the EaS representation and the activated response regions $C_{j}$ of a data point in Fig. 1. Comparing whether this conditional probability estimate exceeds $1/2$ , we predict the class label of $y$ . In particular, we present two algorithms using different sparsification schemes and analyze their theoretical properties. ", "page_idx": 1}, {"type": "text", "text": "One may find similarity between our proposed algorithm and a random forest classifier - for any $x\\in\\textbf{\\textit{x}}$ $k$ activated response regions $C_{j},x\\in C_{j}$ , may be interpreted as the leaf nodes (containing $x$ )of $k$ decision trees in a random forest. However, unlike random forest, we can not simply increase $k$ (number of trees) without changing other hyper-parameters (such as $m$ ) hoping to achieving better prediction perfor", "page_idx": 1}, {"type": "image", "img_path": "0d50Il6enG/tmp/b9aaedefe2fd4e5cdd90b12cf56392266e227e8661fd09861c9a349c374146ac.jpg", "img_caption": ["Figure 1: Top: A point $x\\in\\mathbb{R}^{2}$ (coordinate-wise values are different shades of gray) and its projection $\\boldsymbol{y}\\,=\\,\\Theta\\boldsymbol{x}\\,\\in\\,\\mathbb{R}^{15}$ (coordinate-wise values are different shades of red). The sparsification step sets the largest 5 values of $y$ to 1 (black squares) and the rest to zero. Bottom: Activated response regions $C_{j},x\\,\\in\\,C_{j}$ $x$ is a red dot), are shown using different colors. The points from the training set that intersects with these activated response regions are shown using black dots. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "mance. Therefore, even-though the consistency property of random forest is studied under different settings Biau et al. [2008], Scornet et al. [2015], Scornet [2016], Tang et al. [2018], those ideas can not be applied for our theoretical analysis. ", "page_idx": 1}, {"type": "text", "text": "We summarize our contributions below: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We present an interesting connection between non-parametric estimation and EaS representation, and propose two algorithms for non-parametric classification via EaS representation, and present empirical evaluations on benchmark machine learning datasets. \u00b7 For our first algorithm (using $k$ -WTA sparsification), we establish its universal consistency and prove that it achieves minimax-optimal convergence rate that depends on dimension $d$ \u00b7 When data lie on a low dimensional manifold having dimension $d_{0}\\ll d$ , we present a second algorithm (using empirical $k$ -thresholding sparsification) and prove that its convergence rate is minimax-optimal and depends only on $d_{0}$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. We discuss related work in 2. We present our first algorithm and its theoretical analysis including consistency and convergence rate in section 3. We present our second algorithm and derive its convergence rate in section 4. We present our empirical results in section 5 and conclude discussing limitations and future work in section 6. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Non-parametric estimation is an important branch of statistics with a rich history and classical results. Interested readers may refer to Tsybakov [2008], Gyorfi et al. [2002], which provide excellent treatment of this subject. Here we briefly review consistency and convergence rates of well-known non-parametric methods such as, partitioning estimates (histograms), $k$ -nearest neighbors, kernel methods, and random forests. In the non-parametric literature, it is typical to estimate the regression function $\\eta(x)\\,=\\,\\mathbb{E}(y|x)$ from data, and use the resulting estimate $\\eta_{n}$ (using a sample of size $n$ ) to construct plug-in decision function (classification rule). Under mild conditions, such regression estimates and the resulting plug-in classification rules are consistent for histograms, $k^{\\th}$ nearest neighbors, and kernel methods Gyorfi et al. [2002], Devroye et al. [1996]. Under mild conditions, different variations of random forest methods are also known to be consistent Biau et al. [2008], Scornet et al. [2015], Scornet [2016], Tang et al. [2018]. Under the assumption that the regression function $\\eta(x)$ is Lipschitz continuous, the $L_{2}$ error of the regression estimate of histogram convergences at a rate $O\\left(n^{-1/(d+2)}\\right)$ (Theorem 4.3 Gyorfi et al. [2002], the $L_{2}$ error of the regression estimate of the kernel method convergence at a rate $O\\left(n^{-1/(d+2)}\\right)$ (Theorem 5.2 Gyorfi et al. [2002]), and the $L_{2}$ error of the regression estimate of $k$ -nearest neighbors method convergences at a rate $O\\left(n^{-2/(d+2)}\\right)$ (Theorem 6.2 Gyorfi et al. [2002]). For random forest, Gao and Zhou [2020] established finite-sample rate $O(n^{-1/(8d+2)})$ on the convergence of pure random forests for classfication, which was be improved to be of $O(n^{-1/(3.87d+2)})$ by considering the midpoint splitting mechanism. They introduced another variant of random forests, which follow Breiman's original random forests but with different mechanisms for splitting dimensions and positions, to get a convergence rate $O(n^{-1/(d+2)}(\\ln n)^{1/(d+2)})$ , which reaches the minimax rate, except for a factor $(\\ln n)^{1/(d+2)}$ ", "page_idx": 2}, {"type": "text", "text": "For EaS representation, when $\\nu$ is the uniform distribution over $S^{d-1}$ and the sparsification scheme is a $k$ -winners-take-all ( $k$ -WTA) operation that sets the $k$ largest entries of a vector in $\\mathbb{R}^{m}$ to one the rest to zero, Dasgupta and Tosh [2020] proposed a function approximation scheme using such EaS representation that can approximate any Lipschitz continuous function $f:S^{d-1}\\to\\mathbb{R}$ in the $L_{\\infty}$ norm, where the error decays exponentially slowly with $d$ . Further, they showed when the data lie on a low dimensional submanifold of $S^{d-1}$ having dimension $d_{0}\\ll d$ using a different sparsification step, termed as $k$ -thresholding, any Lipschiz continuous function defined on this manifold can be approximated in the $L_{\\infty}$ norm, where the error decays exponentially slowly with $d_{0}$ . A different EaS representation, where the projection matrix is a sparse binary matrix and the sparsification step is $k$ -WTA, was proposed in Dasgupta et al. [2017] that effectively hash input data points and such hashing has been shown to provide improved performance in accurately solving the similarity search problems compared to the state-of-the-art locality sensitive hashing (LSH) based techniques (Gionis et al. [1999], Andoni and Indyk [2008], Datar et al. [2004]). Such EaS representation has also been used to summarize data in the form of a bloom filter, termed as fly bloom filter (FBF), and has been successfully used in solving the novelty detection problems in Dasgupta et al. [2018] and classification problem in Sinha and Ram [2021], Ram and Sinha [2021, 2022]. ", "page_idx": 2}, {"type": "text", "text": "While our work is inspired by the results of Dasgupta and Tosh [2020], there are key differences. First, we explicitly expand upon and apply the idea of Dasgupta and Tosh [2020] to the supervised binary classification setting, and derive the rate at which the error probability of our proposed classifier converges to that of the Bayes optimal classifier. In comparison, the main motivation of Dasgupta and Tosh [2020] was to prove the existence of a weight vector that results in arbitrarily well function approximation in the unsupervised learning setting using EaS representation. Because of this existential nature of their result, the effect of sample size was neither needed nor considered in their result. Second, the $k$ -thresholding sparsification scheme proposed in Dasgupta and Tosh [2020] for function approximation result assumed that the coordinate-wise thresholds were known. We make no such assumption and explicitly describe how to compute such thresholds from dataresulting in realizable algorithm and derive convergence rate of our proposed classifier that takes care of the uncertainly associated with random natures of these thresholds. Third, while Dasgupta and Tosh [2020] only considers LipSchitz continuous functions, we consider that conditional probability function $\\eta(x)=\\operatorname*{Pr}(y=1|x)$ to be Holder continuous (a broader function class), and prove that our proposed classifier achieves minimax-optimal convergence rate - whether such optimal convergence rate was achievable in the proposed setting was, unknown prior to our result. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Finally, we note that random Fourier features Rahimi and Recht [2007] are generated using a construction similar to EaS, however the choice of random directions there are chosen using a kernel function that measures a notion of similarity in the input space. EaS representation can also be though of an opposite process of compressed sensing Candes et al. [2006], Donoho [2006], where the goal is to recover a sparse vector given random projections to it, while random projection is used to generate a sparse representation in case of EaS. ", "page_idx": 3}, {"type": "text", "text": "3 Algorithm 1 ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "0d50Il6enG/tmp/66a14d093474d88afba4c7d0e429f942651efe6387f5716a2748b23e286956a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "In this section we present our first algorithm and analyze its statistical properties. For the rest of our presentation, we use the following notations. For any positive integer $k\\,\\in\\,\\mathbb{N}$ ,we use $[k]$ to denote the set $\\{1,\\ldots,k\\}$ . For any vector $v$ \uff0cwe use the notation $v[i]$ to denote its $i^{t h}$ coordinate value. We use $S^{d-1}$ to denote the $d_{\\cdot}$ -dimensional unit sphere and $B(x,r)$ to denote a closed ball of radius $r$ centered at $x$ .We use $\\mathbb{I}$ to denote indicator variable such thatfor anybooleanvariable $A,\\mathbb{I}[A]$ is 1 if $A$ is True, and O otherwise. We consider the binary classification setting, where the instance space is $\\mathcal{X}\\subset S^{d-1}$ and the output class label is ${\\mathcal{D}}\\,=\\,\\{0,1\\}$ .Let $\\mu$ denotes the measure on $\\mathcal{X}$ and let $\\eta:\\mathcal{X}\\rightarrow[0,1]$ , defined as $\\eta(x)=\\operatorname*{Pr}(y=1|x),$ represents the conditional probability distribution. Then, the joint probability distribution on $\\mathcal X\\times\\mathcal X$ is completely defined by $\\mu$ and $\\eta$ . Let $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ , be a training set of size $n$ sampled i.i.d.from $\\mu\\times\\eta$ . Due to space limitation, proofs of all our results (organized section wise) are deferred to the Appendix. ", "page_idx": 3}, {"type": "text", "text": "For $x\\,\\in\\,S^{d-1}$ . the EaS representation of $x$ , that uses $k$ WTA sparsification step, is given by the function $h_{1}\\colon S^{d-1}\\to\\{0,\\dot{1}\\}^{m}$ defined as, ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{1}(x)=\\Gamma_{k}(\\Theta x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Theta$ is a $m\\times d$ random projection matrix whose rows $\\theta_{1},\\ldots,\\theta_{m}$ are sampled i.i.d from the uniform distribution over $S^{d-\\hat{1}}$ and $\\Gamma_{k}\\colon\\mathbb{R}^{m}\\rightarrow\\{0,1\\}^{m}$ is the $k$ WTA function converting a vector in $\\mathbb{R}^{m}$ to one in $\\{0,1\\}^{m}$ by setting the largest $k\\ll m$ elements of $\\Theta x$ to 1 and the rest to zero. For any $j\\in[m]$ , let $\\dot{C_{j}}=\\bar{\\{x\\in\\mathcal{X}:h_{1}(x)[j]=1\\}}$ . We note that the subsets (response regions) $C_{1},\\ldots,C_{m}$ does not form a partition since they can be overlapping. We summarize our first algorithm for binary classification using $h_{1}$ in Alg. 1. During its learning/training phase, a vector $\\bar{w}\\in[0,1]^{m}$ summarizes the average $y$ value over $m$ response regions $C_{j},j\\,\\in\\,[m]$ using the training set. In particular, $w[j],j\\in[m]$ learned during the training phase is precisely given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\eta}_{j}=\\frac{\\sum_{i=1}^{n}y_{i}\\mathbb{I}[x_{i}\\in C_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in C_{j}]}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using (2),for any $x\\in\\mathscr{X}$ , we further define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\eta}(x)=\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\hat{\\eta}_{j}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "During the inference phase, for any test point $x$ , Alg. 1 first computes $(\\boldsymbol{w}\\cdot\\boldsymbol{h}_{1}(\\boldsymbol{x}))/k$ , which is an average of average $y$ values over $k$ response regions $\\bigl\\{C_{j}:h_{1}(x)[\\dot{j}]=1\\bigr\\}$ , which can be interpreted as an estimate of the conditional probability $\\eta(x)$ and is precisely the quantity $\\hat{\\eta}(x)$ given in (3). Alg. 1 then makes its prediction based on whether $\\hat{\\eta}(x)$ is greater than or equal to $1/2$ . We can rewrite this conditional probability estimate as follows. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\hat{\\eta}(x)}&{=}&{\\displaystyle\\frac{w\\cdot h_{1}(x)}{k}=\\frac{1}{k}\\sum_{j:x\\in C_{j}}w[j]=\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\hat{\\eta}_{j}=\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\frac{\\sum_{i=1}^{n}y_{i}\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}}\\\\ &{=}&{\\displaystyle\\sum_{i=1}^{n}\\left(\\underbrace{\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\frac{\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}}_{w_{n,i}(x)}\\right)y_{i}=\\sum_{i=1}^{n}w_{n,i}(x)y_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using viewpoint (4), Alg. 1 can be interpreted as a \u201cplug-in\" classifier Devroye et al. [1996] where prediction is based on whether the estimated conditional probability $\\hat{\\eta}$ exceeds $1/2$ Or not. In particular, classifier in Alg. 1 can be represented as $g:\\mathcal{X}\\to\\{0,1\\}$ described as ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(x)=\\left\\{{1,\\begin{array}{l l}{{\\mathrm{if}\\;\\hat{\\eta}(x)\\geq1/2,}}\\\\ {{\\mathrm{otherwise.}}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In comparison, the Bayes optimal classifier $g_{*}:\\mathcal{X}\\to\\{0,1\\}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{*}(x)=\\left\\{{1,\\begin{array}{l l}{{\\mathrm{if}}\\;\\;{\\mathrm{if}}\\;\\eta(x)\\geq1/2,}\\\\ {0,}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In equation 4, the weights $w_{n,i}(x)\\,=\\,w_{n,i}(x,x_{1},\\ldots,x_{n})\\,\\in\\,\\mathbb{R}$ depends on $x_{1},\\ldots,x_{n}$ . Next we show that for sufficiently large $n$ sum of these weights is 1. Therefore, $\\hat{\\eta}$ is simply a weighted average estimator of $\\eta$ and is a non-parametric classifier Devroye et al. [1996]. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. For any $x\\in\\mathscr{X}$ , suppose n is suficiently large such that $\\{x_{i},\\ldots,x_{n}\\}\\cap C_{j}\\neq\\varnothing$ for each $j$ satisfying $x\\in C_{j}$ Then, $\\begin{array}{r}{\\bar{\\sum_{i=1}^{n}w_{n,i}}(x)\\bar{=}1}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Indeed, we show in Lemma D.7 (in the Appendix), that for any $x\\in\\mathscr{X}$ , whenever $n\\to\\infty$ and $m^{k}/n\\stackrel{.}{\\rightarrow}0$ as $n\\to\\infty$ $\\left|\\left\\{x_{1},\\ldots,x_{n}\\right\\}\\cap\\mathcal{C}_{j}\\right|\\rightarrow\\infty$ for all $j$ such that $x\\in C_{j}$ ", "page_idx": 4}, {"type": "text", "text": "3.1  Consistency of Algorithm 1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we prove that Alg. 1 is universally consistent. We start with the definition of consistent and universally consistent classification rule Devroye et al. [1996]. Let ${\\cal D}_{n}\\ =$ $\\left\\{(x_{1},y_{1}),\\ldots,((x_{n},y_{n})\\right\\}$ be a training set sampled i.i.d. from a certain distribution of $(x,y)$ and let $g_{n}:\\mathcal{X}\\to\\{0,1\\}$ be a classification rule learned using $D_{n}$ . Then $g_{n}$ is consistent (or asymptotically Bayes-risk efficient) for a certain distribution of $(x,y)$ if the expected error probability $\\mathbb{E}L_{n}=\\operatorname*{Pr}\\left\\{g_{n}(x,D_{n})\\neq y\\right\\}\\rightarrow L^{*}$ as $n\\to\\infty$ , where $L^{*}$ is the Bayes error probability. A sequence of decision rules is called universally consistent, if it is consistent for any distribution of the pair $(x,y)$ . It is well known that a general theorem by Stone Stone [1977] (presented below) provides a recipe for establishing universal consistency of any classification rule of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{n}(x)=\\binom{1,\\ \\ \\mathrm{if}\\;\\eta_{n}(x)\\geq1/2,}{0,\\ \\ \\mathrm{otherwise}.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "based on an estimate $\\eta_{n}(x)$ of the conditional probability $\\eta(x)$ , satisfying $\\begin{array}{r}{\\eta_{n}(x)=\\sum_{i=1}^{n}w_{n,i}(x)y_{i}}\\end{array}$ where weights $w_{n,i}(x)=w_{n,i}(x,x_{1},\\ldots,x_{n})$ are non-negative and $\\textstyle\\sum_{i=1}^{n}w_{n,i}(x)=1$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. [Stone's Theorem (Theorem 6.3 [Devroye et al., 1996])] Assume that for any distributionof $x$ the weights satisfy the following three conditions: ", "page_idx": 4}, {"type": "text", "text": "$(i)$ There is a constant c such that, for every non-negative measurable function $f$ satisfying $\\begin{array}{r l r l r l r}{\\mathbb{E}f(x)}&{{}<}&{\\infty,}&{\\mathbb{E}\\left(\\sum_{i=1}^{n}w_{n,i}(x)f(x_{i})\\right)}&{\\le}&{{}\\stackrel{}{c}\\mathbb{E}f(x)}\\end{array}$ (i) For all $\\mathrm{~\\textit~{~a~}~}\\mathrm{~\\textit~{~>~}~}\\mathrm{~\\textit~{~0~}~}$ \uff0c $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}\\left(\\sum_{i=1}^{n}w_{i,n}(x)\\mathbb{I}_{\\{\\|x_{i}-x\\|>a\\}}\\right)=0.}\\end{array}$ (ii) $l i m_{n\\rightarrow\\infty}\\mathbb{E}$ $\\mathbf{\\Psi}_{\\circ}\\mathbb{E}\\left(\\operatorname*{max}_{1\\leq i\\leq n}w_{n,i}(x)\\right)=0$ ", "page_idx": 4}, {"type": "text", "text": "Then $g_{n}$ is universally consistent. ", "page_idx": 4}, {"type": "text", "text": "Since we have already established in the previous section that the classification rule given in Alg. 1 admits the form given in (5), in conjunction with (4) and Lemma 3.1, we are ready to use Theorem 3.2 to establish universal consistency of Alg. 1. We state our consistency result below. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. Let $\\left(k_{n}\\right)$ and $\\left(m_{n}\\right)$ be increasing functions of n and let $\\left(\\delta_{n}\\right)$ be a decreasing function of n satisfying $\\operatorname*{lim}_{n\\to\\infty}\\delta_{n}=0$ Then theclassificationruleusing. $E a S$ representation (1) given in Alg. 1 that admits the form given in (5) is universally consistent whenever the following conditions hold:(i) $\\frac{k_{n}}{m_{n}}\\rightarrow0$ $n\\to\\infty$ (i) $\\begin{array}{r}{|\\frac{\\log(1/\\delta_{n})}{m_{n}}\\to0}\\end{array}$ $n\\to\\infty$ and,(i) $\\frac{m_{n}^{k_{n}}}{n}\\rightarrow0$ as $n\\to\\infty$ ", "page_idx": 5}, {"type": "text", "text": "Once $m$ and $k$ are fixed, for any $x\\in\\mathscr{X}$ , there are $\\binom{m}{k}$ possible subsets of $k$ indices, from the $m$ possible indices in the EaS representation, that $h_{1}(x)$ can set to 1. For $i\\in\\{1,\\ldots,{\\binom{m}{k}}\\}$ , let $\\sigma(i)$ represents the set of $k$ indices in the $i^{t h}$ subset and we let $C_{i}^{k}=\\{x\\in\\mathcal{X}:h_{1}(x)[j]=\\overset{.}{1}$ \uff0c $\\forall j\\in\\sigma(i)\\}$ to denote the set of instances whose EaS representation precisely sets the indices in $\\sigma(i)$ to 1 and the rest to zero. The following two results are crucial in proving our consistency results above, as well as in deriving convergence rate in the next section. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.4.For any positive integers $m,k$ ,where $k<m$ the following relations hold. $i_{,}$ Forany $i\\in\\{1,\\ldots,{\\binom{m}{k}}\\}$ $C_{i}^{k}=\\cap_{j\\in\\sigma(i)}C_{j}$ $\\{C_{i}^{k}\\}_{i=1}^{\\binom{m}{k}}$ $\\mathcal{X}$ $j\\in[m]$ $C_{j}=\\cup_{i:j\\in\\sigma(i)}C_{i}^{k}$ . iv) For any $j\\in[m]$ \uff0c $\\begin{array}{r}{\\mu(C_{j})=\\sum_{i:j\\in\\sigma(i)}\\mu(C_{i}^{k})}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.5. Let $d\\geq3$ andpickany $0<\\delta<1$ There is an absolute constant $c_{0}>0$ suchthatthe following holds. With probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[m]}\\mathrm{diam}(C_{j})\\leq8\\left(\\frac{k+c_{0}(d\\log m+\\log(1/\\delta)}{m}\\right)^{\\frac{1}{d-1}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\Delta}f k\\geq c_{0}(d\\log m+\\log(1/\\delta))\\,t h e n,\\,\\operatorname*{max}_{j\\in[m]}\\mathrm{diam}(C_{j})\\leq8\\left(\\frac{2k}{m}\\right)^{\\frac{1}{d-1}}\\!.}\\end{array}\n$$In particular, t ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.2  Convergence rate of Algorithm 1 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While the consistency result established in the previous section provides the behavior of Alg. 1 in the large sample limit, it does not provide the rate at which the excess Bayes risk converges to zero. In this section we establish such finite sample convergence rate. From the plug-in classifier view point of Alg. 1 given in (5) and the definition of Bayes optimal classifier in (6), it is easy to see that how well the prediction of Algorithm 1 relates to the prediction of the Bayes optimal classifier will depend on howwell $\\hat{\\eta}(x)$ approximates $\\eta(x)$ ", "page_idx": 5}, {"type": "text", "text": "Recall that for any $x\\in\\mathscr{X}$ , point-wise error probability or risk of the Bayes optimal classifier is defined as $L^{*}(x)\\;=\\;\\mathrm{Pr}(g^{*}(x)\\;\\neq\\;y|x)\\;=\\;\\mathrm{min}\\{\\eta(x),1\\,-\\,\\eta(x)\\}$ [Devroye et al., 1996]. Taking expectation over $x$ , risk of the Bayes optimal classifier is $L^{*}\\dot{=}\\,\\widetilde{\\mathbb{E}\\,}(\\operatorname*{min}\\{\\eta(\\dot{x}),1-\\eta(x)\\})$ . Now, for any plug-in classification rule $g_{n}(x)$ defined in (7) that uses a conditional probability estimate $\\eta_{n}$ using training data $D_{n}$ , the excess Bayes risk is given by the following well known result (Corollary 6.1 of Devroye et al. [1996]. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.6 ([Devroye et al., 1996]). The error probability ofa classifier $g_{n}(x)$ defined in (7) satisfies the inequality: $\\begin{array}{r}{\\mathrm{Pr}\\left(g_{n}(x)\\neq y|D_{n}\\right)-L^{*}\\leq2\\mathbb{E}_{x}\\left(|\\eta(x)-\\eta_{n}(x)|\\bigg|D_{n}\\right).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Taking expectation over $D_{n}$ , the above lemma immediately translates to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(g_{n}(x)\\neq y)-L^{*}\\leq2\\mathbb{E}_{x,D_{n}}\\left(|\\eta(x)-\\eta_{n}(x)|\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Therefore, in order to establish convergence rate of Alg. 1, following Lemma 3.6 and (8), we need to bound expected value of $|{\\hat{\\eta}}(x)-\\eta({\\bar{x}})|$ . Towards this end, we first extend $\\eta$ defined over points $x\\in\\mathscr{X}$ to over measurable sets $A\\subset\\mathcal{X}$ with $\\mu({\\mathcal{A}})>0$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta(A)\\!=\\!{\\frac{1}{\\mu(A)}}\\int_{A}\\eta(x)\\mu(d x)\\!=\\!{\\frac{1}{\\mu(A)}}\\int_{A}\\operatorname*{Pr}(y\\!=\\!1|x)\\mu(d x)\\!=\\!\\operatorname*{Pr}(y\\!=\\!1|x\\!\\in{\\mathcal{A}})\\!=\\!{\\mathbb{E}}(y|x\\!\\in{\\mathcal{A}})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, $\\eta(\\mathcal{A})$ is the probability that $y=1$ for a point $x$ chosen at random from the distribution $\\mu$ restricted to the set $\\boldsymbol{\\mathcal{A}}$ , or in other words, it is the expected value of $y$ when $x$ is restricted to the set $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 5}, {"type": "text", "text": "With this definition, we introduce an intermediate quantity ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{\\eta}(x)=\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\bar{\\eta}_{j}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where, ${\\bar{\\eta}}_{j}$ is defined as, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{\\eta}_{j}=\\frac{1}{\\mu(C_{j})}\\int_{C_{j}}\\eta(x)\\mu(d x)=\\eta(C_{j})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Using triangle inequality, this allows us to write: $\\mathbb{E}_{x,D_{n}}|\\hat{\\eta}(x)-\\eta(x)|=\\mathbb{E}_{x,D_{n}}|\\hat{\\eta}(x)-\\bar{\\eta}(x)+\\bar{\\eta}(x)-$ $\\eta(\\boldsymbol{X})\\bar{|}\\leq\\mathbb{E}_{\\boldsymbol{x},D_{n}}|\\hat{\\eta}(\\boldsymbol{x})-\\bar{\\eta}(\\boldsymbol{x})|+\\mathbb{E}_{\\boldsymbol{x},D_{n}}|\\bar{\\eta}(\\boldsymbol{x})-\\eta(\\boldsymbol{x})|$ , and we show how to individually bound each term on the right-hand side of this inequality next. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.7. Note that, once $m$ and $k$ are fixed, our hypothesis space is the set of all linear models On the $k$ -sparse $m$ dimensional binary vectors. The two terms on the right-hand side of the above inequality correspond to estimation error (the error of our proposed classifier with respect to the best hypothesis from the hypothesis space) and approximation error (the error difference between the best hypothesis from the hypothesis space and the target classifier, i.e., Bayes optimal), respectively. ", "page_idx": 6}, {"type": "text", "text": "3.3 Bounding $\\mathbb{E}_{x,D_{n}}|\\hat{\\eta}(x)-\\bar{\\eta}(x)|$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Note that there is an inherent randomness in our proposed algorithm associated with the choice of $\\Theta$ In particular, the response regions $C_{j}$ are random quantities that depend on the choice of $\\Theta$ . In this section, we fix $\\Theta$ and conditioned on this, we bound $\\mathbb{E}_{x,D_{n}}|\\hat{\\eta}(x)-\\bar{\\eta}(x)|$ . This ensures that for any $\\delta>0$ , the same bound holds with probability at least $1-\\delta$ over the choice of $\\Theta$ ", "page_idx": 6}, {"type": "text", "text": "Lemma3.8.Fix any $\\Theta$ Thenwehave, $\\begin{array}{r}{{\\mathbb E}_{x,D_{n}}|\\hat{\\eta}(x)-\\bar{\\eta}(x)|\\le\\sqrt{\\frac{m}{k n}}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Sketch of proof:From the definition of $\\hat{\\eta}(x)$ and $\\bar{\\eta}(x)$ given in (3) and (10) and applying Jensen's inequality, crux of the proof is to focus on the expected value of the random quantity $\\begin{array}{r}{\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\left(\\frac{\\sum_{i=1}^{n}y_{i}\\mathbb{I}[x_{i}\\in C_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in C_{j}]}-\\eta(C_{j})\\right)^{2}}\\end{array}$ $\\begin{array}{r}{j\\in[m],\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in C_{j}]=n\\mu_{n}(C_{j})}\\end{array}$ number of the points from $D_{n}$ that fll in $C_{j}$ , where $\\mu_{n}(C_{j})$ is the empirical probability estimate. We bound the quantity of interest in Lemma 3.8 as a sum of two quantities corresponding to the following two cases: ", "page_idx": 6}, {"type": "text", "text": "Case1:when $\\mu_{n}(C_{j})\\,=\\,\\mathbf{0}{\\mathrm{.}}$ Using the notation $0/0\\,=\\,0$ , the quantity of interest becomes $\\begin{array}{r}{\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\big(\\eta^{2}(C_{j})\\mathbb{I}[\\bar{\\mu_{n}}(C_{j})=0]\\big)}\\end{array}$ . Utilizing the propertie of the response regions established in Lemma 3.4, we show in Lemma E.3 that the expected value of the quantity of interest at most $\\frac{m}{n k e}$ ", "page_idx": 6}, {"type": "text", "text": "Case 2: when $\\begin{array}{r l r}{n\\mu_{n}(C_{j})}&{{}>}&{0.}\\end{array}$ Here the quantity of interest becomes \u2211j:xEC; (Zi=(os-m(C)le:eCal ) In\u03bc,(CG)> 0]. Conditioned on ,2,,, we frst show in Lemma E.1 that expected value (w.r.t. $y_{1},\\ldots,y_{n})$ of this quantity becomes at most $\\begin{array}{r}{\\frac{1}{4k}\\sum_{j:x\\in C_{j}}\\left(\\frac{\\mathbb{I}[n\\mu_{n}(C_{j})>0]}{n\\mu_{n}(C_{j})}\\right)}\\end{array}$ . Next, conditioned on $x_{1},\\ldots,x_{n}$ , and utilizing the properties of the response regions established in Lemma 3.4, we show in Lemma E.2 that expected value (w.r.t. $x$ of this quantity is at most  \u2265=1 ( $\\begin{array}{r}{\\frac{1}{4}\\sum_{j=1}^{m}\\left(\\frac{\\mu(C_{j})\\mathbb{I}[n\\mu_{n}(C_{j})>0]}{n\\mu_{n}(C_{j})}\\right)}\\end{array}$ . Finally, using standard Binomial bound (Lemma E.4) we show that expected value w.r.t. 1, ., En) of this qguantity s at most 2en - ", "page_idx": 6}, {"type": "text", "text": "3.4   Bounding $\\mathbb{E}_{x,D_{n}}|\\bar{\\eta}(X)-\\eta(x)|$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In order to bound the expected value of $|{\\bar{\\eta}}(x)\\!-\\!\\eta(x)|$ , we need to impose certain smoothness condition on $\\eta$ . We consider a general form of smoothness, known as Holder continuty for $\\eta$ ", "page_idx": 6}, {"type": "text", "text": "Definition 3.9. We say that $\\eta:\\mathcal{X}\\rightarrow[0,1]$ is $(L,\\beta)$ smooth if for all $x,x^{\\prime}\\in\\mathcal{X}$ , we have $\\vert\\eta(x)-$ $\\eta(x^{\\prime})\\vert\\leq L\\Vert x-x^{\\prime}\\Vert^{\\beta}$ ", "page_idx": 6}, {"type": "text", "text": "Using Holder continuity assumption above, we first show the following: ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.10. Suppose $\\eta$ $(L,\\beta)$ smooth.Then, $\\begin{array}{r}{\\operatorname*{sup}_{x\\in S}|\\eta(x)\\!-\\!\\bar{\\eta}(x)|\\leq L\\!\\cdot\\!\\operatorname*{max}_{j\\in[m]}(\\mathrm{diam}(C_{j}))^{\\beta}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "We have already shown in Lemma 3.5 how to bound the diameters of $C_{j}$ . Combining these two results we have ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.11. Let $d\\geq3$ and pick any $0\\,<\\,\\delta\\,<\\,1$ Assume $\\eta$ to be $(L,\\beta)$ smooth. There is an absolute constant $c_{0}>0$ such that the following holds. If $k\\geq c_{0}(d\\log m+\\log(1/\\delta))$ then with probability at least 1 - 8, $\\mathbb{E}_{x,D_{n}}|\\bar{\\eta}(x)-\\eta(x)|\\le8L\\,(2k/m)^{\\frac{\\beta}{d-1}}$ ", "page_idx": 7}, {"type": "text", "text": "Combining Lemma 3.8, 3.11 and 3.6 we are present the main result of this section. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.12. Let $D_{n}=\\{(x_{i},y_{i})\\}_{i=1}^{n}\\subset\\mathcal{X}\\times\\{0,1\\}$ be the training data and consider the Eas representation given in (1). Let $d\\geq3$ and pick any $0<\\delta<1$ Assume $\\eta$ to be $(L,\\beta)$ smooth. There is an absolute constant $c_{0}>0$ such that the following holds. If $k\\geq c_{0}(d\\log m+\\log(1/\\delta))$ then with probability at least $1-\\delta$ over the random choice of $\\Theta$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(g(x)\\neq y)-L^{*}\\leq2\\left({\\sqrt{\\frac{m}{k n}}}+8L\\left({\\frac{2k}{m}}\\right)^{\\frac{\\beta}{d-1}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Corollary 3.13. In Theorem 3.12, set $m=k n^{\\frac{(d-1)}{2\\beta+(d-1)}}$ . Then, with probability at least $1-\\delta$ over the choice of $\\Theta$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(g(x)\\neq y)-L^{*}=O\\left(n^{-{\\frac{\\beta}{2\\beta+(d-1)}}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 3.14. Since $\\mathcal{X}\\,\\subset\\,S^{d-1}$ , the effective dimension in our setting is $d^{\\prime}\\,=\\,(d\\,-\\,1)$ and the convergence rate of Corollary 3.13 can be rewritten as $O\\left(n^{-\\frac{\\beta}{2\\beta+d^{\\prime}}}\\right)$ which is minimax-optimal for plug-in classifiers under the assumption that $\\eta$ is $(L,\\beta)$ -smooth [Audibert and Tsybakov, 2007]. ", "page_idx": 7}, {"type": "text", "text": "3.5  I1nability to adapt to manifold structure ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Theorem 3.12, we derived the convergence rate of the classifier presented in Alg. 1 by bounding $\\mathbb{E}_{x,D_{n}}|\\eta(x)\\,-\\,\\hat{\\eta}(x)|$ , which upper bounds the excess Bayes risk, from above and the resulting convergence rate decays exponentially slowly with the dimension $d$ .We now show that even if the data lie on a low dimensional manifold having dimesnion $d_{0}\\ll d$ ,there exists a smooth $\\eta$ suchthat the quantity $\\mathbb{E}_{x,D_{n}}|\\eta(x)-\\hat{\\eta}(x)|$ decreases at a rate no faster than n- ai . To prove claim, we assume data to lie on the following one-dimensional manifold: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{X}_{1}=\\{(x_{1},x_{2},0,0,\\dotsc...,0)\\in\\mathbb{R}^{d}:x_{1}^{2}+x_{2}^{2}=1\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We further assume that $k\\,=\\,\\beta\\,=\\,1$ which implies that $\\eta$ is $L$ -Lipschitz from the definition of $(L,\\beta)$ -smoothness. Our lower bound result is as follows. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.15. For any $d>3$ let input space $\\chi_{1}$ be the one-dimensional sub-manifold of $\\mathbb{R}^{d}$ given in (12). Take $k=1$ and $\\beta=1$ .Suppose the random matrix $\\Theta$ has rows chosen from a distribution that is uniform over $S^{d-1}$ . Then there exists a $\\frac{1}{2}$ -Lipschitz.function $\\eta:\\mathcal{X}_{1}\\rightarrow[0,1]$ such that the following holds with probability at least $1/2$ over the choice of $\\Theta$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x,D_{n}}|\\eta(x)-\\hat{\\eta}(x)|=\\Omega\\left(n^{-\\frac{1}{d+1}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4 Algorithm 2 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we present an alternate EaS representation and an associated classification algorithm that adapts to intrinsic dimension $d_{0}$ when data lie on a low dimensional manifold with dimension $d_{0}\\ll d$ . Here, we assume that the rows $\\theta_{i},i\\in[m]$ of matrix $\\Theta$ are sampled i.i.d. from a multivariate Gaussian distribution $N(0,1/\\sqrt{d}I_{d})$ , denote by $\\nu$ ,where $I_{d}$ is $d\\times d$ identity matrix. This EaS representation is denoted by $h_{2}:\\mathcal{X}\\rightarrow\\{0,1\\}^{m}$ , where for any point $x\\in\\mathscr{X}$ , the $j^{t h}$ coordinate of $h_{2}(x)$ is set to 1, as given in (14), using a data dependent threshold $\\tau_{n}$ . In particular, given a training set of sizel $2n$ sampled i.i.d.from $(\\mu,\\eta)$ , using the first half of it, namely, $\\boldsymbol{D}_{n}^{\\prime}=\\{(\\bar{x_{1}^{\\prime}},y_{1}^{\\prime}),\\dots,(\\bar{x_{n}^{\\prime}},y_{n}^{\\prime})\\}$ define $\\tau_{n}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ to be: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tau_{n}(\\theta){\\mathrm{=sup}}\\!\\left\\{\\!\\tau{\\mathrm{:}}{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbb{I}[\\theta\\cdot x_{i}^{\\prime}\\geq\\tau]\\geq{\\frac{k}{m}}\\!\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\nh_{2}(x)[j]=\\mathbb{I}[\\theta_{j}\\cdot x\\geq\\tau_{n}(\\theta_{j})]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We call the sparsification scheme given in (14)empirical. $\\boldsymbol{\\cdot}\\boldsymbol{k}$ -thresholding.For $j\\in[m]$ , the $j^{t h}$ response region $C_{j}$ is defined as: ", "page_idx": 8}, {"type": "equation", "text": "$$\nC_{j}=C(\\theta_{j})=\\{x\\in\\mathcal{X}:\\theta_{j}\\cdot x\\geq\\tau_{n}(\\theta_{j})\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Using the second half of the training data, namely $D_{n}=\\{(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\}$ average $y$ values over different $C_{j}$ are estimated and summarized in vector $w$ in the same way as in Alg. 1 implying that $w[j]=\\hat{\\eta}_{j},\\dot{\\forall j}\\in[m]$ where $\\hat{\\eta}_{j}$ is given in (2). This completes the training phase of our proposed algorithm summarized in Alg. 2. ", "page_idx": 8}, {"type": "text", "text": "Algorithm2 Training set $D_{n}^{\\prime}$ $\\{(\\bar{x_{i}^{\\prime}},y_{i}^{\\prime})\\}_{i=1}^{n},D_{n}=\\{(x_{i},\\bar{y_{i}})\\}_{i=1}^{n}\\subset\\mathcal{X}\\overset{\\cdot}{\\times}\\{0,1\\},$ Projection dimensionality $m\\in\\mathbb{N}$ , integer $k\\ll m$ integer $t$ , random seed $R$ , and inference with test point c E Sd-1. ", "page_idx": 8}, {"type": "text", "text": "TrainEaSClassif $\\mathrm{i}\\!\\,\\mathsf{e r}(D_{n},D_{n}^{\\prime}m,k,t,R)$ Sample $\\Theta$ with seed $R$ Initialize $w[i],\\mathsf{c t}[i]\\gets0,\\,\\forall i\\in[m]$ for $j\\in[m]$ do |Compute $\\tau_{n}(\\theta_{j})$ using (13) and $D_{n}^{\\prime}$ end for $(x,y)\\in D_{n}$ do e ${\\mathsf{a s}}\\gets h_{2}(x)$ $w[i]\\leftarrow w[i]+y$ $\\forall i\\in[m]:\\mathtt{e a s}[i]=1$ $\\mathsf{c t}[i]\\gets\\mathsf{c t}[i]\\!+\\!1$ $\\forall i\\in[m]:\\mathtt{e a s}[i]=1$ end $w[i]\\leftarrow w[i]/\\mathsf{c t}[i],\\;\\forall i\\in[m]$ return $\\Theta,w$   \nend   \nInferEaSClassifier $(x,\\Theta,t,w)$ eas \u2190 h2(x) $\\Theta_{x}\\gets\\{\\theta_{i}:h_{2}(x)[i]=1\\}$ eas $\\gets\\mathtt{e a s}$ $\\mathtt{e a s}[i]\\gets0,\\forall i\\in[m]:i\\notin A_{t}(x)$ return $\\mathbb{I}[(e{\\tilde{a}}s\\cdot w)/{\\bar{t}}\\geq{\\frac{\\mathrm{i}}{2}}]$   \nend ", "page_idx": 8}, {"type": "text", "text": "The inference phase of Alg. 2 is slightly different from Alg. 1. While EaS representation using $h_{2}$ is not $k$ -sparse anymore, we show that for large enough sample size, with high probability, it is at least $k/2$ -sparse and at most $2k$ -sparse in expectation (see Lemma F.1 in Appendix F). Let $t$ be an integer passed as an argument to Alg. 2. For any $x\\in\\mathscr{X}$ , let $\\Theta(x)=\\{\\theta_{j}:x\\in C_{j}\\}$ and we define $A_{t}(x)$ to be the set containing the indices $j\\in[m]$ , such that $\\theta_{j}$ is one of the $t$ closest points to $x$ from $\\Theta(x)$ . To make an inference for any $x\\in\\mathscr{X}$ ,Alg. 2 first computes $(\\mathtt{e a s}\\cdot w)/t$ and makes it prediction based on whether this quantity is greater than $1/2$ . Clearly, $(\\mathtt{e a s}\\cdot w)/t$ is the average of $w[j]$ for $j\\in A_{t}(x)$ and therefore, the conditional probability estimate $\\hat{\\eta}(x)$ of $\\mathrm{Alg}$ 2 can be represented as, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\eta}(x)=\\frac1t\\sum_{j}\\hat{\\eta}_{j}\\mathbb{I}[j\\in A_{t}(x)]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Remark 4.1. Note that $h_{1}$ and $h_{2}$ are different in a specific way. When the support of data does not cover the whole unit sphere and is concentrated possibly in a small region and $m$ is large, many of the $m$ coordinates in EaS representation will never be activated (set to 1) for any data point as the corresponding projection direction $\\theta_{j}$ may not be one of the $k$ closest ones to any data point. Thus, many of the $\\theta_{j}$ will be unused. This prob", "page_idx": 8}, {"type": "text", "text": "lem is avoided in $h_{2}$ , where every $\\theta_{j}$ is used but the respective response region $C_{j}$ may not be local to the manifold. For this purpose we need to identify \u201cgood\u201d projection directions. Later in Lemma B.4 we show that, for any $\\delta>0$ , with probability at least $1-2\\delta$ over the choice of $\\Theta$ and $D_{n}^{\\prime}$ , the number of \u201cgood\u2019 projection directions $t$ is linear in $k$ ", "page_idx": 8}, {"type": "text", "text": "Due to space limitation, manifold assumptions and other important details of analysis of Alg. 2 are presented in Appendix B and we present the main theoretical results below. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2. Let $D_{n}=\\{(x_{i},y_{i})\\}_{i=1}^{n}\\cup D_{n}^{\\prime}=\\{(x_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{n}\\subset\\mathcal{X}\\times\\{0,1\\}$ be the training data where the data lies on a low dimensional manifold satisfying manifold assumption presented in section B.1 and suppose the EaS representation is given as in (14). Pick any $0<\\delta<1$ Assume $\\eta$ to be $(L,\\beta)$ smooth.if $k\\geq c_{d}^{\\prime}\\ln(m/\\bar{\\delta})$ ,where $c_{d}^{\\prime}$ is a constant that depend on $d$ thenwithprobability at least $1-2\\delta$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(g(x)\\neq y)-L^{*}\\leq2\\left({\\sqrt{\\frac{2m}{\\alpha_{d}k n}}}+4L\\left({\\frac{2k}{c_{1}m}}\\right)^{\\frac{\\beta}{d_{0}}}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\alpha_{d}$ is a constant that depends on $d$ ", "page_idx": 8}, {"type": "text", "text": "Corollary 4.3. In Theorem 4.2, setting m = kn23+ao ensure that with probability at least $1-\\delta$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(g(x)\\neq y)-L^{*}=O\\left(n^{-{\\frac{\\beta}{2\\beta+d_{0}}}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Remark 4.4. The convergence rate of Corollary 4.3 depends only on $d_{0}$ and is minimax-optimal for plug-in classifiers under the assumption that $\\eta$ is $(L,\\beta)$ -smooth [Audibert and Tsybakov, 2007]. ", "page_idx": 8}, {"type": "image", "img_path": "0d50Il6enG/tmp/70420c59ef55acf04edb303225aceef0dced3787b33fd8a64e5cea250e0583a2.jpg", "img_caption": ["Figure 2: Empirical evaluation of Alg. 1, $k$ -NN (for $k=1$ and 10) and RF on eight datasets Here expansion factor is $m/d$ . An error bar in the form of a shaded graph is provided for Alg. 1 over 10 independent runs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5  Empirical evaluations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We investigate the effectiveness of our proposed method by evaluating it on eight benchmark datasets, details of which are provided in appendix A. We address the following questions: ", "page_idx": 9}, {"type": "text", "text": "1. Does performance our proposed classifier improve with increasing $m$ as suggested by the theory?   \n2. How does our proposed classifier perform compared to other non-parametric classifiers? ", "page_idx": 9}, {"type": "text", "text": "For each dataset, we generate train and test set using scikit-learn's train_test_split method $(80~:~20~\\$ split). We compare our proposed method against two non-parametric classifiers - scikit-learn's implementation of $k$ -nearest neighbor classifier $(k{-}\\mathbb{N}\\mathbb{N})$ and random forest (RF). For $k$ -NN, we used two values of $k$ $k=1$ and 10. For RF we use a grid search over the number of estimators (trees) from the set $\\{250,500,750,1000\\}$ and perform a 3 fold cross validation to choose the final model. We preset our experimental results in Fig. 2 where we plot test accuracy of Alg. 1, $k$ -NN (for $k\\,=\\,1$ and 10) and RF by varying expansion factor, where we define expansion factor to be $m/d$ . As per Theorem 3.12, we set $k$ to be $d\\log m$ . As can be seen from Fig. 2, with increasing $m$ test accuracy of Alg. 1 increases in all eight datasets and becomes comparable to that of $k$ -NN and RF for large $m$ , thus corroborating our theoretical findings. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions, limitations and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present an interesting connection between non-parametric estimation and expansionand-sparsify representation. We presented two non-parametric classification algorithms using EaS representation and proved that both algorithms yield minimax-optimal convergence rates. The convergence rate of the first algorithm depends on the ambient dimension $d$ , while the convergence rate of the second algorithm, under manifold assumption, depends only on the intrinsic dimension $d_{0}\\ll d$ . In both algorithms, the projection directions are chosen in a data-independent manner. One limitation of our current work is that, even though the second algorithm adapts to the manifold structure, there is a large constant, possibly depending exponentially on $d$ , involved in bounding the excess Bayes risk, that is hidden under the Big Oh notation. In the future, we plan to investigate various data-dependent projection direction choices for a sparse representation, that would adapt to a manifold structure, and the constant involved in bounding of the excess Bayes risk from above, would be independent of ambient dimension $d$ ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements: We thank the anonymous reviewers for their constructive feedback. This work is supported by funding from the \u201cNSF AI Institute for Foundations of Machine Learning (IFML) (FAIN:2019844). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Commun. ACM, 51(1):117-122, jan 2008. URL https: //doi.org/10. 1145/1327452.1327494.   \nJean-Yves Audibert and Alexandre B. Tsybakov. Fast learning rates for plug-in classifiers. The Annals of Statistics, 35(2):608 - 633, 2007. doi: 10.1214/009053606000001217. URL https : //doi.0rg/10.1214/009053606000001217.   \nGerard Biau, Luc Devroye, and Gabor Lugosi. Consistency of random forests and other averaging classifiers. Journal of Machine Learning Research, 9(66):2015-2033, 2008. URL http: //jmlr . org/papers/v9/biau08a.html.   \nEmmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 59(8):1207-1223, 2006.   \nS. J. Caron, V. Ruta, L. F. Abbott, and R. Axel. Random convergence of olfactory inputs in the Drosophila mushroom body. Nature, 497(7447):113-117, May 2013.   \nMaurice J Chacron, Andre Longtin, and Leonard Maler. Efficient computation via sparse coding in electrosensory neural networks. Current Opinion in Neurobiology, 21(5):752-760, 2011.   \nKamalika Chaudhuri and Sanjoy Dasgupta._ Rates of convergence for the cluster tree. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010. URL https://proceedings.neurips. cc/paper_files/paper/2010/file/ b534ba68236ba543ae44b22bd110a1d6-Paper.pdf.   \nSanjoy Dasgupta and Christopher Tosh. Expressivity of expand-and-sparsify representations. arXiv preprint arXiv:2006.03741, 2020. URL https : //arxiv. org/pdf /2006. 03741 .pdf.   \nSanjoy Dasgupta, Charles F Stevens, and Saket Navlakha. A neural algorithm for a fundamental computing problem. Science, 358(6364):793-796, 2017.   \nSanjoy Dasgupta, Timothy C Sheehan, Charles F Stevens, and Saket Navlakha. A neural data structure for novelty detection. Proceedings of the National Academy of Sciences, 115(51):13093-13098, 2018.   \nMayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the Twentieth Annual Symposium on Computational Geometry, SCG '04, page 253-262. Association for Computing Machinery, 2004. doi: 10.1145/997817.997857. URL https : //doi org/10 .1145/997817.997857.   \nLuc Devroye, Laszl6 Gyorfi, and Gabor Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31 of Stochastic Modelling and Applied Probability. Springer, 1996. ISBN 978-1-4612- 0711-5.   \nD.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-1306, 2006. doi: 10.1109/TIT.2006.871582.   \nWei Gao and Zhi-Hua Zhou. Towards convergence rate analysis of random forests for classification. In H. Larochell, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9300-9311, 2020.   \nAristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search in high dimensions via hashing. VLDB '99, page 518-529. Morgan Kaufmann Publishers Inc., 1999. ISBN 1558606157.   \nLaszl6 Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of Nonparametric Regression. Springer series in statistics. Springer, 2002. ISBN 978-0-387-95441-7.   \nA. C. Lin, A. M. Bygrave, A. de Calignon, T. Lee, and G. ck. Sparse, decorrelated odor coding in the mushroom body enhances learned odor discrimination. Nat Neurosci, 17(4):559-568, Apr 2014.   \nN. Y. Masse, G. C. Turner, and G.S. Jefferis. Olfactory information processing in Drosophila. Curr Biol, 19(16):R700-713, Aug 2009.   \nPartha Niyogi, Stephen Smale, and Shmuel Weinberger. Finding the homology of submanifolds with high confidence from random samples. Discrete Comput. Geom., 39(1):419-441, mar 2008. ISSN 0179-5376.   \nB. A. Olshausen and D. J. Field. Sparse coding of sensory inputs. Curr Opin Neurobiol, 14(4): 481-487, Aug 2004.   \nAli Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https: //proceedings . neurips . cc/paper_ files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf.   \nParikshit Ram and Kaushik Sinha. Flynn: Fruit-fly inspired federated nearest neighbor classification. In International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2021 (FL-ICML'21), 2021.   \nParikshit Ram and Kaushik Sinha. Federated nearest neighbor classification with a colony of fruitflies. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pages 8036-8044. AAAI Press, 2022.   \nErwan Scornet. On the asymptotics of random forests. J. Multivar. Anal., 146(C):72-83, apr 2016. ISSN 0047-259X. URL https: //doi.org/10.1016/j . jmva.2015.06.009.   \nErwan Scornet, Gerard Biau, and Jean-Philippe Vert. Consistency of random forests. The Annals of Statistics, 43(4):1761-1741, 2015.   \nKaushik Sinha and Parikshit Ram. Fruit-fly inspired neighborhood encoding for classification. In The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2021.   \nD. D. Stettler and R. Axel. Representations of odor in the piriform cortex. Neuron, 63(6):854-864, Sep 2009.   \nCharles J. Stone. Consistent nonparametric regression. The Annals of Statistics, 5(4):595-620, 1977.   \nCheng Tang, Damien Garreau, and Ulrike von Luxburg. When do random forests fail? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 4, 2018.   \nAlexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incorporated, 1st edition, 2008. ISBN 0387790519.   \nG. C. Turner, M. Bazhenoy, and G. Laurent. Olfactory representations by Drosophila mushroom body neurons. J Neurophysiol, 99(2):734-746, Feb 2008.   \nJoaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: Networked science in machine learning. SIGKDD Explorations, 15(2):49-60, 2013. doi: 10.1145/2641190.2641198. URL http: //doi.acm.org/10.1145/2641190.2641198.   \nR. 1. Wilson. Early olfactory processing in Drosophila: mechanisms and principles. Annu Rev Neurosci, 36:217-241, Jul 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "0d50Il6enG/tmp/fc592262502d655f6e2c76ac37e974464013f159015d82d18159faa4bfc31ecd.jpg", "table_caption": [], "table_footnote": ["Table 1: Dataset statistics "], "page_idx": 12}, {"type": "text", "text": "A   Dataset details and computing environment ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Details of the eight datasets used in our experiments are listed in Table 1. Among the eight datasets, the twomoons is a synthetic dataset generated using make_moons method from sklearn. dataset? with noise parameter 0.2. All the remaining datasets are taken from the OpenML repository3 Vanschoren et al. [2013]. Both mnist and fmnist (fashion-mnist for short) are 10 class classification problem with 784 features. We convert them to binary classification problems by using the label 3 and 5 for the mnist dataset and label 2 (Pullover) and 4 (Coat)for fmnist dataset. For efficiency purpose, the feature dimensions for both these datasets are reduced to 20 using principal component analysis (PCA). For the remaining six datasets, the task is that of binary classification. For all eight datasets, the features are normalized using StandardScaler option in scikit-learn and are made to be unit norm. ", "page_idx": 12}, {"type": "text", "text": "We run our experiments on a laptop with Intel Xeon W-10855M Processor, 64GB memory and NVDIA Quadro RTX 5000 Mobile GPU (with 16GB memory). ", "page_idx": 12}, {"type": "text", "text": "B  Missing details of convergence rate of Algorithm 2 from section 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section we present missing details of convergence rate analysis of Algorithm 2 from section 4. Proofs of various technical results presented in this section as well as the proof of Theorem 4.2 are deferred to sectionB. ", "page_idx": 12}, {"type": "text", "text": "B.1  Manifold assumption ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To analyze convergence rate of Algorithm 2, we proceed in the same was as in section 3.2. However, to take advantage of the fact that data lie on a low-dimensional manifold in our analysis, we make similar manifold assumptions as in Dasgupta and Tosh [2020]. The input space $\\mathcal{X}$ is a compact $d_{0}$ -dimensional Riemannian sub manifold $M$ of $\\mathbb{R}^{d}$ contained in the unit sphere, that is $M\\subset\\dot{S^{d-1}}$ $M$ has nice boundaries and that the distribution on it $\\mu$ , is almost uniform: formally, there exists constants $c_{1},c_{2},c_{3}>0$ such that for all $x\\in M$ and for all $r\\leq r_{0}$ (where $r_{0}$ is an absolute constant) ", "page_idx": 12}, {"type": "equation", "text": "$$\nc_{1}r^{d_{0}}\\leq\\mu(B_{M}(x,r)<c_{2}r^{d_{0}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{Vol}(B_{M}(x,r))\\geq c_{3}r^{d_{0}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Here $B_{M}(x,r)=B(x,r)\\cap M$ ", "page_idx": 12}, {"type": "text", "text": "To effectively analyze the data distribution supported on a manifold, we impose conditions on the curvature by adopting the common requirement that $M$ has a positive reach $\\rho>0$ : that is, every point in an open tubular neighborhood of $M$ of of radius $\\rho$ has a unique nearest neighbor in $M$ [Niyogi et al., 2008]. For each $x\\in M$ , let $N(x)$ denotes the $(d-d_{0})$ dimensional subspace of normal vectors to the tangent plane at $x$ . For each $x\\in M$ , the sets $\\Gamma_{\\rho}(x)=\\{x+r u\\in\\mathbb{R}^{d}:u\\in N(x),\\lVert u\\rVert=$ $1,0<r<\\rho\\}$ are disjoint and let $\\tau_{\\rho}$ be their union. Let $\\pi_{M}:\\tau_{\\rho}\\to M$ be the projection map that sends any point in $\\tau_{\\rho}(x)$ to $x$ , its nearest neighbor in $M$ ", "page_idx": 12}, {"type": "text", "text": "The response regions $C_{j},j\\in[m]$ are now random quantities that depend on the choice of $\\Theta$ as well as $D_{n}^{\\prime}$ used to compute individual thresholds. The quantity $\\hat{\\eta}(x)$ given in (16) depends on $t$ . In this section, we fix $\\Theta$ and $t$ and conditioned on this, we bound $\\mathbb{E}_{x,D_{n},D_{n}^{\\prime}}|\\hat{\\eta}(x)-\\bar{\\eta}(x)|$ Later in Lemma B.4 we show that, for any $\\delta>0$ , with probability at least $1-2\\delta$ over the choice of $\\Theta$ and $D_{n}^{\\prime}$ \uff0c $t$ is linear in $k$ ", "page_idx": 13}, {"type": "text", "text": "Lemma B.1. Fix any $\\Theta$ and $t$ Then the following holds ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x,D_{n},D_{n}^{\\prime}}|\\hat{\\eta}(x)-\\bar{\\eta}(x)|\\leq\\sqrt{\\frac{m}{t n}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Sketch of proof: The proof strategy is similar to Lemma 3.8, but requires careful analysis to accommodate the facts that $C_{j},j\\in[m]$ depend on the choice of $D_{n}^{\\prime}$ and $\\hat{\\eta}$ in (16) has an indicator function. Details are provided in Appendix $\\mathbf{B}$ ", "page_idx": 13}, {"type": "text", "text": "B.3 Bounding $\\mathbb{E}_{x,D_{n}}|\\bar{\\eta}(x)-\\eta(x)|$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Using identical proof strategy of Lemma 3.10, we have, ", "page_idx": 13}, {"type": "text", "text": "Lemma B.2. Under empirical $k$ -thresholdingscheme,if $\\eta$ is $(L,\\beta)$ smooth, then for all $x\\in$ $C_{1}\\cup\\cdot\\cdot\\cdot,\\cup C_{m}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\eta(x)-\\bar{\\eta}(x)|\\leq L\\operatorname*{max}_{j\\in[m]}(\\mathrm{diam}(C_{j}))^{\\beta}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Unfortunately, unlike before, all $C_{j}$ such that $x\\in C_{j}$ may not be local to $x$ and thus average $y$ values in some of these $C_{j}$ , may be very different from $\\eta(x)$ . To address this, we call a $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ good if it lies in $\\Gamma_{\\rho/2}$ . Good $\\theta_{j}$ 's are close to the manifold $M$ and are guaranteed to be activated by a single neighborhood of $M$ Thus, for large enough $m$ , if the diameters of the $C_{j}$ 's corresponding to good $\\theta_{j}$ 's are made reasonable small then the average $y$ values in such $C_{j}$ 's will be a reasonably close of $\\eta(\\boldsymbol{x})$ for $x\\in C_{j}$ . First, for any good $\\theta_{j}$ , we bound the diameter of $C_{j}$ emphasizing that the response region of a good $\\theta_{j}$ is local. ", "page_idx": 13}, {"type": "text", "text": "Lemma B.3. Pick any good $\\theta\\in\\mathbb{R}^{d}$ .Define $\\Delta=\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\pi}_{M}(\\boldsymbol{\\theta})\\rVert$ to be the distance from $\\theta$ to its projection on $M$ Let $C(\\theta)$ be the response region associated with $\\theta$ as defined in (15). Pick any $0<\\delta<1$ .Then with probability at least $1-\\delta$ , over the choice of $D_{n}^{\\prime}$ the following holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\nB_{M}\\left(\\pi_{M}(\\theta),\\sqrt{\\frac{\\rho-\\Delta}{\\rho+\\Delta}}\\left(\\frac{k}{2c_{2}m}\\right)^{1/d_{0}}\\right)\\subset C(\\theta)\\subset B\\left(\\pi_{M}(\\theta),\\sqrt{\\frac{\\rho+\\Delta}{\\rho-\\Delta}}\\left(\\frac{2k}{c_{1}m}\\right)^{1/d_{0}}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In paricular thisimplies, diam(C(0))\u22644 (2m ) 1/do , provided $(2k/(c_{1}m))^{1/d_{0}}<\\operatorname*{min}(\\rho,r_{0})$ and n satisfies $n\\geq\\frac{c_{0}m}{k}$ $\\textstyle{\\left^{\\left/{\\log n}\\right.+\\log\\left({\\frac{m}{\\delta}}\\right)}}\\right)$ , where $c_{0}>0$ is a universal constant. ", "page_idx": 13}, {"type": "text", "text": "Next we show that each $x\\in M$ has number of good $\\theta_{j}$ linear in $k$ with high probability. ", "page_idx": 13}, {"type": "text", "text": "Lemma B.4. Pick $\\theta_{1},\\ldots,\\theta_{m}\\sim\\nu$ There is a constant $c_{d}^{\\prime},$ depending on $d$ for which the following holds. Pick any $0<\\delta<1$ . Set $k\\geq c_{d}^{\\prime}\\ln(m/\\delta)$ then with probability at least $1-2\\delta$ over the choice of $\\theta_{j}$ 's and $D_{n}^{\\prime}$ for every $x\\in M$ there are $\\alpha_{d}k/2$ good $\\theta_{j}$ 's with $x\\in C(\\theta_{j})$ where $c_{d}^{\\prime}$ and $\\alpha_{d}$ are constants that depend on dimension $d$ ", "page_idx": 13}, {"type": "text", "text": "Combining4 Lemma B.3 and B.4, we have ", "page_idx": 13}, {"type": "text", "text": "Lemma B.5. Suppose the data distribution is supported on a $d_{0}$ -dimensional submanifold $M$ of $\\mathbb{R}^{d}$ with reach $\\rho\\,>\\,0$ that additionally satisfied (17) and (18). Suppose that the rows of $\\Theta$ are chosenfrom $N(0,I_{d})$ There is a constant $c_{d}^{\\prime}$ depending on the dimension $d,$ and $c_{0}>0$ universal constant, for which the following holds. Pick any $0\\,<\\,\\delta\\,<\\,1$ .Let $k,m$ and nbe chosen so that $k\\geq c_{d}^{\\prime}\\ln(m/d e l t a)$ \uff0c $(2k/(c_{1}m))^{1/d_{0}}<\\operatorname*{min}(\\rho,r_{0})$ and $n\\geq(c_{0}m/k)(\\log n+\\log(m/\\delta))$ . Then Withprobabilityat least $1-2\\delta$ over the choice of $\\Theta$ and $D_{n}^{\\prime}$ \u4e00\uff0c ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in{\\mathcal{X}}}|\\bar{\\eta}(x)-\\eta(x)|\\leq4L\\left(\\frac{2k}{c_{1}m}\\right)^{\\frac{\\beta}{d_{0}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining everything leads to the main result presented in Theorem 4.2. ", "page_idx": 14}, {"type": "text", "text": "C Various proofs from section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Simply interchange the summation. That is, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}w_{i,n}(x)=\\sum_{i=1}^{n}\\left(\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\frac{\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}\\right)=\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\left(\\frac{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in\\mathcal{C}_{j}]}\\right)=1\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "D Various proofs from section 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we present various technical results that are needed to prove our main theorem (Theorem 3.3). Due to space limitation proofs of some of these technical results are deferred to the supplementarymaterial. ", "page_idx": 14}, {"type": "text", "text": "D.1Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. For part (i), note that if $x\\in C_{i}^{k}$ , then by definition, $\\forall j\\in\\sigma(i),h_{\\Theta,k}(x)[j]=1$ which implies $x\\,\\in\\,C_{j}$ . Therefore, $C_{i}^{k}\\,\\subseteq\\,\\cap_{j\\in\\sigma(i)}C_{j}$ . On the other hand, if $x\\,\\in\\,\\cap_{j\\in\\sigma(i)}C_{j}$ then $h_{\\Theta,k}(x)[j]\\,=$ $1\\,\\forall j\\in\\sigma(i)$ , which implies $x\\in C_{i}^{k}$ . Therefore, $\\cap_{j\\in\\sigma(i)}C_{j}\\subseteq C_{i}^{k}$ ", "page_idx": 14}, {"type": "text", "text": "For part (i), take any $i,j\\,\\in\\,[{\\binom{m}{k}}],i\\,\\neq\\,j$ , then $\\sigma(i)\\neq\\sigma(j)$ . Therefore, if $x\\in C_{i}^{k}$ then $x\\notin C_{j}^{k}$ Since $x$ was arbitrary, $C_{i}^{k}\\cap C_{j}^{k}=\\emptyset$ . Now, for any $x\\in\\mathscr{X}$ , since $h_{\\Theta,k}(x)$ has exactly $k$ bits activated (set to 1), there must exist $l\\in[\\left({m\\atop k}\\right)]$ such that $x\\in C_{l}^{k}$ Therefore, U2 C = X. ", "page_idx": 14}, {"type": "text", "text": "For part (i), take any $x\\in C_{j}$ . From the definition of $C_{j}$ $h_{\\Theta,k}(x)[j]=1$ and therefore, $x\\in C_{i}^{k}$ for any $i$ such that $j\\in\\sigma(i)$ . From part(ii) since such $C_{i}^{k}\\mathrm{s}$ are disjoint and $x$ was arbitrary, the result follows. ", "page_idx": 14}, {"type": "text", "text": "Finally, part (iv) follows immediately from part (i) and (i) since $C_{i}^{k}\\mathrm{s}$ are disjoint. ", "page_idx": 14}, {"type": "text", "text": "D.2 proof of Lemma 3.5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. While general idea of this result appeared in Dasgupta and Tosh [2020], we provide a simplified proof. Let $\\nu(r)\\ \\ =\\ \\ \\operatorname{inf}_{x\\in\\mathcal{X}}\\nu(B(x,r))$ . We first claim that if $\\nu(r)^{\\bar{\\quad}}\\geq$ $\\begin{array}{r}{\\frac{2}{m}\\left(k+c_{0}\\left(d\\log m+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}\\end{array}$ then for every $j~\\in~[m]$ \uff0c\uff0c $C_{j}\\ \\subset\\ B(\\theta_{j},r)$ .By definition, every ball of radius $r$ , centered at some $x\\in\\mathscr{X}$ , has $\\nu$ -mass at least $\\nu(r)$ . Thus, by Lemma D.1, with probability at least $1-\\delta$ ,every $x\\in\\mathscr{X}$ will have its nearest $k\\:\\theta_{j}$ 's within a distance $r$ . Therefore, the $j^{t h}$ bit of the EaS representation given in (1) will be activated by points $x$ within distance $r$ of $\\theta_{j}$ Therefore, $\\mathrm{diam}(C_{j})\\leq2r$ for all $j=1,\\dots,m$ where $r$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nu(B(x,r))\\geq\\frac{2\\left(k+c_{0}\\left(d\\log m+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}{m}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Lemma D.2, we can take $r$ to be the value ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{r=\\left(\\displaystyle\\frac{3\\sqrt{d}}{\\left(3/4\\right)^{\\left(d-1\\right)/2}}\\cdot\\frac{2\\left(k+c_{0}\\left(d\\log m+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}{m}\\right)^{1/\\left(d-1\\right)}}}\\\\ {{=\\displaystyle\\frac{2}{\\sqrt{3}}\\left(\\frac{6\\sqrt{d}\\left(k+c_{0}\\left(d\\log m+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}{m}\\right)^{1/\\left(d-1\\right)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{diam}(C_{j})}&{\\leq}&{2r\\leq\\displaystyle\\frac{4}{\\sqrt{3}}\\left(\\frac{6\\sqrt{d}\\left(k+c_{0}\\left(d\\log m+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}{m}\\right)^{1/(d-1)}}\\\\ &{\\leq}&{8\\left(\\displaystyle\\frac{\\left(k+c_{0}\\left(d\\log m+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)}{m}\\right)^{1/(d-1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where, for the last inequality, we have used the fact that $d^{1/(2(d-1))}\\leq\\sqrt{2}$ and for $d\\geq3$ itholds that $6^{1/(d-1)}\\leq\\sqrt{6}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma D.1 ([Chaudhuri and Dasgupta, 2010]). There is an absolute constant $c_{0}>0$ for which the followingholds.Pick any $0<\\delta<1$ Pick $\\theta_{1},\\ldots,\\theta_{m}$ independently at randomfrom a distribution $\\mu$ on $\\mathbb{R}^{d}$ .Thenwithprobability at least $1-\\delta$ any ball $B$ in $\\mathbb{R}^{d}$ with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nu(B)\\geq{\\frac{2}{m}}\\left(k+c_{0}\\left(d\\log m+\\log\\left({\\frac{1}{\\delta}}\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "contains at least $k$ of the $\\theta_{i}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma D.2 (Lemma 12 [Dasgupta and Tosh, 2020]). Suppose $d\\geq3,$ $r\\,\\in\\,(0,1)$ and $\\nu$ is the uniform distribution over $S^{d-1}$ . Then for any $x\\in S^{d-1}$ \uff0c ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nu(B(x,r))\\geq\\frac{1}{3\\sqrt{d}}r^{d-1}\\left(1-r^{2}/4\\right)^{(d-1)/2}\\geq\\frac{1}{3\\sqrt{d}}\\left(3/4\\right)^{(d-1)/2}r^{d-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Since the weights $w_{n,i}$ given in equation 4 are non-negative, in order to satisfy condition (i) of $c$ $f$ $\\mathbb{E}f(x)<\\infty$ $n$ $\\begin{array}{r}{,\\mathbb{E}\\left(\\sum_{i=1}^{n}w_{n,i}(x)f(x_{i})\\right)\\leq c\\mathbb{E}(\\bar{f}(x))}\\end{array}$   \nUsing Lemma D.3, we show that this condition is satisfied for $c=1$ ", "page_idx": 15}, {"type": "text", "text": "Concerning condition (i) of Stone's theorem, first define $\\begin{array}{r l r}{a_{n}}&{{}=}&{a_{n}(k_{n},m_{n},\\delta_{n})\\quad=}\\end{array}$ $\\begin{array}{r}{8\\left(\\frac{k_{n}+c_{0}\\left(d\\log m_{n}+\\log\\left(\\frac{1}{\\delta_{n}}\\right)\\right)}{m_{n}}\\right)^{1/(d-1)}}\\end{array}$ , where $c_{\\mathrm{0}}$ is an absolute constant. Then $\\operatorname*{lim}_{n\\to\\infty}\\delta_{n}\\;=\\;0$ and using condition (i) and (i) of this theorem we also have $\\operatorname*{lim}_{n\\to\\infty}a_{n}\\;=\\;0$ Now pick any $a>0$ . We can always find a positive integer $N$ such that for $n>N$ , we have $a>a_{n}$ satisfying $\\begin{array}{r}{\\sum_{i=1}^{n}w_{i,n}(x){\\mathbb I}_{\\{\\|x_{i}-x\\|>a\\}_{.}}\\!\\le\\!\\sum_{i=1}^{n}w_{i,n}(\\underline{{\\bar{x}}}){\\mathbb I}_{\\{\\|x_{i}-x\\|>a_{n}\\}}}\\end{array}$ for all $x\\in\\mathscr{X}$ . Replacing $k,m,\\delta$ and $a$ with $k_{n},\\delta_{n}$ and $a_{n}$ respectively in Lemma D.6, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\sum_{i=1}^{n}w_{i,n}(x)\\mathbb{I}[\\|x_{i}-x\\|>a]\\right)\\leq\\mathbb{E}\\left(\\sum_{i=1}^{n}w_{i,n}(x)\\mathbb{I}[\\|x_{i}-x\\|>a_{n}]\\right)\\leq\\delta_{n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking limit yields, $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}\\left(\\sum_{i=1}^{n}w_{i,n}(x)\\mathbb{I}[\\|x_{i}-x\\|>a]\\right)=0.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Concerning condition (i) of Stone's Theorem, note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left(\\underset{1\\leq i\\leq n}{\\operatorname*{max}}w_{n,i}(x)\\right)}&{\\leq}&{\\mathbb{E}\\left(\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\frac{1}{\\sum_{l=1}^{n}\\mathbb{I}[x_{l}\\in C_{j}]}\\right)=\\mathbb{E}\\left(\\frac{1}{k}\\sum_{i=1}^{k}\\frac{1}{\\sum_{l=1}^{n}\\mathbb{I}[x_{i}\\in C_{j_{i}(x,\\Theta)}]}\\right)}\\\\ &{=}&{\\mathbb{E}\\left(\\frac{1}{k}\\sum_{i=1}^{k}\\frac{1}{N_{j_{i}(x,\\Theta)}}\\right)\\leq\\mathbb{E}\\left(\\frac{1}{\\operatorname*{min}_{1\\leq i\\leq k}\\{N_{j_{i}(x,\\Theta)}\\}}\\right)~~~~~~~~~~~~~~~~~~~~~(19)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where, for $i=1,\\ldots,k$ , we have used the notation $j_{i}(x,\\Theta)$ to denote the $k$ random coordinates of $h_{\\Theta,k}(x)$ that are set to 1 and $N_{j_{i}(x,\\Theta)}$ to denote the number of points from $x_{1},\\ldots,x_{n}$ that fall in $C_{j_{i}(x,\\Theta)}$ . Now from equation 19, is it easy to see that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathbb{E}\\left(\\operatorname*{max}_{1\\leq i\\leq n}w_{n,i}(x)\\right)=0}\\end{array}$ since using lemma D.7, $\\mathrm{min}_{1\\le i\\le k}\\{N_{j_{i}(x,\\Theta)}\\}\\to\\infty$ in probability, as $n\\to\\infty$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D.4  Technical results for satisfying condition (i) of Stone's theorem (Theorem 3.2) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma D.3. For any non-negative measurable function $f:\\mathcal{X}\\to\\mathbb{R}_{+}$ satisfying $\\mathbb{E}f(x)<\\infty$ and for any $n$ $\\begin{array}{r}{\\mathbb{E}\\left(\\sum_{i=1}^{n}\\dot{w}_{n,i}(x)f(\\bar{x}_{1})\\right)\\leq\\mathbb{E}(f(x))}\\end{array}$ , where the weights $w_{n,i}$ are as given in (4). ", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\Bigg(\\frac{\\sum_{t=0}^{T}m_{t}(t)(x_{t})}{\\sum_{t=0}^{T}D(x_{t})}\\Bigg)}&{=\\mathbb{E}\\Bigg(\\frac{\\sum_{t=0}^{T}\\sum_{t=1}^{T}D(x_{t})G(\\hat{x}_{t})}{\\sum_{t=0}^{T}D(x_{t})\\prod_{s=1}^{T}D(x_{t})G(\\hat{x}_{t})}\\Bigg)\\Bigg)}\\\\ &{=\\mathbb{E}\\Bigg(\\frac{1}{b}\\sum_{t=0}^{T}\\Bigg(\\sum_{t=1}^{T}\\frac{\\sum_{t=0}^{T}C_{t}(\\hat{x}_{t})G(\\hat{x}_{t})}{\\sum_{t=0}^{T}D(x_{t})}\\Bigg)\\Bigg)}\\\\ &{=\\mathbb{E}\\Bigg(\\frac{1}{b}\\Bigg)\\Bigg[\\sum_{t=0}^{T}\\Bigg(\\sum_{t=1}^{T}\\frac{\\sum_{t=1}^{T}C_{t}(\\hat{x}_{t})G(\\hat{x}_{t})}{\\sum_{t=1}^{T}D(x_{t})G(\\hat{x}_{t})}\\Bigg)\\Bigg]\\Bigg)}\\\\ &{=\\mathbb{E}\\Bigg(\\frac{1}{b}\\Bigg[\\sum_{t=0}^{T}\\mathbb{E}\\Bigg(\\frac{\\sum_{t=1}^{T}D(x_{t})G(\\hat{x}_{t})}{\\sum_{t=1}^{T}D(x_{t})G(\\hat{x}_{t})}\\Bigg)\\Bigg]\\Bigg)}\\\\ &{\\stackrel{\\mathrm{d}}{\\leq}\\mathbb{E}_{x_{t}}\\Bigg(\\frac{1}{b}\\Bigg[\\sum_{t=0}^{T}\\frac{\\sum_{t=1}^{T}D(x_{t})G(\\hat{x}_{t})}{\\sum_{t=1}^{T}D(x_{t})G(\\hat{x}_{t})}\\Bigg]\\Bigg)}\\\\ &{=\\frac{\\sum_{t=1}^{T}\\bigg(\\sum_{t=0}^{T}D(x_{t})G(\\hat{x}_{t})\\bigg)\\int_{x_{t}}\\int_{x_{t}}(\\hat{x}_{t})G(\\hat{x}_{t})d x_{t}\\bigg)\\Bigg]}{\\sum_{t=1}^{T}\\bigg(\\sum_{t=0}^{T}D(x_{t})G(\\hat{x}_{t})\\bigg)\\int_{x_{t}}(\\hat{x}_{t})d x_ \n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{={\\mathbb{E}}_{\\frac{1}{\\alpha}}\\left(\\frac{\\displaystyle{\\sum_{i=0}^{3}\\sum_{j=1}^{n}\\mathbb{E}\\left(\\frac{\\displaystyle{\\sum_{i=0}^{n}\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]}}{\\displaystyle{\\sum_{i=0}^{n}\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]}}\\right)}\\right)}\\\\ &{\\leq{\\mathbb{E}}_{\\frac{1}{\\alpha}}\\left(\\frac{\\displaystyle{\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]}}{\\displaystyle{\\sum_{i=0}^{n}\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]}}\\right)}\\\\ &{={\\frac{\\displaystyle{\\sum_{i=1}^{3}\\mathbb{E}\\left(\\prod_{i=0}^{n}\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]\\right)}}{\\displaystyle{\\sum_{i=1}^{3}\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]}}}\\\\ &{={\\frac{\\displaystyle{\\sum_{i=1}^{3}\\sum_{j=1}^{n}\\mathbb{E}\\left(Z_{j}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]}}{\\displaystyle{\\sum_{i=0}^{n}\\mathbb{E}\\left(Z_{i}\\wedge\\ddots\\right)[\\rho_{i}(\\lambda)]}}}\\\\ &{\\vdots}\\\\ &{\\vdots}\\\\ &{\\frac{\\displaystyle{\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sum_{i=0}^{n}\\mathbb{E}\\left(Z_{j}^{i}\\wedge\\rho_{i}^{(2)}\\right)}}{\\displaystyle{\\sum_{i=1}^{3}\\sum_{j=1}^{n}\\sum_{i=0}^{n}\\mathbb{E}\\left(Z_{i}\\wedge\\rho_{i}^{(2)}\\right)}}\\int_{\\mathbb{Z}_{\\frac{1}{\\alpha}}}\\int_{\\mathbb{Z}_{\\frac{1}{\\alpha}}}\\int_{\\mathbb{Z}_{\\alpha}}\\int_{\\mathbb{Z}_{\\alpha}}|\\rho_{i}\\rangle\\rangle\\langle\\rho_{i}|\\rangle}\\\\ &{\\vdots}\\\\ &{\\frac{\\displaystyle{\\sum_{i=1}^{3}\\sum_{j=1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where, inequality $a$ follows from Lemma D.4. Equality $b$ follows from the following observation. In the line above inequality $b$ , we are summing $k\\,\\times\\,{\\binom{m}{k}}$ terms. Since $\\begin{array}{r}{k\\times\\binom{m}{k}=m\\stackrel{\\_}{\\times}\\binom{m-1}{k-1}}\\end{array}$ and any $j\\in[m]$ can appear in exactly $\\scriptstyle{\\binom{m-1}{k-1}}$ different subsets of of size $k$ , equality $b$ is simply rearranging the terms from the line above by changing the indices appropriately. Equality $c$ follows from part (iv) ", "page_idx": 16}, {"type": "text", "text": "of Lemma 3.4. Equality $d$ follows from part (i) of Lemma 3.4.Equality $e$ follows from the following observation. In the line above equality $e$ we are summing $m\\times\\bar{(}_{k-1}^{m-1})$ terms since any $j\\in[m]$ can appear in exactly $\\binom{m-1}{k-1}$ diffent subsetso of size $k$ Again noticing that $\\begin{array}{r}{m\\times\\binom{m-1}{k-1}=k\\times\\binom{m}{k}}\\end{array}$ and each $\\sigma(i)$ has $k$ terms in it, we are simply rearranging the terms from the line above by changing the indices appropriately. Finally, equality $f$ follows from part (i) of Lemma E.4 and equality $g$ follows from the fact that $x$ and $x_{1}$ are i.i.d. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "LemmaD.4. $\\begin{array}{r}{\\mathbb{E}\\left(\\sum_{i=1}^{n}\\frac{\\mathbb{I}[x_{i}\\in C_{j}]f(x_{i})}{\\sum_{l=1}^{n}\\mathbb{I}[x_{l}\\in C_{j}]}\\Big|x\\right)\\le\\frac{1}{\\mu(C_{j})}\\int_{C_{j}}f(x_{1})\\mu(d x_{1}).}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{\\xi}\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{\\|[x_{i}\\in C_{j}]f(x_{i})\\|}{\\sum_{l=1}^{n}\\|x_{l}\\in C_{j}\\|}\\left\\vert x\\right\\rangle\\right)}&{=}&{\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left(\\frac{\\|[x_{i}\\in C_{j}]f(x_{i})\\|}{\\sum_{l=1}^{n}\\|[x_{l}\\in C_{j}]\\|}x\\right)}\\\\ &{\\triangleq}&{\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left(\\frac{\\|[x_{i}\\in C_{j}]f(x_{i})\\|}{1+\\sum_{l\\neq i}\\|[x_{l}\\in C_{j}]\\|}x\\right)}\\\\ &{=}&{n\\mathbb{E}\\left(I_{\\{x_{i}\\in C_{j}\\}}f(x_{1})\\frac{1}{1+\\sum_{l=2}^{n}\\|[x_{l}\\in C_{j}]\\|}x\\right)}\\\\ &{=}&{n\\mathbb{E}_{x_{1}}\\left(I_{\\{x_{i}\\in C_{j}\\}}f(x_{1})\\mathbb{E}_{\\mathbb{Z}_{2},\\ldots,x_{n}}\\left[\\frac{1}{1+\\sum_{l=2}^{n}\\|[x_{l}\\in C_{j}]\\|}x,x_{1}\\right]\\right)}\\\\ {\\overset{b}{\\leq}}&{n\\mathbb{E}_{x_{1}}\\left(\\frac{1}{[x_{1}\\in C_{1}]}f(x_{1})\\frac{1}{n\\mu(C_{j})}\\right)}\\\\ &{=}&{\\displaystyle\\frac{1}{\\mu(C_{j})}\\int_{C_{j}}f(x_{1})\\mu(d x_{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where, equality $a$ follows from that fact, that if $x_{i}\\in C_{j}$ , then the term in the parenthesis does not change and if $x_{i}\\notin C_{j}$ , then the term within the parenthesis is zero, and inequality $b$ follows from lemma D.5 since conditioned on $x$ and $x_{1}$ , the random variable $\\stackrel{\\rightharpoonup}{\\sum}_{l=2}^{n}\\mathbb{I}_{\\{x_{l}\\in\\bar{c_{j}}\\}}$ is Binomially distributed with parameters $(n-1)$ and $\\mu(C_{j})$ \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma D.5. (Binomial bound Gyorf et al. [2002]) Let the random variable $B(n,p)$ beBinomially distributedwithparametersnand $p$ Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left({\\frac{1}{1+B(n,p)}}\\right)\\leq{\\frac{1}{(n+1)p}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.5 Technical results for satisfying condition (i) of Stone's theorem (Theorem 3.2) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma D.6. Pick any $0<\\delta<1$ andlet $\\begin{array}{r}{a=8\\left(\\frac{k+c_{0}\\left(d\\log m+\\log\\left(\\frac{1}{\\delta}\\right)\\right)}{m}\\right)^{1/(d-1)}}\\end{array}$ 1), where $c_{0}>0$ is an absolute constant defined in Lemma 3.5. Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\sum_{i=1}^{n}w_{i,n}(x)\\mathbb{I}[\\|x_{i}-x\\|>a]\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. From Lemma 3.5, we have that with probability at least $1\\!-\\!\\delta$ \uff0c $\\mathrm{diam}(\\mathscr{C}_{j})\\leq a$ for all $1\\leq j\\leq m$ Let us define the random variable: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\cal A}_{1}=\\left\\{\\sum_{i=1}^{n}\\left({\\frac{1}{k}}\\sum_{j:x\\in C_{j}}{\\frac{\\mathbb{I}[x_{i}\\in C_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in C_{j}]}}\\right)\\mathbb{I}[\\|x_{i}-x\\|>a]\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the event: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{A}_{2}=\\left\\{\\cup_{j:x\\in C_{j}}C_{j}\\subset B(x,a)\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\mathcal{A}_{2}^{c}$ denotes complement of the event $\\boldsymbol{A}_{2}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left(\\displaystyle\\sum_{i=1}^{n}w_{i,n}(x)\\mathbb{I}[\\|x_{i}-x\\|>a]\\right)}&{=}&{\\mathbb{E}\\left[\\displaystyle\\sum_{i=1}^{n}\\!\\left(\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\frac{\\mathbb{I}[x_{i}\\in C_{j}]}{\\sum_{i=1}^{n}\\mathbb{I}[x_{i}\\in C_{j}]}\\right)\\mathbb{I}_{\\{\\|x_{i}-x\\|>a\\}}\\right]}\\\\ &{=}&{\\mathbb{E}\\left(A_{1}|A_{2}\\right)\\operatorname*{Pr}\\left(A_{2}\\right)+\\mathbb{E}\\left(A_{1}|A_{2}^{c}\\right)\\operatorname*{Pr}\\left(A_{2}^{c}\\right)}\\\\ &{\\stackrel{a}{\\le}}&{0\\cdot\\operatorname*{Pr}(A_{2})+1\\cdot\\delta=\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequality follows from the fact that maximum value of $\\mathcal{A}_{1}$ is 1 and conditioned on the event $\\boldsymbol{A}_{2}$ ,valueof $\\mathcal{A}_{1}$ is O. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "D.6 Technical results for satisfying condition (ii) of Stone's theorem (Theorem 3.2) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma D.7. For any $x\\ \\sim\\ \\mu,$ let $j_{1}(x,\\Theta),\\dots,j_{k}(x,\\Theta)\\;\\in\\;[m]$ be the $k$ random coordinates of $h_{\\Theta,k}(x)$ that are set to $^{\\,l}$ .Let $N_{j_{1}(x,\\Theta)},\\dots,N_{j_{k}(x,\\Theta)}$ be the number of data points faling in $C_{j_{1}(x,\\Theta)},\\dots,C_{j_{k}(x,\\Theta)}$ respectively. Then $\\operatorname*{min}\\{N_{j_{1}(x,\\Theta)},\\ldots,N_{j_{k}(x,\\Theta)}\\}\\rightarrow\\infty$ in probability, whenever $n\\to\\infty$ and $m^{k}/n\\rightarrow0$ as $n\\to\\infty$ ", "page_idx": 18}, {"type": "text", "text": "Proof. For any random $x\\in\\mathscr{X}$ et $C_{i(x,\\Theta)}^{k}$ be the rand cellof the paiion $\\{C_{i}^{k}\\}_{i=1}^{\\binom{m}{k}}$ which sets k random cordiaes ;(,(),,(2, () of h,(x) oe. Then, Cl.) ;(:) Let $\\begin{array}{r}{N_{n}(x,\\Theta)=\\sum_{i=1}^{n}\\mathbb{I}_{\\{x_{i}\\in C_{i(x,\\Theta)}^{k}\\}}}\\end{array}$ bthenefdatapnts fllinacelas $x$ Wwe first we show that $N_{n}(x,\\Theta)\\to\\infty$ in probability Let $N_{1},N_{2},\\ldots N_{{\\binom{m}{k}}}$ be the number of points of $x,x_{1},\\ldots,x_{n}$ falling in the $\\binom{m}{k}$ respective cells $C_{1}^{k},\\ldots,C_{\\binom{m}{k}}^{k}$ $S=\\{x,x_{1},\\ldots,x_{n}\\}$ set of positions of these $n+1$ points. Since these points are independent and identically distributed, fixing the set $S$ (but not the order of the points) and $\\Theta$ , the conditional probability that $x$ falls in the $i^{t h}$ cell $C_{i}^{k}$ .s $N_{i}/(n+1)$ . Then for any fixed integer $t>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\operatorname*{Pr}(N_{n}(x,\\Theta)<t)}&{=}&{\\mathbb{E}\\left[\\operatorname*{Pr}\\left(N_{n}(x,\\Theta)<t|S,\\Theta\\right)\\right]}\\\\ &{=}&{\\mathbb{E}\\left[\\displaystyle\\sum_{i:N_{i}<t}\\frac{N_{i}}{n+1}\\right]\\leq(t-1)\\frac{\\binom{m}{k}}{n+1}\\leq(t-1)\\frac{m^{k}}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which converges to zero on our assumption on $n$ .Since Ch(,) \u2264 Ci(x,) for l = 1,...,k, we have $N_{n}(x,\\Theta)\\le\\mathrm{min}\\{N_{j_{1}(x,\\Theta)},\\cdot\\cdot\\cdot,N_{j_{k}(x,\\Theta)}\\}$ and the statement of the Lemma follows. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E  Various proofs from section 3.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For ease of exposition, we use the notation $x_{[1,n]}$ to denote $x_{1},\\ldots,x_{n}$ and $y_{[1,n]}$ todenote $y_{1},\\ldots,y_{n}$ for various proofs appearing in the section. ", "page_idx": 19}, {"type": "text", "text": "E.1Proof of lemma 3.8 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Fix any $\\Theta$ . This ensures that $C_{1},\\ldots,C_{m}$ are fixed. Now, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathcal{U},\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,1}\\otimes_{\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,1}^{T}\\right]\\otimes_{\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,2}^{T}\\right]\\otimes_{\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,1}^{T}\\right]\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,1}^{T}\\otimes_{\\mathcal{U},\\mathcal{U}}\\left[\\left(\\frac{\\frac{1}{\\mathcal{Z}}}{\\mathcal{U}_{i,2}}\\sum_{l=0}^{T}\\left(l_{0}-\\Theta_{l}\\right)^{T}\\right)\\left[\\mathcal{U}_{i,3}\\right]\\right]\\right]}\\\\ &{\\vdots}\\\\ &{\\mathbb{E}_{\\mathcal{U},\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,n-1,i}\\left[\\frac{1}{\\mathcal{Z}}\\sum_{l=0}^{T}\\left(l_{0}-\\Theta_{l}\\right)^{T}\\left[\\mathcal{U}_{i,n}\\right]\\right]\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,n-1,i}\\left[\\frac{1}{\\mathcal{Z}}\\sum_{l=0}^{T}\\left(\\frac{\\sum_{t=0}^{T}\\|h_{t}\\|^{2}\\|h_{t}\\|^{2}\\|^{2}}{\\mathcal{U}_{i,n}^{T}}\\mathbb{E}_{t}\\mathbb{E}_{t}\\right)^{T}\\left[1\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\mathbb{E}_{\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,1}^{T}\\right]\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{U},\\mathcal{U}}\\left[\\mathcal{U}_{i,n-1,i}\\left[\\frac{1}{\\mathcal{Z}}\\sum_{l=0}^{T}\\left(\\frac{\\sum_{t=0}^{T}\\|h_{t}\\|^{2}\\|h_{t}\\|^{2}\\|^{2}}{\\mathcal{U}_{i,n}^{T}}\\mathbb{E}_{t}\\right)-\\mathbb{E}_{\\mathcal{U},\\mathcal{U}}\\left(\\mathcal{U}_{i,n}^{T}\\right)\\right]\\mathbb{I}_{\\mathcal{U},\\mathcal{U}}\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{U},\n$$$$\n\\begin{array}{r l}{\\lefteqn{-\\mathbb{E}_{\\eta_{1},\\ldots,\\ldots,\\eta_{n}}\\left[\\sum_{k_{\\ell}=0}^{\\infty}\\left(\\frac{\\sum_{k_{\\ell}=0}^{n-1}\\sum_{i=0}^{i-1}e^{-\\xi_{i}}}{\\eta_{k_{\\ell}}(e_{1})}e^{\\xi_{i}}\\right)^{2}|\\eta_{k_{\\ell}}|\\right]}}\\\\ {=\\mathbb{E}_{\\eta_{1},\\ldots,\\eta_{n}}\\left[\\mathbb{E}_{\\eta_{1},\\ldots,\\eta_{n}}\\left[\\frac{\\sum_{k_{\\ell}=0}^{n-1}\\sum_{i=0}^{i-1}e^{-\\xi_{i}}}{\\eta_{k_{\\ell}}(e_{1})}e^{\\xi_{i}}\\right]-\\psi_{1}e_{1}(\\xi_{1})\\right]}\\\\ {=\\mathbb{E}_{\\eta_{1},\\ldots,\\eta_{n}}\\left[\\mathbb{E}_{\\eta_{2},\\ldots,\\eta_{n}}\\left[\\frac{\\sum_{k_{\\ell}=0}^{n-1}(\\sum_{i=0}^{n-1}(\\sum_{j=0}^{n-1}\\sum_{i=0}^{i-1}e_{-\\xi_{i}}))^{2}|\\eta_{k_{\\ell}}(e_{1})>\\eta_{1}|\\right]}{\\eta_{k_{\\ell}}(e_{1})}e^{-\\xi_{i}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\right.}\\\\ {\\vdots\\mathbb{E}_{\\eta_{n}}\\left[\\left[\\frac{\\sum_{k_{\\ell}=0}^{n-1}\\sum_{i=0}^{i-1}e^{-\\xi_{i}}}{\\eta_{k_{\\ell}}(e_{1})}\\left[\\frac{\\sum_{k_{\\ell}=0}^{n-1}(\\sum_{i=0}^{n-1}\\sum_{i=0}^{i-1}e_{-\\xi_{i}})}{\\eta_{k_{\\ell}}(e_{1})}e^{-\\xi_{i}}\\right]\\right]+\\eta_{n}(e_{1})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\right.}\\\\ {=\\mathbb{E}_{\\eta_{n}}\\left[\\mathbb{E}_{\\eta_{1},\\ldots,\\eta_{n}}\\left[\\frac{\\sum_{k \n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where, inequality $a$ is due to Jensen's inequality. Inequality $b$ follows from lemma E.3. Inequality $c$ follows from lemma E.1 and inequality $d$ follows from lemma E.2. Finally inequality $e$ follow from the observation that $n\\mu_{n}(C_{j})$ is Binomially distributed with parameters $n$ and $\\mu(C_{l})$ and by an application of lemma E.4. The result follows noting that by Jensen's inequality $\\mathbb{E}|\\hat{\\eta}(x)-\\bar{\\eta}(x)|\\le$ $\\sqrt{\\mathbb{E}(\\hat{\\eta}(x)-\\bar{\\eta}(x))^{2}}$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma E.1. Pick $m\\,\\times\\,d$ projection matrix $\\Theta$ Suppose theEaSrepresentationuses $(i)\\ a$ mapping $\\Theta$ and (ii) $k$ -winner-take-allsparsification.Let $x$ be sampled from $\\mu$ and let ${\\cal D}_{n}\\ =$ ${\\big(}(x_{1},y_{1}),\\ldots,(x_{n},y_{n}){\\big)}$ isarandomtrainingsetwhere $x_{i}$ is sampled from $\\mu$ and $y_{i}$ is distributed as $\\eta(\\boldsymbol{x}_{i})$ for $i\\in[n]$ .Thenthefollowingholds. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{G}_{y_{[1,n]}}\\left[\\sum_{j:x\\in C_{j}}\\left(\\frac{\\sum_{i=1}^{n}(y_{i}-\\eta(C_{j}))\\mathbb{I}[x_{i}\\in C_{j}]}{n\\mu_{n}(C_{j})}\\right)^{2}\\mathbb{I}[n\\mu_{n}(C_{j})>0]\\right]x,x_{[1,n]}\\right]\\leq\\frac{1}{4}\\sum_{j:x\\in C_{j}}\\left(\\frac{\\mathbb{I}[n\\mu_{n}(C_{j})>\\eta(C_{j})]}{n\\mu_{n}(C_{j})}\\right)^{2}\\mathbb{I}[n\\mu_{n}(C_{j})>0].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Conditioned on $x$ , only $k$ of the $m$ coordinates in the EaS representation of $x$ are non-zero. WLOG, for ease of exposition, assume these $k$ non-zero coordinate to be $j_{1},\\dots,j_{k}\\in[m]$ . Then the number of $x_{i}$ that falls in any such $C_{j\\imath}$ , where $l\\in[k]$ , is $n\\mu_{n}(C_{j_{l}})$ . The $y_{i}$ values corresponding to these $x_{i}$ points (there are $n\\mu_{n}(C_{j_{l}})$ of them in total) are identically and independently distributed with expectation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{\\mathbb{E}(y_{i}|x_{i}\\in C_{j_{l}})}&{=}&{\\operatorname*{Pr}(y_{i}=1|x_{i}\\in C_{j_{l}})=\\displaystyle\\frac{1}{\\mu(C_{j_{l}})}\\int_{C_{j_{l}}}\\operatorname*{Pr}(y_{i}=1|x_{i}=x)\\mu(d x)}\\\\ &{=}&{\\displaystyle\\frac{1}{\\mu(C_{j_{l}}}\\int_{C_{j_{l}}}\\eta(x)\\mu(d x)=\\eta(C_{j_{l}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{y_{1},\\ldots}\\left[\\underset{j\\neq\\ell,j}{\\sum}_{i=1}^{n}\\left(\\mathcal{Y}_{i}-\\eta(\\ell_{j}))\\mathbb{I}[x_{i}\\in C_{j}]\\right)^{2}\\mathbb{I}[\\eta u_{n}(C_{j})>0]\\Bigg|x,x_{1,n}]}\\\\ &{=\\sum_{l=1}^{k}\\left[\\frac{\\mathbb{E}_{y_{1},\\ldots}\\left[(\\sum_{i=1}^{n}(y_{i}-\\eta(C_{j})))\\mathbb{I}[x_{i}\\in C_{j}]\\right]^{2}\\mathbb{I}[\\eta u_{n}(C_{j})>0]\\Bigg|x_{1,n}]}{(n\\mu_{n}(C_{j}))^{2}}\\right]^{2}}\\\\ &{\\overset{=}\\frac{k}{l-1}\\left[\\frac{\\sum_{i=1}^{n}\\mathbb{E}_{y_{i}}\\left[(y_{i}-\\eta(C_{j}))^{2}\\mathbb{I}[x_{i}\\in C_{i}]\\mathbb{I}[\\eta u_{n}(C_{j})>0]\\right|x_{i}]}{(n\\mu_{n}(C_{j}))^{2}}\\right]}\\\\ &{\\overset{,}{\\le}\\frac{k}{l-1}\\left[\\frac{\\sum_{i=1}^{n}\\eta(C_{j})(1-\\eta(C_{j}))\\mathbb{I}[x_{i}\\in C_{j}]\\mathbb{I}[\\eta u_{n}(C_{j})>0]}{(n\\mu_{n}(C_{j}))^{2}}\\right]}\\\\ &{\\overset{,}{\\le}\\frac{1}{4}\\sum_{l=1}^{k}\\left(\\frac{[\\mathbb{I}[\\eta u_{n}(C_{j})>0]]}{\\eta\\mu_{n}(C_{j})}\\right)^{2}}\\\\ &{\\overset{\\le}\\frac{1}{4}\\frac{k}{l-1}\\left(\\frac{[\\mathbb{I}[\\eta u_{n}(C_{j})>0]]}{\\eta\\mu_{n}(C_{j})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where, equality $a$ is due to the following observation. For any $i,j\\,\\in\\,[m],i\\,\\neq\\,j$ and $x_{i},x_{j}\\in C_{l}$ for some $\\bar{l}\\in[m]$ \uff0c $y_{i}$ and $y_{j}$ are identically and independently distributed with expectation $\\eta(C_{l})$ Therefore, the expectation of the cross product is simply: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}_{y_{i},y_{j}}\\left[(y_{i}-\\eta(C_{l}))(y_{j}-\\eta(C_{l})\\right]}&{=}&{\\mathbb{E}_{y_{i},y_{j}}\\left[y_{i}y_{j}-\\eta(C_{l})(y_{i}+y_{j})+\\eta(C_{l})^{2}\\right]\\quad\\quad}\\\\ &{=}&{\\mathbb{E}y_{i}\\mathbb{E}y_{j}-\\eta(C_{l})(\\mathbb{E}y_{i}+\\mathbb{E}y_{j})+\\eta(C_{l})^{2}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Equality $b$ follows from variance computation. In particular for any $Y_{i},i\\in[m]$ with $x_{i}\\in C_{l}$ for some $l\\in[m],\\mathbb{E}\\left[(y_{i}-\\eta(C_{l}))^{2}\\right]=\\mathbb{E}y_{i}^{2}-2\\eta(C_{l})\\mathbb{E}y_{i}+\\eta(C_{l})^{2}=\\eta(C_{l})-\\dot{2}\\eta(C_{l})^{2}+\\eta(C_{l})^{2}=\\eta(C_{l})-\\dot{2}\\eta(C_{l})^{2},$ $\\eta(C_{l})(1-\\eta(C_{l}))$ . Finally, inequality $c$ follows from the fact that for any $z\\in[0,1]$ , the maximum value of $z(1-z)$ .s $\\textstyle{\\frac{1}{4}}$ ", "page_idx": 20}, {"type": "text", "text": "It is easy to observe that the fnal result is equivalent to :EC $\\begin{array}{r}{\\frac{1}{4}\\sum_{j:x\\in C_{j}}\\left(\\frac{\\mathbb{I}[n\\mu_{n}(C_{j})>0]}{n\\mu_{n}(C_{j})}\\right)}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma E.2.Pick $m\\,\\times\\,d$ projection matrix $\\Theta$ \uff1aSuppose theEaSrepresentationuses (i)a mapping $\\Theta$ and (ii) $k$ winner-take-allsparsification.Let $x$ be sampled from $\\mu$ and let ${\\cal D}_{n}\\ =$ ${\\big(}(x_{1},y_{1}),\\ldots,(x_{n},y_{n}){\\big)}$ is a randomtrainingsetwhere $x_{i}$ is sampled from $\\mu$ and $y_{i}$ is distributed as $\\eta(\\boldsymbol{x}_{i})$ for $i\\in[n]$ . Then conditioned on $x_{1},\\ldots,x_{n}$ , the following holds. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left(\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\frac{\\mathbb{I}_{\\{n\\mu_{n}(C_{j})>0\\}}}{n\\mu_{n}(C_{j})}\\right)\\le\\frac{1}{k}\\sum_{j=1}^{m}\\left(\\frac{\\mu(C_{j})\\mathbb{I}_{\\{n\\mu_{n}(C_{j})>0\\}}}{n\\mu_{n}(C_{j})}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{x}\\left(\\frac{1}{k}\\sum_{j=\\in\\mathcal{C}_{j}}\\frac{\\|[\\mu_{t n}(C_{j})>0]\\|}{n\\mu_{t}(C_{j})}\\right)}&{\\triangleq\\displaystyle\\sum_{i=1}^{(\\widetilde{x})}\\left(\\operatorname*{Pr}(x\\in C_{i}^{k})\\left(\\frac{1}{k}\\sum_{j\\in\\mathcal{C}_{i}}\\frac{\\|[\\mu_{t n}(C_{j})>0]\\|}{n\\mu_{t n}(C_{j})}\\right)\\right)}\\\\ &{=\\displaystyle\\sum_{i=1}^{(\\widetilde{x})}\\left(\\frac{1}{k}\\left(\\sum_{j\\in\\mathcal{C}_{i}}\\frac{\\mu(C_{i}^{k})\\|[\\mu_{t n}(C_{j})>0]}{n\\mu_{t n}(C_{j})}\\right)\\right)}\\\\ &{\\triangleq\\displaystyle\\frac{1}{k}\\sum_{j=1}^{m}\\sum_{i\\neq j\\in\\mathcal{C}_{i}}\\left(\\frac{\\mu(C_{i}^{k})^{\\|[\\mu_{t n}(C_{j})>0]\\|}}{n\\mu_{t n}(C_{j})}\\right)}\\\\ &{=\\displaystyle\\:\\frac{1}{k}\\sum_{j=1}^{m}\\left(\\frac{\\|[\\mu_{t n}(C_{j})>0]\\|}{n\\mu_{t n}(C_{j})}\\right)\\sum_{i,j\\in\\mathcal{C}_{i}(i)}\\mu(C_{i}^{k})}\\\\ &{\\triangleq\\displaystyle\\:\\frac{1}{k}\\sum_{j=1}^{m}\\left(\\frac{\\mu(C_{j})\\|[\\mu_{t n}(C_{j})>0]}{n\\mu_{t n}(C_{j})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where guaity $a$ $\\{C_{i}^{k}\\}_{i=1}^{\\binom{m}{k}}$ forms a arition of $\\mathcal{X}$ and the definition of $\\sigma$ in section 3.2. Equality $b$ follows from the following observation. In the line above inequality $b$ , we are summing $k\\,\\times\\,{\\binom{m}{k}}$ terms. Since $\\begin{array}{r}{k\\times\\binom{m}{k}=m\\times\\binom{m-1}{k-1}}\\end{array}$ and any $j\\in[m]$ can appear in exactly $\\binom{m-1}{k-1}$ different subsets of of size $k$ , equality $b$ is simply rearranging the terms from the line above by changing the indices appropriately. Equality $c$ follows from part (iv) of lemma 3.4. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma E.3. Pick $m\\,\\times\\,d$ projection matrix $\\Theta$ Suppose theEaSrepresentation uses $(i)\\ a$ mapping $\\Theta$ and (ii) $k$ -winner-take-all sparsification.Let $x$ be sampled from $\\mu$ and let ${\\cal D}_{n}\\ =$ ${\\big(}(x_{1},y_{1}),\\ldots,(x_{n},y_{n}){\\big)}$ is arandomtrainingsetwhere $x_{i}$ is sampled from $\\mu$ and $y_{i}$ is distributed as $\\eta(\\boldsymbol{x}_{i})$ for $i\\in[n]$ .Thenthefollowingholds. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{[1,n]}}\\left[\\mathbb{E}_{x,y_{[1,n]}}\\left[\\frac{1}{k}\\sum_{j:x\\in C_{j}}\\eta^{2}(C_{j})\\mathbb{I}[\\mu_{n}(C_{j})=0]\\bigg|x_{[1,n]}\\right]\\right]\\leq\\frac{m}{n k e}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\operatorname{\\mathbb{E}}_{\\lambda\\geq0,\\eta\\in\\mathbb{Z}}\\Bigg[\\mathbb{E}_{\\lambda}\\Bigg[\\sum_{i=0}^{\\infty}\\exp(C_{j}(X_{i}(x))\\log(C_{j}))=\\alpha\\Bigg]\\Bigg]}\\\\ &{\\leq\\mathbb{E}_{\\lambda\\geq\\eta\\in\\mathbb{Z}}\\Bigg[\\sum_{i=0}^{\\infty}\\exp\\Big(\\sum_{i=1}^{\\infty}\\sum_{j=0}^{\\infty}(C_{j}(X_{i}(x))\\log(C_{j})-\\alpha_{j}(x))\\Big)\\Bigg]}\\\\ &{=\\mathbb{E}_{\\lambda\\geq\\eta\\in\\mathbb{Z}}\\Bigg[\\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty}\\exp\\Big(E_{\\lambda}\\sum_{i=0}^{\\infty}\\exp(C_{j}(X_{i}(x))\\log(C_{j})-\\alpha_{j}(x)\\Big)\\Big)\\Bigg]}\\\\ &{=\\mathbb{E}_{\\lambda\\geq\\eta\\in\\mathbb{Z}}\\Bigg[\\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty}\\exp(C_{j}(X_{i}(x))\\log(C_{j})-\\alpha_{j}(x))\\Bigg]}\\\\ &{\\leq\\mathbb{E}_{\\lambda\\geq\\eta\\in\\mathbb{Z}}\\Bigg[\\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty}(\\sum_{i=0}^{\\infty}(C_{j}(X_{i}(x))\\log(C_{j})-\\alpha_{j}(x)))\\Bigg]}\\\\ &{=\\frac{1}{\\lambda^{2}}\\sum_{i=0}^{\\infty}(\\gamma_{1}\\zeta_{1}\\zeta_{2}\\zeta_{1}\\zeta_{2}\\zeta_{2})(\\zeta_{1}\\zeta_{2}\\zeta_{1}\\zeta_{2})(\\zeta_{2}-\\alpha_{1}\\zeta_{1}\\zeta_{1}\\zeta_{2})(\\zeta_{2})}\\\\ &{=\\frac{1}{\\lambda}\\sum_{i=0}^{\\infty}(\\gamma_{1}\\zeta_{1}\\zeta_{2}\\zeta_{1}\\zeta_{2})(\\zeta_{1}\\zeta_{2}\\zeta_{2})(\\zeta_{1}-\\alpha_{1}\\zeta_{2}))^{\\lambda}-\\frac{1}{\\alpha}\\sum_{i=1}^{\\infty}(\\gamma_{2}\\zeta_{2}\\zeta_{1})(\\zeta_{2}\\zeta_{2})(\\zeta_{1}-\\alpha_{1}\\zeta_{1}) \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where equality $a$ follows from the fact that the quantity with the inner square bracket is unaffected by the $Y_{i}\\mathbf{s}$ Equality $b$ $\\{C_{i}^{k}\\}_{i=1}^{\\bar{(}_{k}^{m})}$ forms a partion of $\\mathcal{X}$ and the definition of $\\sigma$ in section 3.2. Equality $c$ follows from the following observation. In the line above inequality $c$ we are summing $k\\,\\times\\,{\\binom{m}{k}}$ terms. Since $\\textit{k}\\times\\binom{m}{k}=\\overset{\\cdot}{m}\\times\\binom{m-1}{k-1}$ and any $j\\in[m]$ can appear in exactly $\\binom{m-1}{k-1}$ different subsets of of size $k$ equality $c$ is simply rearranging the terms from the line above by changing the indices appropriately. Inequality $d$ follows from the fact $\\operatorname*{max}_{j}\\eta(C_{j})\\leq1$ Finally inequality $e$ follows from the fact that $\\begin{array}{r}{\\operatorname*{sup}_{z}\\dot{z}e^{-z}=\\frac{1}{e}}\\end{array}$ \uff1a \u53e3 e' ", "page_idx": 22}, {"type": "text", "text": "Lemma E.4. (Binomial bound Gyorfi et al. [2002]) Let the random variable $B(n,p)$ beBinomially distributedwithparametersnand $p$ Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(\\frac{1}{B(n,p)}\\mathbb{I}[B(n,p)>0]\\right)\\leq\\frac{2}{(n+1)p}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E.2Proof of Lemma 3.10 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. Take any $x\\in\\mathscr{X}$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert\\eta(x)-\\bar{\\eta}(x)\\right\\rvert=\\displaystyle\\left\\lvert\\frac{1}{b}\\sum_{j>\\epsilon\\in\\mathcal{C}_{j}}\\frac{1}{b}\\right\\rvert=\\displaystyle\\left\\lvert\\frac{1}{b}\\sum_{\\epsilon\\in\\mathcal{C}_{j}}(\\eta(x)-\\eta(x^{\\prime})\\right\\rvert\\leq\\displaystyle\\frac{1}{b}\\sum_{j>\\epsilon\\in\\mathcal{C}_{j}}\\vert\\eta(x)-\\bar{\\eta}_{j}\\vert}\\\\ {\\displaystyle=\\frac{1}{b}\\sum_{j>\\epsilon\\in\\mathcal{C}_{j}}\\vert\\eta(x)-\\frac{1}{\\beta(C)}\\int_{\\mathcal{C}_{j}}\\eta(x^{\\prime})/\\theta(x^{\\prime})\\vert}\\\\ {\\displaystyle=\\frac{1}{b}\\sum_{j>\\epsilon\\in\\mathcal{C}_{j}}\\left\\lvert\\frac{1}{\\mu(C)}\\int_{\\mathcal{C}_{j}}\\eta(x)-\\eta(x^{\\prime})\\right\\rvert\\mu(x^{\\prime})}\\\\ {\\displaystyle\\leq\\frac{1}{b}\\sum_{j>\\epsilon\\in\\mathcal{C}_{j}}\\frac{1}{\\beta(C)}\\int_{\\mathcal{C}_{j}}\\vert\\eta(x)-\\eta(x^{\\prime})\\vert\\mu(d x^{\\prime})}\\\\ {\\displaystyle\\leq\\frac{1}{b}\\sum_{j>\\epsilon\\in\\mathcal{C}_{j}}\\frac{1}{\\beta(C)}\\int_{\\mathcal{C}_{j}}L\\vert\\vert x-x^{\\prime}\\vert^{3}\\mu(d x^{\\prime})}\\\\ {\\displaystyle\\leq\\frac{1}{b}\\sum_{j>\\epsilon\\in\\mathcal{C}_{j}}\\frac{L\\cdot(\\mathrm{dim}(C_{j}))^{3}}{\\mu(C)}\\int_{\\mathcal{C}_{j}}\\mu(d x^{\\prime})\\leq L\\cdot\\operatorname*{max}_{j\\in\\mathcal{C}_{j}}(\\mathrm{dim}(C_{j}))^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E.3Proof of Theorem 3.15 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. Define $\\eta:\\mathcal{X}_{1}\\rightarrow[0,1]$ to be a triangular function defined below: for $0<\\theta\\leq2\\pi$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta(\\cos\\theta,\\sin\\theta,0,0,\\dots,0)={\\binom{\\frac{\\theta}{\\pi},}{2-{\\frac{\\theta}{\\pi}}}},\\quad{\\mathrm{if~}}\\theta\\leq\\pi\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Clearly, $\\eta$ .sb $\\frac{1}{2}$ Lipschiz and for $k=1$ \uff0c1 $\\begin{array}{r}{\\bar{\\eta}(x)=\\sum_{j=1}^{m}{\\eta(C_{j})\\mathbb{I}_{\\{x\\in C_{j}\\}}}}\\end{array}$ Therefore, using Theorem E.5, with probability at least $1/2$ over the choice of $\\Theta$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathcal{X}_{1}}|\\eta(x)-\\bar{\\eta}(x)|\\ge c_{d}^{\\prime\\prime}\\cdot\\frac{1}{m^{1/(d-1)}\\log m}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $c_{d}^{\\prime\\prime}$ is am absolute constant depending on $d$ . Taking expectation, with probability at least $1/2$ over the choice of $\\Theta$ , we have, $\\begin{array}{r}{\\mathbb{E}_{X,D_{n}}|\\bar{\\eta}(X)-\\eta(X)|\\ge c_{d}^{\\prime\\prime}\\cdot\\frac{1}{m^{1/(d-1)}\\log m}}\\end{array}$ Next, using Lemma 3.8 with $k=1$ yields, $\\begin{array}{r}{\\mathbb{E}_{X,D_{n}}|\\hat{\\eta}(X)-\\bar{\\eta}(X)|\\le\\sqrt{\\frac{m}{n}}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Using triangle inequality combining these results, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}_{X,D_{n}}|\\eta(X)-\\hat{\\eta}(X)|}&{=}&{\\mathbb{E}_{X,D_{n}}|\\eta(X)-\\bar{\\eta}(X)+\\bar{\\eta}(X)-\\hat{\\eta}(X)|}\\\\ &{\\geq}&{\\mathbb{E}_{X,D_{n}}|\\bar{\\eta}(X)-\\eta(X)|-\\mathbb{E}_{X,D_{n}}|\\hat{\\eta}(X)-\\bar{\\eta}(X)|}\\\\ &{\\geq}&{c_{d}^{\\prime\\prime}\\cdot\\frac{1}{m^{1/(d-1)}\\log m}-\\sqrt{\\frac{m}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Ignoring the log term we see that for large enough $n$ , the first term on the right hand side of (20) will dominate. In particular, for $n=\\Omega\\left(m^{\\bar{1}+\\frac{2}{d-1}}\\right)$ , we get $\\mathbb{E}_{X,D_{n}}|\\eta(X)-\\hat{\\eta}(X)|\\ge\\Omega\\left(m^{-\\frac{1}{d-1}}\\right)$ To se this (ignoring the log term) if $\\scriptstyle{\\sqrt{\\frac{m}{n}}}\\,\\leq\\,{\\frac{c_{d}^{\\prime\\prime}}{2}}\\,\\cdot\\,{\\frac{1}{m^{1/(d-1)}}}$ , then $\\begin{array}{r}{\\mathbb{E}_{X,D_{n}}|\\eta(X)-\\hat{\\eta}(X)|\\ge\\frac{c_{d}^{\\prime\\prime}}{2}\\cdot\\frac{1}{m^{1/(d-1)}}}\\end{array}$ Now, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{m}{n}}\\leq\\frac{c_{d}^{\\prime\\prime}}{2}\\cdot\\frac{1}{m^{1/(d-1)}}\\Rightarrow n\\geq\\frac{4}{(c_{d}^{\\prime\\prime})^{2}}\\cdot m^{1+\\frac{2}{d-1}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, setting (e)m+\u00b21, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X,D_{n}}|\\eta(X)-\\hat{\\eta}(X)|\\ge\\frac{c_{d}^{\\prime\\prime}}{2}m^{-\\frac{1}{d-1}}=\\left(\\frac{c_{d}^{\\prime\\prime}}{2}\\right)^{\\frac{d-3}{d-1}}n^{-\\frac{1}{d+1}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Theorem E.5. [Theorem $^{4}$ ofDasgupta and Tosh[2020]] For any $d>3$ let input space $\\chi_{1}$ be the one-dimensional sub-manifold of $\\mathbb{R}^{d}$ given in (12). Take $k=1$ Suppose that random matrix $\\Theta$ has rows chosen from the distribution $\\nu$ that is uniform over $S^{d-1}$ . For any $0<\\lambda<1$ there exists a $\\lambda$ -Lipschitz.function $f:\\mathcal{X}_{1}\\to\\mathbb{R}$ suchthatwithprobability atleast $1/2$ over the choice of $\\Theta$ no mater how the weighs $w_{1},\\dots,w_{m}$ are set,thereuling ftion $\\begin{array}{r}{\\hat{f}(x)=\\sum_{j=1}^{m}w_{j}\\mathbb{I}_{\\{x\\in C_{j}\\}}}\\end{array}$ has approximation error at least ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in{\\mathcal{X}}_{1}}|\\hat{f}(x)-f(x)|\\geq c_{d}^{\\prime}\\cdot\\lambda\\cdot\\frac{1}{m^{1/(d-1)}\\log m}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $c_{d}^{\\prime}$ is some absolute constant depending on $d$ ", "page_idx": 24}, {"type": "text", "text": "F Various proofs from section 4 and Appendix B ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "F.1  Lemma F.1 and its proof ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We first show that while EaS representation using $h_{2}$ is not $k$ -sparse,for large enough sample size, with high probability, it is at least $k/2$ -sparse and at most $2k$ -sparse in expectation. F). ", "page_idx": 24}, {"type": "text", "text": "Lemma F.1. Pick any $\\delta>0,$ For $k,m$ and $n,$ satisfying $\\begin{array}{r}{n\\geq\\frac{c_{0}m}{k}\\left(\\log n+\\log\\left(\\frac{m}{\\delta}\\right)\\right)}\\end{array}$ where $c_{0}>0$ is a universal constant, with probability at least $1-\\delta$ over the random choice of training set $D_{n}^{\\prime}$ the following holds for all $j\\in[m]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n(k/2m)\\leq\\operatorname*{Pr}_{x\\sim\\mu}(\\theta_{j}\\cdot x\\geq\\tau_{n}(\\theta_{j}))\\leq(2k/m).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We start with a version of relative generalization error bound, originally due to Vapnik and Chervonenkis, that appeared in Chaudhuri and Dasgupta [2010]. ", "page_idx": 24}, {"type": "text", "text": "Theorem F.2. (Theorem $15$ of Chaudhuri and Dasgupta [2010]) Let $\\mathcal{G}$ be a class of functions from $\\mathcal{X}$ to $\\{0,1\\}$ with VC dimension $d<\\infty,$ and $\\mathbb{P}$ a probability distribution on $\\mathcal{X}$ Let $\\mathbb{E}$ be the expectation with respect to $\\mathbb{P}.$ Suppose n points are drawn independently from $\\mathbb{P}$ :let $\\mathbb{E}_{n}$ denotes expectation with respect to this sample. Then for any $\\delta>0$ with probability at least $1-\\delta$ the following holds for all $g\\in{\\mathcal{G}}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\operatorname*{min}\\left(\\beta_{n}\\sqrt{\\mathbb{E}_{n}g},\\beta_{n}^{2}+\\beta_{n}+\\sqrt{\\mathbb{E}g}\\right)\\le\\mathbb{E}g-\\mathbb{E}_{n}g\\le\\operatorname*{min}\\left(\\beta_{n}^{2}+\\beta_{n}\\sqrt{\\mathbb{E}_{n}g},\\beta_{n}\\sqrt{\\mathbb{E}g}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\theta_{i}$ be the $i^{t h}$ row of the projection matrix $\\Theta$ and let $\\mu^{i}$ be the distribution of $\\theta_{i}\\cdot x$ where $x$ is distribution according to $\\mu$ . Then, for any $a\\in\\mathbb{R}$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim\\mu}(\\theta_{i}\\cdot x\\geq a)=\\mu^{i}(\\theta_{i}\\cdot x\\geq a).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Suppose $x_{1}^{\\prime},\\ldots,x_{n}^{\\prime}$ are sampledindependently at random from $\\mu$ . For any interval $A_{a}=[a,\\infty)$ ,let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mu_{n}^{i}(A_{a})={\\frac{1}{n}}\\sum_{j=1}^{n}\\mathbb{I}[\\theta_{i}\\cdot x_{j}^{\\prime}\\geq a].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consider the class of indicator functions over the intervals $[a,\\infty)$ for $a~\\in~\\mathbb{R}$ .This class has VC dimension 1 and define $\\begin{array}{r}{\\beta^{2}=\\frac{4}{n}\\left(\\log2n+\\log(\\frac{8m}{\\delta})\\right)}\\end{array}$ . Suppose $\\mu_{n}^{i}(A_{a})$ satisfies the condition $\\mu_{n}^{i}(A_{a})\\geq4\\beta^{2}$ ", "page_idx": 24}, {"type": "text", "text": "The bound $\\mu^{i}(A_{a})-\\mu_{n}^{i}(A_{a})\\leq\\beta^{2}+\\beta\\sqrt{\\mu_{n}^{i}(A_{a})}$ from Theorem F.2 yields, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu^{i}(A_{a})\\leq\\mu_{n}^{i}(A_{a})+\\beta^{2}+\\beta\\sqrt{\\mu_{n}^{i}(A_{a})}\\leq\\mu_{n}^{i}(A_{a})+\\frac{1}{4}\\cdot\\mu_{n}^{i}(A_{a})+\\frac{1}{2}\\cdot\\mu_{n}^{i}(A_{a})=\\frac{7}{4}\\cdot\\mu_{n}^{i}(A_{a})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, the bound $-\\beta\\sqrt{\\mu_{n}^{i}(A_{a})}\\leq\\mu^{i}(A_{a})-\\mu_{n}^{i}(A_{a})$ from Theorem F.2 yields, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mu_{i}(A_{a})\\geq\\mu_{n}^{i}(A_{a})-\\beta{\\sqrt{\\mu_{n}^{i}(A_{a})}}\\leq\\mu_{n}^{i}(A_{a})-{\\frac{1}{2}}\\cdot\\mu_{a}^{i}(A_{a})={\\frac{1}{2}}\\cdot\\mu_{n}^{i}(A_{a})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining these two bounds, we can can conclude that for all intervals $A_{a}$ satisfying $\\mu_{n}^{i}(A_{a})\\geq4\\beta^{2}$ with probability at least $\\textstyle1-{\\frac{\\delta}{m}}$ it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot\\mu_{n}^{i}(A_{a})\\leq\\mu^{i}(A_{a})\\leq\\frac{7}{4}\\cdot\\mu_{n}^{i}(A_{a}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now if we only consider the intervals $A_{\\tau_{n}(\\theta_{j})}$ , then from (13) it is clear that $\\begin{array}{r}{\\mu_{n}^{i}(A_{\\tau_{n}(\\theta_{j})})\\leq\\frac{k}{m}+\\frac{1}{n}}\\end{array}$ This is because for any interval $\\mu_{n}^{i}(\\cdot)$ can take $(n+1)$ possible values in steps of $\\scriptstyle{\\frac{1}{n}}$ ,namely, $\\textstyle0,{\\frac{1}{n}},{\\frac{2}{n}},\\ldots,1$ . Therefore, from the definition of $\\tau_{n}(\\theta_{j})$ in (13),if $\\mu_{n}^{i}(A_{\\tau_{n}(\\theta_{j})})\\ \\neq\\ {\\frac{\\ddot{k}}{m}}$ \uff0cthen to ensure that $\\begin{array}{r}{\\mu_{n}^{i}\\bigl(A_{\\tau_{n}(\\theta_{j})}\\bigr)\\geq\\frac{k}{m}}\\end{array}$ , its maximum value can be at most $\\textstyle{\\frac{k}{m}}+{\\frac{1}{n}}$ . Therefore, the condition $\\mu_{n}^{i}(A_{\\tau_{n}(\\theta_{j})})\\geq4\\beta^{2}$ implies, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{\\frac{k}{m}+\\frac{1}{n}}}&{{\\ge}}&{{\\frac{16}{n}\\left(\\log2n+\\log\\left(\\frac{8m}{\\delta}\\right)\\right)=\\frac{16}{n}\\left(\\log2+\\log n+\\log8+\\log\\left(\\frac{m}{\\delta}\\right)\\right)}}\\\\ {{\\ }}&{{=}}&{{\\frac{64}{n}+\\frac{16}{n}\\left(\\log n+\\log\\left(\\frac{m}{\\delta}\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "or in other words, $\\begin{array}{r}{\\frac{k}{m}\\geq\\frac{63}{n}+\\frac{16}{n}\\left(\\log n+\\log\\left(\\frac{m}{\\delta}\\right)\\right)}\\end{array}$ . Noting that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{7}{4}\\cdot\\mu_{n}^{i}(A_{\\tau_{n}(\\theta_{j})})\\leq\\frac{7k}{4m}+\\frac{7}{4n}\\leq\\frac{7k}{4m}+\\frac{7}{4}\\cdot\\frac{k}{63m}\\leq\\frac{2k}{m}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore,if $\\begin{array}{r}{\\frac{k}{m}\\geq\\frac{c_{0}}{n}\\left(\\log n+\\log\\left(\\frac{m}{\\delta}\\right)\\right)}\\end{array}$ for some universal constant $c_{0}>0$ , then with probability at least $\\textstyle1-{\\frac{\\delta}{m}}$ m.' ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{k}{2m}}\\leq\\operatorname*{Pr}_{x\\sim\\mu}\\left(\\theta_{j}\\cdot x\\geq\\tau_{n}(\\theta_{j})\\leq{\\frac{2k}{m}}.\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking union bound over $\\theta_{1},\\ldots,\\theta_{m}$ yields the desired results. ", "page_idx": 25}, {"type": "text", "text": "F.2Proof of Lemma B.3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. Pick any good $\\theta$ , and let $x=\\pi_{M}(\\theta)$ be its projection on $M$ . Since $\\mathcal{X}$ consists of unit vectors, using Lemma F.1 with probability at least $1-\\delta$ , the points that lie in $C(\\theta)$ are at least $k/(2m)$ fraction or at most $2k/m$ fraction of $x$ 's (under distribution $\\mu$ ) that have highest dot product with $\\theta$ or equivalent to closest to $\\theta$ . Thus, $C(\\theta)$ is the set of the form $B(\\theta,r^{\\prime})$ where radius $r^{\\prime}$ is so chosen that $\\bar{k^{\\prime}}(2m)\\leq\\mu(B(\\theta,r^{\\prime}))\\leq2k/m$ . However, it is not of the form $B(x,r^{\\prime\\prime})$ which causes complication. In particular, two questions need to be answered: (i) if a point $x^{\\prime}\\in M$ lies within distance $r<\\rho$ of $x$ , how far can it possibly be from $\\theta$ , and conversely, (ii) if $x^{\\prime}\\in M$ lies within distance $r<\\rho$ of $\\theta$ \uff0c how far can it possibly be from $x?$ These two questions are answered in Lemma 6 of Dasgupta and Tosh [2020] showing that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{M}(x,r)\\subset B\\left(\\theta,\\sqrt{\\Delta^{2}+\\frac{\\rho+\\Delta}{\\rho}r^{2}}\\right)}\\\\ {B_{M}(\\theta,r^{\\prime})\\subset B\\left(x,\\sqrt{\\frac{\\rho}{\\rho-\\Delta}((r^{\\prime})^{2}-\\Delta^{2})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the left-hand containment pick $\\begin{array}{r}{r=\\sqrt{\\frac{\\rho-\\Delta}{\\rho+\\Delta}}\\left(\\frac{k}{2c_{2}m}\\right)^{1/d_{0}}}\\end{array}$ . Further taking, ", "page_idx": 25}, {"type": "equation", "text": "$$\nr^{\\prime}=\\sqrt{\\Delta^{2}+\\frac{\\rho+\\Delta}{\\rho}r^{2}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\nr^{\\prime\\prime}=\\sqrt{\\frac{\\rho}{\\rho-\\Delta}((r^{\\prime})^{2}-\\Delta^{2})}=\\left(\\frac{k}{2c_{2}m}\\right)^{1/d_{0}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and using (17) yields that $\\mu(B_{M}(x,r^{\\prime\\prime})<k/(2m)$ . Now using (21) and (22) we have, $B_{M}(x,r)\\subset$ $B_{M}(\\theta,r^{\\bar{\\prime}})\\subset\\bar{B}_{M}(x,r^{\\prime\\prime})$ .Since $\\mu(B_{M}(x,r^{\\prime\\prime}))<k/(2m)$ , this ensures $B_{M}(x,r)\\subset B_{M}(\\theta,r^{\\prime})\\subset$ $C(\\theta)$ ", "page_idx": 26}, {"type": "text", "text": "Pick $\\begin{array}{r}{r=\\left(\\frac{2k}{c_{1}m}\\right)^{1/d_{0}}}\\end{array}$ Using (17), thsensures that $\\mu(B(x,r))\\geq2k/m$ Further taking ", "page_idx": 26}, {"type": "equation", "text": "$$\nr^{\\prime}=\\sqrt{\\Delta^{2}+\\frac{\\rho+\\Delta}{\\rho}r^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\nr^{\\prime\\prime}=\\sqrt{\\frac{\\rho}{\\rho-\\Delta}((r^{\\prime})^{2}-\\Delta^{2})}=\\sqrt{\\frac{\\rho+\\Delta}{\\rho-\\Delta}}\\left(\\frac{2k}{c_{1}m}\\right)^{1/d_{0}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and using (21) and (22), we have, $B_{M}(x,r)\\,\\subset\\,B_{M}(\\theta,r^{\\prime})\\,\\subset\\,B_{M}(x,r^{\\prime\\prime})$ Since $\\mu(B_{M}(x,r))\\geq$ $2k/m$ , this ensures that $C(\\theta)\\subset B_{M}(x,r^{\\prime})$ which in turn is contained in $B_{M}(x,r^{\\prime\\prime})$ . Since for any good $\\theta$ $\\Delta\\le\\rho/2$ , the result follows. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "F.3Proof of Lemma B.4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. Let $E_{1}$ be the event that bounds the diameter of the response region of any good $\\theta$ , as specified in Lemma B.3, with probability at least $1-\\delta$ , over the random choice of the training set. We condition on this event. For any $x\\in M$ and $r>0$ , let ", "page_idx": 26}, {"type": "equation", "text": "$$\nA(x,r)=\\{\\theta\\in\\Gamma_{\\rho/2}:\\|\\pi_{M}(\\theta)-x\\|<r\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{1}{2}\\ <\\ \\sqrt{\\frac{\\rho-\\Delta}{\\rho+\\Delta}}}\\end{array}$ for $\\Delta\\,\\leq\\,\\rho/2$ using LmmaB3andchsing large nugh $m$ satisfying $(2k/(c_{1}m))^{1/d_{0}}<\\operatorname*{min}(\\rho,r_{0})$ , if we set ", "page_idx": 26}, {"type": "equation", "text": "$$\nr_{1}=\\frac{1}{2}\\left(\\frac{k}{2c_{2}m}\\right)^{1/d_{0}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{\\Sigma}^{\\prime}\\in A(x,r_{1})\\Rightarrow x\\in B_{M}\\left(\\pi_{M}(\\theta),\\frac{1}{2}\\left(\\frac{k}{2c_{2}m}\\right)^{1/d_{0}}\\right)\\Rightarrow x\\in B_{M}\\left(\\pi_{M}(\\theta),\\sqrt{\\frac{\\rho-\\Delta}{\\rho+\\Delta}}\\left(\\frac{k}{2c_{2}m}\\right)^{1/d_{0}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which implies $\\Rightarrow x\\in C(\\theta)$ . It has been established that $A(x,r)$ has non-negligible probability mass under $\\nu$ ", "page_idx": 26}, {"type": "text", "text": "Lemma F.3 (Lemma 14 of Dasgupta and Tosh [2020]). Suppose $\\nu$ ismultivariateGaussian $N(0,I)$ There is a constant $c_{d}$ that depends on the dimension $d$ such that any $x\\in M$ and $0<r<r_{0}$ ,we have $\\nu(A(x,r))\\ge c_{d}r^{d_{0}}$ ", "page_idx": 26}, {"type": "text", "text": "Using (17), $M$ has a $r_{1}/2$ cover $\\hat{M}$ of size at most $(2c_{2}/c_{1})8^{d_{0}}m/k$ .To see this pick points $x_{1},\\ldots,x_{n}\\in M$ that are at a distance $r_{1}/2$ from each other. The balls $B(x_{1},r_{1}/4)$ are disjoint and each has $\\mu(B(x_{i},r_{1}/4))\\geq c_{1}(r_{1}/4)^{d_{0}}=(c_{1}/(2c_{2}))(1/8^{d_{0}})k/m$ . Since total probability mass of these $N$ balls is at most 1, this gives the bound on $N$ ", "page_idx": 26}, {"type": "text", "text": "Pick any ${\\hat{x}}\\in{\\hat{M}}$ For $i\\in[m]$ , let $U_{i}$ be a binary random variable that takes value 1 if $\\theta_{i}\\in A(\\hat{x},r_{1}/2)$ and O otherwise. Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\nU_{i}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{w.p.~}\\nu(A(\\hat{x},r_{1}/2))}\\\\ {0}&{\\mathrm{w.p.1-}\\nu(A(\\hat{x},r_{1}/2))}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $\\nu(\\hat{x},r_{1}/2)\\ge c_{d}(r_{1}/2)^{d_{0}}=(c_{d}/(2c_{2}))(1/4^{d_{0}})k/m=\\alpha_{d}k/m$ where we have set $\\alpha_{d}=$ $(c_{d}/(2c_{2}))(1/4^{d_{0}})$ . Let $\\textstyle U\\,=\\,\\sum_{i=1}^{m}U_{i}$ be that number of $\\theta_{i}$ 's that fall in $A(\\hat{x},r_{1}/2)$ and $\\mathbb{E}U=$ ", "page_idx": 26}, {"type": "text", "text": "$\\textstyle\\sum_{i=1}^{m}\\mathbb{E}U_{i}\\geq\\alpha_{d}k$ $E_{2}$ be the event that $U>\\alpha_{d}k/2$ and let $E_{2}^{c}$ be the complement event. Using Chernoff bound, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(E_{2}^{c}|E_{1})=\\mathrm{Pr}\\left(U\\leq\\alpha_{d}k/2|E_{1}\\right)\\leq\\mathrm{Pr}\\left(U\\leq(1/2)\\mathbb{E}U|E_{1}\\right)\\leq e^{-\\mathbb{E}U/8}\\leq e^{-\\alpha_{d}k/8}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Bounding the right most quantity above to be at most $\\delta/\\hat{M}$ ,for $k$ as specified in the lemma statement, for suitable choice of $c_{d}^{\\prime}$ , we conclude that $E_{2}$ holds (conditioned on $E_{1}$ ) with probability at least $1-\\delta$ : Therefore, with probability at least $1-2\\delta$ , both $E_{1}$ and $E_{2}$ hold, impying that for every $\\hat{x}\\in\\hat{M}$ , there are at least $\\alpha_{d}k/2$ good $\\theta_{i}$ 's in $A(\\hat{x},r_{1}/2)$ ", "page_idx": 27}, {"type": "text", "text": "Now pick any arbitrary $x\\in M$ . There is some ${\\hat{x}}\\in{\\hat{M}}$ with $\\|x-\\hat{x}\\|\\leq r_{1}/2$ . Moreover, for any $\\theta_{j}\\in\\bar{A}(\\hat{x},r_{1}/2)\\Rightarrow\\theta_{j}\\overleftarrow{\\in}A(x,r_{1}/2)\\Rightarrow x\\in C(\\theta_{j})$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "F.4Proof of Lemma B.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For ease of exposition, we use the notation $x_{[1,n]}$ to denote $x_{1},\\ldots,x_{n}$ and $y_{[1,n]}$ to denote $y_{1},\\ldots,y_{n}$ \uff0c $x_{[1,n]}^{\\prime}$ to denote $x_{1}^{\\prime},\\ldots,x_{n}^{\\prime}$ and $y_{[1,n]}^{\\prime}$ to denote $y_{1}^{\\prime},\\ldots,y_{n}^{\\prime}$ for various proofs appearing in the section. Note number of non-zero entries in the EaS representation given in (14) is variable and we use the notation $k(x)$ to denote the number of non-zero entries in $h_{2}(x)$ ", "page_idx": 27}, {"type": "text", "text": "Proof. Fix any $\\Theta$ . Then, conditioning on $x_{[1,n]}^{\\prime}$ ensures that $C_{1},\\ldots,C_{m}$ are fixed. Now, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k-1},\\sigma_{k}\\right]}\\Big(t(1-\\theta)-q\\Big)^{*}=\\left[\\sigma_{k}\\Big(t(1)(\\theta-q)+q\\Big)\\right]^{1/2}}\\\\ &{=\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k}\\Big(\\Big(\\theta-q\\Big)+q\\Big)\\Big]^{1/2}}}\\\\ &{\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1}\\Big(\\theta-q\\Big)\\right]^{1/2}}+\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k}\\Big(t(1)+\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1}\\Big)\\right]^{1/2}}}\\\\ &{=\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1}\\right]^{1/2}}\\Bigg[\\left(\\sum_{s=0}^{1}\\frac{W_{t}^{1/2}\\sigma_{k+1}}{\\sigma_{k}(1)}(\\theta-q)\\right)^{1/2}\\sigma_{k+1}\\sigma_{k+1}^{1}\\sigma_{k+1}^{1}\\Bigg]\\Bigg]}\\\\ &{\\stackrel{(a)}{\\le}\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k+1},\\sigma_{k+1},\\sigma_{k+1}\\right]^{1/2}}\\Bigg[\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k+1}\\right]}\\left[\\frac{1}{t}\\sum_{s=0}^{1}\\Big(\\theta-q)^{s}\\Big)^{1/2}\\sigma_{k+1}\\sigma_{k+1}^{1}\\sigma_{k+1}^{1}\\Bigg]\\Bigg]}\\\\ &{=\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k+1},\\sigma_{k+1}\\right]^{1/2}}\\Bigg[\\mathbb{E}_{\\left[\\sigma_{k},\\sigma_{k+1}\\right]}\\left[\\frac{1}{t}\\sum_{s=0}^{1}\\Big(\\sum_{s=1}^{1}\\|\\sigma_{k+1}\\|^{2}\\sigma_{k}\\Big(\\zeta_{k}\\Big)-\\epsilon\\zeta_{k}\\Big(\\zeta_{k}\\\n$$$$\n\\begin{array}{r l}&{\\left\\langle\\sum_{k_{2}\\in\\mathcal{N}_{D_{k}}}\\left\\lvert u_{k_{2},n_{1}}\\right\\rvert\\sum_{\\vec{t}=0}^{T}\\left(y_{k_{2},n_{2}}^{(1)}\\left\\lvert u_{k_{2},n_{1}}^{(2)}\\right\\rvert\\right)^{2}\\right\\rvert}\\\\ &{=\\mathbb{E}_{y_{n_{2},n_{1}}^{(1)}\\left(\\left\\lVert u_{k_{2},n_{1}}^{(1)}\\right\\rVert^{2}\\right)}\\Bigg[\\sum_{\\vec{t}=0}^{T}\\left(\\frac{\\sum_{k_{1}=0}^{T}\\left\\lVert u_{k_{1},n_{2}}^{(1)}C_{k_{1}}^{(1)}-u_{k_{2}}^{(1)}\\right\\rVert^{2}}{y_{k_{2},n_{1}}^{(1)}\\left\\lVert u_{k_{1},n_{2}}^{(1)}C_{k_{1}}^{(1)}-u_{k_{2}}^{(1)}\\right\\rVert^{2}}\\right)^{2}\\Bigg]\\left(y_{k_{2},n_{1}}^{(1)}\\left\\lVert u_{k_{1},n_{2}}^{(1)}\\right\\rVert^{2}\\right)}\\\\ &{=\\mathbb{E}_{y_{n_{2},n_{1}}^{(1)}\\left(\\left\\lVert u_{k_{2},n_{1}}^{(1)}\\right\\rVert^{2}\\right)}\\Bigg[\\sum_{\\vec{t}=0}^{T}\\left(\\frac{\\sum_{k_{1}=0}^{T}\\left\\lVert u_{k_{1},n_{2}}^{(1)}C_{k_{1}}^{(1)}-u_{k_{2}}^{(1)}\\right\\rVert^{2}}{y_{k_{2},n_{1}}^{(1)}\\left\\lVert u_{k_{1},n_{2}}^{(1)}C_{k_{1}}^{(1)}-u_{k_{2}}^{(1)}\\right\\rVert^{2}}\\right)^{2}\\left\\lVert u_{k_{1},n_{1}}^{(1)}\\right\\rVert^{2}\\Bigg]\\Bigg[\\sum_{k_{2}=0}^{T}\\left(\\frac{\\sum_{k_{1}=0}^{T}\\left\\lVert u_{k_{1},n_{2}}^{(1)}C_{k_{1}}^{(1)}-u_{k_\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\underset{t\\neq t,u=0}{\\operatorname*{lim}}\\left[\\underset{\\mathcal{T}_{t}^{3}\\geq\\sigma_{t,u}}\\left(\\frac{\\sum_{i=1}^{n}\\{(B_{t}-\\sigma(Z))\\}_{\\lfloor i\\sigma(Z)\\rfloor}}{B_{\\rho,0}(Z)}\\right)^{2}\\mathbb{I}[\\eta_{\\mu+}(C)>0\\,0]\\right]+\\lambda\\frac{1-\\lambda/2}{\\eta_{\\infty}^{2}}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\ }\\\\ &{\\left[\\frac{1}{t}{\\sum_{\\rho\\geq\\sigma_{t,u}}\\frac{u+\\sigma_{t,u}^{2}}}\\left[{\\underset{\\mathcal{T}_{t,u}^{*}\\geq\\sigma_{t,u}^{2}}{\\sum_{i=1}^{n}\\sigma_{t,u}^{2}}}\\left[\\frac{1}{\\sum_{j=1}^{B_{t}+\\sigma_{t,u}^{2}}}\\right]^{2}(\\mu_{u}(C))=0\\{\\}\\eta_{t}(C)\\right]+\\lambda\\alpha\\,\\lambda(0)\\left\\vert\\eta_{\\infty}\\right\\vert,\\eta_{\\infty}\\right\\vert\\right],}\\\\ &{=\\left[\\underset{s\\geq0}{\\operatorname*{lim}}\\left[\\underset{\\mathcal{T}_{t}^{3}\\geq\\sigma_{t,u}^{2}}{\\sum_{i,j=1}^{B_{\\sigma}}}\\left(\\frac{\\sum_{i=1}^{B_{t}+\\sigma_{t,u}^{2}}(D_{t})(B_{t}-\\sigma(Z))}{\\eta_{\\infty}(D_{t})}\\right)^{2}\\mathbb{I}[\\eta_{\\mu+}(C)>0\\,0]\\right]+\\lambda\\eta_{\\infty}\\,\\lambda(0)\\left\\vert\\eta_{\\infty}\\right\\vert,\\eta_{\\infty}\\right\\vert\\right],}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where equality $a$ follows from the fact that the quantity with the inner square bracket is unaffected by the $y_{i}$ 's. Inequality $b$ is due to Jensen's inequality. Inequality $c$ follows from Lemma F.4, in equality $d$ follows from Lemma F.6 and inequality $e$ follows from Lemma F.7. The result finally follows noting that by Jensen's inequality $\\mathbb{E}|\\hat{\\eta}(x)-\\bar{\\eta}(x)|\\le\\sqrt{\\mathbb{E}(\\hat{\\eta}(x)-\\bar{\\eta}(x))^{2}}$ \u53e3 ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Lemma F.4. Pick $m\\,\\times\\,d$ projection matrix $\\Theta$ \uff1aSuppose the EaS representation uses $(i)\\ a$ mapping $\\Theta$ and $(i i)$ empirical $k$ -threshold sparsification. Let $x$ be sampledfrom $\\mu$ and let $D_{n}\\;=\\;\\left((x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\right)$ is a random training set where $x_{i}$ is sampled from $\\mu$ and $y_{i}$ is distributed as $\\eta(x_{i})$ for $i\\in[n]$ . Similarly, $\\boldsymbol{D}_{n}^{\\prime}=((x_{1}^{\\prime},y_{1}^{\\prime}),\\dots,(x_{n}^{\\prime},y_{n}^{\\prime}))$ be another random training set independent of $D_{n}$ where $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ is sampled from $\\mu$ and $y_{i}^{\\prime}$ is distributed as $\\eta(x_{i}^{\\prime})$ for $i\\,\\in\\,[n]$ Then the following holds. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{[1,n]},x_{[1,n]}^{\\prime}}\\left[\\mathbb{E}_{x,y_{[1,n]}}\\left[\\frac{1}{t}\\sum_{j:x\\in C_{j}}\\eta^{2}(C_{j})\\mathbb{I}[\\mu_{n}(C_{j})=0]\\mathbb{I}[j\\in A_{t}(x)]\\Big|x_{[1,n]},x_{[1,n]}^{\\prime}\\right]\\right]\\leq\\frac{m}{n t e r m}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We start by taking the $1/t$ factor outside. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\Delta t}\\mathop{\\operatorname*{lim}}_{t\\to0}\\Biggr[\\sum_{m_{i}=0}^{\\infty}\\Biggl\\{\\sum_{\\Tilde{p}=0}^{\\infty}\\mathrm{e}^{\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}+\\mathrm{e}^{\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\Biggr\\}\\Biggr]}\\\\ &{\\leq\\frac{1}{\\Delta t}\\mathop{\\operatorname*{lim}}_{t\\to0}\\Biggr[\\sum_{m_{i}=0}^{\\infty}\\mathrm{e}^{\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{-\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{-\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\Biggr]}\\\\ &{=\\frac{1}{\\Delta t}\\mathop{\\operatorname*{lim}}_{t\\to0}\\Biggr[\\sum_{m_{i}=0}^{\\infty}\\mathrm{e}^{\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{-\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{-\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{-\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{-\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}}\\\\ &{\\ \\ \\ \\ \\mathrm{+}\\frac{1}{\\Delta t}\\mathop{\\operatorname*{lim}}_{t\\to0}\\Biggr]\\Biggr[\\sum_{m_{i}=0}^{\\infty}\\mathrm{e}^{\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{-\\mathrm{i}\\left(Z_{T}\\right)\\ln\\left(m_{i}(C_{T})-m_{i}^{0}\\right)}\\mathrm{e}^{\\mathrm{i}\\left(Z \n$$$$\n\\begin{array}{r l}&{\\frac{1}{n}\\frac{1}{n}\\mathbb{E}_{\\theta\\rightarrow\\theta_{n}}\\Bigg[\\sum_{m=0}^{n}\\theta_{n}(1,t)=K\\frac{\\theta^{\\star}}{2}\\Bigg[\\sum_{\\theta\\in\\theta_{n}}\\int_{0}^{\\theta}\\Big(Z_{m}^{(\\theta)}(B(\\theta,x))\\theta\\theta(1,\\theta)=K\\Big)\\Bigg]\\Bigg]}\\\\ &{\\frac{1}{n}\\frac{1}{n}\\mathbb{E}_{\\theta\\rightarrow\\theta_{n}}\\Bigg[\\sum_{m=0}^{n}\\theta_{n}(1,t)=K\\frac{\\theta^{\\star}}{2}\\Bigg[\\sum_{\\theta\\in\\theta_{n}}\\Big[Z_{m}^{(\\theta)}(B(\\theta,x))\\theta\\theta(1,\\theta)=K\\Big]\\Bigg]\\Bigg|_{\\theta\\rightarrow\\theta_{n}}}\\\\ &{\\frac{2}{n}\\frac{1}{n}\\frac{1}{n}\\Bigg[\\sum_{0}^{n}\\theta_{1}(1,t)=K\\frac{\\theta^{\\star}}{2}\\Bigg[\\sum_{\\theta\\in\\theta_{n}}\\Big[Z_{m}^{(\\theta)}(B(\\theta,x))\\theta\\theta(1,\\theta)=K\\Big]\\Bigg]\\Bigg]\\Bigg|_{\\theta\\rightarrow\\theta_{n}}}\\\\ &{\\frac{2}{n}\\frac{1}{n}\\frac{1}{n}\\mathbb{E}_{\\theta\\rightarrow\\theta_{n}}\\Bigg[\\sum_{m=0}^{n}\\theta_{n}(1,t)=K\\frac{\\theta^{\\star}}{2}\\Bigg[\\sum_{\\theta\\in\\theta_{n}}\\Big[Z_{m}^{(\\theta)}(B(\\theta,x))\\Big]\\sum_{\\theta\\in\\theta_{n}}(1,t)=K\\frac{\\theta^{\\star}}{2}\\Bigg]\\Bigg]\\Bigg|_{\\theta\\rightarrow\\theta_{n}}}\\\\ &{\\frac{2}{n}\\frac{1}{n}\\frac{1}{n}\\mathbb{E}_{\\theta\\rightarrow\\theta_{n}}\\Bigg[\\sum_{\\theta\\in\\theta_{n}}\\Big[Z_{m}^{(\\theta)}(B(\\theta,x))\\theta\\Big]\\Big]\\Bigg|_{\\theta\\rightarrow\\theta_{n}}+\\sum_{m=0}^{n}\\theta\\Big[\\sum_{\\theta\\in\\theta_{n}}(1,t)=K\\frac{\\theta^{\\star}}{2}\\Bigg[\\sum_{\\theta\\in\\theta_{n}}(2\\prod_{\\theta\\in\\theta_{n}}(C_{m}))}\\\\ &{+\\frac{1}{n}\\mathbb{E}_{\\theta\\rightarrow\\theta_{n}}\\\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where equality $a$ follows from the fact that the quantity with the inner square bracket is unaffected by the $y_{i}$ 's. Equality $b$ follows from the fact that for $k(x)~=~0$ . number of non-zero entries in the EaS of $x$ using (14) is zero and $x\\,\\notin\\,C_{j},\\forall j$ . Therefore, we denote the quantity ", "page_idx": 29}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{j:x\\in C_{j}}\\eta^{2}(C_{j})\\mathbb{I}[n\\mu_{n}(C_{j})]\\mathbb{I}[j\\in A_{t}^{x}]=0}\\end{array}$ . Inequality $c$ follows from the fact that $\\eta(C_{j})\\leq1,\\forall j$ Inequality $d$ follows from Lemma F.5. Inequality $e$ follows since we are adding a non-negative quantity.Finally, inequality $f$ followsfromthefact that $\\begin{array}{r}{\\operatorname*{sup}_{z}z e^{-z}=\\frac{1}{e}}\\end{array}$ \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma F.5. ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left[\\sum_{j:x\\in C_{j}}\\mathbb{I}[n\\mu_{n}(C_{j})=0]\\mathbb{I}[j\\in A_{t}(x)]\\bigg|k(x)=k^{\\prime}\\right]\\le\\sum_{j=1}^{m}\\mu(C_{j})\\mathbb{I}[n\\mu_{n}(C_{j})=0]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Conditioned on $k(x)=k^{\\prime}$ , EaS representation given in (14) has exactly $k^{\\prime}$ non-zero entries. Therefore, using Lemma 3.4, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{\\sigma}\\left[\\sum_{i>\\sigma(\\epsilon)}^{\\mathrm{op}}\\|\\boldsymbol{\\it m}_{\\#}(C_{i})-\\boldsymbol{0}\\|\\boldsymbol{1}\\{\\boldsymbol{j}\\in A_{\\sigma}(\\boldsymbol{x})\\big\\vert k(\\boldsymbol{x})-\\boldsymbol{k}\\}\\right]}\\\\ {\\displaystyle\\triangleq\\sum_{i=1}^{\\infty}\\|\\mathbf{\\hat{m}}_{\\#}\\left(\\boldsymbol{z}\\in C_{i}^{\\epsilon}\\right)\\sum_{j=\\sigma(\\epsilon)}^{\\mathrm{th}}\\|\\boldsymbol{\\mu}_{\\mathtt{m}}(C_{j})=0\\|\\boldsymbol{l}\\|\\in A_{i}(\\boldsymbol{x})\\|}\\\\ {\\displaystyle\\triangleq\\sum_{i=1}^{\\infty}\\sum_{j=\\sigma(\\epsilon)}^{\\mathrm{th}}\\mu(C_{j}^{\\epsilon})\\|\\boldsymbol{\\eta}_{\\mu}(C_{j})=0\\|\\boldsymbol{l}\\|\\in A_{i}(\\boldsymbol{x})\\|}\\\\ {\\displaystyle\\triangleq\\sum_{i=1}^{\\infty}\\sum_{j=\\sigma(\\epsilon)}^{\\mathrm{th}}\\mu(C_{i}^{\\epsilon})\\|\\boldsymbol{\\eta}_{\\mu}(C_{j})=0\\|\\boldsymbol{l}\\|\\in A_{i}(\\boldsymbol{x})\\|}\\\\ {\\displaystyle\\triangleq\\sum_{i=1}^{\\infty}\\sum_{j=\\sigma(\\epsilon)}^{\\mathrm{th}}\\mu(C_{i}^{\\epsilon})\\|\\boldsymbol{\\eta}_{\\mu}(C_{i})=0\\|\\boldsymbol{l}\\|\\in A_{i}(\\boldsymbol{x})\\|}\\\\ {\\displaystyle=\\sum_{j=1}^{\\infty}\\|\\boldsymbol{\\eta}_{\\mu}(C_{j})=0\\|\\left(\\sum_{\\sigma\\in\\sigma(\\epsilon)}^{\\mathrm{th}}\\mu(C_{j}^{\\epsilon})\\|\\boldsymbol{l}\\|\\in A_{i}(\\boldsymbol{x})\\right)}\\\\ {\\displaystyle\\triangleq\\sum_{j=1}^{\\infty}\\|\\boldsymbol{\\eta}_{\\mu\\mu}(C_{j})=0\\|\\left(\\sum_{\\sigma\\in\\sigma(\\epsilon)}^{\\mathrm{th}}\\mu(C_{j}^{\\epsilon})\\right)\\|\\sum_{j=1}^{\\infty}\\|\\boldsymbol{\\eta}_{\\sigma}(C_{j})\\|\\|\\boldsymbol{\\mu}_{\\mu\\nu}(C_{j})=0\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where equality a follows from part i of lemma 3.4 since C) forms a partition of $\\mathcal{X}$ and the definition of $\\sigma$ in section 3.2 with $k=k^{\\prime}$ . Equality $b$ follows from the following observation. In the line above inequality $b$ , we are summing $k^{\\prime}\\times\\left({m\\atop k^{\\prime}}\\right)$ terms. Since $\\begin{array}{r}{k^{\\prime}\\times\\binom{m}{k^{\\prime}}=\\bar{m}\\times\\binom{m-1}{k^{\\prime}-1}}\\end{array}$ and any $j\\in[m]$ can appear in exactly $\\binom{m-1}{k^{\\prime}-1}$ different subsets of of size $k^{\\prime}$ , equality $b$ is simply rearranging the terms from the line above by changing the indices appropriately. Equality $c$ follows from part (iv) of lemma 3.4. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma F.6. Pick $m\\,\\times\\,d$ projection matrix $\\Theta$ SupposetheEaSrepresentationuses $(i)\\ a$ mapping $\\Theta$ and $(i i)$ empirical $k$ -threshold sparsification. Let $x$ be sampled from $\\mu$ and let $D_{n}\\;=\\;\\left((x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\right)$ is a random training set where $x_{i}$ is sampled from $\\mu$ and $y_{i}$ is distributed as $\\eta(x_{i})$ for $i\\in[n]$ . Similarly, $\\boldsymbol{D}_{n}^{\\prime}=((x_{1}^{\\prime},y_{1}^{\\prime}),\\dots,(x_{n}^{\\prime},y_{n}^{\\prime}))$ be another random training set independent of $D_{n}$ where $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ is sampled from $\\mu$ and $y_{i}^{\\prime}$ is distributed as $\\eta(x_{i}^{\\prime})$ for $i\\,\\in\\,[n]$ Then the following holds. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{y_{[1,n]}}\\left[\\displaystyle\\sum_{j:x\\in C_{j}}\\left(\\frac{\\sum_{i=1}^{n}(y_{i}-\\eta(C_{j}))\\mathbb{I}[x_{i}\\in C_{j}]}{n\\mu_{n}(C_{j})}\\right)^{2}\\mathbb{I}[n\\mu_{n}(C_{j})>0]\\mathbb{I}[j\\in A_{t}(x)]\\Bigg|x,x_{[1,n]},x_{[1,n]}^{\\prime}\\right]}\\\\ &{\\le\\displaystyle\\frac{1}{4}\\sum_{j:x\\in C_{j}}\\left(\\frac{\\mathbb{I}[n\\mu_{n}(C_{j})>0]\\mathbb{I}[j\\in A_{t}(x)]}{n\\mu_{n}(C_{j})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Conditioned on $x$ and $x_{[1,n]}^{\\prime}$ , only $k(x)=k^{\\prime}$ of the $m$ coordinates in the EaS representation of $x$ are non-zero. If $k^{\\prime}=0$ then the right hand side of the Lemma statement still holds with tghe value being zero since each $C_{j}$ will be an empty set in that case. WLOG, for ease of exposition, ", "page_idx": 30}, {"type": "text", "text": "assume that $k^{\\prime}>0$ and these $k^{\\prime}$ non-zero coordinate to be $j_{1},\\ldots,j_{k^{\\prime}}\\in[m]$ . Then the number of $x_{i}$ that falls in any such $C_{j\\imath}$ , where $l\\in[k^{\\prime}]$ ,is $n\\mu_{n}(C_{j_{l}})$ . The $y_{i}$ values corresponding to these $x_{i}$ points (there are $n\\mu_{n}(C_{j_{l}})$ of them in total) are identically and independently distributed with expectation ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}(y_{i}|x_{i}\\in C_{j_{l}})}&{=}&{\\operatorname*{Pr}(y_{i}=1|x_{i}\\in C_{j_{l}})=\\displaystyle\\frac{1}{\\mu(C_{j_{l}})}\\int_{C_{j_{l}}}\\operatorname*{Pr}(y_{i}=1|x_{i}=x)\\mu(d x)}\\\\ &{=}&{\\displaystyle\\frac{1}{\\mu(C_{j_{l}})}\\int_{C_{j_{l}}}\\eta(x)\\mu(d x)=\\eta(C_{j_{l}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{P}_{\\theta\\mid n}}\\Bigg[\\sum_{m\\in\\mathcal{E}_{\\theta}}\\left(\\frac{\\sum_{t=1}^{n}(y_{t}-\\eta(C_{h}))\\mathbb{I}[x_{t}\\in C_{h}]}{n\\mu_{t}(C_{h})}\\right)^{2}[\\eta\\mu_{t n}(C_{h})>0][\\mathrm{i}\\xi\\in A_{\\mathbb{R}}(x)][x,x_{1:1,n},x_{1:1,n}^{\\prime}]}\\\\ &{=\\frac{k^{\\prime}}{\\sum_{t=1}^{n}}\\left[\\frac{\\mathbb{I}[x_{t}],\\eta\\left[(\\sum_{t=1}^{n}(y_{t}-\\eta(C_{h}))\\mathbb{I}[x_{t}\\in C_{h}]\\right)^{2}[\\eta\\mu_{t n}(C_{h})>0][\\mathrm{i}\\xi\\in A_{\\mathbb{R}}(x)][x,x_{1:1,n},x_{1:1,n}^{\\prime}]}{(n\\mu_{t}(C_{h}))^{2}}\\right]^{-}}\\\\ &{\\overset{,,,}{=}\\frac{k^{\\prime}}{\\sum_{t=1}^{n}}\\left[\\frac{\\sum_{t=1}^{n}\\mathbb{I}[y_{t}\\left(y_{t}-\\eta(C_{h})\\right)\\mathbb{I}[x_{t}\\in C_{h}][y_{t}\\mu_{t n}(C_{h})>0][\\mathrm{i}\\xi\\in A_{\\mathbb{R}}(x)][x,x_{t},x_{t}^{\\prime}]}{(n\\mu_{t}(C_{h}))^{2}}\\right]}\\\\ &{\\overset{,,}{\\le}\\frac{k^{\\prime}}{\\sum_{t=1}^{n}}\\left[\\frac{\\sum_{t=1}^{n}\\eta(C_{h})(1-\\eta(C_{h}))\\mathbb{I}[x_{t}\\in C_{h}][\\eta\\mu_{t n}(C_{h})>0][\\mathrm{i}\\xi\\in A_{\\mathbb{R}}(x)]}{(n\\mu_{t}(C_{h}))^{2}}\\right]}\\\\ &{\\overset{,,}{\\le}\\frac{k^{\\prime}}{\\sum_{t=1}^{n}}\\left[\\frac{\\mathbb{I}[y_{t}](y_{t}-\\eta(C_{h}))\\mathbb{I}[x_{t}\\in C_{h}][\\eta\\mu_{t n}(C_{h})>0][\\mathrm{i}\\xi\\in A_{\\mathbb\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where, equality $a$ is due to the following observation. For any $i,j\\,\\in\\,[m],i\\,\\neq\\,j$ and $x_{i},x_{j}\\in C_{l}$ for some $l\\in[m]$ \uff0c $y_{i}$ and $y_{j}$ are identically and independently distributed with expectation $\\eta(C_{l})$ Therefore, the expectation of the cross product is simply: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}_{y_{i},y_{j}}\\left[(y_{i}-\\eta(C_{l}))(y_{j}-\\eta(C_{l})\\right]}&{=}&{\\mathbb{E}_{y_{i},y_{j}}\\left[y_{i}y_{j}-\\eta(C_{l})(y_{i}+y_{j})+\\eta(C_{l})^{2}\\right]\\ \\ }\\\\ &{=}&{\\mathbb{E}y_{i}\\mathbb{E}y_{j}-\\eta(C_{l})(\\mathbb{E}y_{i}+\\mathbb{E}y_{j})+\\eta(C_{l})^{2}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Equality $b$ follows from variance computation. In particular for any $y_{i},i\\in[m]$ with $x_{i}\\in C_{l}$ for some $l\\in[m]$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\natural\\left[(y_{i}-\\eta(C_{l}))^{2}\\right]=\\mathbb{E}y_{i}^{2}-2\\eta(C_{l})\\mathbb{E}y_{i}+\\eta(C_{l})^{2}=\\eta(C_{l})-2\\eta(C_{l})^{2}+\\eta(C_{l})^{2}=\\eta(C_{l})(1-\\eta(C_{l})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, inequality $c$ follows from the fact that for any $z\\in[0,1]$ , the maximum value of $z(1-z)$ is $\\textstyle{\\frac{1}{4}}$ Itiseasy tobserve that the fnal result is equivalent to: $\\begin{array}{r}{\\frac{1}{4}\\sum_{j:x\\in C_{j}}\\left(\\frac{\\mathbb{I}[n\\mu_{n}(C_{j})>0]\\mathbb{I}[j\\in A_{t}(x)]}{n\\mu_{n}(C_{j})}\\right)}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "Lemma F.7. Pick $m\\,\\times\\,d$ projection matrix $\\Theta$ SupposetheEaSrepresentationuses $(i)\\ a$ mapping $\\Theta$ and(ii\uff09empirical $k$ -threshold sparsification.Let $x$ be sampled from $\\mu$ and let $D_{n}\\;=\\;\\left((x_{1},y_{1}),\\ldots,(x_{n},y_{n})\\right)$ is a random training set where $x_{i}$ is sampled from $\\mu$ and $y_{i}$ is distributed as $\\eta(x_{i})$ for $i\\in[n]$ . Similarly, $\\boldsymbol{D}_{n}^{\\prime}=\\left((x_{1}^{\\prime},y_{1}^{\\prime}),\\dots,(x_{n}^{\\prime},y_{n}^{\\prime})\\right)$ be another random training set independent of $D_{n}$ ,where $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ is sampled from $\\mu$ and $y_{i}^{\\prime}$ is distributed as $\\eta(x_{i}^{\\prime})$ for $i\\,\\in\\,[n]$ Then the following holds. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{[1,n]},x_{[1,n]}^{\\prime}}\\left[\\mathbb{E}_{x}\\left[\\frac{1}{t}\\sum_{j:x\\in C_{j}}\\frac{\\mathbb{I}[\\mu_{n}(C_{j})>0]\\mathbb{I}[j\\in A_{t}(x)]}{n\\mu_{n}(C_{j})}\\Big|x_{[1,n]},x_{[1,n]}^{\\prime}\\right]\\right]\\leq\\frac{2m}{t(n+1)}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\mathbb{E}_{\\sigma_{1}\\sim\\sigma_{2}\\sim\\sigma_{3}}\\Bigg[\\mathbb{E}_{\\sigma_{2}\\sim\\sigma_{3}}\\Bigg(X\\sum_{i=1}^{n}(\\widehat{x}_{i}(X))\\sum_{\\widehat{\\theta}\\in\\mathcal{X}_{i}}(Z)\\Big)\\Bigg|_{(1,0)}\\mathcal{F}_{i,\\widehat{\\theta}\\in\\mathcal{X}_{i}}\\Bigg]\\Bigg]}\\\\ &{=\\frac{1}{n}\\mathbb{E}_{\\sigma_{1}\\sim\\sigma_{2}\\sim\\sigma_{3}}\\Bigg[\\sum_{i=1}^{n}\\widehat{F}_{i,\\widehat{\\theta}\\in\\mathcal{X}_{i}}\\Bigg[\\sum_{p=0}^{\\infty}\\left[\\sum_{q=i}^{n}\\widehat{F}_{i,q}(X)\\right]\\sum_{\\theta\\in\\mathcal{X}_{i}}(Z)\\Big|_{(1,0)}\\mathcal{F}_{i,\\theta}(X)\\Bigg]\\Bigg|_{(1,0)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{1}{n}}\\\\ &{\\frac{1}{2}\\frac{\\sum_{q=i,q=i,q=i,n}}\\bigg\\{\\sum_{q=i}^{n}\\sum_{q=i}^{n}\\sum_{q=i}^{q}\\mathbb{E}_{\\sigma_{2}}\\Bigg[\\sum_{p=0}^{\\infty}\\widehat{F}_{i,q}(X)\\sum_{\\theta\\in\\mathcal{X}_{i}}(Z)\\Big|_{(1,0)}\\mathcal{F}_{i,q}(X)\\bigg]}\\Bigg|_{(1,0)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where equality $a$ follows from the fact that for $k(x)=0$ , number of non-zero entries in the EaS of $x$ using(14)iszeand,Threfore, wedenote thequantity $\\begin{array}{r}{\\sum_{j:x\\in C_{j}}\\frac{\\mathbb{I}[n\\mu_{n}(C_{j})>0]\\mathbb{I}[j\\in A_{t}(x)]}{n\\mu_{n}(C_{j})}=}\\end{array}$ 0. Inequality $b$ follows from Lemma F.8. Inequality $c$ follows since we are adding a non-negative quantity. Finally, inequality $d$ follows from the fact that $n\\mu_{n}(C_{j})$ is Binomially distributed with parameters $n$ and $\\mu(C_{j})$ and by an application of lemma E.4. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Lemma F.8. ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left[\\sum_{j:x\\in C_{j}}\\frac{\\mathbb{I}[n\\mu_{n}(C_{j})>0]\\mathbb{I}[j\\in A_{t}(x)]}{n\\mu_{n}(C_{j})}\\Big|k(x)=k^{\\prime}\\right]\\le\\sum_{j=1}^{m}\\frac{\\mu(C_{j})\\mathbb{I}[n\\mu_{n}(C_{j})>0]}{n\\mu_{n}(C_{j})}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Conditioned on $k(x)=k^{\\prime}$ , EaS representation given in (14) has exactly $k^{\\prime}$ non-zero entries. Therefore, using Lemma 3.4, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x}\\left[\\displaystyle\\sum_{j=0}^{(\\infty)}\\|u_{j}(C_{j})\\rangle\\!\\!\\right.\\otimes_{t\\equiv}^{(\\pm)}\\|\\big[\\mathcal{G}\\!\\!\\!\\to\\!\\!\\pmb{A}(x)\\Big]_{t}|x(z)\\rangle}\\\\ &{\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where equality a follows from part(i of lemma 3.4 since C) forms a partition of $\\mathcal{X}$ and the definition of $\\sigma$ in section 3.2 with $k=k^{\\prime}$ . Equality $b$ follows from the following observation. In the line above inequality $b$ , we are summing $k^{\\prime}\\times\\left({m\\atop k^{\\prime}}\\right)$ terms. Since $\\begin{array}{r}{k^{\\prime}\\times\\binom{m}{k^{\\prime}}=\\bar{m}\\times\\binom{m-1}{k^{\\prime}-1}}\\end{array}$ and any $j\\in[m]$ can appear in exactly $\\binom{m-1}{k^{\\prime}-1}$ different subsets of of size $k^{\\prime}$ , equality $b$ is simply rearranging the terms from the line above by changing the indices appropriately. Equality $c$ follows from part (iv) of lemma 3.4. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See section 3,4,5. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: See section 6. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See section 3,4 and Appendix B-F. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: section 5, Appendix A. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to acces this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Data is available from OpenML repository. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Section 5. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: Error bar provided in the form of shaded graph. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See Appendix A. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 37}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Data used from OpenML repository. KNN and Random Forest models are run from Scikit-learn. See section 5. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "juidelines: \u00b7 The answer NA means that the paper does not release new assets. \u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \u00b7 The paper should discuss whether and how consent was obtained from people whose assetis used. \u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: Gnidelines. ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]