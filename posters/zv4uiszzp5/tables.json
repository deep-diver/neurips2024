[{"figure_path": "zv4UISZzp5/tables/tables_4_1.jpg", "caption": "Table 1: Score Evaluation Criteria.", "description": "This table presents the scoring rubric used to evaluate the quality of the LLM's answers.  Each answer is assigned a score from 0 to 4, based on criteria such as relevance, accuracy, and completeness. A score of 0 indicates an irrelevant or harmful response, while a score of 4 indicates an answer that exceeds expectations.", "section": "2.5 Discrimination Estimation Model"}, {"figure_path": "zv4UISZzp5/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of Discrimination Indexes and Difficulty Score on Public Datasets.", "description": "This table compares the discrimination indexes and difficulty scores of six different datasets used for evaluating Large Language Models (LLMs).  The datasets include: WizardLM, Instruction Tuning with GPT-4, SELF-INSTRUCT_seed_data, SELF-INSTRUCT, SELF-INSTRUCT-Ours (a dataset created by applying the authors' proposed method to the SELF-INSTRUCT seed data), and Ours (hard seed data), which represents the authors' dataset generated from more challenging seed data.  Higher discrimination indexes indicate better ability to differentiate between LLMs with varying capabilities, while higher difficulty scores suggest the dataset poses more challenging questions for the LLMs. The results highlight that the authors' method, particularly when applied to more challenging seed data, produces a dataset with superior discrimination and difficulty compared to existing benchmark datasets.", "section": "3.2 Comparison to Public Datasets"}, {"figure_path": "zv4UISZzp5/tables/tables_7_2.jpg", "caption": "Table 3: Evaluation Scores for Various Models on Different Datasets.", "description": "This table presents the evaluation scores for various large language models (LLMs) across different datasets.  The scores represent the average performance of each LLM on each dataset, offering a comparison of their relative strengths and weaknesses.  The \"Var.\" column shows the variance of the scores for each model across datasets, indicating the consistency of the LLM's performance. The datasets used include WizardLM, Instruction Tuning with GPT-4, SELF-INSTRUCT_seed_data, SELF-INSTRUCT, SELF-INSTRUCT-Ours, and Ours (hard seed data), representing various methods for generating and curating evaluation datasets. The table provides valuable insights into the discriminative power of the different datasets and the relative performance of LLMs.", "section": "3.2 Comparison to Public Datasets"}, {"figure_path": "zv4UISZzp5/tables/tables_8_1.jpg", "caption": "Table 4: Evaluation Scores for Seed Data and Generalization Questions.", "description": "This table presents a comparison of the usability, discrimination index, and difficulty scores for seed data and generalization questions.  The usability score reflects the percentage of usable questions.  Discrimination index and difficulty score are metrics used to evaluate the quality of the generated questions, representing how well the questions differentiate between high and low-performing LLMs and how challenging they are, respectively.", "section": "3.3 Analysis on the generalization questions"}, {"figure_path": "zv4UISZzp5/tables/tables_12_1.jpg", "caption": "Table 2: Comparison of Discrimination Indexes and Difficulty Score on Public Datasets.", "description": "This table compares the discrimination indexes and difficulty scores of several publicly available datasets used for evaluating large language models (LLMs).  The datasets include WizardLM, Instruction Tuning with GPT-4, SELF-INSTRUCT (seed data and full dataset), SELF-INSTRUCT-Ours (dataset generated by the authors' method using SELF-INSTRUCT seed data), and Ours (hard seed data - a dataset generated by the authors using more challenging seed data). The discrimination index reflects the ability of the dataset to distinguish between high and low-performing LLMs, while the difficulty score indicates the overall difficulty of the questions. The table shows that the datasets generated by the authors' method, especially using more challenging seed data, achieve higher discrimination indexes and difficulty scores compared to other public datasets, indicating that the proposed method produces more effective evaluation data.", "section": "3.2 Comparison to Public Datasets"}, {"figure_path": "zv4UISZzp5/tables/tables_13_1.jpg", "caption": "Table 2: Comparison of Discrimination Indexes and Difficulty Score on Public Datasets.", "description": "This table compares the discrimination index and difficulty score of six different datasets, including three baseline datasets (WizardLM, Instruction Tuning with GPT-4, and SELF-INSTRUCT) and three datasets generated using the proposed method in the paper (SELF-INSTRUCT_seed_data, SELF-INSTRUCT-Ours, and Ours (hard seed data)). The discrimination index measures the ability of a dataset to differentiate between high and low performers, while the difficulty score measures the overall difficulty of the dataset. The results show that the datasets generated using the proposed method achieve significantly higher discrimination indexes and difficulty scores, indicating that these datasets are more effective at evaluating the performance of large language models.", "section": "3.2 Comparison to Public Datasets"}, {"figure_path": "zv4UISZzp5/tables/tables_13_2.jpg", "caption": "Table 2: Comparison of Discrimination Indexes and Difficulty Score on Public Datasets.", "description": "This table compares the discrimination indexes and difficulty scores of several public datasets used for evaluating large language models (LLMs).  The datasets include SELF-INSTRUCT, WizardLM, Instruction Tuning with GPT-4, and the authors' own datasets (both with standard and hard seed data). The discrimination index measures how well a dataset can differentiate between high-performing and low-performing LLMs, while the difficulty score represents the overall difficulty of the questions in the dataset.  The table shows that the authors' dataset, particularly the one using \"hard seed data,\" achieves higher discrimination and difficulty scores than most of the other public datasets.", "section": "3.2 Comparison to Public Datasets"}, {"figure_path": "zv4UISZzp5/tables/tables_16_1.jpg", "caption": "Table 11: Comparison of different models based on performance metrics.", "description": "This table presents the results of an ablation study comparing the performance of four different large language models (LLMs) in generating high-quality data for evaluating LLMs. The models are evaluated based on the discrimination power of the generated questions, which are categorized into four levels: Low, Relatively Low, Relatively High, and High. The table shows the number of questions generated by each model that fall into each category, revealing the relative strengths and weaknesses of the different models in generating discriminative questions.", "section": "A.2 Supplementary Experiment"}, {"figure_path": "zv4UISZzp5/tables/tables_16_2.jpg", "caption": "Table 12: Usability of Data after Filtering by Different Models", "description": "This table compares the usability of a dataset before and after it was filtered using different models. The dataset was produced using a method described in the paper that includes a chain of thought (CoT) based approach to assess the usability of questions. The first column indicates which model was used to evaluate the usability, and the second column presents the usability after filtering. The third column shows the usability after a second filtering pass intended to correct previously deemed unusable questions.  The combined use of both models yielded the highest usability scores.", "section": "3.4 Discrimination and Difficulty Estimation Models Performance Evaluation"}]