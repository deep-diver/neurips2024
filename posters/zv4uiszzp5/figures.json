[{"figure_path": "zv4UISZzp5/figures/figures_2_1.jpg", "caption": "Figure 1: Self-Correct Instruction Generalization Framework with \"Instruction Gradient\"", "description": "This figure illustrates the self-correct instruction generalization framework used in the paper.  It starts with a handcrafted batch of seed data, divided into mathematical and general text categories.  The framework then uses \"Instruction Gradient\" (rules based on instruction perspective) and \"Response Gradient\" (rules based on LLM responses) to generalize the questions.  For general text, the process involves generating LLM responses and then creating new questions based on those responses. For mathematical questions, a Chain of Thought (CoT) check is used for correctness, and corrections are iteratively applied via a self-correct mechanism.  The overall goal is to generate a high-quality dataset of questions for evaluating LLMs.", "section": "2 Method"}, {"figure_path": "zv4UISZzp5/figures/figures_4_1.jpg", "caption": "Figure 1: Self-Correct Instruction Generalization Framework with \"Instruction Gradient\"", "description": "This figure illustrates the self-correct instruction generalization framework used in the paper.  It starts with a small set of hand-crafted seed data, which is then generalized using two methods: instruction gradient and response gradient. The instruction gradient method generalizes questions based on pre-defined rules derived from the instruction, while the response gradient method generalizes questions based on the model's responses. A self-correction mechanism is used to rectify the generated questions, ensuring high quality and usability. This framework is designed to generate questions automatically to test and differentiate Large Language Models (LLMs).", "section": "2 Method"}, {"figure_path": "zv4UISZzp5/figures/figures_15_1.jpg", "caption": "Figure 1: Self-Correct Instruction Generalization Framework with \"Instruction Gradient\"", "description": "This figure illustrates the self-correct instruction generalization framework.  It starts with handcrafted seed data (divided into math and general text categories).  The \"Instruction Gradient\" uses rules to generalize questions from the instruction perspective. Then, the \"Response Gradient\" method generalizes questions based on LLM responses. A CoT (Chain of Thought) check mechanism ensures the quality of the mathematical questions through iterative feedback and correction.  The framework uses two gradient methods to generate a larger dataset from a smaller initial dataset.", "section": "2 Method"}]