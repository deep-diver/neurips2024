{"references": [{"fullname_first_author": "Dan Alistarh", "paper_title": "Byzantine stochastic gradient descent", "publication_date": "2018-00-00", "reason": "This paper is foundational for Byzantine-resilient training in machine learning, introducing the concept and establishing key results in the field."}, {"fullname_first_author": "Sai Praneeth Karimireddy", "paper_title": "Byzantine-robust learning on heterogeneous datasets via bucketing", "publication_date": "2020-00-00", "reason": "This paper proposes a novel bucketing approach for handling Byzantine failures in heterogeneous distributed systems, significantly advancing the robustness of distributed machine learning."}, {"fullname_first_author": "Sai Praneeth Karimireddy", "paper_title": "Learning from history for byzantine robust optimization", "publication_date": "2021-00-00", "reason": "This work introduces a powerful meta-aggregation framework that leverages historical information to improve robustness against Byzantine failures, demonstrating a significant improvement over previous state-of-the-art methods."}, {"fullname_first_author": "Youssef Allouah", "paper_title": "Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity", "publication_date": "2023-00-00", "reason": "This recent paper presents a comprehensive approach to optimal Byzantine-resilient machine learning, addressing the challenges of heterogeneity and establishing new theoretical guarantees."}, {"fullname_first_author": "Yi-Rui Yang", "paper_title": "Buffered asynchronous sgd for byzantine learning", "publication_date": "2023-00-00", "reason": "This work tackles the unique challenges of Byzantine failures in asynchronous settings, introducing new algorithmic solutions and improving convergence rates in this challenging environment."}]}