[{"type": "text", "text": "Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tehila Dahan Kfr Y. Levy   \nDepartment of Electrical Engineering Department of Electrical Engineering Technion Technion Haifa, Israel Haifa, Israel   \nt.dahan@campus.technion.ac.il kfirylevy@technion.ac.il ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous computing resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent metaaggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, there has been significant growth in the development of large-scale machine learning (ML) models and the volume of data they require [Zhao et al., 2023]. To efficiently accelerate largescale training processes, Distributed ML has emerged as a crucial approach that can be categorized into synchronous and asynchronous paradigms. In synchronous learning, workers update the model simultaneously using the average of their outputs, similar to the Minibatch approach [Dekel et al., 2012]. Asynchronous learning, however, allows workers to operate independently, sending updates as they are ready without waiting for others [Arjevani et al., 2020]. This prevents slow workers from hindering the process, making it especially practical as the number of workers increases. ", "page_idx": 0}, {"type": "text", "text": "A major challenge of distributed ML is fault-tolerance, and Byzantine ML [Alistarh et al., 2018, Lamport et al., 2019, Guerraoui et al., 2023] is a powerful framework for tackling this aspect. Byzantine ML captures a broad spectrum of failures within distributed environments, including random malfunctions or even malicious workers aiming to disrupt the training process. This makes Byzantine ML widely applicable across various domains to ensure robust performance. ", "page_idx": 0}, {"type": "text", "text": "Addressing the Byzantine problem in synchronous distributed learning is well-established [Karimireddy et al., 2020, 2021, Allouah et al., 2023, Farhadkhani et al., 2022, Alistarh et al., 2018, Dahan and Levy, 2024]. Two primary ingredients were found to be crucial towards tackling Byzantine ML in synchronous settings: (i) Robust Aggregators [Yin et al., 2018, Blanchard et al., 2017, Chen et al., 2017]: such aggregators combine the gradient estimates sent by the workers to a single estimate while filtering out the outliers which may hinder the training process. While the use of robust aggregators is crucial, it was found to be insuffcient, and an additional ingredient of (i) learning from history was shown to be vital in mitigating Byzantine faults [Karimireddy et al., 2021]. And, the performance of robust aggregators was systematically explored within a powerful generic framework [Karimireddy et al., 2020, 2021, Allouah et al., 2023, Farhadkhani et al., 2022, Dahan and Levy, 2024]. Moreover, due to the diversity of Byzantine scenarios [Xie et al., 2020a, Allen-Zhu et al., 2020, Baruch et al., 2019], it was found that relying on a single aggregator is insufficient, making the variety of robust aggregators essential. Unfortunately, many existing aggregators have sub-optimal performance. This drawback was elegantly resolved by the design of meta-aggregators [Karimireddy et al., 2020, Allouah et al., 2023, Dahan and Levy, 2024], that enable to boost the performance of baseline aggregators. Unfortunately, in the asynchronous case, the use of robust aggregators is not straightforward, as updates are typically applied individually per-worker, rather than averaging outputs from all workers at once [Arjevani et al., 2020]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite its advantages, asynchronous distributed learning presents unique challenges, particularly when dealing with Byzantine faults. The delays inherent in asynchronous settings introduce additional bias to the system and obscure the disruptions caused by Byzantine faults. In fact, in contrast to the synchronous Byzantine setting, all existing approaches towards the asynchronous Byzantine case do not ensure a generalization error (excess loss) that diminishes with the number of honest data-samples and updates. This applies to works for both convex [Fang et al., 2022] as well as non-convex scenarios [Xie et al., 2020b, Yang and Li, 2023]; as well as to works that further assume the availability of a trusted dataset possessed by the central-server [Xie et al., 2020b, Fang et al., 2022]. Furthermore, the performance guarantees of all existing approaches towards that setting include an explicit dependence on the dimension of the problem \u2014\u2014 a drawback that does not exist for SOTA synchronous Byzantine approaches. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We explore the asynchronous Byzantine setting under the fundamental framework of Stochastic Convex Optimization (SCO) [Hazan et al., 2016]. Our work is the first to achieve a convergence rate that diminishes with the number of honest data samples and updates and does not explicitly depend on the problem's dimension. In the absence of Byzantine workers, our rate matches the optimal performance of Byzantine-free asynchronous settings. This stands in contrast to previous efforts on Byzantine, which did not attain diminishing rates or dimensionality independence, even without Byzantine workers. We also show the effectiveness of our approaches in practice. Our contributions: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We quantify the dificulty in asynchronous scenarios by considering the number of Byzantine updates, which is more natural than the standard measure of number of Byzantine workers. \u00b7 We identify the need to utilize weighted aggregators rather than standard ones in favor of asynchronous Byzantine problems. Towards doing so, we extend the robust aggregation framework to allow and include weights and develop appropriate (weighted) rules and a meta-aggregator. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Achieving Optimal Convergence: We incorporate our weighted robust framework with a recent double momentum mechanism, leveraging its unique features to achieve an optimal convergence rate for the first time in asynchronous Byzantine ML. ", "page_idx": 1}, {"type": "text", "text": "Related Work.  A long line of studies has explored the synchronous Byzantine setting (see e.g., Alistarh et al. [2018], Karimireddy et al. [2020, 2021], Allouah et al. [2023], Farhadkhani et al. [2022], Allen-Zhu et al. [2020], E1 Mhamdi et al. [2021], Dahan and Levy [2024]). Alistarh et al. [2018], Karimireddy et al. [2021] demonstrated that historical information is crucial for optimal performance in Byzantine scenarios; and Karimireddy et al. [2021] introduced the idea of combining generic aggregation rules, together with standard momentum with a parameter of $1/\\sqrt{T}$ to effectively incorporates $\\sqrt{T}$ iterations of historical gradients. Additionally, Dahan and Levy [2024] showed that a double momentum approach is effective by taking a momentum parameter of $1/T$ ,capturing the entire gradient history. ", "page_idx": 1}, {"type": "text", "text": "Robust aggregators such as Coordinate-wise Trimmed Mean (CWTM) [Yin et al., 2018], Krum [Blanchard et al., 2017], Geometric Median (GM) [Chen et al., 2017], CWMed [Yin et al., 2018], and Minimum Diameter Averaging [Guerraoui et al., 2018] have also proven to be highly beneficial in synchronous settings and have been evaluated within robust frameworks [Allouah et al., 2023, Karimireddy et al., 2020, Farhadkhani et al., 2022, Dahan and Levy, 2024]. However, not all robust aggregators achieve optimal performance, leading to the development of meta-aggregators [Karimireddy et al., 2020, Allouah et al., 2023, Dahan and Levy, 2024] to enhance their effectiveness. While standard aggregation works well in synchronous settings, where outputs are averaged across all workers, it is less suitable for asynchronous settings, where updates are processed individually as they arrive [Arjevani et al., 2020]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To adapt these approaches to asynchronous settings, Yang and Li [2023] devised BASGDm, an extension of BASGD [Yang and Li, 2021], that groups worker momentums into buckets that are then aggregated using a robust aggregator. Other methods, like $Z_{\\mathrm{eno++}}$ [Xie et al., 2020b] and AFLGuard [Fang et al., 2022], rely on a trusted dataset on the central server, which hinders their practicality. Kardam [Damaskinos et al., 2018] uses the Lipschitzness of gradients to filter out outliers. Unfortunately, none of these approaches ensure a generalization error (excess loss) that diminishes with the number of honest data-samples and updates, and suffers from an explicit dependence on the problem's dimension. And this applies even in the absence of Byzantine faults. ", "page_idx": 2}, {"type": "text", "text": "Asynchronous Byzantine ML faces unique challenges as inherent delays add bias that obscures Byzantine disruptions. To mitigate this delay-bias in asynchronous, non-Byzantine scenarios, Cohen et al. [2021], Aviv et al. [2021] propose methods to keep model weights relatively close during iterations. Other approaches [Stich and Karimireddy, 2019, Arjevani et al., 2020, Mishchenko et al., 2022] suggest adjusting the step size proportionally to the delay. These strategies have proven useful in reducing the negative impact of delays, and achieve optimal performance. ", "page_idx": 2}, {"type": "text", "text": "Our work extends several concepts from Dahan and Levy [2024] to the asynchronous scenario. We devise a novel generalization of their Centered Trimmed Meta Aggregator (CTMA) towards weighted meta-aggregation, making it amenable to asynchronous scenarios. In the spirit of Dahan and Levy [2024], we also adopt a recent variance reduction technique called $\\mu^{2}$ -SGD [Levy, 2023]. Nevertheless, while Dahan and Levy [2024] used this technique in a straightforward manner, we found it crucial to appropriately incorporate individual per-worker weights to overcome the challenge of asynchronicity in Byzantine ML. ", "page_idx": 2}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our discussion focuses on the minimization of a smooth convex objective $f:K\\rightarrow\\mathbb{R}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\mathbf{x}):=\\mathbb{E}_{\\mathbf{z}\\sim\\mathcal{D}}[f(\\mathbf{x};\\mathbf{z})]\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\kappa\\subseteq\\mathbb{R}^{d}$ is a compact convex set and $\\mathcal{D}$ denotes an unknown distribution from which we can draw i.i.d samples $\\{\\mathbf{z}_{t}\\sim\\mathcal{D}\\}_{t}$ . Our work considers first-order methods that iteratively utilize gradient information to approach an optimal point. Such methods output a solution $\\mathbf{x}_{T}$ , which is evaluated by the expected excess loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{ExcessLoss}:=\\mathbb{E}[f({\\bf x}_{T})-f({\\bf x}^{*})]\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}^{*}$ is a solution that minimizes $f$ over $\\kappa$ and $\\mathbf{x}_{T}\\in\\mathcal{K}$ approximates this optimal solution. ", "page_idx": 2}, {"type": "text", "text": "Asynchronous Training. We explore these methods within a distributed environment involving multiple workers. Our discussion focuses on a centralized distributed framework characterized by a central Parameter Server $(\\mathcal{P}S)$ that may communicate with $m$ workers. Each of these workers may draw i.i.d. samples $\\mathbf{z}\\sim\\mathcal{D}$ ; and based on these samples, compute unbiased gradient estimate $\\mathbf{g}\\in\\mathbb{R}^{\\bar{d}}$ at a point $\\mathbf{x}\\in{\\mathcal{K}}$ . Concretely, a worker may compute $\\mathbf{g}:=\\nabla f(\\mathbf{x};\\mathbf{z})$ ; implying that $\\mathbb{E}[\\mathbf{g}|\\mathbf{x}]=\\nabla f(\\mathbf{x})$ Specifically, our main focus is on Asynchronous systems, where the $\\mathcal{P}S$ does not wait to receive the stochastic gradient computations from all machines; instead, it updates its model whenever a worker completes a (stochastic) gradient computation. That worker then proceeds to compute a gradient estimate for the updated model, while the other workers continue to compute gradients based on 'stale\u2019 models. This staleness leads to the use of staled (and therefore biased) gradient estimates, which is a major challenge in designing and analyzing asynchronous training methods. ", "page_idx": 2}, {"type": "text", "text": "Asynchronous Byzantine Framework. We assume that an unknown subset of the $m$ workersare Byzantine, implying that these workers may transmit arbitrary or malicious information during the training process, and these \"Byzantine\" workers may even collaborate to disrupt the training. We assume that the fraction of updates that arrive from Byzantine workers during the asynchronous training process is bounded and strictly smaller than $^1\\!/\\!2$ and denote this fraction by $\\lambda$ ", "page_idx": 2}, {"type": "text", "text": "Remark 2.1 (Fraction of Byzantine Updates vs. Byzantine Workers). In both synchronous and asynchronous settings, it is common to consider a bound on the fraction of Byzantine workers (up to I/2) [Allouah et al., 2023, Farhadkhani et al., 2022, Karimireddy et al., 2020, 2021, Yang and Li, 2023, 2021, Damaskinos et al., 2018]. In synchronous scenarios this is meaningful since the server equally treats the information from all workers; which is done by equally averaging gradients of all workers in each iteration in a mini-batch fashion [Dekel et al., 2012]. Conversely, in asynchronous scenarios, faster workers contribute to more updates than slower workers, leading to an unequal influence on the training process. In such scenarios, the fraction of Byzantine workers is less relevant; and it is therefore much more natural to consider the fraction of Byzantine updates. Interestingly, our definition aligns with the standard one (for the synchronous case), which considers the number of Byzantineworkers. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Notation.For each worker $i\\in[m]$ and iteration $t$ $s_{t}^{(i)}$ represents the total number of updates by worker $i$ upto $t$ and $\\tau_{t}^{(i)}$ isthedelay compared totecurrent model. $t^{(i)}$ is the lastupdate before $t$ , making $\\tau_{t}^{(i)}$ the timesinethescond lastupdate(Figure I). $\\tau_{t}$ denotesthedlay forthe worker arriving at iteration $t$ i.e., if worker $j$ arrives at iteration $t$ then $\\tau_{t}=\\tau_{t}^{(j)}$ ", "page_idx": 3}, {"type": "image", "img_path": "v1kpc060aC/tmp/aa5c45c0a40c97dfa5f484bd2a3c0f825a2953f5884062d0ca1d9cd6ed8cebdd.jpg", "img_caption": ["FigureI:stratonofthedelay $\\tau_{t}^{(i)}$ for worker $i$ at teration $t$ , marking $t$ (ourret iteration),. $t^{(i)}$ (most ecet update fom worker $i$ ), and $t-\\tau_{t}^{(i)}$ (previous update from worker $i$ "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "For a given time (iteration) $t$ let $t^{(i)}$ be the last iteration when worker $i$ made an update. We denote $\\mathbf{d}_{t}^{(i)}:=\\mathbf{d}_{t^{(i)}},\\,\\mathbf{g}_{t}^{(i)}:=\\mathbf{g}_{t^{(i)}},\\,\\tilde{\\mathbf{g}}_{t}^{(i)}:=\\tilde{\\mathbf{g}}_{t^{(i)}}.$ and $\\mathbf{x}_{t}^{(i)}=\\mathbf{x}_{t^{(i)}}$ wheretlatraivdual that we will later define for any worker $i$ . Throughout, $\\lVert\\cdot\\rVert$ represents the $L_{2}$ -norm. For any natural $N$ $[N]=\\{1,\\dots,N\\}$ We use the compressed sum notation $\\begin{array}{r}{\\alpha_{1:t}=\\sum_{k=1}^{t}\\alpha_{k}}\\end{array}$ For every $\\mathbf{x}\\in\\mathbb{R}^{d}$ the orthogonal projection of $\\mathbf{x}$ onto a set $\\kappa$ is denoted by $\\begin{array}{r}{\\Pi_{K}(\\mathbf{x})\\,=\\,\\arg\\operatorname*{min}_{\\mathbf{y}\\in\\mathcal{K}}||\\mathbf{y}-\\mathbf{x}||^{2}}\\end{array}$ .We denote $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and $\\mathcal{G}$ as the subsets of Byzantine workers and honest workers, respectively, such that $|m|=|\\mathcal{G}|+|B|$ ", "page_idx": 3}, {"type": "text", "text": "Assumptions.  We use the following conventional assumptions: ", "page_idx": 3}, {"type": "text", "text": "Bounded Diameter: we assume there exists $D>0$ such that $\\underset{\\mathbf{x},\\mathbf{y}\\in\\mathcal{K}}{\\operatorname*{max}}\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|\\leq D.$ ", "page_idx": 3}, {"type": "text", "text": "Bounded Variance: there exists $\\sigma>0$ such that $\\forall\\mathbf{x}\\in\\mathcal{K}$ $\\mathbf{z}\\in\\mathbf{Support}\\{\\mathcal{D}\\}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|\\nabla f(\\mathbf{x};\\mathbf{z})-\\nabla f(\\mathbf{x})\\|^{2}\\leq\\sigma^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Expectation over Smooth Functions: we assume that $f(\\cdot)$ is an expectation of smooth functions, i.e. $\\forall\\mathbf{x},\\mathbf{y}\\in{\\mathcal{K}}\\,,\\mathbf{z}\\in\\mathbf{Support}\\{{\\mathcal{D}}\\}$ there exist $L>0$ such that, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(\\mathbf{x};\\mathbf{z})-\\nabla f(\\mathbf{y};\\mathbf{z})\\|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above assumption also implies that the expected loss $f(\\cdot)$ is $L$ smooth. ", "page_idx": 3}, {"type": "text", "text": "Bounded Smoothness Variance [Levy, 2023]: in Appendix A we show that Eq. (3) implies that, $\\forall\\mathbf{x},\\mathbf{y}\\in{\\mathcal{K}},\\mathbf{z}\\in\\mathbf{Support}\\{{\\mathcal{D}}\\}$ there exists $\\sigma_{L}^{2}\\in[0,L^{2}]$ such, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\left(\\nabla f(\\mathbf{x};\\mathbf{z})-\\nabla f(\\mathbf{x})\\right)-\\left(\\nabla f(\\mathbf{y};\\mathbf{z})-\\nabla f(\\mathbf{y})\\right)\\right\\|^{2}\\leq\\sigma_{L}^{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Bounded Delay: $\\exists K>0$ such that fo each worker $i\\in[m],\\ \\tau_{m i n}^{(i)}\\leq\\tau_{t}^{(i)}\\leq K\\tau_{m i n}^{(i)}$ ", "page_idx": 3}, {"type": "text", "text": "where Tmin is theminmdlayoforkeKbus thevarianefthdelayforachorkr. ", "page_idx": 3}, {"type": "text", "text": "Bounded Byzantine Iterations: there exists $0\\leq\\lambda<1/2$ such that $t\\in[T]\\colon t_{B}\\leq\\lambda t$ ", "page_idx": 3}, {"type": "text", "text": "where $t_{B}$ is the total number of iterations made by Byzantine workers up to iteration $t$ ", "page_idx": 3}, {"type": "text", "text": "Sample-Arrival Independence: we asume that the delays in the system (i.e. $\\tau_{t}^{(i)}$ 's)are independent of the data samples. This is a standard assumption in asynchronous training scenarios, see e.g., Arjevani et al. [2020], Aviv et al. [2021]. ", "page_idx": 3}, {"type": "text", "text": "3 Weighted Robust Aggregation Rules ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As we have mentioned, robust aggregation rules have played a major role in designing fault-tolerant ML training methods for synchronous settings (see, e.g., Allouah et al. [2023], Karimireddy et al. [2020, 2021], Dahan and Levy [2024]). These existing aggregation rules treat inputs from all workers equally, which makes sense in synchronous cases where all workers contribute the same number of updates and data samples. Conversely, this symmetry breaks down in asynchronous settings, where faster (honest) workers contribute more updates and samples compared to slower workers. ", "page_idx": 4}, {"type": "text", "text": "Inspired by this asymmetry, we have identified the need to define a notion of weighted robust aggregators that generalizes the standard definition of robust aggregators. In this section, we provide such a definition, derive weighted variants of standard aggregators that satisfy our new definition, and design a generic meta-approach to derive optimal weighted aggregation rules. Later, in Section 4, we demonstrate the benefits of using weighted robust aggregators as a crucial building block in designing asynchronous fault-tolerant training methods (see Alg. 2). ", "page_idx": 4}, {"type": "text", "text": "3.1  Robust Weighted Aggregation Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Below, we generalize the definition introduced by Dahan and Levy [2024], Karimireddy et al. [2020, 2021] to allow and associate weights to the inputs of the robust aggregation rule, therefore allowing the aggregator to unequally treat its inputs. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1. $(c_{\\lambda},\\lambda)$ -weighted robust. Assume we have m random vectors $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m}\\,\\in\\,\\mathbb{R}^{d}$ and corresponding weights $s_{1},\\ldots,s_{m}\\,>\\,0$ Also assume we have an \"honest\" subset ${\\mathcal{G}}\\subseteq[m]$ implying $\\{\\mathbf{x}_{i}\\}\\in\\mathcal{G}$ are independent of each other.Finally,assume that there exists $\\lambda\\in[0,{^1\\mathord{/}}2)$ such that $\\begin{array}{r}{\\sum_{i\\in\\mathcal{G}}s_{i}\\geq(1-\\lambda)s_{1:m}}\\end{array}$ . Moreover, assume that for any $i\\in\\mathcal G$ there exist $\\rho_{i}\\geq0$ such that, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|{\\bf x}_{i}-\\bar{\\bf x}_{\\mathcal{G}}\\|^{2}\\leq\\rho_{i}^{2},\\quad\\forall i\\in\\mathcal{G}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then an aggregation rule $\\mathcal{A}_{\\omega}$ is called $(c_{\\lambda},\\lambda)$ -weighted robust if for any such $m$ random vectors and weights and $\\lambda\\geq0$ it outputs $\\hat{\\mathbf{x}}\\leftarrow A_{\\omega}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m};s_{1},\\ldots,s_{m})$ such that, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\hat{\\mathbf{x}}-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\|\\leq c_{\\lambda}\\rho^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some $c_{\\lambda}\\geq0$ Above, $\\begin{array}{r}{\\bar{\\mathbf{x}}_{\\mathcal{G}}:=\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbf{x}_{i},\\,\\rho^{2}:=\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\rho_{i}^{2}}\\end{array}$ and thexpectaion is w.rt. $\\{{\\bf x}_{i}\\}_{i=1}^{m}$ and (possible) randomization in the $\\mathcal{A}_{\\omega}$ ", "page_idx": 4}, {"type": "text", "text": "Here, $\\lambda$ represents the fraction of the sum of the non-honest vectors\u2019 weights, unlike the unweighted definition (in synchronous cases) [Karimireddy et al., 2020, 2021, Allouah et al., 2023, Farhadkhani et al., 2022] where it indicates the fraction of non-honest vectors. These definitions align when all weights are equal [Dahan and Levy, 2024]. Similarly to the unweighted version, the optimal $c_{\\lambda}$ shouldbe $c_{\\lambda}\\leq O(\\lambda)$ [Dahan and Levy, 2024]. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1. Note that our definition is generic and may be applied in both convex and non-convex scenarios. Moreover, it is natural to consider such weighted aggregators beyond asynchronous settings.For example,in synchronous settings where workers havevarying batch sizes,weighted aggregation based on batch sizes may be more effective than uniform aggregation. ", "page_idx": 4}, {"type": "text", "text": "Next, we present two weighted variants of standard (non-weighted) aggregators that satisfy the above definition (we defer the proof into Appendix C). Table 1 summarizes their $c_{\\lambda}$ values. ", "page_idx": 4}, {"type": "text", "text": "3.2 Weighted Variant of Geometric Median and Coordinate- Wise ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Weighted Geometric Median (WeightedGM) The Weighted Geometric Median (WeightedGM) minimizes the weighted sum of Euclidean distances to a set of points. Formally, for points $\\{{\\bf x}_{i}\\}_{i=1}^{m}$ and corresponding weights $\\{s_{i}\\}_{i=1}^{m}$ ,Weightecd $\\begin{array}{r}{|\\mathbf{G}\\mathbf{M}\\in\\arg\\operatorname*{min}_{\\mathbf{y}\\in\\mathbb{R}^{d}}\\sum_{i\\in[m]}s_{i}\\big\\|\\mathbf{\\bar{y}}-\\bar{\\mathbf{x_{i}}}\\big\\|}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Weighted Coordinate-Wise Median (WeightedCWMed) The Weighted Coordinate-Wise Median (WeightedCWMed) aggregates multi-dimensional data by finding the weighted median of each coordinate separately. Thus, for given coordinate if $\\{{\\bf x}_{i}\\}_{i=1}^{m}$ are sorted and weights $\\{s_{i}\\}_{i=1}^{m}$ , the weighted median $\\mathbf{x}_{j}*$ is the element where: $\\begin{array}{r}{j^{*}\\,=\\,\\arg\\operatorname*{min}_{j\\in[m]}\\left\\{\\sum_{i\\in[j]}s_{i}>\\frac{1}{2}\\sum_{i\\in[m]}s_{i}\\right\\}}\\end{array}$ .If $\\begin{array}{r}{\\sum_{i=1}^{j}s_{i}={\\frac{1}{2}}\\sum_{i=1}^{m}s_{i}}\\end{array}$ for some $j$ ,then:WeigtedMedian $\\begin{array}{r}{=\\frac{\\mathbf{x}_{j}+\\mathbf{x}_{j+1}}{2}}\\end{array}$ ", "page_idx": 4}, {"type": "table", "img_path": "v1kpc060aC/tmp/f9d4ef8d8bfb262ca1e93863a04bf751ea53466aa0892389c6dfafb6c8ac97e5.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of weighted aggregation rules and their respective $c_{\\lambda}$ values. "], "page_idx": 5}, {"type": "table", "img_path": "v1kpc060aC/tmp/cd8d66e25d26194f4af9e635e1e94432696a2c7a6f198269758652a7dad3837e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3Weighted Centered Trimmed Meta Aggregator ( $\\omega$ -CTMA) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Table 1 illustrates that $\\omega$ -GM and $\\omega$ -CWMed fail to achieve the desired optimal $c_{\\lambda}=O(\\lambda)$ ; typically for $\\lambda\\,\\leq\\,^{1}\\!/3$ , their $c_{\\lambda}$ remains $\\leq\\,O(1)$ . To address this suboptimality, we propose $\\omega$ CTMA, a weighted extension of the Centered Trimmed Meta Aggregator (CTMA) [Dahan and Levy, 2024]. This extension enables us to achieve the optimal bound $c_{\\lambda}\\leq O(\\lambda)$ for $\\lambda\\leq1/3$ (see Table 1). ", "page_idx": 5}, {"type": "text", "text": "The $\\omega$ -CTMA algorithm (Algorithm 1) operates on a set of vectors along with their associated weights, a threshold $\\lambda\\in[0,{^1\\mathord{/}{\\vphantom{^{1}}^{2}}})$ , and a $(c_{\\lambda},\\lambda)$ -weighted robust aggregator. It sorts the distances between each vector and the weighted robust aggregator, trims the set based on the threshold to satisfy $\\begin{array}{r}{\\sum_{i\\in S}s_{i}=(1-\\lambda)s_{1:m}}\\end{array}$ and callatweghtdaveragef thvtor xcling ulirbasd on their proximity to an anchor point--the weighted robust aggregator. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. Under the assumptions outlined in Definition 3.1, if $\\omega$ -CTMA receives a $(c_{\\lambda},\\lambda)$ weighted robust aggregator, $\\mathcal{A}_{\\omega}$ ; then the output of $\\omega$ -CTMA, x, is $(60\\lambda(1+c_{\\lambda}),\\lambda)$ -robust. ", "page_idx": 5}, {"type": "text", "text": "For the complete analysis, please refer to Appendix C.2. Like CTMA [Dahan and Levy, 2024], $\\omega$ -CTMA is highly efficient, with a computational complexity of $O(d m\\!+\\!m\\log m)$ ,similar to $\\omega$ GM, $\\omega$ -CWMed, and weighted averaging, differing by at most an additional logarithmic factor. ", "page_idx": 5}, {"type": "text", "text": "4  Asynchronous Robust Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Weleveragethe $\\mu^{2}$ -SGD algorithm [Levy, 2023], a double momentum mechanism that enhances variance reduction. By seamlessly incorporating our weighted robust framework as a black box into the $\\mu^{2}$ -SGD, we derive an optimal asynchronous Byzantine convergence rate. ", "page_idx": 5}, {"type": "text", "text": "$\\mu^{2}$ SGD:The $\\mu^{2}$ -SGD is a variant of standard SGD, incorporating several key modifications in its update rule: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\Pi_{K}\\left(\\mathbf{w}_{t}-\\eta\\alpha_{t}\\mathbf{d}_{t}\\right),\\quad\\mathbf{x}_{t+1}={\\frac{1}{\\alpha_{1:t+1}}}\\sum_{k\\in[t+1]}\\alpha_{k}\\mathbf{w}_{k};\\quad\\mathbf{w}_{1}=\\mathbf{x}_{1}\\in K,\\;\\forall t>1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\{\\alpha_{t}>0\\}_{t}$ are importance weights that emphasize different update steps, with $\\alpha_{t}\\propto t$ toplace more weight on recent updates. The sequence $\\left\\{{\\bf x}_{t}\\right\\}_{t}$ represents weighted averages of the iterates $\\{\\mathbf{w}_{t}\\}_{t}$ , and ${\\bf d}_{t}$ is an estimate of the gradient at the average point, $\\nabla_{\\bar{f}}(\\mathbf{x}_{t})$ , differing from standard SGD, which estimates gradients at the iterates, $\\nabla f(\\mathbf{w}_{t})$ ", "page_idx": 5}, {"type": "text", "text": "This approach relates to Anytime-GD [Cutkosky, 2019], which is strongly connected to momentum and acceleration concepts [Cutkosky, 2019, Kavis et al., 2019]. While the stochastic version of Anytime-GD typically uses the estimate $\\nabla f({\\bf x}_{t};{\\bf z}_{t})$ $\\mu^{2}$ -SGD employs a variance reduction mechanism to produce a corrected momentum estimate ${\\bf d}_{t}$ [Cutkosky and Orabona, 2019]. Specifically, $\\begin{array}{r}{\\mathbf{d}_{1}=\\nabla f(\\mathbf{x}_{1};\\mathbf{z}_{1})}\\end{array}$ , and for $t>2$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{d}_{t}=\\nabla f(\\mathbf{x}_{t};\\mathbf{z}_{t})+(1-\\beta_{t})(\\mathbf{d}_{t-1}-\\nabla f(\\mathbf{x}_{t-1};\\mathbf{z}_{t})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "1: Input: learning rate $\\eta_{t}\\,>\\,0$ , starting point $\\mathbf{x}_{1}\\in\\mathcal{K}$ , number of steps $T$ , importance weights   \n$\\left\\{\\alpha_{t}\\right\\}_{t}$ , momentum correction weights $\\{\\beta_{t}\\}_{t}$ \uff0c $(c_{\\lambda},\\lambda)$ -robust weighted aggregation function $\\mathcal{A}_{\\omega}$   \n2: Initialize: Vi E [m], set s) $s_{0}^{(i)}=0$ Set $\\mathbf{w}_{1}=\\mathbf{x}_{1}$ . Each honest worker $i\\in\\mathcal G$ draws $\\mathbf{z}^{(i)}\\sim\\mathcal{D}$ and   \nset $\\mathbf{d}_{1}^{(i)}=\\nabla f(\\mathbf{x}_{1};\\mathbf{z}^{(i)})$   \n3: for $t=1$ to $T$ do Server update   \n4: Receive ${\\bf d}_{t-\\tau_{t}}$ from worker $i\\in[m]$ and update:   \n5: d(i)= dt-tsl $s_{t}^{(i)}=s_{t-1}^{(i)}+1$ = st-1 +1; $\\forall j\\neq i$ set $s_{t}^{(j)}=s_{t-1}^{(j)}$ and for $\\underline{{t}}>1{:}\\mathbf{d}_{t}^{(j)}=\\mathbf{d}_{t-1}^{(j)}$ \uff1b   \n6: Update server model:   \n7: $\\begin{array}{r}{{\\bf w}_{t+1}=\\Pi_{K}\\left({\\bf w}_{t}-\\eta_{t}\\alpha_{t}A_{\\omega}(\\{{\\bf d}_{t}^{(j)},{s}_{t}^{(j)}\\}_{j=1}^{m})\\right),\\;\\&\\;\\;{\\bf x}_{t+1}=\\frac{1}{\\sum_{k=1}^{t+1}\\alpha_{k}}\\sum_{k=1}^{t+1}\\alpha_{k}{\\bf w}_{k}}\\end{array}$   \n8: Send $\\mathbf{x}_{t}$ to worker $i$ . If $i$ is an honest worker, it performs the following update:   \n9: Worker $i$ draws $\\mathbf{z}_{t}\\sim\\mathcal{D}$ , computes $\\mathbf{g}_{t}=\\nabla f(\\mathbf{x}_{t};\\mathbf{z}_{t})$ \uff0c& $\\tilde{\\mathbf{g}}_{t-\\tau_{t}}=\\bar{\\nabla}f(\\mathbf{x}_{t-\\tau_{t}};\\mathbf{z}_{t})$ \uff0c   \n10: and updates: $\\mathbf{d}_{t}=\\mathbf{g}_{t}+(1-\\beta_{t})(\\mathbf{d}_{t-\\tau_{t}}-\\tilde{\\mathbf{g}}_{t-\\tau_{t}})$ $\\triangleright$ Worker update   \n11: end for ", "page_idx": 6}, {"type": "text", "text": "Here, $\\beta_{t}\\,\\in\\,[0,1]$ are corrected momentum weights. It can be shown by induction that $\\mathbb{E}[\\mathbf{d}_{t}]\\;=$ $\\mathbb{E}[\\nabla f(x_{t})]$ ; however, in general, $\\mathbb{E}[\\mathbf{d}_{t}|x_{t}]\\neq\\nabla f(\\bar{x}_{t})$ , unlike standard SGD estimators. Nevertheless, [Levy, 2023] demonstrates that choosing corrected momentum weights $\\beta_{t}:=1/t$ results in significant error reduction, with $\\mathbb{E}\\|\\varepsilon_{t}\\|^{2}:=\\mathbb{E}\\|\\mathbf{d}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}\\leq O(\\tilde{\\sigma}^{2}/t)$ at step $t$ where $\\tilde{\\sigma}^{2}\\le{\\cal O}(\\sigma^{2}\\!+\\!D^{2}\\sigma_{L}^{2})$ This indicates that variance decreases with $t$ , contrasting with standard SGD where the variance $\\mathbb{E}\\|\\varepsilon_{t}^{\\mathrm{SGD}}\\|^{2}:=\\mathbb{E}\\|\\mathbf{g}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}$ remains uniformly bounded. ", "page_idx": 6}, {"type": "text", "text": "4.1 Asynchronous Robust $\\mu^{2}$ -SGD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Building upon these, we integrate the $\\mu^{2}$ -SGD with a $(c_{\\lambda},\\lambda)$ -weighted robust aggregator $\\mathcal{A}_{\\omega}$ , as described in Alg. 2. At each iteration $t\\,\\in\\,[T]$ , the global $\\mathcal{P}S$ receives an output from a certain worker and aggregates all workers' recent updates $\\left\\{{\\bf d}_{t}^{(i)}\\right\\}_{i=1}^{m}$ by employing weights accordingly to the number of updates of each worker $\\left\\{s_{t}^{(i)}\\right\\}_{i=1}^{m}$ . An honest worker $i$ arriving at iteration $t$ returns its corrected momentum $\\mathbf{d}_{t}^{(i)}$ to the $\\mathcal{P}S$ , computed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{d}_{t}^{(i)}=\\mathbf{d}_{t-\\tau_{t}}=\\mathbf{g}_{t-\\tau_{t}}+(1-\\beta_{t-\\tau_{t}})(\\mathbf{d}_{t-\\tau_{t}-\\tau_{t-\\tau_{t}}}-\\tilde{\\mathbf{g}}_{t-\\tau_{t}-\\tau_{t-\\tau_{t}}})\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{g}_{t}:=\\nabla f(\\mathbf{x}_{t};\\mathbf{z}_{t})$ , and $\\tilde{\\mathbf{g}}_{t-\\tau_{t}}:=\\nabla f(\\mathbf{x}_{t-\\tau_{t}};\\mathbf{z}_{t})$ . Afterwards, the $\\mathcal{P}S$ performs the AnyTime update step as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\Pi_{K}\\left(\\mathbf{w}_{t}-\\eta\\alpha_{t}A_{\\omega}(\\{\\mathbf{d}_{t}^{(i)},s_{t}^{(i)}\\}_{i=1}^{m})\\right),\\;\\mathbf{x}_{t+1}=\\frac{1}{\\alpha_{1:t+1}}\\sum_{k\\in[t+1]}\\alpha_{k}\\mathbf{w}_{k}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the spirit of Levy [2023], Dahan and Levy [2024], we suggest employing $\\beta_{t}:=1/s_{t}$ , which effectively considers the entire individual gradient's history of each worker; this translates to a stochastic error bound of $\\lVert\\varepsilon_{t}^{(i)}\\rVert\\leq O(\\tilde{\\sigma}/s_{t})$ for an honest worker $i$ arriving at iteration $t$ To achieve an error corresponding to the total number of honest iterations $t_{\\mathcal{G}}$ , specifically $\\lVert\\varepsilon_{t}\\rVert\\leq O(\\tilde{\\sigma}/t_{\\mathcal{G}})$ , as in the non-distributed setting [Levy, 2023], a weighted collective error across all honest workers should be considered with weights determined by_the number of honest worker arrivals, as detailed in Theorem 4.1. The unique characteristics of $\\bar{\\mu}^{2}$ -SGD make it well-suited for the asynchronous Byzantine setting, where $\\bar{\\lambda}<{^1\\!/2}$ relates to the fraction of Byzantine iterations. The total iteration number t matches the sum of the workers frequencies (ie[g] st) $\\begin{array}{r}{(\\sum_{i\\in[\\mathcal{G}]}s_{t}^{(i)}=t\\mathcal{G})}\\end{array}$ , aligning with the weighted robust definition in Definition 3.1. Using other approaches like momentum [Karimireddy et al., 2020, 2021, Allouah et al., 2023] is less straightforward in the asynchronous Byzantine setting with the weighted robust definition. This complexity arises because an individual honest error $\\lVert\\varepsilon_{t}^{(i)}\\rVert\\lesssim O(\\bar{\\tilde{\\sigma}}/\\sqrt{s_{t}})$ implies that weights should be $\\sqrt{s_{t}}$ instead of $s_{t}$ , which can be more challenging. Remark 4.1 (Memory and Computational Overhead of Algorithm 2). Algorithm 2 incurs additional memory and computational costs compared to the asynchronousByzantine-free setting[Arjevani et al., 2020], where the server stores only one worker's output and the global model. For robust performance, Algorithm 2 stores the latest outputs from all workers, increasing memory usage to $O(d m)$ Robust aggregation methods like $\\omega$ -CWMed [Yin et al., 2018] and $\\omega$ -GM [Chen et al., 2017, Acharya et al., 2022] add a computational cost of $O(d m\\log m)$ per round, unlike asynchronous Byzantine-free settings where worker outputs are used without aggregation. Comparable overheads are observed in synchronous Byzantine-resilient methods, which similarly aggregate outputs from all workers. This reflects a necessary trade-off: achieving robustness inherently requires leveraging information from all workers to counteract the influence of potentially faulty ones. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1. For a convex set $\\kappa$ with bounded diameter $D$ and a function $f:K\\mapsto\\mathbb{R}_{}$ andassume the assumptions in Equations (2),(3),(4). Then Alg. 2 with parameters $\\{\\alpha_{t}=t\\}_{t}$ and $\\{\\beta_{t}=1/s_{t}\\}_{t}$ ensures thefollowingfor every $t\\in[T]$ andeachhonestworker $i\\in\\mathcal G$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\varepsilon_{t}^{(i)}\\right\\|^{2}\\leq\\frac{\\tilde{\\sigma}^{2}}{s_{t}^{(i)}},\\quad\\mathbb{E}\\left\\|\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}}\\!\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\varepsilon_{t}^{(i)}\\right\\|^{2}\\leq\\frac{\\tilde{\\sigma}^{2}}{t_{\\mathcal{G}}}\\;,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where Et $\\varepsilon_{t}^{(i)}\\,=\\,{\\bf d}_{t}^{(i)}\\,-\\,\\nabla f({\\bf x}_{t}^{(i)})$ \uff0c $\\tilde{\\sigma}^{2}\\,=\\,2\\sigma^{2}\\,+\\,32D^{2}K^{2}\\sigma_{L}^{2}$ and $t_{\\mathcal{G}}$ isthe total number of honest iterations up to the $t^{\\mathrm{th}}$ iteration. ", "page_idx": 7}, {"type": "text", "text": "Proof Sketch of Thm. 4.1. The complete analysis is provided in App. B.1. It involves several key stepsfor anhonest $i$ worker who arrives at iteration $t$ ", "page_idx": 7}, {"type": "text", "text": "1.Foo dstsq $\\begin{array}{r}{\\|\\mathbf{x}_{t}^{(i)}-\\mathbf{x}_{t-\\tau_{t}}^{(i)}\\|\\leq\\frac{4K}{s_{t}^{(i)}-1}D}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "2.Wele $\\varepsilon_{t}^{(i)}$ by seting $\\begin{array}{r}{\\beta_{t}=\\frac{1}{s_{t}^{(i)}}}\\end{array}$ andobtini ", "page_idx": 7}, {"type": "equation", "text": "$$\ns_{t}^{(i)}\\varepsilon_{t}^{(i)}=(\\mathbf{g}_{t}^{(i)}-\\nabla f(\\mathbf{x}_{t}^{(i)}))+(s_{t}^{(i)}-1)Z_{t}^{(i)}+(s_{t}^{(i)}-1)\\varepsilon_{t-\\tau_{t}}^{(i)}\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $Z_{t}^{(i)}:=\\mathbf{g}_{t}^{(i)}-\\nabla f(\\mathbf{x}_{t}^{(i)})-(\\tilde{\\mathbf{g}}_{t-\\tau_{t}}^{(i)}-\\nabla f(\\mathbf{x}_{t-\\tau_{t}}^{(i)}))$ Urling tisrsopoi explit expresonr $\\begin{array}{r}{s_{t}^{(i)}\\varepsilon_{t}^{(i)}=\\sum_{k\\in[s_{t}^{(i)}]}\\mathcal{M}_{k}^{(i)}}\\end{array}$ , where $\\mathring{\\mathcal{M}}_{s_{t}^{(i)}}^{(i)}:=\\mathbf{g}_{t}^{(i)}-\\nabla f(\\mathbf{x}_{t}^{(i)})+(s_{t}-1)Z_{t}^{(i)}$ thus, $\\{\\mathcal{M}_{k}^{(i)}\\}_{k\\in[s_{t}^{(i)}]}$ ", "page_idx": 7}, {"type": "text", "text": "3. Employing the above with Eq. (2) and (4), we have: $\\begin{array}{r}{\\mathbb{E}\\|\\mathcal{M}_{k}^{(i)}\\|^{2}\\le2\\sigma^{2}+32D^{2}K^{2}\\sigma_{L}^{2}=\\tilde{\\sigma}^{2}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "4. Leveraging the properties of a martingale difference sequence, we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|s_{t}^{(i)}\\varepsilon_{t}^{(i)}\\right\\|^{2}=\\mathbb{E}\\left\\|\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathcal{M}_{k}^{(i)}\\right\\|^{2}=\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathbb{E}\\left\\|\\mathcal{M}_{k}^{(i)}\\right\\|^{2}\\leq\\tilde{\\sigma}^{2}s_{t}^{(i)}\\;,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\varepsilon_{t}^{(i)}\\right\\|^{2}=\\mathbb{E}\\left\\|\\sum_{i\\in\\mathcal{G}}\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathcal{M}_{k}^{(i)}\\right\\|^{2}=\\sum_{i\\in\\mathcal{G}}\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathbb{E}\\left\\|\\mathcal{M}_{k}^{(i)}\\right\\|^{2}\\leq\\tilde{\\sigma}^{2}\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}=\\tilde{\\sigma}^{2}t_{\\mathcal{G}}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 4.2. Compared to synchronous scenarios [Levy, 2023, Dahan and Levy, 2024], the variance $\\tilde{\\sigma}$ inThm.2 additionally includes thevariance in the delay, denoted as $K$ (Eq. (5)). In balanced scheduling methods,likeRound Robin [Langford et al.,2009],the impact of $K$ on theerror becomes minor, as the delay $\\tau_{t}^{(i)}=m$ is constant. In the case of constant delays, the factor $K$ equals 1. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.1. Let $\\mathcal{A}_{\\omega}$ be $(c_{\\lambda},\\lambda)$ -weightedrobust aggregationruleand let $f:K\\mapsto\\mathbb{R}_{}$ where $\\kappa$ is $a$ convexsetwithboundeddiameter $D$ ,and presume that the assumption in Equations (2),(3),(4) hold. Then invokingAlg. 2with $\\{\\alpha_{t}=t\\}_{t}$ and $\\{\\beta_{t}=1/s_{t}\\}_{t}$ ensuresthefollowingforany $t\\in[T]$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\hat{\\mathbf{d}}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\|^{2}\\leq O\\left(\\underbrace{\\frac{\\tilde{\\sigma}^{2}}{t}+\\frac{c_{\\lambda}m\\tilde{\\sigma}^{2}}{t}}_{\\mathrm{Variance}}+\\underbrace{\\frac{(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}+\\frac{c_{\\lambda}(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}}_{\\mathrm{Bias}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 4.1 shows that the error between our gradient estimator $\\hat{\\mathbf{d}}_{t}$ and the true gradient includes a bias term arising from the aggregation of delayed momentums. This is in contrast to the synchronous scenario [Dahan and Levy, 2024] where the error is solely variance-dependent without any bias component. However, this bias does not affect the overall excess loss (Theorem 4.2), which remains comparable to the optimal rate achieved in synchronous Byzantine settings (see Remark 4.5). ", "page_idx": 8}, {"type": "text", "text": "By integrating the weighted robust aggregator with the double momentum mechanism, we achieve the optimal convergence rate for the first time in an asynchronous Byzantine setting\u2014-a significant advancement over previous efforts. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2 (Asynchronous Byzantine $\\mu^{2}$ -SGD Guarantees). Let $\\mathcal{A}_{\\omega}$ be $(c_{\\lambda},\\lambda)$ -weighted robust aggregation rule and let $f$ be a convex function. Also, let us make the same assumptions as in Thm. 4.1, and let us denote $G^{*}:=\\|\\nabla f(\\mathbf{x}^{*})\\|$ ,where $\\mathbf{x}^{*}\\in\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{K}}f(\\mathbf{x})$ .Then invoking Alg. 2 with $\\{\\alpha_{t}=t\\}_{t}$ and $\\{\\beta_{t}=1/s_{t}\\}_{t}$ ,and using a learning rate $\\eta\\leq1/4L T$ guarantees, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\mathbf{x}_{T})-f(\\mathbf{w}^{*})\\right]\\leq O\\left(\\frac{G^{*}D+L D^{2}\\mu_{m a x}(\\sqrt{1+c_{\\lambda}})}{T}+\\frac{D\\tilde{\\sigma}(\\sqrt{1+c_{\\lambda}m})}{\\sqrt{T}}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\sigma}^{2}=2\\sigma^{2}+32D^{2}K^{2}\\sigma_{L}^{2},\\ \\mu_{m a x}=\\frac{1}{T}\\sum_{t\\in[T]}\\tau_{t}^{m a x},\\ a n d\\,\\tau_{t}^{m a x}=\\operatorname*{max}_{i\\in[m]}\\{\\tau_{t}^{(i)}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Remark 4.3. In the absence of Byzantine iterations $\\lambda=0$ ),the parameter $c_{\\lambda}$ ofa $(c_{\\lambda},\\lambda)$ -weighted robust aggregator can diminish to $\\boldsymbol{O}$ when we use $\\omega$ -CTMA (see Table 1). This aligns with the asynchronous SGD analysis [Arjevani et al., 2020] and represents the first work to achieve optimal convergence without Byzantine workers compared to previous efforts [Yang and Li, 2021, 2023, Fang et al., 2022, Zhu et al., 2023, Damaskinos et al., 2018, Xie et al., 2020b, Zhu et al., 2024]. ", "page_idx": 8}, {"type": "text", "text": "Remark 4.4. Unlike previous works [Yang and Li, 2021, 2023, Fang et al., 2022, Zhu et al., 2023, Damaskinos et al., 2018, Xie et al., 2020b, Zhu et al., 2024], our convergence rate is independent of datadimensionality $d$ andissublinearat $T$ ,evenin the presence of Byzantineworkers. ", "page_idx": 8}, {"type": "text", "text": "Remark 4.5. This result is consistent with the synchronous scenario [Dahan and Levy, 2024], where the delay is constant $\\tau_{t}=m$ as in Round Robin [Langford et al., 2009]. In this case, the proportion of Byzantine workers is $\\lambda$ andtheasynchronousexcesslossis $\\begin{array}{r}{\\mathrm{~\\,~}\\backslash\\leq O\\left(\\frac{L D^{2}m}{T}+\\frac{D\\tilde{\\sigma}\\sqrt{1+c_{\\lambda}m}}{\\sqrt{T}}\\right)}\\end{array}$ In comparison to the synchronous case,where mworkers perform $R$ rounds, here we make $R$ query point updates and $T=R m$ data-samples,resultinginsynchronousexcessloss $\\begin{array}{r l r}{\\mathrm{~}}&{{}}&{\\leq O\\left(\\frac{L D^{2}}{R}+\\frac{D\\tilde{\\sigma}\\sqrt{1/m+c_{\\lambda}}}{\\sqrt{R}}\\right)=}\\end{array}$ $\\begin{array}{r}{O\\left(\\frac{L D^{2}m}{T}+\\frac{D\\tilde{\\sigma}\\sqrt{1+m c_{\\lambda}}}{\\sqrt{T}}\\right)}\\end{array}$ [Dahan andLevy,2024]. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the effectiveness of our proposed approach, we conducted experiments on MNIST [LeCun et al., 2010] and CIFAR-10 [Krizhevsky et al., 2014] datasets\u2014two recognized benchmarks in image classification tasks. We employed a two-layer convolutional neural network architecture for both datasets, implemented using the PyTorch framework. The training was performed using the cross-entropy loss function, and all computations were executed on an NVIDIA RTX 3090 GPU. To ensure the robustness of our findings, each experiment was repeated with three different random seeds, and the results were averaged accordingly. Our experimental results demonstrate consistent performance across both datasets. Further details about the experimental setup and the complete results are provided in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Weighted vs. Non-Weighted Robust Aggregators. We evaluated the test accuracy of weighted and non-weighted robust aggregators in imbalanced asynchronous Byzantine environments. Our experiments show that weighted robust aggregators consistently achieved higher test accuracy than the non-weighted ones (see Figure 2 and Figure 5). This highlights the benefit of prioritizing workers who contribute more updates in asynchronous setups. ", "page_idx": 8}, {"type": "image", "img_path": "v1kpc060aC/tmp/1e872e011e66c1021a9076c8f4edfbe25ae52418f487d3f19688f7a631f01789.jpg", "img_caption": ["Figure 2: CIFAR-10. Test Accuracy of Weighted vs. Non-Weighted Robust Aggregators. This scenario involves 17 workers, including 8 Byzantine workers, with workers arrival probabilities proportional to the square of their IDs. We usedthe $\\mu^{2}$ -SGD in this scenario. Left: label fipping, $\\lambda=0.3$ Right:signflipping, $\\lambda=0.4$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Effectiveness of $\\omega$ -CTMA. We evaluated the test accuracy of weighted robust aggregators with and without the integration of $\\omega$ -CTMA, as shown in Figure 3 and Figure 6. The results demonstrate that $\\omega$ -CTMA can enhance the performance of weighted robust aggregators in various Byzantine scenarios. Notably, $\\omega$ -CTMA may maintain high accuracy even when other robust aggregators fail, as seen with the Empire attack result for both datasets. ", "page_idx": 9}, {"type": "image", "img_path": "v1kpc060aC/tmp/86f2cddbbd760dc8aba1bc1948577e816021f3bce673cdfe8d604eb99e132d3e.jpg", "img_caption": ["Figure 3: CIFAR-10. Test Accuracy Comparison of Weighted Robust Agregators With and Without $\\omega$ CTMA. This scenario involves 9 workers with 1 or 3 Byzantine workers. Workers? arrival probabilities are proportional to their IDs, and we used $\\mu^{2}$ SGD. On the left, we have the label fipping and litleattacks with $\\lambda=0.1$ and $\\lambda=0.2$ for 1 and 3 Byzantine workers, respectively. On the right, the empire attack, each with $\\lambda=0.4$ and 3 Byzantine workers. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Performance of $\\mu^{2}$ -SGD vs. Standard Momentum and SGD. We evaluated the test accuracy of $\\mu^{2}$ -SGD in comparison to standard momentum [Polyak, 1964] and SGD [eon Bottou, 1998] within an asynchronous Byzantine setup. Figure 4 and Figure 7 show that $\\mu^{2}$ -SGD performs on par with standard momentum, while SGD generally exhibits poorer performance relative to both. These results underscore the importance of utilizing historical information when addressing Byzantine scenarios. ", "page_idx": 9}, {"type": "image", "img_path": "v1kpc060aC/tmp/42e459fc2f2026292b6462899c4a88fabc1dba9d2133d51733b2b081df7adb6e.jpg", "img_caption": ["Figure 4: CIFAR-10. Test Accuracy Comparison Among Different Optimizers. This scenario involves 9 workers (4 Byzantine)with $\\lambda=0.4$ for the first three from left to right, and $\\lambda=0.3$ for the label flipping attack on the left. Workers' arrival probabilities are proportional to their IDs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper shows that using a double momentum approach, which incorporates the entire history of each honest worker, improves the error bound to be proportional to the total number of updates when considering their weighted average in asynchronous settings. By integrating this method with a weighted robust framework, $\\mu^{2}$ -SGD achieves an optimal convergence rate, making it particularly effective for asynchronous Byzantine environments. However, integrating other optimization algorithms, like momentum, into this weighted robust framework can be challenging, as they do not achieve an error bound proportional to the total number of updates and may complicate the adjustment of weights based on the update count. This highlights the need for further research to adapt different methods to the spirit of this framework in non-convex and convex settings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially supported by Israel PBC-VATAT, the Technion Artificial Intelligent Hub (Tech.AI), and the Israel Science Foundation (grant No. 3109/24). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Anish Acharya, Abolfazl Hashemi, Prateek Jain, Sujay Sanghavi, Inderjit S Dhillon, and Ufuk Topcu. Robust training in high dimensions via block coordinate geometric median descent. In International Conference on Artificial Intelligence and Statistics, pages 11145-11168. PMLR, 2022.   \nDan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. Advances in Neural Information Processing Systems, 31, 2018.   \nZeyuan Allen-Zhu, Faeze Ebrahimian, Jerry Li, and Dan Alistarh. Byzantine-resilient non-convex stochastic gradient descent. arXiv preprint arXiv:2012.14368, 2020.   \nYoussef Allouah, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan. Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity. In International Conference on Artificial Intelligence and Statistics, pages 1232-1300. PMLR, 2023.   \nYossi Arjevani, Ohad Shamir, and Nathan Srebro. A tight convergence analysis for stochastic gradient descent with delayed updates. In Algorithmic Learning Theory, pages 111-132. PMLR, 2020.   \nRotem Zamir Aviv, Ido Hakimi, Assaf Schuster, and Kfir Yehuda Levy. Asynchronous distributed learning: Adapting to gradient delays without prior knowledge. In International Conference on Machine Learning, pages 436-445. PMLR, 2021.   \nGilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. Advances in Neural Information Processing Systems, 32, 2019.   \nPeva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. Advances in neural information processing systems, 30, 2017.   \nYudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial setings: Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2):1-25, 2017.   \nAlon Cohen, Amit Daniely, Yoel Drori, Tomer Koren, and Mariano Schain. Asynchronous stochastic optimization robust to arbitrary delays. Advances in Neural Information Processing Systems, 34: 9024-9035, 2021.   \nAshok Cutkosky. Anytime online-to-batch, optimism and acceleration. In International conference on machine learning, pages 1446-1454. PMLR, 2019.   \nAshok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd. Advances in neural information processing systems, 32, 2019.   \nTehila Dahan and Kfr Yehuda Levy. Fault tolerant ml: Efficient meta-aggregation and synchronous training. In Forty-first International Conference on Machine Learning, 2024.   \nGeorgios Damaskinos, Rachid Guerraoui, Rhicheek Patra, Mahsa Taziki, et al. Asynchronous byzantine machine learning (the case of sgd). In International Conference on Machine Learning, pages 1145-1154. PMLR, 2018.   \nOfer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13(1), 2012.   \nEl Mahdi El Mhamdi, Rachid Guerraoui, and Sebastien Louis Alexandre Rouault. Distributed momentum for byzantine-resilient stochastic gradient descent. In 9th International Conference on Learning Representations (ICLR), number CONF, 2021.   \nL eon Bottou. Online learning and stochastic approximations. Online learning in neural networks, 17(9):142, 1998.   \nMinghong Fang, Jia Liu, Neil Zhenqiang Gong, and Elizabeth S Bentley. Afguard: Byzantinerobust asynchronous federated learning. In Proceedings of the 38th Annual Computer Security Applications Conference, pages 632-646, 2022.   \nSadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan. Byzantine machine learning made easy by resilient averaging of momentums. In International Conference on Machine Learning, pages 6246-6283. PMLR, 2022.   \nRachid Guerraoui, Sebastien Rouault, et al. The hidden vulnerability of distributed learning in byzantium. In International Conference on Machine Learning, pages 3521-3530. PMLR, 2018.   \nRachid Guerraoui, Nirupam Gupta, and Rafael Pinot. Byzantine machine learning: A primer. ACM Computing Surveys, 2023.   \nElad Hazan et al. Introduction to online convex optimization. Foundations and Trends? in Optimization, 2(3-4):157-325, 2016.   \nSai Praneeth Karimireddy, Lie He, and Martin Jaggi. Byzantine-robust learning on heterogeneous datasets via bucketing. arXiv preprint arXiv:2006.09365, 2020.   \nSai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for byzantine robust optimization. In International Conference on Machine Learning, pages 5311-5319. PMLR, 2021.   \nAli Kavis, Kfr Y Levy, Francis Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algorithm with optimal guarantees for constrained optimization. Advances in neural information processing systems, 32, 2019.   \nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: ht tps : / www. cs. toronto. edu/ \\~kriz/ cifar. html,55(5), 2014.   \nLeslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. In Concurrency: the works of leslie lamport, pages 203-226. 2019.   \nJohn Langford, Alexander Smola, and Martin Zinkevich. Slow learners are fast. arXiv preprint arXiv:0911.0491, 2009.   \nYann LeCun, Corinna Cortes, Chris Burges, et al. Mnist handwritten digit database, 2010. URL http: / /yann .lecun . com/exdb/mnist/. Licensed under CC BY-SA 3.0, available at https : //creativecommons.org/licenses/by-sa/3.0/.   \nKfir Y Levy. $\\mu^{2}$ -sgd: Stable stochastic optimization via a double momentum mechanism. arXiv preprint arXiv:2304.04172, 2023.   \nKonstantin Mishchenko, Francis Bach, Mathieu Even, and Blake E Woodworth. Asynchronous sgd beats minibatch sgd under arbitrary delays. Advances in Neural Information Processing Systems, 35:420-433, 2022.   \nBoris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics, 4(5):1-17, 1964.   \nSebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd with delayed gradients and compressed communication. arXiv preprint arXiv: 1909.05350, 2019.   \nCong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant sgd by inner product manipulation. In Uncertainty in Artificial Intelligence, pages 261-270. PMLR, 2020a.   \nCong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd. In International Conference on Machine Learning, pages 10495-10503. PMLR, 2020b.   \nYi-Rui Yang and Wu-Jun Li. Basgd: Buffered asynchronous sgd for byzantine learning. In International Conference on Machine Learning, pages 11751-11761. PMLR, 2021.   \nYi-Rui Yang and Wu-Jun Li. Buffered asynchronous sgd for byzantine learning. Journal of Machine Learning Research, 24(204):1-62, 2023.   \nDong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In International Conference on Machine Learning, pages 5650-5659. PMLR, 2018.   \nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.   \nZehan Zhu, Yan Huang, Chengcheng Zhao, and Jinming Xu. Asynchronous byzantine-robust stochastic aggregation with variance reduction for distributed learning. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 151-158. IEEE, 2023.   \nZehan Zhu, Yan Huang, Chengcheng Zhao, and Jinming Xu. Asynchronous byzantine-robust stochastic aggregation with variance reduction for distributed learning. 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Bounded Smoothness Variance Assumption ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We show that Eq. (3) implies that Eq. (4) holds for some $\\sigma_{L}^{2}\\in[0,L^{2}]$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|(\\nabla f(\\mathbf{x};\\mathbf{z})-\\nabla f(\\mathbf{x}))-(\\nabla f(\\mathbf{y};\\mathbf{z})-\\nabla f(\\mathbf{y}))\\|^{2}=\\mathbb{E}\\|\\nabla f(\\mathbf{x};\\mathbf{z})-\\nabla f(\\mathbf{y};\\mathbf{z})\\|^{2}-\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le L^{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, we also used $\\mathbb{E}[\\nabla f(\\mathbf{x};\\mathbf{z})-\\nabla f(\\mathbf{y};\\mathbf{z})]=(\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y}))$ , and followed Eq. (3). Therefore, we establish that $\\sigma_{L}^{2}\\in[0,L^{2}]$ ", "page_idx": 13}, {"type": "text", "text": "B Asynchronous Robust Convex Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Thm. 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Thm. 4.1. To simplify the discussion, let's introduce some notations for a worker $i\\in\\mathcal G$ who arrives at time $t$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{x}}_{s_{t}}:=\\mathbf{x}_{t-\\tau_{t}}=\\mathbf{x}_{t}^{(i)},\\quad\\tilde{\\mathbf{x}}_{s_{t}-1}:=\\mathbf{x}_{t-\\tau_{t}-\\tau_{t-\\tau_{t}}}=\\mathbf{x}_{t-\\tau_{t}}^{(i)}}\\\\ {\\tilde{\\varepsilon}_{s_{t}}:=\\varepsilon_{t-\\tau_{t}}=\\varepsilon_{t}^{(i)},\\quad\\tilde{\\varepsilon}_{s_{t-1}}:=\\varepsilon_{t-\\tau_{t}-\\tau_{t}}=\\varepsilon_{t-\\tau_{t}}^{(i)}}\\\\ {\\mathbf{h}_{s_{t}}:=\\mathbf{g}_{t-\\tau_{t}}=\\mathbf{g}_{t}^{(i)},\\quad\\tilde{\\mathbf{h}}_{s_{t-1}}:=\\tilde{\\mathbf{g}}_{t-\\tau_{t}-\\tau_{t}}=\\tilde{\\mathbf{g}}_{t-\\tau_{t}}^{(i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, we will employ the following lemma that bounds the distance between the averages $\\mathbf{x}_{t},\\mathbf{x}_{t-\\tau_{t}}$ Recall that $\\mathbf{x}_{t}$ and $\\mathbf{x}_{t-\\tau_{t}}$ are consecutive query points for the worker $i$ that arrives at time $t$ ", "page_idx": 13}, {"type": "text", "text": "Lemma B.1 (Aviv et al. [2021]). Let $f:\\mathcal{K}\\mapsto\\mathbb{R},$ where $\\kappa$ is a convex set with bounded diameter $D$ Then invoking Alg. 2 with $\\{\\alpha_{t}=t\\}_{t}$ ensures the following for any $t\\in[T]$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-\\tau_{t}}\\|\\leq\\frac{4D\\tau_{t}}{t}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For completeness, we provide a proof in Section B.1.1. ", "page_idx": 13}, {"type": "text", "text": "Next, we define $\\mu_{t}$ be the average delay of the worker $i$ that arrives at iteration $t$ ,i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\ns_{t}=\\frac{t}{\\mu_{t}},\\quad s_{t}-1=s_{t-\\tau_{t}}=\\frac{t-\\tau_{t}}{\\mu_{t-\\tau_{t}}}\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From Equation (5), we infer that, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tau_{m i n}^{(i)}\\leq\\mu_{t}\\leq K\\tau_{m i n}^{(i)}\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Following this, we analyze the upper bound on the distance between two successive query points for anhonestworker $i$ that arrives attime $t$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bigl\\|\\tilde{\\mathbf{x}}_{s_{t}}-\\tilde{\\mathbf{x}}_{s_{t}-1}\\bigr\\|=\\bigl\\|\\mathbf{x}_{t-\\tau_{t}}-\\mathbf{x}_{t-\\tau_{t}-\\tau_{t-\\tau_{t}}}\\bigr\\|\\leq\\frac{4\\tau_{t-\\tau_{t}}}{t-\\tau_{t}}D=\\frac{4\\tau_{t-\\tau_{t}}}{\\bigl(\\mu_{t-\\tau_{t}}\\bigr)\\bigl(s_{t}-1\\bigr)}D\\leq\\frac{4K}{s_{t}-1}D\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality follows Lemma B.1. The second equality utilizes the relation $\\begin{array}{r}{s_{t}\\!-\\!1=\\frac{t-\\tau_{t}}{\\mu_{t-\\tau_{t}}}}\\end{array}$ The final inequality stems from the assumptions in Eq. (5) and Eq. (7). ", "page_idx": 13}, {"type": "text", "text": "Remark: Before proceeding with the analysis, we shall condition the (possible randomization) in the delays of all workers; and recall that the data-samples are independent of the delays. Thus, the expectations that we take are only with respect to the randomization in the data-samples and are conditioned on the delays. Thus, this conditioning allows us to treat the delays $\\tau_{t}^{(i)}$ 's and number of updates $s_{t}^{(i)}$ 's as fixed and predefined. ", "page_idx": 13}, {"type": "text", "text": "We proceed to analyze the recursive dynamics of $\\tilde{\\varepsilon}_{s_{t}}$ for each $i\\in\\mathcal G$ . Based on the definitions of ${\\bf d}_{t}$ and $\\varepsilon_{t}$ , we can present the recursive relationship in the following way: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\varepsilon}_{s_{t}}=\\beta_{t}\\big(\\mathbf h_{s_{t}}-\\nabla f\\big(\\tilde{\\mathbf x}_{s_{t}}\\big)\\big)+(1-\\beta_{t})Z_{s_{t}}+(1-\\beta_{t})\\tilde{\\varepsilon}_{s_{t}-1}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where $Z_{s_{t}}:=\\mathbf{h}_{s_{t}}-\\nabla f(\\tilde{\\mathbf{x}}_{s_{t}})-(\\tilde{\\mathbf{h}}_{s_{t}-1}-\\nabla f(\\tilde{\\mathbf{x}}_{s_{t}-1}))$ . Upon choosing $\\begin{array}{r}{\\beta_{t}=\\frac{1}{s_{t}}}\\end{array}$ , we can eformulate the above equation as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s_{t}\\tilde{\\varepsilon}_{s_{t}}=\\big(\\mathbf{h}_{s_{t}}-\\nabla f(\\tilde{\\mathbf{x}}_{s_{t}})\\big)+(s_{t}-1)Z_{s_{t}}+(s_{t}-1)\\tilde{\\varepsilon}_{s_{t}-1}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Unrolling this recursion yields an explicit expression for any $s_{t}\\geq1$ ", "page_idx": 14}, {"type": "equation", "text": "$$\ns_{t}\\tilde{\\varepsilon}_{s_{t}}=\\sum_{k\\in[s_{t}]}{\\mathcal{M}}_{k}^{(i)}\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have defined, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{M}_{k}^{(i)}:=\\mathbf{h}_{k}-\\nabla f(\\widetilde{\\mathbf{x}}_{k})+(k-1)Z_{k}\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $k$ is a counter for the iterations where worker $i$ makes an update. ", "page_idx": 14}, {"type": "text", "text": "Following this, we derive an upper bound for the expected square norm of $\\mathcal{M}_{k}^{(i)}$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\|\\mathcal{M}_{k}^{(i)}\\|^{2}\\le2\\mathbb{E}\\|{\\mathbf{h}}_{k}-\\nabla f(\\tilde{{\\mathbf{x}}}_{k})\\|^{2}+2(k-1)^{2}\\mathbb{E}\\|({\\mathbf{h}}_{k}-\\nabla f(\\tilde{{\\mathbf{x}}}_{k}))-(\\tilde{{\\mathbf{h}}}_{k-1}-\\nabla f(\\tilde{{\\mathbf{x}}}_{k-1}))\\|^{2}}&{}\\\\ {\\le2\\sigma^{2}+2\\sigma_{L}^{2}(k-1)^{2}\\mathbb{E}\\|{\\mathbf{x}}_{k}-{\\mathbf{x}}_{k-1}\\|^{2}}&{}\\\\ {\\le2\\sigma^{2}+32D^{2}K^{2}\\sigma_{L}^{2}=\\tilde{\\sigma}^{2}\\,,}&{(\\mathrm{3}\\sigma^{2}+\\sigma^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality uses $\\|\\mathbf{a}+\\mathbf{b}\\|^{2}\\leq2\\|\\mathbf{a}\\|^{2}+2\\|\\mathbf{b}\\|^{2}$ ,whichholds $\\forall\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^{d}$ .Thesecond inequality aligns with the assumptions outlined in Equations (2) and (4). The third inequality uses Eq. (8). ", "page_idx": 14}, {"type": "text", "text": "Establishing the First Part of the Theorem: Before continuing, it is natural to define an ordered set of samples $\\{\\mathbf{z}_{1},\\mathbf{z}_{2},\\ldots,\\mathbf{z}_{t_{\\mathcal{G}}}\\}$ such that these samples are associated with honest and consecutive updates (or iterates) of the $\\mathcal{P}S$ , and $t_{\\mathcal{G}}$ is the total number of honest updates up to time $t$ . Concretely, the $\\tau^{\\mathrm{th}}$ honest update of the $\\mathcal{P}S$ is based on an honest worker that utilizes a fresh sample $\\mathbf{z}_{\\tau}$ ", "page_idx": 14}, {"type": "text", "text": "Now, for a given worker $i$ we shall define the filtration associated with his updates. Concretely, let $k\\in\\{1,\\ldots\\bar{s}_{T}^{(i)}\\}$ aeua $\\mathcal{F}_{k}^{(i)}$ $\\{\\mathbf{z}_{1},\\mathbf{z}_{2},\\ldots,\\mathbf{z}_{t_{\\mathcal{G}}}\\}$ $k^{\\mathrm{th}}$ $i$ $\\{\\mathcal{F}_{k}^{(i)}\\}_{k\\in[s_{t}^{(i)}]}$ filtration. Moreover, it can be directly shown that for a given worker $i$ , then the above defined sequence $\\{\\mathcal{M}_{k}^{(i)}\\}_{k\\in[s_{t}^{(i)}]}$ $\\{\\mathcal{F}_{k}^{(i)}\\}_{k\\in[s_{t}^{(i)}]}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|s_{t}^{(i)}\\varepsilon_{t}^{(i)}\\right\\|^{2}=\\mathbb{E}\\left\\|\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathcal{M}_{k}^{(i)}\\right\\|^{2}=\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathbb{E}\\left\\|\\mathcal{M}_{k}^{(i)}\\right\\|^{2}\\leq\\tilde{\\sigma}^{2}s_{t}^{(i)}\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have also used Equations (9) and (11). Thus, the above bounds establish the first part of the theorem. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2 (See e.g. Lemma B.1 in Levy [2023]). Let $\\{M_{t}\\}_{t}$ be a martingale difference sequence with respect to a filtration $\\{\\mathcal{F}_{t}\\}_{t}$ then the following holds for any $t_{;}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\sum_{\\tau\\in[t]}M_{\\tau}\\right\\|^{2}=\\sum_{\\tau\\in[t]}\\mathbb{E}\\left\\|M_{\\tau}\\right\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Establishing the Second Part of the Theorem: As before, we define an ordered set of samples $\\{\\mathbf{z}_{1},\\mathbf{z}_{2},\\ldots,\\mathbf{z}_{t_{\\mathcal{G}}}\\}$ such that these samples are associated with honest and consecutive updates (or iterates) of the $\\mathcal{P}S$ , and $t_{\\mathcal{G}}$ is the total number of honest updates up to time $t$ . Concretely, the $\\tau^{\\mathrm{th}}$ honest update of the $\\mathcal{P}S$ is based on an honest worker that utilizes a fresh sample $\\mathbf{z}_{\\tau}$ . We shall also define $\\{{\\mathcal{F}}_{\\tau}\\}_{\\tau\\in[t_{\\mathcal{G}}]}$ be the natural filtration induced by the ordered sequence of data samples. ", "page_idx": 14}, {"type": "text", "text": "Moreover, for a given sample $\\mathbf{z}_{\\tau}\\in\\{\\mathbf{z}_{1},\\mathbf{z}_{2},\\dots,\\mathbf{z}_{t_{\\mathcal{G}}}\\}$ , let $i_{\\tau}\\in[m]$ be the worker that is associated with the $\\tau^{\\mathrm{th}}$ honest update of the $\\mathcal{P}S$ , with a fresh sample $\\mathbf{z}_{\\tau}$ . In this case, we shall define: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{\\tau}:=\\mathcal{M}_{s_{\\tau}^{(i_{\\tau})}}^{(i_{\\tau})}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Where $\\mathcal{M}_{k}^{(i)}$ is defned in Eqg 10) It simmediate to show that $\\{\\mathcal{M}_{\\tau}\\}_{\\tau\\in[t_{\\mathcal{G}}]}$ is a martingale ifference sequence with respect to $\\{{\\mathcal{F}}_{\\tau}\\}_{\\tau\\in[t_{\\mathcal{G}}]}$ .Moreover, the following holds directly be the defnition of $\\boldsymbol{\\mathcal{M}}_{\\tau}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{G}}\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathcal{M}_{k}^{(i)}=\\sum_{\\tau=1}^{t_{\\mathcal{G}}}\\mathcal{M}_{\\tau}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now using Eq. (9) the following holds, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\varepsilon_{t}^{(i)}=\\sum_{i\\in\\mathcal{G}}\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathcal{M}_{k}^{(i)}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining Equations (12) and (13) together with Lemma B.2 establishes the second part of the theorem, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\varepsilon_{t}^{(i)}\\right\\|^{2}=\\mathbb{E}\\left\\|\\sum_{i\\in\\mathcal{G}}\\sum_{k\\in\\left[s_{t}^{(i)}\\right]}\\mathcal{M}_{k}^{(i)}\\right\\|^{2}=\\mathbb{E}\\left\\|\\sum_{\\tau=1}^{t_{\\mathcal{G}}}\\mathcal{M}_{\\tau}\\right\\|^{2}=\\sum_{\\tau=1}^{t_{\\mathcal{G}}}\\mathbb{E}\\left\\|\\mathcal{M}_{\\tau}\\right\\|^{2}\\leq\\tilde{\\sigma}^{2}t_{\\mathcal{G}}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality uses the bound in Eq. (11). ", "page_idx": 15}, {"type": "text", "text": "B.1.1 Proof of Lemma B.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We borrowed the following steps from Aviv et al. [2021]. Let's define $\\mathbf{y}\\in{\\mathcal{K}}$ as theaverage of $\\mathbf{x}_{t}$ over the interval $[t-\\tau_{t},t]$ i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{y}:=\\frac{1}{\\alpha_{t-\\tau_{t}+1:t}}\\sum_{i=t-\\tau_{t}+1}^{t}\\alpha_{i}\\mathbf{w}_{i}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we have the following relationship: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{1:t}\\mathbf{x}_{t}=\\sum_{i=1}^{t}\\alpha_{i}\\mathbf{w}_{i}=\\sum_{i=1}^{t-\\tau_{t}}\\alpha_{i}\\mathbf{w}_{i}+\\sum_{i=t-\\tau_{t}+1}^{t}\\alpha_{i}\\mathbf{w}_{i}=\\alpha_{1:t-\\tau_{t}}\\mathbf{x}_{t-\\tau_{t}}+\\alpha_{t-\\tau_{t}+1:t}\\mathbf{y}\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{1:t-\\tau_{t}}(\\mathbf{x}_{t}-\\mathbf{x}_{t-\\tau_{t}})=\\alpha_{t-\\tau_{t}+1:t}(\\mathbf{y}-\\mathbf{x}_{t})\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By setting $\\alpha_{t}=t$ , we have that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\mathbf x}_{t}-{\\mathbf x}_{t-\\tau_{t}}\\|=\\frac{\\alpha_{t-\\tau_{t}+1:t}}{\\alpha_{1:t-\\tau_{t}}}\\|{\\mathbf y}-{\\mathbf x}_{t}\\|}\\\\ {\\displaystyle=\\frac{\\tau_{t}(t-\\tau_{t}+1+t)}{(t-\\tau_{t})(t-\\tau_{t}+1)}\\|{\\mathbf y}-{\\mathbf x}_{t}\\|}\\\\ {\\displaystyle\\leq\\frac{\\tau_{t}(t-\\tau_{t}+1)}{(t-\\tau_{t})(t-\\tau_{t}+1)}\\|{\\mathbf y}-{\\mathbf x}_{t}\\|+\\frac{t\\tau_{t}}{(t-\\tau_{t})^{2}}\\|{\\mathbf y}-{\\mathbf x}_{t}\\|}\\\\ {\\displaystyle=\\frac{\\tau_{t}}{t-\\tau_{t}}\\|{\\mathbf y}-{\\mathbf x}_{t}\\|+\\frac{t\\tau_{t}}{(t-\\tau_{t})^{2}}\\|{\\mathbf y}-{\\mathbf x}_{t}\\|~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $t\\geq3\\tau_{t}$ , we have that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-\\tau_{t}}\\|\\leq\\frac{3\\tau_{t}D}{2t}+\\frac{9\\tau_{t}D}{4t}\\leq\\frac{4\\tau_{t}D}{t}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given that the domain s bounded, $\\|{\\mathbf x}_{t}\\!-{\\!\\mathbf x}_{t-\\tau_{t}}\\|\\leq D\\ \\forall t$ for $t<3\\tau_{t}$ we have $\\begin{array}{r}{D<{\\frac{4\\tau_{t}D}{t}}}\\end{array}$ . Combining these results, we conclude: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-\\tau_{t}}\\|\\leq\\frac{4\\tau_{t}D}{t}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.2Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 4.1. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3. Let $f:K\\mapsto\\mathbb{R}$ where $\\kappa$ is a convex set with bounded diameter $D$ . Then invoking Alg. 2 with $\\{\\alpha_{t}=t\\}_{t}$ ensures the following for any $t\\in[T]$ and every $i,j\\in[m]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{x}_{t}^{(i)}-\\mathbf{x}_{t}^{(j)}\\right\\|\\leq\\frac{4D\\left(\\tau_{t}^{(i)}+\\tau_{t}^{(j)}\\right)}{t}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{x}_{t}^{(i)}-\\mathbf{x}_{t}^{(j)}\\right\\|\\leq\\left\\|\\mathbf{x}_{t}^{(i)}-\\mathbf{x}_{t}\\right\\|+\\left\\|\\mathbf{x}_{t}-\\mathbf{x}_{t}^{(j)}\\right\\|\\leq\\frac{4D\\tau_{t}^{(i)}}{t}+\\frac{4D\\tau_{t}^{(j)}}{t}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is a result of the triangle inequality, and the second follows Lemma B.1. ", "page_idx": 16}, {"type": "text", "text": "Bias Bounds. Here's a refined version: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We begin by analyzing the upper bound of the bias in the collective gradients of honest workers up to time $t$ in relation to the gradient at that time, denoted as $B_{t}^{1}$ . Following this, we derive the upper bound for the bias between the collective gradients of these honest workers and the gradient of an individual honest worker, also up to time $t$ , which we denote as $B_{t}^{2}$ . For clarity, we define $\\begin{array}{r}{\\bar{\\nabla}\\mathcal{G},t:=\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}}\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\nabla f(\\mathbf{x}_{t}^{(i)})}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\cal{S}}_{t}^{1}\\right\\|:=\\mathbb{E}\\left\\|\\bar{\\nabla}_{\\bar{Q},t}-\\nabla f({\\bf x}_{t})\\right\\|}\\\\ &{\\qquad=\\mathbb{E}\\left\\|\\frac{1}{\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}}\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}\\nabla f({\\bf x}_{t}^{(i)})-\\nabla f({\\bf x}_{t})\\right\\|}\\\\ &{\\qquad\\leq\\mathbb{E}\\left[\\frac{1}{\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}}\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}\\left\\|\\nabla f({\\bf x}_{t}^{(i)})-\\nabla f({\\bf x}_{t})\\right\\|\\right]}\\\\ &{\\qquad\\leq\\frac{L}{\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}\\left\\|{\\bf x}_{t}^{(i)}-{\\bf x}_{t}\\right\\|\\right]}\\\\ &{\\qquad\\leq\\frac{4D L}{\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}\\frac{\\nabla_{t}^{(i)}}{t}\\right]}\\\\ &{\\qquad\\leq\\frac{4D L}{\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}}\\mathbb{E}\\left[\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}\\frac{\\nabla_{t}^{(i)}}{t}\\right]}\\\\ &{\\qquad\\leq\\frac{4\\pi^{2}m L}{\\sum_{i\\in\\mathcal{C}}s_{i}^{(i)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, the first inequality leverages Jensen's inequality, and the second follows the smoothness assumption in Eq. (3). The third follows Lemma B.1, and the last inequality follows that $\\tau_{t}^{m a x}:=$ $\\operatorname*{max}_{i\\in[m]}\\{\\tau_{t}^{(i)}\\}$ ", "page_idx": 16}, {"type": "text", "text": "For the second bias $B_{t}^{2}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|{\\cal B}_{t}^{2}\\right\\|:=\\mathbb{E}\\left\\|\\nabla f({\\bf x}_{t}^{(t)})-\\bar{\\nabla}\\bar{\\varrho}_{t,\\epsilon}\\right\\|}\\\\ &{\\qquad=\\mathbb{E}\\left\\|\\nabla f({\\bf x}_{t}^{(t)})-\\frac{1}{\\sum_{j\\in\\mathcal{G}}s_{t}^{(t)}}\\sum_{\\ell\\in\\mathcal{G}}s_{f}^{(t)}\\nabla f({\\bf x}_{t}^{(j)})\\right\\|}\\\\ &{\\qquad\\le\\frac{1}{\\sum_{j\\in\\mathcal{G}}s_{t}^{(t)}}\\mathbb{E}\\left[\\sum_{\\ell\\in\\mathcal{G}}s_{t}^{(t)}\\left\\|\\nabla f({\\bf x}_{t}^{(t)})-\\nabla f({\\bf x}_{t}^{(t)})\\right\\|\\right]}\\\\ &{\\qquad\\le\\frac{L}{\\sum_{j\\in\\mathcal{G}}s_{t}^{(t)}}\\left\\|\\sum_{\\ell\\in\\mathcal{G}}s_{t}^{(t)}\\left\\|{\\bf x}_{t}^{(t)}-{\\bf x}_{t}^{(t)}\\right\\|\\right]}\\\\ &{\\qquad\\le\\frac{4D L}{\\sum_{j\\in\\mathcal{G}}s_{t}^{(t)}}\\left\\|\\sum_{\\ell\\in\\mathcal{G}}s_{t}^{(t)}\\left(\\frac{\\tau_{\\ell}^{(t)}+\\tau_{\\ell}^{(t)}}{t}\\right)\\right\\|}\\\\ &{\\qquad\\le\\frac{8\\tau_{\\ell}^{\\prime\\prime\\prime}m L}{\\sum_{j\\in\\mathcal{G}}s_{t}^{(t)}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Like before, the first inequality leverages Jensen's inequality, and the second follows the smoothness assumption in Eq. (3), the third inequality follows Lemma B.3, and the last one follows that $\\tau_{t}^{m a x}:=\\operatorname*{max}_{i\\in[m]}\\{\\tau_{t}^{(i)}\\}$ ", "page_idx": 17}, {"type": "text", "text": "Variance Bound. We start by determining $\\rho_{i}$ as outlined in Definition 3.1: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\mathbf{d}_{t}^{(i)}-\\bar{\\mathbf{d}}_{\\mathcal{G},t}\\|^{2}\\leq3\\mathbb{E}\\|\\mathbf{d}_{t}^{(i)}-\\nabla f(\\mathbf{x}_{t}^{(i)})\\|^{2}+3\\mathbb{E}\\|\\bar{\\nabla}_{\\mathcal{G},t}-\\bar{\\mathbf{d}}_{\\mathcal{G},t}\\|^{2}+3\\mathbb{E}\\left\\|\\mathcal{B}_{t}^{2}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=3\\mathbb{E}\\left\\|\\varepsilon_{t}^{(i)}\\right\\|^{2}+3\\mathbb{E}\\left\\|\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}}\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\varepsilon_{t}^{(i)}\\right\\|^{2}+3\\mathbb{E}\\left\\|\\mathcal{B}_{t}^{2}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\frac{3\\tilde{\\sigma}^{2}}{s_{t}^{(i)}}+\\frac{3\\tilde{\\sigma}^{2}}{t_{\\mathcal{G}}}+\\frac{192\\left(\\tau_{t}^{m a x}D L\\right)^{2}}{t^{2}}}\\\\ &{\\qquad\\qquad\\leq\\frac{6\\tilde{\\sigma}^{2}}{s_{t}^{(i)}}+\\frac{192\\left(\\tau_{t}^{m a x}D L\\right)^{2}}{t^{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{d}}_{\\mathcal{G},t}\\,:=\\,\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}}\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\mathbf{d}_{t}^{(i)}}\\end{array}$ The frst and inequality uses $\\|\\mathbf{a}+\\mathbf{b}+\\mathbf{c}\\|^{2}\\,\\leq\\,3\\|\\mathbf{a}\\|^{2}\\,+$ $3\\|\\mathbf{b}\\|^{2}+3\\|\\mathbf{c}\\|^{2}$ , which holds $\\forall\\mathbf{a},\\mathbf{b},\\mathbf{c}\\in\\mathbb{R}^{d}$ . The second inequality follows Theorem 4.1 and employs the second bias bound in Eq. (15). The third uses the fact that $s_{t}^{(i)}\\leq t_{\\mathcal{G}},\\forall i\\in\\mathcal{G}$ .Accordingly, we set $\\begin{array}{r}{\\rho_{i}^{2}:=\\frac{6\\tilde{\\sigma}^{2}}{s_{t}^{(i)}}+\\frac{192(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Following this, we derive $\\rho$ as outlined in Definition 3.1: ", "page_idx": 17}, {"type": "equation", "text": "$$\n)^{2}=\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}}\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\rho_{i}^{2}=\\frac{1}{t_{\\mathcal{G}}}\\sum_{i\\in\\mathcal{G}}s_{t}^{(i)}\\left(\\frac{6\\tilde{\\sigma}^{2}}{s_{t}^{(i)}}+\\frac{192(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}\\right)=\\frac{6m\\tilde{\\sigma}^{2}}{t_{\\mathcal{G}}}+\\frac{192(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we establish an upper bound for $\\mathbb{E}\\|\\mathcal{E}_{t}\\|^{2}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\bar{\\varepsilon}_{t}\\|^{2}=\\mathbb{E}\\left\\|\\hat{\\mathbf{d}}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\|^{2}}\\\\ &{\\qquad\\le2\\mathbb{E}\\left\\|\\hat{\\mathbf{d}}_{t}-\\hat{\\mathbf{d}}_{\\bar{t},t}\\right\\|^{2}+2\\mathbb{E}\\left\\|\\hat{\\mathbf{d}}_{\\bar{t},t}-\\nabla f(\\mathbf{x}_{t})\\right\\|^{2}}\\\\ &{\\qquad\\le2c_{\\mathrm{x}}\\left(\\frac{6m\\bar{\\sigma}^{2}}{t_{Q}}+\\frac{192(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}\\right)+4\\mathbb{E}\\left\\|\\hat{\\mathbf{d}}_{\\bar{t},t}-\\bar{\\nabla}_{\\bar{t},t}\\right\\|^{2}+4\\mathbb{E}\\left\\|B_{t}^{1}\\right\\|^{2}}\\\\ &{\\qquad=2c_{\\mathrm{x}}\\left(\\frac{6m\\bar{\\sigma}^{2}}{t_{Q}}+\\frac{192(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}\\right)+4\\mathbb{E}\\left\\|\\frac{1}{\\sum_{i\\in\\mathcal{C}}s_{t}^{(i)}}\\sum_{i\\in\\mathcal{C}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!s_{t}^{(i)}\\frac{\\rho!}{t_{\\ell}!}\\bar{\\varepsilon}_{t}^{(i)}\\right\\|^{2}+4\\mathbb{E}\\left\\|B_{t}^{1}\\right\\|^{2}}\\\\ &{\\qquad\\le\\frac{12c_{\\mathrm{x}}m\\bar{\\sigma}^{2}}{t_{Q}}+\\frac{4\\bar{\\sigma}^{2}}{t_{Q}}+\\frac{(\\tau_{t}^{m a x}D L)^{2}(384c_{\\mathrm{x}}+64)}{t^{2}}}\\\\ &{\\qquad\\le\\frac{8\\bar{\\sigma}^{2}}{t}+\\frac{24c_{\\mathrm{x}}m\\bar{\\sigma}^{2}}{t}+\\frac{64(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}+\\frac{384c_{\\mathrm{x}}(\\tau_{t}^{m a x}D L)^{2}}{t^{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality uses $\\|\\mathbf{a}+\\mathbf{b}\\|^{2}\\leq2\\|\\mathbf{a}\\|^{2}+2\\|\\mathbf{b}\\|^{2}$ , which holds Va $\\mathbf{\\theta},\\mathbf{b}\\in\\mathbb{R}^{d}$ . The second inequality utilizes the same inequality and is further supported by Definition 3.1 and Equation (16). The third aligns with Theorem 4.1, and employs the first bias bound in Eq. (14). The last one utilizes the fact that $\\dot{t}\\varsigma\\geq(1-\\lambda)t\\geq t/2$ , given $\\lambda<1/2$ \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3Proof of Thm. 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Thm. 4.2. Following Lemma 4.1 and applying Jensen's inequality, we derive the following bound: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert\\mathcal{E}_{t}\\Vert=\\mathbb{E}\\sqrt{\\Vert\\mathcal{E}_{t}\\Vert^{2}}\\le\\sqrt{\\mathbb{E}\\Vert\\mathcal{E}_{t}\\Vert^{2}}\\le O\\left(\\frac{\\tilde{\\sigma}\\sqrt{1+m c_{\\lambda}}}{\\sqrt{t}}+\\frac{\\tau_{t}^{m a x}D L\\sqrt{1+c_{\\lambda}}}{t}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the third inequality uses ${\\sqrt{a+b}}\\,\\leq\\,{\\sqrt{a}}+{\\sqrt{b}}$ for non-negative $a,b\\in\\mathbb{R}$ . The explanation behind this can be seen through the following steps: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left({\\sqrt{a}}+{\\sqrt{b}}\\right)^{2}=a+2{\\sqrt{a b}}+b\\geq a+b\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "whereby taking the square root of both sides of this equation, we obtain the desired inequality. ", "page_idx": 18}, {"type": "text", "text": "Next, let's revisit the AnyTime guarantee as outlined in Cutkosky [2019] and proceed to delve into the regret analysis of the update rule. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.1 (Rephrased from Theorem 1 in Cutkosky [2019]). Let $f:K\\rightarrow\\mathbb{R}$ be a convexfunction withaminimum $\\mathbf{x}^{*}\\in\\arg\\operatorname*{min}_{\\mathbf{w}\\in K}f(\\mathbf{w})$ Also let $\\{\\alpha_{t}\\geq0\\}_{t}$ , and $\\{\\mathbf{w}_{t}\\in K\\}_{t}$ $\\{\\mathbf{x}_{t}\\in\\boldsymbol{K}\\}_{t}$ such that $\\left\\{{\\bf x}_{t}\\right\\}_{t}$ is an $\\left\\{\\alpha_{t}\\right\\}_{t}$ weighted averaged of $\\{\\mathbf{w}_{t}\\}_{t}$ i.e. such that $\\mathbf{x}_{1}=\\mathbf{w}_{1}$ , and for any $t\\geq1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t+1}=\\frac{1}{\\alpha_{1:t+1}}\\sum_{\\tau\\in[t+1]}\\alpha_{\\tau}\\mathbf{w}_{\\tau}\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then the following holds for any $t\\geq1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{1:t}(f(\\mathbf{x}_{t})-f(\\mathbf{x}^{*}))\\leq\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\nabla f(\\mathbf{x}_{\\tau})(\\mathbf{w}_{\\tau}-\\mathbf{x}^{*})\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma B.4. Let $f:K\\to\\mathbb{R}$ be a convex function with a minimum $\\begin{array}{r}{\\mathbf{x}^{*}\\in\\arg\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{K}}f(\\mathbf{w})}\\end{array}$ and assume that the assumption in Eq. (1) holds. Also let $\\{\\alpha_{t}\\geq0\\}_{t}$ and $\\{\\mathbf{w}_{t}\\in\\mathcal{K}\\}_{t}$ . Then, for any $t\\geq1$ , an arbitrary vector $\\hat{\\mathbf{d}}_{t}\\in\\mathbb{R}^{d}$ , and the update rule: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\Pi_{\\mathcal{K}}\\left(\\mathbf{w}_{t}-\\eta\\alpha_{t}\\hat{\\mathbf{d}}_{t}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t}\\alpha_{\\tau}\\langle\\hat{\\mathbf{d}}_{\\tau},\\mathbf{w}_{\\tau+1}-\\mathbf{x}^{*}\\rangle\\leq\\frac{D^{2}}{2\\eta}-\\frac{1}{2\\eta}\\sum_{\\tau=1}^{t}\\left\\lVert\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\right\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma B.5. let $f:K\\rightarrow\\mathbb{R}$ be an $L$ -smooth and convex function, and let $\\begin{array}{r}{\\mathbf{x}^{*}\\in\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{K}}f(\\mathbf{x}),}\\end{array}$ then for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ we have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{x}^{*})\\|^{2}\\leq2L(f(\\mathbf{x})-f(\\mathbf{x}^{*}))\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, for every iteration $t\\leq T$ , we define: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{d}}_{t}:=\\mathcal{A}_{\\omega}(\\{\\mathbf{d}_{t}^{(i)},s_{t}^{(i)}\\}_{i=1}^{m})}\\\\ {\\mathcal{E}_{t}:=\\hat{\\mathbf{d}}_{t}-\\nabla f(\\mathbf{x}_{t})\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, combining Theorem B.1 with Lemma B.4, we have that, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(f(\\mathbf{x}_{t})-f(\\mathbf{x}^{*}))\\leq\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\nabla f(\\mathbf{x}_{\\tau}),\\mathbf{w}_{\\tau}-\\mathbf{x}^{*}\\rangle}}\\\\ &{=\\displaystyle\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\hat{\\mathbf{d}}_{\\tau},\\mathbf{w}_{\\tau+1}-\\mathbf{x}^{*}\\rangle+\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\hat{\\mathbf{d}}_{\\tau},\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\rangle-\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\hat{\\varepsilon}_{\\tau},\\mathbf{w}_{\\tau}-\\mathbf{x}^{*}\\rangle}\\\\ &{\\leq\\displaystyle\\frac{D^{2}}{2\\eta}-\\frac{1}{2\\eta}\\sum_{\\tau\\in[t]}\\|\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\|^{2}+\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\hat{\\mathbf{d}}_{\\tau},\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\rangle-\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\hat{\\varepsilon}_{\\tau},\\mathbf{w}_{\\tau}-\\hat{\\mathbf{d}}_{\\tau}\\rangle}\\\\ &{=\\displaystyle\\frac{D^{2}}{2\\eta}-\\frac{1}{2\\eta}\\sum_{\\tau\\in[t]}\\|\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\|^{2}+\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\nabla f(\\mathbf{x}_{\\tau}),\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\rangle-\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\hat{\\varepsilon}_{\\tau},\\mathbf{w}_{\\tau}-\\hat{\\mathbf{d}}_{\\tau}\\rangle}\\\\ &{\\leq\\displaystyle\\frac{D^{2}}{2\\eta}-\\frac{1}{2\\eta}\\sum_{\\tau\\in[t]}\\|\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\|^{2}+\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\nabla f(\\mathbf{x}_{\\tau}),\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\rangle+D\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\|\\hat{\\varepsilon}_{\\tau}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality is derived from the Anytime guarantee, as outlined in Theorem B.1. The second inequality follows Lemma B.4. The third inequality is a result of applying the Cauchy-Schwarz inequality and the assumption in Eq. (1). ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(A):=-\\displaystyle\\frac{1}{2\\eta}\\displaystyle\\sum_{e=\\{|\\eta|}}\\|\\mathbf{w}_{e}-\\mathbf{w}_{r+1}\\|^{2}+\\displaystyle\\sum_{r=1\\}^{\\infty}\\alpha_{r}\\langle\\nabla f(\\mathbf{x},\\mathbf{w}_{r}-\\mathbf{w}_{r+1})}\\\\ &{~~=-\\displaystyle\\frac{1}{2\\eta}\\displaystyle\\sum_{e=\\{|\\eta|}}\\|\\mathbf{w}_{r}-\\mathbf{w}_{r+1}\\|^{2}+\\displaystyle\\sum_{r=1}^{\\infty}\\alpha_{r}\\langle\\nabla f(\\mathbf{x}_{r})-\\nabla f(\\mathbf{x}^{\\prime}),\\mathbf{w}_{r}-\\mathbf{w}_{r+1}\\rangle+\\displaystyle\\sum_{r\\in[\\eta]}\\alpha_{r}\\langle\\nabla f(\\mathbf{x}^{\\prime})\\mathbf{w}_{r-1}\\|^{2}}\\\\ &{~~\\le\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{r\\in\\{|\\eta|}}\\alpha_{r}^{2}\\langle\\nabla f(\\mathbf{x}_{r})-\\nabla f(\\mathbf{x}^{\\prime})\\|^{2}+\\displaystyle\\sum_{r\\in\\{|\\eta|}}\\alpha_{r}\\langle\\nabla f(\\mathbf{x}^{\\prime}),\\mathbf{w}_{r}-\\mathbf{w}_{r+1}\\rangle}\\\\ &{~~\\le2\\eta\\displaystyle\\sum_{r\\in\\{|\\eta|}}\\alpha_{r+1}\\gamma_{r}\\langle\\mathbf{x}_{r}\\rangle+\\displaystyle\\sum_{r\\in[\\eta]}\\alpha_{r}\\langle\\nabla f(\\mathbf{x}^{\\prime}),\\mathbf{w}_{r}\\rangle-\\alpha_{r}\\langle\\nabla f(\\mathbf{x}^{\\prime}),\\mathbf{w}_{+1}\\rangle}\\\\ &{~~=2\\eta L\\displaystyle\\sum_{r\\in\\{|\\eta|}}\\alpha_{1}\\gamma_{r}\\alpha_{r}+\\displaystyle\\sum_{r\\in[\\eta]}(\\alpha_{r}-\\alpha_{r-1})\\langle\\nabla f(\\mathbf{x}^{\\prime}),\\mathbf{w}_{r}-\\mathbf{w}_{r+1}\\rangle}\\\\ &{~~\\le2\\eta L\\displaystyle\\sum_{r\\in\\{|\\eta|}}\\alpha_{1}\\gamma_{r}\\alpha_{r}+\\displaystyle\\sum_{r\\in[\\eta]}(\\alpha_{r}-\\alpha_{r-1})\\|\\nabla f(\\mathbf{x}^{\\prime})\\|\\mathbf{w}_{r}-\\mathbf{w}_{r+1}\\|}\\\\ &{~~\\le2\\eta L\\displaystyle\\sum_{r\\in\\{|\\eta|}}\\alpha_{1}\\gamma_{r}\\langle\\mathbf{\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, the first inequality employs the Young's inequality. For the second inequality, we introduce the notation $\\Delta_{t}:=f(\\mathbf{x}_{t})-f(\\mathbf{x}^{*})$ , and we follow Lemma B.5, which relates to the smoothness of the function $f$ . In this step, we also set $\\alpha_{0}=0$ and utilizes the property $\\alpha_{\\tau}^{2}\\leq2\\alpha_{1:\\tau}$ , given that $\\alpha_{\\tau}=\\tau$ The third inequality uses the Cauchy-Schwarz inequality. The last inequality follows the assumption in Eq. (1). It uses the fact that $t\\leq T$ and $\\Delta_{t}\\geq0,\\forall t$ . This step also incorporates the choice of an appropriate learning rate parameter $\\eta\\leq1/4L T$ ", "page_idx": 19}, {"type": "text", "text": "Plugging (A) into Eq. (18), gives us, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\alpha_{1:t}\\Delta_{t}\\leq{\\frac{1}{2T}}\\sum_{\\tau\\in[T]}\\alpha_{1:\\tau}\\Delta_{\\tau}+{\\frac{D^{2}}{2\\eta}}+\\alpha_{t}G^{*}D+D\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\|{\\mathcal{E}}_{\\tau}\\|\\ .\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma B.6 (Lemma C.2 in Levy [2023]). let $\\{A_{t}\\}_{t\\in[T]},\\{B_{t}\\}_{t\\in[T]}$ be sequences of non-negative elements, and assume that for any $t\\leq T$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{t}\\leq B_{T}+\\frac{1}{2T}\\sum_{t\\in[T]}A_{t}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then the following bound holds, ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{T}\\leq2B_{T}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$A_{t}:=\\alpha_{1:t}\\mathbb{E}\\left[f(\\mathbf{x}_{t})-f(\\mathbf{x}^{*})\\right]$ and $\\begin{array}{r}{B_{t}:=\\frac{D^{2}}{2\\eta}+\\alpha_{t}G^{*}D+}\\end{array}$ $D\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\mathbb{E}\\|\\mathcal{E}_{\\tau}\\|$ . Note that the series $\\{B_{t}\\}_{t}$ forms a non-decreasing series of non-negative values, implying $B_{t}\\leq B_{T}$ for any $t\\in[T]$ . As a result of Eq. (19), we have that $\\begin{array}{r}{A_{t}\\leq B_{T}+\\frac{1}{2T}\\sum_{\\tau\\in[T]}A_{\\tau}}\\end{array}$ Leveraging Lemma B.6, Eq. (17), and acknowledging that $\\alpha_{1:T}=\\Theta(T^{2})$ , as $\\alpha_{t}=t$ , it follows that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(\\mathbf{x}_{T})-f(\\mathbf{x}^{*})]\\leq\\displaystyle\\frac{2}{T^{2}}B_{T}}\\\\ &{\\phantom{\\sum}=\\frac{D^{2}}{T^{2}\\eta}+\\frac{2G^{*}D}{T}+\\frac{2D}{T^{2}}\\sum_{t\\in[T]}\\alpha_{t}\\mathbb{E}\\|\\mathcal{E}\\|}\\\\ &{\\phantom{\\sum}\\leq\\mathcal{O}\\left(\\frac{D^{2}}{T^{2}\\eta}+\\frac{G^{*}D}{T}+\\frac{D}{T^{2}}\\sum_{t\\in[T]}\\left(\\sqrt{t}\\tilde{\\sigma}\\sqrt{1+m c_{\\lambda}}+\\tau_{t}^{m a x}D L\\sqrt{1+c_{\\lambda}}\\right)\\right)}\\\\ &{\\phantom{\\sum}\\leq\\mathcal{O}\\left(\\frac{D^{2}}{T^{2}\\eta}+\\frac{G^{*}D}{T}+\\frac{D\\tilde{\\sigma}\\sqrt{1+m c_{\\lambda}}}{\\sqrt{T}}+\\frac{\\mu^{m a x}D^{2}L\\sqrt{1+c_{\\lambda}}}{T}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\mu^{m a x}:=\\frac{1}{T}\\sum_{t\\in[T]}\\tau_{t}^{m a x}}\\end{array}$ Finally, choosing the optmal $\\begin{array}{r}{\\eta\\leq\\frac{1}{4T L}}\\end{array}$ gives us: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\mathbf{x}_{T})-f(\\mathbf{x}^{*})]\\leq O\\left(\\frac{L D^{2}\\mu^{m a x}\\sqrt{1+c_{\\lambda}}}{T}+\\frac{G^{*}D}{T}+\\frac{D\\tilde{\\sigma}\\sqrt{1+m c_{\\lambda}}}{\\sqrt{T}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.3.1 Proof of Lemma B.4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Lemma B.4. The update rule $\\mathbf{w}_{\\tau+1}=\\Pi_{K}(\\mathbf{w}_{\\tau}-\\eta\\alpha_{\\tau}\\hat{\\mathbf{d}}_{\\tau})$ can be expressed as a convex optimization problem within the set $\\kappa$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{\\tau+1}=\\Pi_{K}\\left(\\mathbf{w}_{\\tau}-\\eta\\alpha_{\\tau}\\hat{\\mathbf{d}}_{\\tau}\\right)}\\\\ &{\\quad\\quad=\\arg\\underset{\\mathbf{w}\\in\\mathcal{K}}{\\operatorname*{min}}\\left\\|\\mathbf{w}_{\\tau}-\\eta\\alpha_{\\tau}\\hat{\\mathbf{d}}_{\\tau}-\\mathbf{w}\\right\\|^{2}}\\\\ &{\\quad\\quad=\\arg\\underset{\\mathbf{w}\\in\\mathcal{K}}{\\operatorname*{min}}\\big\\{\\alpha_{\\tau}\\langle\\hat{\\mathbf{d}}_{\\tau},\\mathbf{w}-\\mathbf{w}_{\\tau}\\rangle+\\frac{1}{2\\eta}\\|\\mathbf{w}-\\mathbf{w}_{\\tau}\\|^{2}\\big\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, the first equality is derived from the definition of the update rule, the second stems from the property of projection, and the final equality is obtained by reformulating the optimization problem in a way that does not affect the minimum value. ", "page_idx": 20}, {"type": "text", "text": "Given that ${\\bf w}_{\\tau+1}$ is the optimal solution of the above convex problem, by the optimality conditions, we have that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle\\alpha_{\\tau}\\hat{\\mathbf{d}}_{\\tau}+\\frac{1}{\\eta}(\\mathbf{w}_{\\tau+1}-\\mathbf{w}_{\\tau}),\\mathbf{w}-\\mathbf{w}_{\\tau+1}\\right\\rangle\\geq0,\\quad\\forall\\mathbf{w}\\in\\mathcal{K}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Rearranging this, summing over $t\\geq1$ iterations, and taking $\\mathbf{w}=\\mathbf{x}^{*}$ , we derive: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\tau\\in[t]}\\alpha_{\\tau}\\langle\\hat{\\mathbf{d}}_{\\tau},\\mathbf{w}_{\\tau+1}-\\mathbf{x}^{*}\\rangle\\leq\\frac{1}{\\eta}\\displaystyle\\sum_{\\tau\\in[t]}\\langle\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1},\\mathbf{w}_{\\tau+1}-\\mathbf{x}^{*}\\rangle}\\\\ &{\\quad=\\displaystyle\\frac{1}{2\\eta}\\displaystyle\\sum_{\\tau\\in[t]}\\left(\\|\\mathbf{w}_{\\tau}-\\mathbf{x}^{*}\\|^{2}-\\|\\mathbf{w}_{\\tau+1}-\\mathbf{x}^{*}\\|^{2}-\\|\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\|^{2}\\right)}\\\\ &{\\quad=\\displaystyle\\frac{1}{2\\eta}\\left(\\|\\mathbf{w}_{1}-\\mathbf{x}^{*}\\|^{2}-\\|\\mathbf{w}_{t+1}-\\mathbf{x}^{*}\\|^{2}-\\displaystyle\\sum_{\\tau\\in[t]}\\|\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\|^{2}\\right)}\\\\ &{\\quad\\leq\\displaystyle\\frac{D^{2}}{2\\eta}-\\frac{1}{2\\eta}\\displaystyle\\sum_{\\tau\\in[t]}\\|\\mathbf{w}_{\\tau}-\\mathbf{w}_{\\tau+1}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first equality equality is achieved through algebraic manipulation, and the last inequality follows the assumption in Eq. (1). \u53e3 ", "page_idx": 21}, {"type": "text", "text": "B.3.2 Proof of Lemma B.5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Lemma B.5. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.7 (Lemma C.1 in Levy [2023]). let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ bean $L$ -smooth function with a global minimum $\\mathbf{x}^{*}$ ,then for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ we have, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lVert\\nabla f(\\mathbf{x})\\rVert^{2}\\leq2L(f(\\mathbf{x})-f(\\mathbf{x}^{*}))\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let us define the function $h(\\mathbf{x})=f(\\mathbf{x})-f(\\mathbf{x}^{*})-\\left\\langle\\nabla f(\\mathbf{x}^{*}),\\mathbf{x}-\\mathbf{x}^{*}\\right\\rangle$ . Due to the convexity of $f(\\mathbf{x})$ we have the gradient inequality $f(\\mathbf{x})-f(\\mathbf{x}^{*})\\geq\\langle\\nabla f(\\mathbf{x}^{*}),\\mathbf{x}-\\mathbf{x}^{*}\\rangle$ , which implies $h(\\mathbf{x})\\geq0$ .As $h(\\mathbf{x}^{*})=0$ , this implies that $\\mathbf{x}^{*}$ is the global minimum of $h$ . Applying Lemma B.7, gives us, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{x}^{*})\\|^{2}=\\|\\nabla h(\\mathbf{x})\\|^{2}\\leq2L(f(\\mathbf{x})-f(\\mathbf{x}^{*})-\\langle\\nabla f(\\mathbf{x}^{*}),\\mathbf{x}-\\mathbf{x}^{*}\\rangle)\\leq2L(f(\\mathbf{x})-f(\\mathbf{x}^{*}))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where last inequality holds due to the convexity of $f$ , which implies that $\\langle\\nabla f(\\mathbf{x}^{*}),\\mathbf{x}-\\mathbf{x}^{*}\\rangle\\geq0$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C Robust Aggregators Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Weighted Robust Aggregators ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1.1 Weighted Geometric Median (WeightedGM) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The Weighted Geometric Median (WeightedGM) is an aggregation method that seeks a point minimizing the weighted sum of Euclidean distances to a set of points. Formally, for a given set of points $\\{\\mathbf{x}_{i}\\}_{i=1}^{m}$ and corresponding weights $\\{s_{i}\\}_{i=1}^{m}$ , the WeightedGM aggregator is defined as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{{WeightedGM}\\in\\underset{\\mathbf{y}\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}}\\sum_{i\\in[m]}s_{i}\\|\\mathbf{y}-\\mathbf{x}_{i}\\|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "LemmaC.1.Let $\\hat{\\bf x}$ beaWeightedGMaggregatorthen $\\hat{\\bf x}$ is $(c_{\\lambda},\\lambda)$ -weightedrobustwith $c_{\\lambda}=$ $\\begin{array}{r}{\\left(1+\\frac{\\lambda}{1-2\\lambda}\\right)^{2}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\frac{1}{\\kappa}}-\\kappa_{c}\\mathbf{||}={\\Bigg|}\\kappa-{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in\\mathcal{K}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}}}\\right|}\\\\ &{\\leq{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}}{\\Bigg|}\\kappa-\\mathbf{||}}\\\\ &{={\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in\\mathcal{N}}{\\sum_{i\\in\\mathcal{N}}\\underset{i\\in\\mathcal{N}}{\\sum_{i\\in\\mathcal{N}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}}}-{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}}|\\kappa-\\mathbf{x}_{i}|}\\\\ &{\\leq{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in G^{\\lambda}}\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}}|\\kappa-\\mathbf{x}_{i}|-{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in G^{\\lambda}}\\leq i\\in\\mathcal{N}}}}|\\kappa-\\mathbf{x}_{i}||}\\\\ &{={\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}}|\\kappa_{i}\\mathbf{||}\\leq{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}|{\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}-{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\underset{i\\in G^{\\lambda}}{\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\mathbf{x}_{i}\\right|}}}|\\kappa-\\mathbf{x}_{i}|-}\\\\ &{\\leq{\\frac{1}{\\sum_{i\\in G^{\\lambda}}\\left|\\mathcal{K}-\\\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first inequality leverages Jensen's inequality. The second inequality follows the WeightedGM definition. The third is derived using the following triangle inequality: $\\|\\bar{\\mathbf{x}}_{\\mathcal{G}}\\,-\\,\\mathbf{x}_{i}\\|\\;\\leq\\;\\|\\bar{\\mathbf{x}}_{\\mathcal{G}}\\;-$ $\\hat{\\mathbf{x}}\\|+\\|\\hat{\\mathbf{x}}\\mathrm{~-~}\\mathbf{x}_{i}\\|$ The final inequality is based on the assumptions that $\\textstyle\\sum_{i\\in B}s_{i}\\ \\leq\\ \\lambda s_{1:m}$ and $\\begin{array}{r}{\\sum_{i\\in\\mathcal{G}}s_{i}\\geq(1-\\lambda)s_{1:m}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "By rearranging, we obtain: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\hat{\\mathbf{x}}-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\|\\leq\\left(1+\\frac{\\lambda}{1-2\\lambda}\\right)\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\left\\|\\bar{\\mathbf{x}}_{\\mathcal{G}}-\\mathbf{x}_{i}\\right\\|\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking the square of both sides and applying Jensen's inequality gives us: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\big\\|\\hat{\\mathbf{x}}-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\big\\|^{2}\\leq\\bigg(1+\\frac{\\lambda}{1-2\\lambda}\\bigg)^{2}\\,\\frac{1}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\,\\big\\|\\bar{\\mathbf{x}}_{\\mathcal{G}}-\\mathbf{x}_{i}\\big\\|^{2}\\,\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking the exception of both sides gives us the following: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\hat{\\mathbf{x}}-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\|^{2}\\leq\\left(1+\\frac{\\lambda}{1-2\\lambda}\\right)^{2}\\rho^{2}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.1.2 Weighted Coordinate-Wise Median (WeightedCWMed) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The Weighted Coordinate-Wise Median (WeightedCWMed) is an aggregation technique that operates on a per-cordinatbasisForagiven set ofmulti-dmensionaldata points $\\{{\\bf x}_{i}\\}_{i=1}^{m}$ and corresponding weights $\\{s_{i}\\}_{i=1}^{m}$ , the WeightedCWMed is computed by independently finding the weighted median of each coordinate across all points. Formally, for the $k^{\\mathrm{th}}$ dimension, the WeightedCWMed aggregator is defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n[\\mathrm{WeightedCWMed}]_{k}:=\\mathrm{WeightedMedian}(\\{[\\mathbf{x}_{i}]_{k}\\}_{i=1}^{m};\\{[s_{i}]_{k}\\}_{i=1}^{m})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $[\\mathbf{x}]_{k}$ is the $k^{\\mathrm{th}}$ element of a vector $\\mathbf{x}$ and the WeightedMedian is defined as follows: given the elements $\\left\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{m}\\right\\}$ of each dimension are sorted in ascending order and their corresponds weights $\\left\\{s_{1},\\ldots,s_{m}\\right\\}$ , the weighted median is the element $\\mathbf{x}_{j}*$ , where $j^{*}$ is determined by the condition: ", "page_idx": 22}, {"type": "equation", "text": "$$\nj^{*}\\in\\arg\\operatorname*{min}_{j\\in[m]}\\left\\{\\sum_{i\\in[j]}s_{i}>\\frac{1}{2}\\sum_{i\\in[m]}s_{i}\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If there exists a value $j$ such that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i\\in[j]}s_{i}=\\frac{1}{2}\\sum_{i\\in[m]}s_{i}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, the WeightedMedian is the average of the $j$ -th and $(j+1)$ -th elements: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\mathrm{WeightedMedian}}={\\frac{\\mathbf{x}_{j}+\\mathbf{x}_{j+1}}{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, we extend the theoretical guarantee of the Coordinate-Wise Median (CWMed) to its weighted version, following the procedure in Allouah et al. [2023]. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.2. Let $A_{\\omega}:\\mathbb{R}^{d\\times m}\\rightarrow\\mathbb{R}^{d}$ be a weighted coordinate-wise aggregation function. Given set of points $\\{{\\bf x}_{i}\\}_{i=1}^{m}$ and corresponding weights $\\{s_{i}\\}_{i=1}^{m}$ this function incorporates d real-valued functions $A_{\\omega}^{1},\\ldots,A_{\\omega}^{d}$ where each $[A_{\\omega}(\\{\\mathbf{x}_{i}\\}_{i=1}^{m};\\{s_{i}\\}_{i=1}^{m^{-}})]_{k}=A_{\\omega}^{k}(\\{[\\mathbf{x}_{i}]_{k}\\}_{i=1}^{m};\\{[s_{i}]_{k}\\}_{i=1}^{m})$ .If for each $k\\in[d]$ $A_{\\omega}^{k}$ is $(c_{\\lambda},\\lambda)$ -weighted robust that satisfies: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left|A_{\\omega}^{k}(\\{[\\mathbf{x}_{i}]_{k}\\}_{i=1}^{m};\\{[s_{i}]_{k}\\}_{i=1}^{m})-[\\bar{\\mathbf{x}}_{\\mathcal{G}}]_{k}\\right|^{2}\\leq\\frac{c_{\\lambda}}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbb{E}\\left|[\\mathbf{x}_{i}]_{k}-[\\bar{\\mathbf{x}}_{\\mathcal{G}}]_{k}\\right|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then $A_{\\omega}$ is $(c_{\\lambda},\\lambda)$ -weighted robust. ", "page_idx": 23}, {"type": "text", "text": "Proof.Since $A_{\\omega}$ is a coordinate-wise aggregator, it applies the same aggregation rule across each dimension. Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|A_{\\omega}(\\{\\mathbf{x}_{i}\\}_{i=1}^{m};\\{s_{i}\\}_{i=1}^{m})-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\right\\|^{2}=\\sum_{k\\in[d]}\\left|A_{\\omega}^{k}(\\{[\\mathbf{x}_{i}]_{k}\\}_{i=1}^{m};\\{[s_{i}]_{k}\\}_{i=1}^{m})-[\\bar{\\mathbf{x}}_{\\mathcal{G}}]_{k}\\right|^{2}~.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Given that each $A_{\\omega}^{k}$ , for $k\\in[d]$ ,is $(c_{\\lambda},\\lambda)$ weighted robust that satisfies: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left|A_{\\omega}^{k}(\\{[\\mathbf{x}_{i}]_{k}\\}_{i=1}^{m};\\{[s_{i}]_{k}\\}_{i=1}^{m})-[\\bar{\\mathbf{x}}_{\\mathcal{G}}]_{k}\\right|^{2}\\leq\\frac{c_{\\lambda}}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbb{E}\\left|[\\mathbf{x}_{i}]_{k}-[\\bar{\\mathbf{x}}_{\\mathcal{G}}]_{k}\\right|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can express the overall aggregation function $A_{\\omega}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k\\in[d]}\\mathbb{E}\\left|A_{\\omega}^{k}(\\{\\mathbf{x}_{i}\\}_{k})_{i=1}^{m};\\{[s_{i}]_{k}\\}_{i=1}^{m}\\right)-\\left[\\bar{\\mathbf{x}}_{\\mathcal{G}}\\right]_{k}\\right|^{2}\\leq\\displaystyle\\sum_{k\\in[d]}\\frac{c_{\\lambda}}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbb{E}\\left|\\big[\\mathbf{x}_{i}\\big]_{k}-\\big[\\bar{\\mathbf{x}}_{\\mathcal{G}}\\big]_{k}\\right|^{2}}\\\\ {=\\displaystyle\\frac{c_{\\lambda}}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbb{E}\\sum_{k\\in[d]}\\left|[\\mathbf{x}_{i}]_{k}-\\big[\\bar{\\mathbf{x}}_{\\mathcal{G}}\\big]_{k}\\right|^{2}}\\\\ {=\\displaystyle\\frac{c_{\\lambda}}{\\sum_{i\\in\\mathcal{G}}s_{i}}\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbb{E}\\left\\|\\mathbf{x}_{i}-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\right\\|^{2}}\\\\ {\\leq c_{\\lambda}\\rho^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality is derived from the assumption stated in this lemma. The second aligns with thedefinition of $(c_{\\lambda},\\lambda)$ -weighted robust as detailed in Definition 3.1. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma C.3. Let & be a WeightedCWMed aggregator then X is $(c_{\\lambda},\\lambda)$ -weighted robust with $c_{\\lambda}=$ $\\begin{array}{r}{\\left(1+\\frac{\\lambda}{1-2\\lambda}\\right)^{2}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. In the context of the $k^{\\mathrm{th}}$ coordinate, [WeightedCWMed] $k$ functions equivalently to WeightedGM for a one-dimensional case. Consequently, each coordinate of the WeightedCWMed aggregator s $(c_{\\lambda},\\lambda)$ -weighted robust wih $\\begin{array}{r}{c_{\\lambda}=\\left(1+\\frac{\\lambda}{1-2\\lambda}\\right)^{2}}\\end{array}$ as established in Lemma C.1. Furthermore, since the WeightedCWMed functions on a coordinate-wise basis, it follows from Lemma C.2 that the entire WeightedCWMed agregatoris $(c_{\\lambda},\\lambda)$ -weighted robst with $\\begin{array}{r}{c_{\\lambda}=\\left(1+\\frac{\\lambda}{1-2\\lambda}\\right)^{2}}\\end{array}$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.2  Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Lemma 3.1. We denote $\\mathbf{y}_{i}:=\\mathbf{x}_{i}-\\mathbf{x}_{0}$ $\\textstyle\\Sigma_{\\mathcal{G}}:=\\sum_{i\\in\\mathcal{G}}s_{i}$ \uff0c $\\textstyle\\Sigma_{S}:=\\sum_{i\\in S}s_{i}$ \uff0c $\\textstyle\\Sigma_{\\mathcal{B}}:=\\sum_{i\\in\\mathcal{B}}s_{i}$ and $\\begin{array}{r}{\\Sigma_{m}:=\\sum_{i\\in[m]}s_{i}}\\end{array}$ . Recall that $\\Sigma\\mathcal{G}\\geq(1-\\lambda)\\Sigma_{m}$ and $\\vec{\\Sigma_{S}}=(1-\\lambda)\\Sigma_{m}$ (Alg. 1). ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\boldsymbol{\\kappa}}-{\\boldsymbol{\\mathcal{B}}}_{\\theta}={\\frac{1}{\\sum_{S}\\theta}}\\sum_{s,x}s_{\\theta,x}}\\\\ &{=\\mathbf{x}_{0}-{\\frac{\\mathbf{x}_{0}}{\\sum_{S}\\theta}}+{\\frac{1}{\\sum_{S}{\\theta}}}\\sum_{s}s_{\\theta}(\\mathbf{x}_{i}-\\mathbf{x}_{0})}\\\\ &{=-{\\frac{1}{\\sum_{S}\\theta}}\\sum_{\\ell\\in\\mathcal{S}_{1}}(\\mathbf{x}_{i}-\\mathbf{x}_{0})+{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\theta}(\\mathbf{x}_{i}-\\mathbf{x}_{0})}\\\\ &{=-{\\frac{1}{\\sum_{S}\\theta}}\\sum_{\\ell\\in\\mathcal{S}_{1}}s_{\\theta,x}+{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\ell}}\\\\ &{=\\left({\\frac{1}{\\sum_{S}}}-{\\frac{1}{\\sum_{S}\\theta}}\\right)\\sum_{s}s_{\\ell}(\\mathbf{x}_{0}-{\\frac{1}{\\sum_{S}\\theta}})}\\\\ &{=\\left({\\frac{1}{\\sum_{S}}}-{\\frac{1}{\\sum_{S}\\theta}}\\right)\\sum_{\\ell\\in\\mathcal{S}_{1}}-{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\ell}+{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\ell}s_{\\ell}}\\\\ &{=\\left({\\frac{1}{\\sum_{S}}}-{\\frac{1}{\\sum_{S}\\theta}}\\right)\\sum_{\\ell\\in\\mathcal{S}_{1}}-{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\ell}+{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\ell}s_{\\ell}}\\\\ &{=\\left({\\frac{\\sum_{S}=\\Sigma_{S}}}\\right)\\sum_{\\ell\\in\\mathcal{S}_{1}}s_{\\ell}}\\\\ &{=\\left({\\frac{\\sum_{S}=\\Sigma_{S}}{\\sum_{S}\\theta}}\\right)\\sum_{\\ell\\in\\mathcal{S}_{1}}-{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\ell}+{\\frac{1}{\\sum_{S}\\theta}}\\sum_{s}s_{\\ell},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking the squared norm of both sides, we obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\mathbf{x}}-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\|^{2}=\\left\\|\\left(\\frac{\\Sigma_{\\mathcal{G}}-\\Sigma_{\\mathcal{S}}}{\\Sigma_{\\mathcal{S}}\\Sigma_{\\mathcal{G}}}\\right)\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbf{y}_{i}-\\frac{1}{\\Sigma_{\\mathcal{S}}}\\sum_{i\\in\\mathcal{G}\\setminus\\mathcal{S}}s_{i}\\mathbf{y}_{i}+\\frac{1}{\\Sigma_{\\mathcal{S}}}\\sum_{i\\in\\mathcal{S}\\backslash\\mathcal{G}}s_{i}\\mathbf{y}_{i}\\right\\|^{2}}\\\\ {\\displaystyle\\leq3\\left\\|\\left(\\frac{\\Sigma_{\\mathcal{G}}-\\Sigma_{\\mathcal{S}}}{\\Sigma_{\\mathcal{S}}\\Sigma_{\\mathcal{G}}}\\right)\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbf{y}_{i}\\right\\|^{2}+3\\left\\|\\frac{1}{\\Sigma_{\\mathcal{S}}}\\sum_{i\\in\\mathcal{G}\\setminus\\mathcal{S}}s_{i}\\mathbf{y}_{i}\\right\\|^{2}+3\\left\\|\\frac{1}{\\Sigma_{\\mathcal{S}}}\\sum_{i\\in\\mathcal{S}\\backslash\\mathcal{G}}s_{i}\\mathbf{y}_{i}\\right\\|^{2}}\\\\ {\\displaystyle\\leq3\\Sigma_{\\mathcal{G}}\\left(\\frac{\\Sigma_{\\mathcal{G}}-\\Sigma_{\\mathcal{S}}}{\\Sigma_{\\mathcal{S}}\\Sigma_{\\mathcal{G}}}\\right)^{2}\\sum_{i\\in\\mathcal{G}}s_{i}|\\mathbf{y}_{i}|^{2}+\\frac{3\\sum_{i\\in\\mathcal{S}\\backslash\\mathcal{G}}s_{i}}{\\Sigma_{\\mathcal{S}}^{2}}\\sum_{i\\in\\mathcal{S}\\backslash\\mathcal{G}}s_{i}|\\mathbf{y}_{i}|^{2}+\\frac{3\\sum_{i\\in\\mathcal{G}\\backslash\\mathcal{S}}s_{i}}{\\Sigma_{\\mathcal{S}}^{2}}\\sum_{i\\in\\mathcal{G}\\backslash\\mathcal{S}}s_{i}^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality follows the inequality $\\|\\mathbf{a}+\\mathbf{b}+\\mathbf{c}\\|^{2}\\leq3\\|\\mathbf{a}\\|^{2}\\!+\\!3\\|\\mathbf{b}\\|^{2}\\!+\\!3\\|\\mathbf{c}\\|^{2},\\forall\\mathbf{a},\\mathbf{b},\\mathbf{c}\\in\\mathbb{R}^{2}$ and the second follow Jensen's inequality. ", "page_idx": 24}, {"type": "text", "text": "Note that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{\\boldsymbol{\\Sigma}_{Q}-\\boldsymbol{\\Sigma}_{S}}{\\boldsymbol{\\Sigma}_{S}\\boldsymbol{\\Sigma}_{Q}}\\right)^{2}=\\left(\\frac{\\boldsymbol{\\Sigma}_{m}-\\boldsymbol{\\Sigma}_{B}-(1-\\lambda)\\boldsymbol{\\Sigma}_{m}}{\\boldsymbol{\\Sigma}_{S}\\boldsymbol{\\Sigma}_{Q}}\\right)^{2}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\leq\\left(\\frac{2\\lambda}{\\boldsymbol{\\Sigma}_{Q}}\\right)^{2}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{<\\frac{2\\lambda}{\\boldsymbol{\\Sigma}_{Q}^{2}}\\,,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality holds because $\\Sigma_{B}\\leq\\lambda\\Sigma_{m}$ and $\\Sigma_{S}=(1-\\lambda)\\Sigma_{m}$ . The second inequality follows from the fact that $1-\\lambda\\ge{^1\\!/2}$ , and the last since $\\lambda<{^1\\!/2}$ ", "page_idx": 24}, {"type": "text", "text": "In addition, we have that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{i\\in S\\cup S}g_{i}}{\\sum_{s}^{2}}=\\frac{\\sum_{i\\in S\\cup S}g_{i}-\\sum_{i\\in Q}s_{i}}{\\sum_{s}^{2}}}\\\\ &{\\phantom{\\frac{(x)^{2}}{\\sum_{s}^{2}}}\\leq\\frac{\\sum_{m=}^{}-(1-\\lambda)\\sum_{i}m_{m}^{}}{\\sum_{s}^{2}}}\\\\ &{=\\frac{\\lambda\\sum_{m=}^{}}{\\sum_{s}^{2}}}\\\\ &{=\\frac{\\lambda\\sum_{m}}{(1-\\lambda)^{2}\\sum_{m=}^{2}}}\\\\ &{\\leq\\frac{4\\lambda}{\\sum_{m}}}\\\\ &{\\leq\\frac{4\\lambda}{\\sum_{q}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality follows the facts that $S\\cup\\mathcal{G}\\subseteq[m]$ \uff0c\uff0c $\\{s_{i}\\geq0\\}_{i\\in[m]}$ and $\\Sigma\\mathcal{G}\\geq(1-\\lambda)\\Sigma_{m}$ The second inequality is based on that $1-\\lambda\\ge{^1\\!/2}$ , and the last since $\\Sigma_{\\mathcal{G}}\\leq\\Sigma_{m}$ . And in a similar way, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in{\\mathcal{G}}\\setminus S}s_{i}}{\\Sigma_{S}^{2}}\\le\\frac{4\\lambda}{\\Sigma_{\\mathcal{G}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying Eq. (21), Eq. (22) and Eq. (23) into Eq. (20), gives us, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\mathbf{x}}-\\bar{\\mathbf{x}}_{\\mathcal{G}}\\|^{2}\\leq\\displaystyle\\frac{6\\lambda}{\\Sigma_{\\mathcal{G}}}\\sum_{i\\in\\mathcal{G}}s_{i}\\|\\mathbf{y}_{i}\\|^{2}+\\displaystyle\\frac{12\\lambda}{\\Sigma_{\\mathcal{G}}}\\sum_{i\\in S\\setminus\\mathcal{G}}s_{i}\\|\\mathbf{y}_{i}\\|^{2}+\\displaystyle\\frac{12\\lambda}{\\Sigma_{\\mathcal{G}}}\\sum_{i\\in\\mathcal{G}\\setminus S}s_{i}\\|\\mathbf{y}_{i}\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{12\\lambda}{\\Sigma_{\\mathcal{G}}}\\sum_{i\\in S\\setminus\\mathcal{G}}s_{i}\\|\\mathbf{x}_{i}-\\mathbf{x}_{0}\\|^{2}+\\displaystyle\\frac{18\\lambda}{\\Sigma_{\\mathcal{G}}}\\sum_{i\\in\\mathcal{G}}s_{i}\\|\\mathbf{x}_{i}-\\mathbf{x}_{0}\\|^{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the latter holds since $\\begin{array}{r}{\\sum_{i\\in\\mathcal{G}\\setminus S}s_{i}\\|\\mathbf{y}_{i}\\|^{2}\\leq\\sum_{i\\in\\mathcal{G}}s_{i}\\|\\mathbf{y}_{i}\\|^{2}.}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Next, we define: ", "page_idx": 25}, {"type": "equation", "text": "$$\nS^{*}:=\\bigcup_{i\\in S}\\left\\{i\\right\\}_{j=1}^{s_{i}},\\quad\\mathcal{G}^{*}:=\\bigcup_{i\\in\\mathcal{G}}\\left\\{i\\right\\}_{j=1}^{s_{i}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\sum_{i\\in S\\setminus\\mathcal{G}}s_{i}\\|\\mathbf{x}_{i}-\\mathbf{x}_{0}\\|^{2}=\\sum_{i\\in S^{*}\\setminus\\mathcal{G}^{*}}\\|\\mathbf{x}_{i}-\\mathbf{x}_{0}\\|^{2}}\\end{array}$ . We'll show that there exists an injective function $\\Phi:S^{*}\\backslash{\\mathcal{G}}^{*}\\rightarrow{\\mathcal{G}}^{*}\\backslash S^{*}$ such that, $\\forall i\\in S^{*}\\setminus\\mathcal{G}^{*},\\,\\|\\mathbf{x}_{\\Phi(i)}-\\mathbf{x}_{0}\\|\\geq\\|\\mathbf{x}_{i}-\\mathbf{x}_{0}\\|$ is satisfied. This follows from our selection of $S$ , which consists of the closest elements $\\{\\mathbf{x}_{i}\\}_{i\\in S}$ to $\\mathbf{x}_{\\mathrm{0}}$ (see Alg. 1), and from: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|S^{*}\\middle<\\mathcal{G}^{*}\\right|=\\sum_{i\\in S\\backslash\\mathcal{G}}s_{i}=\\sum_{i\\in S}s_{i}+\\sum_{i\\in\\mathcal{G}\\backslash S}s_{i}-\\sum_{i\\in\\mathcal{G}}s_{i}\\leq\\sum_{i\\in\\mathcal{G}\\backslash S}s_{i}=\\left|\\mathcal{G}^{*}\\middle<S^{*}\\right|\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows that $\\begin{array}{r}{\\sum_{i\\in S}s_{i}-\\sum_{i\\in\\mathcal{G}}s_{i}=(1-\\lambda)\\Sigma_{m}-\\Sigma_{\\mathcal{G}}\\leq0.}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigl\\|\\hat{x}-\\bar{x}_{G}\\bigr\\|^{2}\\leq\\frac{11\\Delta}{\\gamma}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}s_{\\ell}\\bigl\\|x_{1}-x_{0}\\bigr\\|^{2}+\\frac{18\\Delta}{\\gamma}\\displaystyle\\sum_{\\ell\\leq\\ell\\leq\\ell}\\displaystyle\\sum_{i\\in\\mathcal{G}}\\bar{x}_{i}\\|x_{i}-x_{0}\\|^{2}}\\\\ {=\\frac{12\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}\\|x_{i}-x_{0}\\|^{2}+\\frac{18\\Delta}{\\gamma}\\displaystyle\\sum_{\\ell\\leq\\ell\\leq\\ell}\\displaystyle\\sum_{\\ell\\leq\\ell\\leq\\ell}\\|x_{i}-x_{0}\\|^{2}}\\\\ {\\leq\\frac{12\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}\\|x_{\\ell+\\ell}-x_{0}\\|^{2}+\\frac{18\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq\\ell\\leq\\ell}\\|x_{i}-x_{0}\\|^{2}}\\\\ {\\leq\\frac{12\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}\\|x_{i}-x_{0}\\|^{2}+\\frac{18\\Delta}{\\gamma}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}s_{\\ell}\\|x_{i}-x_{0}\\|^{2}}\\\\ {=\\frac{12\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}s_{\\ell}\\|x_{i}-x_{0}\\|^{2}+\\frac{18\\Delta}{\\gamma}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}s_{\\ell}\\|x_{i}-x_{0}\\|^{2}}\\\\ {=\\frac{30\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq t\\leq\\ell}\\|x_{i}-x_{0}\\|^{2}}\\\\ {=\\frac{60\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq t\\leq\\ell}\\|x_{i}-x_{0}\\|^{2}}\\\\ {\\leq\\frac{60\\Delta}{\\gamma\\phi_{i}}\\displaystyle\\sum_{\\ell\\leq t\\leq N_{0}}^{}s_{\\ell}\\|x_{i}-x_{0}\\|^{2}+\\frac{60\\Delta}{\\gamma}\\displaystyle\\sum_{\\ell\\leq t\\leq\\ell}\\|x_{\\ell}-x \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality follows from the definition of the injective function $\\Phi$ .The third inequalityis justifed by thefact that $\\begin{array}{r}{\\sum_{i\\in\\mathcal{G}^{\\ast}\\setminus S^{\\ast}}\\|\\mathbf{y}_{i}\\|^{2}\\leq\\sum_{i\\in\\mathcal{G}^{\\ast}}\\|\\bar{\\mathbf{y}_{i}}\\|^{2}}\\end{array}$ . Finally, the last inequality leverages the property $\\|\\mathbf{a}+\\mathbf{b}\\|^{2}\\leq2\\|\\mathbf{a}\\|^{2}+2\\|\\mathbf{b}\\|^{2}$ , which holds $\\forall\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^{d}$ ", "page_idx": 26}, {"type": "text", "text": "Taking the expectations of both sides gives us the following: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\|\\hat{{\\mathbf{x}}}-\\bar{{\\mathbf{x}}}_{\\mathcal{G}}\\|^{2}\\leq\\frac{60\\lambda}{\\Sigma_{\\mathcal{G}}}\\displaystyle\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbb{E}\\|{\\mathbf{x}}_{i}-\\bar{{\\mathbf{x}}}_{\\mathcal{G}}\\|^{2}+\\frac{60\\lambda}{\\Sigma_{\\mathcal{G}}}\\displaystyle\\sum_{i\\in\\mathcal{G}}s_{i}\\mathbb{E}\\|\\bar{{\\mathbf{x}}}_{\\mathcal{G}}-{\\mathbf{x}}_{0}\\|^{2}}\\\\ {\\displaystyle\\leq60\\lambda\\rho^{2}+60\\lambda c_{\\lambda}\\rho^{2}}\\\\ {\\displaystyle=60\\lambda(1+c_{\\lambda})\\rho^{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequaility stems from Def. 3.1 ", "page_idx": 26}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Technical Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Datasets. We simulated over the MNIST [LeCun et al., 2010] and CIFAR-10 [Krizhevsky et al., 2014] datasets. The datasets were accessed through torchvision (version 0.16.2). ", "page_idx": 26}, {"type": "text", "text": "\u00b7 MNIST Dataset. MNIST is a widely used benchmark dataset in the machine learning community, consisting of 70,000 grayscale images of handwritten digits (0-9) with a resolution of $28\\mathrm{x}28$ pixels. The dataset is split into 60,000 training images and 10,000 test images. \u00b7 CIFAR-10 Dataset. CIFAR-10 is a widely recognized benchmark dataset in the machine learning community, containing 60,000 color images categorized into 10 different classes. Each image has a resolution of $32\\mathrm{x}32$ pixels and represents objects such as airplanes, automobiles, birds, cats, and more. The dataset is evenly split into 50,000 training images and 10,000 test images. ", "page_idx": 26}, {"type": "text", "text": "Imbalanced Arrival Scenarios. We simulated two types of imbalanced arrival scenarios: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 Proportional Arrival Probability: The probability of arrival for the $i^{\\th}$ -th worker in the honest group was set proportionally to $i/\\sum_{j\\in\\mathcal{G}}j$ , ensuring that workers with higher indices had a higher chance of arriving. The same distribution method was applied to Byzantine workers. \u00b7 Squared IM Arrival Probability: In a more skewed scenario, the arrival probability was proportional to the square of the worker's ID, i.e.. $i^{2}/\\sum_{j\\in\\mathcal{G}}j^{2}$ . This setup further accentuated the imbalance by favoring workers with larger IDs. ", "page_idx": 26}, {"type": "table", "img_path": "v1kpc060aC/tmp/91f51838c099b62272337968a4affddb63a2d5a663eaa0fd0cd23f2c9dc98e11.jpg", "table_caption": [], "table_footnote": ["Table 2: Experimental Setup for MNIST and CIFAR-10 "], "page_idx": 27}, {"type": "text", "text": "For simplicity, Byzantine workers were introduced after a fixed number of iterations, controlled by a parameter $\\lambda$ . However, it is worth noting that when Byzantine iterations are concentrated, they can cause significant performance degradation. Such patterns can lead to increased delays for honest updates, ultimately affecting the overall convergence of the algorithm. ", "page_idx": 27}, {"type": "text", "text": "Optimization Setup.  We optimized the cross-entropy loss across all experiments. For comparisons, we configured $\\mu^{2}$ -SGD with fixed parameters $\\gamma\\,=\\,0.1$ and $\\beta\\:=\\:0.25$ . This was tested against Standard SGD, and Momentum-based SGD, where the momentum parameter was set to $\\beta=0.9$ as recommended by Karimireddy et al. [2021], and also fine-tuning $\\beta$ to 0.8. ", "page_idx": 27}, {"type": "text", "text": "Attack Simulations.  We simulated four types of attacks to evaluate the robustness of our approach: ", "page_idx": 27}, {"type": "text", "text": "Label Flipping [Allen-Zhu et al., 2020]: The labels of the data were flipped to incorrect values, by subtracting the original labels from 9. ", "page_idx": 27}, {"type": "text", "text": "2. Sign Flipping [Allen-Zhu et al., 2020]: The signs of the workers\u2032 output were flipped. ", "page_idx": 27}, {"type": "text", "text": "3. Little [Baruch et al., 2019]: Adapted from the synchronous case. It computes the maximum allowabledeviation $z_{\\mathrm{max}}$ based on iterations count rather than the number of workers. Then, it perturbs the honest updates by subtracting the product of the weighted standard deviation and $z_{\\mathrm{max}}$ from the weighted mean of the honest updates. Ru7ontinendata $=$ weiahted mean/honect momentume)-weighted ctd(honect momantume ", "page_idx": 27}, {"type": "text", "text": "4. Empire [Xie et al., 2020a]: Adapted from the synchronous case. This attack scales the weighted mean of the honest momentums by a factor $\\epsilon$ in the negative direction, Byzantine_update $\\mathbf{\\Sigma}=-\\mathbf{\\Sigma}\\epsilon\\mathbf{\\Sigma}\\cdot\\mathbf{\\Sigma}$ weighted_mean(honest _momentums). ", "page_idx": 27}, {"type": "text", "text": "In the two latter attacks, the mean and standard deviation are calculated coordinate-wise with respect to weights, setting $\\epsilon=0.1$ ", "page_idx": 27}, {"type": "text", "text": "AnyTime Update Formulation. Regarding the AnyTime update, defined as $\\mathbf{x}_{t}\\quad:=$ $\\frac{\\alpha_{t}\\mathbf{w}_{t}\\!+\\!\\alpha_{1:t-1}\\mathbf{x}_{t-1}}{\\alpha_{1:t}}$ we emplyed a momentum-aed formulaion thatequivalent to the stadard AnyTime update. Specifically, we updated the model parameters according to the formula: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\gamma_{t}\\mathbf{w}_{t}+(1-\\gamma_{t})\\mathbf{x}_{t-1}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\gamma_{t}$ is difined as $\\begin{array}{r}{\\gamma_{t}:=\\frac{\\alpha_{t}}{\\alpha_{1:t}}}\\end{array}$ . By setting $\\alpha_{t}=C\\alpha_{1:t-1}$ with $C>0$ being a constant, we derived that $\\begin{array}{r}{\\gamma_{t}=\\frac{C}{C+1}}\\end{array}$ and remains consistent across al time steps $t\\geq1$ ", "page_idx": 27}, {"type": "image", "img_path": "v1kpc060aC/tmp/152546db960fe55e841c5c8c8f07a2de82c7e4244b0869d060d325e0c5fc862d.jpg", "img_caption": ["D.2  Experimental Results on MNIST ", "Figure 5: MNIST. Test Accuracy of Weighted vs. Non-Weighted Robust Aggregators. This scenario involves 17 workers, including 8 Byzantine workers, with workers\u2019 arrival probabilities proportional to the square of their IDs. We used the $\\mu^{2}$ SGD in this scenario. Left: label fipping, $\\lambda=0.3$ Right: signflipping, $\\lambda=0.4$ "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "v1kpc060aC/tmp/052f10d3ca89943b0e0d8081471540c8f240afd72e2cfad5ff85b28bfab3b1d2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 6: MNIST. Test Accuracy Comparison of Weighted Robust Aggregators With and Without $\\omega$ CTMA. This scenario involves 9 workers, with a very fast Byzantine worker, and workers? arrival probabilities proportional to their IDs. On the left, we have a sign flipping attack with standard momentum ( $\\bar{\\beta}=0.9,\\lambda=0.4)$ \uff0c and on the right, we have little $\\lambda=0.2$ )and empire( $\\lambda=0.4$ attacks with $\\mu^{2}$ -SGD. ", "page_idx": 28}, {"type": "image", "img_path": "v1kpc060aC/tmp/bcf4cd9a3e5dbcda0dfa6d881d4bf47f76061fc6d7f78e1b4af8bf81c80c9530.jpg", "img_caption": ["Figure 7: MNIST. Test Accuracy Comparison Among Different Optimizers. This scenario involves 9 workers,with $\\lambda=0.4$ , 4 Byzantine workers, and workers? arrival probabilities proportional to their IDs. We also compared between momentum with the standard parameter $\\beta=0.9$ suggested by Karimireddy et al. [2021] and a fine-tuned parameter $\\beta=0.8$ "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS paper checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "- 1le as wel INA imeals uat ue papei as no itauon wme ue as wel Ino means uat the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications wouldbe.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that infuence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 Ine answer INA means inat ine paper aoes not inciuqe experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we ensured that our paper conforms with the NeurIPs Code of Ethics Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: we do not foresee any special societal impact that arise due to our work Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate ", "page_idx": 32}, {"type": "text", "text": "deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: our work does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]