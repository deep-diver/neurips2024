[{"figure_path": "0jld45XGgJ/figures/figures_4_1.jpg", "caption": "Figure 1: Strongly regular graph (SRG) solution with L = 4, K = 10 and r = 5. Left: Class-mean matrix of the third layer M3. The non-zero entries of each row have the same value and their number is r \u2212 1, which corresponds to the degree of the complete graph Kr. Middle: Class-mean matrix of the fourth layer before ReLU M4 (middle left), and its Gram matrix MTM4 (middle right). The SRG construction has very low rank before ReLU: rank(M4) = r and rank(\u03c3(M4)) = K. Right: MTM4 for DNC. The DNC solution has rank K in all layers before and after ReLU.", "description": "This figure shows a comparison between a strongly regular graph (SRG) solution and a deep neural collapse (DNC) solution for a 4-layer deep unconstrained features model (DUFM) with 10 classes. The left panel displays the class-mean matrix of the third layer (M3) for the SRG solution, highlighting its low rank (non-zero entries in each row are same, and their number is r-1). The middle panels show the class-mean matrix before ReLU application (M4) and its Gram matrix (MTM4) for the SRG solution, demonstrating its low rank before ReLU (rank(M4)=r, rank(\u03c3(M4))=K). The right panel shows MTM4 for the DNC solution, which has full rank (K) in all layers.", "section": "Low-rank solutions outperform deep neural collapse"}, {"figure_path": "0jld45XGgJ/figures/figures_7_1.jpg", "caption": "Figure 2: Training loss compared against DNC and SRG losses (left), DNC1 metric training progression (middle) and singular value distribution at convergence (right). Top row: 4-DUFM training with K = 10, \u03bb = 0.004 for all regularization parameters, learning rate of 0.5 and width 30. Results are averaged over 10 runs, and we show the confidence intervals at 1 standard deviation. Bottom row: Training of a ResNet20 with a 4-layer MLP head on CIFAR10, using a DUFM-like regularization. We use weight decay 0.005 except \u03bbH1 = 0.000005 (to compensate for n = 5000, which significantly influences the total regularization strength), learning rate 0.05 and width 64 for all the MLP layers. Results are averaged over 5 runs, and we show the confidence intervals at 1 standard deviation.", "description": "This figure compares the training performance of a 4-layer deep unconstrained features model (DUFM) and a ResNet20 with a 4-layer MLP head on CIFAR-10.  The top row shows the results for the DUFM, illustrating training loss, DNC1 metric (measuring within-class collapse), and singular value distribution at convergence. The bottom row presents the same metrics for the ResNet20/MLP model, using a DUFM-like regularization scheme.  The results demonstrate that low-rank solutions, in agreement with the theory, outperform deep neural collapse (DNC) solutions.", "section": "6.1 DUFM training"}, {"figure_path": "0jld45XGgJ/figures/figures_7_2.jpg", "caption": "Figure 3. All experiments refer to the training of an L-DUFM model. Results are averaged over 5 runs, and we show the confidence intervals at 1 standard deviation. Left: Ratio between SRG and DNC loss (LSRG/LDNC), as a function of r, where the number of classes is K = (2). Different curves correspond to different values of L \u2208 {3,4,5}. Middle: Average rank at convergence, as a function of the weight decay in log2-scale, when L = 4 and K = 10. Right: Empirical probability of finding a DNC solution as a function of the width, when L = 4 and K = 10.", "description": "This figure presents the results of experiments conducted on an L-DUFM model, illustrating the impact of various hyperparameters on the model's performance and the emergence of neural collapse. The left plot shows the ratio of losses between the SRG solution and the DNC solution, highlighting the superiority of the SRG solution in certain parameter regimes. The middle plot demonstrates the effect of weight decay on the average rank of the obtained solutions, while the right plot illustrates the influence of network width on the probability of obtaining a DNC solution.", "section": "6 Numerical results"}, {"figure_path": "0jld45XGgJ/figures/figures_8_1.jpg", "caption": "Figure 2: Training loss compared against DNC and SRG losses (left), DNC1 metric training progression (middle) and singular value distribution at convergence (right). Top row: 4-DUFM training with K = 10, \u03bb = 0.004 for all regularization parameters, learning rate of 0.5 and width 30. Results are averaged over 10 runs, and we show the confidence intervals at 1 standard deviation. Bottom row: Training of a ResNet20 with a 4-layer MLP head on CIFAR10, using a DUFM-like regularization. We use weight decay 0.005 except \u03bbH\u2081 = 0.000005 (to compensate for n = 5000, which significantly influences the total regularization strength), learning rate 0.05 and width 64 for all the MLP layers. Results are averaged over 5 runs, and we show the confidence intervals at 1 standard deviation.", "description": "This figure compares the training performance of a 4-layer deep unconstrained features model (4-DUFM) and a ResNet20 with a 4-layer MLP head on CIFAR-10.  The left plots show the training loss compared to the losses of deep neural collapse (DNC) and the strongly regular graph (SRG) solutions. The center plots display the DNC1 metric's training progression, measuring within-class variability. The right plots illustrate the singular value distributions at convergence.  The top row focuses on 4-DUFM training, while the bottom row shows results from training on CIFAR-10 with DUFM-like regularization.", "section": "6 Numerical results"}, {"figure_path": "0jld45XGgJ/figures/figures_28_1.jpg", "caption": "Figure 5: 4-DUFM training for K = 3 (top), K = 4 (middle), and K = 5 (bottom). Left: Loss progression, also decomposed into the fit and regularization terms. Middle left: Visualization of the matrix M3. Middle right: Visualization of the matrix M4. Right: Visualization of the matrix M3M3.", "description": "This figure presents the results of 4-layer deep unconstrained features model (DUFM) training with three different numbers of classes (K=3, 4, and 5).  The left column shows the loss progression, broken down into total loss, fitting loss, and neural collapse (NC) loss. The middle column shows visualizations of the class-mean matrices M3 (middle-left) and M4 (middle-right) for each case. The right column visualizes the Gram matrix M3M3, showing the relationships between class means. The figure demonstrates that even with a small number of classes and layers, low-rank solutions outperform deep neural collapse.", "section": "6.1 DUFM training"}, {"figure_path": "0jld45XGgJ/figures/figures_29_1.jpg", "caption": "Figure 6: Class-mean matrices and singular values at convergence for a DUFM model with K = 15 and L = 7. Top row: Singular values of M2, and visualization of the matrices M2, M6, M6 and M6. Bottom row: Singular values of H6 and H6.", "description": "This figure visualizes the class-mean matrices and singular values at convergence for a deep unconstrained features model (DUFM) with 15 classes and 7 layers. The top row shows singular values of the second layer's class-mean matrix (M2) alongside visualizations of the matrices M2, M6, and their Gram matrix (M6M6).  The bottom row presents singular values for the sixth layer's features (H6) before and after the ReLU activation (\u03c3(H6)). This illustrates the low-rank properties observed in intermediate layers.", "section": "6 Numerical results"}, {"figure_path": "0jld45XGgJ/figures/figures_29_2.jpg", "caption": "Figure 5. 4-DUFM training for K = 3 (top), K = 4 (middle), and K = 5 (bottom). Left: Loss progression, also decomposed into the fit and regularization terms. Middle left: Visualization of the matrix M3. Middle right: Visualization of the matrix M4. Right: Visualization of the matrix M3M3.", "description": "This figure presents the results of 4-layer deep unconstrained feature model (DUFM) training experiments for different numbers of classes (K=3, 4, and 5).  Each row shows results for a specific value of K. The left column displays the training loss progression, broken down into the total loss, fitting loss, and regularization loss.  The middle columns show visualizations of the class-mean matrices M3 and M4 (matrices of the class means stacked into columns) at layer 3 and layer 4 respectively. The right column visualizes the Gram matrix, M3M3, showing the relationships between the class means in layer 3.", "section": "6.1 DUFM training"}, {"figure_path": "0jld45XGgJ/figures/figures_30_1.jpg", "caption": "Figure 5. 4-DUFM training for K = 3 (top), K = 4 (middle), and K = 5 (bottom). Left: Loss progression, also decomposed into the fit and regularization terms. Middle left: Visualization of the matrix M3. Middle right: Visualization of the matrix M4. Right: Visualization of the matrix M3M3.", "description": "This figure shows the results of 4-layer deep unconstrained feature model (DUFM) training for different numbers of classes (K=3, 4, and 5).  The left column displays the loss progression, broken down into total loss, fit loss (how well the model fits the data), and regularization loss. The middle column presents visualizations of the class-mean matrices (M3 and M4) for the third and fourth layers. The right column visualizes the Gram matrices (M3M3) which shows the relationships between class means. The results demonstrate that even for a small number of layers and classes, the low-rank solutions outperform the deep neural collapse (DNC).", "section": "6.1 DUFM training"}, {"figure_path": "0jld45XGgJ/figures/figures_30_2.jpg", "caption": "Figure 2: Training loss compared against DNC and SRG losses (left), DNC1 metric training progression (middle) and singular value distribution at convergence (right). Top row: 4-DUFM training with K = 10, \u03bb = 0.004 for all regularization parameters, learning rate of 0.5 and width 30. Results are averaged over 10 runs, and we show the confidence intervals at 1 standard deviation. Bottom row: Training of a ResNet20 with a 4-layer MLP head on CIFAR10, using a DUFM-like regularization. We use weight decay 0.005 except \u03bbH1 = 0.000005 (to compensate for n = 5000, which significantly influences the total regularization strength), learning rate 0.05 and width 64 for all the MLP layers. Results are averaged over 5 runs, and we show the confidence intervals at 1 standard deviation.", "description": "This figure compares the performance of three different approaches (standard training, deep neural collapse, and the proposed low-rank solution) on a four-layer deep unconstrained features model (4-DUFM) with 10 classes.  The top row shows results for the 4-DUFM, while the bottom row illustrates the results when applying the same approaches on a ResNet20 backbone with a 4-layer multi-layer perceptron (MLP) head, trained on CIFAR10 dataset. The plots depict training loss, the DNC1 metric (measuring within-class collapse), and singular value distributions at the end of training.  The results demonstrate that the proposed approach achieves lower training loss than DNC, exhibiting a strong low-rank bias. The results confirm that DNC1 (within-class variability collapse) holds.", "section": "6.1 DUFM training"}, {"figure_path": "0jld45XGgJ/figures/figures_31_1.jpg", "caption": "Figure 3. All experiments refer to the training of an L-DUFM model. Results are averaged over 5 runs, and we show the confidence intervals at 1 standard deviation. Left: Ratio between SRG and DNC loss (LSRG/LDNC), as a function of r, where the number of classes is K = (2). Different curves correspond to different values of L \u2208 {3,4,5}. Middle: Average rank at convergence, as a function of the weight decay in log2-scale, when L = 4 and K = 15. Right: Empirical probability of finding a DNC solution as a function of the width, when L = 4 and K = 10.", "description": "This figure shows the results of experiments on the L-DUFM model, which explores the impact of various hyperparameters on the loss and rank of solutions at convergence. It consists of three subplots: The left subplot illustrates the ratio of SRG loss to DNC loss as a function of r, showing that SRG outperforms DNC for different depths (L). The middle subplot shows how the average rank varies with weight decay, revealing a low-rank bias. Finally, the right subplot demonstrates the relationship between the width and the probability of obtaining a DNC solution, indicating that larger widths favor DNC.", "section": "6 Numerical results"}]