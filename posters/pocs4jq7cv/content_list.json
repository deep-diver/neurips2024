[{"type": "text", "text": "Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Benjamin Eysenbach\u2217 Princeton University eysenbach@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Vivek Myers\u2217 UC Berkeley vmyers@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Ruslan Salakhutdinov Carnegie Mellon University rsalakhu@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Sergey Levine UC Berkeley svlevine@eecs.berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given time series data, how can we answer questions like \u201cwhat will happen in the future?\u201d and \u201chow did we get here?\u201d These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Probabilistic modeling of time-series data has applications ranging from robotic control [84] to material science [42], from cell biology [74] to astrophysics [55]. These applications are often concerned with two questions: predicting future states (e.g., what will this cell look like in an hour), and inferring trajectories between two given states. However, answering these questions often requires reasoning over high-dimensional data, which can be challenging as most tools in the standard probabilistic toolkit require generation. Might it be possible to use discriminative methods (e.g., contrastive learning) to perform such inferences? ", "page_idx": 0}, {"type": "text", "text": "Many prior works aim to learn representations that are easy to predict while retaining salient bits of information. For time-series data, we want the representation to remain a sufficient statistic for distributions related to time \u2013 for example, they should retain bits required to predict future states (or representations thereof). While generative methods [26, 56, 101, 103] have this property, they tend to be computationally expensive (see, e.g., [72]) and can be challenging to scale to high-dimensional observations. ", "page_idx": 0}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/0cd1a1ddf2c1e424cbf9fd86fb0cc8620a8d47e046317796b3da3090645196fa.jpg", "img_caption": ["Figure 1: We apply temporal contrastive learning to observation pairs to obtain representations $(\\psi(x_{0}),\\psi(x_{t+k}))$ such that $A\\psi(\\bar{x_{0}})$ is close to $\\psi(x_{t+k})$ . While inferring waypoints in the high-dimensional observation space is challenging, we show that the distribution over intermediate latent representations has a closed form solution corresponding to linear interpolation between the initial and final representations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we will study how contrastive methods (which are discriminative, rather than generative) can perform inference over times series. Ideally, we want representations of observations $x$ to be a sufficient statistic for temporal relationships (e.g., does $x^{\\prime}$ occur after $x?$ ) but need not retain other information about $x$ (e.g. the location of static objects). This intuition motivates us to study how contrastive representation learning methods [18, 65, 81, 87, 96] might be used to solve prediction and planning problems on time series data. While prior works in computer vision [16, 65] and natural language processing (NLP) [59] often study the geometry of learned representations, our results show how geometric operations such as interpolation are related to inference. Our analysis will focus on a regularized version of the symmetrized infoNCE objective [71], generating positive examples by sampling pairs of observations from the same time series data. We will study how representations learned in this way can facilitate two inference questions: prediction and planning.2 As a stepping stone, we will build upon prior work [90] to show that regularized contrastive learning should produce representations whose marginal distribution is an isotropic Gaussian distribution. ", "page_idx": 1}, {"type": "text", "text": "The main contribution of this paper is to demonstrate how intermediate and future time steps in a time series can be inferred easily using contrastive representations. This inference problem captures a number of practical tasks: interpolation, in-filling, and even planning and control, where the intermediate steps represent states between a stand and goal. While ordinarily these problems require an iterative inference or optimization procedure, with contrastive representations this can be done simply by inverting a low-dimensional matrix. In one special case, inference will correspond to linear interpolation. Our first step is to prove that, under certain assumptions, the distribution over future representations has a Gaussian distribution, with a mean that is a linear function of the initial state representation (Lemma 1). This paves the main to our main result (Theorem 4.1): given an initial and final state, we show that the posterior distribution over an intermediate state representations also follows a Gaussian distribution. Said in other words, the representations follow a Gauss-Markov chain,3 wherein any joint or conditional distribution can be computed by inverting a low-dimensional matrix [57, 93] (See Fig. 1). In one special case, inference will correspond to linearly interpolating between the representations of an initial state and final state. Section 5 provides numerical experiments. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Representations for time-series data. In applications ranging from robotics to vision to NLP, users often want to learn representations of observations from time series data such that the spatial arrangement of representations reflects the temporal arrangement of the observations [28, 59, 65, 70]. Ideally, these representations should retain information required to predict future observations and infer likely paths between pairs of observations. Many approaches use an autoencoder, learning representations that retain the bits necessary to reconstruct the input observation, while also regularizing the representations to compressed or predictable [12, 20, 43, 45, 68, 104]. A prototypical method is the sequential VAE [101], which is computationally expensive to train because of the reconstruction loss, but is easy to use for inference. Our work shares the aims of prior prior methods that attempt to linearize the dynamics of nonlinear systems [7, 23, 63, 64, 80, 92], including videos [33, 40]. Our work aims to retain uncertainty estimates over predictions (like the sequential VAE) without requiring reconstruction. Avoiding reconstruction is appealing practically because it decreases the computational requirements and number of hyperparameters; and theoretically because it means that representations only need to retain bits about temporal relationships and not about the bits required to reconstruct the original observation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Contrastive Learning. Contrastive learning methods circumvent reconstruction by learning representations that merely classify if two events were sampled from the same joint distribution [17, 35, 71]. When applied to representing states along trajectories, contrastive representations learn to classify whether two points lie on the same trajectory or not [29, 65, 70, 78, 97]. Empirically, prior work in computer vision and NLP has observed that contrastive learning acquires representations where interpolation between representations corresponds to changing the images in semantically meaningful ways [19, 50, 59, 67, 95, 98]. ", "page_idx": 2}, {"type": "text", "text": "Our analysis will be structurally similar to prior theoretical analysis on explaining why word embeddings can solve analogies [3, 5, 48]. Our work will make a Gaussianity assumption similar to Arora et al. [5] and our Markov assumption is similar to the random walks analyzed in Arora et al. [5], Hashimoto et al. [36]. Our paper builds upon and extends these results to answer questions such as: \u201cwhat is the distribution over future observations representations?\u201d and \u201cwhat is the distribution over state (representations) that would occur on the path between one observation and another?\u201d While prior work is primarily aimed at explaining the good performance of contrastive word embeddings (see, e.g., [5]), we are primarily interested in showing how similar contrastive methods are an effective tool for inference over high-dimensional time series data. Our analysis will show how representations learned via temporal contrastive learning (i.e., without reconstruction) are sufficient statistics for inferring future outcomes and can be used for performing inference on a graphical model (a problem typically associated with generative methods). ", "page_idx": 2}, {"type": "text", "text": "Goal-oriented decision making. Much work on time series representations is done in service of learning goal-reaching behavior, an old problem [46, 61] that has received renewed attention in recent years [14, 15, 21, 37, 39, 52, 76, 99]. Some of the excitement in goal-conditioned RL is a reflection of the recent success of self-supervised methods in computer vision [73] and NLP [66]. Our analysis will study a variant of contrastive representation learning proposed in prior work for goal-conditioned RL [29, 78]. These methods are widespread, appearing as learning objectives for learning value functions [1, 28, 29, 49, 51, 53, 60, 86, 91, 102], as auxiliary objectives [4, 9, 13, 60, 77, 82, 83], in objectives for model-based RL [2, 32, 58, 80], and in exploration methods [25, 34]. Our analysis will highlight connections between these prior methods, the classic successor representation [8, 24], and probabilistic inference. ", "page_idx": 2}, {"type": "text", "text": "Planning. Planning lies at the core of many RL and control methods, allowing methods to infer the sequence of states and actions that would occur if the agent navigated from one state to a goal state. While common methods such as PRM [44] and RRT [47] focus on building random graphs, there is a strong community focusing on planning methods based on probabilistic inference [6, 85, 94]. The key challenge is scaling to high-dimensional settings. While semi-parametric methods make progress on this problem this limitation through semi-parametric planning [27, 30, 100], it remains unclear how to scale any of these methods to high-dimensional settings when states do not lie on a low-dimensional manifold. Our analysis will show how contrastive representations may lift this limitation, with experiments validating this theory on 39-dimensional and 46-dimensional tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our aim is to learn representations of time series data such that the spatial arrangement of representations corresponds to the temporal arrangement of the underlying data: if one example occurs shortly after another, then they should be mapped to similar representations. This problem setting arises in many areas, including video understanding and reinforcement learning. To define this problem formally, we will define a Markov process with states $x_{t}$ indexed by time $t$ :4 $\\begin{array}{r}{p(x_{1:T}\\mid x_{0})=\\prod_{t=0}^{T}p(x_{t+1}\\mid x_{t})}\\end{array}$ . The dynamics $p(x_{t+1}\\mid x_{t})$ tell us the immediate next state, and we can define the distribution over states $t$ steps in the future by marginalizing over the intermediate states, $\\begin{array}{r}{p_{t}(x_{t}\\mid x_{0})=\\int p(x_{1:t}\\mid x_{0})\\,\\mathrm{d}x_{1:t-1}}\\end{array}$ . A key quantity of interest will be the $\\gamma$ -discounted state occupancy measure, which corresponds to a time-averaged distribution over future states: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\np_{t+}(x_{t+}=x)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}p_{t}(x_{t}=x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Contrastive learning. Our analysis will focus on applying contrastive learning to a particular data distribution. Contrastive learning [35, 65, 75] acquires representations using \u201cpositive\u201d pairs $(x,x^{+})$ and \u201cnegative\u201d pairs $(x,x^{-})$ . While contrastive learning typically learns just one representation, we will use two different representation for the two elements of the pair; that is, our analysis will use terms like $\\phi(x),\\psi(x^{+})$ and $\\psi(x^{-})$ . We assume all representations lie in $\\mathbb{R}^{k}$ . ", "page_idx": 3}, {"type": "text", "text": "The aim of contrastive learning is to learn representations such that positive pairs have similar representations $(\\phi(x)~\\approx~\\psi(x^{\\bar{+}}))$ while negative pairs have dissimilar representations $(\\phi(x)\\ \\neq$ $\\bar{\\psi(}x^{-}))$ . Let $p(x,x^{+})$ be the joint distribution over positive pairs (i.e., $(x,x^{+})\\sim p(x,x^{+}))$ . We will use the product of the marginal distributions to sample negative pairs $((x,x^{-})\\sim p(x)p(x))$ . Let $B$ be the batch size, and note that the positive samples $x_{j}^{+}$ at index $j$ in the batch serve as negatives for $x_{i}$ for any $i\\neq j$ . Our analysis is based on the infoNCE objective without resubstitution [65, 81]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\phi(\\cdot),\\psi(\\cdot)}{\\operatorname*{max}}\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\left[\\underset{i=1}{\\overset{B}{\\sum}}\\log\\frac{e^{-\\frac{1}{2}\\|\\phi(x_{i})-\\psi(x_{i}^{+})\\|_{2}^{2}}}{\\sum_{j\\neq i}e^{-\\frac{1}{2}\\|\\phi(x_{i})-\\psi(x_{j}^{+})\\|_{2}^{2}}}+\\log\\frac{e^{-\\frac{1}{2}\\|\\phi(x_{i})-\\psi(x_{i}^{+})\\|_{2}^{2}}}{\\sum_{j\\neq i}e^{-\\frac{1}{2}\\|\\phi(x_{j})-\\psi(x_{i}^{+})\\|_{2}^{2}}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We will use the symmetrized version of this objective [71], where the denominator is the sum across rows of a logits matrix and once where it is a sum across the. ", "page_idx": 3}, {"type": "text", "text": "While contrastive learning is typically applied to an example $x$ and an augmentation $x^{+}\\sim p(x\\mid x)$ of that same example (e.g., a random crop), we will follow prior work [65, 78] in using the time series dynamics to generate the positive pairs, so $x^{+}$ will be an observation that occurs temporally after $x$ . While our experiments will sample positive examples from the discounted state occupancy measure $(x^{+}\\sim p_{t+}(\\bar{x}_{t+}\\mid x))$ in line with prior work [29], our analysis will also apply to different distributions (e.g., always sampling a state $k$ steps ahead). ", "page_idx": 3}, {"type": "text", "text": "While prior work typically constrains the representations to have a constant norm (i.e., to lie on the unit hypersphere) [65], we will instead constrain the expected norm of the representations is bounded, a difference that will be important for our analysis: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{k}\\,\\mathbb{E}_{p(x)}\\left[\\|\\psi(x)\\|_{2}^{2}\\right]\\leq c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Because the norm scales with the dimension of the representation, we have scaled down the left side by the representation dimension, $k$ . In practice, we will impose this constraint by adding a regularization term $\\lambda\\mathbb{E}_{p(x)}\\left[\\|\\psi(x)\\|_{2}^{2}\\right]$ to the infoNCE objective (Eq. 2) and dynamically tuning the weight $\\lambda$ via dual gradient descent. ", "page_idx": 3}, {"type": "text", "text": "3.1 Key assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section outlines the two key assumptions behind our analysis, both of which have some theoretical justification. Our main assumption examines the distribution over representations: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. Regularized, temporal contrastive learning acquires representations whose marginal distribution representations $\\begin{array}{r}{p(\\psi)\\triangleq\\int p(x)\\,\\mathbb{1}(\\psi(x)=\\psi)\\,\\mathrm{d}x}\\end{array}$ is an isotropic Gaussian distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\psi)={\\mathcal{N}}(\\psi;\\mu=0,\\sigma=c\\cdot I).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Appendix A.1 we extend prior work [90] provide some theoretical intuition for why this assumption should hold: namely, that the isotropic Gaussian is the distribution that maximizes entropy subject to an expected L2 norm constraint (Eq. 3) [22, 41, 79]. Our analysis also assumes that the learned representations converge to the theoretical minimizer of the infoNCE objective: ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. Applying contrastive learning to the symmetrized infoNCE objective results in representations that encode a probability ratio: ", "page_idx": 3}, {"type": "equation", "text": "$$\ne^{-\\frac{1}{2}\\|\\phi(x_{0})-\\psi(x)\\|_{2}^{2}}=\\frac{p_{t+}(x_{t+}=x\\mid x_{0})}{p(x)C}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This assumption holds under ideal conditions [54, 69] (see Appendix A.5),5 but we nonetheless call this an \u201cassumption\u201d because it may not hold in practice due to sampling and function approximation error. This assumption means the learned representations are sufficient statistics for predicting the probability (ratio) of future states: these representations must retain all the information pertinent to reasoning about temporal relationships, but need not retain information about the precise contents of the observations. As such, they may be much more compressed than representations learned via reconstruction. ", "page_idx": 4}, {"type": "text", "text": "Combined, these assumptions will allow us to express the distribution over sequences of representations as a Gauss-Markov chain. The denominator in Assumption 2, $p(x)$ , may have a complex distribution, but Assumption 1 tells us that the distribution over representations has a simpler form. This will allow us to rearrange Assumption 2 to express the conditional distribution over representations as the product of two Gaussian likelihoods. Note that the left hand side of Assumption 2 already looks like a Gaussian likelihood. ", "page_idx": 4}, {"type": "text", "text": "4 Contrastive Representations Make Inference Easy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, our main result will be to show how representations learned by (regularized) contrastive learning are distributed according to a Gauss-Markov chain, making it straightforward to perform inference (e.g., planning, prediction) over these representations. Our proof technique will combine (known) results about Gaussian distributions with (known) results about contrastive learning. We start by discussing an important choice of parametrization (Section 4.1) that facilitates prediction (Section 4.2) before presenting the main result in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "4.1 A Parametrization for Shared Encoders ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section describes the two encoders $(\\psi(\\cdot),\\phi(\\cdot))$ to compute representations of $x$ and $x^{+}$ . While prior work in computer vision and NLP literature use the same encoder for both $x$ and $x^{+}$ , this decision does not make sense for many time-series data as it would imply that our prediction for $p(x_{t}\\mid x_{0}\\right)$ is the same as our prediction for $p(x_{0}\\mid x_{t})$ . However, the difficulty of transiting from $x_{0}$ to $x_{t}$ (e.g., climbing to the peak of a mountain) might be more difficult than the reverse (e.g., sledding down a mountain). Our proposed parametrization will handle this asymmetry. ", "page_idx": 4}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/790089adf5633a3b77821d109b55f88b9b8bf9416e7d9e914c8f6fcfed6c4649.jpg", "img_caption": ["Figure 2: A parametrization for temporal contrastive learning. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We will treat the encoder $\\psi(\\cdot)$ as encoding the contents of the state. We will additionally learn a matrix $A$ so that the function $\\psi\\mapsto A\\psi$ corresponds to a (multi-step) prediction of the future representation. To map this onto contrastive learning, we will use $\\phi(x)\\triangleq A\\psi(x)$ as the encoder for the initial state. One way of interpreting this encoder is as an additional linear projection applied on top of $\\psi(\\cdot)$ , a design similar to those used in other areas of contrastive learning [17]. Once learned, we can use these encoders to answer questions about prediction (Section 4.2) and planning (Section 4.3). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.2 Representations Encode a Predictive Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given an initial state $x_{0}$ , what states are likely to occur in the future? Answering this question directly in terms of high-dimensional states is challenging, but our learned representations provide a straightforward answer. Let $\\psi_{0}\\,=\\,\\psi(x_{0})$ and $\\psi_{t+}\\,=\\,\\psi(x_{t+})$ be random variables representing the representations of the initial state and a future state. Our aim is to estimate the distribution over these future representations, $p(\\psi_{t+}\\mid\\psi_{0})$ . We will show that the learned representations encode this distribution. ", "page_idx": 4}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/509a8942ba8dbab4f0a6c1b6e804a5819944a3c6cfa6fef2d197a5541438bfc6.jpg", "img_caption": ["Figure 3: Predicting representations of future states. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Lemma 1. Under the assumptions from Section 3, the distribution over representations of future states follows a Gaussian distribution with mean parameter given by the initial state representation: ", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\psi_{t+}=\\psi\\mid\\psi_{0})=\\mathcal{N}\\Big(\\mu=\\frac{c}{c+1}A\\psi_{0},\\Sigma=\\frac{c}{c+1}I\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "5While the result of Ma and Collins [54] has $C(x)$ depending on $_x$ , the symmetrized version [71] removes the dependence on $_x$ . ", "page_idx": 4}, {"type": "text", "text": "The main takeaway here is that the distribution over future representations has a convenient, closed form solution. The representation norm constraint, $c$ , determines the shrinkage factor $\\textstyle{\\frac{c}{c+1}}\\in[0,1)$ ; highly regularized settings (small ) move the mean closer towards the origin and decrease the variance, as visualized in Fig. 3. Regardless of the constraint $c$ , the predicted mean is a linear function $\\psi\\mapsto{\\textstyle{\\frac{c}{c+1}}}A\\psi$ . The proof is in Appendix A.2. The proof technique is similar to that of the law of the unconscious statistician. ", "page_idx": 5}, {"type": "text", "text": "4.3 Planning over One Intermediate State ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now show how these representations can be used for a specific type of planning: given an initial state $x_{0}$ and a future state $x_{t+}$ , infer the representation of an intermediate \u201cwaypoint\u201d state $x_{w}$ . The next section will extend this analysis to inferring the entire sequence of intermediate states. We assume $x_{0}\\,\\to\\,x_{w}\\,\\to\\,x_{t+}$ form a Markov chain where $x_{w}\\,\\sim\\,p(x_{t+}\\mid\\,x_{0}\\,=\\,x_{0})$ and $x_{t+}\\sim p(x_{t+}\\mid x_{0}=x_{w})$ are both drawn from the discounted state occupancy measure (Eq. 1). Let random variable $\\psi_{w}=\\dot{\\psi}(x_{w})$ be the representation of this intermediate state. Our main result is that the posterior distribution over waypoint representations has a closed form solution in terms of the initial state representation and future state representation: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Under Assumptions $^{\\,l}$ and 2, the posterior distribution over waypoint representations is a Gaussian whose mean and covariance are linear functions of the initial and final state representations: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\psi_{w}\\mid\\psi_{0},\\psi_{t+})=\\mathcal{N}\\Big(\\psi_{w};\\mu=\\Sigma(A^{T}\\psi_{t+}+A\\psi_{0}),\\Sigma^{-1}\\!=\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof (Appendix A.3) uses the Markov property together with Lemma 1. The main takeaway from this lemma is that the posterior distribution takes the form of a simple probability distribution (a Gaussian) with parameters that are linear functions of the initial and final representations. ", "page_idx": 5}, {"type": "text", "text": "We give three examples to build intuition: ", "page_idx": 5}, {"type": "text", "text": "Example 1: $A=I$ and the $c$ is very large (little regularization). Then, the covariance is $\\Sigma^{-1}\\approx2I$ and the mean is the simple average of the initial and final representations $\\begin{array}{r}{\\mu\\approx\\frac{1}{2}(\\psi_{0}+\\psi_{t+})}\\end{array}$ . In other words, the waypoint representation is the midpoint of the line $\\psi_{0}\\rightarrow\\psi_{t+}$ . ", "page_idx": 5}, {"type": "text", "text": "Example 2: $A$ is a rotation matrix and $c$ is very large. Rotation matrices satisfy $A^{T}=A^{-1}$ so the covariance is again $\\Sigma^{-1}\\approx2I$ . As noted in Section 4.2, we can interpret $A\\psi_{0}$ as a prediction of which representations will occur after $\\psi_{0}$ . Similarly, $A^{-1}\\psi_{t+}=A^{T}\\dot{\\psi_{t+}}$ is a prediction of which representations will occur before $\\psi_{t+}$ . Theorem 4.1 tells us that the mean of the waypoint distribution is the simple average of these two predictions, $\\begin{array}{r}{\\mu\\approx\\frac{1}{2}(A^{T}\\psi_{t+}+A\\psi_{0})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Example 3: $A$ is a rotation matrix and $c=0.01$ (very strong regularization). In this case $\\Sigma^{-1}=$ $\\begin{array}{r}{\\frac{0.01}{0.01+1}\\sp{\\bullet}A^{T}A\\sp+\\frac{0.01+1}{0.01}I\\approx100I}\\end{array}$ , so $\\begin{array}{r}{\\mu\\approx\\frac{1}{100}(\\psi_{0}\\!+\\!\\psi_{t+})\\stackrel{.}{\\approx}0}\\end{array}$ . Thus, in the case of strong regularization, the posterior concentrates around the origin. ", "page_idx": 5}, {"type": "text", "text": "4.4 Planning over Many Intermediate States ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section extends the analysis to multiple intermediate states. Again, we will infer the posterior distribution of the representations of these intermediate states, $\\psi_{w_{1}},\\psi_{w_{2}},\\cdot\\cdot\\cdot$ . We assume that these states form a Markov chain. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. Given observations from a Markov chain $x_{0}\\rightarrow x_{1}\\cdot\\cdot\\cdot x_{t+}$ , the joint distribution over representations is a Gaussian distribution. Using $\\psi_{1:n}=(\\psi_{w_{1}},\\cdot\\cdot\\cdot\\,,\\psi_{w_{n}})$ to denote the concatenated representations of each observation, we can write this distribution as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\psi_{1:n})\\propto\\exp\\!\\left(-\\frac{1}{2}\\psi_{1:n}^{T}\\Sigma^{-1}\\psi_{1:n}+\\eta^{T}\\psi_{1:n}\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Sigma^{-1}$ is a tridiagonal matrix ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Sigma^{-1}=\\left(\\!\\!\\begin{array}{c c}{{\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I}}&{{-A^{T}}}\\\\ {{-A}}&{{\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I-A^{T}}}\\\\ {{A^{T}\\psi_{t+}}}\\end{array}\\!\\!\\right)\\quad a n d\\,\\eta=\\left(\\!\\!\\begin{array}{c}{{A\\psi_{0}}}\\\\ {{0}}\\\\ {{\\vdots}}\\\\ {{A^{T}\\psi_{t+}}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This distribution can be written in the canonical parametrization as $\\Sigma=\\Lambda^{-1}$ and $\\mu=\\Sigma\\eta$ . Recall that Gaussian distributions are closed under marginalization. Thus, once in this canonical parametrization, ", "page_idx": 5}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/449bd77a7d51a152f0c0299ffc9d3c6e02ffc5ee1cdd3ffb99fe00e5a224fb50.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: Numerical simulation of our analysis. (Top Left) Toy dataset of time-series data consisting of many outwardly-spiraling trajectories. We apply temporal contrastive learning to these data. (Top Right) For three initial observations $(\\boxed{\\Pi})$ , we use the learned representations to predict the distribution over future observations. Note that these distributions correctly capture the spiral structure. (Bottom Left) For three observations $(\\star)$ , we use the learned representations to predict the distribution over preceding observations. (Bottom Right) Given an initial and final observation, we plot the inferred posterior distribution over the waypoint (Section 4.3). The representations capture the shape of the distribution. ", "page_idx": 6}, {"type": "text", "text": "the marginal distributions can be obtained by reading off individual entries of these parameters: ", "page_idx": 6}, {"type": "equation", "text": "$$\np(\\psi_{i}\\mid\\psi_{0},\\psi_{t+})=\\mathcal{N}\\left(\\psi_{i};\\mu_{i}=(\\Sigma\\eta)^{(i)},\\Sigma_{i}=(\\Lambda^{-1})^{(i,i)}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The key takeaway here is that this posterior distribution over waypoints is Gaussian, and it has a closed form expression in terms of the initial and final representations (as well as regularization parameter $c$ and the learned matrix $A$ ). ", "page_idx": 6}, {"type": "text", "text": "In the general case of $n$ intermediate states, the posterior distribution is ", "page_idx": 6}, {"type": "equation", "text": "$$\np(\\psi_{w_{1}}\\cdot\\cdot\\cdot\\psi_{w_{n}}\\mid\\psi_{0},\\psi_{t+})\\propto e^{-\\frac{1+\\frac{1}{c}}{2}\\sum_{i=1}^{n}\\|\\frac{c}{c+1}A\\psi_{w_{i}}-\\psi_{w_{i+1}}\\|_{2}^{2}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\psi_{w_{0}}=\\psi_{0}$ and $\\psi_{w_{n+1}}=\\psi_{t+}$ . This corresponds to a chain graphical model with edge potentials $f(\\psi,\\psi^{\\prime})=e^{-\\frac{1+\\frac{1}{c}}{2}\\|\\frac{c}{c+1}A\\psi-\\psi^{\\prime}\\|_{2}^{2}}$ ", "page_idx": 6}, {"type": "text", "text": "Special case. To build intuition, consider the special case where $A$ is a rotation matrix and $c$ is very large, so $\\begin{array}{r}{\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}\\approx2I}\\end{array}$ . In this case, $\\dot{\\Sigma}^{-1}$ is a (block) second difference matrix [38]: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Sigma^{-1}=\\left(\\!\\!\\begin{array}{c}{{2I\\ -I}}\\\\ {{-I\\ ^{2I}\\ -I}}\\\\ {{}}\\\\ {{}}\\end{array}\\!\\!\\cdot\\!\\!\\phantom{\\Sigma^{-1}+\\Sigma^{-1}}\\!\\!\\!\\cdot\\!\\!\\phantom{\\Sigma^{-1}+\\Sigma^{-1}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The inverse of this matrix has a closed form solution [62, Pg. 471], allowing us to obtain the mean of each waypoint in closed form: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mu_{i}=(1-\\lambda(i))A\\psi_{0}+\\lambda(i)A^{T}\\psi_{t+},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\lambda(i)=\\frac{i}{n+1}}\\end{array}$ . Thus, each posterior mean is a convex combination of the (forward prediction from the) initial representation and the (backwards prediction from the) final representation. When is the identity matrix, the posterior mean is simple linear interpolation between the initial and final representations! ", "page_idx": 6}, {"type": "text", "text": "5 Numerical Simulation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We include several didactic experiments to illustrate our results. All results and figures can be reproduced by running make in the source code: https://github.com/vivekmyers/contrastive_ planning. The expected compute time is a few hours on a A6000 GPU. Figures in this section show error across different training and dataset split seeds. ", "page_idx": 6}, {"type": "text", "text": "5.1 Synthetic Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate our analysis, we design a time series task with 2D points where inference over intermediate points (i.e., in-filling) requires nonlinear interpolation. Fig. 4 (Top Left) shows the dataset of time series data, starting at the origin and spiraling outwards, with each trajectory using a randomly-chosen initial angle. We applied contrastive learning with the parametrization in Section 4.1 to these data and used the learned representations to solve prediction and planning problems (see Fig. 4 for details). Note that these predictions correctly handle the nonlinear structure of these data \u2013 states nearby the initial state in Euclidean space that are not temporally adjacent are assigned low likelihood. ", "page_idx": 6}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/a9a8f2ee3ef187e594fa4c36063693e5c924a6054d37a8578863dfbc48a5568f.jpg", "img_caption": ["Figure 5: Using inferred paths over our contrastive representations for control boosts success rates by $4.5\\times$ on the most difficult goals $(18\\%\\,\\rightarrow\\,84\\%)$ ). Alternative representation learning techniques fail to improve performance when used for planning. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Solving Mazes with Inferred Representations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our next experiment studies whether the inferred representations are useful for solving a control task. We took a 2d maze environment and dataset from prior work (Fig. 5, Left) [31] and learned encoders from this dataset. To solve the maze, we take the observation of the starting state and goal state, compute the representations of these states, and use the analysis in Section 4.3 to infer the sequence of intermediate representations. We visualize the results using a nearest neighbor retrieval (Fig. 5, Left). Appendix Fig. 7 contains additional examples. ", "page_idx": 7}, {"type": "text", "text": "Finally, we studied whether these representations are useful for control. We implemented a simple proportional controller for this maze. As expected, this proportional controller can successfully navigate to close goals, but fails to reach distant goals (Fig. 5, Right). However, if we use the proportional controller to track a series of waypoints planned using our representations (i.e., the orange dots shown in Fig. 5 (Left)), the success rate increases by up to $4.5\\times$ . To test the importance of nonlinear representations, we compare with a \u201cPCA\u201d baseline that predicts waypoints by interpolating between the principal components of the initial state and goal state. The better performance of our method indicates the importance of doing the interpolation using representations that are nonlinear functions of the input observations. While prior methods learn representations to encode temporal distances, it is unclear whether these methods support inference via interpolation. To test this hypothesis, we use one of these methods (\u201cVIP\u201d [51]) as a baseline. While the VIP representations likely encode similar bits as our representations, the better performance of the contrastive representations indicates that the VIP representations do not expose those bits in a way that makes planning easy. ", "page_idx": 7}, {"type": "text", "text": "5.3 Higher dimensional tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section we provide preliminary experiments showing the planning approach in Section 4 scales to higher dimensional tasks. We used two datasets from prior work [31]: door-human-v0 (39-dimensional observations) and hammer-human-v0 (46-dimensional observations). After learning encoders on these tasks, we evaluated the inference capabilities of the learned representations. Given the first and last observation from a trajectory in a validation set, we use linear interpolation (see Eq. (7)) to infer the representation of five intermediate waypoint representations. ", "page_idx": 7}, {"type": "text", "text": "We evaluate performance in two ways. Quantitatively, we measure the mean squared error between each of the true waypoint observations and those inferred by our method. Since our method infers representations, rather than observations, we use a nearest-neighbor retrieval on a validation set so that we can measure errors in the space of observations. Qualitatively, we visualize the high-dimensional observations from the validation trajectory using a 2-dimensional TSNE [89] embedding, overlying the infer waypoints from our method; as before, we convert the representations inferred by our method to observations using nearest neighbors. ", "page_idx": 7}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/9694f34ed93e969d3be12922bbb5380d2ce2dea2b96ea8394a734c24b15b6370.jpg", "img_caption": ["Figure 6: Planning for 39-dimensional robotic door opening. (Top Left) We use a dataset of trajectories demonstrating door opening from prior work [31] to learn representations. (Top Right) We use our method and three baselines to infer one intermediate waypoint between the first and last observation in a trajectory from a held-out validation set. Errors are measured using the mean squared error with the true waypoint observation; predicted representations are converted to observations using nearest neighbors on a validation set. (Bottom) We visualize a TSNE [89] of the states along the sampled trajectory as blue circles, with the transparency indicating the index along the trajectory. The inferred plan is shown as red circles connected by arrows. Our method generates better plans than alternative representation learning methods (PCA, VIP). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We compare with three alternative methods in Fig. 6. To test the importance of representation learning, we first na\u00efvely interpolate between the initial and final observations (\u201cno planning\u201d). The poor performance of this baseline indicates that the input time series are highly nonlinear. Similarly, interpolating the principle components of the initial and final observations (\u201cPCA\u201d) performs poorly, again highlighting that the input time series is highly nonlinear and that our representations are doing more than denoising (i.e., discarding directions of small variation). The third baseline, \u201cVIP\u201d [51], learns representations to encode temporal distances using approximate dynamic programming. Like our method, VIP avoids reconstruction and learns nonlinear representations of the observations. However, the results in Fig. 6 highlight that VIP\u2019s representations do not allow users to plan by interpolation. The error bars shown in Fig. 6 (Top Right) show the standard deviation over 500 trajectories sampled from the validation set. For reproducibility, we repeated this entire experiment on another task, the 46-dimensional hammer-human-v0 from D4RL. The results, shown in Appendix Fig. 8, support the conclusions above. Taken together, these results show that our procedure for interpolating contrastive representations continues to be effective on tasks where observations have dozens of dimensions. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Representation learning is at the core of many high-dimensional time-series modeling questions, yet how those representations are learned is often disconnected with the inferential task. The main contribution of this paper is to show how discriminative techniques can be used to acquire compact representations that make it easy to answer inferential questions about time. The precise objective and parametrization we studied is not much different from that used in practice, suggesting that either our theoretical results might be adapted to the existing methods, or that practitioners might adopt these details so they can use the closed-form solutions to inference questions. Our work may also have implications for studying the structure of learned representations. While prior work often studies the geometry of representations as a post-hoc check, our analysis provides tools for studying when interpolation properties are guaranteed to emerge, as well as how to learn representations with certain desired geometric properties. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations. Our analysis hinges on the two assumptions mentioned in Section 3.1, and it remains open how errors in those approximations translate into errors in our analysis. One important open question is whether it is always possible to satisfy these assumptions using sufficiently-expressive representations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. We thank Seohong Park, Gautam Reddy, Chongyi Zheng, and anonymous reviewers for feedback and discussions that shaped this project. This work was supported by Princeton Research Computing resources at Princeton University. This work was partially supported by AFOSR FA9550-22-1-0273. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages 10\u20134, 2019.   \n[2] Cameron S. Allen. Learning markov state abstractions for deep reinforcement learning. In Neural Information Processing Systems, 2021. [3] Carl Allen and Timothy Hospedales. Analogies explained: Towards understanding word embeddings. In International Conference on Machine Learning, pages 223\u2013231. PMLR, 2019. [4] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R Devon Hjelm. Unsupervised state representation learning in atari. Advances in neural information processing systems, 32, 2019. [5] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. Transactions of the Association for Computational Linguistics, 4:385\u2013399, 2016. [6] Hagai Attias. Planning by Probabilistic Inference. In International Workshop on Artificial Intelligence and Statistics, pages 9\u201316. PMLR, January 2003.   \n[7] Ershad Banijamali, Rui Shu, Hung Bui, Ali Ghodsi, et al. Robust locally-linear controllable embedding. In International Conference on Artificial Intelligence and Statistics, pages 1751\u20131759. PMLR, 2018.   \n[8] Andr\u00e9 Barreto, Will Dabney, R\u00e9mi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017. [9] Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, and Sergey Levine. Information prioritization through empowerment in visual model-based rl. In International Conference on Learning Representations, 2021.   \n[10] Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in cognitive sciences, 16(10): 485\u2013488, 2012.   \n[11] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015.   \n[12] Micah Carroll, Orr Paradise, Jessy Lin, Raluca Georgescu, Mingfei Sun, David Bignell, Stephanie Milani, Katja Hofmann, Matthew Hausknecht, Anca Dragan, et al. UniMASK: Unified Inference in Sequential Decision Problems, November 2022.   \n[13] Pablo Samuel Castro, Tyler Kastner, P. Panangaden, and Mark Rowland. Mico: Improved representations via sampling-based state similarity for markov decision processes. In Neural Information Processing Systems, 2021.   \n[14] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Goal-conditioned reinforcement learning with imagined subgoals. In International Conference on Machine Learning, pages 1430\u20131440. PMLR, 2021.   \n[15] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.   \n[16] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[17] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.   \n[18] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.   \n[19] Ying-Cong Chen, Xiaogang Xu, Zhuotao Tian, and Jiaya Jia. Homomorphic Latent Space Interpolation for Unpaired Image-To-Image Translation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2403\u20132411, Long Beach, CA, USA, June 2019. IEEE. ISBN 978-1-72813-293-8. doi: 10.1109/CVPR.2019.00251.   \n[20] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A Recurrent Latent Variable Model for Sequential Data. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.   \n[21] C\u00e9dric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Intrinsically motivated goalconditioned reinforcement learning: a short survey. preprint, 2021.   \n[22] Keith Conrad. Probability distributions and maximum likelihood, 2010. URL https://api. semanticscholar.org/CorpusID:11225251.   \n[23] Brandon Cui, Yinlam Chow, and Mohammad Ghavamzadeh. Control-aware representations for modelbased reinforcement learning. In International Conference on Learning Representations, 2020.   \n[24] Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5:613\u2013624, 1993.   \n[25] Yilun Du, Chuang Gan, and Phillip Isola. Curious representation learning for embodied intelligence. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10388\u201310397, 2021.   \n[26] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. In International Conference on Learning Representations, 2016.   \n[27] Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the Replay Buffer: Bridging Planning and Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[28] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve goals via recursive classification. In International Conference on Learning Representations, 2020.   \n[29] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learning as goal-conditioned reinforcement learning. Advances in Neural Information Processing Systems, 35: 35603\u201335620, 2022.   \n[30] Kuan Fang, Patrick Yin, Ashvin Nair, Homer Rich Walke, Gengchen Yan, and Sergey Levine. Generalization with lossy affordances: Leveraging broad offline data for learning visuomotor tasks. In Conference on Robot Learning, pages 106\u2013117. PMLR, 2023.   \n[31] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[32] Raj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, and Russ Salakhutdinov. Simplifying model-based rl: Learning representations, latent-space models, and policies with one objective. In The Eleventh International Conference on Learning Representations, 2022.   \n[33] Ross Goroshin, Michael F Mathieu, and Yann LeCun. Learning to linearize under uncertainty. Advances in neural information processing systems, 28, 2015.   \n[34] Zhaohan Guo, Shantanu Thakoor, Miruna P\u00eeslar, Bernardo Avila Pires, Florent Altch\u00e9, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-explore: Exploration by bootstrapped prediction. Advances in neural information processing systems, 35:31855\u201331870, 2022.   \n[35] Michael Gutmann and Aapo Hyv\u00e4rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297\u2013304. JMLR Workshop and Conference Proceedings, March 2010.   \n[36] Tatsunori B Hashimoto, David Alvarez-Melis, and Tommi S Jaakkola. Word embeddings as metric recovery in semantic spaces. Transactions of the Association for Computational Linguistics, 4:273\u2013286, 2016.   \n[37] Joey Hejna, Jensen Gao, and Dorsa Sadigh. Distance weighted supervised learning for offline interaction data. arXiv preprint arXiv:2304.13774, 2023.   \n[38] Nick Higham. What is the second difference matrix? https://nhigham.com/2022/01/31/ what-is-the-second-difference-matrix/, 2022.   \n[39] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273\u20131286, 2021.   \n[40] Dinesh Jayaraman and Kristen Grauman. Slow and steady feature analysis: higher order temporal coherence in video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3852\u20133861, 2016.   \n[41] E. T. Jaynes. Information Theory and Statistical Mechanics. Physical Review, 106(4):620\u2013630, May 1957. doi: 10.1103/PhysRev.106.620.   \n[42] Hannes J\u00f3nsson, Greg Mills, and Karsten W Jacobsen. Nudged elastic band method for finding minimum energy paths of transitions. In Classical and quantum dynamics in condensed phase simulations, pages 385\u2013404. World Scientific, 1998.   \n[43] Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-Driven Representation Learning for Robotics, February 2023.   \n[44] Lydia E Kavraki, Petr Svestka, J-C Latombe, and Mark H Overmars. Probabilistic roadmaps for path planning in high-dimensional configuration spaces. IEEE transactions on Robotics and Automation, 12 (4):566\u2013580, 1996.   \n[45] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186, 2019.   \n[46] John E Laird, Allen Newell, and Paul S Rosenbloom. Soar: An architecture for general intelligence. Artificial intelligence, 33(1):1\u201364, 1987.   \n[47] Steven M LaValle, James J Kuffner, BR Donald, et al. Rapidly-exploring random trees: Progress and prospects. Algorithmic and computational robotics: new directions, 5:293\u2013308, 2001.   \n[48] Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations. In Proceedings of the eighteenth conference on computational natural language learning, pages 171\u2013180, 2014.   \n[49] Bo Liu, Yihao Feng, Qiang Liu, and Peter Stone. Metric residual network for sample efficient goalconditioned reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 8799\u20138806, 2023.   \n[50] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You. Data Augmentation via Latent Space Interpolation for Image Classification. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 728\u2013733, August 2018. doi: 10.1109/ICPR.2018.8545506.   \n[51] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. In The Eleventh International Conference on Learning Representations, 2022.   \n[52] Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i\u2019ll go: Offline goalconditioned reinforcement learning via $f$ -advantage regression. arXiv preprint arXiv:2206.03023, 2022.   \n[53] Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. LIV: LanguageImage Representations and Rewards for Robotic Control. International Conference on Machine Learning, May 2023.   \n[54] Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. In Conference on Empirical Methods in Natural Language Processing, 2018. URL https://api.semanticscholar.org/CorpusID:52171904.   \n[55] Steven R Majewski, Ricardo P Schiavon, Peter M Frinchaboy, Carlos Allende Prieto, Robert Barkhouser, Dmitry Bizyaev, Basil Blank, Sophia Brunner, Adam Burton, Ricardo Carrera, et al. The apache point observatory galactic evolution experiment (apogee). The Astronomical Journal, 154(3):94, 2017.   \n[56] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.   \n[57] Dmitry M Malioutov, Jason K Johnson, and Alan S Willsky. Walk-sums and belief propagation in gaussian graphical models. The Journal of Machine Learning Research, 7:2031\u20132064, 2006.   \n[58] Bogdan Mazoure, Benjamin Eysenbach, Ofir Nachum, and Jonathan Tompson. Contrastive value learning: Implicit models for simple offline rl. In Conference on Robot Learning, pages 1257\u20131267. PMLR, 2023.   \n[59] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.   \n[60] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. In Conference on Robot Learning, pages 892\u2013909. PMLR, 2023.   \n[61] Allen Newell, John C Shaw, and Herbert A Simon. Report on a general problem solving program. In IFIP congress, volume 256, page 64. Pittsburgh, PA, 1959.   \n[62] Morris Newman and John Todd. The evaluation of matrix inversion programs. Journal of the Society for Industrial and Applied Mathematics, 6(4):466\u2013476, 1958.   \n[63] Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Non-markovian predictive coding for planning in latent space, 2020.   \n[64] Tung D Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for model-based planning in latent space. In International Conference on Machine Learning, pages 8130\u20138139. PMLR, 2021.   \n[65] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[66] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \n[67] Alon Oring, Zohar Yakhini, and Yacov Hel-Or. Autoencoder image interpolation by shaping the latent space. In International Conference on Machine Learning, pages 8281\u20138290. PMLR, 2021.   \n[68] Seongmin Park and Jihwa Lee. Finetuning Pretrained Transformers into Variational Autoencoders, November 2021.   \n[69] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171\u20135180. PMLR, 2019.   \n[70] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal Contrastive Video Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6964\u20136974, 2021.   \n[71] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[72] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019.   \n[73] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[74] Wouter Saelens, Robrecht Cannoodt, Helena Todorov, and Yvan Saeys. A comparison of single-cell trajectory inference methods. Nature biotechnology, 37(5):547\u2013554, 2019.   \n[75] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, pages 5628\u20135637. PMLR, 2019.   \n[76] Yannick Schroecker and Charles Isbell. Universal value density estimation for imitation learning and goal-conditioned reinforcement learning. arXiv preprint arXiv:2002.06473, 2020.   \n[77] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2020.   \n[78] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1134\u20131141. IEEE, 2018.   \n[79] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379\u2013423, 1948.   \n[80] Rui Shu, Tung Nguyen, Yinlam Chow, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano Ermon, and Hung Bui. Predictive coding for locally-linear control. In International Conference on Machine Learning, pages 8862\u20138871. PMLR, 2020.   \n[81] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. Advances in neural information processing systems, 29, 2016.   \n[82] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In International Conference on Machine Learning, pages 9870\u20139879. PMLR, 2021.   \n[83] Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila Pires, Yash Chandak, R\u00e9mi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan, Clare Lyle, et al. Understanding self-predictive learning for reinforcement learning. In International Conference on Machine Learning, pages 33632\u201333656. PMLR, 2023.   \n[84] Evangelos Theodorou, Jonas Buchli, and Stefan Schaal. A generalized path integral control approach to reinforcement learning. The Journal of Machine Learning Research, 11:3137\u20133181, 2010.   \n[85] Sep Thijssen and H. J. Kappen. Path integral control and state-dependent feedback. Physical Review E, 91(3):032104, March 2015. ISSN 1539-3755, 1550-2376. doi: 10.1103/PhysRevE.91.032104.   \n[86] Stephen Tian, Suraj Nair, Frederik Ebert, Sudeep Dasari, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Model-based visual planning with self-supervised functional distances. In International ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Conference on Learning Representations, 2020. ", "page_idx": 13}, {"type": "text", "text": "[87] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 776\u2013794. Springer, 2020.   \n[88] George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical review, 36 (5):823, 1930.   \n[89] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[90] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929\u20139939. PMLR, 2020.   \n[91] Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching reinforcement learning via quasimetric learning. In International Conference on Machine Learning. PMLR, 2023.   \n[92] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. Advances in neural information processing systems, 28, 2015.   \n[93] Yair Weiss and William Freeman. Correctness of belief propagation in gaussian graphical models of arbitrary topology. Advances in neural information processing systems, 12, 1999.   \n[94] Grady Williams, Andrew Aldrich, and Evangelos Theodorou. Model Predictive Path Integral Control using Covariance Variable Importance Sampling, October 2015.   \n[95] Laurenz Wiskott and Terrence J. Sejnowski. Slow Feature Analysis: Unsupervised Learning of Invariances. Neural Computation, 14(4):715\u2013770, April 2002. ISSN 0899-7667. doi: 10.1162/089976602317318938.   \n[96] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733\u20133742, 2018.   \n[97] Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, and Shuran Song. Xskill: Cross embodiment skill discovery. In Conference on Robot Learning, pages 3536\u20133555. PMLR, 2023.   \n[98] Jia-Wei Yan, Ci-Siang Lin, Fu-En Yang, Yu-Jhe Li, and Yu-Chiang Frank Wang. Semantics-Guided Representation Learning with Applications to Visual Synthesis. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 7181\u20137187, January 2021. doi: 10.1109/ICPR48806.2021.9413171.   \n[99] Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. In International Conference on Learning Representations, 2021.   \n[100] Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, and Joseph E Gonzalez. C-planning: An automatic curriculum for learning goal-reaching tasks. In International Conference on Learning Representations, 2021.   \n[101] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational autoencoding models. arXiv preprint arXiv:1702.08658, 2017.   \n[102] Chongyi Zheng, Benjamin Eysenbach, Homer Rich Walke, Patrick Yin, Kuan Fang, Ruslan Salakhutdinov, and Sergey Levine. Stabilizing contrastive RL: Techniques for robotic goal reaching from offline data. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=Xkf2EBj4w3.   \n[103] Yizhe Zhu, Martin Renqiang Min, Asim Kadav, and Hans Peter Graf. S3vae: Self-supervised sequential vae for representation disentanglement and data generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6538\u20136547, 2020.   \n[104] Yizhe Zhu, Martin Renqiang Min, Asim Kadav, and Hans Peter Graf. S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation, May 2020. ", "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Marginal Distribution over Representations is Gaussian ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The infoNCE objective (Eq. (2)) can be decomposed into an alignment term and a uniformity term [90], where the uniformity term can be simplified as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x\\sim p(x)}\\left[\\log\\mathbb{E}_{x^{-}\\sim p(x)}\\left[e^{-\\frac{1}{2}\\|A\\psi(x^{-})-\\psi(x)\\|^{2}}\\right]\\right]}\\\\ &{=\\frac{1}{N}\\sum_{i=1}^{N}\\log\\left(\\frac{1}{N-1}\\sum_{j=1,\\cdots N,j\\neq i}^{N}\\epsilon^{-\\frac{1}{2}\\|A\\psi(x_{i})-\\psi(x_{j})\\|^{2}}\\right)}\\\\ &{=\\frac{1}{N}\\sum_{i=1}^{N}\\log\\left(\\frac{1}{N-1}\\sum_{j=1,\\cdots N,j\\neq i}^{N}\\frac{1}{\\sum_{j=1}^{N/2}\\|A\\psi(x_{j})\\|^{2}}e^{-\\frac{1}{2}\\|A\\psi(x_{i})-\\psi(x_{j})\\|_{2}^{2}}\\right)+\\frac{k}{2}\\log(2\\pi)}\\\\ &{=\\frac{1}{N}\\sum_{i=1}^{N}\\log\\hat{p}_{\\mathrm{ound}}(\\psi(x_{i}))+\\frac{k}{2}\\log(2\\pi)}\\\\ &{=-\\hat{\\mathcal{H}}[\\psi(x)]+\\frac{k}{2}\\log(2\\pi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The derivation above extends that in Wang and Isola [90] by considering a Gaussian distribution rather than a von Mises Fisher distribution. We are implicitly making the assumption that the marginal distributions satisfy $p(x)=p(x^{-})$ . This difference corresponds to our choice of using a negative squared L2 distance in the infoNCE loss rather than an inner product, a difference that will be important later in our analysis. A second difference is that we do not use the resubstitution estimator (i.e., we exclude data point $x_{i}$ from our estimate of $\\hat{p}_{\\mathrm{GMM}}$ when evaluating the likelihood of $x_{i}$ ), which we found hurt performance empirically. The takeaway from this identity is that maximizing the uniformity term corresponds to maximizing (an estimate of) the entropy of the representations. ", "page_idx": 14}, {"type": "text", "text": "We next prove that the maximum entropy distribution with an expected L2 norm constraint is a Gaussian distribution. Variants of this result are well known [22, 41, 79], but we include a full proof here for transparency. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. The maximum entropy distribution satisfying the expected $L2$ norm constraint in $E q$ . (3) is a multivariate Gaussian distribution with mean $\\mu=0$ and covariance $\\Sigma=c\\cdot I$ ", "page_idx": 14}, {"type": "text", "text": "Proof. We start by defining the corresponding Lagrangian, with the second constraint saying that $p(x)$ must be a valid probability distribution. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(p)=\\mathcal{H}_{p}[x]+\\lambda_{1}\\left(\\mathbb{E}_{p(x)}\\left[\\|x\\|_{2}^{2}\\right]-c\\cdot k\\right)+\\lambda_{2}\\left(\\int p(x)\\,\\mathrm{d}x-1\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We next take the derivative w.r.t. $p(x)$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\partial{\\mathcal{L}}}{\\partial p(x)}}=-p(x)/p(x)-\\log p(x)+\\lambda_{1}\\|x\\|_{2}^{2}+\\lambda_{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Setting this derivative equal to 0 and solving for $p(x)$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\np(x)=e^{-1+\\lambda_{2}+\\lambda_{1}\\|x\\|_{2}^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We next solve for $\\lambda_{1}$ and $\\lambda_{2}$ to satisfy the constraints in the Lagrangian. Note that $x\\sim\\mathcal{N}(\\mu=$ $0,\\Sigma=c\\cdot I)$ has an expected norm $\\mathbb{E}[\\|x\\|_{2}^{2}]=c\\cdot k$ , so we must have $\\begin{array}{r}{\\bar{\\lambda}_{1}=-\\frac{1}{2c}}\\end{array}$ . We determine $\\lambda_{1}$ as the normalizing constant for a Gaussian, finally giving us: ", "page_idx": 14}, {"type": "equation", "text": "$$\np(x)=\\frac{1}{(2c\\pi)^{k/2}}e^{\\frac{-1}{2c}\\|x\\|_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "corresponding to an isotropic Gaussian distribution with mean $\\mu=0$ and covariance $\\Sigma=c\\cdot I$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Below we present the proof of Lemma 1 ", "page_idx": 15}, {"type": "text", "text": "Proof. Our proof technique will be similar to that of the law of the unconscious statistician: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\psi_{i+1}\\mid\\psi_{0})\\triangleq\\frac{\\gamma(y_{+}\\mid\\psi_{0})}{\\beta\\mu(\\psi_{+})}\\propto\\iint p(\\psi_{i+1},x_{t+},\\psi_{0},x_{0})\\,\\mathrm{d}x_{t+}\\,\\mathrm{d}x_{0}}\\\\ &{\\xrightarrow[\\Phi]{\\(j)}f\\,p(\\psi_{i}\\mid x_{t+})p(\\psi_{0}\\mid x_{0})p(x_{t+}\\mid x_{0})p(x_{t+}\\,\\mathrm{d}x_{t+}\\,\\mathrm{d}x_{0}}\\\\ &{\\overset{(a)}{\\cong}\\int\\int\\mathrm{{J}}\\,(\\psi(x_{t+})=\\psi_{+})\\,\\mathrm{{l}}(\\psi(x_{0})=\\psi_{0})p(x_{t+}\\mid e^{-\\frac{1}{2}\\mathrm{l}}\\|\\psi(x_{0})-\\psi(x_{+}\\mid\\psi_{0})\\|_{\\mathcal{P}}^{2}(x_{0})\\,\\mathrm{d}x_{t+}\\,\\mathrm{d}x_{0}}\\\\ &{\\overset{(d)}{\\cong}\\epsilon^{-\\frac{1}{2}\\Vert A\\psi_{0}-\\psi_{+}\\Vert_{\\mathfrak{s}}^{2}}\\int\\mathrm{{J}}\\,\\mathrm{{J}}\\,\\mathrm{{1}}(\\psi(x_{t+})=\\psi_{t+})\\,\\mathrm{1}(\\psi(x_{0})=\\psi_{0})p(x_{t+})p(x_{0})\\,\\mathrm{d}x_{t+}\\,\\mathrm{d}x_{0}}\\\\ &{\\overset{(e)}{\\cong}\\epsilon^{-\\frac{1}{2}\\Vert A\\psi_{0}-\\psi_{+}\\Vert_{\\mathfrak{s}}^{2}}\\underbrace{\\left\\langle\\int p(x_{t+})\\,\\mathrm{1}(\\psi(x_{t+}))\\,\\mathrm{d}x_{t+}\\right\\rangle}_{\\begin{array}{c}{p(\\psi_{t+})}\\\\ {\\vdots}\\\\ {p(\\psi_{t+})}\\end{array}\\,\\mathrm{~f~o~r~}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\psi(\\psi_{0})\\,\\mathrm{1}(\\psi(x_{0}),\\mathrm{d}x_{0})}\\\\ &{\\overset{(j)}{\\cong}\\epsilon^{-\\frac{1}{2}\\Vert A\\psi_{0}-\\psi_{+}\\Vert_{\\mathfrak{s}}^{2}}\\varepsilon^{-\\frac{1}{2}\\Vert\\psi_{+\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In $(a)$ we applied Bayes\u2019 Rule and removed the denominator, which is a constant w.r.t. $\\psi_{t+}$ . In $(b)$ we factored the joint distribution, noting that $\\psi_{t+}$ and $\\psi_{0}$ are deterministic functions of $x_{t+}$ and $x_{0}$ respectively, so they are conditionally independent from the other random variables. In $(c)$ we used Assumption 2 after solving for $p(x_{t+}\\mid x_{0})=p(x_{t+})e^{-{\\frac{1}{2}}\\|A\\psi(x_{0})-\\psi(x)\\|_{2}^{2}}$ . In $(d)$ we noted that when the integrand is nonzero, it takes on a constant value of $e^{-\\frac12\\|A\\psi_{0}-\\psi_{t+}\\|_{2}^{2}}$ , so we can move that constant outside the integral. In (e) we used the definition of the marginal representation distribution (Eq. 6). In $(f)$ we used Assumption 1 to write the marginal distributions $p(\\psi_{t+})$ and $p(\\psi_{0})$ as Gaussian distributions. We removed the normalizing constants, which are independent of $\\psi_{t+}$ . In $(g)$ we completed the square and then recognized the expression as the density of a multivariate Gaussian distribution. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.3 Proof of Theorem 4.1: Waypoint Distribution ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\psi_{w}\\mid\\psi_{0},\\psi_{t+})\\stackrel{(a)}{=}\\frac{p\\left(\\psi_{t+}\\mid\\psi_{w}\\right)p\\left(\\psi_{w}\\mid\\psi_{0}\\right)}{\\underline{{p}}\\left(\\psi_{t+}\\mid\\psi_{0}\\right)}}\\\\ &{\\stackrel{(b)}{\\propto}e^{-\\frac{1+\\frac{1}{c}}{2}\\|\\frac{c}{c+1}A\\psi_{w}-\\psi_{t+}\\|_{2}^{2}}e^{-\\frac{1+\\frac{1}{c}}{2}\\|\\frac{c}{c+1}A\\psi_{0}-\\psi_{w}\\|_{2}^{2}}}\\\\ &{\\stackrel{(c)}{\\propto}e^{-\\frac{1}{2}(\\psi_{w}-\\mu)^{T}\\Sigma^{-1}(\\psi_{w}-\\mu)}=\\mathcal{N}(\\psi_{w};\\mu,\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\Sigma^{-1}=\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I}\\end{array}$ and $\\mu=\\Sigma(A^{T}\\psi_{t+}+A\\psi_{0})$ . ", "page_idx": 15}, {"type": "text", "text": "In line $(a)$ we used the definition of the conditional distribution and then simplified the numerator using the Markov property. Line $(b)$ uses the Lemma 1. Line $(c)$ completes the square, the details of which are below: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\cdot\\cfrac{c+1}{c}\\bigg(\\bigg\\|\\cfrac{c}{c+1}A\\psi_{w}-\\psi_{t+}\\bigg\\|_{2}^{2}+\\bigg\\|\\cfrac{c}{c+1}A\\psi_{0}-\\psi_{w}\\bigg\\|_{2}^{2}\\bigg)}\\\\ &{\\qquad=\\frac{1}{2}\\cdot\\cfrac{c+1}{c}\\bigg(\\psi_{w}^{T}\\Big(\\cfrac{c}{c+1}A\\Big)^{T}\\Big(\\cfrac{c}{c+1}A\\Big)\\psi_{w}-2\\psi_{t+}^{T}\\Big(\\cfrac{c}{c+1}A\\Big)\\psi_{w}+\\underbrace{\\psi_{t+}^{T}\\psi_{t+}}_{+}+}\\\\ &{\\qquad\\qquad+\\underbrace{\\psi_{0}^{T}\\Big(\\cfrac{c}{c+1}A\\Big)^{T}\\Big(\\cfrac{c}{c+1}A\\Big)\\psi_{0}}_{=\\,\\dag}-2\\psi_{0}^{T}\\Big(\\cfrac{c}{c+1}A\\Big)^{T}\\psi_{w}+\\psi_{w}^{T}\\psi_{w}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\mathrm{const.}}{=}\\frac{1}{2}\\frac{c+1}{c}\\Bigg(\\psi_{w}^{T}\\bigg(\\Big(\\frac{c}{c+1}\\Big)^{2}A^{T}A+I\\bigg)\\psi_{w}-2\\frac{c}{c+1}\\big(A^{T}\\psi_{t+}+A\\psi_{0}\\big)^{T}\\psi_{w}\\Bigg)}\\\\ &{=\\frac{1}{2}\\psi_{w}^{T}\\bigg(\\underbrace{\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I}_{\\Sigma^{-1}}I\\bigg)\\psi_{w}-\\big(A^{T}\\psi_{t+}+A\\psi_{0}\\big)^{T}\\psi_{w}}\\\\ &{\\overset{\\mathrm{const.}}{=}(\\psi_{w}-\\mu)^{T}\\Sigma^{-1}(\\psi_{w}-\\mu),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\Sigma^{-1}=\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I}\\end{array}$ and $\\mu=\\Sigma(A^{T}\\psi_{t+}+A\\psi_{0})$ . Above, we have used $\\equiv$ to denote equality up to an additive constant that is independent of $\\psi_{w}$ . ", "page_idx": 16}, {"type": "text", "text": "A.4 Proof of Theorem 4.2: Planning over Many Intermediate States ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We start by recalling that the waypoints form a Markov chain, so we can express their joint density as a product of conditional densities: ", "page_idx": 16}, {"type": "equation", "text": "$$\np(\\psi_{1:n})=p(\\psi_{0})p(\\psi_{1}\\mid\\psi_{0})p(\\psi_{2}\\mid\\psi_{1})\\cdot\\cdot\\cdot p(\\psi_{n}\\mid\\psi_{n-1}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The aim of this lemma is to express the joint distribution over multiple waypoints, given an initial and final state representation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\psi_{1:n}\\mid\\psi_{0},\\psi_{t+})\\triangleq\\frac{p\\big(\\psi_{1:n}\\mid\\psi_{0}\\big)p\\big(\\psi_{t+}\\mid\\psi_{n}\\big)}{p\\big(\\psi_{0}+\\psi_{0}\\big)}}\\\\ &{\\stackrel{(b)}{\\propto}p(\\psi_{1}\\mid\\psi_{0})p(\\psi_{2}\\mid\\psi_{1})\\cdots p(\\psi_{t+}\\mid\\psi_{n})}\\\\ &{\\stackrel{(c)}{\\propto}\\exp\\biggl(-\\frac{1}{2}\\frac{c+1}{c}\\|\\frac{c}{c+1}A\\psi_{0}-\\psi_{1}\\|_{2}^{2}-\\frac{1}{2}\\frac{c+1}{c}\\|\\frac{c}{c+1}A\\psi_{1}}\\\\ &{\\qquad\\qquad-\\psi_{2}\\|_{2}^{2}-\\cdots-\\frac{1}{2}\\frac{c+1}{c}\\|\\frac{c}{c+1}A\\psi_{n}-\\psi_{t+}\\|_{2}^{2}\\biggr)}\\\\ &{\\stackrel{(d)}{=}\\exp\\biggl(-\\frac{1}{2}\\frac{c}{\\epsilon+1}\\psi_{0}^{T}A^{T}\\psi_{0}+\\psi_{0}^{T}A^{T}\\psi_{1}-\\frac{1}{2}\\frac{c+1}{c}\\psi_{1}^{T}\\psi_{1}}\\\\ &{\\qquad\\qquad-\\frac{1}{2}\\frac{c}{\\epsilon+1}\\psi_{1}^{T}A^{T}A\\psi_{1}+\\psi_{1}^{T}A^{T}\\psi_{2}-\\frac{1}{2}\\frac{c+1}{c}\\psi_{2}^{T}\\psi_{2}}\\\\ &{\\qquad\\qquad-\\frac{1}{2}\\frac{c}{\\epsilon+1}\\psi_{2}^{T}A^{T}\\psi_{2}+\\psi_{2}^{T}A^{T}\\psi_{3}-\\frac{1}{2}\\frac{c+1}{c}\\psi_{3}^{T}\\psi_{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad-\\,\\frac{1}{2}\\frac{c}{c+1}\\psi_{n}^{T}A^{T}A\\psi_{n}+\\psi_{n}^{T}A^{T}\\psi_{t+}-\\frac{1}{2}\\frac{c+1}{c}\\psi_{t+}^{T}\\psi_{t+}\\Biggr)}\\\\ &{=\\exp\\bigl(-\\frac{1}{2}\\psi_{1:n}^{T}\\Sigma^{-1}\\psi_{1:n}+\\eta^{T}\\psi_{1:n}\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Sigma^{-1}$ is a tridiagonal matrix ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\boldsymbol{\\Sigma}}^{-1}=\\left(\\begin{array}{c c c c}{\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I}&{-A^{T}}&{-A^{T}}\\\\ {-A}&{\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I}&{-A^{T}}\\\\ &&{-A}&{\\frac{c}{c+1}A^{T}A+\\frac{c+1}{c}I}\\\\ &&&&{\\ddots}\\\\ &&&&&{A^{T}\\psi_{t+}}\\end{array}\\right)\\quad\\mathrm{and~}\\eta=\\left(\\begin{array}{c}{A\\psi_{0}}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\\\ {A^{T}\\psi_{t+}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In $(a)$ we applied Bayes\u2019 rule and removed the denominator because it is a constant with respect to $\\psi_{1:n}$ . In $(b)$ we applied the Markov assumption. In $(c)$ we used Lemma 1 to express the conditional probabilities as Gaussians, ignoring the proportionality constants (which are independent of $\\psi$ . In $(d)$ we simplified the exponents, removing terms that do not depend on $\\psi_{1:n}$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.5 Formalizing Assumption 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Assumption 2 states ", "page_idx": 16}, {"type": "equation", "text": "$$\ne^{-\\frac{1}{2}\\|\\phi(x_{0})-\\psi(x)\\|_{2}^{2}}=\\frac{p(x\\mid x_{0})}{p(x^{+})C}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "when Eq. (2) is optimized. ", "page_idx": 17}, {"type": "text", "text": "We can justify this assumption by analyzing the general solution to the symmetrized version of the Oord et al. [65] infoNCE objective, which we do in Lemma 3. Applying this lemma to our representation learning objective (2) for sufficiently large batch size $B$ then yields Eq. (5), with the function approximator $\\|\\dot{\\phi(x)}-\\dot{\\psi(x^{+})}\\|^{2}\\approx f(x,\\dot{x^{+}})$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. The solution to the optimization problem ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{f(x,x^{+})}\\operatorname*{lim}_{B\\to\\infty}\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\bigg[\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{e^{f(x_{i},x_{i}^{+})}}{\\sum_{j\\neq i}e^{f(x_{i},x_{j}^{+})}}+\\log\\frac{e^{f(x_{i},x_{i}^{+})}}{\\sum_{j\\neq i}e^{f(x_{j},x_{i}^{+})}}\\bigg]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x,x^{+})=\\log\\!\\left({\\frac{p(x^{+}\\mid x)}{p(x^{+})C}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some $C$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 3. We first break down the LHS and RHS of Eq. (2): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{f}{\\operatorname*{max}}\\ \\underset{B\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\Bigg[\\frac{1}{B}\\sum_{i=1}^{B}\\underbrace{\\log\\frac{e^{f(x_{i},x_{i}^{+})}}{\\sum_{j\\neq i}e^{f(x_{i},x_{j}^{+})}}}_{\\mathcal{T}_{1}}+\\underbrace{\\log\\frac{e^{f(x_{i},x_{i}^{+})}}{\\sum_{j\\neq i}e^{f(x_{j},x_{i}^{+})}}}_{\\mathcal{T}_{2}}\\Bigg]}\\\\ {\\mathcal{T}_{1}(f)=\\underset{B\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\Bigg[\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{e^{f(x_{i},x_{i}^{+})}}{\\sum_{j\\neq i}e^{f(x_{i},x_{j}^{+})}}\\Bigg]}\\\\ {\\mathcal{T}_{2}(f)=\\underset{B\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\Bigg[\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{e^{f(x_{i},x_{i}^{+})}}{\\sum_{j\\neq i}e^{f(x_{j},x_{i}^{+})}}\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now use the following result from Ma and Collins [54]: ", "page_idx": 17}, {"type": "text", "text": "Lemma 4. The optimal solutions $f_{1}$ and $f_{2}$ for $\\mathcal{I}_{1}$ and $\\mathcal{I}_{2}$ satisfy ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}(x,x^{+})=\\log p(x\\mid x^{+})-\\log c_{1}(x)}}\\\\ {{f_{2}(x,x^{+})=\\log p(x^{+}\\mid x)-\\log c_{2}(x^{+})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for arbitrary $c_{1}(x),c_{2}(x^{+})$ . ", "page_idx": 17}, {"type": "text", "text": "For any $C$ , when $c_{1}(x)=C p(x)$ and $c_{2}(x^{+})=C p(x^{+})$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{1}(x,x^{+})=\\log\\left({\\frac{p(x\\mid x^{+})}{p(x)C}}\\right)=\\log\\left({\\frac{p(x^{+}\\mid x)}{p(x^{+})C}}\\right)=f_{2}(x,x^{+}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It follows that Eq. (12) maximizes both $\\mathcal{I}_{1}$ and $\\mathcal{I}_{2}$ , and is precisely the optimal solution Eq. (9) for Eq. (8). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "What does $C$ represent? From Eq. (9), we can connect $C$ to the mutual information $I(x,x^{+})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C=\\frac{\\mathbb{E}_{(x,x^{+})\\sim p(x,x^{+})}\\big[f(x,x^{+})\\big]}{I(x,x^{+})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 4. We can first consider $\\mathcal{I}_{1}$ without loss of generality. Denoting $g(x,x^{+})\\;=\\;$ ef(x,x+), we take the functional derivative: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathcal{I}_{1}(\\log g)=\\underset{B\\to\\infty}{\\operatorname*{lim}}\\delta\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\left[\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{g(x_{i},x_{i}^{+})}{\\sum_{j\\neq i}g(x_{i},x_{j}^{+})}\\right]}\\\\ &{\\qquad\\qquad=\\underset{B\\to\\infty}{\\operatorname*{lim}}\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\left[\\frac{1}{B}\\sum_{i=1}^{B}\\frac{(\\sum_{j\\neq i}g(x_{i},x_{j}^{+}))\\delta g(x_{i},x_{i}^{+})-g(x_{i},x_{i}^{+})\\delta(\\sum_{j\\neq i}g(x_{i},x_{j}^{+}))}{g(x_{i},x_{i}^{+})(\\sum_{j\\neq i}g(x_{i},x_{j}^{+}))}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/c550eb4977478c596a98bf803ce69ec79655faeb6d3f8e05cd18f2a8426ebabd.jpg", "img_caption": ["Figure 7: Our approach enables a goal-conditioned policy to reach farther targets (red) from the start (green) by planning over intermediate waypoints (orange). "], "img_footnote": [], "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\underset{\\bar{t}_{s}\\to\\infty}{\\operatorname*{lim}}\\left(g_{(\\varepsilon_{s},\\varepsilon_{s}^{(\\varepsilon)}]_{\\leq\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)}}}\\right)\\bigg[\\frac{h_{t}^{(2)}}{h_{s}^{(2)}}\\frac{\\bar{F}_{t}(\\bar{F}_{t}(s,\\varepsilon_{\\varepsilon}^{(\\varepsilon)})-\\theta_{\\varepsilon}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)})}{h_{s}^{(2)}(\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)})}\\bigg]}\\\\ &{=\\underset{\\bar{t}_{s}\\to\\infty}{\\operatorname*{lim}}\\left(g_{(\\varepsilon_{s},\\varepsilon_{\\varepsilon}^{(\\varepsilon)}]_{\\leq\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)}}}\\right)\\bigg[\\frac{h_{t}^{(2)}}{h_{s}^{(2)}}\\bigg]\\bigg(\\frac{\\big(f_{s}^{(\\varepsilon_{\\varepsilon}^{(\\varepsilon)},\\varepsilon_{\\varepsilon}^{(\\varepsilon)})}\\big)\\big(\\varepsilon^{(\\varepsilon)}\\big)}{h_{s}^{(2)}(\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{\\varepsilon}^{(\\varepsilon)})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\underset{\\bar{t}_{s}\\to\\infty}{\\operatorname*{lim}}\\frac{f_{s}(\\varepsilon_{\\varepsilon},\\varepsilon_{\\varepsilon}^{(\\varepsilon)},\\varepsilon_{\\varepsilon}^{(\\varepsilon)})}{h_{s}^{(2)}(\\varepsilon_{s},\\varepsilon_{\\varepsilon}^{(\\varepsilon)},\\varepsilon_{\\varepsilon}^{(\\varepsilon)},\\varepsilon_{\\varepsilon}^{(\\varepsilon)})}\\bigg)|\\varepsilon^{(\\varepsilon)}\\bigg]\\mathrm{d}\\varepsilon^{\\varepsilon^{\\prime}}\\bigg]}\\\\ &{=\\underset{\\bar{t}_{s}\\to\\infty}{\\operatorname*{lim}}\\left(g_{(\\varepsilon_{s},\\varepsilon_{\\varepsilon}^{(\\varepsilon)}]_{\\leq\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\\varepsilon_{s}^{(\\varepsilon)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\operatorname*{lim}_{B\\to\\infty}\\mathbb{E}_{\\{(x_{i},x_{i}^{+})\\}_{i=1}^{B}\\sim p(x,x^{+})}\\bigg[\\frac{1}{B}\\sum_{i=1}^{B}\\int\\delta g(x_{i},x^{+})\\Big(\\frac{p(x^{+}|x_{i})}{g(x_{i},x^{+})}-k(x_{i})p(x^{+})\\Big)\\,\\mathrm{d}x^{+}\\bigg]}\\\\ {\\displaystyle=\\int\\delta g(x,x^{+})\\big(\\frac{p(x^{+}|x)}{g(x,x^{+})}-k(x)p(x^{+})\\big)\\,\\mathrm{d}x^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This is zero when ", "page_idx": 18}, {"type": "equation", "text": "$$\ng(x,x^{+})=\\frac{p(x\\mid x^{+})}{k(x)p(x)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(x,x^{+})=\\log p(x\\mid x^{+})-\\log\\underbrace{c_{1}(x)}_{k(x)p(x)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as in Eq. (10), and Eq. (11) follows similarly, exchanging $x$ and $x^{+}$ . ", "page_idx": 18}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Fig. 7 visualizes the inferred waypoints from the task in Fig. 5. ", "page_idx": 18}, {"type": "text", "text": "Fig. 8 visualizes the representations learned on a 46-dimensional robotic hammering task (see Section 5.3). ", "page_idx": 18}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/8707f26691720da3fbeea429f956390bfe2c98ec052c7f18cfd779ca6cb75d4a.jpg", "img_caption": ["(a) Contrastive representations ", "(b) PCA representations "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: Planning for 46-dimensional robotic hammering. (Left) A dataset of trajectories demonstrating a hammer knocking a nail into a board [31]. (Center) We visualize the learned representations as blue circles, with the transparency indicating the index of that observation along the trajectory. We also visualize the inferred plan (Section 4.3) as red circles connected by arrows. (Right) Representations learned by PCA on the same trajectory as (a, left). ", "page_idx": 19}, {"type": "text", "text": "B.1 Stock Prediction ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We show results on a stock opening price task in Fig. 9. ", "page_idx": 19}, {"type": "image", "img_path": "PoCs4jq7cV/tmp/d02b572172fedf90283943d7c6bc93b5fe7e853218b7569974293a30e2c4baf8.jpg", "img_caption": ["Figure 9: Stock Prediction. We apply temporal contrastive learning to time series data of the stock market. Data are the opening prices for the 500 stocks in the S&P 500, over a four year window. We remove 30 stocks that are missing data. For evaluation, we choose a 100 day window from a validation set, and use Theorem 4.1 to perform \u201cinpainting\u201d, predicting the intermediate stock prices jointly for all stocks (orange), given the first and last stock price. The true stock prices are shown in blue. While we do not claim that this is a state-of-the-art model for stock prediction, this experiment demonstrates another potential application of our theoretical results. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The claims in the abstract and introduction are supposed by the Lemmas in the main text. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See \u201cLimitations\u201d in Sec. 6, and discussion following the Assumptions in the main text. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Proofs for each Lemma are provided in either the main text or the appendices. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All results and figures in paper can be reproduced by running make in the linked code. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Code has been released. The datasets used have been released by prior work. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Details are in the appendix and/or the supplemental code. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: See Sec. 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. 5 ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We are unaware of any violations. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper provides theoretical analysis of representations learned by contrastive learning. This analysis suggests new ways of using these representations, for both beneficial and harmful purposes. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: N/A ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See citations to the D4RL dataset. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: N/A Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: N/A ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]