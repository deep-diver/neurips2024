[{"figure_path": "manHbkpIW6/tables/tables_6_1.jpg", "caption": "Table 1: Results of strategies applied on BERT", "description": "This table presents the results of different strategies applied to the BERT model.  It compares the performance of a BERT model that underwent additional medical and legal domain-specific pretraining with a BERT model incorporating the Cluster-guided Sparse Expert (CSE) layer (Ours/MoA and Ours/MoF). The performance metrics include overall perplexity, as well as the accuracy scores across several downstream tasks: Overruling, Casehold, GAD, EUADR, and SST2.  The average accuracy across all five tasks is also provided for each model variant.", "section": "5 Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_8_1.jpg", "caption": "Table 1: Results of strategies applied on BERT", "description": "This table presents the results of different strategies applied to the BERT model.  The strategies include continuing pretraining on medical and legal domain-specific data (BERT/med and BERT/legal), using a Mixture of Experts (MoE) approach (MoE/med and MoE/legal), and applying the proposed Cluster-guided Sparse Expert (CSE) method (Ours/MoA and Ours/MoF).  The table shows the pretraining perplexity (Pretrain ppl), and the performance on several downstream tasks (Overruling, Casehold, GAD, EUADR, SST2), along with average performance across tasks.  Note that BERT/med shows a significant forgetting issue, which is discussed in the Appendix. The results highlight the effectiveness of the CSE approach in improving performance on downstream tasks compared to other methods.", "section": "5 Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_8_2.jpg", "caption": "Table 2: Results of strategies applied on GPT", "description": "This table presents the results of different strategies applied to the GPT model.  The strategies include using a GPT model fine-tuned on medical data (GPT/med), a GPT model fine-tuned on legal data (GPT/legal), a MoE (Mixture of Experts) model fine-tuned on medical data (MoE/med), a MoE model fine-tuned on legal data (MoE/legal), and the proposed CSE (Cluster-guided Sparse Expert) method applied to the attention mechanism (Ours/MoA) and the feed-forward network (Ours/MoF). The table shows the average performance across several downstream tasks, including the perplexity scores, and the performance on the Overruling, Casehold, GAD, EUADR, and SST2 datasets.  The results highlight the performance improvement using the CSE approach.", "section": "5 Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_8_3.jpg", "caption": "Table 3: Results of strategies applied on 330M GPT", "description": "This table presents the performance comparison of different methods (GPT/tuned, MoE/tuned, and CSE/w/o tune) on various downstream tasks using a larger 330M GPT model.  It shows the accuracy achieved on multiple tasks across three domains: academic, environment, and financial. Notably, CSE/w/o tune showcases performance on par or exceeding the other methods without requiring any domain-specific fine-tuning. This highlights its ability to learn from long-tail data efficiently during pretraining alone.", "section": "5 Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_14_1.jpg", "caption": "Table 1: Results of strategies applied on BERT", "description": "This table presents the results of different strategies applied to the BERT model.  It compares the performance of the baseline BERT model (with and without further training on medical and legal datasets) against a model using the proposed Cluster-guided Sparse Expert (CSE) approach.  The performance is measured across several downstream tasks, including Overruling, Casehold, GAD, EUADR, and SST2, with the average performance across all these tasks also included.", "section": "5 Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_14_2.jpg", "caption": "Table 5: Hyperparameters of Models", "description": "This table lists the hyperparameters used for both BERT-based and GPT-based models in the experiments.  It details the settings for various aspects of the model architecture and training process, including the number of FFN and attention modules, attention heads, transformer layers, hidden dimension size, dropout rates, sequence length, batch size, maximum training steps, learning rate decay strategy, and the random seed used. These hyperparameters were crucial in configuring and training the models for the experimental results presented in the paper.", "section": "A Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_15_1.jpg", "caption": "Table 6: Checkpoints selected with early-stop where two models show the same degree of forgetting.", "description": "This table displays the results of the Casehold, Overruling, GAD, and EUADR tasks using checkpoints selected by an early-stopping method that controls for catastrophic forgetting.  The early stopping ensures that both BERT/legal and BERT/med models exhibit a similar level of forgetting on the pretraining data, enabling a more fair comparison of their performance on the downstream tasks.", "section": "B Limitations Discussions"}, {"figure_path": "manHbkpIW6/tables/tables_16_1.jpg", "caption": "Table 7: Results of general tasks on BERT with the same small-scale setting in the paper", "description": "This table presents the results of several general knowledge tasks using BERT models.  It compares the performance of a baseline BERT model (fine-tuned), a MoE (Mixture of Experts) version of BERT (fine-tuned), and the proposed CSE (Cluster-guided Sparse Expert) method without fine-tuning. The comparison is based on accuracy scores, frequency scores for the tasks, and the average performance across all the tasks. This table aims to demonstrate that even without fine-tuning, the CSE method can achieve comparable or better performance on general tasks, highlighting its effectiveness in learning long-tail domain knowledge while retaining general capabilities.", "section": "Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_16_2.jpg", "caption": "Table 8: Results of general tasks tested on GPT 330M trained with 20B tokens", "description": "This table presents the results of general knowledge tasks evaluated on a larger GPT model (330M parameters) trained with 20 billion tokens.  It compares the performance of a baseline model, a Mixture of Experts (MoE) model, and the proposed Cluster-guided Sparse Expert (CSE) model without any fine-tuning. The table shows the accuracy scores for each task (COLA, QNLI, MRPC, QQP, SST2), along with the average frequency score of the sentences used in those tasks and the overall average accuracy of each method.", "section": "5 Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_16_3.jpg", "caption": "Table 9: Results of strategies applied on pre-trained model", "description": "This table presents the results of different strategies applied to a pre-trained 110M model.  The strategies include continuing pretraining on medical data (*/med), continuing pretraining on legal data (*/legal), using a Mixture of Experts (MoE) architecture, and the proposed Cluster-guided Sparse Expert (CSE) approach (Ours/MoA and Ours/MoF). The table shows the performance of each strategy across various downstream tasks (Overruling, Casehold, GAD, EUADR, SST2), along with the average performance.  It highlights the improvement achieved by the CSE approach compared to the baseline methods and MoE.", "section": "5 Experiments"}, {"figure_path": "manHbkpIW6/tables/tables_17_1.jpg", "caption": "Table 1: Results of strategies applied on BERT", "description": "This table presents the results of different strategies applied to the BERT model.  The strategies include using a BERT model fine-tuned on medical data (BERT/med), a BERT model fine-tuned on legal data (BERT/legal), a MoE (Mixture of Experts) model fine-tuned on medical data (MoE/med), a MoE model fine-tuned on legal data (MoE/legal), the proposed Cluster-guided Sparse Expert (CSE) model applied to the attention mechanism (Ours/MoA), and the proposed CSE model applied to the feed-forward network (Ours/MoF). The table shows the pretraining perplexity (Pretrain ppl) and the performance on several downstream tasks: Overruling, Casehold, GAD, EUADR, and SST2. The average performance across all tasks is also included for comparison.", "section": "5.1 Main Result"}]