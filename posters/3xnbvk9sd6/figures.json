[{"figure_path": "3XnBVK9sD6/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between standard RM and our information-theoretic reward model (InfoRM). InfoRM distinguishes itself by enhancing RM generalizability through mutual information modeling. Additionally, a distinct feature of InfoRM is its overoptimization detection mechanism, which can guide parameter selection and algorithm design in subsequent RLHF. Specifically, the RM encoder is derived from the standard RM, with modification to the final layer.", "description": "This figure compares the standard reward model (RM) with the proposed Information-Theoretic Reward Model (InfoRM).  The standard RM directly maps responses to a reward using a simple model. InfoRM, however, incorporates an encoder and decoder based on variational information bottleneck principle. The encoder processes the responses and generates a latent representation which captures relevant information to human preference.  An MLP helps filter out irrelevant information in the latent space. The decoder then uses the cleaned latent representation to generate rewards. InfoRM adds an overoptimization detection mechanism, using Cluster Separation Index (CSI) computed from the latent space, to help mitigate overoptimization. ", "section": "1 Introduction"}, {"figure_path": "3XnBVK9sD6/figures/figures_1_2.jpg", "caption": "Figure 2: Response comparison on Anthropic-Helpful between RLHF models using our InfoRM and other baselines, assessed by GPT-4, demonstrating the superior performance of our method.", "description": "This figure displays a comparison of the performance of different RLHF models on the Anthropic-Helpful benchmark, as evaluated by GPT-4. The models compared include those using the proposed InfoRM method and several baselines.  The results are presented as a bar chart showing the percentage of wins, ties, and losses for each model against the others. InfoRM achieves significantly more wins compared to baselines, indicating its superior performance in mitigating reward hacking and enhancing RLHF.", "section": "1 Introduction"}, {"figure_path": "3XnBVK9sD6/figures/figures_2_1.jpg", "caption": "Figure 3: An example of reward overoptimization in RLHF characterized by a declining gold score (i.e., actual human preference) and a rising proxy score (i.e., proxy RM preference).", "description": "This figure illustrates the problem of reward overoptimization in Reinforcement Learning from Human Feedback (RLHF).  The x-axis represents the KL divergence between the proxy reward model and the true human preference. The y-axis shows both the \"gold score\" (actual human preference) and the \"proxy score\" (the reward model's score). Initially, both scores increase, indicating successful alignment. However, after a certain point (marked by the red annotation), the proxy score continues to increase while the gold score decreases. This shows the reward model has overoptimized, focusing on spurious features that do not align with true human preferences.  This phenomenon undermines the goal of RLHF, which is to align language models with human values.", "section": "2.1 Reward Overoptimization in RLHF"}, {"figure_path": "3XnBVK9sD6/figures/figures_4_1.jpg", "caption": "Figure 4: Simulated RLHF results for different proxy RMs (1.4B). Solid and dashed lines represent the gold and proxy scores, respectively. In later RL stages, as KL divergence increases, Standard RM shows a declining gold score and a rising proxy score, indicating overoptimization. Conversely, our InfoRM maintains consistent growth in both scores, effectively mitigating overoptimization.", "description": "This figure compares the performance of InfoRM against standard reward models (Standard RM, Standard RM with KL divergence, and Ensemble RM) in simulated RLHF experiments using a 1.4B proxy model.  It demonstrates InfoRM's ability to mitigate reward overoptimization, a problem where the model's performance on a proxy reward metric diverges from its performance on the true human reward. The plots showcase the gold score (true human reward) and proxy score (model's reward) over the course of RL training, illustrating that the gold score decreases while the proxy score increases in standard models but both scores consistently increase with InfoRM, indicating better alignment with human preferences.", "section": "4.1 Simulation Experiments"}, {"figure_path": "3XnBVK9sD6/figures/figures_5_1.jpg", "caption": "Figure 5: Final gold rewards in simulated RLHF experiments. Left: Using proxy RMs with varying parameter sizes. Right: Conducting RL on Alpaca (in-distribution) and Flan (out-of-distribution). The proxy RMs are all trained on the same simulated preference dataset with 25% label noise.", "description": "This figure displays the results of simulated RLHF experiments comparing three different reward models (Standard RM, Standard RM with KL divergence, and InfoRM) across different parameter sizes (70M, 410M, and 1.4B) and datasets (Alpaca and Flan).  The left panel shows how performance varies with the size of the proxy reward model. The right panel displays the comparative performance of the models trained using two different datasets, one in-distribution (Alpaca) and one out-of-distribution (Flan), all with 25% label noise added to the training data.", "section": "4.1 Main Results"}, {"figure_path": "3XnBVK9sD6/figures/figures_6_1.jpg", "caption": "Figure 4: Simulated RLHF results for different proxy RMs (1.4B). Solid and dashed lines represent the gold and proxy scores, respectively. In later RL stages, as KL divergence increases, Standard RM shows a declining gold score and a rising proxy score, indicating overoptimization. Conversely, our InfoRM maintains consistent growth in both scores, effectively mitigating overoptimization.", "description": "The figure shows the results of simulated RLHF experiments using different proxy reward models with a 1.4B parameter size.  It compares the performance of the standard reward model (Standard RM), the standard reward model with KL divergence penalty (Standard RM w/ KL), and the proposed InfoRM. The plots show the gold score (actual human preference) and proxy score (model's reward) over the course of the RL training. The Standard RM shows a decrease in gold score and an increase in proxy score at later stages, indicative of reward overoptimization. In contrast, InfoRM maintains consistent growth in both gold and proxy scores, demonstrating its ability to prevent reward overoptimization.", "section": "4.1 Simulation Experiments"}, {"figure_path": "3XnBVK9sD6/figures/figures_7_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of model responses in the latent space of InfoRM.  It shows the distributions before and after reinforcement learning from human feedback (RLHF), highlighting the differences between using a standard reward model and InfoRM.  Overoptimized samples (identified by GPT-4) are also shown, demonstrating that InfoRM significantly reduces the number of outliers which correspond to overoptimized responses. The visualization helps illustrate how InfoRM's information bottleneck improves the model's robustness and mitigates reward hacking.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_8_1.jpg", "caption": "Figure 8: CSI values in the RLHF processes of Standard RM and InfoRM across the training steps on Anthropic-Helpful dataset.", "description": "This figure shows the CSI values during the RLHF process using Standard RM and InfoRM on the Anthropic-Helpful dataset.  It illustrates that a sudden and substantial increase in CSI values of Standard RM indicates the emergence of outliers in the latent space, corresponding to reward overoptimization.  In contrast, InfoRM consistently exhibits much lower CSI values, demonstrating its effectiveness in mitigating reward overoptimization.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_16_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the response distribution in the latent IB space of InfoRM using t-SNE. It compares the distributions before and after RLHF, with different reward models (Standard RM and InfoRM) and datasets (Anthropic-Harmless and Anthropic-Helpful). Key observations are that outliers in InfoRM's latent space indicate overoptimized samples, and InfoRM significantly reduces the number of such samples.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-distributed stochastic neighbor embedding (t-SNE) to visualize the distribution of model responses in the latent space of the information bottleneck (IB) learned by InfoRM.  It compares responses generated before RLHF (SFT model) and after RLHF with both Standard RM and InfoRM. Overoptimized samples, identified by GPT-4, are highlighted. The visualization shows that overoptimized samples tend to appear as outliers in the InfoRM's IB latent space, and that InfoRM significantly reduces the number of such outliers compared to the Standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_2.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions in the latent space of InfoRM.  It compares the distributions before and after RLHF training using standard and InfoRM reward models, on both Anthropic-Harmless and Anthropic-Helpful datasets. Overoptimized samples, as identified by GPT-4, are highlighted in red. The figure demonstrates that overoptimized samples appear as outliers in InfoRM's latent space, and that using InfoRM significantly reduces the number of these outliers.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_3.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of model responses in the latent space of InfoRM, both before and after reinforcement learning from human feedback (RLHF).  It highlights a key finding of the paper: overoptimized samples (identified as such by GPT-4) tend to appear as outliers in InfoRM's latent space. The figure compares the distribution using standard reward models and the InfoRM model, showing that InfoRM significantly reduces the number of these outlier, overoptimized samples. The visualization is shown for two datasets: Anthropic-Harmless and Anthropic-Helpful, making it clear that this finding is consistent across different datasets.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_4.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It compares the results using a standard reward model (RM) and the proposed InfoRM.  The key observation is that outliers in InfoRM's latent space correspond to overoptimized samples (as judged by GPT-4), and that InfoRM significantly reduces the number of these outliers compared to the standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_5.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It highlights the relationship between overoptimized samples (as identified by GPT-4) and outliers in the InfoRM's latent space. The top row shows results using the Anthropic-Harmless dataset, while the bottom row uses the Anthropic-Helpful dataset.  The left column shows results using a standard reward model, while the right column shows results using the InfoRM model. The key takeaway is that overoptimized samples tend to be outliers in the InfoRM's latent space, and that using InfoRM significantly reduces the number of these outliers.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_6.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It compares the distributions using a standard reward model (RM) and the proposed InfoRM.  Red dots represent overoptimized samples, identified by GPT-4. The figure demonstrates that InfoRM's latent space effectively separates overoptimized samples, highlighting its ability to mitigate reward hacking and improve model robustness.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_7.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions in InfoRM's latent space.  It compares responses before and after RLHF (Reinforcement Learning from Human Feedback) using Standard RM and InfoRM. The visualizations also include points identified by GPT-4 as overoptimized samples. The key takeaway is that InfoRM's latent space shows fewer outliers (overoptimized samples) compared to the Standard RM's latent space after RLHF. This suggests InfoRM is effective at mitigating reward hacking.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_8.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It shows the distributions for both standard reward models and the proposed InfoRM model, on two different datasets (Anthropic-Harmless and Anthropic-Helpful). The key takeaway is that outliers in the InfoRM latent space strongly correlate with overoptimized samples identified by GPT-4. The InfoRM model shows a significant reduction in these outliers compared to the standard reward model, suggesting its effectiveness in mitigating reward hacking.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_9.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It compares the results using a standard reward model (RM) and the proposed InfoRM model.  The visualization shows the response distribution from the supervised fine-tuning (SFT) model, RLHF model, and overoptimized samples identified by GPT-4. The key observation is that outliers in the InfoRM's latent space often indicate overoptimized samples, and InfoRM significantly reduces the number of these outliers compared to standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_10.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF). It shows how the distribution changes and the emergence of outliers (overoptimized samples) that are identified by GPT-4.  The top row uses the Anthropic-Harmless dataset, while the bottom row uses the Anthropic-Helpful dataset. The left column shows results using the standard reward model, while the right column shows results using the InfoRM model. The key observation is that InfoRM significantly reduces the number of outliers, indicating its effectiveness in mitigating reward hacking.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_11.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions from different models in the latent space of InfoRM.  It compares the distribution before and after reinforcement learning from human feedback (RLHF) using both standard reward models and the proposed InfoRM model. Red points indicate overoptimized samples identified by GPT-4. The figure illustrates that InfoRM reduces the occurrence of these overoptimized samples, which are outliers in the latent space, by filtering out irrelevant information.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_17_12.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions in InfoRM's latent IB space.  It compares distributions before and after RLHF (using SFT and RLHF models), highlighting outliers identified by GPT-4 as overoptimized. The top row uses the Anthropic-Harmless dataset, the bottom row uses Anthropic-Helpful.  The left column uses a Standard Reward Model, the right uses InfoRM. The visualization demonstrates that InfoRM significantly reduces outliers (overoptimized samples) compared to the standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF). It shows that overoptimized samples, identified by GPT-4, tend to appear as outliers in the InfoRM latent space. In contrast, the use of InfoRM substantially reduces the number of outliers compared to standard reward models, thereby effectively mitigating overoptimization.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_2.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows the t-distributed stochastic neighbor embedding (t-SNE) visualizations of response distributions in the latent space of the information bottleneck (IB) before and after reinforcement learning from human feedback (RLHF) using both standard reward models (RMs) and the proposed InfoRM.  The visualizations are separated by dataset (Anthropic-Harmless and Anthropic-Helpful) and RM type. The key observation is that outliers in the InfoRM's IB latent space strongly correlate with overoptimized samples, while InfoRM significantly reduces the number of these outliers compared to standard RMs.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_3.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-distributed stochastic neighbor embedding (t-SNE) to visualize the distribution of model responses in the latent space of the InfoRM reward model.  It compares the response distributions before and after reinforcement learning from human feedback (RLHF), both for a standard reward model and the proposed InfoRM.  The key observation is that overoptimized samples (identified by GPT-4) tend to cluster as outliers in the InfoRM's latent space, but this phenomenon is far less pronounced in the standard reward model.  This visually demonstrates InfoRM's ability to reduce overoptimization and provides evidence for using outlier detection as a method to identify overoptimized samples.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_4.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows the t-SNE visualization of response distributions in InfoRM's latent IB space.  It compares the distributions before and after RLHF training, using both Standard RM and InfoRM.  Overoptimized samples (as judged by GPT-4) are highlighted. The key takeaway is that InfoRM significantly reduces the number of outliers (overoptimized samples) in the latent space compared to the Standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_5.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions in the latent space of InfoRM, comparing before and after RLHF, with and without InfoRM.  The top row shows results using the Anthropic-Harmless dataset, and the bottom row shows results for the Anthropic-Helpful dataset.  The left column uses a Standard Reward Model in the RLHF, while the right uses InfoRM. Red dots represent overoptimized samples, as judged by GPT-4. The visualization highlights InfoRM's ability to reduce overoptimized samples, as indicated by fewer outliers in the latent space.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_6.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of model responses in the latent space of InfoRM.  It shows the distributions before and after reinforcement learning (RLHF), with different reward models (Standard RM and InfoRM) used.  Overoptimized samples, identified by GPT-4, are highlighted.  The key observation is that InfoRM significantly reduces the number of outliers (overoptimized samples) in its latent space compared to the standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_7.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF). It shows that overoptimized samples, identified by GPT-4, tend to appear as outliers in the InfoRM's latent space.  The use of InfoRM significantly reduces the number of these outliers, suggesting that InfoRM effectively mitigates reward hacking by filtering out irrelevant information.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_8.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of responses in the latent space of InfoRM before and after reinforcement learning from human feedback (RLHF).  It shows the distributions for SFT model outputs (blue), RLHF model outputs (red), and overoptimized samples identified by GPT-4 (purple triangles). The top row shows results using the Anthropic-Harmless dataset, and the bottom row uses the Anthropic-Helpful dataset.  The left column shows results using a standard reward model, and the right column shows results using InfoRM. The figure demonstrates that InfoRM reduces overoptimization, as evidenced by fewer outliers (purple triangles) in its latent space compared to the standard reward model.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_9.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions in the latent space of InfoRM.  It compares the distributions before and after RLHF training using Standard RM and InfoRM, highlighting outliers identified by GPT-4 as overoptimized samples. The visualizations demonstrate that InfoRM significantly reduces the number of overoptimized samples compared to the standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_10.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-distributed stochastic neighbor embedding (t-SNE) to visualize the distribution of model responses in the latent space of the information bottleneck (IB) used in InfoRM.  It compares the distributions before and after reinforcement learning from human feedback (RLHF) for both standard reward models (RM) and InfoRM.  Red dots represent overoptimized samples as judged by GPT-4. The figure shows that overoptimized samples appear as outliers in InfoRM's latent space, and that InfoRM significantly reduces the number of these outliers compared to the standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_11.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It compares standard reward models (RM) with the proposed InfoRM. The visualization uses t-SNE to reduce dimensionality.  The red dots represent overoptimized samples identified by GPT-4. The key observation is that InfoRM significantly reduces the number of outliers (overoptimized samples) compared to the standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_18_12.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of model responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It uses t-SNE for dimensionality reduction to allow visualization.  The data is separated into SFT model responses (before RLHF), RLHF model responses (after RLHF), and those RLHF responses identified as overoptimized by GPT-4. The figure shows that overoptimized samples tend to appear as outliers in the InfoRM latent space, while the use of InfoRM significantly reduces the number of such outliers compared to a standard reward model.  The figure is presented as four pairs of plots, separated by the dataset used for response generation (Anthropic-Harmless and Anthropic-Helpful) and the reward model employed (Standard RM and InfoRM).", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_19_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of model responses in the latent space of the InfoRM model before and after reinforcement learning.  It shows that overoptimized samples, identified by GPT-4, tend to appear as outliers in InfoRM's latent space.  The figure demonstrates that using InfoRM significantly reduces the number of these outliers, suggesting its effectiveness in mitigating reward overoptimization.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_20_1.jpg", "caption": "Figure 13: CSI values in the RLHF processes of Standard RM and InfoRM across the training steps. From left to right and from top to bottom: The datasets used for response generation are OpenOrca, Piqa, PKU-SafeRLHF, ShareGPT, SHP, Instruct-GPT, TruthfulQA, and WebGPT datasets, respectively.", "description": "This figure shows the Cluster Separation Index (CSI) values for both Standard Reward Model (RM) and Information-Theoretic Reward Model (InfoRM) across various datasets during the reinforcement learning from human feedback (RLHF) process.  The CSI is an indicator of reward over-optimization.  The plots show that InfoRM consistently maintains lower CSI values across all datasets than Standard RM. The sudden increases in CSI for Standard RM indicate periods of reward over-optimization.  Each plot is associated with a t-SNE visualization of the response distribution in the latent IB space of InfoRM, highlighting how outliers in this space correspond to overoptimized samples.", "section": "C.2 Validations for Outlier Emergencies and Overoptimization Detection by the CSI Indicator"}, {"figure_path": "3XnBVK9sD6/figures/figures_21_1.jpg", "caption": "Figure 14: Average response length of the models at different RLHF steps using Standard RM and InfoRM. From left to right and from top to bottom: The dataset used for response generation is AlpacaFarm, FalseQA, Flan, Anthropic-Helpful, Anthropic-Harmless, Oasst1, OpenOrca, Piqa, PKU-SaveRLHF, SHP, TruthfulQA, and WebGPT datasets, respectively.", "description": "This figure compares the average response length generated by models trained with standard reward models and InfoRM across 12 different datasets during RLHF.  Each subplot shows the response length over training steps for a specific dataset. The goal is to illustrate that InfoRM effectively mitigates the issue of generating excessively long responses (length bias) that is often observed in RLHF.  InfoRM consistently produces shorter responses across all datasets, suggesting improved efficiency and relevance.", "section": "D Analysis of Irrelevant Information Filtering Using Our InfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_22_1.jpg", "caption": "Figure 15: Visualization of output distribution in InfoRM's IB latent space before and after RLHF of Standard RM. (a)-(c) correspond to different IB dimensionalities of InfoRM and (d)-(f) correspond to different tradeoff parameter \u03b2 of InfoRM. The dataset used for response generation is the Anthropic-Harmless dataset. Conclusion: Our overoptimization detection mechanism is robust against variations in InfoRM's hyperparameters.", "description": "This figure shows the robustness of the overoptimization detection mechanism in InfoRM against variations in hyperparameters.  It presents t-SNE visualizations of the response distribution in InfoRM's IB latent space before and after RLHF using the Standard RM.  Subplots (a)-(c) vary the IB dimensionality (64, 128, and 256), while subplots (d)-(f) vary the trade-off parameter \u03b2 (0.0001, 0.1, and 0.5). The Anthropic-Harmless dataset was used. The conclusion drawn is that the overoptimization detection mechanism remains robust across these hyperparameter changes.", "section": "E.1 Impact on Overoptimization Detection Mechanism"}, {"figure_path": "3XnBVK9sD6/figures/figures_22_2.jpg", "caption": "Figure 16: Win rate (%) on Anthropic-Harmless dataset between the models after and before RLHF using our InfoRM with different hyper-parameters, according to GPT-4. In order to remove ties, we calculate the win rate as win/(win + loss).", "description": "This figure shows the win rate of the InfoRM model on the Anthropic-Harmless dataset for different hyperparameter settings (IB dimensionality and Beta).  The win rate is calculated as the number of wins divided by the total number of wins and losses, excluding ties.  The results show the impact of different hyperparameter values on the model's performance.", "section": "E.2 Impact on RLHF performance"}, {"figure_path": "3XnBVK9sD6/figures/figures_23_1.jpg", "caption": "Figure 15: Visualization of output distribution in InfoRM's IB latent space before and after RLHF of Standard RM. (a)-(c) correspond to different IB dimensionalities of InfoRM and (d)-(f) correspond to different tradeoff parameter \u03b2 of InfoRM. The dataset used for response generation is the Anthropic-Harmless dataset. Conclusion: Our overoptimization detection mechanism is robust against variations in InfoRM's hyperparameters.", "description": "This figure shows the robustness of the overoptimization detection mechanism in InfoRM against variations in hyperparameters.  It displays t-SNE visualizations of the response distributions in InfoRM's latent IB space, before and after RLHF.  Different subplots represent varying IB dimensionalities (64, 128, 256) and beta values (0.0001, 0.1, 0.5).  The consistency of outlier behavior across these variations demonstrates robustness.", "section": "E.1 Impact on Overoptimization Detection Mechanism"}, {"figure_path": "3XnBVK9sD6/figures/figures_23_2.jpg", "caption": "Figure 15: Visualization of output distribution in InfoRM's IB latent space before and after RLHF of Standard RM. (a)-(c) correspond to different IB dimensionalities of InfoRM and (d)-(f) correspond to different tradeoff parameter \u03b2 of InfoRM. The dataset used for response generation is the Anthropic-Harmless dataset. Conclusion: Our overoptimization detection mechanism is robust against variations in InfoRM's hyperparameters.", "description": "This figure shows the robustness of the overoptimization detection mechanism in InfoRM against variations in hyperparameters.  It presents t-SNE visualizations of the response distribution in the InfoRM's latent space (IB latent space) before and after reinforcement learning from human feedback (RLHF). Subplots (a)-(c) show the effect of varying the IB dimensionality (64, 128, and 256), while subplots (d)-(f) show the effect of varying the trade-off parameter \u03b2 (0.0001, 0.1, and 0.5). The Anthropic-Harmless dataset was used.  The consistent appearance of outliers in the latent space, regardless of hyperparameter settings, demonstrates the robustness of the method in detecting overoptimized samples.", "section": "E.1 Impact on Overoptimization Detection Mechanism"}, {"figure_path": "3XnBVK9sD6/figures/figures_24_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions in the latent space of InfoRM and Standard RM.  It highlights how InfoRM's latent space separates samples generated before and after RLHF training, with outliers in InfoRM's space often correlating with overoptimized responses (as judged by GPT-4). InfoRM significantly reduces the number of such outliers compared to Standard RM. The visualizations are broken down by dataset (Anthropic-Harmless and Anthropic-Helpful) and RM used (Standard RM and InfoRM).", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_27_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of model responses in the latent space of InfoRM before and after reinforcement learning, highlighting the effectiveness of InfoRM in mitigating overoptimization.  The t-SNE plots show distinct clusters for SFT model outputs, RLHF model outputs, and those deemed \"overoptimized\" by GPT-4. InfoRM demonstrates a tighter clustering of RLHF responses, with fewer outliers, compared to the standard RM.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_28_1.jpg", "caption": "Figure 1: Comparison between standard RM and our information-theoretic reward model (InfoRM). InfoRM distinguishes itself by enhancing RM generalizability through mutual information modeling. Additionally, a distinct feature of InfoRM is its overoptimization detection mechanism, which can guide parameter selection and algorithm design in subsequent RLHF. Specifically, the RM encoder is derived from the standard RM, with modification to the final layer.", "description": "This figure compares the standard reward model (RM) with the proposed information-theoretic reward model (InfoRM).  The standard RM uses a reward loss based on human rankings to train the model. In contrast, InfoRM incorporates a variational information bottleneck objective to filter out irrelevant information during training.  Furthermore, InfoRM includes an overoptimization detection mechanism using a cluster separation index (CSI) that helps in parameter selection and mitigation strategies in Reinforcement Learning from Human Feedback (RLHF).", "section": "1 Introduction"}, {"figure_path": "3XnBVK9sD6/figures/figures_31_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure visualizes the distribution of responses in the latent space of InfoRM (Information-Theoretic Reward Model) before and after reinforcement learning from human feedback (RLHF), and compares them with the distribution of overoptimized samples identified by GPT-4. The top row shows results using the Anthropic-Harmless dataset, and the bottom row shows results using the Anthropic-Helpful dataset.  The left column displays the results using the standard reward model, while the right column shows results using InfoRM. The visualization demonstrates that InfoRM effectively reduces the number of overoptimized samples, which appear as outliers in the latent space.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_32_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows t-SNE visualizations of response distributions in InfoRM's latent IB space.  It compares the distributions before and after RLHF training using both Standard RM and InfoRM, and it highlights overoptimized samples identified by GPT-4. The key takeaway is that InfoRM significantly reduces the number of outliers (overoptimized samples) compared to the Standard RM, confirming its effectiveness in mitigating overoptimization.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_34_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows the t-SNE visualization of response distribution in the latent IB space of InfoRM before and after RLHF.  It also shows the distribution of overoptimized samples identified by GPT-4. The visualization helps illustrate how InfoRM effectively mitigates reward overoptimization by reducing the number of outliers in the latent space, which are associated with overoptimized samples.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_34_2.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure uses t-SNE to visualize the distribution of responses in the latent space of the InfoRM model before and after reinforcement learning from human feedback (RLHF).  It shows that overoptimized samples (identified by GPT-4) tend to appear as outliers in the InfoRM latent space. The use of InfoRM significantly reduces the number of these outliers, suggesting its effectiveness in mitigating reward overoptimization.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}, {"figure_path": "3XnBVK9sD6/figures/figures_35_1.jpg", "caption": "Figure 7: T-SNE visualization of the response distribution in the latent IB space of InfoRM before and after RLHF (SFT model and RLHF model), as well as the distribution of overoptimized samples from the RLHF model as judged by GPT-4. From top to bottom: The datasets used for response generation are Anthropic-Harmless and Anthropic-Helpful, respectively. From left to right: The RMs applied in RLHF are Standard RM and InfoRM, respectively. Observations: (1) Outliers in the IB latent space of InfoRM usually signify overoptimized samples. (2) Using InfoRM significantly reduces the emergence of overoptimized samples.", "description": "This figure shows the t-SNE visualizations of response distributions in the latent IB space of InfoRM.  It compares the distributions before and after RLHF (Reinforcement Learning from Human Feedback) using both a standard reward model and the proposed InfoRM.  Overoptimized samples, as identified by GPT-4, are also highlighted. The key observation is that outliers in InfoRM's latent space are strongly correlated with overoptimized samples, and that InfoRM significantly reduces the number of these outliers compared to the standard reward model.", "section": "5 Detecting Overoptimization: Additional Strength of OurInfoRM"}]