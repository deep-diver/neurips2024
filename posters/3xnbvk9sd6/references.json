{"references": [{"fullname_first_author": "Alexander A Alemi", "paper_title": "Deep variational information bottleneck", "publication_date": "2016-00-00", "reason": "This paper introduces the variational information bottleneck (VIB) method, which is the theoretical foundation for InfoRM's information-theoretic reward modeling."}, {"fullname_first_author": "Leo Gao", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023-00-00", "reason": "This paper investigates scaling laws for reward model overoptimization and provides insights relevant to the problem of reward hacking in RLHF, which InfoRM also addresses."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper is a foundational work in reinforcement learning from human feedback (RLHF), providing the overall RLHF pipeline that InfoRM enhances."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-00-00", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, which is the reinforcement learning algorithm used in the RL stage of the RLHF pipeline improved by InfoRM."}, {"fullname_first_author": "Naftali Tishby", "paper_title": "The information bottleneck method", "publication_date": "2000-00-00", "reason": "This paper introduces the information bottleneck (IB) method, which is a core concept that the InfoRM method is built upon."}]}