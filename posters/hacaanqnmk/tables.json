[{"figure_path": "HAcaANQNMK/tables/tables_8_1.jpg", "caption": "Table 1: GEMM latency, time to first token, Wikitext-103 perplexity (WK-103 PPL), and downstream task accuracy of GPT3, Llama2, and Nemotron4 models compressed with ESPACE.", "description": "This table presents the results of the model compression experiments using ESPACE.  It compares the baseline performance (without compression) to models compressed using ESPACE at different compression rates (20%, 47%, etc.). For each model and compression rate, it provides the number of weights, total GEMM latency, time to first token (TTFT) latency, Wikitext-103 perplexity, and downstream task accuracy across several benchmark tasks (BoolQ, HellaSwag, PIQA, RACE, and WinoGrande).  The table shows the impact of ESPACE on both model size and inference speed.", "section": "4 Model Compression Studies"}, {"figure_path": "HAcaANQNMK/tables/tables_17_1.jpg", "caption": "Table 1: GEMM latency, time to first token, Wikitext-103 perplexity (WK-103 PPL), and downstream task accuracy of GPT3, Llama2, and Nemotron4 models compressed with ESPACE.", "description": "This table presents the results of experiments evaluating the performance of ESPACE on three different families of large language models (LLMs): GPT3, Llama2, and Nemotron4. For each model, different compression rates are considered (20%, 47%, etc.). The table shows the GEMM (General Matrix Multiplication) latency, the time to the first token (TTFT) in Megatron-LM, the perplexity on the Wikitext-103 dataset, and the average downstream task accuracy across several tasks (BQ, HS, PQ, RA, WG).  This allows for a direct comparison of model size, inference speed, and accuracy between the original models and those compressed using ESPACE.", "section": "4 Model Compression Studies"}]