{"references": [{"fullname_first_author": "F. Xue", "paper_title": "To repeat or not to repeat: Insights from scaling LLM under token-crisis", "publication_date": "2023", "reason": "This paper provides insights into the scaling of LLMs, a key topic in the field of large language model compression."}, {"fullname_first_author": "Y. Liu", "paper_title": "Understanding LLMs: A comprehensive overview from training to inference", "publication_date": "2024", "reason": "This paper offers a comprehensive overview of LLMs, encompassing their training and inference aspects, which is relevant to the field of model compression."}, {"fullname_first_author": "G. Xiao", "paper_title": "Smoothquant: Accurate and efficient post-training quantization for large language models", "publication_date": "2023", "reason": "This paper introduces SmoothQuant, an accurate and efficient post-training quantization technique for LLMs, directly relevant to the paper's discussion of LLM compression techniques."}, {"fullname_first_author": "E. Frantar", "paper_title": "SparseGPT: Massive language models can be accurately pruned in one-shot", "publication_date": "2023", "reason": "This paper presents SparseGPT, a one-shot pruning technique for LLMs, which is compared against the proposed method in the current paper."}, {"fullname_first_author": "J. Lin", "paper_title": "Awq: Activation-aware weight quantization for LLM compression and acceleration", "publication_date": "2023", "reason": "This paper introduces activation-aware weight quantization, another LLM compression technique that is discussed and compared to the proposed method in the current paper."}]}