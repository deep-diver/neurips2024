[{"figure_path": "znBiAp5ISn/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison against baseline methods.", "description": "This table compares the performance of TAS-GNN against several baseline methods (ANN, SpikingGNN, SpikeNet, and PGNN) on five graph classification datasets (MUTAG, PROTEINS, ENZYMES, NCI1, and IMDB-BINARY).  The results show the accuracy of each model on each dataset, highlighting the improvements achieved by TAS-GNN compared to the baselines. The improvements are presented as a percentage increase in accuracy relative to the best-performing baseline method for each dataset.", "section": "5.2 Results on Graph Classification"}, {"figure_path": "znBiAp5ISn/tables/tables_6_2.jpg", "caption": "Table 1: Performance comparison against baseline methods.", "description": "This table compares the performance of the proposed TAS-GNN model against several baseline methods on five different graph classification datasets (MUTAG, PROTEINS, ENZYMES, NCI1, IMDB-BINARY).  For each dataset, the table shows the accuracy achieved by various models, including standard Artificial Neural Networks (ANNs) and existing Spiking Graph Neural Networks (SNNs).  The improvement achieved by TAS-GNN over the best-performing baseline is shown in parentheses for each dataset.", "section": "5.2 Results on Graph Classification"}, {"figure_path": "znBiAp5ISn/tables/tables_7_1.jpg", "caption": "Table 6: Extended sensitivity study on threshold learning rate.", "description": "This table presents the results of an extended sensitivity study on the threshold learning rate (\u03b7) for three different GNN models (GCN, GAT, GIN) across various datasets. It shows how the accuracy of each model varies with different learning rates.  The results illustrate the impact of different learning rates on model performance and show the optimal range of learning rate for each model and dataset.", "section": "5.4 Sensitivity Study"}, {"figure_path": "znBiAp5ISn/tables/tables_13_1.jpg", "caption": "Table 4: Summary of datasets used in the study.", "description": "This table presents a summary of the five graph datasets used in the study. For each dataset, it shows the number of graphs, the average number of nodes per graph, the number of nodes in the first graph, the average number of edges per graph, the number of edges in the first graph, and the number of classes.  This information helps to characterize the datasets used and provides context for the experimental results.", "section": "A.3 Detailed Experiment Settings"}, {"figure_path": "znBiAp5ISn/tables/tables_14_1.jpg", "caption": "Table 1: Performance comparison against baseline methods.", "description": "This table compares the performance of TAS-GNN against several baseline methods (SpikingGNN [64], SpikeNet [32], PGNN [16]) and conventional ANN models across five benchmark graph datasets (MUTAG, PROTEINS, ENZYMES, NCI1, IMDB-BINARY).  It shows the accuracy (%) achieved by each method on each dataset, highlighting the improvement achieved by TAS-GNN in most cases.", "section": "5.2 Results on Graph Classification"}, {"figure_path": "znBiAp5ISn/tables/tables_17_1.jpg", "caption": "Table 5: Comparison on using different number of degree group", "description": "This table presents a comparison of the performance of Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Graph Isomorphism Networks (GINs) across five graph datasets (MUTAG, PROTEINS, ENZYMES, NCI1, and IMDB-BINARY) using varying numbers of degree groups for topology-aware group-adaptive neurons. The results show how performance changes as the number of degree groups increases from 1 to the maximum number of degrees observed in each dataset.  This table helps illustrate the impact of topology-aware grouping on the effectiveness of the proposed TAS-GNN.", "section": "5. Evaluation"}, {"figure_path": "znBiAp5ISn/tables/tables_18_1.jpg", "caption": "Table 6: Extended sensitivity study on threshold learning rate.", "description": "This table presents the results of an extended sensitivity analysis on the threshold learning rate (\u03b7) for the proposed TAS-GNN model. It shows the accuracy achieved on five different graph datasets (MUTAG, PROTEINS, ENZYMES, NCI1, and IMDB-Binary) using three different GNN architectures (GCN, GAT, and GIN) across a range of learning rates (0.001, 0.005, 0.01, 0.05, 0.1, and 0.5).  The table highlights how sensitive the model's performance is to the choice of the learning rate and helps determine the optimal learning rate for different datasets and architectures.", "section": "5.4 Sensitivity Study"}]