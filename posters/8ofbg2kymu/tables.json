[{"figure_path": "8Ofbg2KYMu/tables/tables_8_1.jpg", "caption": "Table 1: FP16 forward pass benchmark overview. We benchmark naive neighborhood attention kernels against our proposed GEMM and fused kernels in half precision, over a large set of problem sizes varying in batch size, spatial size, number of attention heads, and dimensions per head, and over different window sizes and dilation values. For every problem size, we also benchmarked self attention running with the xFormers FMHA (our baseline) and Flash Attention V2.", "description": "This table presents the results of a benchmark comparing the performance of naive, GEMM-based, and fused neighborhood attention kernels against standard self-attention methods (FMHA and FlashAttention V2) using FP16 precision.  The benchmark considers various problem sizes with different parameters (batch size, spatial size, number of heads, dimensions per head, window size, and dilation). The table shows the percentage of benchmark problems where each neighborhood attention approach matched or outperformed the others.", "section": "4 Experiments"}, {"figure_path": "8Ofbg2KYMu/tables/tables_8_2.jpg", "caption": "Table 1: FP16 forward pass benchmark overview. We benchmark naive neighborhood attention kernels against our proposed GEMM and fused kernels in half precision, over a large set of problem sizes varying in batch size, spatial size, number of attention heads, and dimensions per head, and over different window sizes and dilation values. For every problem size, we also benchmarked self attention running with the xFormers FMHA (our baseline) and Flash Attention V2.", "description": "This table presents a benchmark comparing the performance of naive, GEMM-based, and fused neighborhood attention kernels against standard self-attention methods (FMHA and FlashAttention V2) using FP16 precision.  The benchmark covers a wide range of problem sizes and parameters (batch size, spatial dimensions, number of heads, window size, dilation). The results show the percentage of problems where each neighborhood attention method outperforms or matches the performance of naive kernels and self-attention baselines.", "section": "4 Experiments"}, {"figure_path": "8Ofbg2KYMu/tables/tables_9_1.jpg", "caption": "Table 3: Forward pass benchmark breakdown. Both GEMM-based and fused NA improve the baseline naive kernels on average. However, there exist cases in which naive kernels may be preferable to GEMM-based in both FP16 and FP32, but naive is rarely a good choice in half precision where both naive and GEMM are more memory bandwidth bound than fused.", "description": "This table presents a breakdown of the forward pass benchmark results, comparing the performance of GEMM-based and fused neighborhood attention (NA) kernels against naive kernels. It shows the average, minimum, and maximum percentage improvement achieved by GEMM-based and fused NA kernels over naive kernels in both FP16 and FP32 precision. The table also highlights cases where naive kernels might outperform GEMM-based kernels.", "section": "4 Experiments"}, {"figure_path": "8Ofbg2KYMu/tables/tables_12_1.jpg", "caption": "Table 4: Forward + backward pass benchmark breakdown. Improvements over naive, while not as significant as in the forward pass, are still significant. We report benchmark the full forward and backward pass in half precision only, because most training is done in lower precision.", "description": "This table shows the performance improvement of GEMM-based and fused neighborhood attention kernels compared to naive kernels for both forward and backward passes in half-precision.  It provides average, minimum, and maximum improvements for 1D, 2D, and 3D problems.  The results highlight that while fused kernels generally offer the best performance, there are some cases where naive or GEMM-based kernels might perform better, particularly in the backward pass.", "section": "Additional experiments"}, {"figure_path": "8Ofbg2KYMu/tables/tables_13_1.jpg", "caption": "Table 5: Model-level throughput changes when using our proposed GEMM-based and fused kernels in ImageNet classification. Hierarchical vision transformers NAT and DiNAT can see between 26% to 104% improvement in FP16 throughput on an A100 (batch size 128) with our proposed fused kernel. Suffering from the memory alignment issue, our half precision GEMM kernels usually result in a much smaller improvement over naive kernels, particularly the tiled variants. The same measurements with FP32 precision are presented in Tab. 6.", "description": "This table presents the results of applying GEMM-based and fused neighborhood attention kernels to several ImageNet classification models (NAT and DiNAT variants).  It shows the throughput improvements (in imgs/sec) and top-1 accuracy for various model sizes and configurations, comparing the naive, GEMM-based, and fused kernel implementations. The table highlights significant improvements in FP16 throughput for the fused kernels, particularly in larger models.  It also notes that the half-precision GEMM kernels show less improvement due to memory alignment issues.", "section": "5 Future work & Conclusion"}, {"figure_path": "8Ofbg2KYMu/tables/tables_14_1.jpg", "caption": "Table 6: Model-level throughput changes when using our proposed GEMM-based and fused kernels in ImageNet classification (full precision). While fused attention kernels are not expected to have as large of an edge over BMM-style attention kernels in FP32, our fused kernels still happen to outperform naive kernels in full precision. It is also visible that our GEMM kernels can outperform naive kernels when we eliminate the memory alignment issue. That said, our FP32 GEMM kernels still impose a maximum alignment of 1 element on the attention weights tensor, which limits its ability to compete with other BMM-style attention kernels.", "description": "This table presents the results of experiments evaluating the performance of GEMM-based and fused neighborhood attention kernels in ImageNet classification using full precision (FP32).  It compares the throughput (images per second) and top-1 accuracy of different models (NAT and DiNAT variants with varying sizes) using naive, GEMM-based, and fused kernels. The table highlights the performance gains achieved by the proposed kernels, particularly in scenarios where memory alignment issues are mitigated.", "section": "4 Experiments"}, {"figure_path": "8Ofbg2KYMu/tables/tables_14_2.jpg", "caption": "Table 7: Model-level throughput changes when using our proposed GEMM-based and fused kernels in style-based image generation. We benchmark StyleNAT [24], a style-based generative adversarial model based on neighborhood attention under different kernels. We experimented with different batch sizes in order to achieve peak performance, and settled for 64 for the 256 \u00d7 256 variant, and 8 for the 1024 \u00d7 1024. StyleNAT does not recommend lower-precision, therefore these measurements are only done in FP32.", "description": "This table shows the performance improvement of the StyleNAT model using different attention kernel implementations (Naive, GEMM, Fused).  It highlights the throughput (images per second) and FID (Fr\u00e9chet Inception Distance) scores for two different resolutions (256x256 and 1024x1024) of the FFHQ dataset. The improvements are shown as percentage increases compared to the naive kernel.", "section": "4 Experiments"}, {"figure_path": "8Ofbg2KYMu/tables/tables_15_1.jpg", "caption": "Table 8: Training time improvement when using fused neighborhood attention kernels. We ran each of the classification models based on neighborhood attention for one warmup epoch and one benchmark epoch, all with half precision (the typical training scenario), and report the estimated training time. Note that these numbers exclude positional biases, as our fused backward kernel does not support it.", "description": "This table presents the training time improvements observed when using fused neighborhood attention kernels compared to naive and GEMM-based kernels.  The results are shown for various models (NAT-M, DINAT-M, etc.) with different numbers of parameters and FLOPs.  Training time is estimated based on one warmup and one benchmark epoch, using half precision.  The table indicates percentage changes in training time for each kernel type (Naive, GEMM, and Fused) relative to the naive approach.  Note that positional biases are excluded as the fused backward kernel doesn't support them.", "section": "4 Experiments"}]