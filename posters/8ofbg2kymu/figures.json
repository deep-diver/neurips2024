[{"figure_path": "8Ofbg2KYMu/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of average improvement in speed on A100 from our proposed implementation. Baseline is the set of naive CUDA kernels introduced in Neighborhood Attention Transformer [9]. GEMM-based NA improves 1-D problems by an average of 548% (forward pass) and 502% (forward + backward), and 2-D problems by an average of 193% (forward pass) and 92% (forward + backward). GEMM-based NA does not implement 3-D problems yet. Fused NA boosts performance further and improves 1-D problems by an average of 1759% (forward pass) and 844% (forward + backward), and 2-D problems by an average of 958% (forward pass) and 385% (forward + backward), and 3-D problems by an average of 1135% (forward pass) and 447% (forward + backward).", "description": "This figure shows the speedup achieved by the proposed GEMM-based and fused neighborhood attention methods compared to naive CUDA kernels on NVIDIA A100 GPUs.  It presents the average speedup for 1D, 2D, and 3D problems, separated into forward pass only and forward plus backward pass scenarios.  The results demonstrate significant performance gains for both methods, particularly in fused neighborhood attention.", "section": "Introduction"}, {"figure_path": "8Ofbg2KYMu/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of the spectrum of possible attention patterns provided by neighborhood attention. Neighborhood attention only attempts to center the query token (red) within the context window (blue), unlike sliding window attention [19] which forces it. Neighborhood attention with window size 1 is equivalent to linear projection (\u201cno attention\u201d). Neighborhood attention approaches self attention as window size grows, and matches it when equal to input size. Dilation introduces sparse global context, and causal masking prevents interaction between query tokens that have a smaller coordinate than neighboring context tokens along the corresponding mode. Window size, dilation, and whether or not causally masked, can be defined per mode/axis.", "description": "This figure shows how neighborhood attention patterns vary based on window size and dilation, illustrating the spectrum between linear projection and self-attention.  It highlights the flexibility of neighborhood attention in controlling the attention span and sparsity.", "section": "2 Related works"}, {"figure_path": "8Ofbg2KYMu/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of our GEMM-based implementation of the 2-D PN operation. Input tensors Q and K are tiled according to their 2-D spatial layout. Q is tiled with a static tile shape, Th \u00d7 Tw. K is tiled with a haloed shape of the Q tile, Th \u00d7 Tu, which is a function of the attention window size (kh \u00d7 kw) and the Q tile coordinates. Once tiles are moved into local memory, they are viewed in matrix layout, and a ThTw \u00d7 TT \u00d7 d shaped GEMM is computed (d is embedding dim). Once done, the tile of dot products with shape ThTw \u00d7 ThTw is scattered into valid attention weights of shape Th \u00d7 Tw \u00d7 khkw.", "description": "This figure illustrates the GEMM-based implementation of the 2D Pointwise-Neighborhood (PN) operation in neighborhood attention.  It shows how input tensors Q and K are tiled spatially, with K having a halo to accommodate the attention window. These tiles are then processed using a GEMM operation, and the resulting dot products are scattered to form the attention weights.", "section": "3 Methodology"}, {"figure_path": "8Ofbg2KYMu/figures/figures_6_1.jpg", "caption": "Figure 4: A simplified illustration of fused neighborhood attention. Q and KV tensors are tiled according to their spatial layout (1-D, 2-D, 3-D), with the latter haloed to include the entire neighborhood for all corresponding queries in the query tile. Resulting attention weights from the first GEMM are masked according to neighborhood attention parameters, before undergoing online softmax scaling, and going through the second GEMM with the corresponding value sub-tile.", "description": "This figure illustrates the process of fused neighborhood attention.  Query (Q) and Key/Value (K/V) tensors are tiled based on their spatial dimensions (1D, 2D, or 3D). The K/V tiles are expanded to include the neighborhood context. The first GEMM operation produces attention weights, which are then masked according to the neighborhood attention parameters and undergo online softmax. A second GEMM operation combines these weights with the Value (V) sub-tiles to produce the final output.", "section": "3.3 Fused NA"}]