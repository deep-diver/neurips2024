[{"figure_path": "YIxKeHQZpi/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of UNIT Architecture. The model processes high-resolution documents and low-resolution images, generating a set of visual tokens. These tokens pass through an input embedding layer, with document tokens fed into the language decoder to predict text sequences, enhancing the model's text recognition capability. Simultaneously, to preserve the model's original image encoding ability, the visual tokens from natural images are reconstructed via a lightweight vision decoder, mimicking the output of the teacher model. Additionally, an image captioning task is included alongside the OCR task to further enhance image understanding.", "description": "The figure shows the architecture of the UNIT model, which processes both high-resolution documents and low-resolution images.  It uses a unified vision encoder, a language decoder for text recognition, and a vision decoder to prevent catastrophic forgetting of image encoding abilities.  The model is trained with multiple objectives: OCR, image captioning, and vision reconstruction.", "section": "3.2 Text Recognition Ability Enhancement"}, {"figure_path": "YIxKeHQZpi/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of the UNIT training paradigm. The (a) intra-scale pretraining stage processes images and documents at their commonly used resolutions to integrate basic text recognition with existing image recognition capabilities. The (b) inter-scale finetuning stage processes scale-exchanged data and tasks to enhance scale robustness, benefiting downstream document analysis tasks when integrated into (c) LVLMs applications.", "description": "This figure illustrates the UNIT training framework, which consists of three stages.  (a) Intra-scale pretraining: The model is trained on images and documents at their typical resolutions to learn both image and text recognition. (b) Inter-scale finetuning: The model is further trained on scale-exchanged data (high-resolution images, low-resolution documents) to improve robustness. (c) Application in LVLMs: The trained UNIT model is integrated into Large Vision-Language Models (LVLMs) for downstream tasks such as visual question answering and document analysis.", "section": "3.4 Intra-Scale Pretraining"}, {"figure_path": "YIxKeHQZpi/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of UNIT Architecture. The model processes high-resolution documents and low-resolution images, generating a set of visual tokens. These tokens pass through an input embedding layer, with document tokens fed into the language decoder to predict text sequences, enhancing the model's text recognition capability. Simultaneously, to preserve the model's original image encoding ability, the visual tokens from natural images are reconstructed via a lightweight vision decoder, mimicking the output of the teacher model. Additionally, an image captioning task is included alongside the OCR task to further enhance image understanding.", "description": "This figure provides a detailed overview of the UNIT architecture.  It shows how the model processes both high-resolution documents and low-resolution images, creating visual tokens that are fed into both a language decoder (for text recognition) and a vision decoder (to preserve the original image encoding capabilities).  The inclusion of image captioning further enhances the model's understanding of natural images. The diagram highlights the key components and their interconnections within the UNIT framework.", "section": "3.2 Text Recognition Ability Enhancement"}]