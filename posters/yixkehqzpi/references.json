{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces the OpenCLIP model, which is the foundation for the vision encoder used in UNIT, demonstrating a strong influence on the architecture and approach."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This foundational paper introduces Vision Transformers (ViTs), which are central to UNIT's architecture, forming the backbone for its image and text recognition capabilities."}, {"fullname_first_author": "J. Li", "paper_title": "BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-07-18", "reason": "This work is highly relevant due to its focus on unified vision-language models, which aligns with the core goal of UNIT, offering a similar multi-modal approach."}, {"fullname_first_author": "Z. Liu", "paper_title": "Swin transformer: Hierarchical vision transformer using shifted windows", "publication_date": "2021-10-27", "reason": "This paper introduces the Swin Transformer architecture, a highly influential improvement to ViTs, and its impact on UNIT's performance, especially with high-resolution images, is significant."}, {"fullname_first_author": "H. Wei", "paper_title": "Vary: Scaling up the vision vocabulary for large vision-language models", "publication_date": "2023-12-10", "reason": "The Vary model, presented in this paper, directly addresses the challenge of high-resolution document processing, offering insights and techniques that inform UNIT's multi-scale approach."}]}