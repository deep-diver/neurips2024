{"references": [{"fullname_first_author": "R. Aljundi", "paper_title": "Memory aware synapses: Learning what (not) to forget", "publication_date": "2018-00-00", "reason": "This paper is foundational to the field of continual learning by introducing the concept of memory-aware synapses to mitigate catastrophic forgetting."}, {"fullname_first_author": "M. De Lange", "paper_title": "A continual learning survey: Defying forgetting in classification tasks", "publication_date": "2021-00-00", "reason": "This survey paper provides a comprehensive overview of continual learning, summarizing different approaches and challenges in the field."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper introduces the vision transformer (ViT), a powerful architecture used extensively in the paper's prompt-based continual learning method."}, {"fullname_first_author": "J. Kirkpatrick", "paper_title": "Overcoming catastrophic forgetting in neural networks", "publication_date": "2017-00-00", "reason": "This seminal paper highlights the problem of catastrophic forgetting in neural networks and proposes a regularization-based solution."}, {"fullname_first_author": "G. I. Parisi", "paper_title": "Continual lifelong learning with neural networks: A review", "publication_date": "2019-00-00", "reason": "This review paper provides a broad overview of continual learning, focusing on neural network approaches and various techniques to address catastrophic forgetting."}]}