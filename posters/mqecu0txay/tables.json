[{"figure_path": "MqeCU0tXAY/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of channel selection (Q = 400) with the CLIP zero-shot on Office Home benchmark", "description": "This table presents a comparison of the performance of CLIP with all its features versus CLIP using only the top 400 channels selected based on minimizing inter-domain variance and maximizing inter-class variance, evaluated on the OfficeHome dataset using a zero-shot approach.  The results show that a simple feature selection technique can improve the performance of the CLIP zero-shot model.", "section": "1 Introduction"}, {"figure_path": "MqeCU0tXAY/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark.  denotes ResNet-50 backbone; denotes frozen CLIP ViT-B/16 encoder; denotes fine-tuning the entire CLIP ViT-B/16 encoder, * denotes the two rounds inference-time fine-tuning. Red and  indicate the best performance in each group.", "description": "This table compares the performance of the proposed CLIPCEIL model with several state-of-the-art domain generalization methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet).  It shows the average accuracy for each method on each dataset, highlighting the superior performance of CLIPCEIL and its variant, CLIPCEIL++, which fine-tunes the entire CLIP model.  Different model architectures (ResNet-50 and CLIP ViT-B/16) are included for a comprehensive comparison.  The table distinguishes between methods that freeze the CLIP encoder and those that fine-tune it, and further differentiates between methods using inference-time fine-tuning.", "section": "4.2 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_7_2.jpg", "caption": "Table 3: Ablation study of each loss in our objective function on OfficeHome dataset.", "description": "This table presents the ablation study results on the OfficeHome dataset, evaluating the impact of different loss components on the model's performance. It compares the performance of the model using only the cross-entropy loss, adding multi-scale feature fusion, incorporating the channel refinement loss (Lref), adding the direction loss (Ldir), and finally, the full model with all components combined. The results show the contribution of each component in improving the model's generalization ability.", "section": "4.3 Ablation Studies"}, {"figure_path": "MqeCU0tXAY/tables/tables_8_1.jpg", "caption": "Table 3: Ablation study of each loss in our objective function on OfficeHome dataset.", "description": "This ablation study investigates the individual and combined effects of the three loss terms in the CLIPCEIL model (Channel Refinement loss, Direction loss, and Cross-Entropy loss) on the OfficeHome dataset.  It compares the performance of CLIPCEIL with different combinations of these losses against the baseline zero-shot performance.  The results show how each loss term contributes to the model's improved performance and the synergistic effect when they are combined.", "section": "4.3 Ablation Studies"}, {"figure_path": "MqeCU0tXAY/tables/tables_14_1.jpg", "caption": "Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark.  denotes ResNet-50 backbone;  denotes frozen CLIP ViT-B/16 encoder;  denotes fine-tuning the entire CLIP ViT-B/16 encoder, * denotes the two rounds inference-time fine-tuning. Red and  indicate the best performance in each group.", "description": "This table compares the performance of the proposed CLIPCEIL model against several state-of-the-art domain generalization methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet).  It shows the average accuracy for each method on each dataset, highlighting the best-performing methods in each category. The table also distinguishes between methods using ResNet-50 and CLIP ViT-B/16 backbones, and those that use fine-tuning versus frozen CLIP encoders.  The results demonstrate the superior performance of CLIPCEIL compared to other methods.", "section": "4 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_14_2.jpg", "caption": "Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark.  denotes ResNet-50 backbone; denotes frozen CLIP ViT-B/16 encoder; denotes fine-tuning the entire CLIP ViT-B/16 encoder, * denotes the two rounds inference-time fine-tuning. Red and  indicate the best performance in each group.", "description": "This table compares the performance of the proposed CLIPCEIL model against several state-of-the-art (SOTA) domain generalization methods.  It shows the average accuracy across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet).  Different model variations are presented (ResNet-50, frozen CLIP ViT-B/16 encoder, fine-tuned CLIP ViT-B/16 encoder), highlighting the superior performance of CLIPCEIL.", "section": "4.2 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_15_1.jpg", "caption": "Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark.  denotes ResNet-50 backbone; denotes frozen CLIP ViT-B/16 encoder; denotes fine-tuning the entire CLIP ViT-B/16 encoder, * denotes the two rounds inference-time fine-tuning. Red and  indicate the best performance in each group.", "description": "This table compares the performance of the proposed CLIPCEIL model against several state-of-the-art (SOTA) domain generalization methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet).  It shows the average accuracy of each method on each dataset, highlighting the superior performance of CLIPCEIL and its variants (CLIPCEIL++) compared to other approaches that use either ResNet-50 or CLIP as the base model.  The table also indicates whether methods fine-tune the whole model, only a part, or use inference-time fine-tuning.", "section": "4.2 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_15_2.jpg", "caption": "Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark.  denotes ResNet-50 backbone; denotes frozen CLIP ViT-B/16 encoder; denotes fine-tuning the entire CLIP ViT-B/16 encoder, * denotes the two rounds inference-time fine-tuning. Red and  indicate the best performance in each group.", "description": "This table compares the performance of the proposed CLIPCEIL model against several state-of-the-art (SOTA) domain generalization methods.  It shows the average accuracy across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet).  Different model variations are included, such as those using ResNet-50 and CLIP ViT-B/16 backbones, and those that fine-tune the entire CLIP model or employ inference-time fine-tuning.  The table highlights CLIPCEIL's superior performance.", "section": "4 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_16_1.jpg", "caption": "Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark.  denotes ResNet-50 backbone;  denotes frozen CLIP ViT-B/16 encoder;  denotes fine-tuning the entire CLIP ViT-B/16 encoder, * denotes the two rounds inference-time fine-tuning. Red and  indicate the best performance in each group.", "description": "This table compares the performance of the proposed CLIPCEIL model with several state-of-the-art domain generalization (DG) methods across five benchmark datasets (PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet).  It shows the average accuracy for each method on each dataset and highlights the best-performing methods in each group, considering different model architectures (ResNet-50, frozen CLIP ViT-B/16, fine-tuned CLIP ViT-B/16) and inference-time fine-tuning.", "section": "4.2 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_16_2.jpg", "caption": "Table 9: Detailed comparison of our proposed method with the State-of-the-art methods on the VLCS dataset. * denotes the models that utilize the ResNet-50 backbone, and the rest utilize CLIP ViT-B/16 backbone.", "description": "This table compares the performance of the proposed CLIPCEIL model with several state-of-the-art domain generalization methods on the VLCS benchmark dataset.  The comparison includes ResNet-50 based models and CLIP-based models. CLIPCEIL demonstrates superior performance compared to other methods on this dataset.  The table shows the average accuracy across different domains for each model.", "section": "4.2 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_17_1.jpg", "caption": "Table 10: Detailed comparison of our proposed method with the State-of-the-art methods on the OfficeHome dataset. * denotes the models that utilize the ResNet-50 backbone, and the rest utilize CLIP ViT-B/16 backbone.", "description": "This table presents a detailed comparison of the proposed CLIPCEIL method with several state-of-the-art domain generalization methods on the OfficeHome benchmark dataset.  It compares the average accuracy across four different domains (Art, Clipart, Product, Real) and overall average accuracy. The models compared use either ResNet-50 or CLIP ViT-B/16 as the backbone architecture, which is noted in the table.", "section": "4.2 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_17_2.jpg", "caption": "Table 11: Detailed comparison of our proposed method with the State-of-the-art methods on the TerraIncognita dataset. * denotes the models that utilize the ResNet-50 backbone, and the rest utilize the CLIP ViT-B/16 backbone.", "description": "This table compares the performance of the proposed CLIPCEIL model with other state-of-the-art domain generalization methods on the TerraIncognita dataset.  It shows the average accuracy achieved by each method across four different sub-datasets (L100, L38, L43, L46) of the TerraIncognita dataset.  The table highlights the superior performance of CLIPCEIL in comparison to other methods, demonstrating its effectiveness in handling domain shift during generalization.", "section": "4.2 Main Results"}, {"figure_path": "MqeCU0tXAY/tables/tables_20_1.jpg", "caption": "Table 13: Performance with different backbones on OfficeHome datasets.", "description": "This table compares the performance of CLIPCEIL using different backbones (ResNet-50 and ViT-based models) on the OfficeHome dataset.  It shows the average accuracy across various image categories (Art, Clipart, Product, Real) and compares it to the performance of other models (SAGM, SWAD, DomainDrop, DISPEL, CLIP Zero-shot). This helps to understand the impact of the backbone architecture on the performance of the proposed method.", "section": "4.3 Ablation Studies"}, {"figure_path": "MqeCU0tXAY/tables/tables_20_2.jpg", "caption": "Table 14: Performance of a linear layer adapter g on OfficeHome dataset with ViT-B/16 backbone.", "description": "This table presents the ablation study on the architecture of the adapter g, comparing different designs, including a single linear projector, a linear projector with added refinement and direction loss, average pooling, a two-layer MLP, and the final CLIPCEIL model with a transformer layer.  The results show the average accuracy across four domains (A, C, P, R) of the OfficeHome dataset for each adapter architecture, demonstrating the impact of each design choice on model performance.", "section": "4.3 Ablation Studies"}, {"figure_path": "MqeCU0tXAY/tables/tables_20_3.jpg", "caption": "Table 15: Performance comparison with text encoder adapter with ViT-B/16 backbone.", "description": "This table presents a comparison of the performance of using both visual and text multi-scale adapters versus using only a visual multi-scale adapter in the CLIPCEIL model.  The results are shown for the Art, Clipart, Product, and Real categories of the OfficeHome dataset, along with the average performance across all four categories.  It highlights the relative contribution of visual versus combined visual and textual features for improved generalization.", "section": "4.3.3 Architecture of adapter g"}]