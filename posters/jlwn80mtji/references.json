{"references": [{"fullname_first_author": "Daniel Soudry", "paper_title": "The implicit bias of gradient descent on separable data", "publication_date": "2018-00-00", "reason": "This paper is foundational for the study of implicit bias in gradient descent, introducing the exponential tail property and establishing the connection between the loss function and the learned model's simplicity."}, {"fullname_first_author": "Ziwei Ji", "paper_title": "The implicit bias of gradient descent on nonseparable data", "publication_date": "2019-00-00", "reason": "This paper extends the theory of implicit bias beyond linearly separable data, which is a significant step in making it more applicable to real-world scenarios."}, {"fullname_first_author": "Mor Shpigel Nacson", "paper_title": "Convergence of gradient descent on separable data", "publication_date": "2019-00-00", "reason": "This paper provides a refined analysis of convergence rates for gradient descent in the separable case, which is essential for understanding the dynamics of implicit bias."}, {"fullname_first_author": "Yutong Wang", "paper_title": "Unified binary and multiclass margin-based classification", "publication_date": "2024-00-00", "reason": "This paper introduces the PERM loss framework, which is crucial to the multiclass generalization of implicit bias results in this paper."}, {"fullname_first_author": "Ohad Shamir", "paper_title": "Gradient methods never overfit on separable data", "publication_date": "2021-00-00", "reason": "This paper provides a strong theoretical guarantee that gradient methods do not overfit on separable data, supporting the concept of implicit bias."}]}