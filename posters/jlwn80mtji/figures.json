[{"figure_path": "JlWn80mTJi/figures/figures_3_1.jpg", "caption": "Figure 1: An illustration of the exponential tail property for the cross entropy/multinomial logistic loss when K = 3. Panel a. Plot of \u03c8(u) = log(1 + exp(-u\u2081) + exp(-u\u2082)), the template for the multinomial logistic loss. Note that the complement of the positive orthant in the domain R\u00b2 is shown in gray. Panel b. and c. Plot of the upper bound (shown in black) and lower bounds (red) of \u2202\u03c8/\u2202u\u2081 (blue) respectively. These bounds are from Appendix C.1.3 where u\u00b0 = 0 and c = 1. Note that the lower bound is valid in the positive orthant, i.e., the red surface is below the blue one there.", "description": "Figure 1 illustrates the exponential tail property for the cross-entropy loss. Panel (a) shows the template function for the multinomial logistic loss which is a 2D function. Panels (b) and (c) show the upper and lower bounds for the partial derivative of the template function. The lower bound is valid in the positive orthant.", "section": "Regularity assumptions on loss functions"}, {"figure_path": "JlWn80mTJi/figures/figures_28_1.jpg", "caption": "Figure 2: Small simulation with N = 10, d = 2 and K = 3. The loss used is the \u201cPairLogLoss\u201d. Top row. Decision regions of classifiers along the gradient path w(t) at t = 100, 1000, and 100000, respectively from left to right. Bottom row. Decision regions of the hard-margin multiclass SVM. Note that most of the progress is made between iterations 100 and 1000.", "description": "This figure shows a small simulation with 10 data points in 2 dimensions and 3 classes.  The PairLogLoss function was used. The top row displays the decision regions of the classifiers along the gradient descent path at different iteration counts (100, 1000, and 100000). The bottom row shows the decision regions for the hard-margin multiclass SVM.  The figure highlights that most of the convergence towards the hard-margin SVM happens in the early iterations.", "section": "Main Result"}, {"figure_path": "JlWn80mTJi/figures/figures_29_1.jpg", "caption": "Figure 3: Large simulations with N = 100, d = 10 and K = 3. The loss used is the \u201cPairLogLoss\u201d. The curves are 10 independent runs with randomly sampled data and random initialization for gradient descent over 100000 iterations. Note that the convergence in direction of the gradient descent iterates to the hard-margin SVM slows down in log-log space.", "description": "This figure shows the results of 10 independent large simulations using the PairLogLoss function.  Each simulation involves 100 data points (N=100) in 10 dimensions (d=10), with 3 classes (K=3).  The y-axis shows the ratio of the Frobenius norm of the weight matrix at a given iteration (||W(t)||F) to the Frobenius norm of the weight matrix of the hard margin SVM solution (||\u0175||F).  The x-axis indicates the number of gradient descent steps.  The plot shows that convergence to the hard-margin solution happens slowly in log-log space, indicating a logarithmic convergence rate.", "section": "I Additional Experiments"}]