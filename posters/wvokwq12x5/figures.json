[{"figure_path": "WvoKwq12x5/figures/figures_3_1.jpg", "caption": "Figure 1: The sequential pipeline for developing PediatricsGPT. We begin by injecting intensive medical and world knowledge into the foundation model through the hybrid instruction mechanism in CPT phase. Then, full-parameter SFT is implemented to improve the model's instruction-following capabilities regarding medical generalists. After that, we introduce the direct following preference optimization to control the model behaviour to align with human preference. In the parameter-efficient SFT phase, the LoRA-based mixture of universal-specific experts is devised to mitigate conflicts across downstream tasks and competition between pediatric expertise and general mastery.", "description": "This figure illustrates the training pipeline of PediatricsGPT, highlighting the key steps: 1) Continuous Pre-training (CPT) using a hybrid instruction mechanism to incorporate medical and world knowledge; 2) Full-parameter Supervised Fine-tuning (SFT) to enhance instruction following for medical generalists; 3) Direct Following Preference Optimization (DFPO) to align model behavior with human preferences; and 4) LoRA-based Parameter-efficient SFT with a mixture of universal-specific experts to address conflicts between pediatric expertise and general medical knowledge.  The figure visually depicts the data used at each stage, such as PedCorpus and its sub-datasets, and how they contribute to the final model. ", "section": "3 Methodology"}, {"figure_path": "WvoKwq12x5/figures/figures_6_1.jpg", "caption": "Figure 2: Response comparisons of PediatricsGPT-13B with other baselines via GPT-4 evaluation.", "description": "This figure presents a comparison of the performance of PediatricsGPT-13B against other baseline models using GPT-4 evaluation.  It shows the win rate, tie rate, and loss rate for PediatricsGPT-13B against each of the other models across three different pediatric medical benchmarks: MedKQ&A, EviDiag, and TreRecom. The results are presented visually using bar charts, allowing for easy comparison of the model's performance on different tasks.", "section": "4.3 Comparison with State-of-the-art Methods"}, {"figure_path": "WvoKwq12x5/figures/figures_6_2.jpg", "caption": "Figure 2: Response comparisons of PediatricsGPT-13B with other baselines via GPT-4 evaluation.", "description": "This figure presents the results of a comparative analysis between PediatricsGPT-13B and several other baseline models using GPT-4 as an evaluation tool.  It visually represents the win rate, tie rate, and loss rate for each model across three different pediatric medical benchmarks: MedKQ&A, EviDiag, and TreRecom.  The color-coded bars offer a quick comparison of the performance of each model, allowing for easy identification of the best-performing model for each benchmark.", "section": "4.3 Comparison with State-of-the-art Methods"}, {"figure_path": "WvoKwq12x5/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison results of different models on the CMD benchmark.", "description": "The figure shows a bar chart comparing the GLEU scores achieved by various large language models (LLMs) across different medical departments.  The LLMs are tested on the Chinese Medical Dialogue (CMD) benchmark.  The chart shows the performance of PediatricsGPT-7B and -13B compared to other LLMs, such as Baichuan2-7B, Baichuan2-13B, HuatuoGPT, DISC-MedLLM, Zhongjing, HuatuoGPT-II, and ChatGPT. Each bar represents an LLM's performance in a specific medical department (Gynecology, Internal Medicine, Andrology, Oncology, Pediatrics, and Surgery). The height of the bar represents the GLEU score, a metric measuring the quality of the model's generated text. The results demonstrate the superior performance of PediatricsGPT models in multiple medical domains.", "section": "4.3 Comparison with State-of-the-art Methods"}, {"figure_path": "WvoKwq12x5/figures/figures_7_2.jpg", "caption": "Figure 4: Comparison results of different models on the CMD benchmark.", "description": "The figure shows a bar chart comparing the GLEU scores of various LLMs across different medical departments within the CMD benchmark.  The LLMs compared include Baichuan2-7B, Baichuan2-13B, HuatuoGPT, DISC-MedLLM, Zhongjing, HuatuoGPT-II, ChatGPT, PediatricsGPT-7B, and PediatricsGPT-13B.  The x-axis represents the different medical departments (Traditional Chinese Medicine, Internal Medicine, Oncology, Pediatrics, Dermatology, and Surgery), and the y-axis represents the GLEU score (%). The chart visually displays the performance of each LLM across various medical specializations, highlighting the relative strengths and weaknesses of each model in different domains.", "section": "4.3 Comparison with State-of-the-art Methods"}, {"figure_path": "WvoKwq12x5/figures/figures_8_1.jpg", "caption": "Figure 6: (a) and (b) show the effect of specific expert numbers on model performance and specific expert utilization in different task data, respectively.", "description": "This figure shows two subfigures. Subfigure (a) presents the effect of using different numbers of specific LoRA experts on the model's performance across three pediatric tasks (MedKQ&A, EviDiag, and TreRecom) and a general healthcare task. It illustrates that using three specific LoRA experts offers an optimal balance between performance and training efficiency. Subfigure (b) visualizes the normalized weights assigned to each specific LoRA expert during the routing process for each task. It showcases how the task type influences the utilization of different LoRA experts.", "section": "Experiments"}]