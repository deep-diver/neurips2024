[{"type": "text", "text": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dingkang Yang1,3 \u2020 Jinjie Wei1,3\u2020 Dongling Xiao2\u2020 Shunli Wang1\u00a7 Tong $\\mathbf{W}\\mathbf{u}^{2\\S}$ Gang $\\mathbf{L}\\mathbf{i}^{2\\S}$ Mingcheng $\\mathbf{Li}^{1\\S}$ Shuaibing Wang1\u00a7 Jiawei Chen1\u00a7 Yue Jiang1\u00a7 Qingyao $\\mathbf{X}\\mathbf{u}^{1\\S}$ $\\ensuremath{\\mathbf{Ke}}\\,\\ensuremath{\\mathbf{Li}}^{2\\,\\mathrm{8}}$ Peng Zhai1,3\u2217 Lihua Zhang1,3,4,5\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Academy for Engineering and Technology, Fudan University, Shanghai, China 2Tencent Youtu Lab, Shanghai, China 3Cognition and Intelligent Technology Laboratory, Shanghai, China 4Engineering Research Center of AI and Robotics, Ministry of Education, Shanghai, China $^5\\mathrm{AI}$ and Unmanned Systems Engineering Research Center of Jilin Province, Changchun, China {dkyang20, pzhai, lihuazhang}@fudan.edu.cn jjwei23@m.fudan.edu.cn, xdluestc@outlook.com tristanli@tencent.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. The project and data will be released at https://github.com/ydk122024/PediatricsGPT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As an essential component of medicine, pediatrics plays an indispensable role in ensuring children\u2019s health growth [22, 23]. The unbalanced distribution of healthcare resources [36] has resulted in a massive shortage of pediatricians, especially in populous countries led by China [37, 19]. With the rapid advances in LLMs exemplified by ChatGPT [33], developing intelligent pediatric consultation systems provides promise for enriching medical services. Although Chinese LLMs [18, 59, 2, 57, 20] have exhibited progress in general language understanding, they are incompetent in the pediatric medical field due to the lack of domain-specific discipline and specialized expertise injection. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To fulfil the interactive demands of Chinese medicine, preliminary efforts [8, 45, 50, 15] have enhanced LLMs\u2019 healthcare mastery through Supervised Fine-Tuning (SFT) training and medically relevant corpus collection. Despite improvements, challenges remain due to unavoidable dilemmas, including inadequate instruction data and vulnerable training procedures. Specifically, (i) existing instruction data typically involve vanilla rephrasing of the general medical corpus [50] or aggregation of doctor-like dialogues [56], which loses the specialization and focus in pediatric applications. More importantly, the current straightforward different round instruction construction paradigms [58, 15] fail to accommodate multi-task healthcare services in real-world scenarios, limiting the model generalization and inducing response hallucination. (ii) Furthermore, prior methods mostly relied on SFT to compensate for medical instruction following capabilities, ignoring the discrepancies between inherent and externally absorbed knowledge within the models. This single pattern causes secondary LLMs to lapse into excessive role-playing rather than understanding [40]. Despite a few attempts in the pre-training and Reinforcement Learning from Human Feedback (RLHF) phases [7, 34], their performance is restricted by actor-critic instability [41] and online sampling bias [61]. ", "page_idx": 1}, {"type": "text", "text": "Motivated by these observations, we construct PedCorpus, a high-quality dataset with over 300,000 instructions across single-turn and multi-turn medical conversations. Besides containing generalist healthcare data, PedCorpus incorporates multi-dimensional corpora from pediatric textbooks, guidelines, and knowledge graphs to ensure medical knowledge\u2019s accuracy. Vanilla instructions can also be readily extended to seed instructions for generating specialized corpora to serve different training phases. Furthermore, we integrate the well-presented GPT-4-distilled data with authentic doctor-patient dialogue data to standardize the fluency and faithfulness of instruction information. ", "page_idx": 1}, {"type": "text", "text": "Among our PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant with pediatric expertise and medical generalist. PediatricsGPT is developed on a systematic training pipeline that includes Continuous Pre-Training (CPT), full-parameter SFT, human preference alignment, and parameter-efficient secondary SFT. In this case, we introduce a hybrid instruction pre-training mechanism in CPT to bridge the capability weakening due to corpus format discrepancies between the internal and injected medical knowledge of foundation models, facilitating knowledge accumulation and extension. Meanwhile, a Direct Following Preference Optimization (DFPO) in human preference alignment is devised to enhance response robustness and align human preferences. Additionally, we present a mixture of universal-specific experts strategy to tackle the competency conflict between medical generalist and pediatric expertise in secondary SFT via Low-Rank Adaptation (LoRA) [27], which strengthens the model\u2019s adaptability to distinct downstream tasks. We conduct three pragmatic pediatric tasks to evaluate the different capabilities of existing models. Extensive experiments on pediatric and public benchmarks show that our PediatricsGPT outperforms open-source Chinese medical LLMs and baselines, yielding competitive performance compared to GPT-3.5-turbo. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Chinese Large Language Model Evolution. The emergence of Large Language Models (LLMs) dominated by ChatGPT [33] and GPT-4 [5] has revolutionized the paradigm for novel human-machine interaction. Driven by learning-oriented technologies [11\u201313, 53\u201355, 48], pragmatic instruction [32, 47] and preference optimization [7, 34] strategies enable LLMs to address complex generation tasks with aligned human intentions. Despite improvements, large-scale resources for training general LLMs [28, 43, 44] are anchored in the English corpora, limiting their abilities to respond reliably in extensive Chinese application scenarios. Recently, researchers [18, 59] have attempted to enhance the comprehension and execution of Chinese instructions in open-source LLMs by augmenting Chinese vocabulary and data (e.g., Chinese LLaMA and Alpaca [18]). To facilitate Chinese-specific demands, several LLMs trained from scratch exhibit remarkable Chinese proficiency due to multilingual data resources, such as the Baichuan [2, 52], General Language Model (GLM) [20, 57], and Qwen [6] families. In this work, the Baichuan2-Base series is utilized as the foundation model for our PediatricsGPT, given its comprehensive potential among similar contenders. ", "page_idx": 1}, {"type": "text", "text": "LLMs in Medical Applications. Current LLMs provide unprecedented opportunities to develop resource-efficient and diagnostic-comprehensive intelligent healthcare systems. Despite universal models [5, 33] equipped with certain internal knowledge regarding biomedicine, they are incompetent in real-world medical applications due to the absence of domain-specific disciplines. In this context, several efforts [45, 50, 15, 30] attempt to construct medically tailored LLMs from multiple perspectives. For instance, ChatDoctor [30] uses patient-doctor conversation data based on LLaMA [43] to enhance the language model\u2019s accuracy in healthcare. DoctorGLM [50] proves that a healthcarepurpose LLM can be implemented with affordable overhead by fine-tuning ChatGLM-6B [20]. After that, more Chinese medical LLMs [51, 8, 58, 14, 56] are progressively presented to generate doctor-like robust responses, such as HuatuoGPT [58], DISC-MedLLM [8], and Zhongjing [56]. Despite advances in general medical knowledge, current models are suboptimal for pressing pediatric applications. In comparison, our sophisticated training procedure and high-quality instruction datasets inject new insights and prospects for developing specialized LLMs with pediatric expertise. ", "page_idx": 1}, {"type": "table", "img_path": "WvoKwq12x5/tmp/6165b94c4f305127054f505d8f3324b3cba1b2344617a9919d48b38e86b7583a.jpg", "table_caption": ["Table 1: Statistical information on the proposed dataset. PedCorpus is well extensible and adaptable by incorporating general domain data and as seed instructions to generate specialized corpora (i.e., PedCorpus-CPT and PedCorpus-DFPO). \u201cKG\u201d means the Knowledge Graphs. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section describes the proposed PedCorpus dataset and the sequential pipeline for developing PediatricsGPT. Figure 1 illustrates the comprehensive method workflow. ", "page_idx": 2}, {"type": "text", "text": "3.1 PedCorpus: Multi-task Medical Instruction Dataset ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To endow the model with versatile diagnostic proficiency, PedCorpus is constructed through the multi-dimensional corpus across three application-oriented medical tasks, including Knowledge Question-Answer (MedKQ&A), Evidence-based Diagnosis (EviDiag), and Treatment Recommendation (TreRecom). Table 1 shows the detailed statistical information from different data sources. We explain the three patterns of PedCorpus construction below. ", "page_idx": 2}, {"type": "text", "text": "Specialized Pediatric Data. Extracting pediatric data from textbooks, guidelines, and knowledge graphs ensures knowledge professionalism. Specifically, we automatically extract standard medical definitions and descriptions from physical textbooks covering 131 disease types in 11 broad categories. Over 500 corresponding disease guidelines are collected, including diagnostic protocols and treatment consensus. Additionally, extensive knowledge entities are sampled from ternary instances in the knowledge graphs. Based on these resources, we introduce a role-playing-driven instruction building rule via GPT-4 API that produces well-organized instructions to enable accurate and humanistic model responses. The detailed building procedure is shown in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "Real Doctor-patient Conversations. To avoid the model collapse dilemma [42], we incorporate authentic doctor-patient dialogues from online treatment platforms and voice transcriptions during medical consultations. The single-/multi-turn instructions are jointly considered to equip the model with healthcare interrogation and contextual understanding. Original responses from real doctors are usually terse and noisy, potentially worsening the generation quality [58]. To this end, we craft 100 high-quality examples to guide the advanced language model by the in-context learning to regularize vanilla conversations in the self-instruct pattern [17, 46]. This approach ensures doctor-like and patient-friendly model responses. More regularization details are shown in Appendix A.2. ", "page_idx": 2}, {"type": "text", "text": "Distilled Medical Datasets. Integrating general medical knowledge from existing datasets [29, 26, 60] is a common practice in previous efforts [15, 50, 51, 8]. However, we find numerous unclear and incomplete representations in the instruction instances from public benchmarks due to the absence of careful calibration, potentially triggering hallucinated outputs. Consequently, we manually sample ", "page_idx": 2}, {"type": "image", "img_path": "WvoKwq12x5/tmp/ab142fbe0b014d429d5b1e2640a18ad61486b22afd3eb756e7bd42a7eed12a25.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: The sequential pipeline for developing PediatricsGPT. We begin by injecting intensive medical and world knowledge into the foundation model through the hybrid instruction mechanism in CPT phase. Then, full-parameter SFT is implemented to improve the model\u2019s instruction-following capabilities regarding medical generalists. After that, we introduce the direct following preference optimization to control the model behaviour to align with human preference. In the parameter-efficient SFT phase, the LoRA-based mixture of universal-specific experts is devised to mitigate conflicts across downstream tasks and competition between pediatric expertise and general mastery. ", "page_idx": 3}, {"type": "text", "text": "107,177 knowledge-intensive instructions from three mainstream benchmarks (i.e., Huatuo-26M [29], MedDialog [26], and CMeKG [10]), adhering to the philosophy of quality over quantity [62]. After that, a progressive instruction reconstruction rule is proposed to distill the sampled instructions to ensure informative and logical model responses. The rule process can be found in Appendix A.3. ", "page_idx": 3}, {"type": "text", "text": "3.2 Hybrid Instruction Pre-training in CPT ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Continuous Pre-Training (CPT) is essential in developing domain-specific models [14, 49, 56] since it can break the scaling law [24] to a certain extent. For this purpose, we introduce the PedCorpus-CPT dataset to ensure a high-quality pre-training corpus. From Table 1, PedCorpus-CPT consists of threepart data components. (i) We integrate plain texts from vanilla pediatric textbooks, guidelines, and knowledge graphs. (ii) The filtered Chinese Wikipedia [3] is also considered to achieve the model\u2019s trade-off for medical-general knowledge memory capacity. (iii) In practice, we observe that CPT leads to catastrophic forgetting of the models at follow-up due to different data distribution and format discrepancies compared to the original pre-training and SFT. Thus, we introduce a hybrid instruction pre-training mechanism to bridge these discrepancies. The core philosophy is to assemble instruction data from PedCorpus with Input-Output forms into Completion forms, which are then assimilated into plain texts to provide multi-task and complementary information. This mechanism effectively mitigates inconsistencies between the internal-injected medical knowledge of the foundation model while reinforcing medical domain adaptation. Moreover, we take PedCorpus as the seed instructions to improve multiple-department corpus density and breadth via knowledge-enhanced prompts. The prompt template is shown in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "We pre-train the foundation model to follow the causal language modelling paradigm. Given any input token sequence $\\pmb{t}=(t_{0},t_{1},t_{2},...)\\in\\mathcal{D}_{c p t}$ from the above multi-channel corpus $\\mathcal{D}_{c p t}$ , the next token $t_{i}$ is autoregressively predicted by minimizing the negative log-likelihood: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{CPT}}(\\theta,\\mathcal{D}_{c p t})=\\mathbb{E}_{t\\sim\\mathcal{D}_{c p t}}\\left[-\\sum_{i}^{|t|}\\mathrm{log}\\,p(t_{i}\\mid t_{0},t_{1},...,t_{i-1};\\theta)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\theta$ is the model parameter and the input context consists of $t_{0},t_{1},...,t_{i-1}$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Full-parameter Supervised Fine-tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "During this phase, we activate the model\u2019s ability to follow medical instructions by the Full-parameter Supervised Fine-tuning (FSFT). The full-parameter pattern enables a fuller invocation of the intensive knowledge in CPT and promotes comprehension and logical reasoning about diverse structured instructions. The training data at this phase is composed of the following three aspects. (i) We utilize the multi-department medical data in the PedCorpus dataset to develop the medical generalist. (ii) ", "page_idx": 3}, {"type": "text", "text": "Chinese instruction data (i.e., Alpaca dataset [35] and ShareGPT [4]) from general domains are selectively integrated to avoid the potential overftiting risk. (iii) Providing safety measures is vital for LLM assistants yet overlooked by prior methods [62]. In contrast, we write 200 training instructions with some degree of maliciousness, hallucinations, and counterfactuals. Correspondingly, the refusal responses with detailed explanations for disobedience are carefully crafted. We also include 300 examples related to self-cognition content. These data significantly improve the robustness and security of the model against unfriendly commands. ", "page_idx": 4}, {"type": "text", "text": "Given any input instruction $\\textbf{\\em x}=\\ (x_{0},x_{1},x_{2},\\ldots)\\ \\in\\ \\mathcal{D}_{f s f t}$ and corresponding target response $\\pmb{\\mathscr{y}}\\,=\\,\\bigl(y_{0},y_{1},y_{2},\\ldots\\bigr)\\,\\in\\,\\mathscr{D}_{f s f t}$ from the above-integrated fine-tuning dataset $\\mathcal{D}_{f s f t}$ , the optimization objective can be formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{FSFT}}(\\theta,\\mathcal{D}_{f s f t})=\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim\\mathcal{D}_{f s f t}}\\left[-\\!\\sum_{i=1}^{|\\mathbf{y}|}\\!\\log p(y_{i}\\mid\\mathbf{x},y_{<i};\\theta)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.4 Direct Following Preference Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Aligning human intention preferences facilitates the model to generate harmless responses. To this end, we introduce PedCorpus-DFPO $\\mathcal{D}_{d f p o}$ , a preference dataset to guide the model in learning human preference behaviours. PedCorpus-DFPO contains the input instruction set $\\pmb{x}=(x_{0},x_{1},x_{2},...)\\in\\mathcal{D}_{d f p o}$ , which is selectively sampled from vanilla PedCorpus. On the one hand, we perform a humanistic stylistic rephrasing of the outputs to generate preferred responses ${\\pmb y}^{w}=(y_{0}^{w},y_{1}^{w},y_{2}^{w},\\ldots)\\in\\mathcal{D}_{d f p o}$ . On the other hand, the corresponding low responses $\\pmb{y}^{l}=(y_{0}^{l},y_{1}^{l},y_{2}^{l},\\dots)\\in\\mathcal{D}_{d f p o}$ are generated from the feedback of a low-capability medical assistant [45] to maintain domain consistency. ", "page_idx": 4}, {"type": "text", "text": "Despite impressive improvements achieved by RLHF-based approaches [56, 58], challenges remain due to unstable reward modelling and significant computational costs [41, 61]. Inspired by singlestage preference learning [38], we propose a stable and lightweight method for domain-specific LLMs called Direct Following Preference Optimization (DFPO). DFPO utilizes variable changes to formulate the preference loss as a policy function that efficiently optimizes the policy with a simple binary cross-entropy objective. Meanwhile, our method directly regularizes model behaviour boundaries in an instruction-following paradigm on medical demonstrations of preferred responses, facilitating robustness and smoothing of the preference learning. ", "page_idx": 4}, {"type": "text", "text": "Theoretically, the observed probability of a particular preference pair usually follows the BradleyTerry model [9], and $\\pmb{y}^{w}$ is preferred over $\\pmb{y}^{l}$ (denoted $\\mathring{\\pmb{y}}^{w}\\succ\\pmb{y}^{l}$ ): ", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\pmb{y}^{w}\\succ\\pmb{y}^{l})=\\sigma(\\gamma(\\pmb{x},\\pmb{y}^{w})-\\gamma(\\pmb{x},\\pmb{y}^{l})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma({\\pmb x},{\\pmb y}^{w/l})$ means the parameterized reward function and $\\sigma(\\cdot)$ is the sigmoid activation. In this case, the overall optimization objective is expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dot{\\cdot}_{\\mathrm{\u1e0aFPo}}(\\theta,\\mathcal{D}_{d f p o})=-\\mathbb{E}_{(\\mathbf{x},\\mathbf{y}^{w},\\mathbf{y}^{t})\\sim\\mathcal{D}_{d f p o}}\\left[\\log\\sigma(\\beta\\log\\frac{\\pi_{\\theta}(y^{w}\\mid\\mathbf{x})}{\\pi_{r}(y^{w}\\mid\\mathbf{x})}-\\beta\\log\\frac{\\pi_{\\theta}(y^{l}\\mid\\mathbf{x})}{\\pi_{r}(y^{l}\\mid\\mathbf{x})})\\right]+\\mu\\Phi(\\mathbf{x},\\mathbf{y}^{w}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi_{\\theta}$ and $\\pi_{r}$ are the desired optimal policy and the reference policy, respectively. $\\beta$ is the control parameter reflecting the deviation from the basic $\\pi_{r}$ . For the fine-tuning regularization term $\\Phi(x,y^{\\bar{w}})$ with the scaling coefficient $\\mu$ , the implementation process is equivalent to maximizing the log probability $p(\\pmb{y}^{w}\\mid\\pmb{x})$ regarding the preferred responses $\\pmb{y}^{w}$ given the input instructions $\\textbf{\\em x}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi(\\boldsymbol{x},\\boldsymbol{y^{w}})=\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y^{w}})\\sim\\mathcal{D}_{d f p o}}\\left[-\\sum_{i=1}^{|\\boldsymbol{y^{w}}|}\\log p(\\boldsymbol{y}_{i}^{w}\\mid\\boldsymbol{x},\\boldsymbol{y_{<i}^{w}};\\boldsymbol{\\theta})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.5 Mixture of Universal-specific Experts in Parameter-efficient SFT ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This phase aims to reinforce the model performance for various pediatric applications through the LoRA-based Parameter-efficient SFT (PSFT). The used dataset $\\mathcal{D}_{p s f t}$ is derived from the pediatric department in PedCorpus and partial general medical/world data. In practice, we observe that competition across different pediatric tasks and the conflicts between medical generalization and specialized knowledge deteriorate instruction-following abilities. Accordingly, we propose a mixture of universal-specific experts strategy to address these challenges. Formally, LoRA adapters [27] act as experts to replace the linear layers in the Feed-Forward Neural (FFN) networks of LLMs, providing trainable parameters. Several specific experts Ejs jT=1 are assigned adaptive activations to master distinct pediatric expertise through soft routing. The routing gating is defined as follows: ", "page_idx": 4}, {"type": "table", "img_path": "WvoKwq12x5/tmp/2c2dab3b39b59eb0cd3ad92181e78fd452aa957b484a617bd201a03f275d583e.jpg", "table_caption": ["Table 2: Comparison results of different models on three pediatric medical benchmarks. In each benchmark, the best results are marked in bold, and the second-best results are marked underlined. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nG(\\pmb{x})=\\mathrm{Softmax}(\\pmb{x}W_{g}+\\mathcal{S}(\\varphi(\\pmb{x}W_{n})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$W_{g}$ and $W_{n}$ are the learnable weights. $S(\\varphi(x W_{n})$ is the noise term for regularizing the expert utilization balance, where $\\mathcal S(\\cdot)$ and $\\varphi(\\cdot)$ represent the Standard Normal distribution sampling and Softplus function, respectively. Moreover, we consistently activate a universal expert $E^{u}$ across all training data to prevent general knowledge forgetting and mitigate competency conflict. The parameterized output $_{z}$ of all the experts in the forward process can be mathematized as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz=\\frac{\\alpha}{r}(\\sum_{j=1}^{T}\\!G(\\pmb{x})_{j}E_{j}^{s}(\\pmb{x})+E^{u}(\\pmb{x})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $r$ is the rank value and $\\alpha$ is a hyper-parameter for approximating the learning rate. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Datasets and Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Extensive experiments are conducted on three application-oriented benchmarks to assess the model\u2019s pediatric medical abilities, including Knowledge Question-Answer (MedKQ&A), Evidence-based Diagnosis (EviDiag), and Treatment Recommendation (TreRecom). Each benchmark contains 300 held-out samples to reject data leakage during training. In addition, we select two publicly available Chinese medical benchmarks to validate the model\u2019s generalizability in general healthcare. Specifically, we sample 50 challenging instances of diagnostic queries from each department from the webMedQA [25] and CMD [1] benchmarks, respectively, leading to testing sets with 300 samples. ", "page_idx": 5}, {"type": "text", "text": "Our PediatricsGPT is developed upon the Baichuan2-Base [52] models in two versions with 7 and 13 billion parameters. The model training is accomplished through the PyTorch platform with Accelerate and DeepSpeed packages using eight Nvidia A800 GPUs. The ZeRO strategy [39] is employed to alleviate the memory overhead during full parameter training. The AdamW optimizer [31] is adopted for network optimization, and the bf16 data accuracy is chosen. More detailed hyper-parameter configurations for different stages are shown in Appendix C. ", "page_idx": 5}, {"type": "image", "img_path": "WvoKwq12x5/tmp/bc396e936a1fade27bfe552b7e3868b6b83ff85d0a9d2be80f188f17d0a453a5.jpg", "img_caption": ["Figure 2: Response comparisons of PediatricsGPT-13B with other baselines via GPT-4 evaluation. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "WvoKwq12x5/tmp/a0ac5ac9e25a2b511dd04afe49928e3be3592f2d84d8dc3a4c0eb44cbb05a43e.jpg", "img_caption": ["Figure 3: Response comparisons of PediatricsGPT-13B with other baselines via Doctor evaluation. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Model Zoo ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare a series of LLMs for comprehensive evaluations. Concretely, Baichuan2-7B/13B (Chat) models [52] are trained on 2.6 trillion tokens as the baselines, which have excellent abilities in different domains. Meditron-7B [16] is a 7 billion parameters model adapted to the medical domain from Llama2-7B through continued pre-training on a comprehensively curated medical corpus. Llama3.1- 8B [21] is a robust multilingual large language model through systematic training. For reproducible Chinese medical works, DISC-MedLLM (13B) [8] is fine-tuned through reconstructed medical dialogues and behavioural preference instructions. HuatuoGPT (13B) [58] performs SFT based on mixed instruction data and introduce human feedback in RLHF. HuatuoGPT-II (13B) [14] enhances the medical-specific domain adaptation of LLMs through one-stage unified training. Zhongjing (13B) [56] implements a complete pipeline based on Ziya-LLaMA-13B to enhance the model\u2019s multiturn medical conversation abilities. ChatGPT [33] and GPT-4 [5] have impressive performance in general medical fields as closed-source models developed by OpenAI. ", "page_idx": 6}, {"type": "text", "text": "4.3 Comparison with State-of-the-art Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Metrics-based Evaluation. In Table 2, we present the comparison results of different models on three pediatric benchmarks through multifaceted metrics, including ROUGE-1/2/L, BLEU-1/2/3/4, GLEU, and Distinct-1/2. The key observations are listed below. (i) PediatricsGPT-13B significantly outperforms the baselines and SOTA medical models on the vast majority of metrics across all benchmarks, demonstrating excellent pediatric expertise. (ii) Our 7B version also achieves competitive results compared to the 13B models. For instance, PediatricsGPT-7B yields absolute improvements of $3.53\\%$ and $4.44\\%$ on metrics ROUGE-L and GLEU in the EviDiag task compared to HuatuoGPT-II, respectively, generating more accurate and informative content. (iii) By contrast to Zhongjing and HuatuoGPT-II with massive training corpora, our method confirms that the training data quality outweighs quantity for performance gains. (iv) The worst results at baselines emphasize that targetoriented fine-tuning is an effective strategy for improving domain-specific abilities. ", "page_idx": 6}, {"type": "text", "text": "Automated GPT-4 Evaluation. Measuring model performance from multiple aspects is essential in the pediatric medical domain. To this end, we consider four dimensions to holistically assess response quality, including usefulness, correctness, consistency, and smoothness. Advanced GPT-4 [5] is prompted to select the winning response between pairwise models based on these dimensions. The dimension explanations and the prompt template for GPT-4 can be found in Appendix D. (i) As Figure 2 shows, PediatricsGPT-13B wins all LLMs by large margins in the MedKQ&A task, implying the necessity of implementing the knowledge-intensive CPT. (ii) The favourable win rates on the TreRecom and EviDiag tasks compared to medical LLMs show the superiority of our model in both single-turn treatment recommendations and multi-turn medical diagnostics. For example, our model beats Zhongjing via the $59\\%$ win rate on the EviDiag, which specializes in multi-round consultations. ", "page_idx": 6}, {"type": "image", "img_path": "WvoKwq12x5/tmp/ec4504746d2db26c932b54a8c650ca98c3f87ff4fa2b09ee44545d6d70a3902c.jpg", "img_caption": ["Figure 4: Comparison results of different models on the CMD benchmark. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "WvoKwq12x5/tmp/0287ccf3bab4d776a88465d96e773b47cb9dc8fd0f91ec0a7972043ea0b947d9.jpg", "img_caption": ["Figure 5: Comparison results of different models on the webMedQA benchmark. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Manual Doctor Evaluation. Doctor approval of LLM assistants is a vital step toward realistic applications. We invite three doctors (each paid $\\mathbb{S}300]$ ) to determine the winner of pairwise models by the majority voting rule. The evaluation requires simultaneous consideration of the responses\u2019 professionalism, factuality, and safety. (i) Excluding ChatGPT, the dominance of our model in Figure 3 shows the effectiveness of considering safety measure data while incorporating specialized pediatric knowledge. (ii) The proposed direct following preference optimization makes PediatricsGPT13B more favoured by human preferences compared to other behavioural alignment efforts [8, 56, 58]. (iii) The competitive performance of ChatGPT when human judgments indicate that the scaling law still holds, stemming from the high agreement between its behaviours and human intentions. ", "page_idx": 7}, {"type": "text", "text": "Generalization Ability Evaluation. We show the GLEU metric-based scores of different models on the Chinese medical benchmarks in Figure 4 for CMD and Figure 5 for webMedQA. (i) PediatricsGPT-13B achieves impressive results across diverse medical departments (including pediatrics), exhibiting medical generalist and pediatric competency mastery. (ii) The 7B counterpart similarly outperforms most 13B Chinese medical LLMs and exceeds ChatGPT in some departments. For instance, PediatricsGPT-7B brings relative gains of $18.8\\%$ and $7.1\\%$ compared to ChatGPT in the Gynecology and Oncology tasks on the CMD benchmark. These findings confirm the robust generalization of our model and its ability to capture the multifaceted medical dialogue distributions. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We perform thorough ablation studies on five medical benchmarks to investigate the effects of different modelling components. Following [58], we compare the responses from each of the proposed model variants with ChatGPT, and then calculate the win rate $(\\%)$ of our model in pairwise responses by GPT-4 and doctor evaluations. Table 3 shows the following observations. ", "page_idx": 7}, {"type": "text", "text": "Importance of Continuous Pre-training. Firstly, we remove the complete continuous pre-training phase to observe performance variations. (i) The significantly deteriorated win rates reveal that injecting specialized knowledge into medical LLMs through rich corpora is indispensable. (ii) Meanwhile, our hybrid instruction pre-training mechanism provides valuable gains to the model. ", "page_idx": 7}, {"type": "table", "img_path": "WvoKwq12x5/tmp/7fc6c17dd0fdfb4bd3dcb0f80646945b43283b2e30c44b6ed95e706fb2bff012.jpg", "table_caption": ["Table 3: Ablation study results on five medical benchmarks. \u201cw/\u201d and \u201cw/o\u201d are short for with and without, respectively. \u201cMUE\u201d means the Mixture of Universal-specific Experts strategy. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Necessity of Supervised Fine-tuning. (i) We observe consistent performance gaps when removing the Fullparameter SFT (FSFT) and Parameterefficient SFT (PSFT) phases, respectively. This makes sense since SFTs are necessary to activate the model\u2019s healthcare instruction-following capabilities. (ii) Moreover, PSFT is more critical for three pediatric applications because it facilitates pediatric-related knowledge accumulation, while FSFT focuses on consolidating general medical semantic representations. (iii) Then, we replace the proposed Mix", "page_idx": 8}, {"type": "image", "img_path": "WvoKwq12x5/tmp/4a20c0c873c630098d99180c1d1faf1151cb9dd61ea528efcb04eb38d450aab0.jpg", "img_caption": ["Figure 6: (a) and (b) show the effect of specific expert numbers on model performance and specific expert utilization in different task data, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "ture of Universal-specific Experts (MUE) version with the vanilla single LoRA. The reduced performance on pediatric EviDiag and TreRecom benchmarks verifies that it is essential to introduce multiple LoRAs that act as specific experts on different tasks. A reasonable explanation is that the single-LoRA model suffers from the task competition between learning the knowledge questionanswer and mastering the diagnostic recommendation abilities. (iv) Furthermore, we find that the universal LoRA expert significantly improves the results on the general medical benchmarks (i.e., CMD and webMedQA), proving that it mitigates the competency conflict between general medical and pediatric knowledge. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Preference Alignment. (i) When the Direct Following Preference Optimization (DFPO) phase is removed, the model exhibits significant performance drops in doctor evaluations compared to the full version. This observation proves that DFPO effectively helps the model to align human preferences, reducing harmful content while generating doctor-like output. (ii) As two candidates, the vanilla DPO and RLHF methods are inferior to the proposed DFPO, suggesting that our strategy can more safely control model behaviour, leading to more favoured humanistic responses. ", "page_idx": 8}, {"type": "text", "text": "4.5 Qualitative Analysis of LoRA Experts ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effect of Specific Expert Numbers. As a complement to the ablation of LoRA experts, Figure 6(a) explores the gain effects of varying the number of specific experts while maintaining the universal expert. (i) Noticeably, our MCE strategy with three specific experts achieves a reasonable performance trade-off across the three tasks with only $0.95\\%$ trainable parameters. (ii) Conversely, excessively introducing LoRA experts does not result in appreciable gains but increases the training overhead. ", "page_idx": 8}, {"type": "text", "text": "Analysis of Expert Utilization. To confirm the duties of specific LoRA experts in the routing process, we visualize the normalized weights assigned by the routing gating when encountering data from different downstream tasks. CMD and webMedQA data are merged to compose general healthcare data. From Figure 6(b), (i) Experts 2 and 3 are emphatically activated on the TreRecom and MedKQ&A tasks, respectively, implying their focal ability to tackle medical knowledge interpretations and treatment recommendations. (ii) In contrast, Expert 1 is more proficient at learning multi-turn diagnosis semantics in the EviDiag task, which is different from the other tasks of instruction content. (iii) Additionally, there is no clear difference in the specific expert utilization on general healthcare, implying that the general task is handled by the consistently universal expert. The above observations demonstrate the effectiveness and necessity of the proposed MCE strategy. ", "page_idx": 9}, {"type": "text", "text": "4.6 Visualization Analysis of Model Responses ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To intuitively compare the output quality of medical LLMs, we show the responses of different models for each of the three types of medical inquiries in Figures 13&14&15 from Appendix E. From the results, Zhongjing offers insufficient information due to limited output content. Although HuatuoGPT-II gives well-organized responses, it lacks accuracy and informativeness. In comparison, our model can provide more specialized and detailed medical knowledge and diagnostic guidance in extended response contexts, confirming its application potential in diverse healthcare services. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents PediatricsGPT, a Chinese medical LLM assistant with medical generalist and pediatric expertise capabilities. Based on the well-designed PedCorpus dataset, PediatricsGPT undergoes a systematic and robust procedure ranging from continuous pre-training and supervised fine-tuning to human preference optimization, leading to competence in different pediatric and general healthcare service scenarios. Extensive experimental results under multi-dimensional evaluation patterns demonstrate that our model outperforms currently available Chinese medical LLMs, providing a potential solution for promoting reliable and intelligent interactive diagnosis and treatment. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. (i) Our model has made meaningful contributions to pediatric medicine by integrating extensive medical data and emerging research. This integration facilitates more accurate and expedited diagnosis of complex pediatric conditions and aids in predicting treatment outcomes, enabling highly personalized and effective treatment strategies for young patients. (ii) The proposed PediatricsGPT provides crucial decision support for medical professionals, giving evidence-based recommendations and specialized medical insights. Additionally, it democratizes access to expert medical suggestions and accurate medical knowledge, empowering parents and caregivers with accurate health information, which is especially crucial in underserved areas. (iii) The training pipeline of PediatricsGPT showcases exemplary generalizability, designed to be applicable across various medical and non-medical domains. This adaptability broadens the model\u2019s applicability and pioneers the development of future AI solutions in healthcare and other fields. ", "page_idx": 9}, {"type": "text", "text": "Limitations. (i) When deployed online, the proposed PediatricsGPT model, like other Large Language Models (LLMs), faces significant security risks, particularly from attacks aimed at manipulating its outputs. These attacks can be strategically designed to exploit the model\u2019s response mechanisms, allowing attackers to induce the model to generate unsafe, biased, or otherwise inappropriate content. (ii) Currently, our PediatricsGPT model does not support all languages. This linguistic barrier can prevent the model from reaching a global audience, particularly in diverse linguistic landscapes where localized medical information is crucial. ", "page_idx": 9}, {"type": "text", "text": "Ethical Issues. We fully recognize the critical importance of privacy and data protection. All data used has been meticulously de-identified, with all sensitive information removed, and this process has been verified by the partnering medical institutions. For the public databases, we strictly follow specific license agreements for use and adaptation. For the constructed corpus, we underwent an internal ethical review by the ethical review board of the partnering medical institutions with license and approval. We will release relevant resources to the extent that they are controlled and permitted. ", "page_idx": 9}, {"type": "text", "text": "We provide more discussions of the future work in Appendix F. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. This work is supported in part by the National Key R&D Program of China under Grant 2021ZD0113502, in part by the Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0103. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Chinese medical dialogue data, 2019. https://github.com/Toyhom/Chinese-medical -dialogue-data. 6   \n[2] Baichuan, 2023. https://github.com/baichuan-inc/Baichuan-13B. 2   \n[3] Chinese wikipedia, 2023. https://huggingface.co/datasets/pleisto/wikipedia-c n-20230720-filtered. 4   \n[4] Sharegpt-raw, 2023. https://huggingface.co/datasets/philschmid/sharegpt-raw. 5   \n[5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3, 7   \n[6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 2   \n[7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 2   \n[8] Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, and Zhongyu Wei. Disc-medllm: Bridging general large language models and real-world medical consultation. arXiv preprint arXiv:2308.14346, 2023. 2, 3, 7, 8   \n[9] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. 5   \n[10] Odma Byambasuren, Yunfei Yang, Zhifang Sui, Damai Dai, Baobao Chang, Sujian Li, and Hongying Zan. Preliminary study on the construction of chinese medical knowledge graph. Journal of Chinese Information Processing, 33(10):1\u20139, 2019. 4   \n[11] Jiawei Chen, Yue Jiang, Dingkang Yang, Mingcheng Li, Jinjie Wei, Ziyun Qian, and Lihua Zhang. Can llms\u2019 tuning methods work in medical multimodal domain? arXiv preprint arXiv:2403.06407, 2024. 2   \n[12] Jiawei Chen, Dingkang Yang, Yue Jiang, Yuxuan Lei, and Lihua Zhang. Miss: A generative pretraining and finetuning approach for med-vqa. arXiv preprint arXiv:2401.05163, 2024.   \n[13] Jiawei Chen, Dingkang Yang, Yue Jiang, Mingcheng Li, Jinjie Wei, Xiaolu Hou, and Lihua Zhang. Efficiency in focus: Layernorm as a catalyst for fine-tuning medical visual language pre-trained models. arXiv preprint arXiv:2404.16385, 2024. 2   \n[14] Junying Chen, Xidong Wang, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, et al. Huatuogpt-ii, one-stage training for medical adaption of llms. arXiv preprint arXiv:2311.09774, 2023. 3, 4, 7   \n[15] Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu, et al. Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt. arXiv preprint arXiv:2310.15896, 2023. 2, 3   \n[16] Zeming Chen, Alejandro Hern\u00e1ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K\u00f6pf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. 7   \n[17] Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al. Phoenix: Democratizing chatgpt across languages. arXiv preprint arXiv:2304.10453, 2023. 3 ", "page_idx": 10}, {"type": "text", "text": "[18] Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177, 2023. 2 ", "page_idx": 11}, {"type": "text", "text": "[19] Enhong Dong, Jie Xu, Xiaoting Sun, Ting Xu, Lufa Zhang, and Tao Wang. Differences in regional distribution and inequality in health-resource allocation on institutions, beds, and workforce: a longitudinal study in china. Archives of Public Health, 79(1):78, 2021. 1 ", "page_idx": 11}, {"type": "text", "text": "[20] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. 2, 3 ", "page_idx": 11}, {"type": "text", "text": "[21] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 7 ", "page_idx": 11}, {"type": "text", "text": "[22] Abdelaziz Y Elzouki, Harb A Harf,i H Nazer, William Oh, FB Stapleton, and Richard J Whitley. Textbook of clinical pediatrics. Springer Science & Business Media, 2011. 1 ", "page_idx": 11}, {"type": "text", "text": "[23] Fielding Hudson Garrison. History of pediatrics. Saunders, 1923. 1 [24] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. 4 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[25] Junqing He, Mingming Fu, and Manshu Tu. Applying deep matching networks to chinese medical question answering: a study and a dataset. BMC medical informatics and decision making, 19:91\u2013100, 2019. 6 ", "page_idx": 11}, {"type": "text", "text": "[26] Xuehai He, Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang, Yue Yang, Jiaqi Zeng, Ruisi Zhang, Ruoyu Zhang, et al. Meddialog: Two large-scale medical dialogue datasets. arXiv preprint arXiv:2004.03329, 2020. 3, 4 ", "page_idx": 11}, {"type": "text", "text": "[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 5 ", "page_idx": 11}, {"type": "text", "text": "[28] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 2 ", "page_idx": 11}, {"type": "text", "text": "[29] Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and Benyou Wang. Huatuo-26m, a large-scale chinese medical qa dataset. arXiv preprint arXiv:2305.01526, 2023. 3, 4 ", "page_idx": 11}, {"type": "text", "text": "[30] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Cureus, 15(6), 2023. 3 ", "page_idx": 11}, {"type": "text", "text": "[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7 ", "page_idx": 11}, {"type": "text", "text": "[32] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. arXiv preprint arXiv:2104.08773, pages 839\u2013849, 2021. 2 ", "page_idx": 11}, {"type": "text", "text": "[33] OpenAI. Introducing chatgpt, 2022. https://openai.com/blog/chatgpt. 2, 3, 7 [34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 35:27730\u201327744, 2022. 2 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[35] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. 5 ", "page_idx": 11}, {"type": "text", "text": "[36] Lida Pu. Fairness of the distribution of public medical and health resources. Frontiers in public health, 9:768728, 2021. 1   \n[37] Xuezheng Qin and Chee-Ruey Hsieh. Economic growth and the geographic maldistribution of health care resources: Evidence from china, 1949-2010. China Economic Review, 31:228\u2013246, 2014. 1   \n[38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024. 5   \n[39] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. pages 1\u201316, 2020. 7   \n[40] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022. 2   \n[41] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 5   \n[42] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023. 3   \n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2, 3   \n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2   \n[45] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023. 2, 3, 5   \n[46] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. 3   \n[47] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 2   \n[48] Jinjie Wei, Dingkang Yang, Yanshu Li, Qingyao Xu, Zhaoyu Chen, Mingcheng Li, Yue Jiang, Xiaolu Hou, and Lihua Zhang. Medaide: Towards an omni medical aide via specialized llm-based multi-agent collaboration. arXiv preprint arXiv:2410.12532, 2024. 2   \n[49] Cheng Wen, Xianghui Sun, Shuaijiang Zhao, Xiaoquan Fang, Liangyu Chen, and Wei Zou. Chathome: Development and evaluation of a domain-specific language model for home renovation. arXiv preprint arXiv:2307.15290, 2023. 4   \n[50] Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, and Dinggang Shen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint arXiv:2304.01097, 2023. 2, 3   \n[51] Ming Xu. Medicalgpt: Training medical gpt model. https://github.com/shibing624/ MedicalGPT, 2023. 3   \n[52] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. 2, 6, 7 [53] Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du, and Lihua Zhang. Disentangled representation learning for multimodal emotion recognition. In Proceedings of the 30th ACM International Conference on Multimedia (ACM MM), pages 1642\u20131651, 2022. 2 [54] Dingkang Yang, Haopeng Kuang, Shuai Huang, and Lihua Zhang. Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences. In Proceedings of the 30th ACM International Conference on Multimedia (ACM MM), pages 1708\u20131717, 2022. [55] Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, and Lihua Zhang. Robust emotion recognition in context debiasing. arXiv preprint arXiv:2403.05963, 2024. 2 [56] Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, and Hongying Zan. Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 38, pages 19368\u201319376, 2024. 2, 3, 4, 5,   \n7, 8 [57] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. 2 [58] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be a doctor. arXiv preprint arXiv:2305.15075, 2023. 2, 3, 5, 7, 8 [59] Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, et al. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. arXiv preprint arXiv:2209.02970, 2022. 2 [60] Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. Multi-scale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:74061\u2013   \n74071, 2018. 3 [61] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. 2, 5 [62] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. 4, 5 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Implementation Details of PedCorpus Construction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Role-playing-driven Instruction Building Rule ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "After integrating pediatric textbooks, guidelines, and knowledge graphs into a consolidated textual database, the content is segmented according to individual diseases. Subsequently, two instances of the GPT-4 model are deployed, designated as the \u201cinquirer\u201d and the \u201cexpert pediatrician\u201d respectively. Disease-specific segments are then fed into the \u201cinquirer\u201d GPT-4, tasked with formulating a series of relevant and scholarly pediatric inquiries. Following this, the original disease segments and the formulated inquiries are fed into the \u201cexpert pediatrician\u201d GPT-4 to generate precise responses for each inquiry, leveraging the segmented text as the contextual reference. We show the prompt templates for the \u201cinquirer\u201d and \u201cexpert pediatrician\u201d in Figures 7 and 8, respectively. ", "page_idx": 14}, {"type": "text", "text": "A.2 Vanilla Doctor-patient Conversation Regularization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We guide GPT-4 to regularize concise and noisy doctor responses in authentic doctor-patient consultations by the context learning strategy. Specifically, we manually craft 100 instruction examples with high-quality content to allow GPT-4 to learn doctor-like and patient-friendly behavioral styles. In each round of regularization, we randomly sample 10 out of 100 examples to perform 10-shot context prompts. Immediately after that, vanilla dialogues are fed to GPT-4 as seed instructions to optimize instructions according to user requirements. Constrained by the space, we show the prompt case with one example in Figure 9. ", "page_idx": 14}, {"type": "text", "text": "A.3 Progressive Instruction Reconstruction Rule ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Medical knowledge integration from existing datasets is common but frequently imprecise, resulting in unclear or incomplete instructions and potentially inaccurate outputs. Consequently, 107,177 instructions are selected from three significant benchmarks, prioritizing quality over quantity. In this case, we design a progressive instruction reconstruction rule to refine these instructions, ensuring informative and logical model responses. ", "page_idx": 14}, {"type": "text", "text": "As shown in Figure 10, we first prompt GPT-4 to take the perspective of the experienced doctor to complete Tasks 1 and 2 in the given instruction and answer scenarios. Task 1 focuses on bridging the gaps in the vanilla instructions and reinforcing the completeness, professionalism, and medical relevance. Based on the refined instructions, Task 2 requires the GPT-4 to make further targeted improvements to the answer parts. In practice, this progressive reconstruction rule can activate better instruction following capabilities in advanced language models. ", "page_idx": 14}, {"type": "text", "text": "B Knowledge-enhanced Prompt ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To enrich the density and breadth of the multiple-department CPT corpus, we transform the structured instruction data from the vanilla PedCorpus dataset into comprehensive medical knowledge texts using knowledge-enhanced prompts. The medical knowledge texts are integrated as complementary content to construct the PedCorpus-CPT dataset. The prompt template is shown in Figure 11. ", "page_idx": 14}, {"type": "text", "text": "C Training Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we list in detail the hyper-parameter configurations for the different training phases. ", "page_idx": 14}, {"type": "text", "text": "Continuous Pre-training. During this procedure, we train each model for just a single epoch, setting the learning rate at 1e-6 and the batch size at 128. We adopt a maximum cutoff length of 4096, enabling the model to process extensive text sequences in one batch. This significantly enhances the model\u2019s contextual understanding and coherence. ", "page_idx": 14}, {"type": "text", "text": "Full-parameter Supervised Fine-tuning. In this configuration, we train all models for three epochs with a learning rate adjusted to 5e-5 and a batch size of 64, capping the maximum sequence length at 2048. We introduce a warmup_steps setting at 200 to gradually ramp up the learning rate from an initial lower value, aiding the optimizer in adapting to gradient changes. This approach boosts stability and performance and guides the model towards a better convergence path. Also, we specify eval_steps at 100 and save the best-performing weights on the validation set to ensure optimal results. ", "page_idx": 14}, {"type": "text", "text": "Human Preference Alignment. In this setup, we train five epochs with the learning rate set to 1e-6 and the batch size maintained at 64. To enhance the robustness and smoothness of preference learning, we adjust the control parameter $\\beta$ to 0.1 and the scaling coefficient $\\mu$ to 1.0. We specify eval_steps at 100, selecting the best-performing weights on the validation set. ", "page_idx": 15}, {"type": "text", "text": "LoRA-based Parameter-efficient SFT. Here, we train three epochs with a learning rate of 1e-6 and adjust the batch size to 32. We configure the LoRA parameters by setting the rank $r$ to 8, $\\alpha$ to 16, and the Dropout rate to 0.05, targeting all modules. The default number of LoRA adapters is set to 4, including one constant universal expert and three specific experts. Ultimately, we select the adapters that perform best on the validation set. ", "page_idx": 15}, {"type": "text", "text": "D GPT-4 Evaluation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We consider four complementary dimensions in the automated evaluation to guide GPT-4 in judging the quality of model responses from a comprehensive perspective. The full definitions of these dimensions are shown as follows. ", "page_idx": 15}, {"type": "text", "text": "Usefulness: measures the extent to which the model response has pediatric expertise and relevance to the instruction intention. ", "page_idx": 15}, {"type": "text", "text": "Correctness: measures the extent to which harmful, misleading, and inaccurate information is present in the model response. ", "page_idx": 15}, {"type": "text", "text": "Consistency: measures the degree to which the model response is logically self-contradictory and the information is coherent in context. ", "page_idx": 15}, {"type": "text", "text": "Smoothness: measures whether the response content is fluent, natural, and conforms to the language expression style of human habits. ", "page_idx": 15}, {"type": "text", "text": "In this case, we present GPT-4 with paired responses from different models, assessing various criteria such as pediatric expertise in the responses, presence of harmful, misleading, or inaccurate information, logical consistency, and the fluency and naturalness of the language, which should conform to human linguistic habits. GPT-4 assesses these responses on their merits and selects the superior one. To maintain fairness and mitigate potential position bias, the order of the responses is randomised. This methodology is supported by recent studies demonstrating GPT-4\u2019s strong agreement with human judgment in evaluating responses. Figure 12 demonstrates the prompt template used to evaluate the quality of paired model responses. ", "page_idx": 15}, {"type": "text", "text": "E Comparison Results of Model Responses ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we visualize the responses of the proposed PediatricsGPT-13B and two SOTA Chinese medical LLMs across three tasks from the same medical inquiries to provide intuitive qualitative comparisons. Specifically, Figures 13 and 15 illustrate the medical knowledge question-answer and treatment recommendation tasks, respectively, which follow a single-turn dialogue pattern. The multi-turn conversation pattern is considered in the evidence-based diagnosis task from Figure 14. ", "page_idx": 15}, {"type": "text", "text": "F Future Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We list future work below to provide potential optimization directions. ", "page_idx": 15}, {"type": "text", "text": "Enhancing Security Against Model Manipulation. To mitigate the security risks associated with online deployment, our future strategy involves implementing multi-layered security measures for the proposed PediatricsGPT model. This will include advanced input validation techniques to detect and neutralize potentially malicious inputs that could manipulate model outputs. Continuous updates and patches will also be prioritized to address emerging security threats and vulnerabilities. ", "page_idx": 15}, {"type": "text", "text": "Expanding Language Support. To overcome the challenge of incomplete language coverage, we are committed to broadening the linguistic capabilities of PediatricsGPT. This expansion will involve training the model on a more diverse dataset that includes a broader range of languages and dialects, particularly those prevalent in underserved regions. By doing so, we aim to make the model more accessible and useful to a global audience, ensuring that non-Chinese speakers also benefit from reliable and localized medical information. ", "page_idx": 15}, {"type": "text", "text": "[INST]<SYS>Please act as an inquirer with a broad reserve of pediatric knowledge and complete the following requirements\uff1a 1. Based on the rich corpus of pediatric knowledge, carefully formulate a series of valuable, logical and inspiring questions\uff1b 2. Ensure that the uniqueness of each question is designed to comprehensively cover the needs of medical applications such as pediatric knowledge and answers, the consultation process, and advice on diagnosis and treatment of diseases, and to avoid simplicity or repetitiveness of questions\uff1b ", "page_idx": 16}, {"type": "text", "text": "3. All questions should be strictly limited to the scope of language processing, does not involve pictures, audio and other nonverbal form of the question, and shall not contain any sensitive or private information that may be involved in the real world; 4. Output the generated questions in the following format: {\"Q1\": \"\", \"Q2\": \"\", ..., \"Qn\": \"\"}\uff1b   \nPlease generate the questions directly.</SYS>[/INST] ", "page_idx": 16}, {"type": "text", "text": "[INST]<USER> [Auxiliary Examinations for Pediatric Craniopharyngioma] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Laboratory Tests ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "(1)  Measurement of anterior pituitary hormone levels: Cortisol (F), Adrenocorticotropic Hormone (ACTH), thyroid function [Free Triiodothyronine (FT3), Free Thyroxine (FT4), Thyroid Stimulating Hormone (TSH), etc.], Growth Hormone (GH) levels, Insulin-like Growth Factor 1 (IGF-1) levels, six sex hormones [Follicle Stimulating Hormone (FSH), Luteinizing Hormone (LH), Testosterone (T), Estradiol (E2), Progesterone (P) and Prolactin (PRL)], and 24-hour urinary free cortisol; morning cortisol levels between $3\\mathrm{mg/L}$ to $18\\mathrm{mg/L}$ require an ACTH stimulation test. ", "page_idx": 16}, {"type": "text", "text": "(2)  Patients with significant polydipsia and polyuria: Monitoring of blood ion levels (potassium, sodium, chloride), plasma osmolarity, 24-hour urine volume, 24-hour urinary free cortisol, urine specific gravity, urine osmolarity, and urinary electrolytes. For patients with difficult-to-diagnose central diabetes insipidus, a vasopressin test should be performed to determine the presence of central diabetes insipidus. ", "page_idx": 16}, {"type": "text", "text": "(3) Blood tests (and cerebrospinal fluid when necessary) for tumor markers such as Alpha-fetoprotein (AFP) and Human Chorionic Gonadotropin (HCG) for differential diagnosis. ", "page_idx": 16}, {"type": "text", "text": "(1)  Wrist joint X-ray, AP view: Measuring bone age to understand child growth and development further. (2)  Head CT: Craniopharyngiomas in the sellar region can grow in various directions with significant individual variatio ", "page_idx": 16}, {"type": "text", "text": "Radiologically, they appear round or lobulated. The tumors may be cystic, solid, or mixed. Cystic fluid typically shows low density on CT. Solid components appear as uneven, iso-, or slightly hyperdense. Typical craniopharyngiomas on CT may show \"eggshell\" calcifications. ", "page_idx": 16}, {"type": "text", "text": "(3) Head MRI: Due to varying components, MRI signals vary. Cystic lesions mostly appear as high signal on T2WI, some as low signal (due to keratin or calcium salts), and T1WI signals vary depending on components, appearing low (contains deoxyhemoglobin) or high (high protein content); post-enhancement, the solid parts may show varying degrees of enhancement. ", "page_idx": 16}, {"type": "text", "text": "(4) Other imaging studies such as CTA, MRA, Digital Subtraction Angiography (DSA), etc., to evaluate the relationship between the tumor and blood vessels. ", "page_idx": 16}, {"type": "text", "text": "[Differential Diagnosis of Pediatric Craniopharyngioma] 1. Biocellular Tumors ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Teratomas, mixed germ cell tumors etc., also show calcifications on CT. Mature teratomas have negative tumor markers like AFP, HCG, but often contain tissues such as fat that provide mixed densities for differential diagnosis; mixed germ cell tumors have varying marker levels depending on the proportion of components, which can be distinguished from craniopharyngiomas. 2. Optic Pathway Glioma ", "page_idx": 16}, {"type": "text", "text": "Clinically may manifest as visual impairment, but typically shows as low-density lesions on CT without clear calcifications; on MRI, T1WI is iso- or hyperintense, T2WI is hyperintense, and enhancement is usually uniform after contrast administration; can be distinguished from craniopharyngiomas. $<\\!/$ USER>[/INST] ", "page_idx": 16}, {"type": "text", "text": "[OUTPUT]<ASSISTANT> ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\"Q1\": \"What are the key laboratory tests required for diagnosing childhood craniopharyngioma, and how do specific hormone levels   \nimpact the evaluation?\",   \n\"Q2\": \"Which specific blood tests and measurements are necessary for patients with pronounced symptoms of polydipsia and polyuria in   \nthe context of childhood craniopharyngioma?\",   \n\"Q3\": \"How do imaging techniques like wrist X-ray, CT, and MRI contribute to the diagnosis and evaluation of childhood   \ncraniopharyngioma?\",   \n\"Q4\": \"What differentiates the imaging features of a craniopharyngioma from other similar intracranial tumors such as teratomas or   \nmixed germ cell tumors on CT and MRI scans?\",   \n\"Q5\": \"Can you discuss the significance of tumor markers such as AFP and HCG in the differential diagnosis of childhood   \ncraniopharyngioma?\",   \n\"Q6\": \"How can the presence or absence of calcification help distinguish between craniopharyngioma and other types of brain tumors   \nlike optic pathway gliomas on imaging studies?\"   \n}</ASSISTANT>[/OUTPUT] ", "page_idx": 16}, {"type": "text", "text": "", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 7: Illustration of the GPT-4 prompt template for the \u201cinquirer\u201d role. The original Chinese content is translated into the corresponding English version to facilitate intuitive understanding. ", "page_idx": 16}, {"type": "text", "text": "[INST]<SYS>Please act as an experienced pediatrician and fulfill the following requirements:   \n1. For each inquiry, provide comprehensive responses, taking into account the intricacies of pediatric healthcare and the needs of patients, ensuring accuracy and humanistic care in the answers.   \n2. Answers should be based on the provided pediatric corpus, focusing on pediatric healthcare to ensure professionalism and relevance. 3. Answers should be logically coherent and semantically clear.   \n4. Generated answers should be formatted as: {\"A1\":\"\",\"A2\":\"\",\"A3\":\"\",\"An\":\"\"}, where each \"An\" represents an independent question. Please generate the responses directly.</SYS>[/INST] ", "page_idx": 17}, {"type": "text", "text": "[INST]<USER> ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\"Q1\": \"What are the key laboratory tests required for diagnosing childhood craniopharyngioma, and how do specific hormone levels   \nimpact the evaluation?\",   \n$\"Q2\"$ : \"Which specific blood tests and measurements are necessary for patients with pronounced symptoms of polydipsia and polyuria in   \nthe context of childhood craniopharyngioma?\",   \n\"Q3\": \"How do imaging techniques like wrist X-ray, CT, and MRI contribute to the diagnosis and evaluation of childhood   \ncraniopharyngioma?\",   \n\"Q4\": \"What differentiates the imaging features of a craniopharyngioma from other similar intracranial tumors such as teratomas or   \nmixed germ cell tumors on CT and MRI scans?\",   \n\"Q5\": \"Can you discuss the significance of tumor markers such as AFP and HCG in the differential diagnosis of childhood   \ncraniopharyngioma?\",   \n\"Q6\": \"How can the presence or absence of calcification help distinguish between craniopharyngioma and other types of brain tumors   \nlike optic pathway gliomas on imaging studies?\" ", "page_idx": 17}, {"type": "text", "text": "}</USER>[/INST] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "[OUTPUT]<ASSISTANT>{ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\"A1\": \"Diagnosing childhood craniopharyngioma involves various key laboratory tests, notably hormonal evaluations, to assess endocrine function. Specific tests include measurements of serum cortisol, ACTH, thyroid-stimulating hormone (TSH), free T4, prolactin, and growth hormone levels. Low levels of these hormones can indicate hypothalamic or pituitary dysfunction, common in craniopharyngioma patients due to the tumor's proximity to these structures. Anterior pituitary hormone deficiencies are a hallmark, and monitoring them helps in assessing the extent of the tumor's impact on the endocrine system.\", ", "page_idx": 17}, {"type": "text", "text": "\"A2\": \"For patients with pronounced symptoms of polydipsia and polyuria, specific blood tests include serum sodium, osmolarity, and blood glucose levels to rule out diabetes mellitus. Additionally, measuring serum antidiuretic hormone (ADH) levels is crucial to differentiate between central and nephrogenic diabetes insipidus. Urine osmolality and a water deprivation test followed by desmopressin (ADH analog) administration can help confirm central diabetes insipidus, which is often associated with craniopharyngioma.\", ", "page_idx": 17}, {"type": "text", "text": "\"A3\": \"Imaging techniques such as wrist X-ray, CT, and MRI are pivotal in diagnosing and evaluating childhood craniopharyngioma. Wrist X-rays can reveal delayed bone age, indicative of growth hormone deficiency. CT scans provide detailed images of bone structures and can detect calcifications commonly seen in craniopharyngiomas. MRI is the most informative, offering high-resolution images of soft tissues, facilitating the identification of cystic components, solid tumors, and involvement of adjacent structures like the hypothalamus and optic chiasm.\", ", "page_idx": 17}, {"type": "text", "text": "\"A4\": \"Craniopharyngiomas often present with a mixture of cystic and solid components and frequent calcifications visible on CT scans. In contrast, teratomas may also show calcifications but often have fat and different tissue types typical of germ cell origin. Mixed germ cell tumors may present with hemorrhagic and necrotic areas and elevate tumor markers like AFP and HCG, which are not elevated in craniopharyngiomas. MRI helps distinguish these tumors based on their unique signal characteristics and anatomical relationships.\", \"A5\": \"Tumor markers such as alpha-fetoprotein (AFP) and beta-human chorionic gonadotropin (HCG) are significant in differentiating craniopharyngiomas from germ cell tumors, which often elevate these markers. Craniopharyngiomas typically do not alter AFP or HCG levels, so normal levels of these markers can help exclude germ cell tumors and support the diagnosis of craniopharyngioma.\", \"A6\": \"Calcification is a prominent feature of craniopharyngiomas and is readily detected on CT scans. In contrast, optic pathway gliomas, typically associated with conditions like neurofibromatosis type 1, usually lack calcification and present as fusiform, enhancing lesions along the optic nerves or chiasm on MRI. Thus, the presence of calcification strongly suggests craniopharyngioma over optic pathway gliomas.\" ", "page_idx": 17}, {"type": "text", "text": "}</ASSISTANT>[/OUTPUT] ", "page_idx": 17}, {"type": "text", "text": "Figure 8: Illustration of the GPT-4 prompt template for the \u201cexpert pediatrician\u201d role. The original Chinese content is translated into the corresponding English version to facilitate intuitive understanding. ", "page_idx": 17}, {"type": "text", "text": "[INST]<SYS>Please act as an AI medical assistant and complete the multi-round medical dialogue refinement task by referring to the already refined honest medical-patient dialogue Med-D. The specific requirements are as follows:   \n1. Keep the patient's questions from the original dialogue and only refine the doctor's responses. Mimic the response style of the doctor in D, ensuring medical professionalism while making the doctor's responses both professional and patient-friendly.   \n2. For questions posed by the patient involving images, audio, or other non-verbal forms, clearly respond: \"I apologise, but as an AI assistant, I cannot receive or process any files or images. I can only communicate and provide information through text.\"   \n3. The generated dialogue should be output in the following format: {\"Patient\u201d:\u201d, \u201cDoctor\u201d:\u201d\u2026 \u201cPatient\u201d:\u201d, \u201cDoctor\u201d:\u201d}.   \n4. The refined medical-patient dialogue Med-D is ${\\mathfrak{S}}\\left\\{{\\begin{array}{r l}\\end{array}}\\right.$ ", "page_idx": 18}, {"type": "text", "text": "\"Patient\": \"Kidney disease. There is slight swelling in the lower legs; ultrasound shows a $0.6\\;\\mathrm{cm}$ kidney stone in the left kidney, multiple kidney cysts, slight hydronephrosis, diffuse changes in the renal parenchyma, and a shrunken left kidney (8 cm). Kidney function tests show urea at 7.26, others normal, and kidney function urine alpha-1 microglobulin at 10.13. Urinalysis shows red blood cells at 31.45, occult blood positive, and other items normal. Taking a quarter of an antihypertensive pill daily, blood pressure has been normal for the past one or two years.\" ", "page_idx": 18}, {"type": "text", "text": "\"Doctor\": \"Your kidney issues include a left kidney stone, multiple cysts, slight hydronephrosis, changes in the renal parenchyma, and a shrunken left kidney. The high red blood cell count in your urine suggests minor bleeding, and elevated alpha-1 microglobulin may indicate kidney damage. Here's a summary:1. Stones and Cysts: The stone might obstruct urine flow, and the cysts could impact kidney function. Shrinkage in your left kidney may result from these structural changes.2. Renal Changes and Hydronephrosis: These signs suggest tissue damage and reduced kidney function.3. Tests: High red blood cells and positive occult blood in urine tests point to slight bleeding. Your urea levels are normal, but further detailed checks are necessary.4. Management: Regularly monitor kidney function, maintain blood pressure with antihypertensive medication, and make dietary and lifestyle changes to reduce kidney stress, like limiting sodium and staying hydrated. Avoid kidney-damaging medications.\" ", "page_idx": 18}, {"type": "text", "text": "\"Patient\": \"Thank you, what medication should I take?\u201d ", "page_idx": 18}, {"type": "text", "text": "\"Doctor\": \"I cannot provide personalized medical advice, but I can offer general information and suggestions:1. Blood Pressure Control: Continue taking your prescribed antihypertensive medications, as managing blood pressure is crucial for reducing kidney stress.2. Prevention of Stones: Depending on their type, your doctor may recommend medications like potassium citrate or hydrochlorothiazide (HCTZ) to manage calcium and uric acid levels in your urine.3. Kidney Function Protection: Medications such as angiotensin-converting enzyme inhibitors (ACEIs) or angiotensin receptor blockers (ARBs) may be recommended to help protect your kidney function. Urinary Tract Health: For urinary tract infections, antibiotics may be necessary. Use NSAIDs cautiously as they can worsen kidney damage.4. Hydration: Adequate hydration can help prevent stone formation and flush the urinary tract.5. Diet: Maintain a low-salt, lowprotein diet, limit meat and other protein-rich foods, increase fruit and vegetable intake, manage calcium intake properly, and avoid highoxalate foods like spinach and strawberries.\"} ", "page_idx": 18}, {"type": "text", "text": "Please follow Med-D\u2019s style and generate compliant dialogue directly. </SYS>[/INST] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "[INST]<USER>{ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\"Patient\": \"I have muscle soreness in my lower limbs and red patches. Tripterygium wilfordii was very effective before, but I can't take it due to stomach ulcers. The symptoms recurred after I stopped taking it.\" ", "page_idx": 18}, {"type": "text", "text": "\"Doctor\": \"I'm very sorry to hear about your discomfort. Based on your description of muscle soreness in the lower limbs accompanied by red patches, it could be related to various diseases. I suggest you visit a hospital for further examination. \" ", "page_idx": 18}, {"type": "text", "text": "\"Patient\": \"The doctor I saw said it's vasculitis.\" ", "page_idx": 18}, {"type": "text", "text": "\"Doctor\": \"Understood. Vasculitis is a possibility, but there are many types of vasculitis. To confirm the diagnosis, I recommend you visit a dermatology department for further examination. \" ", "page_idx": 18}, {"type": "text", "text": "}</USER>[/INST] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "[OUTPUT]<ASSISTANT>{ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\"Patient\": \"I have muscle soreness in my lower limbs and red patches. Tripterygium wilfordii was very effective before, but I can't take it due to stomach ulcers. The symptoms recurred after I stopped taking it.\", ", "page_idx": 18}, {"type": "text", "text": "\"Doctor\": \"Since Tripterygium wilfordii isn't suitable due to your stomach ulcers, you'll need alternative ways to manage your symptoms. Consulting with your healthcare provider is crucial to tailor a treatment plan, especially as your symptoms suggest multiple potential underlying conditions. ", "page_idx": 18}, {"type": "text", "text": "Here are some non-medical strategies to help manage your muscle soreness and skin symptoms:1. Warm Compresses: Apply warmth to sore muscles to increase blood flow and relieve pain. Warm compresses can also soothe inflamed red patches, depending on their cause.2. Gentle Exercise and Stretching: Light movement can alleviate muscle soreness. Be gentle to avoid worsening the pain.3. Over-theCounter Pain Relief: Consider using acetaminophen or topical NSAIDs (if approved by your doctor) to manage pain and inflammation. Use oral NSAIDs cautiously because of your stomach.4. Hydration and Nutrition: Keep hydrated and maintain a balanced diet to support muscle and skin health.5. Skin Care: Adopt gentle skin care routines, avoid irritants, and consider hypoallergenic products, depending on your specific skin condition.\" ", "page_idx": 18}, {"type": "text", "text": "\"Patient\": \"The doctor I saw said it's vasculitis.\", ", "page_idx": 18}, {"type": "text", "text": "\"Doctor\": \"Thanks for sharing your situation. Managing vasculitis can be tricky, especially with stomach ulcers. Since Tripterygium wilfordii isn't an option, consider these alternatives with your doctor:1. Corticosteroids and Immunosuppressants: These reduce inflammation but must be used carefully due to your ulcers.2. Stomach-Protecting Medications: You might combine anti-inflammatory treatments with medications like proton pump inhibitors or H2 blockers to protect your stomach.3. Biologic Therapies: Targeted biologics can effectively treat vasculitis with potentially fewer stomach risks.4. Lifestyle Modifications: Diet adjustments, stress management, and avoiding smoking can help control vasculitis symptoms and prevent flare-ups.5. Regular Monitoring: Since vasculitis can impact various organs, regular health checks are crucial.\" ", "page_idx": 18}, {"type": "text", "text": "}</ASSISTANT>[/OUTPUT] ", "page_idx": 18}, {"type": "text", "text": "Figure 9: Illustration of the GPT-4 prompt template for vanilla doctor-patient conversation regularization. The original Chinese content is translated into the corresponding English version to facilitate intuitive understanding. ", "page_idx": 18}, {"type": "text", "text": "[INST]<SYS>Please assume the role of an experienced doctor, as you are now provided with a pair of medical instruction data, which includes the fields 'instruction' and 'answer'. Please complete Task 1 and Task 2 according to the following guidelines:   \nTask 1: Refine and expand the 'instruction' field.   \nRequirements:1. Utilize your professional medical knowledge to expand 'instruction', turning it into a medical question of depth and relevance. 2. The question should be specific, clear, and capable of eliciting detailed medical discussions.   \n'Instruction' should contain sufficient background information to facilitate understanding and response.   \nTask 2: Based on the refined 'instruction' from Task 1, write the 'output' field.   \nRequirements:1. 'Output' should integrate the information provided in 'answer', enriching the content while maintaining medical accuracy.2. The output should be logically coherent and fluently expressed, and must strictly eliminate any potential references to hospital and personal privacy information. ", "page_idx": 19}, {"type": "text", "text": "The final response should be formatted as follows:{\"instruction\":\"\",\"output\":\"\"}</SYS>[/INST] ", "page_idx": 19}, {"type": "text", "text": "[INST]<USER>{ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\"instruction\": \"What tests are needed for the Vertigo\", ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\"answer\": \"The purpose of testing for vertigo is to identify the cause of the symptoms, as vertigo can be caused by a variety of issues including inner ear problems, cervical spine issues, and brain diseases. Common tests include hearing tests, balance assessments, eye movement tests, and intracranial CT scans. However, the specific testing plan should be determined based on the patient\u2019s specific circumstances (such as the duration of symptoms, frequency of symptoms, accompanying symptoms, etc.). If you can provide more information, I will be better able to guide your treatment plan. Additionally, to alleviate symptoms, it is recommended that you rise slowly when getting up or changing positions, avoid excessive mental fatigue, and engage in appropriate physical activity. ", "page_idx": 19}, {"type": "text", "text": "[OUTPUT]<ASSISTANT>{ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\"instruction\": \"What detailed diagnostic tests should be performed to confirm vertigo, and what are their purposes and specific contexts in which they are used? Please specify whether certain underlying causes, such as inner ear problems, cervical issues, or brain diseases, need to be considered and how to choose appropriate tests based on the patient's clinical symptoms. Are there any general   \nrecommendations that could help alleviate patient symptoms?\" ,   \n\"output\":\"The purpose of diagnostic tests for vertigo is to identify the underlying cause, as vertigo can be triggered by various issues, including inner ear problems, cervical spine issues, and brain diseases. To accurately diagnose vertigo, the following tests are commonly performed:   \n1. Hearing Test (Audiometry): This helps detect issues related to the inner ear or auditory system, as conditions like Meniere's disease or acoustic neuroma often cause vertigo.   \n2. Balance Tests: These assess the vestibular system\u2019s function by evaluating the patient's balance. Standard balance tests include the Vestibular Evoked Myogenic Potentials (VEMP) and Electronystagmography (ENG).   \n3. Eye Movement Tests: Electronystagmography (ENG) involves observing and recording eye movements to identify nystagmus and other abnormalities, which often indicate issues with the vestibular system.   \n4. Cranial CT Scan or MRI: These imaging tests are used to rule out brain disorders or intracranial pathologies, such as strokes, brain tumors, or multiple sclerosis, which can also lead to vertigo symptoms.   \nThe specific combination of tests should be determined based on the patient's clinical presentation, including the duration of symptoms, frequency, and accompanying signs. For instance, hearing and balance tests may be prioritized if a patient also experiences tinnitus or hearing loss. Conversely, imaging studies like a CT scan or MRI may be more warranted if the patient has headaches or other neurological signs.   \nIn addition to these diagnostic tests, some general recommendations can help alleviate vertigo symptoms: patients should rise slowly from bed or when changing positions, avoid overexertion or prolonged periods of mental fatigue, and engage in appropriate physical activities to promote overall circulation. These measures can help somewhat mitigate vertigo symptoms\u2019 severity.\" ", "page_idx": 19}, {"type": "text", "text": "Figure 10: Illustration of the GPT-4 prompt template for progressive instruction reconstruction rule. The original Chinese content is translated into the corresponding English version to facilitate intuitive understanding. ", "page_idx": 19}, {"type": "text", "text": "[INST]<SYS>Please assume the role of an experienced data worker. Your task is to create high-quality medical knowledge texts based on the given instruction data. The specific requirements are as follows:   \n1. Drawing on the provided dialogue and your extensive knowledge base, compile an informative medical knowledge texts from the perspective of writing an encyclopedia.   \n2. Ensure that the generated content is objectively accurate.   \n3. The language should be smooth and concise.</SYS>[/INST] ", "page_idx": 20}, {"type": "text", "text": "[INST]<USER> ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\"instruction\": \"What is hemophilia?\", ", "page_idx": 20}, {"type": "text", "text": "\"output\": \"Hemophilia is a group of genetic bleeding disorders caused by the deficiency of certain clotting factors in the blood, leading to severe coagulation disorders. It can affect both males and females, but the vast majority of patients are male. This includes Hemophilia A, Hemophilia B, and Factor XI deficiency (formerly known as Hemophilia C). The first two are X-linked recessive inherited, while the latter is autosomally inherited as an incomplete recessive trait. Hemophilia is the most common congenital bleeding disorder, with bleeding being the primary clinical manifestation of the disease.\" ", "page_idx": 20}, {"type": "text", "text": "[OUTPUT][ASSISTANT] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Hemophilia is a rare genetic disorder characterized primarily by the inability to properly form blood clots, leading to prolonged bleeding. This condition is primarily due to the deficiency or absence of specific clotting factors, proteins essential for blood clot formation. The most common types of this disorder are Hemophilia A and Hemophilia B, which involve deficiencies in factor VIII and factor IX, respectively. Hemophilia A is about four times more common than Hemophilia B. ", "page_idx": 20}, {"type": "text", "text": "The disorder is generally inherited in an X-linked recessive pattern, which explains its higher prevalence among males. Females can be carriers of the condition, and while they usually do not exhibit severe symptoms, they can still experience bleeding episodes. Another form, known as Hemophilia C or factor XI deficiency, follows an autosomal recessive inheritance pattern, affecting both males and females equally, and generally leads to milder symptoms. ", "page_idx": 20}, {"type": "text", "text": "Individuals with hemophilia may experience spontaneous bleeding events or prolonged bleeding after injuries or surgery. Common complications include bleeding into joints and muscles, leading to pain and, over time, potentially severe joint damage. Modern treatment primarily involves replacing the missing clotting factor, either regularly as a preventive measure or to manage bleeding episodes when they occur. ", "page_idx": 20}, {"type": "text", "text": "Diagnosis typically occurs in childhood and is based on blood tests that measure clotting factor levels. Advances in medical science, particularly the development of recombinant clotting factors and gene therapy, continue to improve the quality of life and life expectancy for those affected with hemophilia.[/ASSISTANT][/OUTPUT] ", "page_idx": 20}, {"type": "text", "text": "Figure 11: Illustration of the GPT-4 prompt template for improving corpus density and breadth. The original Chinese content is translated into the corresponding English version to facilitate intuitive understanding. ", "page_idx": 20}, {"type": "text", "text": "Instruction: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u8bf7\u7b80\u8ff0\u7ea2\u7ec6\u80de\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u7f3a\u4e4f\u75c7\u7684\u75c5\u56e0\u3002 (Please summarize the etiology of glucose-6-phosphate dehydrogenase deficiency in red blood cells.) ", "page_idx": 21}, {"type": "text", "text": "Zhongjing ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u7ea2\u7ec6\u80de\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u7f3a\u4e4f\u75c7\u662f\u4e00\u79cd\u9057\u4f20\u6027\u75be\u75c5\uff0c\u4e3b\u8981\u7531\u4e8e\u9057\u4f20\u56e0\u7d20\u5f15\u8d77\u7684\u57fa\u56e0\u7a81\u53d8\u6240\u81f4\u3002\u8fd9\u79cd\u57fa\u56e0\u7a81\u53d8\u5bfc\u81f4\u7ea2\u7ec6\u80de\u5185\u7684\u7ea2\u7ec6\u80de\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u7684\u7ed3\u6784\u548c\u529f\u80fd\u53d1\u751f\u5f02\u5e38\uff0c\u4ece\u800c\u5f71\u54cd\u8840\u7ea2\u86cb\u767d\u7684\u5408\u6210\u548c\u4ee3\u8c22\uff0c\u5bfc\u81f4\u60a3\u8005\u51fa\u73b0\u8d2b\u8840\u3001\u9ec4\u75b8\u3001\u809d\u529f\u80fd\u5f02\u5e38\u7b49\u75c7\u72b6\u3002", "page_idx": 21}, {"type": "text", "text": "(Glucose-6-phosphate dehydrogenase deficiency in red blood cells is a hereditary disorder primarily caused by genetic mutations. These mutations lead to structural and functional abnormalities in the glucose-6-phosphate dehydrogenase enzyme within red blood cells, affecting the synthesis and metabolism of hemoglobin. As a result, patients experience symptoms such as anemia, jaundice, and abnormal liver function.) ", "page_idx": 21}, {"type": "text", "text": "HuatuoGPT-II ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u7ea2\u7ec6\u80de\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u7f3a\u4e4f\u75c7\u7684\u75c5\u56e0\u4e3b\u8981\u662f\u7531\u4e8e\u9057\u4f20\u57fa\u56e0\u7a81\u53d8\u5f15\u8d77\u7684\u3002\u8fd9\u79cd\u7a81\u53d8\u4f1a\u5bfc\u81f4\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u7684\u529f\u80fd\u53d7\u635f\u6216\u5b8c\u5168\u7f3a\u5931\uff0c\u8fdb\u800c\u5f71\u54cd\u7ea2\u7ec6\u80de\u7684\u6b63\u5e38\u4ee3\u8c22\u8fc7\u7a0b\u3002\u8fd9\u79cd\u75be\u75c5\u901a\u5e38\u662f\u901a\u8fc7\u9057\u4f20\u65b9\u5f0f\u4f20\u9012\u7ed9\u540e\u4ee3\u7684\uff0c\u800c\u5177\u4f53\u7684\u9057\u4f20\u65b9\u5f0f\u5219\u53d6\u51b3\u4e8e\u7a81\u53d8\u7684\u7c7b\u578b\u3002\u6b64\u5916\uff0c\u4e00\u4e9b\u73af\u5883\u56e0\u7d20\u4e5f\u53ef\u80fd\u5bf9\u7ea2\u7ec6\u80de\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u7f3a\u4e4f\u75c7\u7684\u53d1\u75c5\u8d77\u5230\u4e00\u5b9a\u7684\u5f71\u54cd\u3002\u4f8b\u5982\uff0c\u67d0\u4e9b\u836f\u7269\u3001\u611f\u67d3\u3001\u5e94\u6fc0\u7b49\u56e0\u7d20\u90fd\u53ef\u80fd\u5f15\u53d1\u75be\u75c5\u7684\u53d1\u4f5c\u3002\u603b\u7ed3\u8d77\u6765\uff0c\u7ea2\u7ec6\u80de\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u7f3a\u4e4f\u75c7\u662f\u4e00\u79cd\u7531\u9057\u4f20\u57fa\u56e0\u7a81\u53d8\u5f15\u8d77\u7684\u75be\u75c5\uff0c\u5176\u75c5\u56e0\u4e3b\u8981\u662f\u57fa\u56e0\u7a81\u53d8\u5bfc\u81f4\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\u529f\u80fd\u53d7\u635f\u6216\u7f3a\u5931\u3002", "page_idx": 21}, {"type": "text", "text": "(Genetic mutations are the leading cause of glucose-6-phosphate dehydrogenase deficiency in red blood cells. These mutations result in impaired or complete absence of the function of glucose-6-phosphate dehydrogenase, thereby affecting the normal metabolic processes of red blood cells. This disease is typically passed down through generations via genetic inheritance, with the specific mode of inheritance depending on the type of mutation. Certain environmental factors may also play a role in the onset of glucose-6-phosphate dehydrogenase deficiency. For example, certain drugs, infections, and stressors, among other factors, may trigger disease episodes. In summary, glucose-6-phosphate dehydrogenase deficiency in red blood cells is a genetic disorder caused by mutations, primarily resulting in impaired or absent function of glucose-6-phosphate dehydrogenase due to genetic mutations.) ", "page_idx": 21}, {"type": "text", "text": "PediatricsGPT ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u7ea2\u7ec6\u80de\u8461\u8404\u7cd6-6-\u78f7\u9178\u8131\u6c22\u9176\uff08G6PD\uff09\u7f3a\u4e4f\u75c7\u662f\u4e00\u79cd\u9057\u4f20\u6027\u75be\u75c5\uff0c\u7531\u4e8eG6PD\u57fa\u56e0\u7684\u7a81\u53d8\u5bfc\u81f4\u3002G6PD\u662f\u4e00\u79cd\u9176\uff0c\u4e3b\u8981\u5b58\u5728\u4e8e\u7ea2\u7ec6\u80de\u4e2d\uff0c\u5b83\u5bf9\u4e8e\u4fdd\u62a4\u7ea2\u7ec6\u80de\u514d\u53d7\u6c27\u5316\u635f\u4f24\u81f3\u5173\u91cd\u8981\u3002G6PD\u5e2e\u52a9\u7ef4\u6301\u8fd8\u539f\u578b\u8c37\u80f1\u7518\u80bd\uff08GSH\uff09\u7684\u6c34\u5e73\uff0cGSH\u662f\u4e00\u79cd\u91cd\u8981\u7684\u6297\u6c27\u5316\u5242\uff0c\u80fd\u591f\u4e2d\u548c\u6709\u5bb3\u7684\u6c27\u5316\u5242\uff0c\u4fdd\u62a4\u7ea2\u7ec6\u80de\u514d\u53d7\u635f\u4f24\u3002\u5f53\u4eba\u4f53\u4e2d\u7684G6PD\u6d3b\u6027\u964d\u4f4e\u6216\u7f3a\u5931\u65f6\uff0c\u7ea2\u7ec6\u80de\u5728\u9762\u5bf9\u6c27\u5316\u5e94\u6fc0\uff08\u5982\u67d0\u4e9b\u98df\u7269\u3001\u836f\u7269\u3001\u611f\u67d3\u6216\u5176\u4ed6\u5e94\u6fc0\u60c5\u51b5\uff09\u65f6\u66f4\u5bb9\u6613\u53d7\u635f\uff0c\u5bfc\u81f4\u7ea2\u7ec6\u80de\u8fc7\u65e9\u7834\u88c2\uff0c\u4ece\u800c\u5f15\u53d1\u6eb6\u8840\u6027\u8d2b\u8840\u3002\u8fd9\u79cd\u72b6\u51b5\u53ef\u80fd\u5bfc\u81f4\u75b2\u52b3\u3001\u9ec4\u75b8\u3001\u5c3f\u8272\u52a0\u6df1\u7b49\u75c7\u72b6\u3002G6PD\u7f3a\u4e4f\u75c7\u7684\u9057\u4f20\u6a21\u5f0f\u4e3b\u8981\u662fX\u67d3\u8272\u4f53\u8fde\u9501\u9690\u6027\u9057\u4f20\u3002\u56e0\u4e3a\u7537\u6027\uff08XY\uff09\u53ea\u6709\u4e00\u4e2aX\u67d3\u8272\u4f53\uff0c\u6240\u4ee5\u5982\u679c\u4ed6\u4eec\u7684X\u67d3\u8272\u4f53\u4e0a\u643a\u5e26\u6709\u7f3a\u9677\u7684G6PD\u57fa\u56e0\uff0c\u4ed6\u4eec\u5c31\u4f1a\u8868\u73b0\u51fa\u8fd9\u79cd\u75be\u75c5\u3002\u800c\u5973\u6027\uff08XX\uff09\u5219\u9700\u8981\u5728\u4e24\u4e2aX\u67d3\u8272\u4f53\u4e0a\u90fd\u643a\u5e26\u6709\u7f3a\u9677\u7684G6PD\u57fa\u56e0\u624d\u4f1a\u8868\u73b0\u51fa\u75be\u75c5\uff0c\u5982\u679c\u53ea\u6709\u4e00\u4e2aX\u67d3\u8272\u4f53\u643a\u5e26\u7f3a\u9677\u57fa\u56e0\uff0c\u5979\u4eec\u901a\u5e38\u662f\u65e0\u75c7\u72b6\u7684\u643a\u5e26\u8005\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e5f\u53ef\u80fd\u8868\u73b0\u51fa\u8f7b\u5fae\u5230\u4e2d\u7b49\u7a0b\u5ea6\u7684\u75c7\u72b6\u3002G6PD\u7f3a\u4e4f\u75c7\u5728\u5168\u7403\u8303\u56f4\u5185\u5206\u5e03\u5e7f\u6cdb\uff0c\u7279\u522b\u662f\u5728\u975e\u6d32\u3001\u5730\u4e2d\u6d77\u5730\u533a\u3001\u4e2d\u4e1c\u548c\u4e9a\u6d32\u7684\u67d0\u4e9b\u90e8\u5206\uff0c\u8fd9\u4e0e\u8fd9\u4e9b\u5730\u533a\u759f\u75be\u9ad8\u53d1\u6709\u5173\uff0c\u56e0\u4e3aG6PD\u7f3a\u4e4f\u75c7\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u4f9b\u4e86\u5bf9\u67d0\u4e9b\u7c7b\u578b\u759f\u75be\u7684\u4fdd\u62a4\u4f5c\u7528\u3002", "page_idx": 21}, {"type": "text", "text": "(Glucose-6-phosphate dehydrogenase (G6PD) deficiency is a hereditary disorder caused by mutations in the G6PD gene. G6PD is an enzyme primarily found in red blood cells and is crucial for protecting these cells from oxidative damage. G6PD helps maintain reduced glutathione (GSH) levels, an important antioxidant that neutralises harmful oxidants and protects red blood cells from damage. When the activity of G6PD in the body is reduced or absent, red blood cells become more susceptible to damage under oxidative stress conditions, such as exposure to certain foods, medications, infections, or other stressors. This increased susceptibility can lead to the premature rupture of red blood cells, resulting in hemolytic anemia. Symptoms of this condition can include fatigue, jaundice, and dark urine. The inheritance pattern of G6PD deficiency is primarily X-linked recessive. Males (XY) have only one X chromosome, so they will exhibit the disease if their X chromosome carries the defective G6PD gene. Females (XX) need to have the defective G6PD gene on both of their X chromosomes to show the disease; if only one X chromosome carries the defective gene, they are typically asymptomatic carriers, though they may sometimes exhibit mild to moderate symptoms.G6PD deficiency is widespread globally, particularly in regions such as Africa, the Mediterranean, the Middle East, and parts of Asia. This prevalence is linked to the historically high incidence of malaria in these regions, as G6PD deficiency offers some protection against certain types of malaria.) ", "page_idx": 21}, {"type": "text", "text": "Figure 13: Comparison results of model responses on the medical knowledge question-answer task. ", "page_idx": 21}, {"type": "text", "text": "Instruction-1: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u6211\u5bb6\u5b69\u5b50\u60c5\u51b5\u5982\u4e0b\uff1a\u7537\uff0c\u56e0\u201c\u751f\u540e\u547c\u5438\u56f0\u96be10\u5c0f\u65f6\u4f59\u201d\u5c31\u8bca\u3002\u8bf7\u95ee\u662f\u4ec0\u4e48\u539f\u56e0\u5462\uff1f   \n(My child's situation is as follows: male, presented due to \"difficulty breathing for over 10 hours\" after birth. What could be the cause? )   \nInstruction-2: ", "page_idx": 22}, {"type": "text", "text": "\u5b69\u5b50\u8f85\u52a9\u68c0\u67e5\u7ed3\u679c\u5982\u4e0b\uff1a\u8840\u5e38\u89c4\uff1aWBC $24.82\\!\\times\\!10^{\\wedge}9/\\mathrm{L}$ , RBC $8.23\\!\\times\\!10^{\\wedge}12/\\mathrm{L}$ \uff0cHb $198~\\mathrm{g/L}$ , PLT $378\\!\\times\\!10^{\\wedge}9/\\mathrm{L}$ , CRP $25\\mathrm{mg/L}$ \u3002\u8840\u6c14\u5206\u6790\uff1apH 7.20\uff0cPaO2 52mmHg\uff0cPaCO2 56mmHg\uff0ccHCO3- $17.2\\mathrm{mmol/L}$ \u3002\u8840\u57f9\u517b\uff1a\u9634\u6027\u3002\u80f8\u7247\uff1a\u4e24\u80ba\u91ce\u900f\u4eae\u5ea6\u51cf\u4f4e\uff0c\u5f25\u6f2b\u7ec6\u9897\u7c92\u72b6\u5f71\uff0c\u5916\u5468\u578b\u652f\u6c14\u7ba1\u5145\u6c14\u5f81\u3002\u5fc3\u8d85\uff1a\u52a8\u8109\u5bfc\u7ba1\u672a\u95ed $3.3\\mathrm{mm}$ \uff0c\u5375\u5706\u5b54\u672a\u95ed $1.8\\mathrm{mm}$ \uff0c\u4e09\u5c16\u74e3\u8fd4\u6d41\u538b\u5dee $58\\mathrm{mmHg}$ \u3002", "page_idx": 22}, {"type": "text", "text": "(The child's auxiliary examination results: Complete blood count: WBC $24.82\\times10^{\\sim}9/\\mathrm{L}$ , RBC $8.23\\times10^{\\wedge}12/\\mathrm{L}$ , Hb $198~\\mathrm{g/L}$ , PLT $378\\times10^{\\wedge}9/\\mathrm{L}$ , CRP $25\\mathrm{mg/L}$ . Blood gas analysis: $\\mathsf{p H}\\,7.20\\$ , PaO2 52mmHg, PaCO2 56mmHg, cHCO3- $17.2\\mathrm{mmol/L}$ . Blood culture: negative. Chest X-ray: Reduced transparency in both lung fields, diffuse fine granular shadows, peripheral signs of bronchial inflation. Echocardiogram: Patent ductus arteriosus $3.3\\mathrm{mm}$ , patent foramen ovale $1.8\\mathrm{mm}$ , tricuspid regurgitation pressure gradient $58\\mathrm{mmHg.}$ ) ", "page_idx": 22}, {"type": "text", "text": "Zhongjing ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Response-1: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u4f60\u597d\uff0c\u8bf7\u95ee\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7684\u5417\uff1f \u5f88\u9ad8\u5174\u4e3a\u60a8\u670d\u52a1\uff0c\u8bf7\u95ee\u6709\u4ec0\u4e48\u53ef\u4ee5\u5e2e\u52a9\u60a8\u7684\u5417\uff1f (Hello, may I help you with anything? I'm happy to assist you. Is there anything I can help you with? ) ", "page_idx": 22}, {"type": "text", "text": "Response-2: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u6839\u636e\u5b69\u5b50\u7684\u8f85\u52a9\u68c0\u67e5\u7ed3\u679c\uff0c\u8840\u5e38\u89c4\u5404\u4e2a\u6307\u6807\u90fd\u5728\u6b63\u5e38\u8303\u56f4\u3002\u8840\u6c14\u5206\u6790\u6b63\u5e38\uff0c\u4f46\u662fcHCO3-\u504f\u4f4e\uff0c\u8bf4\u660e\u5b69\u5b50\u53ef\u80fd\u5b58\u5728\u9178\u4e2d\u6bd2\u3002\u80f8\u7247\u663e\u793a\u4e24\u80ba\u91ce\u900f\u4eae\u5ea6\u51cf\u4f4e\uff0c\u53ef\u89c1\u5f25\u6f2b\u7ec6\u9897\u7c92\u72b6\u5f71\uff0c\u53ef\u80fd\u662f\u56e0\u4e3a\u547c\u5438\u7cfb\u7edf\u75be\u75c5\u5f15\u8d77\u7684\u3002\u5fc3\u8d85\u663e\u793a\u52a8\u8109\u5bfc\u7ba1\u672a\u95ed\u3001\u5375\u5706\u5b54\u672a\u95ed\u3001\u4e09\u5c16\u74e3\u8fd4\u6d41\u538b\u5dee\u8f83\u9ad8\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u8bc4\u4f30\u548c\u6cbb\u7597\u3002", "page_idx": 22}, {"type": "text", "text": "(Based on the child's auxiliary examination results, all parameters in the complete blood count are within normal ranges. Blood gas analysis is normal, but a low cHCO3- suggests the child may have acidosis. The chest X-ray shows reduced transparency in both lung fields with visible diffuse fine granular shadows, likely due to a respiratory system disease. The echocardiogram reveals a patent ductus arteriosus, a patent foramen ovale, and a high tricuspid regurgitation pressure gradient, necessitating further evaluation and treatment.) ", "page_idx": 22}, {"type": "text", "text": "HuatuoGPT-II ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Response-1: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u5bf9\u4e0d\u8d77\uff0c\u60a8\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u606f\u3002\u8bf7\u518d\u8865\u5145\u4e00\u4e9b\u7ec6\u8282\u3002(Sorry, you haven't provided enough information. Please provide some more details.)", "page_idx": 22}, {"type": "text", "text": "Response-2: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u6839\u636e\u60a8\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u5b69\u5b50\u7684\u75c7\u72b6\u8868\u660e\u4ed6\u53ef\u80fd\u60a3\u6709\u80ba\u708e\u3002\u68c0\u67e5\u663e\u793a\u767d\u7ec6\u80de\u8ba1\u6570\uff08WBC\uff09\u5347\u9ad8\uff0cC\u53cd\u5e94\u86cb\u767d\uff08CRP\uff09\u5347\u9ad8\uff0c\u63d0\u793a \u53ef\u80fd\u5b58\u5728\u611f\u67d3\u548c\u708e\u75c7\u3002\u8840\u6c14\u5206\u6790\u7ed3\u679c\u663e\u793apH\u503c\u964d\u4f4e\uff0cPaO2\u503c\u964d\u4f4e\uff0cPaCO2\u503c\u5347\u9ad8\uff0c\u63d0\u793a\u53ef\u80fd\u5b58\u5728\u547c\u5438\u6027\u9178\u4e2d\u6bd2\u3002\u80f8\u7247\u663e\u793a\u4e24 \u80ba\u91ce\u900f\u4eae\u5ea6\u51cf\u4f4e\uff0c\u53ef\u89c1\u5f25\u6f2b\u7ec6\u9897\u7c92\u72b6\u5f71\uff0c\u5916\u5468\u578b\u652f\u6c14\u7ba1\u5145\u6c14\u5f81\u53ef\u89c1\uff0c\u63d0\u793a\u53ef\u80fd\u5b58\u5728\u80ba\u708e\u3002\u5fc3\u8d85\u7ed3\u679c\u663e\u793a\u52a8\u8109\u5bfc\u7ba1\u672a\u95ed\u548c\u5375\u5706 \u5b54\u672a\u95ed\uff0c\u4ee5\u53ca\u4e09\u5c16\u74e3\u8fd4\u6d41\u538b\u5dee\u5347\u9ad8\uff0c\u63d0\u793a\u53ef\u80fd\u5b58\u5728\u5fc3\u810f\u65b9\u9762\u7684\u95ee\u9898\u3002\u5efa\u8bae\u5c3d\u5feb\u5e26\u5b69\u5b50\u53bb\u533b\u9662\u5c31\u8bca\uff0c\u63a5\u53d7\u8fdb\u4e00\u6b65\u7684\u68c0\u67e5\u548c\u6cbb\u7597\u3002 (Based on the information, the child's symptoms suggest he may have pneumonia. Tests show elevated white blood cell count (WBC) and C-reactive protein (CRP), indicating a possible infection and inflammation. Blood gas analysis results show a decreased pH, reduced PaO2, and increased PaCO2, suggesting potential respiratory acidosis. The chest X-ray shows reduced transparency in both lung fields with visible diffuse fine granular shadows and peripheral signs of bronchial inflation, indicative of pneumonia. Echocardiogram results reveal a patent ductus arteriosus and foramen ovale, and an increased tricuspid regurgitation pressure gradient, suggesting possible heart issues. It is recommended to take the child to the hospital promptly for further examination and treatment.) ", "page_idx": 22}, {"type": "text", "text": "PediatricsGPT ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Response-1: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u60a8\u597d\uff0c\u6211\u7406\u89e3\u60a8\u7684\u62c5\u5fe7\u3002\u4e3a\u4e86\u66f4\u5168\u9762\u5730\u4e86\u89e3\u60a8\u5b69\u5b50\u7684\u72b6\u51b5\uff0c\u6211\u9700\u8981\u60a8\u63d0\u4f9b\u4e00\u4e9b\u533b\u9662\u8f85\u52a9\u68c0\u67e5\u7ed3\u679c\u3002 (Hello, I understand your concern. In order to have a more comprehensive understanding of your child's condition, I need you to provide some hospital auxiliary examination results.) ", "page_idx": 22}, {"type": "text", "text": "Response-2: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u6839\u636e\u60a8\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u5b69\u5b50\u7684\u60c5\u51b5\u6d89\u53ca\u5230\u591a\u4e2a\u7cfb\u7edf\u7684\u95ee\u9898\u3002\u4ee5\u4e0b\u662f\u5bf9\u5404\u9879\u68c0\u67e5\u7ed3\u679c\u7684\u5206\u6790\uff1a1. \u8840\u5e38\u89c4\uff1a- WBC\uff08\u767d\u7ec6\u80de\u8ba1\u6570\uff09$24.82\\times10^{\\wedge}9/\\mathrm{L}$ \uff0c\u63d0\u793a\u6709\u611f\u67d3\u6216\u708e\u75c7\u7684\u53ef\u80fd\u3002- RBC\uff08\u7ea2\u7ec6\u80de\u8ba1\u6570\uff09 $8.23\\times10^{\\wedge}12/\\mathrm{L}$ \u548c Hb\uff08\u8840\u7ea2\u86cb\u767d\uff09 $198~\\mathrm{g/L}$ \uff0c\u663e\u793a\u7ea2\u7ec6\u80de\u548c\u8840\u7ea2\u86cb\u767d\u504f\u9ad8\uff0c\u53ef\u80fd\u5b58\u5728\u591a\u8840\u75c7\u6216\u5176\u4ed6\u539f\u56e0\u3002- PLT\uff08\u8840\u5c0f\u677f\uff09 $378\\times10^{\\wedge}9/\\mathrm{L}$ \uff0c\u6b63\u5e38\u8303\u56f4\u3002- CRP\uff08C\u53cd\u5e94\u86cb\u767d\uff09 $25~\\mathrm{mg/L}$ \uff0c\u63d0\u793a\u6709\u708e\u75c7\u53cd\u5e94\u30022. \u8840\u6c14\u5206\u6790\uff1a- $\\mathsf{p H}\\,7.20\\$ \uff0c $\\mathrm{PaCO2}\\ 56\\ \\mathrm{mmHg}$ \uff0ccHCO3- $17.2\\;\\mathrm{mmol/L}$ \uff0c\u663e\u793a\u4ee3\u8c22\u6027\u9178\u4e2d\u6bd2\u4f34\u6709\u547c\u5438\u6027\u9178\u4e2d\u6bd2\u3002- PaO2$52\\;\\mathrm{mmHg}$ \uff0c\u663e\u793a\u6709\u4f4e\u6c27\u8840\u75c7\u30023. \u80f8\u7247 - \u4e24\u80ba\u91ce\u900f\u4eae\u5ea6\u51cf\u4f4e\uff0c\u5f25\u6f2b\u7ec6\u9897\u7c92\u72b6\u5f71\uff0c\u53ef\u80fd\u63d0\u793a\u80ba\u90e8\u611f\u67d3\u6216\u80ba\u6c34\u80bf\u3002- \u5916\u5468\u578b\u652f\u6c14\u7ba1\u5145\u6c14\u5f81\uff0c\u53ef\u80fd\u4e0e\u80ba\u90e8\u611f\u67d3\u6216\u6162\u6027\u80ba\u75c5\u6709\u5173\u30024. \u5fc3\u8d85\uff1a- \u52a8\u8109\u5bfc\u7ba1\u672a\u95ed\u548c\u5375\u5706\u5b54\u672a\u95ed\uff0c\u5747\u5b58\u5728\u53f3\u5411\u5de6\u5206\u6d41\uff0c\u53ef\u80fd\u5bfc\u81f4\u8840\u6db2\u52a8\u529b\u5b66\u5f02\u5e38\u3002- \u4e09\u5c16\u74e3\u8fd4\u6d41\u538b\u5dee $58\\;\\mathrm{mmHg}$ \uff0c\u63d0\u793a\u6709\u8f83\u91cd\u7684\u80ba\u52a8\u8109\u9ad8\u538b\u3002\u7efc\u5408\u8003\u8651\uff0c\u5b69\u5b50\u53ef\u80fd\u5b58\u5728\u80ba\u90e8\u611f\u67d3\u6216\u80ba\u6c34\u80bf\uff0c\u4f34\u6709\u4ee3\u8c22\u6027\u9178\u4e2d\u6bd2\u548c\u547c\u5438\u6027\u9178\u4e2d\u6bd2\u3002\u52a8\u8109\u5bfc\u7ba1\u672a\u95ed\u548c\u5375\u5706\u5b54\u672a\u95ed\u53ef\u80fd\u5bfc\u81f4\u8840\u6db2\u52a8\u529b\u5b66\u5f02\u5e38\uff0c\u5305\u62ec\u80ba\u52a8\u8109\u9ad8\u538b\u3002\u5efa\u8bae\u8fdb\u4e00\u6b65\u5bc6\u5207\u76d1\u6d4b\u5b69\u5b50\u7684\u751f\u547d\u4f53\u5f81\u548c\u6c27\u5408\u60c5\u51b5\uff0c\u53ef\u80fd\u9700\u8981\u547c\u5438\u652f\u6301\u3001\u6297\u611f\u67d3\u6cbb\u7597\u3001\u7ea0\u6b63\u9178\u78b1\u5e73\u8861\u7b49\u652f\u6301\u6027\u6cbb\u7597\u3002\u540c\u65f6\uff0c\u9700\u8981\u5bc6\u5207\u89c2\u5bdf\u5fc3\u810f\u529f\u80fd\u548c\u8840\u6d41\u52a8\u529b\u5b66\u53d8\u5316\uff0c\u5fc5\u8981\u65f6\u8003\u8651\u5fc3\u810f\u652f\u6301\u6cbb\u7597\u3002\u5efa\u8bae\u4e0e\u513f\u79d1\u5fc3\u810f\u79d1\u3001\u547c\u5438\u79d1\u7b49\u591a\u5b66\u79d1\u56e2\u961f\u5408\u4f5c\uff0c\u5236\u5b9a\u7efc\u5408\u6cbb\u7597\u65b9\u6848\u3002", "page_idx": 22}, {"type": "text", "text": "(Based on the provided information, the child's medical evaluations indicate complex health issues across multiple systems: 1. Complete Blood Count reveals elevated WBC at $24.82\\times10^{\\wedge}9/\\mathrm{L}$ suggesting infection or inflammation, high RBC and Hb possibly indicating polycythemia, normal PLT, and elevated CRP pointing to inflammation. 2. Blood Gas Analysis shows a $\\mathsf{p H}$ of 7.20, elevated PaCO2, low cHCO3- suggesting metabolic acidosis with respiratory compensation, and low PaO2 indicating hypoxemia. 3. Chest X-ray reveals decreased transparency and diffuse granular shadows, suggesting infection or pulmonary edema, with peripheral bronchial inflation signs hinting at chronic lung conditions. 4. Echocardiography shows a patent ductus arteriosus and foramen ovale with right-to-left shunting, and a tricuspid regurgitation pressure gradient of $58\\;\\mathrm{mmHg}$ indicating significant pulmonary hypertension. These findings suggest lung infection or pulmonary edema, metabolic acidosis and respiratory compensation. Patent ductus arteriosus and foramen ovale could cause hemodynamic issues, including pulmonary hypertension. Immediate measures should include monitoring vital signs and oxygen levels, respiratory support, antimicrobial therapy, and acid-base correction. Continuous cardiac monitoring and potential cardiac support are necessary. Coordination with a multidisciplinary team including pediatric cardiology and respiratory specialists is crucial for an integrated treatment approach.) ", "page_idx": 22}, {"type": "text", "text": "Instruction: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u6211\u7684\u5b69\u5b50\u524d\u51e0\u5929\u5728\u73a9\u800d\u65f6\u4e0d\u5c0f\u5fc3\u6454\u5012\uff0c\u8bf4\u8170\u90e8\u5f88\u75bc\uff0c\u53bb\u533b\u9662\u68c0\u67e5\u8bca\u65ad\u4e3a\u8170\u690e\u95f4\u76d8\u7a81\u51fa\uff0c\u6211\u60f3\u4e86\u89e3\u5bf9\u4e8e\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u7684\u513f\u7ae5\uff0c\u5e94\u8be5\u91c7\u53d6\u600e\u6837\u7684\u836f\u7269\u6cbb\u7597\uff1f", "page_idx": 23}, {"type": "text", "text": "(My child fell while playing a few days ago and complained of severe pain in the lower back. After visiting the hospital, the diagnosis was confirmed as lumbar disc herniation. I would like to know what medication treatment should be adopted for children with lumbar disc herniation?) ", "page_idx": 23}, {"type": "text", "text": "Zhongjing: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u5bf9\u4e8e\u513f\u7ae5\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u7684\u6cbb\u7597\uff0c\u9996\u5148\u8981\u786e\u5b9a\u75c5\u60c5\u7684\u4e25\u91cd\u7a0b\u5ea6\u548c\u75c5\u56e0\uff0c\u4ee5\u4fbf\u9009\u62e9\u5408\u9002\u7684\u6cbb\u7597\u65b9\u6cd5\u3002\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u8f7b\u5ea6\u7684\u8170\u690e\u95f4 \u76d8\u7a81\u51fa\u53ef\u4ee5\u901a\u8fc7\u7269\u7406\u6cbb\u7597\u548c\u836f\u7269\u6cbb\u7597\u6765\u7f13\u89e3\u75c7\u72b6\u3002\u4f46\u662f\uff0c\u836f\u7269\u6cbb\u7597\u5e94\u8be5\u5728\u533b\u751f\u7684\u6307\u5bfc\u4e0b\u8fdb\u884c\uff0c\u4e0d\u80fd\u81ea\u884c\u8d2d\u4e70\u548c\u4f7f\u7528\u836f\u7269\u3002 (Glucose-6-phosphate dehydrogenase deficiency in red blood cells is a hereditary disorder primarily caused by genetic mutations. These mutations lead to structural and functional abnormalities in the glucose-6-phosphate dehydrogenase enzyme within red blood cells, affecting the synthesis and metabolism of hemoglobin. As a result, patients experience symptoms such as anemia, jaundice, and abnormal liver function.) ", "page_idx": 23}, {"type": "text", "text": "HuatuoGPT-II: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u5728\u513f\u7ae5\u4e2d\u76f8\u5bf9\u8f83\u5c11\u89c1\uff0c\u901a\u5e38\u4e0e\u9aa8\u9abc\u53d1\u80b2\u4e0d\u5168\u6216\u8fc7\u5ea6\u6d3b\u52a8\u6709\u5173\u3002\u6cbb\u7597\u65b9\u6cd5\u4e3b\u8981\u5305\u62ec\u4fdd\u5b88\u6cbb\u7597\u548c\u624b\u672f\u6cbb\u7597\u3002\u5bf9\u4e8e\u513f\u7ae5\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u7684\u836f\u7269\u6cbb\u7597\uff0c\u901a\u5e38\u5305\u62ec\u4ee5\u4e0b\u51e0\u79cd\uff1a1. \u975e\u753e\u4f53\u6297\u708e\u836f\uff08NSAIDs\uff09\uff1a\u5982\u5e03\u6d1b\u82ac\uff08Ibuprofen\uff09\u6216\u8005\u8418\u666e\u751f", "page_idx": 23}, {"type": "text", "text": "\uff08Naproxen\uff09\u7b49\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7f13\u89e3\u75bc\u75db\u548c\u51cf\u5c11\u708e\u75c7\u30022. \u808c\u8089\u677e\u5f1b\u5242\uff1a\u5982\u7532\u82ef\u78fa\u9178\u6c28\u57fa\u6c2f\u5730\u5e73\uff08Amlodipine Besylate\uff09\u7b49\uff0c\u53ef\u4ee5\u5e2e\u52a9\u7f13\u89e3\u808c\u8089\u75c9\u631b\u30023. \u795e\u7ecf\u8425\u517b\u836f\uff1a\u5982\u7ef4\u751f\u7d20B\u65cf\u7b49\uff0c\u53ef\u4ee5\u5e2e\u52a9\u795e\u7ecf\u4fee\u590d\u30024. \u4e2d\u836f\u6cbb\u7597\uff1a\u5982\u6d3b\u8840\u5316\u7600\u7c7b\u4e2d\u836f\uff0c\u53ef\u4ee5\u5e2e\u52a9\u6539\u5584\u8840\u6db2\u5faa\u73af\uff0c\u7f13\u89e3\u75bc\u75db\u3002\u5bf9\u4e8e\u513f\u7ae5\u6765\u8bf4\uff0c\u836f\u7269\u6cbb\u7597\u9700\u8981\u5728\u533b\u751f\u7684\u6307\u5bfc\u4e0b\u8fdb\u884c\u3002", "page_idx": 23}, {"type": "text", "text": "(Lumbar disc herniation is rare in children and typically associated with skeletal underdevelopment or excessive activity. Treatment methods mainly include conservative and surgical treatments. Pharmacological treatment for pediatric lumbar disc herniation typically includes the following: 1. Nonsteroidal anti-inflammatory drugs (NSAIDs): such as ibuprofen or naproxen, can help alleviate pain and reduce inflammation. 2. Muscle relaxants, such as amlodipine besylate, which can help relieve muscle spasms. 3. Neurotrophic drugs like vitamin B complex can help with nerve repair. 4. Traditional Chinese medicine treatment, such as blood-activating and stasisresolving Chinese medicine, can help improve blood circulation and alleviate pain. Pharmacological treatment for children should be conducted under the guidance of a doctor.) ", "page_idx": 23}, {"type": "text", "text": "PediatricsGPT\uff1a", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u5728\u513f\u7ae5\u4e2d\u8f83\u4e3a\u7f55\u89c1\uff0c\u4f46\u6cbb\u7597\u4ecd\u7136\u9700\u8981\u8c28\u614e\u3002\u5bf9\u4e8e\u513f\u7ae5\u7684\u8170\u690e\u95f4\u76d8\u7a81\u51fa\uff0c\u836f\u7269\u6cbb\u7597\u4e3b\u8981\u5305\u62ec\u4ee5\u4e0b\u51e0\u79cd\uff1a1.\u975e\u753e\u4f53\u6297\u708e\u836f\uff08NSAIDs\uff09\uff1a\u5982\u5e03\u6d1b\u82ac\uff08Ibuprofen\uff09\u6216\u5bf9\u4e59\u9170\u6c28\u57fa\u915a\uff08Acetaminophen\uff09\uff0c\u53ef\u7528\u4e8e\u7f13\u89e3\u75bc\u75db\u548c\u51cf\u8f7b\u708e\u75c7\u30022.\u808c\u8089\u677e\u5f1b\u5242\uff1a\u5bf9\u4e8e\u513f\u7ae5\uff0c\u5e38\u7528\u7684\u5305\u62ec\u786b\u9178\u6eb4\u5df1\u65b0\uff08Bromhexine\uff09\u7b49\uff0c\u53ef\u5e2e\u52a9\u7f13\u89e3\u808c\u8089\u75c9\u631b\u30023.\u795e\u7ecf\u8425\u517b\u836f\u7269\uff1a\u4f8b\u5982\u7ef4\u751f\u7d20B\u65cf\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u795e\u7ecf\u529f\u80fd\uff0c\u51cf\u8f7b\u75bc\u75db\u30024.\u4e2d\u836f\u6cbb\u7597\uff1a\u5728\u513f\u7ae5\u4e2d\u5e94\u614e\u91cd\u9009\u62e9\uff0c\u53ef\u8003\u8651\u4e00\u4e9b\u5177\u6709\u6d3b\u8840\u5316\u7600\u3001\u8212\u7b4b\u6d3b\u7edc\u4f5c\u7528\u7684\u4e2d\u836f\u3002\u9700\u8981\u5f3a\u8c03\u7684\u662f\uff0c\u513f\u7ae5\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u7684\u6cbb\u7597\u9700\u5728\u4e13\u4e1a\u533b\u751f\u6307\u5bfc\u4e0b\u8fdb\u884c\uff0c\u836f\u7269\u6cbb\u7597\u5e94\u614e\u91cd\u9009\u62e9\u5242\u91cf\u548c\u79cd\u7c7b\u3002\u6b64\u5916\uff0c\u513f\u7ae5\u5e94\u52a0\u5f3a\u953b\u70bc\uff0c\u4fdd\u6301\u826f\u597d\u7684\u59ff\u52bf\uff0c\u907f\u514d\u957f\u65f6\u95f4\u4f4e\u5934\u73a9\u624b\u673a\u6216\u7535\u8111\uff0c\u6709\u52a9\u4e8e\u9884\u9632\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u7684\u53d1\u751f\u3002", "page_idx": 23}, {"type": "text", "text": "(Lumbar disc herniation is rare in children, but treatment still requires caution. For pediatric lumbar disc herniation, pharmacological treatment mainly includes: 1. Nonsteroidal anti-inflammatory drugs (NSAIDs), such as ibuprofen or acetaminophen, can relieve pain and reduce inflammation. 2. Muscle relaxants: For children, commonly used ones include bromhexine, which can help relieve muscle spasms. 3. Neurotrophic drugs: For example, vitamin B complex can improve nerve function and alleviate pain. 4. Traditional Chinese medicine treatment: Selection should be cautious in children, and consideration may be given to some traditional Chinese medicines with blood-activating and stasis-resolving effects. It is important to emphasize that the treatment of lumbar disc herniation in children should be conducted under the guidance of a professional doctor, and pharmacological treatment should carefully consider dosage and type. Additionally, children should exercise, maintain good posture, and avoid prolonged periods of looking down at smartphones or computers, which can help prevent lumbar disc herniation.) ", "page_idx": 23}, {"type": "text", "text": "Figure 15: Comparison results of model responses on the treatment recommendation task. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The Abstract and Section 1 show our paper\u2019s contributions and scopes. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We fully discuss the limitations in Section 5. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper focuses on providing results based on metrics, GPT-4 and doctor evaluations. The related results are presented in Sections 4.3 through 4.6. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We disclose the implementation details of reproducing the results of the paper in Section 4.1 and Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the link to release the relevant available resources in the Abstract. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide detailed hyper-parameter configurations in Appendix C. Also, Section 4.1 provides dataset splits and other implementation details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: Performing multiple repetitive experiments in order to compute error bars is labor-intensive and has significant overhead for the large language model development. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the computational resources needed in order to reproduce the experiments in Section 4.1. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our research follows the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We fully discuss the potential social impacts in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: To ensure the safe release of data, we describe measures for dataset construction in detail in Section 3.1. To ensure that the responses of the proposed model are harmless and safe, we provide adversarial instructions to control the model behaviour in Section 3.3. Meanwhile, we perform human preference optimization for the model in Section 3.4, which further strengthens the safety and robustness of the model. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide reasonable references for the datasets and models used in Sections 4.1 and 4.2, respectively. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the relevant documentation in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We offer $\\mathbb{S}300$ each to participating experts. Expert doctors are asked to evaluate the quality of the models\u2019 responses. The relevant descriptions can be found in Section 4.3. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The research in the paper has no risks to be faced by participants who are only used as evaluators. Moreover, we underwent an internal ethical review by the ethical review board of the partnering medical institutions with license and approval. The relevant descriptions can be found in Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]