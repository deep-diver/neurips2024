[{"figure_path": "dCgbyvmlwL/tables/tables_1_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in the dividing and conquering policies used (neural vs. heuristic), whether the impact of sub-optimal dividing policies was considered during training, and whether a unified or separate training scheme was employed. The table demonstrates that UDC is unique in its use of learning-based policies for both stages and its unified training approach, which addresses the negative impact of sub-optimal dividing policies.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_4_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods. It highlights key differences in dividing and conquering policies, how these methods handle the negative impact of suboptimal dividing policies, and whether they utilize unified or separate training schemes. The table demonstrates that UDC is unique in its use of learning-based policies in both stages and its unified training scheme which addresses the negative impacts of sub-optimal dividing policies, resulting in a superior solution generation approach.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_6_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table compares the performance of various methods on solving TSP and CVRP problems of different sizes (500, 1000, and 2000 nodes).  It shows the objective function value (Obj.), the gap between the method's result and the best known result (Gap), and the time taken to solve each instance (Time).  The best overall performance for each problem size is highlighted in bold, and the best performing learning-based method is shaded.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_7_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of the objective function values, gaps to the best-performing algorithm, and solution times for various methods on TSP and CVRP problems with 500, 1000, and 2000 nodes.  The best overall performance for each problem size is highlighted in bold, and the best-performing learning-based method is shaded.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_8_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in their dividing and conquering policies, whether they consider the impact of sub-optimal dividing policies, and if they utilize a unified or separate training scheme. The table demonstrates that UDC is unique in its unified training approach and its consideration of sub-optimal dividing policies' effects.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_8_2.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in the dividing and conquering policies used (whether neural or heuristic), how these methods handle the negative impact of sub-optimal dividing policies, and whether they use a unified or separate training scheme for the two policies.  The table demonstrates that UDC is unique in its use of learning-based policies in both stages and its unified training approach.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_23_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in dividing and conquering policies (neural vs heuristic), whether the methods consider the impact of sub-optimal dividing policies, and whether the training of the two policies is unified or separate.  The table shows that UDC is unique in its use of learning-based policies for both stages and its unified training scheme that addresses the negative effects of suboptimal dividing.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_24_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in the dividing and conquering policies used (neural network-based or heuristic), whether the methods account for the negative impact of suboptimal dividing policies, and whether the training process is unified or separate.  The table shows that UDC uniquely utilizes learning-based policies in both stages and employs a unified training scheme, addressing limitations of prior methods.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_25_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of different methods for solving Traveling Salesperson Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) instances of varying sizes (500, 1000, and 2000 nodes).  The table shows the objective function value (Obj.), the gap between the obtained solution and the best-known solution (Gap), and the time taken to find the solution (Time). The best-performing method overall and the best-performing learning-based method for each problem size are highlighted.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_25_2.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of the objective function values, gaps to the best-performing algorithm, and solving times for various methods on TSP and CVRP problems with 500, 1000, and 2000 nodes.  The best overall performance for each problem size is highlighted in bold, and the best-performing learning-based method is shaded. The table provides a quantitative assessment of the different methods' performance on large-scale combinatorial optimization problems.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_26_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in their dividing and conquering policies (whether neural or heuristic-based), how they handle the impact of sub-optimal dividing policies (ignored or considered), and whether they use a unified or separate training scheme for the two policies. The table shows that UDC uniquely utilizes learning-based policies in both stages and employs a unified training scheme, addressing limitations of previous methods.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_26_2.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of the objective function values, gaps to the best-performing algorithm, and solving times for different methods on TSP and CVRP problems with varying numbers of nodes (500, 1000, and 2000).  The best overall performance for each metric is highlighted in bold, and the best-performing learning-based method is shaded.  The number of instances used in the testing is also provided.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_27_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of different methods for solving TSP and CVRP problems with varying instance sizes (500, 1000, and 2000 nodes).  It shows the objective function value, the gap to the best-performing algorithm, and the solution time for each method.  The best overall performance and the best learning-based methods are highlighted.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_27_2.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of different methods for solving TSP and CVRP problems of varying sizes (500, 1000, and 2000 nodes).  The metrics used for comparison include the objective function value (Obj.), the gap between the method's solution and the best-known solution (Gap), and the time taken to find the solution (Time).  The best overall performance and the best among learning-based methods are highlighted.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_28_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of the objective function values, gaps to the best-performing algorithm, and solving times for various neural combinatorial optimization methods on TSP and CVRP problems with 500, 1000, and 2000 nodes.  The best overall performance for each metric is highlighted in bold, and the best-performing learning-based method is shaded. The table provides a quantitative assessment of the performance of different algorithms on increasingly larger problem sizes.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_29_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of the objective function values, gaps to the best-performing algorithm, and solution times for different methods on TSP and CVRP problems with varying numbers of nodes (500, 1000, and 2000).  The best overall performance for each metric is highlighted in bold, and the best-performing learning-based method is shaded.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_29_2.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of the objective function values, gaps to the best-performing algorithm, and solving times for different methods on TSP and CVRP problems with 500, 1000, and 2000 nodes.  The best overall performance for each problem size is highlighted in bold, and the best-performing learning-based method is shaded.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_29_3.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of different methods for solving TSP and CVRP problems with varying sizes (500, 1000, and 2000 nodes).  It shows the objective function values, the gap between each method's solution and the optimal solution, and the time taken to find the solution.  The best performing method for each problem size is highlighted in bold, and the best-performing learning-based method is shaded.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_31_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of different methods for solving TSP and CVRP problems with varying instance sizes (500, 1000, and 2000 nodes).  The results are shown in terms of objective function value, the gap to the best-performing algorithm, and the time taken to solve the problems.  The best overall performance and the best learning-based method for each problem and size are highlighted.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_32_1.jpg", "caption": "Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node, 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in [13]). The overall best performance is in bold and the best learning-based method is marked by shade.", "description": "This table presents a comparison of the objective function values, gaps to the best algorithm, and solving times for different methods on TSP and CVRP problems with 500, 1000, and 2000 nodes.  The best performing method for each problem size is highlighted in bold, and the best-performing learning-based method is shaded.", "section": "4 Experiment"}, {"figure_path": "dCgbyvmlwL/tables/tables_33_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods in terms of dividing and conquering policies, the consideration of the negative impact of suboptimal dividing policies, and the training schemes used for the two policies.  It highlights that UDC is unique in using learning-based policies for both stages and employing a unified training scheme to address the suboptimality issue.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_33_2.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in the dividing and conquering policies used (neural or heuristic-based), whether the impact of sub-optimal dividing policies was considered during training, and if the training was unified or separate for both policies.  UDC is shown to be unique in its use of learning-based policies in both stages and its unified training scheme.", "section": "1 Introduction"}, {"figure_path": "dCgbyvmlwL/tables/tables_37_1.jpg", "caption": "Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.", "description": "This table compares the proposed UDC framework with other existing neural divide-and-conquer methods.  It highlights key differences in dividing and conquering policies (neural or heuristic-based), how sub-optimal dividing policies are handled (considered or ignored), and whether the training of these policies is unified or separate.  The table shows that UDC is unique in its unified training approach and use of learning-based policies in both stages.", "section": "1 Introduction"}]