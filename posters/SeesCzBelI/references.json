{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the RLHF technique, introducing a method for training LLMs to better align with human instructions and preferences."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "This paper introduces a novel approach to RLHF, suggesting that reward model training is not needed, directly optimizing preferences."}, {"fullname_first_author": "Leo Gao", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023-07-01", "reason": "This paper investigates the scaling properties of reward models and identifies overoptimization as a key challenge in RLHF."}, {"fullname_first_author": "Lichang Chen", "paper_title": "ODIN: disentangled reward mitigates hacking in RLHF", "publication_date": "2024-02-01", "reason": "This paper proposes a method to mitigate reward hacking by disentangling length bias from the actual reward signal, improving the quality of RLHF."}, {"fullname_first_author": "Wei Shen", "paper_title": "Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback", "publication_date": "2023-12-06", "reason": "This paper focuses on mitigating the length bias problem in RLHF, proposing a method to reduce the correlation between length and actual reward."}]}