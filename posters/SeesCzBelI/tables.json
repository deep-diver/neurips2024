[{"figure_path": "SeesCzBelI/tables/tables_6_1.jpg", "caption": "Table 1: Preference order predicted by RMs trained with various methods, where the user prompt is concatenated with the responses in various formats generated by GPT-4.", "description": "This table presents the ranking of responses generated by GPT-4 for various prompts, as determined by reward models (RMs) trained using different methods.  The prompts involve creating text in specific styles (tech article, advertisement, insight, record article, poetry). The table shows how different RM training methods (vanilla RM, RM with Prompt Bias Calibration (PBC), and RM with Length and Prompt Bias Calibration (LPBC)) affect the ranking of responses. Lower scores indicate better ranking.  This demonstrates the impact of different RM training approaches on the quality and style of generated text.", "section": "4.1 Experimental Settings"}, {"figure_path": "SeesCzBelI/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparison of LLMs aligned with RMs trained with various methods.", "description": "This table presents a quantitative comparison of the performance of Large Language Models (LLMs) that have been fine-tuned using different Reward Models (RMs).  The RMs were trained using various methods, including the original RLHF approach and several novel methods proposed in the paper (ODIN, PBC, ODIN+PBC, and LPBC).  The LLM performance is evaluated across four benchmark datasets: MMLU (Massive Multitask Language Understanding), DROP (Discourse Representation in Paragraphs), BBH (BIG-Bench Hard), and TQA (TruthfulQA). The table allows for a direct comparison of the effectiveness of different RM training techniques in improving LLM performance on various downstream tasks.", "section": "4.2 Experimental Results"}, {"figure_path": "SeesCzBelI/tables/tables_11_1.jpg", "caption": "Table 2: Performance comparison of LLMs aligned with RMs trained with various methods.", "description": "This table presents a quantitative comparison of Large Language Models (LLMs) that have been fine-tuned using reward models (RMs) trained with different methods.  It shows the performance of these LLMs on several benchmark datasets (MMLU, DROP, BBH, TQA), comparing the original Llama-2-7b model against versions fine-tuned with RMs trained using different bias removal techniques (length bias and prompt template bias). This allows for an assessment of how effectively different methods improve LLM performance.", "section": "4.2 Experimental Results"}, {"figure_path": "SeesCzBelI/tables/tables_12_1.jpg", "caption": "Table 2: Performance comparison of LLMs aligned with RMs trained with various methods.", "description": "This table presents a quantitative comparison of the performance of Large Language Models (LLMs) that have been fine-tuned using reward models (RMs) trained with different methods.  The methods compared include the original RLHF implementation,  the ODIN method (which addresses length bias), the PBC method (which addresses prompt-template bias), ODIN combined with PBC, and finally the LPBC method (which combines both length and prompt bias calibration).  The LLM performance is evaluated across four benchmarks: MMLU, DROP, BBH, and TQA, providing a comprehensive assessment of the impact of different RM training techniques on the overall quality and capabilities of the resulting LLMs.", "section": "4.2 Experimental Results"}, {"figure_path": "SeesCzBelI/tables/tables_12_2.jpg", "caption": "Table 1: Preference order predicted by RMs trained with various methods, where the user prompt is concatenated with the responses in various formats generated by GPT-4.", "description": "This table presents a qualitative comparison of different reward models (RMs) in ranking responses generated by GPT-4 for various prompts. The RMs were trained using different methods, including the original preference loss, the Prompt Bias Calibration (PBC) method, and the Length and Prompt Bias Calibration (LPBC) method.  Each row represents a prompt and its associated set of responses, which were ranked by each RM. The ranks show the preference orders according to the different RMs, allowing comparison of performance and highlighting the impact of each method on mitigating reward hacking and bias.", "section": "4.1 Experimental Settings"}, {"figure_path": "SeesCzBelI/tables/tables_13_1.jpg", "caption": "Table 1: Preference order predicted by RMs trained with various methods, where the user prompt is concatenated with the responses in various formats generated by GPT-4.", "description": "This table showcases the ranking of responses generated by GPT-4 in various formats (Tech Article, Advertisement, Insight, Record Article, Poetry) for a given prompt, as evaluated by reward models (RMs) trained with different methods: Vanilla RM, RM (PBC), and RM (LPBC).  The rank shows the order of preference as determined by each RM, with lower scores indicating higher preference. This highlights the different biases exhibited by different reward model training methods and the effect on response preferences.", "section": "4.1 Experimental Settings"}]