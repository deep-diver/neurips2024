[{"type": "text", "text": "FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lisha Chen Rensselaer Polytechnic Institute Troy, NY, United States chenl21@rpi.edu ", "page_idx": 0}, {"type": "text", "text": "AFM Saif Rensselaer Polytechnic Institute Troy, NY, United States saifa@rpi.edu ", "page_idx": 0}, {"type": "text", "text": "Yanning Shen University of California, Irvine Irvine, CA, United States yannings@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Tianyi Chen Rensselaer Polytechnic Institute Troy, NY, United States chentianyi19@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Finding specific preference-guided Pareto solutions that represent different tradeoffs among multiple objectives is critical yet challenging in multi-objective problems. Existing methods are restrictive in preference definitions and/or their theoretical guarantees. In this work, we introduce a Flexible framEwork for pREfeRenceguided multi-Objective learning (FERERO) by casting it as a constrained vector optimization problem. Specifically, two types of preferences are incorporated into this formulation \u2013 the relative preference defined by the partial ordering induced by a polyhedral cone, and the absolute preference defined by constraints that are linear functions of the objectives. To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants. Notably, this is the first single-loop primal algorithm for constrained vector optimization to our knowledge. The proposed algorithms adaptively adjust to both constraint and objective values, eliminating the need to solve different subproblems at different stages of constraint satisfaction. Experiments on multiple benchmarks demonstrate the proposed method is very competitive in finding preference-guided optimal solutions. Code is available at https://github.com/lisha-chen/FERERO/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many machine learning tasks inherently involve multiple objectives, which can be different performance metrics such as accuracy, fairness, and privacy; or, the same metrics defined on different data [40, 31]. To tackle such multi-objective problems, it is common to learn a shared model that simultaneously performs well on all the objectives. Compared to learning one model for each objective, learning a shared model has the benefit of reducing both the model size and the inference time. This can be achieved through multi-objective optimization [40, 45, 25, 6], which is to learn a model that minimizes the vector-valued objective. In practical applications, it is of interest to learn solutions with controlled trade-offs or preferences. To further illustrate, we give two examples below. ", "page_idx": 0}, {"type": "text", "text": "In fairness-aware machine learning, a trade-off exists between the fairness $f_{\\mathrm{fair}}(\\theta)$ and accuracy $f_{\\mathrm{acc}}(\\theta)$ , see also Figure 1a. With $\\theta$ denoting the model parameter, and $C$ denoting the partial order ", "page_idx": 0}, {"type": "text", "text": "Table 1: Comparison to existing methods. \u201cFlexibility\u201d represents preference modeling, such as by using weights, preference vectors (rays), or constraints. \u201cExactness\u201d represents the ability to align with a preference vector exactly. \u201cDeter.\u201d, \u201cStoch.\u201d represent deterministic and stochastic, respectively. \u201c\u2717\u201d means not provided in the corresponding work, and \u201c-\u201d means not relevant. ", "page_idx": 1}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/abca4570e2b73cf87e60731ed62d5b11d61b7fe8d531ad4720877c42570feffc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "cone, to find the optimal models that consider different trade-offs, one can solve the following problem with different thresholds $\\epsilon$ [8] ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathrm{maximize~}}\\left(f_{\\mathrm{acc}}(\\theta),f_{\\mathrm{fair}}(\\theta)\\right)^{\\top}{\\mathrm{~s.t.~}}\\;f_{\\mathrm{fair}}(\\theta)\\geq\\epsilon.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Another example is in drug or molecule design, where the goal is to design drugs or molecules with multiple desired properties $\\mathbf{\\bar{\\alpha}}_{f_{1}\\left(\\theta\\right),\\;f_{2}\\left(\\theta\\right),\\;.\\;.\\;.\\;}\\mathbf{\\bar{\\alpha}}_{f_{M}\\left(\\theta\\right)}$ . Aiming to align the values of the properties $F(\\theta)$ with a predefined preference vector $v$ as in Figure 1b, one can solve the following problem [29, 1, 46] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\textstyle\\operatorname{maximize}_{C}\\ F(\\theta):=\\left(f_{1}(\\theta),\\ldots,f_{M}(\\theta)\\right)^{\\top}{\\mathrm{~s.t.~}}B F(\\theta)=B v,\\ B v=0\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $B\\in\\mathbb{R}^{(M-1)\\times M}$ is full row rank. ", "page_idx": 1}, {"type": "text", "text": "Then a natural question arises: ", "page_idx": 1}, {"type": "text", "text": "Can we develop a principled framework to capture flexible preferences and admit provably convergent deterministic and stochastic algorithms? ", "page_idx": 1}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/b6bf63f1d69efe5a291144968b0baf486a6ca6ac55b0689eb8d1a5807db595d5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of preferences in different exOur answer to this question is affirmative. Rec- amples. The solid red curves represent the Pareto ognizing that all the aforementioned applica- front, dashed lines represent preference constraints. tions can be addressed within a unified frame", "page_idx": 1}, {"type": "text", "text": "work, we formulate preference-guided multi-objective learning (PMOL) as a constrained vector optimization problem. Specifically, given a model $\\theta\\in\\mathbb{R}^{q}$ , and the objectives $f_{m}:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ , $m=$ $1,\\dots,M$ , we define the constrained vector optimization problem as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb R^{q}}F(\\theta):=\\big(f_{1}(\\theta),\\ldots,f_{M}(\\theta)\\big)^{\\top},\\ \\ \\mathrm{s.t.}\\ \\ G(\\theta)\\leq0,\\,H(\\theta)=0\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $G(\\theta)$ and $H(\\theta)$ are the vector-valued preference constraints such as the examples in (1.1) and (1.2). Here $\"{\\le}\"$ and $\\bullet\\bullet=\\,^{\\bullet}$ are element-wise relations on the vectors, with each row representing one constraint. In these examples, the preferences are directly defined in the objective space, as intersections of half-spaces defined by the hyperplanes; see Figure 1. Thus, $G(\\cdot)$ and $H(\\cdot)$ in (PMOL) can be expressed as linear functions of $F(\\theta)$ , given by ", "page_idx": 1}, {"type": "equation", "text": "$$\nG(\\theta)=B_{g}F(\\theta)+b_{g},\\;\\;H(\\theta)=B_{h}F(\\theta)+b_{h}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $B_{g}\\,\\in\\,\\mathbb{R}^{M_{g}\\times M},B_{h}\\,\\in\\,\\mathbb{R}^{M_{h}\\times M}$ , and $b_{g}\\,\\in\\,\\mathbb{R}^{M_{g}},b_{h}\\,\\in\\,\\mathbb{R}^{M_{h}}$ . Different $B_{g},B_{h},b_{g},b_{h}$ correspond to different preferences, and thus different trade-offs among the objectives. ", "page_idx": 1}, {"type": "text", "text": "A comparison of our methods to existing methods is summarized in Table 1. Specifically, our contributions are listed as follows: ", "page_idx": 1}, {"type": "text", "text": "C1) We cast the PMOL problem as a constrained vector optimization problem, and develop the FERERO framework to capture flexible preferences. ", "page_idx": 1}, {"type": "text", "text": "C2) We develop a method with an adaptive subprogram that efficiently finds update directions to meet flexible preferences, eliminating the need for multiple subprograms under different active constraints. This approach ensures iterative improvement in a general partial ordering while allowing controlled ascent of objectives to satisfy preferences. ", "page_idx": 1}, {"type": "text", "text": "C3) We propose a practical single-loop algorithm and establish non-asymptotic convergence guarantees for deterministic and stochastic variants of the proposed algorithm. To our best knowledge, this is the first single-loop primal algorithm in constrained vector optimization with convergence guarantees. ", "page_idx": 2}, {"type": "text", "text": "C4) We apply the proposed algorithms to various synthetic and real-world image and speech datasets to demonstrate its ability to find flexible preference-guided optimal models. ", "page_idx": 2}, {"type": "text", "text": "In our theoretical analysis, we address the following technical challenges. ", "page_idx": 2}, {"type": "text", "text": "T1) The commonly used constraint qualification assumptions do not generally hold for the PMOL problem. We overcome this challenge by leveraging the specific structure that the constraints are linear functions of $F$ to prove the calmness condition holds for PMOL. See more details in Lemma 2.   \nT2) The commonly used merit or Lyapunov functions for constrained optimization are usually non-smooth, making it difficult to derive a descent lemma on the functions, and thus difficult to derive the non-asymptotic convergence guarantees. We overcome this challenge by exploiting the optimality conditions and proper step size choices. See Lemma 8.   \nT3) The convergence of our single-loop algorithm is slower with the commonly-used merit functions. We provide a sharper analysis by introducing a different merit function and exploiting the algorithm properties; see Theorem 3. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup and A Meta Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To characterize the optimality conditions of PMOL, we introduce the generalized notion of dominance and the related concept of optimality. We then present a meta-algorithm to solve PMOL. ", "page_idx": 2}, {"type": "text", "text": "2.1 Problem setup and preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce optimality definitions for PMOL that go beyond the standard definitions of Pareto optimality [12, 10, 26]. Given two vectors $v$ and $w$ , we use $v<w$ and $v\\leq w$ to denote $v_{i}<w_{i}$ for all $i$ , and $v_{i}\\leq w_{i}$ for all $i$ , respectively. We use $v\\leq w$ to denote $v\\leq w$ and $v\\neq w$ , and define $>,\\geq,\\geq$ analogously. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 $C_{A}$ -dominance [11, 19]). Given $v,w\\in\\mathbb{R}^{M}$ , $\\dot{A}\\overset{\\cdot}{\\in}\\mathbb{R}^{M\\times M}$ , and $C_{A}:=\\{y\\in$ ob $\\mathbb{R}^{M}\\mid A y\\geq0\\}\\neq\\emptyset,$ , we say $v$ strictly domi- d nates $w$ based on $C_{A}$ if and only if $A(v-w)<0$ ", "page_idx": 2}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/6a2227af1489e77540ae60b70f9f44f18a8741fba7c1c0e4609f6fba9660532d.jpg", "img_caption": ["Figure 2: Illustration of $C_{A}$ -dominance. The solid red curves are the Pareto fronts, green dots are the reference points, gray shaded regions are the set of jectives dominating the reference points, under ifferent $C_{A}$ in (a) and (b). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "The generalized dominance defines a partial order on $\\mathbb{R}^{M}$ , i.e., the relation between two vectors. Illustrations of different partial orders are given in Figure 2. Figure 2a shows the dominance relation under the widely used non-negative orthant cone with $\\mathbf{\\tilde{\\mathit{C}}}_{A}=\\mathbb{R}_{+}^{\\overline{{M}}}$ , corresponding to Pareto optimality. However, as illustrated by the figure, given the initial green reference point, a descent method such as MGDA [12] cannot find points on the Pareto front but outside of the gray shaded region. This poses a critical challenge for applications where specific preference-guided solutions on the Pareto front are needed. Nevertheless, this issue can be addressed by substituting $\\mathbb{R}_{+}^{M}$ with a more general definition of $C_{A}$ as displayed in Figure 2b. Under this partial order, a general descent method is able to reach any points on the Pareto front starting from the green reference point. ", "page_idx": 2}, {"type": "text", "text": "Based on the partial order, one can then find the minimum or optimal elements in the vector-valued objective space, whose formal definition is provided below. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 ( $C_{A}$ -optimal). A point $\\theta\\in\\mathbb{R}^{q}$ is $C_{A}$ -optimal if there is no $\\theta^{\\prime}\\neq\\theta$ such that, $A F(\\theta^{\\prime})\\leq$ $A F(\\theta)$ . A point $\\theta$ is weakly $C_{A}$ -optimal if there is no $\\theta^{\\prime}\\neq\\theta$ such that, $A F(\\theta^{\\prime})<A F(\\theta)$ . ", "page_idx": 2}, {"type": "text", "text": "Note that, $C_{A}$ is a polyhedral cone, or the intersection of half-spaces defined by the rows of the inequality $A y\\ge0$ . When $A=I_{M}$ , an $M\\times M$ identity matrix, $\\stackrel{{\\cal{C}}_{A}}{\\cal{C}}_{A}=\\mathbb{R}_{+}^{M}:=\\dot{\\left\\{y\\in\\mathbb{R}^{M}\\mid y_{m}\\geq\\right.}$ $0\\ \\forall m\\in[M]\\}$ , then Definition 1 reduces to the commonly used notion of dominance associated with Pareto optimality. The cone $C_{A}$ can be interpreted as a relative preference that defines the objectives\u2019 improvement directions, which generalizes the relative preference defined by $\\mathbb{R}_{+}^{M}$ . In contrast, the preference defined by constraints in (1.3) can be interpreted as an absolute preference that defines the feasible or preferred set of objective function values. In practice, $C_{A}$ can be chosen based on the requirements of specific applications. For example, when the controlled ascent of objectives is needed [30], we can choose $C_{A}$ such that the controlled ascent direction belongs to $-C_{A}$ . We defer the detailed implementation to Section 3.2. The $C_{A}$ -optimal set, denoted as $\\mathcal{P}_{A}$ , contains all the $C_{A}$ -optimal models. When $A=I_{M}$ , $\\mathcal{P}_{A}$ is the Pareto optimal set $\\mathcal{P}$ . The Pareto front is the set of function values evaluated at Pareto optimal models, i.e., $\\mathcal{F}=\\{F(\\theta)~|~\\theta\\in\\mathcal{P}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We make the following standard assumptions throughout the paper [12, 17, 6]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. 1. (Non-negative objectives) $A F(\\theta)\\geq0,$ , and $\\mathbf{1}^{\\top}A F(\\theta)\\geq c_{A F}>0.$ for all $\\theta\\in\\mathbb{R}^{q}$ .   \n2. (Differentiable objectives) $F$ is twice continuously differentiable.   \n3. (Ordering cone with non-empty interior) $C_{A}$ has a non-empty interior. ", "page_idx": 3}, {"type": "text", "text": "2.2 Find the preference-guided direction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we proceed to discuss an adaptive method to solve (PMOL). At iteration $t$ , the algorithm finds an update direction $d_{t}$ and performs the iterative update $\\theta_{t+1}=\\theta_{t}+\\alpha_{t}d_{t}$ with a step size $\\alpha_{t}$ . Ideally, the update direction $d_{t}$ is chosen to improve the objective $F(\\theta)$ and to satisfy the preference constraints. It is desirable that when the constraints are not satisfied, $d_{t}$ decreases the violation of constraints and improves the objectives in the general partial ordering sense; when the constraints are satisfied, $d_{t}$ improves the objectives and ensures the constraints are satisfied. To achieve this, we find a direction $d^{*}(\\theta)$ that solves following subprogram ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi(\\theta):=\\underset{(d,c)\\in\\mathbb{R}^{q}\\times\\mathbb{R}}{\\operatorname*{min}}c+\\frac{1}{2}\\|d\\|^{2}\\ \\mathrm{~s.t.~}A\\nabla F(\\theta)^{\\top}d\\leq\\frac{c}{\\mathbf{1}^{\\top}A F(\\theta)}A F(\\theta)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\nabla G(\\theta)^{\\top}d+c_{g}G(\\theta)\\leq0,\\ \\nabla H(\\theta)^{\\top}d+c_{h}H(\\theta)=}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert$ denotes the $\\ell_{2}$ -norm, $c_{g}$ and $c_{h}$ are pre-defined positive constants. Larger $c_{g}$ and $c_{h}$ put more emphasis on constraint satisfaction than objective improvement. We call this subprogram adaptive since it deals with constraints in an adaptive way, which does not require the initial model to be feasible, nor $\\theta_{t}$ to be feasible at each iteration. But rather, it finds an update direction that decreases the constraint violation. Because of this, it neither requires solving different subprograms at different stages nor requires different treatment of the active set of inequalities as in existing works [24, 30, 33]. ", "page_idx": 3}, {"type": "text", "text": "We then show in Lemma 1 that the desired properties can be satisfied. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. For the subprogram (2.1), the following holds:   \nIf $\\theta$ is $a$ local optimal solution with $A F(\\theta)\\,>\\,0,$ , then $d^{*}(\\theta)\\,=\\,0,$ , $\\psi(\\theta)\\,=\\,0$ . Otherwise, if $\\theta$ is not a local optimal solution, then $d^{*}(\\theta)\\neq0,$ , $\\psi(\\theta)<0$ , and when $\\theta$ is feasible,   \n$2\\psi(\\theta)\\leq-\\|d^{*}(\\theta)\\|^{2}<0.$ (2.2) ", "page_idx": 3}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/cf8edce2413e3b5df683d4b2da87e6b5a6b5f38d7aaab77a06507224e580fa34.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Let $\\theta$ be a weak $C_{A}$ -optimal solution, with $(A F(\\theta))_{m}=0$ for some $m\\in[M]$ . If there exists feasible and non-strictly improving directions at $\\theta$ with $A\\nabla F(\\theta)^{\\top}d\\leq0,$ , then $d^{*}(\\theta)\\neq0,$ , $\\psi(\\theta)<0$ . Otherwise, $d^{*}(\\theta)=0,$ , $\\psi(\\theta)=0$ . ", "page_idx": 3}, {"type": "text", "text": "By Lemma 1, $\\|d^{*}(\\theta)\\|\\,=\\,0$ is a stationary condition for PMOL. Recall the feasibility condition requires $[G(\\theta)]_{+}^{\\dot{-}}={\\dot{0}}$ and $|H(\\theta)|_{\\mathrm{ab}}=0$ , where $[\\cdot]_{+}$ and $|\\cdot|_{\\mathrm{ab}}$ are entry-wise ReLU and absolute functions, respectively. And the complementary slackness condition requires $\\lambda_{g}^{*\\,\\top}[-G(\\theta)]_{+}=0$ Thus $\\|d^{*}(\\theta)\\|^{2}+\\lambda_{g}^{*}{}^{\\top}[-G(\\theta)]_{+}+\\|[G(\\theta)]_{+}\\|_{1}+\\|H(\\theta)\\|_{1}$ achieves zero if and only if the model $\\theta$ satisfies the first-order KKT condition. Besides the properties in Lemma 1, it has an additional scale-invariant property that is deferred to Lemma 4 due to space limit. ", "page_idx": 3}, {"type": "text", "text": "By the Lagrangian of (2.1), the optimal update direction can be expressed in a simple form as a weighted combination of the gradients, i.e. $\\dot{d}^{*}(\\theta)=-\\nabla F(\\theta)A_{a g}^{\\top}\\lambda^{*}$ , with $A_{a g}:=[A;B_{g};B_{h}]$ , and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda^{*}\\in\\underset{\\lambda\\in\\Omega_{\\lambda}(\\theta)}{\\mathrm{arg}\\,\\mathrm{min}}\\ \\varphi(\\lambda;\\theta):=\\frac{1}{2}\\|\\nabla F(\\theta)A_{a g}^{\\top}\\lambda\\|^{2}-c_{g}\\lambda_{g}^{\\top}G(\\theta)-c_{h}\\lambda_{h}^{\\top}H(\\theta)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda=[\\lambda_{f};\\lambda_{g};\\lambda_{h}],\\Omega_{\\lambda}(\\theta)$ is the domain of the Lagrangian multipliers, given by 1 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Omega_{\\lambda}(\\theta):=\\Omega_{\\lambda_{f}}(\\theta)\\times\\mathbb{R}_{+}^{M_{g}}\\times\\mathbb{R}^{M_{h}},\\;\\;\\mathrm{with}\\;\\;\\Omega_{\\lambda_{f}}(\\theta):=\\{\\lambda\\in\\mathbb{R}_{+}^{M}\\;|\\;\\lambda^{\\top}A F(\\theta)=\\mathbf{1}_{M}^{\\top}A F(\\theta)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our goal is to design an algorithm that converges to a KKT solution based on (2.1). However, the KKT condition is not necessary unless certain constraint qualifications (CQs) hold. Prior works [14, 24] assume certain CQs hold, e.g., the Linear Independence Constraint Qualification (LICQ). However, the LICQ assumption (c.f., [14, Section 3.1, (A2)]) does not generally hold at a local optimal solution for problem (PMOL), c.f., Example 1 in Appendix D.3.2. Though some commonly used CQs do not hold generally, in our case, leveraging the specific structure that the constraints are linear functions of $F$ , we can justify the calmness CQ in Definition 9 tailored for our problem in Lemma 2, thus the KKT condition is a necessary optimality condition. The proof is deferred to Appendix D.3.2. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2. Let $\\bar{\\theta}\\in\\mathbb{R}^{q}$ be a global solution to (PMOL). Define $\\Sigma(p,q):=\\{y\\in\\mathbb{R}^{M}\\mid B_{g}y+b_{g}\\leq$ $p,B_{h}y+b_{h}=q\\}$ . If $\\Sigma(p,q)$ is a line, the PMOL calmness condition in Definition $^{9}$ is satisfied for (PMOL) at $\\bar{\\theta}$ if $A\\in\\mathbb{R}^{M\\times M}$ is full rank, $H(\\theta),G(\\theta)$ defined by (1.3) satisfy $[B_{h}^{\\top},B_{g}^{\\top}]\\neq0$ , and $B_{h},B_{g}$ are full row rank. Consequently, the KKT condition is a necessary optimality condition. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 provides a sufficient condition for the KKT condition to be a necessary optimality condition without relying on unjustified assumptions. The requirement that the constraint set is a line in the objective space is common for applications such as alignment to a preference vector. ", "page_idx": 4}, {"type": "text", "text": "We then discuss a generic preference-guided multi-objective algorithm based on the subprogram. ", "page_idx": 4}, {"type": "text", "text": "2.3 A meta algorithm for preference-guided multi-objective learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the model $\\theta_{t}$ at iteration $t$ , one can then solve (2.3) to obtain $\\lambda_{t}$ . The direction $d_{t}\\ =$ $-\\nabla F(\\theta)A_{a g}^{\\top}\\lambda_{t}$ is used to update the model $\\theta_{t}$ by $\\theta_{t+1}=\\theta_{t}+\\alpha_{t}d_{t}$ iteratively until convergence. The full procedure of this meta algorithm is summarized in Algorithm 1, where Step 4 is a generic step and can be customized in Section 3. ", "page_idx": 4}, {"type": "text", "text": "To establish the non-asymptotic convergence rate, we use the following standard smoothness assumption that has been commonly used in prior works for multi-objective learning [6, 26]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Smooth objectives). For all $m\\in[M]$ , $\\nabla f_{m}(\\theta)$ is $\\ell_{f,1}$ -Lipschitz continuous.   \nWe then state the convergence result for Algorithm 1 in Theorem 1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Convergence of the generic FERERO algorithm). Suppose Assumptions 1, 2 hold. Let $\\{\\theta_{t}\\}$ be the sequences produced by Algorithm $^{\\,l}$ , with $d_{t}$ being an $\\epsilon$ -optimal solution to the subprogram (2.1). If $\\|\\lambda^{*}(\\theta_{t})\\|_{1}\\leq c,$ , $\\begin{array}{r}{\\alpha_{t}\\leq\\operatorname*{min}\\{\\frac{1}{c\\ell_{f,1}\\|A_{a g}^{\\top}\\|_{\\infty,1}},c_{g}^{-1},c_{h}^{-1}\\}}\\end{array}$ , and $\\alpha_{t}=\\Theta(1)$ , then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\underbrace{\\|\\nabla F(\\theta_{t})A_{\\alpha g}^{\\top}\\lambda^{*}(\\theta_{t})\\|^{2}}_{s t a t i o n a r a l y}+\\underbrace{\\lambda_{g}^{*}(\\theta_{t})^{\\top}[-G(\\theta_{t})]}_{c o m p l e m e n t a r y s l a c k m e s}+\\underbrace{\\|[G(\\theta_{t})]_{+}\\|_{1}+\\|H(\\theta_{t})\\|_{1}}_{f e a s i b l i t y}=\\mathcal{O}\\big(T^{-1}+\\epsilon\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 1 guarantees the non-asymptotic convergence for the generic FERERO algorithm. In Algorithm 1, $\\lambda_{t}$ can be solved through projected gradient descent or Frank Wolfe algorithm iteratively within an inner loop. In practice, we usually do not need to solve the subprogram exactly. Next, we discuss the efficient single-loop approximate algorithm based on Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "3 Efficient Algorithm Development Algorithm 2 FERERO-SA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we first discuss efficient algorithm development with the approximate update rules and practical choice of preferences. We focus on (PMOL) with equality constraints only, i.e., $M_{g}=0$ . Building upon this, we then discuss the stochastic variants of the algorithms that can be applied to large-scale learning problems. ", "page_idx": 4}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/1b91f1e2ce6c57b6fe23ca37cffbd3c31163cab4d35a35ecbf09886a6e01878c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.1 Single-loop approximate algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In practice, if one only requires the converging solutions generated by the algorithm to be feasible, but not all the iterates, then further approximations can be made to the subprogram (2.3). At iteration $t$ , to obtain an approximate direction $d_{t}$ , we adopt the following update ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{t+1}=\\!\\Pi_{\\Omega_{\\lambda}(\\theta_{t})}\\big(\\lambda_{t}-\\gamma_{t}\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The single-loop algorithm with the approximate solution is summarized in Algorithm 2. We name it FERERO with Single-loop Approximate update (FERERO-SA) algorithm. ", "page_idx": 5}, {"type": "text", "text": "We make the following additional assumption of Lipschitz objectives to prove the convergence of Algorithm 2, which is standard in optimization literature. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Lipschitz objectives). For all $m\\in[M]$ , $f_{m}(\\theta)$ is $\\ell_{f}$ -Lipschitz continuous. ", "page_idx": 5}, {"type": "text", "text": "To prove the convergence of Algorithm 2, we can use the same merit function, which leads to a convergence rate of $\\mathcal{O}\\big(T^{-\\frac{1}{6}}\\big)$ . See Theorem 2 below and its proof in Appendix F.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Convergence of the FERERO-SA algorithm). Suppose Assumptions 1, 2, 3 hold, and $M_{g}=0$ . Let $\\{\\theta_{t}\\},\\{\\lambda_{t}\\}$ be the sequences produced by the simplified Algorithm 2 with $\\Omega_{\\lambda_{f}}\\left(\\theta\\right)=$ $\\Delta^{M}$ . Assume $\\lambda^{*}(\\theta_{t}),\\lambda_{\\rho}^{*}(\\theta_{t}),\\lambda_{t}$ are bounded. With properly chosen step sizes $\\alpha\\,=\\,\\Theta(T^{-{\\frac{5}{6}}})$ , $\\gamma=\\Theta(T^{-\\frac{1}{6}})$ , it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}+\\|H(\\theta_{t})\\|_{1}=\\mathcal{O}\\Big(T^{-\\frac{1}{6}}\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To obtain a sharper convergence rate, we consider a different merit function $\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}+$ $\\|H(\\theta_{t})\\|^{2}$ , which achieves zero if $\\theta_{t}$ satisfies the KKT condition. We use this merit function because it provides better properties for sharper analysis. The detailed proof is deferred to Appendix F.3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Sharper convergence of the FERERO-SA algorithm). Suppose Assumptions 1, 2, 3 hold, and $M_{g}=0$ . Let $\\{\\theta_{t}\\},\\{\\lambda_{t}\\}$ be the sequences produced by Algorithm 2. With properly chosen step sizes $\\alpha_{t}=\\Theta(1)$ , $\\gamma_{t}=\\Theta(T^{-1})$ , it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}+\\|H(\\theta_{t})\\|^{2}=\\mathcal{O}\\big(T^{-1}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3 states that $\\{\\theta_{t}\\}$ produced by Algorithm 2 converges to a KKT solution of the PMOL problem in the general nonconvex case. Moreover, both $\\|d_{t}\\|^{\\breve{2}}$ and $\\|H(\\theta_{t})\\|^{2}$ converge to zero at a rate of ${\\mathcal{O}}(T^{-1})$ , implying the convergence of both the objective values and the preference constraints. Note that, the convergence in terms of $\\|H(\\theta_{t})\\|^{2}$ at a rate of ${\\mathcal{O}}(T^{-1})$ is weaker compared to the one with $\\|H(\\theta_{t})\\|_{1}$ at the same rate for Algorithm 1. This is reasonable since Algorithm 2 only uses a one-step approximate update of $\\lambda_{t}$ instead of exactly solving the subprogram. ", "page_idx": 5}, {"type": "text", "text": "The stochastic variant. We employ a stochastic variant of Algorithm 2 based on the double sampling techniques developed in the recent work [6]. The update is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\theta_{t+1}=\\!\\theta_{t}+\\nabla F_{\\xi_{t,1}}(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}}\\\\ {\\lambda_{t+1}=\\!\\Pi_{\\Omega_{\\lambda}(\\theta_{t})}\\big(\\lambda_{t}-\\gamma_{t}\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\big)}\\\\ {\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})=\\!A_{a g}\\nabla F_{\\xi_{t,1}}(\\theta_{t})^{\\top}\\nabla F_{\\xi_{t,2}}(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}-[0^{\\top},c_{h}H_{\\xi_{t,1}}(\\theta_{t})^{\\top}]^{\\top}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tilde{\\nabla}$ is the unbiased stochastic estimate of the gradient, and $\\xi_{t,1}$ and $\\xi_{t,2}$ are two independent stochastic samples obtained at iteration $t$ . ", "page_idx": 5}, {"type": "text", "text": "The full description of the stochastic algorithms and their convergence guarantees are deferred to Appendix G. Note that, compared to [6], we adopt a more efficient implementation that reduces the per-iteration computational complexity. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4 (Convergence of the stochastic FERERO algorithm). Suppose Assumptions 1, 2, 3 hold, and $M_{g}\\,=\\,0$ . Let $\\{\\breve{\\theta}_{t}\\},\\{\\lambda_{t}\\}$ be the sequences produced by Algorithm 3. Suppose the variance of $\\nabla F_{\\xi}(\\theta_{t}),\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})$ are bounded. With properly chosen step sizes $\\alpha_{t}\\,=\\,\\alpha\\,=\\,\\Theta(T^{-\\,\\frac{1}{2}})$ , $\\gamma_{t}=\\gamma=\\Theta(T^{-\\frac{3}{2}})$ , it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\Big[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}+\\|H(\\theta_{t})\\|^{2}\\Big]=\\mathcal{O}\\Big(T^{-\\frac{1}{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 4 generalizes Theorem 3 to its stochastic variants, with a matching convergence rate to the unconstrained stochastic MOO algorithms and stochastic gradient descent. This allows us to apply the algorithm to large-scale machine learning problems, which we detail in Section 5. ", "page_idx": 6}, {"type": "text", "text": "3.2 Choice of relative preferences ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As briefly discussed in Section 2.1, the ordering cone and the corresponding matrix $A$ can be specified according to practical needs. We first discuss how to obtain matrix $A$ for the relative preference given the set of improvement directions. Then we discuss how to choose the relative preference to allow controlled ascent update, which is useful for touring the Pareto front [30]. ", "page_idx": 6}, {"type": "text", "text": "Ordering cone generation. In practice, to obtain the polyhedral cone that defines the partial order, one can usually first define the extreme rays of the polyhedral cone. We then show how to convert the extreme ray description of the cone to the half-space description given by matrix $A$ , i.e., $C_{A}=\\{y\\in\\mathbb{R}^{M}\\mid A y\\stackrel{\\cdot}{\\geq}0\\}$ , by showing how to compute $A$ from the extreme rays. ", "page_idx": 6}, {"type": "text", "text": "Let $Y=[y_{1}\\cdot\\cdot\\cdot y_{M}]\\in\\mathbb{R}^{M\\times M}$ be a matrix that contains all the extreme rays of $C_{A}$ as its column vectors, then $C_{A_{.}}=\\{Y\\lambda\\mid\\lambda\\geq0\\}$ . Let $a_{m}^{\\top}\\in\\mathbb{R}^{1\\times M}$ denote the row vectors of $A$ for all $m\\in[M]$ . Then all $a_{m}$ can be found by $a$ that solves the following linear feasibility program ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{~\\find~\\Sigma~\\s.t.~}\\,Y\\lambda=c,\\enspace c^{\\top}a=0,\\enspace Y^{\\top}a\\geq0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Choice of $C_{A}$ for controlled ascent. If $C_{A}$ is not pre-specified, and the decision maker wants to choose $C_{A}$ to allow controlled ascent, it can be achieved with the following procedure. Let $F_{0}=F(\\theta_{0})$ be the objective of the initial iterate of the algorithm, and $F_{g o}$ be the target function value along the controlled ascent direction. To ensure $F_{g o}-F_{0}\\in-C_{A}$ for controlled ascent, we include $(\\bar{F_{0}}-F_{g o})/\\|F_{0}-F_{g o}\\|$ in the set of extreme rays, then take the extreme rays of the convex hull of the new set to form the columns of $Y$ . Finally, we obtain $C_{A}$ by solving (3.6). ", "page_idx": 6}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To put our work in context, we review the most relevant literature in (preference-guided) multiobjective optimization, constrained optimization, with a focus on gradient-based approaches. ", "page_idx": 6}, {"type": "text", "text": "Multi-objective optimization (MOO). A straightforward approach of MOO is to use scalarization to transform MOO into a single-objective optimization problem [32]. Another popular approach focuses on finding update directions which avoid conflicts with the gradients of the objectives [40, 45, 25]. A foundational algorithm in this domain is the Multiple Gradient Descent Algorithm (MGDA) [10, 26, 6], which dynamically weights gradients to find a steepest common descent direction for all objectives. However, a single solution usually cannot capture different trade-offs on the Pareto front. This motivates the development of preference-guided multi-objective optimization methods. ", "page_idx": 6}, {"type": "text", "text": "Preferences can be modeled through weights or thresholds assigned to different objectives [32]. For example, scalarization-based methods use the $\\ell_{p}$ -norm of the weighted vector-valued objective to convert the vector-valued objective into a scalar-valued objective, e.g., Linear scalarization (LS), Tchebycheff scalarization. Then the problem can be solved by single-objective optimization on the scalar objective. The $\\epsilon\\cdot$ -constraint methods enforce threshold constraints on different objectives, then solve the problem by constrained optimization. More recently, preferences have been modeled by preference vectors defined in the objective space. Then the problem can be formulated as finding Pareto optimal solutions satisfying the constraints defined by the preference vectors [24], or optimizing the distance to the preference vectors [30, 33]. The key difference between FERERO and these works is that FERERO can capture more flexible preferences based on a general partial order. Moreover, we provide non-asymptotic convergence guarantees for the proposed algorithms. ", "page_idx": 6}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/d2adcd97698af53931b79e7d938e83b6076215b2920de64582f0f63e6dab26a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Converging solutions (blue dots) and optimization trajectories (blue lines) on the objective space of different methods on synthetic objectives given in (5.1). Dashed arrows represent prespecified preference vectors. The green dots represent initial objective values. ", "page_idx": 7}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/e9771d468b191f7329c64fcbc968acc04cfbd68b6e06d645a0d6d103f76f737c.jpg", "img_caption": ["Figure 4: Outputs (colored markers) and optimization trajectories (colored lines) of different methods when initial objectives are near the Pareto front. Different colors represent different preferences. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Constrained optimization. Constrained optimization methods include primal methods, penalty and barrier methods, and primal-dual methods [4, 28]. Our proposed method is related to the primal method that finds a feasible update direction to ensure the models are feasible and improving along the optimization trajectory. To address the limitation that it usually requires a stage-one procedure to ensure the initialization is feasible, we use an adaptive approach to ensure the constraint violation is decreasing and converging to zero. This idea can also be found in sequential quadratic programming (SQP) [13]. However, compared to SQP, we use an identity matrix to approximate the Hessian of each objective, and we use an adaptive approach that automatically adjust the descent amount of objectives. Furthermore, existing SQP algorithms typically require an inner loop to solve the optimal Lagrangian multiplier, resulting in double-loop algorithms. In contrast, we develop a single-loop algorithm which is more efficient to implement. ", "page_idx": 7}, {"type": "text", "text": "Vector optimization. Vector optimization [11, 19] generalizes multi-objective optimization by substituting the commonly used component-wise partial order with a more general partial order, such as a general convex-cone induced partial order used in this paper. In the unconstrained setting, the MGDA method is extended to a steepest cone descent method in the vector optimization setting in [17]. Besides gradient-based vector optimization, another line of works focus on black-box vector optimization with discrete design space [3, 2]. In the constrained setting, the first-order optimality conditions are studied in [16, 43]. Algorithms based on projected gradient [9] or conditional gradient [7] are developed to solve vector optimization with parameters constrained in a set, to name a few. To our best knowledge, we are the first to design single-loop (stochastic) primal algorithms for constrained vector optimization with convergence rate guarantees. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conduct experiments to verify our theory and show the applicability of the algorithms to preference-guided multi-task learning, and multi-objective finetuning of large multilingual speech recognition models. We use Linear scalarization (LS), MGDA [40], PMTL [24], EPO [30], XWC-MGDA [33] as baselines for comparison. ", "page_idx": 7}, {"type": "text", "text": "Metrics. Objective loss and accuracy. We report the objective losses and accuracies in classification. Relative loss profile. We use the element-wise product of the preference vector and the objective values as a measure of the relative loss proflie. Hypervolume. Let $F^{\\prime}\\in\\mathbb{R}^{M}$ denote a reference point, and $\\boldsymbol{S}$ denote a set of objective function values of the obtained models. Hypervolume measures the size of the dominated space of $\\boldsymbol{S}$ relative to $F^{\\prime}$ , which can be computed by $H(\\boldsymbol{S})=\\Lambda(\\{\\boldsymbol{q}\\in\\mathbb{R}^{M}\\mid$ $\\exists F\\in S:F\\leq q\\leq F^{\\prime}{\\bar{\\}}\\right)$ ), where $\\Lambda(\\cdot)$ denotes the Lebesgue measure. For a fair comparison, we use the Nadir point, i.e., the worst performance on single-task baselines, as the reference point $F^{\\prime}$ . ", "page_idx": 7}, {"type": "text", "text": "Additional details. The implementation and additional experiments can be found in Appendix H. ", "page_idx": 7}, {"type": "text", "text": "5.1 Synthetic data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Following [24, 30, 33], the first objective we consider is ", "page_idx": 7}, {"type": "equation", "text": "$$\nF(\\theta)=\\left(1-e^{-\\|\\theta-\\frac{1}{\\sqrt{q}}\\mathbf{1}\\|_{2}^{2}},\\ \\ 1-e^{-\\|\\theta+\\frac{1}{\\sqrt{q}}\\mathbf{1}\\|_{2}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/2857528210c6624022e552683f3c9190bb02e71c7815fbd295a7365ee0e4e610.jpg", "img_caption": ["Figure 5: Training losses and accuracies of various methods with different preferences across three image datasets. The horizontal and vertical axes represent results for objective 1 and objective 2, respectively. Different colored dashed arrows indicate various preference vectors. Different markers denote the solutions obtained by different methods, with marker colors matching the preferences. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The objective has a nonconvex Pareto front (PF). See the results of different methods in Figure 3. With uniformly generated weights from a simplex, LS only finds extreme points on the PF with one objective minimized. MGDA can only find points close to the center of the PF. PMTL can find points in the subregions but not aligned well with the exact preference vectors. Similar to EPO, in Figure 3e, our method finds points that align well with the exact preferences; and in Figure 3f, our method can handle different definitions of preferences. ", "page_idx": 8}, {"type": "text", "text": "We conduct another experiment in a more difficult setting where the initial objectives are close to the PF. In Figures $4\\mathrm{a}{-}4\\mathrm{c}$ , we consider a relatively easier case where the initial model is not too close to the Pareto optimal. For our method, by solving (3.6), $\\begin{array}{c}{{a_{1}={[\\frac{1}{\\sqrt{5}};\\frac{2}{\\sqrt{5}}]},a_{2}={[\\frac{2}{\\sqrt{5}};\\frac{1}{\\sqrt{5}}]}}}\\end{array}$ . The corresponding matrix $A$ is given by $A=[a_{1},a_{2}]^{\\top}$ . In this setting, all methods converge to the PF, and our method takes the least number of iterations (PMTL takes 100, EPO Search takes 60, and our method takes only 10 iterations). PMTL does not align exactly with the preference vectors, while EPO and our method do. In Figures 4d-4f, PMTL and our method take 200 iterations, EPO Search takes 80 iterations. Results show that for the green and yellow preferences, PMTL moves further away from the PF in the first stage, and does not perform any update in the second stage. It converges to the PF only in 2 out of 4 cases. In contrast, with controlled ascent updates, EPO and our method can converge to the PF and trace the PF until the objectives align exactly with the preferences. ", "page_idx": 8}, {"type": "text", "text": "5.2 Real data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Multi-patch image classification. Following [24, 30, 33], we consider three datasets for image classification, including Multi-MNIST, Multi-Fashion, and Multi-Fashion+MNIST. The two tasks or objectives in all three datasets are to classify the top-left and the bottom-right images, respectively. For a fair comparison, we use LeNet as the backbone neural network. The training losses and accuracies of different methods given different preference vectors are plotted in Figure 5. Experiments for our method are repeated 5 times. Hypervolumes with means and standard deviations are reported in Table 2. The results for other methods in Table 2 are referenced from [33]. ", "page_idx": 8}, {"type": "text", "text": "One limitation of EPO is that the preference is defined as a ray from the origin in the objective space, whose corresponding objectives can be unattainable, e.g., the yellow preferences in Figure 5. As a result, the losses of all methods are far away from the preference vectors. In this case, a more flexible choice of preferences is helpful to ensure preference satisfaction. To demonstrate this, we conduct experiments with more flexible preferences; see the results in Figure 6, where the obtained solutions align better with the preference lines compared to those in Figure 5. Moreover, it can perform controlled ascent updates during optimization, which cannot be achieved by PMTL or XWC-MGDA. ", "page_idx": 8}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/9f1ffac0d06a6158433fbd00fd0ce43b6a0f0c3ce2e1978b61433a93d5b099a1.jpg", "img_caption": ["Figure 6: Losses and preferences of FERERO when the initial objective is close to the Pareto front. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Emotion recognition. We apply our method to predict 6 types of emotions from 593 songs on the Emotions and Music dataset [41]. We follow the experiment settings in [30], with more details summarized in Appendix H.2. The hypervolumes are reported in Table 2. ", "page_idx": 9}, {"type": "text", "text": "Multi-lingual speech recognition. We further apply the proposed method to the multiobjective finetuning of pre-trained multilingual speech models. We use the Librispeech (100 hours) [36], and AISHELL v1 [5] datasets for multi-lingual speech recognition. A conformer with 8 blocks is used as the model architecture. The total number of parameters is around $64.5\\mathrm{M}$ with $58.4\\mathrm{M}$ ", "page_idx": 9}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/8babd7cf5170f6bd7f28b3854a6e5a8fb0df09f3f1ab9e351e669a83a0c05421.jpg", "table_caption": ["Table 2: Hypervolumes of different methods $(\\times10^{-2})$ ", "Table 3: WERs $(\\%)$ on Librispeech and AISHELL v1 "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "encoder layer parameters and the rest being the classification layer parameters. We consider the objectives associated with the speech recognition Connectionist Temporal Classification (CTC) losses in Chinese and English, denoted as $f_{t}^{\\mathrm{ch}}$ and $f_{t}^{\\mathrm{en}}$ , respectively. We also use the self-supervised Contrastive Predictive Coding (CPC) loss $f_{p}$ for representation learning; that is ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\ \\ \\ F(\\theta):=\\big(f_{p}(\\theta),f_{t}^{\\mathrm{ch}}(\\theta),f_{t}^{\\mathrm{en}}(\\theta)\\big)^{\\top}\\ \\ \\mathrm{s.t.}\\ \\ f_{p}(\\theta)\\leq\\epsilon_{1},\\ f_{t}^{\\mathrm{ch}}(\\theta)-f_{t}^{\\mathrm{en}}(\\theta)=\\epsilon_{2}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the first constraint ensures to learn a good representation with $\\epsilon_{1}=1.2$ , and the second constraint avoids one language loss dominates the other with $\\epsilon_{2}=0.5$ ; see more details in Appendix H.1. ", "page_idx": 9}, {"type": "text", "text": "Results on the word error rate (WER) are reported in Table 3. The baselines include the state-of-the-art result from Komatsu et al. [20] without an additional large language model, our own implementation of training using only the sum of supervised CTC losses (w/o CPC), the initial pre-trained M2ASR model [39] (init.), linear scalarization of all three objectives for finetuning a pre-trained model with the CPC loss (LS-FT). Results show that considering CPC loss besides the supervised CTC loss improves the average WER by $4.2\\%$ , and this can be further improved by $0.3\\%$ by finetuning with linear scalarization. However, the LS-FT model has a much better performance in Chinese compared to English. With our proposed approach, the performance gap between different languages is reduced, and the average WER is further improved by $1.3\\%$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we frame preference-guided multi-objective learning as a constrained vector optimization problem. Specifically, we introduce constraints and partial order to capture the absolute and relative preferences. Under this framework, we develop algorithms to solve the constrained vector optimization problem. Our proposed algorithms use a unified formulation without solving different subprograms at different stages. And they enjoy the benefit of allowing controlled ascent and escaping weak optimal solutions. Theoretical guarantees on the non-asymptotic convergence of the deterministic algorithms and their stochastic variants are provided. Experiments on benchmark datasets demonstrate the broad applicability of the proposed algorithms. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts and Limitations ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This paper casts the preference-guided multi-objective learning as a constrained vector optimization problem and proposes an algorithm with stochastic variants and non-asymptotic convergence guarantees to solve the problem. The proposed method is applied to image classification, speech recognition, and emotion classification. The positive impact is that it is a principled method that has broad applications across various domains. There is no negative social impact. ", "page_idx": 10}, {"type": "text", "text": "The proposed algorithm is able to model flexible preferences but at a cost of higher per-iteration complexity compared to scalarization methods. The theoretical guarantees make standard assumptions that the objectives are lower bounded, Lipschitz continuous and smooth. These are common assumptions in the optimization literature, and can be satisfied for neural networks with smooth activation functions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jaqueline S Angelo, Isabella A Guedes, Helio JC Barbosa, and Laurent E Dardenne. Multi-and many-objective optimization: present and future in de novo drug design. Frontiers in Chemistry, 11, 2023.   \n[2] Cagin Ararat and Cem Tekin. Vector optimization with stochastic bandit feedback. In Proc. International Conference on Artificial Intelligence and Statistics, pages 2165\u20132190, Valencia, Spain, 2023.   \n[3] Peter Auer, Chao-Kai Chiang, Ronald Ortner, and Madalina Drugan. Pareto front identification from stochastic bandit feedback. In Proc. International Conference on Artificial Intelligence and Statistics, pages 939\u2013947, Cadiz, Spain, 2016.   \n[4] Dimitri Bertsekas. Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series). Athena Scientific, 1996.   \n[5] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline. In Conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment, pages 1\u20135, 2017.   \n[6] Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen. Three-way trade-off in multiobjective learning: Optimization, generalization and conflict-avoidance. Journal of Machine Learning Research, 2024. [7] Wang Chen, Xinmin Yang, and Yong Zhao. Conditional gradient method for vector optimization. Computational Optimization and Applications, 85(3):857\u2013896, July 2023. [8] Frank E Curtis, Suyun Liu, and Daniel P Robinson. Fair machine learning through constrained stochastic optimization and an epsilon-constraint method. Optimization Letters, pages 1\u201317, 2023.   \n[9] L. M. Gra\u00f1a Drummond and A.N. Iusem. A projected gradient method for vector optimization problems. Computational Optimization and Applications, 28:5\u201329, April 2004.   \n[10] Jean-Antoine D\u00e9sid\u00e9ri. Multiple-gradient Descent Algorithm (MGDA) for Multi-objective Optimization. Comptes Rendus Mathematique, 350(5-6), 2012.   \n[11] Matthias Ehrgott. Multicriteria optimization. Springer, Berlin; New York, 2nd ed edition, 2005.   \n[12] J\u00f6rg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods of operations research, 51:479\u2013494, 2000.   \n[13] J\u00f6rg Fliege and A. Ismael F. Vaz. A method for constrained multiobjective optimization based on sqp techniques. SIAM Journal on Optimization, 26(4):2091\u20132119, 2016.   \n[14] Bennet Gebken, Sebastian Peitz, and Michael Dellnitz. A descent method for equality and inequality constrained multiobjective optimization problems. In Numerical and Evolutionary Optimization, pages 29\u201361. Springer, 2019.   \n[15] Chengyue Gong, Xingchao Liu, and Qiang Liu. Automatic and harmless regularization with constrained and lexicographic optimization: A dynamic barrier approach. In Proc. Advances in Neural Information Processing Systems, volume 34, pages 29630\u201329642, virtual, 2021.   \n[16] L. M. Gra\u00f1a Drummond, A. N. Iusem, and B. F. Svaiter. On first order optimality conditions for vector optimization. Acta Mathematicae Applicatae Sinica, English Series, 19(3), September 2003.   \n[17] L. M. Gra\u00f1a Drummond and B.F. Svaiter. A steepest descent method for vector optimization. Journal of Computational and Applied Mathematics, 175(2):395\u2013414, March 2005.   \n[18] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.   \n[19] Johannes Jahn. Vector Optimization: Theory, Applications, and Extensions. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.   \n[20] Tatsuya Komatsu, Yusuke Fujita, Jaesong Lee, Lukas Lee, Shinji Watanabe, and Yusuke Kida. Better intermediates improve CTC inference. arXiv preprint arXiv:2204.00176, 2022.   \n[21] Panagiotis Kyriakis, Jyotirmoy Deshmukh, and Paul Bogdan. Pareto policy adaptation. In Proc. International Conference on Learning Representations, virtual, 2021.   \n[22] Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang. Pareto set learning for expensive multi-objective optimization. In Proc. Advances in Neural Information Processing Systems, volume 35, New Orleans, LA, December 2022.   \n[23] Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, and Qingfu Zhang. Smooth tchebycheff scalarization for multi-objective optimization. arXiv preprint arXiv:2402.19078, 2024.   \n[24] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. In Proc. Advances in Neural Information Processing Systems, Vancouver, Canada, December 2019.   \n[25] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-Averse Gradient Descent for Multi-task Learning. In Proc. Advances in Neural Information Processing Systems, virtual, December 2021.   \n[26] Suyun Liu and Luis Nunes Vicente. The Stochastic Multi-gradient Algorithm for Multi-objective Optimization and its Application to Supervised Machine Learning. Annals of Operations Research, pages 1\u201330, 2021.   \n[27] Xingchao Liu, Xin Tong, and Qiang Liu. Profiling Pareto Front With Multi-Objective Stein Variational Gradient Descent. In Proc. Advances in Neural Information Processing Systems, virtual, December 2021.   \n[28] David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming, volume 116 of International Series in Operations Research & Management Science. Springer US, New York, NY, 2008.   \n[29] Sohvi Luukkonen, Helle W. van den Maagdenberg, Michael T.M. Emmerich, and Gerard J.P. van Westen. Artificial intelligence in multi-objective drug design. Current Opinion in Structural Biology, 79:102537, 2023.   \n[30] Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization. In Proc. International Conference on Machine Learning, virtual, 2020.   \n[31] Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax pareto fairness: A multi objective perspective. In Proc. International Conference on Machine Learning, pages 6755\u2013 6764, virtual, 2020.   \n[32] Kaisa Miettinen. Nonlinear Multiobjective Optimization, volume 12. Springer US, Boston, MA, 1998.   \n[33] Michinari Momma, Chaosheng Dong, and Jia Liu. A multi-objective/multi-task learning framework induced by pareto stationarity. In Proc. International Conference on Machine Learning, Baltimore, MD, 2022.   \n[34] Aviv Navon, Aviv Shamsian, Ethan Fetaya, and Gal Chechik. Learning the pareto front with hypernetworks. In Proc. International Conference on Learning Representations, virtual, April 2020.   \n[35] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[36] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an ASR corpus based on public domain audio books. In Proc. International Conference on Acoustics, Speech and Signal Processing, pages 5206\u20135210, 2015.   \n[37] Javier Pe\u00f1a, Juan C. Vera, and Luis F. Zuluaga. New characterizations of hoffman constants for systems of linear constraints. Mathematical Programming, 187(1):79\u2013109, 2021.   \n[38] Hoang Phan, Ngoc Tran, Trung Le, Toan Tran, Nhat Ho, and Dinh Phung. Stochastic multiple target sampling gradient descent. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2022.   \n[39] A F M Saif, Lisha Chen, Xiaodong Cui, Songtao Lu, Brian Kingsbury, and Tianyi Chen. M2ASR: Multilingual multi-task automatic speech recognition via multi-objective optimization. In Interspeech 2024, pages 1240\u20131244, 2024.   \n[40] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In Proc. Advances in Neural Information Processing Systems, Montreal, Canada, December 2018.   \n[41] Konstantinos Trohidis, Grigorios Tsoumakas, George Kalliris, and Ioannis Vlahavas. Multilabel classification of music by emotion. EURASIP Journal on Audio, Speech, and Music Processing, 2011:1\u20139, 2011.   \n[42] Yijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, and Yuhui Shi. Pareto policy pool for model-based offilne reinforcement learning. In Proc. International Conference on Learning Representations, virtual, 2021.   \n[43] Jane J Ye and Qiji J Zhu. Multiobjective optimization problem with variational inequality constraints. Mathematical Programming, 96(1):139\u2013160, 2003.   \n[44] Yiming Ying and Ding-Xuan Zhou. Unregularized online learning algorithms with general loss functions. Applied and Computational Harmonic Analysis, 42(2):224\u2013244, 2017.   \n[45] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. In Proc. Advances in Neural Information Processing Systems, virtual, December 2020.   \n[46] Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sample-efficient multi-objective molecular optimization with gflownets. In Proc. Advances in Neural Information Processing Systems, volume 36, New Orleans, LA, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix for \u201c FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B Related Works and Comparison 14 ", "page_idx": 13}, {"type": "text", "text": "B.1 Extended discussion of related works 15   \nB.2 A detailed comparison with existing works 15   \nC Preliminaries 16   \nC.1 General cone-induced partial ordering . 16   \nC.2 Necessary and sufficient conditions for $C$ -optimality 17   \nD Proof of Auxiliary Lemmas 17   \nD.1 Lagrangian of the subprogram 17   \nD.2 First-order necessary optimality conditions 18   \nD.3 Properties of PMOL 19   \nE Proof of Theorem 1: convergence of Algorithm 1 23   \nE.1 Auxiliary lemmas 23   \nE.2 Proof of Theorem 1 25   \nF Proof of Theorems 2 and 3: convergence of Algorithm 2 27   \nF.1 Auxiliary lemmas . . 27   \nF.2 Analysis with the same merit function: proof of Theorem 2 . 29   \nF.3 Sharper analysis with a different merit function: proof of Theorem 3 32   \nG Stochastic Algorithms 36   \nG.1 Algorithm summary 36   \nG.2 Proof of Theorem 4: convergence of Algorithm 3 36   \nH Implementation Details and Additional Experiment Results 39   \nH.1 Implementation details 39   \nH.2 Additional experiment results 41 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A summary of notations used in this work is listed in Table 4 for ease of reference. ", "page_idx": 13}, {"type": "text", "text": "Recall that given vectors $v,w$ , we use $v<w$ and $v\\leq w$ to denote $v_{i}<w_{i}$ for all $i$ , and $v_{i}\\leq w_{i}$ for all $i$ , respectively. We use $v\\leq w$ to denote $v\\leq w$ and $v\\neq w$ , and define $>,\\geq,\\geq$ analogously. In the proof, we use $\\|\\cdot\\|$ to denote the $\\ell_{2}$ -norm, and $\\|\\cdot\\|_{1}$ to denote the $\\ell_{1}$ -norm. We use $|\\cdot|_{\\mathrm{ab}}$ to denote the operator that takes element-wise absolute value of a matrix. We use 1 and 0 to denote the all-one and all-zero vectors, respectively. Their dimensions are specified only when they are not clear in the context. We use $[v,w]$ to represent column concatenation of matrices or vectors, and use $[v;w]$ to represent row concatenation of matrices or vectors. ", "page_idx": 13}, {"type": "text", "text": "B Related Works and Comparison ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a detailed review and comparison of additional related works in multitask/objective learning, vector optimization, and Pareto front approximation. ", "page_idx": 13}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/4fc8cf854284221317095472e10325574b4ba7e4ec33fa7951a139fbb1fc3c89.jpg", "table_caption": ["Table 4: Notations and their descriptions. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.1 Extended discussion of related works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide an extended discussion of the works that are closely related to ours. ", "page_idx": 14}, {"type": "text", "text": "Pareto front approximation. Pareto front approximation aims to find multiple different solutions whose objective values approximate the Pareto front. Scalarization-based methods can be used to approximate the Pareto front by enumerating different weights of the objectives. However, they cannot find solutions on the nonconvex part of the Pareto front [32]. Decomposition-based methods partition the objective space into different subsets with constraints that represent different trade-off preferences, and solve the constrained multi-objective optimization subproblems with gradientbased or evolutionary algorithms [24, 15]. Probabilistic inference methods update a set of models following a distribution that converges to Pareto stationary [27, 38]. The expected update direction of the models typically follows the steepest common descent direction for all objectives. Pareto set learning methods use a neural network to learn a mapping from user preferences to corresponding models. The learned neural network is able to generate different models with different input user preferences [34, 42, 21, 22]. ", "page_idx": 14}, {"type": "text", "text": "B.2 A detailed comparison with existing works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Preferences as linear constraints of objectives. Different constraints $S$ partition the objectives into sub-regions, as shown in Figure 1. Many preferences can be modeled by linear equality or inequality constraints [24, 30, 33]. For example, below we list different choices of $C$ for different methods in Figure 1. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{g}=[0,I_{2:M}]^{\\top}\\in\\mathbb{R}^{M\\times M},b=-[0,\\epsilon_{2},\\ldots,\\epsilon_{M}]^{\\top};}\\\\ &{B_{h}\\in\\mathbb{R}^{(M-1)\\times M},b=0;}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In Figure 1a, the preferences are based on the function values of $f_{1}$ controlled by different thresholds, corresponding to the inequality constraints defined by (a). In Figure 1b, the constraints are that the objectives $F(\\theta)$ should lie on one of the preference vectors $v$ , therefore should satisfy the equality constraint $B_{h}\\dot{F}(\\theta)=0$ . ", "page_idx": 14}, {"type": "text", "text": "Detailed comparison with the most relevant works. Below we provide a fine-grained comparison with some existing works in Table 5, as an extension of Table 1. ", "page_idx": 14}, {"type": "text", "text": "In terms of preference modeling, the scalarization-based methods such as Linear Scalarization and Smooth Tchebycheff scalarization use weight of different objectives to model preferences. They are not flexible enough to capture preferences illustrated in Figure 1. PMTL uses a constrained multi-objective optimization formulation, with preferences modeled by inequalities. EPO models the preference by an $r^{-1}$ ray, same as the example given in Figure 1b. (X)WC-MGDA uses a shifted ray not necessarily from the origin to model the preferences. In all of these works, they only model the absolute preferences that define the preferred objective values. In contrast, we also consider the relative preference that define the relative improvement directions of objectives. ", "page_idx": 14}, {"type": "text", "text": "In addition to the comparison in Table 1, our framework enjoys additional beneftis including the ability to escape weak optimal solutions and to maintain scale-invariance. These abilities are attributed to the subprogram that is adaptive to the objective values, as detailed in Lemma 4. ", "page_idx": 15}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/30659c290f83ca672914d79584fe9f55c2b68cf916193da1dc9f49bc82e3a8cc.jpg", "table_caption": ["Table 5: Comparison to existing PMOL methods, extension of Table 1. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Below, we further summarize the reasons behind the benefits of our proposed method. We use $\\omega{\\leftarrow}$ \u201d to indicate the reasons on the left and the corresponding benefits on the right. ", "page_idx": 15}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/4256dcbb2d9d01671d5ffedb5e97121424372760f075b82f38c515312bb0c7de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we introduce preliminaries on the general cone-induced partial ordering and the corresponding optimality conditions for completeness since we use these concepts in our proofs. Then we discuss the relation between the Pareto optimality and the optimality induced by a general polyhedral cone. ", "page_idx": 15}, {"type": "text", "text": "C.1 General cone-induced partial ordering ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we introduce basic definitions, lemmas, propositions, and theorems in vector optimization, including the cone-induced partial ordering, the minimum and weakly minimum associated with the partial ordering in real linear space, and necessary conditions for minimum. These concepts are defined in [19]. We restate them following our notations for completeness. We denote $Z$ as a real linear space, $C,S$ as subsets in $Z$ , and $w,x,y,z$ as points or elements in $Z$ , $0_{Z}$ as the zero vector in the space $Z$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 3 (Cone). Let $C$ be a nonempty subset of a real linear space $Z$ .   \nThe set $C$ is called a cone, $i f y\\in C,\\lambda\\geq0\\Longrightarrow\\lambda y\\in C$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 3 (Convex cone). A cone $C$ in a real linear space is convex if and only if $C+C\\subset C$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 4 (Partially ordered linear space). A real linear space equipped with a partial ordering is a partially ordered linear space. ", "page_idx": 15}, {"type": "text", "text": "Theorem 5. (a) $I f\\!\\leq$ is a partial ordering on $Z$ , then the set $C:=\\{z\\in Z\\mid0_{Z}\\leq z\\}$ is a convex cone. If, in addition, $\\leq$ is antisymmetric, then $C$ is pointed. ", "page_idx": 15}, {"type": "text", "text": "$(b)$ If $C$ is a convex cone in $Z$ , then the binary relation $\\leq_{C}:=\\{(x,y)\\in Z\\times Z\\mid y-x\\in C\\}$ is $a$ partial ordering on $Z$ . If, in addition, $C$ is pointed, then $\\leq_{C}$ is antisymmetric. ", "page_idx": 15}, {"type": "text", "text": "Definition 5 (Ordering cone). A convex cone characterizing a partial ordering in a real linear space is an ordering cone. ", "page_idx": 15}, {"type": "text", "text": "Definition 6 (Cone-induced partial ordering). Let $C$ be a closed pointed convex cone of $\\mathbb{R}^{M}$ , with nonempty interior. The partial order in $\\mathbb{R}^{M}$ induced by $C,\\leq_{C}$ is defined by ", "page_idx": 16}, {"type": "equation", "text": "$$\nu\\leq c\\ v,\\ i f\\ v-u\\in C.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The relation induced by $\\operatorname{int}(C)$ in $\\mathbb{R}^{M}$ , $<\\!C$ is defined by ", "page_idx": 16}, {"type": "equation", "text": "$$\nu<_{C}v,\\ i f\\ v-u\\in\\operatorname{int}(C).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition 7 ( $C$ -minimum and $C$ -weakly minimum). Let $S$ be a nonempty subset of a partially ordered linear space with an ordering cone $C$ , then   \n(a) an element $z\\in S$ is called a $C$ -minimum of the set $S$ , $i f\\left(\\left\\{z\\right\\}-C\\right)\\cap S\\subset\\left\\{z\\right\\}+C,$ , in other words, there exists no other $z^{\\prime}\\in S$ with $z^{\\prime}\\leq_{C}z$ and $z^{\\prime}\\neq z$ ;   \n$(b)$ an element $z\\in S$ is called a $C$ -weakly minimum of the set $S$ , $i f\\left(\\{z\\}-\\mathrm{int}(C)\\right)\\cap S=\\emptyset,$ , where $\\operatorname{int}(C)\\neq\\emptyset$ is the algebraic interior of $C$ , in other words, there exists no other $z^{\\prime}\\in S$ with $z^{\\prime}<_{C}z$ and $z^{\\prime}\\neq z$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 8 ( $C$ -stationary). A point $\\theta\\in\\mathbb{R}^{q}$ is $C$ -stationary if there is no first-order common descent direction $d\\in\\mathbb{R}^{q}$ that $\\nabla\\dot{F}(\\theta)^{\\top}d\\in-\\mathrm{int}(C),$ i.e., $\\operatorname{range}({\\dot{\\nabla}}{\\dot{F}}(\\theta)^{\\top})\\cap(-{\\mathrm{int}}(C))=\\emptyset$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 Necessary and sufficient conditions for $C$ -optimality ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Note that, when $C=\\mathbb{R}_{+}^{M}:=\\{z\\in\\mathbb{R}^{M}\\mid z_{m}\\ge0\\}$ for all $m\\in[M]\\}$ , $C$ -minimum and $C$ -weakly minimum in Definition 7 are Pareto minimum and weakly Pareto minimum, respectively. Recall that $F:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}^{M}$ is a continuously differentiable function. The problem we consider is to find the unconstrained $C$ -minimizers of $F$ , denoted as $\\operatorname*{min}_{C}F(\\theta)$ with $\\theta\\in\\mathbb{R}^{q}$ . We then proceed to introduce the relation between $C$ -stationarity and Pareto stationarity in this section. ", "page_idx": 16}, {"type": "text", "text": "Proposition 1. Let $C$ be a closed convex pointed cone. ", "page_idx": 16}, {"type": "text", "text": "1) Suppose $C\\subseteq\\mathbb{R}_{+}^{M}$ . If $\\theta$ is Pareto stationary, $\\theta$ is $C$ -stationary. In other words, $C$ -stationarity is $a$ necessary condition for Pareto stationarity.   \n2) Suppose $\\mathbb{R}_{+}^{M}\\subseteq\\check{C}$ . if $\\theta$ is $C$ -stationary, $\\theta$ is Pareto stationary. In other words, $C$ -stationarity is a sufficient condition for Pareto stationarity. ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . 1) By definition, if $\\theta$ is Pareto stationary, then ran $\\mathrm{ge}(\\nabla F({\\boldsymbol{\\theta}})^{\\top})$ \u2229 $\\bar{(-\\mathrm{int}(\\mathbb{R}_{+}^{M}))}=\\emptyset$ . Since $C\\subseteq\\operatorname{\\dot{R}}_{+}^{M}$ , then $-\\mathrm{int}(C)\\subseteq-\\mathrm{int}(\\mathbb{R}_{+}^{M})$ , and we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{range}(\\nabla F(\\theta)^{\\top})\\cap(-\\mathrm{int}(C))\\subseteq\\mathrm{range}(\\nabla F(\\theta)^{\\top})\\cap(-\\mathrm{int}(\\mathbb{R}_{+}^{M}))=\\emptyset.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, $\\theta$ is $C$ -stationary. ", "page_idx": 16}, {"type": "text", "text": "Following similar arguments, 2) can also be proved. ", "page_idx": 16}, {"type": "text", "text": "D Proof of Auxiliary Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide proof of the main theoretical results in this paper. ", "page_idx": 16}, {"type": "text", "text": "D.1 Lagrangian of the subprogram ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of subprogram reformulation. Define the Lagrangian function ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(c,d,\\lambda_{f},\\lambda_{g},\\lambda_{h}):=\\!c+\\frac{1}{2}\\|d\\|^{2}+\\lambda_{f}^{\\top}\\big(A\\nabla F(\\theta)^{\\top}d-c(\\mathbf1^{\\top}A F(\\theta))^{-1}A F(\\theta)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\lambda_{g}^{\\top}\\big(B_{g}\\nabla F(\\theta)^{\\top}d+c_{g}G(\\theta)\\big)+\\lambda_{h}^{\\top}\\big(B_{h}\\nabla F(\\theta)^{\\top}d+c_{h}H(\\theta)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "cwahne roeb $\\lambda_{f}\\in\\mathbb{R}_{+}^{M},\\lambda_{g}\\in\\mathbb{R}_{+}^{M_{g}},\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ . By the first-order optimality condition w.r.t. $d$ and $c$ , we ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d^{*}+\\nabla F(\\theta)(A^{\\top}\\lambda_{f}^{*}+B_{g}^{\\top}\\lambda_{g}^{*}+B_{h}^{\\top}\\lambda_{h}^{*})=0;}\\\\ {\\mathbf{1}^{\\top}A F(\\theta)-\\boldsymbol{\\lambda_{f}^{*}}^{\\top}A F(\\theta)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the last equation with $\\lambda_{f}\\in\\mathbb{R}_{+}^{M}$ , we obtain $\\lambda_{f}^{*}\\in\\Omega_{\\lambda_{f}}$ . Plugging the above results into the Lagrangian function gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\lambda_{f}^{*};\\lambda_{g}^{*};\\lambda_{h}^{*}]\\in\\underset{[\\lambda_{f};\\lambda_{g};\\lambda_{h}]\\in\\Omega_{\\lambda}}{\\arg\\operatorname*{min}}\\frac{1}{2}\\|\\nabla F(\\theta)(A^{\\top}\\lambda_{f}+B_{g}^{\\top}\\lambda_{g}+B_{h}^{\\top}\\lambda_{h})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-c_{g}\\lambda_{g}^{\\top}G(\\theta)-c_{h}\\lambda_{h}^{\\top}H(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which leads to the dual form in (2.3). Since (2.1) is a constrained convex optimization problem where Slater\u2019s condition holds, therefore, the duality gap is zero. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "D.2 First-order necessary optimality conditions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We then discuss the first-order necessary optimality conditions for problem (PMOL). We begin the discussion with the geometric notions of improving and feasible directions. ", "page_idx": 17}, {"type": "text", "text": "Improving directions. The improvement directions are defined as generalized common descent directions so that the iterates strictly improve or dominate the previous iterates based on $C_{A}$ , i.e., $F(\\theta_{t})\\,-\\,F(\\theta_{t+1})\\,\\in\\,\\mathrm{int}(C_{A})$ . Denote $d_{t}\\,\\in\\,\\mathbb{R}^{q}$ as an update direction at iteration $t$ , and $\\alpha_{t}\\,>\\,0$ as the step size at the $t$ -th iteration. The general update equation given update direction $d_{t}$ is $\\theta_{t+1}=\\theta_{t}+\\alpha_{t}d_{t}$ . Based on first-order Taylor expansion, the amount of improvement at iteration $t$ can be approximately expressed as $F(\\theta_{t})\\stackrel{*}{-}F(\\theta_{t+1}^{\\textit{*}})\\approx-\\alpha_{t}\\nabla F(\\theta_{t})^{\\top}d_{t}\\in\\mathrm{{int}}(C_{A}).$ . We term such directions the general $C_{A}$ -improving directions. The cone of $C_{A}$ -improving directions at $x$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{C_{A}}=\\{d\\in\\mathbb{R}^{q}\\mid\\nabla F(\\boldsymbol{\\theta})^{\\top}d\\in-\\mathrm{int}(C_{A})\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $A=I_{M}$ , they are common descent directions. ", "page_idx": 17}, {"type": "text", "text": "Feasible directions. Similar to the concept in constrained single objective optimization, the feasible directions are those that ensure $F(\\theta_{t}+{\\dot{\\alpha}}_{t}d_{t})\\,\\in\\,S$ . We rewrite problem (PMOL) with explicit $C_{A}$ -induced partial ordering as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{C A}~F(\\theta)~\\mathrm{{s.t.}}~G(\\theta)\\leq0,~H(\\theta)=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $G:\\mathbb{R}^{q}\\,\\rightarrow\\,\\mathbb{R}^{M_{g}},H\\,:\\,\\mathbb{R}^{q}\\,\\rightarrow\\,\\mathbb{R}^{M_{h}}$ are linear functions of $F$ , and are differentiable. Let $I\\,=\\,\\{i\\mid G_{i}(\\theta)\\,=\\,0\\}$ be the index set of the active inequality constraints in $G(\\theta)$ , and $G_{I}(\\theta)\\;=\\;$ $[\\cdot\\cdot\\cdot,G_{i}(\\theta),\\cdot\\cdot\\cdot]^{\\top}$ for $i\\in I$ . A subset of the feasible directions described by the gradients of the equality and active inequality constraints at $\\theta$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{g}=\\{d\\in\\mathbb{R}^{q}\\mid\\nabla G_{I}(\\theta)^{\\top}d<0\\},\\quad D_{H}=\\{d\\in\\mathbb{R}^{q}\\mid\\nabla H(\\theta)^{\\top}d=0\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A necessary optimality condition is that there exists no feasible and improving directions at $\\theta$ , i.e., $D_{C_{A}}\\cap D_{g}\\cap D_{h}=\\emptyset$ . An algebraic description of the necessary optimality conditions for (PMOL) is summarized below. ", "page_idx": 17}, {"type": "text", "text": "Proposition 2 (First-order necessary optimality conditions for (PMOL)). Let $C_{A}:=\\{y\\in\\mathbb{R}^{M}\\}$ $A y\\;\\geq\\;0\\}$ that satisfies $\\operatorname{int}(C_{A})\\,\\neq\\,\\emptyset$ . If $\\bar{\\theta}$ solves (PMOL) locally, then there exists $\\bar{\\lambda}_{f}\\,\\in\\,\\mathbb{R}_{+}^{M}$ $\\lambda_{g}\\in\\mathbb{R}_{+}^{M_{g}}$ , $\\left[\\lambda_{f};\\lambda_{g}\\right]\\neq0,$ , and $\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla F(\\bar{\\theta})A^{\\top}\\lambda_{f}+\\nabla G(\\bar{\\theta})\\lambda_{g}+\\nabla H(\\bar{\\theta})\\lambda_{h}=0,\\;\\;a n d\\;\\;\\lambda_{g}^{\\top}[-G(\\bar{\\theta})]_{+}=0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 2. The geometric description $D_{C_{A}}\\cap D_{g}\\cap D_{h}=\\emptyset$ is equivalent to that the linear system below w.r.t. $d$ is inconsistent ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left[\\overset{A\\nabla F(\\bar{\\theta})^{\\top}}{\\nabla G}\\right]d<0\\,\\mathrm{~and~}\\,\\nabla H(\\bar{\\theta})^{\\top}d=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the Motzkin\u2019s transposition theorem, system (D.8) being inconsistent is equivalent to that the following linear system w.r.t. $p,\\lambda_{h}$ has a solution with $p\\geq0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[\\nabla F(\\bar{\\theta})A^{\\top}}&{{}\\nabla G_{I}(\\bar{\\theta})\\right]p+\\nabla H(\\bar{\\theta})\\lambda_{h}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Letting $p=[\\lambda_{f};\\lambda_{g,I}]$ , where $\\lambda_{g,I}=[\\cdot\\cdot\\cdot;\\lambda_{g,i};\\cdot\\cdot\\cdot],i\\in I$ , and $\\lambda_{g,i^{\\prime}}=0$ , for all $i^{\\prime}\\notin I$ completes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Remark 6. Notice that, Proposition 2 provides a Fritz John (FJ)-type first-order necessary optimality condition, which has been discussed in prior works such as [43, Theorem $I.2J$ with additional variational inequality constraints, and [16, Section 3, (2)-(5)] with inequalities constraints only. In the $F J$ -type necessary optimality condition, the multiplier $\\lambda_{f}$ associated with the objective $F(\\theta)$ can be zero if $\\left|I\\right|\\geq1$ , which is undesirable. We need additional constraint qualifications to ensure the condition in (D.7) with $\\lambda_{f}\\ne0$ is also a necessary optimality condition, i.e., the KKT condition, which is equivalent to $\\mu_{0}=1$ , and without considering the variational inequality constraints in $I^{43}$ , Theorem 1.2]. The constraint qualification is discussed in detail in Appendix D.3.2. ", "page_idx": 18}, {"type": "text", "text": "D.3 Properties of PMOL ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we discuss the properties of PMOL and their proofs. These include the properties of the subprogram in Lemma 1, and the calmness CQ of PMOL in Lemma 2. ", "page_idx": 18}, {"type": "text", "text": "D.3.1 Proof of Lemma 1: properties of the subprogram ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 4 (Additional properties of the subprogram). For the subprogram (2.3), the following properties hold: ", "page_idx": 18}, {"type": "text", "text": "1. The solution $d^{*}(\\theta)$ is unique. ", "page_idx": 18}, {"type": "text", "text": "2. If $\\theta$ is a local weak optimal solution with $A F(\\theta)>0,$ , then $d^{*}(\\theta)=0,$ , $\\psi(\\theta)=0$ . Otherwise, if $\\theta$ is not a local weak optimal solution, then $d^{*}(\\theta)\\neq0,$ , $\\psi(\\theta)<0,$ , and when $\\theta$ is feasible, ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\psi(\\theta)\\leq-\\|d^{*}(\\theta)\\|^{2}<0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3. (Ability to escape weak optimal solutions). Let $\\theta$ be a weak optimal solution, with $(A F(\\theta))_{m}=$ 0 for some $m\\ \\in\\ [M]$ . If there exists feasible and non-strictly improving directions at $\\theta$ with $A\\nabla F(\\theta)^{\\top}d\\leq0,$ , then $d^{*}(\\theta)\\neq0,$ , $\\psi(\\theta)<0.$ . Otherwise, if there exists no feasible and non-strictly improving directions at $\\theta$ with $A\\nabla F(\\theta)^{\\top}d\\lesssim0$ , then $d^{*}(\\theta)=0,$ , $\\psi(\\theta)=0$ . ", "page_idx": 18}, {"type": "text", "text": "4. (Scale invariance) Suppose there are only equality constraints, i.e., $M_{g}=0$ , and $M_{h}=M-1$ , $B_{h}$ is full row rank and is selected such that $B_{h}^{\\bar{-}}(F(\\theta_{1})-F(\\theta_{2}))=0$ with $F({\\theta}_{1}),F({\\theta}_{2})$ being two different reference points in the objective space. For all $\\theta\\in\\mathbb{R}^{q}$ that are feasible, i.e., $H(\\theta)=0$ , when $A=I$ , the normalized solution $d^{*}(\\theta)/\\lVert d^{*}(\\theta)\\rVert$ does not change when the objective $F(\\theta)$ is scaled by an arbitrary positive diagonal matrix. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 4. For Property-1, the uniqueness of $d^{*}(\\theta)$ follows from the strict convexity of the objective function w.r.t. the direction $d$ . ", "page_idx": 18}, {"type": "text", "text": "For Property-2, in the first case if $\\theta$ is a local optimal solution, by definition, there exists no feasible and improving directions $d$ such that $A\\nabla F(\\theta)^{\\top}d<0$ . Let $\\Omega_{d}(\\theta)$ be the set of $d\\in\\mathbb{R}^{q}$ that satisfy the constraints in (2.1), i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Omega_{d}(\\theta):=\\{d\\in\\mathbb{R}^{q}\\ |\\ B_{g}\\nabla F(\\theta)^{\\top}d+c_{g}G(\\theta)\\leq0,B_{h}\\nabla F(\\theta)^{\\top}d+c_{h}H(\\theta)=0\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, since $A F(\\theta)>0$ , for all $d\\in\\Omega_{d}(\\theta)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{m\\in[M]}{\\operatorname*{max}}\\big(A\\nabla F(\\theta)^{\\top}d\\big)_{m}\\geq0}\\\\ &{\\mathrm{and}\\underset{m\\in[M]}{\\operatorname*{max}}\\big(A\\nabla F(\\theta)^{\\top}d\\big)_{m}/(A F(\\theta))_{m}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And since $A F(\\theta)>0$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi(\\theta):=\\operatorname*{min}_{(d,c)\\in\\Omega_{d}(\\theta)\\times\\mathbb{R}}c+\\displaystyle\\frac{1}{2}\\|d\\|^{2}}\\\\ {\\displaystyle\\qquad=\\operatorname*{min}_{d\\in\\Omega_{d}(\\theta)}\\operatorname*{max}_{m\\in[M]}(A\\nabla F(\\theta)^{\\top}d)_{m}(\\mathbf1^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\displaystyle\\frac12\\|d\\|^{2}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\psi(\\theta)=0$ attainable by taking $d=0\\in\\Omega_{d}(\\theta)$ . The first case of Property-2 is proved. ", "page_idx": 18}, {"type": "text", "text": "In the second case, if $\\theta$ is not a local weak optimal solution, then there exists $d\\in\\Omega_{d}(\\theta)$ such that $A\\nabla F(\\theta)^{\\top}d<\\,0$ . Taking $\\begin{array}{r}{\\sigma\\,=\\,-\\operatorname*{max}_{m\\in[M]}(A\\nabla F(\\theta)^{\\top}d)_{m}(\\mathbf1^{\\top}A F(\\theta))/\\big((A F(\\theta))_{m}\\|d\\|^{2}\\big)}\\end{array}$ , and $d_{\\sigma}=\\sigma d$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\psi(\\theta):=\\operatorname*{min}_{(d,c)\\in\\Omega_{d}(\\theta)\\times\\mathbb{R}}c+\\frac{1}{2}\\|d\\|^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\operatorname*{min}_{d\\in\\Omega_{d}(\\theta)}\\operatorname*{max}_{m\\in[M]}(A\\nabla F(\\theta)^{\\top}d)_{m}(\\mathbf{1}^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\displaystyle\\frac{1}{2}\\|d\\|^{2}}\\\\ &{=\\displaystyle\\operatorname*{max}_{m\\in[M]}(A\\nabla F(\\theta)^{\\top}d^{*}(\\theta))_{m}(\\mathbf{1}^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\displaystyle\\frac{1}{2}\\|d^{*}(\\theta)\\|^{2}}\\\\ &{<\\displaystyle\\operatorname*{max}_{m\\in[M]}(A\\nabla F(\\theta)^{\\top}d_{\\sigma})_{m}(\\mathbf{1}^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\displaystyle\\frac{1}{2}\\|d_{\\sigma}\\|^{2}}\\\\ &{=\\displaystyle\\sigma\\operatorname*{max}_{m\\in[M]}(A\\nabla F(\\theta)^{\\top}d)_{m}(\\mathbf{1}^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\displaystyle\\frac{1}{2}\\sigma^{2}\\|d\\|^{2}=-\\displaystyle\\frac{1}{2}\\sigma^{2}\\|d\\|^{2}<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus $d^{\\ast}(\\theta)\\neq0$ . Recall that ", "page_idx": 19}, {"type": "equation", "text": "$$\nd^{*}(\\theta)=-\\nabla F(\\theta)\\Big(A^{\\top}\\lambda_{f}^{*}+B_{g}^{\\top}\\lambda_{g}^{*}+B_{h}^{\\top}\\lambda_{h}^{*}\\Big)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where by the feasibility and optimality conditions, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\lambda_{h}^{*}}^{\\top}\\big(\\boldsymbol{B_{h}}\\boldsymbol{\\nabla}\\boldsymbol{F}(\\boldsymbol{\\theta})^{\\top}\\boldsymbol{d}^{*}(\\boldsymbol{\\theta})+c_{h}\\boldsymbol{H}(\\boldsymbol{\\theta})\\big)=0,}\\\\ {\\boldsymbol{\\lambda_{g}^{*}}^{\\top}\\big(\\boldsymbol{B_{g}}\\boldsymbol{\\nabla}\\boldsymbol{F}(\\boldsymbol{\\theta})^{\\top}\\boldsymbol{d}^{*}(\\boldsymbol{\\theta})+c_{g}\\boldsymbol{G}(\\boldsymbol{\\theta})\\big)=0,}\\\\ {\\boldsymbol{\\lambda_{f}^{*}}^{\\top}\\big(\\boldsymbol{A}\\boldsymbol{\\nabla}\\boldsymbol{F}(\\boldsymbol{\\theta})^{\\top}\\boldsymbol{d}^{*}(\\boldsymbol{\\theta})-c^{*}(\\mathbf{1}_{M}^{\\top}\\boldsymbol{A}\\boldsymbol{F}(\\boldsymbol{\\theta}))^{-1}\\boldsymbol{A}\\boldsymbol{F}(\\boldsymbol{\\theta})\\big)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above with (D.16), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|d^{*}(\\theta)\\|^{2}=-\\,d^{*}(\\theta)^{\\top}\\nabla F(\\theta)\\Big(A^{\\top}\\lambda_{f}^{*}+B_{g}^{\\top}\\lambda_{g}^{*}+B_{h}^{\\top}\\lambda_{h}^{*}\\Big)}\\\\ &{\\qquad\\quad=-\\,d^{*}(\\theta)^{\\top}\\nabla F(\\theta)A^{\\top}\\lambda_{f}^{*}+c_{h}{\\lambda_{h}^{*}}^{\\top}H(\\theta)+c_{g}{\\lambda_{g}^{*}}^{\\top}G(\\theta)}\\\\ &{\\qquad\\quad\\leq-\\,c^{*}(\\theta)(\\mathbf1^{\\top}A F(\\theta))^{-1}{\\lambda_{f}^{*}}^{\\top}A F(\\theta)=-c^{*}(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality uses the fact that $\\theta$ is feasible, and $G(\\theta)\\leq0,H(\\theta)=0$ . ", "page_idx": 19}, {"type": "text", "text": "Then it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n2\\psi(\\theta)=2c^{*}(\\theta)+\\|d^{*}(\\theta)\\|^{2}\\leq-\\|d^{*}(\\theta)\\|^{2}<0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, Property-2 holds. ", "page_idx": 19}, {"type": "text", "text": "For Property-3, let $I\\subseteq[M]$ be the set such that $(A F(\\theta))_{m}\\,=\\,0$ for all $m\\,\\in\\,I$ , then (2.1) is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi(\\theta)=\\operatorname*{min}_{(d,c)\\in\\mathbb{R}^{q}\\times\\mathbb{R}}{c}+\\frac{1}{2}\\|d\\|^{2}}\\\\ {\\mathrm{~s.t.~}(A\\nabla F(\\theta)^{\\top}d)_{m}-c(\\mathbf{1}^{\\top}A F(\\theta))^{-1}(A F(\\theta))_{m}\\leq0,\\ \\mathrm{~for~all~}m\\in[M]\\setminus I}\\\\ {\\displaystyle(A\\nabla F(\\theta)^{\\top}d)_{m}\\leq0,\\ \\mathrm{~for~all~}m\\in I}\\\\ {\\displaystyle B_{g}\\nabla F(\\theta)^{\\top}d+c_{g}G(\\theta)\\leq0}\\\\ {\\displaystyle B_{h}\\nabla F(\\theta)^{\\top}d+c_{h}H(\\theta)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the first case, if there exists feasible and non-strictly improving directions at $\\theta$ with $A\\nabla F(\\theta)^{\\top}d\\leq$ 0, then such $\\textit{d}\\neq\\mathrm{~0,~}\\textit{d}\\in\\mathrm{~\\Omega~}$ . Following similar arguments as (D.15) by taking $\\sigma\\mathrm{~\\}=$ $\\begin{array}{r l}&{-\\operatorname*{max}_{m\\in[M]\\backslash I}(A\\overleftarrow{\\nabla}F(\\theta)^{\\top}d)_{m}(\\mathbf{1}^{\\top}A F(\\theta))/\\big((\\overleftarrow{A F}(\\theta))_{m}\\|d\\|^{2}\\big)}\\end{array}$ , and $d_{\\sigma}=\\sigma d$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi(\\theta):=\\operatorname*{min}_{(d,c)\\in\\Omega_{d}(\\theta)\\times\\mathbb{R}}c+\\frac{1}{2}\\|d\\|^{2}}\\\\ {\\displaystyle=\\operatorname*{min}_{d\\in\\Omega_{d}(\\theta)}\\operatorname*{max}_{m\\in[M]\\backslash I}(A\\nabla F(\\theta)^{\\top}d)_{m}(\\mathbf1^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\frac{1}{2}\\|d\\|^{2}}\\\\ {\\displaystyle<\\operatorname*{max}_{m\\in[M]\\backslash I}(A\\nabla F(\\theta)^{\\top}d_{\\sigma})_{m}(\\mathbf1^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\frac{1}{2}\\|d_{\\sigma}\\|^{2}}\\\\ {\\displaystyle=\\sigma\\operatorname*{max}_{m\\in[M]\\backslash I}(A\\nabla F(\\theta)^{\\top}d)_{m}(\\mathbf1^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\frac{1}{2}\\sigma^{2}\\|d\\|^{2}=-\\frac{1}{2}\\sigma^{2}\\|d\\|^{2}<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "And the corresponding $d^{*}(\\theta)\\neq0$ . ", "page_idx": 20}, {"type": "text", "text": "In the second case, if there exists no feasible and non-strictly improving directions at $\\theta$ , then for all $d\\in\\Omega_{d}(\\theta)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{m\\in[M]\\backslash I}{\\operatorname*{max}}(A\\nabla F(\\boldsymbol{\\theta})^{\\top}d)_{m}\\geq0}\\\\ &{\\mathrm{and}\\underset{m\\in[M]\\backslash I}{\\operatorname*{max}}(A\\nabla F(\\boldsymbol{\\theta})^{\\top}d)_{m}/(A F(\\boldsymbol{\\theta}))_{m}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "And since $(A F(\\theta))_{m}>0$ for all $m\\in[M]\\setminus I$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\psi(\\theta):=\\operatorname*{min}_{(d,c)\\in\\Omega_{d}(\\theta)\\times\\mathbb{R}}c+\\frac{1}{2}\\|d\\|^{2}}\\\\ {\\displaystyle\\quad=\\operatorname*{min}_{d\\in\\Omega_{d}(\\theta)}\\operatorname*{max}_{m\\in[M]\\setminus I}(A\\nabla F(\\theta)^{\\top}d)_{m}(\\mathbf{1}^{\\top}A F(\\theta))/(A F(\\theta))_{m}+\\frac{1}{2}\\|d\\|^{2}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\psi(\\theta)=0$ if and only if $d=0\\in\\Omega_{d}(\\theta)$ . ", "page_idx": 20}, {"type": "text", "text": "Combining the above arguments, Property-3 is proved. ", "page_idx": 20}, {"type": "text", "text": "For Property-4, let $d^{*}(\\theta)$ be the solution to the original problem (2.1) without inequality constraints. Using the fact that $H(\\theta)=0$ , and letting $\\lambda=A^{\\top}\\bar{\\lambda_{f}}+\\bar{B}_{h}^{\\top}\\lambda_{h}=\\lambda_{f}+B_{h}^{\\top}\\lambda_{h}$ , then the original dual problem can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle d^{*}(\\theta)=-\\nabla F(\\theta)\\lambda^{*}}\\\\ {\\mathrm{~s.t.~}\\lambda^{*}\\in\\underset{\\lambda\\in\\Omega_{\\tilde{\\lambda}}(\\theta)}{\\arg\\operatorname*{min}}\\,\\varphi(\\lambda;\\theta):=\\frac{1}{2}\\|\\nabla F(\\theta)\\lambda\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Omega_{\\tilde{\\lambda}}(\\theta)=\\left(\\Omega_{\\lambda_{f}}(\\theta)\\right)+B_{h}^{\\top}\\big(\\mathbb{R}^{M_{h}}\\big)$ , and $\\Omega_{\\lambda_{f}}(\\theta)=\\{\\lambda_{f}\\in\\mathbb{R}_{+}^{M}\\mid\\lambda_{f}^{\\top}F(\\theta)=\\mathbf{1}^{\\top}F(\\theta)\\}.$ . ", "page_idx": 20}, {"type": "text", "text": "Suppose the objective is scaled by a positive diagonal matrix $\\boldsymbol{\\Lambda}\\in\\mathbb{R}^{M\\times M}$ , then the scaled subprogram has a dual given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{d^{*}(\\theta)=-\\nabla F(\\theta)\\Lambda\\lambda^{*}\\displaystyle}\\\\ {\\mathrm{~s.t.~}\\lambda^{*}\\in\\displaystyle\\operatorname*{\\arg\\operatorname*{min}_{\\lambda\\in\\Omega_{\\tilde{\\lambda}}(\\theta;\\Lambda)}\\varphi(\\lambda;\\theta):=\\frac{1}{2}\\|\\nabla F(\\theta)\\Lambda\\lambda\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Omega_{\\tilde{\\lambda}}(\\theta;\\Lambda)\\;=\\;\\left(\\Omega_{\\lambda_{f}}(\\theta;\\Lambda)\\right)\\,+\\,B_{h}^{\\prime\\;\\top}\\bigl(\\mathbb{R}^{M_{h}}\\bigr)$ , and $\\Omega_{\\lambda_{f}}\\bigl(\\theta;\\Lambda\\bigr)\\;=\\;\\{\\lambda_{f}\\;\\in\\;\\mathbb{R}_{+}^{M}\\;\\vert\\;\\;\\lambda_{f}^{\\;\\top}\\Lambda F(\\theta)\\;=\\;\\;$ $\\mathbf{1}^{\\top}\\Lambda F(\\boldsymbol{\\theta})\\dot{\\mathbf{\\jmath}}$ . Letting $\\lambda^{\\prime}=\\Lambda\\lambda$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{*}(\\theta)=-\\nabla F(\\theta)\\lambda^{\\prime*}}\\\\ &{\\mathrm{s.t.~}\\lambda^{\\prime*}\\in\\underset{\\lambda^{\\prime}\\in\\Omega_{\\tilde{\\lambda}^{\\prime}}(\\theta;\\Lambda)}{\\arg\\operatorname*{min}}\\,\\varphi(\\lambda;\\theta):=\\cfrac{1}{2}\\|\\nabla F(\\theta)\\lambda^{\\prime}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Omega_{\\tilde{\\lambda}^{\\prime}}(\\theta;\\Lambda)=\\Lambda\\big(\\Omega_{\\lambda_{f}}(\\theta;\\Lambda)\\big)+\\Lambda B_{h}^{\\prime\\intercal}\\big(\\mathbb{R}^{M_{h}}\\big)$ . The set $\\Lambda\\big(\\Omega_{\\lambda_{f}}(\\theta;\\Lambda)\\big)$ can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\Lambda}\\big(\\Omega_{\\lambda_{f}}(\\theta;\\Lambda)\\big)=\\bigr\\{\\Lambda\\lambda_{f}\\ |\\ \\lambda_{f}\\in\\ensuremath{\\mathbb{R}}_{+}^{M},\\lambda_{f}{}^{\\top}\\Lambda F(\\theta)=\\mathbf{1}^{\\top}\\Lambda F(\\theta)\\bigr\\}}\\\\ &{\\qquad\\qquad\\qquad=\\bigr\\{\\lambda_{f}^{\\prime}\\in\\ensuremath{\\mathbb{R}}_{+}^{M}\\ |\\ F(\\theta)^{\\top}\\lambda_{f}{}^{\\prime}=\\mathbf{1}^{\\top}\\Lambda F(\\theta)\\bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that, ", "page_idx": 20}, {"type": "equation", "text": "$$\nF(\\theta)^{\\top}\\lambda_{f}{'}=\\mathbf{1}^{\\top}\\Lambda F(\\theta)=\\mathbf{1}^{\\top}F(\\theta)c_{s}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $c_{s}=\\mathbf{1}^{\\top}\\Lambda F({\\boldsymbol{\\theta}})/(\\mathbf{1}^{\\top}F({\\boldsymbol{\\theta}}))$ . Therefore, $\\Lambda\\bigl(\\Omega_{\\lambda_{f}}(\\theta;\\Lambda)\\bigr)=c_{s}\\bigl(\\Omega_{\\lambda_{f}}(\\theta)\\bigr).$ ", "page_idx": 20}, {"type": "text", "text": "Also note that, $B_{h}\\in\\mathbb{R}^{(M-1)\\times M}$ is full row rank, and is selected based on $F(\\theta)$ , which satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\nB_{h}(F(\\theta_{1})-F(\\theta_{2}))=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $F({\\theta}_{1}),F({\\theta}_{2})$ are two reference points which fully defines the kernel of $B_{h}$ . Similarly, when $F(\\theta)$ is scaled by $\\Lambda$ , the corresponding $B_{h}^{\\prime}$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\nB_{h}^{\\prime}\\Lambda(F(\\theta_{1})-F(\\theta_{2}))=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This further implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\Lambda B_{h}^{\\prime}}^{\\top}({\\mathbb R}^{M_{h}})=\\mathrm{range}({\\Lambda B_{h}^{\\prime}}^{\\top})=\\ker(B_{h}^{\\prime}\\Lambda)^{\\perp}=\\ker(B_{h})^{\\perp}=B_{h}({\\mathbb R}^{M_{h}})=c_{s}B_{h}({\\mathbb R}^{M_{h}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining with $\\Lambda\\big(\\Omega_{\\lambda_{f}}(\\theta;\\Lambda)\\big)=c_{s}\\big(\\Omega_{\\lambda_{f}}(\\theta)\\big)$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Omega_{\\tilde{\\lambda}^{\\prime}}(\\theta;\\Lambda)=c_{s}\\Omega_{\\tilde{\\lambda}}(\\theta).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, the solution of $\\tilde{\\lambda}$ and $\\lambda^{\\prime}$ is only subject to a scaling factor, which does not change the direction of $d^{*}(\\theta)$ . This proves Property-4, the scale invariance. ", "page_idx": 21}, {"type": "text", "text": "D.3.2 Proof of Lemma 2: calmness of PMOL ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Example 1. Let $F:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}^{2}$ . Consider the problem below as a special case of (PMOL), given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\mathbb{R}_{+}^{2}}\\,F(\\theta)\\ \\mathrm{{s.t.}}\\ \\,f_{2}(\\theta)=\\operatorname*{min}{f_{2}(\\theta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $\\bar{\\theta}=\\arg\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{q}}\\,f_{2}(\\theta)$ , we have $\\nabla f_{2}({\\bar{\\theta}})=0$ , and $\\bar{\\theta}$ satisfies (D.7) with $\\lambda=[0,1]^{\\top}\\neq0$ and $\\lambda_{h}=1$ . However, $\\nabla H({\\bar{\\theta}})=\\nabla f_{2}({\\bar{\\theta}})={\\bar{0}}$ violates the LICQ, the Slater\u2019s $C Q$ , and the MFCQ. ", "page_idx": 21}, {"type": "text", "text": "Below we restate the definition of the Calmness condition for PMOL [43], which generalizes the calmness condition in single-objective optimization. ", "page_idx": 21}, {"type": "text", "text": "Definition 9 (Calmness condition for PMOL [43, Restatement of Definition 4.5]). Let $\\bar{\\theta}$ be a local solution to (PMOL). We say the PMOL problem satisfies the calmness condition at $\\bar{\\theta}$ provided that there exists $\\epsilon>0$ and a Lipschitz function $\\phi:\\mathbb{R}^{M_{g}\\breve{+}M_{h}}\\ \\to\\mathbb{R}^{M}$ satisfying $\\phi(0,0)=0$ such that there exists no $(\\theta,p,q)\\in[(\\bar{\\theta},0,0)\\,\\bar{+}\\,\\epsilon\\mathcal{B}]/\\{(\\bar{\\theta},0,0)\\}$ satisfying ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\theta)+p\\leq0,}\\\\ &{H(\\theta)+q=0,}\\\\ &{F(\\theta)-F(\\bar{\\theta})+\\phi(p,q)\\in-\\mathrm{int}(C_{A}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Our proof relies on the following general version of Hoffman error bound, which bounds the distance of a point to a nonempty solution set defined by constraints by a measure of the constraint violation of the point. ", "page_idx": 21}, {"type": "text", "text": "Lemma 5 (Relative form of Hoffman error bound [37, Proposition 5]). Given $B_{h}\\in\\mathbb{R}^{k_{H}\\times M},b_{h}\\in$ $\\mathbb{R}^{k_{H}}$ , $B_{g}\\in\\mathbb{R}^{k_{G}\\times M},b_{g}\\in\\mathbb{R}^{k_{G}}$ , define $\\Sigma(p,q):=\\{y\\in\\mathbb{R}^{\\hat{M}}\\ |\\ B_{g}y+b_{g}\\leq p,B_{h}y+b_{h}=q\\}$ , and dom $\\Sigma:=\\{(p,q)\\mid\\Sigma(p,q)\\neq\\emptyset\\}$ . Let $\\Omega_{R}\\subseteq\\mathbb{R}^{M}$ be a reference polyhedron (e.g., one defined by the intersection of half-spaces). Then for all $u\\in\\Omega_{R},$ , and $(p,q)\\in\\mathrm{dom}\\;\\Sigma,$ , there exists a relative Hoffman constant $c_{\\mathrm{hof}}$ depending only on $B_{g},B_{h},\\Omega_{R}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{dist}(u,\\Sigma(p,q)\\cap\\Omega_{R})\\leq c_{\\mathrm{hof}}(B_{g},B_{h}\\mid\\Omega_{R})\\left\\|\\left[\\left(B_{g}u+b_{g}-p\\right)_{+}\\right]\\right\\|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(B_{g}u+b_{g}-p)_{+}:=\\operatorname*{max}\\{0,B_{g}u+b_{g}-p\\}$ which replaces each negative component of $B_{g}u+b_{g}-p$ by zero, and $\\mathrm{dist}(u,\\Omega):=\\operatorname*{inf}_{u^{\\prime}\\in\\Omega}\\|u-u^{\\prime}\\|$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 2. We first construct $\\phi(p,q)=\\overline{{{c_{\\mathrm{hof}}}}}\\|[p^{\\top},q^{\\top}]^{\\top}\\|A^{-1}\\mathbf{1}_{M}.$ , where $\\overline{{c_{\\mathrm{hof}}}}$ is the Hoffman constant upper bound in Lemma 5. Then $\\phi(0,0)=0$ , and $\\phi(p,q)$ is Lipschitz because ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\phi(p,q)-\\phi(p^{\\prime},q^{\\prime})\\|\\leq\\!\\!\\frac{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!-1}{{\\cal{c}}_{\\mathrm{hof}}}{\\cal{M}}\\|A^{-1}\\|\\left\\|\\left[\\!\\!\\!\\left[p\\right]\\!\\!\\!\\right]\\right\\|-\\left\\|\\left[\\!\\!\\!\\left[p^{\\prime}\\right]\\!\\!\\!\\right]\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next we prove the PMOL calmness condition holds by contradiction. Suppose for every $\\epsilon>0$ , there exists $(\\hat{\\theta},p,q)\\in[(\\bar{\\theta},0,0)+\\epsilon\\mathcal{B}]/\\{(\\bar{\\theta},0,0)\\}$ satisfying (D.34). ", "page_idx": 21}, {"type": "text", "text": "Define $\\Omega_{F_{1}}\\,:=\\,\\{F(\\theta)\\,\\in\\,\\Sigma(0,0)\\,\\mid\\,\\theta\\,\\in\\,\\mathbb{R}^{q}\\}\\not=\\,\\emptyset.$ , there exists $\\tilde{\\theta}\\,\\in\\,\\mathbb{R}^{q}$ such that $F(\\tilde{\\theta})\\,\\in\\,\\Omega_{F_{1}}$ and $\\|F({\\tilde{\\theta}})\\|<\\infty$ . We then consider the following two cases: ", "page_idx": 21}, {"type": "text", "text": "Case 1 $:F({\\hat{\\theta}})\\in\\Sigma(0,0)$ . In this case, $(\\hat{\\theta},p,q)=(\\hat{\\theta},0,0)\\neq(\\bar{\\theta},0,0)$ , thus ${\\hat{\\theta}}\\neq{\\bar{\\theta}}$ . Take $\\tilde{\\theta}=\\hat{\\theta}\\neq\\bar{\\theta}$ .   \nCase 2: $F({\\hat{\\theta}})\\not\\in\\Sigma(0,0)$ . Take $\\tilde{\\theta}$ such that $F(\\tilde{\\theta})\\in\\Omega_{F_{1}}$ , then $F({\\tilde{\\theta}})\\neq F({\\hat{\\theta}})$ . ", "page_idx": 22}, {"type": "text", "text": "In both cases, let $\\Omega_{R}$ be the convex hull of $\\{F({\\hat{\\theta}}),F({\\tilde{\\theta}})\\}$ , i.e., $\\Omega_{R}=\\operatorname{conv}(\\{F(\\tilde{\\theta}),F(\\hat{\\theta})\\})$ . Then $\\Omega_{R}$ is a line segment (or reduces to a point in case $^{\\,l}$ ), thus a polyhedron. Since $\\Sigma(0,0)$ is a line, $F(\\ensuremath{\\tilde{\\theta}})\\in\\Sigma(0,0)\\cap\\Omega_{R}$ , thus $\\Sigma(0,0)\\cap\\Omega_{R}=\\Omega_{R}=\\{F(\\tilde{\\theta})\\}$ in case $^{\\,l}$ , and $\\Sigma(0,0)\\cap\\Omega_{R}=\\{F(\\tilde{\\theta})\\}$ in case 2. Therefore, in both cases, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Vert F(\\tilde{\\theta})-F(\\hat{\\theta})\\Vert=\\mathrm{dist}(F(\\hat{\\theta}),\\Sigma(0,0)\\cap\\Omega_{R})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathrm{dist}(F,\\Omega):=\\operatorname*{inf}_{F^{\\prime}\\in\\Omega}\\|F-F^{\\prime}\\|$ . ", "page_idx": 22}, {"type": "text", "text": "We also have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{dist}(F(\\hat{\\theta}),\\Sigma(0,0)\\cap\\Omega_{R})\\stackrel{(a)}{\\leq}c_{\\mathrm{hof}}(\\Omega_{R})\\left\\|\\left[\\left(B_{g}F(\\hat{\\theta})+b_{g}\\right)_{+}\\right]\\right\\|}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\overline{{c}}_{\\mathrm{hof}}\\left\\|\\left[\\left(-p\\right)_{+}\\right]\\right\\|\\leq\\overline{{c}}_{\\mathrm{hof}}\\left\\|\\left[\\underline{{p}}\\right]\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $(a)$ follows from Lemma 5; $(b)$ follows from (D.34) that $0\\,\\leq\\,(B_{g}F(\\hat{\\theta})+b_{g})_{+}\\,\\leq\\,(-p)_{+}$ , $B_{h}F(\\hat{\\theta})+b_{h}=-q,$ , and that $c_{\\mathrm{hof}}(\\Omega_{R})\\leq\\overline{{c_{\\mathrm{hof}}}}$ for different bounded $\\Omega_{R}$ . Multiplying $\\|A\\|\\mathbf{1}_{M}$ on both sides of the above inequality yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lVert A\\rVert\\mathrm{dist}(F(\\hat{\\theta}),\\Sigma(0,0)\\cap\\Omega_{R})\\mathbf{1}_{M}\\leq A\\phi(p,q).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It can then be derived that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A F(\\tilde{\\theta})-A F(\\hat{\\theta})\\leq\\lVert A F(\\tilde{\\theta})-A F(\\hat{\\theta})\\rVert\\mathbf{1}_{M}\\leq\\lVert A\\rVert\\lVert F(\\tilde{\\theta})-F(\\hat{\\theta})\\rVert\\mathbf{1}_{M}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\lVert A\\rVert\\mathrm{dist}(F(\\hat{\\theta}),\\Sigma(0,0)\\cap\\Omega_{R})\\mathbf{1}_{M}\\leq A\\phi(p,q).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By rearranging the above inequality and applying (D.34c), we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\nA F(\\tilde{\\theta})\\leq A F(\\hat{\\theta})+A\\phi(p,q)<A F(\\bar{\\theta})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which contradicts to that $\\bar{\\theta}$ is a global solution to (PMOL). ", "page_idx": 22}, {"type": "text", "text": "Therefore, the PMOL calmness condition in Definition 9 is satisfied. ", "page_idx": 22}, {"type": "text", "text": "E Proof of Theorem 1: convergence of Algorithm 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Recall that, we let $\\lambda=[\\lambda_{f};\\lambda_{g};\\lambda_{h}]\\in\\mathbb{R}^{M+M_{g}+M_{h}}$ , $A_{a g}=[A;B_{g};B_{h}]\\in\\mathbb{R}^{(M+M_{g}+M_{h})\\times M}$ , and use the following concise notation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{*}(\\theta)=-\\nabla F(\\theta)A_{a g}^{\\top}\\lambda^{*}(\\theta)}\\\\ &{\\mathrm{s.t.}\\ \\lambda^{*}(\\theta)\\in\\underset{\\lambda\\in\\Omega_{\\lambda}(\\theta)}{\\operatorname{arg\\,min}}\\,\\varphi(\\lambda;\\theta):=\\cfrac{1}{2}\\|\\nabla F(\\theta)A_{a g}^{\\top}\\lambda\\|^{2}-c_{g}\\lambda_{g}^{\\top}G(\\theta)-c_{h}\\lambda_{h}^{\\top}H(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the following discussion in this section, we first present the supporting lemmas and their proofs, then provide the proof of Theorem 1. ", "page_idx": 22}, {"type": "text", "text": "E.1 Auxiliary lemmas ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 6 is a result from the smoothness of $F(\\theta)$ , and thus the smoothness of $G(\\theta)$ and $H(\\theta)$ , whose smoothness constants depend on $B_{g}$ and $B_{h}$ , respectively. ", "page_idx": 22}, {"type": "text", "text": "Lemma 6. Suppose Assumptions 1, 2 hold. Then for all $\\theta,\\theta^{\\prime}\\in\\mathbb{R}^{q}$ , and all $\\lambda_{f}\\in\\mathbb{R}^{M}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\lambda_{f}^{\\top}A F(\\theta_{t+1})-\\lambda_{f}^{\\top}A F(\\theta_{t})\\le\\!\\alpha_{t}\\lambda_{f}^{\\top}A\\nabla F(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}\\|A^{\\top}\\lambda_{f}\\|_{1}}{2}\\alpha_{t}^{2}\\|d_{t}\\|^{2}}&\\\\ &{\\quad}&{G(\\theta_{t+1})-G(\\theta_{t})\\le\\!\\alpha_{t}\\nabla G(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}\\quad}&\\\\ &{\\quad}&{H(\\theta_{t+1})-H(\\theta_{t})\\le\\!\\alpha_{t}\\nabla H(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By Assumption 2, it holds that $\\lambda_{f}^{\\top}A F(\\theta)$ is $\\|A^{\\top}\\lambda_{f}\\|_{1}\\ell_{f,1}$ -smooth. By the definition of smoothness, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda_{f}^{\\top}A F(\\theta_{t+1})\\leq\\lambda_{f}^{\\top}A F(\\theta_{t})+\\alpha_{t}\\lambda_{f}^{\\top}A\\nabla F(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}\\|A^{\\top}\\lambda_{f}\\|_{1}}{2}\\alpha_{t}^{2}\\|d_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $B_{g,m}$ and $B_{h,m}$ be the $m$ -th row of $B_{g}$ and $B_{h}$ , respectively, then by the $\\ell_{f,1}$ -smoothness of $F(\\theta),\\,B_{g,m}F(\\theta)$ is $\\ell_{f,1}\\|B_{g,m}\\|_{1}$ -smooth for all $m\\in[M_{g}]$ . Also because $\\|B_{g,m}\\|_{1}\\,\\le\\,\\|B_{g}^{\\top}\\|_{\\infty,1}$ where $\\|B_{g}^{\\top}\\|_{\\infty,1}=\\operatorname*{max}_{m\\in M_{g}}\\|\\|B_{g,m}\\|_{1}\\|,g_{m}(\\theta)$ is $\\ell_{f,1}\\|B_{g}^{\\top}\\|_{\\infty,1}$ -smooth for all $m\\in[M_{g}]$ . By the definition of smoothness, it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\nG(\\theta_{t+1})-G(\\theta_{t})\\leq\\alpha_{t}\\nabla G(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Following similar arguments as the above for $G(\\theta)$ , (E.4) can be proved. ", "page_idx": 23}, {"type": "text", "text": "Lemma 7. For the subprogram (2.3) or equivalently (E.1), it holds that for any $\\lambda\\in\\Omega_{\\lambda}(\\theta)$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\langle\\nabla F(\\theta)A_{a g}^{\\top}\\lambda,\\nabla F(\\theta)A_{a g}^{\\top}\\lambda^{*}(\\theta)\\rangle-[0^{\\top},c_{g}G(\\theta)^{\\top},c_{h}H(\\theta)^{\\top}](\\lambda-\\lambda^{*}(\\theta))\\geq\\|\\nabla F(\\theta)A_{a g}^{\\top}\\lambda^{*}(\\theta)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 7. Since $\\varphi(\\lambda;\\theta)$ is a convex function w.r.t. $\\lambda$ , by the first order optimality condition, it holds that for all $\\lambda\\in\\Omega_{\\lambda}(\\theta)$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\langle\\nabla_{\\lambda}\\varphi(\\lambda^{\\ast}(\\theta);\\theta),\\lambda-\\lambda^{\\ast}(\\theta)\\rangle\\geq0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which can be further written as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda^{\\top}A_{a g}\\nabla F(\\theta)^{\\top}\\nabla F(\\theta)A_{a g}^{\\top}\\lambda^{*}(\\theta)-[0^{\\top},c_{g}G(\\theta)^{\\top},c_{h}H(\\theta)^{\\top}](\\lambda-\\lambda^{*}(\\theta))\\geq\\|\\nabla F(\\theta)A_{a g}^{\\top}\\lambda^{*}(\\theta)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "We next prove Lemma 8, which can be viewed as a descent lemma for $\\left[G(\\theta)\\right]_{+}$ and $|H(\\theta)|_{\\mathrm{ab}}$ based on the smoothness of $G(\\theta)$ and $H(\\theta)$ , as well as proper hyperparameter choices. This is crucial for proving the convergence result in Theorem 1. One key technical challenge in proving the lemma is that even though $G(\\theta)$ and $H(\\theta)$ are smooth, $[G(\\theta)]_{+}$ and $|H(\\theta)|_{\\mathrm{ab}}$ are not. We address this challenge by exploiting the fact that $\\nabla G(\\theta_{t})^{\\top}d^{*}(\\theta_{t})\\leq-c_{g}G(\\theta_{t})$ and $\\dot{\\nabla}H(\\theta_{t})^{\\top}d^{*}(\\theta_{t})=-c_{g}H(\\theta_{t})$ , as well as choosing $\\alpha_{t}$ properly depending on $c_{g}$ and $c_{h}$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 8. Let $\\epsilon\\,\\geq\\,0$ be a constant. Define $[y]_{+}:=\\,\\operatorname*{max}\\{y,0\\}$ which replaces each negative component of $y$ by zero, and $|y|_{\\mathrm{ab}}$ replaces each component of $y$ by its absolute value. Let $\\{\\theta_{t}\\}$ be the sequence produced by Algorithm $^{\\,l}$ with the update $\\theta_{t+1}=\\theta_{t}+\\alpha_{t}d_{t}$ , where $d_{t}$ satisfies the constraints of the subprogram (2.1) up to an error of \u03f5, i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\nabla G(\\theta_{t})^{\\top}d_{t}+c_{g}G(\\theta_{t})]_{+}\\leq\\epsilon{\\mathbf1},}\\\\ &{|\\nabla H(\\theta_{t})^{\\top}d_{t}+c_{h}H(\\theta_{t})|_{\\mathrm{ab}}\\leq\\epsilon{\\mathbf1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\dot{\\alpha}_{t}\\leq\\operatorname*{min}\\{c_{g}^{-1},c_{h}^{-1}\\}$ , then it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad[G(\\theta_{t+1})]_{+}-[G(\\theta_{t})]_{+}\\leq-\\,\\alpha_{t}c_{g}[G(\\theta_{t})]_{+}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}+\\epsilon{\\bf1}}\\\\ &{\\quad[H(\\theta_{t+1})]_{\\mathrm{ab}}-|H(\\theta_{t})|_{\\mathrm{ab}}\\leq-\\,\\alpha_{t}c_{h}|H(\\theta_{t})|_{\\mathrm{ab}}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}+\\epsilon{\\bf1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By the smoothness of $G(\\theta)$ in Lemma 6 and $\\nabla G(\\theta)^{\\top}d\\!+\\!c_{g}G(\\theta)\\leq[\\nabla G(\\theta)^{\\top}d\\!+\\!c_{g}G(\\theta)]_{+}\\leq$ $\\epsilon\\mathbf{1}$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G(\\theta_{t+1})-G(\\theta_{t})\\leq\\!\\alpha_{t}\\nabla G(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}+\\epsilon{\\bf1}}\\\\ {\\leq-\\,\\alpha_{t}c_{g}G(\\theta_{t})+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}+\\epsilon{\\bf1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For all $m\\in[M_{g}]$ , since $G(\\theta_{t})\\leq[G(\\theta_{t})]_{+}$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{m}(\\theta_{t+1})-[g_{m}(\\theta_{t})]_{+}\\leq g_{m}(\\theta_{t})-[g_{m}(\\theta_{t})]_{+}-\\alpha_{t}c_{g}g_{m}(\\theta_{t})+\\displaystyle\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}+\\epsilon}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq-\\left[-g_{m}(\\theta_{t})\\right]_{+}-\\alpha_{t}c_{g}g_{m}(\\theta_{t})+\\displaystyle\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It can be further derived that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-[-g_{m}(\\theta_{t})]_{+}-\\alpha_{t}c_{g}g_{m}(\\theta_{t})=\\left\\{\\begin{array}{l l}{-\\alpha_{t}c_{g}g_{m}(\\theta_{t}),g_{m}(\\theta_{t})\\geq0}\\\\ {(1-\\alpha_{t}c_{g})g_{m}(\\theta_{t}),g_{m}(\\theta_{t})<0}\\end{array}\\right.}\\\\ &{~~~~~~~~~~~~~~~~~~~~\\leq-\\left.\\alpha_{t}c_{g}[g_{m}(\\theta_{t})]_{+}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality holds since $1-\\alpha_{t}c_{g}\\geq0$ . Plugging this inequality back into (E.16), yields that when $g_{m}\\bigl(\\theta_{t+1}\\bigr)\\geq\\dot{0}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n[g_{m}(\\theta_{t+1})]_{+}-[g_{m}(\\theta_{t})]_{+}\\leq-\\alpha_{t}c_{g}[g_{m}(\\theta_{t})]_{+}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}+\\epsilon.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $g_{m}(\\theta_{t+1})<0$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{[g_{m}(\\theta_{t+1})]_{+}-[g_{m}(\\theta_{t})]_{+}\\leq-[g_{m}(\\theta_{t})]_{+}\\leq-\\alpha_{t}c_{g}[g_{m}(\\theta_{t})]_{+}}\\\\ {\\leq-\\alpha_{t}c_{g}[g_{m}(\\theta_{t})]_{+}+\\displaystyle\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining (E.18) and (E.19) proves (E.12). ", "page_idx": 24}, {"type": "text", "text": "By the smoothness of $H(\\theta)$ and $|\\nabla H(\\theta)^{\\top}d+c_{h}H(\\theta)|_{\\mathrm{ab}}\\leq\\epsilon{\\bf1}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|H(\\theta_{t+1})|_{\\mathrm{ab}}\\leq|H(\\theta_{t})-\\alpha_{t}c_{h}H(\\theta_{t})|_{\\mathrm{ab}}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}+\\epsilon{\\bf1}}\\\\ &{}&{\\quad\\quad\\quad=(1-\\alpha_{t}c_{h})|H(\\theta_{t})|_{\\mathrm{ab}}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}+\\epsilon{\\bf1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last equality holds because $1-\\alpha_{t}c_{h}\\geq0$ , which proves (E.13). ", "page_idx": 24}, {"type": "text", "text": "E.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 1. To consider both objective function minimization and constraint satisfaction, we define a Lyapunov function below with a constant vector $\\lambda=(\\lambda_{f},\\lambda_{g},\\lambda_{h})\\in\\Omega_{\\lambda}(\\theta)$ , where $\\lambda_{f}=1$ , $\\lambda_{g}\\in\\mathbb{R}_{+}^{M_{g}}$ , $\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ , and $\\lambda_{g}>\\lambda_{g}^{*}(\\theta_{t}),\\lambda_{h}>\\lambda_{h}^{*}(\\theta_{t})$ for all $t\\in[T]$ . ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{V}_{t}:=\\underbrace{\\lambda_{f}^{\\top}A F(\\theta_{t})}_{\\mathbb{V}_{f,t}}+\\underbrace{\\lambda_{g}^{\\top}[G(\\theta_{t})]_{+}}_{\\mathbb{V}_{g,t}}+\\underbrace{\\lambda_{h}^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}}_{\\mathbb{V}_{h,t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\mathbb{V}_{t}\\geq0$ for all $t$ since $A F(\\theta)\\ge0,\\lambda_{f}\\ge0$ . ", "page_idx": 24}, {"type": "text", "text": "For notation simplicity, we let $d_{t}^{*}=d^{*}(\\theta_{t})$ . From Assumption 2, the smoothness of the objectives, and Lemma 6, based on the update $\\theta_{t+1}=\\theta_{t}+\\alpha_{t}d_{t}$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{V}_{f,t+1}-\\mathbb{V}_{f,t}\\overset{(a)}{\\leq}\\alpha_{t}\\lambda_{f}^{\\top}A\\nabla F(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}\\lambda_{f}^{\\top}\\mathbf{1}}\\\\ &{\\overset{(b)}{\\leq}\\alpha_{t}\\lambda_{f}^{\\top}A\\nabla F(\\theta_{t})^{\\top}d_{t}^{\\ast}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A^{\\top}\\|_{\\infty,1}\\|d_{t}^{\\ast}\\|^{2}\\lambda_{f}^{\\top}\\mathbf{1}+\\epsilon\\mathbf{1}}\\\\ &{\\overset{(c)}{\\leq}-\\alpha_{t}\\|d_{t}^{\\ast}\\|^{2}+\\alpha_{t}\\big(c_{g}\\lambda_{g}^{\\ast}(\\theta_{t})^{\\top}G(\\theta_{t})+c_{h}\\lambda_{h}^{\\ast}(\\theta_{t})^{\\top}H(\\theta_{t})\\big)+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A^{\\top}\\mathbf{1}\\|_{1}\\|d_{t}^{\\ast}\\|^{2}+\\epsilon\\mathbf{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(a)$ follows Lemma 6; $(b)$ follows from that $d_{t}$ is an $\\epsilon$ -optimal solution to the subprogram; $(c)$   \nfollows from Lemma 7 by setting $\\lambda=[\\mathbf{1}^{\\top},\\mathbf{0}^{\\top},\\mathbf{0}^{\\top}]^{\\top}\\in\\Omega_{\\lambda}({\\boldsymbol{\\theta}})$ . ", "page_idx": 24}, {"type": "text", "text": "From Lemma 8, for $\\alpha_{t}\\leq\\operatorname*{min}\\{c_{g}^{-1},c_{h}^{-1}\\}$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{V}_{g,t+1}-\\mathbb{V}_{g,t}\\leq-\\alpha_{t}c_{g}\\lambda_{g}^{\\top}[G(\\theta_{t})]_{+}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{g}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}\\lambda_{g}^{\\top}\\mathbf{1}+\\epsilon\\lambda_{g}^{\\top}\\mathbf{1}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{V}_{h,t+1}-\\mathbb{V}_{h,t}\\leq-\\alpha_{t}c_{h}\\lambda_{h}^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}\\lambda_{h}^{\\top}\\mathbf{1}+\\epsilon\\lambda_{h}^{\\top}\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the above inequalities for $\\mathbb{V}_{f,t},\\mathbb{V}_{g,t},\\mathbb{V}_{h,t}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{t+1}-\\mathbb{V}_{t}\\leq-\\alpha_{t}\\|d_{t}^{*}\\|^{2}+\\alpha_{t}\\big(c_{g}\\lambda_{g}^{*}(\\theta_{t})^{\\top}G(\\theta_{t})+c_{h}\\lambda_{h}^{*}(\\theta_{t})^{\\top}H(\\theta_{t})\\big)}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A_{a g}^{\\top}\\lambda\\|_{1}\\|d_{t}^{*}\\|^{2}-\\alpha_{t}c_{g}\\lambda_{g}^{\\top}[G(\\theta_{t})]_{+}-\\alpha_{t}c_{h}\\lambda_{h}^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}+\\epsilon\\lambda^{\\top}\\mathbf{1}}\\\\ &{\\quad\\quad\\quad\\quad\\leq-\\alpha_{t}\\|d_{t}^{*}\\|^{2}-\\alpha_{t}c_{g}(\\lambda_{g}-\\lambda_{g}^{*}(\\theta_{t}))^{\\top}[G(\\theta_{t})]_{+}-\\alpha_{t}c_{g}\\lambda_{g}^{*}(\\theta_{t})^{\\top}[-G(\\theta_{t})]_{+}}\\\\ &{\\quad\\quad\\quad\\quad-\\alpha_{t}c_{h}(\\lambda_{h}-\\lambda_{h}^{*}(\\theta_{t}))^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A_{a g}^{\\top}\\lambda\\|_{1}\\|d_{t}^{*}\\|^{2}+\\epsilon\\lambda^{\\top}\\mathbf{1}\\quad\\quad\\mathrm{~(2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality holds because $\\lambda_{g}^{*}(\\theta_{t})^{\\top}G(\\theta_{t})=\\lambda_{g}^{*}(\\theta_{t})^{\\top}[G(\\theta_{t})]_{+}-\\lambda_{g}^{*}(\\theta_{t})^{\\top}[-G(\\theta_{t})]_{+}.$ . Taking telescoping sum of the above inequality from $t=0,\\dots,T-1$ and rearranging, we have $\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\alpha_{t}\\Big(1-\\frac12\\|A_{a g}^{\\top}\\lambda\\|_{1}\\ell_{f,1}\\alpha_{t}\\Big)\\|d_{t}^{*}\\|^{2}+\\alpha_{t}c_{g}(\\lambda_{g}-\\lambda_{g}^{*}(\\theta_{t}))^{\\top}[G(\\theta_{t})]_{+}+\\alpha_{t}c_{g}\\lambda_{g}^{*}(\\theta_{t})^{\\top}[-G(\\theta_{t})]_{+}}\\\\ &{+\\alpha_{t}c_{h}(\\lambda_{h}-\\lambda_{h}^{*}(\\theta_{t}))^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}\\le\\mathbb{V}_{0}-\\mathbb{V}_{T}+T\\epsilon\\lambda^{\\top}\\mathbf1\\le\\mathbb{V}_{0}+T\\epsilon\\|\\lambda\\|_{1}.}&{~~~~~~~~~~~~~~~~~~~~(\\mathbb{E}.26)}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Recall that $\\alpha_{t}\\leq1/(\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1})$ . Plugging this into the above inequality yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\frac{1}{2}\\alpha_{t}\\|d_{t}^{*}\\|^{2}+\\alpha_{t}c_{g}(\\lambda_{g}-\\lambda_{g}^{*}(\\theta_{t}))^{\\top}[G(\\theta_{t})]_{+}+\\alpha_{t}c_{g}\\lambda_{g}^{*}(\\theta_{t})^{\\top}[-G(\\theta_{t})]_{+}}\\\\ &{\\displaystyle+\\alpha_{t}c_{h}(\\lambda_{h}-\\lambda_{h}^{*}(\\theta_{t}))^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}\\leq\\mathbb{V}_{0}+T\\epsilon\\|\\lambda\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking $\\alpha_{t}=\\Theta(1)$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{2}\\|d^{*}(\\theta_{t})\\|^{2}+c_{g}(\\lambda_{g}-\\lambda_{g}^{*}(\\theta_{t}))^{\\top}[G(\\theta_{t})]_{+}+c_{g}\\lambda_{g}^{*}(\\theta_{t})^{\\top}[-G(\\theta_{t})]_{+}}\\\\ &{\\displaystyle+\\,c_{h}(\\lambda_{h}-\\lambda_{h}^{*}(\\theta_{t}))^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}=\\mathcal{O}\\Big(\\frac{1}{T}+\\epsilon\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The proof is complete. ", "page_idx": 25}, {"type": "text", "text": "Next we show that the subprogram converges with a projected gradient descent (PGD) algorithm on $\\lambda$ with $K$ iterations. ", "page_idx": 25}, {"type": "text", "text": "Lemma 9 (Convergence of the subprogram with projected gradient descent). At the t-th iteration, given $\\theta_{t}$ , let $\\{\\lambda_{t,k}\\}_{k}$ be the sequence generated by the projected gradient descent algorithm to solve the subprogram $\\begin{array}{r}{\\operatorname*{min}_{\\lambda\\in\\Omega_{\\lambda}(\\theta_{t})}\\varphi(\\lambda;\\theta_{t}),}\\end{array}$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\varphi(\\lambda_{t,K};\\theta_{t})-\\operatorname*{min}_{\\lambda\\in\\Omega_{\\lambda}(\\theta_{t})}\\varphi(\\lambda;\\theta_{t})\\leq\\frac{\\|\\lambda_{t,0}-\\lambda^{*}(\\theta_{t})\\|^{2}}{2\\gamma K}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The result follows from the convergence result of projected gradient descent for convex objective functions. Note that at each iteration $t$ , given $\\theta_{t}$ , $\\bar{\\Omega_{\\lambda}}\\bar{(\\theta_{t})}$ is fixed. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 10. Suppose Assumption 3 holds. Due to the $\\ell_{\\varphi_{\\lambda},1}$ -smoothness and the convexity of the subprogram, it holds for all $\\lambda\\in\\Omega_{\\lambda}(\\theta)$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\lambda}\\varphi(\\lambda;\\theta)-\\nabla_{\\lambda}\\varphi(\\lambda^{*}(\\theta);\\theta)\\|^{2}\\leq2\\ell_{\\varphi_{\\lambda},1}\\big(\\varphi(\\lambda;\\theta)-\\varphi(\\lambda^{*}(\\theta);\\theta)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Since the objectives $f_{m}(\\theta)$ are Lipschitz continuous for all $m\\:\\in\\:[M]$ , the subprogram objective $\\varphi(\\lambda;\\theta)$ is $\\ell_{\\varphi,\\lambda,1}$ -smooth w.r.t. $\\lambda$ . By Proposition 1 (b) in [44], it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{2\\ell_{\\varphi_{\\lambda,1}}}\\bigl\\|\\nabla_{\\lambda}\\varphi(\\lambda;\\theta)-\\nabla_{\\lambda}\\varphi(\\lambda^{*}(\\theta);\\theta)\\bigr\\|^{2}+\\bigl\\langle\\nabla_{\\lambda}\\varphi(\\lambda^{*}(\\theta);\\theta),\\lambda-\\lambda^{*}(\\theta)\\bigr\\rangle\\leq\\varphi(\\lambda;\\theta)-\\varphi(\\lambda^{*}(\\theta);\\theta).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the convexity of $\\varphi(\\lambda;\\theta)$ w.r.t. $\\lambda$ , for all $\\lambda\\in\\Omega_{\\lambda}(\\theta)$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\langle\\nabla_{\\lambda}\\varphi(\\lambda^{\\ast}(\\theta);\\theta),\\lambda-\\lambda^{\\ast}(\\theta)\\rangle\\geq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining the above two inequalities proves the result. ", "page_idx": 26}, {"type": "text", "text": "Corollary 7 (Convergence of Algorithm 1 with $K$ -iteration PGD for the subprogram). Suppose Assumptions $^{\\,l}$ , 2 hold. Let $\\{\\boldsymbol{\\theta}_{t}\\}$ be the sequence produced by Algorithm $^{\\,l}$ with the update $\\theta_{t+1}=$ $\\theta_{t}+\\alpha_{t}d_{t}$ , where $d_{t}$ is the $\\epsilon$ -optimal solution to the subprogram (2.1) obtained by $K$ -iteration $P G D$ for the subprogram on $\\lambda.$ . Define $\\lambda:=(\\lambda_{f},\\lambda_{g},\\lambda_{h})\\in\\Omega_{\\lambda}(\\theta)$ with $\\lambda_{g}\\geq\\lambda_{g}^{*}(\\theta)$ , $\\lambda_{h}\\geq\\lambda_{h}^{*}(\\theta)$ for all $\\theta\\in\\mathbb{R}^{q}$ . If the step size $\\alpha_{t}\\leq1/(\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1})$ and $\\alpha_{t}=\\Theta(1)$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=T}^{T-1}\\frac{1}{2}\\|d_{t}^{*}\\|^{2}+c_{g}(\\lambda_{g}-\\lambda_{g}^{*}(\\theta_{t}))^{\\top}[G(\\theta_{t})]_{+}+c_{h}(\\lambda_{h}-\\lambda_{h}^{*}(\\theta_{t}))^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}=\\mathcal{O}(1).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. For $t=0,\\dots,T-1$ , we take $K=T^{2}$ , applying Lemma 9, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\varphi(\\lambda_{t,K};\\theta_{t})-\\operatorname*{min}_{\\lambda\\in\\Omega_{\\lambda}(\\theta_{t})}\\varphi(\\lambda;\\theta_{t})\\leq\\frac{\\|\\lambda_{t,0}-\\lambda^{*}(\\theta_{t})\\|^{2}}{2\\gamma T^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From Lemma 10, the above inequality implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\nabla\\varphi(\\lambda_{t};\\theta_{t})-\\nabla\\varphi(\\lambda^{*}(\\theta_{t});\\theta_{t})|\\|^{2}\\leq2\\ell_{\\varphi_{\\lambda},1}\\big(\\varphi(\\lambda_{t};\\theta_{t})-\\varphi(\\lambda^{*}(\\theta_{t});\\theta_{t})\\big)\\leq\\frac{\\ell_{\\varphi_{\\lambda},1}\\|\\lambda_{t-1}-\\lambda^{*}(\\theta_{t})\\|^{2}}{\\gamma T^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging in the gradient $\\nabla\\varphi(\\lambda_{t};\\theta_{t})$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\|A\\nabla F(\\theta_{t})^{\\top}(d_{t}-d_{t}^{*})\\|^{2}+\\|\\nabla G(\\theta_{t})^{\\top}(d_{t}-d_{t}^{*})\\|^{2}+\\|\\nabla G(\\theta_{t})^{\\top}(d_{t}-d_{t}^{*})\\|^{2}}\\\\ &{\\le\\!\\frac{\\ell_{\\varphi_{\\lambda},1}\\|\\lambda_{t-1}-\\lambda^{*}(\\theta_{t})\\|^{2}}{\\gamma T^{2}}\\le\\frac{4\\ell_{\\varphi_{\\lambda},1}c_{\\lambda}^{2}}{\\gamma T^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\begin{array}{r}{\\epsilon=\\frac{4\\ell_{\\varphi_{\\lambda},1}c_{\\lambda}^{2}}{\\gamma T^{2}}}\\end{array}$ , from Theorem 1, it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{t+1}-\\Psi_{t}\\leq-\\alpha_{t}\\|d_{t}^{*}\\|^{2}+\\alpha_{t}c_{g}\\big(\\lambda_{g}^{*}(\\theta_{t})-\\lambda_{g}\\big)^{\\top}[G(\\theta_{t})]_{+}+\\alpha_{t}c_{h}\\big(\\lambda_{h}^{*}(\\theta_{t})-\\lambda_{h}\\big)^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}+\\epsilon^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad+\\cfrac{1}{2}\\gamma\\alpha_{t}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}+\\cfrac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A_{a g}^{\\top}\\lambda\\|_{1}\\|d_{t}^{*}\\|^{2}.\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathrm{E.}37)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking telescoping sum of the above inequality from $t=0,\\dots,T-1$ , rearranging, and letting $\\alpha_{t}\\leq\\breve{1}/(\\|\\lambda\\|_{1}\\dot{\\ell}_{f,1}\\breve{\\|}A_{a g}^{\\top}\\|_{\\infty,1})$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=T}^{T-1}\\frac{1}{2}\\alpha_{t}\\|d_{t}^{*}\\|^{2}+\\alpha_{t}c_{g}(\\lambda_{g}-\\lambda_{g}^{*}(\\theta_{t}))^{\\top}[G(\\theta_{t})]_{+}+\\alpha_{t}c_{h}(\\lambda_{h}-\\lambda_{h}^{*}(\\theta_{t}))^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}\\leq\\mathbb{V}_{T}+T\\epsilon^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Letting $\\alpha_{t}=\\Theta(1),\\gamma=\\Theta(1)$ yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=T}^{T-1}\\frac{1}{2}\\|d_{t}^{*}\\|^{2}+c_{g}(\\lambda_{g}-\\lambda_{g}^{*}(\\theta_{t}))^{\\top}[G(\\theta_{t})]_{+}+c_{h}(\\lambda_{h}-\\lambda_{h}^{*}(\\theta_{t}))^{\\top}|H(\\theta_{t})|_{\\mathrm{ab}}=\\mathcal{O}(1).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The proof is complete. ", "page_idx": 26}, {"type": "text", "text": "F Proof of Theorems 2 and 3: convergence of Algorithm 2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "F.1 Auxiliary lemmas ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma 11 (Smoothness of $\\varphi$ ). Suppose Assumptions 1 and $3$ hold. $\\varphi(\\lambda;\\theta)$ is $\\ell_{\\varphi_{\\lambda},1}$ -smooth w.r.t. \u03bb, with $\\ell_{\\varphi_{\\lambda},1}=M\\|A_{a g}\\|^{2}\\ell_{f}^{2}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. The Hessian of $\\varphi(\\lambda;\\theta)$ w.r.t. $\\lambda$ can be computed by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\lambda}^{2}\\varphi(\\lambda;\\theta)=A_{a g}\\nabla F(\\theta)^{\\top}\\nabla F(\\theta)A_{a g}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Assumption 3, the Lipschitz continuity of $F$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\lambda}^{2}\\varphi(\\lambda;\\theta)\\|\\leq\\|A_{a g}\\nabla F(\\theta)^{\\top}\\nabla F(\\theta)A_{a g}^{\\top}\\|\\leq\\|\\nabla F(\\theta)A_{a g}^{\\top}\\|^{2}\\leq M\\|A_{a g}\\|^{2}\\ell_{f}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The result is proved. ", "page_idx": 27}, {"type": "text", "text": "Lemma 12 $(\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|$ bounded by $\\|d_{t}\\|$ ). Suppose Assumptions $^{\\,l}$ and $^3$ hold. For $\\{\\boldsymbol{\\theta}_{t}\\}$ produced by Algorithm 2, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|\\leq\\|A^{\\top}\\|_{\\infty,1}\\ell_{f}\\|d_{t}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The gradient of $\\varphi(\\lambda_{t};\\theta_{t})$ w.r.t. $\\lambda_{f}$ can be computed by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})=A\\nabla F(\\theta_{t})^{\\top}\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}=-A\\nabla F(\\theta_{t})^{\\top}d_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Assumption , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|\\leq\\|A^{\\top}\\|_{\\infty,1}\\ell_{f}\\|d_{t}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 13. Let $\\lambda_{t}=[\\lambda_{f,t};\\lambda_{h,t}]$ . Consider the sequence $\\{\\lambda_{t}\\}_{t=1}^{T}$ generated by the update (3.1). Then for all $\\lambda\\in\\Omega_{\\lambda}(\\theta_{t})$ with $\\lambda=(\\lambda_{f},\\lambda_{h})$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\gamma_{t}\\langle\\lambda_{f,t}-\\lambda_{f},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle\\leq\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}+\\gamma_{t}^{2}\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2};}\\\\ &{2\\gamma_{t}\\langle\\lambda_{h,t}-\\lambda_{h},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle=\\|\\lambda_{h,t}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,t+1}-\\lambda_{h}\\|^{2}+\\gamma_{t}^{2}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. By the update of $\\lambda_{f,t}$ , and the non-expansiveness of projection, let $\\lambda_{f}=\\mathbf{1}\\in\\Omega_{\\lambda_{f}}(\\theta)$ for all $\\theta\\in\\mathbb{R}^{q}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}\\leq\\|\\lambda_{f,t}-\\gamma_{t}\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})-\\lambda_{f}\\|^{2}}\\\\ &{=\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-2\\gamma_{t}\\langle\\lambda_{f,t}-\\lambda_{f},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle+\\gamma_{t}^{2}\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Rearranging the above inequality gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\gamma_{t}\\langle\\lambda_{f,t}-\\lambda_{f},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle}\\\\ &{\\leq\\lVert\\lambda_{f,t}-\\lambda_{f}\\rVert^{2}-\\lVert\\lambda_{f,t+1}-\\lambda_{f}\\rVert^{2}+\\gamma_{t}^{2}\\lVert\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rVert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which proves the first inequality. ", "page_idx": 27}, {"type": "text", "text": "By the update of $\\lambda_{h,t}$ , for all constant $\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\lambda_{h,t+1}-\\lambda_{h}\\|^{2}=\\|(\\lambda_{h,t}-\\gamma_{t}\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t}))-\\lambda_{h}\\|^{2}}\\\\ &{=\\|\\lambda_{h,t}-\\lambda_{h}\\|^{2}+\\gamma_{t}^{2}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}-2\\gamma_{t}\\langle\\lambda_{h,t}-\\lambda_{h},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Rearranging the above inequality gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\gamma_{t}\\langle\\lambda_{h,t},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle-2\\gamma_{t}\\langle\\lambda_{h},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle}\\\\ &{=\\!\\|\\lambda_{h,t}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,t+1}-\\lambda_{h}\\|^{2}+\\gamma_{t}^{2}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Corollary 8. Let $\\lambda_{t}=[\\lambda_{f,t};\\lambda_{h,t}]$ . Consider the sequence $\\{\\lambda_{t}\\}_{t=1}^{T}$ generated by the update (3.1). Then for all $\\lambda\\in\\Omega_{\\lambda}(\\theta_{t})$ with $\\lambda=\\left(\\lambda_{f},\\lambda_{h}\\right)$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2\\gamma_{t}\\big(\\varphi(\\lambda_{t};\\theta_{t})-\\varphi(\\lambda;\\theta_{t})\\big)\\leq\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}+\\gamma_{t}^{2}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Corollary 8. The result follows from combining the two inequalities in Lemma 13, and applying the convexity property of $\\varphi$ w.r.t. $\\lambda$ . \u53e3 ", "page_idx": 27}, {"type": "text", "text": "F.2 Analysis with the same merit function: proof of Theorem 2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we provide analysis with the same merit function as Theorem 1. The proof follows similar ideas of the proofs of Theorem 3 and Theorem 5 in [6]. ", "page_idx": 28}, {"type": "text", "text": "We first define the following auxiliary functions to assist our analysis. Note that the functions are only used for analysis but not for the algorithm update. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\varphi_{\\rho}(\\lambda;\\theta):=\\varphi(\\lambda;\\theta)+\\frac{\\rho}{2}\\|\\lambda\\|^{2},\\ \\ \\lambda_{\\rho}^{*}(\\theta):=\\underset{\\lambda\\in\\Omega_{\\lambda}(\\theta)}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\varphi_{\\rho}(\\lambda;\\theta).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We then present the following Lemmas that are useful for the proof of convergence of Algorithm 2. ", "page_idx": 28}, {"type": "text", "text": "Lemma 14. Suppose Assumption $^3$ holds, and $\\lambda^{\\ast}(\\theta)$ and $\\lambda_{\\rho}^{\\ast}(\\theta)$ are bounded for $\\theta\\;\\in\\;\\{\\theta_{t}\\}_{t=0}^{T-1}$ produced by Algorithm 2, i.e., $\\lVert\\lambda^{*}({\\boldsymbol{\\theta}})\\rVert\\leq c_{\\overline{{\\lambda}}},\\,\\lVert\\lambda_{\\rho}^{*}({\\boldsymbol{\\theta}})\\rVert\\leq c_{\\overline{{\\lambda}}}.$ . Then on the trajectory of Algorithm 2, with $\\theta\\in\\{\\theta_{t}\\}_{t=0}^{T-1}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\varphi(\\lambda_{\\rho}^{\\ast}(\\theta);\\theta)-\\varphi(\\lambda^{\\ast}(\\theta);\\theta)\\leq\\frac{\\rho}{2}c_{\\overline{{\\lambda}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 14. The proof follows the proof of [6, Lemma 13]. ", "page_idx": 28}, {"type": "text", "text": "Corollary 9. Suppose Assumption 3 holds, and $\\lambda^{\\ast}(\\theta)$ and $\\lambda_{\\rho}^{\\ast}(\\theta)$ are bounded for $\\theta\\:\\in\\:\\{\\theta_{t}\\}_{t=0}^{T-1}$ produced by Algorithm 2, i.e., $\\lVert\\lambda^{*}({\\boldsymbol{\\theta}})\\rVert\\leq c_{\\overline{{\\lambda}}},\\,\\lVert\\lambda_{\\rho}^{*}({\\boldsymbol{\\theta}})\\rVert\\leq c_{\\overline{{\\lambda}}}.$ . Then on the trajectory of Algorithm 2, with $\\theta\\in\\{\\theta_{t}\\}_{t=0}^{T-1}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda;\\theta)\\|^{2}\\leq2\\ell_{\\varphi_{\\lambda},1}\\big(\\varphi(\\lambda;\\theta)-\\varphi(\\lambda_{\\rho}^{*}(\\theta);\\theta)\\big)+\\ell_{\\varphi_{\\lambda},1}\\rho c_{\\overline{{\\lambda}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Corollary 9. By applying Lemma 10, and that $\\nabla_{\\lambda_{h}}\\varphi(\\lambda^{*}(\\theta);\\theta)=0$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda;\\theta)\\|^{2}=\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda;\\theta)-\\nabla_{\\lambda_{h}}\\varphi(\\lambda^{*}(\\theta);\\theta)\\|^{2}\\leq\\|\\nabla_{\\lambda}\\varphi(\\lambda;\\theta)-\\nabla_{\\lambda}\\varphi(\\lambda^{*}(\\theta);\\theta)\\|^{2}}\\\\ &{\\qquad\\qquad\\overset{\\mathrm{Lemma~}10}{\\leq}2\\ell_{\\varphi\\lambda,1}\\big(\\varphi(\\lambda;\\theta)-\\underset{\\lambda\\in\\Omega_{\\lambda}(\\theta)}{\\operatorname*{min}}\\varphi(\\lambda;\\theta)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying Lemma 14, we can further derive ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varphi(\\lambda;\\theta)-\\underset{\\lambda\\in\\Omega_{\\lambda}(\\theta)}{\\operatorname*{min}}\\varphi(\\lambda;\\theta)=\\varphi(\\lambda;\\theta)-\\varphi(\\lambda^{*}(\\theta);\\theta)+\\varphi(\\lambda_{\\rho}^{*}(\\theta);\\theta)-\\varphi(\\lambda_{\\rho}^{*}(\\theta);\\theta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\underset{\\leq\\epsilon(\\lambda;\\theta)-\\varphi(\\lambda;\\theta)}{\\mathrm{Lemma~}14}-\\varphi(\\lambda_{\\rho}^{*}(\\theta);\\theta)+\\frac{\\rho}{2}c_{\\overline{{\\lambda}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining (F.13) and (F.14) yields the result. ", "page_idx": 28}, {"type": "text", "text": "Lemma 15 (Continuity of $\\lambda_{\\rho}^{\\ast}(\\theta);$ ). For $\\lambda_{\\rho}^{\\ast}(\\theta)$ defined in (F.10), and $\\Omega_{\\lambda}(\\theta)=\\Omega_{\\lambda}$ , the following holds ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\lambda_{\\rho}^{*}(\\theta)-\\lambda_{\\rho}^{*}(\\theta^{\\prime})\\|\\leq\\!\\rho^{-1}\\|\\nabla_{\\lambda}^{2}\\varphi(\\lambda_{\\rho}^{*}(\\theta);\\theta)-\\nabla_{\\lambda}^{2}\\varphi(\\lambda_{\\rho}^{*}(\\theta^{\\prime});\\theta^{\\prime})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\!2\\rho^{-1}\\ell_{f,1}\\ell_{f}\\|A_{a g}^{\\top}\\|_{\\infty,1}^{2}\\|\\theta-\\theta^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 15. The proof follows the proof of [6, Lemma 12]. ", "page_idx": 28}, {"type": "text", "text": "Lemma 16. Suppose Assumptions 1, 2, 3 hold. Let $\\{\\theta_{t}\\},\\{\\lambda_{t}\\}$ be the sequences produced by Algorithm 2 with step sizes $\\alpha_{t}=\\alpha>0$ , $\\gamma_{t}=\\gamma>0$ . Assume $\\lVert\\lambda^{*}({\\boldsymbol{\\theta}}_{t})\\rVert,\\lVert\\lambda_{\\rho}^{*}({\\boldsymbol{\\theta}}_{t})\\rVert,\\lVert\\lambda_{t}\\rVert\\leq c_{\\overline{{\\lambda}}}.$ . Then for any $\\rho>0$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\varphi(\\lambda_{t};\\theta_{t})-\\varphi(\\lambda_{\\rho}^{*}(\\theta_{t});\\theta_{t})\\leq\\frac{2c_{\\lambda}^{2}}{\\gamma T}(1+2\\rho^{-1}\\alpha T\\ell_{f,1}\\ell_{f}^{2}\\|A_{a g}^{\\top}\\|_{\\infty,1}^{3})+\\frac{\\gamma}{2T}\\sum_{t=0}^{T-1}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 16. The proof follows the proof techniques of [6, Lemma 15]. ", "page_idx": 29}, {"type": "text", "text": "First, applying Corollary 8 and $\\gamma_{t}=\\gamma$ yields ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\gamma\\big(\\varphi(\\lambda_{t};\\theta_{t})-\\varphi(\\lambda_{\\rho}^{*}(\\theta_{t});\\theta_{t})\\big)\\leq\\|\\lambda_{t}-\\lambda_{\\rho}^{*}(\\theta_{t})\\|^{2}-\\|\\lambda_{t+1}-\\lambda_{\\rho}^{*}(\\theta_{t})\\|^{2}+\\gamma^{2}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking telescoping sum of the above inequality and rearranging, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\varphi(\\lambda_{t};\\theta_{t})-\\varphi(\\lambda_{\\rho}^{*}(\\theta_{t});\\theta_{t})\\leq\\!\\!\\frac{1}{2\\gamma T}\\Big(\\underbrace{\\displaystyle\\sum_{t=0}^{T-1}\\|\\lambda_{t}-\\lambda_{\\rho}^{*}(\\theta_{t})\\|^{2}-\\|\\lambda_{t+1}-\\lambda_{\\rho}^{*}(\\theta_{t})\\|^{2}}_{I_{1}}\\Big)}\\\\ &{\\quad+\\displaystyle\\frac{\\gamma}{2T}\\sum_{t=0}^{T-1}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $I_{1}$ can be further bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}\\leq\\|\\lambda_{0}-\\lambda_{\\rho}^{*}(\\theta_{0})\\|^{2}-\\|\\lambda_{T}-\\lambda_{\\rho}^{*}(\\theta_{T-1})\\|^{2}+\\displaystyle\\sum_{t=0}^{T-2}\\|2\\lambda_{t+1}-\\lambda_{\\rho}^{*}(\\theta_{t+1})-\\lambda_{\\rho}^{*}(\\theta_{t})\\|\\|\\lambda_{\\rho}^{*}(\\theta_{t+1})-\\lambda_{\\rho}^{*}(\\theta_{t})\\|}\\\\ &{\\quad\\leq4c_{\\lambda}^{2}+4c_{\\lambda}\\displaystyle\\sum_{t=0}^{T-2}\\|\\lambda_{\\rho}^{*}(\\theta_{t+1})-\\lambda_{\\rho}^{*}(\\theta_{t})\\|\\leq4c_{\\lambda}^{2}+8c_{\\lambda}\\displaystyle\\sum_{t=0}^{T-2}\\rho^{-1}\\alpha_{t}\\ell_{f,1}\\ell_{f}\\|A_{\\alpha g}^{\\top}\\|_{\\infty,1}^{2}\\|d_{t}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from Lemma 15 and the update of $\\theta_{t}$ . ", "page_idx": 29}, {"type": "text", "text": "Finally, taking $\\alpha_{t}=\\alpha$ , plugging the above bound for $I_{1}$ back into (F.18), and bounding $\\|d_{t}\\|$ by Assumption 3 and that $\\|\\lambda_{t}\\|\\leq c_{\\bar{\\lambda}}$ prove the result. ", "page_idx": 29}, {"type": "text", "text": "Proof of Theorem 2. We consider the following Lyapunov function with a constant vector $\\lambda\\,=$ $\\left[\\lambda_{f};\\lambda_{h}\\right]\\in\\Omega_{\\lambda}(\\theta)$ , where $\\lambda_{f}=1$ , $\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{V}_{t}:=\\underbrace{\\lambda_{f}^{\\top}\\,A F(\\theta_{t})}_{\\mathbb{V}_{f,t}}+\\underbrace{\\lambda_{h}^{\\top}H(\\theta_{t})}_{\\mathbb{V}_{h,1,t}}+\\underbrace{c_{V_{h}}\\|H(\\theta_{t})\\|_{1}}_{\\mathbb{V}_{h,3,t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Recall that $\\lambda_{t}\\,=\\,[\\lambda_{f,t};\\lambda_{h,t}]$ , and the algorithm takes the update $\\theta_{t+1}\\,=\\,\\theta_{t}\\,+\\,\\alpha_{t}d_{t}$ with $d_{t}\\,=$ $\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}$ . From Assumption 2, the smoothness of the objectives, and Lemma 6, the function $\\lambda_{f}^{\\top}A F(\\theta)$ is smooth, thus ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{f,t+1}-\\mathbb{V}_{f,t}\\leq\\langle\\nabla F(\\theta_{t})A^{\\top}\\lambda_{f},\\theta_{t+1}-\\theta_{t}\\rangle+\\frac{\\ell_{f,1}}{2}\\|A^{\\top}\\lambda_{f}\\|_{1}\\|\\theta_{t+1}-\\theta_{t}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\alpha_{t}\\langle\\nabla F(\\theta_{t})A^{\\top}\\lambda_{f},d_{t}\\rangle+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A^{\\top}\\lambda_{f}\\|_{1}\\|d_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Lemma 13, taking $\\gamma_{t}>0$ and rearranging, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla F(\\theta_{t})A^{\\top}\\lambda_{f},d_{t}\\rangle\\le\\!\\displaystyle\\frac{1}{2\\gamma_{t}}\\big(\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{2}\\gamma_{t}\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}-\\langle\\lambda_{f,t},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining (F.20) and (F.21), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{f,t+1}-\\mathbb{V}_{f,t}\\le\\!\\displaystyle\\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}\\big)+\\displaystyle\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A^{\\top}\\lambda_{f}\\|_{1}\\|d_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{2}\\alpha_{t}\\gamma_{t}\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}-\\alpha_{t}\\langle\\lambda_{f,t},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By the smoothness of $\\lambda_{h}^{\\top}H(\\theta)$ , and $\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})=-\\nabla H(\\theta_{t})^{\\top}d_{t}-c_{h}H(\\theta_{t})$ , it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{V}_{h,1,t+1}-\\mathbb{V}_{h,1,t}\\le\\alpha_{t}\\lambda_{h}^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\lambda_{h}\\|_{1}\\|d_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=-\\,\\alpha_{t}c_{h}\\lambda_{h}^{\\top}H(\\theta_{t})+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\lambda_{h}\\|_{1}\\|d_{t}\\|^{2}}\\\\ &{\\quad-\\,\\alpha_{t}\\langle\\lambda_{h},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Bounding the last term in the above inequality by Lemma 13, and taking $\\gamma_{t}>0$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{h,1,t+1}-\\mathbb{V}_{h,1,t}\\le-\\alpha_{t}c_{h}\\lambda_{h}^{\\top}H(\\theta_{t})+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\lambda_{h}\\|_{1}\\|d_{t}\\|^{2}+\\displaystyle\\frac{1}{2}\\alpha_{t}\\gamma_{t}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\alpha_{t}\\langle\\lambda_{h,t},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle+\\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{h,t}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,t+1}-\\lambda_{h}\\|^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Adding up (F.22) and (F.24) yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{f,t+1}-\\mathbb{V}_{f,t}+\\mathbb{V}_{h,1,t+1}-\\mathbb{V}_{h,1,t}\\leq-\\alpha_{t}\\langle\\lambda_{t},\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\rangle+\\displaystyle\\frac{1}{2}\\gamma_{t}\\alpha_{t}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}}\\\\ &{\\quad-\\left.\\alpha_{t}c_{h}\\lambda_{h}^{\\top}H(\\theta_{t})+\\displaystyle\\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big)+\\displaystyle\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A_{a g}^{\\top}\\lambda\\|_{1}\\|d_{t}\\|^{2}}\\\\ &{\\leq-\\alpha_{t}\\|d_{t}\\|^{2}+\\alpha_{t}c_{h}(\\lambda_{h,t}-\\lambda_{h})^{\\top}H(\\theta_{t})+\\displaystyle\\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big)}\\\\ &{\\quad+\\displaystyle\\frac{1}{2}\\gamma_{t}\\alpha_{t}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}+\\displaystyle\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A_{a g}^{\\top}\\lambda\\|_{1}\\|d_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality uses the fact that $\\langle\\lambda_{t},\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\rangle=\\|d_{t}\\|^{2}-c_{h}\\lambda_{h,t}^{\\top}H(\\theta_{t}).$ ", "page_idx": 30}, {"type": "text", "text": "Using the fact that $\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})=-\\nabla H(\\theta_{t})^{\\top}d_{t}-c_{h}H(\\theta_{t})$ , and with similar arguments as (E.20) in Lemma 8, we can further derive that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|H(\\theta_{t+1})|_{\\mathrm{ab}}\\leq\\lvert H(\\theta_{t})-\\alpha_{t}c_{h}H(\\theta_{t})-\\alpha_{t}\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rvert_{\\mathrm{ab}}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}}\\\\ &{\\qquad\\qquad\\leq(1-\\alpha_{t}c_{h})\\lvert H(\\theta_{t})\\rvert_{\\mathrm{ab}}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}{\\bf1}+\\alpha_{t}\\lvert\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rvert_{\\mathrm{ab}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{I}_{h,2,t+1}-\\mathbb{V}_{h,3,t}\\leq-\\alpha_{t}c_{h}c_{V_{h}}\\|H(\\theta_{t})\\|_{1}+\\frac{\\ell_{f,1}}{2}c_{V_{h}}M_{h}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}\\|d_{t}\\|^{2}+\\alpha_{t}c_{V_{h}}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|_{1}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining (F.25) and (F.27), and by choosing step sizes $\\alpha_{t},\\gamma_{t}$ , parameter $c_{V_{h}}$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\ell_{f,1}}{2}c_{V_{h}}M_{h}\\alpha_{t}^{2}\\|B_{h}^{\\top}\\|_{\\infty,1}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A_{a g}^{\\top}\\lambda\\|_{1}\\leq\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{V}_{t+1}-\\mathbb{V}_{t}\\leq-\\frac{1}{2}\\alpha_{t}\\|d_{t}\\|^{2}-\\alpha_{t}c_{h}(c\\nu_{h}-\\|\\lambda_{h}-\\lambda_{h,t}\\|_{1})\\|H(\\theta_{t})\\|_{1}+\\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big)\\|_{1}}}\\\\ &{}&{\\quad+\\frac{1}{2}\\gamma_{t}\\alpha_{t}\\ell_{\\varphi}^{2}+\\alpha_{t}c_{V_{h}}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|_{1}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking telescoping \u221asum of the above inequality over $t~=~0,\\ldots,T\\,-\\,1$ , and applying that $\\|\\nabla_{\\lambda_{h}}\\bar{\\varphi_{}}(\\lambda_{t};\\theta_{t})\\|_{1}^{\\ast}\\leq\\sqrt{M_{h}}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{V}_{t+1}-\\mathbb{V}_{t}\\leq\\displaystyle\\sum_{t=0}^{T-1}-\\frac{1}{2}\\alpha_{t}\\|d_{t}\\|^{2}-\\alpha_{t}c_{h}(c_{V_{h}}-\\|\\lambda_{h}-\\lambda_{h,t}\\|_{1})\\|H(\\theta_{t})\\|_{1}+\\displaystyle\\frac{1}{2}\\gamma_{t}\\alpha_{t}\\ell_{\\varphi}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big)+\\alpha_{t}c_{V_{h}}\\sqrt{M_{h}}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "wwihtehr eJ $\\begin{array}{r l}{\\lefteqn{\\sum_{t=0}^{T-1}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|}}\\end{array}$ lcoawns be further bounded by applying Lemma 16 and Corollary 9 along ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Big(\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|\\Big)^{2}\\leq\\frac{1}{T}\\sum_{t=0}^{T-1}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\overset{\\mathrm{Corollary}\\,9}{\\leq}\\frac{1}{T}\\sum_{t=0}^{T-1}2\\ell_{\\varphi_{\\lambda},1}\\big(\\varphi(\\lambda_{t};\\theta_{t})-\\varphi(\\lambda_{\\rho}^{*}(\\theta_{t});\\theta_{t})\\big)+\\ell_{\\varphi_{\\lambda},1}\\rho c_{\\overline{{\\lambda}}}}\\\\ &{\\displaystyle\\overset{\\mathrm{Lemma}\\,16}{\\leq}4\\ell_{\\varphi_{\\lambda},1}c_{\\overline{{\\lambda}}}^{2}\\frac{1}{\\gamma T}(1+2\\rho^{-1}\\alpha T\\ell_{f,1}\\ell_{f}^{2}\\|A_{a g}^{\\top}\\|_{\\infty,1}^{3})+\\frac{\\gamma}{2T}\\sum_{t=0}^{T-1}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}+\\rho\\ell_{\\varphi_{\\lambda},1}c_{\\overline{{\\lambda}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}=\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}+\\|\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}\\lesssim\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}+\\|d_{t}\\|^{2}.$ . Plugging the above inequality back into (F.30), choosing $\\rho=\\Theta\\Big((\\frac{\\alpha}{\\gamma})^{\\frac{1}{2}}\\Big)$ , and rearranging yield ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\|d_{t}\\|^{2}+\\|H(\\theta_{t})\\|_{1}=\\mathcal{O}\\Big(\\frac{1}{\\alpha T}+\\frac{1}{(\\gamma T)^{\\frac{1}{2}}}+(\\frac{\\alpha}{\\gamma})^{\\frac{1}{4}}+\\gamma\\Big).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Choosing $\\alpha=\\Theta(T^{-}{\\frac{5}{6}})$ , $\\gamma=\\Theta(T^{-\\frac{1}{6}})$ proves the result. ", "page_idx": 31}, {"type": "text", "text": "F.3 Sharper analysis with a different merit function: proof of Theorem 3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma 17. Suppose Assumptions 1, 2, and $^3$ hold. If $\\alpha_{t}\\|H(\\theta_{t})\\|\\leq c_{\\alpha,h}$ , and $\\alpha_{t}\\|d_{t}\\|\\leq c_{d}$ for all $t\\in[T]$ , then for all $\\{\\boldsymbol{\\theta}_{t}\\}_{t=0}^{T}$ produced by Algorithm 2, it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|H(\\theta_{t+1})\\|^{2}-\\|H(\\theta_{t})\\|^{2}\\leq\\alpha_{t}2H(\\theta_{t})^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}+\\frac{1}{2}\\alpha_{t}^{2}\\ell_{H^{2},1}\\|d_{t}\\|^{2}}\\\\ {w i t h~\\,\\ell_{H^{2},1}=2M\\ell_{f}^{2}+2(\\alpha_{t}^{-1}+\\ell_{H})c_{d,h}\\sqrt{M}\\ell_{f,1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. By the mean-value theorem, for all $t\\in[T]$ , there exists $\\tilde{\\theta}$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Vert H(\\theta_{t+1})\\Vert^{2}-\\Vert H(\\theta_{t})\\Vert^{2}\\leq\\alpha_{t}2H(\\theta_{t})^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}+\\frac{1}{2}\\alpha_{t}^{2}\\Vert\\nabla^{2}H(\\tilde{\\theta})^{\\top}H(\\tilde{\\theta})\\Vert\\Vert d_{t}\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The Hessian of $\\|H(\\tilde{\\theta})\\|^{2}$ can be upper bounded by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla^{2}H(\\widetilde{\\theta})^{\\top}H(\\widetilde{\\theta})\\|\\leq2\\|\\nabla H(\\widetilde{\\theta})\\nabla H(\\widetilde{\\theta})^{\\top}\\|+2\\|\\nabla^{2}H(\\widetilde{\\theta})\\|\\|H(\\widetilde{\\theta})\\|\\leq2M\\ell_{f}^{2}+2\\|H(\\widetilde{\\theta})\\|\\sqrt{M}\\ell_{f,1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $H({\\tilde{\\theta}})$ is $\\ell_{H}$ -Lipschitz continuous with $\\ell_{H}=\\|B_{h}\\|\\ell_{F}$ , and $\\tilde{\\theta}$ lies on the line segment of $\\theta_{t}$ and $\\theta_{t+1}$ with $\\|\\theta_{t+1}-\\bar{\\theta_{t}}\\|=\\alpha_{t}\\|d_{t}\\|$ , therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|H(\\tilde{\\theta})\\|\\leq\\|H(\\theta_{t})\\|+\\alpha_{t}\\ell_{H}\\|d_{t}\\|\\leq\\|H(\\theta_{t})\\|+\\ell_{H}c_{d}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plugging the above inequality into (F.33) yields ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\ }&{\\|H(\\theta_{t+1})\\|^{2}-\\|H(\\theta_{t})\\|^{2}\\leq\\alpha_{t}2H(\\theta_{t})^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}+\\frac{1}{2}\\alpha_{t}^{2}\\|\\nabla^{2}H(\\tilde{\\theta})^{\\top}H(\\tilde{\\theta})\\|\\|d_{t}\\|^{2}}\\\\ &{\\ }&{\\leq\\!\\alpha_{t}2H(\\theta_{t})^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}+\\frac{1}{2}\\alpha_{t}^{2}\\Big(2M\\ell_{f}^{2}+2(\\alpha_{t}^{-1}c_{\\alpha,h}+\\ell_{H}c_{d})\\sqrt{M}\\ell_{f,1}\\Big)\\|d_{t}\\|^{2}.\\ \\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The proof is complete. ", "page_idx": 31}, {"type": "text", "text": "Lemma 18. Suppose Assumptions $^{\\,l}$ and 3 hold. For $\\{\\theta_{t}\\},\\{\\lambda_{t}\\}$ produced by Algorithm 2, choose $\\lambda_{0}$ to be bounded. Choose $\\alpha_{t}$ such that $\\alpha_{t}\\|H(\\theta_{t})\\|\\leq c_{\\alpha,h}$ , and $\\alpha_{t}\\|d_{t}\\|\\leq c_{d}$ for all $t\\in[T]$ , and choose $\\begin{array}{r}{\\gamma_{t}=\\Theta\\big(\\frac{\\alpha_{t}}{T}\\big)}\\end{array}$ . Then $\\left\\Vert\\lambda_{t}\\right\\Vert$ is bounded for all $t\\in[T]$ . Consequently, $\\|d_{t}\\|$ is bounded for all $t\\in[T]$ . If we further have $\\|H(\\theta_{t})\\|$ bounded for all $t\\in[T]$ , then $\\|\\nabla_{\\lambda_{h}\\varphi}(\\lambda_{t};\\theta_{t})\\|$ is bounded for all $t\\in[T]$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. By choosing $\\begin{array}{r}{\\gamma_{t}=\\Theta\\big(\\frac{\\alpha_{t}}{T}\\big)}\\end{array}$ , and $\\|\\lambda_{0}\\|$ to be bounded, then for all $\\tau\\in[T]$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\lambda_{\\tau}\\|\\leq\\|\\lambda_{0}\\|+\\sum_{t=0}^{\\tau-1}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|=\\|\\lambda_{0}\\|+\\sum_{t=0}^{\\tau-1}\\gamma_{t}\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\lesssim\\|\\lambda_{0}\\|+\\displaystyle\\frac{1}{T}\\sum_{t=0}^{\\tau-1}\\alpha_{t}(\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\|\\|d_{t}\\|+c_{h}\\|H(\\theta_{t})\\|)}\\\\ {\\displaystyle\\leq\\|\\lambda_{0}\\|+\\ell_{F}\\|A_{a g}\\|c_{d}+c_{h}c_{\\alpha,h}=\\Theta(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This proves that $\\left\\Vert\\lambda_{t}\\right\\Vert$ is bounded for all $t\\in[T]$ . ", "page_idx": 32}, {"type": "text", "text": "Since $d_{t}=\\nabla F(\\theta_{t})A_{a}g\\lambda_{t}$ , consequently we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|d_{t}\\|=\\|\\nabla F(\\theta_{t})A_{a}g\\lambda_{t}\\|=\\Theta(1).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Furthermore, if $\\|H(\\theta_{t})\\|\\,\\leq\\,c_{h t}$ for all $t\\,\\in\\,[T]$ , invoking that $\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\,=\\,-\\nabla H(\\theta_{t})^{\\top}d_{t}\\,-$ $c_{h}H(\\theta_{t})$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|=\\|\\nabla H(\\theta_{t})^{\\top}d_{t}+c_{h}H(\\theta_{t})\\|\\leq\\|\\nabla H(\\theta_{t})\\|\\|d_{t}\\|+c_{h}\\|H(\\theta_{t})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\ell_{H}\\|d_{t}\\|+c_{h}\\|H(\\theta_{t})\\|=\\Theta(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The proof is complete. ", "page_idx": 32}, {"type": "text", "text": "Lemma 19. Suppose Assumptions $^{\\,l}$ , 2, and $3$ hold. For $\\{\\theta_{t}\\},\\{\\lambda_{t}\\}$ produced by Algorithm 2, suppose $\\|\\lambda_{h,t}\\|\\;\\leq\\;c_{\\lambda}\\;<\\;\\infty$ is bounded for all $t\\ \\in\\ [T]$ , and $\\begin{array}{r}{\\frac{\\alpha_{0}}{T\\gamma_{0}}\\;=\\;c_{\\alpha,\\gamma}\\;<\\;\\infty}\\end{array}$ . Choose $\\alpha_{t}$ such that $\\alpha_{t}\\|H(\\theta_{t})\\|\\,\\leq\\,c_{\\alpha,h}$ for all $t\\,\\in\\,[T],$ , and suppose $\\|H(\\theta_{t})\\|$ bounded for all $t\\,\\in\\,[T]$ . Define $S_{T}$ as follows ", "page_idx": 32}, {"type": "equation", "text": "$$\nS_{T}=\\frac{2}{T}\\sum_{t=0}^{T-1}\\alpha_{t}\\bigl(c_{h}\\lambda_{h,t}^{\\top}H(\\theta_{t})-\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})^{\\top}H(\\theta_{t})\\bigr)+\\frac{\\alpha_{0}}{T\\gamma_{0}}(\\|\\lambda_{h,T}\\|^{2}-\\|\\lambda_{h,0}\\|^{2}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then we have that $\\|S_{T}\\|=\\Theta(1)$ is bounded. ", "page_idx": 32}, {"type": "text", "text": "Proof. By the definition of $S_{T}$ , Cauchy-Schwartz inequality, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|S_{T}\\|\\leq\\frac{2}{T}\\sum_{t=0}^{T-1}\\alpha_{t}c_{h}\\|\\lambda_{h,t}\\|\\|H(\\theta_{t})\\|+\\alpha_{t}\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|\\|H(\\theta_{t})\\|+\\frac{\\alpha_{0}}{T\\gamma_{0}}\\|\\lambda_{h,T}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma 18 and that $\\left\\|H(\\theta_{t})\\right\\|$ is bounded, $\\left\\Vert\\lambda_{t}\\right\\Vert$ and $\\|\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|$ are both bounded for all $t\\in[T]$ , therefore, $\\|S_{T}\\|=\\Theta(1)$ is also bounded. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Proof of Theorem 3. We consider the following Lyapunov function with a constant vector $\\lambda\\,=$ $\\left[\\lambda_{f};\\lambda_{h}\\right]\\in\\Omega_{\\lambda}(\\theta)$ , where $\\lambda_{f}=1$ , $\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ . ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{V}_{t}:=\\underbrace{\\lambda_{f}^{\\top}A F(\\theta_{t})}_{\\mathbb{V}_{f,t}}+\\underbrace{\\lambda_{h}^{\\top}H(\\theta_{t})}_{\\mathbb{V}_{h,1,t}}+\\underbrace{\\frac{1}{2}\\|H(\\theta_{t})\\|^{2}}_{\\mathbb{V}_{h,3,t}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Following the same arguments from (F.20)-(F.25), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\boldsymbol{r}_{f,t+1}-\\mathbb{V}_{f,t}+\\mathbb{V}_{h,1,t+1}-\\mathbb{V}_{h,1,t}\\le-\\alpha_{t}\\|\\boldsymbol{d}_{t}\\|^{2}+\\alpha_{t}c_{h}\\big(\\lambda_{h,t}-\\lambda_{h}\\big)^{\\top}\\boldsymbol{H}(\\theta_{t})+\\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda|\\big)}&{}\\\\ {+\\,\\frac{1}{2}\\gamma_{t}\\alpha_{t}\\|\\nabla_{\\lambda}\\varphi\\big(\\lambda_{t};\\theta_{t}\\big)\\|^{2}+\\frac{\\ell_{f,1}}{2}\\alpha_{t}^{2}\\|A_{a g}^{\\top}\\lambda\\|_{1}\\|d_{t}\\|^{2}.}&{\\qquad\\qquad(\\mathrm{F}.42)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Next we proceed to bound $\\mathbb{V}_{h,3,t+1}-\\mathbb{V}_{h,3,t}$ . By Lemma 17, it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{V}_{h,3,t+1}-\\mathbb{V}_{h,3,t}\\le\\alpha_{t}H(\\theta_{t})^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}+\\frac{1}{4}\\alpha_{t}^{2}\\ell_{H^{2},1}\\|d_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\ell_{H^{2},1}=2M\\ell_{f}^{2}+2(\\alpha_{t}^{-1}c_{\\alpha,h}+\\ell_{H}c_{d})\\sqrt{M}\\ell_{f,1}$ . Because $\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\,=\\,-\\nabla H(\\theta_{t})^{\\top}d_{t}\\,-$ $c_{h}H(\\theta_{t})$ , the term $\\dot{H(\\theta_{t})}^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}$ can be further written as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\boldsymbol{H}(\\boldsymbol{\\theta}_{t})^{\\top}\\nabla\\boldsymbol{H}(\\boldsymbol{\\theta}_{t})^{\\top}\\boldsymbol{d}_{t}=-\\boldsymbol{H}(\\boldsymbol{\\theta}_{t})^{\\top}\\big(\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\boldsymbol{\\theta}_{t})+c_{h}\\boldsymbol{H}(\\boldsymbol{\\theta}_{t})\\big)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n=-\\,c_{h}\\|H(\\theta_{t})\\|^{2}-H(\\theta_{t})^{\\top}\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging (F.44) into (F.43) yields ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\|H(\\theta_{t+1})\\|^{2}-\\frac{1}{2}\\|H(\\theta_{t})\\|^{2}\\leq-\\alpha_{t}c_{h}\\|H(\\theta_{t})\\|^{2}-\\alpha_{t}H(\\theta_{t})^{\\top}\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})+\\frac{1}{4}\\alpha_{t}^{2}\\ell_{H^{2},1}\\|d_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Letting $\\ell_{F_{a g},1}=\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1}$ , and adding up (F.42) and (F.45), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{t+1}-\\mathbb{V}_{t}\\leq-\\alpha_{t}\\|d_{t}\\|^{2}-\\alpha_{t}c_{h}\\|H(\\theta_{t})\\|^{2}+\\alpha_{t}\\Big(c_{h}\\big(\\lambda_{h,t}-\\lambda_{h}\\big)-\\nabla_{\\lambda_{h}}\\varphi\\big(\\lambda_{t};\\theta_{t}\\big)\\Big)^{\\top}H\\big(\\theta_{t}\\big)}\\\\ &{\\ +\\ \\frac{\\alpha_{t}}{2\\gamma_{t}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big)+\\frac{1}{2}\\gamma_{t}\\alpha_{t}\\|\\nabla_{\\lambda}\\varphi\\big(\\lambda_{t};\\theta_{t}\\big)\\|^{2}+\\frac{1}{4}\\alpha_{t}^{2}(2\\ell_{F_{a g},1}+\\ell_{H^{2},1})\\|d_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\|\\nabla_{\\lambda\\varphi}(\\lambda_{t};\\theta_{t})\\|^{2}$ can be further bounded via triangle inequality and Cauchy-Schwarz inequality, given by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}=\\|A_{a g}\\nabla F(\\theta_{t})^{\\top}d_{t}-c_{h}H(\\theta_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\|A_{a g}\\nabla F(\\theta_{t})^{\\top}d_{t}\\|^{2}+2\\|c_{h}H(\\theta_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\|A_{a g}\\|^{2}M\\ell_{f}^{2}\\|d_{t}\\|^{2}+2c_{h}^{2}\\|H(\\theta_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging (F.47) back into (F.46) yields ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{t+1}-\\mathbb{V}_{t}\\leq-\\alpha_{t}\\|d_{t}\\|^{2}-\\alpha_{t}c_{h}\\|H(\\theta_{t})\\|^{2}+\\alpha_{t}\\Big(c_{h}(\\lambda_{h,t}-\\lambda_{h})-\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\Big)^{\\top}H(\\theta_{t})}\\\\ &{\\quad\\quad\\quad\\quad+\\underbrace{\\alpha_{t}}_{2\\gamma_{t}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big)+\\underbrace{\\frac{1}{4}\\alpha_{t}^{2}(2\\ell_{F_{a g,1}}+\\ell_{H^{2},1})\\|d_{t}\\|^{2}}_{J_{1}}}\\\\ &{\\quad\\quad\\quad\\quad+\\underbrace{\\gamma_{t}\\alpha_{t}\\|A_{a g}\\|^{2}M\\ell_{f}^{2}\\|d_{t}\\|^{2}}_{J_{2}}+\\underbrace{\\gamma_{t}\\alpha_{t}c_{h}^{2}\\|H(\\theta_{t})\\|^{2}}_{J_{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where by choosing the step sizes $\\begin{array}{r}{\\alpha_{t}\\leq\\frac{1}{2\\ell_{F_{a g},1}+\\ell_{H^{2},1}},\\gamma_{t}\\leq\\operatorname*{min}\\left\\lbrace\\frac{1}{4\\|A_{a g}\\|^{2}M\\ell_{f}^{2}},\\frac{1}{2c_{h}}\\right\\rbrace}\\end{array}$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\nJ_{1}\\leq\\frac{1}{4}\\alpha_{t}\\|d_{t}\\|^{2},\\;\\;J_{2}\\leq\\frac{1}{4}\\alpha_{t}\\|d_{t}\\|^{2},\\;\\;J_{3}\\leq\\frac{1}{2}\\alpha_{t}c_{h}\\|H(\\theta_{t})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging (F.49) into (F.48), choosing $\\begin{array}{r}{\\frac{\\alpha_{t}}{\\gamma_{t}}=\\frac{\\alpha_{0}}{\\gamma_{0}}}\\end{array}$ for all $t\\in[T]$ , and rearranging, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{t+1}-\\mathbb{V}_{t}\\leq-\\displaystyle\\frac{1}{2}\\alpha_{t}\\|d_{t}\\|^{2}-\\frac{1}{2}\\alpha_{t}c_{h}\\|H(\\theta_{t})\\|^{2}+\\alpha_{t}\\Big(c_{h}\\big(\\lambda_{h,t}-\\lambda_{h}\\big)-\\nabla_{\\lambda_{h}}\\varphi\\big(\\lambda_{t};\\theta_{t}\\big)\\Big)^{\\top}H(\\theta_{t})}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\alpha_{0}}{2\\gamma_{0}}\\big(\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Taking telescoping sum of the above inequality over $t=0,\\dots,T-1$ yields ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\alpha_{t}\\Big(\\|d_{t}\\|^{2}+c_{h}\\|H(\\theta_{t})\\|^{2}\\Big)-\\displaystyle\\sum_{t=0}^{T-1}2\\alpha_{t}\\Big(c_{h}\\big(\\lambda_{h,t}-\\lambda_{h}\\big)-\\nabla_{\\lambda_{h}}\\varphi\\big(\\lambda_{t};\\theta_{t}\\big)\\Big)^{\\top}H(\\theta_{t})}\\\\ &{\\leq2(\\mathbb{V}_{0}-\\mathbb{V}_{T})+\\frac{\\alpha_{0}}{\\gamma_{0}}(\\|\\lambda_{0}-\\lambda\\|^{2}-\\|\\lambda_{T}-\\lambda\\|^{2})}\\\\ &{\\leq2\\mathbb{V}_{f,0}+2\\lambda_{h}^{\\top}\\big(H(\\theta_{0})-H(\\theta_{T})\\big)+\\|H(\\theta_{0})\\|^{2}+\\frac{\\alpha_{0}}{\\gamma_{0}}(\\|\\lambda_{0}-\\lambda\\|^{2}-\\|\\lambda_{T}-\\lambda\\|^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality uses the definition of $\\mathbb{V}_{t}$ and that $\\mathbb{V}_{f,t}\\geq0$ . ", "page_idx": 33}, {"type": "text", "text": "Choosing $\\lambda_{f,0}=\\lambda_{f}$ , and rearranging the above inequality, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\alpha_{t}\\Big(\\|d_{t}\\|^{2}+c_{h}\\|H(\\theta_{t})\\|^{2}\\Big)\\leq2\\Psi_{f,0}+\\|H(\\theta_{0})\\|^{2}+\\frac{\\alpha_{0}}{\\gamma_{0}}(\\|\\lambda_{h,0}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,T}-\\lambda_{h}\\|^{2})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n+\\sum_{t=0}^{T-1}2\\alpha_{t}\\bigl(c_{h}\\lambda_{h,t}-\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\bigr)^{\\top}H(\\theta_{t})+2\\lambda_{h}^{\\top}\\Bigl(H(\\theta_{0})-H(\\theta_{T})-c_{h}\\sum_{t=0}^{T-1}\\alpha_{t}H(\\theta_{t})\\Bigr).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that (F.52) holds for all $\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ . And $\\lambda_{h}$ here serves as an auxiliary multiplier to prove the convergence in Theorem 3. We then discuss how to choose a bounded $\\lambda_{h}$ such that the desired convergence result holds. A simple choice is that the additional terms related to $\\lambda_{h}$ and $\\lambda_{h,t}$ on the right hand side of (F.52) amount to a value greater than zero, i.e., ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\lambda_{h}^{\\top}\\Big(-c_{h}\\displaystyle\\sum_{t=0}^{T-1}\\alpha_{t}H(\\theta_{t})\\Big)-\\frac{\\alpha_{0}}{\\gamma_{0}}(\\|\\lambda_{h,0}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,T}-\\lambda_{h}\\|^{2})}\\\\ &{=2\\lambda_{h}^{\\top}\\Big(-c_{h}\\displaystyle\\sum_{t=0}^{T-1}\\alpha_{t}H(\\theta_{t})\\Big)-\\frac{\\alpha_{0}}{\\gamma_{0}}(\\|\\lambda_{h,0}\\|^{2}-\\|\\lambda_{h,T}\\|^{2}-2\\langle\\lambda_{h},\\lambda_{h,0}-\\lambda_{h,T}\\rangle)}\\\\ &{\\geq-\\displaystyle2\\sum_{t=0}^{T-1}\\alpha_{t}\\big(c_{h}\\lambda_{h,t}-\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\big)^{\\top}H(\\theta_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which is a linear inequality w.r.t. $\\lambda_{h}$ . ", "page_idx": 34}, {"type": "text", "text": "Next we discuss when (F.53) has at least one bounded solution w.r.t. $\\lambda_{h}$ . Rearranging (F.53) yields ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\lambda_{h}^{\\top}\\Big(\\frac{\\alpha_{0}}{T\\gamma_{0}}(\\lambda_{h,0}-\\lambda_{h,T})-c_{h}\\frac{1}{T}\\sum_{t=0}^{T-1}\\alpha_{t}H(\\theta_{t})\\Big)}\\\\ {\\displaystyle\\geq-\\,\\frac{2}{T}\\sum_{t=0}^{T-1}\\alpha_{t}\\big(c_{h}\\lambda_{h,t}-\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\big)^{\\top}H(\\theta_{t})+\\frac{\\alpha_{0}}{T\\gamma_{0}}(\\|\\lambda_{h,T}\\|^{2}-\\|\\lambda_{h,0}\\|^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For notation simplicity, define the following ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}=2c_{h}\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\alpha_{t}H(\\theta_{t})-\\frac{2\\alpha_{0}}{T\\gamma_{0}}(\\lambda_{h,0}-\\lambda_{h,T}),}\\\\ &{S_{2}=\\displaystyle\\frac{2}{T}\\sum_{t=0}^{T-1}\\alpha_{t}\\big(c_{h}\\lambda_{h,t}^{\\top}H(\\theta_{t})-\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})^{\\top}H(\\theta_{t})\\big)+\\displaystyle\\frac{\\alpha_{0}}{T\\gamma_{0}}(\\|\\lambda_{h,T}\\|^{2}-\\|\\lambda_{h,0}\\|^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Lemma 19, $\\|S_{2}\\|$ is bounded. In general, $S_{1}\\neq0$ and with proper $\\lambda_{h,0}$ , $\\|S_{1}\\|=\\Omega(1)$ . Then there exists bounded $\\lambda_{h}\\in\\mathbb{R}^{M_{h}}$ with $\\|\\lambda_{h}\\|=\\mathcal{O}(1)$ such that (F.54) holds, thus the last three terms in (F.52) amount to a value no greater than zero, which yields ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\alpha_{t}\\Big(\\|d_{t}\\|^{2}+c_{h}\\|H(\\theta_{t})\\|^{2}\\Big)\\leq2\\mathbb{V}_{f,0}+\\|H(\\theta_{0})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\|H(\\theta_{t})\\|$ and $\\|d_{t}\\|$ are bounded for all $t\\in[T]$ , we can choose $\\alpha_{t}=\\Theta(1)$ , $\\gamma_{t}=\\Theta(T^{-1})$ , and it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\Big(\\|d_{t}\\|^{2}+c_{h}\\|H(\\theta_{t})\\|^{2}\\Big)=\\mathcal{O}\\Big(\\frac{1}{T}\\Big).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We then summarize the best possible choices for $\\alpha_{t},\\gamma_{t}$ . Choosing $\\theta_{0}$ such that $2\\mathbb{V}_{f,0}+\\|H(\\theta_{0})\\|^{2}=$ c0 < \u221e. Recall that we req\u221auire \u03b1t \u2264 2\u2113Fag,11+\u2113H2,1 . Rearranging this inequality with $\\ell_{H^{2},1}\\,=$ $2M\\ell_{f}^{2}+2(\\alpha_{t}^{-1}c_{\\alpha,h}+\\ell_{H}c_{d})\\sqrt{M}\\ell_{f,1}$ yields ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\alpha_{t}(2\\ell_{F_{a g},1}+\\ell_{H^{2},1})=2\\alpha_{t}\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1}+2\\alpha_{t}M\\ell_{f}^{2}+2(c_{\\alpha,h}+\\alpha_{t}\\ell_{H}c_{d})\\sqrt{M}\\ell_{f,1}\\le1.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Recall that we choose $\\alpha_{t}$ such that $\\alpha_{t}\\|d_{t}\\|\\leq c_{d}$ and $\\alpha_{t}\\|H(\\theta_{t})\\|\\leq c_{\\alpha,h}$ . We can choose the following specific constants to ensure the above inequality holds ", "page_idx": 35}, {"type": "equation", "text": "$$\nc_{d}=c_{\\alpha,h}=\\frac{1}{4\\sqrt{M}\\ell_{f,1}},\\quad c_{h}=c_{\\alpha,h}^{-1}c_{0},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which implies that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\alpha_{t}\\leq\\frac{1}{4(\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1}+M\\ell_{f}^{2})+\\ell_{H}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that $\\lambda=[\\lambda_{f};\\lambda_{h}]$ , with $\\lambda_{f}=1$ , and $\\lambda_{h}$ satisfying (F.53). If $\\|\\lambda_{h}\\|_{1}$ is bounded by a constant, then $\\|A_{a g}^{\\top}\\lambda\\|_{1}\\leq\\|A_{a g}^{\\top}\\|_{1}\\|\\lambda\\|_{1}$ is bounded, and we can choose $\\alpha_{t}=\\Omega(1)$ , i.e., $\\alpha_{t}$ is lower bounded by a constant. ", "page_idx": 35}, {"type": "text", "text": "To summarize, the following inequalities should be satisfied by the step sizes ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{t}=\\operatorname*{min}\\Big\\{c_{0},\\displaystyle\\frac{1}{4\\sqrt{M}\\ell_{f,1}\\operatorname*{max}\\{1,\\|H(\\theta_{t})\\|,\\|d_{t}\\|\\}},\\displaystyle\\frac{1}{4(\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1}+M\\ell_{f}^{2})+\\ell_{H}}\\Big\\},}\\\\ &{\\gamma_{t}\\leq\\operatorname*{min}\\Big\\{\\displaystyle\\frac{\\alpha_{0}}{T},\\displaystyle\\frac{1}{4\\|A_{a g}\\|^{2}M\\ell_{f}^{2}},\\displaystyle\\frac{1}{2c_{h}}\\Big\\},\\ \\mathrm{and}\\ \\gamma_{t}=\\displaystyle\\frac{\\gamma_{0}}{\\alpha_{0}}\\alpha_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\ell_{F_{a g},1}=\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1}$ . The proof is complete. ", "page_idx": 35}, {"type": "text", "text": "G Stochastic Algorithms ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we discuss the stochastic algorithms and their convergence guarantees. ", "page_idx": 35}, {"type": "text", "text": "G.1 Algorithm summary ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The stochastic algorithm is summarized in Algorithm 3. Note that, instead of computing $\\nabla F_{\\xi_{t,1}}(\\theta_{t}),\\nabla F_{\\xi_{t,2}}\\overline{{(\\theta_{t})}}$ , which requires $2M$ gradient computation at each iteration, we compute $\\nabla F_{\\xi_{t,1}}(\\theta_{t}),\\nabla F_{\\xi_{t,2}}(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}$ , which requires $M+1$ gradient computation per iteration. This saves nearly half of the per-iteration complexity compared to the most relevant existing stochastic algorithm for multi-objective optimization [6]. ", "page_idx": 35}, {"type": "text", "text": "Algorithm 3 Preference-guided multi-objective algorithm with approximate update ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1: Initialize $t=0$ , $\\theta_{0}$ , step sizes $\\alpha_{t},\\gamma_{t}$ , define $A$   \n2: for $t=0,\\dots,T-1$ do   \n3: Compute stochastic gradient $\\nabla F_{\\xi_{t,1}}(\\theta_{t}),\\nabla F_{\\xi_{t,2}}(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}$ ;   \n4: Compute an update direction $d_{t}=\\nabla F_{\\xi_{t,1}}(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}$ ;   \n5: Choose the step size $\\alpha_{t}$ by a predefined schedule;   \n6: Update $\\theta_{t}$ by $\\theta_{t+1}=\\theta_{t}+\\alpha_{t}d_{t}$ ;   \n7: Update $\\lambda_{t}$ by (3.4);   \n8: end for ", "page_idx": 35}, {"type": "text", "text": "", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "G.2 Proof of Theorem 4: convergence of Algorithm 3 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We first introduce the supporting lemmas, and then present the main proofs. Denote $\\mathcal{F}_{t}$ as the $\\sigma$ -algebra generated by $\\bar{\\nabla^{\\!}F_{\\xi_{0}}}(\\theta_{0}),\\bar{\\nabla^{\\!}F_{\\xi_{1}}}(\\theta_{1}),\\dots,\\nabla\\bar{F_{\\xi_{t}}}(\\theta_{t})$ , where $\\xi_{t}=\\{\\xi_{t,1},\\xi_{t,2}\\}$ . For simplicity, we let $\\mathbb{E}_{t}[\\cdot]:=\\mathbb{E}[\\cdot\\mid\\mathcal{F}_{t}]$ in the proofs. ", "page_idx": 35}, {"type": "text", "text": "Lemma 20. Let $\\lambda_{t}=[\\lambda_{f,t};\\lambda_{h,t}]$ . Consider the stochastic sequence $\\{\\lambda_{t}\\}_{t=0}^{T}$ produced by Algorithm $3$ . Then for all $\\lambda\\in\\Omega_{\\lambda}(\\theta_{t})$ , it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\gamma_{t}\\mathbb{E}_{t}[\\langle\\lambda_{f,t}-\\lambda_{f},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle]\\leq\\mathbb{E}_{t}[\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}+\\gamma_{t}^{2}\\|\\tilde{\\nabla}_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}];}\\\\ &{2\\gamma_{t}\\mathbb{E}_{t}[\\langle\\lambda_{h,t}-\\lambda_{h},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle]\\leq\\mathbb{E}_{t}[\\|\\lambda_{h,t}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,t+1}-\\lambda_{h}\\|^{2}+\\gamma_{t}^{2}\\|\\tilde{\\nabla}_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. By the update of $\\lambda$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}\\leq\\|\\lambda_{f,t}-\\gamma_{t}\\tilde{\\nabla}_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})-\\lambda_{f}\\|^{2}}\\\\ &{=\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-2\\gamma_{t}\\langle\\lambda_{f,t}-\\lambda_{f},\\tilde{\\nabla}_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle+\\gamma_{t}^{2}\\|\\tilde{\\nabla}_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Taking expectation over the stochastic samples and rearranging the above inequality, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\gamma_{t}\\mathbb{E}_{t}\\big[\\langle\\lambda_{f,t}-\\lambda_{f},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle\\big]=2\\gamma_{t}\\mathbb{E}_{t}\\big[\\langle\\lambda_{f,t}-\\lambda_{f},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle\\big]}\\\\ &{\\leq\\!\\mathbb{E}_{t}\\big[\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}+\\gamma_{t}^{2}\\|\\tilde{\\nabla}_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Following similar arguments, it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\gamma_{t}\\mathbb{E}_{t}[\\langle\\lambda_{h,t}-\\lambda_{h},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle]}\\\\ &{\\leq\\!\\mathbb{E}_{t}[\\|\\lambda_{h,t}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,t+1}-\\lambda_{h}\\|^{2}+\\gamma_{t}^{2}\\|\\tilde{\\nabla}_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The proof is complete. ", "page_idx": 36}, {"type": "text", "text": "Proof of Theorem 4. Define the following Lyapunov functions ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{V}_{t}:=\\underbrace{\\lambda_{f}^{\\top}A F(\\theta_{t})}_{\\mathbb{V}_{f,t}}+\\underbrace{\\lambda_{h}^{\\top}H(\\theta_{t})+\\frac{1}{2}\\|H(\\theta_{t})\\|^{2}}_{\\mathbb{V}_{h,t}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $\\lambda_{t}=[\\lambda_{f,t};\\lambda_{h,t}]$ . The algorithm takes the update $\\theta_{t+1}=\\theta_{t}+\\alpha_{t}d_{t}$ with $d_{t}=\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}$ . From Assumption 2, the smoothness of the objectives, and Lemma 6, the function $\\lambda_{f}^{\\top}A F(\\theta)$ is $\\ell_{A F,1}$ -smooth with $\\ell_{A F,1}=\\ell_{f,1}\\|A^{\\top}\\lambda_{f}\\|_{1}$ , thus ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[\\mathbb{V}_{f,t+1}-\\mathbb{V}_{f,t}]\\leq\\!\\mathbb{E}_{t}[\\langle\\nabla F(\\theta_{t})A^{\\top}\\lambda_{f},\\theta_{t+1}-\\theta_{t}\\rangle+\\frac{\\ell_{A F,1}}{2}\\|\\theta_{t+1}-\\theta_{t}\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\!\\alpha_{t}\\mathbb{E}_{t}[\\langle\\nabla F(\\theta_{t})A^{\\top}\\lambda_{f},d_{t}\\rangle]+\\frac{\\ell_{A F,1}}{2}\\alpha_{t}^{2}\\mathbb{E}_{t}[\\|d_{t}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Lemma 20, taking $\\gamma_{t}=\\gamma$ , and rearranging, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}[\\langle\\nabla F(\\theta_{t})A^{\\top}\\lambda_{f},d_{t}\\rangle]\\le\\!\\frac{1}{2\\gamma}\\big(\\mathbb{E}_{t}[\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}]-\\mathbb{E}_{t}[\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}]\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{1}{2}\\gamma\\mathbb{E}_{t}[\\|\\tilde{\\nabla}_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]-\\mathbb{E}_{t}[\\langle\\lambda_{f,t},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining (G.6) and (G.7) and taking total expectation, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbb{V}_{f,t+1}]-\\mathbb{E}[\\mathbb{V}_{f,t}]\\leq\\!\\!\\frac{\\alpha_{t}}{2\\gamma}\\mathbb{E}\\big[\\|\\lambda_{f,t}-\\lambda_{f}\\|^{2}-\\|\\lambda_{f,t+1}-\\lambda_{f}\\|^{2}\\big]+\\displaystyle\\frac{\\ell_{A F,1}}{2}\\alpha_{t}^{2}\\mathbb{E}[\\|d_{t}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\displaystyle\\frac{1}{2}\\alpha_{t}\\gamma\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]-\\alpha_{t}\\mathbb{E}[\\langle\\lambda_{f,t},\\nabla_{\\lambda_{f}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Following similar arguments from (G.6)-(G.8), ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t+1})]-\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t})]\\leq\\alpha_{t}\\mathbb{E}[\\lambda_{h}^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}]+\\frac{\\ell_{H,1}}{2}\\alpha_{t}^{2}\\mathbb{E}[\\|d_{t}\\|^{2}]}\\\\ &{\\leq-\\alpha_{t}c_{h}\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t})]-\\alpha_{t}\\mathbb{E}[\\langle\\lambda_{h},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle]+\\frac{\\ell_{H,1}}{2}\\alpha_{t}^{2}\\mathbb{E}[\\|d_{t}\\|^{2}]}\\\\ &{\\leq-\\alpha_{t}c_{h}\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t})]+\\frac{\\ell_{H,1}}{2}\\alpha_{t}^{2}\\mathbb{E}[\\|d_{t}\\|^{2}]+\\frac{1}{2}\\alpha_{t}\\gamma\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]}\\\\ &{~~-\\alpha_{t}\\mathbb{E}[\\langle\\lambda_{h,t},\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\rangle]+\\frac{\\alpha_{t}}{2\\gamma}\\mathbb{E}\\big[\\|\\lambda_{h,t}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,t+1}-\\lambda_{h}\\|^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Adding up (G.8) and (G.9) gives ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbb{V}_{f,t+1}]-\\mathbb{E}[\\mathbb{V}_{f,t}]+\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t+1})]-\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t})]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq-\\alpha_{t}\\mathbb{E}[\\langle\\lambda_{t},\\nabla_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\rangle]-\\alpha_{t}c_{h}\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t})]+\\displaystyle\\frac{1}{2}\\alpha_{t}\\gamma\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]}\\\\ &{\\quad+\\displaystyle\\frac{\\ell_{F_{a,p,1}}}{2}\\alpha_{t}^{2}\\mathbb{E}[\\|d_{t}\\|^{2}]+\\displaystyle\\frac{\\alpha_{t}}{2\\gamma}\\mathbb{E}[\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}]}\\\\ &{=-\\alpha_{t}\\mathbb{E}[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+\\alpha_{t}c_{h}\\mathbb{E}[\\lambda_{h,t}^{\\top}H(\\theta_{t})]-\\alpha_{t}c_{h}\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t})]+\\displaystyle\\frac{1}{2}\\alpha_{t}\\gamma\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]}\\\\ &{\\quad+\\displaystyle\\frac{\\ell_{F_{a,p,1}}}{2}\\alpha_{t}^{2}\\mathbb{E}[\\|d_{t}\\|^{2}]+\\displaystyle\\frac{\\alpha_{t}}{2\\gamma}\\mathbb{E}[\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By Lemma 17, and that $\\begin{array}{r}{\\mathbb{E}[\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})]=\\mathbb{E}[-\\nabla H(\\theta_{t})^{\\top}d_{t}-c_{h}H(\\theta_{t})]}\\end{array}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{2}\\mathbb{E}[\\|H(\\theta_{t+1})\\|^{2}]-\\frac{1}{2}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]\\leq\\alpha_{t}\\mathbb{E}[H(\\theta_{t})^{\\top}\\nabla H(\\theta_{t})^{\\top}d_{t}]+\\frac{1}{4}\\alpha_{t}^{2}\\ell_{H^{2},1}\\mathbb{E}[\\|d_{t}\\|^{2}]}\\\\ &{\\leq-\\alpha_{t}c_{h}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]-\\alpha_{t}\\mathbb{E}[H(\\theta_{t})^{\\top}\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})]+\\frac{1}{4}\\alpha_{t}^{2}\\ell_{H^{2},1}\\mathbb{E}[\\|d_{t}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Adding up (G.10) and (G.11) yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathbb{V}_{t+1}]-\\mathbb{E}[\\mathbb{V}_{t}]\\leq-\\alpha_{t}\\mathbb{E}[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+\\alpha_{t}c_{h}\\mathbb{E}[\\lambda_{h,t}^{\\top}H(\\theta_{t})]-\\alpha_{t}c_{h}\\mathbb{E}[\\lambda_{h}^{\\top}H(\\theta_{t})]}\\\\ &{\\quad+\\displaystyle\\frac{1}{2}\\alpha_{t}\\gamma\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]+\\frac{2\\ell_{F_{a g},1}+\\ell_{H^{2},1}}{4}\\alpha_{t}^{2}\\mathbb{E}[\\|d_{t}\\|^{2}]}\\\\ &{\\quad-\\alpha_{t}c_{h}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]-\\alpha_{t}\\mathbb{E}[H(\\theta_{t})^{\\top}\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})]+\\frac{\\alpha_{t}}{2\\gamma}\\mathbb{E}\\big[\\|\\lambda_{t}-\\lambda\\|^{2}-\\|\\lambda_{t+1}-\\lambda\\|^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Taking telescoping sum of the above inequality, choosing $\\alpha_{t}=\\alpha$ and rearranging, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=0}^{T-1}\\alpha\\Big(\\mathbb{E}[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+c_{h}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]\\Big)}}\\\\ &{}&{\\leq\\mathbb{E}[\\mathbb{V}_{0}-\\mathbb{V}_{T}]+\\frac{\\alpha}{2\\gamma}\\mathbb{E}[\\|\\lambda_{0}-\\lambda\\|^{2}-\\|\\lambda_{T}-\\lambda\\|^{2}]-\\alpha\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\Big[\\big(c_{h}(\\lambda_{h,t}-\\lambda_{h})-\\nabla_{\\lambda_{h}}\\varphi(\\lambda_{t};\\theta_{t})\\big)^{\\top}H}\\\\ &{}&{\\quad+\\displaystyle\\frac{1}{2}\\alpha\\gamma\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]+\\frac{2\\ell_{F_{a g},1}+\\ell_{H^{2},1}}{4}\\alpha^{2}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}[\\|d_{t}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We choose $\\lambda_{f,0}=\\lambda_{f}$ , and $\\lambda_{h}$ satisfying the following equation ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\frac{\\alpha}{2\\gamma}\\mathbb{E}[\\|\\lambda_{h,0}-\\lambda_{h}\\|^{2}-\\|\\lambda_{h,T}-\\lambda_{h}\\|^{2}]-\\alpha\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\Big[\\big(c_{h}\\big(\\lambda_{h,t}-\\lambda_{h}\\big)-\\nabla_{\\lambda_{h}}\\varphi\\big(\\lambda_{t};\\theta_{t}\\big)\\big)^{\\top}H\\big(\\theta_{t}\\big)\\Big]}&\\\\ &{+\\lambda_{h}^{\\top}\\mathbb{E}[H(\\theta_{0})-H(\\theta_{T})]=0.}&{(\\mathrm{G}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Plugging the above equation into (G.13), it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=0}^{T-1}\\alpha\\Big(\\mathbb{E}[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+c_{h}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]\\Big)\\leq\\mathbb{E}[\\mathbb{V}_{f,0}]+\\displaystyle\\frac{1}{2}\\mathbb{E}[\\|H(\\theta_{0})\\|^{2}]}\\\\ {\\displaystyle+\\frac{1}{2}\\alpha\\gamma\\sum_{t=0}^{T-1}\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]+\\frac{2\\ell_{F_{a g},1}+\\ell_{H^{2},1}}{4}\\alpha^{2}\\sum_{t=0}^{T-1}\\mathbb{E}[\\|d_{t}\\|^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\mathbb{E}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]=\\mathbb{E}[\\mathbb{E}_{t}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]]$ can be further bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\xi}[\\mathbb{E}_{t}[\\|\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})\\|^{2}]]\\leq2\\mathbb{E}[\\|A_{a g}\\nabla F(\\theta_{t})^{\\top}\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+2c_{h}^{2}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]+\\mathbb{E}[\\mathrm{Var}_{t}[\\tilde{\\nabla}_{\\lambda}\\varphi(\\lambda_{t};\\theta_{t})]]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\|A_{a g}\\|^{2}M\\ell_{f}^{2}\\mathbb{E}[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+2c_{h}^{2}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]+\\sigma^{2}.\\qquad\\mathrm{~(G.16)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Similarly, $\\mathbb{E}[\\|d_{t}\\|^{2}]$ can be further bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|d_{t}\\|^{2}]\\leq\\mathbb{E}[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Plugging (G.16) and (G.17) back into (G.15), and setting $\\alpha~\\le~1/(2\\ell_{F_{a g},1}~+~\\ell_{H^{2},1})$ , $\\gamma\\ \\leq$ $\\operatorname*{min}\\left\\{\\frac{1}{4\\|A_{a g}\\|^{2}M\\ell_{f}^{2}},\\frac{1}{2c_{h}}\\right\\}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\alpha\\Big(\\mathbb{E}[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}]+c_{h}\\mathbb{E}[\\|H(\\theta_{t})\\|^{2}]\\Big)\\leq\\mathbb{E}[\\mathbb{V}_{f,0}]+\\frac{1}{2}\\mathbb{E}[\\|H(\\theta_{0})\\|^{2}]}\\\\ &{\\displaystyle+\\frac{1}{2}\\alpha\\gamma T\\sigma^{2}+\\frac{2\\ell_{F_{a g},1}+\\ell_{H^{2},1}}{4}\\alpha^{2}T\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Following similar arguments as (F.61) - (F.62), we require the step sizes ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha\\leq\\frac{1}{4(\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1}+M\\ell_{f}^{2})+\\ell_{H}}}\\\\ {\\gamma\\leq\\operatorname*{min}\\left\\{\\frac{\\alpha}{T},\\frac{1}{4\\|A_{a g}\\|^{2}M\\ell_{f}^{2}},\\frac{1}{2c_{h}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\ell_{F_{a g},1}=\\ell_{f,1}\\|A_{a g}^{\\top}\\lambda\\|_{1}$ . This ensures $\\lambda_{h}$ is bounded and then one can choose $\\alpha=\\Theta(T^{-\\frac{1}{2}})$ , $\\gamma=\\Theta(T^{-\\frac{3}{2}})$ to obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\Big[\\|\\nabla F(\\theta_{t})A_{a g}^{\\top}\\lambda_{t}\\|^{2}+\\|H(\\theta_{t})\\|^{2}\\Big]=\\mathcal{O}\\Big(T^{-\\frac{1}{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "H Implementation Details and Additional Experiment Results ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we report the additional implementation details omitted from the main text in Appendix H.1 and the additional experimental results in Appendix H.2. ", "page_idx": 38}, {"type": "text", "text": "H.1 Implementation details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Computation. All experiments were conducted on a server with an Intel i9-7920X CPU, two NVIDIA A5000 GPUs and two NVIDIA A4500 GPUs. ", "page_idx": 38}, {"type": "text", "text": "For all the experiments reported in the main text except for the multi-lingual speech recognition experiment, we exactly follow the settings from [30]. The implementations of the baselines including LS, PMTL, and EPO are from the official code of the EPO paper in https://github.com/dbmptr/ EPOSearch with their default hyperparameters. The results of XWC-MGDA are directly referenced from the paper due to lack of official implementation. ", "page_idx": 38}, {"type": "text", "text": "Synthetic data. For the results in both Figure 3 and Figure 4, the model parameter $\\theta$ has dimension $q\\,=\\,20$ , the number of objectives is $M=2$ . The angles between the preference vectors and the horizontal axis are generated between $\\textstyle\\left[{\\frac{1}{20}}\\pi,{\\frac{9}{20}}\\pi\\right]$ with equal angular distance. This experiment does not involve stochastic optimization. For our method, we solve the subprogram using PGD with a step size 0.1 up to an error of $10^{-5}$ or with a maximum of 250 iterations. In the experiments, we set the parameter $c_{h}=1$ for the subprogram if not otherwise specified. ", "page_idx": 38}, {"type": "text", "text": "In Figure 3, for all preferences and all methods, the initial model parameter $\\theta_{0}$ is randomly generated from a Gaussian distribution ${\\mathcal{N}}(0,1)$ for each dimension. In Table 6, we provide a summary of the hyperparameters for the baselines and our methods for the experiments in Figure 3. ", "page_idx": 38}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/fea7ba55f3245fe0bc4a5639510fabdd49ebe8a317d6b87079b6421b5ade6134.jpg", "table_caption": ["Table 6: Summary of hyper-parameters for the synthetic data experiments in Figure 3. "], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "In Figures 4a-4c, the initial model parameters are randomly generated from a uniform distribution between $[-0.3,0.3]$ for each dimension. In Figures 4d-4f, the initial model parameters are randomly generated from a uniform distribution between $[-0.5,-0.15]$ or [0.15, 0.5] for each dimension. Table 7 summarizes the hyperparameters for the experiments in Figure 4. ", "page_idx": 39}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/fd31b36cf2258e7395b9bda6506a562066e6c63f5fdfc4de12e39be4a2043262.jpg", "table_caption": ["Table 7: Summary of hyper-parameters for the synthetic data experiments in Figure 4. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Multi-patch image classification. For a fair comparison, we follow the same data splitting and processing procedures as [30] using their official code. In each of the three datasets, there are $120\\mathbf{k}$ samples for training and 20k samples for testing. There are two tasks on each dataset: 1) classifying the top-left image, and 2) classifying the bottom-right image. ", "page_idx": 39}, {"type": "text", "text": "For all methods, we use the SGD optimizer with batch size 256. Note that, for our stochastic method, we use batch size 128 for each batch in the double sampling. Thus the total number of samples taken at each iteration is also 256. The hyperparameters are summarized in Table 8. The results of XWC-MGDA are directly referenced from the paper. ", "page_idx": 39}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/43886c9f7c0ebac72ccb0d53bd03eb2e72e5ef9d531daf49f5c6fa03b914577b.jpg", "table_caption": ["Table 8: Summary of hyper-parameter choices for multi-patch image classification experiments. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "We use the Pymoo 0.6.1 library to compute the hypervolume. The Nadir points, i.e., the worst performance on single task baselines, used for the hypervolume computation are given in Table 9. For a fair comparison, the Nadir points we use are the same with [33] inferred from Figure 4 in the paper. ", "page_idx": 39}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/03b4742c8129ebb3362adabf3490319f980fbd1126e3599df562fe783f29c134.jpg", "table_caption": ["Table 9: Nadir points for the hypervolume computation "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Multi-lingual speech recognition. We use two datasets, Librispeech and AISHELL v1. Librispeech is an English speech dataset that consists of 960 hours of labeled audio data. For our experiments, we use the \"train-clean- $100\"$ subset of the Librispeech dataset for supervised training, which contains 100 hours of clean training data. Additionally, we use the full 960 hours of data for self-supervised training. AISHELL v1 is a 178-hour Mandarin speech corpus designed for various speech and speaker processing tasks. We use the full AISHELL v1 dataset for both self-supervised and supervised training. We combine these two datasets for our multi-lingual speech recognition experiments. ", "page_idx": 39}, {"type": "text", "text": "We use the conformer [18] model with 8 conformer blocks as the encoder. Each block contains 512 hidden units and 8 attention heads. Each attention head has dimension 64. The convolutional kernel size is 31. Two classification heads are used. They contain two linear layers, one with 1000 output size for English, and another with 5000 output size for Chinese. ", "page_idx": 39}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/fd5f36083e99c08f50cdcca41f48c39a408e875e7a53a192e57a2b6207996d4e.jpg", "img_caption": ["Figure 8: Synthetic experiment results with Algorithm 2. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "The loss functions we use include the Contrastive Predictive Coding (CPC) loss, and the Connectionist Temporal Classification (CTC) loss. The CPC loss [35] is a selfsupervised loss to learn robust representations from unlabeled speech data. The CPC loss is designed to maximize the probability of a future sample given a contextual representation generated from the current speech sequence. The CTC loss is defined as the negative log-likelihood of the model parameter given the input sequence and the label sequence. ", "page_idx": 40}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/1c4d325ab31ebc89102b1c5d43210c70cf3d3a1044989630c236dc64a1ecda5f.jpg", "img_caption": ["Figure 7: Scale invariance verification. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "For all methods including the baselines, we use the step sizes $\\alpha_{t,1}=5\\times10^{-4}$ for training backbone conformer parameters and $\\alpha_{t,2}=5\\times10^{-5}$ for training classification head parameters. The step size $\\gamma_{t}=0.1$ and the parameter $c_{h}=0.5$ . ", "page_idx": 40}, {"type": "text", "text": "H.2 Additional experiment results ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Synthetic data. We conduct several additional experiments on the synthetic objectives to further verify our theory. First, we conduct all the experiments on the synthetic objectives reported in the main text, using the single-loop approximate algorithm described in Algorithm 2. The results are plotted in Figure 8. The hyperparameters are the same unless otherwise specified. ", "page_idx": 40}, {"type": "text", "text": "From Figure 8a, we can see that Algorithm 2 with a one-step approximate update of $\\lambda_{t}$ also leads to convergence and preference alignment. However, different from the results obtained by exactly solving for $\\lambda^{\\ast}(\\theta_{t})$ at each iteration, the models on the optimization trajectories do not align exactly with the preference. Similar observations can be found in Figure 8b. In Figure 8c, which is a difficult case due to the initialization, $A=I_{M}$ does not work since it does not incorporate more general relative preference to allow controlled ascent update. This is addressed in Figure 8d, where a general $A$ (the same as in prior experiments) is used. Compared with exactly solving for $\\lambda^{*}(\\theta_{t})$ at each iteration, the approximate algorithm takes more iterations to converge, but has smaller per-iteration complexity, and smaller total time complexity. ", "page_idx": 40}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/a870cada3bd21f825864c83cb9ce6f20968ebbbf1c59d98c1cf3aed04221ffbf.jpg", "table_caption": ["Table 10: Summary of hyper-parameters for the synthetic data experiments in Figure 8. "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "We conduct another experiment to verify that the scale invariance can be preserved. We use the same objective as above, but scale the second one by 2. We use a fixed initialization $\\theta_{0}=0.3\\cdot[\\mathbf{1}_{q/2};-\\mathbf{1}_{q/2}]$ for this experiment. The other hyperparameters are the same as the default. We use both $F(\\theta_{0})$ and ", "page_idx": 40}, {"type": "text", "text": "$F(0)$ as the reference points and choose $B_{h}$ such that $B_{h}(F(\\theta_{0})-F(0))=0$ . Results in Figure 7 show that for different scales, the trajectory and the converging solution are the same. ", "page_idx": 41}, {"type": "image", "img_path": "BmG3NgH5xu/tmp/419fe0e3712f6d13a5c07c2d2507bfc204f65811a9912f1ab8d2ecaa4ece6455.jpg", "img_caption": ["Figure 9: Relative loss profile for all methods on Emotions and Music dataset. "], "img_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/c633f59a7839d39567bb77bac30e4e8d1d79d2fc7c35fb4d37b9c84118fa948d.jpg", "table_caption": ["Table 11: Summary of hyper-parameter choices for emotion recognition experiments. "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "BmG3NgH5xu/tmp/0e2e2ac384bd1e21aacaa8964a50be0d44ec13cf2c6b0879e783923a7356c69a.jpg", "table_caption": ["Table 12: Summary of average run time in seconds (s) or minutes ${(\\mathrm{m})}$ and number of iterations or epochs of different methods on different datasets. We use Algorithm 1 for the synthetic experiments, and Algorithm 3 for the other two experiments. "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "Emotion recognition. The task is to predict 6 types of emotions from 593 songs based on the Tellegen Watson-Clark model of affect. The 6 emotions include: amazed-surprised (E1), happy-pleased (E2), relaxing-calm (E3), quiet-still (E4), sad-lonely (E5), and angry-fearful (E6). Following [30], we use the fully connected neural network with 4 layers as the model architecture. The Sigmoid cross entropy loss is used as the objective for each task. And 10 preference vectors are generated uniformly. The hyperparameters used in this experiment are summarized in Table 11. ", "page_idx": 41}, {"type": "text", "text": "The results on the relative loss proflie (RLP) are reported in Figure 9. Results show that all methods, including LS, work similarly well. EPO achieves the highest hypervolumes, and our proposed approach obtains the second-best hypervolumes. One reason could be that the Pareto front in this problem is convex. ", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See Section 1, introduction. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See the Broader impacts and limitations section. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See Assumptions 1, 2, 3 for the assumptions, and the Appendix D, and G for the proof. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See Section 5 and Appendix H. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] Justification: Code is available at https://github.com/lisha-chen/FERERO/. ", "page_idx": 42}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See Section 5 and Appendix H. ", "page_idx": 42}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See Section 5. We use the standard deviations as the error bars for all experiments except the speech recognition experiments since the speech recognition experiments take much longer time to run. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See Section 5 and Appendix H. ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] Justification: We preserve anonymity. ", "page_idx": 43}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: See the end of the main paper in the Broader impacts and limitations section. ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: See Section 5 and Appendix H. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 43}]