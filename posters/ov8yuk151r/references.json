{"references": [{"fullname_first_author": "Johan SG Chu", "paper_title": "Slowed canonical progress in large fields of science", "publication_date": "2021-12-21", "reason": "This paper provides the overall context and importance of citation prediction in the field of science, setting the stage for the current study."}, {"fullname_first_author": "Xiao Yu", "paper_title": "Citation prediction in heterogeneous bibliographic networks", "publication_date": "2012-12-21", "reason": "This paper is a foundational work in citation prediction, and thus, highly relevant to the current study."}, {"fullname_first_author": "Arman Cohan", "paper_title": "SPECTER: document-level representation learning using citation-informed transformers", "publication_date": "2020-07-01", "reason": "This paper introduces the concept of citation-informed transformers which is relevant to the hybrid language model workflow used in the current paper."}, {"fullname_first_author": "Malte Ostendorff", "paper_title": "Neighborhood contrastive learning for scientific document representations with citation embeddings", "publication_date": "2022-12-01", "reason": "This paper proposes a similar method to the current paper but with different techniques and focuses; thus highly relevant for comparison"}, {"fullname_first_author": "Bowen Jin", "paper_title": "Patton: Language model pretraining on text-rich networks", "publication_date": "2023-07-01", "reason": "This paper provides a strong baseline that uses LLMs; thus highly relevant for comparison"}]}