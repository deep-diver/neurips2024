[{"type": "text", "text": "HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yancheng Zhang1, Mengxin Zheng1, Yuzhang Shang2, Xun Chen3, and Qian Lou1 ", "page_idx": 0}, {"type": "text", "text": "1University of Central Florida 2Illinois Institute of Technology 3Samsung Research America {yancheng.zhang,mengxin.zheng,qian.lou}@ucf.edu; yshang4@hawk.iit.edu; xunchen@outlook.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Non-interactive cryptographic computing, Fully Homomorphic Encryption (FHE), provides a promising solution for private neural network training on encrypted data. One challenge of FHE-based private training is its large computational overhead, especially the multiple rounds of forward and backward execution on each encrypted data sample. Considering the existence of largely redundant data samples, pruning them will significantly speed up the training, as proven in plain non-FHE training. Executing the data pruning of encrypted data on the server side is not trivial since the knowledge calculation of data pruning needs complex and expensive executions on encrypted data. There is a lack of FHE-based data pruning protocol for efficient, private training. In this paper, we propose, HEPrune, to construct a FHE data-pruning protocol and then design an FHE-friendly datapruning algorithm under client-aided or non-client-aided settings, respectively. We also observed that data sample pruning may not always remove ciphertexts, leaving large empty slots and limiting the effects of data pruning. Thus, in HEPrune, we further propose ciphertext-wise pruning to reduce ciphertext computation numbers without hurting accuracy. Experimental results show that our work can achieve a $16\\times$ speedup with only a $0.6\\bar{\\%}$ accuracy drop over prior work. The code is publicly available at https://github.com/UCF-Lou-Lab-PET/Private-Data-Prune. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning, especially deep neural networks, has been widely applied in various domains, including healthcare [1], finance [2], and so forth. Due to the lack of expertise and computational resources, the average user often outsources the training task to the cloud servers in the machinelearning-as-a-service (MLaaS) setting. However, the training data is often highly private and private, and it should not be directly shared because of business, legal, and ethical constraints. Private training enables the cloud server to train machine learning models on encrypted data, where strong privacy guarantees are offered by cryptographic primitives such as FHE [3]. ", "page_idx": 0}, {"type": "text", "text": "However, FHE-based private training is significantly hindered by its substantial execution time. For example, the encrypted training can be $1\\sim3$ orders of magnitudes slower than in the plaintext training [4, 5, 6]. The high execution time primarily stems from the transformation of a plaintext dataset into an encrypted format, along with the substitution of straightforward plaintext computations with more resource-intensive ciphertext operations. Reducing the number of data samples used for training could greatly accelerate the process. To optimize plaintext datasets, methods like dataset pruning are commonly employed, where the model trainer has access to both the dataset and model. ", "page_idx": 0}, {"type": "text", "text": "Dataset pruning involves analyzing data samples, calculating an importance score for each sample during training iterations, and discarding redundant samples to streamline training. Prior research indicates that dataset pruning can enable machine learning model training on a fraction of the data\u2014sometimes as little as $10\\%$ \u2014with minimal to no loss in accuracy [7, 8, 9, 10, 11]. ", "page_idx": 1}, {"type": "text", "text": "Pruning private datasets in encrypted form to accelerate training remains an open problem due to several challenges. First, calculating importance scores and sorting data samples for removal require complex encrypted computations, such as costly non-linear and sorting operations. For instance, calculating the importance score requires tracking changes in training dynamics [11, 12]\u2014such as logits, gradients, and losses\u2014and computing corresponding distances on encrypted data, all of which involve costly FHE operations. These high overheads that may offset\u2014or even surpass\u2014the potential benefits of dataset pruning. Second, plaintext-level data pruning used in the prior plaintext data pruning [7, 8, 9, 10, 11] does not always yield execution and ciphertext reduction beneftis, as a single ciphertext often contains multiple plaintext samples within its slots. Therefore, effective ciphertext pruning is critical to enable faster private training. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose HEPrune, the first encrypted data pruning framework for private training. We begin by constructing an encrypted data pruning method using pure FHE. To accelerate FHEenabled data pruning, we introduce a homomorphic encryption-friendly score (HEFS) to effectively evaluate the importance of data samples. To avoid costly FHE-based sorting, we propose client-aided masking (CAM) to identify less important samples while ensuring data privacy. To bridge the gap between sample-wise importance scores and ciphertext-wise data, we develop a ciphertext-wise pruning (CWP) technique to reduce the number of ciphertexts during private training. We conduct experiments in both training-from-scratch and transfer-learning settings. The proposed encrypted data pruning framework can accelerate private training by $16\\times$ with only a $0.6\\%$ accuracy drop. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Fully Homomorphic Encryption-Enabled Private Training ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Fully Homomorphic Encryption (FHE)[3] is a cryptographic primitive that supports arbitrary computation on encrypted data. An FHE workflow involves four functions: client-side KeyGen, Enc, Dec, and server-side Eval. For Eval, FHE natively supports homomorphic addition, subtraction, and multiplication between ciphertexts and plaintexts, denoted as \u229e, \u229f, and \u22a0, respectively. Nonlinear functions such as the comparison function, max function, SoftMax function, and square root function can be approximated using polynomials. We denote these functions as HE.Cmp[13, 14], HE.Max[14], HE.SoftMax[6, 15], and HE.Sqrt[16], respectively. FHE enables confidential training on encrypted datasets. While early works focused on training simple models on small datasets, such as linear regression and logistic regression models[17, 18, 19, 20], recent research has advanced towards training deep neural networks (DNNs) using HE. Although HE provides theoretical data privacy guarantees, privately training DNNs on large encrypted datasets typically incurs substantial computational overhead. For example, training a dense model for one step on a batch of 60 MNIST samples can take over 1.5 days in FHESGD [4]. ", "page_idx": 1}, {"type": "text", "text": "A line of research has been proposed to accelerate HE-enabled private training. Glyph [5] incorporates two HE schemes, TFHE and BGV, to enable private training in the transfer learning setting, allowing training of an MNIST mini-batch in only 0.04 hours. The most recent work, HETAL [6], achieves private training within 1 hour on the MNIST and CIFAR-10 datasets by using optimized matrix multiplication and GPU acceleration. While other cryptographic tools, such as Multi-Party Computation (MPC), can also achieve private training, most existing MPC approaches [21, 22, 23, 24] rely on a non-colluding server assumption, where multiple non-colluding servers are required to ensure security. Additionally, MPC-based methods typically incur significant communication overhead; training a LeNet model on the MNIST dataset, for instance, can generate approximately $500\\;G B$ of communication overhead [24]. In contrast, FHE-enabled private training does not require the non-collusion assumption and incurs significantly less communication overhead. ", "page_idx": 1}, {"type": "text", "text": "2.2 Dataset Redundancy and Pruning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Not all data samples contribute equally to DNN training [25]; some samples are less important and can be pruned during training. Training on only a subset of data can effectively reduce computational overhead while achieving generalization performance comparable to training on the full dataset [10, 11]. Several methods have been proposed to identify the most informative data samples. Among these, score-based methods [7, 8, 9, 10] are widely used for their simplicity and effectiveness. These methods compute sample-wise importance scores during training and select the most informative samples based on these scores. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The importance score can be as simple as the entropy loss [8, 10] for each sample. Another commonly used score is the forgetting score [7], which counts the number of \"forgetting events\" that occur for each sample during training, where a forgetting event is defined as a change in the model\u2019s prediction for the same sample between two consecutive epochs. GraNd [9] uses the $\\ell_{2}$ -norm of the sample-wise gradient to represent sample importance, while ELN2 [9] approximates GraNd by calculating the $\\ell_{2}$ -norm of the error vector. Additional methods for identifying sample importance include submodularity-based methods like GraphCut [26] and gradient-matching-based methods like GradMatch [12]. These methods typically start with an empty set and gradually add samples, in contrast to score-based methods, where sample importance scores are computed simultaneously and independently. ", "page_idx": 2}, {"type": "text", "text": "Data pruning methods are generally divided into static and dynamic pruning methods. Static pruning refers to data pruning performed once before training [7, 8, 9, 26], while dynamic pruning involves pruning data every few epochs during training [12, 11, 10]. Although these methods were designed with different motivations and target settings, they can be adapted for use in either setting. For instance, [8] uses entropy loss in a static setting, while [11, 10] apply it in a dynamic setting. Similarly, dynamic pruning methods like [26, 12] can be adapted to static settings, as demonstrated by [27], without losing effectiveness. In this paper, we focus on dynamic pruning. ", "page_idx": 2}, {"type": "image", "img_path": "y2fAmldTIf/tmp/7c9b5ee4c99a17d83183ac9a03f6bb5aa8990f237bce43db00a26705f8d0a499.jpg", "img_caption": ["Figure 1: (a). Test accuracy under various pruning fractions in plaintext dataset (ResNet-18 on CIFAR-10). (b) The encrypted training overhead and accuracy of different data pruning methods. (c) The distribution of data sample numbers in a ciphertext after data pruning. Only a small fraction of ciphertexts are empty with a na\u00efve sample-wise pruning. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Motivation. FHE-enabled private training provides a strong privacy guarantee for user data. However, private training remains significantly slower in runtime compared to unencrypted training. One primary reason for this is the large dataset size typically used for training, often comprising tens of thousands of samples. As shown in Figure 1(a), removing a fraction of data during training has minimal impact on model accuracy: even with only $30\\%$ of the data, the accuracy drop is within $2\\%$ . However, directly applying plaintext data pruning methods to private training might not only fail to accelerate the process but could actually extend the training time. As shown in Figure 1(b), computing importance scores with existing methods\u2014such as the forgetting score [7], entropy score [11, 10], GraNd [9], and EL2N [9]\u2014can introduce prohibitive overhead. This inefficiency arises because FHE natively supports only addition and multiplication, while existing data pruning methods often require a range of non-linear operations. Although these non-linear operations can be implemented in FHE through lookup tables or approximations, these computations are extremely costly in FHE, potentially negating the benefits of data pruning. Furthermore, as illustrated in Figure 1(c), although existing methods can effectively prune samples from datasets, they do not necessarily reduce the number of ciphertexts in private training. This is because, during private training, a single ciphertext can contain multiple samples. Na\u00efvely applying sample-wise data pruning can lead to a large number of sparse ciphertexts that cannot be excluded from training. ", "page_idx": 2}, {"type": "text", "text": "Threat Model. We consider a private training scenario where a client outsources model training to a cloud server. Our protocol is designed to allow a client to outsource model training while preserving the privacy of their data and model weights. The client\u2019s private dataset and model weights are treated as intellectual property that must not be disclosed to the server at any point. We assume the server is semi-honest, meaning it will follow the protocol specifications but may attempt to gain unauthorized knowledge, such as the dataset and model weights. Sharing certain meta-information about training, such as the model architecture, dataset size, and early stopping signal, is assumed to be safe [6]. Side-channel attacks are beyond the scope of this paper. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 HEPrune Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin by outlining the pipeline for private training with encrypted data pruning in Section 3.1. In Section 3.2, we provide details of our FHE-based data pruning algorithm. Specifically, we construct an encrypted data pruning baseline to demonstrate how sample importance evaluation and sample removal can be performed using pure FHE operations. We then enhance the efficiency of the data pruning algorithm through FHE-friendly scoring (HEFS) and client-aided masking (CAM). Finally, in Section 3.3, we introduce ciphertext-wise pruning (CWP), which removes sparse ciphertexts and further accelerates the data pruning process. ", "page_idx": 3}, {"type": "image", "img_path": "y2fAmldTIf/tmp/6786c7759d7fa61bb3c5a1b8b477e5a71bebd4362fd274622d98a0a2f3e2cff1.jpg", "img_caption": ["Figure 2: The overall workflow of private training with encrypted data pruning. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Pipeline of Private Training with Encrypted Data Pruning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It has been widely studied how to perform the gradient descent algorithm in the encrypted state during FHE-enabled private training [4, 5, 6]. However, it remains unclear how data pruning can be incorporated into existing private training frameworks. We illustrate our FHE-based encrypted data pruning framework in Figure 2. During private training, $\\pmb{\\mathrm{\\Sigma}}$ the client first encrypts the dataset with FHE and sends the encrypted dataset to the server. From this point forward, the dataset will never be decrypted. We refer to the encrypted data samples as ciphertexts. To identify the most informative subset of samples, $\\pmb{\\varphi}$ , the server first runs the forward pass algorithm on all ciphertexts to obtain the encrypted prediction vector $P$ . $\\pmb{\\otimes}$ With $P$ and the encrypted ground truth label $Y$ , the server computes the sample-wise importance score $S=\\mathcal{H}(x)$ . We propose an HE-friendly importance score in Section 3.2. $\\pmb{\\mathbb{\\otimes}}$ With the importance score, the server can decide which samples to prune. This is achieved by sorting the importance score and finding the threshold importance score $\\bar{\\mathcal{H}}$ corresponding to the pruning ratio [7]. A pruning mask is generated by comparing the importance score with the threshold as $M=\\mathbb{1}\\{\\mathcal{H}(x)<\\mathcal{H}_{t}\\}$ . The unimportant samples can be masked by simply multiplying the ciphertexts with the corresponding masks. $\\pmb{\\mathcal{\\Theta}}$ To fully reduce the number of ciphertexts, the server performs ciphertext-wise pruning according to the current pruning mask. $\\pmb{\\mathfrak{G}}$ The server performs a backpropagation algorithm on the pruned ciphertexts for $\\Delta\\tau$ epochs, and then $\\pmb{\\varphi}$ starts a new round of the data pruning algorithm to update the training subset. $\\pmb{\\varphi}$ At the end of private training, the server sends the encrypted model back to the client, and the client decrypts the model. ", "page_idx": 3}, {"type": "text", "text": "We formalize the above workflow into an encrypted data pruning protocol in Figure 5 in Appendix A. Our protocol is the first of its kind to introduce data pruning to private training and is an enhanced private training framework with encrypted data pruning extension over existing works [4, 5, 6]. Additionally, our protocol can work in both the transfer learning setting [5, 6] and training-fromscratch setting [4]. Our protocol is fully compatible with the early stop techniques adopted in [6]. As in [6], sending meta information during training is allowed in our protocol, such as the logits, early stop signals, and importance scores. The privacy of data and models is strongly guaranteed as they are encrypted and never decrypted by the server. We first instantiate a baseline of encrypted data pruning using only FHE operations in Section 3.2, and then propose HEFS and CAM to optimize the overhead of the encrypted data pruning algorithm. In Section 3.3, we propose CWP to effectively reduce the number of ciphertexts involved in training and thus boost the efficiency of private training. ", "page_idx": 3}, {"type": "text", "text": "3.2 HE-enabled Data Pruning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While FHE can support arbitrary computation on encrypted data, computing non-linear functions is typically expensive. Applying existing data pruning methods to private training can lead to additional overhead that negates or even exceeds the beneftis of performing data pruning. For the entropy-based methods [8, 11, 10], although the entropy loss is typically considered cost-free in the plaintext, it is not explicitly computed during private training. Instead, we compute the gradient directly without calculating the loss itself, as illustrated in Figure 5, step 3(b). Computing the entropy loss in FHE requires approximating the logarithm function, which is computationally expensive. The forgetting score [7] is simpler to compute, requiring only comparison operations between the current prediction and prediction from the last epoch. However, the forgetting score cannot be easily made dynamic, as it usually requires calculations over the full dataset for multiple epochs. The most viable existing data pruning method is the EL2N [9], which simply utilizes the prediction $P$ and label $Y$ . Although originally proposed as a static pruning method, we find that EL2N also remains effective in a dynamic setting, similar to the entropy score [10]. We first instantiate an FHE-enabled data pruning algorithm, detailed in Algorithm 1, based on the dynamic version of EL2N (HE2LN). Subsequently, we demonstrate that even EL2N remains computationally expensive in the encrypted state and propose the HE-friendly score (HEFS) and client-aided masking (CAM) to accelerate the encrypted data pruning. ", "page_idx": 4}, {"type": "image", "img_path": "y2fAmldTIf/tmp/ebf331b406075382f7c2a2422ef5b05c2b296154ac6e992e160cbd58da6caafd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "HEL2N Baseline. The EL2N score is defined as $\\mathcal{H}(\\boldsymbol{x})\\,=\\,\\mathbb{E}_{w_{t}}||p(\\boldsymbol{x};\\boldsymbol{w}_{t})\\,-\\,y||_{2}$ , where $p(x;w_{t})$ is the prediction vector for sample $x$ and $y$ the corresponding ground truth label. In essence, the EL2N score is the $\\ell_{2}$ -norm of the error vector, which can be computed via pure FHE operation as shown in Algorithm 1. We first compute the prediction vector $P$ by encrypted forward pass. The HE.SoftMax in the forward pass is evaluated by polynomial approximation [15] and domain extension technique [28]. Computing the forward pass in the encrypted state is the same as existing private training frameworks [6]. With the encrypted prediction vector $P$ and the ground truth label ", "page_idx": 4}, {"type": "text", "text": "$Y$ , we can compute the EL2N score homomorphically. Specifically, we first compute the sum of squares over the error vector, which requires homomorphic subtraction, multiplication, and rotation. Then we take the square root of sum of squares to compute the $\\ell_{2}$ -norm of the error vector. While HE.Sqrt can be implemented via Newton iterative algorithm [16], the overhead is prohibitive as the Newton iterative algorithm needs $\\sim10$ iterations to compute an accurate square root. Additionally, the Newton iterative algorithm in FHE involves considerable ciphertext-ciphertext multiplication. This leads to a large number of additional relinearization and bootstrapping operations. A single square root over a ciphertext can take up to 2 minutes. ", "page_idx": 5}, {"type": "text", "text": "HE-friendly Score. Simple as the EL2N score is, using it to evaluate the importance of data samples in the encrypted state can still incur prohibitive overhead due to its complex non-linearity. We propose an HE-friendly importance score (HEFS) to address this issue. We first derive the formulation of HEFS, and then demonstrate how it can be computed via pure FHE operation. To determine how a single data sample affects the training, we can quantify the importance of a sample by the difference of the gradient before and after removing a sample. Denote the gradient of a sample $(x,y)$ over the weights at time $t$ as $g_{t}(x,y)\\,=\\,\\nabla_{w_{t}}\\ell(p(w_{t-1},x),y)$ . Given a minibatch of $B$ samples ${\\cal S}=\\{(x_{i},y_{i})\\}_{i=0}^{B-1}$ , the importance of a sample can be quantified by the difference of the time derivative of the loss function, $\\Delta_{t}$ , before and after removing the sample from the minibatch. The difference of the derivative is bounded by the sample\u2019s gradient [9]. Specifically, let $S_{\\neg k}=S\\setminus\\{(x_{k},y_{k})\\}$ be the set after removing a certain sample $(x_{k},y_{k})$ . For $\\forall(x_{i},y_{i})\\in\\Bar{S}$ and $i\\neq k$ , it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\Delta_{t}((x_{i},y_{i}),S)-\\Delta_{t}((x_{i},y_{i}),S_{-k})\\|_{2}\\leq c\\|g_{t}(x_{k},y_{k})\\|_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "EL2N approximates the $\\ell_{2}$ -norm of the gradient by the $\\ell_{2}$ -norm of the error vector. We further streamline the EL2N score using the $\\ell_{1}$ -norm. More formally, we define the HEFS as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\boldsymbol{x})=\\mathbb{E}_{w_{t}}||p(\\boldsymbol{x};\\boldsymbol{w}_{t})-\\boldsymbol{y}||_{1}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "HEFS can be efficiently computed during private training. Specifically, the circuit for HEFS consists of only two FHE subtractions and one max operation, which can be computed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns c o r e=\\mathsf{H E.M a x}((Y\\boxminus P),(P\\boxplus Y))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the above equation, the homomorphic subtraction $\\boxminus$ is significantly faster than other FHE operations. $\\mathsf{H}\\bar{\\mathsf{E}}.\\mathsf{M}\\mathsf{a}\\mathsf{x}$ can be effectively computed via ${\\mathsf{H E}}.{\\mathsf{C m p}}$ . While the $\\mathsf{H E.M a x}$ operation has the same time complexity as ${\\mathsf{H E}}.{\\mathsf{C m p}}$ , the concrete runtime of $\\mathsf{H E.M a x}$ is even more efficient, typically $1.5\\,\\sim\\,2\\times$ faster under the same parameter setting [14]. The proposed HEFS is a close approximation to the original EL2N score, which guarantees the effectiveness of the HEFSbased data pruning. We show the accuracy of the data pruning using HEFS in Section 4. ", "page_idx": 5}, {"type": "table", "img_path": "y2fAmldTIf/tmp/c624228a23737ff3a1b5e46f7173e615509a8a24bd8f17edd009c77bb1a30459.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Client-aided Masking. After evaluating the importance of each data sample, we need to remove the less informative ones from the training set. This requires the server to sort all importance scores homomorphically to determine the threshold of important scores, $\\bar{\\mathcal{H}}$ . Given a dataset with $N$ data samples, $\\bar{O}(N^{2})$ homomorphic comparisons are needed. Since $N$ is typically large for machine learning model training, such sorting incurs prohibitive overhead in the encrypted state, which can offset the beneftis of data pruning or even prolong the total training time. To effectively identify and remove the less important data samples, we propose client-aided masking (CAM) in Algorithm 2. We offer an analysis on the computation, communication overhead, and security implications as follows. ", "page_idx": 5}, {"type": "text", "text": "Efficiency. In contrast to the heavy server-side homomorphic sorting, finding $\\mathcal{H}_{t}$ is extremely fast on the client\u2019s side, with only $O(N)$ runtime via the quick select algorithm. In practice, this process takes only $15\\:m s$ on the CIFAR-10 dataset. Additionally, the communication overhead is minimal. ", "page_idx": 5}, {"type": "text", "text": "For the CIFAR-10 dataset with 43, 750 training samples, only 2 CKKS ciphertexts are needed to transfer the encrypted importance scores when the slots are fully utilized. This incurs only $^{4\\,\\mathrm{MB}}$ of communication overhead. For comparison, the early stopping signals used to determine early stopping incur $18\\,\\mathrm{MB}$ communication overhead when transferring the encrypted logits [6]. ", "page_idx": 6}, {"type": "text", "text": "Security. Directly asking the client to decrypt and reveal the model weights and datasets to the server clearly breaches the client\u2019s privacy and is therefore prohibited. However, exchanging meta information about training does not directly compromise private information and is typically permitted. For instance, HETAL [6] allows the server to send the logits of the validation set to the client, who then computes the loss and determines whether to stop training early. Such exchanges of meta information are crucial to ensure the effectiveness of private training. Further details can be found in Appendix B. ", "page_idx": 6}, {"type": "image", "img_path": "y2fAmldTIf/tmp/702bf4a8b943bdb2a2bb59e2a6de299a3ef42ae674fada25be8abe8394124476.jpg", "img_caption": ["Figure 3: Example of ciphertext-wise pruning. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.3 Ciphertext-wise Pruning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the HEFS and CAM make the encrypted data pruning algorithm much more practical, the resulting ciphertexts remain largely sparse, thus limiting the training time acceleration. To this end, we propose ciphertext-wise pruning to effectively reduce the number of ciphertexts involved in training, as illustrated in Figure 3, which further boosts the efficiency of the private training. We detail the steps of ciphertext-wise pruning in Algorithm 3. Without loss of generality, we refer to all the slots occupied by a single sample within a ciphertext as a sample slot. Once the server obtains the pruning mask, ciphertext-wise pruning can be performed without client involvement. ", "page_idx": 6}, {"type": "text", "text": "We denote the number of samples in each ciphertext as $B$ and $M_{i}$ is a Boolean array that indicates whether each of the $B$ samples should be pruned.\u2776The server first computes the sparsity of each ciphertext, which is done by simply counting the number of 0-s in each $M_{i}$ . $\\pmb{\\varphi}$ After computing the ciphertext-wise sparsity, the server can sort the ciphertexts along with their corresponding masks. We represent the sorted ciphertexts in a dequeue. To perform ciphertextwise pruning, the server first identifies two ciphertexts $c t_{f r o n t}$ and $c t_{b a c k}$ from the queue. $\\pmb{\\otimes}$ After removing the full ciphertexts and empty ciphertexts from the queue via ${\\mathsf{T r i m}}()$ , we then leverage the most sparse ciphertext $c t_{b a c k}$ to flil in the empty slots in the least sparse ciphertexts ", "page_idx": 6}, {"type": "table", "img_path": "y2fAmldTIf/tmp/5a855e7a336d8628ade3511728f4eba185bb8f56184e9016fb5e1fd7d69ed4cd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "return D\u00af\u2032 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "$c t_{f r o n t}$ . The key step is to align samples in $c t_{b a c k}$ with the empty slots in $c t_{f r o n t}$ . The function $\\mathsf{F Z e r o()}$ returns the index of the first empty sample slot $s l o t_{e m p t y}$ in $c t_{f r o n t}$ , sets the corresponding mask to 1, and decreases the sparsity of $c t_{f r o n t}$ by 1. Similarly, $\\mathsf{F O n e}()$ returns the first non-empty sample slot $s l o t_{u s e d}$ in $c t_{b a c k}$ and sets the corresponding mask and sparsity. By taking the difference of $s l o t_{u s e d}$ and $s l o t_{e m p t y}$ , the server can determine how much the $c t_{b a c k}$ should be rotated to align with $c t_{f r o n t}$ . We mask out the other slots in $c t_{b a c k}$ and obtain $c t_{a l i g n}$ and keep only the first sample slot $s l o t_{u s e d}$ . This ensures only the empty sample slot in $c t_{f r o n t}$ is fliled and the non-empty slots in $c t_{f r o n t}$ will not be corrupted. Then, $c t_{a l i g n}$ is rotated and added to $c t_{f r o n t}$ to flil in the empty slot in $c t_{f r o n t}$ . After merging the two ciphertexts, if $c t_{f r o n t}$ is full or there are no remaining ciphertexts to merge, the server adds it to the final set ${\\bar{D^{\\prime}}}$ . We defer the details about $\\mathsf{T r i m}(),\\mathsf{F Z e r o}(),\\mathsf{\\bar{F}o n e}()$ and $\\mathsf{M a s k()}$ to the Appendix D. We remark the pruning mask generated by CAM is not encrypted, while enables efficient operations related to the pruning mask without invoking heavy FHE computation. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Models and Datasets. To demonstrate the generalizability, we evaluate the proposed encrypted data pruning methods in two settings, the transfer learning setting as in HETAL [6] and trainingfrom-scratch setting as in FHESGD [4]. For the transfer learning setting, we adopt the same feature extractors as [6], the pre-trained ViT Base [29] model and the pre-trained MPNet Base [30] model. The client first extracts samples into a 768-dimension feature and then encrypts the features. The server performs private training on the encrypted features. For the training-from-scratch setting, the client encrypts the raw dataset directly, and the server performs private training on the encrypted dataset. We use a 3-layer MLP, with two hidden layers of dimension 128, which is a widely used structure in the private training setting [4, 21]. We perform encrypted data pruning on four widely used image datasets: MNIST [31], CIFAR-10 [32], Face Mask Detection [33], DermaMNIST [34] for image classification and one audio dataset SNIPS [35] for sentiment analysis. We partition the datasets as training, validation, and test set. The size of the training set and validation set is $7:1$ . We defer the number of samples in each set in the Appendix C. ", "page_idx": 7}, {"type": "text", "text": "System Setup and Implementation. We use the RNS version of the CKKS [36, 37] scheme for homomorphic encryption. We use the bootstrapping method for CKKS in [38]. Our implementation is based on the OpenFHE [39] library. For HE parameters, we set the cyclotomic ring dimension as $N\\,=\\,2^{16}$ and ciphertext modulus 1555 bits to guarantee a security level of 128-bit under the Homomorphic Encryption Standard [40]. Each ciphertext has $N/2=32768$ slots and we set the multiplicative depth as $L=12$ . All experiments were conducted using an AMD Ryzen Threadripper PRO 3955WX processor operating at 2.2GHz, equipped with 125GB of memory. We use the SingleInstruction-MultipleData (SIMD) technique [41] to fully utilize the ciphertext slots and amortize the cost of homomorphic operations. Under SIMD, multiple data samples can be coded into one ciphertext. We adopt the most efficient encoding methods proposed in [6, 42]. For nonlinear operations like SoftMax and ReLU, we adopt approximation-based methods. We report the CPU version of HETAL, as their GPU implementation is not publicly available. ", "page_idx": 7}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "End-to-end performance. As shown in Table 1, we compare the end-to-end training time and accuracy on five datasets with HETAL [6] and an unencrypted baseline. HETAL is known to be the most efficient full-data private training framework. For fair comparison, we unify the batch size as 128. For HETAL, we maintain the security parameters used in their original paper. For the proposed encrypted data pruning method, we set the pruning ratio as $p=0.9$ , i.e., only $\\bar{10\\%}$ of the data remains for training in each epoch. The total training time is reduced by $\\sim6.6\\times$ across datasets. The accuracy drop is as small as $\\bar{0}.25\\%$ on the Face Mask Detection dataset. With encrypted data pruning, the accuracy is even $0.14\\%$ higher than the unencrypted baseline and HETAL. ", "page_idx": 7}, {"type": "table", "img_path": "y2fAmldTIf/tmp/840db05b716d0790e6f2fd8763910fdd8c17815d00fba867920598f1b8d93756.jpg", "table_caption": ["Table 1: End-to-end comparison across different datasets The pruning ratio is set as $p=0.9$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Effectiveness of the proposed method. We tested different private training methods on the CIFAR-10 dataset, with a pruning ratio $p=0.9$ . Communication is the size of logits and importance score ciphertexts server sends to client for early stopping and data pruning. ", "page_idx": 8}, {"type": "table", "img_path": "y2fAmldTIf/tmp/656ef1faa3dae301b36704421f26a73604d86700d1723b26d02019c2de44c05f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Effectiveness of the proposed methods. In Table2, we demonstrate the effectiveness of each proposed technique. We start by training on the full CIFAR-10 dataset using the framework of HETAL. For the encrypted data pruning methods, we fix the pruning ratio as $p=0.9$ . In the prune baseline, we naively apply plaintext data pruning method to HETAL, which prolongs the total training time. Sorting 43750 importance score in CIFAR-10 can take more than 200 hours, which offsets the benefits of data pruning. By incorporating the client-aided masking, the server can remove the unimportant samples more effectively. The client-aided methods speed up the private training by $1.49\\times$ with a $0.4\\bar{1}\\%$ accuracy drop. The communication cost increases slightly by approximately $1.2\\times$ . Yet, as the EL2N score involves costly square root computation, the speed-up is modest. By incorporating HEFS, we improve the overhead during importance score computation. With HEFS, the private training can be accelerated by $2.78\\times$ . The HEFS also achieves a $\\iota\\sim\\bar{0}.2\\%$ higher accuracy. When ciphertext-wise pruning is applied, the runtime speed-up is the most significant, achieving a $6.5\\times$ speed-up over HETAL with only $0.2\\%$ accuracy drop. ", "page_idx": 8}, {"type": "image", "img_path": "y2fAmldTIf/tmp/f3160775eaecc87f969192ff001161075deafc7ddc3bdd57d7063d7bf5547f1c.jpg", "img_caption": ["Figure 4: Private training time and accuracy with different fractions of data of (a) CIFAR-10 dataset and (b) MNIST dataset "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablation on the pruning ratio. In Figure 4, we demonstrate the effectiveness of the proposed encrypted data pruning method under different data pruning ratios. In Figure 4 (a), we demonstrate the training time and accuracy achieved using different fractions of the CIFAR-10 dataset. Surprisingly, we find that even using only $1\\%$ of the data, the accuracy drop is only around $0.6\\%$ , and the training can be sped up by $\\sim16\\times$ . Training with the $40\\%\\sim70\\%$ of the data even leads to higher accuracy than training with the full dataset. The same phenomenon is observed in plaintext data pruning [9]. This is because some noisy or low-quality samples are excluded from training while retaining enough informative samples are maintained. We note that we can achieve $\\sim2.2\\times$ speed up when training with $40\\%$ of the data without any loss of accuracy on the CIFAR-10 dataset. In Figure 4 (b), we experiment with the MNIST dataset and observe a similar trend. On the MNIST dataset, using only $1\\%$ of the data achieves $91.29\\%$ accuracy with $\\sim15\\times$ speed-up. Yet, using $5\\%$ of the data achieves a significantly higher accuracy of $95.2\\%$ , which is only $0.5\\%$ lower than training with the full dataset. ", "page_idx": 8}, {"type": "table", "img_path": "y2fAmldTIf/tmp/892445a6d784c0a8d94f655e58e84f57560aafedb2ad34f4fc86a8f0e3a482c3.jpg", "table_caption": ["Table 3: Privately Training an MLP from scratch under different data pruning ratios. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Training from scratch. We show the performance of encrypted data pruning in the training from scratch setting in Table 3. We train a 3-layer MLP on the MNIST dataset. We set the pruning frequency as 10 epoch. When training with only $1\\%$ data, the end-to-end training time could be $60.8\\times$ faster with an accuracy drop of $5.26\\%$ . Increasing the training data fraction can effectively improve the test accuracy. When using $40\\%$ and $50\\%$ fraction of data, the training accuracy is $0.03\\%\\sim0.06\\%$ higher than training with the full dataset. We remark that warm-start strategy can also improve the test accuracy. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Broader Impact. In this paper, we propose a framework that enables encrypted data pruning during confidential training. The proposed techniques can accelerate the private training without sacrificing the model accuracy. By incorporating data homomorphic friendly data pruning techniques, our framework makes the HE-enabled private training more practical while ensuring the data privacy. ", "page_idx": 9}, {"type": "text", "text": "Limitations. (1) More Model Support. Currently, the proposed methods have only been applied to simple models like MLP. Extending the encrypted data pruning to private training on larger models like CNNs and Transformers can enhance its utility. (2) More Dataset Support. In this paper, we have experimented on relatively small datasets. Extending to larger datasets will enhance its utility. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The paper presents a novel framework for encrypted data pruning aimed at enhancing private training of deep neural networks. Plaintext data pruning methods offer limited beneftis in encrypted settings due to their lack of optimization for cryptographic processes. Our approach introduces crypto-oriented optimizations, including HE-friendly score, client-aided masking, and ciphertext-wise pruning, which effectively harness the potential of data pruning, achieving up to a 16-fold acceleration in training times without compromising accuracy. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical image analysis. Annual review of biomedical engineering, 19:221\u2013248, 2017.   \n[2] Matthew F Dixon, Igor Halperin, and Paul Bilokon. Machine learning in finance, volume 1170. Springer, 2020.   \n[3] Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 169\u2013178, 2009.   \n[4] Karthik Nandakumar, Nalini Ratha, Sharath Pankanti, and Shai Halevi. Towards deep neural network training on encrypted data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 0\u20130, 2019.   \n[5] Qian Lou, Bo Feng, Geoffrey Charles Fox, and Lei Jiang. Glyph: Fast and accurately training deep neural networks on encrypted data. Advances in neural information processing systems, 33:9193\u20139202, 2020.   \n[6] Seewoo Lee, Garam Lee, Jung Woo Kim, Junbum Shin, and Mun-Kyu Lee. Hetal: Efficient privacy-preserving transfer learning with homomorphic encryption. In International Conference on Machine Learning, pages 19010\u201319035. PMLR, 2023.   \n[7] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.   \n[8] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. arXiv preprint arXiv:1906.11829, 2019.   \n[9] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important examples early in training. Advances in Neural Information Processing Systems, 34:20596\u201320607, 2021.   \n[10] Truong Thao Nguyen, Balazs Gerof,i Edgar Josafat Martinez-Noriega, Fran\u00e7ois Trahay, and Mohamed Wahib. Kakurenbo: Adaptively hiding samples in deep neural network training. Advances in Neural Information Processing Systems, 36, 2024.   \n[11] Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, et al. Infobatch: Lossless training speed up by unbiased dynamic data pruning. arXiv preprint arXiv:2303.04947, 2023.   \n[12] Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Grad-match: Gradient matching based data subset selection for efficient deep model training. In International Conference on Machine Learning, pages 5464\u20135474. PMLR, 2021.   \n[13] Jung Hee Cheon, Dongwoo Kim, and Duhyeong Kim. Efficient homomorphic comparison methods with optimal complexity. In Advances in Cryptology\u2013ASIACRYPT 2020: 26th International Conference on the Theory and Application of Cryptology and Information Security, Daejeon, South Korea, December 7\u201311, 2020, Proceedings, Part II 26, pages 221\u2013256. Springer, 2020.   \n[14] Eunsang Lee, Joon-Woo Lee, Jong-Seon No, and Young-Sik Kim. Minimax approximation of sign function by composite polynomial for homomorphic comparison. IEEE Transactions on Dependable and Secure Computing, 19(6):3711\u20133727, 2021.   \n[15] Joon-Woo Lee, HyungChul Kang, Yongwoo Lee, Woosuk Choi, Jieun Eom, Maxim Deryabin, Eunsang Lee, Junghyun Lee, Donghoon Yoo, Young-Sik Kim, et al. Privacy-preserving machine learning with fully homomorphic encryption for deep neural network. iEEE Access, 10:30039\u2013 30054, 2022.   \n[16] Hongyuan Qu and Guangwu Xu. Improvements of homomorphic evaluation of inverse square root. Available at SSRN 4258571.   \n[17] Valeria Nikolaenko, Udi Weinsberg, Stratis Ioannidis, Marc Joye, Dan Boneh, and Nina Taft. Privacy-preserving ridge regression on hundreds of millions of records. In 2013 IEEE symposium on security and privacy, pages 334\u2013348. IEEE, 2013.   \n[18] Hao Chen, Ran Gilad-Bachrach, Kyoohyung Han, Zhicong Huang, Amir Jalali, Kim Laine, and Kristin Lauter. Logistic regression over encrypted data from fully homomorphic encryption. BMC medical genomics, 11:3\u201312, 2018.   \n[19] Andrey Kim, Yongsoo Song, Miran Kim, Keewoo Lee, and Jung Hee Cheon. Logistic regression model training based on the approximate homomorphic encryption. BMC medical genomics, 11:23\u201331, 2018.   \n[20] Miran Kim, Yongsoo Song, Shuang Wang, Yuhou Xia, Xiaoqian Jiang, et al. Secure logistic regression based on homomorphic encryption: Design and evaluation. JMIR medical informatics, 6(2):e8805, 2018.   \n[21] Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine learning. In 2017 IEEE symposium on security and privacy $(S P)$ , pages 19\u201338. IEEE, 2017.   \n[22] Payman Mohassel and Peter Rindal. Aby3: A mixed protocol framework for machine learning. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security, pages 35\u201352, 2018.   \n[23] Sameer Wagh, Shruti Tople, Fabrice Benhamouda, Eyal Kushilevitz, Prateek Mittal, and Tal Rabin. Falcon: Honest-majority maliciously secure framework for private deep learning. arXiv preprint arXiv:2004.02229, 2020.   \n[24] Jean-Luc Watson, Sameer Wagh, and Raluca Ada Popa. Piranha: A $\\{{\\mathrm{GPU}}\\}$ platform for secure computation. In 31st USENIX Security Symposium (USENIX Security 22), pages 827\u2013844, 2022.   \n[25] Angelos Katharopoulos and Fran\u00e7ois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In International conference on machine learning, pages 2525\u20132534. PMLR, 2018.   \n[26] Rishabh Iyer, Ninad Khargoankar, Jeff Bilmes, and Himanshu Asanani. Submodular combinatorial information measures with applications in machine learning. In Algorithmic Learning Theory, pages 722\u2013754. PMLR, 2021.   \n[27] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset selection in deep learning. In International Conference on Database and Expert Systems Applications, pages 181\u2013195. Springer, 2022.   \n[28] Jung Hee Cheon, Wootae Kim, and Jai Hyun Park. Efficient homomorphic evaluation on large intervals. IEEE Transactions on Information Forensics and Security, 17:2553\u20132568, 2022.   \n[29] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[30] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33:16857\u201316867, 2020.   \n[31] Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine, 29(6):141\u2013142, 2012.   \n[32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[33] Larxel. Face mask detection, 2020. Accessed: May 20, 2024.   \n[34] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023.   \n[35] Alice Coucke, Alaa Saade, Adrien Ball, Th\u00e9odore Bluche, Alexandre Caulier, David Leroy, Cl\u00e9ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, et al. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190, 2018.   \n[36] Jung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song. Homomorphic encryption for arithmetic of approximate numbers. In Advances in Cryptology\u2013ASIACRYPT 2017: 23rd International Conference on the Theory and Applications of Cryptology and Information Security, Hong Kong, China, December 3-7, 2017, Proceedings, Part I 23, pages 409\u2013437. Springer, 2017.   \n[37] Jung Hee Cheon, Kyoohyung Han, Andrey Kim, Miran Kim, and Yongsoo Song. A full rns variant of approximate homomorphic encryption. In Selected Areas in Cryptography\u2013SAC 2018: 25th International Conference, Calgary, AB, Canada, August 15\u201317, 2018, Revised Selected Papers 25, pages 347\u2013368. Springer, 2019.   \n[38] Jung Hee Cheon, Kyoohyung Han, Andrey Kim, Miran Kim, and Yongsoo Song. Bootstrapping for approximate homomorphic encryption. In Advances in Cryptology\u2013EUROCRYPT 2018: 37th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Tel Aviv, Israel, April 29-May 3, 2018 Proceedings, Part I 37, pages 360\u2013384. Springer, 2018.   \n[39] Ahmad Al Badawi, Jack Bates, Flavio Bergamaschi, David Bruce Cousins, Saroja Erabelli, Nicholas Genise, Shai Halevi, Hamish Hunt, Andrey Kim, Yongwoo Lee, Zeyu Liu, Daniele Micciancio, Ian Quah, Yuriy Polyakov, Saraswathy R.V., Kurt Rohloff, Jonathan Saylor, Dmitriy Suponitsky, Matthew Triplett, Vinod Vaikuntanathan, and Vincent Zucca. Openfhe: Opensource fully homomorphic encryption library. Cryptology ePrint Archive, Paper 2022/915, 2022. https://eprint.iacr.org/2022/915.   \n[40] Martin Albrecht, Melissa Chase, Hao Chen, Jintai Ding, Shaf iGoldwasser, Sergey Gorbunov, Shai Halevi, Jeffrey Hoffstein, Kim Laine, Kristin Lauter, et al. Homomorphic encryption standard. Protecting privacy through homomorphic encryption, pages 31\u201362, 2021.   \n[41] Nigel P Smart and Frederik Vercauteren. Fully homomorphic simd operations. Designs, codes and cryptography, 71:57\u201381, 2014.   \n[42] Eric Crockett. A low-depth homomorphic circuit for logistic regression model training. Cryptology ePrint Archive, 2020.   \n[43] Baiyu Li and Daniele Micciancio. On the security of homomorphic encryption on approximate numbers. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pages 648\u2013677. Springer, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Encrypted data pruning protocol ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we detail the proposed encrypted data pruning protocol. The main overhead associated with performing performing encrypted data pruning is determined by Step 3.(d) in Figure 5. ", "page_idx": 13}, {"type": "text", "text": "Parties: Client $C$ , Server $S$ .   \nInput: $C$ holds a private dataset $D$ , with $N$ samples and labels $\\{(X_{i},Y_{i})\\}_{i=0}^{N-1}$ .   \nOutput: $C$ learns a trained model $\\mathcal{M}$ and $S$ learns nothing.   \nProtocol:   \n1: $C$ encrypts the dataset with HE.Enc, and sends the encrypted dataset $\\bar{D}=\\{\\bar{X}_{i},\\bar{Y}_{i}\\}_{i=0}^{C-1}$ to the server.   \n2: $S$ initializes $\\mathcal{M}$ with random parameters $W_{0}$ and chooses the hyperparamerters for training: the learning rate $\\eta$ , pruning ratio $p$ and the pruning frequency $\\Delta\\tau$ .   \n3: For each epoch $\\tau$ , repeat the following: (a) $S$ performs forward pass homomorphically: $P=\\mathsf{H E.S o f t M a x}(\\mathcal{M}(X,W))$ (b) $S$ computes gradients and updates model: $W_{\\tau+1}\\leftarrow W_{\\tau}+\\eta\\nabla_{W}$ , where $\\begin{array}{r}{\\nabla_{W}\\mathcal{L}_{C E}=\\frac{1}{N}(P-Y)^{T}X}\\end{array}$ (c) $S$ sends $P$ to $C$ , $C$ determines early stop. (d) If $\\tau$ mod $\\Delta\\tau==0$ ), $S$ performs data pruning as follows: (i) $S$ computes the importance score: $s c\\bar{o}r e=\\mathsf{P r u n e.E v a l}(\\bar{P},\\bar{Y})$ (ii) $S$ sendssco\u00afre to $C$ , can $C$ computes a pruning mask mask. (iii) $S$ removes unimportant samples: $\\bar{D^{\\prime}}=\\mathsf{P r u n e.R e m o v e}(\\bar{D},m a s k)$ (iv) Continue step 3(a) to train the model on the pruned dataset $\\bar{D^{\\prime}}=\\{\\bar{X^{\\prime}}_{i},\\bar{Y^{\\prime}}_{i}\\}_{i=0}^{C^{\\prime}-1}$   \n4: $S$ sends the trained model $\\mathcal{M}$ with encrypted optimal weights $\\bar{W}^{*}$ to $C$ . $C$ decrypts the model with HE.Dec and obtain the final model $\\mathcal{M}$ with optimal plaintext weights $W^{*}$ . ", "page_idx": 13}, {"type": "text", "text": "B Analysis of Client-aided Masking ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we offer more details about the proposed Client-aided Masking (CAM). ", "page_idx": 13}, {"type": "text", "text": "The main privacy issue is that (1) the server may learn private information based on the meta information, (2) the server may crack the secret key. Exposing the pruning mask mask to the server only reveals the location of unimportant data samples, while all samples are still encrypted by HE, hence the server still learns nothing about the data sample itself, which avoids the security issue (1). One of the most recent and well-known attacks is the IND-CPA $^{+}$ [43]. However, it operates under the assumption that the server can ask the client to decrypt specific ciphertexts. The assumption is not upheld in Algorithm 2. Since the server can not specify the ciphertext to be decrypted and that the client only sends the server the resulting mask, rather than the decrypted scores. ", "page_idx": 13}, {"type": "text", "text": "The communication depends on the level of ciphertexts. As shown in Table 4, the lower level the ciphertext is at, the smaller the ciphertext is. Therefore, the server can set the ciphertext to $L=0$ to reduce the communication overhead. ", "page_idx": 13}, {"type": "table", "img_path": "y2fAmldTIf/tmp/9741642b924d37ae2a9a2a76abef2292240d220bd8a73dc563517e542fa07f86.jpg", "table_caption": ["Figure 5: Private Data Pruning Protocol. ", "Table 4: Ciphertext sizes under different levels. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Input :The encrypted dataset $\\bar{D}=\\{\\bar{X}_{i},\\bar{Y}_{i}\\}_{i=0}^{C-1}$ and pruning masks $m a s k=\\{M_{i}\\}_{i=0}^{C-1}$ , where $M_{i}\\in\\{0,1\\}^{B}$ and $B$ is the number of samples in each ciphertexts.   \nOutput :The pruned encrypted dataset $\\bar{D^{\\prime}}=\\{\\bar{X^{\\prime}}_{i},\\bar{Y^{\\prime}}_{i}\\}_{i=0}^{C^{\\prime}-1}$ , where $C^{\\prime}\\leq C$ . ;   \n$\\begin{array}{r}{\\ L\\ s p a s i t y_{i}\\gets B-\\sum_{j=0}^{B-1}M_{i}[j];}\\end{array}$ $i\\leftarrow0$ $C-1$ ; // compute the ciphertext-wise sparsity   \n$c t\\_q u e u e\\gets S o r t(s p a s i t y,\\bar{D},m a s k)$ ;   \nwhile ct_queue.is_not_empty() do $c t\\_f r o n t\\leftarrow c t\\_q u e u e.p o p\\_f r o n t();\\ //\\ \\mathrm{The}$ ${\\sf T r i m}()$ removes full and empty ciphertexts from the ct_queue if ct_front.sparsity $\\mathrm{\\Delta}==0$ then $\\bar{D^{\\prime}}\\leftarrow\\bar{D}^{\\prime}\\cup\\{c t\\_f r o n t.c t\\}$ ; continue; else ct_queue.push_front(ct_front); break;   \nwhile ct_queue.is_not_empty() do ct_back \u2190ct_queue.pop_back(); if ct_back.sparsity $\\scriptstyle==B$ then continue; else ct_queue.push_back(ct_back); break;   \nwhile ct_queue.is_not_empty() do $c t_{f r o n t}\\gets c t\\_q u e u e.p o p\\_f r o n t();$ ; $c t_{b a c k}\\gets c t\\textunderscore q u e u e.p o p\\textunderscore b a c k()$ ; $s l o t_{e m p t y}\\gets0$ ; // The FZero() returns index of the first 0 in ctfront for $s l o t\\gets0$ to $B-1$ do if $c t\\_f r o n t.m a s k[s l o t]==0$ then $s l o t_{e m p t y}\\gets s l o t$ ; $c t\\_f r o n t.m a s k[s l o t]\\gets1;$ ; $c t\\_f r o n t.s p a r s i t y\\ -=1;$ $s l o t_{u s e d}\\gets0$ ; // The FOne() returns index of the first 1 in ctback for $s l o t\\gets0$ to $B-1$ do if ct ${\\_b a c k.m a s k[s l o t]=}{=1}$ then $s l o t_{u s e d}\\gets s l o t$ ; ct_back.mask[slot] \u21900; ct_back.sparsity $+\\mathrm{=}\\,1$ ; $s l o t_{u s e d}\\gets\\mathsf{F O n e}(c t_{b a c k});$ $k\\leftarrow s l o t_{u s e d}-s l o t_{e m p t y};$ ; $M_{t}\\gets\\{0\\}^{B}$ ; // The Mask() keep only the sample at slotused in ctback $M_{t}[s l o t_{u s e d}]\\gets1$ ; $c t_{a l i g n}\\leftarrow c t_{b a c k}$ \u22a0 $M_{t}$ ; $c t_{f r o n t}\\leftarrow c t_{f r o n},$ t \u229eH $\\Xi.\\mathsf{R o t}(c t_{a l i g n},k)$ ; if ct_queue.is_empty $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ then $\\bar{D^{\\prime}}\\leftarrow\\bar{D^{\\prime}}\\cup\\{c t_{f r o n t}.c t\\}$ ; else ct_queue.push_front(ctfront); ct_queue.push_back(ctback);   \nreturn $\\bar{D^{\\prime}}$ ", "page_idx": 14}, {"type": "text", "text": "C Datasets and Hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We detail how we divide the training set, validation set and test set in Table 5. For all tasks, we use a batch size of 128. For the transfer learning setting, we use a learning rate of 0.5 and set the pruning frequency as every $\\Delta_{\\tau}=5$ epochs. For the training from scratch setting, we use a leaning rate of ", "page_idx": 14}, {"type": "text", "text": "0.1 and set the pruning frequency as every $\\Delta_{\\tau}=10$ epochs. Prior works [9, 12] adopt a warm start strategy, which trains the model for $10\\sim20\\$ epochs on the full dataset. The warm start period is computation intensive as it uses the whole dataset. Therefore, we use a random start strategy and randomly choose a fraction of data samples according to the pruning ratio $p$ . ", "page_idx": 15}, {"type": "table", "img_path": "y2fAmldTIf/tmp/fbb8f00db920126d4bcc2fe2437bece126fb00dd902088dc1fc550794aeb3878.jpg", "table_caption": ["Table 5: Dataset Distribution for Various Machine Learning Challenges "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Ciphertext-wise pruning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Algorithm 4, we detail the Trim(), FZero(), Fone() and $\\mathsf{M a s k()}$ functions, which are used during ciphertext-wise pruning. When $k>0$ , the server perform left rotation on $c t_{b a c k}$ by $|k|$ sample slot. If $K<0$ , right rotation is performed. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We accurately describe the Abstract and Introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: In Section 5. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: Section 3.1 ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: In Section 4. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Code is released with anonymity. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Section 4 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In Section 4, we report the mean and variance of the results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Section 4 System Setup. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We report Broader Impact in Section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 19}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We cite datasets and models in the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide an anonymous URL to release our code in the abstract. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing or human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines:This work does not involve crowdsourcing or human subjects. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]