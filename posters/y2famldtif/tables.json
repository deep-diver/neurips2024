[{"figure_path": "y2fAmldTIf/tables/tables_5_1.jpg", "caption": "Table 1: End-to-end comparison across different datasets. The pruning ratio is set as p = 0.9.", "description": "This table compares the end-to-end training time and accuracy of the proposed method with HETAL and an unencrypted baseline on five datasets (MNIST, CIFAR-10, Face Mask Detection, DermaMNIST, and SNIPS). The pruning ratio is set to 0.9 (only 10% of data used for training).  The results show the total training time reduction and the minimal accuracy drop achieved by the proposed encrypted data pruning method across different datasets. ", "section": "4.2 Results"}, {"figure_path": "y2fAmldTIf/tables/tables_6_1.jpg", "caption": "Table 1: End-to-end comparison across different datasets. The pruning ratio is set as p = 0.9.", "description": "This table compares the end-to-end training time and accuracy of the proposed method against HETAL [6] and an unencrypted baseline across five different datasets (MNIST, CIFAR-10, Face Mask Detection, DermaMNIST, and SNIPS). The pruning ratio is consistently set to 0.9 (meaning only 10% of the data is used for training in each epoch).  The results show a significant reduction in training time using the proposed method, with minimal impact on accuracy.", "section": "4.2 Results"}, {"figure_path": "y2fAmldTIf/tables/tables_7_1.jpg", "caption": "Table 1: End-to-end comparison across different datasets. The pruning ratio is set as p = 0.9.", "description": "This table compares the end-to-end training time and accuracy of the proposed encrypted data pruning method with HETAL [6] and an unencrypted baseline on five datasets (MNIST, CIFAR-10, Face Mask Detection, DermaMNIST, and SNIPS).  The pruning ratio (p) is set to 0.9, meaning only 10% of the data is used for training in each epoch. The results demonstrate the significant reduction in training time achieved by the proposed method, along with minimal accuracy loss. ", "section": "4.2 Results"}, {"figure_path": "y2fAmldTIf/tables/tables_8_1.jpg", "caption": "Table 2: Effectiveness of the proposed method. We tested different private training methods on the CIFAR-10 dataset, with a pruning ratio p = 0.9. Communication is the size of logits and importance score ciphertexts server sends to client for early stopping and data pruning.", "description": "The table compares the performance of five different private training methods on the CIFAR-10 dataset. The methods include a full-data approach (HETAL), a baseline approach with pruning, and three enhanced versions incorporating client-aided masking, HEFS, and ciphertext-wise pruning.  The comparison focuses on accuracy, training time (runtime), speedup relative to the full-data approach, and the amount of communication between client and server.  The pruning ratio is held constant at 0.9 for all pruning methods.", "section": "4.2 Results"}, {"figure_path": "y2fAmldTIf/tables/tables_8_2.jpg", "caption": "Table 3: Privately Training an MLP from scratch under different data pruning ratios.", "description": "This table presents the results of training a 3-layer MLP on the MNIST dataset from scratch using different data pruning ratios (1%, 5%, 10%, 20%, 40%, 50%, 60%, 70%, 80%, and 90%). For each pruning ratio, the table shows the achieved accuracy (Acc(%)), the difference in accuracy compared to the full dataset (\u0394Acc), the training time in hours (Time(h)), and the speedup factor compared to training with the full dataset. The results demonstrate the trade-off between training time and accuracy when employing data pruning, indicating that significant speedups can be obtained with minimal accuracy loss, especially when using a small fraction of the data.", "section": "4.2 Results"}, {"figure_path": "y2fAmldTIf/tables/tables_13_1.jpg", "caption": "Table 1: End-to-end comparison across different datasets The pruning ratio is set as p = 0.9.", "description": "This table compares the end-to-end training time and accuracy of the proposed method with HETAL and an unencrypted baseline across five datasets (MNIST, CIFAR-10, Face Mask Detection, DermaMNIST, and SNIPS). The pruning ratio was set to 0.9 (meaning only 10% of the data was used for training in each epoch). The table shows that the proposed method significantly reduces training time while maintaining high accuracy, with the accuracy drop as small as 0.25% on the Face Mask Detection dataset and even a slight improvement of 0.14% over both HETAL and the unencrypted baseline on this same dataset.", "section": "4.2 Results"}, {"figure_path": "y2fAmldTIf/tables/tables_15_1.jpg", "caption": "Table 1: End-to-end comparison across different datasets. The pruning ratio is set as p = 0.9.", "description": "This table presents the end-to-end training time and accuracy on five datasets (MNIST, CIFAR-10, Face Mask Detection, DermaMNIST, and SNIPS) comparing three different methods: an unencrypted baseline, the HETAL method (state-of-the-art), and the proposed HEPrune method with a pruning ratio of 0.9 (i.e., 10% of the data). The comparison highlights HEPrune's significant speedup in training while maintaining comparable accuracy.", "section": "4.2 Results"}]