[{"figure_path": "y2fAmldTIf/figures/figures_2_1.jpg", "caption": "Figure 1: (a). Test accuracy under various pruning fractions in plaintext dataset (ResNet-18 on CIFAR-10). (b) The encrypted training overhead and accuracy of different data pruning methods. (c) The distribution of data sample numbers in a ciphertext after data pruning. Only a small fraction of ciphertexts are empty with a na\u00efve sample-wise pruning.", "description": "This figure demonstrates the impact of data pruning on model accuracy and training time, both in plaintext and encrypted settings.  (a) shows how much data can be pruned from a plaintext dataset (ResNet-18 on CIFAR-10) before accuracy significantly suffers. (b) compares the runtime and accuracy of several encrypted data pruning methods against the baseline (no pruning). (c) illustrates that naive sample-wise pruning leaves many empty spaces in the ciphertexts, highlighting the need for ciphertext-wise pruning.", "section": "Motivation"}, {"figure_path": "y2fAmldTIf/figures/figures_3_1.jpg", "caption": "Figure 2: The overall workflow of private training with encrypted data pruning.", "description": "The figure illustrates the pipeline of private training with encrypted data pruning. It starts with the client encrypting the training data and sending it to the server. The server then performs a forward pass on the encrypted data to obtain predictions.  These predictions, along with the ground truth labels, are used to compute sample importance scores.  Unimportant samples are masked using a pruning mask generated by a client-aided masking process, and the ciphertexts containing these samples are pruned. Finally, the server performs backpropagation on the pruned ciphertexts to train the model for a certain number of epochs. The trained model is then sent back to the client for decryption.", "section": "3.1 Pipeline of Private Training with Encrypted Data Pruning"}, {"figure_path": "y2fAmldTIf/figures/figures_4_1.jpg", "caption": "Figure 2: The overall workflow of private training with encrypted data pruning.", "description": "This figure illustrates the workflow of the HEPrune framework for private training with encrypted data pruning. It shows how the client encrypts the dataset and sends it to the server. The server then performs forward and backward passes, computes importance scores, applies a pruning mask, and performs ciphertext-wise pruning. The process repeats for multiple epochs until the model is trained.  Finally, the encrypted model is sent back to the client for decryption. ", "section": "3 HEPrune Design"}, {"figure_path": "y2fAmldTIf/figures/figures_6_1.jpg", "caption": "Figure 3: Example of ciphertext-wise pruning.", "description": "This figure illustrates the process of ciphertext-wise pruning, a technique to further reduce the number of ciphertexts in private training. It begins by computing the sparsity of each ciphertext, which represents the number of empty slots. Ciphertexts are then sorted by their sparsity. The algorithm iteratively merges sparse ciphertexts with denser ciphertexts to minimize empty slots, effectively reducing the overall number of ciphertexts used for training.", "section": "3.3 Ciphertext-wise Pruning"}, {"figure_path": "y2fAmldTIf/figures/figures_8_1.jpg", "caption": "Figure 4: Private training time and accuracy with different fractions of data of (a) CIFAR-10 dataset and (b) MNIST dataset", "description": "This figure shows the results of experiments evaluating the impact of different data pruning ratios on training time and accuracy for the CIFAR-10 and MNIST datasets.  Subfigures (a) and (b) respectively show the effect of using different fractions of the datasets on both training time (HE Time, in minutes) and test accuracy.  The dashed red lines indicate the accuracy achieved without pruning. The light blue bars show the training time and the orange line shows the accuracy. The results demonstrate that, even with a small fraction of the data, accuracy remains relatively high while significantly reducing training time.  The results also show that even using a small fraction of the data can, in some cases, lead to slightly better accuracy than training on the full dataset. ", "section": "4.2 Results"}]