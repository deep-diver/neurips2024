[{"type": "text", "text": "UGC: Universal Graph Coarsening ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mohit Kataria1 Sandeep Kumar2,1,3 Jayadeva2,1 Mohit.Kataria@scai.iitd.ac.in ksandeep@ee.iitd.ac.in jayadeva@ee.iitd.ac.in 1 Yardi School of Artificial Intelligence 2Department of Electrical Engineering 3Bharti School of Telecommunication Technology and Management Indian Institute of Technology Delhi ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the era of big data, graphs have emerged as a natural representation of intricate relationships. However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges. A crucial demand arises for methods that can effectively downsize large graphs while retaining vital insights. Graph coarsening seeks to simplify large graphs while maintaining the basic statistics of the graphs, such as spectral properties and $\\epsilon$ -similarity in the coarsened graph. This ensures that downstream processes are more efficient and effective. Most published methods are suitable for homophilic datasets, limiting their universal use. We propose Universal Graph Coarsening (UGC), a framework equally suitable for homophilic and heterophilic datasets. UGC integrates node attributes and adjacency information, leveraging the dataset\u2019s heterophily factor. Results on benchmark datasets demonstrate that UGC preserves spectral similarity while coarsening. In comparison to existing methods, UGC is $4\\times$ to $15\\times$ faster, has lower eigen-error, and yields superior performance on downstream processing tasks even at $70\\%$ coarsening ratios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphs have emerged as highly expressive tools to represent diverse structures and knowledge in various fields such as social networks, bio-informatics, transportation, and natural language processing [1\u20133]. They are essential for tasks like community detection, drug discovery, route optimization, and text analysis. With the growing importance of graph-based solutions, dealing with large graphs has become a challenge. Graph Coarsening(GC), a widely used technique to simplify graphs while retaining vital information, making them more manageable for analysis [4]. It has been applied successfully in various tasks [5\u201310]. Preserving the structural information of the graph is crucial in graph coarsening algorithms to ensure the fidelity of the coarsened graphs. A high-quality coarsened graph retains essential features and relationships, enabling accurate results for downstream tasks. Additionally, computational efficiency is equally vital for scalability, as large-scale graphs are common in real-world applications. An efficient coarsening method should ensure that the reduction in graph size does not come at the expense of excessive computation time but existing graph coarsening methods often face trade-offs between scalability and the quality of the coarsened graph. Our method draws inspiration from hashing techniques, which provide us with advantages in terms of computational efficiency. As a result, our approach exhibits a linear time complexity, making it highly efficient even for large graphs. ", "page_idx": 0}, {"type": "text", "text": "Graph datasets often exhibit a blend of homophilic and heterophilic traits [11, 12]. Graph Coarsening(GC) has been widely explored on homophilic datasets, but, to the best of our knowledge, has never been applied to heterophilic graphs. We propose Universal Graph Coarsening $U G C$ , an approach that works well on both. Figure 2 illustrates how UGC uses a graph\u2019s adjacency matrix as well as the node feature matrix. UGC relies on hashing, lending computational efficiency. UGC exhibits linear time complexity, enabling fast processing of large datasets. Figure 1 demonstrates the computational time gains of UGC among graph coarsening methods. UGC surpasses the fastest existing methods by about $6\\times$ on the Physics dataset and $9\\times$ on the Squirrel dataset. UGC enhances the performance of Graph Neural Networks (GNN) models in classification tasks, indicating its suitability for downstream processing. UGC coarsened graphs retain essential spectral properties and show low eigen error, hyperbolic error, and $\\epsilon_{}$ -similarity measure. In a nutshell, UGC is fast, universally applicable, and information-preserving. ", "page_idx": 0}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/7ac6b62ecda7f8f7605bfd649bbc0dd6b7b97e3e0af2c74ff8b6ce149ebb39fc.jpg", "img_caption": ["Figure 2: This figure illustrates our framework, UGC, which has three main modules a) Generation of an augmented matrix by incorporating feature and adjacency matrices while using heterophily measure $\\alpha$ , b) Generation of coarsening matrix $\\mathcal{C}$ using augmented features via Hashing, and c) Generation of coarsened graph $\\mathcal{G}_{c}$ from $\\mathcal{C}$ followed by its utilization in downstream tasks. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Key Contributions. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We proposed a novel framework that is extremely fast compared to other existing methods for graph coarsening. It is also shown to be helpful and effective for graphbased downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "\u2022 UGC is the first to handle heterophilic datasets for coarsening. \u2022 UGC can retain important spectral properties, such as eigen error, hyperbolic error, and $\\epsilon$ -similarity measure, which ensures the preservation of key characteristics and information of the original graph during the graph coarsening. ", "page_idx": 1}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/3bd61395d0dbcf4aa72248d840af052251839586c5e7a103a3a4a711a0ee0e71.jpg", "img_caption": ["Figure 1: This figure illustrates the computational time comparison among graph coarsening methods to learn a coarsened graph over ten iterations. UGC outperforms the fastest existing methods by approximately $6\\times$ on the Physics dataset and $9\\times$ on the Squirrel dataset. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2 Background and Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A graph is represented using ${\\mathcal{G}}(V,A,X)$ where $V\\;=\\;\\{v_{1},\\cdots\\,,v_{N}\\}$ denotes set of $N$ vertices, $A\\breve{\\in}\\breve{\\mathbb{R}^{N\\times N}}$ is the adjacency matrix and $A_{i j}>0$ indicates an edge $(v_{i},v_{j})$ between nodes $v_{i}$ and $v_{j}$ . $X\\in\\mathbb{R}^{N\\times\\widetilde{d}}$ denotes the feature matrix where $i^{t h}$ row of $X$ is a feature vector $X_{i}\\in\\mathbb{R}^{\\tilde{d}}$ , associated with node $v_{i}$ . The degree matrix $D$ is a diagonal matrix, where $\\textstyle D_{i i}=\\sum_{j}A_{i j}$ . ${\\cal L}\\,\\in\\,\\mathbb{R}^{N\\times N}$ is a Laplacian matrix, $L=D-A$ [13], and it belongs to the set $S_{L}\\,=\\,\\bigl\\{L\\,\\\\,\\in\\,\\mathbb{R}^{N\\times N}|L_{j i}\\,=\\,L_{i j}\\,\\le$ 0, $\\forall i\\ne j$ ; $\\begin{array}{r}{L_{i i}\\,=\\,-\\sum_{j\\neq i}L_{i j}\\}}\\end{array}$ as defined in [14, 15]. The adjacency matrix $A$ and Laplacian matrix $L$ associated with the graph are related as follows: $A_{i j}=-L_{i j}$ for $i\\neq j$ and $A_{i j}=0$ for $i=j$ . ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{C}^{T}=\\displaystyle\\left[\\begin{array}{l l l l l l l l l}{1}&{1}&{1}&{1}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{1}&{1}&{1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/b7b1bed1ccd231a394ca37453e207067b9841db6cd40a50aa21b61bdf5d3d657.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 3: Graph coarsening toy example, a) Coarsening matrix, b) Original graph $\\mathcal{G}$ and corresponding coarsened graph $\\mathcal{G}_{c}$ ", "page_idx": 2}, {"type": "text", "text": "Both $L$ and $A$ can represent the same graph. Hence, a graph ${\\mathcal{G}}(V,A,X)$ can also be represented as $\\mathcal G(L,X)$ , with either representation utilized as required within the paper. ", "page_idx": 2}, {"type": "text", "text": "Problem. The objective is to reduce an input graph ${\\mathcal{G}}(V,A,X)$ with $N$ -nodes into a new graph $\\mathcal{G}_{c}(\\widetilde{V},\\widetilde{A},\\widetilde{X})$ , with $n$ -nodes and $\\widetilde{X}\\in\\mathbb{R}^{n\\times\\widetilde{d}}$ where $n\\ll N$ . The Graph Coarsening(GC) problem requires learning of a coarsening matrix $\\mathcal{C}\\in\\mathbb{R}^{N\\times n}$ , which is a linear mapping from $V\\rightarrow\\widetilde{V}$ . A linear mapping ensures that similar nodes in $\\mathcal{G}$ are mapped to the same super-node in $\\mathcal{G}_{c}$ , s.t. $\\widetilde{X}={\\mathcal{C}}^{T}X$ . Every non-zero entry $\\mathcal{C}_{i j}$ denotes the mapping of the $i^{t h}$ node of $\\mathcal{G}$ to the $j^{t h}$ super-node $\\mathcal{G}_{c}$ . This $\\mathcal{C}$ matrix belongs to the following set: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{S}=\\left\\{\\mathcal{C}\\in\\mathbb{R}^{N\\times n},\\mathcal{C}_{i j}\\in\\{0,1\\},\\|\\mathcal{C}_{i}\\|=1,\\langle\\mathcal{C}_{i},\\mathcal{C}_{j}\\rangle=0,\\forall i\\neq j,\\langle\\mathcal{C}_{l},\\mathcal{C}_{l}\\rangle=d_{i},\\|\\mathcal{C}_{i}^{T}\\|_{0}\\geq1\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d_{i}$ means the number of nodes in the $i^{t h}$ -supernode. The condition $\\langle\\mathcal{C}_{i},\\mathcal{C}_{j}\\rangle=0$ ensures that each node of $\\mathcal{G}$ is mapped to a unique super-node. The constraint $\\lVert\\mathcal{C}_{i}\\rVert_{0}\\geq1$ requires that each super-node contains at least one node. Consider the 8-node graph in Figure 3b. Nodes 1, 2, 3, and 4 are mapped to super-node $\\mathbf{A}$ , while nodes 6, 7, and 8 are mapped to super-node C. Hence, the coarsening matrix $\\mathcal{C}$ is given in Figure 3a. The goal is to learn this $\\mathcal{C}$ matrix such that $\\mathcal{G}$ and $\\mathcal{G}_{c}$ are similar. The $\\epsilon-$ similarity is a widely used similarity measure for graphs with node features, as it entails comparing the Laplacian norms of the respective feature matrices. The graphs ${\\mathcal{G}}(V,A,X)$ and $\\mathcal{G}_{c}(\\widetilde{V},\\widetilde{\\vec{A}},\\widetilde{X})$ are said to be $\\epsilon_{}$ -similar if there exist $\\epsilon\\geq0$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\|X\\|_{L}\\leq\\|\\widetilde X\\|_{L_{c}}\\leq(1+\\epsilon)\\|X\\|_{L}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $L$ and $L_{c}$ are the Laplacian matrices of $\\mathcal{G}$ and $\\mathcal{G}_{c}$ respectively, $\\|X\\|_{L}={\\sqrt{t r(X^{T}L X)}}$ and $\\|\\widetilde{X}\\|_{L_{c}}=\\sqrt{t r(\\widetilde{X}^{T}L_{c}\\widetilde{X})}$ . The quantity $\\begin{array}{r}{t r(X^{T}L X)=-\\sum_{i,j}L_{i j}\\|x_{i}-x_{j}\\|^{2}}\\end{array}$ is known as Dirichlet Energy (DE), which is employed to measure the smoothness of node features where $x_{i}$ and $x_{j}$ are the node features of nodes $i$ and $j$ [14]. ", "page_idx": 2}, {"type": "text", "text": "Goal: Given a graph ${\\mathcal{G}}(V,A,X)$ of $N$ nodes, construct a coarsened graph $\\mathcal{G}_{c}(\\widetilde{V},\\widetilde{A},\\widetilde{X})$ with $n$ nodes, such that they are \u03f5\u2212similar. ", "page_idx": 2}, {"type": "text", "text": "Homophilic and Heterophilic datasets. Graph datasets may demonstrate homophily and heterophily properties [16\u201319]. Homophily refers to the tendency of nodes to be connected to other nodes of the same class or type, while heterophily signifies the tendency of nodes to connect with nodes of different classes. A heterophily factor $0\\leq\\alpha\\leq1$ may be used to denote the degree of heterophily. $\\alpha$ is calculated as the fraction of edges between nodes of different classes to the total number of edges. A strongly heterophilic graph $\\left(\\alpha\\rightarrow1\\right)$ ) has the most edges between nodes of different classes, suggesting a diverse network with mixed interactions. Conversely, weak heterophily or strong homophily $(\\alpha\\rightarrow0)$ ) occurs in networks where nodes predominantly connect with others of the same class. ", "page_idx": 2}, {"type": "text", "text": "Locality Sensitive Hashing. Locality Sensitive Hashing (LSH) is a linear time, efficient similarity search technique for high dimensional data [20\u201323]. It maps high-dimensional vectors to lower dimensions while ensuring that similar vectors collide with high probability. LSH uses a family of hash functions to map vectors to buckets, enabling fast retrieval and similarity search. It has found applications in image retrieval [24], data mining [25], and similarity search algorithms [26]. LSH family is defined as ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 Let d be a distance measure, and let $d_{1}<d_{2}$ be two distances. A family of functions   \n$F$ is said to be $(d_{1},d_{2},p_{1},p_{2})\\cdot$ \u2212sensitive if for every $f\\in F$ the following two conditions hold:   \n1. If $d(x,y)\\leq d_{1}$ then probability $[f(x)=f(y)]\\geq p_{1}$   \n2. If $d(x,y)\\geq d_{2}$ then probability $[f(x)=f(y)]\\le p_{2}$ ", "page_idx": 2}, {"type": "text", "text": "UGC uses LSH with a set of random projectors to map similar nodes to the same super-node. The projection is computed as $\\left\\lfloor{\\frac{<x\\cdot w_{i}>+b_{i}}{r}}\\right\\rfloor$ , where $w_{i}$ is a randomly selected $d-$ dimensional projector vector from a $p-$ stable distribution (see Appendix A); $x$ represents the $d-$ dimensional data sample, and $r$ is the width of each quantization bin. ", "page_idx": 3}, {"type": "text", "text": "Related Works. The literature is replete with graph reduction methods and their applications; they may be broadly classified into three categories: ", "page_idx": 3}, {"type": "text", "text": "1. Optimization and Heuristics: Loukas [15] proposed advanced spectral graph coarsening algorithms based on local variation to preserve the original graph\u2019s spectral properties. Two variants, viz. edge-based (LVE) and neighborhood-based (LVN), select contraction sets with small local variation in each stage but have limitations in achieving arbitrary coarsening levels. Heavy edge matching (HE) [9, 27], determines the contraction family by computing a maximum-weight matching based on the weight of each contraction set. The Algebraic Distance method proposed in [27, 28] calculates the weight of each candidate set using an algebraic distance measure. The affinity method [29], inspired by algebraic distance, uses the vertex proximity heuristic. The Kron reduction method [30] was originally proposed for electrical networks but is too slow for large networks. FGC [14, 31] considers both the graph structure and the node attributes as the input and, alternatively, optimizes $\\mathcal{C}$ . The above-mentioned methods are computationally and memory-intensive. ", "page_idx": 3}, {"type": "text", "text": "2. GNN based: GCond [32] and SFGC [33] are GNN-based graph condensation methods. These works proposed the online gradient matching schema between the synthesized small-scale graph and the large-scale graph. However, these methods have significant issues regarding computational time and generalization ability. First, they require training GNN models on the original graph to get a smaller graph as they imitate the GNN training trajectory on the original graph through gradient matching. Due to this, these methods are extremely computationally demanding and may not be suitable for the scalability of GNN models. However, these methods can be beneficial for other tasks, like solving storage and visualization issues. Second, the condensed graph obtained using GCond [32] shows poor generalization ability across different GNN models [33] because different GNN models vary in their convolution operations along graph structures.   \n3. Scaling GNN viz. Graph Coarsening: SCAL [34] and GOREN [35] proposed to enhance the scalability for training GNN models using graph coarsening. It is worth noting that SCAL and GOREN are not standalone graph coarsening techniques. SCAL uses Louka\u2019s [15] work to coarsen the graph, then trains GNN models using the coarsened graph. While GOREN trying to improve the coarsening quality of existing methods. ", "page_idx": 3}, {"type": "text", "text": "3 The Proposed Framework: Universal Graph Coarsening (UGC) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The proposed UGC framework comprises three main components: (a) First, obtaining an augmented feature matrix $F$ containing both node feature and structural information, (b) Secondly, using localitysensitive hashing to derive the coarsening matrix $\\mathcal{C}$ , (c) and Finally, obtaining the coarsened graph adjacency matrix $A_{c}$ and coarsened features $F_{c}$ . ", "page_idx": 3}, {"type": "text", "text": "Construction of Augmented Feature Matrix $F$ . In order to create a universal GC framework suitable for all, it is important to consider features at both i) the node level, i.e., features, and ii) the structure-level, i.e., adjacency matrix, together. In this regard, we create an augmented feature matrix $F$ , where each node\u2019s feature vector $X_{i}$ is augmented with its binary adjacency vector $A_{i}$ . We use the heterophily factor $\\alpha$ discussed in Section 2 to balance the emphasis between node-level and structurelevel information. The augmented feature vector for node $v_{i}$ is given using $F_{i}=\\left\\{(1\\!-\\!\\alpha)\\!\\cdot\\!X_{i}\\!\\oplus\\!\\alpha\\!\\cdot\\!A_{i}\\right\\}$ where $\\oplus$ and $\\cdot$ denote the concatenation and dot product operations, respectively. Figure 11 in Appendix K illustrates a toy example of the process involved in calculating the augmented feature vector. While larger graphs may result in long vectors, efficient implementations and sparse tensor methods may alleviate this hurdle. A motivating example demonstrating the need for augmented features while doing GC is discussed in Appendix K (Figure 12). ", "page_idx": 3}, {"type": "text", "text": "Construction of Coarsening Matrix ${\\mathcal{C}}.$ . Let $F_{i}\\in\\mathbb{R}^{d}$ represent the augmented feature vector of node $v_{i}$ . Let $\\boldsymbol{\\mathscr{W}}\\in\\mathbb{R}^{d\\times l}$ and $b\\in\\bar{\\mathbb{R}}^{l}$ be the hashing matrices used in UGC, with $l$ denoting the number of hash functions. The hash indices generated by $k^{t h}$ hash/projector function for $i^{t h}$ node is given as ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{i}^{k}=\\lfloor\\frac{1}{r}*(\\mathcal{W}_{k}\\cdot F_{i}+b_{k})\\rfloor\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r$ is a hyperparameter called bin-width. The hash index that has the maximum occurrence among the hash indices generated by all $l$ hash functions is the hash value assigned to the graph node $v_{i}$ . Hence, the hash value for node $v_{i}$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{i}=m a x O c c u r e d\\{h_{i}^{1},h_{i}^{2}....h_{i}^{l}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$r$ controls the size of the coarsened graph $\\mathcal{G}_{c}$ ; empirically, we find that increasing $r$ means reducing the size of the coarsened graph $\\mathcal{G}_{c}$ . All nodes assigned with the same hash value map to the same super-node in $\\mathcal{G}_{c}$ . The reader may like to refer to Algorithm 1 for the steps in UGC. The element of coarsening matrix, $\\mathcal{C}_{i j}$ equals 1 if vertex $v_{i}$ is associated with super-node $\\widetilde{v_{j}}$ . Crucially, every node is assigned a unique $h_{i}$ value, ensuring an exclusive mapping to a super-node. This constraint aligns with the formulation of super-node and guarantees at least one node per super-node. Thus, each row of $\\mathcal{C}$ contains only one non-zero entry, leading to orthogonal columns. This matrix $\\mathcal{C}$ satisfies the conditions specified in Equation 1. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 UGC: Universal Graph Coarsening ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Require: Input ${\\mathcal{G}}(V,A,X)$ , $l\\leftarrow$ Number of Projectors, $r\\leftarrow b i n W i d t h$   \n1: $\\begin{array}{r}{\\alpha=\\frac{|\\{(v,u)\\in E:y_{v}=y_{u}\\}|}{|E|}}\\end{array}$ ; $\\alpha$ is heterophily factor, $y_{i}\\in\\mathbb{R}^{N}$ is node labels, $E$ denotes edge list   \n2: $F=\\left\\{(1-\\alpha)\\cdot X\\oplus\\alpha\\cdot A\\right\\}$   \n3: $\\mathcal{W}\\sim\\mathcal{D}(.);\\mathcal{W}\\in\\mathbb{R}^{d\\times l}$ denotes $l$ projectors, and $\\mathcal{D}$ is a p-stable distribution   \n4: $b\\sim\\mathcal{D}(.)$ ; $b\\in\\mathbb{R}^{l}$ denotes sampled bias   \n5: $\\begin{array}{r}{\\mathcal{H}=\\left\\lfloor\\frac{<F\\cdot w>+b}{r}\\right\\rfloor;\\mathcal{H}\\in\\mathbb{R}^{N\\times l}}\\end{array}$   \n6: $\\pi_{i}\\gets$ maxOccurence $(\\mathcal{H}_{i};i\\in\\{1,2,3,...,N\\})$ , $\\pi\\in\\mathbb{R}^{N}$   \n7: for every node v in V do   \n8: ${\\mathcal{C}}[v,{\\bar{\\pi}}[v]]\\gets1$   \n9: $\\begin{array}{r}{A_{c}(i,j)\\gets\\sum_{(u\\in\\pi^{-1}(\\widetilde{v}_{i}),v\\in\\pi^{-1}(\\widetilde{v}_{j}))}A_{u v},\\forall i,j\\in\\{1,}\\end{array}$ 2, ..., n}   \n10: $\\begin{array}{r}{F_{c}(i)\\gets\\frac{1}{|\\pi^{-1}(\\tilde{v}_{i})|}\\sum_{u\\in\\pi^{-1}(\\tilde{v}_{i})}F_{u},\\forall i\\in\\{1,2,...,n\\}}\\end{array}$   \n11: return $\\mathcal{G}_{c}(V_{c},A_{c},F_{c}),\\mathcal{C}$ ", "page_idx": 4}, {"type": "text", "text": "Construction of Coarsened Graph $\\mathcal{G}_{c}$ . Let $\\mathcal{G}_{c}(\\widetilde{V},\\widetilde{A},\\widetilde{F})$ represent the coarsened graph that is to be built. A pair of super-nodes, say $\\widetilde{v_{i}}$ and $\\widetilde{v_{j}}$ , in th e c oarsened graph $\\mathcal{G}_{c}$ are connected, if any of the nodes, say $u\\in\\pi^{-1}(\\widetilde{v_{i}})$ has an edge to any of the nodes, say $v\\in\\pi^{-1}(\\widetilde{v_{j}})$ in the original graph, i.e., $\\exists\\;u\\in\\pi^{-1}(\\widetilde{v_{i}}),v\\in\\pi^{-1}(\\widetilde{v_{j}})$ such that $A_{u v}\\ne0$ . The coarsened graph $\\mathcal{G}_{c}$ is weighted, and the weight assigned to the edge between nodes $\\widetilde{v_{i}}$ and $\\widetilde{v_{j}}$ , is given by $\\begin{array}{r}{\\widetilde A_{i j}=\\sum_{(u\\in\\pi^{-1}(\\widetilde v_{i}),v\\in\\pi^{-1}(\\widetilde v_{j}))}A_{u v}}\\end{array}$ where $A_{u v}$ refers to the element $(u,v)$ in the adjacency matrix $A$ of graph $\\mathcal{G}$ . The features of super-nodes are taken to be the average of the features of the nodes in the super-node, i.e., ${\\widetilde{F}}_{i}\\,=$ |\u03c0\u221211(vi)| u\u2208\u03c0\u22121(vi) Fu. The super-node\u2019s label is chosen as the class that has the most inst ances. From the $\\mathcal{C}$ matrix, we can directly calculate the adjacency $\\widetilde{A}$ matrix of $\\mathcal{G}_{c}$ using $\\widetilde{A}={\\mathcal{C}}^{T}A{\\mathcal{C}}$ which is the same as $\\widetilde{A}_{i j}$ . $\\widetilde{F}$ can also be obtained using $\\widetilde{\\boldsymbol{F}}=\\boldsymbol{\\mathcal{C}}^{T}\\boldsymbol{F}$ where $\\mathcal{C}$ is the coarsening matrix discussed earlier. Because each super-edge combines multiple edges from the original graph, the number of edges in the coarse graph is also much less than $m$ . In general, the adjacency matrix $\\widetilde{A}$ has a substantially smaller number of non-zero elements than $A$ . The pseudocode for UGC is listed in Algorithm 1. UGC gives a coarsened graph $\\mathcal{G}_{c}(L_{c},\\widetilde{F})$ which also satisfies $\\epsilon-$ similarity $\\left(\\epsilon\\geq0\\right)$ ). ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 The input graph $\\mathcal{G}(L,F)$ and the coarsened graph $\\mathcal{G}_{c}(L_{c},\\widetilde{F})$ obtained using the proposed UGC algorithm are $\\epsilon$ -similar with $\\epsilon\\geq0$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\|F\\|_{L}\\leq\\|\\widetilde{F}\\|_{L_{c}}\\leq(1+\\epsilon)\\|F\\|_{L}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $L$ and $L_{c}$ are the laplacian matrices of $\\mathcal{G}$ and $\\mathcal{G}_{c}$ respectively. ", "page_idx": 4}, {"type": "text", "text": "Proof: The proof is deferred in Appendix I. ", "page_idx": 4}, {"type": "text", "text": "Universal Graph Coarsening with feature re-learning for Bounded $\\epsilon$ -similarity. The coarsened graph $\\mathcal{G}_{c}$ generated through UGC exhibits a high degree of similarity, within the range of $\\epsilon$ , to the original graph $\\mathcal{G}$ . It has also been empirically demonstrated that this coarsened representation performs exceptionally well across various downstream tasks. Nonetheless, to achieve a tighter $\\epsilon$ -bound, where $(\\epsilon\\le1)$ ), a potential step involves introducing modifications to the feature learning procedure of the super-nodes $\\mathcal{G}_{c}$ . ", "page_idx": 4}, {"type": "text", "text": "It is important to note that the $\\epsilon$ -similarity measure introduced in [15] does not incorporate features. Instead, it relies on the eigenvector of the laplacian matrix to compute similarity, which limits its ability to capture the characteristics of the associated features along with the graph structure. Once we get the loading matrix $\\mathcal{C}$ using UGC as discussed in Section 3 we used $\\begin{array}{r}{\\widetilde{F}_{i}=\\frac{1}{|\\pi^{-1}(\\widetilde{v_{i}})|}\\sum_{u\\in\\pi^{-1}(\\widetilde{v_{i}})}F_{u}}\\end{array}$ to learn the feature-vectors of super-nodes. Using $\\widetilde{F}_{i}$ we can satisfy the Theorem 3.1. However, to give a strict bound on the $\\epsilon$ similarity we updated $\\widetilde{F}$ toF by minimizing the term ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\hat{F}}f(\\widehat{F})=\\mathrm{tr}(\\widehat{F}^{T}\\mathcal{C}^{T}L\\mathcal{C}\\widehat{F})+\\frac{\\alpha}{2}\\|\\mathcal{C}\\widehat{F}-F\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We aim to enforce the Dirichlet smoothness condition in super-node features using Equation 6. The above equation is a convex optimization problem from which we get a closed-form solution by putting the gradient w.r.t toF equal to zero. Update rule forF can be derived as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n2\\mathcal{C}^{T}L\\mathcal{C}\\widehat{F}+\\alpha\\mathcal{C}^{T}(\\mathcal{C}\\widehat{F}-F)=0\\implies\\widehat{F}=\\left(\\frac{2}{\\alpha}\\mathcal{C}^{T}L\\mathcal{C}+\\mathcal{C}^{T}\\mathcal{C}\\right)^{-1}\\mathcal{C}^{T}F\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We now have re-learnt features for super-nodes, please refer to Algorithm 2 in Appendix B which we call as UGC-FL i,e UGC with feature learning. UsingF we can give a more strict bound on $\\epsilon-$ similarity. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 The original graph $\\mathcal{G}(L,F)$ and coarsened graph $\\mathcal{G}_{c}(L_{c},\\widehat{F})$ obtained using the proposed UGC-FL algorithm are $\\epsilon$ similar with $0<\\epsilon\\le1$ , i.e, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\epsilon)\\|\\boldsymbol{F}\\|_{L}\\leq\\|\\widehat{\\boldsymbol{F}}\\|_{L_{c}}\\leq(1+\\epsilon)\\|\\boldsymbol{F}\\|_{L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L$ and $L_{c}$ are the laplacian matrices of $\\mathcal{G}$ and $\\mathcal{G}_{c}$ respectively, and $F$ andF  are features matrix associated with original and coarsened graphs, respectively. ", "page_idx": 5}, {"type": "text", "text": "Proof: The proof is deferred in Appendix J. ", "page_idx": 5}, {"type": "text", "text": "Novelty: The majority of current techniques involve coarsening the original graph and simultaneously learning the graph structure, which makes them computationally intensive. The UGC decouples this process, making it incredibly fast, first learning the coarsening mapping $C$ by capturing the similarity of features through hashing and then using the adjacency matrix only once as $\\bar{A_{c}}\\,\\bar{=}\\,C^{T}\\bar{A}C$ for learning the coarsened graph\u2019s structure all at once. The UGC is easy to use, extremely fast, and produces better results for tasks requiring downstream processing. ", "page_idx": 5}, {"type": "text", "text": "Time Complexity Analysis of UGC. We have three phases for our framework. For the first phase, we can see Algorithm 1, Line 5 is driving the complexity of the algorithm, where we multiply two $F\\in\\mathbb{R}^{N\\times d}$ and $\\mathcal{W}\\in\\mathbb{R}^{d\\times l}$ matrices, which results in $\\mathcal{O}\\dot{(N l d)}$ . In the second pass, the super-nodes for the coarsened graphs are constructed with the help of the accumulation of nodes in the bins. The main contribution of UGC is up to these two phases i.e., Line 1-8. Till now, time-complexity is $\\mathcal{O}(N l d)\\equiv\\mathcal{O}(N C)$ where $C$ is a constant. ", "page_idx": 5}, {"type": "text", "text": "In the third phase, Lines 10-11, we calculate the adjacency and features of the super-nodes of the coarsened graph $\\mathcal{G}_{c}$ . The computational cost of this operation is ${\\mathcal{O}}(m)$ , where $m$ is the number of edges in the original graph $\\mathcal{G}$ , and this is a one-time step. Indeed, the overall time complexity of all three phases combined is $\\mathcal{O}(N+m)$ where $m$ is the number of edges. However, it\u2019s important to note that the primary contribution of UGC lies in the process of finding the coarsening matrix, whose time complexity is $\\mathcal{O}(N)$ . We have compared the computational time for obtaining the coarsening matrix via UGC with the existing methods. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct extensive experiments to evaluate the proposed UGC against the existing graph coarsening algorithms. The conducted experiments establish the performance of UGC concerning i) computational efficiency, ii) preservation of spectral properties, and iii) potential extensions of the coarsened graph $\\mathcal{G}_{c}$ into real-world applications. ", "page_idx": 5}, {"type": "text", "text": "We compare our proposed algorithm with the following coarsening algorithms, as discussed in Section 2. UGC (feat) represents a specific scenario within our framework, wherein only the feature values are considered for hashing, thereby obtaining the mapping of super-nodes. To comprehend the significance of incorporating the adjacency vector, we have added the results for both UGC (feat) and UGC (augmented feat). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Datasets. Our experiments cover widely adopted benchmarks, including Cora ,Citeseer, Pubmed [36], CS, Physics [37], DBLP [38]. Additionally, UGC effectively coarsens large datasets like Flickr, Yelp [39], and Reddit [40], previously challenging for existing techniques. We also present datasets like Squirrel, Chameleon, Texas, Film, Wisconsin [11, 12, 16, 17], characterized by dominant heterophilic factors. Table 6 in Appendix G provides comprehensive dataset details. ", "page_idx": 6}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/2402854aa3fe4da4c29179bb4b27744a39e17847a428f5d006fec8c7e69059c1.jpg", "table_caption": ["Table 1: Summary of run-time in seconds averaged over 5 runs to reduce the graph to $50\\%$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Run-Time Analysis. UGC\u2019s main contribution lies in its computational efficiency. The time required to compute the coarsening matrix $\\mathcal{C}$ is summarized in Table 1. By referring to this Table, it becomes evident that UGC exhibits a remarkable advantage, surpassing all existing methods across diverse datasets. Our model outperforms existing methods by a substantial margin. While other methods struggle at large datasets like Physics(34.4k nodes), UGC is able to coarsen down massive datasets like $Y e l p(7I6.8k\\,n o d e s)$ , which was previously not possible. It should be emphasized that the time taken by UGC on the Reddit $232.9k$ nodes) dataset, which has $7\\times$ the number of nodes compared to Physics is one-third the time taken by the fastest existing methods on Physics dataset. ", "page_idx": 6}, {"type": "text", "text": "Spectral Properties Preservation. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1. Relative Eigen Error (REE):, REE used in [14, 15, 41] gives the means to quantify the measure of the eigen properties of the original graph $\\mathcal{G}$ that are preserved in coarsened graph $\\mathcal{G}_{c}$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1 REE is defined as follows: $\\begin{array}{r}{R E E(L,L_{c},k)=\\frac{1}{k}\\sum_{i=1}^{k}\\frac{|\\widetilde{\\lambda_{i}}-\\lambda_{i}|}{\\lambda_{i}}}\\end{array}$ where $\\lambda_{i}$ and $\\widetilde{\\lambda_{i}}$ are top $k$ eigenvalues of original graph Laplacian $(L)$ and coarsened graph Laplacian $(L_{c})$ matrix, respectively. ", "page_idx": 6}, {"type": "text", "text": "2. Hyperbolic error (HE): HE [42] indicates the structural similarity between $\\mathcal{G}$ and $\\mathcal{G}_{c}$ with the help of a lifted matrix along with the feature matrix $X$ of the original graph. ", "page_idx": 6}, {"type": "text", "text": "Definition 4.2 HE is defined as follows: $\\begin{array}{r}{H E=a r c c o s h(\\frac{||(\\bar{L}-L_{\\mathrm{lift}})\\bar{X}||_{F}^{2}||X||_{F}^{2}}{2t r a c e(X^{T}L X)t r a c e(X^{T}L_{\\mathrm{lift}}X)}+1)}\\end{array}$ where $L$ is the Laplacian matrix and $X\\in\\mathbb{R}^{N\\times d}$ is the feature matrix of the original input graph, $L_{\\mathrm{lift}}$ is the lifted Laplacian matrix defined in [41] as $\\bar{L_{\\mathrm{lift}}}=\\mathcal{C}L_{c}\\mathcal{C}^{T}$ where $\\mathcal{C}\\in\\mathit{\\check{\\mathbb{R}}}^{N\\times n}$ is the coarsening matrix and $L_{c}$ is the Laplacian of $\\mathcal{G}_{c}$ . ", "page_idx": 6}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/e2f89840fbb819075befda5d71b9a679e06a9db278582a48ac898a1c002ab33e.jpg", "img_caption": ["Figure 4: Top 100 eigenvalues of the original graph $\\mathcal{G}$ and coarsened graph $\\mathcal{G}_{c}$ at different coarsening ratios: $30\\%$ , $50\\%$ , and $70\\%$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Eigenvalue preservation can be seen in Figure 4 where we have plotted the top 100 eigenvalues of $\\mathcal{G}$ and of $\\mathcal{G}_{c}$ . We can see that the spectral property is preserved even for $70\\%$ coarsened graphs. This approximation is more accurate for a lower coarsening ratio, i.e., the smaller the graph, the bigger the REE. The REE for all approaches across all datasets is shown in Table 2 for a fixed $50\\%$ coarsening ratio. UGC stands out by giving the best REE values in 8 out of 12 datasets. Although we also have coarsened graphs for large datasets like Yelp and Reddit, eigen error calculation for these datasets was out of memory, so we have used EOOM while other methods fail to find even the coarsened graph, hence the term OOM. Figure 5 illustrates the trends for eigen error, hyperbolic error and GCN accuracy for different methods as the coarsening ratio is altered. ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["Table 2: This table illustrates Relative Eigen Error at $50\\%$ coarsening ratio. UGC stands out by giving the best REE values in 8 out of 12 datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/7eccb1d4e3a1905d27636e8003136b7749455b03f30be4228c0e7159a828c0c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: This figure compares graph coarsening methods in terms of REE, HE, and GCN accuracy on the Pubmed dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "LSH Similarity and $\\epsilon$ -Bounded Results The LSH family used in our framework is based on $\\mathbf{p}$ -stable distributions $\\mathcal{D}$ see Appendix A. This ensures that the probability of two nodes going to the same super-node is directly related to the distance between their features (augmented features $F$ for UGC). ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1 As given in [43], the probability that two nodes v and u will collide and go to a super-node under a hash function drawn uniformly at random from a 2-stable distribution is inversely proportional to $c=||\\boldsymbol{v}-\\boldsymbol{u}||_{2}$ and it is represented by $p(c)=P r_{w,b}\\left[h_{w,b}(v)=h_{w,b}(u)\\right]=$ $\\begin{array}{r}{\\int_{0}^{r}\\frac{1}{c}f_{p}\\left(\\frac{t}{c}\\right)\\left(1-\\frac{t}{r}\\right)d t}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "In our experiments, we empirically validated the Theorem 4.1. We examined if the feature distance between any node pair was below a specific threshold, and then using the coarsening matrix $\\mathcal{C}$ given by UGC, we verified if they shared the same super-node or not. Our evaluation involved counting successful matches, where nodes belonged to the same super-node, and failures, where they did not. We subsequently calculated a probability measure based on these counts. Figure 6a and 6b plot this probabilistic function for two datasets, namely Cora and Citeseer as a function of distance between two nodes. Re-visiting the Definition 2.1 for the Cora dataset, we denote our LSH family as $\\mathcal{H}(1,3,1,0.20)$ . Suppose $d$ denotes the distance between the nodes $\\{u,v\\}$ . In the notation $\\mathcal{H}(1,3,1,0.20)$ , this implies that if $d\\leq1$ , there is a $100\\%$ probability that $u,v$ will be grouped into the same super-node. Conversely, if $d>3$ , the probability of $\\{u,v\\}$ being grouped into the same super-node is $20\\%$ . Figure 6c plots different values of $\\epsilon$ at different coarsening ratios. We used Equation 6 for updating the augmented feature matrix $F$ given by UGC and as mentioned, we got $\\epsilon\\leq1$ similarity guarantees for the coarsened graph. Hence proving Theorem 3.2. ", "page_idx": 7}, {"type": "text", "text": "Scalable Training of Graph Neural Networks. Graph neural networks (GNNs), tailored for nonEuclidean data [44\u201346], have shown promise in various applications [47, 48]. However, scalability remains a challenge. Building on [34], we investigate how our graph coarsening approach can enhance GNN scalability for training, bridging the gap between GNNs and efficient processing of large-scale data. ", "page_idx": 7}, {"type": "text", "text": "GNN parameter details. We employed a single hidden layer GCN model with standard hyperparameters values [13] see Appendix H for the node-classification task. Coarsened graph $\\mathcal{G}_{c}$ is used to train the GCN model, and all the predictions are made on test data from the original graph. The relation between coarsening ratio and accuracy is evident from Table 9 in Appendix H. Specifically, as we progressively coarsen the graph, a slight decrease in accuracy values becomes noticeable. Hence, there will always be a trade-off when it comes to the coarsening ratio and quality of the reduced graph. To emphasize the contribution of UGC in terms of both computational time and node-classification accuracy, we have included Figure 7. This figure illustrates the improve", "page_idx": 7}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/f07801e248c833ff985d826c3cee9b8c5e680753075d4eec38719ea717fac41e.jpg", "img_caption": ["Figure 6: a) Cora and b) Citeseer demonstrate the inverse relationship between the probability of two nodes belonging to the same super-node and the distance between them. c) plots the $\\epsilon$ values $(\\leq1)$ for Cora, Citeseer, and CS datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/68b409a05643a5eed8ff48e203565fc95a6f54aec8d14d066aa3b6dd3b6491d9.jpg", "table_caption": ["Table 4: This table illustrates the accuracy of GCN model when trained with $50\\%$ coarsen graph. UGC demonstrated superior performance compared to existing methods in 7 out of the 9 datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "ments in computational time and the corresponding changes in accuracy values when compared to the currently best-performing model across various datasets. Table 4 compares the accuracy among all the approaches with all datasets when they are coarsened down by $50\\%$ . UGC demonstrated superior performance compared to existing methods in 7 out of the 9 datasets. We have used t-SNE [49] algorithm for visualization of predicted node labels shown in Figure 10 in Appendix H. It is evident that even with highly coarsened graph training, the GCN model can maintain its accuracy. ", "page_idx": 8}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/4849329cc13b3d6312472a2279e43fe2055c9b99bca8d70636c24e2f4e2bf2b2.jpg", "img_caption": ["Figure 7: Computational and accuracy gains of UGC. In the bar plot, dashed bars represent the gain or loss in accuracy when compared to the existing best-performing method, while plain bars indicate the computational gains. All datasets are coarsened down by $50\\%$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "UGC is Model-Agnostic. While our initial validation utilized GCN to assess the quality of our coarsened graph $\\mathcal{G}_{c}$ our framework is not bound to any specific GNN architecture. We extended our evaluations to include other prominent graph neural network models. Results from three diverse models, namely GCN [13], GraphSage [40], GIN [50], and GAT [51], have been incorporated into our analysis. All the models were trained using $50\\%$ coarsened graph $\\mathcal{G}_{c}$ . Results from Table 3 demonstrate the robustness and model-agnostic nature of UGC. Refer to Table 7 in ", "page_idx": 8}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/5d74ef02fac525dea765ef56fca1a18350a26aa64be28d1d9bd1cae4dec03a50.jpg", "table_caption": ["Table 3: This table demonstrates UGC\u2019s model-agnostic nature, as it doesn\u2019t rely on any specific GNN model. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Appendix $_\\mathrm{H}$ for a comprehensive analysis of node classification accuracy results for various GNN models. We believe this flexibility further enhances the applicability and utility of our proposed framework in various graph-based applications. ", "page_idx": 9}, {"type": "text", "text": "Gained Performance on Heterophilic Graphs. Existing work for GC is focused on homophilic datasets. A notable contribution of our framework is its ability to generalize to all datasets, including heterophilic datasets as well. Building upon the observations made in Table 2 and Table 4 our methods, UGC (feat) and UGC (aug. feat.), showcase notable improvements in both node classification accuracy and REE values when applied to heterophilic datasets. A comparison of these results reveals that conventional approaches demonstrate poor node-classification accuracy on heterophilic graphs. In contrast, our UGC (features) method achieves substantial accuracy enhancements, surpassing the performance of these traditional approaches. Furthermore, the true potential of our approach becomes evident with augmented features $F$ i.e., UGC (aug. feat.). This approach exhibits remarkable accuracy gains, outperforming all other methods by a considerable margin, signifying the importance of augmented features $F$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a framework UGC for reducing a larger graph to a smaller graph. We use hashing of augmented node features inspired by Locality Sensitive Hashing (LSH). As expected, the benefits of LSH are also reflected in the proposed coarsening algorithm. To the best of our knowledge, it is the fastest algorithm for graph coarsening. Through extensive experiments, we have also shown that our algorithm is not only fast but also preserves the properties of the original graph. Furthermore, it is worth noting that UGC represents the first work in the domain of graph coarsening for heterophilic datasets. This framework addresses the unique challenges posed by heterophilic graphs and has demonstrated a significant increase in node classification accuracy following graph coarsening. In conclusion, we believe that our framework is a major contribution to the field of graph coarsening and offers a fast and effective solution for simplifying large networks. Our future research goals include the exploration of different hash functions and novel applications for the framework. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Mohit Kataria acknowledges the generous grant received from Google Research India to sponsor his travel to NeurIPS 2024. Additionally, this work is supported by DST INSPIRE faculty grant MI02322G and Yardi-ScAI, IIT Delhi research fund. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun, \u201cGraph neural networks: A review of methods and applications,\u201d ArXiv preprint, 2018.   \n[2] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur, \u201cProtein interface prediction using graph convolutional networks,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[3] Y. Wu, D. Lian, Y. Xu, L. Wu, and E. Chen, \u201cGraph convolutional networks with markov random field reasoning for social spammer detection,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 1054\u20131061, 2020.   \n[4] S. Kumar, J. Ying, J. V. de Miranda Cardoso, and D. P. Palomar, \u201cA unified framework for structured graph learning via spectral constraints.,\u201d J. Mach. Learn. Res., vol. 21, no. 22, pp. 1\u201360, 2020.   \n[5] M. Kataria, A. Khandelwal, R. Das, S. Kumar, and J. Jayadeva, \u201cLinear complexity framework for feature-aware graph coarsening via hashing,\u201d in NeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023.   \n[6] B. Hendrickson and R. Leland, \u201cA multilevel algorithm for partitioning graphs,\u201d in Proceedings of the 1995 ACM/IEEE Conference on Supercomputing, Supercomputing \u201995, (New York, NY, USA), p. 28\u2013es, Association for Computing Machinery, 1995.   \n[7] G. Karypis and V. Kumar, \u201cKumar, v.: A fast and high quality multilevel scheme for partitioning irregular graphs. siam journal on scientific computing 20(1), 359-392,\u201d Siam Journal on Scientific Computing, vol. 20, 01 1999.   \n[8] D. Kushnir, M. Galun, and A. Brandt, \u201cFast multiscale clustering and manifold identification,\u201d Pattern Recognition, vol. 39, no. 10, pp. 1876\u20131891, 2006. Similarity-based Pattern Recognition.   \n[9] I. S. Dhillon, Y. Guan, and B. Kulis, \u201cWeighted graph cuts without eigenvectors a multilevel approach,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 11, pp. 1944\u20131957, 2007.   \n[10] L. Wang, Y. Xiao, B. Shao, and H. Wang, \u201cHow to partition a billion-node graph,\u201d in 2014 IEEE 30th International Conference on Data Engineering, (Chicago, IL, USA), pp. 568\u2013579, IEEE, 2014.   \n[11] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra, \u201cBeyond homophily in graph neural networks: Current limitations and effective designs,\u201d in Advances in Neural Information Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.), vol. 33, pp. 7793\u20137804, Curran Associates, Inc., 2020.   \n[12] H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang, \u201cGeom-gcn: Geometric graph convolutional networks,\u201d arXiv preprint arXiv:2002.05287, 2020.   \n[13] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d 2016.   \n[14] M. Kumar, A. Sharma, and S. Kumar, \u201cA unified framework for optimization-based graph coarsening,\u201d Journal of Machine Learning Research, vol. 24, no. 118, pp. 1\u201350, 2023.   \n[15] A. Loukas, \u201cGraph reduction with spectral and cut guarantees,\u201d Journal of Machine Learning Research, vol. 20, no. 116, pp. 1\u201342, 2019.   \n[16] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and D. Koutra, \u201cGraph neural networks with heterophily,\u201d in Proceedings of the AAAI conference on artificial intelligence, vol. 35, pp. 11168\u201311176, 2021.   \n[17] L. Du, X. Shi, Q. Fu, X. Ma, H. Liu, S. Han, and D. Zhang, \u201cGbk-gnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily,\u201d in Proceedings of the ACM Web Conference 2022, pp. 1550\u20131558, 2022.   \n[18] M. McPherson, L. Smith-Lovin, and J. M. Cook, \u201cBirds of a feather: Homophily in social networks,\u201d Annual review of sociology, no. 1, 2001.   \n[19] C. R. Shalizi and A. C. Thomas, \u201cHomophily and contagion are generically confounded in observational social network studies,\u201d Sociological methods & research, no. 2, 2011.   \n[20] P. Indyk and R. Motwani, \u201cApproximate nearest neighbors: Towards removing the curse of dimensionality,\u201d in Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, STOC \u201998, (New York, NY, USA), p. 604\u2013613, Association for Computing Machinery, 1998.   \n[21] B. Kulis and K. Grauman, \u201cKernelized locality-sensitive hashing for scalable image search,\u201d in 2009 IEEE 12th international conference on computer vision, (Kyoto, Japan), pp. 2130\u20132137, IEEE, IEEE, 2009.   \n[22] J. Buhler, \u201cEfficient large-scale sequence comparison by locality-sensitive hashing,\u201d Bioinformatics, vol. 17, no. 5, pp. 419\u2013428, 2001.   \n[23] V. Satuluri and S. Parthasarathy, \u201cBayesian locality sensitive hashing for fast similarity search,\u201d Proc. VLDB Endow., vol. 5, p. 430\u2013441, jan 2012.   \n[24] B. Kulis and K. Grauman, \u201cKernelized locality-sensitive hashing for scalable image search,\u201d in 2009 IEEE 12th international conference on computer vision, pp. 2130\u20132137, IEEE, 2009.   \n[25] D. Ravichandran, P. Pantel, and E. Hovy, \u201cRandomized algorithms and nlp: Using locality sensitive hash functions for high speed noun clustering,\u201d in Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pp. 622\u2013629, 2005.   \n[26] O. Chum, J. Philbin, M. Isard, and A. Zisserman, \u201cScalable near identical image and shot detection,\u201d in Proceedings of the 6th ACM international conference on Image and video retrieval, pp. 549\u2013556, 2007.   \n[27] D. Ron, I. Safro, and A. Brandt, \u201cRelaxation-based coarsening and multiscale graph organization,\u201d 2010.   \n[28] J. Chen and I. Safro, \u201cAlgebraic distance on graphs,\u201d SIAM J. Scientific Computing, vol. 33, pp. 3468\u20133490, 12 2011.   \n[29] O. E. Livne and A. Brandt, \u201cLean algebraic multigrid (lamg): Fast graph laplacian linear solver,\u201d 2011.   \n[30] F. Dorfler and F. Bullo, \u201cKron reduction of graphs with applications to electrical networks,\u201d IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 60, no. 1, pp. 150\u2013163, 2013.   \n[31] M. Kumar, A. Sharma, S. Saxena, and S. Kumar, \u201cFeatured graph coarsening with similarity guarantees,\u201d in International Conference on Machine Learning, pp. 17953\u201317975, PMLR, 2023.   \n[32] W. Jin, L. Zhao, S. Zhang, Y. Liu, J. Tang, and N. Shah, \u201cGraph condensation for graph neural networks,\u201d 2021.   \n[33] X. Zheng, M. Zhang, C. Chen, Q. V. H. Nguyen, X. Zhu, and S. Pan, \u201cStructure-free graph condensation: From large-scale graphs to condensed graph-free data,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[34] Z. Huang, S. Zhang, C. Xi, T. Liu, and M. Zhou, \u201cScaling up graph neural networks via graph coarsening,\u201d 2021.   \n[35] C. Cai, D. Wang, and Y. Wang, \u201cGraph coarsening with neural networks,\u201d arXiv preprint arXiv:2102.01350, 2021.   \n[36] Z. Yang, W. W. Cohen, and R. Salakhutdinov, \u201cRevisiting semi-supervised learning with graph embeddings,\u201d in Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, JMLR Workshop and Conference Proceedings, 2016.   \n[37] O. Shchur, M. Mumme, A. Bojchevski, and S. G\u00fcnnemann, \u201cPitfalls of graph neural network evaluation,\u201d ArXiv preprint, 2018.   \n[38] X. Fu, J. Zhang, Z. Meng, and I. King, \u201cMagnn: Metapath aggregated graph neural network for heterogeneous graph embedding,\u201d in Proceedings of The Web Conference 2020, pp. 2331\u20132341, 2020.   \n[39] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna, \u201cGraphsaint: Graph sampling based inductive learning method,\u201d arXiv preprint arXiv:1907.04931, 2019.   \n[40] W. L. Hamilton, R. Ying, and J. Leskovec, \u201cInductive representation learning on large graphs,\u201d 2017.   \n[41] A. Loukas and P. Vandergheynst, \u201cSpectrally approximating large graphs with smaller graphs,\u201d in Proceedings of the 35th International Conference on Machine Learning (J. Dy and A. Krause, eds.), vol. 80 of Proceedings of Machine Learning Research, (PMLR 80:3237-3246, 2018), pp. 3237\u20133246, PMLR, 10\u201315 Jul 2018.   \n[42] G. Bravo Hermsdorff and L. Gunderson, \u201cA unifying framework for spectrum-preserving graph sparsification and coarsening,\u201d Advances in Neural Information Processing Systems, vol. 32, p. 12, 2019.   \n[43] P. Indyk, \u201cStable distributions, pseudorandom generators, embeddings, and data stream computation,\u201d Journal of the ACM (JACM), vol. 53, no. 3, pp. 307\u2013323, 2006.   \n[44] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, \u201cSpectral networks and locally connected networks on graphs,\u201d 2013. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[45] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, \u201cSimple and deep graph convolutional networks,\u201d 2020. ", "page_idx": 12}, {"type": "text", "text": "[46] M. Defferrard, X. Bresson, and P. Vandergheynst, \u201cConvolutional neural networks on graphs with fast localized spectral filtering,\u201d in Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, (Red Hook, NY, USA), p. 3844\u20133852, Curran Associates Inc., 2016.   \n[47] C. Li and D. Goldwasser, \u201cEncoding social information with graph convolutional networks forPolitical perspective detection in news media,\u201d in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, (Florence, Italy), pp. 2594\u20132604, Association for Computational Linguistics, July 2019.   \n[48] A. Paliwal, F. Gimeno, V. Nair, Y. Li, M. Lubin, P. Kohli, and O. Vinyals, \u201cReinforced genetic algorithm learning for optimizing computation graphs,\u201d 2019.   \n[49] L. van der Maaten and G. Hinton, \u201cVisualizing data using t-sne,\u201d Journal of Machine Learning Research, vol. 9, no. 86, pp. 2579\u20132605, 2008.   \n[50] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are graph neural networks?,\u201d 2018.   \n[51] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio, \u201cGraph attention networks,\u201d in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.   \n[52] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni, \u201cLocality-sensitive hashing scheme based on p-stable distributions,\u201d in Proceedings of the twentieth annual symposium on Computational geometry, pp. 253\u2013262, 2004.   \n[53] V. Zolatarev, \u201cOne-dimensional stable distributions, vol. 65 of\" translations of mathematical monographs,\",\u201d American Mathematical Society. Translation from the original, 1983.   \n[54] J. M. Chambers, C. L. Mallows, and B. Stuck, \u201cA method for simulating stable random variables,\u201d Journal of the american statistical association, vol. 71, no. 354, pp. 340\u2013344, 1976.   \n[55] J. Nolan, \u201cAn introduction to stable distributions,\u201d in on web, 2005. ", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Stable Distribution ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A distribution $\\mathcal{D}$ over $\\mathcal{R}$ is called p-stable if there exists $\\mathsf{p}\\geq0$ such that for any n real numbers $v_{1}...v_{n}$ and i.i.d. variables $X_{1}...X_{n}$ with distribution $\\mathcal{D}$ , the random variable $\\sum_{i}v_{i}X_{i}$ has the same distribution as the variable $(\\sum_{i}|v_{i}|^{p})^{1/p}X$ where $X$ is a random variable with distribution $\\mathcal{D}$ [52]. It is known [53] that stable distributions exist for $\\mathsf{p}\\in(0,2]$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 Cauchy distribution $\\mathcal{D}_{c}$ , defined by the density function $\\begin{array}{r}{c(x)=\\frac{1}{\\pi}\\frac{1}{1+x^{2}}}\\end{array}$ , is 1-stable. \u2022 Gaussian (normal) distribution $\\mathcal{D}_{g}$ , defined by the density function $\\begin{array}{r}{g(x)=\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^{2}}{2}}}\\end{array}$ is 2-stable. However, it is known [54] that one can create $\\mathbf{p}$ -stable random variables effectively from two independent variables distributed uniformly across [0,1] despite the lack of closed-form density and distribution functions. ", "page_idx": 13}, {"type": "text", "text": "Stable distributions have diverse applications across various fields (see survey [55] for details). In computer science, they are utilized for \"sketching\" high-dimensional vectors, as demonstrated by Indyk ([43]). The key property of p-stable distributions, mentioned in the definition, enables a sketching technique for high-dimensional vectors. This technique involves generating a random vector w of dimension $\\mathbf{d}$ , with each entry independently chosen from a $\\mathfrak{p}$ -stable distribution. Given a vector v of dimension d, the dot product $w\\cdot v$ is also a random variable. A small collection of such dot products, corresponding to different w\u2019s, is termed as the sketch of the vector v and can be used to estimate $||\\boldsymbol{v}||_{p}$ [43]. However, instead of using the sketch to estimate the vector norm, we are using it to assign hash values to each vector. These values map each vector to a point on the real line, which is then split into equal-width segments to represent buckets. If two vectors v and If you are close, they will have a small difference between $l_{p}$ norms $\\lVert\\boldsymbol{v}-\\boldsymbol{u}\\rVert_{p}$ , and they should collide with a high probability ", "page_idx": 13}, {"type": "text", "text": "B Algorithm for UGC-FL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 2 UGC-FL: Universal Graph Coarsening with feature re-learning ", "page_idx": 13}, {"type": "text", "text": "Require: Input ${\\mathcal{G}}(V,A,X)$ , l \u2190Number of Projectors, r \u2190binWidth   \n1: $\\mathcal{G}_{c}(\\widetilde{V},\\widetilde{A},\\widetilde{F}),\\mathcal{C}=U G C(\\mathcal{G},l,r)$   \n2: $\\begin{array}{r}{\\alpha=\\frac{|\\{(v,u)\\in E:y_{v}=y_{u}\\}|}{|E|}}\\end{array}$ ; $\\alpha$ is heterophily factor, $y_{i}\\in\\mathbb{R}^{N}$ denotes node labels, $E$ denotes edge list   \n3: $F=\\left\\{(1-\\alpha)\\cdot X\\oplus\\alpha\\cdot A\\right\\}$ Augmented features   \n4: $\\begin{array}{r}{\\widehat{F}=\\left(\\frac{2}{\\alpha}\\mathcal{C}^{T}L\\mathcal{C}+\\mathcal{C}^{T}\\mathcal{C}\\right)^{-1}\\mathcal{C}^{T}F}\\end{array}$   \n5: return $\\mathcal{G}_{c}(\\widetilde{V},\\widetilde{A},\\widehat{F}),\\mathcal{C}$ ", "page_idx": 13}, {"type": "text", "text": "C Size of coarsened graph Controlled by Bin-Width ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section discusses the impact of bin-width on the coarsening ratio see Figure 8. Algorithm 3 outlines the procedure for determining the appropriate bin-width value that corresponds to a desired coarsening ratio. The parameter bin-width $r$ decides the size of the coarsened graph $\\mathcal{G}_{c}$ . For a particular coarsening ratio $R$ , we find the corresponding $r$ by divide and conquer approach on the real axis, which is similar to binary search. Algorithm 3 shows the method by which we find the $r$ for any given $R$ for $\\mathcal{G}_{c}$ . Figure 8 shows the relation of $r$ with $R$ for two datasets: a) Cora, and Coauthor CS. It is observed that the $R$ increases as the $r$ increases. For each dataset, $r$ is a hyper-parameter that needs to be calculated only once, and hence its computational cost is not included in the reported time complexity. ", "page_idx": 13}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/45d458cc7044a84b1a8b7c27ab2e53e29b60b836d0c5b76ea843b4cea1289d05.jpg", "img_caption": ["Figure 8: This figure shows the trend of coarsening ratio as the bin-width increases on two datasets: Cora and Coauthor CS. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Bin-width Finder ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Require: Input ${\\mathcal{G}}(V,A,X)$ , $L\\leftarrow$ Number of Projectors, $R\\leftarrow$ Coarsening Ratio, $p\\leftarrow$ precision of coarsening   \nEnsure: bin-width $h$   \n1: $r\\gets1$ , ratio $\\gets1$ , $N\\gets\\|V\\|$   \n2: while $|R-r a t i o|>p$ do   \n3: if ratio $>R$ then   \n4: $r\\leftarrow r*0.5$   \n5: else   \n6: $r\\leftarrow r*1.5$   \n7: $n\\leftarrow\\mathrm{UGC}(\\mathcal{G},L,r);$ $n$ denotes number of supernodes in $\\mathcal{G}_{c}$   \n8: $\\begin{array}{r}{r a t i o\\gets(1-\\frac{n}{N})}\\end{array}$   \n9: return $r$ ", "page_idx": 14}, {"type": "text", "text": "D Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $f_{p}(t)$ denote the probability density function of the absolute value of our stable distribution(Normal distribution), and let $c=||\\boldsymbol{v}-\\boldsymbol{u}||_{p}$ for two node vectors v, u, and $\\mathbf{r}$ is the bin-width. Since we have a random vector w from our stable distribution, $v.w-u.w$ is distributed as $\\mathrm{cX}$ where $\\Chi$ is a random variable from our stable distribution. Therefore our probability function is ", "page_idx": 14}, {"type": "equation", "text": "$$\np(c)=P r_{a,b}\\left[h_{a,b}(v)=h_{a,b}(u)\\right]=\\int_{0}^{r}\\frac{1}{c}f_{p}\\left(\\frac{t}{c}\\right)\\left(1-\\frac{t}{r}\\right)d t\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For a fixed bin-width $r$ the probability of collision decreases monotonically with $c=||\\boldsymbol{v}-\\boldsymbol{u}||_{2}$ . For Definition, 2.1 the hash family will be $\\left(r_{1},r_{2},p_{1},p_{2}\\right)$ -sensitive where $p_{1}=p(1)$ and $p_{2}=p(c)$ for r2 = c. ", "page_idx": 14}, {"type": "text", "text": "For 2-stable distribution $\\begin{array}{r}{f_{p}(x)=\\frac{2}{\\sqrt{2\\pi}}e^{-x^{2}/2}}\\end{array}$ . Equation 9 will be ", "page_idx": 14}, {"type": "equation", "text": "$$\np(c)=\\frac{2}{\\sqrt{2\\pi}}\\int_{0}^{r}\\frac{1}{c}e^{-\\left(\\frac{1}{c}\\right)^{2}/2}d t-\\frac{2}{\\sqrt{2\\pi}}\\int_{0}^{r}\\frac{1}{c}e^{-\\left(\\frac{1}{c}\\right)^{2}/2}\\frac{t}{r}d t\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n=S_{1}(c)-S_{2}(c)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $S_{1}(c)\\leq1$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{2}(c)=\\frac{2}{\\sqrt{2\\pi}}\\cdot\\frac{c}{r}\\int_{0}^{r}e^{-\\left(\\frac{t}{c}\\right)^{2}/2}\\frac{t}{c^{2}}d t\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{2}(c)=\\frac{2}{\\sqrt{2\\pi}}\\cdot\\frac{c}{r}\\int_{0}^{\\frac{\\mathbb{R}^{2}}{(2c^{2})}}e^{-y}d y\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{2}(c)=\\frac{2}{\\sqrt{2\\pi}}\\cdot\\frac{c}{r}[1-e^{-\\frac{{\\mathbb R}^{2}}{(2c^{2})}}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have $\\begin{array}{r}{p(1)=S_{1}(1)-S_{2}(1)\\geq1-e^{\\frac{\\mathbb{R}^{2}}{2}}-\\frac{2}{\\sqrt{2\\pi r}}\\geq1-\\frac{A}{r}}\\end{array}$ , for some constant $\\mathrm{A}>0$ . This implies that the probability that u collides with $\\mathbf{V}$ is at least $\\textstyle(1-{\\frac{A}{r}})\\approx e^{-A}$ . Thus the algorithm is correct with the constant probability.   \nIf $c^{2}\\leq{\\frac{\\mathbb{R}^{2}}{2}}$ , then we have ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\np(c)\\leq1-\\frac{2}{\\sqrt{2\\pi}}\\frac{c}{r}(1-\\frac{1}{e})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "E Additional experiments for LSH scheme ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We have further validated our theoretical results through a secondary experiment. This LSH family which we discussed above says as the distance between two nodes increases, the likelihood of them being assigned to the same bin decreases, hence we will have more number of super-nodes now. By increasing the bin-width, we can effectively reduce the number of super-nodes. This phenomenon is evident when considering the average distance between node pairs in various graphs and the corresponding bin-width required to achieve a $30\\%$ coarsening ratio. The table below illustrates these findings: ", "page_idx": 15}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/31e03a3a1d80c0c3d08dfd32544ec804b9374fe4b75f044c914661016a873730.jpg", "table_caption": ["Table 5: Average Distance and Bin-Width for $30\\%$ Coarsening "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "The results in the table clearly demonstrate that as the average distance between nodes increases, the required bin-width also increases when maintaining the same coarsening ratio. This observation highlights the importance of considering the distance metric and bin-width selection during the graph coarsening process to effectively control the number of super-nodes and achieve desired coarsening ratios. Figure 8 shows the trend of coarsening ratio when we change bin-width. ", "page_idx": 15}, {"type": "text", "text": "F System Specifications: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All the experiments conducted for this work were performed on an Intel Xeon W-295 CPU and 64GB of RAM desktop using the Python environment. ", "page_idx": 15}, {"type": "text", "text": "G Datasets ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/79d79dbb71296554f21580264d5194af88accbb11f23ad134110bcc48dd08e19.jpg", "table_caption": ["Table 6: Summary of the datasets. H.R shows heterophily factor. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "H Application of coarsened graph for GNNs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section contains additional results related to the scalable GNN training. Figure 9 shows the GNN training pipeline. Figure 10 shows the visualization of GCN predicted nodes when training is done using the coarsened graph. ", "page_idx": 16}, {"type": "text", "text": "We used four GNN models, namely GCN, GraphSage, GIN, and GAT. Table 7 contains node classification accuracy results for across various methodologies employing different GNN models. Table 8 contains parameter details used in UGC across different GNN models. We have used these parameters across all methods. ", "page_idx": 16}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/fa74b0e729b9f1f846dfc4566cb761fbec591d0b259bbec77042ff38db0b1975.jpg", "table_caption": ["Table 7: Evaluation of node classification accuracy of different GNN models when trained with $50\\%$ coarsen graph. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/7b2068e217d1d18ff71a0c995a47488a32d576f1f1685a7eb86b3c58c3fd764c.jpg", "img_caption": ["Figure 10: Visualization of GCN predicted nodes when training is done using the coarsened graph of Physics dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/07c5ac873824fbe0e47d7895633488eb7e71cfad4db5d632be0dfa812195d17a.jpg", "table_caption": ["Table 8: GNN model parameters. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 9: We report the accuracy of GCN on node classification after coarsening by UGC at different ratios. ", "page_idx": 17}, {"type": "table", "img_path": "nN6NSd1Qds/tmp/a89a0d0e38a52619d2abaf4fe3a281ebfb59c35a879b37c53a46bc65c4f704a5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We randomly split data in $60\\%$ , $20\\%$ , $20\\%$ for the training-validation-test. ", "page_idx": 17}, {"type": "text", "text": "I Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem I.1 Given a Graph $G$ and a coarsened graph $G_{c}$ they are said to be $\\epsilon$ similar if there exists some $\\epsilon\\geq0$ such that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\|x\\|_{L}\\leq\\|x\\|_{L_{c}}\\leq(1+\\epsilon)\\|x\\|_{L}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $L$ and $L_{c}$ are the Laplacian matrices of $G$ and $G_{c}$ respectively. ", "page_idx": 17}, {"type": "text", "text": "Proof: Let S be defined such that ${\\mathrm{L}}=S^{T}S$ , by Cholesky\u2019s decomposition: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\vert x\\vert\\|_{L}-\\|x_{c}\\|_{L_{c}}\\vert=|\\|S x\\|_{2}-\\|S P^{+}P x\\|_{2}|}\\\\ {\\leq\\|S x-S P^{+}P x\\|_{2}=\\|x-\\widetilde{x}\\|_{L}\\leq\\|x\\|_{L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The conversion from $L^{t h}-$ norm to $2^{n d}-$ norm or vice-versa is as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|x\\|_{L}={\\sqrt{x^{T}L x}}={\\sqrt{x^{T}S^{T}S x}}=\\|S\\|_{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "J Proof of Bounded Theorem 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem J.1 Given a graph $\\mathcal{G}(L,F)$ , a coarsened graph $\\mathcal{G}_{c}(L_{c},F_{c})$ , the enhanced features $\\widetilde{F}$ obtained by enforcing smoothness condition. The original graph $\\mathcal{G}(L,F)$ and a coarsened gra ph $\\mathcal{G}_{c}(L_{c},\\widetilde{F})$ , are said to be \u03f5 similar with $0<\\epsilon\\le1$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\|F\\|_{L}\\leq\\|\\widetilde{F}\\|_{L_{c}}\\leq(1+\\epsilon)\\|F\\|_{L}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $L$ and $L_{c}$ are the Laplacian matrices, $F$ and $F_{c}$ are the augmented feature vector given by UGC, $\\widetilde{F}$ is the relearned enhanced features by enforcing the smoothness condition discussed in Equati on $^ \u1e0a 6 \u1e0c$ . ", "page_idx": 18}, {"type": "text", "text": "Proof: ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n||F||_{L}-||\\widetilde{F}||_{L_{c}}|=|\\sqrt{t r(F^{T}L F)}-\\sqrt{t r(\\widetilde{F}^{T}L_{c}\\widetilde{F})}|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As $L$ is a positive semi-definite matrix we can write ${\\cal L}=S^{T}S$ using Cholesky\u2019s decomposition and by writing $L_{c}=C^{T}L C$ we get, ", "page_idx": 18}, {"type": "equation", "text": "$$\n=|\\sqrt{t r(F^{T}S^{T}S F)}-\\sqrt{t r(\\widetilde{F}^{T}C^{T}S^{T}S C\\widetilde{F})}|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n=||S F||_{F}-||S P^{\\dagger}P F|||_{F}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq\\|S F-S P^{\\dagger}P F\\|_{F}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq\\epsilon\\|F\\|_{L}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the new update rule of $\\|\\widetilde{F}\\|_{L_{c}}$ we have $\\widetilde{F}_{L_{c}}\\leq\\|F\\|_{L}$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\epsilon=\\frac{|\\|\\boldsymbol{F}\\|_{L}-\\|\\widetilde{\\boldsymbol{F}}\\|_{L_{c}}|}{\\|\\boldsymbol{F}\\|_{L}}\\leq1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\epsilon\\leq1$ refer [14] for more details. See Figure 6 which plots different values of $\\epsilon$ at different coarsening ratios. As mentioned for fixed values of $\\alpha$ we got $\\epsilon\\leq1$ similarity guarantees for the coarsened graph. ", "page_idx": 18}, {"type": "text", "text": "K Importance of Augmented Features ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "See Figure 12 which showcases the importance of considering the augmented feature vector. It can be seen from the figure that when coarsened using Augmented features super-nodes have more intra-node similarity. ", "page_idx": 18}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/d73ed7255ec7b6bf725930afd0fcee77a4313b3d3ffd6584567f9c47a3af8840.jpg", "img_caption": ["Figure 11: A toy example illustrating the computation of augmented features of a given graph. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "nN6NSd1Qds/tmp/d62879fa65d5450703c16e0699f048b69e88ee0cf30efd09bc158dea3232dacd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 12: This figure highlights the significance of the augmented vector and showcases coarsening outcomes, specifically when coarsening is performed solely using the adjacency or feature matrix compared to when the augmented matrix is taken into account. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] Justification: Yes, all the claims are reflected in paper. See Section 4 and Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Section 3 (Need efficient implementations and sparse tensor methods to represent the $F$ matrix) and Section 5 (Exploration of different hash functions and novel applications for the framework). ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Justification: See Appendix Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experi  \nmental results of the paper to the extent that it affects the main claims and/or conclusions of the   \npaper (regardless of whether the code and data are provided or not)?   \nAnswer: [Yes]   \nJustification: See Section 4 and Appendix   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to   \nfaithfully reproduce the main experimental results, as described in supplemental material?   \nAnswer: [Yes]   \nJustification: All datasets used are publicly available. See Section 4 and link UGC-NeurIPS   \nGuidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?   \nAnswer: [Yes]   \nJustification: See Section 4 and Appendix.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?   \nAnswer: [Yes]   \nJustification: See Section 4 and Appendix.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?   \nAnswer: [Yes]   \nJustification: See Appendix.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics.   \nGuidelines:   \n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal   \nimpacts of the work performed?   \nAnswer: [NA]   \nJustification: There is no societal impact of the work performed.   \nGuidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of   \ndata or models that have a high risk for misuse (e.g., pretrained language models, image generators,   \nor scraped datasets)?   \nAnswer: [NA]   \nJustification: The paper poses no such risks   \nGuidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes]   \nJustification: Assets are properly credited and publicly available.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [NA]   \nJustification: The paper does not release new assets.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper   \ninclude the full text of instructions given to participants and screenshots, if applicable, as well as   \ndetails about compensation (if any)?   \nAnswer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]