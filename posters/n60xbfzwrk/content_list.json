[{"type": "text", "text": "Hyperbolic Embeddings of Supervised Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Richard Nock Ehsan Amid Frank Nielsen Google Research Google DeepMind Sony CS Labs, Inc. richardnock@google.com eamid@google.com frank.nielsen@acm.org ", "page_idx": 0}, {"type": "text", "text": "Alexander Soen Manfred K. Warmuth RIKEN AIP Google Research Australian National University manfred@google.com alexander.soen@anu.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Models of hyperbolic geometry have been successfully used in ML for two main tasks: embedding models in unsupervised learning (e.g. hierarchies) and embedding data. To our knowledge, there are no approaches that provide embeddings for supervised models; even when hyperbolic geometry provides convenient properties for expressing popular hypothesis classes, such as decision trees (and ensembles). In this paper, we propose a full-fledged solution to the problem in three independent contributions. The first linking the theory of losses for class probability estimation to hyperbolic embeddings in Poincar\u00e9 disk model. The second resolving an issue for an easily interpretable, unambiguous embedding of (ensembles of) decision trees in this model. The third showing how to smoothly tweak the Poincar\u00e9 hyperbolic distance to improve its encoding and visualization properties near the border of the disk, a crucial region for our application, while keeping hyperbolicity. This last step has substantial independent interest as it is grounded in a generalization of Leibniz-Newton\u2019s fundamental Theorem of calculus. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Models of hyperbolic geometry have been successfully used to embed hierarchies, i.e. tree-based structures [14, 20, 38, 36]. Through the property of low-distortion hyperbolic embeddings [30], symbolic (hierarchies) and numeric (quality metrics) properties of unsupervised learning models can be represented in an accurate and interpretable manner [29]. When it comes to supervised learning, the current trend involves embedding data in a hyperbolic space, with models trained on the now hyperbolic data [9, 15, 8, 11]. It is important to note that none of these supervised methods embed models. Indeed, the focus of the prior literature is to learn supervised models from hyperbolic data, rather than representing supervised models in hyperbolic geometry. This is in stark contrast to the unsupervised usage described, where the (tree-based) structural properties of unsupervised models are directly exploited for good embeddings. This hints at beneftis which can be used in the supervised case, especially for popular supervised models which are (structurally) tree-based. ", "page_idx": 0}, {"type": "text", "text": "Our paper proposes a full-fledged solution for this problem, in three independent contributions. ", "page_idx": 0}, {"type": "text", "text": "The first focuses on the numerical part (quality metrics) of the embedding and its link with supervised losses: to provide a natural way of embedding classification and the confidence of prediction, we show a link between training with the log-loss (posterior estimation) or logistic loss (real-valued classification) and hyperbolic distances computation in the Poincar\u00e9 disk. ", "page_idx": 0}, {"type": "text", "text": "The second focuses on the symbolic part (hierarchies) of the embeddings for a popular kind of supervised of models: decision trees (and ensembles). Unlike for unsupervised learning, we show that getting an unambiguous embedding of a decision tree requires post-processing the model. Our solution extracts its monotonic sub-tree via a new class of supervised models we introduce, monotonic decision trees. This is also convenient for explainability purposes [32]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The third and most technical one focuses on visualization and the accuracy of the numerical encoding in the Poincar\u00e9 disk model. The more accurate / confident is a supervised prediction, the closer to the border of the disk it is represented. Thus, the best models will have their best predictions squeezed close to the border. In addition to being suboptimal for visualization, this region is also documented for being numerically error-prone for hyperbolic distance computation [29]. Two general fixes currently exist: encoding values with (exponentially) more bits or utilizing a trick from Riemannian geometry [19]. As neither is satisfactory for our usage, we propose a third, principled route: like other distances, Poincar\u00e9 distance is integral. We generalize Leibniz-Newton\u2019s fundamental Theorem of calculus using a generalization of classical arithmetic [22]. This generalized \u201ctempered\u201d integral provides a parameter to smoothly alter the properties of the \u201cclassical\u201d integral. When defining the Poincar\u00e8 distance, the tempering controls the hyperbolic constant of the embedding whilst also improving the visualization and numeric accuracy of the embedded models. The generalization of integrals to distances has independent interest as many application in ML rely on integral based distances and distortions, i.e., Bregman divergences, $f$ -divergences, integral probability metrics, etc. ", "page_idx": 1}, {"type": "text", "text": "Experiments are provided on readily available domains, and all proofs, additional results and additional experiments are given in an Appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Models of hyperbolic geometry have been mainly useful to embed hierarchies, i.e. tree-based structures [14, 20, 38, 36], with a sustained emphasis on coding size and numerical accuracy [19, 29, 37]. In unsupervised learning and clustering, some applications have sought a simple representation of data on the form of a tree or via hyperbolic projections [7, 6, 17, 34]. Approaches dealing with supervised learning assumes the data lies in a hyperbolic space: the output visualized is primarily an embedding of the data itself, with additional details linked to classification of secondary importance, either support vector machines [9], neural nets, logistic regression [15], or (ensembles of) decision trees [8, 11]. We insist on the fact that the aforementioned methods do not represent the models in the hyperbolic space, even when those models tree-shaped. The question of embedding classifiers is potentially important to improve the state of the art visualization: in the case of decision trees, popular packages stick to a topological layer (the tree graph) to which various additional information about classification are superimposed but without principled links to the \u201cembedding\u201d\\*. ", "page_idx": 1}, {"type": "text", "text": "3 Basic definitions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Training a supervised model starts with a set (sample) of examples, each of which is a pair $(\\pmb{x},y)$ , where $\\mathbf{\\Delta}x\\in\\mathcal{X}$ (a domain) and $y\\in\\{-1,1\\}$ (labels or classes). A decision tree (DT) consists of a binary rooted tree $H$ , where at each internal nodes, the two outgoing arcs define a Boolean test over observation variables (see Figure 1, center, for an example); $\\mathbb{N}(\\bar{H})$ is the set of nodes of $H$ , $\\Lambda(H)\\subseteq\\mathcal{N}(H)$ is the set of leaf nodes of $H$ . ", "page_idx": 1}, {"type": "text", "text": "The log-loss is best introduced in the context of the theory of losses for class probability estimation (CPE) [28]. We follow the notations of [18]: A CPE loss function, $\\ell:y\\times[0,1]\\to\\mathbb{R}$ , is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\ell(y,p)}&{{}\\doteq}&{\\left[\\![y=1]\\!]\\cdot\\ell_{1}(p)+\\left[\\![y=-1]\\!]\\cdot\\ell_{-1}(p)\\!,\\!}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $[\\![.]\\!]$ are Iverson brackets [16]. Functions $\\ell_{1},\\ell_{-1}$ are called partial losses. A CPE loss is symme t ri c when $\\ell_{1}(p)\\;=\\;\\ell_{-1}(1\\,-\\,p),\\forall p\\;\\in\\;[0,1]$ . The log-loss is a symmetric CPE loss with $\\dot{\\ell_{1}}^{\\mathrm{{LoG}}}(p)\\doteq-\\log(p)$ . The goal of learning using a CPE loss is to optimize the Bayes risk, $\\underline{{L}}(p)\\doteq$ $\\operatorname{inf}_{\\pi}\\mathsf{E}_{\\mathsf{Y}\\sim p}\\ell(\\mathsf{Y},\\pi)$ . In the case of the log-loss, it is the criterion used to learn DTs in C4.5 [26]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underline{{L}}^{\\mathrm{LOG}}(p)}&{{}=}&{-p\\cdot\\log p-(1-p)\\cdot\\log(1-p).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Models like DTs predict an empirical posterior $\\hat{\\mathrm{Pr}}[\\mathsf{Y}=1|\\mathsf{X}]$ at the leaves: for an observation $\\textbf{\\em x}$ reaching leaf $\\lambda$ , the posterior prediction is the local relative proportion of positive examples in leaf $\\lambda$ , as estimated by the training sample (noted $p_{\\lambda}^{+}$ ). There exists a duality between CPE classification and the real-valued classification setting familiar to e.g. deep learning: optimizing Bayes risk for the posterior is equivalent to minimizing its convex surrogate using the canonical link of the loss to compute real classification [24, Theorem 1]. The canonical link of the log-loss, $\\psi_{\\mathrm{LoG}}:[0,1]\\rightarrow\\mathbb{R}$ is the famed inverse sigmoid, which has a very convenient form for our purpose, ", "page_idx": 1}, {"type": "image", "img_path": "n60xBFZWrk/tmp/17f9093925a5b4b9ad8c75fec32a890153ebc09fc0b07228b29f598854e97563.jpg", "img_caption": ["Figure 1: Left pane: subtree of a decision tree (DT) (colors red, green denote the majority class in each node, grey $=$ random posterior, tests at arcs not shown, $n_{1}$ is the root, no leaves shown) and its embedding following the simple recipe in (3): it is impossible to tell $n_{2}$ from $n_{5}$ and the tree depicted in $\\mathbb{B}$ is not a faithful representation of the DT nor of any of its subtrees. Right pane: a small DT learned on UCI abalone (left) and its corresponding monotonic decision tree (MDT, right) learned using GETMDT. In each node, the real-valued prediction $(\\psi_{\\mathrm{LoG}}(p))$ is indicated, also in color. Observe that, indeed, $H$ does not grant path-monotonic classification but $H^{\\prime}$ does (Definition 4.1). In $H^{\\prime}$ , some nodes have outdegree 1; also, internal node $\\#6$ in the DT, whose prediction is worse than its parent, disappears in $H^{\\prime}$ . One arc in $H^{\\prime}$ is represented with double width as its Boolean test aggregates both tests it takes to go from $\\#3$ to $\\#10$ in $H$ . Observations that would be classified by leaf $\\#11$ (resp. $\\#5$ , resp. $\\#9,$ ) in $H$ are now classified by internal node $\\#3$ (resp. $\\#2$ , resp. $\\#4)$ ) in $H^{\\prime}$ , but predicted labels are the same for $H$ and $H^{\\prime}$ and so the accuracy of $H$ and $H^{\\prime}$ are the same. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi_{\\mathrm{{LoG}}}(p)=\\log\\left({\\frac{p}{1-p}}\\right)\\quad\\Rightarrow\\quad|\\psi_{\\mathrm{{LoG}}}(p)|=\\log\\left({\\frac{1+r}{1-r}}\\right),\\quad r\\,\\doteq\\,|2p-1|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notably, the absolute value $|\\psi_{\\mathrm{LoG}}(p)|$ is a confidence with the sign giving the predicted class. In the case of the log-loss, the convex surrogate is hugely popular in ML: it is the logistic loss. ", "page_idx": 2}, {"type": "text", "text": "We now introduce concepts from hyperbolic geometry, particularly those from the Poincar\u00e9 disk model. Our definitions are simplified, a detailed account can be found in [5, 20]. The distance function $d$ of a metric space is $\\tau$ -hyperbolic for some $\\tau\\geqslant0$ iff for any three geodesic curves $\\gamma^{1},\\gamma^{2},\\gamma^{3}$ linking three points, there exists $_{\\textit{z}}$ such that $\\operatorname{\\,ux}_{i}d(z,\\gamma^{i})\\leqslant\\tau$ , where $d(z,\\gamma)\\doteq\\operatorname*{inf}_{z^{\\prime}\\in\\gamma}d(z,z^{\\prime})$ . A small hyperbolic constant guarantees thin triangles and embeddings of trees with good properties. The Poincar\u00e9 disk model, $\\mathbb{B}$ (negative curvature, $-1$ ) is a popular model of hyperbolic geometry with the hyperbolic distance between the origin and some $z\\in\\mathbb{B}$ with Euclidean norm $r\\doteq\\|z\\|$ being: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd_{\\mathbb{B}}(z,\\mathbf{0})\\quad=\\quad\\log\\left(\\frac{1+r}{1-r}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "4 Posterior embedding in Poincar\u00e9 disk and clean embeddings for DTs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Node embedding We exploit the similarity between log-loss confidences $|\\psi_{\\mathrm{LoG}}(p)|$ and hyperbolic distances $d_{\\mathbb{B}}(z,\\mathbf{0})$ when embedding classification models, similarity which is obvious from (3) and (4). In (3), we see that if $r\\,=\\,0$ , the prediction is as bad as a fair coin\u2019s. As $r$ edges closer to 1, prediction approaches maximum confidence. The connection with the Poincar\u00e9 distance (4) is immediate: a natural embedding of a posterior prediction $p$ is then a point $_{z}$ in the Poincar\u00e9 disk $\\mathbb{B}$ at radius $r\\doteq\\|z\\|$ from the origin (3). The origin of Poincar\u00e9 disk represents the worst possible confidence. As each leaf $\\lambda$ of a DT $H$ corresponds to a posterior $p_{\\lambda}^{+}$ , the leaves can be embedded as described. In addition, all nodes $\\nu\\in\\mathcal{N}(H)$ in the tree also have corresponding posteriors, and thus can also be embedded identically to leaf nodes. It is also a good idea to embed them because, as classical DT induction algorithms first proceed by top-down induction, any node $\\nu$ in a DT was a leaf at some point during training, and has a corresponding posterior. This apparently clean picture for node embedding however becomes messy as soon as we step to embedding full trees. ", "page_idx": 2}, {"type": "text", "text": "No clean embedding for a full DT A simple counterexample indeed demonstrates that a clean embedding of a decision tree is substantially trickier, see Figure 1 (left). In this pathological example, some distinct nodes (resp. arcs) of the DT $H$ are embedded in the same node (resp. edge) in $\\mathbb{B}$ : the depiction has no subgraph relationship to $H$ and some embedded nodes cannot be distinguished between each other without additional information. ", "page_idx": 3}, {"type": "text", "text": "Getting a clean embedding: Monotonic Decision Trees To introduce our fix, we define three broad objectives for the embedding in $\\mathbb{B}$ of DTs ", "page_idx": 3}, {"type": "text", "text": "Embedding objective (A) Embed a tree-based model from $H$ in $\\mathbb{B}$ , which defines an injective mapping of the nodes of $H$ and induces a subtree of $H$ with each edge in $\\mathbb{B}$ corresponding to a path in $H$ ; (B) Locally, each node $\\nu$ of $H$ gets embedded to some $z_{\\nu}$ such that $r_{\\nu}$ is close to $\\left\\Vert z_{\\nu}\\right\\Vert$ (3); (C) Globally, the whole embedding remains convenient and readable to compare, in terms of confidence in classification, different subtrees in the same tree, or even between different trees. ", "page_idx": 3}, {"type": "text", "text": "As we shall see later in this Section, this agenda also carries to embedding ensembles of DTs, thus including their leveraging coefficients, exploiting properties of boosting algorithms. For now, our solution for embedding a single DT that can satisfy (A-C) is simple in principle: ", "page_idx": 3}, {"type": "text", "text": "Embed the monotonic classification part of a DT, ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "meaning for each path from the root to a leaf in $H$ , we only embed the subsequence of nodes whose absolute confidences are strictly increasing. To do so, we replace the DT by a \"close\" path-monotonic approximation model using a new class of models we call Monotonic Decision Trees (MDT). ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1. A Monotonic Decision Tree (MDT) is a rooted tree with a Boolean test labeling each arc and a real-valued prediction at each node. Any sequence of nodes in a path starting from the root is strictly monotonically increasing in absolute confidence. At any internal node, no two Boolean tests at its outgoing arcs can be simultaneously satisfied. The classification of an observation is obtained by the bottom-most node\u2019s prediction reachable from the root. ", "page_idx": 3}, {"type": "table", "img_path": "n60xBFZWrk/tmp/252dea82c52f04f7f102db5b463131fa9a7962466b9b6e6d0a3220cf555cd9cc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "We introduce an algorithm, GETMDT, which takes as input a DT $H$ and outputs an MDT $H^{\\prime}$ , which approximates $H$ via the following key property: ", "page_idx": 3}, {"type": "text", "text": "(M) For any observation $\\pmb{x}\\in\\mathcal{X}$ , the prediction $H^{\\prime}(x)$ is equal to the prediction in the path followed by $\\textbf{\\em x}$ in $H$ of its deepest node in the strictly monotonic subsequence starting from the root of $H$ . Figure 1 (right) presents an example of MDT $H^{\\prime}$ that would be built for some DT $H$ (left) and satisfying $\\mathbf{\\delta}(\\mathbf{M})$ (unless observations have missing values, $H^{\\prime}$ is unique). Figure 1 adopts some additional conventions to ease parsing of $H^{\\prime}$ , $({\\bf D1})$ and $\\left(\\mathbf{D}2\\right)$ : ", "page_idx": 3}, {"type": "text", "text": "$({\\bf D1})$ Some internal nodes of $H^{\\prime}$ are also tagged with labels corresponding to the leaves of $H$ . If a node in $H^{\\prime}$ is tagged with a label of one leaf $\\lambda$ of $H$ , it indicates that examples reaching $\\lambda$ in the original $H$ are being classified by $H^{\\prime}$ \u2019s tagged node; ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{(D2)}$ Arcs in $H^{\\prime}$ have a width proportional to the number of boolean tests it takes to reach its tail from its head in $H$ . A large width thus indicates a long path in $H$ to improve classification confidence. To produce the MDT $H^{\\prime}$ from DT $H$ with Algorithm 1, after having initialized it to a root $=$ single leaf, we just run ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{GETMDT}(\\mathrm{root}(H),\\mathbf{true},\\mathrm{root}(H^{\\prime}),\\underline{{[p_{\\mathrm{root}}^{+}}},\\overline{{p}}_{\\mathrm{root}}^{+}])\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(we let $p_{\\mathrm{root}}^{+}\\doteq\\operatorname*{min}\\{p_{\\mathrm{root}}^{+},1-p_{\\mathrm{root}}^{+}\\}$ , $\\overline{{p}}_{\\mathrm{root}}^{+}\\doteq\\operatorname*{max}\\{p_{\\mathrm{root}}^{+},1-p_{\\mathrm{root}}^{+}\\})$ . Upon finishing, the tree rooted at $\\mathrm{{root}}(H^{\\prime})$ is the MDT sought. ", "page_idx": 4}, {"type": "text", "text": "We complete the description of GETMDT: When a leaf of $H$ does not have sufficient confidence and ends up being mapped to an internal node of the MDT $H^{\\prime}$ , TAGDTLEAF is the procedure that tags this internal node with information from the leaf (see $({\\bf D1})$ above). We consider a tag being just the name of the leaf (Figure 1, leaves $\\#5,9,11)$ , but other conventions can be adopted. The other two methods we use grow MDT $H^{\\prime}$ by creating a new node via NEWNODE and adding a new arc between existing nodes via NEWARC. We easily check the following. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. $H^{\\prime}$ built by GETMDT satisfies (M) with respect to DT $H$ . ", "page_idx": 4}, {"type": "text", "text": "Property (M) is crucial to keep the predictions of the DT and the MDT close to each other: to each node of the MDT indeed corresponds a node in the DT with the same posterior prediction. Because the set of nodes of $H^{\\prime}$ corresponds to a subset of nodes in $H$ , predictions can locally change for some observations. This phenomenon is limited by two factors: (i) the set of leaves of the MDT corresponds to a set of leaves in the DT (thus, predictions do not change for the related observations), (ii) more often than not in our experiments, it is only the confidence that changes, not the sign of the prediction, which means that accuracies of the DT and its corresponding MDT are close to each other (Section 6). It is also worth remarking that the MDT $H^{\\prime}$ always has the same depth as the DT $H$ . Finally, any pruning of $H$ is a subtree of $H$ and its corresponding MDT is a subtree of the MDT of $H$ . The aforementioned troubles to embed the DT in the Poincar\u00e9 disk considering (A-C) do not exist anymore for the MDT because the best embeddings necessarily have all arcs going outwards in the Poincar\u00e9 disk. The algorithm we use to embed the MDT in Poincar\u00e9 disk is a simple alteration of Sarkar\u2019s embedding [30] which, because of the lack of space, we defer to the Appendix (Section C.VIII). ", "page_idx": 4}, {"type": "text", "text": "The quality of the embedding is obtained using an embedding error that takes into account all nodes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho(H^{\\prime})\\quad\\doteq\\quad100\\cdot\\mathbb{E}_{\\boldsymbol{\\nu}\\sim\\mathcal{N}(H^{\\prime})}\\left[\\frac{||\\psi_{\\boldsymbol{\\nu}}|-d_{\\mathbb{B}}(\\mathbf{0},z_{\\nu})|}{|\\psi_{\\boldsymbol{\\nu}}|}\\right]\\quad(\\mathcal{K}_{0}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\psi_{\\nu}$ refers to the relevant $\\psi_{\\mathrm{LoG}}(p_{\\nu}^{+})$ in (3) (and $z_{\\nu}\\in\\mathbb{B}$ is its embedding in $\\mathbb{B}$ ). Two example final representations are in Figure 2. Notice the small errors $\\rho$ in both cases. ", "page_idx": 4}, {"type": "text", "text": "Remark 1. Using a particular boosting algorithm to boost the logistic loss, we can as well represent the leveraging coefficients of a DT/MDT in a boosted combination, following a convention sketched in Figure 2 and explained at length, along with the boosting algorithm and its properties, in the Appendix (Section $C.I X,$ . ", "page_idx": 4}, {"type": "text", "text": "5 Smoothly altering integrals: T-calculus and the t-self of Poincar\u00e9 disk ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "It is apparent from Figure 2 (right, lowest red pentagon), that when utilizing standard hyperbolic distances $d_{\\mathbb{B}}(z,\\mathbf{0})$ (4), the best MDT nodes are embedded close to the border. In addition to obviously not being great for visualization, these nodes are at risk of having numeric error \u201cpush\u201d the nodes to the border $\\partial\\mathbb{B}$ , thereby giving a false depiction of infinite confidence. In this section, we provide alternative distances to prevent this phenomena. First, we quantify the numerical risk, which has been defined by the critical region where high numeric error can occur [19, 29]. ", "page_idx": 4}, {"type": "text", "text": "Definition 5.1. A point $\\pmb{x}\\in\\mathbb{B}$ is said $k$ -close to boundary $\\partial\\mathbb{B}\\,i f\\,\\|\\pmb{x}\\|=1-10^{-k}$ . It is said encodable iff $\\|\\pmb{x}\\|<1$ in machine encoding (it is not \u201cpushed to the boundary\u201d). ", "page_idx": 4}, {"type": "text", "text": "Machine encoding constrains the maximum possible $k$ : in the double-precision floating-point representation (Float64), $k\\approx16$ [19]. In the case of Poincar\u00e9 disk, the maximal distance $d_{*}$ from the origin 0 that this authorizes (before numerical error \u201cwarps\u201d a point to the border) is a small affine order in $k$ [19]: ", "page_idx": 4}, {"type": "image", "img_path": "n60xBFZWrk/tmp/eae9cbf009b9f78aabe6704a6302e66ba02c25cee3d17749cf584fb3618803c1.jpg", "img_caption": ["Figure 2: Two example MDTs learned on UCI domain online_shoppers_intention, showing embedding error $\\rho$ (5). Left: key parts of the embedding. Right: annotations used in Experiments, Section 6. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d_{*}}&{{}\\leqslant}&{\\log(2)+\\log(10)\\cdot k+O(10^{-k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Hence, in practice, only a ball of radius $d_{*}\\approx38$ around the origin can be accurately represented. This is in deep contrast with Euclidean representation, where $d_{*}\\doteq\\Omega(2^{k})$ . We now provide a principled solution to changing $d_{*}$ while keeping hyperbolicity, which relies on a crucial generalization of Leibniz-Newton\u2019s fundamental Theorem of calculus. ", "page_idx": 5}, {"type": "text", "text": "5.1 T-calculus ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We let $[n]\\doteq\\{1,2,...,n\\}$ for $n\\in\\mathbb{N}_{>0}$ . At the core of our generalization is the replacement of the addition by the tempered (or t-)addition [22], $z\\oplus_{t}z^{\\prime}\\doteq z+z^{\\prime}+(1-t)z z^{\\prime}$ , in the notion of Riemann sum. The additional term is a scaled saddle point curve $z z^{\\prime}$ , positive when the sign of $z$ and $z^{\\prime}$ agree and negative otherwise. Hereafter, $f$ is defined on an interval $[a,b]$ . We define a generalization of Riemann integration to t-algebra (see [2] for a preliminary development which does not provide the generalization of Leibniz-Newton\u2019s fundamental Theorem of calculus) and for this objective, given an interval $[a,b]$ and a division $\\Delta$ of this interval using $n\\!+\\!1$ reals $x_{0}\\doteq a<x_{1}<\\ldots<x_{n-1}<x_{n}\\doteq b,$ we define the Riemann $t$ -sum of $f$ over $[a,b]$ using $\\Delta$ , for a set of tags $\\Xi\\doteq\\{\\xi_{i}\\in[x_{i-1},x_{i}]\\}_{i\\in[n]}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{S_{\\Delta}^{(t)}(f)}&{{}\\doteq}&{(\\bigoplus_{t})_{i\\in[n]}(x_{i}-x_{i-1})\\cdot f(\\xi_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$(S_{\\Delta}^{(1)}(f)$ is the classical Riemann summation). Let $s(\\Delta)\\,\\doteq\\,\\operatorname*{max}_{i}x_{i}\\,-\\,x_{i-1}$ denote the step of division $\\Delta$ . The conditions for -Riemann integration are the same as for $t=1$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 5.2. Fix $t\\in\\mathbb{R},$ . A function $f$ is $t$ -(Riemann) integrable over $[a,b]$ iff $\\exists L\\in\\mathbb{R}$ such that $\\forall\\varepsilon>0,\\exists\\delta>0,\\backslash$ @ division $\\Delta$ with $s(\\Delta)<\\delta$ and associated set of tags \u039e, $\\Big|S_{\\Delta}^{(t)}(f)-L\\Big|<\\varepsilon$ . When this happens, we note ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\int_{a}^{b}f(x)\\mathrm{d}_{t}x}=L.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The case $t=1$ , Riemann integration, is denoted using classical notations. We now prove our first main results for this Section, namely the link between $t$ -Riemann integration and Riemann integration. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Any function is either $t$ -Riemann integrable for all $t\\in\\mathbb{R}$ simultaneously, or for none. In the former case, we have the relationship: $(\\log_{t}\\dot{=}(z^{1-\\dot{t}}-1)/(1-t)$ for $t\\neq1$ and $\\mathrm{log}_{1}\\doteq\\mathrm{log})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\int_{a}^{b}f(u)\\mathrm{d}_{t}u}&{=}&{\\displaystyle\\mathfrak{g}_{t}\\left(\\int_{a}^{b}f(u)\\mathrm{d}u\\right),\\forall t\\in\\mathbb{R},\\quad\\,w i t h\\;\\mathfrak{g}_{t}(z)\\doteq\\log_{t}\\exp{z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For example, if $t=0$ , we get $1+{\\sqrt[[object Object]]{_{a}}}\\,f(u)\\mathrm{d}_{t}u=\\exp\\int_{a}^{b}f(u)\\mathrm{d}u$ , which is Volterra\u2019s integral [33, Theorem 5.5.11]. Theorem 2 is impor\u015ftant because it allo\u015fws to compute any $t$ -Riemann integral from the classical ${\\mathit{\\Omega}}(t\\,=\\,1)$ ) case. Classical Riemann integration and derivation are fundamental inverse operations. The classical derivation is sometimes called \u201cEuclidean derivative\u201d [4]. We now elicit the notion of derivative, which is the \u201cinverse\u201d of $t$ -Riemann integration. Unsurprisingly, it generalizes Euclidean derivative. The Theorem stands as a generalization of the classical fundamental Theorem of calculus. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Let $\\begin{array}{r}{z\\ominus_{t}z^{\\prime}\\doteq\\frac{z-z^{\\prime}}{1+(1-t)z^{\\prime}}}\\end{array}$ denote the tempered subtraction. When it exists, define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\mathrm{D}_{t}f(z)}&{\\doteq}&{\\displaystyle\\operatorname*{lim}_{\\delta\\to0}\\frac{f(z+\\delta)\\ominus_{t}f(z)}{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Suppose $f\\,t$ -Riemann integrable. Then, the function $F$ defined by $F(z)\\doteq{\\sqrt{\\sqrt[[object Object]]{a}}}\\,f(u)\\mathrm{d}_{t}u$ is such that $\\mathrm{D}_{t}F=f$ . We call $F$ a t-primitive of $f$ (which zeroes when $z=a_{.}$ ) and $\\mathrm{D}_{t}F$ \u015fthe $t$ -derivative of $F$ . ", "page_idx": 6}, {"type": "text", "text": "The function ${\\mathfrak{g}}_{t}$ (Figure 3) is key to many of our results; it has quite remarkable properties: in particular, it is strictly increasing for any $t\\in\\mathbb R$ , strictly concave for any $t>1$ , strictly convex for any $t<1$ and such that $\\mathrm{sign}(\\mathfrak{g}_{t}(z))\\,=\\,\\mathrm{\\ddot{s}i g n}(z),\\forall t\\in\\mathbb{R}$ . One can also note that $\\operatorname{D}_{t}\\mathfrak{g}_{t}(\\dot{z})=1$ . The Appendix proves these results (Sections C.I and C.II). ", "page_idx": 6}, {"type": "text", "text": "Remark 2. The Appendix also proves many more results showing how Theorems 2 and $^3$ , and properties of t-calculus, naturally \u201cpercolate\" through many properties known for classical integration and the fundamental theorem of calculus (among others, additivity, Chasles\u2019 relationship, monotonicity, the hyperbolic Pythagorean theorem, mean value theorem, chain rule, etc.). ", "page_idx": 6}, {"type": "text", "text": "5.2 The t-self of Poincar\u00e9 disk ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let us now put T-calculus to good use in our context. For a set $\\mathcal{X}$ endowed with function $d:\\mathcal{X}\\times\\mathcal{X}\\to$ $[0,+\\infty]$ (e.g. Poincar\u00e9 disk and the Poincar\u00e9 hyperbolic distance), the t-self of $\\mathcal{X}$ is the set (implicitly) endowed with $d^{(t)}\\doteq\\mathfrak{g}_{t}\\circ d$ . Before going further, let us provide the elegant $d^{(t)}$ associated to Poincar\u00e9 disk\u2019s $t$ -self (with $r\\doteq\\|z\\|$ as in (4)): ", "page_idx": 6}, {"type": "equation", "text": "$$\nd_{\\mathbb{B}^{(t)}}(\\mathbf{0},z)=\\log_{t}\\exp d_{\\mathbb{B}}(\\mathbf{0},z)=\\log_{t}\\left({\\frac{1+r}{1-r}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following Lemma (shown in Appendix) demonstrates the usefulness of T-calculus to address the encoding problem in $\\mathbb{B}$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 1. In Poincar\u00e9 disk model, pick any increasing function $g(k)\\,\\geqslant\\,0$ . For the choice $t\\,=$ $1-f(k)$ where $f(k)\\in\\mathbb{R}$ is any function satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\log{(1+f(k)g(k))}}{f(k)}\\;\\;\\;\\leqslant\\;\\;\\log(10)\\cdot k,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we get in lieu of (6) the maximal $d_{*}^{(t)}\\ o f\\,d^{(t)}$ satisfying $d_{*}^{(t)}\\geq g(k)$ , and the new hyperbolic constant $\\tau_{t}$ of the $t$ -self satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\tau_{t}}&{=}&{\\displaystyle\\frac{\\exp\\left(f(k)\\tau\\right)-1}{f(k)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since both $\\log(1+x)\\;=\\;x\\,+\\,o(x)$ and $\\exp(x)\\,-\\,1\\ =\\ x\\,+\\,o(x)$ in a neighborhood of 0, we check that (11) and (12) get back to properties of the Poincar\u00e9 model as $f(k){\\bar{\\to}}\\,0$ [19]. We then have two cases: we can easily approach back the Euclidean $d_{*}$ by picking $f(k)\\,>\\,0$ : choosing $f(k)=1$ gets there and we still keep a finite hyperbolic constant, albeit exponential in the former one. If however we want to improve further the hyperbolic constant, we can pick some admissible $f(k)<0$ , but then (11) will constrain $g(k)$ to a very small value. Note that depending on the sign of $f(k)$ , the triangle inequality can still hold or be replaced by a weaker version since we can show $d_{\\mathbb{B}^{\\left(t\\right)}}\\!\\left(\\mathbf{x},z\\right)\\leqslant d_{\\mathbb{B}^{\\left(t\\right)}}\\!\\left(\\mathbf{x},y\\right)+d_{\\mathbb{B}^{\\left(t\\right)}}\\!\\left(y,z\\right)+\\operatorname*{max}\\{0,f(k)\\}\\cdot d_{\\mathbb{B}^{\\left(t\\right)}}\\!\\left(\\mathbf{x},y\\right)\\cdot d_{\\mathbb{B}^{\\left(t\\right)}}\\!\\left(y,z\\right)$ in $\\mathbb{B}^{(t)}$ . ", "page_idx": 6}, {"type": "text", "text": "Let us finally investigate how we can perform a general embedding in the $t$ -self $\\mathbb{B}^{(t)}$ of the Poincar\u00e9 disk to be (i) more encoding-savvy and (ii) have points originally close to $\\partial\\mathbb{B}$ substantially further away from the $t$ -self $\\mathbb{B}^{(t)}$ border. We also add a third constraint (iii) that the distortions must be fair $w.r t$ . the original the Poincar\u00e9 disk model embedding; i.e. if some $z\\in\\mathbb{B}$ (e.g. the coordinate of a DT node) gets mapped to $\\pmb{z}^{(t)}\\in\\mathbb{B}^{(t)}$ (the t-self), then we request ", "page_idx": 6}, {"type": "equation", "text": "$$\nd_{\\mathbb{B}^{(t)}}(\\boldsymbol{z}^{(t)},\\mathbf{0})=d_{\\mathbb{B}}(\\boldsymbol{z},\\mathbf{0}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "n60xBFZWrk/tmp/26884bd5dabbae681c97dca741ca590657e5f45f0721f5eb121f18196e2adc72.jpg", "img_caption": ["Figure 3: Left: plot of ${\\mathfrak{g}}_{t}(z)$ (9) for different values of $t$ (color map on the right bar), showing where it is convex / concave. The $t=1$ case $({\\mathfrak{g}}_{1}(z)=z)$ is emphasized in red. Right: suppose $r\\doteq\\|z\\|$ is the Euclidean norm of a point $_{z}$ in Poincar\u00e9 disk $\\mathbb{B}$ . Fix $\\bar{t}\\in[0,1]$ (color map on the right bar). The plot gives the (Euclidean) norm $r^{(t)}$ of a point $\\boldsymbol{z}^{(t)}$ in the t-self such that $d_{\\mathbb{B}^{(t)}}(\\pmb{z}^{(t)},\\mathbf{0})=d_{\\mathbb{B}}(\\pmb{z},\\mathbf{0})$ (13). Remark that even for $r$ very close to 1 we can have $r^{(t)}$ substantially smaller (e.g. $r^{(t)}<0.8)$ . "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "n60xBFZWrk/tmp/aa99c80411ae9466a8bde93664be38928b3d3f5ec093b61bcfac7ab1c6eaea0a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Left pane: embedding in Poincar\u00e9 disk of the MDTs corresponding to the DTs learned at the $1^{s t}$ (top row) and $10^{t h}$ (bottom row) boosting iteration, on four UCI domains. Stark differences emerge between these domains from the plots alone. Right pane: comparison between Poincar\u00e9 disk embedding and its $t=0$ t-self for an MDT learned on UCI online_shoppers_intention (top, boosting coefficients information not shown) and UCI hardware (bottom). The $p\\in\\{0.001,0.99\\bar{9}\\}$ isoline (rectangle) is barely distinguishable from $\\partial\\mathbb{B}$ but is clearly distinct from $\\hat{\\sigma}\\mathbb{B}^{(0)}$ . Note that in $\\mathbb{B}^{(0)}$ , the center looks similar to a scaling (zoom) of $\\mathbb{B}$ ; while near the border, the high nonlinearity of $\\mathbb{B}^{(0)}$ allows us to spot nodes that have high confidence / training accuracy (in orange) but can hardly be distinguished from the bulk of \u201cjust good\u201d nodes in $\\mathbb{B}$ . Note also the set of red nodes in the Poincar\u00e9 disk for $j=70$ (rectangle) that mistakenly look aligned, but not in the t-self. ", "page_idx": 7}, {"type": "text", "text": "All three conditions point to a non-linear embedding $\\mathfrak{p}_{t}:\\mathbb{B}\\,\\rightarrow\\,\\mathbb{B}^{(t)}$ . The $t$ -self offers a simple convenient solution with a trivial design for ${\\mathfrak{p}}_{t}$ : We compute $\\boldsymbol{z}^{(t)}$ by a simple scaling of the Euclidean norm of $_{z}$ in $\\mathbb{B}$ to ensure that (13) holds. Figure 3 (right) displays the corresponding relationship between $r\\doteq\\|z\\|$ and $r^{(t)}\\doteq\\lVert z^{(t)}\\rVert$ when $t\\in[0,1]$ , clearly achieving (ii) in addition to (iii). For (i), even for $t<1$ very close to 1 and $_{z}$ close to $\\partial\\mathbb{B}$ ( $\\textsl{r}$ close to 1), the mapping can send $\\boldsymbol{z}^{(t)}$ substantially \u201cback in\u201d the t-self $\\mathbb{B}^{(t)}$ : for $t\\,=\\,0.7$ and $r\\,=\\,1-10^{-4}\\,-\\,i.e$ . $k\\,=\\,4$ in Definition 5.1 \u2013 we get $r^{(t)}\\,\\approx\\,0.96$ , authorizing encoding with a less \u201crisky\u201d $k\\,=\\,2$ . Two additional benefits from (13), visible from Figure 3 (right) is that the distortion is low near the center and then merely affine in a wide region until near the heavy non-linear changes, typically when $r>0.8$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 3. The Appendix details properties of the t-self of the Lorentz model of hyperbolic geometry, another popular model (Section C.V). ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We summarize a number of experiments (Table 1), provided otherwise in extenso in the Appendix. In the top-down induction scheme for DT, the leaf chosen to be split is the heaviest non pure leaf, i.e. the one among the leaves containing both classes with the largest proportion of training examples. Induction is stopped when the tree reaches a fixed size, or when all leaves are pure, or when no further split allows decreasing the expected log-loss. We do not prune DTs. Our domains are public ( $C f$ Appendix). ", "page_idx": 8}, {"type": "text", "text": "Poincar\u00e9 embeddings of DTs / MDTs See Figure 2 (left) for a summary of the visualization. We remind that the center of the disk is \u201cpoor\u201d classification (confidence 0), or, in the context of boosting, random classification. A striking observation is the sheer variety of patterns that are plainly obvious from our visualization but would otherwise be difficult to spot using classical tools. Some are common to all domains: as boosting iterations increase, low-depth tree nodes tend to converge to the center of the disk, indicating increased hardness in classification. This is the impact of a well known effect of boosting, whose weight modifications make the problem \"harder\". Among domain-dependent patterns, highly imbalanced domains get a root predictably initially embedded \u201cfar\u201d from the origin (online_shoppers_intention, analcatdata_supreme) while more balanced domains get their roots embedded near the origin (abalone). Across the experiment, all trees have at most 200 nodes: while this clearly provides very good models after a large number of iterations on some domains, it is not enough to get good models after just a few iterations on others (analcatdata_supreme). Interpreting the hyperbolic embeddings can also be telling: consider Figure 2 (right), which a MDT of the $(j=3)$ boosted DT learned on online_shoppers_intention. Notice the low embedding error $(5.46\\%)$ . We see in (A) (magenta) that classes are more balanced at the root compared to previous trees (All DTs in Appendix, Table IV) because the root is closer to the center of the disk: thus, hard examples belong to both classes (no class becomes much harder to \u201clearn\u201d than the other). Two clearly distinct subtrees are associated to non-purchase patterns (red, A). The bottom-most subtree achieves very good confidence with several leaves close to the border: these are non-purchase patterns clearly distinct from purchase patterns (green nodes); the whole subtree dangles from the root via a test on feature PageValues (grey, B) achieving a substantial boost in confidence; many nodes are way past posterior isoline $p\\in\\{0.1,0.9\\}$ . Red subtree in (A) is a lot closer to purchase patterns (C). One distinct pattern of purchase emerges, built from tests that always strictly increase its prediction\u2019s confidence (orange). The full rule achieves very high accuracy (leaf posterior nears 0.99). ", "page_idx": 8}, {"type": "text", "text": "Interest of the t-self for visualization As demonstrated by Table 1 (right) and Appendix, the t-self is particularly useful to tell the best parts of a tree. Because it also manages to \u201cpush back\u201d the bulk of nodes from the border $\\hat{\\sigma}\\mathbb{B}^{(t)}$ , it displays the t-self might also be useful as a standard encoding (use (13) to access Poincar\u00e9 disk quantities and embedding). ", "page_idx": 8}, {"type": "text", "text": "MDT vs DT for classification We traded the hyperbolic visualization of a DT \u2013 bound to be poor \u2013 for the visualization of its \u201cmonotonic part\u201d, an MDT. We have seen that the MDT visualization provides lots of clues about the DT as well. In terms of classification, the fact that the set of leaves of the MDT is a subset of the set of leaves of its DT hints on similarities in classification as well, but so far we have not explored a key question whose scope goes beyond this paper\u2019s: how does the classification of a MDT compares to its DT ? This question is important because in addition to being more amenable to visualization than DTs, it might also be the case that classification with MDTs might just be a good alternative to doing it with DTs. The scope of this question goes beyond this paper because of the tremendous popularity of DTs. The left pane of Table 2 provides a partial answer on a subset of our domains (the full set of results is in the Appendix, Section D.II; p-values are that of a Student paired t-test with H0 being the identity of the average errors). Clearly, the classification accuracies are similar for a majority of the domains considered, but interestingly, we observe the two polarities when they are not: on domains ionosphere and online_shoppers_intention, the DT is better than its MDT, but on domain analcatdata_supreme it is the opposite. This is particularly interesting because the MDT is no larger and can be substantially smaller than its corresponding DT and thus could represent an efficient pruning of its DT. ", "page_idx": 8}, {"type": "text", "text": "MDT embedding: visualization error The right pane of Table 2 provides an example of embedding errors $\\rho$ (5) for the first five MDTs in a boosted ensemble (we remind that the description of the boosting algorithm and the embedding of the leveraging coefficients is provided in Appendix, Section C.IX). The full set of results is in Appendix, Section D.II. The results show that errors are small in general, which is good news given our quite heuristic modification to Sarkar\u2019s embedding. There is more: the embedding error tends to decrease \u2013 in some cases very significantly \u2013 with the MDT\u2019s index in the ensemble, see for example winered, german and qsar. The explanation is simple and due to a key boosting feature, the reweighting mechanism underlined above: with weight updates that bring in general the total (new) weight of positive examples closer to that of negative examples, the root of the next MDT gets embedded closer to the center of the disk. As the root moves closer to the center, it is much easier to get a good spread of the tree in the rest of the disk at reduced error, in particular if the MDT is well balanced. For an example, see hardware for $j\\,=\\,70$ in Figure 1 $j<1\\%$ ). ", "page_idx": 8}, {"type": "table", "img_path": "n60xBFZWrk/tmp/d86f4cde25505a2d59bae8990f3401f200ccaa96d6dbb0b30442c09e2263b180.jpg", "table_caption": [], "table_footnote": ["Table 2: Summary of two experiments for a subset of our domains. Left pane: test errors $\\overline{{\\%}})$ of DTs vs their corresponding MDTs (average $\\pm$ std dev). Bold faces on ${\\bf p}$ -values indicate keeping the identity of averages for a 0.05 first-order risk. Right pane: embedding error $\\rho$ (5) $(\\%)$ for the visualization of the first five MDTs in an ensemble learned. Shorthands used: o._s._i. $=$ online_shoppers_intention; a._s. $=$ analcatdata_supreme. See text for details. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes three separate contributions that altogether provide a solution for hyperbolic embedding of (ensembles of) decision trees (DT), via (i) a link between losses for class probability estimation and hyperbolic distances, (ii) the design of a clean, unambiguous embedding of a DT via its monotonic subtree and (iii) a way to improve visualization and encoding properties of the Poincar\u00e9 disk model using its t-self, a notion we introduce. Each of these contributions are indeed separate: for example, one could reuse (i) to embed models different from DTs or (iii) to perform hyperbolic embeddings of objects being not even related to supervised models. Contribution (ii) is of independent interest with the introduction of a new kind of tree-based models, monotonic decision trees. ", "page_idx": 9}, {"type": "text", "text": "We believe that the way we address (iii) opens general applications even outside hyperbolic geometry. Indeed, we generalize classical integration and derivation to a context encompassing the concept of additivity, upon which integration is built, extending standard properties of integration and derivation in a natural way (see the Appendix for many examples). That such properties can be obtained without additional technical \u201ceffort\u201d bodes well for perspectives of developments and applications in other subfields of ML, where such tools could be used, not just in our geometric context, to smoothly tweak distortion measures. Many distortion measures used in ML are indeed integrals in nature: Bregman divergences, $f$ -divergences, integral probability metrics, etc. ", "page_idx": 9}, {"type": "text", "text": "One inherent limitation of our work is linked to the use of DTs: they typically fti very well to tabular variables (attribute-value data) but otherwise should be used with caution. Our work is a first step towards more sophisticated visualization for supervised models (Section 2), looking for tailored fit geometric spaces to best blend their symbolic and numerical properties. More can be done and more has to be done: at an age of data collection and ML compute still ramping up, seeking model \"pictures\" worth a thousand words is a challenge but a necessity for responsible AI and explainability. Finally, our introduction of monotonic decision trees begs for a deeper statistical analysis of such models, given the tremendous popularity of decision trees that they could complete \u2013 or replace \u2013 on mainstream data science pipelines. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Thanks are due to Mathieu Guillame-Bert, reviewers and the area chair for many suggestions that helped to improve the paper\u2019s content. Work was done while AS was visiting RIKEN AIP. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] E. Amid, F. Nielsen, R. Nock, and M.-K. Warmuth. Optimal transport with tempered exponential measures. In AAAI\u201924, 2024.   \n[2] Ehsan Amid, Frank Nielsen, Richard Nock, and Manfred K Warmuth. The tempered Hilbert simplex distance and its application to non-linear embeddings of TEMs. arXiv preprint arXiv:2311.13459, 2023.   \n[3] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with Bregman divergences. In Proc. of the $4^{t h}$ SIAM International Conference on Data Mining, pages 234\u2013245, 2004. [4] A.-F. Beardon and D. Minda. The hyperbolic metric and geometric function theory. In Proc. of the International Workshop on Quasiconformal Mappings and their Applications, 2005.   \n[5] B.-H. Bowditch. A course on geometric group theory. Math. Soc. Japan Memoirs, 2006.   \n[6] I. Chami, A. Gu, V. Chatziafratis, and C. R\u00e9. From trees to continuous embeddings and back: Hyperbolic hierarchical clustering. In NeurIPS\\*33, 2020.   \n[7] I. Chami, A. Gu, D. Nguyen, and C. R\u00e9. Horopca: Hyperbolic dimensionality reduction via horospherical projections. In $38^{t h}$ ICML, volume 139 of Proceedings of Machine Learning Research, pages 1419\u20131429. PMLR, 2021.   \n[8] P. Chlenski, E. Turok, A. Khalil Moretti, and I. Pe\u2019er. Fast hyperboloid decision tree algorithms. In $I2^{t h}$ ICLR, 2024.   \n[9] H. Cho, B. Demeo, J. Peng, and B. Berger. Large-margin classification in hyperbolic space. In $22^{n d}$ AISTATS, volume 89 of Proceedings of Machine Learning Research, pages 1832\u20131840. PMLR, 2019.   \n[10] Z. Cranko and R. Nock. Boosted density estimation remastered. In $36^{t h}$ ICML, pages 1416\u2013 1425, 2019.   \n[11] L. Doorenbos, P. M\u00e1rquez-Neila, R. Sznitman, and P. Mettes. Hyperbolic random forests. CoRR, abs/2308.13279, 2023.   \n[12] D. Dua and C. Graff. UCI machine learning repository, 2021.   \n[13] J. Friedman, T. Hastie, and R. Tibshirani. Additive Logistic Regression $:$ a Statistical View of Boosting. Ann. of Stat., 28:337\u2013374, 2000.   \n[14] O.-E. Ganea, G. B\u00e9cigneul, and T. Hofmann. Hyperbolic entailment cones for learning hierarchical embeddings. In $35^{t h}$ ICML, volume 80 of Proceedings of Machine Learning Research, pages 1632\u20131641. PMLR, 2018.   \n[15] O.-E. Ganea, G. B\u00e9cigneul, and T. Hofmann. Hyperbolic neural networks. In NeurIPS\\*31, pages 5350\u20135360, 2018.   \n[16] D.-E. Knuth. Two notes on notation. The American Mathematical Monthly, 99(5):403\u2013422, 1992.   \n[17] M.-T. Law, R. Liao, J. Snell, and R.-S. Zemel. Lorentzian distance learning for hyperbolic representations. In $36^{t h}$ ICML, volume 97 of Proceedings of Machine Learning Research, pages 3672\u20133681. PMLR, 2019.   \n[18] Y. Mansour, R. Nock, and R.-C. Williamson. Random classification noise does not defeat all convex potential boosters irrespective of model choice. In $40^{t h}$ ICML, 2023.   \n[19] G. Mishne, Z. Wan, Y. Wang, and S. Yang. The numerical stability of hyperbolic representation learning. In $40^{t h}$ ICML, 2023.   \n[20] M. Nickel and D. Kiela. Poincar\u00e9 embeddings for learning hierarchical representations. In NeurIPS\\*30, pages 6338\u20136347, 2017.   \n[21] F. Nielsen and R. Nock. On R\u00e9nyi and Tsallis entropies and divergences for exponential families. CoRR, abs/1105.3259, 2011.   \n[22] L. Nivanen, A. Le M\u00e9haut\u00e9, and Q.-A. Wang. Generalized algebra within a nonextensive statistics. Reports on Mathematical Physics, 52:437\u2013444, 2003.   \n[23] R. Nock, W. Bel Haj Ali, R. D\u2019Ambrosio, F. Nielsen, and M. Barlaud. Gentle nearest neighbors boosting over proper scoring rules. IEEE Trans.PAMI, 37(1):80\u201393, 2015.   \n[24] R. Nock and A. K. Menon. Supervised learning: No loss no cry. In $37^{t h}$ ICML, 2020.   \n[25] M.-C. Pardo and I. Vajda. About distances of discrete distributions satisfying the data processing Theorem of Information Theory. IEEE Trans. IT, 43:1288\u20131293, 1997.   \n[26] J. R. Quinlan. C4.5 : programs for machine learning. Morgan Kaufmann, 1993.   \n[27] J.-G. Ratcliffe. Foundations of Hyperbolic Manifolds. Springer Graduate Texts in Mathematics, 1994.   \n[28] M.-D. Reid and R.-C. Williamson. Information, divergence and risk for binary experiments. JMLR, 12:731\u2013817, 2011.   \n[29] F. Sala, C. De Sa, A. Gu, and C.R\u00e9. Representation tradeoffs for hyperbolic embeddings. In $35^{t h}$ ICML, volume 80 of Proceedings of Machine Learning Research, pages 4457\u20134466. PMLR, 2018.   \n[30] R. Sarkar. Low distortion Delaunay embedding of trees in hyperbolic plane. In GD\u201911, volume 7034, pages 355\u2013366. Springer, 2011.   \n[31] R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. MLJ, 37:297\u2013336, 1999.   \n[32] Andrew D Selbst and Solon Barocas. The intuitive appeal of explainable machines. Fordham L. Rev., 87:1085, 2018.   \n[33] A. Slavik. Product integration, its history and applications. Matfyzpress, Praze, 2007.   \n[34] R. Sonthalia and A.-C. Gilbert. Tree! I am no tree! I am a low dimensional hyperbolic embedding. In NeurIPS\\*33, 2020.   \n[35] T. van Erven and P. Harremo\u00ebs. R\u00e9nyi divergence and kullback-leibler divergence. IEEE Trans. IT, 60:3797\u20133820, 2014.   \n[36] M. Yang, M. Zhou, R. Ying, Y. Chen, and I. King. Hyperbolic representation learning: Revisiting and advancing. In $4O^{t h}$ ICML, volume 202 of Proceedings of Machine Learning Research, pages 39639\u201339659, 2023.   \n[37] T. Yu and C. De Sa. Numerically accurate hyperbolic embeddings using tiling-based models. In NeurIPS\\*32, pages 2021\u20132031, 2019.   \n[38] T. Yu, T.-J.-B. Liu, A. Tseng, and C. De Sa. Shadow cones: Unveiling partial orders in hyperbolic space. CoRR, abs/2305.15215, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To differentiate with the numberings in the main flie, the numbering of Theorems, etc. is letter-based (A, B, ...). ", "page_idx": 12}, {"type": "text", "text": "Table of contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Broader Impact Pg 14 ", "page_idx": 12}, {"type": "text", "text": "$t$ -algebra and $t$ -additivity Pg 14 ", "page_idx": 12}, {"type": "text", "text": "Supplementary material on proofs Pg 15 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "$\\hookrightarrow$ Proof of Theorem 2 Pg 15   \n$\\hookrightarrow$ Proof of Theorem 3 Pg 17   \n$\\hookrightarrow$ Additional and helper results for Theorems 2 and 3 Pg 17   \n$\\hookrightarrow$ Sets endowed with a distortion and their $t$ -self: statistical information Pg 20   \n$\\hookrightarrow$ The t-self of the Lorentz model of hyperbolic geometry Pg 21   \n$\\hookrightarrow$ Proof of Lemma 1 Pg 22   \n$\\hookrightarrow$ Proof of Theorem 1 Pg 22   \n$\\hookrightarrow$ Modifying Sarkar\u2019s embedding in Poincar\u00e9 disk Pg 23   \n$\\hookrightarrow$ Boosting with the logistic loss $\\grave{a}$ -la AdaBoost Pg 24 ", "page_idx": 12}, {"type": "text", "text": "Supplementary material on experiments Pg 25 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "$\\hookrightarrow$ Domains Pg 25   \n$\\hookrightarrow$ Visualizing a DT via its MDT and embedding errors Pg 25   \n$\\hookrightarrow$ All Poincar\u00e9 disk embeddings Pg 27   \n$\\hookrightarrow$ Interest of the t-self for visualization Pg 27 ", "page_idx": 12}, {"type": "text", "text": "A Broader Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. Beyond our proposed tempered calculus which is used to amend the original Poincar\u00e9 disk model of hyperbolic geometry, the monotonic decision trees and $t$ -self hyperbolic embedding contribute to the field of visualization and explainable AI for ML models. Thus, there are two distinct avenues for broader impact and related care: ", "page_idx": 13}, {"type": "text", "text": "1. from the geometry standpoint, the $t$ -self brings a novel coding and visualization advantage over the founding Poincar\u00e9 disk model for the embedding close to the disk\u2019s border, regardless of the nature of the embedding (whether it is data, models, etc. that are embedded). However, being new, it is important to keep the label that the $t$ -self is being used, and also clearly display the relevant value of $t$ or ranges, since the properties of the embedding can change depending on them; ", "page_idx": 13}, {"type": "text", "text": "2. from the model standpoint and a societal perspective, these visualizations will provide better methods for deployed decision tree models to be scrutinized. A potential negative of our approach is the required reduction from normal decision trees to monotonic decision trees, where the visualization does not directly correspond to the initial decision tree. We believe that monotonicity adds value to the interpretation of rules that are progressively built from trees, yet practitioners should be careful when extracting inferences from the reduced monotonic decision tree and clearly label them as being extracted from such a model, to prevent confusion with decision trees, that are overwhelmingly popular. From a pure prediction performance standpoint, we show \u2013 at least empirically within the bounds of our settings \u2013 that monotonic decision trees are similar to their original decision tree counterparts. ", "page_idx": 13}, {"type": "text", "text": "B $t$ -algebra and $t$ -additivity ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide here a few more details on the basis of our paper, the $t$ -algebra and the $t$ -additivity of some divergences. ", "page_idx": 13}, {"type": "text", "text": "B.I Primer on $t$ -algebra ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Classical arithmetic over the reals can be used to display duality relationships between operators using the log, exp functions, such as for example $\\log(a/b)=\\log a-\\log b,\\exp(a+b)=\\exp(a)\\cdot\\exp(b),$ , and so on. They can also be used to define one operator from another one. There is no difference between the operators appearing inside and outside functions. In the $t$ -algebra, a difference appears and such relationships can be used to define the $t$ -operators from those over the reals, as indeed one can define the tempered addition ", "page_idx": 13}, {"type": "text", "text": "the tempered subtraction, ", "page_idx": 13}, {"type": "equation", "text": "$$\nx\\oplus_{t}y\\;\\;\\doteq\\;\\;\\log_{t}(\\exp_{t}(x)\\cdot\\exp_{t}(y)),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "(both simplifying to the expressions we use), and of course the tempered product and division, ", "page_idx": 13}, {"type": "equation", "text": "$$\nx\\otimes_{t}y\\doteq\\exp_{t}(\\log_{t}(x)+\\log_{t}(y))\\quad;\\quad x\\oslash_{t}y\\doteq\\exp_{t}(\\log_{t}(x)-\\log_{t}(y)),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "whose simplified expression appears e.g. in [1]. See also [22]. ", "page_idx": 13}, {"type": "text", "text": "B.II Functional $t$ -additivity ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As is well-known, Boltzman-Gibbs (and so, Shannon) entropy is additive while Tsallis entropy is $t$ -additive. Note that additivity for BG requires being on the simplex, but $t$ -additivity of Tsallis technically requires only positive measures \u2013 the simplex restriction ensures the limit exists for $t\\rightarrow1$ and then $\\mathrm{T}{\\rightarrow}\\mathrm{BG}$ . ", "page_idx": 13}, {"type": "text", "text": "Divergences can also be $t$ -additive The KL divergence ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{\\mathrm{KL}}({\\pmb p}\\|{\\pmb q})\\quad\\doteq\\quad\\sum_{k}p_{k}\\log(p_{k}/q_{k})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "is additive on the simplex but not $t$ -additive: using a decomposition of $\\pmb{p},\\pmb{q}$ as product of (independent) distributions $(p_{1},p_{2}$ and ${\\pmb q}_{1},{\\pmb q}_{2})$ using the cartesian product of their support, we indeed check $D_{\\mathrm{KL}}(\\pmb{p}\\|\\pmb{q})\\,=\\,D_{\\mathrm{KL}}(\\pmb{p}_{1}\\|\\pmb{q}_{1})+D_{\\mathrm{KL}}(\\pmb{p}_{2}\\|\\pmb{q}_{2})$ with $p_{i j}\\,=\\,p_{1i}p_{2j}$ and $q_{i j}\\,=\\,q_{1i}q_{2j}$ . On the other hand, Tsallis divergence [21] is $t$ -additive on positive measures with such a decomposition, with ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{\\mathrm{T}}(\\pmb{p}\\|\\pmb{q})\\quad\\doteq\\quad\\frac{\\sum_{k}p_{k}(p_{k}/q_{k})^{1-t}-1}{1-t},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and we check that $D_{\\mathrm{T}}(p\\|q)\\;=\\;D_{\\mathrm{T}}(p_{1}\\|q_{1})\\,+\\,D_{\\mathrm{T}}(p_{2}\\|q_{2})\\,+\\,(1\\,-\\,t)\\,\\cdot\\,D_{\\mathrm{T}}(p_{1}\\|q_{1})\\,\\cdot\\,D_{\\mathrm{T}}(p_{2}\\|q_{2})$ (additional requirement to be on the simplex for convergence to $D_{\\mathrm{KL}}$ as $t\\rightarrow1$ ). For $t\\notin(1,2)$ and on the simplex, Tsallis divergence is an $f$ -divergence with generator $z\\mapsto(z^{2-t}-1)/(1-\\dot{t})$ . Similarly, the tempered relative entropy is $t$ -additive on the co-simplex, where in this case ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{t}(\\pmb{p}\\|\\pmb{q})\\quad\\stackrel{.}{=}\\quad\\frac{1-\\sum_{k}p_{k}q_{k}^{1-t}}{1-t},\\pmb{p},\\pmb{q}\\in\\tilde{\\Delta}_{m}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The tempered relative entropy is a Bregman divergence with generator $z\\mapsto z\\log_{t}z-\\log_{t-1}z$ . ", "page_idx": 14}, {"type": "text", "text": "C Supplementary material on proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.I Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Theorem is tautological for $t=1$ so we prove it for $t\\neq1$ . Denote $\\mathcal{P}(\\mathcal{S})$ the set of subsets of set S and $\\mathcal{P}_{*}(\\mathcal{S})\\doteq\\mathcal{P}(\\mathcal{S})\\backslash\\{\\mathcal{D}\\}$ . We first transform $S_{\\Delta_{n}}^{(t)}(f)$ (index $n$ shown for readability) in a better suited expression: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{S_{\\Delta_{n}}^{(t)}(f)}&{\\stackrel{,}{=}}&{\\displaystyle(\\bigoplus_{t})_{i=1}^{n}|\\mathbb{I}_{i}|f(\\xi_{i})}\\\\ &{=}&{\\displaystyle\\sum_{P\\in\\mathcal{P}_{*}([n])}(1-t)^{|P|-1}\\cdot\\prod_{i\\in P}|\\mathbb{I}_{i}|f(\\xi_{i})}\\\\ &{=}&{\\displaystyle\\frac{1}{1-t}\\cdot\\sum_{P\\in\\mathcal{P}_{*}([n])}\\prod_{i\\in P}(1-t)|\\mathbb{I}_{i}|f(\\xi_{i})}\\\\ &{=}&{\\displaystyle\\frac{1}{1-t}\\cdot\\left(\\prod_{i=1}^{n}(1+(1-t)|\\mathbb{I}_{i}|f(\\xi_{i}))-1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have used $\\mathbb{I}_{i}\\doteq[x_{i-1},x_{i}]$ for conciseness. We now need a technical Lemma. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. Fix any $0\\leqslant v<8/10$ and consider any $n$ reals $q_{i},i\\in[n]$ such that $\\left|q_{i}\\right|\\leqslant v,\\forall i\\in[n]$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n1\\leqslant\\frac{\\left(1+\\frac{1}{n}\\cdot\\sum_{i}q_{i}\\right)^{n}}{\\prod_{i}(1+q_{i})}\\leqslant\\exp\\left(n v\\cdot v\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Suppose all the $q_{i},i\\in[n]$ satisfy $q_{i}\\in[u,v]$ and let $\\varphi$ be strictly convex differentiable and defined over $[u,v]$ . Then it comes from [10, Lemma 9] that we get the right-hand side of ", "page_idx": 14}, {"type": "equation", "text": "$$\n0\\leqslant\\mathbb{E}_{i}[\\varphi(q_{i})]-\\varphi(\\mathbb{E}_{i}[q_{i}])\\leqslant D_{\\varphi}\\left(w\\left\\|(\\varphi^{\\prime})^{-1}\\left(\\frac{\\varphi(u)-\\varphi(v)}{u-v}\\right)\\right),\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we can pick $w\\in\\{u,v\\}$ and $D_{\\varphi}$ is the Bregman divergence with generator $\\varphi$ (the left-hand side is Jensen\u2019s inequality). Picking $u\\doteq-v$ (assuming wlog $v>0$ ) and letting ", "page_idx": 14}, {"type": "equation", "text": "$$\nY_{v}\\quad\\doteq\\quad\\frac{1-v}{2v}\\cdot\\log\\left(1+\\frac{2v}{1-v}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for the choice $\\varphi(z)\\doteq-\\log(1+z)$ and $w\\doteq-v$ , we obtain after simplification ", "page_idx": 14}, {"type": "equation", "text": "$$\n0\\leqslant\\log\\left(1+{\\frac{1}{n}}\\cdot\\sum_{i}q_{i}\\right)-{\\frac{1}{n}}\\cdot\\sum_{i}\\log(1+q_{i})\\leqslant Y_{v}-\\log(Y_{v})-1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Analizing function $v\\,\\,\\mapsto\\,Y_{v}\\,-\\,\\log(Y_{v})\\,-\\,1$ reveals that it is upperbounded by $v\\,\\mapsto\\,v^{2}$ if $v\\in$ $[-8/10,\\bar{8}/10]$ . Hence, multiplying by $n$ all sides and passing to the exponential, we get that ", "page_idx": 15}, {"type": "equation", "text": "$$\n1\\leqslant\\frac{\\left(1+\\frac{1}{n}\\cdot\\sum_{i}q_{i}\\right)^{n}}{\\prod_{i}(1+q_{i})}\\leqslant\\exp\\left(n v^{2}\\right),\\forall0\\leqslant v<\\frac{8}{10},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which leads to the statement of the Lemma. ", "page_idx": 15}, {"type": "text", "text": "Let us come back to our Riemannian summation setting and let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{q_{i}}&{\\doteq}&{(1-t)\\cdot|\\mathbb{I}_{i}|f(\\xi_{i}),\\forall i\\in[n],}\\\\ {v}&{\\overset{,}{=}}&{\\displaystyle\\operatorname*{max}_{i}|q_{i}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assume now that $f$ is Riemann integrable, which allows to guarantee that, at least for $n$ large enough and a step of division $\\Delta_{n}$ not too large, we have $|q_{i}|\\leqslant8/\\bar{1}0,\\forall i\\in[n]$ and so we can use Lemma 2. We get from (14) and Lemma 2, if $t<1$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{S_{\\Delta_{n}}^{(t)}(f)}}\\\\ {=}&{\\frac{1}{1-t}\\cdot\\left(\\prod_{i=1}^{n}\\left(1+q_{i}\\right)-1\\right)}\\\\ {\\in}&{\\Bigg[\\frac{1}{1-t}\\cdot\\left(\\left(1+\\frac{1}{n}\\cdot\\sum_{i}q_{i}\\right)^{n}\\cdot\\exp(-n v\\cdot v)-1\\right),\\frac{1}{1-t}\\cdot\\Bigg(\\left(1+\\frac{1}{n}\\cdot\\sum_{i}q_{i}\\right)^{n}-1\\Bigg)\\Bigg]9}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and we permute the bounds if $t>1$ . Importantly, we note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\sum_{i}q_{i}}&{{}=}&{(1-t)\\cdot S_{\\Delta_{n}}^{(1)}(f).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So suppose $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}S_{\\Delta_{n}}^{(1)}(f)\\stackrel{}{=}L_{1}\\stackrel{}{=}\\int_{a}^{b}f(u)\\mathrm{u}}\\end{array}$ is finite and choose the step of division $\\Delta_{n}$ not too large so that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{n\\cdot\\operatorname*{max}_{i}|q_{i}|}&{=}&{(1-t)\\cdot\\operatorname*{max}_{i}(n|\\mathbb{I}_{i}|)|f(\\xi_{i})|\\leqslant K\\cdot((1-t)|b-a|\\operatorname*{max}{f([a,b])}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for some constant $K\\geqslant1$ (this is possible because $f$ is (1-)Riemann integrable; we also note that if the division is regular then we can choose $K=1$ ). The value of $K$ is not important: what is important is that $n v$ remains finite in (19) as $n$ increases, while $\\scriptstyle\\operatorname*{lim}_{n\\to+\\infty}v\\ =0$ . Hence, $\\exp(-n v\\cdot v)\\to1$ as $n\\to+\\infty$ and (19) implies, because $\\mathrm{lim}_{n\\rightarrow+\\infty}(1+a/n)^{n}=\\exp a$ , the two first identities in ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{lim}_{n\\to+\\infty}S_{\\Delta_{n}}^{(t)}(f)}&{=}&{\\displaystyle\\frac{1}{1-t}\\cdot\\left(\\operatorname*{lim}_{n\\to+\\infty}\\left(1+\\frac{1}{n}\\cdot\\sum_{i}q_{i}\\right)^{n}-1\\right)}\\\\ &{=}&{\\displaystyle\\frac{1}{1-t}\\cdot\\left(\\exp\\left(\\sum_{i}q_{i}\\right)-1\\right)}\\\\ &{=}&{\\displaystyle\\frac{1}{1-t}\\cdot\\left(\\exp\\left((1-t)L_{1}\\right)-1\\right)}\\\\ &{=}&{\\log_{t}\\exp(L_{1})}\\\\ &{=}&{\\log_{t}\\exp\\left(\\int_{a}^{b}f(u)\\mathrm{d}u\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "((21) follows from (20)) and by definition $\\begin{array}{r l}{\\operatorname*{lim}_{n\\rightarrow+\\infty}S_{\\Delta_{n}}^{(t)}(f)\\ \\dot{=}\\ \\displaystyle\\int_{a}^{t}f(u)\\mathrm{d}_{t}u}\\end{array}$ . So we get that Riemann integration $\\mathit{\\Theta}(t=1)$ ) grants ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{a}^{b}f(u)\\mathrm{d}_{t}u\\quad=\\quad\\log_{t}{\\exp\\int_{a}^{b}f(u)\\mathrm{d}u},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which, in addition to showing (9) (main flie) also shows that $t=1$ -Riemann integration is equivalent to all $t\\neq1$ -Riemann integration, ending the proof of Theorem 2. ", "page_idx": 15}, {"type": "text", "text": "Remark 4. The absence of affine terms in upperbounding $v\\mapsto Y_{v}-\\log(Y_{v})-1$ in the neighborhood of $\\boldsymbol{O}$ is crucial to get to our result. ", "page_idx": 15}, {"type": "text", "text": "C.II Proof of Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Using Theorem 2, we just have to analyze the limit in relationship to the Riemannian case: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{D}_{t}F(z)}&{\\triangleq\\underset{\\delta=0}{\\operatorname*{lim}}\\int_{a}^{\\lfloor\\pi\\rfloor+\\delta}f(u)\\ensuremath{\\mathrm{d}}u\\,\\ensuremath{\\mathrm{d}}\\phi_{t}\\,\\hat{\\ensuremath{\\varepsilon}}_{u}^{(i)}\\int_{a}^{z}f(u)\\ensuremath{\\mathrm{d}}u}\\\\ &{=\\begin{array}{r l}{\\ensuremath{\\operatorname*{lim}}}&{\\frac{1}{\\delta-\\delta}\\frac{\\int_{a}^{a}\\phi_{t}\\,\\ensuremath{\\mathrm{d}}u\\,\\ensuremath{\\phi}_{t}\\,\\ensuremath{\\mathrm{d}}u}{\\delta}}\\\\ &{\\frac{1}{\\delta-\\delta}\\frac{\\int_{a}^{a}\\phi_{t}\\,\\ensuremath{\\mathrm{d}}\\phi_{t}}{\\delta}\\frac{\\int_{a}^{a+\\delta}\\int_{a}^{b}\\left(u\\right)\\,\\ensuremath{\\mathrm{d}}\\phi_{t}\\,\\ensuremath{\\mathrm{d}}u}{\\delta}}\\\\ &{=\\begin{array}{r l}{\\ensuremath{\\operatorname*{lim}}}&{\\frac{1}{\\delta-\\delta}\\int_{a}^{a}\\left(\\frac{\\ensuremath{\\mathrm{d}}\\phi_{t}\\,\\ensuremath{\\mathrm{d}}^{-\\delta}\\int_{a}^{a}(u)\\ensuremath{\\mathrm{d}}u}{\\ensuremath{\\mathrm{d}}\\phi_{t}\\,\\ensuremath{\\mathrm{d}}\\phi_{t}}\\right)}\\end{array}}\\\\ &{=\\begin{array}{r l}{\\ensuremath{\\operatorname*{lim}}}&{\\frac{1}{\\delta-\\delta}\\int_{a}^{a}\\left(\\ensuremath{\\mathrm{d}}\\phi_{t}-\\ensuremath{\\mathrm{d}}\\phi_{t}^{-1}\\left(u\\right)\\ensuremath{\\mathrm{d}}u-\\int_{a}^{a}f(u)\\ensuremath{\\mathrm{d}}u\\right)}\\\\ &{-\\delta\\frac{\\int_{a}^{a}\\phi_{t}\\,\\ensuremath{\\mathrm{d}}u}{\\delta}\\int_{a}^{\\infty+\\delta}\\int_{a}^{a}f(u)\\ensuremath{\\mathrm{d}}u-\\int_{a}^{a}f(u)\\ensuremath{\\mathrm{d}}u\\right)}\\\\ &{=\\begin{array}{r l}{\\ensuremath{\\operatorname*{lim}}}&{\\frac{1}{\\delta-\\delta}\\int_{a}^{a}f\\left(\\ensuremath{\\mathrm{d}}u-\\int_{a}^{a}f(u)\\ensuremath{\\mathrm{d}}u\\right)}\\\\ &{+\\frac{\\operatorname*{lim}}{2\\delta}\\int_{a}^{a}f(u)=f,}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last identity is the classical Riemannian case, (22) follows from Theorem 2, (23) follows from a property of $\\log_{t}$ $:(\\log_{t}a\\ominus\\log_{t}b=\\log_{t}(a/b))$ , (24) follows from the fact that $\\log_{t}\\exp(z)=_{0}$ $z+o(z)$ . This completes the proof of Theorem 3. ", "page_idx": 16}, {"type": "text", "text": "C.III Additional and helper results for Theorems 2 and 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first state a trivial but important Lemma, whose content is partially used in the main file. Lemma 3. ${\\mathfrak{g}}_{t}$ satisfies the following properties: ", "page_idx": 16}, {"type": "text", "text": "1. ${\\mathfrak{g}}_{t}(z)$ is strictly increasing for any $t\\in\\mathbb{R}$ , strictly concave for any $t>1$ , strictly convex for any $t<1$ and such that $\\mathrm{sign}(\\mathfrak{g}_{t}(z))=\\mathrm{sign}(z),\\forall t\\in\\mathbb{R},$ ;   \n2. $\\operatorname{D}_{t}\\mathfrak{g}_{t}(z)=1$ ;   \n3. ( $t$ -integral mean-value) Suppose $f$ Riemann integrable over $[a,b]$ . Then there exists $c\\in$ $(a,b)$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n(b-a)\\cdot\\mathfrak{g}_{t^{\\prime}}\\circ f(c)=\\!\\!\\int_{a}^{b}f(u)\\mathrm{d}_{t}u\\:\\:(t^{\\prime}\\doteq1-(1-t)(b-a)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We list a series of consequences to both theorems. ", "page_idx": 16}, {"type": "text", "text": "General properties of $t$ -integrals Some properties generalize those for classical Riemann integration. ", "page_idx": 16}, {"type": "text", "text": "Theorem C.1. The following relationships hold for any $t\\in\\mathbb{R}$ and any functions $f,g$ t-Riemann integrable over some interval $[a,b]$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{a}^{t}f(u)\\left\\{+\\left.o r-\\right\\}g(u)\\mathrm{d}_{t}u}&{=}&{\\left(\\left(\\displaystyle\\int_{a}^{t}f(u)\\mathrm{d}_{t}u\\right)\\left\\{\\oplus_{t}o r\\odot_{t}\\right\\}\\left(\\left(\\displaystyle\\int_{a}^{t}f(u)\\mathrm{d}_{t}u\\right)\\right.\\right.}\\\\ {\\displaystyle\\left.\\left(\\displaystyle\\int_{a}^{t}\\lambda f(u)\\mathrm{d}_{t}u}&{=}&{\\lambda\\cdot\\left.\\left(1-(1-t)\\lambda\\right)\\right)\\right\\{\\phi_{t}^{b}(u)\\mathrm{d}_{t}u\\quad(\\lambda\\in\\mathbb{R})}\\\\ {\\displaystyle(\\imath)\\int_{a}^{b}f(x)\\mathrm{d}_{t}u}&{=}&{(\\imath)\\displaystyle\\int_{a}^{c}f(u)\\mathrm{d}_{t}u\\quad\\left(\\right.\\left(t\\right)\\right)_{c}f(u)\\mathrm{d}_{t}u\\quad(c\\in[a,b])}\\\\ {\\displaystyle\\int_{a}^{\\left(t\\right)}f(x)\\mathrm{d}_{t}u}&{=}&{\\left(\\displaystyle\\int_{a}^{c}f(u)\\mathrm{d}_{t}u\\right)\\displaystyle\\phi_{t}\\left(\\left.\\displaystyle\\int_{c}^{b}f(u)\\mathrm{d}_{t}u\\quad(c\\in[a,b])\\right.}\\\\ {\\displaystyle\\left.\\left(\\displaystyle\\int_{a}^{t}f(u)\\mathrm{d}_{t}u\\right)\\right\\{\\leqslant}&{(1-1-t)\\int_{a}^{b}|f(u)|\\mathrm{d}_{t}u}\\\\ {\\displaystyle(\\imath)\\int_{a}^{b}f(u)\\mathrm{d}_{t}u}&{\\leqslant}&{\\displaystyle\\int_{a}^{\\left(t\\right)}f(u)\\mathrm{d}_{t}u\\quad(f\\leqslant g)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We show additivity for $\\oplus_{t}/+$ (the same path shows the result for $\\circleddash$ ): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\int_{a}^{b}f(u)\\mathrm{d}_{t}u\\circledast_{t}}&{\\displaystyle\\int_{a}^{(t)}f(u)\\mathrm{d}_{t}u}&{=}&{\\displaystyle\\log_{t}\\exp\\int_{a}^{b}f(u)\\mathrm{d}u\\circledast_{t}\\log_{t}\\exp\\int_{a}^{b}g(u)\\mathrm{d}u}\\\\ &{=}&{\\displaystyle\\log_{t}\\left(\\exp\\int_{a}^{b}f(u)\\mathrm{d}u\\cdot\\exp\\int_{a}^{b}g(u)\\mathrm{d}u\\right)}\\\\ &{=}&{\\displaystyle\\log_{t}\\exp\\int_{a}^{b}(f+g)(u)\\mathrm{d}u}\\\\ &&{=}&{\\displaystyle\\int_{a}^{(t)}(f+g)(u)\\mathrm{d}_{t}u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We show dilativity, using $t^{\\prime}\\doteq1-(1-t)\\lambda$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\int_{a}^{b}\\Lambda J(u)\\d u_{i}u}&{=\\log_{\\Theta}\\exp_{a}\\int_{a}^{b}J(u)\\d u_{i}}\\\\ {=}&{\\log_{\\Theta}\\exp_{a}\\int_{a}^{b}\\int_{0}^{\\Theta}\\d u[\\mathrm{a}u]}\\\\ {=}&{\\frac{1}{1-t}\\cdot\\left(\\exp\\left(\\lambda(1-t)\\int_{a}^{b}f(u)\\d u_{i}\\right)-1\\right)}\\\\ {=}&{\\frac{1-t^{\\prime}}{1-t}\\cdot\\frac{1}{1-t^{\\prime}}\\cdot\\left(\\exp\\left(\\left(1-t^{\\prime}\\right)\\int_{a}^{b}f(u)\\d u_{i}\\right)-1\\right)}\\\\ {=}&{\\lambda\\cdot\\frac{1}{1-t^{\\prime}}\\cdot\\left(\\exp\\left(\\left(1-t^{\\prime}\\right)\\int_{a}^{b}f(u)\\d u_{i}\\right)-1\\right)}\\\\ {=}&{\\lambda\\cdot\\log_{\\Theta}\\exp_{a}\\int_{a}^{b}f(u)\\d u_{i}}\\\\ &{=}&{\\lambda\\cdot\\log_{\\Theta}\\int_{a}^{(b)}\\d u[\\mathrm{a}u]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The triangle inequality follows from the relationship: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{|\\log_{t}\\exp(z)|}&{\\leqslant}&{\\log_{1-|1-t|}\\exp|z|,\\forall z,t\\in\\mathbb{R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "from which we use the fact that Riemann integration satisfies the triangle inequality and $\\log_{1-|1-t|}$ is monotonically increasing on $\\mathbb{R}_{+}$ in the penultimate line of: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left|\\stackrel{(t)}{\\int}_{a}^{b}f(u)\\mathrm{d}_{t}u\\right|}}&{\\doteq}&{\\left|\\log_{t}\\exp\\int_{a}^{b}f(u)\\mathrm{d}u\\right|}\\\\ &{\\leqslant}&{\\log_{1-|1-t|}\\exp\\left|\\int_{a}^{b}f(u)\\mathrm{d}u\\right|}\\\\ &{\\leqslant}&{\\log_{1-|1-t|}\\exp\\int_{a}^{b}|f(u)|\\mathrm{d}u}\\\\ &{}&{\\stackrel{(1-|1-t|)}{\\int}_{a}^{b}|f(u)|\\mathrm{d}u}\\\\ &{=}&{\\int_{a}^{b}|f(u)|\\mathrm{d}_{t}u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We show Chasles\u2019 relationship: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle{\\prod_{a}^{t}}f(u)\\mathrm{d}_{t}u\\oplus\\left.\\int_{c}^{(t)}f(u)\\mathrm{d}_{t}u\\right.}&{\\stackrel{(i)}{=}\\displaystyle{\\operatorname{log}_{t}}\\exp\\left\\int_{a}^{c}f(u)\\mathrm{d}u\\oplus_{t}\\mathrm{log}_{t}\\exp\\int_{c}^{b}f(u)\\mathrm{d}u}\\\\ {\\displaystyle}&{=\\left.\\log_{t}\\left(\\exp\\int_{a}^{c}f(u)\\mathrm{d}u\\cdot\\exp\\int_{c}^{b}f(u)\\mathrm{d}u\\right)\\right.}\\\\ {\\displaystyle}&{=\\left.\\log_{t}\\exp\\left(\\int_{a}^{c}f(u)\\mathrm{d}u+\\int_{c}^{b}f(u)\\mathrm{d}u\\right)\\right.}\\\\ {\\displaystyle}&{=\\left.\\log_{t}\\exp\\int_{a}^{b}f(u)\\mathrm{d}u}\\\\ {\\displaystyle}&{=\\left.\\log_{t}\\exp\\int_{a}^{b}f(u)\\mathrm{d}u\\right.}\\\\ {\\displaystyle}&{=\\left.\\int_{a}^{b}f(u)\\mathrm{d}_{t}u,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second identity uses the property $\\log_{t}a\\oplus\\log_{t}b\\;=\\;\\log_{t}(a b)$ . Monotonicity follows immediately from the fact that $z\\mapsto\\log_{t}\\exp z$ is strictly increasing: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\displaystyle\\int_{a}^{b}f(u)\\mathrm{d}_{t}u}}&{{=}}&{{\\log_{t}\\exp\\int_{a}^{b}f(u)\\mathrm{d}u}}\\\\ {{\\displaystyle}}&{{\\leqslant}}&{{\\log_{t}\\exp\\int_{a}^{b}g(u)\\mathrm{d}u\\doteq\\quad\\int_{a}^{b}g(u)\\mathrm{d}_{t}u.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This ends the proof of Theorem C.1. ", "page_idx": 18}, {"type": "text", "text": "Computing $t$ -integrals Next, classical relationships to compute integrals do generalize to $t$ - integration. We cite the case of integration by part. ", "page_idx": 18}, {"type": "text", "text": "Lemma 4. Integration by part translates to $t$ -integration by part as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\int_{a}^{b}f g^{\\prime}\\mathrm{d}u}&{=}&{\\left(t\\right)\\left[f g\\right]_{a}^{b}\\bigotimes_{t}\\quad\\displaystyle\\int_{a}^{b}f^{\\prime}g\\mathrm{d}u,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{(t)\\left[h\\right]_{a}^{b}}&{\\doteq}&{\\log_{t}\\exp(h(b))\\ominus_{t}\\log_{t}\\exp(h(a)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(Proof immediate from Theorem 2) ", "page_idx": 18}, {"type": "text", "text": "Geometric properties based on $t$ -integrals This is a more specific result, important in the context of hyperbolic geometry: the well-known Hyperbolic Pythagorean Theorem (HPT) does translate to a tempered version with the same relationship to the Euclidean theorem. Consider a hyperbolic right triangle with hyperbolic lengths $a,b,c,c$ being the hyperbolic length of the hypothenuse. Let $a_{t},b_{t},c_{t}$ denote the corresponding tempered lengths, which are therefore explicitly related using ${\\mathfrak{g}}_{t}$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\na_{t}=\\log_{t}\\exp a,\\quad b_{t}=\\log_{t}\\exp b,\\quad c_{t}=\\log_{t}\\exp c.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Define the tempered generalization of cosh: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\cosh_{t}z\\;\\;\\;\\dot{=}\\;\\;\\;\\frac{\\exp_{t}z+\\exp_{t}(-z)}{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The HPT tells us that $\\cosh c=\\cosh a\\cosh b$ . It is a simple matter of plugging ${\\mathfrak{g}}_{t}$ , using the fact that $\\log_{t}$ and $\\mathrm{exp}_{t}$ are inverse of each other and simplifying to get the tempered HPT, which we call $t$ -HPT for short. ", "page_idx": 18}, {"type": "text", "text": "Lemma 5. $^{\\,\\prime}t$ -HPT) For any hyperbolic triangle described above, the tempered lengths are related as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\cosh_{t}c_{t}=\\cosh_{t}a_{t}\\cosh_{t}b_{t}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, remark that for any $t\\neq0$ , a series expansion around 0 gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\exp_{t}(z)\\;\\;\\;=\\;\\;\\;1+\\frac{t z^{2}}{2}+o(z^{3})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\mathrm{exp}_{t}$ is always infinitely differentiable around 0, for any $t$ ) So the $t$ -HPT gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{1+\\displaystyle\\frac{t c_{t}^{2}}{2}+o(c_{t}^{3})}}&{{=}}&{{\\displaystyle\\left(1+\\frac{t a_{t}^{2}}{2}+o(a_{t}^{3})\\right)\\cdot\\left(1+\\frac{t b_{t}^{2}}{2}+o(b_{t}^{3})\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which simplifies, if we multiply both sides by $2/t$ and simplify it into ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{c_{t}^{2}+o(c_{t}^{3})}&{=}&{a_{t}^{2}+b_{t}^{2}+o(a_{t}^{3})+o(b_{t}^{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which for an infinitesimal right triangle gives $c_{t}^{2}\\approx a_{t}^{2}+b_{t}^{2}$ , i.e. Pythagoras Theorem, as does the HPT one gives in this case $(\\bar{c^{2}}\\approx a^{2}+b^{2})$ , which is equivalent of the particular $t=1$ -HPT case. ", "page_idx": 19}, {"type": "text", "text": "$t$ -mean value Theorem The $t$ -derivative yields a generalization of the Euclidean mean-value theorem. ", "page_idx": 19}, {"type": "text", "text": "Lemma 6. Let $t\\in\\mathbb R$ and $f$ be continuous over an interval $[a,b]$ , differentiable on $(a,b)$ and such that $-1/(1-t)\\not\\in f([a,b])$ . Then $\\exists c\\in(a,b)$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\mathrm{D}_{t}f(c)}&{=}&{\\frac{\\displaystyle\\left(f(b)\\ominus_{t}f(c)\\right)-\\left(f(a)\\ominus_{t}f(c)\\right)}{b-a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We can obtain a direct expression of $\\mathrm{D}_{t}f$ by using the definition of $\\circleddash$ and the classical derivative: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{l}_{t}f(x)=\\operatorname*{lim}_{\\delta\\to0}\\frac{1}{\\delta}\\cdot\\frac{f(x+\\delta)-f(x)}{1+(1-t)f(x)}=\\frac{1}{1+(1-t)f(x)}\\cdot\\operatorname*{lim}_{\\delta\\to0}\\frac{f(x+\\delta)-f(x)}{\\delta}=\\frac{f^{\\prime}(x)}{1+(1-t)f(x)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From here, the mean-value theorem tells us that there exists $c\\in[a,b]$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nf^{\\prime}(c)\\;\\;\\;=\\;\\;\\;{\\frac{f(b)-f(a)}{b-a}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Dividing by $1+(1-t)f(c)$ (assuming $f(c)\\neq-1/(1-t))$ and reorganising, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathrm{D}_{t}f(c)}&{=}&{\\displaystyle\\frac{1}{b-a}\\cdot\\frac{f(b)-f(a)}{1+(1-t)f(c)}}\\\\ &{=}&{\\displaystyle\\frac{1}{b-a}\\cdot\\frac{f(b)-f(c)}{1+(1-t)f(c)}-\\frac{1}{b-a}\\cdot\\frac{f(a)-f(c)}{1+(1-t)f(c)}}\\\\ &{=}&{\\displaystyle\\frac{\\left(f(b)\\ominus_{t}f(c)\\right)-\\left(f(a)\\ominus_{t}f(c)\\right)}{b-a},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which completes the proof of the Lemma. ", "page_idx": 19}, {"type": "text", "text": "In fact, the $t$ -derivative of $f$ at some $c$ is \"just\" an Euclidean derivative for an affine transformation of the function, namely $z\\mapsto f(z)\\ominus_{t}f(c)$ , also taken at $z=c$ . This \"proximity\" between $t$ -derivation and derivation is found in the tempered chain rule (proof straightforward). ", "page_idx": 19}, {"type": "text", "text": "Lemma 7. Suppose g differentiable at $z$ and $f$ differentiable at $g(z)$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{D}_{t}(f\\circ g)(z)}&{{}=}&{\\mathrm{D}_{t}(f)(g(z))\\cdot g^{\\prime}(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.IV Sets endowed with a distortion and their $t$ -self: statistical information ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, $\\mathcal{X}$ contains probability distributions or the parameters of probability distributions: $f$ can then be an $f$ -divergence (information theory) or a Bregman divergence (information geometry). Tsallis divergence and the tempered relative entropy are examples of $t$ -additive information theoretic and information geometric divergences. A key property of information theory is the data processing inequality $\\mathbf{(D)}$ , $\\mathcal{X}$ being a probability space, which says that passing random variables through a Markov chain cannot increase their divergence as quantified by $f$ [25, 35]. A key property of information geometry is the population minimizer property $\\mathbf{\\delta}(\\mathbf{P})$ , which elicits a particular function of a set of points as the minimizer of the expected distortion to the set, as quantified by $f$ [3]. We let $\\mathbf{\\delta}(\\mathbf{J})$ denote the joint convexity property, which would state for $f$ and any $\\mathbf{\\Deltax}_{1},\\mathbf{\\Deltax}_{2},\\mathbf{\\Deltay}_{1},\\mathbf{\\Deltay}_{2}$ that ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\lambda\\cdot x_{1}+(1-\\lambda)\\cdot x_{2},\\lambda\\cdot y_{1}+(1-\\lambda)\\cdot y_{2})\\leqslant\\lambda f(x_{1},y_{1})+(1-\\lambda)f(x_{2},y_{2}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and convexity (C), which amounts to picking ${\\pmb y}_{1}={\\pmb y}_{2}$ (convexity in the left parameter) xor ${\\pmb x}_{1}={\\pmb x}_{2}$ (in the right parameter). ", "page_idx": 20}, {"type": "text", "text": "Lemma 8. For any $t\\in\\mathbb R$ , the following holds true: ", "page_idx": 20}, {"type": "text", "text": "$(\\pmb{D})\\textit{f}$ satisfies the data processing inequality iff $f^{(t)}$ satisfies the data processing inequality; ", "page_idx": 20}, {"type": "text", "text": "$(P)$ $\\begin{array}{r}{\\mu_{*}\\in\\arg\\operatorname*{min}_{\\pmb{\\mu}}\\sum_{i}f(\\pmb{x}_{i},\\pmb{\\mu})}\\end{array}$ iff ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\pmb{\\mu}_{*}}&{{}\\in}&{\\arg\\underset{\\pmb{\\mu}}{\\operatorname*{min}}\\left(\\oplus_{t}\\right)_{i}\\pmb{f}^{(t)}(\\pmb{x}_{i},\\pmb{\\mu});}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(J) $f$ satisfies (27) iff the following $(t,t^{\\prime},t^{\\prime\\prime})$ -joint convexity property holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{(t)}(\\lambda\\cdot x_{1}+(1-\\lambda)\\cdot x_{2},\\lambda\\cdot y_{1}+(1-\\lambda)\\cdot y_{2})\\leqslant\\lambda f^{(t^{\\prime})}(x_{1},y_{1})+(1-\\lambda)f^{(t^{\\prime\\prime})}(x_{2},y_{2}),}\\\\ &{w i t h\\,t^{\\prime}\\doteq\\operatorname*{min}\\{t,1-\\lambda+\\lambda t\\}\\,a n d\\,t^{\\prime\\prime}\\doteq\\operatorname*{min}\\{t,\\lambda+(1-\\lambda)t\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. $({\\bf D})$ and $({\\bf P})$ are immediate consequences of Lemma 3 (point [1.], main flie) and properties of $\\log_{t}$ . We prove (J). ${\\mathfrak{g}}_{t}$ being strictly increasing for any $t$ , we get for $t\\leqslant1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{^{\\mathrm{\\scriptsize(}t\\mathrm{)}}(\\lambda\\cdot x_{1}+(1-\\lambda)\\cdot x_{2},\\lambda\\cdot y_{1}+(1-\\lambda)\\cdot y_{2})}&{\\leqslant}&{\\mathfrak{g}_{t}\\left(\\lambda f(x_{1},y_{1})+(1-\\lambda)f(x_{2},y_{2})\\right)}\\\\ &{\\leqslant}&{\\lambda\\cdot\\mathfrak{g}_{t}\\circ f(x_{1},y_{1})+(1-\\lambda)\\cdot\\mathfrak{g}_{t}\\circ f(x_{2},y_{2})}\\\\ &&{=\\lambda f^{\\mathrm{\\scriptsize(}t\\mathrm{)}}(x_{1},y_{1})+(1-\\lambda)f^{\\mathrm{\\scriptsize(}t\\mathrm{)}}(x_{2},y_{2})(30)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "because ${\\mathfrak{g}}_{t}$ is convex. If $t>1$ , we restart from the first inequality and remark that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\iota\\left(\\lambda f(x_{1},y_{1})+(1-\\lambda)f(x_{2},y_{2})\\right)}&{=}&{\\mathfrak{g}_{t}\\left(\\lambda f(x_{1},y_{1})\\right)\\oplus_{t}\\mathfrak{g}_{t}\\left((1-\\lambda)f(x_{2},y_{2})\\right)}\\\\ &{\\leqslant}&{\\mathfrak{g}_{t}\\left(\\lambda f(x_{1},y_{1})\\right)+\\mathfrak{g}_{t}\\left((1-\\lambda)f(x_{2},y_{2})\\right)}\\\\ &&{=\\lambda\\cdot\\mathfrak{g}_{1-\\lambda+\\lambda t}\\circ f(x_{1},y_{1})+(1-\\lambda)\\cdot\\mathfrak{g}_{\\lambda+(1-\\lambda)t}\\circ f(x_{2},y_{2})}\\\\ &{=}&{\\lambda f^{(1-\\lambda+\\lambda t)}(x_{1},y_{1})+(1-\\lambda)f^{(\\lambda+(1-\\lambda)t)}(x_{2},y_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The inequality is due to the fact that $a\\oplus_{t}b=a+b+(1-t)a b\\leqslant a+b$ if $a b\\geqslant0$ and $t\\geqslant1$ . The last equality holds because, for $t^{\\prime}\\doteq1-(1-t)b$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\log_{t}{(a^{b})}={\\frac{a^{(1-t)b}-1}{1-t}}={\\frac{1-t^{\\prime}}{1-t}}\\cdot{\\frac{a^{1-t^{\\prime}}-1}{1-t^{\\prime}}}=b\\cdot\\log_{t^{\\prime}}(a).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Putting altogether (30) and (31), we get that for any $t\\in\\mathbb R$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f^{(t)}(\\lambda\\cdot x_{1}+(1-\\lambda)\\cdot x_{2},\\lambda\\cdot y_{1}+(1-\\lambda)\\cdot y_{2})}&{}\\\\ {\\leqslant\\,}&{\\lambda f^{(\\operatorname*{min}\\{t,1-\\lambda+\\lambda t\\})}(x_{1},y_{1})+(1-\\lambda)f^{(\\operatorname*{min}\\{t,\\lambda+(1-\\lambda)t\\})}(x_{2},y_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as claimed for $\\mathbf{\\delta}(\\mathbf{J})$ . This ends the proof of Lemma 8. ", "page_idx": 20}, {"type": "text", "text": "We note that (29) also translates into a property for (C); also, if $t\\leqslant1$ , then $t=t^{\\prime}=t^{\\prime\\prime}$ in (29). ", "page_idx": 20}, {"type": "text", "text": "C.V The t-self of the Lorentz model of hyperbolic geometry ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As an additional example of use, we consider the Lorentz model for a simple illustration in which one creates approximate metricity in the t-self. This d-dimensional manifold is embedded in Rd\\`1 via the hyperboloid constant ,  wwiitthh $-c<0$ curvature a tnhde  dLeofrineendtz ibayn $\\mathbb{H}_{c}\\;\\doteq\\;\\{{\\pmb x}\\;\\in\\;\\mathbb{R}^{d+1}\\,:\\,x_{0}\\;>$ $0\\wedge\\pmb{x}\\circ\\pmb{x}=-1/c\\}$ $\\begin{array}{r}{\\pmb{x}\\circ\\pmb{y}\\doteq-x_{0}y_{0}+\\sum_{i=1}^{d}x_{i}y_{i}}\\end{array}$   \n$\\mathbb{H}_{c}$   \n$d_{L}(\\pmb{x},\\pmb{y})\\doteq-(2/c)-2\\cdot\\pmb{x}\\circ\\pmb{y}$ . It is notoriously not a distance because it does not satisfy ${(\\Gamma)}$ , yet we show that we can pick $t$ and the curvature in such a way that the t-self is arbitrarily close to a metric space. ", "page_idx": 20}, {"type": "text", "text": "Lemma 9. $\\forall\\delta\\,>\\,0$ , pick $t$ and curvature c as $t\\,=\\,1\\,+\\,(1/\\delta),c\\,=\\,2/\\delta$ . Then the $t$ -self of $\\mathbb{H}_{c}$ is approximately metric: $d_{L}^{(t)}$ satisfies $(R)$ , (S), $(I)$ , and the $\\delta$ -triangle inequality, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{L}^{(t)}(\\pmb{x},z)\\leqslant d_{L}^{(t)}(\\pmb{x},\\pmb{y})+d_{L}^{(t)}(\\pmb{y},z)+\\delta,\\forall\\pmb{x},\\pmb{y},z\\in\\mathbb{H}_{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. $d_{L}^{(t)}$ still obviously satisfies reflexivity, the identity of indiscernibles, and non-negativity, so we check the additional property it now satisfies, the weaker version of the triangle inequality. Given any $\\mathbf{\\Delta}x,y,z$ in $\\mathbb{H}_{c}$ , condition $d_{L}^{(\\bar{t})}({\\pmb x},z)\\leqslant d_{L}^{(t)}({\\pmb x},{\\pmb y})+d_{L}^{(t)}({\\pmb y},z)+\\delta$ for $t>1$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\frac{1-\\exp\\big(-(t-1)\\,\\left(-\\frac{2}{c}-2\\cdot x\\circ z\\right)\\big)}{t-1}}&{\\leqslant}&{\\frac{1-\\exp\\big(-(t-1)\\,\\left(-\\frac{2}{c}-2\\cdot x\\circ y\\right)\\big)}{t-1}}\\\\ &&{+\\frac{1-\\exp\\big(-(t-1)\\,\\left(-\\frac{2}{c}-2\\cdot y\\circ z\\right)\\big)}{t-1}+\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which simplifies to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\exp\\left(2(t-1)\\cdot x\\circ y\\right)+\\exp\\left(2(t-1)\\cdot y\\circ z\\right)}&{\\leqslant}&{\\exp\\left(-\\displaystyle\\frac{2(t-1)}{c}\\right)+(t-1)\\delta\\exp\\left(-\\displaystyle\\frac{2(t-1)^{\\alpha}}{c}\\right)}\\\\ &&{+\\exp\\left(2(t-1)\\cdot x\\circ z\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have $\\mathbf{\\Delta}x\\circ y\\leqslant-1/c$ by definition, so a sufficient condition to get the inequality is to have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\exp{(2(t-1)\\cdot y\\circ z)}\\ \\ \\leqslant\\ \\ (t-1)\\delta\\exp{\\left(-\\frac{2(t-1)}{c}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Function $h(z)\\doteq z\\delta\\exp(-2z/c)$ is maximum over $\\mathbb{R}_{+}$ for $z_{*}=c/2$ , for which it equals $h(z_{*})=$ $c\\delta/(2e)$ . Fix $t=1+(c/2)$ . We then have $\\exp{(2(t-1)\\cdot y\\circ z)}\\leqslant1/e$ so to get (34) for this choice of $t$ , it is sufficient to pick curvature $c=2/\\delta$ , yielding relationship $t=1+(1/\\delta)$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C.VI Proof of Lemma 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We start by proving condition ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d_{*}^{(t)}}&{{}\\geqslant}&{g(k)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in the Lemma. Using the proof of [19, Proposition 3.1], we know that any point $\\textbf{\\em x k}$ -close to the boundary (Definition 5.1) satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{d^{(t)}(\\pmb{x},\\mathbf{0})}&{=}&{\\log_{t}\\left(\\displaystyle\\frac{1+\\|\\pmb{x}\\|}{1-\\|\\pmb{x}\\|}\\right)=\\log_{t}\\left(2\\cdot10^{k}-1\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so to get (35), we want $\\log_{t}(2\\cdot10^{k}-1)\\geqslant g(k)$ . Letting $t=1-f(k)$ with $f(k)\\in\\mathbb{R}$ , we observe $\\log_{t}(2\\cdot10^{k}-1)=\\left((2\\cdot10^{k}-1)^{f(k)}-1\\right)/f(k)$ , so we want, after taking logs, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log(2\\cdot10^{k}-1)\\;\\;\\;\\geqslant\\;\\;\\;{\\frac{\\log{(1+f(k)g(k))}}{f(k)}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(this also holds if $f(k)<0$ because $1\\!+\\!f(k)g(k)<1)$ , and there remains to observe $\\log(2{\\cdot}10^{k}{-}1)=$ $k\\log(10)+\\log(2-1/10^{k})$ with $\\log(2-1/10^{k})\\geqslant0,\\forall k\\geqslant0$ . Hence, to get (36) it is sufficient to request ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\log{(1+f(k)g(k))}}{f(k)}\\;\\;\\;\\leqslant\\;\\;\\log(10)\\cdot k,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is (35). For such $t$ , the new hyperbolic constant satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tau_{t}\\;\\;\\;=\\;\\;\\log_{t}\\exp\\tau=\\frac{1}{f(k)}\\cdot(\\exp\\left(f(k)\\tau\\right)-1)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.VII Proof of Theorem 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Take any observation $\\mathbf{\\Delta}x\\in\\mathcal{X}$ . Trivially, $H^{\\prime}(x)$ is a real value that belongs to the path followed by $\\textbf{\\em x}$ in $H$ . Furthermore, if we consider any path from the root to a leaf in $H$ , all the vertices in its strictly monotonically increasing subsequence of absolute confidences appear in $H^{\\prime}$ (conditions 4, 8 in GETMDT), which guarantees the condition on prediction in (M). Hence $\\mathbf{\\delta}(\\mathbf{M})$ is satisfied. ", "page_idx": 21}, {"type": "image", "img_path": "n60xBFZWrk/tmp/b303674bd153807a6ccd9ff6887ce04cc31c66f85950855cf5eff84337596c13.jpg", "img_caption": ["Figure I: Schematic description of our modification (bottom) of Sarkar\u2019s algorithm (up). Our modification solely changes the step in which the children of a node (here, $a$ ) are computed, this node having been reflected back to the origin. Instead of using a fixed angle and length to position arcs (and thus children of the node), the angle depends on the number of leaves that can be reached from a child, and the length depends on the difference between absolute confidence between the node and the corresponding child. We also define an angle which represents the domain (before reflecting back) in which the embedding is going to take place, shown with the thick dashed line (here, this angle is $\\pi$ ). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.VIII Modifying Sarkar\u2019s embedding in Poincar\u00e9 disk ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Sarkar\u2019s algorithm [30] gives a clean low-distortion embedding when the tree is binary or the arc length is constant [29]. Things are different in our case: MDT nodes have arbitrary out-degrees, and lengths depend on the absolute confidence at the corresponding MDT nodes. Plus, a direct implementation of Sarkar\u2019s algorithm would violate the constraint for strict path-monotonicity in an MDT that nodes in a path from the root need to progressively come closer to the border \u2013 equivalently, it would create substantial embedding errors for confidences and violate (B). Without focusing on an optimal solution (that we leave for future work), one can remark that all these problems can be heuristically addressed by changing one step of Sarkar\u2019s algorithm, replacing the use of the total $2\\pi$ fan out angle for mapping children (Step 5: in Algorithm 1 of [29]) by a variable angle with a special orientation in the disk. ", "page_idx": 22}, {"type": "text", "text": "We refer to the concise and neat description of Sarkar\u2019s embedding in [29] for the full algorithm. Our modification relies on changing one step of the algorithm, as described in Figure I. The key step that we change is step 5: in the description of [29, Algorithm 1]. This step embeds the children of a given node (and then the algorithm proceeds recursively until all nodes are processed). Sarkar\u2019s algorithm corresponds to the simple case where all arcs to/from a node define a fixed angle, which does not change during reflection because Poincar\u00e9 model is conformal. Hence, if the tree is binary, this angle is $2\\pi/3$ , which provides a very clean display of the tree. In our case however, some children may have just one arc to a leaf while others may support big subtrees. Also, arc lengths can vary substantially. We thus design the region of the disk into which the subtrees are going to be embedded by choosing an angle proportional to the number of leaves reachable from the node, and of course lengths have to match the difference between absolute confidence between a node and its children. There is no optimization step to learn a clean embedding, so we rely on a set of hyperparameters to effectively compute this new step of the algorithm. ", "page_idx": 22}, {"type": "image", "img_path": "n60xBFZWrk/tmp/3882153d22a6d5be64949d31a5666504c3808a6beac295407585c7e1bb6dba18.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Output: Classifier $\\begin{array}{r}{\\pmb{\\mathscr{u}}_{T}\\doteq\\sum_{j}\\alpha_{j}H_{j}(.)}\\end{array}$ ; ", "page_idx": 23}, {"type": "text", "text": "The link between distances in the Poincar\u00e9 model of hyperbolic geometry and the canonical link of the log-loss (3) can be extended to leveraging coefficients in boosted combinations [31]. To get there, we need a tailored boosting algorithm which parallels AdaBoost\u2019s design tricks (hence different from LogitBoost [13]). We want a boosting algorithm for the liner combination of DTs which displays classification that can be easily and directly embedded in the Poincar\u00e9 disk. Algorithm LOGISTICBOOST is provided above. For the weight update, we refer to [23]. We already know how to embed DTs via their Monotonic DTs. What a boosting algorithm of this kind does is craft ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{{\\cal H}}_{T}\\quad\\doteq\\quad\\sum_{j=1}^{T}u_{j}(.),\\quad u_{j}({\\pmb x})\\doteq\\alpha_{j}\\cdot H_{j}({\\pmb x});\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Remark that we have merged the leveraging coefficient and the DTs\u2019 outputs $H.$ , on purpose. To compute the leveraging coefficients $\\alpha$ . in Step 4, we use AdaBoost\u2019s secant approximation trick\u2020, applied not to the exponential loss but to the logistic loss: for any $z\\in[-R,R]$ and $\\alpha\\in\\mathbb{R}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\log(1+\\exp(-\\alpha z))~~\\leqslant~~~{\\frac{1+u}{2}}\\cdot\\log(1+\\exp(-\\alpha R))+{\\frac{1-u}{2}}\\cdot\\log(1+\\exp(\\alpha R)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, to compute the leveraging coefficient $\\alpha_{j}$ that approximately minimizes the current loss, $\\begin{array}{r}{\\sum_{i}w_{j i}\\log(1+\\operatorname{exp}(-\\alpha y_{i}H_{j}(\\Bar{\\mathbf{x}}_{i})))}\\end{array}$ , we minimize instead the upperbound using (39). Letting ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{(\\psi_{\\mathrm{LoG}})_{j}^{*}}&{{}\\doteq}&{\\displaystyle\\operatorname*{max}_{\\lambda\\in\\Lambda(H_{j})}\\left\\vert\\log\\left(\\frac{p_{j\\lambda}^{+}}{1-p_{j\\lambda}^{+}}\\right)\\right\\vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(note the slight abuse of notation for readability, since we write $(\\psi_{\\mathrm{LoG}})_{j}^{*}$ instead of $|(\\psi_{\\mathrm{LOG}})_{j}^{*}|$ ; we remind that $\\Lambda(.)$ denotes the set of leaves of a tree; the index in the local proportion of positive example $p_{j\\lambda}^{+}$ reminds that weights used need to be boosting\u2019s weights) which we note can be directly approximated on the Poincar\u00e9 disk by looking at the leaf nearest to the border of the disk. Because the maximal absolute confidence in the DT is also the maximal absolute confidence in its MDT, we obtain the sought minimum, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\alpha_{j}}&{=}&{\\displaystyle\\frac{1}{(\\psi_{\\mathrm{{LoG}}})_{j}^{*}}\\cdot\\log\\left(\\frac{1+r_{j}}{1-r_{j}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $r_{j}\\in[-1,1]$ is the normalized edge ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{r_{j}}&{\\doteq}&{\\displaystyle\\frac{1}{\\sum_{i}w_{j i}}\\cdot\\sum_{i\\in[m]}w_{j i}\\cdot\\frac{y_{i}H_{j}(x_{i})}{\\operatorname*{max}_{k}|H_{j}(x_{k})|}}\\\\ &{=}&{\\displaystyle\\mathbb{E}_{i\\sim\\tilde{w}_{j}}\\left[\\frac{1}{\\left(\\psi_{\\mathrm{LoG}}\\right)_{j}^{*}}\\cdot\\log\\left(\\frac{p_{j\\lambda(x_{i})}^{+}}{1-p_{j\\lambda(x_{i})}^{+}}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "\u2020Explained in [31, Section 3.1]. ", "page_idx": 23}, {"type": "image", "img_path": "n60xBFZWrk/tmp/d9876828c9b8d4dbcfc426ac468150f0302197a709873af4a935631e9e84b099.jpg", "img_caption": ["Figure II: One MDT learned on UCI domain online_shoppers_intention reproducing the left of Figure 2 in the main file, with additional details for leveraging coefficients. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "where $\\tilde{\\pmb{w}}_{j}$ indicates normalized weights. It is not hard to show that because we use the local posterior $p_{j\\lambda}^{+}$ at each leaf, $\\alpha_{j}\\geqslant0$ and also $r_{j}\\geqslant0$ . Hence, everything is like if we had an imaginary node $\\nu_{j}$ with $p_{j\\nu_{j}}^{+}\\doteq(1+r_{j})/2\\r\\left(\\geqslant1/2\\right)$ and positive confidence ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\psi_{\\mathrm{{LoG}}})_{j}\\;\\;\\;\\doteq\\;\\;\\;\\psi_{\\mathrm{{LoG}}}(p_{j\\nu_{j}}^{+})=\\log\\left(\\frac{p_{j\\nu_{j}}^{+}}{1-p_{j\\nu_{j}}^{+}}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "that we can display in Poincar\u00e9 disk (we choose to do it as a circle; see Figure $\\mathrm{II}$ , main file). We deduce from (38) that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{u_{j}(\\pmb{x})}&{=}&{\\frac{\\left(\\psi_{\\mathrm{LoG}}\\right)_{j}}{\\left(\\psi_{\\mathrm{LoG}}\\right)_{j}^{*}}\\cdot\\log\\left(\\frac{p_{j\\lambda(\\pmb{x})}^{+}}{1-p_{j\\lambda(\\pmb{x})}^{+}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and note that all three key parameters can easily be displayed or computed directly from the Poincar\u00e9 disk: ", "page_idx": 24}, {"type": "text", "text": "\u2022 If we use MDTs directly for prediction, $\\begin{array}{r}{\\log\\left(\\frac{p_{j\\lambda(\\mathbf{x})}^{+}}{1-p_{j\\lambda(\\mathbf{x})}^{+}}\\right)=H_{j}(\\mathbf{x})}\\end{array}$ is read directly from the MDT\u2019s plot; the absolute value can be removed and the true sign guessed from the node\u2019s color (or shape); if se use DTs for prediction, the MDT\u2019s plot gives an approximation of the DT\u2019s output which is $100\\%$ accurate for all leaves of the MDT (each corresponds to a leaf in the DT);   \n\u2022 If $H_{j}$ is leveraged (in an ensemble), $(\\psi_{\\mathrm{LoG}})_{j}^{*}$ is read directly from the plot (40), as well as $(\\psi_{\\mathrm{LoG}})_{j}$ . Their ratio gives the leveraging coefficient. We note that $(\\psi_{\\mathrm{LoG}})_{j}^{*}$ is the same for both the DT and its corresponding MDT. Only the value of $(\\psi_{\\mathrm{LoG}})_{j}$ can differ for the DT and the MDT. ", "page_idx": 24}, {"type": "text", "text": "We note that regardless of whether we use the DT or its MDT to classify, the MDT\u2019s representation is also fully accurate for the DT for a subset of its leaves. ", "page_idx": 24}, {"type": "text", "text": "D Supplementary material on experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.I Domains ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The domains we consider are all public domains, from the UCI repository of ML datasets [12], OpenML, or Kaggle, see Table I. ", "page_idx": 24}, {"type": "text", "text": "D.II Visualizing a DT via its MDT and embedding errors ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "All test errors are estimated from a 10-fold stratified cross-validation. One can always choose to directly learn Monotonic DTs instead of DTs, given their natural fit for embedding in the Poincar\u00e9 disk. In this case, the hyperbolic representation of the MDT can be immediately used for assessment. Suppose we stick to learning DTs (because e.g. they have been standard in ML for decades) and wish to use the visualization of its corresponding MDT to make inferences on the original DT itself. Can we make reliable conclusions? We explore this question by observing that the prediction from DT to MDT can only change if the leaf $\\lambda$ that an example reaches in the DT satisfies two conditions: (a) $\\lambda$ does not appear as a leaf in the MDT, and (b) it is tagged to an internal node of the MDT with a confidence of the opposite sign (this is (D1), Step 3: in GETMDT). Without tackling the formal aspect of this question, we have designed a simple experiment for a simple assessment of whether / when this is reasonable. We have trained a set of $T{=}200$ boosted DTs, each with maximal size 200 (total number of nodes). After having computed the corresponding MDTs, we have compared the test errors of the boosted set of trees and that of the ensemble where each DT is replaced by its MDT, but the leveraging coefficients do not change. Intuitively, if test errors are on par, the variation in classification of DTs vs MDTs (including confidences) is negligible, and we can \u201creduce\" the interpretation of the DT to that of its MDT in the Poincar\u00e9 disk. The results are summarized in Table II. ", "page_idx": 24}, {"type": "table", "img_path": "n60xBFZWrk/tmp/8a901245db923e8cf86dd087f513d5869f110afa57da03c6302f27ba265ab17f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "n60xBFZWrk/tmp/390ad4c6dd5884fd5b9250fc18704b25aa66fc11d7102890ee2a7a070050f7a4.jpg", "table_caption": ["Table I: UCI, OpenML (Analcatdata_supreme) and Kaggle (Give_me_some_credit) domains considered in our experiments ( ${\\bf\\zeta}^{m}=$ total number of examples, $d=$ number of features), ordered in increasing $m\\times n$ . Datset licenses listed in last column. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "From Table II, we can safely say that our hypothesis reasonably holds in many cases, with two important domains for which it does not: ionosphere and hillnoise. For the former domain, we attribute it to the small size of the domain, which prevents training big enough trees; for the latter domain, we attribute it to the fact that the domain contains substantial noise, which makes it difficult to substantially improve posteriors by splitting and thus make many DT nodes, including leaves, \u201cdisappear\" in the MDT conversion (Steps 2: and 14: in GETMDT). Table III summarizes the visualization errors of the MDT embeddings obtained in Tables IV - XII. A key feature of the embeddings is that on some domains (e.g. german) the error of the embedding dramatically decreases with the index of the MDT in the ensemble. The reason why this happens is simple: the weight update of boosting tends to equalize weights between classes (to make the problem \"harder\"); thus, new trees tend to have their roots embedded closer to the origin, which makes the embedding easier for the rest of the trees to keep good visualization at low error. ", "page_idx": 25}, {"type": "table", "img_path": "n60xBFZWrk/tmp/54fb84aac78f1e8ed9fc5708e91a0e277483ea35992023e772ffd58c4c82db4e.jpg", "table_caption": [], "table_footnote": ["Table III: Embedding error $\\rho$ (5) for the visualization of the first ten MDTs in an ensemble learned. "], "page_idx": 26}, {"type": "image", "img_path": "n60xBFZWrk/tmp/2a669aa7a03459673a2491741fdca93572f9fb940ec3ef4a2db53bdf58b8da43.jpg", "img_caption": ["Table IV: First 10 Monotonic Decision Trees (MDTs) embedded in Poincar\u00e9 disk, corresponding to several Decision Trees (DTs) with $\\leqslant200$ nodes each, learned by boosting the log / logistic-loss on UCI online_shoppers_intention. Isolines correspond to the node\u2019s prediction confidences, and geometric embedding errors $(\\rho\\%)$ are indicated. See text for details. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "D.III All Poincar\u00e9 disk embeddings ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "They are presented in Table IV through to XII, showing the first models learned in a fold of the cross-validation experiments. producing Table II ", "page_idx": 26}, {"type": "text", "text": "D.IV Interest of the t-self for visualization ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We now test the experimental impact of switching to the t-self in Poincar\u00e9 disk model (See Table XIII for a clear depiction of how changing $t$ can yields better visualization close to the border of the disk). Recall that switching to the t-self moves way model parts close to $\\partial\\mathbb{B}$ but keeps a low non-linear distortion around the center, which is thus roughly only affected by a scaling factor. Figure I presents a few results. For domain online_shoppers_intention, we note that the part of the tree that is within the isoline defined by posterior $p_{.}^{+}\\in\\{0.1,0.9\\}$ gets indeed just scaled: both plots look quite identical. Very substantial differences appear near the border: the best parts of the model could easily be misjudged as equivalent from $\\mathbb{B}$ alone (orange rectangle) but there is little double from $\\mathbb{B}_{1}^{(t)}$ that one of them, which crosses the $p_{.}^{+}\\in\\{0.001,0.009\\}$ isoline, is in fact much better than the others. When many subtrees seem to be aggregating near the border as in buzz_in_social_media, stark differences can appear on the t-self: the best subtrees are immediately spotted from the t-self (orange rectangles). In between, the t-self makes a much more visible ordering between the best nodes and subtrees, compared to Poincar\u00e9 disk. hardware demonstrate that such very good nodes that are hard to differentiate from the others in $\\mathbb{B}$ can appear at any iteration. ", "page_idx": 26}, {"type": "image", "img_path": "n60xBFZWrk/tmp/92fec3a8f2595acf405222524faeec46d72c3d41d5ededf50ec30e74981d836e.jpg", "img_caption": ["Table V: First 10 MDTs for UCI analcatdata_supreme. Convention follows Table IV. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "n60xBFZWrk/tmp/b9849538824903d7b28a28fb0e81c5757fbbbfb08cfc190c18d4dded019d6d31.jpg", "img_caption": ["Table VI: First 10 MDTs for UCI abalone. Convention follows Table IV. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "n60xBFZWrk/tmp/c707a76afc59e6822d10dda63b4977cccdc11ff4b683918630265e1d9e2a03ed.jpg", "img_caption": ["Table VII: First 10 MDTs for UCI winered. Convention follows Table IV. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "n60xBFZWrk/tmp/93ad8949c68c62be85a0fae26b1944c1ffe7aae2c999ad44548d3f6864ba1fca.jpg", "img_caption": ["Table VIII: First 10 MDTs for UCI qsar. Convention follows Table IV. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "n60xBFZWrk/tmp/c99a384e08af711bf588596d4eaa1fa7fb42093054ba22930e5021aa8de7dce4.jpg", "img_caption": ["Table IX: First 10 MDTs for UCI german. Convention follows Table IV. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "n60xBFZWrk/tmp/daabd57f46715605702da48ce32bb8fff27dacf2f288f5d20866e5013662308e.jpg", "img_caption": ["Table X: First 10 MDTs for UCI ionosphere. Convention follows Table IV. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "n60xBFZWrk/tmp/4c2fb365760b597874b33b02b792b32a0fd46b7f650a64302db85458cfdf34ae.jpg", "img_caption": ["Table XI: First 10 MDTs for UCI hardware. Convention follows Table IV. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "n60xBFZWrk/tmp/e7b0b63555850fcc68c19cb1b84fd6b8d722ed15cb4713695f5987fc3704c048.jpg", "img_caption": ["Table XII: Plot of the 10 first MDTs learned on kaggle give_me_some_credit (top panel and bottom panel). In each panel, we plot the embedding in $\\mathbb{B}$ (top row) and the t-self $\\mathbb{B}_{1}^{(0)}$ ${t=0}$ , bottom row). Remark the ability for the t-self to display a clear difference between the best subtrees, subtrees that otherwise appear quite equivalent in terms of confidence from $\\mathbb{B}$ alone. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "n60xBFZWrk/tmp/6ce8c157cb1d1ab6a6115e24f3d73bc24277b6c0ad5466253d7812d686e703c7.jpg", "img_caption": ["Table XIII: T-self $\\overline{{\\mathbb{B}_{1}^{(t)}}}$ for Poincar\u00e9 disk $\\mathbb{B}$ (left), for several $t\\mathbf{s}$ . We plot the same set of isolines of $\\mathbb{B}$ (and their mapping via ${\\mathfrak{p}}_{t.}$ ), parameterized by probability $p\\in[0,1]$ which gives the norm $r\\doteq|2p-1|$ in $\\mathbb{B}$ (From the innermost to the outermost, $p~\\in~\\{0.6,0.7,0.8,0.9,0.999\\}$ , or by symmetry for $p^{\\prime}\\doteq1-p$ , also indicated). Remark the equidistance of isolines in $\\mathbb{B}$ , approximately kept in $\\mathbb{B}_{1}^{(t)}$ for $p$ up to 0.9 $(p^{\\prime}=0.1)$ , while the distortion we need clearly happens near the border: outermost isoline $p\\in\\{0.001,0.999\\}$ (plotted with bigger width) is smoothly and substantially \u201cmoved\" within the t-self as $t$ decreases, guaranteeing good readability and coding convenience (see text). "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "n60xBFZWrk/tmp/c4bfe17d7900f8c83e77482c49186cbf929b6783e13abdf518c337bbae62a622.jpg", "img_caption": ["Figure I: Top pane: comparison between Poincar\u00e9 disk embedding and its t-self for $t=0$ for an MDT learned on UCI online_shoppers_intention (left) and twitter (right). On the left panel, we have not plotted the boosting coefficients information. The isoline distinguished in magenta is the big width one in Table XIII. Note the difference in (non-linear) distortion created in the t-self, in which the central part just enjoys a scaling. Bottom pane: stark differences in visualization between $\\mathbb{B}$ and the t-self do not just appear initially: they can appear at any iteration. We display two MDTs learned on UCI hardware at two different iterations (indicated). It would be very hard to differentiate the best leaves from just the Poincar\u00e9 disk embedding, while it becomes obvious from the t-self (orange arrows). Note also the set of red nodes in the Poincar\u00e9 disk for $t=70$ that mistakenly look aligned, but not in the t-self (blue rectangle). See text for details. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 31}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 31}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 31}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 31}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 31}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Claims are supported by both the theoretical results presented in the paper or experimental exploration. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Limitations of the proposed embedding method are discussed. For instance, the requirement of only embedding the MDT are discussed. Impacts on classification due to this are explored in the \u201cMDT vs DT classification\u201d discussion, with further details in Appendix. A last paragraph in the conclusion also discusses limitations. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Assumptions are detailed in formal results and full proofs are provided in the Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Detailed settings of constructing visualizations are presented. Code provided with an example public domain and a resource file $^+$ README.txt for quick testing and validation. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Code provided with an example public domain and a resource file $^+$ README.txt for quick testing and validation. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Details are provided in main text and Appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Error is stated whenever suitable. Multiple results are presented for visualizations. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] .   \nJustification: The code runs on any standard computer. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The research of the paper follows the code of ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Broader impact and societal impact is briefly mentioned throughout the main text, with further discussion presented in an Appendix section. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No release of data or models. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Appropriate credit / referencesse are provided. Licenses listed in Appendix. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No new assets provided. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: No research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]