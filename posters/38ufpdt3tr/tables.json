[{"figure_path": "38UFpdt3Tr/tables/tables_5_1.jpg", "caption": "Table 1: Relative downstream performance of D2DMoE and MoEfication on BoolQ dataset. Our method only starts to degrade at around 70% compute budget, while MoEfication gradually decreases.", "description": "This table shows the relative downstream performance of D2DMoE and MoEfication on the BoolQ dataset at various compute budgets (100%, 90%, 80%, 70%, 60%, 50%, 25%, 10%).  It highlights that D2DMoE maintains high performance even at significantly reduced compute budgets (down to 70%), whereas MoEfication's performance degrades more substantially as the compute budget decreases.", "section": "4.4 Execution Latency"}, {"figure_path": "38UFpdt3Tr/tables/tables_16_1.jpg", "caption": "Table 2: Wall-clock time measurements (\u00b5s) of execution of our D2DMoE layer when using different data types and GPUs.", "description": "This table presents the wall-clock time measurements in microseconds (\u00b5s) for the execution of a single D2DMoE layer on two different GPUs (RTX 4090 and A100).  The measurements are categorized by data type (float32, float16, int8) and the probability (p) of expert execution, ranging from 0.0 to 1.0.  The table illustrates the impact of data type and expert execution probability on the layer's execution speed.", "section": "4.4 Execution latency"}]