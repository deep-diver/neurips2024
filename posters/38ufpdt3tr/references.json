{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is fundamental to the work described in the current paper."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-11", "reason": "BERT is a highly influential language model that serves as a basis for many of the experiments in the current paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-01-01", "reason": "Vision Transformer (ViT) is a foundational model for computer vision, used in multiple experiments in this paper."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-01", "reason": "This paper introduced the Mixture-of-Experts (MoE) layer, a core concept upon which this paper builds."}, {"fullname_first_author": "Zhengyan Zhang", "paper_title": "Moefication: Transformer feed-forward layers are mixtures of experts", "publication_date": "2022-07-01", "reason": "This paper introduced the MoEfication technique, which is directly compared against in this paper."}]}