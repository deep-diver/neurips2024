[{"figure_path": "lxSmLxlVks/figures/figures_1_1.jpg", "caption": "Figure 1: Experiment results of perplexity \u2193 on WikiText2 dataset with 2048 sequence length.", "description": "This figure presents the perplexity results on the WikiText2 dataset with a sequence length of 2048.  It compares the performance of the proposed method against several state-of-the-art (SOTA) baselines across different LLM families (OPT and LLaMA) and varying model sizes. The x-axis represents the inheriting ratio, and the y-axis shows the perplexity. Lower perplexity indicates better performance.  The figure demonstrates the superior performance of the proposed method across various models and inheriting ratios, showcasing its effectiveness in generating efficient subnets.", "section": "1 Introduction"}, {"figure_path": "lxSmLxlVks/figures/figures_2_1.jpg", "caption": "Figure 2: Framework Overview", "description": "This figure illustrates the three main stages of the proposed framework for efficient large language model (LLM) compression. Stage 1 (Initialization): An initial architecture is constructed based on the weight importance of the original LLM, with weights inheriting a certain ratio. Stage 2 (Search): An evolutionary search process is used to identify the globally efficient architecture/subnet within the original LLM, which includes mask mutation, crossover, and candidate evaluation. Stage 3 (Reformation): A reformation algorithm refines the inherited weights from the original LLM to improve performance using the omitted weights and a small amount of calibration data.", "section": "3 Methodology"}, {"figure_path": "lxSmLxlVks/figures/figures_3_1.jpg", "caption": "Figure 3: Visualization of the subnets generation for LLaMA family based on the selections masks Sattn for the self-attention module colored in blue and Smlp for the MLP module colored in green.", "description": "This figure illustrates the process of generating subnets for the LLaMA family of LLMs. It shows how the selection masks, Sattn (for self-attention modules) and Smlp (for MLP modules), are used to identify specific rows or columns within the original weight matrices that are then kept, while others are omitted, creating smaller subnets.  The colors (blue and green) highlight the areas where the masks apply. The figure also shows how the structural subnets (rows or columns) are searched using a row or column search mechanism. This allows for a training-free approach to model compression by directly modifying the architecture without retraining.", "section": "3.3 Architecture Search"}, {"figure_path": "lxSmLxlVks/figures/figures_5_1.jpg", "caption": "Figure 4: Ablation analysis of the inheriting ratios applied to the self-attention, MLP, or both.", "description": "This figure presents an ablation study on the impact of different inheriting ratios applied to various parts of the model architecture. It shows perplexity results and the ratio of parameters inherited in three scenarios: self-attention only, MLP only, and both modules.  The goal is to determine which parts of the LLM benefit most from reduced parameter counts and to investigate the effects of a non-uniform ratio across the model.", "section": "3.3 Architecture Search"}, {"figure_path": "lxSmLxlVks/figures/figures_5_2.jpg", "caption": "Figure 5: Ablation analysis of convergence speed with or without our initialization.", "description": "This figure displays an ablation study comparing the convergence speed of the proposed architecture search method with and without initialization.  The y-axis represents the log2 perplexity (a measure of model performance), and the x-axis shows the epoch number (iterations of the search process).  Different lines represent different scenarios: using initialization with self-attention only, MLP only, both self-attention and MLP, and the results without any initialization. The results demonstrate the effectiveness of the proposed initialization strategy in speeding up convergence and achieving lower perplexity.", "section": "3.3 Architecture Search"}, {"figure_path": "lxSmLxlVks/figures/figures_8_1.jpg", "caption": "Figure 6: Ablation analysis for reformation with different numbers of samples.", "description": "The figure shows the ablation study on the reformation process using different numbers of samples.  The x-axis represents the inheriting ratio, and the y-axis shows the perplexity.  The bars illustrate the perplexity results with no reformation (w/o reform), and reformation using 128, 512, and 1024 samples, respectively. The results demonstrate that reformation improves the perplexity score, and that using more samples does not dramatically improve the performance past a certain number of samples.", "section": "4.3 Ablation Study"}, {"figure_path": "lxSmLxlVks/figures/figures_8_2.jpg", "caption": "Figure 7: Analysis for memory and generation speed of LLaMA-7B on NVIDIA A100 40G.", "description": "This figure shows the memory consumption and inference speed of the LLaMA-7B model on an NVIDIA A100 40G GPU, for different inheriting ratios (100%, 90%, 80%, 70%, 60%, 50%).  It demonstrates that reducing the inheriting ratio leads to lower memory usage and faster generation speed. The trade-off between model size and performance is clearly visualized.", "section": "4.4 Generation Acceleration"}, {"figure_path": "lxSmLxlVks/figures/figures_14_1.jpg", "caption": "Figure 1: Experiment results of perplexity \u2193 on WikiText2 dataset with 2048 sequence length.", "description": "This figure displays the perplexity results for various LLMs (OPT and LLaMA families) on the WikiText2 dataset using sequences of length 2048.  Different methods for model compression are compared, including SliceGPT, LLM-Pruner, FLAP, and the proposed 'Ours' method. The x-axis represents the percentage of weights retained in the model, and the y-axis represents the perplexity. Lower perplexity indicates better performance. The figure illustrates that the 'Ours' method consistently outperforms other methods across different model sizes and retention rates, achieving significantly lower perplexity scores.", "section": "1 Introduction"}, {"figure_path": "lxSmLxlVks/figures/figures_14_2.jpg", "caption": "Figure 1: Experiment results of perplexity \u2193 on WikiText2 dataset with 2048 sequence length.", "description": "This figure displays the perplexity results achieved by different methods on the WikiText2 dataset, using sequences of length 2048.  It compares the performance of the proposed method against several state-of-the-art (SOTA) baselines across four different LLM families (OPT and LLaMA) and various model sizes, illustrating the impact of varying the proportion of weights inherited from the original model. Lower perplexity indicates better performance.", "section": "1 Introduction"}]