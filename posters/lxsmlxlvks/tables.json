[{"figure_path": "lxSmLxlVks/tables/tables_4_1.jpg", "caption": "Table 2: Results of the compressed LLaMA-7B and LLaMA-13B on the WikiText2 dataset, PTB dataset, and other common sense reasoning datasets. The perplexity on the WikiText2 and PTB is calculated with the 2048 sequence length. The accuracy results are evaluated with the same pipeline as LLM-Pruner [8] to ensure a fair comparison. The average is computed across seven classification datasets. LLM-Pruner (v), (e2), and (el) denote the vector-wise and element-wise importance, (c) and (b) denote the channel and block strategies.", "description": "This table presents the results of compressed LLaMA-7B and LLaMA-13B models on several benchmark datasets.  It shows perplexity scores (lower is better) on WikiText2 and PTB, and accuracy scores (higher is better) across seven common sense reasoning datasets.  Different model compression methods (LLM-Pruner, SliceGPT, FLAP, and the proposed method) are compared at various weight inheritance ratios (e.g., 90%, 80%).  The table highlights the performance improvements achieved by the authors' method across various model sizes and datasets.", "section": "4.2 Main Results"}, {"figure_path": "lxSmLxlVks/tables/tables_4_2.jpg", "caption": "Table 1: Search space for different model sizes of OPT model family and LLaMA model family, where the notation [a, b, c] specifies a range from a to b with an interval of c.", "description": "This table presents the search space used in the evolutionary algorithm to find efficient subnetworks within various large language models (LLMs). It defines the range of values for three key parameters that define the architecture of the searched subnets:\n\n- Model depth: The number of layers in the LLM subnetwork. Separate ranges are defined for OPT and LLaMA model families to account for architectural differences.\n- Inheriting ratio for self-attention: The percentage of weights from the original LLM's self-attention module to be inherited into the subnetwork.\n- Inheriting ratio for MLP: The percentage of weights from the original LLM's Multi-Layer Perceptron (MLP) module to be inherited into the subnetwork. \n\nDifferent inheriting ratios are tested for different model sizes (125M, 1.3B, 2.7B parameters for OPT; 7B, 13B, 30B, 65B parameters for LLaMA).  The table provides the range for each parameter, enabling a systematic exploration of the architecture search space.", "section": "3.3 Architecture Search"}, {"figure_path": "lxSmLxlVks/tables/tables_6_1.jpg", "caption": "Table 2: Results of the compressed LLaMA-7B and LLaMA-13B on the WikiText2 dataset, PTB dataset, and other common sense reasoning datasets. The perplexity on the WikiText2 and PTB is calculated with the 2048 sequence length. The accuracy results are evaluated with the same pipeline as LLM-Pruner [8] to ensure a fair comparison. The average is computed across seven classification datasets. LLM-Pruner (v), (e2). and (el) denote the vector-wise and element-wise importance, (c) and (b) denote the channel and block strategies.", "description": "This table presents the results of compressing LLaMA-7B and LLaMA-13B language models using different methods, including the proposed method and several state-of-the-art baselines.  It shows the perplexity scores (a measure of how well the model predicts text) on the WikiText2 and Penn Treebank (PTB) datasets, along with accuracy scores on seven common sense reasoning datasets.  The comparison is made at different compression ratios (inheriting ratios), showing the trade-off between model size and performance.  Various pruning strategies within LLM-Pruner are also compared.", "section": "4.2 Main Results"}, {"figure_path": "lxSmLxlVks/tables/tables_7_1.jpg", "caption": "Table 2: Results of the compressed LLaMA-7B and LLaMA-13B on the WikiText2 dataset, PTB dataset, and other common sense reasoning datasets. The perplexity on the WikiText2 and PTB is calculated with the 2048 sequence length. The accuracy results are evaluated with the same pipeline as LLM-Pruner [8] to ensure a fair comparison. The average is computed across seven classification datasets. LLM-Pruner (v), (e2), and (el) denote the vector-wise and element-wise importance, (c) and (b) denote the channel and block strategies.", "description": "This table presents the results of compressing two large language models, LLaMA-7B and LLaMA-13B, using different methods.  It shows the perplexity scores (a measure of how well the model predicts text) on two benchmark datasets, WikiText2 and PTB, as well as accuracy scores on seven common sense reasoning datasets. The table compares the performance of the proposed method against three state-of-the-art baselines (LLM-Pruner, SliceGPT, and FLAP) across different weight inheriting ratios (the percentage of original weights kept in the compressed model). Different pruning strategies within LLM-Pruner are also compared.", "section": "4.2 Main Results"}, {"figure_path": "lxSmLxlVks/tables/tables_8_1.jpg", "caption": "Table 6: Results of extra large LLaMA models.", "description": "This table presents the results of applying the proposed method to extra-large LLaMA models (30B and 65B parameters).  It shows the perplexity scores achieved on the Wiki and PTB datasets at various inheriting ratios (100%, 90%, 80%, 70%, 60%, 50%). The results are compared against the FLAP baseline, demonstrating the superior performance of the proposed method even with larger models and lower inheriting ratios.", "section": "4.2 Main Results"}, {"figure_path": "lxSmLxlVks/tables/tables_8_2.jpg", "caption": "Table 7: LLaMA-7B perplexity (\u2193) results on WikiText2 dataset with 128 sequence length.", "description": "This table presents the perplexity results for the LLaMA-7B model on the WikiText2 dataset using sequences of length 128.  The results are shown for different inheriting ratios (90%, 80%, 70%, 60%, and 50%), comparing the performance of the proposed method against several baselines: LLM-Pruner(el), SliceGPT, and FLAP.  Lower perplexity values indicate better performance.", "section": "4.3 Ablation Study"}, {"figure_path": "lxSmLxlVks/tables/tables_14_1.jpg", "caption": "Table A1: Compare with SliceGPT using LLaMA-7B perplexity (\u2193) results on PTB dataset.", "description": "This table compares the perplexity results of SliceGPT and the proposed method on the PTB dataset using the LLaMA-7B model.  It demonstrates the impact of different calibration datasets (PTB and WikiText2) on the performance of SliceGPT, highlighting that SliceGPT's performance is sensitive to the choice of calibration dataset. The proposed method shows consistently better performance across varying inheriting ratios, regardless of the calibration dataset used. This emphasizes the robustness and effectiveness of the proposed approach.", "section": "Appendix C SliceGPT Comparison"}, {"figure_path": "lxSmLxlVks/tables/tables_14_2.jpg", "caption": "Table A2: LLaMA-7B perplexity (\u2193) results on different datasets of search and evaluation.", "description": "This table presents the perplexity scores achieved by the LLaMA-7B model on WikiText2 and Penn Treebank (PTB) datasets, using different combinations of datasets for search and evaluation, along with varying inheriting ratios.  The perplexity is a measure of how well the model predicts the next word in a sequence, with lower scores indicating better performance.  The table shows that the model's performance varies depending on which dataset was used during the search phase, and that a lower inheriting ratio generally leads to worse performance.", "section": "Appendix E Ablation for 128 Sequence Length"}, {"figure_path": "lxSmLxlVks/tables/tables_15_1.jpg", "caption": "Table A3: LLaMA-13B perplexity (\u2193) results on WikiText2 dataset with 128 sequence length.", "description": "This table presents the perplexity results for the LLaMA-13B model on the WikiText2 dataset using different inheriting ratios (90%, 80%, 70%, 60%, 50%).  The perplexity, a measure of how well the model predicts the next word, is compared across four methods: LLM-Pruner(e1), SliceGPT, FLAP, and the authors' proposed method. Lower perplexity indicates better performance.  The table shows that the proposed method achieves lower perplexity than the baselines across all inheriting ratios, demonstrating its superior performance in generating text even with short sequence lengths.", "section": "E Ablation for 128 Sequence Length"}]