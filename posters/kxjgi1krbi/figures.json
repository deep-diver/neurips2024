[{"figure_path": "KxjGi1krBi/figures/figures_2_1.jpg", "caption": "Figure 1: Demonstration of how the proposed framework traverses the combinatorial graph \u011c<k> introduced in \u00a73.1 with an exemplar original graph G of 6 nodes and a subset size of k = 2. At iteration t, we first construct a local combo-subgraph Gt = {Vt, \u0112t} of size Q=6 using Algorithm 1 (\u00a73.1), which is centred at combo-node 1 from last iteration t-1 or initialization. Next, a GP surrogate is fitted on Gt with queried combo-nodes inside Gt being the training set. The next query location is then selected as the combo-node that maximizes the acquisition function v = arg max\u03b5\u03bd, \u03b1(\u03bd). If queried values f(x) \u2265 f(-1), the next combo-subgraph Gt+1 will be re-sampled at a new center \u00fbt, or otherwise remain the same. Finally, we repeat the previous process to obtain a new query location for the next iteration t+1, and the search continues until stopping criteria are triggered.", "description": "This figure illustrates the process of the proposed framework traversing a combinatorial graph.  It shows how a local combo-subgraph is constructed, a Gaussian Process (GP) surrogate model is fitted, and the acquisition function is used to select the next node to query. The process iteratively refines the search until a stopping criterion is met.", "section": "3 BO of Functions over Node Subsets in Graphs"}, {"figure_path": "KxjGi1krBi/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of a combinatorial graph \u011c<2> constructed by the recursive combo-subgraph sampling (Algorithm 1).", "description": "This figure illustrates how the recursive combo-subgraph sampling algorithm constructs a combinatorial graph.  The original graph (a small example graph is shown) is transformed into a combinatorial graph where nodes represent k-node subsets (k=2 in this case).  Edges connect subsets that differ by only one node, and those nodes must be adjacent in the original graph. The algorithm recursively samples subgraphs, starting from a central combo-node, expanding by hops, until a size limit is reached. Different line styles represent different hops from the central node. This process is central to GraphComBO's efficient traversal of the large search space.", "section": "3.1 The Combinatorial Graph for Node Subsets"}, {"figure_path": "KxjGi1krBi/figures/figures_6_1.jpg", "caption": "Figure 3: Results for synthetic problems on BA, WS, SBM and 2D-Grid networks with k = [4, 8, 16, 32], where Regret indicates the difference between ground truth and the best query so far.", "description": "This figure displays the results of four different synthetic experiments on four different types of random graphs (Barab\u00e1si-Albert, Watts-Strogatz, Stochastic Block Model, and 2D grid).  Each experiment aims to find the best subset of k nodes (k=4,8,16,32) that maximizes a specific function (eigen centrality, degree centrality, PageRank scores, and Ackley function).  The y-axis represents the \"Regret\", which is the difference between the optimal value of the objective function and the value found by the algorithm at each iteration.  The x-axis shows the number of queries made by the algorithm. Several algorithms (GraphComBO, k-Random Walk, k-Local Search, Local Search, BFS, DFS, and Random) are compared, and their performance is assessed based on the Regret values.", "section": "Experiments"}, {"figure_path": "KxjGi1krBi/figures/figures_7_1.jpg", "caption": "Figure 3: Results for synthetic problems on BA, WS, SBM and 2D-Grid networks with k = [4, 8, 16, 32], where Regret indicates the difference between ground truth and the best query so far.", "description": "This figure presents the results of four synthetic experiments, each using a different type of random graph (Barab\u00e1si-Albert, Watts-Strogatz, Stochastic Block Model, and 2D grid) and a different objective function.  Each experiment varies the size (k) of the node subsets to be optimized (4, 8, 16, 32). The y-axis shows the regret, which represents the difference between the optimal value found by the algorithm and the actual optimal value. The x-axis represents the number of queries made to the black-box function.  The figure compares the performance of GraphComBO against several baseline algorithms (k-Random Walk, BFS, DFS, Random, k-Local Search, Local Search).", "section": "Experiments"}, {"figure_path": "KxjGi1krBi/figures/figures_8_1.jpg", "caption": "Figure 3: Results for synthetic problems on BA, WS, SBM and 2D-Grid networks with k = [4, 8, 16, 32], where Regret indicates the difference between ground truth and the best query so far.", "description": "This figure displays the results of experiments on four different types of synthetic graph networks (Barab\u00e1si-Albert, Watts-Strogatz, Stochastic Block Model, and 2D grid).  Each network was tested with four different subset sizes (k = 4, 8, 16, 32).  The y-axis shows the regret, which represents the difference between the optimal value found and the true optimal value.  The x-axis represents the number of queries made during the optimization process. Different optimization methods were compared: GraphComBO, k-random walk, BFS, DFS, random, k-local search and local search. The results show how the regret decreases over the number of queries for each method and network type, illustrating the effectiveness of the proposed GraphComBO method.", "section": "Experiments"}, {"figure_path": "KxjGi1krBi/figures/figures_15_1.jpg", "caption": "Figure 6: Visualization of random graphs and underlying functions used in our synthetic experiments. Specifically, node color represents eigenvector centrality on BA network, degree centrality on WS network, PageRank on SBM network, and Ackley function on 2D-Grid. Note that under the synthetic settings, we take the average over the k elements within a node subset as the underlying function.", "description": "This figure visualizes four different types of random graphs used in the synthetic experiments of the paper: Barab\u00e1si-Albert (BA), Watts-Strogatz (WS), Stochastic Block Model (SBM), and 2D-grid. Each graph is color-coded to represent the value of a specific function (eigenvector centrality, degree centrality, PageRank, and Ackley function respectively).  The color intensity represents the magnitude of the function value at each node.  In the synthetic experiments, the average function value over a subset of k nodes is used as the final underlying function for optimization.", "section": "B.1 Synthetic Experiments"}, {"figure_path": "KxjGi1krBi/figures/figures_16_1.jpg", "caption": "Figure 7: Visualization of the real-world networks used in our experiments.", "description": "This figure visualizes four real-world networks used in the paper's experiments.  Each image shows a different network's structure, illustrating the diversity of graph types considered in the study. These networks are used to demonstrate the generalizability of the proposed GraphComBO framework to a range of real-world scenarios.", "section": "Experiments"}, {"figure_path": "KxjGi1krBi/figures/figures_17_1.jpg", "caption": "Figure 8: Demonstration of SIR and its simulations on the real-world proximity contact network.", "description": "This figure demonstrates the results of SIR simulations on a real-world proximity contact network.  It compares two scenarios: (a) randomly protecting 20 nodes and (b) protecting 20 nodes identified by the GraphComBO algorithm. The plots show the number of individuals in each status (susceptible, infected, recovered) over time, along with histograms showing the distribution of the time it takes for 50% of the population to become infected (t*). The results indicate that GraphComBO is more effective at delaying the time it takes for 50% of the population to become infected compared to random node selection, demonstrating its effectiveness in flattening the curve of an epidemic.", "section": "B.2 Flattening the curve in Epidemics"}, {"figure_path": "KxjGi1krBi/figures/figures_18_1.jpg", "caption": "Figure 9: Demonstration of the patient-zero tracing setting, where the underlying contact network is not fully observed initially. To gradually reveal the graph structure, at each step t, we query k nodes (i.e., record the first times they are infected) and reveal their neighbors (e.g., interview the patients and obtain their contacts). The objective is to find the k patients of the earliest infection time.", "description": "This figure illustrates the process of identifying the earliest infected individuals (patient zero) in a partially observable network.  It shows how the network is incrementally revealed by querying k nodes at each time step (t0, t1, t2,...).  The querying process reveals the immediate neighbors of the queried nodes, expanding the known portion of the network. The goal is to identify the k nodes that were infected the earliest.", "section": "B.3 Tracing Patient-zero in the Community"}, {"figure_path": "KxjGi1krBi/figures/figures_18_2.jpg", "caption": "Figure 8: Demonstration of SIR and its simulations on the real-world proximity contact network.", "description": "This figure demonstrates the results of the SIR (Susceptible-Infected-Recovered) simulation model applied to a real-world proximity contact network.  Subfigures (a) and (b) show the results of randomly protecting 20 nodes, while (c) and (d) show the results when using the GraphComBO method to select 20 nodes for protection.  The plots show the number of individuals in each status (Susceptible, Infected, Recovered) over time, as well as the distributions of the time it takes for 50% of the population to become infected (t*). GraphComBO is shown to delay the time it takes to reach 50% infection (increasing the mean from 54 to 62.7) more effectively than random selection.", "section": "B.2 Flattening the curve in Epidemics"}, {"figure_path": "KxjGi1krBi/figures/figures_21_1.jpg", "caption": "Figure 11: Kernel validation on the combinatorial graph based on a BA network (n = 20, m = 2). To design the underlying function, we take the elements from the third eigenvector and average them over k = 3 nodes. Specifically, (a) shows the results on the testing data measured by Spearman's correlation coefficient \u03c1, and (b) shows the results when adding Gaussian noise to the ground truth.", "description": "This figure shows the result of kernel validation on a Barab\u00e1si-Albert network with 20 nodes and m=2. Four different kernels (Polynomial, Sum of inverse polynomial, Diffusion, Diffusion with ARD) are tested.  The underlying function is created by averaging the elements of the third eigenvector over subsets of 3 nodes.  The plots show the validation prediction vs. the ground truth, both without noise (a) and with added Gaussian noise (b). Spearman's rank correlation coefficient (\u03c1) is given for each case.", "section": "3.2 Graph Gaussian Processes Surrogate"}, {"figure_path": "KxjGi1krBi/figures/figures_22_1.jpg", "caption": "Figure 11: Kernel validation on the combinatorial graph based on a BA network (n = 20, m = 2). To design the underlying function, we take the elements from the third eigenvector and average them over k = 3 nodes. Specifically, (a) shows the results on the testing data measured by Spearman's correlation coefficient \u03c1, and (b) shows the results when adding Gaussian noise to the ground truth.", "description": "This figure displays the results of kernel validation on a Barab\u00e1si-Albert (BA) network with 20 nodes and 2 edges.  Two versions are shown: (a) without added noise and (b) with Gaussian noise added to the ground truth. Four different kernels (Polynomial, Sum of Inverse Polynomial, Diffusion, and Diffusion with ARD) are used and compared, with the results visualized using scatter plots and plots of the regularization functions.  The underlying function is derived by averaging the elements of the third eigenvector over subsets of 3 nodes. The Spearman's correlation coefficient (\u03c1) quantifies the correlation between the validation prediction and the ground truth for each kernel.", "section": "3.2 Graph Gaussian Processes Surrogate"}, {"figure_path": "KxjGi1krBi/figures/figures_22_2.jpg", "caption": "Figure 13: Smoothness of different underlying functions (average of different eigenvectors).", "description": "This figure visualizes the smoothness of different underlying functions used in the combinatorial space. The smoothness is evaluated by calculating the cumulative energy of Fourier coefficients, obtained via Graph Fourier Transform (GFT), of eigenvector signals from the original graph. The cumulative energy is then plotted against the eigenvalue index for different eigenvectors, demonstrating that functions corresponding to higher frequencies (larger eigenvalues) exhibit less smoothness.", "section": "F Kernel Performance under Different Signal Smoothness"}, {"figure_path": "KxjGi1krBi/figures/figures_23_1.jpg", "caption": "Figure 14: Performance (Spearman\u2019s rank-based correlation coefficient \u03c1) of kernels for underlying functions with different smoothness, where darker shades use eigenvectors of the higher index and thus indicate less-smooth functions.", "description": "This figure shows the performance of four different kernels (polynomial, polynomial_suminverse, diffusion, diffusion_ard) used in Gaussian Processes on functions with varying smoothness levels. The smoothness is controlled by using different eigenvectors from the graph Laplacian. Each boxplot shows the Spearman's rank correlation coefficient, \u03c1, for each kernel on different eigenvectors (2, 4, 8, 12, 16) representing different levels of smoothness. Darker shades indicate less-smooth functions.", "section": "F Kernel Performance under Different Signal Smoothness"}, {"figure_path": "KxjGi1krBi/figures/figures_23_2.jpg", "caption": "Figure 15: Behavior analysis of maximizing average eigenvector centrality on the BA network.", "description": "This figure presents a detailed behavioral analysis of the GraphComBO algorithm compared to other baselines on the task of maximizing average eigenvector centrality on Barab\u00e1si-Albert (BA) networks.  It shows the regret (difference between the obtained result and the optimal result), the size of the explored combo-graph, and the distance from the starting location for each algorithm. The analysis helps to understand how different algorithms explore and exploit the search space, revealing valuable insights into their exploration-exploitation strategies.", "section": "G Behavior Analysis of GraphComBO"}, {"figure_path": "KxjGi1krBi/figures/figures_24_1.jpg", "caption": "Figure 16: Behavior analysis of flattening the curve experiment on the contact network with SIR.", "description": "This figure presents a detailed behavior analysis of GraphComBO and other baselines on the contact network using the SIR model.  It shows the performance of each method (Population Infection Time, Explored Combo-Graph Size, Distance from Start) over 300 queries for different subset sizes (k=4, 8, 16, 32). The graphs illustrate the cumulative regret, combo-graph size explored by each method, and distance traveled from the starting node over the course of 300 queries.  This allows for a comparison of exploration vs. exploitation strategies.", "section": "G Behavior Analysis of GraphComBO"}, {"figure_path": "KxjGi1krBi/figures/figures_24_2.jpg", "caption": "Figure 17: Comparison with COMBO in maximizing avg. PageRank on small BA and WS networks.", "description": "This figure compares the performance of GraphComBO with COMBO and other baselines in maximizing the average PageRank on small Barab\u00e1si-Albert (BA) and Watts-Strogatz (WS) networks.  The results demonstrate GraphComBO's superior performance across different subset sizes (k).  It highlights GraphComBO's efficiency in handling the combinatorial search space compared to COMBO, especially for larger subset sizes.", "section": "H Comparison with COMBO"}, {"figure_path": "KxjGi1krBi/figures/figures_25_1.jpg", "caption": "Figure 18: Maximizing avg. PageRank on the OGB-arXiv network (|V| = 1.7 \u00d7 105, |E| = 106).", "description": "The figure shows the performance of different algorithms on a large social network (OGB-arXiv) with different subset sizes (k).  It demonstrates the scalability and relative performance of GraphComBO compared to other methods like k-Random Walk, k-Local Search, Local Search, BFS, DFS, and Random search in maximizing the average PageRank score. The results highlight that GraphComBO maintains its advantage even on very large graphs.", "section": "I Scalability on Large Graphs"}, {"figure_path": "KxjGi1krBi/figures/figures_26_1.jpg", "caption": "Figure 19: Density of underlying signals in the combinatorial space at different noise levels.", "description": "This figure shows the density of underlying signals in the combinatorial space at different noise levels (\u03c3 = 0, 0.5, 1.0).  The left panel shows the distribution for a Barab\u00e1si-Albert (BA) network, and the right panel shows the distribution for a Watts-Strogatz (WS) network.  The distributions illustrate how adding noise affects the smoothness and distribution of the signals, which is important for evaluating the performance of Bayesian Optimization methods in noisy environments. The original signal is shown as a blue line, and the signals with noise (\u03c3 = 0.5 and 1.0) are shown in orange and red respectively. The graphs clearly show the impact of increasing noise on the distribution of the underlying signal.", "section": "J Settings under Noisy Observations"}, {"figure_path": "KxjGi1krBi/figures/figures_26_2.jpg", "caption": "Figure 20: Maximizing avg. PageRank (k = 8) on BA and WS at different noise levels.", "description": "This figure shows the results of maximizing average PageRank with k=8 on Barab\u00e1si-Albert (BA) and Watts-Strogatz (WS) networks under different noise levels (\u03c3 = 0.1, 0.25, 0.5, 1).  It compares the performance of GraphComBO and GraphComBO-Noisy (a modification to handle noise) against several baseline methods (k-Random Walk, k-Local Search, Local Search, BFS, DFS, and Random). The plots illustrate the cumulative average PageRank achieved over multiple runs for each method and noise level.  This allows for a comparison of performance under noisy conditions.", "section": "J Settings under Noisy Observations"}, {"figure_path": "KxjGi1krBi/figures/figures_27_1.jpg", "caption": "Figure 15: Behavior analysis of maximizing average eigenvector centrality on the BA network.", "description": "This figure shows the results of a behavior analysis of GraphComBO algorithm on Barabasi-Albert networks.  It includes plots showing regret (difference between achieved value and optimal value), the size of the explored combo-graph, and the distance of the current combo-subgraph center from the starting location.  Different line colors represent different values for the hyperparameter Q (the size of the local combo-subgraph sampled during the search). The purpose is to illustrate how GraphComBO explores the search space and to understand the influence of hyperparameter Q on the algorithm's performance.", "section": "G Behavior Analysis of GraphComBO"}, {"figure_path": "KxjGi1krBi/figures/figures_28_1.jpg", "caption": "Figure 22: Ablation study of Q = [500, 1k, 2k, 4k] with a fixed failtol = 30 on WS network.", "description": "This figure shows the ablation study of the hyperparameter Q (combo-subgraph size) on Watts-Strogatz (WS) networks.  The x-axis represents the number of queries, and the y-axis shows the regret (difference between the best query and the ground truth) and the explored combo-graph size.  Different line colors represent different values of Q.  The results show that increasing Q generally improves performance, particularly for larger values of k (subset size).", "section": "K Ablation Studies on Hyperparameters"}, {"figure_path": "KxjGi1krBi/figures/figures_28_2.jpg", "caption": "Figure 15: Behavior analysis of maximizing average eigenvector centrality on the BA network.", "description": "This figure presents a detailed behavioral analysis of the GraphComBO algorithm on Barab\u00e1si-Albert (BA) networks for the task of maximizing average eigenvector centrality.  For different subset sizes (k=4, 8, 16, 32), it shows the regret (difference between the algorithm's best solution and the true optimum), the size of the explored combo-graph, and the distance of the current combo-subgraph center from the starting point.  These metrics are tracked across various values of the failtol hyperparameter (controlling the tolerance for consecutive non-improvements).  The results compare GraphComBO to other baseline methods (Random, DFS, BFS, Local Search, k-Local Search, and k-Random Walk).", "section": "G Behavior Analysis of GraphComBO"}, {"figure_path": "KxjGi1krBi/figures/figures_29_1.jpg", "caption": "Figure 16: Behavior analysis of flattening the curve experiment on the contact network with SIR.", "description": "This figure provides an in-depth behavioral analysis of the GraphComBO algorithm on a real-world task of flattening the curve in an epidemic process.  It shows the regret, explored combo-graph size, and the distance of the current combo-subgraph center from the starting location for different values of the hyperparameter failtol. This detailed analysis helps in understanding how the algorithm balances exploration and exploitation.", "section": "G Behavior Analysis of GraphComBO"}]