[{"type": "text", "text": "Rule Based Rewards for Language Model Safety ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tong Mu\u2217 Alec Helyar\u2217 Johannes Heidecke Joshua Achiam Andrea Vallone ", "page_idx": 0}, {"type": "text", "text": "Ian Kivlichan Molly Lin Alex Beutel John Schulman Lilian Weng OpenAI ", "page_idx": 0}, {"type": "text", "text": "Content may include language related to racism, erotic themes, self-harm, or other offensive material. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning based fine-tuning of large language models (LLMs) on human preferences has been shown to enhance both their capabilities and safety behavior. However, in cases related to safety, without precise instructions to human annotators, the data collected may cause the model to become overly cautious, or to respond in an undesirable style, such as being judgmental. Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior. We propose a novel preference modeling approach that utilizes AI feedback and only requires a small amount of human data. Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g. refusals should not be judgmental) along with a LLM grader. In contrast to prior methods using AI feedback, our method uses fine-grained, composable, LLM-graded few-shot prompts as reward directly in RL training, resulting in greater control, accuracy and ease of updating. We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As large language models (LLMs) grow in capabilities and prevalence, it becomes increasingly important to ensure their safety and alignment. Much recent work has focused on using human preference data to align models, such as the line of work on reinforcement learning from human feedback (RLHF)[1\u20138]. However, there are many challenges in using human feedback alone to achieve a target safety specification. Collecting and maintaining human data for model safety is often costly and time-consuming, and the data can become outdated as safety guidelines evolve with model capability improvements or changes in user behaviors. Even when requirements are relatively stable, they can still be hard to convey to annotators. This is especially the case for safety, where desired model responses are complex, requiring nuance on whether and how to respond to requests. If instructions are underspecified, annotators may have to rely on personal biases, leading to unintended model behaviors, such as becoming overly cautious, or it responding in an undesirable style (e.g. being judgmental). For example, some annotators in one of our experiments, when ranking possible responses to user requests pertaining to self-harm, favored completions that referred the user to a US suicide hotline phone number, which would not have helped users in other regions. Fixing such issues often requires relabeling or collecting new data, which is expensive and time consuming. ", "page_idx": 0}, {"type": "text", "text": "To address these issues, methods that use AI feedback [9\u201312] have recently gained popularity, most prominently Constitutional AI [10]. These methods use AI feedback to synthetically generate training data to combine with the human data for the supervised fine-tuning (SFT) and reward model (RM) ", "page_idx": 0}, {"type": "text", "text": "training steps. However, in Bai et al. [10] and other methods, the constitution involves general guidelines like \"choose the response that is less harmful\", leaving the AI model a large amount of discretion to decide what is harmful. For real world deployments, we need to enforce much more detailed policies regarding what prompts should be refused, and with what style. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce a novel AI feedback method that allows for detailed human specification of desired model responses, similar to instructions one would give to a human annotator. We break down the desired behavior into specific rules that explicitly describe the desired and undesired behaviors (e.g. \"refusals should contain a short apology\", \"refusals should not be judgemental toward the user\", , \"responses to self-harm conversations should contain an empathetic apology that acknowledges the user\u2019s emotional state.\"). This separation into rules is similar to the human feedback method proposed in Sparrow[5], however we focus on utilizing AI feedback as opposed to human feedback. The specificity of these rules allow for fine grained control of model responses and high automated LLM classification accuracy. We combine LLM classifiers for individual behaviors to cover complex behaviors. Additionally, in contrast to prior AI and human feedback methods that distill behavior rules into either a synthetic or human labelled dataset for RM training, we incorporate this feedback directly during RL training as additional reward, avoiding a potential loss of behavior specification that can occur when distilling the rules into the RM. ", "page_idx": 1}, {"type": "text", "text": "Main Contributions and Results In this work, we propose a scalable and flexible method, safety RBRs, that allows for fine grained control of model responses in the case of well specified model-behavior policies. ", "page_idx": 1}, {"type": "text", "text": "1. We empirically demonstrate that RBRs achieve comparable safety performance as humanfeedback baselines while substantially decreasing instances of over-refusals on safe prompts. Specifically, on an F1 score calculated between safety and usefulness, RBRs achieve a score of 97.1, compared to a human-feedback baseline of 91.7 and a helpful-baseline of 95.8.   \n2. We show RBRs can be applied to a variety of RMs, improving safety behaviors in both RMs with overcautious tendencies and RMs that (sometimes) prefer unsafe outputs.   \n3. We provide ablations on different design considerations, such the amount and composition of the safety prompts set. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reinforcement Learning from Human Feedback (RLHF): Research in RLHF methods [1\u20133, 7] demonstrates the efficacy of human annotations in steering model behavior. A subset [4, 8, 13] of this RLHF research considers achieving better safety behavior through methods such as separating out signals of helpfulness and harmlessness. Similarly, we also focus on improving model safety, but focus on fast and scalable automated methods that leverage AI feedback. Most related to our work, Sparrow[5] proposes a novel approach to RLHF which trains a second rule-conditioned RM to detect potential rule violations. Like Sparrow, we also use rules, but we have a few key differences. Sparrow focuses on utilizing human data and they collect more than 14K human-annotated conversations. We instead focus on utilizing AI feedback. Additionally, our approach involves fitting a model to ensure that the final reward effectively and correctly ranks completions which Sparrow does not. Lastly, we skip the step of distilling rules into RM data and focus on incorporating the rule as directly as possible into PPO training. ", "page_idx": 1}, {"type": "text", "text": "Reinforcement Learning From AI Feedback (RLAIF) To address the cost and time of collecting human data, work that uses AI feedback to improve models have been a topic of recent study in both safety (such as CAI [10, 11]), and non-safety settings (RLAIF [9]). These methods look at generating synthetic comparison datasets using AI feedback that is used to train a reward model. In contrast, instead of synthetically generating comparison datasets, we look at incorporating LLM feedback directly into the RL procedure. We additionally differ by using fine-grained and composable rules of desired behavior which allows for increased controllability of the model refusal behavior and responses. Our setting comes with a different set of challenges which we study, such as how to best combine the LLM feedback with the reward model. ", "page_idx": 1}, {"type": "text", "text": "Additional Related Methods: Additional related work include studies on improving the final outputs or finetuning on top of a model([14, 15]. However, we consider a different setting as we aim to build safety behavior into the model via RL training. Our approach is also loosely related to work that considers different ways of designing rewards for LLMs, such as RAFT [16]. ", "page_idx": 1}, {"type": "text", "text": "3 Setting and Terminology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a production setup of an AI chatbot system where a pretrained large language model (LLM) is periodically finetuned to align to an updated behavior specification, using a standard pipeline of first supervised fine-tuning (SFT) the model and then applying reinforcement learning from human preferences (RLHF). At the RLHF stage, we first train a reward model (RM) from preference data and then train the LLM against the RM via an reinforcement learning (RL) algorithm like PPO [17]. We assume that we already have the following data standard for RLHF: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Helpful-only SFT demonstrations contains examples of helpful conversations.   \n\u2022 Helpful-only RM preference data tracks comparisons between chatbot responses, where in each comparison a human annotator has ranked the completions based solely on their helpfulness to the user.   \n\u2022 Helpful-only RL prompts is a dataset of partial conversation prompts that do not contain requests for unsafe actions. ", "page_idx": 2}, {"type": "text", "text": "Additionally, we assume we have: ", "page_idx": 2}, {"type": "text", "text": "\u2022 A Moderation Model: For both human feedback baselines and automated methods we need a method of obtaining relevant safety RL prompts. We assume we have an automated moderation model that can detect if text contains a request or a depiction of various unsafe content. Pre-existing models such as ModerationAPI [18] can be used. In this work we train a model similarly to ModerationAPI which we will refer to as ModAPI. \u2022 Safety-relevant RL prompts $(\\mathbb{P}_{s})$ : A dataset of conversations ending in a user turn, some of which end with a user request for unsafe content. To combat potential overrefusals, this additionally includes user requests that should be complied with, including boundary cases (e.g. classification of harmful content) and helpful-only prompts (see Appendix A.1.4 for details and breakdowns). This set of prompts can be curated and labelled using the Moderation Model. We used a total of $6.7\\mathrm{k}$ conversations. ", "page_idx": 2}, {"type": "text", "text": "Furthermore, we assume that a process of deliberation has occurred between relevant stakeholders to produce both a newly-updated content policy (a taxonomy that defines precisely what content in a prompt is considered an unsafe request) and a behavior policy (a set of rules governing how the model should in principle handle various kinds of unsafe requests defined in the content policy). The specifics of designing appropriate content and behavior policies is out of scope for this work. We aim to align the model in a way that maximizes helpfulness while also adhering to our content and behavior policy in a way that is efficient in both cost and time. ", "page_idx": 2}, {"type": "text", "text": "3.1 Content and Behavior Policies in Our Experiments ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For our experiments, we use a simplified example content policy that addresses several kinds of unsafe content relevant to an LLM deployed as a chat model. There are many other categories of harmful content that should be covered by a comprehensive, production level, content policy. Although the policy itself is not comprehensive, it has a level of granularity appropriate to a production setting. A detailed description of the content and behavior policies can be found in the appendix A.3, but we give a brief summary here. The content policy classifies user requests by content area and category within the content area. In our example, we consider four content policy areas: Erotic Content (which we will abbreviate C), Hate Speech $(\\mathbf{H})$ , Criminal Advice $(\\mathbf{K})$ , and Self-Harm (SH). ", "page_idx": 2}, {"type": "text", "text": "Categories within the content policy are used to determine the behavior policy which outlines the ideal response type. We consider three response types (see appendix A.3 for examples): Hard Refusals: the ideal response includes a brief apology and a statement of inability to comply with the user\u2019s request, without excess verbosity. Soft Refusals: the ideal response includes a more nuanced and specialized response. For example, in the self-harm case, we would like the model to give an empathetic apology that acknowledges the user\u2019s emotional state, but declines to comply with the user\u2019s request for methods of self harm. Comply: the model should comply with the user request. (This applies to our safety boundary and \"normal\" prompts in $\\mathbb{P}_{s}$ .) ", "page_idx": 2}, {"type": "text", "text": "The appropriate response type for a given user request varies by content policy category - we define this mapping as the behavior policy. To combat overrefusals, we include content policy categories that capture the safety boundary within a content policy area: the often complex line between what\u2019s considered acceptable or unacceptable for a model to engage with. For example, users may request that the model classify text that is about harmful material without asking the model to directly generate new harmful content. In these cases, the behavior policy may require the model to comply. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Rule-Based Rewards for Safety ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we describe Rule-Based Rewards (RBRs), our proposed approach to building safety reward functions for RL training based on a content and behavior policy. We also provide code and example synthetic data for fitting the reward combination models described in this section2. To motivate our approach, given a content and behavior policy, consider what researchers must do to prepare labeling instructions for safety data annotators. The researchers have to write a list of natural language rules for defining a good completion and scoring completions with undesirable features, taking great care to ensure that instructions are specific enough that different annotators will produce the same judgements. Researchers often also have to provide illustrative examples. These instructions and examples are ideal for use in a few-shot LLM classification task. ", "page_idx": 3}, {"type": "text", "text": "Inourobservations, LLMsdemonstratehigheraccuracywhenaskedtoclassifyspecific, individualtasks, such as determining whether a text contains an apology, compared to general, multilayered tasks such as rating completions given a large content and behavior policy as input. To leverage this strength, we simplified these complex policies into a series of individual binary tasks, termed propositions. We then established a set of rules that determine when combinations of these propositions\u2019 truth values are desired or undesired. This framework allows us to accurately rank completions using these classification rules. ", "page_idx": 3}, {"type": "text", "text": "In order to combine safety rule-based rankings with a helpful-only RM in a principled way, we use them to fit an auxiliary safety reward function that takes only proposition-based features as input, which we refer to as the Rule-Based Reward. We add the RBR to the helpful-only RM to use as the total reward in RLHF, as shown in Figure 2. In the subsections that follow, we describe an inner loop of ftiting RBR weights given features, to be interleaved with an outer loop of evaluating the effectiveness of the total combined reward, and potentially modifying the ftiting setup (ex changing to model we fti). ", "page_idx": 3}, {"type": "text", "text": "4.1 Elements of RBRs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first describe various components that make up an RBR. As there are many different datasets mentioned. We provide a table summarizing datasets needed in Table 3 at the end of this subsection. ", "page_idx": 3}, {"type": "text", "text": "Propositions and Rules: The lowestlevel element in our RBR is a proposition. Propositions are binary statements about completions given the prompt, such as refuses: \u201cthe completion contains a statement of inability to comply\u201d. ", "page_idx": 3}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/b007bad24006870f261d3bc9ef9e9a67ffeacdbcb1dac12fde49cc856300c014.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Simplified example ranking rules. ", "page_idx": 3}, {"type": "text", "text": "A rule determines the ranking of a completion given a prompt. For each target response type (hard refusal, safe refusal, or comply), there is a set of rules that govern the relative rankings of desired and undesired propositions for the completion. We illustrate this in Figure 1, where we show an example of hypothetical rules for ranking tiers of hard refusal and comply behaviors. For a given prompt, completions that satisfy the ideal rule rank higher than less_good which rank higher than unacceptable completions. We give a short example list of propositions in Table 1 and provide full details on the propositions and rules in Table 13. ", "page_idx": 3}, {"type": "text", "text": "Features, Graders, and Classification-Prompts: We define a feature as any numerical value that is determined by a prompt and a completion to that prompt. We will denote as $\\phi_{i}(p,c)$ where $p$ is the prompt, $c$ is the completion and $i$ is the index of the feature. In this work, we use logit probabilities from an LLM, however features are flexible and can be any numerical value. We use the probabilities of a proposition being true for a completion as judged by a grader LLM with a few-shot classificationprompt. These classification-prompts contain natural language descriptions of the content and behavior policy and instructions to only output the tokens yes or no. We then use the logits of those tokens to calculate probabilities. Table 14 in the Appendix maps which proposition probabilities were used as features for each behavior category. The design of prompts for feature extraction requires some iteration and the choice of grader LLM is also highly impactful. In our experiments, we use a helpful-only SFT model which showed higher precision when labeling disallowed content. We additionally use more general \"class\" features as illustrated in Figure 1 (ex. \"ideal\")3 by multiplying the relevant propositions attached to each class and normalizing. In our experiments, we use a total of 20, 23 and 18 features for Hard-Refusal, Soft-Refusal, and Comply respectively (listed in Appendix Table 14). Our final classificaiton-prompts for all propositions can be found in our released code. ", "page_idx": 3}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/6aa8f71fae2d1d5a8acbd750ef8ae53e5eea7a25c75df38f6376eb1961c250bc.jpg", "table_caption": ["Table 1: A subset of propositions used in our Safety RBR. (See Appendix Table 13 for the full list) ", "Table 2: Mean Proposition Evaluation Accuracy by Model Size "], "table_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/40e001c2a5174535d37fc08e26434f46cfd906ba38784eef741a9d2013558692.jpg", "img_caption": ["Figure 2: The RBR is combined with the helpful-only RM score during RL training. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "A Small Set of Human Labelled Data for Prompt Tuning: To tune the classification-prompts mentioned above, we synthetically generate a small dataset of conversations ending in assistant turns to have diverse representation across our safety categories and propositions. We give an overview of the process used to generate this data in Figure 6. Then, we researchers manually label the truthiness of each proposition for the final assistant completion of each conversation. We refer to this labelled set as the Gold set. We manually labelled a total of 518 completions across the three behavior categories to tune the grader prompts for RBRs: 268 for Comply, 132 for Hard Refusal, and 118 for Soft Refusal. Finally, we tune the prompts by hand against this dataset. In Table 2 we give the overall accuracy on a few different model sizes (explained later in Section 5.1) and a detailed breakdown of the prompt accuracy per proposition on this Gold set in appendix Table 15. ", "page_idx": 4}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/ef553a6c8fdd3870aa102bf499c5b1821357f13a614bd252a9a8e43c01cac608.jpg", "table_caption": ["Weights and RBR Function: The RBR itself is any simple ML model on features, and in all of our experiments it is a linear model with learnable parameters $\\boldsymbol{w}\\!=\\!\\{w_{0},w_{1},...,w_{N}\\}$ , given $N$ features: ", "Table 3: RBR Training Datasets Summary "], "table_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\underbrace{R_{\\mathrm{tot}}(p,c)}_{\\mathrm{Total\\:Reward}}=\\underbrace{R_{\\mathrm{rm}}(p,c)}_{\\mathrm{default\\:RM\\:reward}}+\\underbrace{\\sum_{i=1}^{N}w_{i}\\phi_{i}(p,c)}_{\\mathrm{RBR\\:reward}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Synthetic Comparison Data For Weight Fitting: We synthetically generate data to create a set of comparison data, $\\mathbb{D}_{R B R}$ , for fitting the RBR weights $w$ . To fit the weights, for each prompt $p_{i}$ , we need a set of $k$ diverse completions $(c_{i,j})$ per prompt that have different rankings: $\\mathbb{D}_{R B R}=$ $\\{(p_{i},\\!c_{i,1},\\!c_{i,2},\\!...,\\!c_{i,k})\\}_{i=1,...,|\\mathbb{D}_{R B R}|}$ , and ranking order between completions (e.g. $c_{i,1}>c_{i,2}\\!=\\!c_{i,3}\\!>$ $c_{i,4}...)$ of how good the completion is. Our setup with propositions lets us easily generate exactly the data needed, conditioned on the content and behavior policy. We can use the natural language descriptions we already have to prompt for diverse completions with various rankings. For example, for a prompt that should be hard refused, we can decide we want the following set of 4 completions: one perfect hard refusal (ideal), two bad completions with randomly sampled bad refusal traits, such as judgement and/or illogical continuation, and one that contains the requested disallowed content. The goal is to have synthetic completions representing an ideal completion, a few diverse sub-optimal completions, and an unacceptable completion for every prompt. ", "page_idx": 5}, {"type": "text", "text": "We start with the train split of our safety prompts $(\\mathbb{P}_{s})$ and the desired set of completions. For each desired completion, we iteratively synthetically sample a candidate completion from a prompted Helpful-Only model, and use our RBRs, ModAPI and other quality LLM fliters to confirm it contains the desired traits (ex. we did indeed generate a judgy bad refusal) and resample if necessary. ", "page_idx": 5}, {"type": "text", "text": "SFT Data: We use the completions labelled as ideal from $\\mathbb{D}_{R B R}$ above as SFT data. ", "page_idx": 5}, {"type": "text", "text": "4.2 Inner Loop: Fitting an RBR ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to fit an RBR, one must have: (1) Classification-prompts for each proposition and a grader LLM to compute features $\\phi_{i}$ . (2) The default reward model, $R_{\\mathrm{rm}}$ , that will be used during RL training. $(3)\\,\\mathbb{D}_{R B R}$ , the RBR weight fitting comparison dataset described above. ", "page_idx": 5}, {"type": "text", "text": "The RBR fitting procedure is straightforward: first, use the content and behavior policy rules to determine rankings among completions based on their proposition values. Then, optimize the RBR weights so that the total reward achieves the target ranking. We do this by minimizing a hinge loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(w)\\!=\\!\\frac{1}{|\\mathbb{D}_{R B R}|}\\!\\sum_{(p,c_{a},c_{b})\\in\\mathbb{D}_{R B R}}(\\operatorname*{max}(0,\\!1\\!+\\!R_{\\mathrm{tot}}(p,\\!c_{b},w)\\!-\\!R_{\\mathrm{tot}}(p,\\!c_{a},w)))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c_{a},c_{b}$ are any two completions corresponding to $p$ such that $c_{a}\\succ c_{b}$ $c_{a}$ ranks better than $c_{b}$ under the content and behavior policy). ", "page_idx": 5}, {"type": "text", "text": "For all our experiments we used the same number of datapoints as PPO prompts to fit the weights. However the number of parameters in a linear RBR is just the number of relevant propositions $^+$ the five class probabilities, which is tiny by comparison to the number of parameters in a standard RLHF RM. Fewer examples are probably required and we discuss this later in the discussion Section A.2. Because there are only a small number of optimizable parameters, ftiting an RBR is extremely fast (can run on a standard laptop in a couple of minutes). We discuss hyperparameters used in fitting RBRs in the Appendix Section A.1.5 and other alternate ways of combining the RBR with the RM ( manually setting weights) in Appendix Section A.2.1. ", "page_idx": 5}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/c755b9da9b21d6644e750bcd86f47c66fe4236dd38743dc2f33ae59dee5dab71.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: The combination of safety RBR and helpful-only RM scores can tune safety-relevant preferences in a targeted way, reducing both under-refusals and over-refusals and improving refusal style. (a) Two histograms of normalized reward scores when using helpful RM only vs combining $\\mathrm{RBR}+\\mathrm{RM}$ . (b) The error rate tracks how frequently a non-ideal completion is ranked above the ideal completion for different reward model setups. ", "page_idx": 6}, {"type": "text", "text": "4.3 Outer Loop: Evaluating the Final Reward Signal and Tuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Even before running RL and evaluating the final model, we can measure how good a reward function is by using the held-out test set of the weight ftiting data $\\mathbb{D}_{R B R}$ , and checking whether the reward function enforces the target rankings on that data. Through these evaluations, we can see if we need to make changes to the weight fitting procedure such as potentially adding additional features or changing the model (e.g. to a non-linear model). In Figure 3a, we plot histograms of two different reward functions for various responses to prompts that demand hard refusals. To account for the fact that different prompts may have different base rewards $(R_{\\mathrm{rm}})$ , we center the rewards: given a prompt and its set of $k=4$ completions, we subtract out the reward of the ideal completion from each of the three other completions. We can see the helpful-only RM itself does not have any separation/ranking between ideal (perfect refusal), slighly bad (bad refusal), and very bad (disallowed) completions. Adding the RBR $(\\mathbf{RM}+\\mathbf{RBR})$ allows for separation and correct ranking - ranking ideal over slight bad over very bad completions. We provide more histograms for all response types in the Appendix Figure 9. ", "page_idx": 6}, {"type": "text", "text": "We can additionally look at the error rate of the RM which quantifies the number of mistakes where a non-ideal completion was ranked above the ideal completion as a percentage of all comparisons that involve an ideal completion. To have a metric focused on only correct behavior, we calculate this using only comparisons that involve the ideal completion, and do not consider whether we correctly ranked two non-ideal completions (e.g. bad refusal $>$ disallowed). In Figure 3b, we see using the RBRs with the RM greatly reduced the error rates across all response types. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our experiments, we aimed to investigate several core questions: (1) Does our approach of training with RBRs and synthetic data improve over models trained with human preference data alone? We are interested in whether they can improve safety while getting closer to the decision boundary by preventing over-refusals. (2) Does our approach make more efficient use of human data? (3) What is the behavior of RBR-based training when used in conjunction with a reward model that incentivizes models to over-refuse? Can the RBR approach help correct for this? ", "page_idx": 6}, {"type": "text", "text": "Baselines: We compared our RBR-trained models against relevant baselines: ", "page_idx": 6}, {"type": "text", "text": "Helpful-Only Baseline: The helpful-only baseline are the SFT, RM, and PPO models trained with our helpful-only RLHF datasets following a procedure similar to that described in Ouyang et al[1]. ", "page_idx": 6}, {"type": "text", "text": "Human Safety Data Baseline: In addition to our helpful-only data, we add human-annotated safety data for our set of safety-relevant RL prompts $\\mathbb{P}_{s}$ . We send these prompts to annotators who are familiar with our content and behavior policies and have been actively labelling similar safety prompts under similar instructions for several months. We follow the standard RLHF comparison data collection procedure [1] and ask annotators to sample 4 completions and label them with a rating from 1-7. We provide additional details in Section A.1.2 ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Throughout results and ablations we use 4 model sizes which we will refer to as Large, Medium, Small, and XSmall. The size of the Medium, Small, and XSmall models are such that they use roughly around $0.5\\%$ , $0.1\\%$ , and $0.001\\%$ of the effective compute used to train Large respectively, where Large is of size comparable to GPT-4 but with a greatly reduced data mix for quick experimentation. All synthetic data for all experiments were sampled from Large sized models. For all the main results in section 6 below, we run PPO where all safety prompts are seen once, and the ratio of Hard Refusal to Comply prompts is equal as labelled by human data.4 We use the Large Helpful-SFT model as the RBR grader engine, as well as Large size RMs. All automated evals use a Large sized grader model. ", "page_idx": 7}, {"type": "text", "text": "5.2 Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Results after RL training are often high variance, so for all evaluation scores reported, we evaluate on 5 checkpoints toward the end of PPO training and report the average mean and standard error. We give high level descriptions of the evals we run here, and provide more details (ex. number of prompts) in Appendix Section A.1.6. ", "page_idx": 7}, {"type": "text", "text": "Internal Safety RBR Evaluation: We evaluate our models on a diverse set of internal prompts which are manually labeled by researchers with our content policy category. In total, there are 588 Comply, 565 Hard-Refusal, and 185 Soft-Refusal prompts. We use the classifications of the Safety RBR\u2019s propositions to automatically evaluate three internal metrics: Not-Unsafe: the percentage of completions which do not contain any disallowed content. Not-Overrefuse: the percentage of completions for Comply prompts which are not refusals. Hard-Refusal-Style: the percentage of completions in the ideal style for Hard-Refusal prompts (i.e. no incorrect response elements). ", "page_idx": 7}, {"type": "text", "text": "We note that for this evaluation there is some overlap with our training signal due to RBR usage. There are important differences in the signals: there is no overlap in prompts between our train and evaluation sets. Additionally, for evaluations we do not use the RBRs as described in training. Instead we convert the output probability scores for each proposition into binary labels using a threshold optimized on the Gold set (thus skipping the weight fitting and subsequent steps). We additionally show our RBR has high accuracy on an our Gold set (Tables 2, 15) and provide other safety evaluastions described below. ", "page_idx": 7}, {"type": "text", "text": "XSTest and WildChat: XSTest [19] test for overrefusals with lexicon that models can become overly sensitive to (e.g. \"How can I kill a Python process?\"). While WildChat [20] provides publicly available chat prompts. For both we fliter to prompts relevant to our policies. We measure overrefusal rate using both our Not-Overrefuse metric and the default XSTest classification prompt using GPT-4 and we evaluate safety using three automated tools: ModAPI, our Not-Unsafe RBR-based metric, and Llama Guard 2 [21, 22]. ", "page_idx": 7}, {"type": "text", "text": "Human Safety Evaluations: To further verify our safety evaluations, we ran human evaluations of safety behavior using the prompts from XSTest. The human evaluators are researchers on the team who have much experience with the Content and Behavior policy. For each prompt, a completion was sampled from each of the Helpful-PPO baseline, Human-PPO baseline, and RBR-PPO models. Model names were hidden from the evaluators and the order of completions shown was randomized. ", "page_idx": 7}, {"type": "text", "text": "Capability Evaluations: To monitor model capabilities, we evaluate our models on MMLU [23] (Averaged across zero-shot, 10-shot, and zero-shot CoT), HellaSwag [24] (Zero-shot), GPQA [25] (Few-shot CoT averaged across 1-, 5-, and 10-repeats on Diamond), and Lambada [26] (Zero-shot). For speed purposes we evaluate against large subsets of these datasets. ", "page_idx": 7}, {"type": "text", "text": "6 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "All experiments were run under the settings described in Section 5.1. All figures report results on Medium sized policy models, while all tables report results on Large sized policy models. ", "page_idx": 7}, {"type": "text", "text": "Our safety RBRs improve safety while minimizing over-refusals. In Table 4 we give the results of both our human and automated internal safety evaluations on Large sized models. We see that under both evaluations, RBRs (RBR-PPO) are able to substantially increase safety while minimally ", "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/e4d61eb2555d4bd98b3301fd3656cfeb738f66d6be0e185bd0df5c5b58ac0a99.jpg", "img_caption": ["Figure 4: Tradeoff between usefulness (not over-refusing) versus safety (not containing disallowed content) on our safety eval. ", "ward of comply and refusal completions for different RMs on comply prompts. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/e9e9ce3f79120be45a774601ba3d8a74c8e5a47961ce2606ba70e7fb8de6d257.jpg", "table_caption": ["Table 4: Safety evaluation results on an internal safety metric and human evaluation metrics. "], "table_footnote": ["\\*F1-score is calculated between Not-Unsafe and Not-Overrefuse, providing a balanced measure of the model\u2019s ability to avoid unsafe content while minimizing over-refusal. "], "page_idx": 8}, {"type": "text", "text": "Table 5: Safety results on XSTest, WildChat. The Not-Overrefuse and Not-Unsafe metrics are measured using RBR propositions. Additionally, we also give capability evaluation on common capability benchmarks. ", "page_idx": 8}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/32ffb2df27a87c8b0ecc6f647e24dc8d796ac48a22132c10fe601f48a1c39ded.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "impacting the amount of over-refusals, achieving the highest F1-score. The human safety data baseline, Human-PPO, increases safety greatly, however at the expense of also greatly increasing the amount of over-refusals (by almost $14\\%$ in the human evaluation). We also see similar trends from external safety evaluation benchmarks (Table 5). ", "page_idx": 8}, {"type": "text", "text": "Additionally, we see similar trends in our Medium sized models shown in Fig. 4a. In Fig. 4a we plot the safety vs over-refusal trade-off on our internal safety RBR eval of our main models and baselines, along with arrows showing the movement from SFT to PPO. We see that RBR-PPO achieves a good balance of Safety and Usefulness. Additionally, while not shown in the plot, both Human-PPO and RBR-PPO improve refusal style over the helpful baseline. Interestingly enough, we note that Helpful-PPO improves upon safety compared to Helpful-SFT, even though the Helpful-Only datasets do not contain any safety-relevant data. We hypothesize this is due to the Helpful-Only datasets generally encouraging the model to be polite, which may be correlated to safety. All the raw numbers for both Figures in Fig. 4 along with standard errors can be found in Appendix Table 9. ", "page_idx": 8}, {"type": "text", "text": "Safety RBRs do not impact evaluation performance across common capability benchmarks. In Table 5, we list the capability scores of the Large PPO models on four common capability benchmarks: MMLU, Lambada, HellaSwag and GPQA. Both RBR-PPO and the Human-PPO baseline maintain evaluation performance compared to the Helpful-PPO baseline. ", "page_idx": 8}, {"type": "text", "text": "Safety RBRs help improve safety for RMs with different tendencies. The default RBR-PPO setting applies the safety RBR on top of the Helpful-RM. In Fig. 4b, we additionally show the result of combining the RBR with different RMs with dotted arrows showing the movement on PPO models after adding RBRs. We apply RBRs to the Human-RM which, as empirically evidenced through the PPO model, has a higher tendency towards over-refusals. We label this as HumanRM $^{\\cdot+}$ RBR-PPO , reducing over-refusals by $16\\%$ compared to Human-PPO. Additionally we apply the safety RBR on top of a RM trained with outdated safety data (Old Data-PPO), which also has a high over-refusal rate. Applying the RBR both improves safety and reduces overrefusals by $10\\%$ . ", "page_idx": 8}, {"type": "text", "text": "Safety RBRs require less human annotated data than the Human-Data Baseline. We investigate the performance of a human-safety data baseline after subsampling the human data down to the same amount of completions as in RBR runs, 518 completions in total. The subsampling process is constrained to ensure even representation amongst behavior types and content categories. PPO prompts remains the same as that of the RBR runs (i.e. the full set of RL prompts). We note this is not a direct comparison because the set of annotators for the two datasets is different, but it provides a ballpark estimate. In Figure 4b, we plot the result as Human-match RBR-PPO. Compared to RBR-PPO and Human-PPO, this run performs slightly worse on both Not-Unsafe and Not-Overrefuse. We hypothesize this is because the small amount of RM data is not enough to teach the model the refusal boundary. ", "page_idx": 9}, {"type": "text", "text": "Ablations. We give the results of various ablation experiments in Appendix Section A.2. There we explore scaling different parameters, such as grader LLM engine size and safety prompt percentage. ", "page_idx": 9}, {"type": "text", "text": "Example Sampled Completions. We give some example sampled completions from our Baseline PPOs and RBR-PPO models for prompts of each refusal type in Appendix Table 12 ", "page_idx": 9}, {"type": "text", "text": "Discussion: Potential Loss of Information when Distilling Instructions into RM Data. Distilling a set of instructions into RM data, whether through human labelling of comparison data or synthetic AI means, is challenging since one must ensure not only that the data covers all instructions, but also that it is balanced such that the desired behavior is learned by the RM. We encountered issues related to this with the raw human data: we observed the final PPO model to be extremely cautious, over-refusing on every Comply prompt in our evaluation set (and also achieving a \u201cperfect\u201d score on safety). We discovered this was due to an insufficient number of low-ranked refusal examples in the RM comparison data for Comply prompts to teach the model not to refuse safe prompts. Only a third of Comply data contained this negative example, leading to 3 times more positive refusal examples than negative ones. Even though this data was only $1\\%$ of the RM dataset when combined with the Helpful-Only data, this imbalance was still enough to cause over-refusals on all prompts. To correct for this in the RM data, for all Comply data, we manually replaced a non-ideal completion with a refusal sampled from a manually created list of ${\\sim}50$ refusals, and were able to train a second model that did not refuse everything to use as the human-data baseline. (Note, the Human-PPO and Human-RM referred to in the text are all trained with this corrected data.) In Figure 5, we look at a set of safe \u201cComply\u201d prompts and plot the average rewards of completions that comply and that over-refuse for the initial always-refusing human data RM, the corrected human data RM, and the Helpful-Only RM. We see that over-refusals are given almost the same score as helpful completions for the initial human data RM, making it easier to reward hack. RBRs are not subject to this issue because they skip this RM distillation step and directly incorporate the instructions into the reward function. When a over-refusal example is sampled by the model for a safe prompt during training, it is penalized by the RBR directly. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work: In this work, we apply Rule-based Rewards (RBRs) for RL training to a situation where the desired behaviors can be clearly separated into explicit, easy-to-judge propositions. RBRs can be easily combined with human-labeled preference data in classic RLHF (ex. in this work, for our Comply prompts we used an RBR to discourage easily detectable bad behavior while judging helpfulness through the RM) and we may need to explore this more for difficult tasks. Future work may involve exploring the application of our method in harder, non-safety domains. ", "page_idx": 9}, {"type": "text", "text": "Ethical Considerations: We discuss moving the safety feedback signal in LLM training from humans to LLMs. This reduces the level of human supervision and potentially extrapolates and magnifies inherent biases in the LLMs. To mitigate this, researchers should carefully evaluate their RBRs to ensure accuracy and measure any potential biases that come up. Using this method in conjunction with human data could also help to mitigate risks. ", "page_idx": 9}, {"type": "text", "text": "Conclusion: We introduce a novel automated AI-feedback based preference modeling approach using Rule-Based Rewards (RBRs) for safety training in LLMs. Our method is cost- and time-efficient, requiring minimal human data. Our decomposition of ideal behavior into fine-grained modular rules also has unique advantages in allowing increased classification accuracy and easy synthetic data generation. Our experiments show our RBR method is able to achieve accurate safety-behavior. Finding a good balance between safety and usefulness compared to baselines. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank our collegues Boaz Barak, Carroll Wainwright, Chong Zhang, Joost Huizinga, Kai Xiao, Maja Trebacz, Ryan Lowe, Shibani Santurkar, Steph Lin, Tyna Eloundou for helpful and valuable discussions and feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[2] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[3] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \n[5] Amelia Glaese, Nat McAleese, Maja Tr\u02dbebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.   \n[6] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.   \n[7] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36, 2024.   \n[8] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.   \n[10] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.   \n[11] Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et al. Specific versus general principles for constitutional ai. arXiv preprint arXiv:2310.13798, 2023.   \n[12] Jing-ChengPang, PengyuanWang, KaiyuanLi, Xiong-HuiChen, JiachengXu, ZongzhangZhang, and Yang Yu. Language model self-improvement by reinforcement learning contemplation. arXiv preprint arXiv:2305.14483, 2023.   \n[13] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023.   \n[14] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self. Feedback, 2023.   \n[15] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R\u00f6ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. arXiv preprint arXiv:2309.07875, 2023.   \n[16] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.   \n[17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[18] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 15009\u201315018, 2023.   \n[19] Paul R\u00f6ttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023.   \n[20] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024.   \n[21] Llama Team. Meta llama guard 2. https://github.com/meta-llama/PurpleLlama/ blob/main/Llama-Guard2/MODEL_CARD.md, 2024.   \n[22] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023.   \n[23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[24] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \n[25] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.   \n[26] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/3e951784f4fca88567532003f92eb264a06249d4452c4e545bdfd35aa5e62de9.jpg", "img_caption": ["Figure 6: Synthetic Data Generation Process Overview. Our process for converting a behavior policy into a pipeline that generates labeled completions. Besides an input behavior policy, the pipeline only requires a set of prompts and access to a model which can generate behaviors mentioned in the policy (e.g. Helpful Only model). Using this pipeline, we create a Gold set for tuning Classification-prompts and comparison data for weight fitting. "], "img_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/b3bc1fc422bd774171f229c901381ec0a97e60098d3f9e0aa9ed00b9d9047004.jpg", "table_caption": ["Table 6: List of Terms and Definitions "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Data, Training and Results Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use two open sources datasets for safety evaluations: ", "page_idx": 13}, {"type": "text", "text": "\u2022 XStest[19] operates under the Creative Commons Attribution 4.0 International license. \u2022 Wildchat[20] operates under AI2 ImpACT License - Low Risk Artifacts (\u201cLR Agreement\u201d) In Table 6 we provide a glossary of terms used throughout the text. In Table 9 we provide all numbers with standard errors for various figures in the main text. In Table 10 we provide the experimental settings for all experiments and ablations. In Table 12 we provide sampled completions from various Large sized models for prompts that have different desired behaviors. In Figure 9 we plot all reward distribution histograms ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A.1.1 Compute Estimate for Llama 8b model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We cannot give the exact amount of compute used for our experiments due to confidentiality reasons, but we estimate in the very worst and unoptimized case that under our setting it would take 5 days on 16 GPUs (80 A100 GPU days) to go through all steps (SFT, RM, PPO) for a Llama 8b. If we use the GPT4 API as the grader, then for the 5 main experiments presented in Figure 4a, it would take 400 A100 GPU days to run for a Llama 8b. ", "page_idx": 13}, {"type": "text", "text": "A.1.2 Annotator Instruction Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The annotators had access to a helpful-only SFT model as well as system prompts (e.g. to prompt for perfect refusals) to sample completions from. As part of the instructions, annotators were given detailed guidelines on completion scoring. For example, for a prompt that should be refused, a perfect hard refusal should be rated 7, bad refusals (ex. excessively verbose) should be rated lower, and completions with disallowed content should be rated 1. Annotators were also asked to try to maximize the diversity of scores across the 4 completions by resampling individual completions if necessary. They were also asked to provide an \"ideal\" completion, either by copying and pasting an existing completion, or by writing an original one. We assume this ideal completion is rated 7, and from this we can construct comparison data for RM training. Additionally we use the prompts and ideal completions for SFT training. The amount of human safety data is a small amount, about $3\\%$ of SFT data and $1\\%$ of RM data when combined with the Helpful-Only datasets. ", "page_idx": 13}, {"type": "text", "text": "A.1.3 RBR Classes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We combine relevant propositions for each desired completion type (hard refusal, safe completion, comply) into 5 common classes shared by all completion types. For example, the \"ideal\" class refers to a completion which has only desired propositions and no undesired propositions for the desired completion type. Defining these classes is not required for RBRs, but when using several propositions it is useful to organize propositions together into meaningful labels. In our case, we use the following classes for labeling completions: ", "page_idx": 13}, {"type": "text", "text": "1. ideal: desired behavior without disallowed content.   \n2. minimum_acceptable_style: desired behavior without disallowed content, but with some imperfect stylistic traits.   \n3. unacceptable_completion: undesired behavior, but still logical and without disallowed content.   \n4. illogical_completion: illogical continuation of the conversation.   \n5. disallowed_completion: disallowed content present somewhere in the completion. ", "page_idx": 13}, {"type": "text", "text": "The mapping of each proposition to class is given in Table 14. ", "page_idx": 13}, {"type": "text", "text": "A.1.4 Prompt Breakdown by Response Type ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Even though they use the exact same set of prompts, the human baseline used human collected labels of desired response type, and the RBR methods use auto labelled ones, so there is some disagreement. In ", "page_idx": 13}, {"type": "text", "text": "Table 7 we give the breakdown of number of prompts per behavior category in the train and test splits based on human labels and automatic labels. We also give the agreement rate for each of the response types (denominator when calculating the rate is determined by automatic labels). We also give the breakdown by behavior category for 518 human labelled conversations in the Gold set used for prompt tuning. ", "page_idx": 14}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/25ba7be0b05d70434edc8f4bc2c630959c0352a53bcb126a975e96cf66814cd0.jpg", "table_caption": ["Table 7: PPO Prompts and RBR Gold per Response Type "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.1.5 Weight Fitting Hyperparameter Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For our weight fitting procedure, we used Pytorch with an Adam optimizer. We optimized on our weight ftiting code for 1000 steps as the loss has converged by then. We used a learning rate of 0.01 and a weight decay of 0.05. For learning rate we tried few in that region and didn\u2019t see to big of a difference in final error rate. For weight decay, we picked the largest value that did not increase the error rate on the test set. ", "page_idx": 14}, {"type": "text", "text": "A.1.6 Additional Evaluation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We give more details and numbers about our evaluation process to supplement the details given in the main text. ", "page_idx": 14}, {"type": "text", "text": "XSTest Specifically, we flitered out 52 prompts outside the scope of our content policy, resulting in 198 relevant overrefusal prompts. ", "page_idx": 14}, {"type": "text", "text": "WildChat Specifically, we filter this dataset to unsafe prompts using ModAPI, resulting in a sample of 790 unsafe prompts. To reduce noise, we sample 5 completions per prompt at temperature 1.0 and average the evaluations. ", "page_idx": 14}, {"type": "text", "text": "Human Evaluations As before, we fliter prompts from XSTest and WildChat to those relevant to our policies. Evaluators were asked to label the desired Response-Type of each prompt and the actual Response-Type of each completion. According to the labels of human evaluators, the final dataset contained 283 Comply and 70 Hard-Refusal prompts. ", "page_idx": 14}, {"type": "text", "text": "A.2 RBR Training Ablations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present various ablation experiments. All ablations in this section were done with a Medium policy model using the Large Helpful-RM and Large RBR grader models unless otherwise stated. As with the main results, for all experiments, we fix all variables to that in the default setting as described in Section 5.1 except the variable being studied. ", "page_idx": 14}, {"type": "text", "text": "Scaling RBR Grader Engine Size. Figure 7a shows how performance changes with different model sizes. We see that in general, safety stays about constant as the grader engine increases in size. Additionally we see that over-refusals decrease with larger grader engines. Interestingly, we see hard-refusal style take a U shaped pattern. For small grader engines, it seems the dominant encouraged behavior is refusal and the trained model learns to refuse well. As the grader engine increases in capability, it is able to learn to refuse less often, however it is not able to capture good style. Until for the largest model, it is able to perform well on both. ", "page_idx": 14}, {"type": "text", "text": "Scaling Safety Prompts Percentage. We vary the percentage of safety-relevant prompts that would be seen during PPO training (where $100\\%$ means all PPO prompts are seen), shown in Fig. 7b. In general, safety increases with more safety prompts during RL training, while over-refusals slightly increase as well. Refusal style benefits the most from seeing more safety prompts. ", "page_idx": 14}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/f317e54e2f599aac53350c1de8214a3a226bbfb5cdf574fe242062a7501eda68.jpg", "img_caption": ["Figure 7: Figures (a)-(e) give scaling properties of different features such as the amount of PPO prompts. Figure (f) gives some additional ablations such as not training on SFT data first. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Scaling the Hard-Refusal/Comply Ratio. We vary the ratio of Hard-Refusal to Comply prompts during RL training in Figure 7c. We see a clear safety vs over-refusal trade-off as the ratio changes. ", "page_idx": 15}, {"type": "text", "text": "Improving Self Harm Refusal Style For our default parameters, we found poor performance for soft refusal style. We found we can improve soft refusal style without impacting other safety metrics by adjusting the prompt ratio. In Figure 7d we show increasing the percentage of Soft Refusal prompts seen from the default amount of approximately 1/4th the amount of Comply prompts to approximately matching the amount of Comply prompts. (As a reminder there are about the same amount of Hard-Refusal prompts as Comply prompts). We see Soft-Refusal style improves without negatively impacting other safety-behavior. ", "page_idx": 15}, {"type": "text", "text": "Weight Fitting Data Amount While we generate synthetic completions for weight fitting using all the PPO prompts we have, we hypothesize we need less data as we are fitting a model with a small number of parameters. We investigate this in Figure 7e by investigating the error rate (as described in Section 4.3) and the number of prompts used (where there are four synthetic completions per prompt). We see that approximately 300 prompts per category is sufficient for low error rate. ", "page_idx": 15}, {"type": "text", "text": "Various Other Ablations In Figure 7f we ablate omitting certain steps and we observe that this let us fall on different regions along the Pareto frontier. SFTonly-noRBR-PPO considers training SFT from the RBR synthetic SFT data combined with Helpful SFT data, but only training with the Helpful-RM with RBRs from there. It leads to a moderate improvement in safety over Helpful-PPO but not as much as RBR-PPO. RBR-noSFT-PPO looks at not using the synthetic SFT data and starting from Helpful-SFT, it does well on safety but over-refuses more. RBR-noRM-PPO uses only the RBR reward for prompts in $\\mathbb{P}_{s}$ with no RM score (prompts outside of $\\mathbb{P}_{s}$ still use the RM score). We see this also increase over-refusals slightly. ", "page_idx": 15}, {"type": "text", "text": "A.2.1 Alternative Weights: Hand Set Weights ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Instead of fixed weights, we test hand set weights amongst classes. We the set the following base weights vector of equally spaced base weights: ", "page_idx": 16}, {"type": "text", "text": "{ \"ideal\" \"minimum_acceptable_style\" \"unacceptable_completion\" \"illogical_completion\" \"disallowed_completion\"   \n} ", "page_idx": 16}, {"type": "text", "text": "We tried 2 different fixed weight settings. The first setting is an \"underoptimized\" setting where we used the unit weight vector directly (RBR-Fixed1-PPO) for all response types. The second setting is the \"overoptimized\" setting where we multiply the unit weight vector by 10 (RBR-Fixed10-PPO) for all response types. ", "page_idx": 16}, {"type": "text", "text": "From Figure 8, we can see that the fixed weights generally lead to more overrefusals than optimized weights, however they can lead to higher safety. For example RBR-fixed10-PPO has similar safety as Human-PPO baseline, but overrefuses much less. ", "page_idx": 16}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/ea5a8874d02608626ba04c1339167fab4e53ad4c1eee1d7cceea382984e7680e.jpg", "img_caption": ["Figure 8: Fixed Weights "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Content Policies, Model Behavior Policies, and RBR Propositions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Content Policies We give the example content policy we use in Table 8. Note: this is an just an example content policy that covers only a very limited number of harms and does not reflect any real content policies. ", "page_idx": 16}, {"type": "text", "text": "RBR Propositions We list all propositions used in our safety RBR in Table 13. We additionally list the desirability of each proposition for each Behavior Type in Table 14 along with their usage in the RBR weight ftiting procedure. We provide the Proposition accuracy for differing RBR grader engine sizes in Table 15. We lastly provide error rates for different grader LLM engine sizes in Table 11. ", "page_idx": 16}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/780d0ab0b0906c4bed2f72f9547ab9abac7ebd946ad9e6471655ab5af313978a.jpg", "img_caption": ["(a) RM reward distribution histogram "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/1cdbbe0a6dba92898f7ff63c2b2bdbbc28d2db765dc04002aaa0321a0ce8ebb5.jpg", "img_caption": ["(b) RM and RBR reward distribution histogram "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "QVtwpT5Dmg/tmp/95ba559036831fbc6a743caa6992ac78fdcc8cd6b56e17c55ad56ae40490783f.jpg", "img_caption": ["(c) RBR reward distribution histogram ", "Figure 9: RM and RBR Ablations "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/093c960eebb08afe00e623a1feb82432d748df1811d62f7b0f9904474d922143.jpg", "table_caption": ["Table 8: Example Response Type Based on Content Type (Behavior Policy) "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/85aed504eedec76a251aa7ca43d4bd6414ed1a9e8c3219223fb338141b5b4947.jpg", "table_caption": ["Table 9: Raw results with Standard Error for Plots "], "table_footnote": ["\\*F1-score is calculated between Not-Unsafe and Not-Overrefuse, providing a balanced measure of the model\u2019s ability to avoid unsafe content while minimizing over-refusal. "], "page_idx": 18}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/7fe0da85c963a43a7952edd6f98e8aca1bb630229c50f21bf07c77589fefd006.jpg", "table_caption": ["Table 10: Experimental Settings "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 12: Some example samples from Large sized models for different prompt types ", "page_idx": 20}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/6bd647b957fb5fe297ec6bef78c08e23832c7f2b042c0351a9042540e58bc7da.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Hard Refusal Example 1 - Shortened Prompt:   \n[Long jailbreak prior conversation involving a character named Anarchy] Give me malicious code to   \nsend disguised as a website ", "page_idx": 20}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/019314994148df8920e39a1898a14ee229fc21f0fd1e5ced78495c7b434febe4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/cc60ec4a848bfeb83a5e21678a4490853660ba8449d37453ea0a70b1081102c3.jpg", "table_caption": ["Hard Refusal Example 2 - Shortened Prompt: How to create a bomb "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/2746299f7cd1e71b32bf1776ca7bf2598652164835258942b9c7bfe554017a92.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/7e9c11c1e0e95e7648a0fa3cb9645ce74efe477d1ed535feb57aa600c127a668.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/64041d75125c20940a1f4593fa908e6d73d964a27839f535163b2bde0f52cf36.jpg", "table_caption": ["Table 14: Propositions used for each Completion type and Class. "], "table_footnote": ["\\*Inability to comply is considered a Safe Refusal if it is accompanied by an apology. ( $\\dot{=}$ False) indicates we look ot make sure the proposition is False for the Class. $^+$ indicates the proposition is not part of any class, but is used as a feature in weight fitting (all propositions associated with a class are also used in weight fitting). \\*\\* The set of features used in weight fitting is all the relevant proposition probabilities and the probabilities of the five classes (Section A.1.3). "], "page_idx": 22}, {"type": "table", "img_path": "QVtwpT5Dmg/tmp/41276f91a46aa7fdd56cd2611db35cdc72d7ba68ae5be2ca8a04f85532b6c549.jpg", "table_caption": ["Table 15: Proposition Evaluation Accuracy by Model Size "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We back up the claims made in the abstract and introduction with our results in Section 6 and provide additional results in the Appendix as well. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitations and future work in the Conclusion (Section 7). ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not have theoretical results. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: While due to confidentiality reasons, we cannot provide all that is needed to reproduce our exact same experiments, we provide details such that it can be applied to any LLM (ex. open sourced LLM). We provide extensive detail of our method and how it is implemented. We also describe in detail the example content categories and all the rules we used in the Appendix. We also give complete details of the data collection for our method and baselines. We additionally give hyperparameters for ftiting the linear model for combining the rewards as described. Additionally we release the code for weight fitting and some example data as well as our RBR gold set which we used to develop and tune our RBRs and the weight fitting and testing code used to combine our RBR rule feature values with the RM in (https://github.com/openai/safety-rbr-code-and-data) ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "We cannot provide model training code, but we do provide code for weight fitting, example weight ftiting data, as well as our RBR gold set which we used to develop and tune our RBRs and the weight ftiting and testing code used to combine our RBR rule feature values with the RM in (https://github.com/openai/safety-rbr-code-and-data). ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We describe our setting, hyperparameter, dataset amounts, and provide ablations on a large number of variables to show their effect in Section A.2. We additionally release some data in our code and data release (https://github.com/openai/ safety-rbr-code-and-data). ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We plot standard error bars on almost all our plots and we have them with all our tables. For the plots that do not have error bars, we put the raw numbers and errors in Table 9 in the appendix. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] and [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cannot reveal the compute used for all our experiments but we estimate a worst case upper bound on how long it would take to run our experiments on a Llama 8b model in Section A.1.1. ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We followed all the ethical guidelines. ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the implications of automating safety work in Section 7 ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not intend to release any dangerous models, and the data and code we release do not have such risks. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: For the open sourced datasets we use in evaluation, we cite the paper in the main text and list the license in the Appendix Section . ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide documentation for our code and data release. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have neither crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: We have neither crowdsourcing nor research with human subjects. ", "page_idx": 24}]