{"importance": "This paper is crucial because it challenges the widely used assumption of zero initialization in Neural Tangent Kernel (NTK) theory, which is inconsistent with real-world applications. The findings highlight the limitations of NTK theory in explaining neural network performance and open avenues for more realistic theoretical frameworks.", "summary": "Standard initialization in neural networks negatively impacts generalization ability under Neural Tangent Kernel theory, contradicting real-world performance, urging the development of improved theoretical models.", "takeaways": ["Standard initialization in neural networks significantly hinders generalization ability under the Neural Tangent Kernel (NTK) framework.", "The NTK theory's assumption of zero initialization deviates from real-world practices, creating limitations in its ability to fully explain the success of deep learning.", "Mirrored initialization is superior to standard initialization in practical applications, as evidenced by the research findings."], "tldr": "Neural Tangent Kernel (NTK) theory, a prominent framework for understanding deep learning, typically assumes networks begin training from a zero-output state (mirrored initialization). This simplification, however, neglects the impact of random initialization common in practice, where networks start with non-zero outputs. This paper investigates this oversight and its effect on generalization ability.\nThe study reveals that under standard (non-zero) initialization, wide neural networks do not generalize well under the NTK framework, exhibiting a slower learning rate and suffering from the curse of dimensionality.  This contrasts sharply with the observed performance of such networks in real-world applications.  The findings highlight that the benefits of mirror initialization and the theoretical gap between NTK theory and practical performance, underscoring the need for more refined theoretical models.", "affiliation": "Tsinghua University", "categories": {"main_category": "AI Theory", "sub_category": "Generalization"}, "podcast_path": "ni3Ud2BV3G/podcast.wav"}