{"references": [{"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-12-01", "reason": "This paper introduced the Neural Tangent Kernel (NTK) theory, which is the foundation of the current paper's theoretical analysis."}, {"fullname_first_author": "Sanjeev Arora", "paper_title": "On exact computation with an infinitely wide neural net", "publication_date": "2019-12-01", "reason": "This paper provided an important theoretical result about the connection between infinitely wide neural networks and kernel regression, which is crucial to the current paper's arguments."}, {"fullname_first_author": "Jianfa Lai", "paper_title": "Generalization ability of wide neural networks on R", "publication_date": "2023-02-01", "reason": "This paper explored the generalization ability of wide neural networks trained by gradient descent, which is closely related to the current paper's focus on generalization error."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "A convergence theory for deep learning via over-parameterization", "publication_date": "2019-06-01", "reason": "This paper established a convergence theory for deep learning via over-parameterization, which is relevant to the current paper's analysis of the convergence of wide neural networks."}, {"fullname_first_author": "Boris Hanin", "paper_title": "Random neural networks in the infinite width limit as Gaussian processes", "publication_date": "2021-07-01", "reason": "This paper provided crucial results on the limit distribution of randomly initialized neural networks, which underpins the current paper's analysis of the impact of initialization."}]}