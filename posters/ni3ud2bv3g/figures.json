[{"figure_path": "ni3Ud2BV3G/figures/figures_8_1.jpg", "caption": "Figure 1: Generalization error decay curve of network. The scatter points show the averaged log error over 20 trials. The dashed lines are computed through least-squares. The scale of n is not broad because a larger n requires a larger m, which would induce higher computational costs.", "description": "This figure compares the generalization error decay curves for networks trained with mirrored and standard initializations.  Two separate plots show results for datasets with 5 and 10 dimensions, respectively. Each plot displays the logarithm of the generalization error (y-axis) against the logarithm of the number of samples (x-axis). The data points represent the average error over 20 trials.  Linear regression lines highlight the different slopes of error decay under the two initialization methods, demonstrating a steeper decline for mirrored initialization.", "section": "5.1 Artificial data"}, {"figure_path": "ni3Ud2BV3G/figures/figures_26_1.jpg", "caption": "Figure 2: Decay curve of the logarithm of sum of squared coefficients for NMIST.", "description": "This figure displays the decay curve of the logarithm of the sum of squared coefficients for the MNIST dataset.  The x-axis represents the logarithm of the index i, and the y-axis represents the logarithm of the sum of squared coefficients from index i to n (where n is the total number of samples, 3000 in this case). The blue dots show the actual data points, and the red dashed line represents a least-squares regression fit to these points, with the slope of the line indicating the decay rate (-0.40 in this case). The figure visually demonstrates the decay of coefficients, illustrating how the contribution of each component decreases as i increases, which is related to the smoothness of the goal function.", "section": "5.2 Real data"}, {"figure_path": "ni3Ud2BV3G/figures/figures_26_2.jpg", "caption": "Figure 3: Decay curve of the logarithm of sum of squared coefficients for Fashion-MNIST.", "description": "This figure shows the decay curve of the logarithm of the sum of squared coefficients for the Fashion-MNIST dataset. The x-axis represents the logarithm of the index i, and the y-axis represents the logarithm of the sum of squared coefficients from index i to n (3000 in this case). The dashed red line is a linear regression fit to the data points, and its slope represents the estimated smoothness of the function. The figure visually demonstrates the decay rate of the coefficients, which is related to the smoothness of the underlying function.", "section": "5.2 Real data"}, {"figure_path": "ni3Ud2BV3G/figures/figures_27_1.jpg", "caption": "Figure 2: Decay curve of the logarithm of sum of squared coefficients for NMIST.", "description": "This figure displays the decay curve of the logarithm of the sum of squared coefficients for the MNIST dataset.  The x-axis represents the logarithm of the index i, and the y-axis represents the logarithm of the sum of squared coefficients from i to n (where n is the total number of samples). A least-squares regression line is fitted to the data points to estimate the decay rate, which provides an approximation of the smoothness of the underlying function. The figure visually demonstrates the relationship between the index i and the magnitude of the squared coefficients, illustrating how the coefficients decay as the index increases.", "section": "5.2 Real data"}]