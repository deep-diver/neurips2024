[{"figure_path": "UaJErAOssN/tables/tables_2_1.jpg", "caption": "Table 1: Comparisons of different neural network architectures for sequence modeling.", "description": "This table compares four different neural network architectures commonly used for sequence modeling: RNNs, SNNs, Transformers, and SSMs.  For each architecture, it provides a qualitative assessment of training speed, inference speed, the number of parameters, overall performance, and limitations.", "section": "2 Related work"}, {"figure_path": "UaJErAOssN/tables/tables_7_1.jpg", "caption": "Table 2: Node classification performance (%) on four small scale temporal graphs. The best and the second best results are highlighted as red and blue, respectively.", "description": "This table presents the performance comparison of various node classification methods on four small-scale temporal graphs (DBLP-3, Brain, Reddit, DBLP-10).  The results are reported using Micro-F1 and Macro-F1 scores, with the best and second-best performances highlighted for each dataset.  The table allows for a direct comparison of GRAPHSSM against several established baselines, including static graph embedding methods, temporal graph embedding methods, and temporal graph neural networks. ", "section": "4.1 Experimental results"}, {"figure_path": "UaJErAOssN/tables/tables_7_2.jpg", "caption": "Table 3: Node classification performance (%) on large scale temporal graphs. OOM: out-of-memory.", "description": "This table compares the performance of various methods on two large-scale temporal graph datasets (arXiv and Tmall) in terms of Micro-F1 and Macro-F1 scores for node classification.  It highlights the scalability challenges faced by some methods, indicated by \"OOM\" (out of memory) entries, showcasing GRAPHSSM's superior performance on large, long-range temporal graph datasets.", "section": "Scalability to large temporal graphs"}, {"figure_path": "UaJErAOssN/tables/tables_7_3.jpg", "caption": "Table 2: Node classification performance (%) on four small scale temporal graphs. The best and the second best results are highlighted as red and blue, respectively.", "description": "This table presents the performance comparison of different methods on four small-scale temporal graph datasets (DBLP-3, Brain, Reddit, DBLP-10) in terms of node classification.  The metrics used are Micro-F1 and Macro-F1 scores.  The best and second-best results for each metric and dataset are highlighted in red and blue, respectively.  The table shows that the proposed GRAPHSSM method outperforms other methods on most datasets.", "section": "4.1 Experimental results"}, {"figure_path": "UaJErAOssN/tables/tables_8_1.jpg", "caption": "Table 5: Ablation results (%) of GRAPHSSM-S4 with different mixing configurations.", "description": "This table presents the ablation study results on the impact of different mixing mechanisms used in the GRAPHSSM-S4 model. It compares the performance (Micro-F1 and Macro-F1 scores) of four configurations: no mixing, feature mixing only in the first layer, representation mixing only in the first layer, and representation mixing in the second layer. The results are presented for four different datasets: DBLP-3, Brain, Reddit, and DBLP-10.", "section": "4.1 Experimental results"}, {"figure_path": "UaJErAOssN/tables/tables_20_1.jpg", "caption": "Table 2: Node classification performance (%) on four small scale temporal graphs. The best and the second best results are highlighted as red and blue, respectively.", "description": "This table presents the performance comparison of different methods on four small-scale temporal graph datasets (DBLP-3, Brain, Reddit, DBLP-10) in terms of node classification.  The metrics used are Micro-F1 and Macro-F1 scores, representing the average F1 score across all classes and the macro-averaged F1 score, respectively.  The best and second-best results for each dataset are highlighted for easy comparison. The table includes both static graph embedding methods and temporal graph neural network methods.", "section": "4.1 Experimental results"}]