{"references": [{"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-06-01", "reason": "This paper introduced the VQGAN model, a crucial component for the proposed TiTok model, significantly improving image generation quality."}, {"fullname_first_author": "Huiwen Chang", "paper_title": "MaskGIT: Masked generative image transformer", "publication_date": "2022-06-01", "reason": "The MaskGIT framework is used as the basis for image generation in this work, providing the methodology for the TiTok model to generate images."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, a key component of many modern multimodal LLMs, is introduced and used extensively for image understanding tasks in this field."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduced Vision Transformer (ViT), a critical element of TiTok's architecture, revolutionizing image processing using transformers."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "The MAE framework provides a foundation for the self-supervised pretraining and two-stage training strategy, improving training stability and image quality."}]}