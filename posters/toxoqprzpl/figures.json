[{"figure_path": "tOXoQPRzPL/figures/figures_0_1.jpg", "caption": "Figure 1: We propose TiTok, a compact 1D tokenizer leveraging region redundancy to represent an image with only 32 tokens for image reconstruction and generation.", "description": "This figure demonstrates the core idea of the TiTok model.  It visually compares image reconstruction and generation results using different numbers of tokens. The top row shows the results using TiTok (32 tokens), showcasing high-quality reconstruction and generation. The middle row shows results using the VQGAN (256 tokens), illustrating a noticeable reduction in quality. The bottom row is of the original images,  highlighting the significant compression achieved by TiTok. The figure highlights TiTok's ability to effectively represent images using just 32 tokens, in contrast to existing methods requiring significantly more.", "section": "Abstract"}, {"figure_path": "tOXoQPRzPL/figures/figures_2_1.jpg", "caption": "Figure 2: A speed and quality comparison of TiTok and prior arts on ImageNet 256 \u00d7 256 and 512 \u00d7 512 generation benchmarks. Speed-up is compared against DiT-XL/2 [52]. The sampling speed (de-tokenization included) is measured with an A100 GPU.", "description": "This figure compares the speed and image quality (measured by FID score) of the proposed TiTok model against several state-of-the-art image generation models on ImageNet datasets with resolutions of 256x256 and 512x512.  The speed-up is calculated relative to the DiT-XL/2 model, and all speed measurements are done using an A100 GPU.  The figure demonstrates that TiTok achieves competitive or superior image quality with significantly faster generation times.", "section": "Main Experiments"}, {"figure_path": "tOXoQPRzPL/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of image reconstruction (a) and generation (b) with the TiTok framework (c). TiTok contains an encoder Enc, a quantizer Quant, and a decoder Dec. Image patches, along with a few (e.g., 32) latent tokens, are passed through the Vision Transformer (ViT) encoder. The latent tokens are then vector-quantized. The quantized tokens, along with the mask tokens [15, 24], are fed to the ViT decoder to reconstruct the image.", "description": "This figure illustrates the TiTok framework for image reconstruction and generation.  It shows the process of encoding image patches and latent tokens using a Vision Transformer (ViT) encoder, vector quantization of latent tokens, and decoding using a ViT decoder to reconstruct the image.  The generation process involves masking tokens and using a bidirectional transformer to predict masked tokens, which are then decoded into an image. The figure also demonstrates the overall architecture of TiTok, which includes an encoder, quantizer, and decoder.", "section": "3 Method"}, {"figure_path": "tOXoQPRzPL/figures/figures_6_1.jpg", "caption": "Figure 4: Preliminary experimental results with different TiTok variants. We provide a comprehensive exploration in (a) ImageNet-1K reconstruction. (b) ImageNet-1K linear probing. (c) ImageNet-1K generation. (d) Training and inference throughput of MaskGIT-ViT as generator and TiTok as tokenizer (evaluated on A100 GPUs, inference includes de-tokenization step with TiTok-B). Detailed numbers can be found in supplementary material Sec. B.", "description": "This figure presents the results of preliminary experiments conducted to analyze the impact of various factors on TiTok's performance. It displays four sub-figures: (a) Reconstruction FID on ImageNet-1K; (b) Linear probe accuracy on ImageNet-1K; (c) Generation FID on ImageNet-1K; and (d) Sampling speed. Each sub-figure shows how different variants of TiTok perform with different numbers of latent tokens (from 16 to 256).", "section": "4.1 Preliminary Experiments of 1D Tokenization"}, {"figure_path": "tOXoQPRzPL/figures/figures_17_1.jpg", "caption": "Figure 5: Visualization of generated images from TiTok variants with MaskGIT [9]. Corresponding ImageNet class names are shown below the images.", "description": "This figure displays sample images generated using the MaskGIT framework with different TiTok variants (TiTok-L-32, TiTok-B-64, TiTok-S-128). Each row represents a different TiTok variant, and each column shows a generated image corresponding to a specific ImageNet class (macaw, lion, jack-o'-lantern, orange, daisy, bubble, valley).  The figure demonstrates the ability of TiTok to generate images at different levels of detail and quality based on the number of tokens used and the model size. The ImageNet class name is displayed below each column of images.", "section": "4 Experimental Results"}, {"figure_path": "tOXoQPRzPL/figures/figures_17_2.jpg", "caption": "Figure 6: Visualization of generated images from TiTok-L-32 with MaskGIT [9] across random ImageNet classes.", "description": "This figure shows a diverse set of images generated using the TiTok-L-32 model in conjunction with the MaskGIT framework.  The images represent a wide range of ImageNet classes, highlighting the model's ability to generate varied and high-quality samples across different visual concepts and styles. The diversity showcased demonstrates the model's robustness and capacity to generalize well to unseen data.", "section": "4 Experimental Results"}, {"figure_path": "tOXoQPRzPL/figures/figures_18_1.jpg", "caption": "Figure 7: Visual comparison of reconstruction results. Scaling model size enables a better image quality while using a more compact latent space size. It is also observed that TiTok tends to keep the salient regions when latent space is limited.", "description": "This figure shows the reconstruction results obtained using different model sizes (S, B, L) and numbers of latent tokens (16, 32, 64, 128, 256). Each row represents a different class of images, and each column represents a different number of latent tokens. The results demonstrate that larger model sizes lead to better image quality, even with fewer latent tokens. This is because larger models are able to learn more complex representations of the images, which allows them to reconstruct the images more accurately even when the number of latent tokens is limited.", "section": "4 Experimental Results"}, {"figure_path": "tOXoQPRzPL/figures/figures_19_1.jpg", "caption": "Figure 8: Uncurated 256 \u00d7 256 TiTok-L-32 samples. Class labels (class ids) from left to right and top to down are: \u201cmacaw", "description": "This figure displays a collection of 256x256 images generated using the TiTok-L-32 model in an uncurated fashion, meaning the images were not specially selected or filtered.  The caption indicates the model variant used (TiTok-L-32), the image resolution (256x256), and lists the ImageNet class labels for each row of images from left to right and top to bottom. This provides a visual representation of the model's ability to generate diverse and realistic-looking images from random class labels.", "section": "Visualizations"}, {"figure_path": "tOXoQPRzPL/figures/figures_19_2.jpg", "caption": "Figure 7: Visual comparison of reconstruction results. Scaling model size enables a better image quality while using a more compact latent space size. It is also observed that TiTok tends to keep the salient regions when latent space is limited.", "description": "This figure shows the results of image reconstruction experiments using different model sizes and numbers of tokens.  The top row demonstrates how increasing the number of tokens improves reconstruction quality, but this improvement plateaus after a certain number of tokens.  The subsequent rows show how increasing the model size allows for comparable or better reconstruction quality even with fewer tokens.  This indicates that a larger model can better utilize image redundancy and achieve high-quality results even with very compact latent representations.", "section": "4 Experimental Results"}]