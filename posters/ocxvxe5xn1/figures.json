[{"figure_path": "ocxVXe5XN1/figures/figures_4_1.jpg", "caption": "Figure 1: Comparison of different \u03c6*\u22121 (Left) and examples of x \u2212 ax\u00b2 for lower-bounding log(1 + x) (Right).", "description": "The figure compares various functions that can be used to lower-bound log(1+x), a key component in deriving f-information-based generalization bounds in the paper. The left panel shows the functions: log(1+x), 2(\u221ax+1-1), x/(x+1), and log(2-e^-x), which are inverse functions of the convex conjugates of different f-divergences.  The right panel zooms in on the region near x=0 to highlight how well x-ax\u00b2 approximates the target log(1+x) function for various values of 'a'. Different values of 'a' yield different levels of approximation.", "section": "3 Conditional f-Information Bounds: Bounded Loss Difference Case"}, {"figure_path": "ocxVXe5XN1/figures/figures_8_1.jpg", "caption": "Figure 2: Comparison of bounds on synthetic dataset, MNIST (\"4 vs 9\"), and CIFAR10. (a-b) Linear classification for two-class and ten-class data. (c-d) Dynamics of generalization bounds as dataset size changes. (e-f) Dynamics of generalization bounds during SGLD training.", "description": "The figure compares various generalization bounds (CMI(Oracle), CSHI(Oracle), CJSI(Oracle), Id-CMI, Sharpness, Binary kl) across different datasets and training methods. It shows the error rate and bound values plotted against the number of samples (n) or training epochs.  Subfigures (a) and (b) illustrate results for linear classifiers on two-class and ten-class synthetic Gaussian data. Subfigures (c) and (d) present results for CNN and ResNet-50 models trained on MNIST and CIFAR10 datasets, showcasing how bounds evolve with increasing dataset size. Subfigures (e) and (f) display bound behavior during stochastic gradient Langevin dynamics (SGLD) training on MNIST and CIFAR10 datasets.", "section": "5 Numerical Results"}]