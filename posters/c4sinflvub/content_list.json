[{"type": "text", "text": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thomas Nagler\u2217 Lennart Schneider\u2217 Bernd Bischl Matthias Feurer t.nagler@lmu.de ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics, LMU Munich Munich Center for Machine Learning (MCML) ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hyperparameter optimization is crucial for obtaining peak performance of machine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide optimization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed crossvalidation scheme, are often recommended. We show that, surprisingly, reshuffilng the splits for every configuration often improves the final model\u2019s generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simulation study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hyperparameters have been shown to strongly influence the performance of machine learning models (van Rijn & Hutter, 2018; Probst et al., 2019). The primary goal of hyperparameter optimization (HPO; also called tuning) is the identification and selection of a hyperparameter configuration (HPC) that minimizes the estimated generalization error (Feurer & Hutter, 2019; Bischl et al., 2023). Typically, this task is challenged by the absence of a closed-form mathematical description of the objective function, the unavailability of an analytic gradient, and the large cost to evaluate HPCs, categorizing HPO as a noisy, black-box optimization problem. An HPC is evaluated via resampling, such as a holdout split or $M$ -fold cross-validation (CV), during tuning. ", "page_idx": 0}, {"type": "text", "text": "These resampling splits are usually constructed in a fixed and instantiated manner, i.e., the same training and validation splits are used for the internal evaluation of all configurations. On the one hand, this is an intuitive approach, as it should facilitate a fair comparison between HPCs and reduce the variance in the comparison.1 On the other hand, such a fixing of train and validation splits might steer the optimization, especially after a substantial budget of evaluations, towards favoring HPCs which are specifically tailored to the chosen splits. Such and related effects, where we \"overoptimize\" the validation performance without effective reward in improved generalization performance have been sometimes dubbed \"overtuning\" or \"oversearching\". For a more detailed discussion of this topic, including related work, see Section 5 and Appendix B. The practice of reshuffilng resampling splits during HPO is generally neither discussed in the scientific literature nor HPO software tools.2 To the best of our knowledge, only L\u00e9vesque (2018) investigated reshuffilng train-validation splits for every new HPC. For both holdout and $M$ -fold CV using reshuffled resampling splits resulted in, on average, slightly lower generalization error when used in combination with Bayesian optimization (BO, Garnett, 2023) or CMA-ES (Hansen & Ostermeier, 2001) as HPO algorithms. Additionally, reshuffling was used by a solution to the NeurIPS 2006 performance prediction challenge to estimate the final generalization performance (Guyon et al., 2006). Recently, in the context of evolutionary optimization, reshuffling was applied after every generation (Larcher & Barbosa, 2022). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we systematically examine the effect of reshuffilng on HPO performance. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We show theoretically that reshuffling resampling splits during HPO can result in finding a configuration with better overall generalization performance, especially when the loss surface is rather flat and its estimate is noisy (Section 2).   \n2. We confirm these theoretical insights through controlled simulation studies (Section 3).   \n3. We demonstrate in realistic HPO benchmark experiments that reshuffilng splits can lead to a real-world improvement of HPO (Section 4). Especially in the case of reshuffled holdout, we find that the final generalization performance is often on par with 5-fold CV under a wide range of settings. ", "page_idx": 1}, {"type": "text", "text": "We discuss results, limitations, and avenues for future research in Section 5. ", "page_idx": 1}, {"type": "text", "text": "2 Theoretical Analysis ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Problem Statement and Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Machine learning (ML) aims to fit a model to data, so that it generalizes well to new observations of the same distribution. Let $\\mathbf{\\mathcal{D}}\\,=\\,\\{{Z_{i}}\\}_{i=1}^{n}$ be the observed dataset consisting of i.i.d. random variables from a distribution $P$ , i.e., in the supervised setting $Z_{i}\\ =\\ (X_{i},Y_{i})$ .3,4 Formally, an inducer $g$ configured by an HPC $\\pmb{\\lambda}\\in\\Lambda$ maps a dataset $\\mathcal{D}$ to a model from our hypothesis space $h=g_{\\mathsf{\\lambda}}({\\mathcal{D}})\\in{\\mathcal{H}}$ . During HPO, we want to find a HPC that minimizes the expected generalization error, i.e., find ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\arg\\operatorname*{min}_{\\lambda\\in\\Lambda}\\mu(\\lambda),\\quad\\mathrm{where}\\quad\\mu(\\lambda)=\\mathbb{E}[\\ell(Z,g_{\\lambda}(\\mathcal{D}))],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\ell(\\boldsymbol{Z},h)$ is the loss of model $h$ on a fresh observation $_{z}$ . In practice, there is usually a limited computational budget for each HPO run, so we assume that there is only a finite number of distinct HPCs $\\bar{\\Lambda}=\\{\\lambda_{1},\\bar{.}\\bar{.}\\bar{.},\\lambda_{J}\\}$ to be evaluated, which also simplifies the subsequent analysis. Naturally, we cannot optimize the generalization error directly, but only an estimate of it. To do so, a resampling is constructed. For every $\\mathrm{HPC}\\,\\lambda_{j}$ , draw $M$ random sets $\\bar{\\mathcal{T}}_{1,j},\\ldots,\\bar{\\mathcal{T}}_{M,j}\\subset\\{1,\\ldots,n\\}$ of validation indices with $n_{\\mathrm{valid}}=\\lceil\\dot{\\alpha}n\\rceil$ instances each. The random index draws are assumed to be independent of the observed data. The data is then split accordingly into pairs $\\nu_{m,j}\\,=\\,\\{Z_{i}\\}_{i\\in{\\mathbb{Z}}_{m,j}},T_{m,j}\\,=$ $\\{Z_{i}\\}_{i\\notin{\\cal Z}_{m,j}}$ of disjoint validation and training sets. Define the validation loss on the $m$ -th fold ", "page_idx": 1}, {"type": "equation", "text": "$$\nL(\\mathcal{V}_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))=\\frac{1}{n_{\\mathrm{valid}}}\\sum_{i\\in\\mathbb{Z}_{m,j}}\\ell(\\mathbf{Z}_{i},g_{\\lambda_{j}}(\\mathcal{T}_{m,j})),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "and the $M$ -fold validation loss as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mu}(\\lambda_{j})=\\frac{1}{M}\\sum_{m=1}^{M}L(\\mathcal{V}_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j})).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since $\\mu$ is unknown, we minimize $\\widehat{\\pmb{\\lambda}}\\,=\\,\\arg\\operatorname*{min}_{\\pmb{\\lambda}\\in\\Lambda}\\widehat{\\mu}(\\pmb{\\lambda})$ , hoping that $\\mu(\\widehat{\\lambda})$ will also be small. Typically, the same splits are used  for every HPC, so $\\mathcal{T}_{m,j}\\,=\\,\\mathcal{T}_{m}$ for all $j=1,\\dots,J$ and $m=$ $1,\\dots,M$ . In the following, we investigate how reshuffilng train-validation splits (i.e., ${\\mathcal{I}}_{m,j}\\neq{\\mathcal{I}}_{m,j^{\\prime}}$ for $j\\neq j^{\\prime}$ ) affects the HPO problem. ", "page_idx": 2}, {"type": "text", "text": "2.2 How Reshuffling Affects the Loss Surface ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first investigate how different validation and reshuffilng strategies a\u221affect the empirical loss surface $\\widehat{\\mu}$ . In particular, we derive the limiting distribution of the sequence $\\sqrt{n}(\\widehat{\\mu}(\\lambda_{j})\\stackrel{\\cdot}{-}\\mu(\\lambda_{j}))_{j=1}^{J}$ . This limiting regime will not only reveal the effect of reshuffling on the loss surface, but also give us a tractable setting to study HPO performance. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1. Under regularity conditions stated in Appendix C.1, it holds ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left(\\widehat{\\mu}(\\lambda_{j})-\\mu(\\lambda_{j})\\right)_{j=1}^{J}\\to\\mathcal{N}(0,\\Sigma)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Sigma_{i,j}=\\tau_{i,j,M}K(\\lambda_{i},\\lambda_{j}),\\quad\\tau_{i,j,M}=\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n M^{2}\\alpha^{2}}\\sum_{s=1}^{n}\\sum_{m=1}^{M}\\sum_{m^{\\prime}=1}^{M}\\operatorname*{Pr}(s\\in\\mathbb{Z}_{m,i}\\cap\\mathbb{Z}_{m^{\\prime},j}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and ", "page_idx": 2}, {"type": "equation", "text": "$$\nK(\\lambda_{i},\\lambda_{j})=\\operatorname*{lim}_{n\\to\\infty}\\mathsf{C o v}[\\bar{\\ell}_{n}(Z^{\\prime},\\lambda_{i}),\\bar{\\ell}_{n}(Z^{\\prime},\\lambda_{j})],\\quad\\bar{\\ell}_{n}(z,\\lambda)=\\mathbb{E}[\\ell(z,g_{\\lambda}(\\mathcal{T}))]-\\mathbb{E}[\\ell(Z,g_{\\lambda}(\\mathcal{T}))],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the expectation is taken over a training set $\\tau$ of size n and two fresh samples $Z,Z^{\\prime}$ from the same distribution. ", "page_idx": 2}, {"type": "text", "text": "The regularity conditions are rather mild and discussed further in Appendix C.1. The kernel $K$ reflects the (co-)variability of the losses caused by validation samples. The contribution of training samples only has a higher-order effect. The validation scheme enters the distribution through the quantities $\\tau_{i,j,M}.$ . In what follows, we compute explicit expressions for some popular examples. The following list provides formal definitions for the index sets $\\mathcal{T}_{m,j}$ . ", "page_idx": 2}, {"type": "text", "text": "(i) (holdout) Let $M=1$ and $\\mathcal{T}_{1,j}=\\mathcal{T}_{1}$ for all $j=1,\\dots,J$ , and some size- $\\lceil\\alpha n\\rceil$ index set ${\\mathcal{T}}_{1}$ .   \n(ii) (reshuffled holdout) Let $M=1$ and $\\mathcal{T}_{1,1},\\dotsc,\\mathcal{T}_{1,J}$ be independently drawn from the uniform distribution over all size- $\\lceil\\alpha n\\rceil$ subsets from $\\{1,\\ldots,n\\}$ .   \n(iii) ( $M$ -fold CV) Let $\\alpha=1/M$ and $\\mathcal{T}_{1},\\dots,\\mathcal{T}_{M}$ be a disjoint partition of $\\{1,\\ldots,n\\}$ , and $\\mathcal{Z}_{m,j}=$ ${\\mathcal{T}}_{m}$ for all $j=1,\\dots,J$ .   \n(iv) (reshuffled $M$ -fold CV) Let $\\alpha=1/M$ and $({\\mathcal{T}}_{1,j},\\ldots,{\\mathcal{T}}_{M,j}),j=1,\\ldots,J.$ , be independently drawn from the uniform distribution over disjoint partitions of $\\{1,\\ldots,n\\}$ .   \n(v) ( $M$ -fold holdout) Let $\\mathcal{I}_{m},m=1,\\ldots,M$ , be independently drawn from the uniform distribution over size- $\\lceil\\alpha n\\rceil$ subsets of $\\{1,\\ldots,n\\}$ and set $\\mathcal{T}_{m,j}=\\mathcal{T}_{m}$ for all $m=1,\\ldots,M,j=1,\\ldots,J$ .   \n(vi) (reshuffled $M$ -fold holdout) Let $\\mathcal{Z}_{m,j},m=1,\\ldots,M,j=1,\\ldots,J$ , be independently drawn from the uniform distribution over size- $\\lceil\\alpha n\\rceil$ subsets of $\\{1,\\ldots,n\\}$ . ", "page_idx": 2}, {"type": "text", "text": "The value of $\\tau_{i,j,M}$ for each example is computed explicitly in Appendix E. In all these examples, we in fact have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\left\\{\\sigma^{2},\\quad\\quad i=j\\right.,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for some method-dependent parameters $\\sigma,\\tau$ shown in Table 1. The parameter $\\sigma^{2}$ captures any increase in variance caused by omitting an observation from the validation sets. The parameter $\\tau$ quantifies a potential decrease in correlation in the loss surface due to reshuffling. More precisely, the observed losses $\\widehat{\\mu}(\\lambda_{i}),\\widehat{\\mu}(\\lambda_{j})$ at distinct HPCs $\\lambda_{i}\\ \\neq\\lambda_{j}$ become less correlated when $\\tau$ is small. Generally, an  i ncreas e  in variance leads to worse generalization performance. The effect of a correlation decrease is less obvious and is studied in detail in the following section. ", "page_idx": 2}, {"type": "table", "img_path": "C4SInFLvuB/tmp/55c1e91a539647a40f8e8da07c81b4cca6d557c5a16d47484b5ebded6dd9bbbc.jpg", "table_caption": ["Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We make the following observations about the differences between methods in Table 1: ", "page_idx": 3}, {"type": "text", "text": "\u2022 $M$ -fold CV incurs no increase in variance $\\sigma^{2}=1$ ) and \u2014 because every HPC uses the same folds \u2014 no decrease in correlation. Interestingly, the correlation does not even decrease when reshuffilng the folds. In any case, all samples are used exactly once as validation and training instance. At least asymptotically, this leads to the same behavior, and reshuffling should have almost no effect on $M$ -fold CV.   \n\u2022 The two (1-fold) holdout methods bear the same $1/\\alpha$ increase in variance. This is caused by only using a fraction $\\alpha$ of the data as validation samples. Reshuffled holdout also decreases the correlation parameter $\\tau^{2}$ . In fact, if HPCs $\\lambda_{i}\\neq\\lambda_{j}$ are evaluated on largely distinct samples, the validation losses $\\widehat{\\mu}(\\lambda_{i})$ and $\\widehat{\\mu}(\\lambda_{j})$ become almost independent.   \n\u2022 $M$ -fold holdout also increases the variance, because some samples may still be omitted from validation sets. This increase is much smaller for large $M$ . Accordingly, the correlation is also decreased by less in the reshuffled variant. ", "page_idx": 3}, {"type": "text", "text": "2.3 How Reshuffling Affects HPO Performance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In practice, we are mainly interested in the performance of a model trained with the optimal HPC\u03bb. To simplify the analysis, we explore this in the large-sample regime derived in the previous secti on. Assume ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\mu}(\\lambda_{j})=\\mu(\\lambda_{j})+\\epsilon(\\lambda_{j})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon(\\lambda)$ is a zero-mean Gaussian process with covariance kernel ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{C o v}(\\epsilon(\\lambda),\\epsilon(\\lambda^{\\prime}))=\\left\\{\\!\\!\\begin{array}{l l}{{K(\\lambda,\\lambda)}}&{{\\mathrm{if}\\;\\lambda=\\lambda^{\\prime},}}\\\\ {{\\tau^{2}K(\\lambda,\\lambda^{\\prime})}}&{{\\mathrm{else}.}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\Lambda\\subseteq\\{\\pmb{\\lambda}\\in\\mathbb{R}^{d}\\colon\\|\\pmb{\\lambda}\\|\\leq1\\}$ with $|\\Lambda|=J<\\infty$ be the set of hyperparameters. Theorem 2.2 ahead gives a bound on the expected regret $\\mathbb{E}[\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})]$ . It depends on several quantities characterizing the difficulty of the HPO problem. The constant ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\kappa=\\operatorname*{sup}_{\\|\\pmb{\\lambda}\\|,\\|\\pmb{\\lambda}^{\\prime}\\|\\leq1}\\frac{|K(\\pmb{\\lambda},\\pmb{\\lambda})-K(\\pmb{\\lambda},\\pmb{\\lambda}^{\\prime})|}{K(\\pmb{\\lambda},\\pmb{\\lambda})\\|\\pmb{\\lambda}-\\pmb{\\lambda}^{\\prime}\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "can be interpreted as a measure of correlation of the process $\\epsilon$ . In particular, ${\\sf C o r r}(\\epsilon(\\lambda),\\epsilon(\\lambda^{\\prime}))\\geq$ $1-\\kappa\\|\\pmb{\\lambda}-\\bar{\\pmb{\\lambda}}^{\\prime}\\|^{2}$ . The constant is small when $\\epsilon$ is strongly correlated, and large otherwise. Further, define $\\eta$ as the minimal number such that any $\\eta$ -ball contained in $\\{\\|\\pmb{\\lambda}\\|\\leq1\\}$ contains at least one element of $\\Lambda$ . It measures how densely the set of candidate HPCs $\\Lambda$ covers set of all possible HPCs. If $\\Lambda$ is a deterministic uniform grid, we have about $\\eta\\approx J^{-1/d}$ . Similarly, Lemma D.1 in the Appendix shows that $\\eta\\lesssim J^{-1/2d}$ when randomly sampling HPCs. Finally, the constant ", "page_idx": 3}, {"type": "equation", "text": "$$\nm=\\operatorname*{sup}_{\\lambda\\in\\Lambda}\\frac{|\\mu(\\lambda)-\\mu(\\lambda^{*})|}{\\|\\lambda-\\lambda^{*}\\|^{2}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "C4SInFLvuB/tmp/0c15323edf14c3069dae6555062289029a558c548e33ee7df6d8d19c3cb71d6c.jpg", "img_caption": ["Figure 1: Example of reshuffled empirical loss yielding a worse (left) and better (right) minimizer. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "measures the local curvature at the minimum of the loss surface $\\mu$ . Finding an HPC $\\lambda$ close to the theoretical optimum $\\lambda^{*}$ is easier when the minimum is more pronounced (large $m$ ). On the other hand, the regret $\\mu(\\lambda)-\\mu(\\lambda^{*})$ is also punishing mistakes more quickly. Defining $\\log(x)_{+}=$ $\\operatorname*{max}\\{0,\\log(x)\\}$ , we can now state our main result. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2. Let $\\widehat{\\mu}$ follow the Gaussian process model (2). Suppose $\\kappa<\\infty$ , $0<\\underline{{\\sigma}}^{2}\\leq\\mathsf{V a r}[\\epsilon(\\lambda)]\\leq$ $\\sigma^{2}<\\infty$ for all $\\lambda\\in\\Lambda$ , and $m>0$ . Then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})]\\leq\\sigma\\sqrt{d}[8+B(\\tau)-A(\\tau)].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\nB(\\tau)=48\\left[\\sqrt{1-\\tau^{2}}\\sqrt{\\log{J}}+\\tau\\sqrt{1+\\log(3\\kappa)_{+}}\\right],\\quad A(\\tau)=\\sqrt{1-\\tau^{2}}(\\sigma/\\sigma)\\sqrt{\\log\\left(\\frac{\\sigma}{2m\\eta^{2}}\\right)_{+}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The numeric constants result from several simplifications in a worst-case analysis, which lowers their practical relevance. A qualitative analysis of the bound is still insightful. The bound is increasing in $\\sigma$ and $d$ , indicating that the HPO problem is harder when there is a lot of noise or there are many parameters to tune. The terms $B(\\tau)$ and $A(\\tau)$ have conceptual interpretations: ", "page_idx": 4}, {"type": "text", "text": "\u2022 The term $B(\\tau)$ quantifies how likely it is to pick a bad $\\widehat{\\lambda}$ because of bad luck: a $\\lambda$ far away from $\\lambda^{*}$ had such a small $\\epsilon(\\lambda)$ that it outweighs the i ncrease in $\\mu$ . Such events are more likely when the process $\\epsilon$ is weakly correlated. Accordingly, $B(\\tau)$ is decreasing in $\\tau$ and increasing in $\\kappa$ . \u2022 The term $A(\\tau)$ quantifies how likely it is to pick a good $\\widehat{\\sf\\lambda}$ by luck: a $\\lambda$ close to $\\lambda^{*}$ had such a small $\\epsilon(\\lambda)$ that it overshoots all the other fluctuation s. Also such events are more likely when the process $\\epsilon$ is weakly correlated. Accordingly, the term $A(\\tau)$ is decreasing in $\\tau$ . ", "page_idx": 4}, {"type": "text", "text": "The $B$ , as stated, is unbounded, but a closer inspection of the proof shows that it is upper bounded by $\\sqrt{\\log J}$ . This bound is attained only in the unrealistic scenario when the validation losses are essentially uncorrelated across all HPCs. The term $A$ is bounded from below by zero, which is also the worst case because the term enters our regret bound with a negative sign. ", "page_idx": 4}, {"type": "text", "text": "Both $A$ and $B$ are decreasing in the reshuffilng parameter $\\tau$ . There are two regimes. If $\\sigma/2m\\eta^{2}\\leq e$ , then $A(\\tau)=0$ and reshuffilng cannot lead to an improvement of the bound. The term $\\sigma/m\\eta^{2}$ can be interpreted as noise-to-signal ratio (relative to the grid density). If the signal is much stronger than the noise, the HPO problem is so easy that reshuffling will not help. This situation is illustrated in Figure 1a. ", "page_idx": 4}, {"type": "text", "text": "If on the other hand $\\sigma/m\\eta^{2}>e$ , the terms $A(\\tau)$ and $B(\\tau)$ enter the bound with opposing signs. This creates tension: reshuffilng between HPCs increases $B(\\tau)$ , which is countered by a decrease in $A(\\tau)$ . So which scenarios favor reshuffling? When the process $\\epsilon$ is strongly correlated, $\\kappa$ is small and reshuffling (decreasing $\\tau$ ) incurs a high cost in $B(\\tau)$ . This is intuitive: When there is strong correlation, the validation loss surface $\\widehat{\\mu}$ is essentially just a vertical shift of $\\mu$ . Finding the optimal $\\lambda$ is then almost as easy as if we woul d  know $\\mu$ , and decorrelating the surface through reshuffling would make it unnecessarily hard. When $\\epsilon$ is less correlated $\\kappa$ large) however, reshuffling does not hurt the term $B(\\tau)$ as much, but we can reap all the benefits of increasing $A(\\tau)$ . Here, the effect of reshuffling can be interpreted as hedging against the catastrophic case where all $\\widehat{\\mu}(\\lambda)$ close to the optimal $\\lambda^{*}$ are simultaneously dominated by a region of bad hyperparameters. This is illustrated in Figure 1b. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3 Simulation Study ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To test our theoretical understanding of the potential beneftis of reshuffilng resampling splits during HPO, we conduct a simulation study. This study helps us explore the effects of reshuffling in a controlled setting. ", "page_idx": 5}, {"type": "text", "text": "3.1 Design ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We construct a univariate quadratic loss surface function $\\mu:\\Lambda\\subset\\mathbb{R}\\mapsto\\mathbb{R},\\lambda\\to m(\\lambda-0.5)^{2}/2$ which we want to minimize. The global minimum is given at $\\mu(0.5)\\,=\\,0$ . Combined with a kernel for the noise process $\\epsilon$ as in Equation (3), this allows us to simulate an objective as observed during HPO by sampling $\\widehat{\\mu}(\\lambda)\\overline{{{\\;}}}=\\;\\mu(\\lambda)\\,+\\,\\epsilon(\\lambda)$ . We use a squared exponential kernel $K(\\lambda,\\lambda^{\\prime})=\\bar{\\sigma}_{K}^{2}\\exp{(-\\bar{\\kappa}(\\lambda-\\lambda^{\\prime})^{2}/2)}$ that is plugged into the covariance kernel of the noise process $\\epsilon$ in Equation (3). The parameters $m$ and $\\kappa$ in our simulation setup correspond exactly to the curvature and correlation constants from the previous sections. Recall that Theorem 2.2 states that the effect of reshuffling strongly depends on the curvature $m$ of the loss surface $\\mu$ (a larger $m$ implies a stronger curvature) and the constant $\\kappa$ as a measure of correlation of the noise $\\epsilon$ (a larger $\\kappa$ implies weaker correlation). Combined with the possibility to vary $\\tau$ in the covariance kernel of $\\epsilon$ , we can systematically investigate how curvature of the loss surface, correlation of the noise and the extent of reshuffling affect optimization performance. In each simulation run, we simulate the observed objective $\\hat{\\mu}(\\lambda)$ , identify the minimizer $\\hat{\\lambda}=\\arg\\operatorname*{min}_{\\lambda\\in\\Lambda}\\hat{\\mu}(\\lambda).$ , and calculate its true risk, $\\mu(\\hat{\\lambda})$ . We repeat this process 10000 times for various combinations of $\\tau$ , $m$ , and $\\kappa$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Figure 2 visualizes the true risk of the configuration \u03bb\u02c6 that minimizes the observed objective. We observe that for a loss surface with low curvature (i.e., $m\\leq2$ ), reshuffilng is beneficial (lower values of $\\tau$ resulting in a better true risk of the configuration that optimizes the observed objective) as long as the noise process is not too correlated (i.e., $\\kappa\\geq1$ ). As soon as the noise process is more strongly correlated, even flat valleys of the true risk $\\mu$ remain clearly visible in the observed risk $\\widehat{\\mu}$ , and reshuffling starts to hurt the optimization performance. Moving to scenarios of high curvatu r e, the general relationship of $m$ and $\\kappa$ remains the same, but reshuffling starts to hurt optimization performance already with weaker correlation in the noise. In summary, the simulations show that in cases of low curvature of the loss surface, reshuffling (reducing $\\tau$ ) tends to improve the true risk of the optimized configuration, especially when the loss surface is flat (small $m$ ) and the noise is not strongly correlated (i.e., $\\kappa$ is large). This exactly confirms our theoretical predictions from the previous section. ", "page_idx": 5}, {"type": "text", "text": "4 Benchmark Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present benchmark experiments of real-world HPO problems where we investigate the effect of reshuffling resampling splits during HPO. First, we discuss the experimental setup. Second, we present results for HPO using random search (Bergstra & Bengio, 2012). Third, we also show the effect of reshuffling when applied in BO using HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). Recall that our theoretical insight suggests that 1) reshuffling might be beneficial during HPO and 2) holdout should be affected the most by reshuffilng and other resamplings should only be affected to a lesser extent. ", "page_idx": 5}, {"type": "image", "img_path": "C4SInFLvuB/tmp/dc3d6d5095d680329964bc574a1d2857e98b33f9aa865196b0683c16fbfd3da1.jpg", "img_caption": ["Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature $m$ , correlation strength $\\kappa$ of the noise (a larger $\\kappa$ implying weaker correlation), and extent of reshuffilng $\\tau$ (lower $\\tau$ increasing reshuffilng). A $\\tau$ of 1 indicates no reshuffling. Error bars represent standard errors. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As benchmark tasks, we use a set of standard HPO problems defined on small- to medium-sized tabular datasets for binary classification. We suspect the effect of the resampling variant used and whether the resampling is reshuffled to be larger for smaller datasets, where the variance of the validation loss estimator is naturally higher. Furthermore, from a practical perspective, this also ensures computational feasibility given the large number of HPO runs in our experiments. We systematically vary the learning algorithm, optimized performance metric, resampling method, whether the resampling is reshuffled, and the size of the dataset used for training and validation during HPO. Below, we outline the general experimental design and refer to Appendix F for details. ", "page_idx": 6}, {"type": "text", "text": "We used a subset of the datasets defined by the AutoML benchmark (Gijsbers et al., 2024), treating these as data generating processes (DGPs; Hothorn et al., 2005). We only considered datasets with less than 100 features to reduce the required computation time and required the number of observations to be between 10000 and 1000000; for further details see Appendix F.1. Our aim was to robustly measure the generalization performance when varying the size $n$ , which, as defined in Section 2 denotes the size of the combined data for model selection, so one training and validation set combined. First, we sampled 5000 data points per dataset for robust assessment of the generalization error; these points are not used during HPO in any way. Then, from the remaining points we sampled tasks with $\\bar{n}\\in\\{500,1000,5000\\}$ . ", "page_idx": 6}, {"type": "text", "text": "We selected CatBoost (Prokhorenkova et al., 2018) and XGBoost (Chen & Guestrin, 2016) for their state-of-the-art performance on tabular data (Grinsztajn et al., 2022; Borisov et al., 2022; McElfresh et al., 2023; Kohli et al., 2024). Additionally, we included an Elastic Net (Zou & Hastie, 2005) to represent a linear baseline with a smaller search space and a funnel-shaped MLP (Zimmer et al., 2021) as a cost-effective neural network baseline. We provide details regarding training pipelines and search spaces in Appendix F.2. ", "page_idx": 6}, {"type": "text", "text": "We conduct a random search with $500\\,\\mathrm{HPC}$ evaluations for every resampling strategy we described in Table 1, for both fixed and reshuffled splits. We always use 80/20 train-validation splits for holdout and 5-fold CVs, so that training set size (and negative estimation bias) are the same. Anytime test performance of an HPO run is assessed by re-training the current incumbent (i.e. the best HPC until the current HPO iteration based on validation performance) on all available train and validation data and evaluating its performance on the outer test set. Note we do this for scientific evaluation in this experiment; obviously, this is not possible in practice. Using random search allows us to record various metrics and afterwards simulate optimizing for different ones, specifically, we recorded accuracy, area under the ROC curve (ROC AUC) and logloss. ", "page_idx": 6}, {"type": "image", "img_path": "C4SInFLvuB/tmp/ad6dcfa8846869d94e3d35c316c278d6ff1a9a5142730fa30ff7bae4f1e5e167.jpg", "img_caption": ["Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We also investigated the effect of reshuffilng on two state-of-the-art BO variants (Eggensperger et al., 2021; Turner et al., 2021), namely HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). The experimental design was the same as for random search, except for the budget, which we reduced from 500 HPCs to $250\\,\\mathrm{HPCs}$ , and only optimized ROC AUC. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the following, we focus on the results obtained using ROC AUC. We present aggregated results over different tasks, learning algorithms and replications to get a general understanding of the effects. Unaggregated results and results involving accuracy and logloss can be found in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Results of Reshuffling Different Resamplings For each resampling (holdout, 5-fold holdout, 5-fold CV, and 5x 5-fold CV), we empirically analyze the effect of reshuffling train and validation splits during HPO. ", "page_idx": 7}, {"type": "text", "text": "In Figure 3 we exemplarily show how test performance develops over the course of an HPO run on a single task for different resamplings (with and without reshuffling). Naturally, test performance does not necessarily increase in a monotonic fashion, and especially holdout without reshuffling tends to be unstable. Its reshuffled version results in substantially better test performance. ", "page_idx": 7}, {"type": "text", "text": "Next, we look at the relative improvement (compared to standard 5-fold CV, which we consider our baseline) with respect to test ROC AUC performance of the incumbent over time in Figure 4, i.e., the difference in test performance of the incumbent between standard 5-fold CV and a different resampling protocol; hence a positive difference tells us how much better in test error we are, if we would have chosen the other protocol instead 5-fold CV. We observe that reshuffling generally results in equal or better performance compared to the same resampling protocol without reshuffling. For 5-fold holdout and especially 5-fold CV and $5\\mathrm{x}$ 5-fold CV, reshuffling has a smaller effect on relative test performance improvement, as expected. Holdout is affected the most by reshuffling and results in substantially better relative test performance compared to standard holdout. We also observe that an HPO protocol based on reshuffled holdout results in similar final test performance as standard 5-fold CV while overall being substantially cheaper due to requiring less model fits per HPC evaluation. In Appendix G.2, we further provide an ablation study on the number of folds when using $M$ -fold holdout, where we observed that \u2013 in line with our theory \u2013 the more folds are used, the less reshuffling affects $M$ -fold holdout. ", "page_idx": 7}, {"type": "image", "img_path": "C4SInFLvuB/tmp/9d47a0f8d1664ae5cb784cbccd5e2183847e85289984feeb316d2d03647535b7.jpg", "img_caption": ["Figure 4: Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over different tasks, learning algorithms and replications separately for increasing $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "However, this general trend can vary for certain combinations of classifier and performance metric, see Appendix G. Especially for logloss, we observed that reshuffling rarely is beneficial; see the discussion in Section 5. Finally, the different resamplings generally behave as expected. The more we are willing to invest compute resources into a more intensive resampling like 5-fold CV or $\\mathsf{5x}$ 5-fold CV, the better the generalization performance of the final incumbent. ", "page_idx": 8}, {"type": "text", "text": "Results for BO and Reshuffling Figure 5 shows that, generally HEBO and SMAC3 outperform random search with respect to generalization performance (i.e., comparing HEBO and SMAC3 to random search under standard holdout, or comparing under reshuffled holdout). More interestingly, HEBO, SMAC3 and random search all strongly benefti from reshuffilng. Moreover, the performance gap between HEBO and random search but also SMAC3 and random search narrows when the resampling is reshuffled, which is an interesting finding of its own: As soon as we are concerned with generalization performance of HPO and not only investigate validation performance during optimization, the choice of optimizer might have less impact on final generalization performance compared to other choices such as whether the resampling is reshuffled during HPO or not. We present results for BO and reshuffling for different resamplings in Appendix G. ", "page_idx": 8}, {"type": "image", "img_path": "C4SInFLvuB/tmp/191372e5c3c493f26f3d4d002e9510c67ebdce4f6ab81bc103102e5f1eae7005.jpg", "img_caption": ["Figure 5: Average improvement (compared to random search on standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learning algorithms and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the previous sections, we have shown theoretically and empirically that reshuffling can enhance generalization performance of HPO. The main purpose of this article is to draw attention to this surprising fact about a technique that is simple but rarely discussed. Our work goes beyond a preliminary experimental study on reshuffling (L\u00e9vesque, 2018), in that we also study the effect of reshuffilng on random search, multiple metrics and learning algorithms, and most importantly, for the first time, we provide a theoretical analysis that explains why reshuffling can be beneficial. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations To unveil the mechanisms underlying the reshuffling procedures, our theoretical analysis relies on an asymptotic approximation of the empirical loss surface. This allows us to operate on Gaussian loss surfaces, which exhibit convenient concentration and anti-concentration properties required in our proof. The latter are lacking for general distributions, which explains our asymptotic approach. The analysis was further facilitated by a loss stability assumption regarding the learning algorithms that is generally rather mild; see the discussion in Bayle et al. (2020). However, it typically fails for highly sensitive losses, which has practical consequences. In fact, Figure 9 in Appendix G shows that reshuffling usually hurts generalization for the logloss and small sample sizes. It is still an open question whether this problem can be fixed by less naive implementations of the technique. Another limitation is our focus on generalization after search through a fixed, finite set of candidates. This largely ignores the dynamic nature of many HPO algorithms, which would greatly complicate our analysis. Finally, our experiments are limited in that we restricted ourselves to tabular data and binary classification and we avoided extremely small or large datasets. ", "page_idx": 9}, {"type": "text", "text": "Relation to Overfitting The fact that generalization performance can decrease during HPO (or computational model selection in general) is sometimes known as oversearching, overtuning, or overftiting to the validation set (Quinlan & Cameron-Jones, 1995; Escalante et al., 2009; Koch et al., 2010; Igel, 2012; Bischl et al., 2023), but has arguably not been studied very thoroughly. Given recent theoretical (Feldman et al., 2019) and empirical (Purucker & Beel, 2023) findings, we expect less overtuning on multi-class datasets, making it interesting to see how reshuffilng would affect the generalization performance. ", "page_idx": 9}, {"type": "text", "text": "Several works suggest strategies to counteract this effect. First, LOOCVCV proposes a conservative choice of incumbents $(\\mathrm{Ng},1997)$ at the cost of leave-one-out analysis or an additional hyperparameter. Second, it is possible to use an extra selection set (Igel, 2012; L\u00e9vesque, 2018; Mohr et al., 2018) at the cost of reduced training data, which was found to lead to reduced overall performance (L\u00e9vesque, 2018). Third, by using early stopping one can stop hyperparameter optimization before the generalization performance degrades again. This was so far demonstrated to be able to save compute budget at only marginally reduced performance, but also requires either a sensitivity hyperparameter or correct estimation of the variance of the generalization estimate and was only developed for cross-validation so far (Makarova et al., 2022). Reshuffilng itself is orthogonal to these proposals and a combination with the above-mentioned methods might result in further improvements. ", "page_idx": 9}, {"type": "text", "text": "Outlook Generally, the related literature detects overftiting to the validation set either visually $\\mathrm{(Ng_{\\mathrm{:}}}$ , 1997) or by measuring it (Koch et al., 2010; Igel, 2012; Fabris & Freitas, 2019). Developing a unified formal definition of the above-mentioned terms and thoroughly analyzing the effect of decreased generalization performance after many HPO iterations and how it relates to our measurements of the validation performance is an important direction for future work. ", "page_idx": 9}, {"type": "text", "text": "We further found, both theoretically and experimentally, that investing more resources when evaluating each HPC can result in better final HPO performance. To reduce the computational burden on HPO again, we suggest further investigating the use of adaptive CV techniques, as proposed by AutoWEKA (Thornton et al., 2013) or under the name Lazy Paired Hyperparameter Tuning (Zheng & Bilenko, 2013). Designing more advanced HPO algorithms exploiting the reshuffling effect should be a promising avenue for further research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Martin Binder and Florian Karl for helpful discussions. Lennart Schneider is supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy through the Center for Analytics - Data - Applications (ADACenter) within the framework of BAYERN DIGITAL II (20-3410-2-9-8). Lennart Schneider acknowledges funding from the LMU Mentoring Program of the Faculty of Mathematics, Informatics and Statistics. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Arlot, S. and Celisse, A. A survey of cross-validation procedures for model selection. Statistics Surveys, 4:40 \u2013 79, 2010. B ", "page_idx": 10}, {"type": "text", "text": "Austern, M. and Zhou, W. Asymptotics of cross-validation. arXiv:2001.11111 [math.ST], 2020. C.1   \nAwad, N., Mallik, N., and Hutter, F. DEHB: Evolutionary hyberband for scalable, robust and efficient Hyperparameter Optimization. In Zhou, Z. (ed.), Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI\u201921), pp. 2147\u20132153, 2021. B   \nBayle, P., Bayle, A., Janson, L., and Mackey, L. Cross-validation confidence intervals for test error. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.-F., and Lin, H. (eds.), Proceedings of the 33rd International Conference on Advances in Neural Information Processing Systems (NeurIPS\u201920), pp. 16339\u201316350. Curran Associates, 2020. 5, C.1, C.1, C.1   \nBergman, E., Purucker, L., and Hutter, F. Don\u2019t waste your time: Early stopping cross-validation. In Eggensperger, K., Garnett, R., Vanschoren, J., Lindauer, M., and Gardner, J. (eds.), Proceedings of the Third International Conference on Automated Machine Learning, volume 256 of Proceedings of Machine Learning Research, pp. 9/1\u201331. PMLR, 2024. B   \nBergstra, J. and Bengio, Y. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281\u2013305, 2012. 4, B   \nBischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A., Deng, D., and Lindauer, M. Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, pp. e1484, 2023. 1, 5, B   \nBlum, A., Kalai, A., and Langford, J. Beating the hold-out: Bounds for k-fold and progressive cross-validation. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT \u201999, pp. 203\u2013208, 1999. B   \nBorisov, V., Leemann, T., Se\u00dfler, K., Haug, J., Pawelczyk, M., and Kasneci, G. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201321, 2022. 4.1   \nBouckaert, Remcoand Frank, E. Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms. In Dai, H., Srikant, R., and Zhang, C. (eds.), Advances in Knowledge Discovery and Data Mining, pp. 3\u201312. Springer, 2004. B   \nBousquet, O. and Zhivotovskiy, N. Fast classification rates without standard margin assumptions. Information and Inference: A Journal of the IMA, 10(4):1389\u20131421, 2021. C.1   \nBouthillier, X., Delaunay, P., Bronzi, M., Trofimov, A., Nichyporuk, B., Szeto, J., Sepahvand, N. M., Raff, E., Madan, K., Voleti, V., Kahou, S. E., Michalski, V., Arbel, T., Pal, C., Varoquaux, G., and Vincent, P. Accounting for variance in machine learning benchmarks. In Smola, A., Dimakis, A., and Stoica, I. (eds.), Proceedings of Machine Learning and Systems 3, volume 3, pp. 747\u2013769, 2021. B   \nBuczak, P., Groll, A., Pauly, M., Rehof, J., and Horn, D. Using sequential statistical tests for efficient hyperparameter tuning. AStA Advances in Statistical Analysis, 108(2):441\u2013460, 2024. B   \nCawley, G. and Talbot, N. On Overfitting in Model Selection and Subsequent Selection Bias in Performance Evaluation. Journal of Machine Learning Research, 11:2079\u20132107, 2010. B   \nChen, T. and Guestrin, C. XGBoost: A scalable tree boosting system. In Krishnapuram, B., Shah, M., Smola, A., Aggarwal, C., Shen, D., and Rastogi, R. (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\u201916), pp. 785\u2013794. ACM Press, 2016. 4.1   \nCowen-Rivers, A., Lyu, W., Tutunov, R., Wang, Z., Grosnit, A., Grifftihs, R., Maraval, A., Jianye, H., Wang, J., Peters, J., and Ammar, H. HEBO: Pushing the limits of sample-efficient hyper-parameter optimisation. Journal of Artificial Intelligence Research, 74:1269\u20131349, 2022. 4, 4.1, B ", "page_idx": 10}, {"type": "text", "text": "Dem\u0161ar, J. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1\u201330, 2006. 1 ", "page_idx": 11}, {"type": "text", "text": "Dietterich, T. G. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895\u20131923, 1998. 1 ", "page_idx": 11}, {"type": "text", "text": "Dunias, Z., Van Calster, B., Timmerman, D., Boulesteix, A.-L., and van Smeden, M. A comparison of hyperparameter tuning procedures for clinical prediction models: A simulation study. Statistics in Medicine, 43(6):1119\u20131134, 2024. B ", "page_idx": 11}, {"type": "text", "text": "Eggensperger, K., Lindauer, M., Hoos, H., Hutter, F., and Leyton-Brown, K. Efficient benchmarking of algorithm configurators via model-based surrogates. Machine Learning, 107(1):15\u201341, 2018. 5 ", "page_idx": 11}, {"type": "text", "text": "Eggensperger, K., Lindauer, M., and Hutter, F. Pitfalls and best practices in algorithm configuration. Journal of Artificial Intelligence Research, pp. 861\u2013893, 2019. B ", "page_idx": 11}, {"type": "text", "text": "Eggensperger, K., M\u00fcller, P., Mallik, N., Feurer, M., Sass, R., Klein, A., Awad, N., Lindauer, M., and Hutter, F. HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. Curran Associates, 2021. 4.1, B ", "page_idx": 11}, {"type": "text", "text": "Escalante, H., Montes, M., and Sucar, E. Particle Swarm Model Selection. Journal of Machine Learning Research, 10:405\u2013440, 2009. 5 ", "page_idx": 11}, {"type": "text", "text": "Fabris, F. and Freitas, A. Analysing the overfit of the auto-sklearn automated machine learning tool. In Nicosia, G., Pardalos, P., Umeton, R., Giuffrida, G., and Sciacca, V. (eds.), Machine Learning, Optimization, and Data Science, volume 11943 of Lecture Notes in Computer Science, pp. 508\u2013520, 2019. 5 ", "page_idx": 11}, {"type": "text", "text": "Falkner, S., Klein, A., and Hutter, F. BOHB: Robust and efficient Hyperparameter Optimization at scale. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning (ICML\u201918), volume 80, pp. 1437\u20131446. Proceedings of Machine Learning Research, 2018. B ", "page_idx": 11}, {"type": "text", "text": "Feldman, V., Frostig, R., and Hardt, M. The advantages of multiple classes for reducing overfitting from test set reuse. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning (ICML\u201919), volume 97, pp. 1892\u20131900. Proceedings of Machine Learning Research, 2019. 5 ", "page_idx": 11}, {"type": "text", "text": "Feurer, M. and Hutter, F. Hyperparameter Optimization. In Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.), Automated Machine Learning: Methods, Systems, Challenges, chapter 1, pp. 3 \u2013 38. Springer, 2019. Available for free at http://automl.org/book. 1, B ", "page_idx": 11}, {"type": "text", "text": "Feurer, M., Eggensperger, K., Falkner, S., Lindauer, M., and Hutter, F. Auto-Sklearn 2.0: Hands-free automl via meta-learning. Journal of Machine Learning Research, 23(261):1\u201361, 2022. B ", "page_idx": 11}, {"type": "text", "text": "Garnett, R. Bayesian Optimization. Cambridge University Press, 2023. 1, B ", "page_idx": 11}, {"type": "text", "text": "Gijsbers, P., Bueno, M., Coors, S., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. AMLB: an automl benchmark. Journal of Machine Learning Research, 25(101):1\u201365, 2024. 4.1 ", "page_idx": 11}, {"type": "text", "text": "Gin\u00e9, E. and Nickl, R. Mathematical Foundations of Infinite-Dimensional Statistical Models, volume 40. Cambridge University Press, 2016. C.2 ", "page_idx": 11}, {"type": "text", "text": "Grinsztajn, L., Oyallon, E., and Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, pp. 507\u2013520, 2022. 4.1 ", "page_idx": 11}, {"type": "text", "text": "Guyon, I., Alamdari, A., Dror, G., and Buhmann, J. Performance prediction challenge. In The 2006 IEEE International Joint Conference on Neural Network Proceedings, 2006. 1 ", "page_idx": 11}, {"type": "text", "text": "Guyon, I., Saffari, A., Dror, G., and Cawley, G. Model selection: Beyond the Bayesian/Frequentist divide. Journal of Machine Learning Research, 11:61\u201387, 2010. B ", "page_idx": 11}, {"type": "text", "text": "Guyon, I., Bennett, K., Cawley, G., Escalante, H. J., Escalera, S., Ho, T. K., Maci\u00e0, N., Ray, B., Saeed, M., Statnikov, A., and Viegas, E. Design of the 2015 ChaLearn AutoML challenge. In 2015 International Joint Conference on Neural Networks (IJCNN\u201915), pp. 1\u20138. International Neural Network Society and IEEE Computational Intelligence Society, IEEE, 2015. B ", "page_idx": 12}, {"type": "text", "text": "Guyon, I., Sun-Hosoya, L., Boull\u00e9, M., Escalante, H., Escalera, S., Liu, Z., Jajetic, D., Ray, B., Saeed, M., Sebag, M., Statnikov, A., Tu, W., and Viegas, E. Analysis of the AutoML Challenge Series 2015-2018. In Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.), Automated Machine Learning: Methods, Systems, Challenges, chapter 10, pp. 177\u2013219. Springer, 2019. Available for free at http://automl.org/book. B ", "page_idx": 12}, {"type": "text", "text": "Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evolutionary C., 9(2):159\u2013195, 2001. 1 ", "page_idx": 12}, {"type": "text", "text": "Hothorn, T., Leisch, F., Zeileis, A., and Hornik, K. The design and analysis of benchmark experiments. Journal of Computational and Graphical Statistics, 14(3):675\u2013699, 2005. 4.1, F.1 ", "page_idx": 12}, {"type": "text", "text": "Igel, C. A note on generalization loss when evolving adaptive pattern recognition systems. IEEE Transactions on Evolutionary Computation, 17(3):345\u2013352, 2012. 5, 5 ", "page_idx": 12}, {"type": "text", "text": "Jamieson, K. and Talwalkar, A. Non-stochastic best arm identification and Hyperparameter Optimization. In Gretton, A. and Robert, C. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201916), volume 51. Proceedings of Machine Learning Research, 2016. B ", "page_idx": 12}, {"type": "text", "text": "Kadra, A., Janowski, M., Wistuba, M., and Grabocka, J. Scaling laws for hyperparameter optimization. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 47527\u201347553, 2023. B ", "page_idx": 12}, {"type": "text", "text": "Kallenberg, O. Foundations of modern probability, volume 2. Springer, 1997. D ", "page_idx": 12}, {"type": "text", "text": "Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Singh, A. and Zhu, J. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201917), volume 54. Proceedings of Machine Learning Research, 2017. B ", "page_idx": 12}, {"type": "text", "text": "Koch, P., Konen, W., Flasch, O., and Bartz-Beielstein, T. Optimizing support vector machines for stormwater prediction. Technical Report TR10-2-007, Technische Universit\u00e4t Dortmund, 2010. Proceedings of Workshop on Experimental Methods for the Assessment of Computational Systems joint to PPSN2010. 5, 5 ", "page_idx": 12}, {"type": "text", "text": "Kohli, R., Feurer, M., Bischl, B., Eggensperger, K., and Hutter, F. Towards quantifying the effect of datasets for benchmarking: A look at tabular machine learning. In Data-centric Machine Learning (DMLR) workshop at the International Conference on Learning Representations (ICLR), 2024. 4.1 ", "page_idx": 12}, {"type": "text", "text": "Lang, M., Kotthaus, H., Marwedel, P., Weihs, C., Rahnenf\u00fchrer, J., and Bischl, B. Automatic model selection for high-dimensional survival analysis. Journal of Statistical Computation and Simulation, 85:62\u201376, 2015. B ", "page_idx": 12}, {"type": "text", "text": "Larcher, C. and Barbosa, H. Evaluating models with dynamic sampling holdout in auto-ml. SN Computer Science, 3(506), 2022. 1 ", "page_idx": 12}, {"type": "text", "text": "Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to Hyperparameter Optimization. Journal of Machine Learning Research, 18(185):1\u201352, 2018. B ", "page_idx": 12}, {"type": "text", "text": "Lindauer, M., Eggensperger, K., Feurer, M., Biedenkapp, A., Deng, D., Benjamins, C., Ruhkopf, T., Sass, R., and Hutter, F. SMAC3: A versatile bayesian optimization package for Hyperparameter Optimization. Journal of Machine Learning Research, 23(54):1\u20139, 2022. 4, 4.1, B ", "page_idx": 12}, {"type": "text", "text": "Loshchilov, I. and Hutter, F. CMA-ES for Hyperparameter Optimization of deep neural networks. In International Conference on Learning Representations Workshop track, 2016. Published online: iclr.cc. B ", "page_idx": 12}, {"type": "text", "text": "L\u00e9vesque, J. Bayesian Hyperparameter Optimization: Overfitting, Ensembles and Conditional Spaces. PhD thesis, Universit\u00e9 Laval, 2018. 1, 5, 5 ", "page_idx": 13}, {"type": "text", "text": "Makarova, A., Shen, H., Perrone, V., Klein, A., Faddoul, J., Krause, A., Seeger, M., and Archambeau, C. Automatic termination for hyperparameter optimization. In Guyon, I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (eds.), Proceedings of the First International Conference on Automated Machine Learning. Proceedings of Machine Learning Research, 2022. 5   \nMallik, N., Bergman, E., Hvarfner, C., Stoll, D., Janowski, M., Lindauer, M., Nardi, L., and Hutter, F. PriorBand: Practical hyperparameter optimization in the age of deep learning. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS\u201923). Curran Associates, 2023. B   \nMcElfresh, D., Khandagale, S., Valverde, J., Prasad C., V., Ramakrishnan, G., Goldblum, M., and White, C. When do neural nets outperform boosted trees on tabular data? In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS\u201923), pp. 76336\u2013 76369. Curran Associates, 2023. 4.1, F.2   \nMohr, F., Wever, M., and H\u00fcllermeier, E. ML-Plan: Automated machine learning via hierarchical planning. Machine Learning, 107(8-10):1495\u20131515, 2018. 5, B   \nMolinaro, A., Simon, R., and Pfeiffer, R. Prediction error estimation: A comparison of resampling methods. Bioinformatics, 21(15):3301\u20133307, 2005. B   \nNadeau, C. and Bengio, Y. Inference for the generalization error. In Solla, S., Leen, T., and M\u00fcller, K. (eds.), Proceedings of the 13th International Conference on Advances in Neural Information Processing Systems (NeurIPS\u201999). The MIT Press, 1999. 1   \nNadeau, C. and Bengio, Y. Inference for the generalization error. Machine Learning, 52:239\u2013281, 2003. 1   \nNg, A. Preventing \u201coverfitting\u201d\u2019 of cross-validation data. In Fisher, D. H. (ed.), Proceedings of the Fourteenth International Conference on Machine Learning (ICML\u201997), pp. 245\u2013253. Morgan Kaufmann Publishers, 1997. 5, 5   \nPfisterer, F., Schneider, L., Moosbauer, J., Binder, M., and Bischl, B. YAHPO Gym \u2013 an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization. In Guyon, I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (eds.), Proceedings of the First International Conference on Automated Machine Learning. Proceedings of Machine Learning Research, 2022. B, 5   \nPineda Arango, S., Jomaa, H., Wistuba, M., and Grabocka, J. HPO-B: A large-scale reproducible benchmark for black-box HPO based on OpenML. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. Curran Associates, 2021. B, 5   \nProbst, P., Boulesteix, A., and Bischl, B. Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53):1\u201332, 2019. 1   \nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A., and Gulin, A. Catboost: Unbiased boosting with categorical features. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS\u201918), pp. 6639\u20136649. Curran Associates, 2018. 4.1   \nPurucker, L. and Beel, J. CMA-ES for post hoc ensembling in automl: A great success and salvageable failure. In Faust, A., Garnett, R., White, C., Hutter, F., and Gardner, J. R. (eds.), Proceedings of the Second International Conference on Automated Machine Learning, volume 224 of Proceedings of Machine Learning Research, pp. 1/1\u201323. PMLR, 2023. 5   \nQuinlan, J. and Cameron-Jones, R. Oversearching and layered search in empirical learning. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, volume 2 of IJCAI\u201995, pp. 1019\u20131024, 1995. 5   \nRao, R., Fung, G., and Rosales, R. On the dangers of cross-validation. an experimental evaluation. In Proceedings of the 2008 SIAM International Conference on Data Mining (SDM), pp. 588\u2013596, 2008. B   \nSalinas, D., Seeger, M., Klein, A., Perrone, V., Wistuba, M., and Archambeau, C. Syne Tune: A library for large scale hyperparameter tuning and reproducible research. In Guyon, I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (eds.), Proceedings of the First International Conference on Automated Machine Learning, pp. 16\u20131. Proceedings of Machine Learning Research, 2022. B   \nSchaffer, C. Selecting a classification method by cross-validation. Machine Learning Journal, 13: 135\u2013143, 1993. B   \nSwersky, K., Snoek, J., and Adams, R. Freeze-thaw Bayesian optimization. arXiv:1406.3896 [stats.ML], 2014. B   \nTalagrand, M. The generic chaining: upper and lower bounds of stochastic processes. Springer Science & Business Media, 2005. C.2   \nThornton, C., Hutter, F., Hoos, H., and Leyton-Brown, K. Auto-WEKA: Combined selection and Hyperparameter Optimization of classification algorithms. In Dhillon, I., Koren, Y., Ghani, R., Senator, T., Bradley, P., Parekh, R., He, J., Grossman, R., and Uthurusamy, R. (eds.), The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\u201913), pp. 847\u2013855. ACM Press, 2013. 5, B   \nTurner, R., Eriksson, D., McCourt, M., Kiili, J., Laaksonen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the Black-Box Optimization Challenge 2020. In Escalante, H. and Hofmann, K. (eds.), Proceedings of the Neural Information Processing Systems Track Competition and Demonstration, pp. 3\u201326. Curran Associates, 2021. 4.1   \nvan der Vaart, A. Asymptotic statistics, volume 3. Cambridge university press, 2000. C.1   \nvan Erven, T., Gr\u00fcnwald, P., Mehta, N., Reid, M., and Williamson, R. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16(54):1793\u20131861, 2015. C.1   \nvan Rijn, J. and Hutter, F. Hyperparameter importance across datasets. In Guo, Y. and Farooq, F. (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\u201918), pp. 2367\u20132376. ACM Press, 2018. 1   \nVanschoren, J., van Rijn, J., Bischl, B., and Torgo, L. OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2):49\u201360, 2014. 4   \nWainer, J. and Cawley, G. Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters. Journal of Machine Learning Research, 18:1\u201335, 2017. B   \nWainwright, M. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019. C.2   \nWistuba, M., Schilling, N., and Schmidt-Thieme, L. Scalable Gaussian process-based transfer surrogates for Hyperparameter Optimization. Machine Learning, 107(1):43\u201378, 2018. G.1   \nWu, J., Toscano-Palmerin, S., Frazier, P., and Wilson, A. Practical multi-fidelity Bayesian optimization for hyperparameter tuning. In Peters, J. and Sontag, D. (eds.), Proceedings of The 36th Uncertainty in Artificial Intelligence Conference (UAI\u201920), pp. 788\u2013798. PMLR, 2020. B   \nZheng, A. and Bilenko, M. Lazy paired hyper-parameter tuning. In Rossi, F. (ed.), Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI\u201913), pp. 1924\u20131931, 2013. 5, B   \nZimmer, L., Lindauer, M., and Hutter, F. Auto-Pytorch: Multi-fidelity metalearning for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43:3079\u20133090, 2021. 4.1, F.2   \nZou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301\u2013320, 2005. 4.1 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "C4SInFLvuB/tmp/789c58859f078faf215ca7388f7b49ae7d5285cb3f56971c8ad1921cf7ac1aa1.jpg", "table_caption": ["Table 2: Notation table. We discuss all symbols used in the main paper. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Extended Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Due to the black box nature of the HPO problem (Feurer & Hutter, 2019; Bischl et al., 2023), gradient free, zeroth-order optimization algorithms such as BO (Garnett, 2023), Evolutionary Strategies (Loshchilov & Hutter, 2016) or a simple random search (Bergstra & Bengio, 2012) have become standard optimization algorithms to tackle vanilla HPO problems. ", "page_idx": 15}, {"type": "text", "text": "In the last decade, most research on HPO has been concerned with constructing new algorithms that excel at finding configurations with a low estimated generalization error. Examples include BO variants such as as HEBO (Cowen-Rivers et al., 2022) or SMAC3 (Lindauer et al., 2022). Another direction of HPO research has been concerned with speeding up the HPO process to allow more efficient spending of compute resources. Multifidelity HPO, for example, turns the black box optimization problem into a gray box one by making use of lower fidelity approximations to the target function, i.e., using fewer numbers of epochs or subsets of the data for cheap low-fidelity evaluations that approximate the costly high-fidelity evaluation. Examples include bandit-based budget allocation algorithms such as Successive Halving (Jamieson & Talwalkar, 2016), Hyperband (Li et al., 2018) and their extensions that use non-random search mechanisms (Falkner et al., 2018; Awad et al., 2021; Mallik et al., 2023) or algorithms making use of multi-fidelity information in the context of BO (Swersky et al., 2014; Klein et al., 2017; Wu et al., 2020; Kadra et al., 2023). Several works address the problem of speeding up cross-validation techniques and use techniques that could be described as grey box optimization techniques. Besides the ones mentioned in the main paper (Thornton et al., 2013; Zheng & Bilenko, 2013), it is possible to employ racing techniques for model selection in machine learning as demonstrated by Lang et al. (2015), and there has been a recent interest in methods that adapt the cost of running full cross-validation procedures (Bergman et al., 2024; Buczak et al., 2024). ", "page_idx": 15}, {"type": "text", "text": "When addressing the problem of HPO, we must acknowledge an inherent mismatch between the explicit objective we optimize \u2013 namely, the estimated generalization performance of a model \u2013 and the actual implicit optimization goal, which is to identify a configuration that yields the best generalization performance on new, unseen data. Typically, evaluations and comparisons of different HPO algorithms focus exclusively on the final best validation performance (i.e., the objective that is directly optimized), even though an unbiased estimate of performance on an external unseen test set might be available. While this approach is logical for assessing the efficacy of an optimization algorithm based on the metric it seeks to improve, relying solely on finding an optimal validation configuration is beneficial only if there is reason to assume a strong correlation between the optimized validation performance and true generalization ability on new, unseen test data. This discrepancy can be found deeply within the HPO community, where the evaluation of HPO algorithms on standard benchmark libraries is usually done solely with respect to the validation performance (Eggensperger et al., 2021; Pineda Arango et al., 2021; Salinas et al., 2022; Pfisterer et al., 2022).5 This relationship between validation performance (i.e., the estimated generalization error derived from resampling) and true generalization performance (e.g., assessed through an outer holdout test set or additional resampling) of an optimal validation configuration found during HPO remains a largely unexplored area of research. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "In general, little research has focused on the selection of resampling types, let alone the automated selection of resampling types (Guyon et al., 2010; Feurer et al., 2022). While we usually expect that a more intensive resampling will reduce the variance of the estimated generalization error and thereby improve the (rank) correlation between optimized validation and unbiased outer test performance within HPCs, this benefti is naturally offset by a higher computational expense. Overall, there is little research on which resampling method to use in practice for model selection, and we only know of a study for support vector machines (Wainer & Cawley, 2017), a simulation study for clinical prediction models (Dunias et al., 2024), a study on feature selection (Molinaro et al., 2005) and a study on fast CV (Bergman et al., 2024). In addition, ML-Plan (Mohr et al., 2018) proposed a two-stage procedure. In a first stage (search), the tool uses planning on hierarchical task networks to find promising machine learning pipelines on $70\\%$ of the training data. In a second step (selection), it uses $\\bar{1}00\\%$ of the training data and retrains the most promising candidates from the search step. Finally, it uses a combination of the internal generalization error estimation that was used during search and the 0.75 percentile of the generalization error estimation from the selection step to make a more unbiased selection of the final model. The paper found that this improves performance over using only regular cross-validation for search and selection. The general consensus, that is in agreement with our findings, is that CV or repeated CV generally leads to better generalization performance. In addition, while there are theoretical works that compare the accuracy of estimating the generalization error of holdout and CV (Blum et al., 1999), our goals is to correctly identify a single solution, which generalizes well, see the excellent survey by Arlot & Celisse (2010) for a discussion on this topic. ", "page_idx": 16}, {"type": "text", "text": "Bouthillier et al. (2021) studied the sources of variance in machine learning experiments, and find that the split into training and test data has the largest impact. Consequently, they suggest to reshuffle the data prior to splitting it into the training, which is then used for HPO, and the test set. We followed their suggestion when designing our experiments and draw a new test sample for every replication, see Section 4.1 and Appendix F. This dependence on the exact split was further already discussed in the context of how much the outcome of a statistical test on results of machine learning experiments depended on the exact train-test split (Bouckaert, 2004). ", "page_idx": 16}, {"type": "text", "text": "Finally, the first warning against comparing too many hypothesis using cross-validation was raised by Schaffer (1993), and in addition to the works discussed in Section 5 in the main paper, also picked up by Rao et al. (2008); Cawley & Talbot (2010). Moreover, the problem of finding a correct \"upper objective\" in a bilevel optimization problem has been noted (Guyon et al., 2010, 2015, 2019). Also, in the related field of algorithm configuration the problem has been identified (Eggensperger et al., 2019). ", "page_idx": 16}, {"type": "text", "text": "B.1 Current Treatment of Resamplings in HPO Libraries and Software ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Table 3, we provide a brief summary of how resampling is handled in popular HPO libraries and software.6 For each library, we checked whether the core functionality, examples, or tutorials mention the possibility of reshuffling the resampling during HPO or if the resampling is considered fixed. If reshuffilng is used in an example, mentioned, or if core functionality uses it, we mark it with a $\\checkmark$ . If it is unclear or inconsistent across examples and core functionality, we mark it with a ?. Otherwise, we use a $\\pmb{x}$ . Our conclusion is that the concept of reshuffling resampling generally receives little attention. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "C4SInFLvuB/tmp/3df7b9631cae75b3123c83e8294fd4e7654222ec8297b232746e672e51944172.jpg", "table_caption": ["Table 3: Exemplary Treatment of Resamplings in HPO Libraries and Software "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "\u2717: no reshuffling, ?: both reshuffling and no reshuffling or unclear, \u2713: reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1263   \n2 https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1644 3 https://github.com/huawei-noah/HEBO/blob/b60f41aa862b4c5148e31ab4981890da6d41f2b1/HEBO/hebo/sklearn_t uner.py#L73 4 https://github.com/optuna/optuna-integration/blob/15e6b0ec6d9a0d7f572ad387be8478c56257bef7/optuna_in tegration/sklearn/sklearn.py#L223 here sklearn\u2019s cross_validate is used which by default does not reshuffle the resampling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_validation.py#L186   \n5 htotrtcphs_:s/i/mgpilteh.upby.#cLo7m9/ hoeprte,u ndaat/a olpotaduenras -foerx tarmaipnl aensd/ vballoidb /arded i5n6stba9n6ti9at2eed6 dw1itfh4inf tah8e3 o9b3j3e2cteivdeb cofd tdh9e3 tfrida4l 8bcut1 t6hde8 d/aptya twoirthcihn /tphye loaders is fixed   \n6 https://github.com/optuna/optuna-examples/blob/dd56b9692e6d1f4fa839332edbcdd93fd48c16d8/xgboost/xgbo ost_simple.py#L22 here, the train validation split is performed within the objective of the trial and no seed is set which results in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/s klearn/model_selection/_split.py#L2597 7 functionality relies on sklearn\u2019s cross_val_score which by default does not reshuffle the resampling https://github.com/sciki t-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/model_selection/_validati on.py#L631   \n8 https://github.com/bayesian-optimization/BayesianOptimization/blob/c7e5c3926944fc6011ae7ace29f7b5ed0f 9c983b/examples/sklearn_example.py#L32   \n9 https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/tutorials/tune_cnn_serv ice.ipynb#L39 and https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/ax/util s/tutorials/cnn_utils.py#L154   \n10 https://github.com/scikit-optimize/scikit-optimize/blob/a2369ddbc332d16d8ff173b12404b03fea472492/ex amples/hyperparameter-optimization.py#L82C21-L82C36   \n11 mh_tctvp.sp:y/#/Lg6i3thub.com/automl/SMAC3/blob/9aaa8e94a5b3a9657737a87b903ee96c683cc42c/examples/1_basics/2_sv   \n12 ehgt/tspksl:t/r/egei.tphyu#bL.1c1o1m/dragonfly/dragonfly/blob/3eef7d30bcc2e56f2221a624bd8ec7f933f81e40/examples/tree_r   \n13 https://aws.amazon.com/blogs/architecture/field-notes-build-a-cross-validation-machine-learning-mod el-pipeline-at-scale-with-amazon-sagemaker/   \n14 https://github.com/ray-project/ray/blob/3f5aa5c4642eeb12447d9de5dce22085512312f3/doc/source/tune/exa mples/tune-pytorch-cifar.ipynb#L120 here, data loaders for train and valid are instantiated within the objective but the data   \n15 https://github.com/ray-projec the/rre,a tyh/eb tlraoibn/ v3afli5daaati5ocn 4s6p4li2t eise bpe1r2f4or4m7ded9 dweit5hdicn et2he2 0o8bj5e5c1ti2v3e 1a2nfd3 n/od soece/ds ios usretc ew/hticuhn ree/seuxltas in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3 /sklearn/model_selection/_split.py#L2597   \n16 https://github.com/hyperopt/hyperopt-sklearn/blob/4bc286479677a0bfd2178dac4546ea268b3f3b77/hpsklearn /estimator/_cost_fn.py#L144 dependence on random seed which by default is not set and there is no discussion of reshuffilng and behavior is somewhat unclear ", "page_idx": 17}, {"type": "text", "text": "C Proofs of the Main Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We impose stability assumptions on the learning algorithm similar to Bayle et al. (2020); Austern & Zhou (2020). Let $Z,Z_{1},\\ldots,Z_{n},Z_{1}^{\\prime}$ , be iid random variables. Define $\\boldsymbol{\\mathcal{T}}=\\{\\boldsymbol{Z}_{i}\\}_{i=1}^{n}$ , and $\\mathcal{T}^{\\prime}$ as $\\tau$ but with $Z_{n}$ replaced by the independent copy ${\\mathbf{Z}}_{n}^{\\prime}$ . Define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{n}(z,\\lambda)=\\ell(z,g_{\\lambda}(\\mathcal{T}))-\\mathbb{E}[\\ell(Z,g_{\\lambda}(\\mathcal{T}))\\mid\\mathcal{T}],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "assume that each $g_{\\lambda}(\\mathcal{T})$ is invariant to the ordering in $\\tau,\\ell$ is bounded, and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\in\\Lambda}\\mathbb{E}\\{[\\widetilde{\\ell}(\\pmb{Z},g_{\\lambda}(\\mathcal{T}))-\\widetilde{\\ell}(\\pmb{Z},g_{\\lambda}(\\mathcal{T}^{\\prime}))]^{2}\\}=o(1/n).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This loss stability assumption is rather mild, see Bayle et al. (2020) for an extensive discussion. Further, define the risk $\\bar{R}(g)=\\mathbb{E}[\\ell(Z,g)]$ and assume that for every $\\lambda\\in\\Lambda$ , there is a prediction rule $g_{\\lambda}^{\\ast}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\in\\Lambda}\\mathbb{E}\\left[|R(g_{\\lambda}(\\mathcal{T}))-R(g_{\\lambda}^{*})|\\right]=o(1/\\sqrt{n}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This assumption requires $g_{\\lambda}(\\mathcal{T})$ to converge to some fixed prediction rule sufficiently fast and serves as a reasonable working condition for our purposes. It is satisfied, for example, when $\\ell$ is the square loss and $g_{\\lambda}$ is an empirical risk minimizer over a hypothesis class $\\mathcal{G}_{\\lambda}$ with finite VC-dimension. For further examples, see, e.g., Bousquet & Zhivotovskiy (2021), van Erven et al. (2015), and references therein. The assumption could be relaxed, but this would lead to a more complicated limiting distribution but with the same essential interpretation. ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1. Under assumptions (4) and (5), it holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left(\\widehat{\\mu}(\\lambda_{j})-\\mu(\\lambda_{j})\\right)_{j=1}^{J}\\to_{d}\\mathcal{N}(0,\\Sigma),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Sigma_{j,j^{\\prime}}=\\tau_{i,j,M}\\operatorname*{lim}_{n\\to\\infty}\\mathsf{C o v}[\\bar{\\ell}_{n}(Z,\\lambda_{j}),\\bar{\\ell}_{n}(Z,\\lambda_{j^{\\prime}})],}\\\\ &{\\tau_{j,j^{\\prime},M}=\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n M^{2}\\alpha^{2}}\\sum_{i=1}^{n}\\sum_{m=1}^{M}\\sum_{m^{\\prime}=1}^{M}\\operatorname*{Pr}(i\\in\\mathbb{Z}_{m,j}\\cap\\mathbb{Z}_{m^{\\prime},j^{\\prime}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\mu}(\\lambda_{j})=\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}[L(\\mathcal{V}_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))\\mid\\mathcal{T}_{m,j}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the triangle inequality (first and second step), Jensen\u2019s inequality (third step), and (5) (last step), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{E}[\\left|\\widetilde{\\mu}(\\lambda_{j})-\\mu(\\lambda_{j})\\right|]}\\\\ &{\\le\\underset{1\\le m\\le M}{\\operatorname*{max}}\\mathbb{E}\\left[\\big|\\mathbb{E}[L(\\gamma_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))\\mid\\mathcal{T}_{m,j}]-\\mathbb{E}[L(\\mathcal{V}_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))]\\big|\\right]}\\\\ &{\\le\\underset{1\\le m\\le M}{\\operatorname*{max}}\\mathbb{E}\\left[\\big|\\mathbb{E}[L(\\gamma_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))\\mid\\mathcal{T}_{m,j}]-\\mathbb{E}[L(\\mathcal{V}_{m,j},g_{\\lambda_{j}}^{*})]\\big|\\right]}\\\\ &{\\quad+\\underset{1\\le m\\le M}{\\operatorname*{max}}\\mathbb{E}\\left[\\big|\\mathbb{E}[L(\\gamma_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))]-\\mathbb{E}[L(\\mathcal{V}_{m,j},g_{\\lambda_{j}}^{*})]\\big|\\right]}\\\\ &{\\le2\\underset{1\\le m\\le M}{\\operatorname*{max}}\\mathbb{E}\\left[\\big|\\mathbb{E}[L(\\mathcal{V}_{m,j},g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))\\mid\\mathcal{T}_{m,j}]-\\mathbb{E}[L(\\mathcal{V}_{m,j},g_{\\lambda_{j}}^{*})]\\big|\\right]}\\\\ &{=2\\underset{1\\le m\\le M}{\\operatorname*{max}}\\mathbb{E}\\left[\\big|R(g_{\\lambda_{j}}(\\mathcal{T}_{m,j}))-R(g_{\\lambda_{j}}^{*})\\big|\\right]}\\\\ &{=o(1/\\sqrt{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, assumption (4) together with Theorem 2 and Proposition 3 of Bayle et al. (2020) yield ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left(\\widehat{\\mu}(\\lambda_{j})-\\widetilde{\\mu}(\\lambda_{j})\\right)-\\frac{1}{M}\\sum_{m=1}^{M}\\frac{1}{\\alpha\\sqrt{n}}\\sum_{i\\in\\mathbb{Z}_{m,j}}\\bar{\\ell}_{n}(\\pmb{Z}_{i},\\lambda_{j})\\to_{p}0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now rewrite ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{M\\alpha\\sqrt{n}}\\sum_{m=1}^{M}\\sum_{i\\in{\\cal Z}_{m,j}}\\bar{\\ell}_{n}({\\pmb Z}_{i},{\\pmb\\lambda}_{j})=\\frac{1}{M\\alpha\\sqrt{n}}\\sum_{i=1}^{n}\\sum_{\\underset{\\substack{\\because=\\xi_{i,n}^{(j)}}}{\\underbrace{\\sum_{m=1}^{M}\\mathbb{I}(i\\in{\\mathbb{Z}_{m,j}})\\bar{\\ell}_{n}({\\pmb Z}_{i},{\\pmb\\lambda}_{j})}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The sequence $(\\pmb{\\xi}_{i,n})_{i=1}^{n}\\,=\\,(\\xi_{i,n}^{(j)},\\dots,\\xi_{i,n}^{(j)})_{i=1}^{n}$ is a triangular array of independent, centered, and bounded random vectors. Because $\\mathbb{1}(Z_{i}\\in\\mathcal{V}_{m,j})$ and are independent, it holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{C o v}(\\xi_{i,n}^{(j)},\\xi_{i,n}^{(j^{\\prime})})=\\sum_{m=1}^{M}\\sum_{m^{\\prime}=1}^{M}\\mathbb{E}[\\mathbb{1}(i\\in\\mathbb{Z}_{m,j}\\cap\\mathbb{Z}_{m^{\\prime},j^{\\prime}})]\\mathbb{E}[\\bar{\\ell}_{n}(\\boldsymbol{Z}_{i},\\boldsymbol{\\lambda}_{j})\\bar{\\ell}_{n}(\\boldsymbol{Z}_{i},\\boldsymbol{\\lambda}_{j^{\\prime}})],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathsf{C o v}\\left[\\frac{1}{M\\alpha\\sqrt{n}}\\sum_{i=1}^{n}\\xi_{i,n}^{(j)},\\frac{1}{M\\alpha\\sqrt{n}}\\sum_{i=1}^{n}\\xi_{i,n}^{(j^{\\prime})}\\right]=\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n M^{2}\\alpha^{2}}\\sum_{i=1}^{n}\\mathsf{C o v}\\left[\\xi_{i,n}^{(j)},\\xi_{i,n}^{(j^{\\prime})}\\right]\\quad=\\Sigma_{j,j^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now the result follows from Lindeberg\u2019s central limit theorem for triangular arrays (e.g., van der Vaart, 2000, Proposition 2.27). \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C.2 Proof of Theorem 2.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We want to bound the probability that $\\mu(\\hat{\\lambda})-\\mu(\\lambda^{*})$ is large. For some $\\delta>0$ , define the set of \u2018good\u2019 hyperparameters ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Lambda_{\\delta}=\\{\\lambda_{j}:\\mu(\\lambda_{j})-\\mu(\\lambda^{*})\\leq\\delta\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{Pr}\\Big(\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})>\\delta\\Big)=\\operatorname*{Pr}\\Big(\\widehat{\\lambda}\\neq\\Lambda_{\\delta}\\Big)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{=\\operatorname*{Pr}\\left(\\frac{\\sinh}{\\sinh}\\widehat{\\mu}(\\lambda)<\\frac{\\operatorname*{min}}{\\lambda\\leq\\lambda_{\\delta}}\\widehat{\\mu}(\\lambda)\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta(\\lambda)<\\nu(\\lambda)\\Big\\}\\\\ &{=\\operatorname*{Pr}\\left(\\frac{\\operatorname*{min}}{\\lambda\\sqrt{\\lambda}}\\mu)\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\epsilon\\stackrel{d}{=}-\\epsilon)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "There is a tension between the two maxima. The more $\\lambda$ \u2019s there are in $\\Lambda_{\\delta/2}$ and the less they are correlated, the more likely it is to find one $\\epsilon(\\lambda)$ that is large. This makes the probability small. However, the less $\\epsilon$ is correlated, the larger is $\\operatorname*{max}_{\\substack{\\lambda\\notin\\Lambda_{\\delta}}}\\epsilon(\\bar{\\lambda})$ , making the probability large. To formalize this, use the Gaussian concentration inequality (Talagrand, 2005, Lemma 2.1.3): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}\\left(\\underset{\\lambda\\not\\in\\Lambda_{\\delta}}{\\operatorname*{max}}\\,\\epsilon(\\lambda)-\\underset{\\lambda\\in\\Lambda_{\\delta/2}}{\\operatorname*{max}}\\,\\epsilon(\\lambda)>\\delta/2\\right)}\\\\ &{\\leq\\operatorname*{Pr}\\left(2\\left|\\underset{\\lambda\\in\\Lambda}{\\operatorname*{max}}\\,\\epsilon(\\lambda)-\\mathbb{E}\\left[\\underset{\\lambda\\in\\Lambda}{\\operatorname*{max}}\\,\\epsilon(\\lambda)\\right]\\right|>\\delta/2-\\mathbb{E}\\left[\\underset{\\lambda\\in\\Lambda_{\\delta/2}}{\\operatorname*{max}}\\,\\epsilon(\\lambda)\\right]+\\mathbb{E}\\left[\\underset{\\lambda\\not\\in\\Lambda_{\\delta}}{\\operatorname*{max}}\\,\\epsilon(\\lambda)\\right]\\right)}\\\\ &{\\leq2\\exp\\left\\{-\\frac{\\left(\\delta/2-\\mathbb{E}\\left[\\operatorname*{max}_{\\lambda\\in\\Lambda_{\\delta/2}}\\epsilon(\\lambda)\\right]+\\mathbb{E}\\left[\\operatorname*{max}_{\\lambda\\not\\in\\Lambda_{\\delta}}\\epsilon(\\lambda)\\right]\\right)^{2}}{8\\sigma^{2}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "provided $\\begin{array}{r}{\\delta/2\\!-\\!\\mathbb{E}\\left[\\operatorname*{max}_{\\lambda\\in\\Lambda_{\\delta/2}}\\epsilon(\\lambda)\\right]\\!+\\!\\mathbb{E}\\left[\\operatorname*{max}_{\\lambda\\not\\in\\Lambda_{\\delta}}\\epsilon(\\lambda)\\right]\\geq0}\\end{array}$ . We bound the two maxima separately. ", "page_idx": 19}, {"type": "text", "text": "Lower Bound for Maximum over the Good Set ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall the definition of $m$ right before Theorem 2.2 and observe ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{\\delta/2}=\\{\\lambda:\\mu(\\lambda)-\\mu(\\lambda^{*})\\leq\\delta/2\\}\\supset\\{\\lambda:m\\|\\lambda-\\lambda^{*}\\|^{2}\\leq\\delta/2\\}=\\{\\lambda:\\|\\lambda-\\lambda^{*}\\|\\leq(\\delta/2m)^{1/2}\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=B(\\lambda^{*},(\\delta/2m)^{1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Pack the ball $B(\\pmb{\\lambda}^{\\ast},(\\delta/2m)^{1/2})$ with smaller balls with radius $\\eta$ . We can always construct such a packing with at least $(\\delta/2m\\eta^{2})^{d/2}$ elements. By assumption, each small ball contains at least one element of $\\Lambda$ . Pick one element from each small ball and collect them into the set $\\Lambda_{\\delta/2}^{\\prime}$ . By construction, $|\\Lambda_{\\delta/2}^{\\prime}|\\geq(\\delta/2m\\eta^{2})^{d/2}$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda\\neq\\lambda^{\\prime}\\in\\Lambda_{\\delta/2}^{\\prime}|}\\|\\lambda-\\lambda^{\\prime}\\|\\geq\\eta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Sudakov\u2019s minoration principle (e.g., Wainwright, 2019, Theorem 5.30) gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{\\lambda\\in\\Lambda_{\\delta/2}}{\\operatorname*{max}}\\epsilon(\\lambda)\\right]\\geq\\frac{1}{2}\\sqrt{\\log{|\\Lambda_{\\delta/2}^{\\prime}|}}\\underset{\\{\\lambda\\neq\\lambda^{\\prime}\\}\\cap\\Lambda_{\\delta/2}^{\\prime}}{\\operatorname*{min}}\\sqrt{\\mathsf{V a r}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda^{\\prime})\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\sqrt{\\log{|\\Lambda_{\\delta/2}^{\\prime}|}}\\underset{\\|\\lambda-\\lambda^{\\prime}\\|\\geq\\eta}{\\operatorname*{min}}\\sqrt{\\mathsf{V a r}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda^{\\prime})\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In general, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\forall\\mathrm{ar}\\,[\\epsilon(\\lambda)-\\epsilon(\\lambda^{\\prime})]}\\\\ &{=K(\\lambda,\\lambda)+K(\\lambda^{\\prime},\\lambda^{\\prime})-2\\tau^{2}K(\\lambda,\\lambda^{\\prime})}\\\\ &{=(1-\\tau^{2})[K(\\lambda,\\lambda)+K(\\lambda^{\\prime},\\lambda^{\\prime})]+\\tau^{2}[K(\\lambda,\\lambda)-K(\\lambda,\\lambda^{\\prime})]+\\tau^{2}[K(\\lambda^{\\prime},\\lambda^{\\prime})-K(\\lambda,\\lambda^{\\prime})]}\\\\ &{\\geq2\\underline{{\\sigma}}^{2}(1-\\tau^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\|\\lambda-\\lambda^{\\prime}\\|\\geq\\eta}{\\mathsf{V a r}}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda^{\\prime})\\right]\\geq2\\underline{{\\sigma}}^{2}(1-\\tau^{2}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{\\lambda\\in\\Lambda_{\\delta/2}}\\epsilon(\\lambda)\\right]\\geq\\frac{1}{2}\\underline{{\\sigma}}\\sqrt{d}\\sqrt{1-\\tau^{2}}\\sqrt{\\log(\\delta/2m\\eta^{2})}=:\\sigma\\sqrt{d}A(\\tau,\\delta)/2.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Upper Bound for Maximum over the Bad Set ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Dudley\u2019s entropy bound (e.g., Gin\u00e9 & Nickl, 2016, Theorem 2.3.6) gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{\\pmb{\\lambda}\\notin\\Lambda_{\\delta}}\\epsilon(\\pmb{\\lambda})\\right]\\leq12\\int_{0}^{\\infty}\\sqrt{\\log N(s)}d s,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $N(s)$ is the minimum number of points $\\lambda_{1},\\ldots,\\lambda_{N(s)}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\lambda\\in\\Lambda}\\operatorname*{min}_{1\\leq k\\leq N(s)}\\sqrt{\\mathsf{V a r}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda_{k})\\right]}\\leq s.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\lambda,\\lambda^{\\prime}\\in\\Lambda}\\sqrt{\\mathsf{V a r}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda^{\\prime})\\right]}\\le2\\sigma,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so $N(s)\\,=\\,1$ for all $s\\;\\geq\\;2\\sigma$ . For $s^{2}\\,\\leq\\,4\\sigma^{2}(1-\\tau^{2})$ , we can use the trivial bound $N(s)\\,\\leq\\,J.$ . For $s^{2}\\,>\\,4\\sigma^{2}(1-\\tau^{2})$ , cover $\\Lambda$ with $\\ell_{2}$ -balls of size $(s/2\\sigma\\tau\\kappa)$ . We can do this with less than $N(s)\\leq(6\\sigma\\kappa/s)^{d}\\vee1$ such balls. Let $\\lambda_{1},\\ldots,\\lambda_{N}$ be the centers of these balls. In general, it holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lor\\mathrm{ar}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda^{\\prime})\\right]}\\\\ &{=K(\\lambda,\\lambda)+K(\\lambda^{\\prime},\\lambda^{\\prime})-2\\tau^{2}K(\\lambda,\\lambda^{\\prime})}\\\\ &{=(1-\\tau^{2})[K(\\lambda,\\lambda)+K(\\lambda^{\\prime},\\lambda^{\\prime})]+\\tau^{2}[K(\\lambda,\\lambda)-K(\\lambda,\\lambda^{\\prime})]+\\tau^{2}[K(\\lambda^{\\prime},\\lambda^{\\prime})-K(\\lambda,\\lambda^{\\prime})]}\\\\ &{\\leq2(1-\\tau^{2})\\sigma^{2}+2\\tau^{2}\\sigma^{2}\\kappa^{2}\\|\\lambda-\\lambda^{\\prime}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For $s^{2}>4\\sigma^{2}(1-\\tau^{2})$ , we thus have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\lambda\\in\\Lambda}{\\operatorname*{sup}}\\underset{1\\leq k\\leq N(s)}{\\operatorname*{min}}\\mathsf{V a r}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda_{k})\\right]\\leq\\underset{\\|\\lambda-\\lambda^{\\prime}\\|_{2}\\leq(s/2\\tau\\sigma\\kappa)^{2}}{\\operatorname*{sup}}\\mathsf{V a r}\\left[\\epsilon(\\lambda)-\\epsilon(\\lambda^{\\prime})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2(1-\\tau^{2})\\sigma^{2}+2\\tau^{2}\\sigma^{2}\\kappa^{2}(s/2\\tau\\sigma\\kappa)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq s^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as desired. Now decompose the integral ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{\\infty}\\sqrt{\\log N(s)}d s=\\int_{0}^{2\\sigma\\sqrt{1-\\tau^{2}}}\\sqrt{\\log N(s)}d s+\\int_{2\\sigma\\sqrt{1-\\tau^{2}}}^{2\\sigma}\\sqrt{\\log N(s)}d s}}\\\\ &{}&{\\leq2\\sigma\\sqrt{d}\\sqrt{1-\\tau^{2}}\\sqrt{\\log J}+\\int_{2\\sigma\\sqrt{1-\\tau^{2}}}^{2\\sigma}\\sqrt{\\log N(s)}d s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the second term, compute ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\int_{o\\sqrt{1-\\tau^{2}}}^{2\\sigma}\\sqrt{\\log N(s)}d s\\leq\\sqrt{d}\\int_{2o\\sqrt{1-\\tau^{2}}}^{2\\sigma}\\sqrt{\\log(6\\sigma\\kappa/s)_{+}}\\,d s}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sigma\\sqrt{d}\\int_{2\\sqrt{1-\\tau^{2}}}^{2\\sigma}\\sqrt{\\log(6\\kappa/s)_{+}}\\,d s}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sigma\\sqrt{d}\\left(\\int_{0}^{2}\\log(6\\kappa/s)_{+}\\,d s\\right)^{1/2}\\left(2(1-\\sqrt{1-\\tau^{2}})\\right)^{1/2}}\\\\ &{\\qquad\\qquad\\qquad=\\sigma\\sqrt{d}\\sqrt{2+2\\log(3\\kappa)_{+}}\\left(2(1-\\sqrt{1-\\tau^{2}})\\right)^{1/2}}\\\\ &{\\qquad\\qquad\\qquad=2\\sigma\\sqrt{d}\\sqrt{1+\\log(3\\kappa)_{+}}\\frac{\\tau}{(1+\\sqrt{1-\\tau^{2}})^{1/2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\sigma\\sqrt{d}\\tau\\sqrt{1+\\log(3\\kappa)_{+}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have shown that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{\\lambda\\not\\in\\Lambda_{\\delta}}\\epsilon(\\lambda)\\right]\\leq24\\sigma\\sqrt{d}\\left[\\sqrt{1-\\tau^{2}}\\sqrt{\\log{J}}+\\tau\\sqrt{1+\\log(3\\kappa)_{+}}\\right]=:\\sigma\\sqrt{d}B(\\tau)/4.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Integrating Probabilities ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Summarizing the two previous steps, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\mu({\\widehat{\\lambda}})-\\mu(\\lambda^{*})>\\delta\\right)\\leq2\\exp\\left\\{-{\\frac{\\left(\\delta-\\sigma{\\sqrt{d}}[B(\\tau)-A(\\tau,\\delta)]\\right)^{2}}{36\\sigma^{2}}}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "provided $t\\geq\\sigma\\sqrt{d}[B(\\tau)-A(\\tau,\\delta)]$ . Now for any $s\\geq0$ and $t\\geq2e^{s^{2}}m\\eta^{2}$ , it holds ", "page_idx": 21}, {"type": "equation", "text": "$$\nA(\\tau,s)\\geq(\\underline{{\\sigma}}/\\sigma)\\sqrt{1-\\tau^{2}}s=:A(\\tau)s.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, if ", "page_idx": 21}, {"type": "equation", "text": "$$\nt\\geq2e^{s^{2}}m\\eta^{2}+\\sigma{\\sqrt{d}}[B(\\tau)-A(\\tau)s]=:C,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\mu({\\widehat{\\lambda}})-\\mu(\\lambda^{*})>\\delta\\right)\\leq4\\exp\\left\\{-\\frac{\\left(\\delta-\\sigma\\sqrt{d}[B(\\tau)-A(\\tau)s]\\right)^{2}}{36\\sigma^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Integrating the probability gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})]=\\displaystyle\\int_{0}^{\\infty}\\operatorname*{Pr}\\left(\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})>\\delta\\right)d\\delta}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{0}^{C}\\operatorname*{Pr}\\left(\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})>\\delta\\right)d\\delta+\\displaystyle\\int_{C}^{\\infty}\\operatorname*{Pr}\\left(\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})>\\delta\\right)d\\delta}\\\\ &{\\qquad\\qquad\\qquad\\leq C+\\displaystyle\\int_{C}^{\\infty}\\exp\\left\\lbrace-\\frac{\\left(\\delta-\\sigma\\sqrt{d}[B(\\tau)-A(\\tau)s]\\right)^{2}}{36\\sigma^{2}}\\right\\rbrace d\\delta}\\\\ &{\\qquad\\qquad\\leq C+\\sqrt{36}\\sigma}\\\\ &{\\qquad\\qquad=2e^{s^{2}}m\\eta^{2}+\\sigma\\sqrt{d}[B(\\tau)-A(\\tau)s]+6\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Simplifying ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The bound can be optimized with respect to $s$ , but the solution involves the Lambert $W$ -function, which has no analytical expression. Instead choose $s$ for simplicity as ", "page_idx": 22}, {"type": "equation", "text": "$$\ns=\\sqrt{\\log\\left(\\frac{\\sigma}{2m\\eta^{2}}\\right)_{+}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mu(\\widehat{\\lambda})-\\mu(\\lambda^{*})]\\leq\\sigma\\sqrt{d}\\left[8+B(\\tau)-A(\\tau)\\sqrt{\\log\\left(\\frac{\\sigma}{2m\\eta^{2}}\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D Additional Results on the Density of Random HPC Grids ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma D.1. Suppose that the $J$ elements in \u039b are drawn independently from a continuous density $p$ with $c:=\\operatorname*{min}_{\\|\\pm\\|\\leq1}p(\\pmb{\\lambda})>0$ . Then with probability at least $1-\\delta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta\\lesssim\\left(\\sqrt{\\log(1/\\delta)/J}\\right)^{1/d},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and with probability $^{\\,l}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta\\lesssim\\left(\\sqrt{\\log(J)/J}\\right)^{1/d},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $J$ sufficiently large. ", "page_idx": 22}, {"type": "text", "text": "Proof. We want to bound the probability that there is a $\\lambda$ such that $|B(\\lambda,\\eta)\\cap\\Lambda|=0$ . In what follows $\\lambda$ is silently understood to have norm bounded by 1. Let $\\widetilde\\lambda_{1},\\l.\\l.\\l.\\widetilde\\lambda_{N}$ the centers of $\\eta/2$ -balls covering $\\{\\|\\pm\\|\\le1\\}$ , for which we may assume $N\\leq(6/\\eta)^{d}$ . For $\\widetilde{\\lambda}_{k}$ the closest center to $\\lambda$ , it holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\pmb{\\lambda}^{\\prime}-\\pmb{\\lambda}\\|\\leq\\|\\pmb{\\lambda}^{\\prime}-\\widetilde{\\pmb{\\lambda}}_{k}\\|+\\|\\widetilde{\\pmb{\\lambda}}_{k}-\\pmb{\\lambda}\\|\\leq\\|\\pmb{\\lambda}^{\\prime}-\\widetilde{\\pmb{\\lambda}}_{k}\\|+\\eta/2,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so $\\|\\mathsf{\\boldsymbol{\\lambda}}^{\\prime}-\\widetilde{\\mathsf{\\boldsymbol{\\lambda}}}_{k}\\|\\leq\\eta/2$ implies $\\|\\pmb{\\lambda}^{\\prime}-\\pmb{\\lambda}\\|\\leq\\eta$ . We thus have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(\\exists\\lambda\\colon|B(\\lambda,\\eta)\\cap\\Lambda|=0)=\\operatorname*{Pr}\\left(\\underset{\\lambda}{\\operatorname*{inf}}\\sum_{i=1}^{J}\\mathbb{1}\\{\\|\\lambda_{i}-\\lambda\\|\\le\\eta\\}\\le0\\right)}\\\\ &{\\qquad\\qquad\\qquad\\le\\operatorname*{Pr}\\left(\\underset{1\\le k\\le N}{\\operatorname*{min}}\\sum_{i=1}^{J}\\mathbb{1}\\{\\|\\lambda_{i}-\\widetilde{\\lambda}_{k}\\|\\le\\eta/2\\}\\le0\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Further ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}\\left(\\underset{1\\leq k\\leq N}{\\operatorname*{min}}\\displaystyle\\sum_{i=1}^{J}\\mathbb{1}\\{\\|\\lambda_{i}-\\widetilde{\\lambda}_{k}\\|\\leq\\eta/2\\}\\leq0\\right)}\\\\ &{=\\operatorname*{Pr}\\left(\\underset{1\\leq k\\leq N}{\\operatorname*{max}}\\displaystyle\\sum_{i=1}^{J}-\\mathbb{1}\\{\\|\\lambda_{i}-\\widetilde{\\lambda}_{k}\\|\\leq\\eta/2\\}\\geq0\\right)}\\\\ &{\\leq\\operatorname*{Pr}\\left(\\underset{1\\leq k\\leq N}{\\operatorname*{max}}\\displaystyle\\sum_{i=1}^{J}\\mathbb{E}\\left[\\mathbb{1}\\{\\|\\lambda_{i}-\\widetilde{\\lambda}_{k}\\|\\leq\\eta/2\\}\\right]-\\mathbb{1}\\{\\|\\lambda_{i}-\\widetilde{\\lambda}_{k}\\|\\leq\\eta/2\\}\\geq J\\underset{\\lambda}{\\operatorname*{inf}}\\,\\mathbb{E}\\left[\\mathbb{1}\\{\\|\\lambda_{i}-\\lambda\\|\\leq\\eta/2\\}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{E}\\left[\\mathbb{1}\\{\\|\\lambda_{i}-\\lambda\\|\\leq\\eta/2\\}\\right]=\\operatorname*{Pr}\\left(\\|\\lambda_{i}-\\lambda\\|\\leq\\eta/2\\right)=\\int_{\\|\\lambda^{\\prime}-\\lambda\\|\\leq\\eta/2}p(\\lambda^{\\prime})d\\lambda^{\\prime}\\geq c\\,{\\mathrm{vol}}(B(0,\\eta/2))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $v_{d}=\\mathrm{vol}(B(0,1))$ . Now the union bound and Hoeffding\u2019s inequality give ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left(\\underset{1\\leq k\\leq N}{\\operatorname*{min}}\\displaystyle\\sum_{i=1}^{J}\\mathbb{1}\\{\\|\\mathbf{1}_{i}-\\widetilde{\\mathbf{\\lambda}}_{k}\\|\\leq\\eta/2\\}\\leq0\\right)\\leq N\\exp\\left(-\\frac{J c^{2}v_{d}^{2}(\\eta/2)^{2d}}{2}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq(6/\\eta)^{d}\\exp\\left(-\\frac{J c^{2}v_{d}^{2}(\\eta/2)^{2d}}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Choosing ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta=2\\left(\\sqrt{2\\log(3^{d}\\sqrt{J}c v_{d}/\\delta)}/\\sqrt{J}c v_{d}\\right)^{1/d}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(\\exists\\lambda\\colon|B(\\lambda,\\eta)\\cap\\Lambda|=0)\\le\\delta/\\sqrt{2\\log(3^{d}\\sqrt{J}c v_{d})},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is bounded by $\\delta$ when $\\sqrt{J}\\,\\ge\\,e^{1/2}/3^{d}c v_{d}$ . Further, setting $\\eta\\,=\\,2(\\sqrt{6\\log(J)}/\\sqrt{J}c v_{d})^{1/d}$ gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\operatorname*{min}_{1\\leq k\\leq N}\\sum_{i=1}^{J}\\mathbb{1}\\{\\|\\pmb{\\lambda}_{i}-\\widetilde{\\pmb{\\lambda}}_{k}\\|\\leq\\eta/2\\}\\leq0\\right)\\lesssim J^{-5/2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{J=1}^{\\infty}\\operatorname*{Pr}\\left(\\operatorname*{min}_{1\\leq j\\leq J}\\operatorname*{min}_{1\\leq k\\leq N}\\displaystyle\\sum_{i=1}^{j}\\mathbb{1}\\{\\|\\lambda_{i}-\\widetilde{\\lambda}_{k}\\|\\leq\\eta/2\\}\\leq0\\right)}\\\\ &{\\leq\\displaystyle\\sum_{J=1}^{\\infty}J\\operatorname*{Pr}\\left(\\operatorname*{min}_{1\\leq k\\leq N}\\displaystyle\\sum_{i=1}^{J}\\mathbb{1}\\{\\|\\lambda_{i}-\\widetilde{\\lambda}_{k}\\|\\leq\\eta/2\\}\\leq0\\right)}\\\\ &{\\lesssim\\displaystyle\\sum_{J=1}^{\\infty}\\frac{1}{J^{3/2}}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now the Borel-Cantelli lemma (e.g., Kallenberg, 1997, Theorem 4.18) implies that, with probability 1, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|B(\\pmb{\\lambda},\\eta)\\cap\\Lambda|\\geq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $J$ sufficiently large. ", "page_idx": 23}, {"type": "text", "text": "E Selected Validation Schemes ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Definition of Index Sets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "(i) (holdout) Let $M=1$ and $\\mathcal{T}_{1,j}=\\mathcal{T}_{1}$ for all $j=1,\\dots,J$ , and some size- $\\lceil\\alpha n\\rceil$ index set ${\\mathcal{T}}_{1}$ .   \n(ii) (reshuffled holdout) Let $M=1$ and $\\mathcal{T}_{1,1},\\dotsc,\\mathcal{T}_{1,J}$ be independently drawn from the uniform distribution over all size- $\\lceil\\alpha n\\rceil$ subsets from $\\{1,\\ldots,n\\}$ .   \n(iii) ( $M$ -fold CV) Let $\\alpha=1/M$ and $\\mathcal{T}_{1},\\dots,\\mathcal{T}_{M}$ be a disjoint partition of $\\{1,\\ldots,n\\}$ , and $\\mathcal{Z}_{m,j}=$ ${\\mathcal{T}}_{m}$ for all $j=1,\\dots,J$ .   \n(iv) (reshuffled $M$ -fold CV) Let $\\alpha=1/M$ and $({\\mathcal{T}}_{1,j},\\ldots,{\\mathcal{T}}_{M,j}),j=1,\\ldots,J.$ , be independently drawn from the uniform distribution over disjoint partitions of $\\{1,\\ldots,n\\}$ .   \n(v) ( $M$ -fold holdout) Let $\\mathcal{I}_{m},m=1,\\ldots,M$ , be independently drawn from the uniform distribution over size- $\\lceil\\alpha n\\rceil$ subsets of $\\{1,\\ldots,n\\}$ and set $\\mathcal{T}_{m,j}=\\mathcal{T}_{m}$ for all $m=1,\\ldots,M,j=1,\\ldots,J$ .   \n(vi) (reshuffled $M$ -fold holdout) Let $\\mathcal{Z}_{m,j},m=1,\\ldots,M,j=1,\\ldots,J$ , be independently drawn from the uniform distribution over size- $\\lceil\\alpha n\\rceil$ subsets of $\\{1,\\ldots,n\\}$ . ", "page_idx": 24}, {"type": "text", "text": "E.2 Derivation of Reshuffling Parameters in Limiting Distribution ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\frac{1}{n M^{2}\\alpha^{2}}\\sum_{s=1}^{n}\\sum_{m=1}^{M}\\sum_{m^{\\prime}=1}^{M}\\operatorname*{Pr}(s\\in\\mathbb{Z}_{m,i}\\cap\\mathcal{T}_{m^{\\prime},j}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For all schemes in the proposition, the probabilities are independent of the index $s$ , so the average over $s=1,\\ldots,n$ can be omitted. We now verify the constants $\\sigma,\\tau$ from Table 1. ", "page_idx": 24}, {"type": "text", "text": "(i) It holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(s\\in{\\mathcal{Z}}_{1,i}\\cap{\\mathcal{Z}}_{1,j})=\\operatorname*{Pr}(s\\in{\\mathcal{Z}}_{1})=\\alpha.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{i,j,1}=1/\\alpha=1/\\alpha\\times1=\\sigma^{2}\\times\\tau^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(ii) (reshuffled holdout) This is a special case of part (vi) with $M=1$ . ", "page_idx": 24}, {"type": "text", "text": "(iii) ( $M$ -fold CV) It holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(s\\in\\mathcal{Z}_{m,i}\\cap\\mathcal{Z}_{m^{\\prime},j})=\\mathrm{Pr}(s\\in\\mathcal{Z}_{m}\\cap\\mathcal{Z}_{m^{\\prime}})=\\left\\{1/M,\\begin{array}{l l}{m=m^{\\prime},}\\\\ {0,}&{m\\neq m^{\\prime}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Only $M$ probabilities in the double sum are non-zero, whence ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\frac{1}{M^{2}\\alpha^{2}}\\times M/M=1/\\alpha^{2}M^{2}=1\\times1=\\sigma^{2}\\times\\tau^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used $\\alpha=1/M$ . ", "page_idx": 24}, {"type": "text", "text": "(iv) (reshuffled $M$ -fold CV) It holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(s\\in\\mathbb{Z}_{m,i}\\cap\\mathbb{Z}_{m^{\\prime},j})=\\left\\{\\!\\!\\begin{array}{l l}{1/M,}&{m=m^{\\prime},i=j}\\\\ {0,}&{m\\neq m^{\\prime},i=j}\\\\ {1/M^{2},}&{m=m^{\\prime},i\\neq j}\\\\ {1/M^{2},}&{m\\neq m^{\\prime},i\\neq j.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $i=j$ , only $M$ probabilities in the double sum are non-zero. Also using $\\alpha=1/M$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\frac{1}{M^{2}\\alpha^{2}}\\times M\\times1/M=1=\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $i\\neq j$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\frac{1}{M^{2}\\alpha^{2}}\\times M^{2}\\times1/M^{2}=1\\times1=\\sigma^{2}\\times\\tau^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(v) ( $M$ -fold holdout) It holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(s\\in\\mathbb{Z}_{m,i}\\cap\\mathcal{Z}_{m^{\\prime},j})=\\operatorname*{Pr}(s\\in\\mathbb{Z}_{m}\\cap\\mathbb{Z}_{m^{\\prime}})={\\binom{\\alpha,\\quad m=m^{\\prime},}{\\alpha^{2},\\,\\,\\,\\,\\mathrm{else.}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\frac{1}{M^{2}\\alpha^{2}}\\times[M\\times\\alpha+(M-1)M\\times\\alpha^{2}]=[1/\\alpha M+(M-1)/M]\\times1=\\sigma^{2}\\times\\tau^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all $i,j$ ", "page_idx": 25}, {"type": "text", "text": "(vi) (reshuffled $M$ -fold holdout) It holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(s\\in\\mathbb{Z}_{m,i}\\cap\\mathbb{Z}_{m^{\\prime},j})={\\binom{\\alpha,\\quad m=m^{\\prime},i=j}{\\alpha^{2},\\quad{\\mathrm{else}}.}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $i=j$ , this gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\frac{1}{M^{2}\\alpha^{2}}\\times[M\\times\\alpha+(M-1)M\\times\\alpha^{2}]=1/\\alpha M+(M-1)/M.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $i\\neq j$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau_{i,j,M}=\\frac{1}{M^{2}\\alpha^{2}}\\times(M^{2}\\times\\alpha^{2})=1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This implies that (1) holds with $\\sigma^{2}=1/M\\alpha+(M-1)/M$ , $\\tau^{2}=1/(1/M\\alpha+(M-1)/M)$ . Remark E.1. Although not technically covered by Theorem 2.1, performing independent boo\u221atstraps for each $\\lambda_{j}$ correspond to reshuffled $n$ -fold holdout with $\\alpha\\,=\\,1/n$ . Accordingly, $\\sigma\\,\\approx\\,{\\sqrt{2}}$ and $\\tau\\approx\\sqrt{1/2}$ . ", "page_idx": 25}, {"type": "text", "text": "F Details Regarding Benchmark Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We list all datasets used in the benchmark experiments in Table 4. ", "page_idx": 25}, {"type": "text", "text": "Table 4: List of datasets used in benchmark experiments. All information can be found on OpenML (Vanschoren et al., 2014). ", "page_idx": 25}, {"type": "table", "img_path": "C4SInFLvuB/tmp/cfdbc99e1a300d9cbe4baae3a31867c1a6a6e2ea6c7386cbf7ea1e1f56def9d2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Note that datasets serve as data generating processes (DGPs; Hothorn et al., 2005). As we are mostly concerned with the actual generalization performance of the final best HPC found during HPO based on validation performance we rely on a comparably large held out test set that is not used during HPO. We therefore use 5000 data points sampled from a DGP as an outer test set. To further be able to measure the generalization performance robustly for varying data sizes available during HPO, we construct concrete tasks based on the DGPs by sampling subsets of (train_valid; $n$ ) size 500, 1000 and 5000 from the DGPs. This results in 30 tasks in total ( $10\\,\\mathrm{DGPS}\\times3$ train_valid sizes). For more details and the concrete implementation of this procedure, see Appendix F.3. We also collected another 5000 data points as an external validation set, but did not use it. Therefore, we had to tighten the restriction to 10000 data points mentioned in the main paper to 15000 data points as the lower bound on data points. To allow for stronger variation over different replications, we decided to use 20000 as the final lower bound. ", "page_idx": 25}, {"type": "text", "text": "F.2 Learning Algorithms ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we briefly present training pipeline details and search spaces of the learning algorithms used in our benchmark experiments. ", "page_idx": 26}, {"type": "text", "text": "The funnel-shaped MLP is based on sklearn\u2019s MLP Classifier and is constructed in the following way: The hidden layer size for each layer is determined by num_layers and max_units. We start with max_units and half the number of units for every subsequent layer to create a funnel. max_batch_size is the largest power of 2 that is smaller than the number of training samples available. We use ReLU as activation function and train the network optimizing logloss as a loss function via SGD using a constant learning rate and Nesterov momentum for 100 epochs. Table 5 lists the search space (inspired from Zimmer et al. (2021)) used during HPO. ", "page_idx": 26}, {"type": "text", "text": "The Elastic Net is based on sklearn\u2019s Logistic Regression Classifier. We train it for a maximum of 1000 iterations using the \"saga\" solver. Table 6 lists the search space used during HPO. ", "page_idx": 26}, {"type": "text", "text": "The XGBoost and CatBoost search spaces are listed in Table 7 and Table 8, both inspired from their search spaces used in McElfresh et al. (2023). ", "page_idx": 26}, {"type": "text", "text": "For both the Elastic Net and Funnel MLP, missing values are imputed in the preprocessing pipeline (mean imputation for numerical features and adding a new level for categorical features). Categorical features are target encoded in a cross-validated manner using a 5-fold CV. Features are then scaled to zero mean and unit variance via a standard scaler. For XGBoost, we impute missing values for categorical features (adding a new level) and target encode them in a cross-validated manner using a 5-fold CV. For CatBoost, no preprocessing is performed. ", "page_idx": 26}, {"type": "text", "text": "XGBoost and CatBoost models are trained for 2000 iterations and stop early if the validation loss (using the default internal loss function used during training, i.e., logloss) does not improve over a horizon of 20 iterations. For retraining the best configuration on the whole train and validation data, the number of boosting iterations is set to the number of iterations used to find the best validation performance prior to the stopping mechanism taking action.7 ", "page_idx": 26}, {"type": "text", "text": "F.3 Exact Implementation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In the following, we outline the exact implementation of performing one HPO run for a given learning algorithm on a concrete task (dataset $\\times$ train_valid size) and a given resampling. We release all code to replicate benchmark results and reproduce our analyses via https://github.com/slds-l mu/paper_2024_reshuffling. For a given replication (in total 10): ", "page_idx": 26}, {"type": "text", "text": "1. We sample (without replacement) train_valid size (500, 1000 or 5000 points) and test size (always 5000) points from the DGP (i.e. a concrete dataset in Table 4). These are shared for every learning algorithm (i.e. all learning algorithms are evaluated on the same data). ", "page_idx": 26}, {"type": "text", "text": "2. A given HPC is evaluated in the following way: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The resampling operates on the train validation8 set of size train_valid.   \n\u2022 The learning algorithm is configured by the HPC.   \n\u2022 The learning algorithm is trained on training splits and evaluated on validation splits according to the resampling strategy. In case reshuffling is turned on, the training and validation splits are recreated for every HPO. We compute the Accuracy, ROC AUC and logloss when using a random search and compute ROC AUC when using HEBO or SMAC3 and average performance over all folds for resamplings involving multiple folds.   \n\u2022 For each HPC we then always re-train the model on all train_valid data being available and evaluate the model on the held-out test set to compute an outer estimate of generalization performance for each HPC (regardless of whether it is the incumbent for a given iteration or not). ", "page_idx": 26}, {"type": "table", "img_path": "C4SInFLvuB/tmp/ab9a0617dabcb0e2a83cf482432dee5848bb9b3575c7999308ff46fdff11372c.jpg", "table_caption": ["Table 5: Search Space for Funnel-Shaped MLP Classifier. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "C4SInFLvuB/tmp/1d49b8318b82467eeae135a4a002798fedd22a4566312882351725752620b45a.jpg", "table_caption": ["Table 6: Search Space for Elastic Net Classifier. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "C4SInFLvuB/tmp/895d64a66ac31d993476e7dcaa9b43bf72daa873d51331de16faa1ff17691c79.jpg", "table_caption": ["Table 7: Search Space for XGBoost Classifier. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "C4SInFLvuB/tmp/82c75e057f4e270288400cecf61a495cdb79c03cb1fe6eb34a3d3459bb781d33.jpg", "table_caption": ["Table 8: Search Space for CatBoost Classifier. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "3. We evaluate $500\\;\\mathrm{HPCs}$ when using random search and $250\\;\\mathrm{HPC}$ when using HEBO or SMAC3 (SMAC4HPO facade). ", "page_idx": 27}, {"type": "text", "text": "As resamplings, we use holdout with a 80/20 train-validation split and 5 folds for CV, so that the holdout strategy is just one fold of the CV and the fraction of data points being used for training and respectively validation are the same across different resampling strategies. 5-fold holdout simply repeats the holdout procedure five times and 5x 5-fold CV repeats the 5-fold CV five times. Each of the four resamplings can be reshuffled or not (standard). ", "page_idx": 27}, {"type": "text", "text": "As mentioned above, the test set is only varied for each of the 10 replica (repetitions with different seeds), but consistent for different tasks (i.e. the different learning algorithms are evaluated on the same test set, similarly, also the different dataset subsets all share the same test set). This allows for fair comparisons of different resamplings on a concrete problem (i.e. a given dataset, train_valid size and learning algorithm). Additionally, for the random search, the $500\\,\\mathrm{HPCs}$ evaluated for a given learning algorithm are also fixed over different dataset and train_valid size combinations. This is done to allow for an isolation of the effect, the concrete resampling (and whether it is reshuffled or not) has on generalization performance, reducing noise arising due to different HPCs. Learning algorithms themselves are not explicitly seeded to allow for variation during model training over different replications. Resamplings and partitioning of data are always performed in a stratified manner with respect to the target variable. ", "page_idx": 27}, {"type": "text", "text": "For the random search, we only ran (standard and reshuffled) holdout and (standard and reshuffled) $5\\mathrm{x}\\;5$ -fold CV experiments (because we can simulate 5-fold CV and 5-fold holdout experiments based on the results obtained from the 5x 5-fold CV (by only considering the first repeat or the first fold for each of the five repeats).9 ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "For running HEBO or SMAC3, each resampling (standard and reshuffled for holdout, 5-fold holdout, 5-fold CV, 5x 5-fold CV) has to be actually run due to the adaptive nature of BO. ", "page_idx": 28}, {"type": "text", "text": "For the random search experiments, this results in $10\\left(\\mathrm{DGPs}\\right)\\times3$ (train_valid sizes) $\\times\\,4$ (learning algorithms) $\\times\\,2$ (holdout or $\\mathsf{5x}$ 5-fold $\\mathrm{CV})\\times2$ (standard or reshuffled) $\\times\\,10$ (replications) $=4800$ HPO runs,10 each involving the evaluation of $500\\;\\mathrm{HPCs}$ and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data) or 26 (for 5x 5-fold CV; due to retraining on train validation data) model fits. In summary, the random search experiments involve the evaluation of 2.4 Million HPCs with in total 33.6 Million model fits. ", "page_idx": 28}, {"type": "text", "text": "Similarly, for the HEBO and SMAC3 experiments, this each results in $10\\,(\\mathrm{DGPs})\\times3$ (train_valid sizes) $\\times\\ 4$ (learning algorithms) $\\times\\ 4$ (holdout, 5-fold CV, 5x 5-fold CV or 5-fold holdout) $\\times\\ 2$ (standard or reshuffled) $\\times\\ 10$ (replications) $\\mathsf{\\Delta}=9600~\\mathrm{{HPO\\runs}^{11}}$ , each involving the evaluation of $250\\,\\mathrm{HPCs}$ and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data), 6 (for 5-fold CV or 5-fold holdout; due to retraining on train validation data) or 26 (for $5\\mathrm{x}$ 5-fold CV; due to retraining on train validation data) model ftis. In summary, the HEBO and SMAC3 experiments each involve the evaluation of 2.4 Million HPCs with in total 24 Million model fits. ", "page_idx": 28}, {"type": "text", "text": "F.4 Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We estimate our total compute time for the random search, HEBO and SMAC3 experiments to be roughly 11.86 CPU years. Benchmark experiments were run on an internal HPC cluster equipped with a mix of Intel Xeon E5-2670, Intel Xeon E5-2683 and Intel Xeon Gold 6330 instances. Jobs were scheduled to use a single CPU core and were allowed to use up to 16GB RAM. Total emissions are estimated to be an equivalent of roughly 6508.67 kg CO2. ", "page_idx": 28}, {"type": "text", "text": "G Additional Benchmark Results Visualizations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "G.1 Main Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we provide additional visualizations of the results of our benchmark experiments. ", "page_idx": 28}, {"type": "text", "text": "Figure 6 illustrates the trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. We can see that the reshuffled holdout on average comes close to the final test performance of the overall more expensive 5-fold CV. ", "page_idx": 28}, {"type": "text", "text": "Below, we give an overview of the different types of additional analyses and visualizations we provide. Normalized metrics, i.e., normalized validation or test performance refer to the measure being scaled to $[0,1]$ based on the empirical observed minimum and maximum values obtained on the raw results level (ADTM; see Wistuba et al., 2018). More concretely, for each scenario consisting of a learning algorithm that is run on a given task (dataset $\\times$ train_valid size) given a certain performance metric, the performance values (validation or test) for all resamplings and optimizers are normalized on the replication level to $[0,1]$ by subtracting the empirical best value and dividing by the range of performance values. Therefore a normalized performance value of 0 is best and 1 is worst. Note that we additionally provide further aggregated results on the learning algorithm level and raw results of validation and test performance via https://github.com/slds-lmu/paper_2024_reshuffl ing. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Random search ", "page_idx": 28}, {"type": "text", "text": "\u2013 Normalized validation performance in Figure 7. ", "page_idx": 28}, {"type": "image", "img_path": "C4SInFLvuB/tmp/52b8d47cc9acaaeb1d6f2a633768deefa9c55e909f664001b255e570cfa06b91.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 29}, {"type": "text", "text": "\u2013 Normalized test performance in Figure 8. \u2013 Improvement in test performance over 5-fold CV in Figure 9. \u2013 Rank w.r.t. test performance in Figure 10.   \n\u2022 HEBO and SMAC3 vs. random search holdout \u2013 Normalized validation performance in Figure 11. \u2013 Normalized test performance in Figure 12. \u2013 Improvement in test performance over standard holdout in Figure 13. \u2013 Rank w.r.t. test performance in Figure 14.   \n\u2022 HEBO and SMAC3 vs. random search 5-fold holdout \u2013 Normalized validation performance in Figure 15. \u2013 Normalized test performance in Figure 16. \u2013 Improvement in test performance over standard 5-fold holdout in Figure 17. \u2013 Rank w.r.t. test performance in Figure 18.   \n\u2022 HEBO and SMAC3 vs. random search 5-fold CV \u2013 Normalized validation performance in Figure 19. \u2013 Normalized test performance in Figure 20. \u2013 Improvement in test performance over 5-fold CV in Figure 21. \u2013 Rank w.r.t. test performance in Figure 22.   \n\u2022 HEBO and SMAC3 vs. random search 5x 5-fold CV \u2013 Normalized validation performance in Figure 23. \u2013 Normalized test performance in Figure 24. \u2013 Improvement in test performance over 5x 5-fold CV in Figure 25. \u2013 Rank w.r.t. test performance in Figure 26. ", "page_idx": 29}, {"type": "image", "img_path": "C4SInFLvuB/tmp/0285507eb2f4cc75eeb8b70b88778dffb729dbac9b4c259b5c49021ed72d287f.jpg", "img_caption": ["Figure 7: Random search. Average normalized performance over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "C4SInFLvuB/tmp/3c526a24e1ee6bb6800a1e3d742dc7b9e8f198305d7bd5d649113021b3f34556.jpg", "img_caption": ["Figure 8: Random search. Average normalized test performance over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "C4SInFLvuB/tmp/e55dbf86078f35790876a912f2c4c487ece49058140f4d5bcf434a4382bb9b14.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 9: Random search. Average improvement (compared to standard 5-fold CV) with respect to test performance of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 31}, {"type": "image", "img_path": "C4SInFLvuB/tmp/40f88febbeb96501409b2eef889b2b8db15b3be08cb180830391a5da60f4b174.jpg", "img_caption": ["Figure 10: Random search. Average ranks (lower is better) with respect to test performance over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "C4SInFLvuB/tmp/26e4b16fb90b64eba2874ca1a5ed147a7332c009b3ab22e914e74ec05768ab43.jpg", "img_caption": ["Figure 11: HEBO and SMAC3 vs. random search for holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "C4SInFLvuB/tmp/9264a9b57bcb17765be2a0396830a596907fdd76be783a45ed1caf90caafa4eb.jpg", "img_caption": ["Figure 12: HEBO and SMAC3 vs. random search for holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "C4SInFLvuB/tmp/50e171621c7c4bcc414e4e847ffaf4964ecdfbaedd9920b4c5a1234bf17ef14e.jpg", "img_caption": ["Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 13: HEBO and SMAC3 vs. random search for holdout. Average improvement (compared to standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 32}, {"type": "image", "img_path": "C4SInFLvuB/tmp/cb5d782e94ae35e10b7732e81c4d56ceef66b8f2a94f4e69316e157c545fb293.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 14: HEBO and SMAC3 vs. random search for holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 33}, {"type": "image", "img_path": "C4SInFLvuB/tmp/a8dd6df383f8b7173f2a537965c7c5315efa2291eba0c758ec6741c94d748cb1.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 15: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 33}, {"type": "image", "img_path": "C4SInFLvuB/tmp/183fcb12ed932788463b810a28f7786bebf979efc1e117d7f7ff2b553ec8558b.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 16: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 33}, {"type": "image", "img_path": "C4SInFLvuB/tmp/22af83b0d65d9389f5200978ddfd6471dfed7a2c51e2792e83dc8ea53fed8cdd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 17: HEBO and SMAC3 vs. random search for 5-fold holdout. Average improvement (compared to standard 5-fold holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 34}, {"type": "image", "img_path": "C4SInFLvuB/tmp/0c2f5c072c818eb2109aba8a0a6097f0179aa1c0fe3e29993229ead5ef7bd3d9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 18: HEBO and SMAC3 vs. random search for 5-fold holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 34}, {"type": "image", "img_path": "C4SInFLvuB/tmp/86019b46064961aa4a9100e7765557779c35d046fd1ee5c6f2cdf115e23d6c2c.jpg", "img_caption": ["Optimizer Random Search HEBO SMAC3 Reshuffling FALSE .... TRUE "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 19: HEBO and SMAC3 vs. random search for 5-fold CV. Average normalized validation performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 34}, {"type": "image", "img_path": "C4SInFLvuB/tmp/4d90cd5b50024827141e5a188b5f7fba39de75ded4ce19b57721296db3819d7a.jpg", "img_caption": ["Figure 20: HEBO and SMAC3 vs. random search for 5-fold CV. Average normalized test performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "C4SInFLvuB/tmp/8d35fbffea856f87361c6881948680ab69fcfb5a277bc4c278c6d438341ca2d5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 21: HEBO and SMAC3 vs. random search for 5-fold CV. Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 35}, {"type": "image", "img_path": "C4SInFLvuB/tmp/4bf6e15ebf7de996a6f517e2691093bafde388a5dbcee81f62d3403e78fb91f0.jpg", "img_caption": ["Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 22: HEBO and SMAC3 vs. random search for 5-fold CV. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 35}, {"type": "image", "img_path": "C4SInFLvuB/tmp/09f908617e0b6cbb968cefec494386539bcc93369e3cab64b58384c759b24f9b.jpg", "img_caption": ["Figure 23: HEBO and SMAC3 vs. random search for 5x 5-fold CV. Average normalized validation performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "C4SInFLvuB/tmp/9bf17e81d63f4c2373fdbdb54ee9b2a16675f61e15b9677338742dc22e01eba7.jpg", "img_caption": ["Figure 24: HEBO and SMAC3 vs. random search for $\\mathsf{5x}$ 5-fold CV. Average normalized test performance (ROC AUC) over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "C4SInFLvuB/tmp/9050512fd26be08d4e92423c9aa731a8ed97cb3439987a14e52a7feaaaaf757e.jpg", "img_caption": ["Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Figure 25: HEBO and SMAC3 vs. random search for 5x 5-fold CV. Average improvement (compared to standard 5x 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 36}, {"type": "image", "img_path": "C4SInFLvuB/tmp/fe83a686256b7ac30b93700bf1610a5f6aacea90d5ee02be6d33c4a1f6a4b56c.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 26: HEBO and SMAC3 vs. random search for $5\\mathrm{x}\\,5$ -fold CV. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. ", "page_idx": 37}, {"type": "text", "text": "G.2 Ablation on M-fold holdout ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Based on the $5\\mathrm{x}$ 5-fold CV results we further simulated different $M$ -fold holdout resamplings (standard and reshuffled) by taking M repeats from the first fold of the $5\\mathrm{x}$ 5-fold CV. This allows us to get an understanding of the effect more folds have on $M$ -fold holdout, especially in the context of reshuffling. ", "page_idx": 38}, {"type": "text", "text": "Regarding normalized validation performance we observe that more folds generally result in a less optimistically biased validation performance (see Figure 27). Looking at normalized test performance (Figure 28) we observe the general trend that more folds result in better test performance \u2013 which is expected. Reshuffilng generally results in better test performance compared to the standard resampling (with the exception of logloss where especially in the case of a single holdout, reshuffling can hurt generalization performance). This effect is smaller, the more folds are used, which is in line with our theoretical results presented in Table 1. Looking at improvement compared to standard 5-fold holdout with respect to test performance and ranks with respect to test performance, we observe that often reshuffled 2-fold holdout results that are highly competitive with standard 3, 4 or 5-fold holdout. ", "page_idx": 38}, {"type": "image", "img_path": "C4SInFLvuB/tmp/5a70154266a3c2381a02a2b63d946217878e0eab3e73c35296e60b9e04eabcb9.jpg", "img_caption": ["Figure 27: Random search. Average normalized validation performance over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "C4SInFLvuB/tmp/cff19dcbdac033821ab3909e40d294935a797607549c8442d2d52fe87ac82905.jpg", "img_caption": ["Figure 28: Random search. Average normalized test performance over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "C4SInFLvuB/tmp/62d8265d77e51fdcb63e1c5fb9428fb7b4c02e4f7f4ccd529266a070b188f31b.jpg", "img_caption": ["Figure 29: Random search. Average improvement (compared to standard 5-fold holdout) with respect to test performance of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "C4SInFLvuB/tmp/a893aa4cfc8456ffce3242e5fab7bb9637446a541ffdb2a70ab45282d78aab8d.jpg", "img_caption": ["Figure 30: Random search. Average ranks (lower is better) with respect to test performance of the incumbent over tasks, learners and replications for different $n$ (train-validation sizes, columns). Shaded areas represent standard errors. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We outline our three main contributions in the introduction (Section 1). We do not discuss generalization in the introduction, but rather in the discussion in Section 5. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The paper provides an analysis of reshuffilng data in the context of estimating the generalization error for hyperparameter optimization. Our theoretical analysis explains why reshuffling works, and we experimentally verify the theoretical analysis. We discuss the limitations of our work in Section 5. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: Full assumptions and proofs for our main results (Theorem 2.1 and Theorem 2.2) are given in Appendix C.1 and Appendix C.2, respectively. Derivations for the parameters in Table 1 are provided in Appendix E. The additional results for the grid density are stated and proven directly in Appendix D. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 42}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Regarding datasets, we rely on OpenML.org. We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu /paper_2024_reshuffling. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We report the standard error in every analysis. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide details in Appendix F.4. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our work provides a study on reshuffilng data when estimating the generalization error in hyperparameter tuning. Therefore, our work is applicable wherever standard machine learning is applicable, and we do not see any ethical concerns in our method. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper conducts fundamental research that is not tied to particular applications, let alone deployment. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper conducts fundamental research that is not tied to particular applications, let alone deployment. The paper does not develop models that have a high risk for misuse. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We used datasets from OpenML.org and reference the dataset pages. Further information of the datasets, including their licenses, are available at OpenML.org. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 46}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide code as a new asset and describe how we make our code available in Point 5 of the NeurIPS Paper Checklist. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: The paper does neither involve crowdsourcing nor research with human subjects. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does neither involve crowdsourcing nor research with human subjects. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 47}]