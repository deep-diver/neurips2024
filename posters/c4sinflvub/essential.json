{"importance": "This paper is crucial for researchers in hyperparameter optimization (HPO).  It challenges the common practice of using fixed data splits, offering **a computationally cheaper and often superior alternative** that improves model generalization.  The theoretical analysis and large-scale experiments provide strong support for adopting reshuffling in HPO workflows, **opening new avenues for improving HPO efficiency and effectiveness**.", "summary": "Reshuffling data splits during hyperparameter optimization surprisingly improves model generalization, offering a computationally cheaper alternative to standard methods.", "takeaways": ["Reshuffling resampling splits in hyperparameter optimization often improves generalization performance.", "The benefit of reshuffling is particularly pronounced for holdout validation, sometimes making it competitive with cross-validation.", "Theoretical analysis explains the effect, linking it to the signal-to-noise ratio and loss surface characteristics."], "tldr": "Hyperparameter optimization (HPO) is crucial for machine learning, but current methods often rely on fixed data splits during model evaluation which can lead to overfitting. This paper introduces a simple yet effective technique called reshuffling, where data splits are randomly reassigned for every hyperparameter configuration evaluated.  This approach is shown to improve model generalization, especially with holdout validation, challenging the traditional approach.\nThe study combines theoretical analysis with extensive simulations and real-world experiments.  The theoretical findings connect the benefits of reshuffling to the inherent characteristics of the HPO problem, such as signal-to-noise ratio and the shape of the loss function. The experimental results demonstrate that reshuffling significantly boosts performance, making holdout validation a competitive alternative to cross-validation while reducing computational cost.", "affiliation": "Munich Center for Machine Learning (MCML)", "categories": {"main_category": "AI Theory", "sub_category": "Optimization"}, "podcast_path": "C4SInFLvuB/podcast.wav"}