[{"figure_path": "C4SInFLvuB/figures/figures_4_1.jpg", "caption": "Figure 1: Example of reshuffled empirical loss yielding a worse (left) and better (right) minimizer.", "description": "This figure shows two examples of how reshuffling affects the empirical loss surface. In the left panel (high signal-to-noise ratio), reshuffling results in a worse minimizer because the empirical loss surface is a very noisy approximation of the true loss surface. In the right panel (low signal-to-noise ratio), reshuffling results in a better minimizer because the empirical loss surface is a less noisy approximation of the true loss surface. The true loss function is shown as a black curve in both panels. The empirical loss functions with and without reshuffling are shown as light blue and dashed light blue curves respectively.", "section": "3 Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_6_1.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "This figure visualizes the results of a simulation study conducted to investigate the effects of reshuffling resampling splits during hyperparameter optimization.  The study systematically varied three key parameters: the curvature of the loss surface (m), the correlation strength of the noise (\u03ba), and the extent of reshuffling (\u03c4). The true risk (the generalization error of the model trained using the chosen hyperparameter configuration) is shown for each combination of parameters. The results show that reshuffling is particularly beneficial when the loss surface is flat (small m) and the noise is weakly correlated (large \u03ba).", "section": "3 Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_7_1.jpg", "caption": "Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "The figure shows the average test performance over time for different resampling strategies (holdout, 5-fold CV, 5-fold holdout, 5x5-fold CV) with and without reshuffling.  The x-axis represents the number of hyperparameter configuration evaluations, and the y-axis represents the negative ROC AUC (a lower value indicates better performance).  Shaded areas show the standard error of the mean performance across multiple replications. The figure illustrates how reshuffling can improve the final test performance, especially for holdout.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_8_1.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the trade-off between computational cost (number of model fits) and test performance for various resampling strategies.  It compares the standard and reshuffled versions of holdout, 5-fold CV, 5-fold holdout, and 5x5-fold CV. The results are averaged across multiple tasks, learning algorithms, and replications for different training/validation set sizes.  The shaded areas represent the standard errors, indicating the uncertainty in the measurements. The figure demonstrates that reshuffled holdout often achieves test performance comparable to the more computationally expensive 5-fold CV.", "section": "Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_8_2.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the trade-off between computational cost (number of model fits) and the final test performance achieved by different resampling strategies.  It compares standard and reshuffled versions of holdout, 5-fold CV, 5-fold holdout, and 5x5-fold CV across different dataset sizes.  The results indicate that reshuffled holdout can achieve performance comparable to more computationally expensive 5-fold CV methods.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_29_1.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the trade-off between computational cost (number of model fits) and performance (normalized AUC-ROC) for different resampling strategies in hyperparameter optimization using random search.  It compares the performance of holdout, 5-fold CV, 5-fold holdout, and 5x5-fold CV, both with and without reshuffling.  The results are averaged across multiple tasks, learning algorithms, and replications.  The plot reveals the impact of resampling and reshuffling on both computational cost and generalization performance for various training dataset sizes.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_30_1.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "The figure shows the mean true risk of the configuration that minimizes the observed objective in a simulation study, systematically varying curvature, correlation strength of noise, and the extent of reshuffling.  Lower curvature, lower correlation and more reshuffling generally leads to better true risk.  The error bars indicate the standard error of the mean.", "section": "3.2 Results"}, {"figure_path": "C4SInFLvuB/figures/figures_30_2.jpg", "caption": "Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "The figure shows how the test performance of the best hyperparameter configuration found so far (incumbent) changes during the hyperparameter optimization process using different resampling strategies. The x-axis represents the number of hyperparameter configurations evaluated, and the y-axis represents the test performance. Each colored line represents a different resampling method, and the shaded area represents the standard error.  The figure demonstrates the performance on dataset \"albert\" for different sizes of the training and validation sets.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_31_1.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "This figure displays the results of a simulation study investigating the effects of reshuffling on the true risk of the configuration minimizing the observed objective during hyperparameter optimization. The x-axis represents the reshuffling parameter \u03c4, ranging from 0 to 1 (1 being no reshuffling). The y-axis represents the mean true risk. Different lines and colors represent different combinations of curvature (m) and correlation (\u03ba). The results show that reshuffling can be beneficial for loss surfaces with low curvature when noise is not strongly correlated, confirming the theoretical insights.", "section": "3 Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_31_2.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "The figure shows the results of a simulation study conducted to test the theoretical understanding of the potential benefits of reshuffling resampling splits during HPO.  The mean true risk (lower is better) of the configuration that minimizes the observed objective function is plotted against the reshuffling parameter (\u03c4). The study systematically varied the curvature of the loss surface (m), the correlation strength of the noise (\u03ba), and the extent of reshuffling (\u03c4). The results show that, for a loss surface with low curvature, reshuffling is beneficial as long as the noise process is not too correlated.  As the noise process becomes more strongly correlated, reshuffling starts to hurt the optimization performance. When the loss surface has high curvature, reshuffling starts to hurt optimization performance when correlation in the noise is weaker.", "section": "Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_32_1.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the relationship between the number of model fits and the test performance for various resampling strategies with and without reshuffling. The results are averaged over different tasks, learning algorithms, and replications, and are shown separately for different train-validation set sizes. It illustrates the trade-off between computational cost and performance. Notably, the reshuffled holdout achieves a test performance close to that of the more expensive 5-fold CV.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_32_2.jpg", "caption": "Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure displays the average test performance, measured using negative ROC AUC, of the best-performing hyperparameter configuration found so far (the incumbent) during the hyperparameter optimization process.  The optimization was performed using XGBoost on the 'albert' dataset. The x-axis shows the number of hyperparameter configurations evaluated, and the y-axis represents the average test performance. Separate lines are shown for different train-validation set sizes (n): 500, 1000, and 5000.  Shaded areas indicate standard errors, providing a measure of the variability in the performance results. The figure illustrates how test performance changes over time and across different train/validation data sizes, and whether reshuffling the train-validation split affects the results.  It shows that the reshuffling does not have a strong impact for large train-validation data sizes.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_32_3.jpg", "caption": "Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows how the average test performance (measured by negative ROC AUC) of the best hyperparameter configuration found so far changes over the course of hyperparameter optimization (HPO) for the XGBoost algorithm on the 'albert' dataset.  The x-axis represents the number of hyperparameter configurations evaluated, and the y-axis represents the average test performance. The figure is broken down into three columns, each representing a different size of training data used in the HPO (n = 500, 1000, and 5000). Each column shows two lines, one for when resampling splits were reshuffled during HPO (TRUE) and one for when they were not (FALSE). The shaded regions represent the standard errors associated with the average test performance. The figure illustrates the relative impact of reshuffling and training data size on the test performance.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_33_1.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure displays a comparison of different resampling techniques (holdout, 5-fold CV, 5-fold holdout, 5x5-fold CV) in hyperparameter optimization, with and without reshuffling.  The x-axis represents the number of model fits required, reflecting computational cost.  The y-axis shows the average normalized test performance (AUC-ROC), indicating generalization ability. The figure demonstrates the trade-off between computational cost and performance, showcasing that reshuffling can often improve performance without a significant increase in cost, particularly for holdout.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_33_2.jpg", "caption": "Figure 11: HEBO and SMAC3 vs. random search for holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure compares the performance of three different hyperparameter optimization (HPO) algorithms: HEBO, SMAC3, and random search.  The algorithms are tested on a holdout validation scheme with different dataset sizes (n). The y-axis shows the average normalized validation performance (ROC AUC), and the x-axis represents the number of hyperparameter configuration evaluations. Shaded regions represent standard errors, indicating the variability of the results.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_33_3.jpg", "caption": "Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the test performance of XGBoost on the 'albert' dataset for different training and validation set sizes (n).  It compares the performance of models trained with different hyperparameter optimization (HPO) strategies: holdout, 5-fold cross-validation (CV), 5-fold holdout, and 5x5-fold CV.  Both standard and reshuffled versions of each strategy are evaluated.  The shaded areas indicate standard errors.  The key takeaway is that the reshuffled strategies often lead to more stable and comparable test performance across different training sizes.  There is an improvement in the reshuffled holdout test performance compared to the standard holdout, demonstrating the practical benefit of reshuffling.", "section": "Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_34_1.jpg", "caption": "Figure 4: Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the relative improvement in test ROC AUC performance when using different resampling techniques compared to the standard 5-fold cross-validation (CV).  The improvement is shown separately for different training/validation dataset sizes (n).  It illustrates how much better test performance is achieved using various resampling methods, including reshuffled versions of holdout, 5-fold CV, and 5-fold holdout, against the standard 5-fold CV.  Shaded regions represent the standard errors, showing the variability of the results.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_34_2.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the trade-off between computational cost (number of model fits) and performance (AUC-ROC).  It compares different resampling strategies (holdout, 5-fold CV, 5-fold holdout, 5x5-fold CV), both with and without reshuffling. The results are averaged across multiple tasks, learning algorithms, and replications, and are shown separately for different training data sizes (n).  The plot reveals that reshuffled holdout can achieve performance comparable to more computationally expensive methods like 5-fold CV.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_34_3.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "This figure shows the results of a simulation study designed to test the theoretical understanding of reshuffling's effects during hyperparameter optimization. The simulation uses a univariate quadratic loss surface with added noise, allowing systematic investigation of how curvature, noise correlation, and reshuffling affect optimization performance.  The results show that for loss surfaces with low curvature, reshuffling is beneficial as long as the noise is not highly correlated.  High curvature surfaces show that reshuffling hurts optimization performance. This simulation supports the theory that reshuffling is most beneficial when the loss surface is flat and the noise is not strongly correlated.", "section": "3 Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_35_1.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "This figure shows the results of a simulation study on the effect of reshuffling on hyperparameter optimization. The true risk (the generalization error of the best hyperparameter configuration found by the algorithm) is plotted against the reshuffling parameter (\u03c4).  The curvature of the loss surface (m) and the correlation strength of the noise (\u03ba) are also varied, demonstrating how different factors affect the usefulness of reshuffling. Lower values of true risk are better, indicating that reshuffling can significantly improve the performance of hyperparameter optimization, especially when the loss surface is relatively flat and the noise is weakly correlated.", "section": "3 Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_35_2.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the trade-off between the computational cost (number of model fits) and the test performance for various resampling strategies.  It compares the performance of holdout, 5-fold CV, 5-fold holdout, and 5x5-fold CV, both with and without reshuffling. The results are averaged across different tasks, learning algorithms, and replications, and are shown for different training/validation set sizes (n). Shaded regions represent standard errors, illustrating variability in the results. The key takeaway is that reshuffled holdout often achieves performance comparable to more expensive methods like 5-fold CV but with a significantly lower computational cost.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_35_3.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the trade-off between computational cost (number of model fits) and test performance for various resampling methods.  It compares standard and reshuffled versions of holdout, 5-fold CV, 5-fold holdout and 5x5-fold CV, across different training set sizes.  The results suggest that reshuffled holdout can achieve similar test performance to 5-fold CV but with significantly fewer model fits, highlighting its computational efficiency.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_36_1.jpg", "caption": "Figure 11: HEBO and SMAC3 vs. random search for holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure compares the performance of three hyperparameter optimization (HPO) algorithms: HEBO, SMAC3, and random search, when using a holdout validation strategy. The performance is measured using the area under the ROC curve (AUC) and is averaged across various tasks, learners, and replications. The results are shown separately for different training and validation dataset sizes (n). Shaded regions in the figure represent standard errors, indicating the uncertainty in the performance estimates.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_36_2.jpg", "caption": "Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the trade-off between computational cost (number of model fits) and test performance for various resampling strategies in hyperparameter optimization using random search.  It compares fixed and reshuffled versions of holdout, 5-fold CV, 5-fold holdout, and 5x5-fold CV across different training dataset sizes. The results indicate that reshuffled holdout often achieves test performance comparable to more computationally expensive methods like 5-fold CV.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_36_3.jpg", "caption": "Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the test performance (negative ROC AUC) over time, for the best model found so far (incumbent), trained on different training set sizes (500, 1000, 5000).  It compares the standard 5-fold cross-validation (CV) approach to holdout, 5-fold holdout, and 5x5-fold CV, both with and without reshuffling the data splits for each hyperparameter configuration. The shaded areas represent the standard errors, showing the variability in performance.", "section": "Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_37_1.jpg", "caption": "Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure displays the test performance of the best hyperparameter configuration found so far during hyperparameter optimization (HPO) over the number of HPO iterations.  The experiment uses the XGBoost algorithm on the 'albert' dataset, with varying training dataset sizes (n = 500, 1000, 5000).  The plot compares two different resampling strategies: standard 5-fold cross-validation (CV) and reshuffled 5-fold CV.  The shaded areas represent the standard error, indicating the variability of the results across multiple runs of the HPO process.  The results suggest that reshuffling the splits slightly improves performance, particularly noticeable at higher training set sizes.", "section": "4 Benchmark Experiments"}, {"figure_path": "C4SInFLvuB/figures/figures_38_1.jpg", "caption": "Figure 27: Random search. Average normalized validation performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the result of the random search algorithm. The y-axis represents the mean normalized validation performance for three different metrics: Accuracy, ROC AUC, and Logloss. The x-axis represents the number of hyperparameter configurations (HPCs) evaluated.  The figure is divided into three panels based on the training and validation data size (n).  Each panel further shows results for different resampling methods (Holdout, 1-fold, 2-fold, 3-fold, 4-fold, and 5-fold), with each resampling method shown with and without reshuffling. The shaded areas represent standard errors. This visual helps illustrate how reshuffling and varying the number of folds in resampling affect the validation performance in different settings.", "section": "G.2 Ablation on M-fold holdout"}, {"figure_path": "C4SInFLvuB/figures/figures_39_1.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "This figure displays the results of a simulation study exploring the impact of reshuffling on hyperparameter optimization performance. The mean true risk of the selected configuration is plotted against the reshuffling parameter (\u03c4), for various levels of loss surface curvature (m) and noise correlation (\u03ba).  Lower curvature and weaker correlation generally benefit from reshuffling, while the opposite is true for high curvature and strong correlation. Error bars represent the standard error of the mean.", "section": "3 Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_39_2.jpg", "caption": "Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength of the noise (a larger \u03ba implying weaker correlation), and extent of reshuffling \u03c4 (lower \u03c4 increasing reshuffling). A \u03c4 of 1 indicates no reshuffling. Error bars represent standard errors.", "description": "This figure shows the results of a simulation study designed to test the theoretical understanding of the potential benefits of reshuffling resampling splits during hyperparameter optimization.  The study used a univariate quadratic loss surface function with added noise, varying parameters controlling curvature (m), noise correlation (\u03ba), and the extent of reshuffling (\u03c4).  The plot shows the mean true risk (lower is better) of the configuration that minimizes the observed objective for different combinations of these parameters.  Lower values of \u03c4 indicate more reshuffling, and the results demonstrate how reshuffling can be beneficial in scenarios of low curvature and weak correlation.", "section": "3 Simulation Study"}, {"figure_path": "C4SInFLvuB/figures/figures_40_1.jpg", "caption": "Figure 29: Random search. Average improvement (compared to standard 5-fold holdout) with respect to test performance of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors.", "description": "This figure shows the results of the random search algorithm for different train-validation sizes (500, 1000, 5000). It compares the performance of different resampling strategies (holdout, 1-fold to 5-fold) both with and without reshuffling, against the standard 5-fold holdout strategy. The improvement in test performance (Accuracy, ROC AUC, and Logloss) is shown with respect to the standard 5-fold holdout. Shaded areas represent standard errors, illustrating the variability of the results.", "section": "G.2 Ablation on M-fold holdout"}]