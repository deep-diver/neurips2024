[{"type": "text", "text": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tianyue $\\mathbf{O}\\mathbf{u}^{\\mathbf{\\bullet}}$ Frank F. $\\mathbf{X}\\mathbf{u}^{\\star}$ Aman Madaan\u2662 Jiarui Liu\u2660 Robert Lo\u2660 Abishek Sridhar\u2660 Sudipta Sengupta\u2663 Dan Roth\u2663 Graham Neubig\u2660 Shuyan Zhou\u2660 ", "page_idx": 0}, {"type": "text", "text": "\u2660Carnegie Mellon University \u2663Amazon AWS AI \u2662xAI {tianyueo, fangzhex, gneubig, shuyanzh}@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (e.g., arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use $100k$ such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB $^{++}$ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only $3\\%$ the cost of human demonstrations (at $\\mathbb{S}0.031$ each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "AI agents that operate within a digital environment (e.g., a browser in a computer) intelligently to accomplish complex tasks (e.g., \u201cCreate my July online shopping expense report.\u201d) have the potential to improve the productivity across a broad swath of tasks performed by humans every day [2, 33, 6, 58]. However, agents still lack the ability to complete tasks with a high degree of reliability, partly due to a paucity of training data for such agentic tasks. Typically, supervised finetuning is a standard way to adapt large language models (LLMs) to tasks such as text generation or classification, large-scale demonstration collections for digital agents are not readily available. ", "page_idx": 0}, {"type": "text", "text": "For AI agents, demonstrations typically involve specifying a sequence of actions and observations that results in successful task completion, as shown on the right of Figure 1. Existing works that automatically collect demonstrations (1) set up environments for an agent to interact with, (2) run a baseline agent within this environment, and (3) employ scoring functions to remove low quality demonstrations [9] or perform relabeling [1, 26]. All three of these requirements limit applicability to a variety of practical applications. Setting up an environment that is representative of the actual environments in which we would like agents to act is a difficult task, and existing environments for digital agents are generally limited in scope to a few websites or digital apps [58, 7, 45]. Even within these constrained settings, strong LLMs such as GPT-4 struggle on tackling tasks [58], making collecting successful demonstrations with LLMs inefficient. In addition, human annotation is costly and its scope can still be limited [17, 6]. For example, gathering a demonstration for canceling a PayPal order requires an actual PayPal account with a legitimate subscription history. ", "page_idx": 0}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/4aee37905da6613aab11ce5696252b2a0e7e40c717fc935b115ec2352f76556e.jpg", "img_caption": ["Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we propose Synatra, a data generation approach that synthesizes high-quality trajectories for complex digital tasks at scale. This approach is based on the intuition that there is a rich trove of existing knowledge that encodes indirect supervision about how to perform digital tasks (\u00a72.2). One example of a piece of indirect knowledge is a tutorial that details the sequential breakdown of a complex web task, such as \u201chow to cancel a recurring payment on Paypal\u201d for human readers (Figure 1 upper left) [53, 57]. While this tutorial provides some procedural guidance, such as \u201cEnter the keyword,\u201d it does not specify the executable actions or the tangible observations associated with them. The key idea of Synatra is that, given this indirect knowledge, we can leverage an LLM to re-purpose it into more usable form that directly demonstrates the exact action to take given a concrete observation (\u00a73). In doing so, we leverage LLMs\u2019 ability to paraphrase and code, as well as its general knowledge of how tasks are performed (Figure 1). The main benefit of this approach is that it can scale with the availability of indirect knowledge created for humans, rather than rely on direct human annotations or LLM trajectories. ", "page_idx": 1}, {"type": "text", "text": "We carefully study the sources of indirect knowledge, the design of demonstration formats, and the mechanisms for iterative refinement in order to synthesize high-quality demonstrations using an LLM $(\\S3,\\,\\S4)$ . We generate demonstrations of $100k$ tasks from 21 domains, and finetune a 7b Codellama-instruct model with this synthetic data. The resulting agent, Synatra-CodeLlama, surpasses existing open-source models of similar size, excluding those finetuned with human annotations, on three web-based task benchmarks: Mind2Web, MiniWoB $^{++}$ , and WebArena. Moreover, it consistently outperforms models that are ten times larger and have been finetuned with interactive data (\u00a76.1). Our findings also indicate that our model is on-par or a more accurate option in browser copilot settings, in comparison to GPT-3.5. Importantly, while each synthetic example incurs only approximately $3\\%$ of the cost of a human-annotated demonstration, we demonstrate that synthetic data with good domain coverage can be more effective than an identical quantity of limited-domain human demonstrations in real-world web-based tasks in WebArena (\u00a76.2). ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Controlling Digital Agents through Natural Language ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "An agent interacts with a computer environment $\\mathcal{E}=\\langle\\mathcal{S},\\mathcal{A},\\mathcal{O},\\mathcal{T}\\rangle$ with state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , observation space $\\scriptscriptstyle\\mathcal{O}$ and the environment dynamics $\\mathcal T:\\mathcal S\\times\\mathcal A\\longrightarrow\\mathcal S$ . While this framework can be applied to different types of task, in this work, we consider a web browser as the unified entry point to access different web applications. We follow WebArena [58] to define the observation space as the contents on screen, represented as in text-based accessibility tree format. We use the same universal action space as in WebArena. This action space resembles the keyboard and mouse operations of a computer (e.g., click, type), and is applicable to arbitrary web applications. Appendix A lists all valid actions. The environment dynamics (e.g., the effect of clicking a button) and the states (e.g., the database status) are decided by the implementation of each web application. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Given the natural language intent $\\pmb{i}$ , at each time step $t$ , an agent issues an action $a_{t}\\in\\mathcal A$ based on $s_{t}$ . The environment state is updated to $s_{t+1}$ with new observation $o_{t+1}$ . This process ends when the agent predicts the stop action. We follow existing works [6, 47, 58, 54] to represent $s_{t}$ with current observation and all previous actions in the form of $(i,a_{1},...,a_{t-1},o_{t})$ . A benchmark (e.g., WebArena) supplies a scoring function $r(i,s_{n})$ that examines the final state and returns 1 if the desired goal state is satisfied, and 0 otherwise. ", "page_idx": 2}, {"type": "text", "text": "2.2 Definition of Direct Demonstrations and Indirect Knowledge ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the expected action $a_{t}$ given $s_{t}$ as a form of direct demonstration, i.e., $\\left({{s}_{t}},{{a}_{t}}\\right)$ . This allows an agent to directly learn how to predict the next action under a given state. On the other hand, indirect knowledge is broadly defined as resources that can benefit the task execution, but is not in the format of state and expected action tuple. We mainly focus on three types of indirect knowledge: ", "page_idx": 2}, {"type": "text", "text": "1. Procedural knowledge details the sequence of steps $\\langle a_{1}^{\\prime},a_{2}^{\\prime},...,a_{n}^{\\prime}\\rangle$ required to complete a specific task $\\pmb{i}$ . Unlike an action $a_{t}$ in trajectories, the steps in procedural knowledge are ungrounded, they lack a direct association with any particular observation and are not tied to specific action spaces. For instance, the tutorial in Figure 1 instructs the user to \u201clogin with your credentials\u201d without providing the concrete PayPal login page and the input fields to type. ", "page_idx": 2}, {"type": "text", "text": "2. Environment knowledge $\\tau$ that describe the effects of applying different actions in some hypothetical states. Example knowledge includes verbal descriptions such as \u201c... after clicking the cancel button, you will see a pop up window ...\u201d. ", "page_idx": 2}, {"type": "text", "text": "3. Ungrounded observations $o$ that are not associated with particular tasks or trajectories. In the context of web-based tasks, an observation could be any random web page with different content and status (e.g., a product page with a query in the search field). ", "page_idx": 2}, {"type": "text", "text": "3 Scalable Demonstration Synthesis for Digital Agents ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first introduce our design choices on the canonical formalization of trajectories, which account for the structural nature of procedures. Then, we delve into the sources for acquiring indirect knowledge, and the mechanisms for re-purposing this knowledge into direct supervision. ", "page_idx": 2}, {"type": "text", "text": "3.1 Trajectories as Programs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Existing works demonstrate that representing task-related procedures as programs is beneficial for several reasons. This includes benefits from the structural nature of programs compared to free-form text [56, 23], and the flexibility of using tools [4, 10, 41]. Inspired by these observations, we represent a Python function that interleaves natural language planning articulated in comments and actions as API calls, as shown on the right. The pink background represents the prompt, while the blue background corresponds to the model\u2019s response format. ", "page_idx": 2}, {"type": "text", "text": "The planning process includes both task-level planning, which breaks the task into multiple sub-tasks, and actionlevel planning, which explains the low-level goals of each executable action. The model\u2019s generation consists of chain-of-thought (CoT, [43, 47]) reasoning that analyzes the objective, previous actions, and current observations. Since CoT reasoning includes detailed information about the current step that may not be relevant for future steps, we further design the model response to include an action summary. This summary serves as a description of the predicted action that is added to the prompt for future action predictions. A concrete example can be found in Appendix B.2 We study the empirical effect of program formalization in $\\S6.3$ . ", "page_idx": 2}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/5e7d36892239ea919449778f48e153a1adc9f0883c6649356f09bdecb3c3ae83.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Synthesizing from Text Procedural Knowledge with Generative Environment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Internet offers fairly extensive procedural knowledge that describes how to perform high-level tasks by breaking down the task into detailed lower-level steps, such as how-tos and tutorials. ", "page_idx": 3}, {"type": "text", "text": "Source We use wikiHow3 as our main source for these tutorials due to its comprehensive coverage of diverse tasks as well as its consistent format. Each article consists of a high-level task description and step-by-step instructions. We performed a filtering step and only kept the articles that involve navigation through the graphical user interface (GUI) of a computer or a mobile phone. We prompted GPT-3.5-turbo with six examples mixing the target articles (e.g., How to redeem an Amazon gift card online4) and non-target articles (e.g., How to make a pizza) to perform the classification of all wikiHow articles. The prompt is shown in Appendix C. As a result, we obtained $25k$ articles that can be used to perform data synthesis. ", "page_idx": 3}, {"type": "text", "text": "Synthesis Approach We want to bridge two gaps to re-purpose $\\langle a_{1}^{\\prime},a_{2}^{\\prime},...,a_{n}^{\\prime}\\rangle$ for task $\\pmb{i}$ into $\\langle a_{1},...,a_{t-1},o_{t}\\rangle$ . When re-purposing a sequence of actions $\\langle a_{1}^{\\prime},a_{2}^{\\prime},...,a_{n}^{\\bar{\\prime}}\\rangle$ for task $\\pmb{i}$ into a new sequence $\\langle a_{1},...,a t-1,o_{t}\\rangle$ , many challenges arise. First, the action descriptions provided in tutorials are not constrained to specific action spaces. Instead, they are presented as free-form natural language (NL) expressions, which can lead to ambiguity. For instance, various verbs such as \u201center,\u201d \u201cinput,\u201d and others may all correspond to the same underlying action, type. Second, NL descriptions are often abstract, omitting concrete actions. For example, the process of \u201clogging in\u201d involves a series of actions, including typing in a username and password, but these specific actions may not be explicitly mentioned. Finally, the steps outlined in tutorials are ungrounded, meaning they are not directly associated with observable states or outcomes. Tutorials typically employ generic descriptions to accommodate various instances of conceptually similar tasks. For example, as illustrated in Figure 1, the tutorial merely instructs to \u201center the keyword\u201d without addressing any specific scenario. ", "page_idx": 3}, {"type": "text", "text": "Based on these findings, we propose an iterative approach that first uses an LLM to rewrite an article into a hypothetical trajectory in the format shown in $\\S3.1$ , then we leverage a generative model to synthesize the intermediate observation between two consecutive actions. First, in the rewriting step, we ask the assistant LM to perform: (1) propose a hypothetical concrete scenario relevant to the task (2) perform basic parsing such as translating \u201center the keyword [...]\u201d into type(\u201csearch bar\u201d, \u201cAmazon Prime\u201d); (3) categorize actions into groups that reflect the sub-task structures outlined by coding blocks. These tasks mainly demand a LLMs\u2019s creativity, language processing ability, and event understanding respectively. An example of rewriting a how-to article into a trajectory in program format is showed in Appendix E. The prompt for the rewriting step is in Appendix D. ", "page_idx": 3}, {"type": "text", "text": "Because the previous procedures still only result in a sequence of ungrounded actions, we next leverage the assistant LM to generate the observations between randomly sampled consecutive actions. We use the consecutive actions of type(\u201csearch bar\u201d, \u201cAmazon Prime\u201d) and click(\u201cAmazon Inc\u201d, $\\scriptstyle{\\dot{1}}{\\dot{\\mathsf{d}}}=156^{\\circ}$ ) in Figure 1 as an example. There are mainly two requirements for generated observations. First, the observation must reflect the outcomes of past actions. In the example, this corresponds to a page with a user logged in, and a search input field filled with \u201cAmazon Prime\u201d. Second, the observation encodes the necessary elements to perform the next action. In the example, this corresponds to a payment history list with a payment to Amazon. We prompt the assistant LM with the action sequence to generate a HTML snippet that fulfills the above requirements. Since the next action requires the concrete element to interact with, we ask the model to insert a tag of $\\mathrm{\\dot{1}}\\mathrm{d}{=}^{\\epsilon}$ \u201cnext-action-target-element\u201d in the corresponding HTML node to indicate the grounding.5 This step mainly requires a model\u2019s coding capabilities, particularly in web development. However, we find that it is not necessary for the LLM to generate HTML with high fidelity and complexity, which is a open research question [36]. The full prompt is in Appendix D and an example for this step is in Figure 5. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Note that in addition to wikiHow, there are other resources that share similar a similar procedural flavor, such as the captions of YouTube how-to videos [25]. Our transformation mechanism is generally applicable to such resources, but we leave concrete examination of them as future work. ", "page_idx": 4}, {"type": "text", "text": "3.3 Synthesizing from Random Observations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While the procedure in the previous section results in real procedures, the LLM-based method generates simplified observations. To compensate for this, we also perform data synthesis with real observations, and use synthesis to generate simplified procedures. We show that these two sources can compensate each other by examining the generated data in $\\S4$ and comparing the actual web-based task performance in $\\S6.3$ . ", "page_idx": 4}, {"type": "text", "text": "Source We utilize ClueWeb [27] as our data source, which comprises HTML snapshots of more than 10 billion web pages. Our initial analysis indicates that a random sampling approach would likely lead to a homogeneous distribution dominated by less interactive pages, such as news articles. In contrast, more complex web-based tasks typically require interactions with various web elements to advance the tasks. To diversify the sampled web pages, we employed a temperature sampling approach to select pages based on their content categories. In general, web pages from higher frequency top-level domains in ClueWeb are typically more interactive, such as Amazon and Reddit, while domains with lower frequency are less interactive, such as news article. We use a temperature sampling with $T=0.6$ so that the sample probability of choosing a page in domain $i$ , $P_{i}^{\\prime}=p_{i}^{\\frac{1}{T}}/\\sum_{k}p_{k}^{\\frac{1}{T}}$ , where $p_{i}$ is the original probability of choosing a page in domain $i$ , and $k$ is all the available domains. In doing so, we up-sample more interactive sites while maintaining diversity on more rare sites. More details are listed in Appendix J. ", "page_idx": 4}, {"type": "text", "text": "Synthesis Approach We treat each sampled web page as an intermediate observation at time step $t$ , aiming to synthesize the specific task $\\pmb{i}$ , the previous actions $a_{1},...,a_{t-1}$ and the subsequent action $a_{t}$ consisting of an action, a corresponding target element in the observation, and a natural language summary of the action. We first convert a web page into its corresponding accessibility tree at the beginning of each node, and sample a segment to present to the assistant LM. We follow the WebArena convention of assigning a unique ID to each node in the tree to ease the challenge of referencing the nodes. To increase the diversity of the tasks, we first instruct the model to brainstorm $k$ task categories relevant to the web domain. Then the model randomly selects three of these categories and develops them into concrete scenarios with past actions leading up to the current observation and the next action to take. The prompt is in Appendix F and an example generation is in Appendix G. ", "page_idx": 4}, {"type": "text", "text": "3.4 Data Filtering ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To ensure the quality of the training set, we apply a two-part filtering pipeline. In the first part, we ensure that data samples are both complete and coherent. For a sample to pass this filter, it must include all required components in the correct format. These components include: (1) an action that falls within the defined action space, (2) a valid and meaningful action target element in the corresponding web page, (3) NL texts that are well formed, e.g. without the use of \u201c...\u201d in the texts as an abbreviation by the generative model, (4) overall comprehensive generation without placeholders from the prompt (e.g., <brief step description>). ", "page_idx": 4}, {"type": "text", "text": "To further eliminate accurately formatted but unresponsive actions, we apply a second filtering step using next state prediction. Here, we use the LLM to predict the next state, $o_{t+1}$ , based on the current state $o_{t}$ and action $a_{t}$ in our synthesized data. If the model predicts that $o_{t+1}=o_{t}$ , the action is deemed to have no impact, and we filter out the action accordingly. ", "page_idx": 4}, {"type": "text", "text": "4 Data Statistics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We assess the distribution of history lengths, task objectives, and observations in the form of accessibility trees. The first statistic reflects general task complexity, as longer trajectories typically indicate ", "page_idx": 4}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/f0db7cec5f4217f26d51fae912d92dd345051d6216dc3851d2c2a5619f7a214d.jpg", "img_caption": ["Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings more complex tasks. The latter two measure the domain diversity of our synthetic data. We also present these statistics for the Mind2Web dataset as an example of human-collected trajectories. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "As shown on the right, the majority of our synthetic data consists of trajectories with a history length of fewer than eight steps, regardless of the source. Fewer trajectories have longer histories, with the longest exceeding 20 steps. Examples with longer histories represent more complex scenarios such as \u201cCreate a new YouTube ad campaign for the Summer Collection with a focus on the 25-34 age group interested in sports, fitness, fashion, and style\u201d. In addition, we analyze the distribution of generated intents by projecting their high", "page_idx": 5}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/f8ff247aa16043e9d70c14ead5c2290e2e9c1da6866e472c605e8254cc41a4f5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "dimensional vectors using the embedding model all-mpnet-base-v2 [30]. We visualize these embeddings with t-SNE [40]. As shown on the left of Figure 2, intents synthesized from random observations exhibit broader coverage compared to those from tutorials. This may be because humans often prefer to write tutorials for critical domains, while randomly sampled observations offer a more objective reflection of the diverse internet. Although our data synthesis process is entirely independent of Mind2Web, the generated intents show substantial coverage of Mind2Web tasks, which were created by human annotators from top-use websites. Finally, we apply the same embedding and visualization approach for accessibility trees and display the results on the right of Figure 2. The generated observations from tutorials overlap significantly with real web pages, both from random observations and those in Mind2Web. ", "page_idx": 5}, {"type": "text", "text": "Cost The average end-to-end cost per sample is $\\mathbb{S}0.031$ , out of which $\\mathbb{S}0.028$ is used to generate the actual sample, and $\\mathbb{S}0.003\\$ is used by our prediction-based filtering pipeline. These values represent the final average cost to generate one valid sample, including the additional cost for samples that were generated but later filtered out, which we distributes across all non-filtered samples. ", "page_idx": 5}, {"type": "text", "text": "5 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Agent training We finetune CodeLlama-7b [31] with 99, 920 web-navigation instruction examples, sourced from WikiHow articles and ClueWeb web pages 6. Training details can be found in Appendix K. ", "page_idx": 5}, {"type": "text", "text": "Evaluation tasks We select three evaluation tasks in the domain of web navigation. (1) the Mind2Web test set [6]. It consists of three categories, namely, cross-task, cross-website, and crossdomain\u2014ranked by their degree of deviation from the training data. Since our model is not trained on the Mind2Web training set, all categories are treated as held-out sets. Therefore, we report an overall step accuracy as the average across all examples. In addition, we simplify the two-stage setup used in Mind2Web, where the model first selects the top 50 candidate elements from the HTML before predicting the action on a selected element. Instead, we input in-viewport accessibility trees that include the ground-truth element. This approach is more challenging because it introduces more irrelevant elements, but it streamlines the prediction process by eliminating the need for an additional element selection model. (2) MiniWoB $^{++}$ [21]. It is an interactive benchmark with simplified web pages and low-level tasks (e.g., \u201cEnter BUL to the box\u201d). On average, completing a task in MiniWoB $^{++}$ requires fewer steps than that of WebArena. We tested on a text-only subset of tasks used in [51, 41]. To get the final score, we take the average score of five runs. (3) WebArena. In WebArena, agents are required to navigate in real-world web applications to accomplish high-levels web-based tasks (e.g., \u201cHow much I spent last month on grocery\u201d). Both MiniWoB $^{++}$ and WebArena offer outcome-based evaluations, using programmatic checkers to validate key features after execution and assess whether the task has been successfully completed. ", "page_idx": 5}, {"type": "table", "img_path": "KjNEzWRIqn/tmp/b13819db18c56a79e7c6e68449ddb599abf29c04cdc22bc6bf538e779aed16de.jpg", "table_caption": ["Table 1: Performance of various models in three web-based task benchmarks. We measure step accuracy $(\\%)$ for Mind2Web, and task success rate $(\\%)$ for $\\scriptstyle\\mathrm{MiniWoB++}$ and WebArena. The numbers of FireAct-7b is taken from [5]; AutoWebGLM-7b(S1) represents the model trained with only synthetic data in [19]. All other numbers are produced by our work. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Baselines We compare the performance of our model with 12 baseline models, including API-based models such as GPT-4, open-source instructed models like CodeLlama, and models finetuned with interactive data. For instance, Agent-LM [51] is finetuned on supervised data for tasks such as web navigation and interactive coding. Since most baseline models are not optimized for code generation, we follow Zhou et al. [58] and prompt the models to generate natural language responses instead of programs. The same prompt, providing general web navigation instructions without dataset-specific explanations, is used across three datasets. We show the prompt in Appendix H. Notably, we adopt the original prompts of AgentLM and CodeActAgent [41] when evaluating them on $\\mathbf{M}\\mathrm{iniWoB++}$ to be consistent with their original evaluations. We further remove the task-specific in-context examples to ensure a fair comparison with other models in zero-shot settings. ", "page_idx": 6}, {"type": "text", "text": "6 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 1 presents the performance of various models across three web-based task benchmarks. Overall, Synatra-CodeLlama achieves the best performance among models of comparable size. Notably, ", "page_idx": 6}, {"type": "table", "img_path": "KjNEzWRIqn/tmp/3c4d9ccc1712ea17eb5420f4ebe7bdd003c9b936ade0c57f41ddd3bfe829318d.jpg", "table_caption": ["Table 2: Error rates $(\\%)$ on five fine-grained metrics. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Synatra-CodeLlama significantly outperforms its base model CodeLlama-instruct-7b. While CodeLlama-instruct-7b fails to complete any tasks in WebArena, Synatra-CodeLlama successfully executes $6.28\\%$ of them. Furthermore, Synatra-CodeLlama elevates the performance of CodeLlama-instruct-7b from $6.62\\%$ to $15.85\\%$ in Mind2Web (a $139.42\\%$ relative improvement) and from $23.04\\%$ to $38.20\\%$ in $\\scriptstyle\\mathrm{MiniWoB++}$ . More encouragingly, Synatra-CodeLlama demonstrates superior performance on Mind2Web and WebArena compared to GPT-3.5. It also outperforms Lemur-chat-70b, which is finetuned with interactive data and is ten times larger, across all three benchmarks. The results suggest that our data synthesis approach is effective in helping the model predict the next action (as in Mind2Web) and performing simple tasks with a few steps (as in $\\mathrm{MiniWoB++}$ ). The synthesized data can also guide the model towards executing real-world complex tasks more accurately. ", "page_idx": 7}, {"type": "text", "text": "Synatra-CodeLlama surpassed the performance of all open-source model finetuned with interactive data. Among these models, AgentLM, CodeActAgent and AgentFlan include demonstrations to perform web-based tasks in their instruction finetuning dataset. However, we find that these models may not serve as capable agents to perform web-based tasks due to the special design choice encoded in the finetuned models. For instance, AgentLM and CodeActAgent use Regex expression to match interactive element on a web page and require carefully selected in-context examples to showcase which are the proper Regex expression for different examples. However, Regex expressions only work for simple web pages with a few elements as in $\\scriptstyle\\mathrm{MiniWoB++}$ , while it is prohibitive to do pattern matching in complex web pages as in Mind2Web and WebArena. As a result, when we experiment with the more generic action space which is suitable for all three benchmarks without in-context examples, we see these models have a significant performance degradation. On the other hand, Synatra-CodeLlama targets at the generic web-based tasks and does not encode any dataset artifacts during training. Even though all three benchmarks are completely held-out during data generation, Synatra-CodeLlama achieves consistent superior performance on all benchmarks. ", "page_idx": 7}, {"type": "text", "text": "6.2 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "What does the model learn from the synthetic data? Our analysis suggests that the baseline acquires essential knowledge at multiple levels from synthetic data. The error rates for different error types are shown in Table 2. At the low level, synthetic data helps the model emit valid actions. For example, while approximately $95\\%$ of actions predicted by the base model CodeLlama-instruct-7b involve interacting with non-existent elements on the web page, Synatra reduces this error to just $0.7\\%$ . This low error rate is closer to that of larger models such as Llama3-chat-70b and GPT-4, which are better at following complex instructions with multiple requirements. Additionally, the synthetic data improves the model\u2019s ability to understand web pages more accurately. To assess this, we measure the ratio of invalid actions, including both invalid click and type, where the predicted action attempts to interact with an element that is not clickable (e.g., a piece of text) or not typable (e.g., a button). We observe that models in the 7b\u20138b range have high error rates in interpreting basic web elements. For instance, the strong 8b model Llama3-chat-8b mistakenly click a non-clickable element over $15\\%$ of the time, while Synatra reduces this error to under $6\\%$ approaching the performance of GPT-4. Finally, at task completion level, Synatra shows a stronger ability to track progress accurately. Specifically, when filling out forms on web pages, Synatra-CodeLlama is $74\\%$ less likely than GPT-4-turbo to repeatedly input the same words into the same field, a common error in models. This indicates that our synthetic data enables the model to more precisely recognize completed actions and identify the remaining steps needed to achieve the goal. More qualitative study can be found in Appendix I. ", "page_idx": 7}, {"type": "text", "text": "Can Synatra be as effective as human annotations? We compare models trained with $9k$ human demonstrations (human only) from Mind2Web [6] and $9k$ demonstrations generated by Synatra. The results are shown in Figure 3. Simply training CodeLlama with the Mind2Web human annotations provides modest improvement of $3.96\\%$ on $\\scriptstyle\\mathrm{MiniWoB++}$ , while failing entirely on WebArena. In contrast, Synatra led to a substantial im", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "provement of $17.81\\%$ on $\\scriptstyle\\mathrm{MiniWoB++}$ and $4.56\\%$ on WebArena. The limited performance of the human annotations can be partially attributed to their restricted task coverage: notably, Mind2Web lacks information seeking tasks that require a string answer. Furthermore, the human trajectories did not specify conditions for terminating task execution. Despite adding such trajectories from Synatra (human $^+$ synthesis), the model still under-performed. ", "page_idx": 8}, {"type": "text", "text": "We hypothesize the diversity of tasks plays a role in this discrepancy, since many tasks cannot be covered without pre-set environment (e.g., return an order) (Figure 2). These findings underscore the efficacy of Synatra, which also ", "page_idx": 8}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/7cb12321acb81d2ab9ee3b9ae08b21d893f924158fa3093372606952d5caf850.jpg", "img_caption": ["Figure 3: the comparison between the models trained with trajectories generated by our approach and the data collected from human. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "exempts from the complexities of developing recording tools and managing communication with annotators. However, it is important to recognize that the quality of human demonstrations is presumably higher, but they require meticulous design and control during the data collection process. ", "page_idx": 8}, {"type": "text", "text": "6.3 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we perform an ablation to validate the design choices of our data synthesis approach. ", "page_idx": 8}, {"type": "text", "text": "Model performance improves with more synthetic data To assess the impact of scaling our synthetic data, we trained three CodeLlama 7B models with $18k,50k$ , and $100k$ samples respectively, while keeping all other parameters constant. We then evaluated the models on WebArena. As shown on the right, the success rates on WebArena steadily increase as the synthetic training data scales from $18k$ to $100k$ samples. This highlights the potential of scaling synthetic data with our approach. ", "page_idx": 8}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/ec4b9168c477eac68b3b1d73c7584e8ef242c31dc2106220b279d01fdc72495d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Representing trajectories as programs is beneficial To verify if the programs format is helpful, we convert $30k$ trajectories to the NL format similar to setting in WebArena [58] and compare its performance with model trained with the exact data, but in our program format. The results are shown in Figure 4a. We can see that performance drops on both $\\mathbf{MiniWoB++}$ and WebArena when using the NL format. We hypothesize that program, in the form of API calls, is potentially a more natural format to represent sequence of actions. Our observation also echo the observations of using program representation for non-programming tasks [23, 29, 41], while our experiments further contributes insights towards finetuning setups for interactive tasks. ", "page_idx": 8}, {"type": "text", "text": "Different sources of indirect knowledge complement each other Our indirect knowledge comes from two sources: tutorials and randomly sampled web pages. In the former source, the procedural knowledge from tutorials are written and verified by human. However, there is no guarantee on the authenticity of the generated observations of web pages. In contrast, the observations from the latter sources are completely real, while there is no guarantee of the trajectory accuracy. We hypothesize that the two sources can compensate each other. To test this hypothesis, we trained three models: one using $9k$ synthetic data mixed from both sources, and two others each using $9k$ data exclusively from one of the sources and the results are shown at Figure 4b. We observe a noticeable performance degradation when models are trained with data from only one source. This indicates that utilizing multiple sources yields a more comprehensive dataset by integrating the precise procedural knowledge from tutorials with the realistic observations of web snapshot data. ", "page_idx": 8}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/7d22cb6aba9873293acd5f8400231d8c86041f6725f394628104d3a7df8c406b.jpg", "img_caption": ["Figure 4: ablation on trajectory formats, sources of knowledge, and ways to use knowledge. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Models have difficulty learning from indirect sources alone To access if turning knowledge into trajectories is helpful, we tested a retrieval-augmented approach that directly uses the collection of tutorials as the knowledge base. We first projected text tutorials to embeddings with all-mpnet-base-v2 [30]. Then a retriever retrieves the most relevant three tutorials measured by cosine similarity. These tutorials were included as additional context in the prompt. We tested this approach with LLama3.1-instruct-8B. As shown in Figure 4c, feeding in tutorials directly improves model performance on WebArena marginally , and finetuning on our data shows a substantial advantage. This comparison indicates that models have difficulty making use of indirect knowledge to solve agent tasks, and trajectories is a preferable format for agent learning. ", "page_idx": 9}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Learning from Indirect Supervision Due to the costly nature of human supervision, many digital agent learning works explore learning from existing yet indirect knowledge sources [8, 35, 12, 55], reinforcement learning optimization that learn from environment feedback [37, 21, 32, 39]. More recently, LLM self-improvement during inference time [34, 49, 28, 44, 14, 52] has been applied on creating more complex digital agents in the wild. Our work fills the gap of training with synthetic data generated from existing resources, i.e., indirect supervision, on complex web navigation tasks. ", "page_idx": 9}, {"type": "text", "text": "Prompting Approaches for AI Agents Existing methods include performing reasoning about the current statues before proceeding to next actions [43, 47, 22], search and planning [48, 13], and self-verification [18, 34, 24]. Our focus on instruction tuning data generation without human supervision can hopefully enable synthetic data generation recipes for various kinds of instruction prompts for agent tasks. ", "page_idx": 9}, {"type": "text", "text": "Data Generation for Interactive Agents and Instruction Finetuning Existing works design ways of generating training data that adapt LLMs to agent-specific tasks [11, 19], while our work aim to generate more realistic data without human intervention. Considering more broadly over all instruction finetuning: [59, 50, 38, 16] generate instructions of a specific task given a few examples. [42, 15] generates task-agnostic large-scale instruction tuning data without given examples. Li et al. [20] adopts an iterative instruction data generation and model finetuning approach, While our work generates instruction tuning datasets from readily available, indirect, unstructured knowledge resources. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a data synthesis approach Synatra. Empirically we showcase that finetuning with data generated from our approach can improve existing general-purpose LLMs to perform better on digital agent tasks. Since Synatra can synthesize trajectories given a single, static piece of indirect knowledge, we argue that when equipped with a capable LLM, a regular tutorial designed for human consumption and a random observation, can also be re-purposed a trajectory. We show that even considering the relative high cost of calling state-of-the-art LLMs such as GPT-4, synthesizing is more cost-effective than collecting human demonstrations of similar quantity for model training. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program, and a grant from Amazon Web Services. We would also like to thank Center of AI Safety (CAIS) and CMU FLAME center for compute support. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.   \n[2] S. Branavan, H. Chen, L. Zettlemoyer, and R. Barzilay. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82\u201390, Suntec, Singapore, 2009. Association for Computational Linguistics. URL https://aclanthology.org/P09-1010.   \n[3] B. Chen, C. Shu, E. Shareghi, N. Collier, K. Narasimhan, and S. Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023.   \n[4] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.   \n[5] Z. Chen, K. Liu, Q. Wang, W. Zhang, J. Liu, D. Lin, K. Chen, and F. Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024.   \n[6] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2web: Towards a generalist agent for the web, 2023.   \n[7] A. Drouin, M. Gasse, M. Caccia, I. H. Laradji, M. D. Verme, T. Marty, L. Boisvert, M. Thakkar, Q. Cappart, D. Vazquez, N. Chapados, and A. Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024.   \n[8] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=rc8o_j8I8PX.   \n[9] H. Furuta, O. Nachum, K.-H. Lee, Y. Matsuo, S. S. Gu, and I. Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023.   \n[10] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Programaided language models. In International Conference on Machine Learning, pages 10764\u201310799. PMLR, 2023.   \n[11] I. Gur, H. Furuta, A. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.   \n[12] J. Han, L. Yang, D. Zhang, X. Chang, and X. Liang. Reinforcement cutting-agent learning for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9080\u20139089, 2018.   \n[13] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   \n[14] H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, and D. Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024.   \n[15] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.   \n[16] O. Honovich, U. Shaham, S. R. Bowman, and O. Levy. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022.   \n[17] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson, P. Georgiev, A. Santoro, and T. Lillicrap. A data-driven approach for learning to control computers. In International Conference on Machine Learning, pages 9466\u20139482. PMLR, 2022.   \n[18] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. ArXiv preprint, abs/2303.17491, 2023. URL https://arxiv.org/abs/2303.17491.   \n[19] H. Lai, X. Liu, I. L. Iong, S. Yao, Y. Chen, P. Shen, H. Yu, H. Zhang, X. Zhang, Y. Dong, et al. Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent. arXiv preprint arXiv:2404.03648, 2024.   \n[20] X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis. Selfalignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023.   \n[21] E. Z. Liu, K. Guu, P. Pasupat, T. Shi, and P. Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id $=$ ryTp3f-0-.   \n[22] C.-f. Lo, A. Sridhar, H. Zhu, F. F. Xu, and S. Zhou. Hierarchical prompting assists large language model on web navigation. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.   \n[23] A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neubig. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384\u20131403, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.90.   \n[24] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.   \n[25] A. Miech, D. Zhukov, J. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 2630\u20132640. IEEE, 2019. doi: 10.1109/ICCV.2019.00272. URL https://doi.org/10.1109/ICCV.2019.00272.   \n[26] S. Murty, C. Manning, P. Shaw, M. Joshi, and K. Lee. Bagel: Bootstrapping agents by guiding exploration with language. arXiv preprint arXiv:2403.08140, 2024.   \n[27] A. Overwijk, C. Xiong, X. Liu, C. VandenBerg, and J. Callan. Clueweb22: 10 billion web documents with visual and semantic information. arXiv preprint arXiv:2211.15848, 2022.   \n[28] J. Pan, Y. Zhang, N. Tomlin, Y. Zhou, S. Levine, and A. Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.   \n[29] H. Puerto, M. Tutek, S. Aditya, X. Zhu, and I. Gurevych. Code prompting elicits conditional reasoning abilities in text+ code llms. arXiv preprint arXiv:2401.10065, 2024.   \n[30] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908. 10084.   \n[31] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \n[32] P. Shaw, M. Joshi, J. Cohan, J. Berant, P. Pasupat, H. Hu, U. Khandelwal, K. Lee, and K. N. Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. Advances in Neural Information Processing Systems, 36, 2024.   \n[33] T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of bits: An open-domain platform for web-based agents. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3135\u20133144. PMLR, 2017. URL http://proceedings.mlr.press/v70/shi17a.html.   \n[34] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. ArXiv preprint, abs/2303.11366, 2023. URL https://arxiv.org/abs/ 2303.11366.   \n[35] M. Shridhar, X. Yuan, M. C\u00f4t\u00e9, Y. Bisk, A. Trischler, and M. J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id $\\equiv$ 0IOX0YcCdTn.   \n[36] C. Si, Y. Zhang, Z. Yang, R. Liu, and D. Yang. Design2code: How far are we from automating front-end engineering? arXiv preprint arXiv:2403.03163, 2024.   \n[37] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n[38] C. Singh, J. X. Morris, J. Aneja, A. Rush, and J. Gao. Explaining data patterns in natural language with language models. In Y. Belinkov, S. Hao, J. Jumelet, N. Kim, A. McCarthy, and H. Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 31\u201355, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.3. URL https://aclanthology.org/2023.blackboxnlp-1.3.   \n[39] Y. Song, D. Yin, X. Yue, J. Huang, S. Li, and B. Y. Lin. Trial and error: Exploration-based trajectory optimization for llm agents. arXiv preprint arXiv:2403.02502, 2024.   \n[40] L. van der Maaten and G. E. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9:2579\u20132605, 2008. URL https://api.semanticscholar.org/CorpusID: 5855042.   \n[41] X. Wang, Y. Chen, L. Yuan, Y. Zhang, Y. Li, H. Peng, and H. Ji. Executable code actions elicit better llm agents, 2024.   \n[42] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022.   \n[43] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[44] Z. Wu, C. Han, Z. Ding, Z. Weng, Z. Liu, S. Yao, T. Yu, and L. Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024.   \n[45] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, Y. Liu, Y. Xu, S. Zhou, S. Savarese, C. Xiong, V. Zhong, and T. Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024.   \n[46] Y. Xu, H. Su, C. Xing, B. Mi, Q. Liu, W. Shi, B. Hui, F. Zhou, Y. Liu, T. Xie, et al. Lemur: Harmonizing natural language and code for language agents. arXiv preprint arXiv:2310.06830, 2023.   \n[47] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. ArXiv preprint, abs/2210.03629, 2022. URL https: //arxiv.org/abs/2210.03629.   \n[48] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Grifftihs, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601, 2023. URL https://arxiv.org/abs/2305.10601.   \n[49] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[50] S. Ye, D. Kim, J. Jang, J. Shin, and M. Seo. Guess the instruction! flipped learning makes language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969, 2022.   \n[51] A. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y. Dong, and J. Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.   \n[52] C. Zhang, Z. Yang, J. Liu, Y. Han, X. Chen, Z. Huang, B. Fu, and G. Yu. Appagent: Multimodal agents as smartphone users, 2023.   \n[53] L. Zhang, Q. Lyu, and C. Callison-Burch. Reasoning about goals, steps, and temporal ordering with wikihow. arXiv preprint arXiv:2009.07690, 2020.   \n[54] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024.   \n[55] S. Zhou, U. Alon, F. F. Xu, Z. Wang, Z. Jiang, and G. Neubig. Docprompting: Generating code by retrieving the docs. ArXiv preprint, abs/2207.05987, 2022. URL https://arxiv.org/ abs/2207.05987.   \n[56] S. Zhou, P. Yin, and G. Neubig. Hierarchical control of situated agents through natural language. In Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI), pages 67\u201384, Seattle, USA, 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.suki-1.8. URL https://aclanthology.org/2022.suki-1.8.   \n[57] S. Zhou, L. Zhang, Y. Yang, Q. Lyu, P. Yin, C. Callison-Burch, and G. Neubig. Show me more details: Discovering hierarchies of procedures from semi-structured web data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2998\u20133012, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.214. URL https://aclanthology.org/2022.acl-long. 214.   \n[58] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and G. Neubig. Webarena: A realistic web environment for building autonomous agents. In International Conference on Learning Representations (ICLR), Vienna, Austria, May 2024. URL https://arxiv.org/abs/2307.13854.   \n[59] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Table of Contents in Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Action Space 16   \nB Trajectory Representation 16   \nC Prompt to Filter wikiHow Articles 16   \nD Prompt to Generate Demonstrations from Tutorials 17   \nE Example Synthetic Demonstration from Tutorials from a wikiHow Article 19   \nF Prompt to Generate Direct Demonstrations from Random Observations 20   \nG Example Generated Trajectories from Random Observation 22   \nH Prompt Example 25   \nI Case Study 26   \nJ Data Selection from Random Observations in ClueWeb 28   \nK Training Settings 28   \nL Broader Impacts 29   \nM Limitations 29 ", "page_idx": 14}, {"type": "text", "text": "A Action Space ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "KjNEzWRIqn/tmp/6742b29551ec69add584ab3c808d04e8ff74e895bf3229c34850a43230511a40.jpg", "table_caption": ["Table 3: Action Space of WebArena "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Trajectory Representation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "website = \"<url>\"   \nobservation $=$ \"<AXtree of the page>\"   \nobjective $=$ \"Find and review the estimated value of your property on the website.\"   \n# past actions   \ndef solve(): # sub-task 1: Look up your property on Zillow # step 1: Search for your address on Zillow\u2019s homepage search bar to open the property page. type(element_i $\\mathsf{\\Omega}^{11}6135^{\\mathsf{m}}$ , string=\"Main St\") # step 2: From the property details page, navigate to the \"Zestimate\" section. scroll(down) # sub-task 2: Begin adjusting the estimated value # step 3: Click on 'Edit home facts' to adjust details that might affect the home's estimated value. click(element_id=\"9945\") ", "page_idx": 15}, {"type": "text", "text": "C Prompt to Filter wikiHow Articles ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/2ff0fc28373356ae42f48bb57a347c0832d8cf36d153676ff878669391deb79d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "input: How to delete a file using command line in Linux   \noutput: Command line interface (CLI) in Linux is a text-based interface not a graphical user interface (GUI),   \nso the answer is \"No\"   \ninput: How to Reboot an iPad (Frozen iPads)   \noutput: Rebooting an iPad usually involves physical actions like pressing and holding buttons on the iPad, so   \nthe answer is \"No\" ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "input: How to Connect the Kindle Fire to the Internet (Connecting to an Existing Wi-Fi Network) output: Kindle is neither a computer nor a mobile phone, so the answer is \"No\" ", "page_idx": 16}, {"type": "text", "text": "input: How to Pair AirPods to an iPhone (If Your AirPods Won\u2019t Connect) output: Pairing AirPods with an iPhone typically includes physical actions such as opening the AirPods case near the iPhone and possibly pressing a button on the AirPods case, so the answer is \"No\" ", "page_idx": 16}, {"type": "text", "text": "input: {{Title of the article}} output: {{Model prediction}} ", "page_idx": 16}, {"type": "text", "text": "D Prompt to Generate Demonstrations from Tutorials ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Prompt to rewrite an article to a trajectory in program format ", "page_idx": 16}, {"type": "text", "text": "# Task overview ", "page_idx": 16}, {"type": "text", "text": "You are given an article about performing a task in a web browser. Your goal is to make this article as accessible as possible to a user who is not familiar with the functionalities of the websites and the task at all. ", "page_idx": 16}, {"type": "text", "text": "# Guideline Read the article carefully and follow the instructions below: ", "page_idx": 16}, {"type": "text", "text": "- Assume you start with the home page of the web application, skip the initial \u2018goto\u2018 action.   \n- Break down the article into a sequence of steps. ", "page_idx": 16}, {"type": "text", "text": "- In every step, provide a concrete example that reflects a real execution. This example should clearly describe the element you are interacting with, the concrete value of an element you select, the precise content you type and other details. Never use broad descriptions. The example should be creative and realistic, avoid boilerplate text such as email $@$ example.com. Make sure that the example is consistent across steps. ", "page_idx": 16}, {"type": "text", "text": "- Following the concrete example, provide the Python API call corresponding to the example. ", "page_idx": 16}, {"type": "text", "text": "Group all API calls into multiple sub-sections, each section corresponds to a logical and actionable sub-task. ", "page_idx": 16}, {"type": "text", "text": "There are special scenarios and here are the ways to deal with them: - If the article describes multiple scenarios or multiple ways to approach the same goal, you can use your own judgement to choose the most common one to describe. - If there are repeated steps, make sure to unroll the steps and describe each of them canonically. - Always assume you perform this task using a web browser, if the original article uses a desktop app or mobile phone app, simply assume the corresponding web app exists. Hence, any steps regarding installation or login can be skipped. ", "page_idx": 16}, {"type": "text", "text": "# APIs   \nThe APIs are as follows: \u2018click(element_desc: str)\u2018 - Click on an element.   \n\u2018element_desc\u2018 is the the displayed text or the most representative attribute of the HTML element. \u2018hover(element_desc: str)\u2018 - Hover over an element. ", "page_idx": 16}, {"type": "text", "text": "\u2018click_and_type(element_desc: str, content: str)\u2018 - Click an input element and type \u2018content\u2018 into it. \u2018key_press(key_comb: str)\u2018 - Press a key combination. \u2018key_comb\u2018 is the combination of keys you want to press on. The default OS is MacOS if there is no explicit specification in the article. \u2018goto(url: str)\u2018 - Navigate to \u2018url\u2018 ", "page_idx": 16}, {"type": "text", "text": "\u2018go_forward()\u2018 - Go forward to the next page. ", "page_idx": 16}, {"type": "text", "text": "\u2018new_tab()\u2018 - Open a new tab.   \n\u2018close_tab()\u2018 - Close the current tab.   \n\u2018switch_tab(tab_index: int)\u2018 - Switch to the tab with index \u2018tab_index\u2018, starting from 0. # Response format   \nYour response should follow the following format.   \n\u201c\u2018python   \nsub-task <index>: <sub-task description>   \n# step <index>: <the real execution with concrete values for each argument> <API, do not skip the keys in the API calls> ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "# step <index>: <the real execution with concrete values for each argument> <API, do not skip the keys in the API calls> ", "page_idx": 17}, {"type": "text", "text": "<repeat for all sub tasks> ", "page_idx": 17}, {"type": "text", "text": "# task: <task command given to a smart assistant, only the necessary details on expectation are needed.> ", "page_idx": 17}, {"type": "text", "text": "# Article {{Article here}} ", "page_idx": 17}, {"type": "text", "text": "Prompt to generate observation for two consecutive actions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "# HTML Background Knowledge   \nCommonly used interactable elements in HTML:   \n[\u2019a\u2019, \u2019button\u2019, \u2019input\u2019, \u2019textarea\u2019, \u2019select\u2019, \u2019option\u2019, \u2019label\u2019, \u2019form\u2019, \u2019details\u2019, \u2019summary\u2019, \u2019map\u2019, \u2019area\u2019,   \n\u2019iframe\u2019, \u2019embed\u2019, \u2019object\u2019, \u2019dialog\u2019, \u2019menu\u2019, \u2019fieldset\u2019, \u2019legend\u2019, \u2019datalist\u2019, \u2019output\u2019, \u2019progress\u2019, \u2019meter\u2019,   \n\u2019keygen\u2019] # Task Overview   \nYou are given:   \n- A browser-based task   \n- A seuqnece of past actions to perform the task and - The next action to perform the task. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Your goal is to recover the HTML and the dynamic of a web application with the following requirements: - The web page embodies a same level of content richness as advanced web applications on the internet. That is, the web page should have around 80 elements and at least 20 interactable elements. The depth of the DOM tree should be around 7. The length is at least 3000 tokens. ", "page_idx": 17}, {"type": "text", "text": "- Analyze the past actions and determine which of these actions have visible or functional impacts on the web page you design. Reflect the effects of these past actions in your HTML code. This may involve updating text, adding new elements, or modifying the layout or styles to represent the state of the web page after these actions. ", "page_idx": 17}, {"type": "text", "text": "- Design the interactable element that enables the next action. Make sure the choice of element type, attributes, and other essential characteristics are correct. For example, a text field is not interactable. Once the element is designed, assign the attribute id=\"next-action-target-element\" to this interactable element. - Please focus on making the static HTML visually rich. Ignore CSS animations & style and JavaScript functionality in your HTML code. - Provide the concrete reason to perform the next action. ", "page_idx": 17}, {"type": "text", "text": "# Response format   \n\u201c\u2018html   \n<HTML that fullfils the requirements, make sure \u2018next-action-target-element\u2018 is always included> ", "page_idx": 17}, {"type": "text", "text": "<Summarize the progress by analyzing past actions. Provide a brief reason for performing the next action. Keep it short. Use imperative sentences.> ", "page_idx": 17}, {"type": "text", "text": "E Example Synthetic Demonstration from Tutorials from a wikiHow Article ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A wikiHow article ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "How to Use Google Chat on iPhone (Enabling Google Chat in Gmail)   \nOpen Gmail on your iPhone or iPad. This app icon looks like a white and red envelope. If you don\u2019t have it, you can download the Gmail app for free from the App Store.   \nTap and tap Settings. The three-line menu icon is in the top left corner of your screen, and the option is generally at the bottom of the menu next to a gear icon. If you have multiple Gmail accounts, select the one you want to use Google Chats with. You can repeat these steps if you want to use Google Chats with multiple Gmail accounts.   \nTap to check the box next to \"Chat. A checked box indicates that Google Chat is active in your Gmail account and you\u2019ll see the Chat and Spaces tabs near the bottom of your screen. If Google Chat is enabled, you can tap these tabs to move between conversations in Chat and your emails in Gmail. If this is turned off, you\u2019ll need to use the Google Chats app to see your conversations in Chat. ", "page_idx": 18}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/5a0761c04b8910b81885a3d851361ba61b7f24406791a0b0b104657b3737273d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/a014db1eb507c1e13b087f30b530958fbc3dee08b5c9fb57c60595d64eb0d791.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: The rendered generated HTML between step 2 and step 3, where step 2 is goto(\u201cGoogle Chat\u201d) and step 3 is click(\u201cjane.doe@gmail.com\u201d). The concrete element to interact with is tagged with id=\"next-action-target-element\". ", "page_idx": 19}, {"type": "text", "text": "F Prompt to Generate Direct Demonstrations from Random Observations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Prompt to Generate Direct Demonstrations from Random Observations ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/d05328985234e1a3cc529a8f07cbd1244c50320dbb12bc26bd3ab933588b8018.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "## Guidelines   \nYou will follow the guidelines below to perform the task:   \n1. Examine the web page to understand the the domain of the web page.   \n2. Brainstorm 8 task categories that could be performed the website. Be creative.   \n3. For each task category, propose a concrete task that has this web page as one of its steps. You want the concrete task to be unambiguous and clear so that no further clarification is needed to perform the task. ", "page_idx": 20}, {"type": "text", "text": "4. Given a concrete task, you are ask to come up with the past actions that leads to the current page, as well as the next action. ", "page_idx": 20}, {"type": "text", "text": "\\* Requirement for past actions: You should write down each past action in the details. You want to group all actions into multiple sub-sections, each section corresponds to a logical and actionable sub-task. The next action could start with a new sub-task. You can omit the \u2018elemement_id\u2018 if they are not in the current page. There should only be one action at each step. DO NOT give goto() or new_tab() as first step. ", "page_idx": 20}, {"type": "text", "text": "\\* Requirement for next action: Provide the reasoning behind your past actions and the progress in completing the task. Also, describe your understanding of the current page and the concrete reason to execute the next action. If the action takes an element as the argument, it is important that you understand the role and the attributes of that element so that the action can be appropriately applied. Make sure to always include the \u2018element_id\u2018 in your next action if there is any. Any \u2018element_id\u2018 must come from the given Accessibility Tree. ", "page_idx": 20}, {"type": "text", "text": "## Format of the response   \nYou are asked to provide the action sequence for task #1 with roughly 7 past actions; task #2 with roughly 0 past actions; task $\\#4$ with roughly 6 past actions; task #5 with roughly 4 past actions; task #6 with roughly 10 past actions. Your answer should follow the following format: ", "page_idx": 20}, {"type": "text", "text": "<Analysis and understanding about the domain and the concrete content of the web page> <The list of 8 creative task categories> <The concrete tasks for task category #1 #2 #4 #5 #6. Remember, a concrete task needs to include concrete details so that no further clarification is required when performing the task. Use imperative sentences.> ", "page_idx": 20}, {"type": "text", "text": "\u201c\u2018python # task: <repeat concrete task #1> ", "page_idx": 20}, {"type": "text", "text": "# # past actions (history) # sub-task 1: <sub-task description> # step 1: <step description> <action> # step 2: <step description> <action> # sub-task 2: <sub-task description> # step 3: <step description> <action> # step 4: <step description> <action> # sub-task 3: <sub-task description> # step 5: <step description> <action> # step 6: <step description> <action> # sub-task 4: <sub-task description> # step 7: <step description> <action> ", "page_idx": 20}, {"type": "text", "text": "# next action   \n# step <index>: <summarize the progress so far and analyze the current state of the web page. Provide the   \nconcrete reason to perform the next action>   \n<action>   \n# step summary: <brief step description>   \n\u201c\u2018   \n\u201c\u2018python   \n# task: <repeat concrete task #2> ", "page_idx": 20}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/35365083460a4bab78a7df0fc6ef0c53a80d1ab1c2fdf752e13e7eec9d45b05f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "G Example Generated Trajectories from Random Observation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Generated trajectories from random observation ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/51f9414ad183547b784a38006f6c88432fdf07719414245f5a6a002577fe64be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Concrete Task #1: Product Searching ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Find and search for popular fitness trackers within the \"Sports & Outdoors\" category. ", "page_idx": 21}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/ee065e5dd424a995da874cd3bfca02c327a752a44be2199164bfdcda3fdac687.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Concrete Task #2: Account Management ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Sign into your personal account to check on active subscriptions. ", "page_idx": 22}, {"type": "text", "text": "# t a s k : Sign i n t o your p e r s o n a l account to check on a c t i v e s u b s c r i p t i o n s .   \n#   \n# p a s t a c t i o n s ( h i s t o r y )   \n#   \n# sub \u2212t a s k 1: Sign i n t o the account .   \n# next a c t i o n   \n# s t e p 1: The user i s a l r e a d y on a page where they can i n i t i a t e the sign \u2212in process .   \nc l i c k _ a n d _ t y p e ( eleme $\\mathrm{nt}\\mathrm{=}\\mathrm{~}$ Search ' , c o n t e n $\\mathrm{t}=\\mathrm{:}$ myusername@example . com ' element_id $=7657$ )   \n# s t e p summary : Try to sign in by using the se arc h bar to e n t e r the username . ", "page_idx": 22}, {"type": "text", "text": "Concrete Task #6: Media Consumption ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Navigate to Prime Video to browse the latest movie releases. ", "page_idx": 22}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/2a7cd82ed5696da299fb9452a996053c51395c7814089395bf684cef638e2931.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/400ed814b80b0a9cba6b22e4812d821926557250acdd012ace0bb5cebcf3b9b5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "H Prompt Example ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Example prompt used in baseline ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "System ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "You are an autonomous intelligent agent tasked with navigating a web browser.   \nYou will be given web-based tasks.   \nThese tasks will be accomplished through the use of specific actions you can issue.   \nHere\u2019s the information you\u2019ll have:   \nThe user\u2019s objective: This is the task you\u2019re trying to complete.   \nThe current web page\u2019s accessibility tree: This is a simplified representation of the webpage, providing key information.   \nThe current web page\u2019s website: This is the page you\u2019re currently navigating.   \nThe open tabs: These are the tabs you have open.   \nThe previous action: These are the actions you have performed. It may be helpful to track your progress.   \nThe actions you can perform fall into several categories:   \nPage Operation Actions:   \nclick [id]: This action clicks on an element with a specific id on the webpage.   \ntype [id] [content] [press_enter_after ${\\it\\Delta}=0$ |1]: Use this to type the content into the field with id. By default, the \"Enter\" key is pressed after typing unless press_enter_after is set to 0. hover [id]: Hover over an element with id.   \nTo be successful, it is very important to follow the following rules:   \n1. You should only issue an action that is valid given the current observation.   \n2. You should only issue one action at a time.   \n3. You should follow the examples to reason step by step and then issue the next action.   \n4. Generate the action in the correct format. Start with a \"In summary, the next action I will perform is\" phrase, followed by action inside \u201c\u201c\u201c. For example, \"In summary, the next action I will perform is \u201c\u2018click [1234]\u201c\u2018\". ", "page_idx": 24}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/547d3f622bda1f3c76486b482ec6c74283a5ddf4594171b7f97a0af7f082ab7a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "I Case Study ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We conducted a detailed examination of instances where Synatra-CodeLlama successfully completes tasks that GPT-4-turbo fails to accomplish on WebArena. Two key patterns emerged from these cases, which we outline here. Our analysis focuses on hard-level tasks requiring multiple steps to complete. ", "page_idx": 25}, {"type": "text", "text": "Synatra-CodeLlama identifies details more effectively In several scenarios, Synatra-CodeLlama outperforms GPT-4-turbo by detecting and utilizing detailed information such as hidden links and buttons on the page, whereas GPT-4-turbo focuses only on the most prominent components. In the scenario illustrated in Figure 6, both agents are tasked with displaying issues labeled as \u201cbug.\u201d GPT-4-turbo immediately attempts to use the search box, but lacks the knowledge of how to search effectively in this case and guesses the keyword, which does not yield results. In contrast, Synatra-CodeLlama pays closer attention to the content displayed on the page and accurately identifies a link that lists all issues labeled as \u201cbug.\u201d ", "page_idx": 25}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/817d3293438503831f8d986096e83bf39a7d8257ef77076638e969cdf086c13b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 6: An example task where Synatra-CodeLlama is successful by paying more attention to the displayed web page while GPT-4-turbo is not. ", "page_idx": 26}, {"type": "text", "text": "Similarly, in the scenario shown in Figure 7, both agents are tasked with \u201cfinding the number of commits Killian made on 3/5/2023.\u201d At this stage, both agents have already navigated to the page displaying Killian\u2019s commits, and they can now provide the answer by simply counting. However, GPT-4-turbo fails to register the displayed information: in its reasoning, it attempts to \u201cview all the commits,\u201d even though they are already visible. In contrast, Synatra-CodeLlama pays closer attention to the information on the page and accurately identifies the relevant link showing Killian\u2019s commits. ", "page_idx": 26}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/010426ae1af44547b7e553fe4d38c9023f8bf0ac653d5874b5723a2cedd75469.jpg", "img_caption": ["Figure 7: An example task where Synatra-CodeLlama is successful paying more attention to the displayed web page while GPT-4-turbo is not. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Synatra-CodeLlama demonstrates clearer logical flow between steps In the example shown in Figure 8, both agents are filling out a form and have correctly entered the start and end dates. However, GPT-4-turbo incorrectly predicts that the next step is to re-enter the starting date, while Synatra-CodeLlama accurately interprets the status of the web page (\u201cwith the date range set to include the entirety of Q1 2023\u201d), plans accordingly (\u201cwe are ready to generate the refund report\u201d), and executes the correct action. This example illustrates that data generated by Synatra enhances the model\u2019s capacity to accurately interpret and respond to context, leading to more efficient and reliable task execution in dynamic settings. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "[1504] button 'Show Report' ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "[2278] StaticText 'Date Used'   \n[2279] combobox 'Date Used'   \nhasPopup: menu expanded: False   \n[2283] combobox 'Period' hasPopup:   \nmenu expanded: False   \n[2324] StaticText 'From'   \n[1281] textbox 'From \\*' required: True [2346] StaticText '01/01/2023'   \n[2286] button 'undefined \\ue627'   \n[2333] StaticText 'To'   \n[1284] textbox 'To \\*' focused: True   \nrequired: True [2497] StaticText '03/31/2023'   \n[2289] button 'undefined \\ue627'   \n[2292] StaticText 'Order Status'   \n[2293] combobox 'Order Status'   \nhasPopup: menu expanded: False ", "page_idx": 27}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/5626ac38606aa5c9085e5d059023b41750bc2f9f198bdf7c97fb3bb2e0ff9176.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 8: An example task where Synatra-CodeLlama is successful while GPT-4-turbo is not. ", "page_idx": 27}, {"type": "text", "text": "J Data Selection from Random Observations in ClueWeb ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We inspect a random sample of ClueWeb on its domain distribution. As shown in Figure 9, the majority of domains appear only once, making up $69.1\\%$ of all the web pages. ", "page_idx": 27}, {"type": "image", "img_path": "KjNEzWRIqn/tmp/ae74deeb07f2fd0709712e31677dfbf5f0e976b56781691e88a1c5cc799e2c96.jpg", "img_caption": ["Figure 9: Frequencies of domains and proportion of each frequency in ClueWeb "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "K Training Settings ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The CodeLlama checkpoints are fine-tuned with A100 GPUs with deepspeed 7 acceleration framework. We set the context length to 4096 tokens. To train on $100k$ dataset, we train with $^{6\\,\\mathrm{x}}$ A100 ", "page_idx": 27}, {"type": "text", "text": "80G GPUs for about 40 hours. We use a batch size of 48, and learning rate of 4e-5. We use cosine annealing and a warm-up ratio of 0.03. ", "page_idx": 28}, {"type": "text", "text": "L Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The broader impacts of this work extend across several domains. Firstly, the approach has the potential to democratize the development of digital agents by making the training process more affordable and accessible. Organizations with limited resources can utilize existing public data to train competent agents without the need for expensive data collection efforts. This could lead to a more widespread adoption and innovation in AI applications, particularly in regions or sectors that previously could not afford such technology. ", "page_idx": 28}, {"type": "text", "text": "Secondly, by enabling digital agents to perform more complex tasks effectively, this work can significantly enhance productivity and efficiency in various industries. For example, customer service, online troubleshooting, and data management tasks could be accelerated, allowing human workers to focus on more creative or complex problem-solving tasks. ", "page_idx": 28}, {"type": "text", "text": "Furthermore, the technology has implications for accessibility, as it could help develop more intuitive and user-friendly interfaces for people with disabilities or those who are not tech-savvy. Agents trained with a diverse range of demonstrations can offer more personalized and context-aware assistance, improving user experience across digital platforms. ", "page_idx": 28}, {"type": "text", "text": "Lastly, the ethical and societal implications of this technology also constitute a critical area of impact. While the technology can lead to significant efficiencies and capabilities, it also raises questions about the potential for job displacement, and the need for robust guidelines to ensure that the deployment of such agents aligns with ethical standards. These broader impacts underscore the importance of interdisciplinary approaches to the development and governance of AI technologies, ensuring they contribute positively to society. ", "page_idx": 28}, {"type": "text", "text": "M Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "One major limitation to the work is the potential variability in the quality and relevance of the indirect knowledge sources. We attempted to mitigate this through use of high-quality sources such as WikiHow and broad sources such as ClueWeb with intelligent sampling strategies, but still the danger of unrepresentative data remains. ", "page_idx": 28}, {"type": "text", "text": "Another concern is the generalizability of the synthesized demonstrations. While the approach allows for the generation of a large volume of training data, the synthetic nature of these demonstrations may not fully capture the complexity and nuances of real human interactions with digital environments. As a result, agents trained on this data may still struggle with unexpected or less typical scenarios not covered in the training data. ", "page_idx": 28}, {"type": "text", "text": "Furthermore, there is the risk of overftiting to the specific formats and tasks represented in the indirect knowledge sources. If the diversity of these sources is limited, the agents may not develop the flexibility needed to handle a broad range of tasks across different platforms or environments. ", "page_idx": 28}, {"type": "text", "text": "Lastly, the reliance on large language models and complex synthesis processes might introduce significant computational costs and environmental impacts. The energy consumption and carbon footprint associated with training such large models are concerns that need to be addressed to ensure sustainable development in AI technologies. These limitations highlight the need for ongoing research, improved data curation methods, and the development of more robust models that can better generalize from synthetic training environments to real-world applications. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We empirically show the claims. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the limitations in the analysis of empirical results, as well as in Appendix M. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No theoretical results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We provide hyperparameters, experimental settings, and data generation prompts for LLMs throughout the paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All the code and data is submitted through Supplementary Material. We will publicly release all the data and code on GitHub. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We detail experimental settings in our experiment section. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Due to costly nature of LLMs (expensive pricing for competitive API-based models and GPU requirements for open-weight models), we do not perform multiple runs for the same experiment setting. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We detail compute resource usage in Appendix K. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Our research conform with NeurIPS Code of Ethics ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Appendix L. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We use existing models (CodeLlama, OpenAI GPT) and datasets (ClueWeb22, WikiHow) that have been either safeguarded or processed with moderation when released. We consider the issue not applicable to our work as we all work with existing safeguarded datasets and models. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We properly linked and cited the base model and source data corpus used in our experiments. They all have permissive licenses. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: There are generated synthetic data as part of our proposed data synthesis method, and finetuned models using these data. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No human subjects involved. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No human subjects involved. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]