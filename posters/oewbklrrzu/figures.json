[{"figure_path": "OEWBkLrRZu/figures/figures_0_1.jpg", "caption": "Figure 1: (A). The task illustration. PIP involves predicting if there is an interaction between two residues from different proteins. (B). The task challenge. During training, the input consists of bound structures of two proteins. However, for testing, one can only access their unbound structures.", "description": "This figure illustrates the protein interface prediction (PIP) task.  Panel (A) shows a schematic of two proteins interacting, highlighting the interface region where residues from different proteins interact.  Positive interactions are shown in red, and negative interactions are shown in dashed red lines. Panel (B) highlights the core challenge of PIP: training is typically performed using bound protein structures (after binding), while testing must be performed on unbound structures (before binding). This mismatch between training and testing data, due to conformational changes in proteins upon binding, poses a significant challenge for accurate PIP.", "section": "Introduction"}, {"figure_path": "OEWBkLrRZu/figures/figures_1_1.jpg", "caption": "Figure 2: The impact of flexibility on results with the DB5.5 dataset [45]. (A) The testing results of two baselines (SASNet [43], NEA [15]) and our method. \u2018B-U' represents the popular formulation, i.e., training with bound structures and testing with unbound ones. 'B-B' refers to the formulation where both training and testing are conducted with bound structures. (B) Loss trends for three methods.", "description": "This figure demonstrates the effect of protein flexibility on the performance of protein interface prediction (PIP) models.  Panel (A) shows the testing AUC scores for three methods (SASNet, NEA, and the proposed method, ATProt) under two different scenarios: B-U (training on bound structures, testing on unbound structures) and B-B (training and testing on bound structures). The results highlight the significant performance drop when testing with unbound structures. Panel (B) displays the training loss curves for the three methods, further illustrating the challenges posed by protein flexibility. The curves clearly show that training with only bound structures (B-B) leads to lower training loss compared to the standard B-U setting. ", "section": "1 Introduction"}, {"figure_path": "OEWBkLrRZu/figures/figures_2_1.jpg", "caption": "Figure 3: AUC vs. representation perturbation.", "description": "This figure shows the relationship between the Area Under the Curve (AUC) of the protein interface prediction (PIP) task and the perturbation in protein representation.  The x-axis represents the magnitude of perturbation in protein representation, and the y-axis represents the AUC in percentage.  The blue line shows the AUC when testing with unbound structures, which gradually decreases as the perturbation increases.  The red arrow highlights the large drop in AUC when moving from test-bound to test-unbound scenarios. The yellow arrow shows how the performance with unbound structures is significantly lower than that with bound structures. This illustrates the impact of protein flexibility on the accuracy of the PIP task. The figure demonstrates a clear negative correlation between representation perturbation and the AUC, which emphasizes the importance of stable protein representations.", "section": "3.1 The importance of protein representation stability"}, {"figure_path": "OEWBkLrRZu/figures/figures_4_1.jpg", "caption": "Figure 4: The framework overview with the BernNet encoder. The whole framework contains the stability-regularized graph encoder for stable protein representations, the cross attention layers for communication and the final binary classifier. ATProt takes in two protein graphs as inputs, and extracts features with the pre-defined graph encoder (BernNet is taken as an example here). The PIP results are obtained after the learned representations have passed through the cross attention module and classifier. The Ls loss for stability regularization and classification loss LBCE jointly optimize the model.", "description": "This figure shows the architecture of ATProt, a framework for protein interface prediction.  It uses a stability-regularized graph neural network (GNN), specifically the BernNet encoder, to generate stable protein representations that are robust to variations in protein structure. These representations are then processed through cross-attention layers to combine information from both proteins, before being fed into a binary classifier to predict the interaction. The model is trained to minimize both classification loss (LBCE) and stability regularization loss (Ls).", "section": "4 Method"}, {"figure_path": "OEWBkLrRZu/figures/figures_8_1.jpg", "caption": "Figure 5: The t-SNE visualization for the last layer representations. The x and y axes of all three subplots are uniformly scaled to (0, 1).", "description": "This figure visualizes the last layer representations of three different models: The NEA method, ATProt-Bern w/o SR, and ATProt-Bern, using t-SNE for dimensionality reduction.  Each point represents a residue pair, colored by whether it's a positive or negative interaction. The visualizations illustrate how the stable representation strategy in ATProt improves the clustering of positive and negative samples, leading to a clearer separation between classes (indicated by higher Silhouette score values).", "section": "Benefits of representation stability"}, {"figure_path": "OEWBkLrRZu/figures/figures_15_1.jpg", "caption": "Figure 6: The way to find upper bound of C, a case to explain Proposition 4.1.", "description": "This figure illustrates how to find the upper bound of C, which is the Lipschitz constant of the BernNet encoder.  It demonstrates the relationship between the function f(x) and the learnable coefficients {\u03b8k} of the Bernstein basis.  The figure shows that the maximum absolute slope (MAS) of any possible version of f(x) is greater than or equal to the MAS of the piecewise linear function connecting the points (k/K, \u03b8k).  The stability of h is therefore always at least that of fi.  The MAS of the broken line (piecewise linear function) is then used as an upper bound for the Lipschitz constant C* of the graph filter h.  This approach is crucial to prove Proposition 4.1 which guarantees the stability of the BernNet encoder.", "section": "4.2 Guaranteeing stability of the BernNet encoder"}]