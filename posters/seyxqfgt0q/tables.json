[{"figure_path": "seYXqfGT0q/tables/tables_2_1.jpg", "caption": "Table 1: Comparison between different category discovery settings. * indicates that the number of new classes (Cls) is set as the ground-truth or previously estimated.", "description": "The table compares three different settings for category discovery: Novel Category Discovery (NCD), Generalized Category Discovery (GCD), and On-the-fly Category Discovery (OCD).  It shows whether the training data includes samples from old and/or new categories, whether the test data includes samples from old and/or new categories, whether the number of new classes needs to be known a priori, and whether the setting requires online inference. The OCD setting is unique in that it only uses old categories in training and does not require the number of new classes to be known beforehand, with the need for online feedback during inference.", "section": "2 Related Works"}, {"figure_path": "seYXqfGT0q/tables/tables_7_1.jpg", "caption": "Table 2: Comparison with the State of the Art methods. The best results are marked in bold, and the second best results are marked by underline.", "description": "This table presents a comparison of the proposed PHE method against several state-of-the-art methods for on-the-fly category discovery across eight fine-grained datasets.  For each dataset and each method, the table shows the 'All', 'Old', and 'New' class accuracies.  'All' refers to the overall accuracy, 'Old' refers to the accuracy on known categories, and 'New' represents the accuracy on newly discovered categories.  The best results for each metric are highlighted in bold, while the second-best results are underlined.  The average accuracy across all datasets is also provided for each method.", "section": "4.2 Comparison with State of the Art"}, {"figure_path": "seYXqfGT0q/tables/tables_7_2.jpg", "caption": "Table 3: Ablation study on training components. The best results are marked in bold.", "description": "This table presents the ablation study results focusing on the three loss functions used in the Prototypical Hash Encoding (PHE) framework: Lp (prototype generation loss), Lc (category encoding loss), and Lf (hash feature loss). The table shows the performance results for CUB and SCars datasets in terms of overall accuracy (All), accuracy on known categories (Old), and accuracy on new categories (New). Each row represents a different combination of the three loss functions, indicating which were used and which were excluded during training. The results demonstrate the importance of each loss function in achieving optimal performance.", "section": "4.3 Ablation Study"}, {"figure_path": "seYXqfGT0q/tables/tables_7_3.jpg", "caption": "Table 4: Ablation study on training strategy. The best results are marked in bold.", "description": "This table presents the ablation study on different training strategies used in the Prototypical Hash Encoding (PHE) framework.  It shows the performance of the model with various components removed or modified: 1) using fixed-h (handcrafted hash points); 2) using only linear classification; 3) using supervised contrastive learning classification; and 4) the full PHE model (Ours). The results are presented in terms of overall accuracy (All), accuracy for known categories (Old), and accuracy for new categories (New), on CUB and SCars datasets. The goal is to demonstrate the contribution of each component to the overall performance of the model. ", "section": "4.3 Ablation Study"}, {"figure_path": "seYXqfGT0q/tables/tables_8_1.jpg", "caption": "Table 5: Results with different hash code length L. The best results are marked in bold for each L.", "description": "This table presents the performance comparison of SMILE and PHE methods with varying hash code lengths (L). The results are shown for CUB-200 and SCars-196 datasets, including overall accuracy, accuracy on old and new categories, and the estimated number of categories.  The table highlights the impact of hash code length on the performance of both methods, especially SMILE, which shows a significant decrease in accuracy as the hash code length increases. PHE demonstrates more stability across different hash code lengths.", "section": "4.4 Evaluation"}, {"figure_path": "seYXqfGT0q/tables/tables_13_1.jpg", "caption": "Table 6: Statistics of datasets used in our experiments.", "description": "This table presents the statistics of eight datasets used in the experiments. For each dataset, it shows the number of seen classes (|Ys|), the number of seen and unseen classes (|YQ|), the number of samples in the training set (|Ds|), and the number of samples in the testing set (|DQ|). The datasets include CUB-200, Stanford Cars, Oxford-IIIT Pet, Food-101, and four sub-categories from iNaturalist (Fungi, Arachnida, Animalia, and Mollusca).", "section": "A.1 Datasets Details and Evaluation Metric Details"}, {"figure_path": "seYXqfGT0q/tables/tables_14_1.jpg", "caption": "Table 7: Values of dmax for different hash code lengths L and datasets.", "description": "This table shows the calculated values of the maximum Hamming distance (dmax) between hash centers for different hash code lengths (L=12, 16, 32, 64 bits) and eight datasets (CUB, Stanford Cars, Oxford Pets, Food-101, Fungi, Arachnida, Animalia, Mollusca).  The dmax values are determined using the Gilbert-Varshamov bound, ensuring sufficient separation between hash centers for effective category discrimination.  These values are crucial hyperparameters in the Discriminative Hash Encoding (DHE) module of the Prototypical Hash Encoding (PHE) framework, influencing the training process and the overall performance of the OCD task.", "section": "A.2 Training Details"}, {"figure_path": "seYXqfGT0q/tables/tables_15_1.jpg", "caption": "Table 8: Mean and std of accuracy in three independent runs.", "description": "This table presents the mean and standard deviation of the accuracy results across three independent runs for each dataset. The accuracy is broken down into three categories: \"All\", \"Old\", and \"New\".  \"All\" represents the overall accuracy, \"Old\" represents the accuracy for known categories, and \"New\" represents the accuracy for newly discovered categories. The standard deviation provides a measure of the variability of the results.", "section": "B Additional Experiment Results and Analysis"}, {"figure_path": "seYXqfGT0q/tables/tables_16_1.jpg", "caption": "Table 9: Results with inference on different input sequences. The best results are marked in bold.", "description": "This table presents the results of the proposed PHE method and compares its performance on two datasets (CUB and SCars) under two different input scenarios: fixed sequences and random sequences. The \"All\", \"Old\", and \"New\" columns represent the overall accuracy, accuracy on known classes, and accuracy on novel classes, respectively.  The results show the robustness of the PHE method to different input orders.", "section": "B.2 Inference on Different Input Sequences"}, {"figure_path": "seYXqfGT0q/tables/tables_16_2.jpg", "caption": "Table 10: Online vs. offline predictions with different hash code lengths L in SMILE on CUB.", "description": "This table compares the performance of SMILE in on-the-fly and offline settings with various hash code lengths (L).  The \"On-the-fly\" columns show the All, Old, and New class accuracy when SMILE is used for online category discovery. The \"Offline Clustering Acc\" column indicates the clustering accuracy achieved by applying k-means clustering to the high-dimensional features before hashing, representing the upper bound of performance achievable without the constraints of online inference.  The results show a trade-off; shorter hash lengths improve on-the-fly performance but reduce offline clustering accuracy, highlighting the sensitivity issue of hash-based methods.", "section": "B.3 Impact of Feature Dimension on SMILE Performance"}, {"figure_path": "seYXqfGT0q/tables/tables_17_1.jpg", "caption": "Table 11: Comparative results of various hashing methods on CUB and Stanford Cars datasets using 12-bit and 32-bit hash code lengths. \u201cMC\u201d denotes manually obtained centers compliant with the Gilbert-Varshamov bound.", "description": "This table compares the performance of different deep hashing methods on CUB and Stanford Cars datasets.  The methods are evaluated using 12-bit and 32-bit hash code lengths, and results show all class accuracy, old class accuracy, and new class accuracy. The table highlights the superior performance of the proposed method (Ours) compared to other state-of-the-art hashing techniques, particularly in terms of achieving higher accuracies on unseen ('New') categories.", "section": "4.2 Comparison with State of the Art"}, {"figure_path": "seYXqfGT0q/tables/tables_17_2.jpg", "caption": "Table 12: Comparison of training time per sample across CUB, Stanford Cars and Food datasets.", "description": "This table compares the training time per sample for three different datasets: CUB-200, Stanford Cars, and Food-101.  The number of training samples and the total training time (in minutes) are shown for each dataset. The final column calculates the training time per sample (in seconds), which provides a useful metric for comparing training efficiency across datasets of varying sizes. The significant difference in training time per sample suggests that dataset size affects training efficiency, although the smaller number of categories in Food-101 is likely a contributing factor.", "section": "B.6 Training Efficiency Analysis"}, {"figure_path": "seYXqfGT0q/tables/tables_17_3.jpg", "caption": "Table 13: Comparison of training times (in minutes)", "description": "This table compares the training times of the proposed PHE method and the SMILE method on four datasets: CUB, SCars, Food, and Pets.  The results show that PHE requires significantly less training time than SMILE across all four datasets. This difference in training time is attributed to the higher computational demands of SMILE's supervised contrastive learning approach.", "section": "B.6 Training Efficiency Analysis"}, {"figure_path": "seYXqfGT0q/tables/tables_18_1.jpg", "caption": "Table 2: Comparison with the State of the Art methods. The best results are marked in bold, and the second best results are marked by underline.", "description": "This table compares the performance of the proposed PHE method with several state-of-the-art methods on eight fine-grained datasets.  The datasets include CUB-200, Stanford Cars, Oxford-IIIT Pet, Food-101, and four super-categories from iNaturalist (Fungi, Arachnida, Animalia, and Mollusca). The results are presented as All (overall accuracy), Old (accuracy on seen categories), and New (accuracy on unseen categories). The table shows that PHE consistently outperforms other methods across all datasets and metrics, particularly demonstrating significant improvements in accuracy on unseen categories.", "section": "4.2 Comparison with State of the Art"}, {"figure_path": "seYXqfGT0q/tables/tables_18_2.jpg", "caption": "Table 2: Comparison with the State of the Art methods. The best results are marked in bold, and the second best results are marked by underline.", "description": "This table presents a comparison of the proposed PHE method with several state-of-the-art methods on eight fine-grained datasets.  The performance is evaluated using clustering accuracy, separately for all classes, old (known) classes, and new (unknown) classes. The table shows that PHE significantly outperforms existing methods on all metrics and datasets, especially for new classes.", "section": "4.2 Comparison with State of the Art"}]