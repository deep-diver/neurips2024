[{"figure_path": "XswQeLjJo5/figures/figures_3_1.jpg", "caption": "Figure 1: One head in Transformer architecture with Softmax Attention.", "description": "This figure shows the architecture of a single attention head in a Transformer model. It illustrates the flow of information from the input tokens (Xi) through the query (WQ), key (WK), and value (WV) weight matrices.  The Softmax attention mechanism (Sih) is applied to the inner products of query and key vectors, and then the weighted values are combined and transformed by WO before passing through MLP layers. This figure is crucial in understanding the model structure described in the paper, particularly Section 3, where the notations and problem descriptions are laid out.", "section": "3 Notations and Problem Description"}, {"figure_path": "XswQeLjJo5/figures/figures_7_1.jpg", "caption": "Figure 2: Test performance on text classification task with different attention kernels", "description": "This figure shows the test accuracy and test loss curves for a text classification task using both Softmax and Gaussian attention kernels. The x-axis represents the training epoch, while the y-axis represents the test accuracy (left panel) and test loss (right panel).  The shaded regions indicate the variance across multiple runs. The results show that the Gaussian kernel achieved higher accuracy and lower loss compared to the Softmax kernel, suggesting faster convergence and better generalization.", "section": "5.3.1 Test Loss & Accuracy Curve comparison"}, {"figure_path": "XswQeLjJo5/figures/figures_7_2.jpg", "caption": "Figure 2: Test performance on text classification task with different attention kernels", "description": "This figure shows the test accuracy and test loss curves for both Gaussian and Softmax attention mechanisms on a text classification task.  The x-axis represents the training epoch, and the y-axis shows the test accuracy (left panel) and test loss (right panel). The shaded areas represent the standard deviation across multiple runs.  The figure demonstrates that the Gaussian kernel consistently outperforms Softmax, achieving higher accuracy and lower loss with faster convergence. This supports the paper's claims regarding the advantages of the Gaussian attention kernel.", "section": "5.3 Test Loss & Accuracy Curve comparison"}, {"figure_path": "XswQeLjJo5/figures/figures_8_1.jpg", "caption": "Figure 4: The loss landscapes on text classification task and Pathfinder task. For both tasks, we use the two-stage training in Section 5.2 with the same training hyperparameters, while the only difference is the attention structure in the second training stage. The two axes represent the two directions d\u2081 and d2 as defined in Section 5.2.", "description": "This figure visualizes the loss landscapes of both the text classification and Pathfinder tasks, using Softmax and Gaussian attention mechanisms.  The two-stage training process is described, highlighting that the only difference between the landscapes is the attention mechanism used in the second stage.  The axes represent the parameter directions d1 and d2, explained further in section 5.2 of the paper. The visualizations allow for a comparison of the optimization landscapes under different attention mechanisms and across tasks.", "section": "5.3.2 Optimization Landscape Comparison"}]