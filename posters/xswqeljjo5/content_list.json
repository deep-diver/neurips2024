[{"type": "text", "text": "Unraveling the Gradient Descent Dynamics of Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bingqing Song\\* University of Minnesota, Twin Cities song0409@umn.edu ", "page_idx": 0}, {"type": "text", "text": "Boran Han Amazon Web Services boranhan@amazon.com ", "page_idx": 0}, {"type": "text", "text": "Shuai Zhang Amazon Web Services shuaizs@amazon.com ", "page_idx": 0}, {"type": "text", "text": "Jie Ding University of Minnesota, Twin Cities dingj@umn.edu ", "page_idx": 0}, {"type": "text", "text": "Mingyi Hong University of Minnesota, Twin Cities mhong@umn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While the Transformer architecture has achieved remarkable success across various domains, a thorough theoretical foundation explaining its optimization dynamics is yet to be fully developed. In this study, we aim to bridge this understanding gap by answering the following two core questions: (1) Which types of Transformer architectures allow Gradient Descent (GD) to achieve guaranteed convergence? and (2) Under what initial conditions and architectural specifics does the Transformer achieve rapid convergence during training? By analyzing the loss landscape of a single Transformer layer using Softmax and Gaussian attention kernels, our work provides concrete answers to these questions. Our findings demonstrate that, with appropriate weight initialization, GD can train a Transformer model (with either kernel type) to achieve a global optimal solution, especially when the input embedding dimension is large. Nonetheless, certain scenarios highlight potential pitfalls: training a Transformer using the Softmax attention kernel may sometimes lead to suboptimal local solutions.In contrast, the Gaussian attention kernel exhibits a much favorable behavior. Our empirical study further validate the theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer model architectures have become popular in machine learning, delivering remarkable performance across a wide array of tasks. From natural language processing [Vaswani et al., 2017, Beltagy et al., 2020] to computer vision [Dosovitskiy et al., 2020],these models have set new standards in performance and efficiency. Popular models include BERT [Devlin et al., 2018], RoBERTa [Liu et al., 2019], DeBERTa [He et al., 2020], GPT models [Radford et al., 2019, Brown et al., 2020] and ViT [Dosovitskiy et al., 2020]. Despite their empirical success, a comprehensive understanding of their optimization process remains elusive. As highlighted in Liu et al. [2020], the training of large Transformers can sometimes result in deteriorated performance. It is therefore critical to develop theoretical insights for researchers and practitioners to better understand the practical performance of Transformers. However, the complexity of their architectures, coupled with the non-convex nature of the associated optimization problems, has made the theoretical analysis of these models very challenging. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The optimization landscape can be pivotal for understanding a certain type of neural network and providing the practical guidance [Liu et al., 2020]. Existing literature offers numerous studies on achieving zero-loss solutions in networks with ReLU activation. These studies encompass various network structures, including fully-connected, convolutional, and residual networks, as explored in [Jain et al., 2017], [Jin et al., 2021], and [Danilova et al., 2022]. They delve into the analysis of network optimization landscapes and provide assurances of rapid global convergence when using gradient descent (GD) or stochastic gradient descent (SGD) algorithms. For instance, in Du et al. [2019], the authors focus on fully-connected networks and ResNets with smooth activation functions, and they have demonstrated that global convergence can be achieved using GD with a network size proportional to $\\mathcal{O}\\big(\\mathrm{poly}(N)\\big)$ , where $N$ is the sample size. Similarly, [Allen-Zhu et al., 2019] show that ReLU fully-connected networks with at least $\\mathcal{O}\\big(\\mathrm{poly}(N)\\big)$ neurons can achieve global convergence using GD or SGD. From a statistical perspective, [Li et al., 2023] have shown that for two-layer ReLU neural networks (with input dimension $p$ ) that admit a sparse subnetwork representation, a sample size of $O(\\log^{4}(p/\\delta))$ can guarantee the global convergence with probability at least $\\delta$ using GD. Despite this extensive body of work on traditional architectures, it is not clear what conditions we need (e.g. network size, optimizer, initialization) to ensure training Transformer models to find high-quality solutions. ", "page_idx": 1}, {"type": "text", "text": "Compared to traditional deep learning architectures, Transformers incorporate a unique level of intricacy through their attention kernel [Vaswani et al., 2017], which is designed to effectively handle sequence inputs. This mechanism incorporates Softmax activation to the inner products of query and key vectors, and this inherently non-convex operation poses considerable challenges to theoretical analysis. Consequently, existing frameworks for analyzing the convergence of classical deep learning models are not directly applicable to Transformers. Further, many recent works have pointed out that the performance of Transformers depends on a number of factors such as the choice of kernel function, initialization, choice of optimizers, and forms of token embeddings [Huang et al., 2020, Pan and Li, 2023, Shazeer, 2020, Li et al., 2018, Tian et al., 2023]. In deep learning, these factors have been studied in a line works. For example, Li et al. [2018] show that the good training performance is not universal ; skip connections have the effect of smoothing the training landscape, and the Adam algorithm tends to follow a more direct trajectory towards optimal solutions compared to SGD. Therefore, it is imperative to understand what kind of conditions, including initialization, network structure, data properties, and optimizer choices, will lead to high-performing Transformers. ", "page_idx": 1}, {"type": "text", "text": "In this work, we will delve into the intricacies of attention kernels, discussing both their advantages and limitations in the context of model optimization. The main contributions of this work are threefold. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We derive the conditions that will make the one-layer Softmax attention Transformer reach global optimality with vanilla gradient descent. The convergence guarantee is largely attributed to the linear layer $(W^{\\tilde{V}})$ in the attention mechanism.   \n\u00b7 We investigate the attention kernel's effectiveness, revealing Gaussian attention achieves zero training loss, while Softmax can lead to non-optimal stationary points.   \n\u00b7 Our experiments validate that Softmax attention Transformers converge slower and present more challenging training landscapes than Gaussian counterparts, potentially leading to more local optimal solutions. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A number of research works have focused on the theoretical analysis and interpretation of Transformer models, revealing crucial insights into their practical performance. ", "page_idx": 1}, {"type": "text", "text": "Liu et al. [2020] showed that heavy reliance on the residual branch in multi-layer Transformer models can lead to training instability, which amplifies small parameter perturbations, causing significant disturbances in the model's output. In Bhojanapalli et al. [2020], the authors illustrated the existence of a low-rank bottleneck in Transformer models with sufficiently large embedding and hidden size $(D=d)$ . However, this work focuses on the representation ability of large size attention, while falling short of analyzing Transformer models from an optimization perspective. In Noci et al. [2022], the authors explored rank collapse issues in token representations and their impact on training. The authors discussed the origin of the phenomenon of rank collapse and proposed depth-dependent scaling of residual branches as a potential solution. They specifically investigated scenarios where token rank equals one, which can hinder Transformer training. Their findings demonstrate the occurrence of the vanishing gradient issue, however, this work does not comprehensively characterize the vanishing gradient problem throughout the entire training process. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "A recent work Wu et al. [2024] analyzes the convergence behaviour of shallow Transformer, which shows the global convergence can be achieved with GD algorithm. However, the focus of our paper is different from Wu et al. [2024]. We not only derive the global convergence analysis (Our Theorem 2), but also investigates the role of different variables in optimization. ", "page_idx": 2}, {"type": "text", "text": "Some other works focus on improving the optimization of Transformers empirically. [Huang et al., 2020] have proposed an initialization strategy such that no warm-up or layer normalization is needed to train Transformers efficiently; in Shazeer [2020], the GLU variant of token embedding has been showed to be better than plain embedding in the optimization of Transformer models with Softmax attention kernel. It is worth noting that the above works all primarily focus on empirical investigations into the training of Transformer models, lacking a comprehensive theoretical analysis of the underlying mechanisms. ", "page_idx": 2}, {"type": "text", "text": "Some recent research has focused on the convergence analysis of Transformer-based models within the in-context learning (ICL) framework. For instance, Huang et al. [2023], Zhang et al. [2023] explores the learning dynamics of a one-layer Transformer with Softmax attention trained via gradient descent to learn linear function classes in-context. However, this line of study primarily addresses the general convergence performance of Transformers within the ICL setting and does not delve into the role of individual variables. ", "page_idx": 2}, {"type": "text", "text": "3   Notations and Problem Description ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we define the structure of the Transformer model and describe the training problem. We consider a one-layer attention Transformer model with multiple heads and a dataset with $N$ samples. Each data sample consists of $n$ discrete tokens, each with embedding dimension $D$ .We denote the dataset as $\\{(\\bar{X}_{i},y_{i})\\}_{i=1}^{N}$ , where $X_{i}\\,\\in\\,\\mathbb{R}^{n\\times D}$ , and $y_{i}\\,\\in\\,\\mathbb{R}^{n}$ is the label of the dataset. The output from the Transformer model is the prediction of the label. The Transformer structure is formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Attention}(W_{h}^{Q},W_{h}^{K},W_{h}^{V};X_{i}):=S(W_{h}^{Q},W_{h}^{K};X_{i})X_{i}W_{h}^{V}}\\\\ &{\\mathsf{M H}(W^{Q},W^{K},W^{V};X_{i}):=\\mathrm{Concat}\\,(\\mathrm{head}_{1},\\dots,\\mathrm{head}_{\\mathrm{H}})\\cdot W^{O},}\\\\ &{\\mathrm{where~head}_{\\mathrm{h}}:=\\mathrm{Attention}(W_{h}^{Q},W_{h}^{K},W_{h}^{V};X_{i}),h=1,\\cdots,H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the above notation, ${W}_{h}^{Q},{W}_{h}^{K}\\ \\in\\ \\mathbb{R}^{D\\times d}$ is the query weight matrix and key weight matrix, respectively; $W_{h}^{V}\\,\\in\\,\\mathbb{R}^{D\\times\\ddot{d}}$ is the value weight matrix; these matrices are the main optimization variables throughout the paper. Further $W^{O}\\in\\mathbb{R}^{H d\\times1}$ is a fixed matrix, representing the weight of the output layer; $H$ is the number of attention heads; $S(\\cdot)$ is a kernel function of variables $\\bar{W}^{Q^{\\bar{}}},\\bar{W}^{K}$ and input $X_{i}$ . Attention $(\\cdot)$ is the attention head function; $\\mathsf{M H}(\\cdot)$ represents the multi-head attention function. For example, with the Softmax attention [Vaswani et al., 2017], $S(\\cdot)$ can be written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nS\\left(W_{h}^{Q},W_{h}^{K};X_{i}\\right):=\\mathrm{Softmax}\\left(\\frac{X_{i}W_{h}^{Q}\\left(X_{i}W_{h}^{K}\\right)^{\\top}}{\\sqrt{d}}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where for a given $n\\times n$ matrix $Z$ $,\\operatorname{Sofmax}(Z):=[\\operatorname{Softmax}(Z_{1}),\\cdot\\cdot\\cdot\\,,\\operatorname{Softmax}(Z_{n})]$ Throughout, let us denote $S(\\cdot)_{k j}$ as the element of $k$ -th row and $j$ -th column in matrix $S(\\cdot)$ . Let $X_{i k}$ $\\bar{\\in}\\ \\mathbb{R}^{D}$ denote the embedding of the $k$ -th token in data $X_{i}$ , which is the $k$ -th row of matrix $X_{i}$ . The structure of Transforemdl canefoudnFig  wherewent $S_{i h}:=S\\left(W_{h}^{Q},W_{h}^{K};X_{i}\\right)$ ", "page_idx": 2}, {"type": "text", "text": "Based on the above Transformer model, we consider minimizing the following empirical $\\ell_{2}$ loss function for the entire data set { Xi, yi\u2032=1: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{M}\\frac{1}{2}\\sum_{i=1}^{N}\\|\\mathsf{M}\\mathsf{H}(M;X_{i})-y_{i}\\|^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $M:=(W^{Q},W^{K},W^{V})$ is the set of variables that can be optimized. ", "page_idx": 3}, {"type": "text", "text": "For notation simplicity, next we define the vector version of the Transformer model given in Equation (1), for the entire dataset $\\{(X_{i},y_{i})\\}_{i=1}^{N}$ . Towards this end, let $\\boldsymbol{X}\\in\\mathbb{R}^{N n\\times D}$ denote the columnstacked matrix of each single data $X_{i}$ . Similarly, define the stacked label $y\\in\\mathbb{R}^{N n}$ . Then we can define: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{M H}(M;X):=\\left(\\begin{array}{c c c}{S_{11}X_{1}}&{\\cdot\\cdot\\cdot}&{S_{1H}X_{1}}\\\\ {\\cdot\\cdot}&{\\cdot\\cdot}&{\\cdot\\cdot}\\\\ {S_{N1}X_{N}}&{\\cdot\\cdot}&{S_{N H}X_{N}}\\end{array}\\right)\\cdot\\mathrm{diag}(W_{1}^{V},\\cdot\\cdot\\cdot,W_{H}^{V})\\cdot W^{O},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$i=1,2,\\cdot\\cdot\\cdot,N,h=1,2,\\cdot\\cdot\\cdot,H$ for simplicity. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{B}&{{}:=}&{\\binom{S_{11}X_{1}}{\\cdots\\cdot}\\,\\,\\,\\cdots\\,\\,\\,\\,\\,S_{1H}X_{1}}\\\\ {B}&{{}:=}&{\\binom{S_{11}X_{1}}{S_{N1}X_{N}}\\,\\,\\,\\,\\cdots\\,\\,\\,\\,\\,\\,S_{N H}X_{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathrm{diag}(W_{1}^{V},\\cdot\\cdot\\cdot\\cdot,W_{H}^{V})\\in\\mathbb{R}^{H D\\times H d}$ denote the diagonalized weight matrices that include all value weight matrices for all attention heads. Using these definitions, We can simplify Equation (5) as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{M H}(M;X)=B\\cdot W^{V}\\cdot W^{O}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "XswQeLjJo5/tmp/e6565f8c221ed031696917faa5b9c733a7a80399399a456b3f8f4ce374fb69ad.jpg", "img_caption": ["Figure 1: One head in Transformer architecture with Softmax Attention. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Thus the empirical loss function given in Equation (4) can be simplified as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{M}\\frac{1}{2}\\|\\mathsf{M}\\mathsf{H}(M;X)-y\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For more notations in the following sections, we will use subscript $t$ to represent the variables in $t$ -th iteration, e.g, $M_{t}\\,:=$ $\\{W_{t}^{Q},W_{t}^{K},W_{t}^{V}\\}$ Similaly, we denote $B_{t}$ as the matrix $B$ at $t$ -th iteration. ", "page_idx": 3}, {"type": "text", "text": "It is important to note that, in the above description and throughout the paper, we model the Transformer training problem by using a single-layer Transformer, with a regression loss. In practice Transformer models can exhibit greater complexity (different loss functions, multiple layers, etc). For example, the text classification task has an additional mean pooling layer followed by the output of the Transformer structure. Further, they usually contain downstream MLP modules. However, we choose to use the simplified version due to the following reasons: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "First, the primary objective of this work is to understand how different attention kernels affect the training dynamics of the Transformers, so we do not include the layer normalization in our model. In fact, in the literature, many works that analyze popular network structures also do not consider layer normalization. For example, in [Huang et al., 2023, Zhang et al., 2023], both analyze the convergence performance of Transformers but normalization is not considered. ", "page_idx": 3}, {"type": "text", "text": "Second, we do not include the downstream MLP module in our work since we are interested in the role of self-attention layer in convergence analysis, and the single-attention model is also the standard model used in [Huang et al., 2023, Zhang et al., 2023]. Further, the analysis of MLP is standard in literature [Allen-Zhu et al., 2019, Du et al., 2019, Nguyen and Mondelli, 2020]. And it is worth noting that our choice to focus on a one-layer Transformer is consistent with other works that similarly aim to investigate the core training dynamics of Transformers, e.g, in [Tian et al., 2023], a single-layer Transformer is considered as a basic model. ", "page_idx": 3}, {"type": "text", "text": "4 Convergence Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our theoretical analysis for solving problem (6). We focus on the behavior of the vanilla GD algorithm for optimizing the variable set $M$ ,where $\\dot{M^{\\subset}}\\subset\\{W^{Q},W^{K},W^{V}\\}$ .Below wesummarize our results. ", "page_idx": 3}, {"type": "text", "text": "Common convergence conditions with Softmax Attention: When the activation function $S(\\cdot)$ is either the Softmax or Gaussian function, and the embedding dimension $D$ is at least $O(N n)$ ", "page_idx": 3}, {"type": "text", "text": "optimizing Equation (6) can achieve a global optimal solution when $M\\;=\\;\\{W^{V}\\}$ and $M\\,=$ $\\{\\dot{W}^{Q},W^{\\check{K}},\\dot{W}^{V}\\}$ ", "page_idx": 4}, {"type": "text", "text": "Different behavior between Softmax and Gaussian Kernel Attention. When $S(\\cdot)$ is Gaussian and the embedding dimension $D$ is at least $\\mathcal{O}(N n)$ , convergence to global optimal is also ensured for $M=\\{W^{Q}\\}$ . Interestingly, under the same conditions of large $D$ , convergence to global optimal is not guaranteed when $\\bar{S(\\cdot)}$ is Softmax. ", "page_idx": 4}, {"type": "text", "text": "In the subsequent sections, we will elaborate on these convergence results in detail, providing a deeper understanding of the nuances in Transformer behavior under varying configurations. To set up our analysis, we introduce $\\underline{{\\boldsymbol{\\lambda}}}^{V}$ as the smallest eigenvalue of $W_{0}^{V},\\underline{{{\\lambda}}}^{B}$ as the smallest eigenvalue of $B_{0}$ $\\bar{\\lambda}_{h}^{Q},\\bar{\\lambda}_{h}^{K},\\bar{\\lambda}^{V}$ Wk.o, WV , respectively. We denote I l2 as $\\ell_{2}$ norm and $\\|\\cdot\\|_{F}$ as Frobenius norm. Further, we denote $\\sigma_{\\mathrm{max}}(\\cdot)$ and $\\sigma_{\\mathrm{min}}(\\cdot)$ as the largest and smallest singular value of a matrix, respectively. For any vector $v$ , let $\\operatorname*{min}(|v|)$ denote the smallest absolutevalue of vector $v$ ", "page_idx": 4}, {"type": "text", "text": "4.1  Convergence to global optimal ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "First, we examine the role of $W^{V}$ in the optimization of multi-head attention network structure. Our analysis demonstrates that with the hidden dimension $H D\\geq N n$ and proper initialization, the global optimal solution of (6) can be found using a vanilla gradient descent algorithm. The initialization requires that the matrix $B_{0}$ has full rank. Our first result shows that, overparameterized Transformer can be trained to global optimal solution. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Consider problem (4) with $S(\\cdot)$ being instantiated as the Softmax kernel given in $(3)$ Considethfollwingupdatefortheariabl $\\breve{M}\\,=\\,\\{W^{V}\\}$ $\\boldsymbol{W}_{t+1}^{V}=\\overset{\\circ}{\\boldsymbol{W}}_{t}^{V}-\\eta\\nabla_{\\boldsymbol{W}^{V}}f(\\boldsymbol{M}_{t};\\boldsymbol{X})$ where $\\eta>0$ is the stepsize. ", "page_idx": 4}, {"type": "text", "text": "Suppose $W_{0}^{Q}$ and $W_{0}^{K}$ are initialized such that $\\underline{{\\lambda}}^{B}>0$ Then we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf\\left(M_{t};X\\right)\\leq\\left(1-\\eta\\alpha\\right)^{t}f\\left(M_{0};X\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha:=\\|W^{O}\\|^{2}(\\underline{{{\\lambda}}}^{B})^{2}>0;\\eta>$ is defined in Appendix 1.3, and chosen such as $\\eta\\alpha<1$ ", "page_idx": 4}, {"type": "text", "text": "Remark 1. The aforementioned theorem focuses on the convergence behavior when only $W^{V}$ is being updated.We further elaborate on the initial conditions ensuring $\\underline{{\\lambda}}^{B}>0$ ", "page_idx": 4}, {"type": "text", "text": "Note that $\\underline{{\\lambda}}^{B}>0$ implies that the objective function $f$ exhibits a landscape that is nearly convex, which is crucial for optimization. By definition, this condition implies that $B_{0}$ has full rank, which can befulflled by selectingappropriate $W_{0}^{Q}$ and $W_{0}^{K}$ plus having large enough embedding size, satisfying $D\\geq N n/H$ .We refer the readers to Appendix 1.3 for the derivation of this condition, which can be guaranteed by random initialization with high probability. ", "page_idx": 4}, {"type": "text", "text": "Furthermore, it is important to note that our work aligns with existing literature on the subject of embedding size in Transformer models. For example, in [Bhojanapalli et al., 2020], the authors restrict their focus to the simplified case of $N=1,H=1$ . They establish the necessary condition for Softmax attention to overcome its low-rank bottleneck, which requires $D\\geq n$ . In our analysis, we derive a similar necessary condition on Transformer model size $(D\\geq n\\times(N/H))$ toguarantee the global convergence when a Transformer model is trained with GD. ", "page_idx": 4}, {"type": "text", "text": "In Theorem 1, we have illustrated the case where only updating $W^{V}$ already leads to global convergence. However, in practice, all parameters $W^{V},\\dot{W^{Q}},W^{\\breve{K}}$ are updated. This case is more challenging to analyze due to the non-linearity introduced by the Softmax function. Next, we show that a similar result in Theorem 1 still holds when all the parameters are updated simultaneously. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Consider problem (4), with $S(\\cdot)$ being instantiated as the Softmax kernel. Consider the $G D$ updatewhere $M=\\{W^{Q},W^{K},W^{V}\\}$ :Suppose $\\underline{{\\lambda}}^{B}>0$ and the initialization $M_{0}$ satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{n^{2}\\sqrt{N H}\\|X\\|_{F}^{5}\\sum_{h=1}^{H}\\left((\\bar{\\lambda}_{h}^{Q})^{2}+(\\bar{\\lambda}_{h}^{K})^{2}\\right)\\bar{\\lambda}^{V}}{\\|W^{O}\\|_{2}\\cdot(\\underline{{\\lambda}}^{B})^{2}\\operatorname*{min}\\big(\\bar{\\lambda}_{h}^{Q},\\bar{\\lambda}_{h}^{K},\\underline{{\\lambda}}^{B}\\big)}\\times\\|\\mathsf{M H}(M_{0};X)-y\\|_{2}\\le\\nu.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then there exists stepsize $\\eta>0$ suchthat ", "page_idx": 4}, {"type": "equation", "text": "$$\nf\\left(M_{t};X\\right)\\leq\\left(1-\\eta\\beta\\right)^{t}f\\left(M_{0};X\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta:=\\|W^{O}\\|^{2}(\\underline{{\\lambda}}^{B})^{2}>0,$ . and the constants $\\eta,\\nu$ are defined in Appendix 1.3. ", "page_idx": 5}, {"type": "text", "text": "Remark 2. In the stated theorem, we simplify our analysis by excluding the downstream MLP module in the typical Transformermodel, sinceit is easy to combine the model in Equation(2) with downstream MLP layers.Further, it can be directly showed that the Transformer with MLP will lead to the same convergence rate of the optimization problem as updating $W^{Q},W^{K},W^{V}$ Oonly. $T o$ illustratethis,consider thefollowingTransformermodel: ", "page_idx": 5}, {"type": "equation", "text": "$$\nG\\left({\\boldsymbol{W^{Q},W^{K},W^{V};X_{i}}}\\right)=\\mathsf{M H}({\\boldsymbol{W^{Q},W^{K},W^{V};X_{i}}})\\cdot{\\boldsymbol{W^{1}}}{\\boldsymbol{W^{2}}}\\cdot\\cdot\\cdot{\\boldsymbol{W^{L}}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W^{l}\\in\\mathbb{R}^{n_{l-1}\\times n_{l}}$ and $n_{0}=d^{O}$ BasedontTrafoemdl deedinqutn $(I O)$ we have the following corollary. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1.Consider problem $\\operatorname*{min}_{M}\\frac{1}{2}\\|G(M;X)-y\\|^{2}$ Wwih $G(\\cdot)$ beingdefined inEquation $(I O)$ and $S(\\cdot)$ being instantiated as theSoftmax kernel. Suppose that theMLPmodule satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\nn_{1}\\geq n_{2}\\cdot\\cdot\\cdot\\geq n_{L}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Consider the following $G D$ update (where $M=\\{W^{Q},W^{K},W^{V},W^{1},\\cdots\\,,W^{L}\\}\\}$ : Suppose $\\underline{{\\lambda}}^{B}>0$ Then, there exists a step size $\\eta>0$ and initialization weight $M_{0}$ such that the loss function linearly converges to 0. ", "page_idx": 5}, {"type": "text", "text": "Remark3.The above theorem and corollary describe the global convergence guarantee when $W^{Q}$ $W^{K}$ and $W^{V}$ are updated. This is in line with the insights gained from Theorem 1. However, theconditionsforinitializationaremorestringent,andtheoptimizationlandscapebecomesinherently more complex due to the involvement of the Softmax attention through $W^{Q}$ and $W^{K}$ ", "page_idx": 5}, {"type": "text", "text": "To ensure the initial condition 8, we have two options: $^{\\,l}$ )Initializing $M_{0}$ suchthat $\\Vert\\mathsf{M H}(M_{0};X)-$ $\\boldsymbol y\\Vert_{F}$ is small,which implies that the optimization starts in a region close to the global optimal solution and that the initial weight is close to the global optimal solution; 2) Balancing between $W^{O}$ and $W^{V}$ , in the sense that $\\|\\boldsymbol{W}^{\\mathcal{O}}\\|_{2}$ is large and ${\\bar{\\lambda}}^{V}$ is small. For a detailed account of these initialization strategies, please refer to Appendix 1.3. ", "page_idx": 5}, {"type": "text", "text": "Finally, we need to point out that for Transformers with Gaussian kernel attention, we can derive similar convergence results as long as the attention kernel maintains full rank and weights are initialized appropriately. Here we do not include the theoretical statement since it is similar to the resultforSoftmaxattention. ", "page_idx": 5}, {"type": "text", "text": "4.2  Softmax vs Gaussian kernel: Softmax attention Transformers may exhibit slower convergence. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the previous section, we explored the global convergence of training Transformer models. However, from Theorem 2, it was not clear what roles do matrices $W^{Q}$ and $W^{\\breve{K}}$ play in the entire convergence process, since Theorem 1 indicates that optimizing $W^{V}$ alone already ensures the desired convergence. Nevertheless,it is the matrices $W^{K}$ and $W^{Q}$ that truly represent the power of a Transformer model, because they are used to extract token correlations. ", "page_idx": 5}, {"type": "text", "text": "To study how well a Transformer model can extract the token correlation, in this section, we will study the GD dynamics for Transformer models, where only $W^{K}$ and $W^{Q}$ are optimized (while fixing $W^{V}$ ). If optimizing these two parameters alone can still achieve zero training loss, then we claim that the input token correlation can be optimally extracted by the Transformer model. ", "page_idx": 5}, {"type": "text", "text": "4.2.1 Notations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To begin our study, let us define that Gaussian kernel to be an $n\\times n$ matrix, where its $k$ -throw and $j$ -th column of is given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS\\left(W_{h}^{Q},W_{h}^{K};X_{i}\\right)_{k j}=\\exp\\left(-\\frac{1}{\\sqrt{d}}\\left(X_{i k}.W_{h}^{Q}-X_{i j}.W_{h}^{K}\\right)^{2}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the training dynamics/gradients of variables $W^{Q}$ and $W^{K}$ have the same property in (3) and (11), we will only concentrate on optimizing $W^{Q}$ ", "page_idx": 5}, {"type": "text", "text": "With some abuse of notation, define a matrix $C$ for Softmax attention and Gaussian kernel attention, respectively. Softmax attention: $\\begin{array}{r}{C_{i h}:=\\frac{X_{i}W_{h}^{Q}\\left(X_{i}W_{h}^{K}\\right)^{\\top}}{\\sqrt{d}}\\in\\mathbb{R}^{n\\times n}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Gaussian kerne atention: $C_{i h}\\in\\mathbb{R}^{n\\times n}$ $\\begin{array}{r}{\\left(\\boldsymbol{C}_{i h}\\right)_{k j}=-\\frac{\\left\\|\\boldsymbol{X}_{i k},\\boldsymbol{W}_{h}^{Q}-\\boldsymbol{X}_{i j},\\boldsymbol{W}_{h}^{K}\\right\\|^{2}}{2\\sqrt{d}}}\\end{array}$ For both Softmax attention and Gaussian kernel attention: ", "page_idx": 6}, {"type": "equation", "text": "$$\nC_{i}\\in\\mathbb{R}^{n\\times H n}=[C_{i1},C_{i2},\\cdot\\cdot\\cdot\\,,C_{i H}]\\,;\\;C\\in\\mathbb{R}^{N n\\times H n}=[C_{1}^{\\top},C_{2}^{\\top},\\cdot\\cdot\\cdot\\,,C_{N}^{\\top}]^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Using the above notation, the activation function $S(\\cdot)$ in (3) and (11) can be related to the matrices $C$ 's in the following manner: ", "page_idx": 6}, {"type": "equation", "text": "$$\n:S_{i h}=\\operatorname{Softmax}\\left(C_{i h}\\right),{\\mathrm{~Gaussian~attention:~}}\\left(S_{i h}\\right)_{k j}=\\exp{\\left(\\left(C_{i h}\\right)_{k j}\\right)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Additionally, note that $C$ is a function of variables $M$ . Therefore we will sometimes use $C(M)$ when we need to emphasize the dependency of $C$ on $M$ ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we will outline the conditions under which GD can still successfully find global optimal solutions for Transformers with Gaussian kernel attention (when only $W^{Q}$ is updated), while under the same set of conditions, but with Softmax kernel attention, GD fails. ", "page_idx": 6}, {"type": "text", "text": "Theorem . Solve poble 4) ith thefolowing $G D$ update wvith $M\\;=\\;\\{W^{Q}\\},$ $W_{t+1}^{Q}\\;=\\;$ $W_{t}^{Q}\\,-\\,\\eta\\nabla_{W^{Q}}f(M_{t};X)$ .Suppose $\\delta_{h}\\ :=\\ \\sigma_{\\mathrm{min}}\\big(\\frac{\\partial C(M_{0})}{\\partial W_{h}^{Q}}\\big)\\ >\\ 0$ $\\forall\\ h\\ \\in\\ [1,2,\\cdot\\cdot\\cdot\\cdot,H]$ \uff0cand the initialization condition further satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{n\\|X\\|_{F}^{5}\\big(\\bar{\\lambda}_{h}^{Q}+\\bar{\\lambda}_{h}^{K}\\big)\\exp\\big(\\frac{9}{4}\\|X\\|_{F}^{2}\\big((\\bar{\\lambda}_{h}^{Q})^{2}+(\\bar{\\lambda}_{h}^{K})^{2}\\big)\\big)}{\\big(\\operatorname*{min}(|V^{\\prime}W^{O}|)\\big)^{2}\\cdot\\operatorname*{min}(\\delta_{h},\\bar{\\lambda}_{h}^{Q})}\\times\\bar{\\lambda}^{V}\\|W^{O}\\|_{2}\\cdot\\|\\mathsf{M}\\mathsf{H}\\,(M_{0};X)-y\\|_{2}\\leq\\nu^{\\prime},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\nu^{\\prime}$ is defined in Appendix 1.5. ", "page_idx": 6}, {"type": "text", "text": "$(I)$ When $S(\\cdot)$ is a Gaussian kernel function, there exists a stepsize  and a positive constant $\\gamma_{;}$ sucCh that ", "page_idx": 6}, {"type": "equation", "text": "$$\nf\\left(M_{t};X\\right)\\leq\\left(1-\\eta\\gamma\\right)^{t}f\\left(M_{0};X\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma,\\eta$ are defined in Appendix 1.5. ", "page_idx": 6}, {"type": "text", "text": "(2) When $S(\\cdot)$ is a Softmax function, suppose $W_{t}^{Q}$ is bounded during the training phase, then there exists stepsize $\\eta,$ suchthat ", "page_idx": 6}, {"type": "equation", "text": "$$\nf\\left(M_{t};X\\right)\\leq f\\left(M_{0};X\\right)-\\eta^{\\prime}\\sum_{r=0}^{t-1}{\\left\\Vert\\nabla_{W}\\varrho\\,f\\left(M_{r};X\\right)\\right\\Vert^{2}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\eta^{\\prime}$ is defined in Appendix 1.5 ", "page_idx": 6}, {"type": "text", "text": "Remark 4.First, it's important to note that the parameter size must satisfy $D d\\geq N n^{2}$ for $\\delta>0$ to hold. It is crucial to emphasize thefundamental distinction in convergence outcomes between Transformers employing Gaussian kernel attention and those utilizingSoftmaxattentionunder these conditions. With equivalent initialization conditions,training Transformers equipped with Gaussian kernel attention achieves global convergence using gradient descent $(G D)$ Second, it is essential to emphasizethatthedimensionsize $D d\\bar{\\geq N n}^{2}$ is similar to the findings of works that have analyzed the convergence performance of over-parameterized neural networks Allen-Zhu et al. [2019], Du et al. [2019]. The total number of samples, consisting of $N$ samples each with n tokens, can be calculated as Nn. Meanwhile, the total feature dimension is $D d$ The inequality implies that the width of the parameters is at least $\\mathcal{O}(N)$ ,a relationship also illustrated in Nguyen and Mondelli [2020]. ", "page_idx": 6}, {"type": "text", "text": "In part (2), we demonstrate that the PL condition does not hold. In particular, we identify an initial solution that satisfies all the conditions given in Theorem 3, yet fails to satisfy the PL condition. Therefore, in this case, GD leads to vanishing gradients without being able to find a global optimal solution. The details of this specific example are provided below. ", "page_idx": 6}, {"type": "text", "text": "Example: Consider Transformer with Softmax attention, and $N=1,n=2,H=1$ . Let us first write down the close form of the gradient over $W_{1}^{Q}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\partial f\\left(M_{0};X_{1}\\right)}{\\partial W_{1}^{Q}}=\\frac{1}{\\sqrt{d}}X_{1}^{\\top}\\frac{\\partial f\\left(M_{0};X_{1}\\right)}{\\partial C_{11}}X_{1}W_{1,0}^{K}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Next, we show there exists $W^{O},W^{V},X_{1},W_{1,0}^{Q},W_{1,0}^{K}$ such hathe los futionison-ero with Equation (12) satisfied, while ", "page_idx": 7}, {"type": "text", "text": "$\\frac{\\partial f\\left(M_{0};X_{1}\\right)}{\\partial C_{11}}=\\mathbf{0}\\in\\mathbb{R}^{2\\times2}.$ Denote $\\begin{array}{r}{L:=\\frac{\\partial f(M_{0};X_{1})}{\\partial\\mathbb{M}\\mathbb{H}(M_{0};X_{1})}\\left(W^{O}\\right)^{\\top}\\left(X_{1}W_{0}^{V}\\right)^{\\top}\\in\\mathbb{R}^{2\\times2}}\\end{array}$ E R2\u00d72. 0f(Mo;X1) can be expressed as follows:/ 0C11 $\\left(\\frac{\\partial f\\left(M_{0};X_{1}\\right)}{\\partial C_{11}}\\right)_{11}=\\delta\\cdot\\left(L_{11}-L_{12}\\right),\\ \\left(\\frac{\\partial f\\left(M_{0};X_{1}\\right)}{\\partial C_{11}}\\right)_{12}=\\delta\\cdot\\left(L_{12}-L_{11}\\right),\\delta$ is some constant. Next, we will give the value of $W^{O},W_{0}^{V}$ to show the case where GD leads to vanishing gradient. Let $D=d=2$ $\\begin{array}{r}{W^{O}=(\\frac{1}{a},\\frac{1}{a}),X_{1}=\\binom{1}{0}\\quad}\\end{array}$ , and $W_{0}^{V}=\\left({2a\\atop a}\\quad{a\\atop2a}\\right)$ , where $a$ is a constant. It is easyto show that therexists $W_{1}^{Q}$ and $W_{1}^{K}$ such that Equation (12) holds. Further, itis easy to verify that for this scenario, the following holds: ", "page_idx": 7}, {"type": "equation", "text": "$$\nL_{11}=L_{12},L_{21}=L_{22}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Next, we can easily deduce that $\\begin{array}{r}{\\left(\\frac{\\partial f(M_{0};X_{1})}{\\partial C_{11}}\\right)_{11}=\\left(\\frac{\\partial f(M_{0};X_{1})}{\\partial C_{11}}\\right)_{12}=0.}\\end{array}$ . Similarly, we can demonstrate that $\\begin{array}{r}{\\left(\\frac{\\partial f(M_{0};X_{1})}{\\partial C_{11}}\\right)_{21}=\\left(\\frac{\\partial f(M_{0};X_{1})}{\\partial C_{11}}\\right)_{22}=0}\\end{array}$ Consequenly we ave $\\begin{array}{r}{\\frac{\\partial f(M_{0};X_{1})}{\\partial W_{1}^{Q}}=\\mathbf{0}}\\end{array}$ However, $y_{1}$ satisfies that $\\begin{array}{r}{\\frac{\\partial f(M_{0};X_{1})}{\\partial\\sf M H}\\frac{\\partial\\sf}{\\partial\\sf M_{0};X_{1}}\\ne{\\bf0}}\\end{array}$ it fllows $f(M_{0};X_{1})\\neq0$ , which means $M_{0}$ is not global optimal solution. ", "page_idx": 7}, {"type": "text", "text": "5  Experiment: Softmax v.s. Gaussian ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present numerical results to illustrate the behaviors of Transformers models with Softmax attention and Gaussian kernel attention across various tasks. ", "page_idx": 7}, {"type": "text", "text": "5.1 Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We investigate two distinct tasks: Text Classification using the IMDb review dataset [Maas et al., 2011] and Pathfinder [Linsley et al., 2018]. While both tasks involve processing ", "page_idx": 7}, {"type": "image", "img_path": "XswQeLjJo5/tmp/55baa5dd32f68c5db7f0f9533d82e3b3d078ce559dbffe8b7b1a8f3a9e217d30.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Test performance on text classification task with different attention kernels ", "page_idx": 7}, {"type": "text", "text": "long sequences, they exhibit different characteristics. Text Classification is a well-known NLP task that focuses on discerning relationships among token embeddings, while the Pathfinder task prioritizes capturing spatial information within the input pixels. ", "page_idx": 7}, {"type": "text", "text": "5.2  Model and Experiment Method ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We follow the experiment setting in [Chen et al., 2021]. For both tasks, we employ a 2-layer Transformer model with the following specifications: embedding dimension $D\\,=\\,64$ , hidden dimension $d=128$ , and number of attention heads $H=2$ . To align the model with the classification task, we use an additional mean pooling layer as the final layer. We determine the batch size based on available memory constraints. Specifically, we set a batch size of 16 for the Text Classifi", "page_idx": 7}, {"type": "image", "img_path": "XswQeLjJo5/tmp/4def69f848575de7fb99bfad2f914753e3970b97ff53762e27a758260d707614.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Test performance on pathfinder task with different attentionkernels ", "page_idx": 7}, {"type": "text", "text": "cation task with a learning rate of $1\\times10^{-4}$ , and a batch size of 128 for the Pathfinder task with a learning rate of $2\\times10^{-4}$ . For optimization, we use Stochastic Gradient Descent (SGD) for the Text Classification task and Adam for the Pathfinder task. We conduct two types of experiments. ", "page_idx": 7}, {"type": "text", "text": "In the first experiment, we plot the test accuracy and test loss within the training steps with both Softmax and Gaussian kernel attention on both tasks. We repeat the training for 10 times and make the shadow plot on the test performance. ", "page_idx": 8}, {"type": "image", "img_path": "XswQeLjJo5/tmp/dfa9c9c80499c7bfb04b428dff97745cdf149cf26db331f590fa6fdff0a76f09.jpg", "img_caption": ["Figure 4: The loss landscapes on text classification task and Pathfinder task. For both tasks, we use the two-stage training in Section 5.2 with the same training hyperparameters, while the only difference is the attention structure in the second training stage. The two axes represent the two directions $d_{1}$ and $d_{2}$ as defined in Section 5.2. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In our second experiment, the training process consists of two stages: In the first stage, we train the Transformer model equipped with Softmax attention (defined in Equation (3)) for 8,000 steps. In the second stage, we continue training from the pre-trained model for an additional 500 steps, with the option of using either Softmax or Gaussian kernel. To explore the optimization landscape around the trained model, we employed a technique inspired by Li et al. [2018]. We select two parameter directions, specifically the $\\boldsymbol{W}^{Q}$ and $W^{\\dot{K}}$ matrices in the first Transformer layer. These two directions, denoted as $d_{1},d_{2}$ , are centered at the trained model $M$ , and represent the parameter space of $W^{Q},\\dot{W}^{K}$ , respectively. We evaluate the loss function on the set $\\{\\bar{M}+0.02(r\\stackrel{-}{-}25)d_{1}+$ $0.02(s-25)d_{2}\\}$ , where $r,s\\in[1,2,\\cdots\\,,50]$ . The above set is the neighborhood of the trained model $M$ , and we chose the evaluation stepsize as 0.02 along the two directions $d_{1},d_{2}$ , with the total steps limit as 100. Within this parameter space, we plot a 3-D surface representing the landscape around the trained model. ", "page_idx": 8}, {"type": "text", "text": "5.3 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.3.1 Test Loss & Accuracy Curve comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To begin with, we present some observations in our first experiment. We plot the test performance of these two tasks on Transformers with two different types of attention. From Fig 2 and Fig 3, we can conclude that in both tasks, Transformers with Gaussian kernel attention exhibit faster convergence and higher test accuracy than Softmax attention with the same model size and learning rate. Especially, training Transformers with Softmax attention in the Pathfinder task can lead to unstable performance as indicated in Fig 3. The test accuracy has a significantly higher variance at the same training epoch. Further, the worst test accuracy after 20, 000 epochs is around 0.58 for the Softmax attention Transformer, compared with 0.62 for the Gaussian kernel Transformer. These observations align with the experiment results in [Chen et al., 2021] and [Tay et al., 2020], where Transformers with different attention kernels are trained with the same model size and learning rate, while Softmax attention Transformers show instability in a few tasks. ", "page_idx": 8}, {"type": "text", "text": "5.3.2  Optimization Landscape Comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 4, we present a comparison of the optimization landscape between Transformers with Softmax and Gaussian kernel attention. Notably, we observe distinct differences in the training landscapes of these two attention types for both tasks. We follow the visualization method described in Section 5.2. We conduct a visualization of the optimization landscape around the trained models after a two-stage training process, with identical learning rates, network sizes, and training epochs. Keeping all other factors consistent, the disparity in the landscape provides a direct representation of the difference in the attention structure during the optimization procedure. With Softmax attention, the landscape appears more complicated compared with Gaussian kernel attention. This complexity can be interpreted as the presence of a greater number of local optima in the optimization landscape, suggesting that Transformers utilizing Softmax attention may encounter more challenges in reaching global optimal solutions. In contrast, the landscape with the Gaussian kernel is flatter. This observation aligns with our earlier findings in Figure 2 and Figure 3, where Softmax attention exhibited certain convergence issues. These observations also provide empirical evidence supporting our Theorem 3, which reflects in a slightly different perspective the complicated optimization landscape within the Softmaxkernel. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, our study addresses critical gaps in our understanding of why Transformer models perform exceptionally well in a variety of machine learning tasks. Our work also provides a nuanced understanding of the advantages and disadvantages of using classical Softmax attention in Transformers. We find that while shallow Softmax attention Transformers can achieve global convergence with overparameterization, there are scenarios where this attention structure can lead to local solutions. However, those issues can be mitigated by the Gaussian kernel-based attention. In our work, we need strong initialization and large embedding size, i.e, $H D\\geq N n$ to obtain theglobal convergence, which exhibits a gap towards real case. In the future work, we will investigate how to relax the assumptions. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of B. Song was partially done while interning at Amazon Web Services. M. Hong holds concurrent appointments as an Amazon Scholar and as a faculty at the University of Minnesota. This paper describes their work performed at Amazon. The work of Jie Ding was supported in part by the Army Research Office Early Career Program Award under grant number W911NF2310315. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242-252. PMLR, 2019.   \nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.   \nS. Bhojanapalli, C. Yun, A. S. Rawat, S. Reddi, and S. Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020.   \nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.   \nY. Chen, Q. Zeng, H. Ji, and Y. Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr\\\" om method. Advances in Neural Information Processing Systems, 34:2122-2135, 2021.   \nM. Danilova, P. Dvurechensky, A. Gasnikov, E. Gorbunov, S. Guminov, D. Kamzolov, and I. Shibaev. Recent theoretical advances in non-convex optimization. In High-Dimensional Optimization and Probability: With a View Towards Data Science, pages 79-163. Springer, 2022.   \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805, 2018.   \nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \nS. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675-1685. PMLR, 2019.   \nP. He, X. Liu, J. Gao, and W. Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.   \nX. S. Huang, F. Perez, J. Ba, and M. Volkovs. Improving transformer optimization through better initialization. In International Conference on Machine Learning, pages 4475-4483. PMLR, 2020.   \nY. Huang, Y Cheng, and Y. Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.   \nP. Jain, P. Kar, et al. Non-convex optimization for machine learning. Foundations and Trends@ in Machine Learning, 10(3-4):142-363, 2017.   \nC. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points. Journal of the ACM (JACM), 68(2):1-29, 2021.   \nP. Langley. Crafting papers on machine learning. In P. Langley, editor, Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pages 1207-1216, Stanford, CA, 2000. Morgan Kaufmann.   \nG. Li, G. Wang, and J. Ding. Provable identifiability of two-layer relu neural networks via lasso regularization. IEEE Transactions on Information Theory, 2023.   \nH. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \nD. Linsley, J. Kim, V. Veerabadran, C. Windolf and T. Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. Advances in neural information processing systems, 31, 2018.   \nL. Liu, X. Liu, J. Gao, W. Chen, and J. Han. Understanding the diffculty of training transformers. arXiv preprint arXiv:2004.08249, 2020.   \nY. Liu, M. Ot, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \nA. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. $\\mathrm{Ng}$ , and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150, 2011.   \nQ. N. Nguyen and M. Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal topology. Advances in Neural Information Processing Systems, 33:11961-11972, 2020.   \nL. Noci, S. Anagnostidis, L. Biggio, A. Orvieto, S. P. Singh, and A. Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022.   \nY. Pan and Y. Li. Toward understanding why adam converges faster than sgd for transformers. arXiv preprint arXiv:2306.00204, 2023.   \nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nN. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   \nY. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for effcient transformers. arXiv preprint arXiv:201l.04006, 2020.   \nY. Tian, Y Wang, B. Chen, and S. Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is allyou need. Advances in neural information processing systems, 30, 2017.   \nY. Wu, F. Liu, G. Chrysos, and V. Cevher. On the convergence of encoder-only shallow transformers. Advances in Neural Information Processing Systems, 36, 2024.   \nR. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "1 Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1.1  Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Recall that we have defined the structure of a single Transformer model in Equation (1) and Equation (5). We will further define a few notations before we introduce a few useful lemmas that are needed in our proof. ", "page_idx": 12}, {"type": "text", "text": "(1) Operator: Denote $\\operatorname{vec}({\\cdot})$ as the vectorization operator on a matrix; $\\otimes$ as Kronecker product operator; $\\odot$ as the element product. Denote $\\Upsilon(\\cdot)$ as a matrix operator, such that for any matrix $X$ without zero element ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\Upsilon\\left(X_{m\\times n}\\right)=\\left[\\begin{array}{c c c}{{1/x_{11}}}&{{\\cdots}}&{{1/x_{1n}}}\\\\ {{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{1/x_{m1}}}&{{\\cdots}}&{{1/x_{m n}}}\\end{array}\\right]}_{m\\times n}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "(2) Matrix: Denote $\\mathbb{I}$ as the identity matrix. Define matrix $\\mathbb{E}$ and $E$ as following: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}=\\left[\\begin{array}{l l l}{E}&&\\\\ &{\\ddots}&\\\\ &&{E}\\end{array}\\right]_{H n\\times H n},\\quad E=\\left[\\begin{array}{l l l}{1}&{\\ddots}&{1}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {1}&{\\ddots}&{1}\\end{array}\\right]_{n\\times n}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Define matrix $\\mathbb{P}_{h}$ as following: $\\mathbb{P}_{h}=\\left(\\ldots,E_{n\\times n}^{h},\\ldots\\right)$ Enxn....,h=I....,H. ", "page_idx": 12}, {"type": "text", "text": "(3) Matrix in Transformer: Define the following matrix $C$ related to the attention layer Softmax kernel: ", "page_idx": 12}, {"type": "equation", "text": "$$\nC_{i h}=\\frac{X_{i}W_{h}^{Q}\\left(X_{i}W_{h}^{K}\\right)^{\\top}}{\\sqrt{d}},\\ S_{i h}=\\mathrm{Softmax}(C_{i h})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Gaussian kernel: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(C_{i h})_{k j}=-\\frac{\\displaystyle\\left\\|X_{i k},W_{h}^{Q}-X_{i j},W_{h}^{K}\\right\\|^{2}}{2\\sqrt{d}},\\;(S_{i h})_{k j}=\\exp\\left((C_{i h})_{k j}\\right)}}\\\\ {{C_{i}=[C_{i1},\\cdots,C_{i H}],\\;S_{i}=[S_{i1},\\cdots,S_{i H}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Define matrix $V_{i}^{\\prime}$ for each data $X_{i}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\nV_{i}^{\\prime}=\\left[\\begin{array}{c c c c}{X_{i}W_{1}^{V}}&&&\\\\ &{\\ddots}&&\\\\ &&{X_{i}W_{H}^{V}}\\end{array}\\right]_{H n\\times d},~V=[V_{1}^{\\top},\\cdot\\cdot\\cdot~,V_{N}^{\\top}]^{\\top}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Next, let us introduce several useful lemma which leads to Theorem 2: ", "page_idx": 12}, {"type": "text", "text": "1.2 Lemmas of Theorem 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1. ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial f\\left(M;X\\right)}{\\partial W^{V}}=B^{\\top}\\left(\\mathbb{M}\\mathbb{H}\\left(M;X\\right)-y\\right)\\left(W^{O}\\right)^{\\top}}\\\\ &{\\mathrm{~wec~}\\left(\\frac{\\partial f\\left(M;X\\right)}{\\partial W^{V}}\\right)=\\left\\langle\\left(W^{O}\\right)^{\\top}\\otimes B,\\,\\mathrm{wec}(\\mathbb{M}\\mathbb{H}\\left(M;X\\right)-y)\\right\\rangle}\\\\ &{=\\left(\\mathbb{I}\\mu\\otimes B^{T}\\right)\\cdot\\left(W^{O}\\otimes\\mathbb{I}_{N}\\right)\\cdot\\left(\\mathbb{M}\\mathbb{H}\\left(M;X\\right)-y\\right)}\\\\ &{\\frac{\\partial f\\left(M;X\\right)}{\\partial W_{R}^{G}}=\\frac{1}{\\sqrt{d}}X^{\\top}\\mathbb{P}_{h}\\frac{\\partial f\\left(M;X\\right)}{\\partial C}X W_{h}^{K}=\\underbrace{N}_{i=1}^{N}\\frac{1}{\\sqrt{d}}X_{i}^{\\top}\\mathbb{P}_{h}\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}X_{i}W_{h}^{K}}\\\\ &{\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}=\\left(\\left(\\mathbb{M}\\mathbb{H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}}\\\\ &{-\\left(\\left(\\left(\\mathbb{H}\\left(\\mathbb{H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}\\odot\\mathbb{T}\\left(\\left(\\exp C_{i}\\mathbb{E}\\right)\\right)\\mathbb{E}^{\\top}\\right)\\odot\\mathrm{exp}\\,C_{i}}\\\\ &{\\frac{\\partial f\\left(M;X\\right)}{\\partial C}=\\mathrm{diag}\\left(\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{1}},\\cdots,\\frac{\\partial f\\left(M;X_{N}\\right)}{\\partial C_{N}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Remark: The above lemma derives the closed form of the gradient of objective over $W^{V},W^{Q}$ Notice that we can derive the derivative of $W^{K}$ in the same way as $W^{Q}$ due to symmetry, sowe do not include the derivation here. Some of the lemmas here refers https://say-hello2y.github.io/2022- 09-07/attention-gradient ", "page_idx": 13}, {"type": "text", "text": "Lemma 2. Consider updating $W^{Q},W^{K},W^{V}$ at iteration $t$ Suppose $\\sigma_{\\operatorname*{max}}(W^{Q})$ $\\sigma_{\\operatorname*{max}}(W^{K})$ $\\sigma_{\\operatorname*{max}}(W^{V})$ are bounded during in the optimization phase, then we have the following conclusion: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|d(S_{i})\\|_{F}\\leq\\phi_{i}\\|d(W^{Q})\\|_{F},\\quad\\boldsymbol{w h e r e}~\\phi_{i}=\\frac{n}{\\sqrt d}\\left\\|X_{i}\\right\\|_{F}^{2}\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}}\\\\ {\\displaystyle\\|d(S_{i})\\|_{F}\\leq\\psi_{i}\\|d(W^{K})\\|_{F},\\quad\\boldsymbol{w h e r e}~\\psi_{i}=\\frac{n}{\\sqrt d}\\left\\|X_{i}\\right\\|_{F}^{2}\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{Q}\\right)}.}\\\\ {\\displaystyle\\|d(S_{i})\\|_{F}\\leq\\sqrt{\\phi_{i}^{2}+\\psi_{i}^{2}}\\cdot\\|d(W^{Q}),d(W^{K})\\|_{F}.}\\\\ {\\displaystyle\\|\\frac{\\partial f(M;X_{i})}{\\partial W^{Q}}\\|_{F}\\leq Q_{i}\\|\\mathbb{M}\\|\\left(M;X_{i}\\right)-y_{i}\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 3. Consider updating $W^{Q},W^{K},W^{V}$ at iteration $t$ Suppose $\\sigma_{\\operatorname*{max}}(W^{Q})$ $\\sigma_{\\operatorname*{max}}(W^{K})$ \uff0c $\\sigma_{\\operatorname*{max}}(W^{V})$ are bounded during in the optimization phase, then we have the following conclusion: (1) IIMH(Mt+1; X) - MH(Mt; X)IIF \u2264 ZIIMt+1 \uff0d MtllF, where $Z$ is some positive constant. (32) (2) ${\\|\\nabla f\\left(M_{t+1};X\\right)-\\nabla f\\left(M_{t};X\\right)\\|_{2}}\\leq G{\\|M_{t+1}-M_{t}\\|_{F}},w h e r e\\;G\\;i s\\;s o n$ nepositiveconstant. ", "page_idx": 13}, {"type": "text", "text": "Lemma 4. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a second order differentiable function. Let $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ be given, and assume that $\\|\\nabla f(z)-\\nabla f(x)\\|_{2}\\leq C\\|z-x\\|_{2}$ for every $z=x+t(y-x)$ with $t\\in[0,1]$ . Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(y)\\leq f(x)+\\langle\\nabla f(x),y-x\\rangle+\\frac{C^{\\prime}}{2}\\|x-y\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 5. For matrix $A\\in\\mathbb{R}^{k\\times l},B\\in\\mathbb{R}^{l\\times m},C\\in\\mathbb{R}^{m\\times n}.$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{vec}(A B C)=\\left(I_{n}\\otimes A B\\right)\\mathrm{vec}(C)=\\left(C^{\\mathrm{T}}B^{\\mathrm{T}}\\otimes I_{k}\\right)\\mathrm{vec}(A)}\\\\ &{\\quad\\mathrm{vec}(A B)=\\left(I_{m}\\otimes A\\right)\\mathrm{vec}(B)=\\left(B^{\\mathrm{T}}\\otimes I_{k}\\right)\\mathrm{vec}(A)}\\\\ &{\\mathrm{vec}(A\\odot B)=\\mathrm{vec}(A)\\odot\\mathrm{vec}(B).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "1.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof Sketch of Theorem 2: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The main idea of the proof follows from [Nguyen and Mondelli, 2020]. Let us first recall a few notations. $\\begin{array}{r}{\\bar{\\lambda}^{V}:=\\frac{2}{3}\\big(1\\!+\\!\\sigma_{\\operatorname*{max}}(W_{0}^{V})\\big)}\\end{array}$ $\\underline{{\\lambda}}^{B}:=\\sigma_{\\operatorname*{min}}(B_{0})$ . Using GD update rule, we aim to iteratively show ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\sigma_{\\operatorname*{max}}(W_{r}^{V})\\leq\\frac{3}{2}\\bar{\\lambda}^{V},r\\in\\{0,\\ldots,t\\},}\\\\ {\\sigma_{\\operatorname*{max}}(W_{r}^{Q})\\leq\\frac{3}{2}\\bar{\\lambda}^{Q},r\\in\\{0,\\ldots,t\\},}\\\\ {\\sigma_{\\operatorname*{max}}(W_{r}^{K})\\leq\\frac{3}{2}\\bar{\\lambda}^{K},r\\in\\{0,\\ldots,t\\},}\\\\ {\\sigma_{\\operatorname*{min}}\\left(B_{r}\\right)\\geq\\frac{1}{2}\\underline{{\\lambda}}^{B},r\\in\\{0,\\ldots,t\\},}\\\\ {f\\left(M_{r};X\\right)\\leq\\left(1-\\eta\\mu\\right)^{r}f\\left(M_{0},X\\right),\\;r\\in\\{0,\\ldots,t\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Denote $\\mu\\,:=\\,{\\textstyle\\frac{1}{4}}(\\underline{{{\\lambda}}}^{B})^{2}\\|W^{O}\\|_{2}^{2}$ . Let us discuss about the value of $\\mu$ .We know $W^{O}\\,\\in\\,\\mathbb{R}^{H d\\times1}$ $B_{0}^{\\top}\\in\\mathbb{R}^{H D\\times N n}$ We require $\\mu>0$ i.e. $\\underline{{\\lambda}}^{B}>0$ which implies $B$ has full row rank For simplicity, let us consider the $H=1$ case. Recall the definition of $B$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nB:=\\left(\\begin{array}{c}{S_{11}X_{1}}\\\\ {\\cdot\\cdot\\cdot}\\\\ {S_{N1}X_{N}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Suppose we initialize $W_{1}^{Q},W_{1}^{K}$ such that each $S_{i1}\\in\\mathbb{R}^{n\\times n}$ is full rank, then we can easily show that $\\operatorname{rank}(S_{i1}X_{i})\\,=\\,n$ if $X_{i}$ has full row rank. Suppose embedding dimension $D$ is large, with certain assumption on $X$ ,we can show $B$ has full row rank. For example, if each $X_{i}$ follows standard Gaussian distribution with $D>>N$ , then $\\mathrm{rank}(B)=N n$ with probability 1 if we initialize $W_{1}^{Q},W_{1}^{K}$ such that $S_{i1}$ is full rank. ", "page_idx": 14}, {"type": "text", "text": "Further, let us assume that $\\sum_{h=1}^{H}(\\bar{\\lambda}_{h}^{Q})^{2}>1,\\sum_{h=1}^{H}(\\bar{\\lambda}_{h}^{K})^{2}>1.$ and initialization condition satisfies: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{54n^{2}\\sqrt{N H}\\|X\\|_{F}^{6}\\bar{\\lambda}^{V}\\big(\\sum_{h=1}^{H}(\\bar{\\lambda}_{h}^{Q})^{2}+(\\bar{\\lambda}_{h}^{K})^{2}\\big)}{(\\underline{{\\lambda}}^{B})^{2}\\|W^{O}\\|_{2}\\operatorname*{min}\\big(\\bar{\\lambda}_{h}^{Q},\\bar{\\lambda}_{h}^{K},1,\\underline{{\\lambda}}^{B}\\big)}\\le1\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Remark 5. The initialization condition can be satisfied if $\\|\\boldsymbol{W}^{O}\\|_{2}$ is large and $\\sigma_{\\operatorname*{max}}(W^{V})$ is small. v in Equation (8) is 54\u00b7 ", "page_idx": 14}, {"type": "text", "text": "It is clear that Equation (34) holds when $t=0$ . Suppose it holds at iteration $t$ , we prove it holds at iteration $t+1$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\big|W_{r+1}^{V}-W_{0}^{V}\\big|\\big\\|_{F}\\overset{(i)}{\\leq}\\sum_{s=0}^{r}\\big\\|W_{r+1}^{V}-W_{r}^{V}\\big\\|_{F}=\\eta\\sum_{s=0}^{r}\\|\\nabla_{W^{V}}f\\left(M_{t};X\\right)\\|_{F}}\\\\ &{\\overset{(i i)}{\\leq}\\eta\\displaystyle\\sum_{s=0}^{r}\\|B_{r}\\|_{F}\\|W^{O}\\|_{2}\\,\\|{\\sf M}\\mathsf{H}(M_{r};X)-y\\|_{2}\\overset{(i i i)}{\\leq}\\eta\\|B_{r}\\|_{F}\\|W^{O}\\|_{2}\\sum_{s=0}^{r}\\left(1-\\eta\\mu\\right)^{s/2}\\|{\\sf M}\\mathsf{H}(M_{0};X)-y\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (i) uses the triangle inequality; (i) plugs in the expression of $\\nabla_{W^{V}}f\\left(M_{t};X\\right)$ and uses the Cauchy-Schwartz inequality; (ii) is because we assume the loss function $f(\\cdot)$ linearly decreases until $t$ -th iteration. Let $u=\\sqrt{1-\\eta\\mu}$ .Sowehave ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\left\\|{B_{r}}\\right\\|_{F}\\left\\|{W^{O}}\\right\\|_{2}\\displaystyle\\sum_{s=0}(1-\\eta\\mu)^{s/2}\\left\\|{\\mathbb{M}}\\mathsf{H}\\left(M_{0};X\\right)-y\\right\\|_{2}}\\\\ &{\\leq\\displaystyle\\frac1\\mu\\|{B_{r}}\\|{F}\\|{W^{O}}\\|\\displaystyle\\frac{1-u^{r+1}}{1-u}(1-u^{2})\\left\\|{\\mathbb{M}}\\mathsf{H}(M_{0};X)-y\\right\\|_{2}}\\\\ &{=\\displaystyle\\frac1\\mu\\|\\left[S_{r,1}X_{1},\\cdots,S_{r,N}X_{N}\\right]\\|_{F}\\|{W^{O}}\\|\\displaystyle\\frac{1-u^{r+1}}{1-u}(1-u^{2})\\left\\|{\\mathbb{M}}\\mathsf{H}(M_{0};X)-y\\right\\|_{2}}\\\\ &{\\overset{(i)}{\\leq}\\displaystyle\\frac{2n\\sqrt{H N}}{\\mu}\\|X\\|_{F}\\|{W^{O}}\\|_{2}\\|\\mathsf{M}\\mathsf{H}(M_{0};X)-y\\|_{F}\\overset{(i i)}{\\leq}1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (i) is because each element in $S_{r,i}$ has magnitude at most 1 and $\\|S_{r,i}\\|_{F}\\le n\\sqrt{H}$ , then by Cuachy-Schwartz inequality, we have $\\|B\\|_{r}\\leq\\sqrt{H N}\\|X\\|_{F}$ (i) is due to the initialization condition. Then by Weyl's inequality, there is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sigma_{\\operatorname*{max}}\\left(W_{r+1}^{V}\\right)\\leq\\sigma_{\\operatorname*{max}}(W_{0}^{V})+1=\\frac32\\bar{\\lambda}^{V}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, let us derive the upper bound for $\\sigma_{\\operatorname*{max}}(W_{h,r}^{Q})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|W_{h,r+1}^{Q}-W_{h,0}^{Q}\\right|\\right|_{F}\\stackrel{(i)}{\\leq}\\displaystyle\\sum_{s=0}^{r}\\left\\|\\mathbb{W}_{h,r+1}^{Q}-W_{h,r}^{Q}\\right\\|_{F}=\\eta\\displaystyle\\sum_{s=0}^{r}\\left\\|\\nabla_{W_{h}^{\\alpha}}f\\left(M_{t};X\\right)\\right\\|_{F}}\\\\ &{\\stackrel{(i i)}{\\leq}\\eta\\displaystyle\\sum_{s=0}^{r}\\left\\|\\sum_{i=1}^{N}Q_{i}^{2}\\left\\|\\mathbb{M}\\mathbb{H}\\left(M_{r};X\\right)-y\\right\\|_{2}\\stackrel{(i i)}{\\leq}\\eta\\displaystyle\\sqrt{\\sum_{i=1}^{N}Q_{i}^{2}\\sum_{s=0}^{r}(1-\\eta\\mu)^{s/2}\\left\\|\\mathbb{M}\\mathbb{H}\\left(M_{0};X\\right)-y\\right\\|_{2}}}\\\\ &{\\leq\\displaystyle\\sum_{\\mu=1}^{\\sqrt{\\sum_{i=1}^{N}Q_{i}^{2}}}\\frac{1-u^{r+1}}{1-u}\\left(1-u^{2}\\right)\\left\\|\\mathbb{M}\\mathbb{H}\\left(M_{0};X\\right)-y\\right\\|_{2}}\\\\ &{\\leq\\displaystyle\\frac{2\\sqrt{\\sum_{i=1}^{N}Q_{i}^{2}}}{\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (i) uses triangle inequality; (i) uses Lemma 2 (4); (i) comes from the assumption that loss function $f(\\cdot)$ linearly decreases until $t$ -th iteration; (iv) is due to the initialization condition Equation (35). Similarly, we can show ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta\\sqrt{\\displaystyle\\sum_{i=1}^{N}K_{i}^{2}\\sum_{s=0}^{r}(1-\\eta\\mu)^{s/2}\\left\\|\\mathsf{M}\\mathsf{H}\\left(M_{0};X\\right)-y\\right\\|_{2}}}\\\\ &{\\leq\\frac{2\\sqrt{\\displaystyle\\sum_{i=1}^{N}K_{i}^{2}}}{\\mu}\\left\\|\\mathsf{M}\\mathsf{H}\\left(M_{0};X\\right)-y\\right\\|_{2}\\leq\\frac{1}{2}\\bar{\\lambda}_{h}^{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then by Weyl's inequality, there is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tau_{\\operatorname*{max}}\\left(W_{h,t+1}^{Q}\\right)\\leq\\sigma_{\\operatorname*{max}}(W_{h,0}^{Q})+\\frac12\\bar{\\lambda}_{h}^{Q}=\\frac32\\bar{\\lambda}_{h}^{Q};\\;\\sigma_{\\operatorname*{max}}\\left(W_{h,t+1}^{K}\\right)\\leq\\sigma_{\\operatorname*{max}}(W_{h,0}^{K})+\\frac12\\bar{\\lambda}_{h}^{K}=\\frac32\\bar{\\lambda}_{h}^{K}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we aim to bound the eigenvalues of $B_{r+1}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|B_{r+1}-B_{0}\\|_{F}\\leq\\sum_{s=0}^{r}\\|B_{s+1}-B_{s}\\|_{F}=\\displaystyle\\sum_{i=1}^{N}\\sum_{s=0}^{r}\\|S_{i,s+1}X_{i}-S_{i,s}X_{i}\\|_{F}}}\\\\ &{\\stackrel{(i)}{\\leq}\\displaystyle\\sum_{i=1}^{N}\\sum_{s=0}^{r}\\|X_{i}\\|_{F}\\|S_{i,s+1}-S_{i,s}\\|_{F}\\stackrel{(i i)}{\\leq}\\sum_{i=1}^{N}\\sum_{s=0}^{r}\\|X_{i}\\|_{F}\\displaystyle\\sum_{h=1}^{H}\\|S_{i h,s+1}-S_{i h,s}\\|_{F}}\\\\ &{\\stackrel{(i i i)}{\\leq}\\eta\\displaystyle\\sum_{i=1}^{N}\\sum_{s=0}^{r}\\|X_{i}\\|_{F}\\cdot\\sqrt{\\phi_{i}^{2}+\\psi_{i}^{2}}\\cdot\\|\\left(\\nabla_{W^{Q}}f(M_{s};X_{i}),\\nabla_{W^{K}}f(M_{s};X_{i})\\right)\\|_{F}\\;\\;,}\\\\ &{\\stackrel{(i v)}{\\leq}\\displaystyle\\sum_{i=1}^{N}\\sum_{s=0}^{r}\\|X_{i}\\|_{F}\\cdot\\sqrt{\\phi_{i}^{2}+\\psi_{2}^{2}}\\cdot\\sqrt{Q_{i}^{2}+K_{i}^{2}}\\cdot\\|\\mathbb{M}\\|\\mathcal{M}(M_{s},X_{i})-y_{i}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overset{(v)}{\\leq}\\eta\\sum_{s=0}^{r}\\sqrt{\\sum_{i=1}^{N}\\|X_{i}\\|_{F}^{2}(\\phi_{i}^{2}+\\psi_{i}^{2})(Q_{i}^{2}+K_{i}^{2})(1-\\eta\\mu)^{s/2}\\left\\|\\mathsf{M}\\mathsf{H}\\left(M_{0};X\\right)-y\\right\\|_{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (i) and (i) uses triangle inequality and Cauchy-Schwartz inequality; (i) comes from Lemma 2 (5); (iv) uses Lemma 2 and Cauchy-Schwartz inequality; (v) comes from Cauchy-Schwartz inequality. Together with our initialization condition, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|B_{r+1}-B_{0}\\|_{F}\\leq\\frac{1}{\\mu}\\sqrt{\\sum_{i=1}^{N}\\|X_{i}\\|_{F}^{2}(\\phi_{i}^{2}+\\psi_{i}^{2})(Q_{i}^{2}+K_{i}^{2})}\\cdot\\frac{1-u^{r+1}}{1-u}\\left\\|\\mathsf{M}\\mathsf{H}\\left(M_{0};X\\right)-y\\right\\|_{2}}\\\\ &{\\leq\\displaystyle\\frac{2}{\\mu}\\sqrt{\\sum_{i=1}^{N}\\|X_{i}\\|_{F}^{2}(\\phi_{i}^{2}+\\psi_{i}^{2})(Q_{i}^{2}+K_{i}^{2})}\\left\\|\\mathsf{M}\\mathsf{H}\\left(M_{0};X\\right)-y\\right\\|_{2}\\overset{(i)}{\\leq}\\frac{1}{2}\\lambda^{B},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (i) comes from the initialization condition 35. By Weyl's inequality, we can derive the bound for the singular values of $B_{t}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}(B_{r+1})\\geq\\sigma_{\\mathrm{min}}(B_{0})-\\|B_{r+1}-B_{0}\\|_{F}\\geq\\frac{1}{2}\\underline{{\\lambda}}^{B}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The  final  step  is  to  show  the  last  inequality  holds.  Since  we  have  already  showed $\\sigma_{\\operatorname*{max}}(W_{h}^{Q}),\\sigma_{\\operatorname*{max}}^{-}(W_{h}^{K}),\\sigma_{\\operatorname*{max}}(W_{h}^{V})$ are bounded, by Lemma 3 (2) we can conclude that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f\\left(M_{t+1};X\\right)-\\nabla f\\left(M_{t}\\right)\\|_{2}\\leq G\\|M_{t+1}-M_{t}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus by Lemma 4, we choose $\\begin{array}{r}{\\eta<\\frac{1}{2G}}\\end{array}$ , then the following hold true: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f\\left(M_{t+1;\\cdot}X\\right)=f\\left(M_{t}-\\eta\\nabla f\\left(M_{t};X\\right);X\\right)}\\\\ {\\overset{(i)}{\\leq}\\mathcal{G}\\left(M_{t};X\\right)-\\eta\\left\\Vert\\nabla f\\left(M_{t};X\\right)\\right\\Vert^{2}+\\frac{G}{2}\\eta^{2}\\left\\Vert\\nabla f\\left(M_{t};X\\right)\\right\\Vert^{2}}\\\\ {\\overset{(i i)}{\\leq}\\mathcal{G}\\left(M_{t};X\\right)-\\frac{1}{2}\\eta\\left\\Vert\\nabla f\\left(M_{t};X\\right)\\right\\Vert^{2}}\\\\ {\\overset{(i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{2}\\eta\\left\\Vert\\frac{\\partial f}{\\partial W^{\\prime}}\\right\\Vert^{2}}\\\\ {\\overset{(i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{2}\\eta\\left\\Vert\\mathbb{H}^{O}\\left(\\mathscr{B}_{t}^{-1}\\right)\\right\\Vert^{2}}\\\\ {\\overset{(i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{2}\\eta\\left\\Vert\\mathbb{H}^{O}\\left(\\mathscr{B}_{t}^{-1}\\left(\\mathrm{vec}\\left(\\mathrm{Mi}(M_{t};X)-y\\right)\\right)\\right\\Vert^{2}\\right.}\\\\ {\\overset{(i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{8}\\eta\\left\\Vert\\mathbb{H}^{O}\\right\\Vert_{2}^{2}\\left(\\Delta^{B}\\right)^{2}\\cdot f\\left(M_{t};X\\right)}\\\\ {\\left.=\\left(1-\\frac{1}{4}\\Vert W^{O}\\right\\Vert_{2}^{2}\\left(\\Delta^{B}\\right)^{2}\\right)\\cdot f\\left(M_{t};X\\right)}\\\\ {\\overset{(i i i)}{\\leq}\\left(1-\\eta\\eta\\right)f\\left(M_{t};X\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (i) uses Lemma 4; (i) is because we set $\\begin{array}{r}{\\eta<\\frac{1}{2G}}\\end{array}$ ; (i) only considers the gradient over $W^{V}$ (iv) plugs in the closed form gradient in Lemma 1; (v) uses the property of smallest singular value and induction assumption; (vi) comes from the definition of $\\mu$ ", "page_idx": 16}, {"type": "text", "text": "1.4Lemma for Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following lemmas all consider the Transformers with Gaussian kernel attention 11. ", "page_idx": 16}, {"type": "text", "text": "Lemma 6. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mid\\frac{\\partial f\\left(M;X\\right)}{\\partial W^{\\nu}}=B^{\\top}\\left(\\mathsf{M H}(M;X)-y\\right)\\left(W^{\\ o}\\right)^{\\top}}\\\\ &{\\mid\\mathrm{vec}\\left(\\frac{\\partial f\\left(M;X\\right)}{\\partial W^{\\nu}}\\right)=\\left<(W^{o})^{\\top}\\otimes B,\\mathrm{vec}(\\mathsf{M H}(M;X)-y)\\right>}\\\\ &{=\\left(\\mathbb{I}_{H d}\\otimes B^{\\top}\\right)\\cdot\\left(W^{o}\\otimes\\mathbb{I}_{N}\\right)\\cdot\\left(\\mathsf{M H}(M;X)-y\\right)}\\\\ &{\\mid\\frac{\\partial f\\left(M;X\\right)}{\\partial W_{\\theta}^{\\theta}}=\\frac{\\partial f\\left(M;X\\right)}{\\partial C}\\cdot\\frac{\\partial C}{\\partial W_{h}^{\\theta}}=\\sum_{i=1}^{N}\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}\\cdot\\frac{\\partial C_{i}}{\\partial W_{h}^{\\theta}}}\\\\ &{\\mid\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}=\\left(\\mathsf{M H}(M;X_{i})-y_{i}\\right)\\left(W^{o}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}}\\\\ &{\\mid\\frac{\\partial f\\left(M;X\\right)}{\\partial C}=\\left[\\frac{\\partial f\\left(M;X_{1}\\right)}{\\partial C_{1}},\\cdots,\\frac{\\partial f\\left(M;X_{N}\\right)}{\\partial C_{N}}\\right]^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 7. Consider updating $W^{Q},W^{K},W^{V}$ at iteration $t$ Suppose $\\sigma_{\\mathrm{max}}(W^{Q})$ $\\sigma_{\\operatorname*{max}}(W^{K})$ \uff0c $\\sigma_{\\operatorname*{max}}(W^{V})$ are bounded during in the optimization phase, then we have the following conclusion: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1)\\|d(C_{t h})\\|_{F}\\leq\\sqrt{\\displaystyle\\frac{2n}{d}}\\,\\|X_{i}\\|_{F}^{2}\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{Q}\\right)+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}\\|d(W_{h}^{Q})\\|_{F}}\\\\ &{(2)\\left\\|d\\left(\\frac{\\partial C_{h h}}{\\partial W_{h}^{Q}}\\right)\\right\\|_{F}\\leq\\sqrt{n}\\|X_{i}\\|_{F}^{2}\\cdot\\|d(W_{h}^{Q})\\|_{F}.}\\\\ &{(3)\\|\\frac{\\partial f(M;X_{i})}{\\partial C_{i}}\\|_{F}\\geq\\operatorname*{min}|V_{i}W^{O}|\\cdot\\operatorname*{min}S_{i}\\cdot\\|\\mathsf{M}\\|\\left(M;X_{i}\\right)-y_{i}\\|_{2},}\\\\ &{\\quad\\mathrm{~where~}R_{i}=(\\mathsf{M}\\mathsf{H}(M;X_{i})-y_{i})\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime\\prime}\\right)^{\\top}.}\\\\ &{(4)\\left\\|\\frac{\\partial f(M;X_{i})}{\\partial W_{h}^{Q}}\\right\\|_{F}\\leq Q_{i}^{\\prime}\\|\\mathsf{M}\\mathsf{H}(M;X_{i})-y_{i}\\|_{2},}\\\\ &{\\quad Q_{i}^{\\prime}=\\sqrt{\\displaystyle\\frac{2n}{d}}\\,\\|X_{i}\\|_{F}^{3}\\left\\|W^{O}\\|_{\\mathcal{I}^{\\mathrm{max}}}(W^{V})\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{Q}\\right)+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "wheremin $|V^{\\prime}W^{O}|$ is the smallest absolute value of each element in vector $V^{\\prime}W^{O}$ $\\operatorname*{min}S$ isthe smallest element inmatrix $S$ ", "page_idx": 17}, {"type": "text", "text": "Lemma8.Consider updating $W^{Q},W^{K},W^{V}$ at iteration $t$ Supose $\\sigma_{\\mathrm{max}}(W^{Q})$ $\\sigma_{\\mathrm{max}}(W^{K})$ \uff0c $\\sigma_{\\operatorname*{max}}(W^{V})$ are bounded during in the optimization phase, then we have the following conclusion: $\\|\\mathsf{M H}(M_{t+1};X)-\\mathsf{M H}(M_{t};X)\\|_{F}\\leq Z^{\\prime}\\|M_{t+1}-M_{t}\\|_{F},w h e r e\\;Z^{\\prime}$ some positive constant. (50) $\\begin{array}{r}{\\|\\nabla f\\left(M_{t+1};X\\right)-\\nabla f\\left(M_{t};X\\right)\\|_{2}\\leq G^{\\prime}\\|M_{t+1}-M_{t}\\|_{F},w h e r e\\,G^{\\prime}\\,i s}\\end{array}$ somepositiveconstant. ", "page_idx": 17}, {"type": "text", "text": "1.5 Proof Sketch of Theorem 3. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "(1)Using GD update rule, we aim to iteratively show ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\sigma_{\\operatorname*{max}}(W_{r}^{Q})\\leq\\frac{3}{2}\\bar{\\lambda}^{Q},r\\in\\{0,\\ldots,t\\},}\\\\ {\\sigma_{\\operatorname*{min}}\\left(\\frac{\\partial C_{h}(M_{r})}{\\partial W_{h}^{Q}}\\right)\\geq\\frac{1}{2}\\delta,r\\in\\{0,\\ldots,t\\},}\\\\ {\\operatorname*{min}S_{r}\\geq\\kappa,r\\in\\{0,\\ldots,t\\},}\\\\ {f\\left(M_{r};X\\right)\\leq\\left(1-\\eta\\gamma\\right)^{r}f\\left(M_{0},X\\right),\\;r\\in\\{0,\\ldots,t\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\gamma:=\\frac{1}{2}\\delta^{2}\\kappa^{2}\\left(\\operatorname*{min}\\left|V^{\\prime}W^{O}\\right|\\right)^{2}}\\end{array}$ . Let us discuss about the value of $\\gamma$ . We know $W^{O}\\in\\mathbb{R}^{H d^{V}\\times1}$ $\\boldsymbol{B}_{0}^{\\top}\\in\\mathbb{R}^{H D\\times\\overline{{N}}n}$ , where $H d>1,H D>N n$ We require $\\gamma>0$ ,i.e, $\\delta>0,\\kappa>0,\\operatorname*{min}\\left|V^{\\prime}W^{O}\\right|>$ 0. It is clear that $\\kappa>0$ can hold as long as $W_{h}^{Q}$ is bounded. And it is easy to show that if $X_{i}\\neq\\mathbf{0}$ ,we can always choose $W^{V}$ and $W^{O}$ , such that $\\operatorname*{min}\\left|V^{\\prime}W^{O}\\right|>0$ . Since $\\frac{\\partial C_{h}(M)}{\\partial W_{h}^{Q}}\\in\\mathbb{R}^{N n^{2}\\times D d}$ , suppose we initialize $W_{h}^{Q},W_{h}^{K}$ such that $\\operatorname{rank}({\\frac{\\partial C_{h}(M_{0})}{\\partial W_{h}^{Q}}})=N n^{2}$ ) = Nn\u00b2, then we have Omin $\\begin{array}{r}{\\sigma_{\\mathrm{min}}\\left(\\frac{\\partial C_{h}(M_{0})}{\\partial W_{h}^{Q}}\\right)\\geq\\delta}\\end{array}$ for some positive constant $\\delta$ . Further, we assume the initialization condition satisfies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{8n\\|X\\|_{F}^{5}\\|W^{O}\\|_{2}\\bar{\\lambda}^{V}(\\bar{\\lambda}_{h}^{Q}+\\bar{\\lambda}_{h}^{K})\\exp\\left(\\frac{9}{4}\\|X\\|_{F}^{2}\\left(\\left(\\bar{\\lambda}_{h}^{Q}\\right)^{2}+\\left(\\bar{\\lambda}_{h}^{K}\\right)^{2}\\right)\\right)}{\\delta^{2}\\left(\\operatorname*{min}\\left(\\vert V^{\\prime}W^{O}\\vert\\right)\\right)^{2}\\cdot\\operatorname*{min}\\left(\\delta,\\bar{\\lambda}_{h}^{Q}\\right)}\\left\\|\\mathrm{MH}\\left(M_{0};X\\right)-y\\right\\|_{2}\\le1\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Remark 6. The initialization condition can be satisfied $i f\\,\\|W^{O}\\|_{2}$ is large and $\\sigma_{\\operatorname*{max}}(W^{V})$ is small.   \nv' in Equation (12) is . ", "page_idx": 17}, {"type": "text", "text": "Similar to the proof of Theorem 2, we use induction to prove the theorem. Equation (52) holds when $t=0$ . Suppose it holds at iteration $t$ , we prove it holds at iteration $t+1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{W_{h,r+1}^{Q}-W_{h,0}^{Q}}\\right\\|_{F}\\stackrel{(i)}{\\leq}\\displaystyle\\sum_{s=0}^{r}{\\left\\|{W_{h,r+1}^{Q}-W_{h,r}^{Q}}\\right\\|_{F}}=\\eta\\displaystyle\\sum_{s=0}^{r}{\\left\\|{\\nabla_{W_{h}^{Q}}f\\left({M_{t}};X\\right)}\\right\\|_{F}}}\\\\ &{\\stackrel{(i i)}{\\leq}\\eta\\displaystyle\\sum_{s=0}^{r}\\sqrt{\\sum_{i=1}^{N}Q_{i}^{\\prime2}}\\left\\|{\\sf M H}({M_{r}};X)-{y}\\right\\|_{2}\\stackrel{(i i i)}{\\leq}\\eta\\sqrt{\\displaystyle\\sum_{i=1}^{N}Q_{i}^{\\prime2}}\\sum_{s=0}^{r}\\left(1-\\eta\\gamma\\right)^{s/2}\\left\\|{\\sf M H}({M_{0}};X)-{y}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) uses triangle inequality; (ii) comes from Lemma 7 and Cauchy-Schwartz inequality; (ii) is from the induction assumption that loss function $f(\\cdot)$ linearly decreases until $t$ -th iteration. Let $u=\\sqrt{1-\\eta\\gamma}$ .Sowehave ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{W_{h,r+1}^{Q}-W_{h,0}^{Q}}\\right\\|_{F}\\leq\\eta\\sqrt{\\displaystyle\\sum_{i=1}^{N}Q_{i}^{r^{2}}}\\displaystyle\\sum_{s=0}^{r}(1-\\eta\\gamma)^{s/2}\\left\\|{\\mathbb{M}}\\mathsf{H}\\left(M_{0};X\\right)-y\\right\\|_{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{\\gamma}\\sqrt{\\displaystyle\\sum_{i=1}^{N}Q_{i}^{r^{2}}}\\displaystyle\\frac{1-u^{r+1}}{1-u}(1-u^{2})\\left\\|{\\mathbb{M}}\\mathsf{H}(M_{0};X)-y\\right\\|_{2}}\\\\ &{\\leq\\displaystyle\\frac{2\\sqrt{\\displaystyle\\sum_{i=1}^{N}Q_{i}^{r^{2}}}}{\\gamma}\\left\\|{\\mathbb{M}}\\mathsf{H}(M_{0};X)-y\\right\\|_{F}\\overset{(i)}{\\leq}\\frac{1}{2}\\bar{\\lambda}_{h}^{Q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) comes from the initialization condition. Then by Weyl's inequality, there is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname{\\sigma}_{\\operatorname{max}}\\left(W_{\\mathbf{A},\\mathbf{A}}^{0}\\right)\\leq\\sigma_{\\operatorname*{max}}(W_{\\mathbf{A},\\mathbf{A}}^{0})+\\frac{1}{2}\\frac{1}{\\mathcal{M}}\\alpha-\\frac{3}{2}\\frac{3}{4},}}\\\\ &{\\left\\|\\frac{3\\sigma_{\\mathrm{A}}(M_{\\mathbf{A}})}{\\partial W_{\\mathbf{A}}^{0}}-\\frac{\\partial\\sigma_{\\mathrm{A}}(M_{\\mathbf{B}})}{\\partial W_{\\mathbf{A}}^{0}}\\right\\|_{r_{0}}^{1}\\leq\\frac{\\sigma_{\\mathrm{B}}^{\\prime}}{\\omega_{\\operatorname*{max}}}\\left\\|\\frac{\\partial C_{\\mathbf{A}}(M_{\\mathbf{A}})}{\\partial W_{\\mathbf{A}}^{0}}-\\frac{\\partial C_{\\mathbf{A}}(M_{\\mathbf{B}})}{\\partial W_{\\mathbf{A}}^{0}}\\right\\|_{r_{0}}^{1},}\\\\ &{\\overset{(a)}{\\leq}\\frac{(1)\\sigma_{\\mathrm{B}}^{\\mathrm{A}}\\left[1\\right]\\left\\|\\frac{3}{r_{0}^{2}}\\sum\\Biggl\\|\\nabla_{\\mathbf{w}^{2}}f(\\mathbf{A}_{r,\\mathbf{X}})\\right\\|_{r_{0}}}{\\omega_{\\mathbf{A}}^{0}}}\\\\ &{\\overset{(b)}{\\leq}\\frac{(1)\\sigma_{\\mathrm{B}}^{\\mathrm{A}}\\left[1\\right]\\left\\|\\frac{3}{r_{0}^{2}}\\sum\\Biggl\\|\\frac{\\sqrt{\\mathbf{Z}}}{r_{0}\\omega_{\\mathbf{B}}^{2}}(1)\\ensuremath{\\mathbb{H}}(M_{\\mathbf{A}};\\mathbf{X})-\\boldsymbol{y}\\|_{\\mathbf{Z}}}{\\ddots\\Biggr\\|\\prod_{s=1}^{N}\\frac{(1)}{r_{0}^{2}}\\prod_{s=1}^{N}(M_{\\mathbf{B}};\\mathbf{X})}}\\\\ &{\\overset{(c)}{\\leq}\\frac{1}{2}\\sqrt{\\nu_{\\mathbf{A}}\\!\\left\\|\\mathbf{X}\\right\\|_{r_{0}}^{1}\\!}\\Biggl\\|\\frac{\\sum_{\\mathbf{Z}}Q_{2}^{\\prime}\\sum\\Biggl\\{(1-\\sigma_{\\mathrm{B}}^{\\mathrm{A}})^{s/2}\\left\\|\\mathbf{M}\\right\\|(M_{\\mathbf{G}};\\mathbf{X \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) uses triangle inequality; (i) applies Lemma 7 (2) and Cauchy-Schwartz inequality; (i) uses Lemma 7 (4); (iv) applies the induction assumption that the loss function $f(\\cdot)$ linearlydecreases until $t$ -th iteration; (v) comes from the initialization condition. Then by Weyl's inequality, there is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sigma_{\\operatorname*{max}}\\left(\\frac{\\partial C_{h}\\left(M_{t+1}\\right)}{\\partial W_{h}^{Q}}\\right)\\geq\\sigma_{\\operatorname*{max}}\\left(\\frac{\\partial C_{h}\\left(M_{0}\\right)}{\\partial W_{h}^{Q}}\\right)-\\frac{1}{2}\\delta=\\frac{1}{2}\\delta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For each element in $S_{i h}$ , we have close form ", "page_idx": 18}, {"type": "equation", "text": "$$\nS\\left(W_{h}^{Q},W_{h}^{K};X_{i}\\right)_{k j}=\\exp\\left(-\\frac{1}{2\\sqrt{d}}\\left\\|X_{i k}\\cdot W_{h}^{Q}-X_{i j}\\cdot W_{h}^{K}\\right\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since we have already showed that $\\begin{array}{r}{\\sigma_{\\operatorname*{max}}\\left(W_{h,r}^{Q}\\right)\\leq\\frac{3}{2}\\bar{\\lambda}_{h}^{Q}}\\end{array}$ i folows diretly each lemen in matrix $S_{t}$ is lower bounded by some constant $\\kappa$ for any $t$ . Now we derive the expression of $\\kappa$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp\\left(-\\displaystyle\\frac{1}{2\\sqrt{d}}\\left\\|X_{i k}\\cdot W_{h,t}^{Q}-X_{i j}\\cdot W_{h}^{K}\\right\\|^{2}\\right)}\\\\ &{\\overset{(i)}{\\geq}\\exp\\left(-\\displaystyle\\frac{1}{\\sqrt{d}}\\big(\\|X_{i k}\\cdot W_{h,t}^{Q}\\|^{2}+\\|X_{i j}\\cdot W_{h}^{K}\\|^{2}\\big)\\right)}\\\\ &{\\overset{(i i)}{\\geq}\\exp\\left(-\\displaystyle\\frac{1}{\\sqrt{d}}\\big(\\textstyle\\frac{9}{4}(\\bar{\\lambda}_{h}^{Q})^{2}\\|X_{i k}.\\|^{2}+(\\bar{\\lambda}_{h}^{K})^{2}\\|X_{i j}.\\|^{2}\\big)\\right)}\\\\ &{\\overset{(i i i)}{\\geq}\\exp\\left(-\\displaystyle\\frac{9}{4}\\|X\\|_{F}^{2}\\big((\\bar{\\lambda}_{h}^{Q})^{2}+(\\bar{\\lambda}_{h}^{K})^{2}\\big)\\right)}\\\\ &{:=\\kappa,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (i) uses Cauchy-Schwartz inequality; (i) applies the induction assumption $\\begin{array}{r}{\\sigma_{\\operatorname*{max}}(W_{h,t}^{Q})\\leq\\frac{3}{2}\\bar{\\lambda}_{h}^{Q}}\\end{array}$ and property of singular value; (ii) is because $d\\geq1$ . Thus, we have $\\operatorname*{min}S_{t}\\geq\\kappa$ . Finally, we aim to show $f\\left(M_{t+1};X\\right)\\leq(1-\\eta\\gamma)f\\left(M_{t},X\\right)$ . By Lemma 8, since we have showed that $\\sigma_{\\mathrm{max}}(W_{h}^{Q})$ is bounded, we can directly derive that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla f\\left(M_{t+1};X\\right)-\\nabla f\\left(M_{t};X\\right)\\right\\|_{2}}\\\\ &{=\\left\\|\\nabla_{W_{h}^{Q}}f\\left(M_{t+1};X\\right)-\\nabla_{W_{h}^{Q}}f\\left(M_{t};X\\right)\\right\\|_{2}}\\\\ &{\\leq G^{\\prime}\\|M_{t+1}-M_{t}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, by Lemma 4, choose $\\begin{array}{r}{\\eta<\\frac{1}{2G^{\\prime}}}\\end{array}$ , we have the following holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f\\left(M_{t+1};X\\right)=f\\left(M_{t}-\\eta\\nabla f\\left(M_{t};X\\right);X\\right)}&{}\\\\ {\\overset{(i)}{\\leq}f\\left(M_{t};X\\right)-\\eta\\left\\Vert\\nabla\\circ f\\left(M_{t};X\\right)\\right\\Vert^{2}+\\frac{G^{'}}{2}\\eta^{2}\\left\\Vert\\nabla\\scriptscriptstyle{W\\circ f\\left(M_{t};X\\right)}\\right\\Vert^{2}}&{}\\\\ {\\overset{(i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{2}\\eta\\left\\Vert\\nabla\\scriptscriptstyle{W_{\\alpha}^{\\prime}f\\left(M_{t};X\\right)}\\right\\Vert^{2}}&{}\\\\ {\\overset{(i i i)}{=}f\\left(M_{t};X\\right)-\\frac{1}{2}\\eta\\left\\Vert\\frac{\\partial f\\left(M_{t};X\\right)}{\\partial C(M_{t})}\\cdot\\left(\\frac{\\partial C\\left(M_{t}\\right)}{\\partial W_{\\alpha}^{\\prime}}\\right)\\right\\Vert_{F}^{2}}&{}\\\\ {\\overset{(i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{4}\\eta^{2}\\left\\Vert\\frac{\\partial f\\left(M_{t};X\\right)}{\\partial C(M_{t})}\\right\\Vert_{F}^{2}}&{}\\\\ {\\overset{(i i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{4}\\eta^{2}\\left\\Vert\\left(\\mathbb{M}\\!\\!\\left(M;X\\right)-\\!\\eta\\right)\\left(\\mathbf{W}^{\\ O}\\right)^{T}\\left(V^{\\prime}\\right)^{\\top}\\right\\Vert\\leq S\\right\\Vert_{F}^{2}}&{}\\\\ {\\overset{(i i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{4}\\eta^{2}g^{2}\\left(\\mathbb{M}\\!\\left|V^{\\prime}W^{\\prime}\\right|^{2}\\right)^{2}\\left\\Vert\\mathbb{M}\\!\\left(M_{0};X\\right)-y\\right\\Vert_{Z}^{2}}&{}\\\\ {\\overset{(i i i i)}{\\leq}f\\left(M_{t};X\\right)-\\frac{1}{4}\\eta^{2}g^{2}\\left(\\mathbb{M}\\!\\left|V^{\\prime}W^{\\prime}\\right|^{2}\\right)^{2}\\left\\Vert\\mathbb{M}\\!\\left(M_{0};X\\right)-y\\right\\Vert_{Z}^{2}}&{}\\\\ {\\overset{(i i i i)}{\\in}\\left(1-\\eta\\right)\\int\\left(M_{t};X\\right),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where (i) uses Lemma 4 (2); (i) is because we choose $\\begin{array}{r l r}{\\eta}&{{}<}&{\\frac{1}{2G^{\\prime}}}\\end{array}$ ;(ii) writes down the expression of gradient according to chain rule in Lemma 6; (iv) uses the induction assumption $\\begin{array}{r}{\\sigma_{\\operatorname*{max}}\\left(\\frac{\\partial C_{h}(\\bar{M}_{t+1})}{\\partial W_{h}^{Q}}\\right)\\geq\\frac{1}{2}\\delta}\\end{array}$ and propertyof sngular alue; () uses Lemma (4); (vi comes fom Lemma 7 (3); (vi) uses the definition of $\\gamma$ ", "page_idx": 19}, {"type": "text", "text": "(2)Next, we show the convergence result for Transformer with Softmax kernel with only $W^{Q}$ updated. Since we assume parameters are all bounded during optimization phase, by Lemma 8, we can easily show that there exists constant $G^{\\prime}$ (see xx for details), such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla_{W_{h}^{Q}}f\\left(M_{t+1};X\\right)-\\nabla_{W_{h}^{Q}}f\\left(M_{t};X\\right)\\right\\|_{2}\\leq G^{\\prime}\\left\\|M_{t+1}-M_{t}\\right\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then by Lemma 4, choose $\\begin{array}{r}{\\eta^{\\prime}<\\frac{1}{2G^{\\prime}}}\\end{array}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f\\left({M_{t+1};X}\\right)=f\\left({M_{t}-\\eta\\nabla f\\left({M_{t};X}\\right);X}\\right)}\\\\ {\\qquad\\qquad\\leq f\\left({M_{t};X}\\right)-\\eta^{\\prime}\\left\\Vert{\\nabla f\\left({M_{t};X}\\right)}\\right\\Vert^{2}+\\displaystyle\\frac{G^{\\prime}}{2}\\eta^{\\prime2}\\left\\Vert{\\nabla f\\left({M_{t};X}\\right)}\\right\\Vert^{2}}\\\\ {\\qquad\\qquad\\leq f\\left({M_{t};X}\\right)-\\displaystyle\\frac{1}{2}\\eta^{\\prime}\\left\\Vert{\\nabla f\\left({M_{t};X}\\right)}\\right\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "1.6Proof of Lemma in Section 1.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 2 (1). ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Step 1: When $W^{Q},W^{K}$ are updated, we aim to prove ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|d(S_{i})\\|_{F}\\leq n\\|d(C_{i})\\|_{F}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 2: We aim to show $\\begin{array}{r}{\\|d(C_{i})\\|_{F}\\le\\frac{n}{\\sqrt{d}}\\left\\|X_{i}\\right\\|_{F}^{2}\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}\\cdot\\left\\|d\\left(W^{Q}\\right)\\right\\|_{F}.}\\end{array}$ Combinethe above two steps, we can derive the bound in Equation (27). ", "page_idx": 20}, {"type": "text", "text": "Proof of Step 1: First, we can write down the closed form of the differential of $S_{i}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|d(S_{i})\\|_{F}=\\|S_{i}\\odot d(C_{i})-S_{i}\\odot\\Upsilon((\\exp C_{i})\\mathbb{E})\\odot d(\\exp(C_{i})\\mathbb{E}))\\|_{F}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We reorganize the terms on the right side of Equation (57), we have the following equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|d(S_{i})\\|_{F}=\\|S_{i}\\odot\\left(d(C_{i})-\\Upsilon((\\exp C_{i})\\mathbb{E})\\odot d((\\exp C_{i})\\mathbb{E})\\right)\\|_{F}}\\\\ &{\\qquad\\qquad=\\|S_{i}\\odot\\left(d(C_{i})-\\Upsilon((\\exp C_{i})\\mathbb{E})\\odot((\\exp C_{i})\\odot d(C_{i}))\\,\\mathbb{E}\\right)\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $C_{i}=[C_{i1},\\cdots\\,,C_{i H}]$ , we will investigate each $C_{i h},\\;h=1,2,\\cdots\\,,H$ We focus on the term $d(C_{i})-\\Upsilon((\\exp C_{i})\\mathbb{E})\\odot(\\exp(C_{i})\\odot d(C_{i}))\\,\\mathbb{E}$ in Equation (58). We write down the close form of the element in the $k$ -th row and $j$ -th column: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|d(C_{i h})-\\Upsilon(\\exp(C_{i h})\\mathbb{E})\\odot\\left(\\exp(C_{i h})\\odot d(C_{i h})\\right)\\mathbb{E}\\right|_{k\\ell}}\\\\ &{\\overset{(i)}{=}\\left(1-\\frac{\\exp\\left(C_{i h k j}\\right)}{\\sum_{j=1}^{n}\\exp\\left(C_{i h k j}\\right)}\\right)d(C_{i h k j})-\\frac{\\sum_{j=1}^{n}\\exp\\left(C_{i h k j}\\right)\\,d(C_{i h k j})}{\\sum_{j=1}^{n}\\exp\\left(C_{i h k j}\\right)}}\\\\ &{\\overset{(i i)}{\\le}\\sqrt{\\left(1-\\frac{\\exp\\left(C_{i h k j}\\right)}{\\sum_{j=1}^{n}\\exp\\left(C_{i h k j}\\right)}\\right)^{2}+\\sum_{p\\neq j}\\left(\\frac{\\exp(C_{i h k p})}{\\sum_{j=1}^{n}\\exp\\left(C_{i h k j}\\right)}\\right)^{2}}\\cdot\\sqrt{\\sum_{j=1}^{n}\\left(d(C_{i h k j})\\right)^{2}}}\\\\ &{\\overset{(i i i)}{\\le}\\sqrt{n}\\|d(C_{i h k j})\\|_{\\ell},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (i) is expand the closed form of Equation (59); (ii) uses the Cauchy-Schwartz inequality; (ii) is because each element in the square root in (i) is upper bounded by 1. With Equation (61), we can easilyshow ", "page_idx": 20}, {"type": "equation", "text": "$$\n|d(C_{i h})-\\Upsilon((\\exp C_{i h})\\mathbb{E})\\odot((\\exp C_{i h})\\odot d(C_{i h}))\\mathbb{E}\\|_{F}\\le\\sqrt{n}\\sqrt{\\sum_{k=1}^{n}\\sum_{j=1}^{n}\\|d(C_{i h k})\\|_{F}^{2}}\\le n\\|d(C_{i h})\\|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since every element in $S_{i}$ has magnitude less than 1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|d(S_{i})\\|_{F}=\\|S_{i}\\odot(d(C_{i})-\\Upsilon\\left((\\exp C_{i})\\mathbb{E}\\right)\\odot((\\exp C_{i})\\odot d(C_{i}))\\mathbb{E})\\|_{F}}\\\\ &{\\le\\|d(C_{i h})-\\Upsilon((\\exp C_{i h})\\mathbb{E})\\odot((\\exp C_{i h})\\odot d(C_{i h}))\\mathbb{E}\\|_{F}}\\\\ &{\\overset{(i)}{\\le}n\\sqrt{H}\\|d(C_{i})\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (i) is from Cauchy-Schawatz inequality. ", "page_idx": 21}, {"type": "text", "text": "Proof of Step 2: We aim to show $\\begin{array}{r}{\\|d\\left(\\boldsymbol{C}_{i}\\right)\\|_{F}\\leq\\frac{n}{\\sqrt{d}}\\left\\|\\boldsymbol{X}_{i}\\right\\|_{F}^{2}\\sqrt{\\sum_{h=1}^{H}\\sigma_{\\operatorname*{max}}^{2}\\left(\\boldsymbol{W}_{h}^{K}\\right)}\\cdot\\left\\|d\\left(\\boldsymbol{W}^{Q}\\right)\\right\\|_{F}}\\end{array}$ Similarly, we investigate $\\|d(C_{i h})\\|_{F}$ \uff0c $h=1,2,\\cdots\\,,H$ We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|d(C_{i h})\\|_{F}=\\left\\|\\frac{X_{i}d(W_{h}^{Q})\\left(X_{i}W_{h}^{K}\\right)^{\\top}}{\\sqrt{d}}\\right\\|_{F}\\leq\\frac{1}{\\sqrt{d}}\\|X_{i}\\|_{F}^{2}\\sigma_{\\operatorname*{max}}(W_{h}^{K})\\|d(W_{h}^{Q})\\|_{F}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then plug the above inequality to Equation (63), we can derive ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|d(S_{i h})\\|_{F}\\leq\\frac{n}{\\sqrt{d}}\\|X_{i}\\|_{F}^{2}\\sigma_{\\operatorname*{max}}\\left(W_{h}^{K}\\right)\\left\\|d(W_{h}^{Q})\\right\\|_{F}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus by Cauchy-Schwartz inequality, it is easy to show ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|d(S_{i})\\|_{F}\\leq\\frac{n}{\\sqrt{d}}\\|X_{i}\\|_{F}^{2}\\sqrt{\\sum_{h=1}^{H}\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}\\cdot\\left\\|d\\left(W^{Q}\\right)\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 2 (4). ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. We first write down the close form of gradient of $f(\\cdot)$ over $W_{h}^{Q}$ by Lemma 1, and derive the upper bound of the norm of the gradient. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial f(M;X_{i})}{\\partial W_{h}^{Q}}\\right\\|_{F}=\\left\\|\\frac{1}{\\sqrt{d}}X_{i}^{\\top}\\frac{\\partial f(M;X_{i})}{\\partial C_{i}}\\mathbb{P}_{h}^{\\top}X_{i}W_{h}^{K}\\right\\|_{F}\\leq\\left\\|X_{i}\\right\\|_{F}^{2}\\sigma_{\\operatorname*{max}}(W_{h}^{K})\\left\\|\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}\\right\\|_{F}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lemma 1, there is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}=\\left(\\left(\\mathsf{M H}\\!\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}}\\\\ &{\\quad-\\left(\\left(\\left(\\left(\\mathsf{M H}\\!\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}\\odot\\Upsilon\\left(\\left(\\exp C_{i}\\right)\\mathbb{E}\\right)\\right)\\mathbb{E}^{\\top}\\right)\\odot\\exp C_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denote $R_{i}\\,=\\,\\left(\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}$ \uff0c $R_{i}\\,=\\,[R_{i1},\\cdot\\cdot\\cdot\\,,R_{i H}]$ . Write down the close form of the element in the $k$ -th row and $j$ -th column: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\prod_{i}{S_{i}}\\ h_{i}=\\left(\\ R_{i}\\ \\phi_{i}\\ C_{i k}\\ \\odot\\ \\Gamma\\left(\\exp{C_{i k}}\\ h_{i}\\right)\\mathbb{E}^{\\top}\\right)\\odot\\left(\\exp{C_{i k}}h_{i}\\right)}\\\\ &{=R_{i\\mathrm{abj}}\\ S_{i k\\,j}-\\frac{\\exp(C_{i k,j})}{\\sum_{j=1}^{N}\\exp(C_{i k,j})}\\frac{\\sum_{t}R_{i k,j}\\ S_{i k,j}}{\\sum_{t}\\exp(C_{i k,j})}}\\\\ &{=\\left(S_{i k,j}-\\frac{(\\exp{C_{i k,j}})S_{i k,j}}{\\displaystyle\\sum_{j=1}^{N}\\exp(C_{i k,j})}\\right)\\cdot R_{i k,j}-\\displaystyle\\sum_{p\\neq j}\\frac{(\\exp{C_{i k,p}})S_{i k,j}}{\\displaystyle\\sum_{j=1}^{N}\\exp(C_{i k,j})}R_{i k,p}}\\\\ &{\\overset{(i)}{\\leq}\\sqrt{\\left(1-\\frac{\\exp(C_{i k,j})}{\\sum_{j=1}^{N}\\exp(C_{i k,j})}\\right)^{2}+\\sum_{p\\neq j}^{N}\\left(\\frac{\\exp(C_{i k,j})}{\\displaystyle\\sum_{j=1}^{N}\\exp(C_{i k,j})}\\right)^{2}}\\cdot\\|{R}_{i k,j}\\|_{L}}\\\\ &{\\overset{(i i)}{\\leq}\\sqrt{\\eta}\\|R_{i k}\\|_{L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (1) is due to the Cauchy-Schwartz inequality; (i) is because each element within the squre root term in (i) has magnitude at most 1. Thus, we can further derive ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i h}}\\right\\|_{F}=\\left\\|R_{i h}\\odot S_{i h}-\\left(\\left(R_{i h}\\odot S_{i h}\\odot\\Upsilon(\\left(\\exp C_{i h}\\right)\\mathbb{E}\\right)\\right)\\mathbb{E}^{\\top}\\right)\\odot\\exp C_{i h}\\right\\|_{F}}\\\\ &{\\stackrel{(i)}{\\le}\\sqrt{n}\\displaystyle\\sum_{k=1}^{n}\\sum_{j=1}^{n}\\|R_{i h k}\\|_{F}\\stackrel{(i i)}{\\le}n\\|R_{i h}\\|_{F}}\\\\ &{\\stackrel{(i i i)}{\\le}n\\|X_{i}\\|_{F}\\|W^{O}\\|_{2}\\sigma_{\\operatorname*{max}}(W_{h}^{V})\\|\\mathbb{M}\\mathsf{H}(M;X_{i})-y_{i}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) if from the bound in Equation (71); (ii) comes from Cauchy-Schwatz inwquality; (i) uses the property of Frobenious norm. Thus, by Cauchy-Schwartz inequality, we can derive the upper bound for $\\left\\|\\frac{\\partial f(M;X_{i})}{\\partial C_{i}}\\right\\|_{F}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bigg\\|\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}\\bigg\\|_{F}\\leq n\\sqrt{H}\\|X_{i}\\|_{F}\\|W^{O}\\|_{2}\\sigma_{\\operatorname*{max}}(W^{V})\\|\\mathsf{M}\\mathsf{H}(M;X_{i})-y_{i}\\|_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So plug the above inequality into Equation (69), we can derive the upper bound for $\\left\\|\\frac{\\partial f(M;X_{i})}{\\partial W_{h}^{Q}}\\right\\|_{F}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial W_{h}^{Q}}\\right\\|_{F}\\leq\\|X_{i}\\|_{F}^{2}\\,\\sigma_{\\operatorname*{max}}\\left(W_{h}^{K}\\right)\\left\\|\\frac{\\partial f\\left(M;X\\right)}{\\partial C_{i}}\\right\\|_{F}}\\\\ &{\\leq n\\sqrt{H}\\left\\|X_{i}\\right\\|_{F}^{3}\\left\\|W^{O}\\right\\|_{2}\\sigma_{\\operatorname*{max}}\\left(W_{h}^{K}\\right)\\sigma_{\\operatorname*{max}}\\left(W_{h}^{V}\\right)\\left\\|\\mathsf{M}\\mathsf{H}\\left(M;X_{i}\\right)-y_{i}\\right\\|_{2}}\\\\ &{\\leq n\\sqrt{H}\\left\\|X_{i}\\right\\|_{F}^{3}\\left\\|W^{O}\\right\\|_{2}\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}\\sigma_{\\operatorname*{max}}\\left(W^{V}\\right)\\left\\|\\mathsf{M}\\mathsf{H}\\left(M;X_{i}\\right)-y_{i}\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 3 (1). By Mean Value Theorem and Cauchy-Schwartz inequality, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f\\left({M_{t+1};X_{i}}\\right)-f\\left({M_{t};X_{i}}\\right)|}\\\\ &{=\\left\\langle{\\frac{\\partial f\\left({M_{t}^{\\prime};X_{i}}\\right)}{\\partial W},M_{t+1}-M_{t}}\\right\\rangle}\\\\ &{\\leq\\sqrt{\\left\\|\\frac{\\partial f\\left({M_{t}^{\\prime};X_{i}}\\right)}{\\partial W^{Q}}\\right\\|^{2}+\\left\\|\\frac{\\partial f\\left({M_{t}^{\\prime};X_{i}}\\right)}{\\partial W^{K}}\\right\\|^{2}+\\left\\|\\frac{\\partial f\\left({M_{t};X_{i}}\\right)}{\\partial W^{V}}\\right\\|^{2}}\\|{M_{t+1}-M_{t}}\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $M_{t}^{\\prime}$ is between $M_{t}$ and $M_{t+1}$ .We can derive the upper bound of the norm of $\\nabla_{W^{V}}f(M;X_{i})$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\frac{\\partial f\\left(M_{t};X_{i}\\right)}{\\partial W^{V}}\\right\\|_{F}=\\|B_{i}^{\\top}\\left(\\mathsf{M H}(M_{t};X_{i})-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\|_{F}}\\\\ &{\\leq\\|B_{i}\\|_{F}\\|\\mathsf{M H}(M_{t};X_{i})-y_{i}\\|_{F}\\|W^{O}\\|_{2}}\\\\ &{\\leq n\\sqrt{H}\\|X_{i}\\|_{F}\\|W^{O}\\|_{2}\\|\\mathsf{M H}(M_{t};X_{i})-y_{i}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma 2, we know ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial f(M_{t};X_{i})}{\\partial W^{Q}}\\right\\|_{F}\\leq Q_{i}\\left\\|\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\right\\|_{2};\\ \\left\\|\\frac{\\partial f(M_{t};X_{i})}{\\partial W^{K}}\\right\\|_{F}\\leq K_{i}\\left\\|\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f\\left(M_{t+1};X_{i}\\right)-f\\left(M_{t};X_{i}\\right)\\|_{2}\\leq\\sqrt{Q_{i}^{2}+K_{i}^{2}+n^{2}H\\sigma_{\\operatorname*{max}}^{2}(X_{i})\\|W^{O}\\|^{2}}\\|M_{t+1}-M_{t}\\|_{F}}\\\\ &{:=Z_{i}\\|M_{t+1}-M_{t}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, together with Equation (73), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f\\left(M_{t+1};X\\right)-f\\left(M_{t};X\\right)\\|_{2}\\leq N\\sqrt{\\operatorname*{max}_{i}Q_{i}^{2}+\\operatorname*{max}_{i}K_{i}^{2}+n^{2}H\\operatorname*{max}_{i}\\|X_{i}\\|_{F}^{2}}\\|M_{t+1}-M_{t}\\|_{F}}\\\\ &{:=Z\\|M_{t+1}-M_{t}\\|_{F}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 3 (2). ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. By triangle inequality, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{W}f(M_{t+1};X)-\\nabla_{W}f(M_{t};X)\\right\\|_{F}}\\\\ &{\\le\\|\\nabla_{W}q\\,f(M_{t+1};X)-\\nabla_{W}q\\,f(M_{t+1};X)\\|_{F}+\\left\\|\\nabla_{W^{K}}f(M_{t+1};X)-\\nabla_{W^{K}}f(M_{t+1};X)\\right\\|_{F}}\\\\ &{\\quad+\\left\\|\\nabla_{W^{\\nu}}f(M_{t+1};X)-\\nabla_{W^{\\nu}}f(M_{t+1};X)\\right\\|_{F}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{N}\\left(\\|\\nabla_{W^{Q}}f(M_{t+1};X)-\\nabla_{W^{Q}}f(M_{t+1};X)\\|_{F}+\\|\\nabla_{W^{K}}f(M_{t+1};X)-\\nabla_{W^{K}}f(M_{t+1};X)\\|_{F}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n+\\left\\|\\nabla_{W^{v}}f\\left(M_{t+1};X\\right)-\\nabla_{W^{v}}f\\left(M_{t+1};X\\right)\\right\\|_{F}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Step 1: Derive upper bound for ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "$\\nabla_{W}\\circ f(M_{t+1};X_{i}))-\\nabla_{W^{Q}}f(M_{t};X_{i}))\\|_{F}=\\|\\operatorname{vec}(\\nabla_{W^{Q}}f(M_{t+1};X_{i}))-\\operatorname{vec}(\\nabla_{W^{Q}}f(M_{t};X_{i}))\\|_{2}.$ First, we give the vectorized expression of $\\nabla_{W^{Q}}f\\left(M_{t};X_{i}\\right)$ . Recall we denote $\\begin{array}{r l}{U_{i}}&{{}=}\\end{array}$ $\\left(\\left(\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}$ . By Lemma 1, we can derive the close form of $\\mathrm{vec}(\\nabla_{W^{Q}}f(M_{t};X_{i}))$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{vec}(\\nabla_{W}\\!\\circ\\!f(M;X_{i}))\\xrightarrow{(i)}\\mathrm{vec}(U_{i})-\\mathrm{vec}\\left((U_{i}\\odot\\Upsilon((\\exp C_{i})\\mathbb{E}))\\mathbb{E}^{\\top}\\right)\\odot\\mathrm{vec}(\\exp C_{i})}\\\\ &{\\overset{(i i)}{=}\\mathrm{vec}(U_{i})-\\left(\\mathbb{E}\\otimes\\mathbb{I}_{n}\\right)\\mathrm{vec}\\left(U_{i}\\odot\\Upsilon((\\exp C_{i})\\mathbb{E})\\right)\\odot\\mathrm{vec}(\\exp C_{i})}\\\\ &{\\overset{(i i i)}{=}\\mathrm{vec}(U_{i})-\\left(\\mathbb{E}\\otimes\\mathbb{I}_{n}\\right)\\mathrm{vec}(U_{i})\\odot\\mathrm{vec}\\left(\\Upsilon((\\exp C_{i})\\mathbb{E})\\right)\\odot\\mathrm{vec}(\\exp C_{i})}\\\\ &{\\overset{(i v)}{=}\\mathrm{vec}(U_{i})-\\left(\\mathbb{E}\\otimes\\mathbb{I}_{n}\\right)\\mathrm{vec}(U_{i})\\odot\\mathrm{vec}(S_{i})}\\\\ &{\\overset{(v)}{=}\\mathbb{I}_{n^{2}H}\\mathrm{vec}(U_{i})\\odot\\mathrm{vec}(\\mathbf{1}_{n}\\mathbf{1}_{n^{T}}^{\\top})-\\left(\\mathbb{E}\\otimes\\mathbb{I}_{n}\\right)\\mathrm{vec}(U_{i})\\odot\\mathrm{vec}(S_{i})}\\\\ &{\\overset{(v)}{=}\\left(\\mathbb{I}_{n^{2}H}-\\left(\\mathbb{E}\\otimes\\mathbb{I}_{n}\\right)\\right)\\mathrm{vec}(U_{i})\\odot\\mathrm{vec}(\\mathbf{1}_{n}\\mathbf{1}_{n^{T}}^{\\top}-S_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (i) uses the Lemma 1; (i) and (i) comes from the property of vectorization in Lemma 5; (vi) uses the definition of $S_{i}\\,;(\\mathbf{v})$ gives an equivalent expression of $\\operatorname{vec}(U_{i})$ ; (vi) reorganizies (v). Further, it is easy to verify that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{U_{i}}\\right\\|_{F}=\\left\\|\\left(\\left(\\mathsf{M}\\mathsf{H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}\\right\\|_{F}\\leq\\left\\|{R_{i}}\\right\\|_{F}}\\\\ &{=\\left(\\left\\|\\mathsf{M}\\mathsf{H}\\left(M;X_{i}\\right)\\right\\|_{2}+\\left\\|y_{i}\\right\\|_{2}\\right)\\left\\|{W^{O}}\\right\\|_{2}\\left\\|X_{i}\\right\\|_{F}\\sigma_{\\operatorname*{max}}(W^{V})}\\\\ &{\\leq\\left(n\\sqrt{H}\\sigma_{\\operatorname*{max}}(W^{V})\\right\\|X_{i}\\right\\|_{F}\\left\\|W^{O}\\right\\|_{2}+\\left\\|y_{i}\\right\\|_{2}\\right)\\left\\|{W^{O}}\\right\\|_{2}\\left\\|X_{i}\\right\\|_{F}\\sigma_{\\operatorname*{max}}\\left(W^{V}\\right)}\\\\ &{\\leq\\left(n\\sqrt{H}\\sigma_{\\operatorname*{max}}\\left(W^{V}\\right)\\left\\|X\\right\\|_{F}\\left\\|W^{O}\\right\\|_{2}+\\left\\|y\\right\\|_{2}\\right)\\left\\|{W^{O}}\\right\\|_{2}\\left\\|X\\right\\|_{F}\\sigma_{\\operatorname*{max}}\\left(W^{V}\\right)}\\\\ &{:=\\bar{R}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, let us derive upper bound for $\\begin{array}{r}{\\|\\nabla_{W^{Q}}f\\left(M_{t+1};X_{i}\\right)-\\nabla_{W^{Q}}f\\left(M_{t+1};X_{i}\\right)\\|_{F}.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "IVwe f(Mt+1; Xi) -Vwq f(Mt+1; Xa)ll $\\begin{array}{r l}&{\\frac{\\langle t\\rangle}{c}\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots}\\\\ &{\\overset{(i)}{=}\\lVert\\left(\\mathbf T_{n^{2}H}-(\\mathbf{E\\otimes\\left\\mathbb{D}}_{n})\\right)\\big(\\mathbf v\\mathbf e(U_{i,t+1})\\odot\\mathbf v\\mathbf e(S_{i,t+1})-\\mathbf v\\mathbf e(U_{i,t})\\odot\\mathbf v\\mathbf e(S_{i,t})\\big)\\rVert_{F}}\\\\ &{=\\lVert\\big(\\mathbf T_{n^{2}H}-(\\mathbf{E\\otimes\\left\\mathbb{D}}_{n})\\right)\\big(\\mathbf v\\mathbf e(U_{i,t+1})\\odot\\mathbf v\\mathbf e(S_{i,t+1})-\\mathbf v\\mathbf e(U_{i})\\odot\\mathbf v\\mathbf e(S_{i,t+1})+\\mathbf v\\mathbf e(U_{i,t+1})\\odot\\mathbf v\\mathbf e(c_{i,t})\\big)\\times}\\\\ &{\\overset{(i)}{\\leq}\\lVert\\mathbf T_{n^{2}H}-(\\mathbf{E\\otimes\\left\\mathbb{D}}_{n})\\rVert_{F}\\Bigg(\\lVert\\mathbf v\\mathbf e(U_{i,t+1}-U_{i,t})\\rVert_{F}+\\lVert U_{i,t}\\rVert_{F}\\lVert S_{i,t+1}-S_{i,t}\\rVert_{F}\\Bigg)}\\\\ &{\\overset{(i i)}{\\leq}n\\sqrt{H}\\left(\\lVert\\mathbf v e(U_{i,t+1}-U_{i,t})\\rVert_{F}+\\tilde{H}\\lVert S_{i,t+1}-S_{i,t}\\rVert_{F}\\right)}\\\\ &{\\overset{(i i)}{=}n\\sqrt{H}\\left(\\lVert\\mathbf R_{i,t+1}\\odot S_{i,t+1}-R_{i,t}\\odot S_{i,t}\\rVert_{F}+\\tilde{H}\\lVert S_{i,t+1}-S_{i,t}\\rVert_{F}\\right)}\\\\ &{=n\\sqrt{H}\\left(\\lVert\\mathbf R_{i,t+1}\\odot S_{i,t+1}-R_{i,t}\\odot S_{i,t+1}+R_{i,t}\\odot S_{i,t+1}-R_{i,t}\\odot S_{i,t}\\rVert_{F}+\\tilde{H}\\lVert S_{i,t+1}-S_{i,t}\\rVert_{F}\\right)}\\\\ &{\\overset{(e)}{\\leq}n\\sqrt{H}\\left(\\lVert\\mathbf R_{i,t+1}-R_{i,t}\\odot S_{i,t+1}\\rVert_{F}+\\lVert R_{i,t}\\odot S_{i,t+1}-R_{i,t}\\odot S_{i}\\rVert_{F}+\\tilde{H}\\lVert S_{i+1}-S_{i,t}\\rVert_{F}\\right)}\\\\ &{\\overset{(e$ Si,t+1) - vec(Ui,t)  vec(< F) ", "page_idx": 23}, {"type": "text", "text": "where (i) plugs in the expression in Equation (79); (i) uses the fact that each element in $S_{i,t+1}$ has magnitude at most 1, and Cauchy-Schwartz inequality; (ii) comes from the definition of $\\mathbb{I},\\mathbb{E}$ and $\\bar{R}$ (iv) uses the definition of $U_{i,t}$ ; (v) is because triangle inequality; (vi) uses the fact that each element in $S_{i,t+1}$ has magnitude at most 1, and Cauchy-Schwartz inequality. Next, we aim to derive upper bound of $\\|R_{i,t+1}-R_{i,t}\\|_{F}$ in Equation (81). ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\varepsilon,i+1}-R_{i,\\uparrow\\uparrow}[r=]\\left[\\langle\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_{i})-y_{\\Re}\\rangle W^{\\mathrm{O}}(V_{i,\\uparrow+1}^{\\prime})-\\langle\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_{i})-y_{\\Re}\\rangle W^{\\mathrm{O}}(V_{i,\\uparrow}^{\\prime})\\right]_{\\boldsymbol{F}}}\\\\ &{=\\left\\lVert\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_{i})-y_{\\Re}\\right\\rVert^{2}V_{i,i+1}^{\\prime}-\\left\\lVert\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_{i})-y_{\\Re}\\right\\rVert^{2}V_{i,i+1}^{\\prime}\\right\\rVert+}\\\\ &{\\quad\\left(\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_{i})-y_{\\Re}\\right)W^{\\mathrm{O}}(V_{i,i+1}^{\\prime})-\\left(\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_{i})-y_{\\Re}\\right)W^{\\mathrm{O}}(V_{i,i}^{\\prime})\\right\\rVert_{\\boldsymbol{F}}}\\\\ &{\\stackrel{(a)}{\\le}\\left\\lVert\\mathbf{M}^{\\mathrm{H}}(M_{H+1:}X_{i})-\\mathbf{M}^{\\mathrm{H}}(M_{H};X_{i})\\right\\rVert_{\\boldsymbol{F}}^{(r)}+\\left\\lVert\\mathbf{M}^{\\mathrm{H}}(\\mathbf{M}_{H+1:}X_{i})-y_{\\Re}\\right\\rVert(V_{i,i+1}^{\\prime}-V_{i,i}^{\\prime})W^{\\mathrm{O}}\\rVert_{\\boldsymbol{F}}}\\\\ &{\\stackrel{(c)}{\\le}\\frac{r}{2}\\left\\lVert\\mathbf{M}^{\\mathrm{H}}(\\mathbf{M}_{H+1:}X_{i})-\\mathbf{M}^{\\mathrm{H}}(\\mathbf{M}_{H};X_{i})\\right\\rVert_{\\boldsymbol{F}}^{(r)}\\left\\lVert\\mathbf{M}^{\\mathrm{O}}\\right\\rVert_{\\boldsymbol{F}}+\\left\\lVert\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_{i})-y_{\\Re}\\right\\rVert(V_{i,i+1}^{\\prime}-V_{i,i}^{\\prime})W^{\\mathrm{O}}\\rVert_{\\boldsymbol{F}}}\\\\ &{\\quad+\\left(\\lVert\\mathbf{M}^{\\mathrm{H}}(A_{H+1:}X_\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (i) is because of the triangle inequality; (ii) uses the definition of $Z_{i}$ in Equation (74), CauchySchwartz inequality and triangle inequality; (i) uses the Cauchy-Schwartz inequality; (iv) reorganizes the terms in (i). Plug Equation (82) into Equation (81), we can finally derive the bound for $\\begin{array}{r}{\\|\\nabla_{W^{Q}}f\\left(M_{t+1};X_{i}\\right)-\\mathbf{\\bar{V}}_{W^{Q}}f\\left(M_{t+1};X_{i}\\right)\\|_{F}}\\end{array}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W^{Q}}f(M_{t+1};X_{i})-\\nabla_{W^{Q}}f(M_{t+1};X_{i})\\|_{F}}\\\\ &{\\overset{(i)}{\\le}n\\sqrt{H}\\left(\\|R_{i,t+1}-R_{i,t}\\|_{F}+\\|R_{i,t}\\|_{F}\\left\\|S_{i,t+1}-S_{i,t}\\right\\|_{F}+\\bar{R}\\left\\|S_{i,t+1}-S_{i,t}\\right\\|\\right)}\\\\ &{\\overset{(i i)}{\\le}n\\sqrt{H}P_{i}\\|M_{t+1}-M_{t}\\|_{F}+2\\bar{R}n\\sqrt{H}\\|S_{i,t+1}-S_{i,t}\\|_{F}}\\\\ &{\\overset{(i i i)}{\\le}n\\sqrt{H}P_{i}\\|M_{t+1}-M_{t}\\|_{F}+2\\bar{R}n\\sqrt{H}\\sqrt{\\phi_{i}^{2}+\\psi_{i}^{2}}\\|M_{t+1}-M_{t}\\|_{F}}\\\\ &{:=L_{i}^{Q}\\|M_{t+1}-M_{t}\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (i) is from Equation (81); (i) uses the definition of $\\bar{R}$ in Equation (80); (iii) comes from Lemma 3 (3). Since $W^{Q}$ and $W^{K}$ are symmetric in the Transormer structure, similarly, we can derive $L_{i}^{K}$ ", "page_idx": 24}, {"type": "text", "text": "Step 2: In this step, we aim to derive bound for $\\begin{array}{r}{\\|\\nabla_{W^{\\upsilon}}f\\left(M_{t+1};X_{i}\\right)-\\nabla_{W^{\\upsilon}}f\\left(M_{t};X_{i}\\right)\\|_{F}.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r l}&{|\\nabla\\pi\\nu^{\\prime}\\big/(H_{\\mathbb{I}+1};X_{i}\\big)-\\nabla\\mathrm{tr}^{\\prime}\\big/(H_{\\mathbb{I}}+\\chi_{1}\\big)|\\nu_{\\mathbb{I}}\\big/\\qquad}\\\\ &{\\overset{(a)}{\\leq}\\left\\lVert\\mathbf{B}_{i,t+1}^{\\prime}\\left(\\mathbf{M}(M_{i+1};X_{i})-y\\right)\\left({\\mathbf{b}}^{\\prime}\\right)^{\\top}-\\mathbf{B}_{i,t}^{\\prime}\\left(\\mathbf{M}\\mathbf{I}\\left(M_{i};X_{i}\\right)-y_{i}\\right)\\left({\\mathbf{b}}^{\\prime}\\right)^{\\top}\\right\\rVert_{F}}\\\\ &{\\overset{(b)}{\\leq}\\left\\lVert\\mathbf{B}_{i,t+1}^{\\prime}\\left(\\mathbf{M}(M_{i+1};X_{i})-y_{i}\\right)\\left({\\mathbf{b}}^{\\prime}\\right)^{\\top}-B_{i,t+1}^{\\prime}\\left(\\mathbf{M}(M_{i};X_{i})-y_{i}\\right)\\left({\\mathbf{b}}^{\\prime}\\right)^{\\top}\\right\\rVert_{F}}\\\\ &{\\ \\ \\ \\ \\ +\\left\\lVert\\mathbf{B}_{i,t+1}^{\\prime}\\left(\\mathbf{M}(M_{i};X_{i})-y_{i}\\right)\\left({\\mathbf{b}}^{\\prime}\\right)^{\\top}-B_{i,t}^{\\prime}\\left(\\mathbf{M}(M_{i};X_{i})-y_{i}\\right)\\left({\\mathbf{b}}^{\\prime}\\right)^{\\top}\\right\\rVert_{F}}\\\\ &{\\overset{(c)}{\\leq}\\left\\lVert\\mathbf{B}_{i,t+1}^{\\prime}\\left\\rVert\\mathbf{I}\\left\\lVert\\mathbf{I}\\left\\lVert\\mathbf{I}\\left(M_{i+1};X_{i}\\right)-\\mathbf{M}(M_{i};X_{i})\\right\\rVert_{F}\\left\\lVert{\\mathbf{I}}^{\\prime}\\right\\rVert_{F}^{(b)}\\right\\rVert_{+1}\\left\\lVert\\mathbf{B}_{i,t+1}^{\\prime}-\\mathbf{B}_{i,t}\\right\\rVert_{F}\\left\\lVert\\mathbf{I}\\left(\\mathbf{M}(M_{i};X_{i})-\\mathbf{B}_{i,t}\\right)\\right\\rVert_{F}}\\\\ &{\\overset{(c)}{\\leq}\\left\\lVert\\mathbf{B}_{i,t}^{\\prime}\\right\\rVert_{\\mathbb{I}}\\left\\lVert{\\mathbf{I}}^{\\prime}\\right\\rVert_{\\mathbb{I}}^{(b)}\\left\\lVert\\mathbf{I}\\left(1$ -yillwl12 ", "page_idx": 25}, {"type": "text", "text": "where (i) is from Lemma 1 (1); (i) uses triangle inequality; (ii) uses Cauchy-Schwartz inequality; (iv) comes from the definition of $B_{i,t}$ $Z_{i}$ (in Equation (74), Cauchy-Schwartz inequality and triangle inequality; (v) comes from Lemma 2 (3) and Cauchy-Schwartz inequality; (vi) reorganizes (v). ", "page_idx": 25}, {"type": "text", "text": "Now we combine the result in Step 1 and Step 2, and plug into Equation (78), we can finally derive ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W}f\\left(M_{t+1};X\\right)-\\nabla_{W}f\\left(M_{t};X\\right)\\|_{F}\\leq\\displaystyle\\sum_{i=1}^{N}(L_{i}^{Q}+L_{i}^{K}+L_{i}^{V})\\|M_{t+1}-M_{t}\\|_{F}}\\\\ &{\\leq N(\\operatorname*{max}_{i}L_{i}^{Q}+\\operatorname*{max}_{i}L_{i}^{K}+\\operatorname*{max}_{i}L_{i}^{V})\\|M_{t+1}-M_{t}\\|_{F}}\\\\ &{:=G\\|M_{t+1}-M_{t}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "1.7Proof of Lemma in Section 1.4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. Proof of Lemma 6 (1): We consider the differential of the element in the $k$ -throwand $j$ -th column. First, let us write down the closed form of each element: ", "page_idx": 25}, {"type": "equation", "text": "$$\n(C_{i h})_{k j}=-\\|X_{i k}.W_{h}^{Q}-X_{i j}.W_{h}^{K}\\|^{2}/2\\sqrt{d}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we consider the differential of each element over $W_{h}^{Q}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d\\left({C_{i h}}\\right)_{k j}=-\\displaystyle\\frac{1}{{2\\sqrt{d}}}\\left({\\left\\|{{X_{i k\\cdot}}\\left({W_{h}^{Q}+d(W_{h}^{Q})}\\right)-{X_{i j}}{W_{h}^{K}}}\\right\\|^{2}-\\left\\|{{X_{i k\\cdot}}(W_{h}^{Q})-{X_{i j}}{W_{h}^{K}}}\\right\\|^{2}}\\right)}}\\\\ {{=-\\displaystyle\\frac{1}{{\\sqrt{d}}}\\langle{X_{i k\\cdot}}d(W_{h}^{Q}),{X_{i k\\cdot}}W_{h}^{Q}-{X_{i j}}{W_{h}^{K}}\\rangle+o\\big(d(W_{h}^{Q})\\big),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $o\\big(d(W_{h}^{Q})\\big)$ denotes the higher order of $d(W_{h}^{Q})$ . Leave out the higher order differential term, we derive ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d(C_{i h})_{k j}\\parallel_{F}\\leq\\frac{1}{\\sqrt{d}}\\left(\\|X_{i k\\cdot}\\|_{2}\\|d(W_{h}^{Q})\\|_{F}\\cdot\\sigma_{\\operatorname*{max}}(W_{h}^{Q})\\|X_{i k\\cdot}\\|_{2}+\\|d(W_{h}^{Q})\\|_{F}\\cdot\\sigma_{\\operatorname*{max}}(W_{h}^{K})\\|X_{i k\\cdot}\\|_{2}\\right)}\\\\ &{\\leq\\frac{1}{\\sqrt{d}}\\|X_{i k\\cdot}\\|_{2}\\|d(W_{h}^{Q})\\|_{F}(\\sigma_{\\operatorname*{max}}(W_{h}^{Q})\\|X_{i k\\cdot}\\|_{2}+\\sigma_{\\operatorname*{max}}(W_{h}^{K})\\|X_{i j\\cdot}\\|_{2})}\\\\ &{\\leq\\frac{1}{\\sqrt{d}}\\|X_{i k\\cdot}\\|_{2}\\sqrt{\\sigma_{\\operatorname*{max}}^{2}(W_{h}^{Q})+\\sigma_{\\operatorname*{max}}^{2}(W_{h}^{K})}\\cdot\\sqrt{\\|X_{i k\\cdot}\\|_{2}^{2}+\\|X_{i j\\cdot}\\|_{2}^{2}}\\|d(W_{h}^{Q})\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|d(C_{h})\\|_{F}=\\displaystyle\\sum_{k=1}^{n}\\displaystyle\\sum_{l=1}^{n}\\|d(C_{h})_{k}\\|_{F}^{2}\\|^{2}}\\\\ &{\\le\\displaystyle\\frac{1}{\\sqrt{d}}\\displaystyle\\sum_{k=1}^{n}\\displaystyle\\sum_{j=2\\lceil1\\rceil}^{n}\\|X_{k+\\frac{1}{2}\\rfloor}\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{0}\\right)}+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)\\cdot\\sqrt{\\|X_{\\hat{\\pi}^{k}}\\|_{2}^{2}+\\|X_{\\hat{\\pi}^{j}}\\|_{2}^{2}}\\|d(W_{h}^{0})\\|_{F}}\\\\ &{\\le\\displaystyle\\frac{1}{\\sqrt{d}}\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{0}\\right)}+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)\\displaystyle\\sum_{k=1}^{n}\\|X_{k+\\frac{1}{2}\\lceil\\sqrt{n}\\rceil}C_{h}\\|\\|X_{k+\\frac{1}{2}\\rceil^{2}}+\\displaystyle\\sum_{j=1}^{n}\\|X_{\\hat{\\pi}^{j}}\\|_{2}^{2}\\|d(W_{h}^{0})\\|_{F}}\\\\ &{\\le\\displaystyle\\frac{1}{\\sqrt{d}}\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{0}\\right)}+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)\\cdot\\sqrt{\\sum_{k=1}^{n}\\|X_{k+\\frac{1}{2}\\rceil^{2}}}\\cdot\\sqrt{\\sum_{k=1}^{n}\\|X_{k+\\frac{1}{2}\\rceil^{2}}+\\sum_{j=1}^{n}\\|X_{\\hat{\\pi}^{j}}\\|_{2}^{2}}}\\\\ &{=\\displaystyle\\frac{1}{\\sqrt{d}}\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{0}\\right)}+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)\\cdot\\|X_{1}\\|_{F}\\cdot\\sqrt{2n}\\|X_{1}\\|\\,\\mathrm{e}\\,\\|(W_{h}^{0})\\|_{F}}\\\\ &{=\\displaystyle\\sqrt{\\frac{2n}{d}}\\|X_{1} \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "PrfPost srel $\\frac{\\partial(C_{i h})_{k j}}{\\partial W_{h}^{Q}}$ We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\partial(C_{i h})_{k j}}{\\partial W_{h}^{Q}}=-(X_{i k}.W_{h}^{Q}-X_{i j.}W_{h}^{K})\\mathbb{I}_{d}\\otimes X_{i k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, we can derive upper bound for $\\left\\|d\\left(\\frac{\\partial(C_{i h})_{k j}}{\\partial W_{h}^{Q}}\\right)\\right\\|_{F}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|d\\left(\\frac{\\partial\\left(C_{i h}\\right)_{k j}}{\\partial W_{h}^{Q}}\\right)\\right\\|_{F}=\\left\\|-\\left(X_{i k\\cdot}(W_{h}^{Q}+d(W_{h}^{Q}))-X_{i j\\cdot}W_{h}^{K}\\right)\\mathbb{I}_{d}\\otimes X_{i k}+\\left(X_{i k\\cdot}W_{h}^{Q}-X_{i j\\cdot}W_{h}^{K}\\right)\\mathbb{I}_{d}\\otimes X_{i k}}}\\\\ &{=\\|X_{i k\\cdot}d(W_{h}^{Q})\\mathbb{I}_{d}\\otimes X_{i k\\cdot}\\|_{F}/\\sqrt{d}}\\\\ &{\\leq\\|X_{i k\\cdot}\\|_{2}\\|\\mathbb{I}_{d}\\|_{F}\\|d(W_{h}^{Q})\\|_{F}/\\sqrt{d}}\\\\ &{=\\|X_{i k\\cdot}\\|_{2}^{2}\\|d(W_{h}^{Q})\\|_{F}}&{\\overset{\\mathrm{(86)}}{=}\\|X_{i k\\cdot}\\|_{2}^{2}\\|d(W_{h}^{Q})\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, we have the following: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|d\\left(\\frac{\\partial\\left(C_{i h}\\right)}{\\partial W_{h}^{Q}}\\right)\\right\\|_{F}\\leq\\displaystyle\\sum_{k=1}^{n}\\sum_{j=1}^{n}\\left\\|d\\left(\\frac{\\partial\\left(C_{i h}\\right)_{k j}}{\\partial W_{h}^{Q}}\\right)\\right\\|_{F}}\\\\ &{\\leq\\|d(W_{h}^{Q})\\|_{F}\\displaystyle\\sum_{k=1}^{n}\\sum_{j=1}^{n}\\|X_{i k}.\\|_{2}^{2}}\\\\ &{\\leq n\\|X_{i}\\|_{F}^{2}\\|d(W_{h}^{Q})\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Proof of Lemma 7 (3): ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}\\right\\|_{F}=\\left\\|\\left(\\left(\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}\\right\\|_{F}}\\\\ &{\\geq\\left\\|\\left(\\left(\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\right\\|_{F}\\cdot\\operatorname*{min}{|S_{i}|}}\\\\ &{\\geq\\operatorname*{min}{|V_{i}^{\\prime}W^{O}|\\cdot\\operatorname*{min}{|S_{i}|\\cdot\\|\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\|_{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Proof of Lemma 7 (4): ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial W_{h}^{Q}}\\right|\\right|_{F}=\\left|\\left|\\mathrm{vec}\\left(\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial W_{h}^{Q}}\\right)\\right|\\right|_{2}=\\left\\|\\mathrm{vec}\\left(\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}\\right)\\cdot\\frac{\\partial C_{i}}{\\partial W_{h}^{Q}}\\right|\\right|_{2}}\\\\ &{\\leq\\left\\|\\frac{\\partial f\\left(M;X_{i}\\right)}{\\partial C_{i}}\\right\\|_{F}\\cdot\\left\\|\\frac{\\partial C_{i}}{\\partial W_{h}^{Q}}\\right\\|_{2}}\\\\ &{=\\left\\|\\left(\\left(\\left(\\mathsf{M}\\mathsf{H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}\\right\\|_{F}\\cdot\\sqrt{\\frac{2n}{d}}\\left\\|X_{i}\\right\\|_{F}^{2}\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{Q}\\right)+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}}\\\\ &{\\leq\\sqrt{\\frac{2n}{d}}\\left\\|X_{i}\\right\\|_{F}^{3}\\left\\|W^{O}\\right\\|_{2}\\sigma_{\\operatorname*{max}}(W^{V})\\sqrt{\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{Q}\\right)+\\sigma_{\\operatorname*{max}}^{2}\\left(W_{h}^{K}\\right)}\\left\\|\\mathsf{M}\\mathsf{H}\\left(M;X_{i}\\right)-y_{i}\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Proof of Lemma 8 (1): The proof is similar to the proof of Lemma 3 (1). So we do not include the details here. We can similarly derive ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f\\left(M_{t+1};X\\right)-f\\left(M_{t};X\\right)||_{2}\\le N\\sqrt{\\operatorname*{max}_{i}Q_{i}^{\\prime2}+\\operatorname*{max}_{i}K_{i}^{\\prime2}+n^{2}H\\operatorname*{max}_{i}\\left\\|X_{i}\\right\\|_{F}^{2}\\left\\|W^{O}\\right\\|_{2}^{2}}\\left\\|M_{t+1}-M_{t}\\right\\|_{F}}\\\\ &{:=Z^{\\prime}\\left\\|M_{t+1}-M_{t}\\right\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Proof of Lemma 8 (2): By triangle inequality, we have ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\nabla_{W}f(M_{t+1};X)-\\nabla_{W}f(M_{t};X)||_{F}}\\\\ &{\\le\\|\\nabla_{W^{Q}}f(M_{t+1};X)-\\nabla_{W^{Q}}f(M_{t+1};X)\\|_{F}+\\|\\nabla_{W^{K}}f(M_{t+1};X)-\\nabla_{W^{K}}f(M_{t+1};X)\\|_{F}}\\\\ &{\\quad+\\left\\|\\nabla_{W^{V}}f(M_{t+1};X)-\\nabla_{W^{V}}f(M_{t+1};X)\\right\\|_{F}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{N}\\left(\\|\\nabla_{W^{Q}}f\\left(M_{t+1};X\\right)-\\nabla_{W^{Q}}f\\left(M_{t+1};X\\right)\\|_{F}+\\left\\|\\nabla_{W^{K}}f\\left(M_{t+1};X\\right)-\\nabla_{W^{K}}f\\left(M_{t+1};X\\right)\\right\\|_{F}\\right.}\\\\ &{\\quad+\\left\\|\\nabla_{W^{v}}f\\left(M_{t+1};X\\right)-\\nabla_{W^{v}}f\\left(M_{t+1};X\\right)\\|_{F}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Step 1: Derive upper bound for ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{W}\\circ f(M_{t+1};X_{i}))-\\nabla_{W^{Q}}f(M_{t};X_{i}))\\|_{F}=\\|\\operatorname{vec}(\\nabla_{W^{Q}}f(M_{t+1};X_{i}))-\\operatorname{vec}(\\nabla_{W^{Q}}f(M_{t};X_{i}))\\|_{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "First,  we  give  the  vectorized expression  of $\\nabla_{W^{Q}}f\\left(M_{t};X_{i}\\right)$ .Recall we denote $\\begin{array}{r l}{U_{i}}&{{}=}\\end{array}$ $\\left(\\left(\\mathsf{M H}\\left(M;X_{i}\\right)-y_{i}\\right)\\left(W^{O}\\right)^{\\top}\\left(V_{i}^{\\prime}\\right)^{\\top}\\right)\\odot S_{i}$ . By Lemma 6, we can derive the close form of $\\mathrm{vec}(\\nabla_{W^{Q}}f(M_{t};X_{i}))$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{vec}(\\nabla_{W}\\!e\\,f(M;X_{i}))\\overset{(i)}{=}\\operatorname{vec}(U_{i})\\cdot\\operatorname{vec}\\left(\\frac{\\partial C_{i}}{\\partial W_{h}^{Q}}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Further, recall we have defined $\\bar{R}$ and the following inequality holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|U_{i}\\|_{F}\\le\\bar{R}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, let us derive upper bound for $\\begin{array}{r}{\\|\\nabla_{W^{Q}}f\\left(M_{t+1};X_{i}\\right)-\\nabla_{W^{Q}}f\\left(M_{t+1};X_{i}\\right)\\|_{F}.}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\nabla_{\\mathbf{v}}\\circ[(M_{t+1})\\,...\\,\\nabla_{v}\\circ\\mathbf{v}_{i}(M_{t+1})\\,...\\,N_{t}],}\\\\ &{\\Vert\\nabla_{v}[\\mathbf{v}_{i,t+1}]\\cdot\\left(\\frac{\\partial C_{v}(M_{t+1})}{\\partial(v)}\\right)-\\Vert\\mathbf{v}(v)\\,...\\,\\left(\\frac{\\partial C_{v}(M_{t})}{\\partial(v)}\\right)\\Vert_{v},}\\\\ &{=\\left\\Vert\\mathbf{w}\\circ[(v)_{i,t+1}]\\cdot\\left(\\frac{\\partial C_{v}(M_{t+1})}{\\partial(v)}\\right)-\\mathbf{v}(v)_{i,t+1}\\cdot\\left(\\frac{\\partial C_{v}(M_{t})}{\\partial(v)}\\right)\\right.}\\\\ &{\\qquad+\\mathbf{w}\\circ[(v)_{i,t+1}]\\cdot\\left(\\frac{\\partial C_{v}(M_{t})}{\\partial(v)}\\right)-\\mathbf{w}\\circ[(v)_{i,t}\\cdot\\left(\\frac{\\partial C_{v}(M_{t})}{\\partial(v)}\\right)],}\\\\ &{\\left.\\,\\frac{\\mathrm{D}}{\\mathrm{D}}\\left[\\mathbf{w}_{v}(v)_{i,t+1}\\right]\\left\\Vert\\frac{\\partial C_{v}(M_{t+1})}{\\partial(v)}-\\frac{\\partial C_{v}(M_{t})}{\\partial(v)}\\right\\Vert_{v}^{2}+\\Vert\\mathbf{v}(v)_{i,t+1}-v\\right]_{0}\\Vert\\left\\Vert\\frac{\\partial C_{v}(M_{t})}{\\partial(v)}\\right\\Vert_{v}}\\\\ &{\\overset{(a)}{\\leq}\\Vert\\mathbf{w}_{v}[\\mathbf{v}_{i,t+1}]\\Vert_{v}^{2}\\left\\Vert\\left(\\frac{\\partial C_{v}(M_{t+1})}{\\partial(v)}\\right)\\Vert_{v}+\\Vert\\mathbf{v}(v)_{i,t}-v\\right\\Vert_{v}\\cdot\\sqrt{\\mathbf{v}}\\Vert_{v}^{3},}\\\\ &{\\leq R\\sqrt{\\Vert\\mathbf{v}\\Vert}\\mathbf{x}_{t}^{2}\\cdot\\left\\Vert\\left(\\mathbf{w}_{v}^{2}\\right)\\Vert_{v}+\\Vert\\mathbf{v}\\Vert\\cdot\\left(\\frac{\\partial C_{v}(M_{t+1})}{\\partial(v)}\\right)\\Vert_{v}+\\Vert\\mathbf{v}_{i,t}-\\left(\\frac{\\partial C_{v}}{\\partial(v)}\\right)\\Vert_{v+1}\\cdot\\left(\\mathbf \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Next, we aim to derive upper bound of $\\|R_{i,t+1}-R_{i,t}\\|_{F}$ in Equation (92). Similar to the derivation in Equation (81), we can derive ", "page_idx": 28}, {"type": "text", "text": "$\\begin{array}{r l r}&{\\left\\|{R_{i,t+1}-R_{i,t}}\\right\\|_{F}\\stackrel{(i v)}{\\leq}\\left(Z_{i}^{\\prime}\\left\\|{X_{i}}\\right\\|_{F}\\sigma_{\\operatorname*{max}}\\left(W^{V}\\right)\\left\\|{W^{O}}\\right\\|_{2}+\\left(n\\sqrt{H}\\sigma_{\\operatorname*{max}}\\left(W^{V}\\right)\\left\\|{X_{i}}\\right\\|_{F}\\left\\|{W^{O}}\\right\\|_{2}+\\left\\|{X_{i}}\\right\\|_{F}\\right)\\left\\|{W^{O}}\\right\\|_{2}}\\\\ &{\\quad\\times\\left\\|{M_{t+1}-M_{t}}\\right\\|_{F}}&{}\\\\ &{:=P_{i}^{\\prime}\\|M_{t+1}-M_{t}\\|_{F},}&{(93)}\\end{array}$ \uff09", "page_idx": 28}, {"type": "text", "text": "Plug Equation (82) into Equation (92), we can finally derive the bound for $\\|\\nabla_{W^{Q}}^{-}f\\left(M_{t+1};X_{i}\\right)-\\nabla_{W^{Q}}f\\left(\\bar{M_{t+1}};X_{i}\\right)\\|_{F}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W}q\\,f(M_{t+1};X_{i})-\\nabla_{W}q\\,f(M_{t+1};X_{i})\\|_{F}}\\\\ &{\\leq\\bar{R}\\sqrt{n}\\,\\|X_{i}\\|_{F}^{2}\\cdot\\Big\\|d\\Big(W_{h}^{Q}\\Big)\\Big\\|_{F}+\\sqrt{n}\\,\\|X_{i}\\|_{F}^{2}\\cdot\\Big(\\sigma_{\\operatorname*{max}}\\left(W_{h}^{Q}\\right)+\\sigma_{\\operatorname*{max}}\\left(W_{h}^{K}\\right)\\Big)}\\\\ &{\\quad\\times\\,(\\|R_{i,t+1}-R_{i,t}\\|_{F}+\\|R_{i,t}\\|_{F}\\,\\|S_{i,t+1}-S_{i,t}\\|_{F})}\\\\ &{\\leq\\bar{R}\\sqrt{n}\\,\\|X_{i}\\|_{F}^{2}\\cdot\\|M_{t+1}-M_{t}\\|_{F}+\\sqrt{n}\\,\\|X_{i}\\|_{F}^{2}\\cdot\\Big(\\sigma_{\\operatorname*{max}}\\left(W_{h}^{Q}\\right)+\\sigma_{\\operatorname*{max}}\\left(W_{h}^{K}\\right)\\Big)}\\\\ &{\\quad\\times\\,\\Big(P_{i}^{\\prime}\\|M_{t+1}-M_{t}\\|_{F}+\\sqrt{n}\\bar{R}\\,\\|X_{i}\\|_{F}^{2}\\cdot\\Big(\\sigma_{\\operatorname*{max}}\\left(W_{h}^{Q}\\right)+\\sigma_{\\operatorname*{max}}\\left(W_{h}^{K}\\right)\\Big)\\,\\|M_{t+1}-M_{t}\\|_{F}\\Big)}\\\\ &{:=L_{i}^{Q^{\\prime}}\\|M_{t+1}-M_{t}\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and plug into Equation (78), we can finally derive ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{W}f\\left(M_{t+1};X\\right)-\\nabla_{W}f\\left(M_{t};X\\right)\\|_{F}\\leq\\displaystyle\\sum_{i=1}^{N}(L_{i}^{Q}+L_{i}^{K}+L_{i}^{V})\\|M_{t+1}-M_{t}\\|_{F}}\\\\ &{\\leq N(\\operatorname*{max}_{i}L_{i}^{Q}+\\operatorname*{max}_{i}L_{i}^{K}+\\operatorname*{max}_{i}L_{i}^{V})\\|M_{t+1}-M_{t}\\|_{F}}\\\\ &{:=G\\|M_{t+1}-M_{t}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "2 NeurIPS paper checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: See Theorem 1,2,3 ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Please see our conclusion 6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\"\u2019 section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Appendix, which provides proof for each Theorem. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See experiment setting in Section 5.2 ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not include the open access to code. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See Section 5.2 for experiment setting. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Please see Fig 2 and Fig 3. We have a 1- $\\sigma$ error bar. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: We do not include the compute resources detail. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper has no harm in the research process or negative social impact. The paper is anonymous. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: There is no societal impact ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification:The paper poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have cited the code framework we use Chen et al. [2021]. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]