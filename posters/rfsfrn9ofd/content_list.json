[{"type": "text", "text": "EEG2Video: Towards Decoding Dynamic Visual Perception from EEG Signals ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuan-Hao $\\mathrm{\\mathbf{Liu^{1*\\dagger}}}$ , Yan-Kai Liu1\u2217, Yansen $\\mathbf{Wang}^{2\\ddagger}$ , $\\mathbf{Kan\\Ren^{3\\ddagger}}$ , Hanwen $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{1}$ , Zilong $\\mathbf{Wang^{2}}$ , Dongsheng $\\mathbf{Li}^{2}$ , Bao-Liang $\\mathbf{L}\\mathbf{u}^{1}$ , Wei-Long Zheng1\u2021 1Shanghai Jiao Tong University 2Microsoft Research Asia 3ShanghaiTech University https://bcmi.sjtu.edu.cn/home/eeg2video ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Our visual experience in daily life are dominated by dynamic change. Decoding such dynamic information from brain activity can enhance the understanding of the brain\u2019s visual processing system. However, previous studies predominately focus on reconstructing static visual stimuli. In this paper, we explore to decode dynamic visual perception from electroencephalography (EEG), a neuroimaging technique able to record brain activity with high temporal resolution $\\mathrm{[1000\\,Hz)}$ for capturing rapid changes in brains. Our contributions are threefold: Firstly, we develop a large dataset recording signals from 20 subjects while they were watching 1400 dynamic video clips of 40 concepts. This dataset flils the gap in the lack of EEG-video pairs. Secondly, we annotate each video clip to investigate the potential for decoding some specific meta information (e.g., color, dynamic, human or not) from EEG. Thirdly, we propose a novel baseline EEG2Video for video reconstruction from EEG signals that better aligns dynamic movements with high temporal resolution brain signals by Seq2Seq architecture. EEG2Video achieves a 2-way accuracy of $79.8\\%$ in semantic classification tasks and 0.256 in structural similarity index (SSIM). Overall, our works takes an important step towards decoding dynamic visual perception from EEG signals. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Our visual experience are composed of continuously evolving scenes caused by the movement of objects and viewing perspective [1]. The intricate and complex visual system in our brains which enables us to explore the wonderful and ever-changing visual world has been appealing interests from philosophers and scientists for centuries [2\u20136]. To investigate the mechanism of our visual system, various neuroimaging techniques have been used to analyze brain activities, especially non-invasive methods like functional Magnetic Resonance Imaging (fMRI) [2, 7, 8], magnetoencephalography(MEG) [9\u201311], and electroencephalography(EEG) [12, 13]. ", "page_idx": 0}, {"type": "text", "text": "Compared to fMRI and MEG which need to be recorded by large and expensive medical devices, EEG is relatively low-cost and portable and thus has been applied across many human visual studies [14\u201318]. For instance, a recent work achieved an accuracy of $15.6\\%$ in 200-way zero-shot tasks on an EEG-image dataset [19], demonstrating the rich visual information in EEG signals. However, these studies adopt either artificial strobe (SSVEP) [14, 15] or static image [16\u201318] as stimulation, which is far different from the dynamic visual world and not suitable for studying brain activities in naturalistic paradigm. As far as we know, there is currently no research on decoding video from EEG signals. Consequently, people have limited knowledge about 1) whether can we decode video from EEG signals? 2) if yes, what kind of visual information can we decode? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In order to fill the gap, we develop a large EEG dataset, called SJTU EEG Dataset for Dynamic Vision (SEED-DV) dataset, collected from 20 subjects while they were watching a series of natural videos belonging to 40 different concepts. Also, we annotate some meta information for each video clip to comprehensively explore the boundary of which visual information can be decoded from EEG signals, offering a benchmark containing various visual decoding tasks across object recognition, color/motion perception, and human/face detection. ", "page_idx": 1}, {"type": "text", "text": "Besides the fundamental classification tasks, reconstructing visual perceptions from corresponding brain signals helps to advance the understanding of our visual neural system. With the development of the representation learning and artificial intelligence generated content (AIGC), numerous works have reconstructed vivid images from brain activities [20\u201330], which utilize the text-to-image generation models pretrained on large amounts of visual-language pairs by aligning the brain signals with corresponding text embeddings. Recently, some works reconstruct high-quality two-second videos from a single fMRI data frame [31, 32]. However, limited by the low temporal resolution of fMRI, these video generation frameworks lack the ability of capturing high dynamic changes. ", "page_idx": 1}, {"type": "text", "text": "To this end, we propose a novel baseline, EEG2Video, for video reconstruction from EEG signals based on Seq2Seq architectures which extract continuous low-level dynamic visual perception such as color and position from the brain signals of high temporal resolution. Afterwards, a dynamicaware noise-adding (DANA) method is adopted for the diffusion process according to the dynamic information decoded from EEG. At last, we adopt the inflated diffusion model [33] fine-tuned on our dataset for video generation using the semantic information predicted from EEG. Our method densely extract visual information from high temporal resolution brain signals thus can better recover fast changes. Overall, our video reconstruction results take an important step towards decoding dynamic visual perception from EEG. ", "page_idx": 1}, {"type": "text", "text": "In conclusion, our contribution are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 For the first time, we develop a large EEG dataset named SEED-DV dataset collected from 20 subjects, offering 1400 EEG-video pairs from 40 concepts for studying dynamic visual information in EEG signals.   \n\u2022 We annotate the meta information of each video clip for comprehensively analyzing the visual information in EEG, presenting the EEG-VP benchmark.   \n\u2022 We evaluate various EEG models on the EEG-VP benchmark to determine the decoding ability of different visual information in raw EEG signals and human-extracted features.   \n\u2022 We propose a novel framework named EEG2Video for video reconstruction from EEG signals based on Seq2Seq architecture to densely utilize the highly dynamic information.   \n\u2022 The ablation study showcases the effectiveness of Seq2Seq and DANA modules in EEG2Video, which are designed based on the decoding results of different visual information on the EEG-VP benchmark. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Decoding Static Visual Perception from Brain Activities ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Researchers have been trying to decode low-level static visual perception (e.g., shape, color, and position) from brain activities for decades [34\u201337], revealing the abundant visual information hidden in brain signals. Early approaches tried to generate hazy silhouette or higher-quality images with Deconvolutional Neural Networks (DeCNNs)[38, 39], Generative Adversarial Networks (GANs)[40\u2013 44], and Variational Autoencoders (VAEs) [44]. Recent studies have obtained impressive results on decoding static visual stimuli from various brain activities like fMRI [20\u201327], MEG [28], and EEG [29, 30] with the Latent Diffusion Models (LDMs), also named Stable Diffusion (SD) [45\u201347], as image generation module, which is pretrained on a large-scale image dataset to generate vivid images based on text prompts. Specifically, text prompts are embedded into a text-image sharing latent space by CLIP [48], a multi-modal encoder to align visual and language representations. By encoding brain signals into the same sharing space, these methods can decode static visual perception with high diversity and fidelity using LDMs. ", "page_idx": 1}, {"type": "text", "text": "Compared to the high spatial resolutions of brain signals recorded by large devices like fMRI ( $\\approx$ 100,000 voxels) and MEG $\\approx300$ sensors), the limited spatial resolution of EEG ( $\\approx60$ sensors) brings difficulties to decode accurate visual perception from EEG in both semantic and pixel levels. Although previous studies claimed to achieve over $60\\%$ of semantic decoding accuracy on an EEG dataset with 40 classes [29, 30], the dataset[49] was blamed due to the block design [50]. The followup rigorous experiment [51] by randomly arranging all images showed at most $7.0\\%$ classification accuracy of the EEG siganls (chance level is $2.5\\%$ ), exposing the fact that it is still challenging and insufficiently supported by appropriate datasets to decode visual perception from EEG. ", "page_idx": 2}, {"type": "text", "text": "2.2 Decoding Dynamic Visual Perception from Brain Activities ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Observing the great success in decoding static visual perception, many endeavours have been devoted to decoding dynamic visual perception from fMRI [52\u201354, 31, 32]. These works were conducted on an fMRI-video dataset collected from 3 female subjects while watching a series of videos, including animals, humans, and natural scenery [52]. Due to the data sparsity, the DeCNN, VAE and GANbased methods can only decode hazy perception of dynamic videos from fMRI [52\u201354]. To generate high-quality videos, MinD-Video[31] and NeuroCine[32] utilized an inflated SD model [33] for video generation, which incorporates network temporal inflation by adding temporal attention modules in the original SD to ensure the consistency between frames. The video SD is firstly fine-tuned using the text-video pairs in the training dataset (the text prompts were generated by an automatic image caption model called BLIP [55]), then the pre-trained fMRI encoder were co-trained with the video SD to enhance the fMRI guidance. ", "page_idx": 2}, {"type": "text", "text": "However, fMRI intrinsically lacks the ability of capturing dynamic visual perception due to the low temporal resolution, which is limited by the time scale of blood flow and results in a single fMRI frame every two seconds. MinD-Video[31] and NeuroCine[32] all decoded two-second videos from only one fMRI frame. Thus, they are unable to decode changes faster than $0.5\\:\\mathrm{Hz}$ . Instead, other neuroimaging techniques such as EEG with high temporal resolution up to thousands $\\mathrm{Hz}$ can offer more appropriate alternatives. To the best of our knowledge, there is no such dataset studying decoding video stimuli from such signals, and we are the first to support this research direction with the dataset, benchmarks, and the decoding framework. ", "page_idx": 2}, {"type": "text", "text": "3 EEG Dynamic Vision Dataset and Benchmarks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first describe how we construct the SJTU EEG Dataset for Dynamic Vision (SEEDDV). Then we introduce two benchmarks built on the SEED-DV dataset: EEG visual perception classification benchmark and video reconstruction benchmark. The purpose of building this new dataset is to answer the following research questions: ", "page_idx": 2}, {"type": "text", "text": "RQ1 $:$ Whether can we decode dynamic visual information from EEG signals? ", "page_idx": 2}, {"type": "text", "text": "RQ2 $:$ If yes, which visual information can be decoded? ", "page_idx": 2}, {"type": "text", "text": "RQ3 : To what extent can we reconstruct video from EEG signals? ", "page_idx": 2}, {"type": "text", "text": "Hence, we carefully selected video clips suitable for studying dynamic vision and annotated their meta information. ", "page_idx": 2}, {"type": "text", "text": "3.1 Participants ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Twenty healthy students from Shanghai Jiao Tong University (SJTU) participated (mean age: 22.07, STD: 2.23; 10 females, 10 males), all having normal or corrected-to-normal vision. All subjects were informed of the experimental process and signed informed consent forms before the experiment, then received monetary reimbursement after finishing. This study was approved by the ethical committee of SJTU Institutional Review Board for Human Research Protections. ", "page_idx": 2}, {"type": "text", "text": "3.2 Visual Stimuli Selection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We elaborately selected 40 concepts of videos in our experiment to study. The number of concepts follows previous research on EEG-image pairs [51, 49]. It is worth noting that EEG-Things [13] dataset (with 1854 object concepts) employed a rapid serial visual presentation (RSVP) paradigm [56, 57], which cannot be used for EEG-video experiment as video is a continuous stimulation. We further cluster the concepts into 9 coarser classes, as is demonstrated in Figure 1(A): {land animal, water animal, plant, exercise, human, natural scene, food, musical instrument, transportation}. ", "page_idx": 2}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/602b9cb71ca9e5da5e44b73bad4f6c7a8a0d5d50f4a749344dcbd1f01d551ce9.jpg", "img_caption": ["Figure 1: The meta information of video clips of 40 concepts, and experiment protocol. (A) Visualizations of the meta information for all video clips of 40 concepts, we plot the average of each meta information for each concept. (B) The data collecting environment. (C) Demonstration of a whole data collecting session. A session will contain 7 video blocks to be watched, and there are rest phases of at least 30 seconds each between blocks. (D) Demonstration of a video block, there is a 3-second hint before 5 different video clips of the same concept. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "All video clips are collected from two online video websites, Bilibili1 and YouTube2. We selected 35 different two-second video clips for each concept, which are divided randomly into 7 groups, each group has 5 video clips. Afterwards, 40 groups of different concepts are arranged sequentially to form a single block, each block containing $40\\times5=200$ video clips. Consequently, there are 7 blocks in our experiment, where the order of 40 categories of videos within each block is random and different from each other to mitigate the temporal bias of EEG signals. ", "page_idx": 3}, {"type": "text", "text": "3.3 Experiment Protocol ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To ensure the quality of the acquired EEG data, the experiments were conducted in a controlled laboratory environment to minimize noise and other environmental disturbances. 62-channel EEG signals were collected by an active $\\mathrm{AgCl}$ electrode cap with an international 10-10 system. The EEG signals were acquired using the ESI NeuroScan System at a sampling rate of $1000\\:\\mathrm{Hz}$ . Besides EEG data, EOG and ECG signals were recorded simultaneously during the experiment. We also adopt a Tobii Pro Fusion eye tracker to collect eye movements at a sampling rate of $250\\mathrm{Hz}$ . ", "page_idx": 3}, {"type": "text", "text": "During the experiment, all subjects were instructed to watch a series of color video clips presented with the resolution of $1980\\times1080$ (16:9) in full-screen mode on a 25-inch display. In each block, the 5 video clips with the same category were displayed continuously, and before playing these 5 same-class videos, there is a hint on the screen to inform the subjects what class they will see next, which will last for three seconds. Consequently, there are $40\\times(3+5\\times2)=520$ seconds, i.e., 8 min 40 s for each block. ", "page_idx": 3}, {"type": "text", "text": "There are 7 blocks of videos to watch in our experiment for a subject. After finishing watching a block (40 classes), the subject was required to rest for at least 30 seconds to mitigate fatigue and assesses his/her own attention level (ranging from 1 to 5. 1 means sleepy and 5 means very concentrating) ", "page_idx": 3}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/919323db01c2fbb028e2e65a97c5a9edccfc6952d2ed70c3063034b1ccd7bbee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Statistics of each meta information: (A) the fraction of human appearance. (B) the fraction of face appearance (only count the videos with humans). (C) the distribution of different object numbers. (D) the distribution of different object colors. (E) the histogram of OFS. ", "page_idx": 4}, {"type": "text", "text": "before starting the next block. As a result, the average attention level across all the subjects and blocks is $4.01\\pm0.83$ , ensuring high-quality EEG signals with acceptable concentrations. ", "page_idx": 4}, {"type": "text", "text": "3.4 Meta Information of Video Clips ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Besides the 40 concepts and 9 coarser classes, we also labeled some other meta information for each video clip. ", "page_idx": 4}, {"type": "text", "text": "Color: The main color of the main object. There are 7 color categories: {Red, Yellow, Blue, Green, White, Grey, Colorful}. Colorful indicates the color is too complex for identifying an accurate color from a single video clip. ", "page_idx": 4}, {"type": "text", "text": "Optical Flow Score: The optical flow score (OFS) of each 24 FPS two-second video clip obtained by averaging the length of the optical flow vectors, ranging from 0.008 (almost static) to 6.252 (rapidly changing). Further, based on the OFS, we divide all the video clips into 2 categories: $\\{F a s t,S l o w\\}$ . We choose the median OFS of 1.799 as the threshold to make sure the label is balanced. ", "page_idx": 4}, {"type": "text", "text": "Object Number: The number of the main objects. There are 3 categories: {One, Two, Many}. Many indicates the number of the main objects is equal to or more than three. ", "page_idx": 4}, {"type": "text", "text": "Human: If there are any humans appearing in the video, the label is $^{\\,l}$ , otherwise is $\\boldsymbol{O}$ . ", "page_idx": 4}, {"type": "text", "text": "Human Face: If there are any human faces appearing in the video, the label is $^{\\,I}$ , otherwise is $\\boldsymbol{O}$ . We depict the average meta information of each concept in Figure 1(A): ", "page_idx": 4}, {"type": "text", "text": "3.5 EEG Visual Perception Classification Benchmark ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To better understand the visual information in EEG signals, some low-level and high-level visual perception tasks are investigated in our dataset, presenting an EEG Viusal Perception (EEG-VP) benchmark. There are 7 EEG classification tasks based on the video label and meta information detailed in Section 3.4, whose statistics are presented in Figure 2: ", "page_idx": 4}, {"type": "text", "text": "\u2022 The 40-class classification of the fine-grained concept of the video clip.   \n\u2022 The 9-class classification of the course concept of the video clip.   \n\u2022 The 6-class classification of which color of the main object in the video clip. The data while watching Colorful videos are discarded in this task.   \n\u2022 The binary classification of whether the video is fast or slow based on the OFS.   \n\u2022 The 3-class classification of the number of the main objects in the video clip.   \n\u2022 The binary classification of whether human appears in the video or not.   \n\u2022 The binary classification of whether any human face appears in the video or not. The data while watching videos without human appearance are discarded in this task. ", "page_idx": 4}, {"type": "text", "text": "3.6 Video Reconstruction Benchmark ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This task is to reconstruct two-second video clips from the corresponding EEG segments. Following the previous video reconstruction from fMRI studies [31, 32, 52\u201354], we utilize the same metrics to ", "page_idx": 4}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/54bf9041ed4d3cf3c4ca1918cbf6924a48edb4b4cfc5076b9e8538822c4e923c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: (A-B) GLMNet Encoders. (C-E) Overview of our EEG2Video framework. (A) The visual cortex, basically in the occipital lobe. (B) GLMNet architecture which combines the global and local embedding. (C)The framework of EEG2Video which predicts the latent variabels $\\hat{z}_{0}$ and semantic guidance $\\hat{e}_{t}$ with Seq2Seq model and a predictor, respectively. A video diffusion model is then be adopted for generating videos using $\\hat{z}_{0}$ and $\\hat{e}_{t}$ . (D) Dynamic-aware noise-adding process based on the decoded dynamic information $\\beta$ . (E) Using large amounts of video-text pairs to fine-tune the inflated diffusion UNet for video generation. The texts are obtained by BLIP. ", "page_idx": 5}, {"type": "text", "text": "evaluate the quality of generated videos, roughly classified as frame-based metrics and video-based metrics. Definition of these metrics can be found in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 EEG Encoder: Mixture of Global and Local Features. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "EEG classifiers take into an EEG segment $\\boldsymbol{x}\\in\\mathcal{X}\\subset\\mathbb{R}^{C\\times T}$ , where $C$ denotes EEG channels and $T$ denotes time samples, and decode the target information $y\\in\\mathcal{V}$ , which can be defined as $f:\\mathcal X\\to\\mathcal Y$ . ", "page_idx": 5}, {"type": "text", "text": "Previous studies on EEG classification always treat all channels equally [49, 58, 59]: $f(x)\\;=$ $\\mathcal{E}_{g l o b a l}(x)$ , where $\\mathcal{E}_{g l o b a l}$ is a global encoder for decoding EEG. However, research in neuroscience indicates that human visual cortex are basically in the occipital lobe, as shown in Figure 3(A), treating all channels equally is unable to highlight the visual cortex features. To this end, we propose a simple yet effective network to combine the global features decoded from whole channels and local features decoded from the visual associated channels called Global Local Mixture Network (GLMNet). Depicted in Figure 3(B), GLMNet utilizes a local encoder $\\mathcal{E}_{l o c a l}$ to extract the vision-associated features, which can be denoted as $f(x)=e m b(\\mathsf{C o n c a t}(\\mathcal{E}_{g l o b a l}(x),\\mathcal{E}_{l o c a l}(x)))$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 EEG2Video: High Temporal Resolution Brain Decoding Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Compared to the fMRI-video reconstruction framework [31, 32] which are constrained by the limited temporal resolution that results in decoding two-second dynamic videos from one fMRI data frame, EEG signals have higher temporal resolution for better capturing dynamic visual perception to facilitate video reconstruction. As demonstrated in Figure 3(C), we introduce EEG2Video, a novel framework which utilize a Seq2Seq model to densely reconstruct low-level visual perception from continuous EEG embeddings extracted by an overlapping sliding window, and decode the semantic and dynamic information with two other modules for guiding an inflated diffusion model to recreate videos. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "EEG Embeddings Extraction For a two-second EEG segment $\\boldsymbol{x}\\in\\mathbb{R}^{C\\times T}$ , we apply an overlapping   \nselimdbinegd d ngds ne netxs $\\{x_{1},x_{2},\\ldots,x_{t}\\}$ ,e  wshheorret $t$ iss etghem teonttasl  nwuitmhb aern.  EEEEGG $e_{e e g}=\\{e_{e}^{1},e_{e}^{2},\\ldots,e_{e}^{t}\\}$ $\\mathcal{E}$ $\\mathcal{E}(x_{i})=e_{e}^{i}$ $i=\\{1,2,\\dots,t\\}$ ", "page_idx": 6}, {"type": "text", "text": "Seq2Seq Model In contrast to video reconstruction from fMRI (generating several frames from a single data frame), it is essential to align the high temporal resolution brain signals with videos in video reconstruction from EEG (generating several frames from several EEG segments) for capturing rapid changes. The Seq2Seq models are naturally introduced for extracting the continuous visual information from high temporal resolution brain signals. We employ the Transformer architecture as the Seq2Seq model in our framework, which can be formulated as a stack of several blocks, each block containing a multi-head attention (MHA) layer and a feed-forward network (FFN) layer[60]. Denoting the input of the $i$ -th Transformer block as $\\left\\vert x_{\\mathrm{in}}^{i}\\right\\vert$ , the calculation of the output $x_{\\mathrm{out}}^{i}$ is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{\\mathrm{mid}}^{i}=f_{\\mathrm{MHA}}(\\mathrm{LA}(x_{\\mathrm{in}}^{i}))+x_{\\mathrm{in}}^{i},\\enspace x_{\\mathrm{out}}^{i}=f_{\\mathrm{FFN}}(\\mathrm{LA}(x_{\\mathrm{mid}}^{i}))+x_{\\mathrm{mid}}^{i},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $f_{\\mathrm{MHA}}$ is the MHA layer, $f_{\\mathrm{FFN}}$ is the FFN layer, and LA is layer normalization. In our framework, the input of Transformer is the addition of EEG embeddings and position embeddings (PE): $x_{\\mathrm{in}}^{0}=e_{e e g}+P E$ , and the output is the latent variables $\\hat{z}_{0}$ of the corresponding video frames. As depicted in Figure 3(E), the Ground Truth (GT) video frames are fed into the frozen VAE encoder to obatin the GT latent variables $z_{0}$ . We apply mean squared error (MSE) loss $M S E(\\hat{z}_{0},z_{0})$ for training the Seq2Seq model that densely predicts the continuous visual information of frames. ", "page_idx": 6}, {"type": "text", "text": "Semantic Predictor In order to utilize the pre-trained diffusion models for generating high-quality videos, we first generate the corresponding text description of each video by feeding the medium frame to a caption model called BLIP [55], then align the EEG signals with the text embeddings $e_{t}\\in\\mathbb{R}^{77\\times768}$ , which are acquired by the frozen CLIP text encoder as shown in Figure 3(E). An MLP is adopted as the semantic predictor to project EEG data into the same dimension to obtain $\\hat{e}_{t}$ . Finally, the MSE loss $M S E(\\hat{e}_{t},e_{t})$ is employed for aligning EEG and text embeddings. ", "page_idx": 6}, {"type": "text", "text": "Dynamic-Aware Noise-Adding Process The frames in a video with high OFS are more diverse from each other than those in videos with low OFS. Based on the dynamic decoding results, we can roughly classify whether the video is high dynamic or not from EEG signals. Hence, we introduce the static noise $\\epsilon_{s}$ and diverse noise $\\epsilon_{d}$ into the diffusion process and balance them by the decoded dynamic information $\\hat{e}_{d}$ . The diverse noise $\\epsilon_{d}=\\{\\epsilon_{d}^{1},\\epsilon_{d}^{2},\\dot{\\cdot}\\cdot\\cdot,\\epsilon_{d}^{n}\\}$ consists of $n$ different noises, each $\\epsilon_{d}^{i}\\sim\\mathcal{N}(0,1)$ . The static noise $\\epsilon_{s}$ has the same noise $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ replicated $n$ times. The ratio of the static noise is smaller when the $\\hat{e}_{d}$ indicates the video is more dynamic. The diffusion process can be formulated as follows to acquire the noise $z\\tau$ at time steps $\\tau$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{\\mathcal{T}}=\\sqrt{\\overline{{\\alpha}}_{\\mathcal{T}}}\\times\\hat{z}_{0}+\\sqrt{1-\\overline{{\\alpha}}_{\\mathcal{T}}}\\times(\\sqrt{\\beta}\\times\\epsilon_{s}+\\sqrt{1-\\beta}\\times\\epsilon_{d}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\overline{{\\alpha}}\\tau$ is the coefficient for adding noise directly to the noise at time steps $\\tau,\\beta$ is depended by the decoded dynamic information. Here, We set $\\beta=0.2$ when the video is high dynamic and $\\beta=0.3$ when the video is low dynamic. ", "page_idx": 6}, {"type": "text", "text": "Video Diffusion Model For reconstructing vivid videos from EEG signals, we utilize the TuneA-Video technique which fine-tunes an inflated text-to-image diffusion model [33]. The network inflation trick is adding a sparse temporal attention layer in the image generation model to ensure the consistency between frames, in which each frame is calculated with the the first frame and the frame before it. Using the same notations in [33], the attention is formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nQ=W^{Q}\\cdot z_{v_{i}},\\,\\,\\,K=W^{K}\\cdot[z_{v_{i-1}},z_{v_{1}}],\\,\\,\\,V=W^{V}\\cdot[z_{v_{i-1}},z_{v_{1}}],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $[\\cdot]$ denotes concatenation operation, and $z_{v_{i}}$ is the $i$ -th frame. To fine-tune the video generation model in our framework, the corresponding text description of each video by feeding the medium frame to a caption model called BLIP [55]. Afterwards, all video-text pairs in the training set are used for fine-tuning the Stable Diffusion Model V1-4 [45]. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment on SEED-DV Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the performance of our methods and other baselines on the proposed two benchmarks: the EEG-VP benchmark and the video reconstruction benchmark. The details of data pre-processing, model implementation and training can be found in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "5.1 EEG-VP Benchmark ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1.1 Quantitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We present the overall accuracy of different EEG classifiers in Table 1. Besides raw EEG Signals, we also run experiments on the PSD features and the DE features [61] of 5 frequency bands. ", "page_idx": 7}, {"type": "text", "text": "From the result, we can see that (1) our GLMNet outperformed the baselines consistently across all the classification tasks, which indicates the importance of extracting vision-associated features for visual perception tasks. (2) Different types of EEG features yield similar results. (3) Different tasks have different difficulties. Some meta information are distinguishable via EEG signals, e.g., color, dynamics, while the statistical significance indicates that the number of main object and whether human/face appear are difficult or even impossible to be classified. The difference may be attributed to different processing mechanism by our brains which can inspire research in neuroscience. ", "page_idx": 7}, {"type": "text", "text": "To conclude, we can now answer RQ1 and RQ2: Yes, some of visual information can be decoded from EEG signals, including category, color, dynamics. However, the overall visual perception benchmark is rather challenging and calls for more advanced algorithms. Refer to Appendix C for the confusion matrix and analysis across subjects. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Average classification accuracy $(\\%)$ and std across all subjects with different EEG classifiers on different tasks. Chance level is the percentage of the largest class. The star symbol $(^{*})$ represents the result is above chance level with statistical significance (two-sample t-test: $p<0.05)$ ). ", "page_idx": 7}, {"type": "table", "img_path": "RfsfRn9OFd/tmp/e08d86cb1e1b36c1f9a1db83074198c922b362459b6608bee0fcbe5099cded63.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1.2 Analysis of Brain Areas ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To find electrodes or brain areas most associated with dynamic visual perception, we conduct a one-channel classification task to test the classification quality of each electrode. Due to the reason that only one channel is used, we simplify the task to binary classification: Human/Animal and Fast/Slow tasks, which is related to object recognition and dynamic perception, respectively. It can be observed from Figure 4(A) that the electrodes in the occipital area have higher accuracy on Human/Animal tasks, demonstrating the object recognition are related to the occipital area where the visual cortex is located, presenting a similar result as previous works [64, 19, 30]. However, not all dynamic visual perception are in the occipital region. Figure 4(B) reveals that the brain area associated to movements are around the temporal region where the sensory and motor cortex lies, consistent with the previous neuroscience study [65]. ", "page_idx": 7}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/1e47e95713e001a1074eeb3e6ca3eaa109d72ad1ffd04cd243178a7f619c9829.jpg", "img_caption": ["Figure 4: Spatial Analysis. (A-B). Topographies of each electrode\u2019s accuracy for Human/Animal and Fast/Slow tasks. (C). Ablate electrodes of different brain regions. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To verify the findings, we conduct the ablation study by removing electrodes from different brain regions and show the $40\\mathrm{-c}$ top-1 accuracy in Figure 4(C). Removing occipital region significantly damages the performance $p<0.01)$ ). The performance also declines without the temporal region. ", "page_idx": 8}, {"type": "text", "text": "5.2 Video Reconstruction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.2.1 Quantitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate our framework on the SEED-DV dataset and three subsets which contains less categories to answer Q3. The quantitative results under four cases are reported in Table 2. With the number of classes increases, the reconstruction performances decrease. Our framework achieves $34.0\\%$ of 40-way semantic-level accuracy when dealing with the subset contains 10 classes and $15.9\\%$ when facing the whole 40 classes. Meanwhile, the structural similarity index measure (SSIM) [66], which reflects the pixel-level similarity between reconstructed samples and ground truth samples, is up to 0.300 when facing the smallest subset, and drops to 0.256 when facing whole dataset. While the scores cannot be directly compared, our reconstructed videos have very similar and even higher SSIM to that reconstructed by fMRI reported in [32], intuitively demonstrating the capability of EEG signals to reconstruct dynamic visual perceptions. And it is also worth noting that with this more sophisticated model, the generated videos have much higher 40-way classification accuracy than that reported in EEG-VP benchmark, highlighting the potential to advance this research direction. ", "page_idx": 8}, {"type": "text", "text": "Based on the findings from the results of the EEG-VP benchmark (the statistic significance proves that Category, Color, and Dynamics can be decoded from EEG signals), we design the DANA module for injecting the Fast/Slow into diffusion process, the semantic predictor to inject Category information, and the general Seq2Seq for decoding low-level visual information like Color. We further conduct the ablation study by removing the Seq2Seq module and the DANA process respectively, and we can see huge performance drop without either module. This indicates that capturing the dynamics of both EEG and video is crucial for successful video reconstruction. ", "page_idx": 8}, {"type": "table", "img_path": "RfsfRn9OFd/tmp/51b4cd9311f7b32c7d7952b27e7b03642c741fe4e0aea5715df85078276fc5d8.jpg", "table_caption": ["Table 2: Quantitative results of each methods on different size of subsets. Standard deviation is calculated across random seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2.2 Reconstructed Examples ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present some visual examples in Figure 5. It can be observed that various videos are reconstructed, and as long as the semantic and low-level visual information is correctly predicted by the model, the downstream diffusion process can generate vivid and high-quality videos. More examples including failure cases with full 6 frames can be found in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/bd25282ec23ab2273e8626c2901b835defff3dbf8bbf9737d90fd898574dd0b8.jpg", "img_caption": ["Figure 5: Reconstruction Presentations. Various video clips with low dynamics (e.g., Mountain, Beach, Face) and high dynamics (e.g., Skiing, Fireworks, Dancing) across animals, scenes, persons, and activities can be correctly recovered. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we developed the large dataset, SEED-DV, to reconstruct videos from EEG signals, upon which we built the EEG Visual Perception Classification benchmark and the Video Reconstruction benchmark to support evaluating the advances of EEG-based video reconstruction. Moreover, we proposed a novel baseline EEG2Video for video reconstruction from EEG signals that can align visual dynamics with EEG based on the Seq2Seq architecture, and we presented vivid generated examples by training our framework on SEED-DV. ", "page_idx": 9}, {"type": "text", "text": "As the first attempt, we open a new possibility for BCI researchers to decode dynamic visual perception from EEG signals. Although the overall performance on SEED-DV are still in a preliminary stage, we hold the strong belief that the game-changing results for the BCI area can soon be discovered. ", "page_idx": 9}, {"type": "text", "text": "7 Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Reconstructing dynamic visual perception from brain activities helps to advance the understanding of our visual system in brains. EEG is a physiological signal widely used in clinical practice and brain-computer interfaces. Compared to non-portable and expensive neuroimaging techniques like fMRI and MEG, our work provides a convenient and cheap solution for decoding visual information from brain activities. This technique can be used for visualize our mind, offering a novel approach for listening the inner world of people patients with mental illnesses like autistic and depression. ", "page_idx": 9}, {"type": "text", "text": "However, every coin has two sides. Personal privacy may leak through our brain activities and be abused by malicious attackers for reading one\u2019s mind from EEG signals without acknowledgment. More strict regulations are supposed to be made for protecting the privacy of people\u2019s biological data by government and medical institutions. ", "page_idx": 9}, {"type": "text", "text": "8 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our SEED-DV dataset currently records the EEG signals of each subject with one session, leading to the requirement for collecting more EEG-Video pairs with more sessions, which is significant for studying the stable neural patterns over time. Our framework is evaluated under the subject-dependent settings, the cross-subject ability remains unexplored due to individual variations. Future work can be focused on the transferability and stability of the video reconstruction framework by exploiting generalizable EEG encoders and Seq2Seq model. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by grants from National Natural Science Foundation of China (Grant No. 62376158), STI 2030-Major Projects+2022ZD0208500, Shanghai Municipal Science and Technology Major Project (Grant No. 2021SHZD ZX), Shanghai Pujiang Program (Grant No. 22PJ1408600), Medical-Engineering Interdisciplinary Research Foundation of Shanghai Jiao Tong University \u201cJiao Tong Star\u201d Program (YG2023ZD25, YG2024ZD25 and YG2024QNA03), Shanghai Pilot Program for Basic Research - Shanghai Jiao Tong University (No. 21TQ1400203) and GuangCi Professorship Program of RuiJin Hospital Shanghai Jiao Tong University School of Medicine. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] W. R. Hendee and P. N. Wells, The perception of visual information. Springer Science & Business Media, 1997.   \n[2] K. Grill-Spector and R. Malach, \u201cThe human visual cortex,\u201d Annu. Rev. Neurosci., vol. 27, pp. 649\u2013677, 2004.   \n[3] F. Tong, \u201cPrimary visual cortex and visual awareness,\u201d Nature Reviews Neuroscience, vol. 4, no. 3, pp. 219\u2013229, 2003.   \n[4] D. H. Hubel and T. N. Wiesel, \u201cReceptive fields of single neurones in the cat\u2019s striate cortex,\u201d The Journal of Physiology, vol. 148, no. 3, p. 574, 1959.   \n[5] H. Jang and F. Tong, \u201cImproved modeling of human vision by incorporating robustness to blur in convolutional neural networks,\u201d Nature Communications, vol. 15, no. 1, p. 1989, 2024.   \n[6] S. Lees, N. Dayan, H. Cecotti, P. McCullagh, L. Maguire, F. Lotte, and D. Coyle, \u201cA review of rapid serial visual presentation-based brain\u2013computer interfaces,\u201d Journal of neural engineering, vol. 15, no. 2, p. 021001, 2018.   \n[7] S. A. Engel, D. E. Rumelhart, B. A. Wandell, A. T. Lee, G. H. Glover, E.-J. Chichilnisky, M. N. Shadlen et al., \u201cfmri of human visual cortex,\u201d Nature, vol. 369, no. 6481, pp. 525\u2013525, 1994.   \n[8] K. Han, H. Wen, J. Shi, K.-H. Lu, Y. Zhang, D. Fu, and Z. Liu, \u201cVariational autoencoder: An unsupervised model for encoding and decoding fmri activity in visual cortex,\u201d NeuroImage, vol. 198, pp. 125\u2013136, 2019.   \n[9] F. Moradi, L. Liu, K. Cheng, R. A. Waggoner, K. Tanaka, and A. A. Ioannides, \u201cConsistent and precise localization of brain activity in human primary visual cortex by meg and fmri,\u201d Neuroimage, vol. 18, no. 3, pp. 595\u2013609, 2003.   \n[10] P. Adjamian, A. Hadjipapas, G. R. Barnes, A. Hillebrand, and I. E. Holliday, \u201cInduced gamma activity in primary visual cortex is related to luminance and not color contrast: An meg study,\u201d Journal of Vision, vol. 8, no. 7, pp. 4\u20134, 2008.   \n[11] P. Ramkumar, B. C. Hansen, S. Pannasch, and L. C. Loschky, \u201cVisual information representation and rapid-scene categorization are simultaneous across cortex: An meg study,\u201d Neuroimage, vol. 134, pp. 295\u2013304, 2016.   \n[12] N. A. Busch, J. Dubois, and R. VanRullen, \u201cThe phase of ongoing eeg oscillations predicts visual perception,\u201d Journal of neuroscience, vol. 29, no. 24, pp. 7869\u20137876, 2009.   \n[13] A. T. Gifford, K. Dwivedi, G. Roig, and R. M. Cichy, \u201cA large and rich EEG dataset for modeling human visual object recognition,\u201d NeuroImage, vol. 264, p. 119754, 2022.   \n[14] D. Regan, \u201cSome characteristics of average steady-state and transient responses evoked by modulated light,\u201d Electroencephalography and clinical neurophysiology, vol. 20, no. 3, pp. 238\u2013248, 1966.   \n[15] N. K. N. Aznan, S. Bonner, J. Connolly, N. Al Moubayed, and T. Breckon, \u201cOn the classification of ssvep-based dry-eeg signals via convolutional neural networks,\u201d in 2018 IEEE international conference on systems, man, and cybernetics (SMC). IEEE, 2018, pp. 3726\u20133731.   \n[16] S. Bagchi and D. R. Bathula, \u201cEeg-convtransformer for single-trial eeg-based visual stimulus classification,\u201d Pattern Recognition, vol. 129, p. 108757, 2022.   \n[17] D. Veniero, J. Gross, S. Morand, F. Duecker, A. T. Sack, and G. Thut, \u201cTop-down control of visual cortex by the frontal eye fields through oscillatory realignment,\u201d Nature communications, vol. 12, no. 1, p. 1757, 2021.   \n[18] T. Grootswagers, I. Zhou, A. K. Robinson, M. N. Hebart, and T. A. Carlson, \u201cHuman EEG recordings for 1,854 concepts presented in rapid serial visual presentation streams,\u201d Scientific Data, vol. 9, no. 1, p. 3, 2022.   \n[19] Y. Song, B. Liu, X. Li, N. Shi, Y. Wang, and X. Gao, \u201cDecoding natural images from EEG for object recognition,\u201d in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=dhLIno8FmH   \n[20] Y. Takagi and S. Nishimoto, \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 14 453\u201314 463.   \n[21] Z. Chen, J. Qing, T. Xiang, W. L. Yue, and J. H. Zhou, \u201cSeeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 22 710\u201322 720.   \n[22] J. Sun, M. Li, Z. Chen, Y. Zhang, S. Wang, and M.-F. Moens, \u201cContrast, attend and diffuse to decode highresolution images from brain activities,\u201d Advances in Neural Information Processing Systems (NeurIPS) (NeurIPS), vol. 36, 2024.   \n[23] B. Zeng, S. Li, X. Liu, S. Gao, X. Jiang, X. Tang, Y. Hu, J. Liu, and B. Zhang, \u201cControllable mind visual diffusion model,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 7, 2024, pp. 6935\u20136943.   \n[24] P. Scotti, A. Banerjee, J. Goode, S. Shabalin, A. Nguyen, A. Dempster, N. Verlinde, E. Yundler, D. Weisberg, K. Norman et al., \u201cReconstructing the mind\u2019s eye: fmri-to-image with contrastive learning and diffusion priors,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 36, 2024.   \n[25] W. Xia, R. de Charette, C. Oztireli, and J.-H. Xue, \u201cDream: Visual decoding from reversing human visual system,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024, pp. 8226\u20138235.   \n[26] F. Ozcelik and R. VanRullen, \u201cNatural scene reconstruction from fmri signals using generative latent diffusion,\u201d Scientific Reports, vol. 13, no. 1, p. 15666, 2023.   \n[27] P. S. Scotti, M. Tripathy, C. K. T. Villanueva, R. Kneeland, T. Chen, A. Narang, C. Santhirasegaran, J. Xu, T. Naselaris, K. A. Norman et al., \u201cMindeye2: Shared-subject models enable fmri-to-image with 1 hour of data,\u201d arXiv preprint arXiv:2403.11207, 2024.   \n[28] Y. Benchetrit, H. Banville, and J.-R. King, \u201cBrain decoding: toward real-time reconstruction of visual perception,\u201d in The Twelfth International Conference on Learning Representations (ICLR), 2024. [Online]. Available: https://openreview.net/forum?id=3y1K6buO8c   \n[29] Y. Bai, X. Wang, Y.-p. Cao, Y. Ge, C. Yuan, and Y. Shan, \u201cDreamdiffusion: Generating high-quality images from brain EEG signals,\u201d arXiv preprint arXiv:2306.16934, 2023.   \n[30] Y.-T. Lan, K. Ren, Y. Wang, W.-L. Zheng, D. Li, B.-L. Lu, and L. Qiu, \u201cSeeing through the brain: image reconstruction of visual perception from human brain signals,\u201d arXiv preprint arXiv:2308.02510, 2023.   \n[31] Z. Chen, J. Qing, and J. H. Zhou, \u201cCinematic mindscapes: High-quality video reconstruction from brain activity,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 36, 2024.   \n[32] J. Sun, M. Li, Z. Chen, and M.-F. Moens, \u201cNeurocine: Decoding vivid video sequences from human brain activties,\u201d arXiv preprint arXiv:2402.01590, 2024.   \n[33] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou, \u201cTune-a-video: One-shot tuning of image diffusion models for text-to-video generation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 7623\u20137633.   \n[34] Y. Miyawaki, H. Uchida, O. Yamashita, M.-a. Sato, Y. Morito, H. C. Tanabe, N. Sadato, and Y. Kamitani, \u201cVisual image reconstruction from human brain activity using a combination of multiscale local image decoders,\u201d Neuron, vol. 60, no. 5, pp. 915\u2013929, 2008.   \n[35] T. Naselaris, R. J. Prenger, K. N. Kay, M. Oliver, and J. L. Gallant, \u201cBayesian reconstruction of natural images from human brain activity,\u201d Neuron, vol. 63, no. 6, pp. 902\u2013915, 2009.   \n[36] G. J. Brouwer and D. J. Heeger, \u201cDecoding and reconstructing color from responses in human visual cortex,\u201d Journal of Neuroscience, vol. 29, no. 44, pp. 13 992\u201314 003, 2009.   \n[37] S. Schoenmakers, M. Barth, T. Heskes, and M. Van Gerven, \u201cLinear reconstruction of perceived images from human brain activity,\u201d NeuroImage, vol. 83, pp. 951\u2013961, 2013.   \n[38] R. Beliy, G. Gaziv, A. Hoogi, F. Strappini, T. Golan, and M. Irani, \u201cFrom voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019.   \n[39] G. Gaziv, R. Beliy, N. Granot, A. Hoogi, F. Strappini, T. Golan, and M. Irani, \u201cSelf-supervised natural image reconstruction and large-scale semantic classification from brain activity,\u201d NeuroImage, vol. 254, p. 119121, 2022.   \n[40] S. Lin, T. Sprague, and A. K. Singh, \u201cMind reader: Reconstructing complex images from brain activities,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 29 624\u201329 636, 2022.   \n[41] F. Ozcelik, B. Choksi, M. Mozafari, L. Reddy, and R. VanRullen, \u201cReconstruction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans,\u201d in 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 2022, pp. 1\u20138.   \n[42] L. Meng and C. Yang, \u201cSemantics-guided hierarchical feature encoding generative adversarial network for visual image reconstruction from brain activity,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 32, pp. 1267\u20131283, 2024.   \n[43] S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, and M. Shah, \u201cGenerative adversarial networks conditioned by brain signals,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017, pp. 3410\u20133418.   \n[44] I. Kavasidis, S. Palazzo, C. Spampinato, D. Giordano, and M. Shah, \u201cBrain2image: Converting brain signals into images,\u201d in Proceedings of the 25th ACM International Conference on Multimedia, 2017, pp. 1809\u20131817.   \n[45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 10 684\u201310 695.   \n[46] G. Kim, T. Kwon, and J. C. Ye, \u201cDiffusionclip: Text-guided diffusion models for robust image manipulation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 2426\u20132435.   \n[47] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 33, pp. 6840\u20136851, 2020.   \n[48] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML). PMLR, 2021, pp. 8748\u20138763.   \n[49] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, and M. Shah, \u201cDeep learning human mind for automated visual classification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6809\u20136817.   \n[50] R. Li, J. S. Johansen, H. Ahmed, T. V. Ilyevsky, R. B. Wilbur, H. M. Bharadwaj, and J. M. Siskind, \u201cThe perils and pitfalls of block design for EEG classification experiments,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 1, pp. 316\u2013333, 2020.   \n[51] H. Ahmed, R. B. Wilbur, H. M. Bharadwaj, and J. M. Siskind, \u201cObject classification from randomized EEG trials,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 3845\u20133854.   \n[52] H. Wen, J. Shi, Y. Zhang, K.-H. Lu, J. Cao, and Z. Liu, \u201cNeural encoding and decoding with deep learning for dynamic natural vision,\u201d Cerebral Cortex, vol. 28, no. 12, pp. 4136\u20134160, 2018.   \n[53] G. Kupershmidt, R. Beliy, G. Gaziv, and M. Irani, \u201cA penny for your (visual) thoughts: Self-supervised reconstruction of natural movies from brain activity,\u201d arXiv preprint arXiv:2206.03544, 2022.   \n[54] C. Wang, H. Yan, W. Huang, J. Li, Y. Wang, Y.-S. Fan, W. Sheng, T. Liu, R. Li, and H. Chen, \u201cReconstructing rapid natural vision with fmri-conditional video generative adversarial network,\u201d Cerebral Cortex, vol. 32, no. 20, pp. 4502\u20134511, 2022.   \n[55] J. Li, D. Li, C. Xiong, and S. Hoi, \u201cBlip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation,\u201d in International Conference on Machine Learning (ICML). PMLR, 2022, pp. 12 888\u201312 900.   \n[56] H. Intraub, \u201cRapid conceptual identification of sequentially presented pictures.\u201d Journal of Experimental Psychology: Human Perception and Performance, vol. 7, no. 3, p. 604, 1981.   \n[57] K. N. Kay, T. Naselaris, R. J. Prenger, and J. L. Gallant, \u201cIdentifying natural images from human brain activity,\u201d Nature, vol. 452, no. 7185, pp. 352\u2013355, 2008.   \n[58] V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung, and B. J. Lance, \u201cEEGnet: a compact convolutional neural network for EEG-based brain\u2013computer interfaces,\u201d Journal of neural engineering, vol. 15, no. 5, p. 056013, 2018.   \n[59] Y. Song, Q. Zheng, B. Liu, and X. Gao, \u201cEEG conformer: Convolutional transformer for EEG decoding and visualization,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 31, pp. 710\u2013719, 2022.   \n[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[61] A. Hyv\u00e4rinen, \u201cNew approximations of differential entropy for independent component analysis and projection pursuit,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 10, 1997.   \n[62] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter, K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball, \u201cDeep learning with convolutional neural networks for EEG decoding and visualization,\u201d Human brain mapping, vol. 38, no. 11, pp. 5391\u20135420, 2017.   \n[63] C. Cortes and V. Vapnik, \u201cSupport-vector networks,\u201d Machine learning, vol. 20, pp. 273\u2013297, 1995.   \n[64] P. Bao, L. She, M. McGill, and D. Y. Tsao, \u201cA map of object space in primate inferotemporal cortex,\u201d Nature, vol. 583, no. 7814, pp. 103\u2013108, 2020.   \n[65] J. H. Maunsell and D. C. Van Essen, \u201cFunctional properties of neurons in middle temporal visual area of the macaque monkey. ii. binocular interactions and sensitivity to binocular disparity,\u201d Journal of neurophysiology, vol. 49, no. 5, pp. 1148\u20131167, 1983.   \n[66] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \u201cImage quality assessment: from error visibility to structural similarity,\u201d IEEE transactions on image processing, vol. 13, no. 4, pp. 600\u2013612, 2004.   \n[67] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248\u2013255.   \n[68] Z. Tong, Y. Song, J. Wang, and L. Wang, \u201cVideomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training,\u201d Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 10 078\u201310 093, 2022.   \n[69] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., \u201cThe kinetics human action video dataset,\u201d arXiv preprint arXiv:1705.06950, 2017.   \n[70] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit models,\u201d arXiv preprint arXiv:2010.02502, 2020.   \n[71] E. Orchard-Mills, D. Alais, and E. Van der Burg, \u201cCross-modal associations between vision, touch, and audition influence visual search through top-down attention, not bottom-up capture,\u201d Attention, Perception, & Psychophysics, vol. 75, pp. 1892\u20131905, 2013.   \n[72] J. Gu, B. Liu, X. Li, P. Wang, and B. Wang, \u201cCross-modal representations in early visual and auditory cortices revealed by multi-voxel pattern analysis,\u201d Brain Imaging and Behavior, vol. 14, no. 5, pp. 1908\u2013 1920, 2020.   \n[73] V. Marian, S. Hayakawa, and S. R. Schroeder, \u201cCross-modal interaction between auditory and visual input impacts memory retrieval,\u201d Frontiers in Neuroscience, vol. 15, p. 661477, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Evaluation Metrics for Video Reconstruction Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we detail the metrics we use for the video reconstruction benchmark. The metrics to evaluate the quality of generated videos can be roughly classified as frame-based metrics and video-based metrics. ", "page_idx": 14}, {"type": "text", "text": "Frame-based Metrics Two levels of metrics are considered to judge the quality of generated frames: the pixel-level and the semantics-level metrics. For the pixel level, we calculate the average structural similarity index measure (SSIM) [66] of each frame between the ground-truth video and the reconstructed video. For the semantic level, a CLIP-based classifier [48] trained on ImageNet [67] is adopted to compute the $N_{\\;}$ -way top- $\\cal{K}$ accuracy of predicted frames. If the ground-truth class is within the top- $\\cal{K}$ probability of the predicted frames classification results from $N$ arbitrary classes (including ground-truth class), the semantic-level reconstruction is regarded successful. ", "page_idx": 14}, {"type": "text", "text": "Video-based Metric As the ImageNet classifier is unable to well understand videos, a VideoMAEbased [68] video classifier trained on Kinetics-400 dataset [69], which can understand 400 dynamic concepts (e.g., changes, human motions), is applied to compute the video semantic-level accuracy. ", "page_idx": 14}, {"type": "text", "text": "B Experiment Setup and Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 EEG Signals Preprocessing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The raw data was recorded with the 62-channel EEG cap with a sample rate of $1000\\,\\mathrm{{Hz}}$ and stored in the continuous EEG data flie format (.cnt), a single flie for the experiment of each subject. We applied the $0.1{-}100\\ \\mathrm{Hz}$ band-pass fliter to fliter out the DC interference and very high-frequency interference and down-sampled the EEG data to ${200}\\mathrm{Hz}$ to accelerate computations. A one-second sliding window with $500\\,\\mathrm{ms}$ overlapping is used for EEG segmentation and frequency feature extraction. Specifically, power spectral density (PSD) and differential entropy (DE) [61] of five frequency bands $\\langle\\delta;1{-}4\\,\\mathrm{Hz}$ , \u03b8: $4{-}8\\ \\mathrm{Hz}$ , $\\alpha$ : ${8-14\\:\\mathrm{Hz}}$ , $\\beta$ : $14{-}31\\,\\mathrm{Hz}$ , and $\\gamma$ : $31{-}99\\,\\mathrm{Hz}$ ) are extracted. ", "page_idx": 14}, {"type": "text", "text": "B.2 Classification Experiment Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We perform a 7-fold cross-validation and report the average accuracy. Specifically, we select each single video block as the testing set one by one, the block before testing set as the validation set, and the remaining 5 blocks compose the training set. Several EEG classifiers, including ShallowNet, DeepNet [62], EEGNet [58], Conformer [59], and TSConv [19], are adopted as baselines for processing raw EEG. For frequency features, we compare the performance of support vector machine (SVM) [63] and multilayer perceptron (MLP). In our experiment, Our GLMNet uses a concise global encoder, which has the same architecture of ShallowNet on raw signals, and MLP on frequency featrues. All models are implemented with PyTorch and evaluated on an Nvidia A100 GPU. Adam optimizer is used with the learning rate $\\eta=0.001$ . Batch size is set to 256 for all methods, and the number of training epochs is 100. ", "page_idx": 14}, {"type": "text", "text": "B.3 Video Reconstruction Experiment Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We reconstruct a two-second video clip from the corresponding two-second EEG segment. For efficient training and testing, we down-sampled the 24 FPS 1080p original videos to a small video of resolution of $512\\times288$ (16:9) with 3 FPS, resulting in 6 frames for each video. A $500\\,\\mathrm{ms}$ sliding window with $250\\;\\mathrm{ms}$ overlapping is used for EEG segmentation, forming a total of 7 segments in an EEG embeddings $e_{e e g}\\in\\mathbb{R}^{\\hat{7}\\hat{\\times}d}$ , where $d$ is the embedding dimension and we set $d=512$ . ", "page_idx": 14}, {"type": "text", "text": "We use the first 6 blocks from all the sessions as the training set and the last blocks as the testing set in our experiment. The Transformer used as the Seq2Seq model has 2 encoder layers and 4 decoder layers. Semantic predictor has 4 layers with ReLU activation. Dynamic predictor is GLMNet. An Adam optimizer with learning rate of 0.0005 and cosine scheduler was adopted for training the above models with 200 epochs. The inflated video generation model is fine-tuned using the same setting in [33] on the training set with learning rate of 0.00003 and cosine scheduler for 200 epochs, which takes about 5 hours. The inference is performed with 100 DDIM [70] steps. ", "page_idx": 14}, {"type": "text", "text": "Limited to the low signal-to-noise ratio (SNR) and spatial resolution, decoding videos from EEG is somehow difficult. Thus, besides applying the full dataset of 40 concepts, we select {Cat, Shark, ", "page_idx": 14}, {"type": "text", "text": "Flower, Dancing, Face, Buildings, Road, Pizza, Guitar, Airplane} to form a 10-class subset, and the first {1-20} categories and the first {1-30} categories to form other two subsets whose sizes are 20 and 30. ", "page_idx": 15}, {"type": "text", "text": "C More Results on EEG-VP benchmark ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 40-class Classification Task ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We plot the confusion matrices of GLMNet on the 40-class task. It can be seen that though the accuracy is not high, there is a faint diagonal lines in both confusion matrices. Moreover, a small square in the right bottom corner is being observed, of which categories are {Drum, Guitar, and Piano} $32\\mathrm{~-~}34\\$ class). This discovery may caused by the reason that the musical instruments stimulate the auditory cortex in our brains with these visual cues, a well-studied phenomenon named cross-modal perception[71\u201373], which then has similar reflections in EEG signals. ", "page_idx": 15}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/a0a6fbab1bd7e6ea168326ccd724b442df0967f3ecc6aa6dabdbf520eb3cb6b6.jpg", "img_caption": ["Figure 6: Confusion Matrices of GLMNet (A). The performance using DE features. (B). The performance using raw EEG signals. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Performance of Each Subject ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "EEG signal is notorious for its large variations among different subjects. In order to verify whether the dynamic visual perception decoding generalize across different people, we plot the results on the EEG-VP benchmark of all the subjects separately in Figure 7. Generally speaking, model trained on all the individual subject can achieve reasonable results across different tasks, demonstrating that the phenomenon of visual perception information being contained in the EEG is common across different people. Meanwhile, we can see the differences in visual specificity contained in the EEG signals among different subjects. For example, EEG from Subject 3 predicts well on the class of the objects, EEG from Subject 4 and 13 are more sensitive to color information, and EEG from Subject 9 is better at capturing motor information. ", "page_idx": 15}, {"type": "text", "text": "D More Reconstructed Samples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We display more reconstruction results for demonstrating the effectiveness of our EEG2Video framework. It can be observed that various videos across animals, plants, people, and activities. Please kindly refer to the supplementary files to find GIFs of these reconstructed videos. ", "page_idx": 15}, {"type": "text", "text": "Some failure samples are displayed in Figure 13. These failures are typically caused by the inability of the model to infer either the semantic information or the low-level visual information correctly, resulting the irrelevantly generated videos. However, we can still see from these failed examples that the model reconstructs some features of the real video, from shapes, movements, to the scene dynamics of the video. For example, the man practicing boxing is reconstructed as a panda practicing ", "page_idx": 15}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/4a650c55e837f396288fc7d4f02c91a884d75804e4ad04ced0302ea08499d4a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 7: Performance of each subject on different tasks on EEG-VP benchmark. (A). The performance using DE features. (B). The performance using raw EEG signals. ", "page_idx": 16}, {"type": "text", "text": "boxing, the ship sailing on the sea is reconstructed as a shark, and the fast-moving car is reconstructed as a fast-moving person. ", "page_idx": 16}, {"type": "text", "text": "E Algorithm of EEG2Video ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we write the algorithm of EEG2Video framework in Algorithm 1. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Training Stage of EEG2Video Framework ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: (1) training set $\\mathcal{D}_{t r a i n}=\\{x_{i},v_{i},d_{i}\\}$ , where $x_{i}$ is EEG segments, $v_{i}$ is video clips, $d_{i}$ is the fast/slow label (2) stable diffusion model $T2I$ , whose VAE encoder is $\\mathcal{E}_{v a e}$ (3) image caption model $B$   \nOutput: (1) video diffusion model $T2V$ , (2) Seq2Seq model Seq2Seq, (3) semantic predictor $\\mathcal{P}_{s}$ , (4) dynamic predictor $\\mathcal{P}_{d}$   \n1: Initialize text prompts of training dataset $T=\\{t_{i}\\}$   \n2: for each $(v_{i})\\overset{=}{\\in}\\mathcal{D}_{t r a i n}\\,\\epsilon$ do   \n3: $v_{i}=\\{f_{1},f_{2},\\ldots,f_{n}\\}$   \n4: $t_{i}\\gets B(f_{1})$   \n5: end for   \n6: Initialize latent vectors of all frames $L=\\{z_{i}\\}$   \n7: for each $(v_{i})\\in\\mathcal{D}_{t r a i n}$ do   \n8: $v_{i}=\\{f_{1},f_{2},\\ldots,f_{n}\\}$   \n9: $z_{i}=\\{l_{1},l_{2},\\ldots,l_{n}\\}$   \n10: for each $(f_{j})\\in v_{i}$ do   \n11: $l_{j}\\leftarrow V\\bar{A}E(f_{j})$   \n12: end for   \n13: end for   \n14: Fine-tune the $T2I$ with $\\{v_{i},t_{i}\\}$ to obtain video diffusion model $T2V$   \n15: Train the Seq2Seq model $S e q2S e q$ with all $\\{x_{i},z_{i}\\}$ using MSE loss   \n16: Train the semantic predictor $\\mathcal{P}_{s}$ with all $\\{x_{i},t_{i}\\}$ using MSE loss   \n17: Train the dynamic predictor $\\mathcal{P}_{d}$ with all $\\{x_{i},d_{i}\\}$ using Cross Entropy loss   \n18: return $T2V,S e q2S e q,\\mathcal{P}_{s},\\mathcal{P}_{d}$ ", "page_idx": 16}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/899ffb3beac3c7837e0befebc2cb4d4d2814e787c025315beedef0d36c78fd56.jpg", "img_caption": ["Figure 8: Various videos reconstruction samples. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/ab1a83ed4866339776ec2707fac8918d0c9a44f0ea3ee5469c10bb10d940cedc.jpg", "img_caption": ["Figure 9: Various videos reconstruction samples. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/b3bdf0edd679d5d5b82d88ff4006c49c8d2d9c330664417f3d62b87f608295cc.jpg", "img_caption": ["Figure 10: Various videos reconstruction samples. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/595872aa9ce5d5c029cfbf1cb7ba085cade37175558df6fc09c5ca2a77651c0e.jpg", "img_caption": ["Figure 11: Various videos reconstruction samples. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/7c50d83643688060f0106b98685644565734f4606a79c930013fc989502448af.jpg", "img_caption": ["Figure 12: Various videos reconstruction samples. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "RfsfRn9OFd/tmp/a40d64c7cdf7ae52e972540044366c6951542ca1c30e76c9f0918639798d4c4c.jpg", "img_caption": ["Figure 13: Some failure samples reconstructed in ours. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We made clear claims of our contributions in the abstract and introduction. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discussed the limitations of our research. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper dose not include theoretical results. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we demonstrate all the information needed to reproduce the main experimental results. Moreover, we will public our code for reproducibility. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we have provide the code for implementing our experiments and part of the datasets (one data and labels) in the supplementary. However, due to the size limitation and the principle of anonymity, we are unable to upload whole dataset. We will soon publish our dataset and code on our official website of the institute. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We demonstrated our method and the experiment settings clearly in Section 5.1. Also, we will public our code of training and testing process. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we present the error bar and all statistical significances of our experiments. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, we demonstrated in Section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Yes, we conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Yes, we discuss the broader impacts of the EEG2Video technique. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We will check the purpose of people who applied our dataset. Our models and data do not have high risks for misuse. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We cite all original papers of EEG models used in our experiments. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes, our novel dataset is well documented. And the documentation will be provided alongside the dataset. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we provide the paper include the full text of instructions and sufficient monetary reimbursement to subjects, which is above the minimum wage. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Our experiment has obtained the IRB approvals from our local institution. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]