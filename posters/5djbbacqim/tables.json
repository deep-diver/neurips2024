[{"figure_path": "5DJBBACqim/tables/tables_7_1.jpg", "caption": "Table 1: Sum of objectives. On all starting morphologies, optimizing the noise injection objective results in lower loss than directly optimizing the dual loss.", "description": "This table presents the results of an ablation study comparing two methods for optimizing modularity objectives: noise injection and the direct optimization of the dual loss.  The study was performed on four different robot morphologies (centipede, worm, hybrid, and claw). For each morphology, the table shows the average sum of the modularity objectives achieved after optimizing using either noise injection or the dual loss. The results consistently demonstrate that noise injection produces lower loss values, indicating that it is a more effective optimization strategy.", "section": "5.3 Ablation Study"}, {"figure_path": "5DJBBACqim/tables/tables_14_1.jpg", "caption": "Table 2: RL Hyperparameters", "description": "This table lists the hyperparameters used for reinforcement learning (RL) in the paper's experiments.  It shows the values used for various parameters such as the value loss factor, entropy bonus factor, discount factor, GAE lambda, PPO clip value, gradient clip value, starting learning rate, number of iterations per update, learning rate scheduler, number of processes, batch size, number of timesteps, and base hidden size.  Different values are provided for locomotion and grasping tasks, reflecting the different requirements of these distinct robotic control problems.", "section": "3.2 Modular Architecture and Training Pipeline"}, {"figure_path": "5DJBBACqim/tables/tables_15_1.jpg", "caption": "Table 3: NerveNet Hyperarameter Search", "description": "This table shows the hyperparameters used for the NerveNet model, including the number of layers, message size, and whether or not a skip connection was used.  The values tried for each parameter during hyperparameter tuning are also listed.", "section": "5.1 Transfer Learning"}, {"figure_path": "5DJBBACqim/tables/tables_15_2.jpg", "caption": "Table 4: Transformer Hyperparameters", "description": "This table lists the hyperparameters used for the Transformer-based architecture in the MetaMorph baseline.  It includes the number of attention heads, the embedding dimension, the feedforward dimension, the activation function used (ReLU), and the dropout rate.", "section": "5.2 Results"}, {"figure_path": "5DJBBACqim/tables/tables_16_1.jpg", "caption": "Table 5: Locomotion Observation Space", "description": "This table details the observation space used in the locomotion experiments.  It breaks down the observations by controller type (boss, boss, module), node type (root, joint), token type (global, joint), observation type (position, velocity, orientation), and axis (x, y, z).  The table shows which observations are available to different parts of the control system for the locomotion task.", "section": "5.1 Transfer Learning"}, {"figure_path": "5DJBBACqim/tables/tables_16_2.jpg", "caption": "Table 6: Grasping Observation Space", "description": "This table details the observation space used in the grasping experiments.  It shows what type of controller (boss or boss, module) is used, the node type (root or joint), the token type (global or joint), the observation type (relative fingertip position to object, joint position, joint velocity, joint relative orientation, joint relative position), and the axis (x, y, z) along which the observation is made.  This information is crucial for understanding the input data used by the model during the grasping tasks.", "section": "5 Experiments"}]