[{"type": "text", "text": "Monomial Matrix Group Equivariant Neural Functional Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Viet-Hoang Tran\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thieu N. Vo\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Mathematics National University of Singapore hoang.tranviet@u.nus.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics National University of Singapore thieuvo@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Tho Tran-Huu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "An T. Nguyen FPT Software AI Center annt68@fpt.com ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics National University of Singapore thotranhuu@u.nus.edu.vn ", "page_idx": 0}, {"type": "text", "text": "Tan M. Nguyen Department of Mathematics National University of Singapore tanmn@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural functional networks (NFNs) have recently gained significant attention due to their diverse applications, ranging from predicting network generalization and network editing to classifying implicit neural representation. Previous NFN designs often depend on permutation symmetries in neural networks\u2019 weights, which traditionally arise from the unordered arrangement of neurons in hidden layers. However, these designs do not take into account the weight scaling symmetries of ReLU networks, and the weight sign flipping symmetries of sin or Tanh networks. In this paper, we extend the study of the group action on the network weights from the group of permutation matrices to the group of monomial matrices by incorporating scaling/sign-flipping symmetries. Particularly, we encode these scaling/sign-flipping symmetries by designing our corresponding equivariant and invariant layers. We name our new family of NFNs the Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFN). Because of the expansion of the symmetries, Monomial-NFN has much fewer independent trainable parameters compared to the baseline NFNs in the literature, thus enhancing the model\u2019s efficiency. Moreover, for fully connected and convolutional neural networks, we theoretically prove that all groups that leave these networks invariant while acting on their weight spaces are some subgroups of the monomial matrix group. We provide empirical evidences to demonstrate the advantages of our model over existing baselines, achieving competitive performance and efficiency. The code is publicly available at https://github.com/MathematicalAI-NUS/MonomialNFN. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks (DNNs) have become highly versatile modeling tools, finding applications across a broad spectrum of fields such as Natural Language Processing [15, 29, 51, 66], Computer ", "page_idx": 0}, {"type": "text", "text": "Vision [27, 37, 61], and the Natural Sciences [31, 49]. There has been growing interest in developing specialized neural networks to process the weights, gradients, or sparsity masks of DNNs as data. These specialized neural networks are called neural functional networks (NFNs) [71]. NFNs have found diverse applications, ranging from predicting network generalization and network editing to classifying implicit neural representations. For instance, NFNs have been employed to create learnable optimizers for neural network training [5, 11, 42, 52], extract information from implicit neural representations of data [43, 52, 60], perform corrective editing of network weights [13, 44, 56], evaluate policies [26], and conduct Bayesian inference using networks as evidence [59]. ", "page_idx": 1}, {"type": "text", "text": "Developing NFNs is inherently challenging due to their high-dimensional nature. Some early methods to address this challenge assume a restricted training process that effectively reduced the weight space [9, 19, 41]. More recent efforts have focused on building permutation equivariant NFNs that can process neural network weights without such restrictions [35, 45, 71, 72]. These works construct NFNs that are equivariant to permutations of weights, corresponded to the rearrangement of neurons in hidden layers. Such permutations, known as neuron permutation symmetries, preserve the network\u2019s behavior. However, these approaches often overlook other significant symmetries in weight spaces [12, 24]. Notable examples are weight scaling transformations for ReLU networks [7, 12, 46] and sign flipping transformations for sin and tanh networks [14, 22, 38]. Consequently, two weight spaces of a ReLU networks, that differ by a scaling transformation, two weight spaces of a sin, or tanh networks that differ by a sign flipping transformation, can produce different results when processed by existing permutation equivariant NFNs, despite representing the same functions. This highlights a fundamental limitation of the current permutation equivariant NFNs. ", "page_idx": 1}, {"type": "text", "text": "Contribution. In this paper, we extend the study of symmetries in weight spaces of Fully Connected Neural Networks (FCNNs) and Convolution Neural Networks (CNNs) by formally establishing a group of symmetries that includes both neuron permutations and scaling/sign-flipping transformations. These symmetries are represented by monomial matrices, which share the nonzero pattern of permutation matrices but allow nonzero entries to be any value rather than just 1. We then introduce a novel family of NFNs that are equivariant to groups of monomial matrices, thus incorporating both permutation and scaling/sign-flipping symmetries into the NFN design. We name this new family Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFNs). Due to the expanded set of symmetries, Monomial-NFN requires significantly fewer independent trainable parameters compared to baseline NFNs, enhancing the model\u2019s efficiency. By incorporating equivariance to neuron permutations and weight scaling/sign-flipping, our NFNs demonstrate competitive generalization performance compared to existing models. Our contribution is three-fold: ", "page_idx": 1}, {"type": "text", "text": "1. We formally describe a group of monomial matrices satisfying the condition that the transformation of weight spaces of FCNNs and CNNs using these group elements does not change the function defined by the networks. For ReLU networks, this group covers permutation and scaling symmetries of the weight spaces, while for sin or tanh networks, this group covers permutation and sign-filpping symmetries. The group is proved to be maximal in certain cases. 2. We design Monomial-NFNs, the first family of NFNs that incorporate scaling and sign-filpping symmetries of weight spaces as far as we are aware. The main building blocks of MonomialNFNs are the equivariant and invariant linear layers for processing weight spaces. 3. We show that the number of parameters in our equivariant linear layer is much lower than in recent permutation equivariant NFNs. In particular, our method is linear in the number of layers and dimensions of weights and biases, compared to quadratic as in [71]. This demonstrates that Monomial-NFNs have the ability to process weight spaces of large-scale networks. ", "page_idx": 1}, {"type": "text", "text": "We evaluate Monomial-NFNs on three tasks: predicting CNN generalization from weights using Small CNN Zoo [64], weight space style editing, and classifying INRs using INRs data [71]. Experimental results show that our model achieves competitive performance and efficiency compared to existing baselines. ", "page_idx": 1}, {"type": "text", "text": "Organization. We structure this paper as follows: After summarizing some related work in Section 2, we recall the notions of monomial matrix group and describe their maximal subgroups preserved by some nonlinear activations in Section 3. In Section 4, we formalize the general weight space of FCNNs and CNNs, then discuss the symmetries of these weight spaces using the monomial matrices. In Section 5, we construct monomial matrix group equivariant and invariant layers, which are building blocks for our Monomial-NFNs. In Section 6, we present our experimental results to justify the advantages of Monomial-NFNs over the existing permutation equivariant NFN baselines. The paper ends with concluding remarks. More experimental details are provided in the Appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Symmetries of Weight Spaces. The challenge of identifying the symmetries in the weight spaces of neural networks, or equivalently, determining the functional equivalence of neural networks, is a well-explored area in academic research [3, 10, 16, 23, 47]. This problem was initially posed by Hecht-Nielsen in [28]. Results for various types of networks have been established as in [1, 2, 12, 14, 22, 38, 63]. ", "page_idx": 2}, {"type": "text", "text": "Neural Functional Networks. Recent research has focused on learning representations for trained classifiers to predict their generalization performance and other insights into neural networks [8, 20, 64, 54, 53, 55]. In particular, low-dimensional encodings for Implicit Neural Representations (INRs) have been developed for downstream tasks [18, 41]. Other studies have encoded and decoded neural network parameters mainly for reconstruction and generation purposes [6, 21, 34, 48]. ", "page_idx": 2}, {"type": "text", "text": "Equivariant Neural Functional Networks. Permutations and scaling, for ReLU networks, as well as sign-filpping, for sine or tanh networks, symmetries, are fundamental symmetries of weight networks. Permutation-equivariant NFNs are successfully built in [4, 35, 40, 45, 70, 71, 72]. In particular, the authors in [35, 40] carefully construct computational graphs representing the input neural networks\u2019 parameters and process the graphs using graph neural networks. In [4], neural network parameters are efficiently encoded by carefully choosing appropriate set-to-set and set-to-vector functions. The authors in [70] view network parameters as a special case of a collection of tensors and then construct maximally expressive equivariant linear layers for processing any collection of tensors given a description of their permutation symmetries. These methods are applicable to several types of networks, including those with branches or transformers. However, these models were not necessarily equivariant to scaling nor sign-filpping transformations, which are important symmetries of the input neural networks. ", "page_idx": 2}, {"type": "text", "text": "Our method makes the first step toward incorporating both permutation and non-permutation symmetries into NFNs. In particular, the model proposed in our paper is equivariant to permutations and scaling, for ReLU networks, or sign-flipping, for sine and tanh networks. This leads to a significant reduction in the number of parameters, a property that is particularly useful for large neural networks in modern deep learning, while achieving comparable or better results than those in the literature. The authors in [32, 67] have also developed NFNs that incorporates scaling symmetries. ", "page_idx": 2}, {"type": "text", "text": "3 Monomial Matrices Perserved by a Nonlinear Activation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given two sets $X,Y$ , and a group $G$ acts on them, a function $\\phi\\colon X\\rightarrow Y$ is called $G$ -equivariant if $\\phi(g\\cdot x)=g\\cdot\\phi(x)$ for all $x\\in X$ and $g\\in G$ . If $G$ acts trivially on $Y$ , then we say $\\phi$ is $G$ -invariant. In this paper, we consider NFNs which are equivariant with respect to certain symmetries of deep weight spaces. These symmetries will be represented by monomial matrices. In Subsection 3.1, we recall the notion of monomial matrices, as well as their actions on space of matrices. We then formalize the maximal group of matrices preserved by the activations ReLU, sin and Tanh in Subsection 3.2. ", "page_idx": 2}, {"type": "text", "text": "3.1 Monomial Matrices and Monomial Matrix Group Actions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "All matrices considered in this paper have real entries and $n$ is a positive integer. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (See [50, page 46]). A matrix of size $n\\times n$ is called a monomial matrix (or generalized permutation matrix) if it has exactly one non-zero entry in each row and each column, and zeros elsewhere. We will denote by $\\mathcal{G}_{n}$ the set of such all matrices. ", "page_idx": 2}, {"type": "text", "text": "Permutation matrices and invertible diagonal matrices are special cases of monomial matrices. In particular, a permutation matrix is a monomial matrix in which the non-zero entries are all equal to 1. In case the nonzero entries of a monomial matrix are in the diagonal, it becomes an invertible diagonal matrix. We will denote by ${\\mathcal{P}}_{n}$ and $\\Delta_{n}$ the sets of permutation matrices and invertible diagonal matrices of size $n\\times n$ , respectively. It is well-known that the groups $\\mathcal{G}_{n}$ , ${\\mathcal{P}}_{n}$ , and $\\Delta_{n}$ are subgroups of the general linear group ${\\mathrm{GL}}(n)$ . ", "page_idx": 2}, {"type": "text", "text": "Permutation matrix group ${\\mathcal{P}}_{n}$ is a representation of the permutation group $S_{n}$ , which is the group of all permutations of the set $\\{1,2,\\ldots,n\\}$ with group operator as the composition. Indeed, for each permutation $\\pi\\in S_{n}$ , we denote by $P_{\\pi}$ the square matrix obtained by permuting $n$ columns of the identity matrix $I_{n}$ by $\\pi$ . We call $P_{\\pi}$ the permutation matrix corresponding to $\\pi$ . The correspondence $\\pi\\mapsto P_{\\pi}$ defines a group homomorphism $\\rho\\colon S_{n}\\to{\\mathrm{GL}}(n)$ with the image $\\mathscr{P}_{n}=\\rho(S_{n})$ . ", "page_idx": 2}, {"type": "text", "text": "Each monomial matrix in $\\mathcal{G}_{n}$ is a product of an invertible diagonal matrix in $\\Delta_{n}$ and a permutation matrix in ${\\mathcal{P}}_{n}$ , i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{G}}_{n}=\\{D P\\,:\\,D\\in\\Delta_{n}\\;{\\mathrm{and}}\\;P\\in{\\mathcal{P}}_{n}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In general, we have $P D\\neq D P$ . However, for $D=\\operatorname{diag}(d_{1},d_{2},\\ldots,d_{n})$ and $P=P_{\\pi}$ , we have $P\\bar{D}=(P D P^{-1})P$ which is again a product of the invertible diagonal matrix ", "page_idx": 3}, {"type": "equation", "text": "$$\nP D P^{-1}=\\mathrm{diag}(d_{\\pi^{-1}(1)},d_{\\pi^{-1}(2)},\\ldots,d_{\\pi^{-1}(n)})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and the permutation matrix $P$ . As an implication of Eq. (2), there is a group homomorphism $\\varphi\\colon{\\mathcal{P}}_{n}\\ {\\overset{}{\\to}}\\ \\operatorname{Aut}(\\Delta_{n}).$ , defined by the conjugation, i.e. $\\varphi(\\mathbf{\\dot{P}})(D)\\,=\\,P D P^{-1}$ for all $P\\in\\mathcal{P}_{n}$ and $D\\in\\Delta_{n}$ . The map $\\varphi$ defines the group $\\mathcal{G}_{n}$ as the semidirect product $\\mathcal{G}_{n}=\\Delta_{n}\\rtimes_{\\varphi}\\mathcal{P}_{n}$ (see [17]). For convenience, we sometimes denote element $D P$ of $\\mathcal{G}_{n}$ as a pair $(D,P)$ . ", "page_idx": 3}, {"type": "text", "text": "The groups $\\mathcal{G}_{n}$ , ${\\mathcal{P}}_{n}$ and $\\Delta_{n}$ act on the left and the right of $\\mathbb{R}^{n}$ and $\\mathbb{R}^{n\\times m}$ in a canonical way (by matrix-vector or matrix-matrix multiplications). More precisely, we have: ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2. Let $\\mathbf{x}\\in\\mathbb{R}^{n}$ and $A=(A_{i j})\\in\\mathbb{R}^{n\\times m}$ . Then for $D=\\mathrm{diag}(d_{1},\\ldots,d_{n})\\in\\Delta_{n}$ , $\\overline{{D}}=\\mathrm{diag}(\\overline{{d}}_{1},\\ldots,\\overline{{d}}_{m})\\in\\Delta_{m}$ , $P_{\\pi}\\in\\mathcal{P}_{n}$ , and $P_{\\sigma}\\in\\mathcal{P}_{m}$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{P_{\\pi}\\cdot{\\bf x}=\\left(x_{\\pi^{-1}(1)},x_{\\pi^{-1}(2)},\\ldots,x_{\\pi^{-1}(n)}\\right)^{\\top},}\\\\ {D\\cdot{\\bf x}=\\left(d_{1}\\cdot x_{1},d_{2}\\cdot x_{2},\\ldots,d_{n}\\cdot x_{n}\\right)^{\\top},}\\\\ {\\left(D\\cdot P_{\\pi}\\cdot A\\cdot P_{\\sigma}\\cdot\\overline{{D}}\\right)_{i j}=d_{i}\\cdot A_{\\pi^{-1}(i)\\sigma(j)}\\cdot\\overline{{d}}_{j},}\\\\ {\\left(D\\cdot P_{\\pi}\\cdot A\\cdot(\\overline{{D}}\\cdot P_{\\sigma})^{-1}\\right)_{i j}=d_{i}\\cdot A_{\\pi^{-1}(i)\\sigma^{-1}(j)}\\cdot\\overline{{d}}_{j}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above proposition can be verified by a direct computation, and is used in subsequent sections. ", "page_idx": 3}, {"type": "text", "text": "3.2 Monomial Matrices Preserved by a Nonlinear Activation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We characterize the maximal matrix groups preserved by the activations $\\sigma=\\mathrm{{ReLU}}$ , sin or tanh. Here, ReLU is the rectified linear unit activation function which has been used in most of modern neural networks, sin is the sine function which is often used as an activation function in implicit neural representations [57], and tanh is the hyperbolic tangent activation function. Different variants of the results in this subsection can also be found in [24, 68]. We refine them using the terms of monomial matrices and state explicitly here for the completeness of the paper. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3. A matrix $A\\,\\in\\,\\mathrm{GL}(n)$ is said to be preserved by an activation $\\sigma$ if and only if $\\sigma(A\\cdot\\mathbf{x})=A\\cdot\\sigma(\\mathbf{x})$ for all $\\mathbf{x}\\in\\mathbb{R}^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "We adopt the term matrix group preserved by an activation from [68]. This term is then referred to as the intertwiner group of an activation in [24]. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.4. For every matrix $A\\in\\operatorname{GL}(n)$ , we have: ", "page_idx": 3}, {"type": "text", "text": "(i) $A$ is preserved by the activation ReLU if and only if $A\\in\\mathcal{G}_{n}^{>0}$ . Here, $\\mathcal{G}_{n}^{>0}$ is the subgroup of $\\mathcal{G}_{n}$ containing only monomial matrices whose nonzero entries are positive numbers.   \n(ii) $A$ is preserved by the activation $\\sigma=\\sin o r$ tanh if and only if $A\\in\\mathcal G_{n}^{\\pm1}$ . Here, $\\mathcal{G}_{n}^{\\pm1}$ is the subgroup of ${\\mathcal{G}}_{n}$ containing only monomial matrices whose nonzero entries are $\\pm1$ . ", "page_idx": 3}, {"type": "text", "text": "A detailed proof of Proposition 3.4 can be found in Appendix C.1. As a consequence of the above theorem, $\\mathcal{G}_{n}^{>0}$ (respectively, $\\mathcal{G}_{n}^{\\pm1}$ ) is the maximal matrix subgroup of the general linear group ${\\mathrm{GL}}(n)$ that is preserved by the activation ReLU (respectively, sin and tanh). ", "page_idx": 3}, {"type": "text", "text": "Remark 3.5. Intuitively, $\\mathcal{G}_{n}^{>0}$ is generated by permuting and positive scaling the coordinates of vectors in $\\mathbb{R}^{n}$ , while $\\mathcal{G}_{n}^{\\pm\\mathrm{i}}$ is generated by permuting and sign filpping. Formally, these groups can be written as the semidirect products: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{G}_{n}^{>0}=\\Delta_{n}^{>0}\\rtimes_{\\varphi}\\mathcal{P}_{n},\\ \\ \\ a n d\\ \\ \\mathcal{G}_{n}^{\\pm1}=\\Delta_{n}^{\\pm1}\\rtimes_{\\varphi}\\mathcal{P}_{n},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{n}^{>0}=\\{D=\\mathrm{diag}(d_{1},\\ldots,d_{n})\\ :\\ d_{i}>0\\}\\,,\\ a n d}\\\\ &{\\Delta_{n}^{\\pm1}=\\{D=\\mathrm{diag}(d_{1},\\ldots,d_{n})\\ :\\ d_{i}\\in\\{-1,1\\}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "are two subgroups of $\\Delta_{n}$ . ", "page_idx": 3}, {"type": "text", "text": "4 Weight Spaces and Monomial Matrix Group Actions on Weight Spaces ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we formulate the general structure of the weight spaces of FCNNs and CNNs. We then determine the group action on these weight spaces using monomial matrices. The activation function $\\sigma$ using on the considered FCNNs and CNNs are assumed to be ReLU or sin or Tanh. ", "page_idx": 4}, {"type": "text", "text": "4.1 Weight Spaces of FCNNs and CNNs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Weight Spaces of FCNNs. Consider an FCNN with $L$ layers, $n_{i}$ neurons at the $i$ -th layer, and $n_{0}$ and $n_{L}$ be the input and output dimensions, together with the activation $\\sigma$ , as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\mathbf{x}\\,;\\,U,\\sigma)=W^{(L)}\\cdot\\sigma\\left(\\dots\\sigma\\left(W^{(2)}\\cdot\\sigma\\left(W^{(1)}\\cdot\\mathbf{x}+b^{(1)}\\right)+b^{(2)}\\right)\\dots\\right)+b^{(L)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $U=(W,b)$ is the parameters with the weights $W=\\{W^{(i)}\\in\\mathbb{R}^{n_{i}\\times n_{i-1}}\\}_{i=1}^{L}$ and the biases $b=\\{b^{(i)}\\in\\mathbb{R}^{n_{i}\\times1}\\}_{i=1}^{L}$ . The pair $U=(W,b)$ belongs to the weight space $\\mathcal{U}=\\mathcal{W}\\times\\mathcal{B}$ , where: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}=\\mathbb{R}^{n_{L}\\times n_{L-1}}\\times...\\times\\mathbb{R}^{n_{2}\\times n_{1}}\\times\\mathbb{R}^{n_{1}\\times n_{0}},}\\\\ &{\\mathcal{B}=\\mathbb{R}^{n_{L}\\times1}\\times...\\times\\mathbb{R}^{n_{2}\\times1}\\times\\mathbb{R}^{n_{1}\\times1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Weight Spaces of CNNs. Consider a CNN with $L$ convolutional layers, ending with an average pooling layer then fully connected layers, together with activation $\\sigma$ . Let $n_{i}$ and $w_{i}$ be the number of channels and the size of the convolutional kernel at the $i^{\\mathrm{th}}$ convolutional layer. We will only take account of the $L$ convolutional layers, since the weight space of the fully connected layers are already considered above, and the pooling layer has no learnable parameters: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\mathbf{x}\\,;\\,U,\\sigma)=\\sigma\\left(W^{(L)}*\\sigma\\left(\\dots\\sigma\\left(W^{(2)}*\\sigma\\left(W^{(1)}*\\mathbf{x}+b^{(1)}\\right)+b^{(2)}\\right)\\dots\\right)+b^{(L)}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $U=(W,b)$ is the learnable parameters with the weights $W=\\{W^{(i)}\\in\\mathbb{R}^{w_{i}\\times n_{i}\\times n_{i-1}}\\}_{i=1}^{L}$ and the biases $b=\\{b^{(i)}\\in\\mathbb{R}^{1\\times n_{i}\\times1}\\}_{i=1}^{L}$ . The convolutional operator $^*$ is defined depending on the purpose of the model, and adding $b$ means adding $b_{j}^{(i)}$ to all entries of $j^{-}$ th channel at $i^{\\mathrm{th}}$ layer. The pair $U=(W,b)$ belongs to the weight space $\\mathcal{U}=\\mathcal{W}\\times\\mathcal{B}$ , where: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}=\\mathbb{R}^{w_{L}\\times n_{L}\\times n_{L-1}}\\times...\\times\\mathbb{R}^{w_{2}\\times n_{2}\\times n_{1}}\\times\\mathbb{R}^{w_{1}\\times n_{1}\\times n_{0}},}\\\\ &{\\mathcal{B}=\\mathbb{R}^{1\\times n_{L}\\times1}\\times...\\times\\mathbb{R}^{1\\times n_{2}\\times1}\\times\\mathbb{R}^{1\\times n_{1}\\times1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 4.1. See in Appendix. C.2 for concrete descriptions of weight spaces of FCNNs and CNNs. ", "page_idx": 4}, {"type": "text", "text": "4.2 Monomial Matrix Group Action on Weight Spaces ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The weight space $\\boldsymbol{\\mathcal{U}}$ of an FCNN or CNN with $L$ layers and $n_{i}$ channels at $i^{\\mathrm{th}}$ layer has the general form $\\mathcal{U}=\\mathcal{W}\\times\\mathcal{B}$ , where: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{W}=\\mathbb{R}^{w_{L}\\times n_{L}\\times n_{L-1}}\\times...\\times\\mathbb{R}^{w_{2}\\times n_{2}\\times n_{1}}\\times\\mathbb{R}^{w_{1}\\times n_{1}\\times n_{0}},}\\\\ &{\\mathcal{B}=\\mathbb{R}^{b_{L}\\times n_{L}\\times1}\\times...\\times\\mathbb{R}^{b_{2}\\times n_{2}\\times1}\\times\\mathbb{R}^{b_{1}\\times n_{1}\\times1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $n_{i}$ is the number of channels at the $i^{\\mathrm{th}}$ layer, in particular, $n_{0}$ and $n_{L}$ are the number of channels of input and output; $w_{i}$ is the dimension of weights and $b_{i}$ is the dimension of the biases in each channel at the $i$ -th layer. The dimension of the weight space $\\boldsymbol{\\mathcal{U}}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{dim}\\mathcal{U}=\\sum_{i=1}^{L}\\left(w_{i}\\times n_{i}\\times n_{i-1}+b_{i}\\times n_{i}\\times1\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notation. When working with weight matrices in $\\mathcal{W}$ , the space $\\mathbb{R}^{w_{i}\\times n_{i}\\times n_{i-1}}=(\\mathbb{R}^{w_{i}})^{n_{i}\\times n_{i-1}}$ at the $i^{\\mathrm{th}}$ layer will be considered as the space of $n_{i}\\times n_{i-1}$ matrices, whose entries are real vectors in $\\mathbb{R}^{w_{i}}$ . s In particular, the symbol $W^{(i)}$ denotes a matrix in $\\mathbb{R}^{w_{i}\\times n_{i}\\times n_{i-1}}=(\\mathbb{R}^{w_{i}})^{n_{i}\\times n_{i-1}}$ , while $W_{j k}^{(i)}\\in\\mathbb{R}^{w_{i}}$ denotes the entry at row $j$ and column $k$ of $W^{(i)}$ . Similarly, the notion $b^{(i)}$ denotes a bias column vector in $\\mathbb R^{b_{i}\\times n_{i}\\times1}=(\\mathbb R^{b_{i}})^{n_{i}\\times1}$ , while $b_{j}^{(i)}\\in\\mathbb{R}^{b_{i}}$ denotes the entry at row $j$ of $b^{(i)}$ . To define the group action of $\\boldsymbol{\\mathcal{U}}$ using monomial matrices, denote $\\mathcal{G}_{\\mathcal{U}}$ as the group: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{G}_{\\mathcal{U}}:=\\mathcal{G}_{n_{L}}\\times...\\times\\mathcal{G}_{n_{1}}\\times\\mathcal{G}_{n_{0}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Ideally, each monomial matrix group $\\mathcal{G}_{n_{i}}$ will act on the weights and the biases at the $i^{\\mathrm{th}}$ layer of the network. Each element of $\\mathcal{G}_{\\mathcal{U}}$ will be of the form $g=\\left(g^{(L)},\\bar{\\dots},g^{(0)}\\right)$ , where: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{g}^{(i)}=\\boldsymbol{D}^{(i)}\\cdot\\boldsymbol{P}_{\\pi_{i}}=\\operatorname{diag}\\left(\\boldsymbol{d}_{1}^{(i)},\\ldots,\\boldsymbol{d}_{n_{i}}^{(i)}\\right)\\cdot\\boldsymbol{P}_{\\pi_{i}}\\in\\mathcal{G}_{n_{i}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some invertible diagonal matrix $D^{(i)}$ and permutation matrix $P_{\\pi_{i}}$ . The action of $\\mathcal{G}_{\\mathcal{U}}$ on $\\boldsymbol{\\mathcal{U}}$ is defined formally as follows. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.2 (Group action on weight spaces). With the notation as above, the group action of $\\mathcal{G}_{\\mathcal{U}}$ on $\\boldsymbol{\\mathcal{U}}$ is defined to be the map $\\mathcal{G}_{\\mathcal{U}}\\times\\mathcal{U}\\to\\mathcal{U}$ with $(g,U)\\mapsto g U=(g W,g b)$ , where: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(g W)^{(i)}:=\\left(g^{(i)}\\right)\\cdot W^{(i)}\\cdot\\left(g^{(i-1)}\\right)^{-1}\\;\\mathrm{~and~}\\;(g b)^{(i)}:=\\left(g^{(i)}\\right)\\cdot b^{(i)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In concrete: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(g W)_{j k}^{(i)}:=\\frac{d_{j}^{(i)}}{d_{k}^{(i-1)}}\\cdot W_{\\pi_{i}^{-1}(j)\\pi_{i-1}^{-1}(k)}^{(i)}\\ \\ \\mathrm{and}\\ \\left(g b\\right)_{j}^{(i)}:=d_{j}^{(i)}\\cdot b_{\\pi_{i}^{-1}(j)}^{(i)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 4.3. The group $\\mathcal{G}_{\\mathcal{U}}$ is determined only by the number of layers $L$ and the numbers of channels $n_{i}$ , not by the dimensions of weights $w_{i}$ and biases $b_{i}$ at each channel. ", "page_idx": 5}, {"type": "text", "text": "The group $\\mathcal{G}_{\\mathcal{U}}$ has nice behaviors when acting on the weight spaces of FCNNs given in Eq. (5) and CNNs given in Eq. (8). In particular, depending on the specific choice of the activation $\\sigma$ , the function $f$ built by the given FCNN or CNN is invariant under the action of a subgroup $G$ of $\\mathcal{G}_{\\mathcal{U}}$ , as we will see in the following proposition. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.4 ( $G$ -equivariance of neural functionals). Let $f\\,=\\,f(\\cdot\\,;\\,U,\\sigma)$ be an FCNN given in Eq. (5) or CNN given in Eq. (8) with the weight space $\\textbf{\\textit{U}}\\in\\textbf{\\textit{u}}$ and an activation $\\sigma\\ \\in$ $\\{\\mathrm{ReLU},\\mathrm{Tanh},\\mathrm{sin}\\}$ . Let us defined a subgroup $G$ of $\\mathcal{G}_{\\mathcal{U}}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$G=\\{{\\mathrm{id}}_{{\\mathcal{G}}_{n_{L}}}\\}\\times{\\mathcal{G}}_{n_{L-1}}^{>0}\\times...\\times{\\mathcal{G}}_{n_{1}}^{>0}\\times\\{{\\mathrm{id}}_{{\\mathcal{G}}_{n_{0}}}\\}.$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then $f$ is $G$ -invariant under the action of $G$ on its weight space, i.e. ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(\\mathbf{x}\\,;\\,U,\\sigma)=f(\\mathbf{x}\\,;\\,g U,\\sigma)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $g\\in G$ , $U\\in\\mathcal{U}$ and $\\mathbf{x}\\in\\mathbb{R}^{n_{0}}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 4.5 (Maximality of $G$ ). The proof of Proposition 4.4 can be found in Appendix C.2. The group $G$ defined above is even proved to be the maximal choice in the case: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\,\\,\\sigma=\\mathrm{ReLU}\\,a n d\\,n_{L}\\geqslant...\\geqslant n_{2}\\geqslant n_{1}>n_{0}=1\\,(s e e\\,/\\,l2,\\,25J),\\,o r}\\\\ &{\\bullet\\,\\,\\sigma=\\operatorname{tanh}\\,(s e e\\,/\\,l4,\\,22J).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $G$ is maximal in the sense that: if $U^{\\prime}$ is another element in $\\boldsymbol{\\mathcal{U}}$ with $f(\\cdot\\,;\\,U,\\sigma)=f(\\cdot\\,;\\,U^{\\prime},\\sigma)$ , then there exists an element $g\\in G$ such that $U^{\\prime}=g U$ . It is natural to ask whether the group $G$ is still maximal in the other case. This question still remains open and we leave it for future exploration. ", "page_idx": 5}, {"type": "text", "text": "According to Proposition 4.4, the symmetries of the weight space of an FCNN or CNN must include not only permutation matrices but also other types of monomial matrices resulting from scaling (for ReLU networks) or sign flipping (for sin and tanh networks) the weights. Recent works on NFN design only take into account the permutation symmetries of the weight space. Therefore, it is necessary to design a new class of NFNs that incorporates these missing symmetries. We will introduce such a class in the next section. ", "page_idx": 5}, {"type": "text", "text": "5 Monomial Matrix Group Equivariant and Invariant NFNs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce a new family of NFNs, called Monomial-NFNs, by incorporating symmetries arising from monomial matrix groups which have been clarified in Proposition 4.4. The main components of Monomial-NFNs are the monomial matrix group equivariant and invariant linear layers between two weight spaces which will be presented in Subsections 5.1 and 5.2, respectively. We will only consider the case of ReLU activation. Network architectures with other activations will be considered in detail in Appendices A and B. ", "page_idx": 5}, {"type": "text", "text": "In the following, $\\mathcal{U}=(\\mathcal{W},\\mathcal{B})$ is the weight space with $L$ layers, $n_{i}$ channels at $i^{\\mathrm{th}}$ layer, and the dimensions of weight and bias are $w_{i}$ and $b_{i}$ , respectively (see Eqs. (11) and (12)). Since we consider ReLU network architectures, according to Proposition 4.4, the symmetries of the weight space is given by the subgroup $G=\\{{\\mathrm{id}}_{{\\mathcal{G}}_{n_{L}}}\\}\\times{\\overleftarrow{{\\mathcal{G}}}}_{n_{L-1}}^{>0}\\,\\hat{\\times}\\ldots\\times{\\mathcal{G}}_{n_{1}}^{>0}\\times\\{{\\mathrm{id}}_{{\\mathcal{G}}_{n_{0}}}\\}$ of $\\mathcal{G}_{\\mathcal{U}}$ . ", "page_idx": 5}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/63ecb353ea11434fea75b40831ff4391c64d45d143cff15f052a33ac1ba066a0.jpg", "table_caption": ["Table 1: Number of parameters in a linear equivariant layer $E\\colon\\mathcal{U}\\rightarrow\\mathcal{U}^{\\prime}$ with respect to permutation matrix groups in [71], and monomial matrix groups. Here, $c=\\operatorname*{max}\\{w_{i},b_{j}\\}$ and $\\bar{c^{\\prime}}=\\operatorname*{max}\\{w_{i}^{\\prime},b_{j}^{\\prime}\\}$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Equivariant Layers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now construct a linear $G^{\\prime}$ -equivariant layer between weight spaces. These layers form the fundamental building blocks for our Monomimal-NFNs. Let $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{U}^{\\prime}$ be two weight spaces of the same network architecture described in Eqs. (11) and (12), i.e. they have the same number of layers as well as the same number of channels at each layer. Denote the dimension of weights and biases in each channel at the $i$ -th layer of $\\mathcal{U}^{\\prime}$ as $w_{i}^{\\prime}$ and $\\dot{b_{i}^{\\prime}}$ , respectively. Note that, in this case, we have $\\mathcal{G}_{\\mathcal{U}}\\,=\\,\\mathcal{G}_{\\mathcal{U^{\\prime}}}$ . We construct $G$ -equivariant afine maps $E\\colon\\mathcal{U}\\to\\mathcal{U}^{\\prime}$ with $\\mathbf{x}\\mapsto\\mathfrak{a x}+\\mathfrak{b}$ , where ${\\mathfrak{a}}\\in\\mathbb{R}^{\\dim\\mathcal{U}^{\\prime}\\times\\dim\\mathcal{U}}$ and $\\mathfrak{b}\\in\\mathbb{R}^{\\dim\\mathcal{U}^{\\prime}\\times1}$ are learnable parameters. ", "page_idx": 6}, {"type": "text", "text": "To make $E$ to be $G$ -equivarient, $\\mathfrak{a}$ and $\\mathfrak{b}$ have to satisfy a system of constraints (usually called parameter sharing), which are induced from the condition ${\\dot{E}}(g U)\\,=\\,g E(U)$ for all $g\\,\\in\\,G$ and $U\\in\\mathcal{U}$ . We show in details what are these constraints and how to derive the concrete formula of $E$ in Appendix A. The formula of $E$ is presented as follows: For $U\\,=\\,(W,b)\\,\\in\\mathcal{U}$ , the image $E(U)\\stackrel{=}{=}(W^{\\prime},b^{\\prime})\\in\\mathcal{U}^{\\prime}$ is computed by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{W_{j k}^{\\prime(1)}=\\displaystyle\\sum_{q=1}^{n_{0}}\\mathfrak{p}_{1j q}^{1j k}W_{j q}^{(1)}+\\mathfrak{q}_{1j}^{1j k}b_{j}^{(1)},}}&{{b_{j}^{\\prime(1)}=\\displaystyle\\sum_{q=1}^{n_{0}}\\mathfrak{r}_{1j q}^{1j}W_{j q}^{(1)}+\\mathfrak{s}_{1j}^{1j}b_{j}^{(1)},}}\\\\ {{W_{j k}^{\\prime(i)}=\\mathfrak{p}_{i j k}^{i j k}W_{j k}^{(i)},}}&{{b_{j}^{\\prime(i)}=\\mathfrak{s}_{i j}^{i j}b_{j}^{(i)},\\quad1<i<L,}}\\\\ {{W_{j k}^{\\prime(L)}=\\displaystyle\\sum_{p=1}^{n_{L}}\\mathfrak{p}_{L p k}^{L j k}W_{p k}^{(L)},}}&{{b_{j}^{\\prime(L)}=\\displaystyle\\sum_{p=1}^{n_{L}}\\mathfrak{s}_{L p}^{L j}b_{p}^{(L)}+\\mathfrak{t}^{L j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $({\\mathfrak{p}},{\\mathfrak{q}},{\\mathfrak{r}},{\\mathfrak{s}},{\\mathfrak{t}})$ is the hyperparameter of $E$ . We discuss in detail the dimensions and sharing information between these parameters in Appendix A.1. Note that, we also show that all linear $G$ -equivariant functional are in this form in Appendix A. To conclude, we have: ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1. With notation as above, the linear functional map $E\\colon\\mathcal{U}\\rightarrow\\mathcal{U}^{\\prime}$ defined by Eq. (18) is $G$ -equivariant. Moreover, every $G$ -equivariant linear functional map from $\\boldsymbol{\\mathcal{U}}$ to $\\mathcal{U}^{\\prime}$ are in that form. ", "page_idx": 6}, {"type": "text", "text": "Number of parameters and comparison to previous works. The number of parameters in our layer is linear in $L,n_{0},n_{L}$ , which is significantly smaller than the number of parameters in layers described in [71], where it is quadratic in $L,n_{0},n_{L}$ (see Table 1). This reduction in parameter count means that our model is suitable for weight spaces of large-scale networks and deep NFNs. Intuitively, the advantage of our layer arises because the group $G$ acting on the weight spaces in our setting is much larger, resulting in a significantly smaller number of orbits in the quotient space $\\mathcal{U}/G$ . Since the number of orbits is equal to the number of parameters, this leads to a more compact representation. Additionally, the presence of the group $\\Delta_{*}^{>0^{\\circ}}$ forces many coefficients of the linear layer $E$ to be zero, further contributing to the efficiency of our model. ", "page_idx": 6}, {"type": "text", "text": "5.2 Invariant Layers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We will construct an $G$ -invariant layer $I\\colon\\mathcal{U}\\to\\mathbb{R}^{d}$ for a fixed integer $d>0$ . In order to do that, we will seek a map $I$ in the form: ", "page_idx": 6}, {"type": "equation", "text": "$$\nI=\\mathrm{MLP}\\circ I_{\\mathcal{P}}\\circ I_{\\Delta^{>0}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $I_{\\Delta>0}\\;:\\;\\mathcal{U}\\to\\mathcal{U}$ is an $\\Delta_{*}^{>0}$ -invariance and $\\mathcal{P}_{*}$ -equivariance map, $I_{\\mathcal{P}}:\\mathcal{U}\\to\\mathbb{R}^{\\dim\\mathcal{U}}$ is an $\\mathcal{P}_{*}$ -invariant map, and $\\mathrm{MLP}:\\mathbb{R}^{\\dim\\mathcal{U}}\\longrightarrow\\mathbb{R}^{d}$ is an arbitrary multilayer perceptron to adjust the output dimension. Since $G=\\mathcal{G}_{*}^{>0}=\\Delta_{*}^{>0}\\rtimes_{\\varphi}\\mathcal{P}_{*}$ (see Remark 3.5), the composition $I=\\mathrm{MLP}\\circ I_{\\mathcal{P}}\\circ I_{\\Delta^{>0}}$ is clearly $G$ -invariant as expected. The construction of $I_{\\Delta>0}$ and $I_{\\mathcal{P}}$ will be presented below. ", "page_idx": 6}, {"type": "text", "text": "Table 2: CNN prediction on Tanh subset of Small CNN Zoo with original and augmented data. ", "page_idx": 7}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/c0c4973cc3fb0a505e6df5810427ca2538afa4c98f5c5184b2c5009af043b6de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Construct $I_{\\Delta>0}$ . To capture $\\Delta_{*}^{>0}$ -invariance, we recall the notion of positively homogeneous of degree zero maps. For $n\\,>\\,0$ , a map $\\alpha$ from $\\mathbb{R}^{n}$ is called positively homogeneous of degree zero if for all $\\lambda\\,>\\,0$ and $(x_{1},\\ldots,x_{n})\\,\\in\\,\\mathbb{R}^{n}$ , we have $\\alpha(\\lambda x_{1},\\ldots,\\lambda x_{n})\\,=\\,\\alpha(x_{1},\\ldots,x_{n})$ . We construct $I_{\\Delta>0}:\\mathcal{U}\\to\\mathcal{U}$ by taking collections of positively homogeneous of degree zero functions $\\{\\alpha_{j k}^{(i)}\\colon\\mathbb{R}^{w_{i}}\\ \\longrightarrow\\mathbb{R}^{w_{i}}\\}$ and $\\bar{\\{\\alpha_{j}^{(i)}\\colon\\bar{\\mathbb{R}}^{b_{i}}\\ \\rightarrow\\mathbb{R}^{b_{i}}\\}}$ , each one corresponds to weight and bias of $\\boldsymbol{\\mathcal{U}}$ . The maps $I_{\\Delta>0}:\\mathcal{U}\\to\\mathcal{U}$ that $(\\bar{W},b)\\mapsto(W^{\\prime},b^{\\prime})$ is defined by simply applying these functions on each weight and bias entries as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nW_{j k}^{\\prime(i)}=\\alpha_{j k}^{(i)}(W_{j k}^{(i)})\\;\\;\\mathrm{and}\\;\\;b_{j}^{\\prime(i)}=\\alpha_{j}^{(i)}(b_{j}^{(i)}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$I_{\\Delta>0}$ is $\\Delta_{*}^{>0}$ -invariant by homogeneity of the $\\alpha$ functions. To make it become $\\mathcal{P}_{*}$ -equivariant, some $\\alpha$ functions have to be shared arross any axis that have permutation symmetry. We derive this relation in Appendix B. Some candidates for positively homogeneous of degree zero functions are also presented in Appendix B. They can be fixed or learnable. ", "page_idx": 7}, {"type": "text", "text": "Construct $I_{\\mathcal{P}}$ . To capture $\\mathcal{P}_{*}$ -invariance, we simply take summing or averaging the weight and bias across any axis that have permutation symmetry as in [71]. In concrete, we have $I_{\\mathcal{P}}\\colon\\mathcal{U}\\ {\\overset{=}{\\to}}\\ \\mathbb{R}^{\\dim\\mathcal{U}}$ is computed as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nI_{\\mathcal{P}}(U)=\\left(W_{\\star,:}^{(1)},W_{:\\,,\\star}^{(L)},W_{\\star,\\star}^{(2)},\\dots,W_{\\star,\\star}^{(L-1)};v^{(L)},v_{\\star}^{(1)},\\dots,v_{\\star}^{(L-1)}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here, $\\star$ denotes summation or averaging over the rows or columns of the weight and bias. ", "page_idx": 7}, {"type": "text", "text": "Remark 5.2. In our experiments, we use averaging operator since it is empirically more stable. ", "page_idx": 7}, {"type": "text", "text": "Finally we compose an MLP before $I_{\\mathcal{P}}$ and $I_{\\Delta>0}$ to obtain an $G$ -invariant map. We summarize the above construction as follows. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3. The functional map $I\\colon\\mathcal{U}\\to\\mathbb{R}^{d}$ defined by Eq. (19) is $G$ -invariant. ", "page_idx": 7}, {"type": "text", "text": "5.3 Monomial Matrix Group Equivariant Neural Functionals (Monomial-NFNs) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We build Monomial-NFNs by the constructed equivariant and invariant functional layers, with activations and additional layers discussed below. The equivariant NFN is built by simply stacking $G$ -equivariant layers. For the invariant counterpart, we follow the construction in [71]. In particular, we first stack some $G^{\\prime}$ -equivariant layers, then a $\\Delta_{*}^{>0}$ -invariant and $\\mathcal{P}_{*}$ -equivariant layer. This makes our NFN to be $\\Delta_{*}^{>0}$ -invariant and $\\mathcal{P}_{*}$ -equivariant. Then we finish the construction by stacking a $\\mathcal{P}_{*}$ -invariant layer and the end. This process makes the whole NFN to be $G$ -invariant as expected. ", "page_idx": 7}, {"type": "text", "text": "Activations of $G$ -equivariant functionals. Dealing with equivariance under action of $\\mathcal{P}_{*}$ only requires activation of the NFN is enough, since $\\mathcal{P}_{*}$ acts on only the order of channels in each channel of the weight space. For our $G$ -equivariant NFNs, between each layer that is $\\Delta_{*}^{>0}$ -equivariant, we have to use the same type of activations as the activation in the network input (i.e. either ReLU, sin or tanh in our consideration) to maintain the equivariance of the NFN. ", "page_idx": 7}, {"type": "text", "text": "Fourier Features and Positional Embedding. As mentioned in [35, 71, 72], Fourier Features [30, 62] and (sinusoidal) position embedding play a significant role in the performance of their functionals. Also, in [71], position embedding breaks the symmetry at input and output neurons, and allows us to use equivariant layers that act on input and output neurons. In our $G$ -equivariant layers, we do not consider action on input and output neurons as mentioned. Also, using Fourier Features does not maintain $\\Delta_{*}^{>0}$ , so we can not use this Fourier layer for our equivariant Monomial-NFNs, and in our invariant Monomial-NFNs, we only can use Fourier layer after the $\\Delta_{*}^{>0}$ -invariant layer. This can be considered as a limitation of Monomial-NFNs. ", "page_idx": 7}, {"type": "text", "text": "6 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this session, we empirically demonstrate the performance of our Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFNs) on various tasks that are either invariant (predicting CNN generalization from weights and classifying INR representations of images) or equivariant (weight space style editing). We aim to establish two key points. First, our model exhibits more stable behavior when the input undergoes transformations from the monomial matrix groups. Second, our model, Monomial-NFN, achieves competitive performance compared to other baseline models. Our results are averaged over 5 runs. Hyperparameter settings and the number of parameters can be found in Appendix D. ", "page_idx": 7}, {"type": "image", "img_path": "rQYyWGYuzK/tmp/5d2185ffa7744f7a246a955c8eed51e13a82ae167652b7d1f82fd10aea371891.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/ef1ddc34fc45037bd51172c87b2d83038dc0d692152749bc767ec28cad0b7b9f.jpg", "table_caption": ["Figure 1: CNN prediction on ReLU subset of Small CNN Zoo with different ranges of augmentations. Here the x-axis is the augment upper scale, presented in log scale. The metric used is Kendall\u2019s $\\tau$ . Table 3: Classification train and test accuracies $(\\%)$ for implicit neural representations of MNIST, FashionMNIST, and CIFAR-10. Uncertainties indicate standard error over 5 runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.1 Predicting CNN Generalization from Weights ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experiment Setup. In this experiment, we evaluate how our Monomial-NFN predicts the generalization of pretrained CNN networks. We employ the Small CNN Zoo [64], which consists of multiple network weights trained with different initialization and hyperparameter settings, together with activations Tanh or ReLU. Since Monomial-NFNs depend on activations of network inputs, we divide the Small CNN Zoo into two smaller datasets based on their activations. The ReLU dataset considers the group $\\mathcal{G}_{n}^{>0}$ , while the Tanh dataset considers the group $\\mathcal{G}_{n}^{\\pm1}$ . ", "page_idx": 8}, {"type": "text", "text": "We construct the dataset with additional weights that undergo random hidden vector permutation and scaling based on their monomial matrix group. For the ReLU dataset with the group $\\mathcal{G}_{n}^{>0}$ , we uniformly sample the diagonal indices of $D$ (see Eq. 14) for various ranges: [1, 10], $[1,1\\times$ $10^{2}],\\dotsc,[1,\\dot{1}\\times1\\dot{0}^{6}]$ , while belonging to $\\{-1,1\\}$ in the case of Tanh dataset with the group $\\mathcal{G}_{n}^{\\pm1}$ . For both datasets, we compare our model with STATNN [65], and with two permutation equivariant neural functional networks from [71], referred to as HNP and NP. To compare the performance of all models, we use Kendall\u2019s $\\tau$ rank correlation metric [33]. ", "page_idx": 8}, {"type": "text", "text": "Results. We demonstrate the results of all models on the ReLU subset in Figure 1, showing that our model attains stable Kendall\u2019s $\\tau$ when the scale operators are sampled from different ranges. Specifically, when the log of augmentation upper scale is 0, i.e. the data remains unaltered, our model performs as well as the HNP model. However, as the weights undergo more extensive scaling and permutation, the performance of the HNP and STATNN models drops significantly, indicating their lack of scaling symmetry. The NP model exhibits a similar trend, albeit to a lesser extent. In contrast, our model maintains stable performance throughout. ", "page_idx": 8}, {"type": "text", "text": "Table 2 illustrates the performance of all models on both the original and augmented Tanh subsets of CNN Zoo. Our model achieves the highest performance among all models and shows the greatest improvement after training with the augmented dataset. The gap between our model and the secondbest model (HNP) increases from 0.006 to 0.008. Additionally, in both experiments, our model utilizes significantly fewer parameters than the baseline models, using only up to $50\\%$ of the parameters compared to HNP. ", "page_idx": 8}, {"type": "text", "text": "6.2 Classifying implicit neural representations of images ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experiment Setup. In this experiment, our focus is on extracting the original data information encoded within the weights of implicit neural representations (INRs). We utilize the dataset from [71], which comprises pretrained INR networks [58] that encode images from the CIFAR-10 [36], MNIST [39], and FashionMNIST [69] datasets. Each pretrained INR network is designed to map image coordinates $(x,y)$ to color pixel values - 3-dimensional RGB values for CIFAR-10 and 1-dimensional grayscale values for MNIST and FashionMNIST. ", "page_idx": 8}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/6ee3febdbdc3f25a108bc532d9466336e2a77373a69a3d816b88487fe0db6d52.jpg", "table_caption": ["Table 4: Test mean squared error (lower is better) between weight-space editing methods and ground-truth image-space transformations. Uncertainties indicate standard error over 5 runs. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Results. We compare our model with NP, HNP, and MLP baselines. The results in Table 3 demonstrate that our model outperforms the second-best baseline, NP, for the FashionMNIST and CIFAR10 datasets by $2.94\\bar{\\%}$ and $0.49\\%$ , respectively. For the MNIST dataset, our model also obtains comparable performance. ", "page_idx": 9}, {"type": "text", "text": "6.3 Weight space style editing. ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Experiment setup. In this experiment, we explore altering the weights of the pretrained SIREN model [58] to change the information encoded within the network. We use the network weights provided in the HNP paper for the pretrained SIREN networks on MNIST and CIFAR-10 images. Our focus is on two tasks: the first involves modifying the network to dilate digits from the MNIST dataset, and the second involves altering the SIREN network weights to enhance the contrast of CIFAR-10 images. The objective is to minimize the mean squared error (MSE) training loss between the generated image from the edited SIREN network and the dilated/enhanced contrast image. ", "page_idx": 9}, {"type": "text", "text": "Results. Table 4 shows that our model performs on par with the best-performing model for increasing the contrast of CIFAR-10 images. For the MNIST digit dilation task, our model also achieves competitive performance compared to the NP baseline. Additionally, Figure 2 presents random samples of the digits that each model encodes for the dilation and contrast tasks, demonstrating that our model\u2019s results are visually comparable to those of HNP and NP in both tasks. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we formally describe a group of monomial matrices that preserves FCNNs and CNNs while acting on their weight spaces. For ReLU networks, this group includes permutation and scaling symmetries, while for networks with sin or Tanh activations, it encompasses permutation and sign-flipping symmetries. We introduce Monomial-NFNs, a first-of-a-kind class of NFNs that incorporates these scaling or sign-flipping symmetries in weight spaces. We demonstrate that the low number of trainable parameters in our equivariant linear layer of Monomial-NFNs compared to previous works on NFNs, highlighting their capability to efficiently process weight spaces of deep networks. Our NFNs exhibit competitive generalization performance and efficiency compared to existing models across several benchmarks. ", "page_idx": 9}, {"type": "text", "text": "One limitation of our model is that, due to the large size of the group considered, the resulting linear layers can be limited in terms of expressivity. For example, a weight corresponding to an edge between two neurons will be updated based only on its previous value, ignoring other edges across the same or other layers. To resolve this issue, it is necessary to construct an equivariant nonlinear layer to encode further relations between these weights, thus enhancing the expressivity. Another limitation is that we are uncertain about the maximality of the group $G$ acting on the weight space of the ReLU network. Therefore, other types of symmetries may exist in the weight space beyond neuron permutation and weight scaling, and our model is not equivariant with respect to these symmetries. We leave the problem of identifying such a maximal group for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research / project is supported by the National Research Foundation Singapore under the AI Singapore Programme (AISG Award No: AISG2-TC-2023-012-SGIL). This research / project is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2023) (A-8002040-00-00, A-8002039-00-00). This research / project is also supported by the NUS Presidential Young Professorship Award (A-0009807-01-00). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Francesca Albertini and Eduardo D Sontag. For neural networks, function determines form. Neural networks, 6(7):975\u2013990, 1993.   \n[2] Francesca Albertini and Eduardo D Sontag. Identifiability of discrete-time neural networks. In Proc. European Control Conference, pages 460\u2013465. Springer Berlin, 1993.   \n[3] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242\u2013252. PMLR, 2019.   \n[4] Bruno Andreis, Bedionita Soro, and Sung Ju Hwang. Set-based neural network encoding. CoRR, abs/2305.16625, 2023.   \n[5] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. Advances in neural information processing systems, 29, 2016.   \n[6] Maor Ashkenazi, Zohar Rimon, Ron Vainshtein, Shir Levi, Elad Richardson, Pinchas Mintz, and Eran Treister. Nern: Learning neural representations for neural networks. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[7] Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Understanding symmetries in deep networks. CoRR, abs/1511.01029, 2015.   \n[8] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings. OpenReview.net, 2018.   \n[9] Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Schwarz, and Hyunjik Kim. Spatial functa: Scaling functa to imagenet classification and generation. CoRR, abs/2302.03130, 2023.   \n[10] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machinelearning practice and the classical bias\u2013variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849\u201315854, 2019.   \n[11] Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule. In Optimality in Biological and Artificial Networks?, pages 265\u2013287. Routledge, 2013.   \n[12] Phuong Bui Thi Mai and Christoph Lampert. Functional vs. parametric equivalence of relu networks. In 8th International Conference on Learning Representations, 2020.   \n[13] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 6491\u20136506. Association for Computational Linguistics, 2021.   \n[14] An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. On the geometry of feedforward neural network error surfaces. Neural computation, 5(6):910\u2013927, 1993.   \n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics, 2019.   \n[16] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675\u20131685. PMLR, 2019.   \n[17] David Steven Dummit and Richard M Foote. Abstract algebra, volume 3. Wiley Hoboken, 2004.   \n[18] Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you can treat it like one. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 5694\u20135725. PMLR, 2022.   \n[19] Emilien Dupont, Yee Whye Teh, and Arnaud Doucet. Generative models as distributions of functions. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event, volume 151 of Proceedings of Machine Learning Research, pages 2989\u20133015. PMLR, 2022.   \n[20] Gabriel Eilertsen, Daniel J\u00f6nsson, Timo Ropinski, Jonas Unger, and Anders Ynnerman. Classifying the classifier: Dissecting the weight space of neural networks. In Giuseppe De Giacomo, Alejandro Catal\u00e1, Bistra Dilkina, Michela Milano, Sen\u00e9n Barro, Alberto Bugar\u00edn, and J\u00e9r\u00f4me Lang, editors, ECAI 2020 - 24th European Conference on Artificial Intelligence, 29 August-8 September 2020, Santiago de Compostela, Spain, August 29 - September 8, 2020 - Including 10th Conference on Prestigious Applications of Artificial Intelligence (PAIS 2020), volume 325 of Frontiers in Artificial Intelligence and Applications, pages 1119\u20131126. IOS Press, 2020.   \n[21] Ziya Erko\u00e7, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner, and Angela Dai. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14300\u201314310, 2023.   \n[22] Charles Fefferman and Scott Markel. Recovering a feed-forward net from its output. Advances in neural information processing systems, 6, 1993.   \n[23] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[24] Charles Godfrey, Davis Brown, Tegan Emerson, and Henry Kvinge. On the symmetries of deep learning models and their internal representations. Advances in Neural Information Processing Systems, 35:11893\u201311905, 2022.   \n[25] Elisenda Grigsby, Kathryn Lindsey, and David Rolnick. Hidden symmetries of relu networks. In International Conference on Machine Learning, pages 11734\u201311760. PMLR, 2023.   \n[26] Jean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon. Policy evaluation networks. arXiv preprint arXiv:2002.11833, 2020.   \n[27] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2015.   \n[28] Robert Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In Advanced Neural Computers, pages 129\u2013135. Elsevier, 1990.   \n[29] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735\u2013 1780, 1997.   \n[30] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[31] John M. Jumper, Richard O. Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russell Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, R. D. Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David L. Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, July 2021. https://europepmc.org/article/MED/34265844 ; https://www.nature. com/articles/s41586-021-03819-2 ; https://www.mendeley.com/catalogue/ bde88f33-525c-3af0-823a-3bb305a93020/ ; https://facultyopinions.com/ prime/740477161 ; https://pubmed.ncbi.nlm.nih.gov/34265844/ ; https: //www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605/ ; https://www.nature.com/ articles/s41586-021-03819-2.pdf ; https://europepmc.org/abstract/MED/ 34265844 ; https://econpapers.repec.org/RePEc:nat:nature:v:596:y:2021:i: 7873:d:10.1038_s41586-021-03819-2 ; https://www.scienceopen.com/document? vid=b1f93136-f0f0-4c78-8e45-27bd037e9bb9 ; http://rucweb.tsg211.com/http/ 77726476706e69737468656265737421e7e056d229317c456c0dc7af9758/articles/ s41586-021-03819-2 ; http://pubmed02.keyan123.cn/34265844/. ", "page_idx": 12}, {"type": "text", "text": "[32] Ioannis Kalogeropoulos, Giorgos Bouritsas, and Yannis Panagakis. Scale equivariant graph metanetworks. arXiv preprint arXiv:2406.10685, 2024.   \n[33] M. G. KENDALL. A NEW MEASURE OF RANK CORRELATION. Biometrika, 30(1-2):81\u2013 93, 06 1938.   \n[34] Boris Knyazev, Michal Drozdzal, Graham W Taylor, and Adriana Romero Soriano. Parameter prediction for unseen deep architectures. Advances in Neural Information Processing Systems, 34:29433\u201329448, 2021.   \n[35] Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, and David W. Zhang. Graph neural networks for learning equivariant representations of neural networks. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.   \n[36] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009.   \n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00e9on Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 1106\u20131114, 2012.   \n[38] Vera Kurkova and Paul C Kainen. Functionally equivalent feedforward neural networks. Neural Computation, 6(3):543\u2013558, 1994.   \n[39] Yann LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005.   \n[40] Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, and James Lucas. Graph metanetworks for processing diverse neural architectures. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.   \n[41] Luca De Luigi, Adriano Cardace, Riccardo Spezialetti, Pierluigi Zama Ramirez, Samuele Salti, and Luigi Di Stefano. Deep learning on implicit neural representations of shapes. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[42] Luke Metz, James Harrison, C Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, et al. Velo: Training versatile learned optimizers by scaling up. arXiv preprint arXiv:2211.09760, 2022.   \n[43] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[44] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[45] Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, and Haggai Maron. Equivariant architectures for learning in deep weight spaces. In International Conference on Machine Learning, pages 25790\u201325816. PMLR, 2023.   \n[46] Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. Advances in neural information processing systems, 28, 2015.   \n[47] Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha SohlDickstein. Sensitivity and generalization in neural networks: an empirical study. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.   \n[48] William Peebles, Ilija Radosavovic, Tim Brooks, Alexei A Efros, and Jitendra Malik. Learning to learn with generative models of neural network checkpoints. arXiv preprint arXiv:2209.12892, 2022.   \n[49] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, 2019.   \n[50] Joseph J Rotman. An introduction to the theory of groups, volume 148. Springer Science & Business Media, 2012.   \n[51] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by error propagation. 1986.   \n[52] Thomas Philip Runarsson and Magnus Thor Jonsson. Evolution and design of distributed learning rules. In 2000 IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks. Proceedings of the First IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks (Cat. No. 00, pages 59\u201363. IEEE, 2000.   \n[53] Konstantin Sch\u00fcrholt, Boris Knyazev, Xavier Gir\u00f3-i Nieto, and Damian Borth. Hyperrepresentations as generative models: Sampling unseen neural network weights. Advances in Neural Information Processing Systems, 35:27906\u201327920, 2022.   \n[54] Konstantin Sch\u00fcrholt, Dimche Kostadinov, and Damian Borth. Self-supervised representation learning on neural network weights for model characteristic prediction. Advances in Neural Information Processing Systems, 34:16481\u201316493, 2021.   \n[55] Konstantin Sch\u00fcrholt, Diyar Taskiran, Boris Knyazev, Xavier Gir\u00f3-i Nieto, and Damian Borth. Model zoos: A dataset of diverse populations of neural network models. Advances in Neural Information Processing Systems, 35:38134\u201338148, 2022.   \n[56] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry V. Pyrkin, Sergei Popov, and Artem Babenko. Editable neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[57] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33:7462\u20137473, 2020.   \n[58] Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[59] Samuel Sokota, Hengyuan Hu, David J Wu, J Zico Kolter, Jakob Nicolaus Foerster, and Noam Brown. A fine-tuning approach to belief state modeling. In International Conference on Learning Representations, 2021.   \n[60] Kenneth O Stanley. Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines, 8:131\u2013162, 2007.   \n[61] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 1\u20139. IEEE Computer Society, 2015.   \n[62] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:7537\u20137547, 2020.   \n[63] Viet-Hoang Tran, Thieu N Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, and Tan Minh Nguyen. Equivariant neural functional networks for transformers. arXiv preprint arXiv:2410.04209, 2024.   \n[64] Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya Tolstikhin. Predicting neural network accuracy from weights. arXiv preprint arXiv:2002.11448, 2020.   \n[65] Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya Tolstikhin. Predicting neural network accuracy from weights, 2021.   \n[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008, 2017.   \n[67] Thieu N Vo, Viet-Hoang Tran, Tho Tran Huu, An Nguyen The, Thanh Tran, Minh-Khoi NguyenNhat, Duy-Tung Pham, and Tan Minh Nguyen. Equivariant polynomial functional networks. arXiv preprint arXiv:2410.04213, 2024.   \n[68] Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. Discrete applied mathematics, 69(1-2):33\u201360, 1996.   \n[69] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017.   \n[70] Allan Zhou, Chelsea Finn, and James Harrison. Universal neural functionals. CoRR, abs/2402.05232, 2024.   \n[71] Allan Zhou, Kaien Yang, Kaylee Burns, Adriano Cardace, Yiding Jiang, Samuel Sokota, J Zico Kolter, and Chelsea Finn. Permutation equivariant neural functionals. Advances in Neural Information Processing Systems, 36, 2024.   \n[72] Allan Zhou, Kaien Yang, Yiding Jiang, Kaylee Burns, Winnie Xu, Samuel Sokota, J Zico Kolter, and Chelsea Finn. Neural functional transformers. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Supplement to \u201cMonomial Matrix Group Equivariant Neural Functional Networks\u201d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "page_idx": 15}, {"type": "text", "text": "A Construction of Monomial Matrix Group Equivariant Layers 16 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 ReLU activation . 19   \nA.2 Sin or Tanh activation 20 ", "page_idx": 15}, {"type": "text", "text": "B Construction of Monomial Matrix Group Invariant Layers 21 ", "page_idx": 15}, {"type": "text", "text": "B.1 ReLU activation . 21   \nB.2 Sin or Tanh activation 22 ", "page_idx": 15}, {"type": "text", "text": "C Proofs of Theoretical Results 23 ", "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Proposition 3.4 . . 23   \nC.2 Proof of Proposition 4.4 . . 24 ", "page_idx": 15}, {"type": "text", "text": "D Additional experimental details 27 ", "page_idx": 15}, {"type": "text", "text": "D.1 Runtime and Memory Consumption . . . . 27   \nD.2 Comparison of Monomial-NFNs and GNN-based NFNs 27   \nD.3 Predicting generalization from weights . . . . . 28   \nD.4 Classifying implicit neural representations of images . . . . 29   \nD.5 Weight space style editing . . . . \u00b7\u00b7\u00b7\u00b7 30   \nD.6 Ablation Regarding Design Choices . . . 31 ", "page_idx": 15}, {"type": "text", "text": "A Construction of Monomial Matrix Group Equivariant Layers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this appendix, we present how we constructed Monomial Matrix Group Equivariant Layers. We adopt the idea of notation in [71] to derive the formula of linear functional layers. For two weight spaces $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{U}^{\\prime}$ with the same number of layers $L$ as well as the same number of channels at $i$ -th layer $n_{i}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}=\\mathcal{W}\\times\\mathcal{B}\\mathrm{~where}\\colon}\\\\ &{\\mathcal{W}=\\mathbb{R}^{w_{L}\\times n_{L}\\times n_{L-1}}\\times\\ldots\\times\\mathbb{R}^{w_{2}\\times n_{2}\\times n_{1}}\\times\\mathbb{R}^{w_{1}\\times n_{1}\\times n_{0}},}\\\\ &{\\mathcal{B}=\\mathbb{R}^{b_{L}\\times n_{L}\\times1}\\times\\ldots\\times\\mathbb{R}^{b_{2}\\times n_{2}\\times1}\\times\\mathbb{R}^{b_{1}\\times n_{1}\\times1};}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}^{\\prime}=\\mathcal{W}^{\\prime}\\times\\mathcal{B}^{\\prime}\\mathrm{~where:~}}\\\\ &{\\mathcal{W}^{\\prime}=\\mathbb{R}^{w_{L}^{\\prime}\\times n_{L}\\times n_{L-1}}\\times...\\times\\mathbb{R}^{w_{2}^{\\prime}\\times n_{2}\\times n_{1}}\\times\\mathbb{R}^{w_{1}^{\\prime}\\times n_{1}\\times n_{0}},}\\\\ &{\\mathcal{B}^{\\prime}=\\mathbb{R}^{b_{L}^{\\prime}\\times n_{L}\\times1}\\times...\\times\\mathbb{R}^{b_{2}^{\\prime}\\times n_{2}\\times1}\\times\\mathbb{R}^{b_{1}^{\\prime}\\times n_{1}\\times1};}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "our equivariant layer $E\\colon\\mathcal{U}\\rightarrow\\mathcal{U}^{\\prime}$ will has the form as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{E\\,:(W,b)=U\\longmapsto U^{\\prime}=(W^{\\prime},b^{\\prime})\\ \\mathrm{~where:}}}\\\\ {{}}&{{}}&{{W_{j k}^{\\prime(i)}:=\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\sum_{q=1}^{n_{s-1}}{\\mathfrak{p}}_{s p q}^{i j k}W_{p q}^{(s)}+\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}{\\mathfrak{q}}_{s p}^{i j k}b_{p}^{(s)}+\\mathfrak{t}^{i j k}}}\\\\ {{}}&{{}}&{{b_{j}^{\\prime(i)}:=\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\sum_{q=1}^{n_{s-1}}{\\mathfrak{r}}_{s p q}^{i j}W_{p q}^{(s)}+\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\mathfrak{s}_{s p}^{i j}b_{p}^{(s)}+\\mathfrak{t}^{i j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, the map $E$ is parameterized by hyperparameter $\\theta\\,=\\,({\\mathfrak{p}},{\\mathfrak{q}},{\\mathfrak{s}},{\\mathfrak{r}},{\\mathfrak{t}})$ with dimensions of each component as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\mathfrak{q}_{s p}^{i j k}\\in\\mathbb{R}^{w_{i}^{\\prime}\\times b_{s}}$ represents the contribution of $b_{p}^{(s)}$ to $W_{j k}^{\\prime(i)}$ , \u2022 $\\mathbf{t}^{i j k}\\in\\mathbb{R}^{w_{i}^{\\prime}}$ is the bias of the layer for $W_{j k}^{\\prime(i)}$ ; \u2022 $\\mathfrak{r}_{s p q}^{i j}\\in\\mathbb{R}^{b_{i}^{\\prime}\\times w_{s}}$ represents the contribution of $W_{p q}^{(s)}$ to $b_{j}^{\\prime(i)}$ , $\\mathfrak{s}_{s p}^{i j}\\in\\mathbb{R}^{b_{i}^{\\prime}\\times b_{s}}$ represents the contribution of $b_{p}^{(s)}$ to $b_{j}^{\\prime(i)}$ , \u2022 $\\mathbf{t}^{i j}\\in\\mathbb{R}^{b_{i}^{\\prime}}$ is the bias of the layer for $b_{j}^{\\prime(i)}$ . ", "page_idx": 16}, {"type": "text", "text": "We want to see how an element of the group $\\mathcal{G}_{\\mathcal{U}}$ acts on input and output of layer $E$ . Let ", "page_idx": 16}, {"type": "equation", "text": "$$\ng=\\Big(g^{(L)},\\ldots,g^{(0)}\\Big)\\in\\mathcal{G}_{n_{L}}\\times...\\times\\mathcal{G}_{n_{0}}=\\mathcal{G}_{\\mathcal{U}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{g}^{(i)}=\\boldsymbol{D}^{(i)}\\cdot\\boldsymbol{P}_{\\pi_{i}}=\\operatorname{diag}\\left(\\boldsymbol{d}_{1}^{(i)},\\ldots,\\boldsymbol{d}_{n_{i}}^{(i)}\\right)\\cdot\\boldsymbol{P}_{\\pi_{i}}\\in\\mathcal{G}_{n_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall the definition of the group action $g U=(g W,g b)$ where: ", "page_idx": 16}, {"type": "equation", "text": "$$\n(g W)^{(i)}:=\\left(g^{(i)}\\right)\\cdot W^{(i)}\\cdot\\left(g^{(i-1)}\\right)^{-1}\\;\\mathrm{~and~}\\;(g b)^{(i)}:=\\left(g^{(i)}\\right)\\cdot b^{(i)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "or in term of entries: ", "page_idx": 16}, {"type": "equation", "text": "$$\n(g W)_{j k}^{(i)}:=\\frac{d_{j}^{(i)}}{d_{k}^{(i-1)}}\\cdot W_{\\pi_{i}^{-1}(j)\\pi_{i-1}^{-1}(k)}^{(i)}\\ \\ \\mathrm{and}\\ \\left(g b\\right)_{j}^{(i)}:=d_{j}^{(i)}\\cdot b_{\\pi_{i}^{-1}(j)}^{(i)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$g E(U)=g U^{\\prime}=(g W^{\\prime},g b^{\\prime})$ is computed as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle g W_{j,j}^{\\prime}|\\hat{u}\\rangle=\\frac{d_{j}^{(0)}}{d_{i}^{(0)}}\\cdot W_{x_{i}^{-1}(j)}^{\\prime(i)}v_{i}^{-1}(k)}\\\\ &{\\qquad=\\frac{d_{j}^{(0)}}{d_{i}^{(0)}}\\cdot\\left(\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\sum_{u=j}^{n-1}\\|\\hat{v}_{p,q}^{-1}(k)v_{j}^{(s)}+\\hat{t}(k)\\|_{p,q}^{-1}(k)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\|q_{p}^{-1}(k)v_{i}^{(s)}+\\hat{t}^{(s_{i}^{-1}(j)}v_{i-1}^{-1}(k)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}q_{p}^{(s_{i}^{-1}(j)}v_{i-1}^{-1}(k)\\hat{y}_{p}^{(s)}+\\hat{t}^{(s_{i}^{-1}(j)}v_{i-1}^{-1}(k)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\int d v\\hat{y}^{(s)}}\\\\ &{=d_{j}^{(i)}\\cdot\\left(\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\sum_{u=j}^{n_{s}-1}\\hat{u}_{p,q}^{(s_{i}^{-1}(j)}\\hat{U}_{p,q}^{(s)}+\\hat{t}^{(s_{i}^{-1}(j)}\\hat{U}_{p,q}^{(s_{i}^{-1}(j)}+\\hat{t}^{(s_{i}^{-1}(j)}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\hat{u}_{p,q}^{(s_{i}^{-1}(j)}\\hat{U}_{p,q}^{(s)}+\\hat{t}^{(s_{i}^{-1}(j)}\\hat{U}_{p-1}^{-1}(k)\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$E(g U)=(g U)^{\\prime}=((g W)^{\\prime},(g U)^{\\prime})$ is computed as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(g U)_{j k}^{\\prime(i)}=\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\sum_{q=1}^{n_{s-1}}\\mathfrak{p}_{s p q}^{i j k}\\cdot\\frac{d_{p}^{(s)}}{d_{q}^{(s-1)}}\\cdot W_{\\pi_{s}^{-1}(p)\\pi_{s-1}^{-1}(q)}^{(s)}+\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\mathfrak{q}_{s p}^{i j k}\\cdot d_{p}^{(s)}\\cdot b_{\\pi_{s}^{-1}(p)}^{(s)}+t^{i j k}\\quad\\mathrm{(or~}p=1)}\\\\ &{}&{\\displaystyle=\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\sum_{q=1}^{n_{s-1}}\\mathfrak{p}_{s\\pi_{s}(p)\\pi_{s-1}(q)}^{i j k}\\cdot\\frac{d_{\\pi_{s}^{(s)}}^{(s)}}{d_{\\pi_{s-1}(q)}^{(s-1)}}\\cdot W_{p q}^{(s)}+\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\mathfrak{q}_{s\\pi_{s}(p)}^{i j k}\\cdot d_{\\pi_{s}(p)}^{(s)}\\cdot b_{p}^{(s)}+t^{i j k}\\quad\\mathrm{(or~}p=2,\\pi_{s-1}^{(s)}=0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(g b)_{j}^{\\prime(i)}=\\displaystyle\\sum_{s=1}^{L}\\displaystyle\\sum_{p=1}^{n_{s}}\\sum_{q=1}^{n_{s-1}}\\mathfrak{r}_{s p q}^{i j}\\cdot\\frac{d_{p}^{(s)}}{d_{q}^{(s-1)}}\\cdot W_{\\pi_{s}^{-1}(p)\\pi_{s-1}^{-1}(q)}^{(s)}+\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\mathfrak{s}_{s p}^{i j}\\cdot d_{p}^{(s)}\\cdot b_{\\pi_{s}^{-1}(p)}^{(s)}+\\mathfrak{t}^{i j}\\qquad(2)}\\\\ {=\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\sum_{q=1}^{n_{s-1}}\\mathfrak{r}_{s\\pi_{s}(p)\\pi_{s-1}(q)}^{i j}\\cdot\\frac{d_{\\pi_{s}(p)}^{(s)}}{d_{\\pi_{s-1}(q)}^{(s-1)}}\\cdot W_{p q}^{(s)}+\\displaystyle\\sum_{s=1}^{L}\\sum_{p=1}^{n_{s}}\\mathfrak{s}_{s\\pi_{s}(p)}^{i j}\\cdot d_{\\pi_{s}(p)}^{(s)}\\cdot b_{p}^{(s)}+\\mathfrak{t}^{i j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We need $E$ is $G^{\\prime}$ -equivariant under the action of subgroups of $\\mathcal{G}_{\\mathcal{U}}$ as in Theorem 4.4. From the above computation, if $g{\\bar{E(U)}}=E(g U)$ , the hyperparameter $\\theta=({\\mathfrak{p}},{\\mathfrak{q}},{\\mathfrak{r}},{\\mathfrak{s}},{\\mathfrak{t}})$ have to satisfy the system of constraints as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{d_{j}^{(i)}}{d t_{k}^{(i-1)}}}&{\\cdot\\frac{\\partial_{x y}^{(i-1)}(j)\\pi_{x\\bar{i}-1}^{-1}(k)}{\\partial x_{y}^{(i)}}=\\mathfrak{p}_{s_{x x},(p)_{\\pi-1}(\\bar{y})}^{\\{i\\}}\\cdot\\frac{d_{n_{x}(\\bar{y})}^{\\{i,1\\}}}{d_{n_{x-1}(\\bar{y})}^{\\{i,1\\}}}}\\\\ {\\frac{d_{j}^{(i)}}{d t_{k}^{(i-1)}}}&{\\cdot\\mathfrak{q}_{s y}^{\\prime\\prime}}\\\\ {d_{j}^{(i)}}&{\\cdot\\mathfrak{r}_{s y}^{\\prime\\prime}}\\\\ &{d_{j}^{(i)}:\\cdot\\mathfrak{r}_{s y}^{\\prime\\prime}\\overset{i.j}{\\sim}\\mathfrak{r}_{s}^{\\prime\\prime};\\qquad\\mathfrak{r}_{s y}^{\\prime\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{d_{n}^{(i)}}{d_{n_{x}\\bar{i}}^{(i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all possible tuples $((i,j,k),(s,p,q))$ and all $g\\in G$ . Since the two subgroups $G$ considered in Theorem 4.4 satisfy that: $G\\cap\\mathcal{P}_{i}$ is trivial (for $i=0$ or $i=L$ ) or the whole $\\mathcal{P}_{i}$ (for $0<i<L$ ), so we can simplify the above system of constraints by moving all the permutation $\\pi$ \u2019s to LHS, then replacing $\\pi^{-1}$ by $\\pi$ . The system, denoted as $(^{*})$ , now is written as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d_{j}^{(j)}}{d_{k}^{(i-1)}}\\cdot\\mathfrak{p}_{x,x_{(j)}[\\pi_{-1}(i)]}^{(i)}=\\mathfrak{p}_{x,y_{H}}^{(j)}\\cdot\\frac{d_{j}^{(k)}}{d_{q}^{(j-1)}}}\\\\ &{\\frac{d_{j}^{(j)}}{d_{k}^{(i-1)}}\\cdot\\mathfrak{q}_{x,y_{(j)}[\\pi_{-1}(k)]}^{(i)}=\\mathfrak{q}_{x,p}^{(j)}\\cdot\\mathfrak{q}_{p}^{(k)}}\\\\ &{\\qquad\\qquad+\\mathfrak{q}_{y,x_{(j)}[\\pi_{-1}(i)]}^{(i)}=\\mathfrak{c}_{y,y_{H}}^{(j)}\\cdot\\frac{d_{j}^{(k)}}{d_{q}^{(j-1)}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,}\\\\ &{d_{j}^{(i)}\\cdot\\mathfrak{s}_{x,x_{(j)}[\\pi_{-1}(i)]}^{(i)}=\\mathfrak{c}_{y,y_{H}}^{(j)}\\cdot\\frac{d_{j}^{(k)}}{d_{p}^{(j-1)}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad d_{j}^{(i)}\\cdot\\mathfrak{s}_{x,(j)}^{(i)}=\\mathfrak{s}_{y,p}^{(j)}\\cdot d_{p}^{(j)}}\\\\ &{\\frac{d_{j}^{(i)}}{d_{k}^{(i-1)}}\\cdot\\mathfrak{q}^{(i-1)}\\mathfrak{s}_{x,(-1)}^{(i)}=\\mathfrak{c}^{(j),k}}\\\\ &{\\qquad\\qquad\\qquad\\qquad d_{j}^{(i)}\\cdot\\mathfrak{q}_{x}^{(i-1)}(j)=\\mathfrak{c}^{(j)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We treat each case of activation separately. ", "page_idx": 17}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/74ef0fe3a491cda7f1134fe7fd397705aaa1cc166cf537ec79a9ade135a9f0c5.jpg", "table_caption": ["Table 5: Hyperparameter of Equivariant Layers with ReLU activation. Left presents all possible case of tuple $((\\bar{i},j,\\bar{k}),(s,p,q))$ , and Right presents the parameter at the corresponding position. Here, we have three types of notations: 0 means the parameter equal to 0; equations with $\\pi$ \u2019s in LHS means the equation holds for all possible $\\pi$ ; and a single term with no further information means the term can be arbitrary. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/1f8dd1296bfa53c3611e3adeabb508a96dba5964e245b86220f4df4f48b01120.jpg", "table_caption": ["Table 6: Construction of equivariant functional layer with ReLU activation. Note that all parameters have to satisfy the conditions presented in Table 5. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.1 ReLU activation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall that, in this case: ", "page_idx": 18}, {"type": "equation", "text": "$$\nG:=\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{L}}}\\}\\times{\\mathcal{G}}_{n_{L-1}}^{>0}\\times\\ldots\\times{\\mathcal{G}}_{n_{1}}^{>0}\\times\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{0}}}\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So the system of constraints $(^{*})$ holds for: ", "page_idx": 18}, {"type": "text", "text": "By treat each case of tuples $((i,j,k),(s,p,q))$ , we solve Eq. $^{*1}$ , Eq. $^{*2}$ , Eq. $^{*3}$ , Eq. $^{*4}$ in the system $(^{*})$ for hyperparameter $({\\mathfrak{p}},{\\mathfrak{q}},{\\mathfrak{r}},{\\mathfrak{s}})$ as in Table 5. For $\\mathbf{t}^{i j k}$ and $\\mathbf{t}^{i j}$ , by Eq. $^{*5}$ , Eq. $^{\\ast6}$ , we have $\\mathbf{t}^{i j k}=0$ for all $(\\bar{i},\\bar{j},\\bar{k})$ , $\\mathsf{t}^{i j}=0$ if $i<L$ , and $\\mathrm{t}^{L j}$ is arbitrary for all $1\\leqslant j\\leqslant n_{L}$ . In conclusion, the formula of equivariant layers $E$ in case of activation ReLU is presented as in Table 6. ", "page_idx": 18}, {"type": "text", "text": "Example A.1. Let us consider a two-hidden-layers MLP with activation $\\sigma\\,=\\,\\mathrm{ReLU}$ . Assume that $n_{0}\\,=\\,n_{1}\\,=\\,n_{2}\\,=\\,n_{3}\\,=\\,2$ , i.e., all layers have two neurons. This MLP defines a function $f:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ given by ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\boldsymbol{x})=W^{(3)}\\sigma\\left(W^{(2)}\\sigma\\left(W^{(1)}\\boldsymbol{x}+b^{(1)}\\right)+b^{(2)}\\right)+b^{(3)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where W (i) $W^{(i)}=\\binom{W_{11}^{(i)}}{W_{21}^{(i)}}_{W_{22}^{(i)}}$ is a 2 \u00d7 2 matrix and b(i) $b^{(i)}=\\Biggl[b_{1}^{(i)}\\Biggr]$ for each $i={1,2,3}$ . In this case, the weight space $\\boldsymbol{\\mathcal{U}}$ consists of the tuples ", "page_idx": 19}, {"type": "equation", "text": "$$\nU=(W^{(1)},W^{(2)},W^{(3)},b^{(1)},b^{(2)},b^{(3)})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and it has dimension 18. ", "page_idx": 19}, {"type": "text", "text": "According to Eq. (27), an equivariant layer $E$ over $\\boldsymbol{\\mathcal{U}}$ has the form ", "page_idx": 19}, {"type": "equation", "text": "$$\nE(U)=\\left(W^{\\prime(1)},W^{\\prime(2)},W^{\\prime(3)},b^{\\prime(1)},b^{\\prime(2)},b^{\\prime(3)}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{W_{j k}^{\\prime(1)}=\\mathfrak{p}_{1j_{1}}^{1j k}W_{j_{1}1}^{(1)}+\\mathfrak{p}_{1j_{2}}^{1j k}W_{j_{2}2}^{(1)}+\\mathfrak{q}_{1j}^{1j k}b_{j}^{(1)},\\quad}}&{{b_{j}^{\\prime(1)}=\\mathfrak{r}_{j_{1}}^{1j}W_{j_{1}1}^{(1)}+\\mathfrak{r}_{j_{2}}^{1j}W_{j_{2}2}^{(1)}+\\mathfrak{s}_{1j}^{1j}b_{j}^{(1)},}}\\\\ {{W_{j k}^{\\prime(2)}=\\mathfrak{p}_{2j}^{2j k}W_{j k}^{(2)},\\quad}}&{{b_{j}^{\\prime(2)}=\\mathfrak{s}_{2j}^{2j}b_{j}^{(2)},}}\\\\ {{W_{j k}^{\\prime(3)}=\\mathfrak{p}_{3k_{1}}^{3j k}W_{3k}^{(3)}+\\mathfrak{p}_{3k_{2}}^{3j k}W_{2k}^{(3)},}}&{{b_{j}^{\\prime(3)}=\\mathfrak{s}_{3j_{1}}^{3j}b_{1}^{(3)}+\\mathfrak{s}_{3j_{2}}^{3j}b_{2}^{(3)}+\\mathfrak{r}_{j}^{3j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "These equations can be written in a friendly matrix form as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[W_{11}^{\\prime(1)}\\right]}\\\\ &{\\left[W_{12}^{\\prime(1)}\\right]}\\\\ &{\\left[W_{21}^{\\prime(1)}\\right]=\\left[\\begin{array}{c c c c c c c}{\\mathfrak{p}_{111}^{111}}&{\\mathfrak{p}_{112}^{111}}&{0}&{0}&{\\mathfrak{q}_{111}^{11}}&{0}\\\\ {\\mathfrak{p}_{111}^{112}}&{\\mathfrak{p}_{112}^{112}}&{0}&{0}&{\\mathfrak{q}_{111}^{112}}&{0}\\\\ {0}&{0}&{\\mathfrak{p}_{121}^{121}}&{\\mathfrak{p}_{122}^{121}}&{0}&{\\mathfrak{q}_{112}^{121}}\\\\ {0}&{0}&{\\mathfrak{p}_{121}^{122}}&{\\mathfrak{p}_{122}^{122}}&{0}&{\\mathfrak{q}_{112}^{122}}\\\\ {\\mathfrak{r}_{111}^{111}}&{\\mathfrak{r}_{112}^{111}}&{0}&{0}&{\\mathfrak{s}_{111}^{111}}&{0}\\\\ {0}&{0}&{\\mathfrak{r}_{121}^{121}}&{\\mathfrak{r}_{122}^{122}}&{0}&{\\mathfrak{s}_{112}^{112}}\\end{array}\\right]\\left[\\begin{array}{c}{W_{11}^{(1)}}\\\\ {W_{12}^{(1)}}\\\\ {W_{21}^{(1)}}\\\\ {W_{22}^{(1)}}\\\\ {b_{1}^{(1)}}\\\\ {b_{2}^{(1)}}\\end{array}\\right]}\\end{array},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[W_{11}^{\\prime(2)}\\right]}\\\\ {\\left[W_{12}^{\\prime(2)}\\right]}\\\\ {W_{21}^{\\prime(2)}}\\\\ {W_{22}^{\\prime(2)}}\\\\ {b_{1}^{\\prime(2)}}\\\\ {b_{2}^{\\prime(2)}}\\end{array}\\right]=\\left[\\begin{array}{c c c c c c}{\\mathsf{p}_{211}^{211}}&{0}&{0}&{0}&{0}&{0}\\\\ {0}&{\\mathsf{p}_{212}^{22}}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{\\mathsf{p}_{221}^{221}}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{\\mathsf{p}_{222}^{22}}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{\\mathsf{s}_{211}^{211}}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{\\mathsf{s}_{222}^{22}}\\end{array}\\right]\\left[\\begin{array}{c}{W_{11}^{(2)}}\\\\ {W_{12}^{(2)}}\\\\ {W_{21}^{(2)}}\\\\ {W_{22}^{(2)}}\\\\ {b_{1}^{(2)}}\\\\ {b_{2}^{(2)}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{11}^{\\prime(3)}\\;\\;\\;\\;\\;\\;\\;\\left[W_{11}^{31}\\;\\;\\;\\;\\;\\;0\\;\\;\\;\\;\\;\\;0\\;\\;\\;\\;\\;\\;0\\;\\;\\;\\;\\;\\;0\\;\\;\\;\\;\\;\\;0\\;\\;\\;\\;\\;\\;0\\;\\;\\;\\;\\left[W_{11}^{(3)}\\right]\\;\\;\\;\\;\\;\\;\\;\\;\\left[0\\right]}\\\\ {W_{21}^{\\prime(3)}\\;\\;\\;\\;\\;\\;\\left[\\begin{array}{c c c c c c c}{\\mathfrak{p}_{311}^{311}}&{\\mathfrak{p}_{321}^{312}}&{\\mathfrak{p}_{322}^{322}}&{\\mathfrak{p}_{0}}&{0}&{0}\\\\ {0}&{\\mathfrak{p}_{312}^{321}}&{\\mathfrak{p}_{321}^{321}}&{\\mathfrak{p}_{322}^{322}}&{0}&{0}\\\\ {0}&{\\mathfrak{p}_{312}^{322}}&{0}&{\\mathfrak{p}_{322}^{322}}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{\\mathfrak{p}_{311}^{322}}&{\\mathfrak{p}_{312}^{312}}\\\\ {0}&{0}&{0}&{0}&{\\mathfrak{p}_{321}^{321}}&{\\mathfrak{p}_{322}^{322}}\\end{array}\\right]\\;\\;\\;\\left[W_{21}^{(3)}\\right]\\;\\;+\\;\\;\\left[0\\right]\\;\\;.}\\\\ {b_{2}^{\\prime(3)}\\;\\;\\;\\;\\;\\left[\\begin{array}{c c c c c c}{0}\\\\ {0}\\\\ {0}\\\\ {0}\\\\ {0}\\end{array}\\right]\\;\\;\\;\\;\\;\\left[\\begin{array}{c c c c c c}{0}\\\\ {1}\\\\ {0}\\\\ {0}\\\\ {1}\\\\ {0}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.2 Sin or Tanh activation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall that, in this case: ", "page_idx": 19}, {"type": "equation", "text": "$$\nG:=\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{L}}}\\}\\times{\\mathcal{G}}_{n_{L-1}}^{\\pm1}\\times\\ldots\\times{\\mathcal{G}}_{n_{1}}^{\\pm1}\\times\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{0}}}\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So the system of constraints $(^{*})$ holds for: ", "page_idx": 19}, {"type": "text", "text": "We assume $L\\ \\geqslant\\ 3$ , the case $L\\ \\leqslant\\ 2$ can be solved similarly. By treat each case of tuples $((i,j,k),(s,p,q))$ , we solve Eq. $^{*1}$ , Eq. $^{*2}$ , Eq. $^{*3}$ , Eq. $^{*4}$ in the system $({*})$ for hyperparameter $({\\mathfrak{p}},{\\mathfrak{q}},{\\mathfrak{r}},{\\mathfrak{s}})$ as in Table 7. For $\\mathbf{t}^{i j k}$ and $\\mathbf{t}^{i j}$ , by Eq. $^{*5}$ , Eq. $^{*}6.$ , we have $\\mathsf{t}^{i j k}=0$ for all $(i,j,k)$ , $\\mathrm{t}^{i j}=0$ if $i<L$ , and $\\mathrm{t}^{L j}$ is arbitrary for all $1\\leqslant j\\leqslant n_{L}$ . In conclusion, the formula of equivariant layers $E$ in case of sin or Tanh activation is presented as in Table 8. ", "page_idx": 19}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/552649cc3bb890036d2a85c6d96708dd0f63908c9f3054b6a1b508ecf475fd25.jpg", "table_caption": ["Table 7: Hyperparameter of Equivariant Layers with sin or Tanh activation. Left presents all possible case of tuple $((i,j,k),(s,p,\\bar{q}))$ , and $R i g h t$ presents the parameter at the corresponding position. Here, we have three types of notations: 0 means the parameter equal to 0; equations with $\\pi\\ 's$ in LHS means the equation holds for all possible $\\pi$ ; and a single term with no further information means the term can be arbitrary. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/9369e7e4bf64ddfec399e044a491ea5d931beee2d5ce7dbf3a15e5304fd6bfcd.jpg", "table_caption": ["Table 8: Construction of equivariant functional layer with sin or Tanh activation. Note that all parameters have to satisfy the conditions presented in Table 5. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B Construction of Monomial Matrix Group Invariant Layers ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this appendix, we present how we constructed Monomial Matrix Group Invariant Layers. Let $\\boldsymbol{\\mathcal{U}}$ be a weight spaces with the number of layers $L$ as well as the number of channels at $i$ -th layer $n_{i}$ . We want to construct $G$ -invariant layers $\\dot{I^{\\vdots}}\\ \\mathcal{U}\\rightarrow\\mathbb{R}^{d}$ for some $d>0$ . We treat each case of activations separately. ", "page_idx": 20}, {"type": "text", "text": "B.1 ReLU activation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall that, in this case: ", "page_idx": 20}, {"type": "equation", "text": "$$\nG:=\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{L}}}\\}\\times{\\mathcal{G}}_{n_{L-1}}^{\\pm1}\\times\\ldots\\times{\\mathcal{G}}_{n_{1}}^{\\pm1}\\times\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{0}}}\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathcal{G}_{*}^{>0}$ is the semidirect product of $\\Delta_{*}^{>0}$ and $\\mathcal{P}_{*}$ with $\\Delta_{*}^{>0}$ is the normal subgroup, we will treat these two actions consecutively, $\\Delta_{*}^{>0}$ first then $\\mathcal{P}_{*}$ . We denote these layers by $I_{\\Delta>0}$ and $I_{\\mathcal{P}}$ . Note that, since $I_{\\Delta>0}$ comes before $I_{\\mathcal{P}}$ , $I_{\\Delta>0}$ is required to be $\\Delta_{*}^{>0}$ -invariant and $\\mathcal{P}_{*}$ -equivariant, and $I_{\\mathcal{P}}$ is required to be $\\mathcal{P}_{*}$ -invariant. ", "page_idx": 20}, {"type": "text", "text": "$\\Delta_{*}^{>0}$ -invariance and $\\mathcal{P}_{*}$ -equivariance. To capture $\\Delta_{*}^{>0}$ -invariance, we recall the notion of positively homogeneous of degree zero maps. For $n~>~0$ , a map $\\alpha$ from $\\mathbb{R}^{n}$ is called positively ", "page_idx": 20}, {"type": "text", "text": "Table 9: Constraints of $\\alpha$ component in invariant functional layer with ReLU, sin, Tanh activations. ", "page_idx": 21}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/c3e23ab96ebf3afa23da28ea610bf9e29f695635c42fad38a8230a4f0cec76a6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "homogeneous of degree zero if ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha(\\lambda x_{1},\\ldots,\\lambda x_{n})=\\alpha(x_{1},\\ldots,x_{n}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $\\lambda>0$ and $(x_{1},\\ldots,x_{n})\\in\\mathbb{R}^{n}$ . We construct $I_{\\Delta^{>0}}:\\mathcal{U}\\to\\mathcal{U}$ by taking collections of positively homogeneous of degree zero functions $\\{\\alpha_{j k}^{(i)}\\colon\\mathbb{R}^{w_{i}}\\ \\to\\ \\mathbb{R}^{w_{i}}\\}$ and $\\{\\alpha_{j}^{(i)}\\colon\\mathbb{R}^{b_{i}}\\ \\rightarrow\\ \\mathbb{R}^{b_{i}}\\}$ , each one corresponds to weight and bias of $\\boldsymbol{\\mathcal{U}}$ . The maps $I_{\\Delta^{>0}}:\\mathcal{U}\\rightarrow\\mathcal{U}$ that $(W,b)\\mapsto(W^{\\prime},b^{\\prime})$ is defined by simply applying these functions on each weight and bias entries as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\nW_{j k}^{\\prime(i)}=\\alpha_{j k}^{(i)}(W_{j k}^{(i)})\\;\\;\\mathrm{and}\\;\\;b_{j}^{\\prime(i)}=\\alpha_{j}^{(i)}(b_{j}^{(i)}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$I_{\\Delta^{>0}}$ is $\\Delta_{*}^{>0}$ -invariant by homogeneity of the $\\alpha$ functions. To make it become $\\mathcal{P}_{*}$ -equivariant, some $\\alpha$ functions have to be shared arross any axis that have permutation symmetry, presented in Table 9. ", "page_idx": 21}, {"type": "text", "text": "Candidates of function $\\alpha$ . We simply choose positively homogeneous of degree zero function $\\alpha:\\,\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ by taking $\\alpha(0)=0$ and: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha(x_{1},\\ldots,x_{n})=\\beta\\left(\\frac{x_{1}^{2}}{x_{1}^{2}+\\ldots+x_{n}^{2}},\\ldots,\\frac{x_{n}^{2}}{x_{1}^{2}+\\ldots+x_{n}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\beta\\colon\\ensuremath{\\mathbb{R}}^{n}\\to\\ensuremath{\\mathbb{R}}^{n}$ is an arbitrary function. The function $\\beta$ can be fixed or parameterized to make $\\alpha$ to be fixed or learnable. ", "page_idx": 21}, {"type": "text", "text": "$\\mathcal{P}_{*}$ -invariance. To capture $\\mathcal{P}_{*}$ -invariance, we simply take summing or averaging the weight and bias across any axis that have permutation symmetry as in [71]. In concrete, some $d>0$ , we have $I_{\\mathcal{P}}\\colon\\mathcal{U}\\to\\mathbb{R}^{d}$ is computed as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\nI_{\\mathcal{P}}(U)=\\left(W_{\\star,:}^{(1)},W_{:\\,,\\star}^{(L)},W_{\\star,\\star}^{(2)},\\dots,W_{\\star,\\star}^{(L-1)};v^{(L)},v_{\\star}^{(1)},\\dots,v_{\\star}^{(L-1)}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, $\\star$ denotes summation or averaging over the rows or columns of the weight and bias. ", "page_idx": 21}, {"type": "text", "text": "$G$ \u2212invariance. Now we simply compose $I_{\\mathcal{P}}\\circ I_{\\Delta>0}$ to get an $G$ -invariant map. We use an MLP to complete constructing an $G$ -invariant layer with output dimension $d$ as desired: ", "page_idx": 21}, {"type": "equation", "text": "$$\nI={\\bf M L P}\\circ I_{\\mathcal{P}}\\circ I_{\\Delta^{>0}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.2 Sin or Tanh activation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall that, in this case: ", "page_idx": 21}, {"type": "equation", "text": "$$\nG:=\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{L}}}\\}\\times{\\mathcal{G}}_{n_{L-1}}^{\\pm1}\\times\\ldots\\times{\\mathcal{G}}_{n_{1}}^{\\pm1}\\times\\{\\mathrm{id}_{{\\mathcal{G}}_{n_{0}}}\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\mathcal{G}_{*}^{\\pm1}$ is the semidirect product of $\\Delta_{*}^{\\pm1}$ and $\\mathcal{P}_{*}$ with $\\Delta_{*}^{\\pm1}$ is the normal subgroup, we will treat these two actions consecutively, $\\Delta_{*}^{\\pm1}$ first then $\\mathcal{P}_{*}$ . We denote these layers by $I_{\\Delta^{\\pm1}}$ and $I_{\\mathcal{P}}$ . Note that, since $I_{\\Delta^{\\pm1}}$ comes before $I_{\\mathcal{P}},I_{\\Delta^{\\pm1}}$ is required to be $\\Delta_{*}^{\\pm1}$ -invariant and $\\mathcal{P}_{*}$ -equivariant, and $I_{\\mathcal{P}}$ is required to be $\\mathcal{P}_{*}$ -invariant. ", "page_idx": 21}, {"type": "text", "text": "$\\Delta_{*}^{\\pm1}$ -invariance and $\\mathcal{P}_{*}$ -equivariance. To capture $\\Delta_{*}^{\\pm1}$ -invariance, we use even functions, i.e. $\\alpha(x)\\,=\\,\\alpha(-x)$ for all $x$ . We construct $I_{\\Delta^{\\pm1}}\\colon\\mathcal{U}\\ \\to\\mathcal{U}$ by taking collections of even functions $\\{\\alpha_{j k}^{(i)}\\colon\\mathbb{R}^{w_{i}}\\ \\longrightarrow\\mathbb{R}^{w_{i}}\\}$ and $\\{\\alpha_{j}^{(i)}\\colon\\mathbb{R}^{b_{i}}\\ \\rightarrow\\mathbb{R}^{b_{i}}\\}$ , each one corresponds to weight and bias of $\\boldsymbol{\\mathcal{U}}$ . The maps $I_{\\Delta^{\\pm1}}:\\mathcal{U}\\to\\mathcal{U}$ that $(W,b)\\mapsto(W^{\\prime},b^{\\prime})$ is defined by simply applying these functions on each weight and bias entries as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nW_{j k}^{\\prime(i)}=\\alpha_{j k}^{(i)}(W_{j k}^{(i)})\\;\\;\\mathrm{and}\\;\\;b_{j}^{\\prime(i)}=\\alpha_{j}^{(i)}(b_{j}^{(i)}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$I_{\\Delta^{\\pm1}}$ is $\\Delta_{*}^{\\pm1}$ -invariant by design. To make it become $\\mathcal{P}_{*}$ -equivariant, some $\\alpha$ functions have to be shared arross any axis that have permutation symmetry, presented in Table 9. ", "page_idx": 22}, {"type": "text", "text": "Candidates of function $\\alpha$ . We simply choose even function $\\alpha:\\;\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ by: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha(x_{1},\\ldots,x_{n})=\\beta\\left(|x_{1}|,\\ldots,|x_{n}|\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\beta\\colon\\ensuremath{\\mathbb{R}}^{n}\\to\\ensuremath{\\mathbb{R}}^{n}$ is an arbitrary function. The function $\\beta$ can be fixed or parameterized to make $\\alpha$ to be fixed or learnable. ", "page_idx": 22}, {"type": "text", "text": "$\\mathcal{P}_{*}$ -invariance. To capture $\\mathcal{P}_{*}$ -invariance, we simply take summing or averaging the weight and bias across any axis that have permutation symmetry as in [71]. In concrete, some $d>0$ , we have $I_{\\mathcal{P}}\\colon\\mathcal{U}\\to\\mathbb{R}^{d}$ is computed as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nI_{\\mathcal{P}}(U)=\\left(W_{\\star,:}^{(1)},W_{:\\,,\\star}^{(L)},W_{\\star,\\star}^{(2)},\\dots,W_{\\star,\\star}^{(L-1)};v^{(L)},v_{\\star}^{(1)},\\dots,v_{\\star}^{(L-1)}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, $\\star$ denotes summation or averaging over the rows or columns of the weight and bias. ", "page_idx": 22}, {"type": "text", "text": "$G$ \u2212invariance. Now we simply compose $I_{\\mathcal{P}}\\circ I_{\\Delta^{\\pm1}}$ to get an $G$ -invariant map. We use an MLP to complete constructing an $G$ -invariant layer with output dimension $d$ as desired: ", "page_idx": 22}, {"type": "equation", "text": "$$\nI={\\bf M L P}\\circ I_{\\mathcal{P}}\\circ I_{\\Delta^{\\pm1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C Proofs of Theoretical Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Proof of Proposition 3.4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. We simply denote the activation ReLU or sin or tanh by $\\sigma$ . Let $A\\in\\operatorname{GL}(n)$ that satisfies: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma(A\\cdot\\mathbf{x})=A\\cdot\\sigma(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $\\mathbf{x}\\in\\mathbb{R}^{n}$ . This means: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma\\left(\\left[{\\begin{array}{r r r}{a_{11}}&{\\cdot\\cdot}&{a_{1n}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {a_{n1}}&{\\cdot\\cdot}&{a_{n n}}\\end{array}}\\right]\\cdot\\left[{\\begin{array}{r}{x_{1}}\\\\ {\\vdots}\\\\ {x_{n}}\\end{array}}\\right]\\right)=\\left[{\\begin{array}{r r r}{a_{11}}&{\\cdot\\cdot}&{a_{1n}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {a_{n1}}&{\\cdot\\cdot}&{a_{n n}}\\end{array}}\\right]\\cdot\\sigma\\left(\\left[{\\begin{array}{r}{x_{1}}\\\\ {\\vdots}\\\\ {x_{n}}\\end{array}}\\right]\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $x_{1},\\ldots,x_{n}\\in\\mathbb{R}$ . We rewrite this equation as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma\\left(\\left[\\begin{array}{c}{\\phantom{-}\\!\\!a_{11}x_{1}+a_{12}x_{2}+\\ldots+a_{1n}x_{n}}\\\\ {\\vdots}\\\\ {\\phantom{-}\\!\\!a_{n1}x_{1}+a_{n2}x_{2}+\\ldots+a_{n n}x_{n}\\right]\\rangle}\\end{array}\\right)=\\left[\\begin{array}{c c c}{\\phantom{-}\\!\\!a_{11}}&{\\cdot\\cdot\\cdot}&{\\phantom{-}\\!\\!a_{1n}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\phantom{-}\\!\\!a_{n1}}&{\\cdot\\cdot\\cdot}&{\\phantom{-}\\!\\!a_{n n}}\\end{array}\\right]\\cdot\\left[\\begin{array}{c}{\\sigma(x_{1})}\\\\ {\\vdots}\\\\ {\\sigma(x_{n})}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "or equivalently: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c c}{\\sigma(a_{11}x_{1}+a_{12}x_{2}+...+a_{1n}x_{n})}\\\\ {\\vdots}\\\\ {\\sigma(a_{n1}x_{1}+a_{n2}x_{2}+...+a_{n n}x_{n})}\\end{array}\\right]=\\left[\\begin{array}{c c c}{a_{11}\\sigma(x_{1})+a_{12}\\sigma(x_{2})+...+a_{1n}\\sigma(x_{n})}\\\\ {\\vdots}\\\\ {a_{n1}\\sigma(x_{1})+a_{n2}\\sigma(x_{2})+...+a_{n n}\\sigma(x_{n})}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma\\left(\\sum_{j=1}^{n}a_{i j}x_{j}\\right)=\\sum_{j=1}^{n}a_{i j}\\sigma(x_{j}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $x_{1},\\ldots,x_{n}\\in\\mathbb{R}$ and $i=1,\\hdots,n$ . We will consider the case $i=1$ , i.e. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma\\left(\\sum_{j=1}^{n}a_{1j}x_{j}\\right)=\\sum_{j=1}^{n}a_{1j}\\sigma(x_{j}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and treat the case $i>1$ similarly. Now we consider the activation $\\sigma$ case by case as follows. ", "page_idx": 22}, {"type": "text", "text": "(i) Case 1. $\\sigma=\\mathrm{ReLU}$ . We have some observations: ", "page_idx": 23}, {"type": "text", "text": "1. Let $x_{1}=1$ , and $x_{2}=.\\,.\\,.=x_{n}=0$ . Then from Eq. (60), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma(a_{11})=a_{11},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies that $a_{11}\\geqslant0$ . Similarly, we also have $a_{12},\\dots,a_{1n}\\geqslant0$ . ", "page_idx": 23}, {"type": "text", "text": "2. Since $A$ is an invertible matrix, the entries $a_{11},\\ldots,a_{1n}$ in the first row of $A$ can not be simultaneously equal to 0. ", "page_idx": 23}, {"type": "text", "text": "3. There is at most only one nonzero number among the entries $a_{11},\\ldots,a_{1n}$ . Indeed, assume by the contrary that $a_{11},a_{12}>0$ . Let $x_{3}=...=x_{n}=0$ , from Eq. (60), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma(a_{11}x_{1}+a_{12}x_{2})=a_{11}\\sigma(x_{1})+a_{12}\\sigma(x_{2}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $x_{2}=-1$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma(a_{11}x_{1}-a_{12})=a_{11}\\sigma(x_{1}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, let $x_{1}>0$ be a sufficiently large number such that $a_{11}x_{1}-a_{12}>0$ . (Note that this number exists since $a_{11}>0$ ). Then we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\na_{11}x_{1}-a_{12}=a_{11}x_{1},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies $a_{12}=0$ , a contradiction. ", "page_idx": 23}, {"type": "text", "text": "It follows from these three observations that there is exactly one non-zero element among the entries $a_{11},\\ldots,a_{1n}$ . In other words, matrix $A$ has exactly one nonzero entry in the first row. This applies for every row, so $A$ has exactly one non-zero entry in each row. Since $A$ is invertible, each column of $A$ has at least one non-zero entry. Thus $A$ also has exactly one non-zero entry in each column. Hence, $A$ is in $\\mathcal{G}_{n}$ . Moreover, all entries of $A$ are non-negative, so $A$ is in $\\mathcal{G}_{n}^{>0}$ . ", "page_idx": 23}, {"type": "text", "text": "It is straight forward to check that for all $A$ in $\\mathcal{G}_{n}^{>0}$ we have $\\sigma(A\\cdot\\mathbf{x})=A\\cdot\\sigma(\\mathbf{x})$ . ", "page_idx": 23}, {"type": "text", "text": "(ii) Case 2. $\\sigma=\\mathrm{Tanh}$ or $\\sigma=\\sin$ . We have some observations: ", "page_idx": 23}, {"type": "text", "text": "1. Let $x_{2}=...=x_{n}=0$ . Then from Eq. (60), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma(a_{11}x_{1})=a_{11}\\sigma(x_{1}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies $a_{11}\\in\\{-1,0,1\\}$ . Similarly, we have $a_{12},...\\,,a_{1n}\\in\\{-1,0,1\\}$ . ", "page_idx": 23}, {"type": "text", "text": "2. Since $A$ is an invertible matrix, the entries $a_{11},\\ldots,a_{1n}$ in the first row of $A$ can not be simultaneously equal to 0. ", "page_idx": 23}, {"type": "text", "text": "3. There is at most only one nonzero number among the entries $a_{11},\\ldots,a_{1n}$ . Indeed, assume by the contrary that $a_{11},a_{12}\\neq0$ . Let $x_{3}=...=x_{n}=0$ , from Eq. (60), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma(a_{11}x_{1}+a_{12}x_{2})=a_{11}\\sigma(x_{1})+a_{12}\\sigma(x_{2}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $a_{11},a_{12}\\in\\{-1,1\\}$ , so by consider all the cases, we will lead to a contradiction. ", "page_idx": 23}, {"type": "text", "text": "It follows from the above three observations that there is exactly one non-zero element among the entries $a_{11},\\ldots,a_{1n}$ . In other words, matrix $A$ has exactly one nonzero entry in the first row. This applies for every row, so $A$ has exactly one non-zero entry in each row. Note that, since $A$ is invertible, each column of $A$ has at least one non-zero entry. Therefore, $A$ also has exactly one non-zero entry in each column. Hence, $A$ is in $\\mathcal{G}_{n}$ . Moreover, all entries of $A$ are in $\\{-1,0,1\\}$ , so $A$ is in $\\mathcal{G}_{n}^{\\pm1}$ . ", "page_idx": 23}, {"type": "text", "text": "It is straight forward to check that for all $A$ in $\\mathcal{G}_{n}^{\\pm1}$ we have $\\sigma(A\\cdot\\mathbf{x})=A\\cdot\\sigma(\\mathbf{x})$ . ", "page_idx": 23}, {"type": "text", "text": "The proposition is then proved completely. ", "page_idx": 23}, {"type": "text", "text": "C.2 Proof of Proposition 4.4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. For both Fully Connected Neural Networks case and Convolutional Neural Networks case, we consider a network $f$ with three layers, with $n_{0},n_{1},n_{2},n_{3}$ are number of channels at each layer, and its weight space $\\boldsymbol{\\mathcal{U}}$ . We will show the proof for part $(i)$ where activation $\\sigma$ is ReLU, and part $(i i)$ can be proved similarly. For part $(i)$ , we prove $f$ to be $G$ -invariant on its weight space $\\boldsymbol{\\mathcal{U}}$ , for the group $G$ that is defined by: ", "page_idx": 23}, {"type": "equation", "text": "$$\nG=\\{\\mathrm{id}_{\\mathcal{G}_{n_{3}}}\\}\\times\\mathcal{G}_{n_{2}}^{>0}\\times\\mathcal{G}_{n_{1}}^{>0}\\times\\{\\mathrm{id}_{\\mathcal{G}_{n_{0}}}\\}<\\mathcal{G}_{n_{3}}\\times\\mathcal{G}_{n_{2}}\\times\\mathcal{G}_{n_{1}}\\times\\mathcal{G}_{n_{0}}=\\mathcal{G}_{u};\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Case 1. $f$ is a Fully Connected Neural Network with three layers, with $n_{0},n_{1},n_{2},n_{3}$ are number of channels at each layer as in Eq. 5: ", "page_idx": 24}, {"type": "equation", "text": "$$\nf({\\bf x}\\,;\\,U,\\sigma)=W^{(3)}\\cdot\\sigma\\left(W^{(2)}\\cdot\\sigma\\left(W^{(1)}\\cdot{\\bf x}+b^{(1)}\\right)+b^{(2)}\\right)+b^{(3)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Case 2. $f$ is a Convolutional Neural Network with three layers, with $n_{0},n_{1},n_{2},n_{3}$ are number of channels at each layer as in Eq. 8: ", "page_idx": 24}, {"type": "equation", "text": "$$\nf({\\bf x}\\,;\\,U,\\sigma)=W^{(3)}*\\sigma\\left(W^{(2)}*\\sigma\\left(W^{(1)}*{\\bf x}+b^{(1)}\\right)+b^{(2)}\\right)+b^{(3)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have some observations: ", "page_idx": 24}, {"type": "text", "text": "For case 1. For $W\\in\\mathbb{R}^{m\\times n},\\mathbf{x}\\in\\mathbb{R}^{n}$ and $a>0$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\na\\cdot\\sigma(W\\cdot{\\mathbf x}+b)=\\sigma\\left((a W)\\cdot{\\mathbf x}+(a b)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For case 2. For simplicity, we consider $^*$ as one-dimentional convolutional operator, and other types of convolutions can be treated similarly. For $W\\,=\\,\\left(w_{1},\\ldots,w_{m}\\right)\\,\\in\\,\\mathbb{R}^{m},b\\,\\in\\,\\mathbb{R}$ and $\\textbf{x}=$ $({\\dot{x_{1}}},\\dots,x_{n})\\in\\mathbb{R}^{n}$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\nW*{\\bf x}+b={\\bf y}=(y_{1},\\ldots,y_{n-m+1})\\in\\mathbb{R}^{n-m+1},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where: ", "page_idx": 24}, {"type": "equation", "text": "$$\ny_{i}=\\sum_{j=1}^{m}w_{j}x_{i+j-1}+b.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So for $a>0$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\na\\cdot\\sigma(W*{\\mathbf x}+b)=\\sigma\\left((a W)*{\\mathbf x}+(a b)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With these two observations, we can see the proofs for both cases are similar to each other. We will show the proof for case 2, when $f$ is a convolutional neural network since it is not trivial as case 1. Now we have $U=(W,b)$ with: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{W=\\left(W^{(3)},W^{(2)},W^{(1)}\\right),}}\\\\ {{\\,\\,\\,\\,b=\\left(b^{(3)},b^{(2)},b^{(1)}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $g$ be an element of $G$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\ng=\\Bigl(\\mathrm{id}_{\\mathcal{G}_{n_{3}}},g^{(2)},g^{(1)},\\mathrm{id}_{\\mathcal{G}_{n_{0}}}\\Bigr),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{g^{(2)}=D^{(2)}\\cdot P_{\\pi_{2}}=\\mathrm{diag}\\left(d_{1}^{(2)},\\ldots,d_{n_{2}}^{(2)}\\right)\\cdot P_{\\pi_{2}}\\in\\mathscr{G}_{n_{2}}^{>0},}\\\\ &{}&{g^{(1)}=D^{(1)}\\cdot P_{\\pi_{1}}=\\mathrm{diag}\\left(d_{1}^{(1)},\\ldots,d_{n_{1}}^{(1)}\\right)\\cdot P_{\\pi_{1}}\\in\\mathscr{G}_{n_{1}}^{>0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We compute $g U$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{g U=(g W,g b),}}\\\\ {{g W=\\Big((g W)^{(3)},(g W)^{(2)},(g W)^{(1)}\\Big),}}\\\\ {{g b=\\Big((g b)^{(3)},(g b)^{(2)},(g b)^{(1)}\\Big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(g W)_{j k}^{(3)}=\\displaystyle\\frac{1}{d_{k}^{(2)}}\\cdot W_{j\\pi_{2}^{-1}(k)}^{(3)},}}\\\\ {{\\displaystyle(g W)_{j k}^{(2)}=\\frac{d_{j}^{(2)}}{d_{k}^{(1)}}\\cdot W_{\\pi_{2}^{-1}(j)\\pi_{1}^{-1}(k)}^{(2)},}}\\\\ {{\\displaystyle(g W)_{j k}^{(1)}=\\frac{d_{j}^{(1)}}{1}\\cdot W_{\\pi_{1}^{-1}(j)k}^{(1)},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(g b)_{j}^{(3)}=b_{j}^{(3)},}\\\\ &{(g b)_{j}^{(2)}=d_{j}^{(2)}\\cdot b_{\\pi_{2}^{-1}(j)}^{(2)},}\\\\ &{(g b)_{j}^{(1)}=d_{j}^{(1)}\\cdot b_{\\pi_{1}^{-1}(j)}^{(1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we show that $f(\\mathbf{x}\\ ;\\ U,\\sigma)=f(\\mathbf{x}\\ ;\\ g U,\\sigma)$ for all $\\mathbf{x}=\\left(x_{1},\\ldots,x_{n_{0}}\\right)\\in\\mathbb{R}^{n_{0}}$ . For $1\\leqslant i\\leqslant n_{3}$ , we compute the $i$ -th entry of $f(\\mathbf{x}\\;;\\;g U,\\sigma)$ as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{k_{\\mathrm{s}}+k_{\\parallel}\\cdots}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle}&{=\\displaystyle\\sum_{j=1}^{m}\\frac{1}{\\mu_{j}}\\prod_{s=1}^{m/2}\\mu_{j-1/s}^{(1)}\\left(\\mu_{j}^{(1)}-\\left(\\displaystyle\\sum_{s=1}^{m/2}\\mu_{j}^{(1)}\\sigma_{j+1/s}^{(2)}(\\mu_{j}^{(1)})+\\mu_{j-1}^{(1)}\\mu_{j}^{(1)}\\right)\\right)+\\mu_{j}^{(1)}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\quad\\quad\\quad\\quad\\left(\\displaystyle\\sum_{s=1}^{m}\\mu_{j}^{(1)}(\\mu_{j+1/s}^{(2)}+\\mu_{j}^{(1)}\\sigma_{j+1/s}^{(2)}(\\mu_{j}^{(1)})\\right)+\\mu_{j}^{(1)}\\sigma_{j+1/s}^{(2)}(\\mu_{j}^{(1)})\\right)}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}\\frac{1}{\\mu_{j}}\\prod_{s=1}^{m}\\mu_{j-1/s}^{(2)}\\left(\\mu_{j}^{(1)}-\\mu_{j}^{(1)}\\sigma_{j-1/s}^{(2)}(\\mu_{j+1/s}^{(2)})\\right)+\\mu_{j}^{(1)}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\quad\\quad\\quad\\left(\\displaystyle\\sum_{s=1}^{m}\\mu_{j-1}^{(1)}(\\mu_{j+1/s}^{(2)}+\\mu_{j-1}^{(1)}\\sigma_{j+1/s}^{(1)})\\right)+\\mu_{j}^{(1)}}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}\\mu_{j-1/s}^{(1)}(\\mu_{j+1/s}^{(2)}+\\mu_{j}^{(1)})\\left(\\frac{\\mu_{j}^{(1)}}{\\mu_{j+1/s}^{(2)}}\\right)+\\mu_{j}^{(1)}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\quad\\quad\\quad\\quad\\quad\\left(\\displaystyle\\sum_{s=1}^{m}\\mu_{j}^{(1)}(\\mu_{j+1/s}^{(1)}+\\mu_{j}^{(1)})\\sigma_{j-1/s}^{(1)}\\right)+\\mu_{j}^{(1)}}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}\\mu_{j}^{(\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "End of proof. ", "page_idx": 26}, {"type": "text", "text": "D Additional experimental details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Runtime and Memory Consumption ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide the runtime and memory consumption of Monomial-NFNs and the previous NFNs in Tables 10 and 11 to compare the computational and memory costs in the task of predicting CNN generalization (see Section 6.1). It is observable that our model runs faster and consumes significantly less memory than NP/HNP in [71] and GNN-based method in [35]. This highlights the benefits of parameter savings in Monomial-NFN. ", "page_idx": 26}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/c5addba3fc5667025d01b5f3fc64b737ff7ba01858ee9177353c47896fda35f8.jpg", "table_caption": ["Table 10: Runtime of models. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.2 Comparison of Monomial-NFNs and GNN-based NFNs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide experimental result to compare the efficiency of our model and a permutation equivariant GNN-based NFN [35] in two scenarios below. ", "page_idx": 26}, {"type": "text", "text": ". Training the model on augmented train data and testing with the augmented test data (see Tables 12 and 13). Here, we present the experimental results on the original dataset and the results on the augmented dataset. The augmentation levels for the ReLU subset are 1, 2, 3, and 4, corresponding to augmentation ranges of [1, 10], $\\lbrack1,10^{2}]$ , [1, $10^{3}]$ , $\\lbrack1,10^{4}]$ . The augmented dataset for the Tanh subset corresponds to the augmentation range of $[-1,1]$ ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/deda96dfa85e104f654a18a39f5f00f7662c3765a37bb917db18c229e9c4e371.jpg", "table_caption": ["Table 12: Predict CNN generalization on ReLU subset (augmented train data) "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 13: Predict CNN generalization on Tanh subset (augmented train data) Original Augmented GNN [35] 0.893 0.902 Monomial-NFN (ours) 0.939 0.943 ", "page_idx": 27}, {"type": "text", "text": "The results for GNN exhibit a similar trend as other baselines that do not incorporate the scaling symmetry into their architectures. In contrast, our model has stable performance. A notable observation is that the GNN model uses 5.5M parameters (4 times more than our model), occupies 6000MB of memory, and takes 4 hours to train. ", "page_idx": 27}, {"type": "text", "text": "2. Training the model on original train data and testing with the augmented test data (see Tables 14 and 15). ", "page_idx": 27}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/a823f78eaf8cb1e04e47aebf72ad808a719faae75c9a7a0242e8f8ecce478d28.jpg", "table_caption": ["Table 14: Predict CNN generalization on ReLU subset (original train data) "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/ca6415d07501ec832db480ed7f4f337cdef43745122890f121d314ae8807c751.jpg", "table_caption": ["Table 15: Predict CNN generalization on Tanh subset (original train data) "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "In these more challenging scenario, GNN\u2019s performance drops significantly, which highlights the lack of scaling symmetry in the model. Our model maintains consistent performance, matching the case in which we train with the augmented data. ", "page_idx": 27}, {"type": "text", "text": "D.3 Predicting generalization from weights ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Dataset. The original ReLU subset of the CNN Zoo dataset includes 6050 instances for training and 1513 instances for testing. For the Tanh dataset, it includes 5949 training and 1488 testing instances. For the augmented data, we set the augmentation factor to 2, which means that we augment the original data once, resulting in a new dataset of double the size. The complete size of all datasets is presented in Table 16 ", "page_idx": 27}, {"type": "text", "text": "Implementation details. Our model follows the same architecture as in [71], comprising three equivariant Monomial-NFN layers with 16, 16, and 5 channels, respectively, each followed by ReLU activation (ReLU dataset) or Tanh activation (Tanh dataset). The resulting weight space features are input into an invariant Monomial-NFN layer with Monomial-NFN pooling (Equation 19) with learnable parameters (ReLU case) or mean pooling (Tanh case). Specifically, the Monomial-NFN pooling layer normalizes the weights across the hidden dimension and takes the average for rows (first layer), columns (last layer), or both (other layers). The output of this invariant Monomial-NFN layer is flattened and projected to $\\mathbb{R}^{200}$ (ReLU case) or $\\mathbb{R}^{1000}$ (Tanh case). This resulting vector is then passed through an MLP with two hidden layers with ReLU activations. The output is linearly projected to a scalar and then passed through a sigmoid function. We use the Binary Cross Entropy (BCE) loss function and train the model for 50 epochs, with early stopping based on $\\tau$ on the validation set, which takes 35 minutes to train on an A100 GPU. The hyperparameters for our model are presented in Table 18. ", "page_idx": 27}, {"type": "text", "text": "Table 16: Datasets information for predicting generalization task. ", "page_idx": 28}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/7b740843381096f2cb726f872969b8311252ac715106b275f7847758cdc5e5c1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/66d148e94cb7c12f139d038d6535f21386b4535be2d20cd62656645bfd4a6579.jpg", "table_caption": ["Table 17: Number of parameters of all models for prediciting generalization task. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/b4e4706252f477978a926ec417912ccecd1be38b6bd22577c2af56789cc3fe00.jpg", "table_caption": ["Table 18: Hyperparameters for Monomial-NFN on prediciting generalization task. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "For the baseline models, we follow the original implementations described in [71], using the official code (available at: https://github.com/AllanYangZhou/nfn). For the HNP and NP models, there are 3 equivariant layers with 16, 16, and 5 channels, respectively. The features go through an average pooling layer and 3 MLP layers with 1000 hidden neurons. The hyperparameters of our model and the number of parameters for all models in this task can be found in Table 17. ", "page_idx": 28}, {"type": "text", "text": "D.4 Classifying implicit neural representations of images ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Dataset. We utilize the original INRs dataset provided by [71], with no augmentation. The data is obtained by implementing a single SIREN model for each image in each dataset: CIFAR-10, MNIST, and Fashion-MNIST. The size of training, validation, and test samples for each dataset is provided in Table 19. ", "page_idx": 28}, {"type": "text", "text": "Implementation details. In these experiments, our general architecture includes 2 MonomialNFN layers with sine activation, followed by 1 Monomial-NFN layer with absolute activation. The choice of hidden dimension in the Monomial-NFN layer depends on each dataset and is described in Table 20. The architecture then follows the same design as the NP and HNP models in [71], where a Gaussian Fourier Transformation is applied to encode the input with sine and cosine components, mapping from 1 dimension to 256 dimensions. If the base layer is NP, the features will go through IOSinusoidalEncoding, a positional encoding designed for the NP layer, with a maximum frequency of 10 and 6 frequency bands. After that, the features go through 3 HNP or NP layers with ReLU activation functions. Then, an average pooling is applied, and the output is flattened, and the resulting vector is passed through an MLP with two hidden layers, each containing 1000 units and ReLU activations. Finally, the output is linearly projected to a scalar. For the MNIST dataset, there is an additional Channel Dropout layer after the ReLU activation of each HNP layer and a Dropout layer after the ReLU activation of each MLP layer, both with a dropout rate of 0.1. We use the Binary Cross Entropy (BCE) loss function and train the model for 200,000 steps, which takes 1 hour and 35 minutes on an A100 GPU. For the baseline models, we follow the same architecture in [71], with minor modifications to the model hidden dimension, reducing it from 512 to 256 to avoid overftiting. We use a hidden dimension of 256 for all baseline models and our base model. The number of parameters of all models can be found in Table 21 ", "page_idx": 28}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/01db016e24bebc952ec387315eeca4a087e253fc0a0388edf4053eb61ef221ef.jpg", "table_caption": ["Table 20: Hyperparameters of Monomial-NFN for each dataset in Classify INRs task. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/25d298f5d245501e21e950d877d411d1be0e2c5f4c4fddaf33d9fa37a5acded4.jpg", "table_caption": ["Table 21: Number of parameters of all models for classifying INRs task. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/7389b5d326bf533e5a1b465872f2dd6674d6e18986645f679db244be1f00f318.jpg", "table_caption": ["Table 22: Number of parameters of all models for Weight space style editing task. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/7baada0b726113d29ade1f9518b65f39d8695bdaa5dd1fe29c2c01bb6f4447df.jpg", "table_caption": ["Table 23: Hyperparameters for Monomial-NFN on weight space style editing task. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "D.5 Weight space style editing ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Dataset. We use the same INRs dataset as used for classification task, which has the size of train, validation and test set described in Table 19. ", "page_idx": 29}, {"type": "text", "text": "Implementation details. In these experiments, our general architecture includes 2 Monomial-NFN layers with 16 hidden dimensions. The architecture then follows the same design as the NP model in [71], where a Gaussian Fourier Transformation with a mapping size of 256 is applied. After that, the features go through IOSinusoidalEncoding and then through $3\\;\\mathrm{NP}$ layers, each with 128 hidden dimensions and ReLU activation. Finally, the output goes through an NP layer to project into a scalar and a LearnedScale layer described in the Appendix of [71]. We use the Binary Cross Entropy (BCE) loss function and train the model for 50,000 steps, which takes 35 minutes on an A100 GPU. For the baseline models, we keep the same settings as the official implementation. Specifically, the HNP or NP model will have 3 layers, each with 128 hidden dimensions, followed by a ReLU activation. An NFN of the same type will be applied to map the output to 1 dimension and pass it through a LearnedScale layer. The number of parameters of all models can be found in Table 22. The detailed hyperparameters for our model can be found in Table 23. ", "page_idx": 29}, {"type": "image", "img_path": "rQYyWGYuzK/tmp/559adb06ce2833b0e5b72045d954a5ab999deebb0a0a3d7513e8a1a875daf2fe.jpg", "img_caption": ["Figure 2: Random qualitative samples of INR editing behavior on the Dilate (MNIST) and Contrast (CIFAR-10) editing tasks. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "D.6 Ablation Regarding Design Choices ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We provide the ablation study on the choice of architecture for the task Predict CNN Generalization on ReLU subset in Table 24. We denote: ", "page_idx": 30}, {"type": "text", "text": "\u2022 Monomial Equivariant Functional Layer (Ours): MNF   \n\u2022 Activation: ReLU   \n\u2022 Scaling Invariant and Permutation Equivariant Layer (Ours): Norm   \n\u2022 Hidden Neuron Permutation Invariant Layer (in [71]): HNP   \n\u2022 Permutation Invariant Layer: Avg   \n\u2022 Multilayer Perceptron: MLP ", "page_idx": 30}, {"type": "table", "img_path": "rQYyWGYuzK/tmp/7dd205a560e682d070b0dccd1c17a7d4fd2a5f3353d35d4e7df631b2428226ab.jpg", "table_caption": ["Table 24: Ablation study on design choices for the task Predict CNN generalization on ReLU subset "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Among these designs, the architecture incorporating three layers of Monomial-NFN with ReLU activation achieves the best performance. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction are clearly stated in the Contribution in the Section 1. These claims accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The limitations are discussed in the Section 7. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All theoretical results in the paper are given together with the full set of assumptions and complete/correct proofs (See Appendix C.2 and Appendix C.1). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the experiment details in the Implementation details section in the Appendix D of our manuscript. We also provide the source code so that the results in the paper can be easily reproduced. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the source code in the supplementary resources with detailed guide to run so that the results in the paper can be easily reproduced. We verify our proposed methods using public benchmarks (See the Section 6 in our manuscript) ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We specify all the training and test details necessary to understand the results in the Implementation details section in the Appendix D of our manuscript. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We report error bars suitably and correctly defined of the experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources for all experiments in our Implementation details in Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We discuss broader impacts in Appendix ??. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We cite the githubs we use and the baselines we compare with in the Implementation details part in Appendix D of our manuscript. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]