[{"figure_path": "rQYyWGYuzK/tables/tables_6_1.jpg", "caption": "Table 1: Number of parameters in a linear equivariant layer E: U \u2192 U' with respect to permutation matrix groups in [71], and monomial matrix groups. Here, c = max{wi, bj} and c' = max{w', b';}.", "description": "This table compares the number of parameters required for a linear equivariant layer in neural functional networks when using different subgroups of the monomial matrix group.  It shows that using the full monomial matrix group (ours), which incorporates scaling and sign-flipping symmetries in addition to permutations, significantly reduces the number of parameters compared to methods that only consider permutation symmetries ([71]). The notation 'O(...)' indicates the order of complexity. ", "section": "5.1 Equivariant Layers"}, {"figure_path": "rQYyWGYuzK/tables/tables_7_1.jpg", "caption": "Table 2: CNN prediction on Tanh subset of Small CNN Zoo with original and augmented data.", "description": "This table presents the CNN prediction results on the Tanh subset of the Small CNN Zoo dataset.  It compares the performance of four different models (STATNN, NP, HNP, and Monomial-NFN) using both original and augmented data.  The \"Gap\" column shows the performance difference between the Monomial-NFN model and the next best performing model. Augmented data refers to data that has undergone random hidden vector permutation and scaling based on their monomial matrix group.", "section": "6 Experimental Results"}, {"figure_path": "rQYyWGYuzK/tables/tables_8_1.jpg", "caption": "Table 3: Classification train and test accuracies (%) for implicit neural representations of MNIST, FashionMNIST, and CIFAR-10. Uncertainties indicate standard error over 5 runs.", "description": "This table presents the classification accuracy results for three different datasets (MNIST, FashionMNIST, and CIFAR-10) using four different methods: Monomial-NFN (the proposed method), NP, HNP, and MLP.  The accuracy is reported as a percentage, with standard error over 5 runs indicating variability.  The results show a comparison of the proposed method's performance against existing techniques for classifying implicit neural representations.", "section": "6.2 Classifying implicit neural representations of images"}, {"figure_path": "rQYyWGYuzK/tables/tables_9_1.jpg", "caption": "Table 4: Test mean squared error (lower is better) between weight-space editing methods and ground-truth image-space transformations. Uncertainties indicate standard error over 5 runs.", "description": "This table presents the mean squared error (MSE) for two weight-space style editing tasks: contrast enhancement on CIFAR-10 images and dilation on MNIST digits.  The MSE is a measure of the difference between the edited image generated using the neural network weights and the ground truth (desired) transformation of the image. Lower MSE values indicate better performance. The results are reported for four methods: Monomial-NFN (the proposed method), NP and HNP (baseline methods from prior work), and MLP (a simple multi-layer perceptron).  Standard errors are included to show the variability of the results.", "section": "6.3 Weight space style editing"}, {"figure_path": "rQYyWGYuzK/tables/tables_18_1.jpg", "caption": "Table 1: Number of parameters in a linear equivariant layer E: U \u2192 U' with respect to permutation matrix groups in [71], and monomial matrix groups. Here, c = max{wi, bj} and c' = max{w', b';}.", "description": "This table compares the number of parameters required for a linear equivariant layer in different scenarios. The comparison is made between the use of permutation matrix groups (as in a previous work [71]) and monomial matrix groups (as proposed in this paper).  The number of parameters is shown to be significantly reduced when using monomial matrix groups, especially for deep networks. The notation 'c' represents the maximum of the weight and bias dimensions for the input layer, and 'c'' represents the same for the output layer.  'L' represents the number of layers.", "section": "5.1 Equivariant Layers"}, {"figure_path": "rQYyWGYuzK/tables/tables_18_2.jpg", "caption": "Table 1: Number of parameters in a linear equivariant layer E: U \u2192 U' with respect to permutation matrix groups in [71], and monomial matrix groups. Here, c = max{wi, bj} and c' = max{w', b';}.", "description": "This table compares the number of parameters required for a linear equivariant layer in different scenarios. It contrasts the parameter counts when using permutation matrix groups (as in previous work cited as [71]) against the use of monomial matrix groups (the approach proposed in this paper).  The number of parameters is shown to be significantly reduced when using the monomial matrix groups, improving model efficiency. The notation 'c' represents the maximum of weight and bias dimensions in the input layer, while 'c'' is the same for the output layer.", "section": "5.1 Equivariant Layers"}, {"figure_path": "rQYyWGYuzK/tables/tables_20_1.jpg", "caption": "Table 1: Number of parameters in a linear equivariant layer E: U \u2192 U' with respect to permutation matrix groups in [71], and monomial matrix groups. Here, c = max{wi, bj} and c' = max{w', b';}.", "description": "This table compares the number of parameters required for a linear equivariant layer in different neural network architectures.  It contrasts the parameter count when using permutation matrix groups (as in prior work [71]) versus monomial matrix groups (the approach introduced in this paper). The key takeaway is that the proposed method using monomial matrix groups significantly reduces the number of parameters, making it more efficient, especially for larger networks. The notation 'c' represents the maximum of the weight (w) and bias (b) dimensions.", "section": "5.1 Equivariant Layers"}, {"figure_path": "rQYyWGYuzK/tables/tables_20_2.jpg", "caption": "Table 1: Number of parameters in a linear equivariant layer E: U \u2192 U' with respect to permutation matrix groups in [71], and monomial matrix groups. Here, c = max{wi, bi} and c' = max{w', b';}.", "description": "This table compares the number of parameters required for a linear equivariant layer in different neural network architectures.  It contrasts the parameter counts for models using permutation matrix groups (as in a previous work) against those using monomial matrix groups (as proposed by the authors).  The key takeaway is that the monomial matrix group approach results in a significantly smaller number of parameters (linear vs. quadratic in L, no, nL). This improved efficiency is a key advantage of the authors' method, especially when dealing with large-scale networks.", "section": "5.1 Equivariant Layers"}, {"figure_path": "rQYyWGYuzK/tables/tables_21_1.jpg", "caption": "Table 1: Number of parameters in a linear equivariant layer E: U \u2192 U' with respect to permutation matrix groups in [71], and monomial matrix groups. Here, c = max{wi, bi} and c' = max{w', b';}.", "description": "This table compares the number of parameters required for a linear equivariant layer in different scenarios. It contrasts the parameter count for layers using permutation matrix groups (as in a previous work cited as [71]) against layers utilizing monomial matrix groups (the proposed method). The parameter counts are expressed using Big O notation, highlighting the order of growth with respect to various factors such as the number of layers (L) and the maximum dimensions of weights and biases (c and c').  The results show that the proposed method using monomial matrix groups leads to a significantly smaller number of parameters compared to the prior work.", "section": "5.1 Equivariant Layers"}, {"figure_path": "rQYyWGYuzK/tables/tables_26_1.jpg", "caption": "Table 11: Memory consumption.", "description": "This table compares the memory usage of different neural functional network models (NP, HNP, GNN, and Monomial-NFN) on two subsets of the Small CNN Zoo dataset: Tanh and ReLU.  The results show that the Monomial-NFN model has significantly lower memory consumption than the other models.", "section": "D Additional experimental details"}, {"figure_path": "rQYyWGYuzK/tables/tables_27_1.jpg", "caption": "Table 12: Predict CNN generalization on ReLU subset (augmented train data)", "description": "This table presents the results of predicting CNN generalization on the ReLU subset of the Small CNN Zoo dataset using augmented training data.  It compares the performance of the GNN [35] method and the Monomial-NFN (the authors' model) across different levels of augmentation (1, 2, 3, and 4), which correspond to different scaling ranges.  The metric used is Kendall's tau, a rank correlation measure.", "section": "D.2 Comparison of Monomial-NFNs and GNN-based NFNs"}, {"figure_path": "rQYyWGYuzK/tables/tables_27_2.jpg", "caption": "Table 14: Predict CNN generalization on ReLU subset (original train data)", "description": "This table presents the results of predicting CNN generalization on the ReLU subset of the Small CNN Zoo dataset using the original train data. The performance of the GNN model from [35] and the proposed Monomial-NFN are compared across four different augmentation levels (1-4).  Each augmentation level represents a different range of scaling applied to the weights. The metric used is Kendall's Tau, a measure of rank correlation.", "section": "D.2 Comparison of Monomial-NFNs and GNN-based NFNs"}, {"figure_path": "rQYyWGYuzK/tables/tables_27_3.jpg", "caption": "Table 2: CNN prediction on Tanh subset of Small CNN Zoo with original and augmented data.", "description": "This table presents the performance comparison of different neural functional networks (NFNs) on the task of predicting CNN generalization. The models are evaluated on the Tanh subset of the Small CNN Zoo dataset, both with original and augmented data. The augmented data includes weight scaling and permutation.  The table shows the performance of each model using Kendall's Tau, a measure of rank correlation.  The results demonstrate that the proposed Monomial-NFN model outperforms other baselines.", "section": "6.1 Predicting CNN Generalization from Weights"}, {"figure_path": "rQYyWGYuzK/tables/tables_28_1.jpg", "caption": "Table 16: Datasets information for predicting generalization task.", "description": "This table presents the number of training and validation instances for four different datasets used in the CNN generalization prediction task.  The datasets are categorized by activation function (ReLU or Tanh) and whether or not they have been augmented. Augmented datasets are created by doubling the size of the original datasets with additional weight samples.", "section": "Additional experimental details"}, {"figure_path": "rQYyWGYuzK/tables/tables_28_2.jpg", "caption": "Table 1: Number of parameters in a linear equivariant layer E: U \u2192 U' with respect to permutation matrix groups in [71], and monomial matrix groups. Here, c = max{wi, bj} and c' = max{w', b';}.", "description": "This table compares the number of parameters required for a linear equivariant layer in different neural network models. It contrasts the parameter counts for models using permutation matrix groups (as in a previous work) and monomial matrix groups (as proposed in this paper).  The number of parameters is shown to scale differently with the number of layers (L) and the maximum dimensions of weights and biases (c and c'). The proposed method using monomial matrix groups shows a significant reduction in the number of parameters compared to methods based on permutation groups.", "section": "5.1 Equivariant Layers"}, {"figure_path": "rQYyWGYuzK/tables/tables_28_3.jpg", "caption": "Table 2: CNN prediction on Tanh subset of Small CNN Zoo with original and augmented data.", "description": "This table compares the performance of four different models (STATNN, NP, HNP, and Monomial-NFN) on the task of predicting the generalization performance of Convolutional Neural Networks (CNNs). The models are evaluated on a subset of the Small CNN Zoo dataset, using both the original dataset and an augmented version of the dataset. The augmented dataset includes additional samples generated by applying random hidden vector permutation and scaling transformations. The results are presented as the accuracy of the CNN prediction, along with the difference (gap) between Monomial-NFN's performance and the second-best performing model (HNP).", "section": "6 Experimental Results"}, {"figure_path": "rQYyWGYuzK/tables/tables_29_1.jpg", "caption": "Table 20: Hyperparameters of Monomial-NFN for each dataset in Classify INRs task.", "description": "This table lists the hyperparameters used for the Monomial-NFN model when classifying Implicit Neural Representations (INRs) for three different datasets: MNIST, Fashion-MNIST, and CIFAR-10.  The hyperparameters include the hidden layer dimension of the Monomial-NFN, the base model used (either HNP or NP), the base model's hidden dimension, the number of neurons in the MLP layers, the dropout rate, the learning rate, the batch size, the number of training steps, and the loss function employed (Binary cross-entropy). The values vary for each dataset, indicating dataset-specific optimizations.", "section": "6.2 Classifying implicit neural representations of images"}, {"figure_path": "rQYyWGYuzK/tables/tables_29_2.jpg", "caption": "Table 3: Classification train and test accuracies (%) for implicit neural representations of MNIST, FashionMNIST, and CIFAR-10. Uncertainties indicate standard error over 5 runs.", "description": "This table shows the classification accuracy results of four different neural network models (Monomial-NFN, NP, HNP, MLP) on three different datasets (MNIST, FashionMNIST, CIFAR-10). The results are shown as the mean accuracy and the standard error across five runs.  The table highlights the performance of Monomial-NFN compared to traditional methods and other state-of-the-art approaches.", "section": "6.2 Classifying implicit neural representations of images"}, {"figure_path": "rQYyWGYuzK/tables/tables_29_3.jpg", "caption": "Table 2: CNN prediction on Tanh subset of Small CNN Zoo with original and augmented data.", "description": "This table presents the performance comparison of different neural functional network models (STATNN, NP, HNP, and Monomial-NFN) on a CNN prediction task using the Tanh subset of the Small CNN Zoo dataset.  The results are shown for both original and augmented data, indicating the performance improvement of Monomial-NFN with data augmentation. The 'Gap' column represents the performance difference between Monomial-NFN and the second-best model.  The results demonstrate that Monomial-NFN outperforms all other models, especially when using augmented data.", "section": "6 Experimental Results"}, {"figure_path": "rQYyWGYuzK/tables/tables_29_4.jpg", "caption": "Table 23: Hyperparameters for Monomial-NFN on weight space style editing task.", "description": "This table lists the hyperparameters used for the Monomial-NFN model during the weight space style editing experiments.  It shows the values chosen for the model's hidden dimension, NP dimension (referencing another model), optimizer, learning rate, batch size, and the number of training steps.", "section": "D.5 Weight space style editing"}, {"figure_path": "rQYyWGYuzK/tables/tables_30_1.jpg", "caption": "Table 2: CNN prediction on Tanh subset of Small CNN Zoo with original and augmented data.", "description": "This table presents the CNN prediction results on the Tanh subset of the Small CNN Zoo dataset.  It compares the performance of four different neural functional network models (STATNN, NP, HNP, and Monomial-NFN) using both original and augmented data. The 'Gap' column indicates the performance difference between the best-performing model (Monomial-NFN) and the second-best performing model for each dataset. The results highlight the superior performance and stability of the Monomial-NFN model, particularly when using augmented data, showcasing its ability to generalize effectively to variations within the weight space.", "section": "6 Experimental Results"}]