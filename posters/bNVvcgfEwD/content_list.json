[{"type": "text", "text": "Convergence of Adafactor under Non-Convex Smooth Stochastic Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 As model sizes in deep learning continue to expand, memory-efficient optimizers   \n2 are increasingly critical to manage the substantial memory demands of popular   \n3 algorithms like Adam and AdamW. Among these, Adafactor has emerged as one   \n4 of the widely adopted choices for training deep learning tasks, particularly large   \n5 language models. However, despite its practical success, there is limited theoretical   \n6 analysis on Adafactor's convergence. This paper presents a comprehensive analysis   \n7 on Adafactor in a non-convex smooth setting, demonstrating its convergence to find   \n8 a stationary point at a rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ . We find that the default hyper-parameter   \n9 setting results in a sub-optimal rate in our framework, and propose an alternative   \n0 setting that could theoretically achieve optimal convergence rate. This finding   \n1 is further supported by some experimental results. We also prove that Adafactor   \n2 with a suitable time-varying clipping threshold could also converge, achieving   \n3 performance in experiments comparable to that of the standard constant setting. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 The adaptive gradient-based methods, such as the well-known AdaGrad [9, 29], RMSProp [30],   \n16  Adadelta [35], Adam [15] and AdamW [22], have become the preferred approaches in solving the   \n17 following unconstrained stochastic optimization problem in deep learning fields: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{X}\\in\\mathbb{R}^{n\\times m}}f(\\pmb{X})=\\mathbb{E}_{\\pmb{Z}\\in\\mathcal{P}}[l(\\pmb{X};\\pmb{Z})],\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "18where the object function $f$ is non-convex and $\\mathcal{P}$ denotes a probability distribution. During the   \n19 training process, these adaptive methods require to store the historical gradients\u2019 information so as   \n20 to adaptively tune their step-sizes. For example, both Adam and Adam W maintain the exponential   \n21 average of gradients and squared gradients, and AdaGrad stores the cumulative of squared gradients.   \n22 Despite their effectiveness, these algorithms pose substantial memory challenges for GPUs to save   \n23 these additional gradients\u2019 information, especially when training large language models (LLMs),   \n24 such as GPT-3 [5], which contains over 175 billion parameters.   \n25  To address memory constraints, several memory-efficient optimization algorithms have been devel  \n26 oped, e.g., [26, 1, 23, 17]. One of the most popular optimizers is Adafactor [26] which employs   \n27 a rank-1 matrix factorization to approximate the second moment matrix in Adam. For an $n\\times m$   \n28 weight matrices, this technique reduces memory usage from $\\mathcal{O}(m n)$ to $\\mathcal{O}(m+n)$ by onlytracking   \n29 the moving averages of the row and column sums of the squared gradients matrix. Additionally,   \n30 Adafactor eliminates the first-order momentum used in Adam and incorporates update clipping to   \n31 enhance training stability.   \n32  The empirical results reveal that Adafactor achieves comparable performance to Adam in training   \n33  Transformer models [26] . In real applications, several LLMs including PaLM [6] and T5 [24] have   \n34  applied Adafactor as their main optimizers [38]. In spite of Adafactor's widely usage, there is still   \n35  limited understanding on its convergence in theory, especially the effect of the matrix approximation   \n36  and update clipping, and the explanation for its hyper-parameter setting in experiments.   \n37 In this paper, we take a closer look on Adafactor's convergence under non-convex smooth optimization   \n38 problems, considering the typical bounded gradient setting as those for AdaGrad [19, 32] and Adam   \n39 [34]. We aim to provide a convergence rate for Adafactor and explain the influence of the hyper  \n40 parameters for the convergence speed. We also prove in theory why the default parameter setting is   \n41 effective in practical scenarios. The analysis to Adafactor is non-trivial compared to other adaptive   \n42 methods such as AdaGrad and Adam due to the unique matrix factorization and update clipping   \n43  mechanisms. Based on a new proxy step-size construction and some new compositions as well as   \n44 estimations, we analyze the additional error terms in the Descent Lemma introduced by the matrix   \n45 approximation and update clipping. Our main contributions are summarized as follows. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "46 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "47 \u00b7 We provide a convergence analysis for the full-batch Adafactor considering bounded gradients   \n48 and a broader range of parameter setting which covers the default one in [26]. The result shows   \n49 that Adafactor could converge to find a stationary point with a rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ where $T$   \n50 denotes the total iteration number.   \n51 \u00b7 We further investigate the more realistic stochastic Adafactor. It's found that a simple variant of   \n52 Adafactor, which drops the update clipping, could attain the best convergence rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$   \n53 when the second moment decay rate is $\\bar{1}-1/k$ .We also verify that the default decay rate   \n54 $1-1/k^{0.8}$ could lead to a sub-optimal convergence rate in our framework. To illustrate this   \n55 finding, we provide some empirical results, showing that the potential best hyper-parameter   \n56 setting in theory could perform better than the default one used in experiments.   \n57 \u00b7 We extend our study to include a time-varying clipping threshold. Our analysis implies that   \n58 with proper selections of clipping threshold and hyper-parameters, Adafactor could also achieve   \n59 the best convergence rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ . We also do some experiments to show that the new   \n60 clipping threshold scheme achieves comparable performance and training stability to the original   \n61 constant threshold setting.   \n62 The rest of the paper is organized as follows. The next section provides some most relevant works.   \n63  Section 3 presents some necessary notations definitions and problem setup. Section 4 reviews   \n64 Adafactor and introduces its essential mechanism. In Section 5 and Section 6, we separately provide   \n65 convergence bounds for full-batch Adafactor and stochastic Adafactor without update clipping. We   \n66 further discuss the hyper-parameters' dependency. In Section 7, we investigate Adafactor using a   \n67 time-increasing update clipping threshold. Section 8 provides experimental results to support our   \n68 theory. All the detailed proof could be found in the appendix. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "692Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 In this paper, we mainly investigate the theoretical convergence of Adafactor. Although there is   \n71  limited works on Adafactor in theory, it's necessary to briefly discuss related works on the convergence   \n72of other adaptive methods, particularly on non-convex smooth optimization. Here, we briefly list   \n73some of the most related works.   \n74  Convergence of adaptive methods  Several studies address the convergence of AdaGrad in non  \n75 convex settings. For example, [19] considered a simple variant with delayed step-size, while [32]   \n76 and [39] assumed bounded stochastic gradients. Other works [14, 10, 21, 3, 31, 27, 33] derived   \n77 convergence bounds under more relaxed assumptions. Another line of research has investigated the   \n78 convergence of Adam. For instance, [34, 7, 39, 11, 8] assumed bounded gradients. [28, 36, 31]   \n79 considered more relaxed noise assumptions without relying on bounded gradients. Additionally, [18]   \n80 derived convergence bounds for Adam under generalized smooth conditions.   \n81 Overall, the convergence analysis of optimizers typically starts with standard assumptions, such as   \n82  bounded gradients and smooth objective functions. In subsequent studies, these assumptions are   \n83 gradually relaxed to investigate the convergence properties of the optimizers under less stringent   \n84conditions.   \n85  Memory efficient algorithms   As large models are increasingly used in deep learning, memory   \n86  constraints have become a central issue during training. Consequently, several memory-efficient   \n87 optimizers have been developed to address this challenge.   \n88 One approach to save memory involves applying matrix factorization to oeptimization algorithms.   \n89 For instance, [25] used matrix factorization in the second moment estimator of gradients in Adam,   \n90 similar to the concept behind Adafactor. [23] introduced CAME, a variant of Adafactor, which   \n91 incorporates a confidence-guided strategy to mitigate instability caused by erroneous updates. [37]   \n92 proposed Adapprox, leveraging randomized low-rank matrix approximation for Adam's second   \n93 moment estimator, demonstrating superior performance and reduced memory usage compared to   \n94AdamW.   \n95  There are some other techniques to save the memory. For example, [12] relied on a \u201cShampoo\"   \n96 technique to reduce the storage requirement of full-matrix preconditioning methods. Notably, their   \n97 method could be further extended to the more realistic tensor case. [1] presented a memory-saved   \n98 version of AdaGrad, called SM3, by maintaining $k$ sets gradient accumulator. They proved the   \n99 convergence guarantee of SM3 on online convex optimization and the effectiveness in experiments.   \n100 Recently, [17] built a 4-bit Adam using quantization techniques to compress the first and second   \n101 moment estimators in Adam, also reducing memory usage.   \n102  In summary, many existing optimizers, particularly adaptive methods like AdaGrad and Adam, face   \n103 memory overhead. In response, the discussed works have designed memory-efficient optimizers that   \n104  aim to achieve comparable performance to these existing methods while achieving memory benefits. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1053 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "106  To start with, we introduce some necessary notations. ", "page_idx": 2}, {"type": "text", "text": "107  Notations  For any two matrices $\\pmb{X}\\;=\\;(x_{i j})_{i j},\\pmb{Y}\\;=\\;(y_{i j})_{i j}\\;\\in\\;\\mathbb{R}^{n\\times m}$ , we define $\\left\\langle X,Y\\right\\rangle\\;=$   \n108 $\\textstyle\\sum_{i=1}^{n}\\sum_{j=1}^{m}x_{i j}y_{i j}$ $\\pmb{X}\\odot\\pmb{Y},\\pmb{X}/\\pmb{Y}$ and $\\sqrt{X}$ denote the coordinate-wise product, quotient and   \n109 squared root respectively. ${\\bf0}_{n}$ and ${\\mathbf{1}}_{n}$ denote the zero and one $n$ -dimensionalvectorrespectively,   \n110 and ${\\mathbf{1}}_{n\\times m}$ denotes the one $n\\times m$ -dimensional matrix. The index set $[n]$ denotes $\\{1,{\\bar{2}},\\cdot\\cdot\\cdot\\,,n\\}$   \n111 $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. For a positive sequence $\\{\\alpha_{i}\\}_{i\\ge1}$ , we define $\\textstyle\\sum_{i=a}^{b}\\alpha_{i}=0$ and   \n112 $\\textstyle\\prod_{i=a}^{b}\\alpha_{i}=1$ $a>b$ The operator $\\mathrm{{RMS}(\\cdot)}$ denotes ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{RMS}(\\boldsymbol{X})=\\sqrt{\\frac{1}{m n}\\sum_{i=1}^{n}\\sum_{j=1}^{m}x_{i j}^{2}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "113 We consider unconstrained stochastic optimization (1) over $\\mathbb{R}^{n\\times m}$ with the Frobenius norm. The   \n114  bjective function $f:\\mathbb{R}^{n\\times m}\\rightarrow\\mathbb{R}$ is differentiable. Given an $n\\times m$ matrix $\\mathbf{\\deltaX}$ , we assume a gradient   \n115  oracle that returns a random matrix $g(X,Z)\\in\\mathbb{R}^{n\\times m}$ dependent by the random sample $_{z}$ . The   \n116  deterministic gradient of $f$ at $\\mathbf{\\deltaX}$ is denoted by $\\nabla f(\\pmb{X})\\in\\bar{\\mathbb{R}^{n\\times m}}$ ", "page_idx": 2}, {"type": "text", "text": "117 Assumptions  We make the following standard assumptions throughout the paper. ", "page_idx": 2}, {"type": "text", "text": "118   \n119   \n120   \n121   \n122 ", "page_idx": 2}, {"type": "text", "text": "\u00b7 (A1) $L$ -smoothness: For any $\\boldsymbol{X},\\boldsymbol{Y}\\in\\mathbb{R}^{n\\times m}$ $\\begin{array}{r}{\\|\\nabla f(\\pmb{Y})-\\nabla f(\\pmb{X})\\|_{F}\\leq L\\|\\pmb{Y}-\\pmb{X}\\|_{F};}\\end{array}$ \u00b7 (A2) Bounded below: There exists $f^{*}>-\\infty$ such that $f(\\pmb{X})\\geq f^{*},\\forall\\pmb{X}\\in\\mathbb{R}^{n\\times m}$ \u00b7 (A3) Unbiased estimator: The gradient oracle provides an unbiased estimator of $\\nabla f(\\mathbf{\\boldsymbol{X}})$ , i.e., $\\mathbb{E}_{Z}\\left[g(X,Z)\\right]=\\nabla f(X),\\forall X\\in\\mathbb{R}^{n\\times m}$ \u00b7 (A4) Almost surely bounded stochastic gradient: for any $\\b X\\in\\mathbb{R}^{n\\times m}$ \uff0c $\\|g(X,Z)\\|_{F}\\leq G$ a.s.. ", "page_idx": 2}, {"type": "text", "text": "123Combining (A3) and (A4), it's easy to verify that $\\|\\nabla f(\\pmb{X})\\|\\,\\le\\,G,\\forall\\pmb{X}\\,\\in\\,\\mathbb{R}^{n\\times m}$ .Assumptions   \n124 (A1)-(A3) are standard in the non-convex smooth convergence analysis. Although Assumption (A4)   \n125 is a bit strong since it requires an almost surely bounded stochastic gradients instead of an expected   \n126 one, it's still commonly used to derive the high probability convergence bound, see e.g., [32, 14],   \n127 which is a stronger result than an expected convergence. In coordinate-wise algorithm, another   \n128 standard assumption is $l_{\\infty}$ -bounded gradient where $\\|g(\\boldsymbol{X},Z)\\|_{\\infty}\\leq G_{\\infty}$ , see e.g., [8]. These two   \n129 types of assumption are equivalent up to dimension factors.   \n131 In this section, we briefly discuss Adafactor based on the reference [26]. The pseudocode for   \n132  Adafactor is presented in Algorithm 1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Algorithm1Adafactor Input: Initialization point $X_{1}\\in\\mathbb{R}^{n\\times m}$ \uff0c ${\\cal R}_{0}={\\bf0}_{m}$ \uff0c $C_{0}=\\mathbf{0}_{n}^{\\top}$ , relative step-sizes $\\{\\rho_{k}\\}_{k\\ge1}$ , decay rate $\\{\\beta_{2,k}\\}_{k\\geq1}\\in[0,1)$ , regularization constants $\\epsilon_{1},\\epsilon_{2}>0$ , clipping threshold $d$ for $k=1,\\cdots,T$ do $G_{k}=g(X_{k},Z_{k})$ $\\begin{array}{r l}&{\\rightharpoonup\\kappa\\ltimes\\ltimes\\mathbf{\\Theta}_{J\\backslash\\mathbf{k}}^{\\times},\\ltimes\\mathbf{\\Theta}_{\\kappa\\rightarrow\\mathbf{k}}^{\\times\\prime},}\\\\ &{\\rightharpoonup\\ R_{k}=\\beta_{2,k}\\mathbf{R}_{k-1}+(1-\\beta_{2,k})(G_{k}\\odot G_{k}+\\epsilon_{1}\\mathbf{1}_{n}\\mathbf{1}_{m}^{\\top})\\mathbf{1}_{m};}\\\\ &{\\rightharpoonup\\alpha_{k}=\\beta_{2,k}G_{k-1}+(1-\\beta_{2,k})\\mathbf{1}_{n}^{\\top}(G_{k}\\odot G_{k}+\\epsilon_{1}\\mathbf{1}_{n}\\mathbf{1}_{m}^{\\top});}\\\\ &{\\rightharpoonup\\ W_{k}=(R_{k}G_{k})/\\mathbf{1}_{n}^{\\top}R_{k};}\\\\ &{\\rightharpoonup U_{k}=G_{k}/\\sqrt{W_{k}};}\\\\ &{\\eta_{k}=\\mathrm{max}\\{\\epsilon_{2},\\mathrm{RMS}(X_{k})\\}\\rho_{k}/\\operatorname*{max}\\{1,\\mathrm{RMS}(U_{k})/d\\};}\\\\ &{\\rightharpoonup\\chi_{k+1}=X_{k}-\\eta_{k}\\cdot G_{k}/\\sqrt{W_{k}};}\\end{array}$ end for ", "page_idx": 3}, {"type": "text", "text": "133 Matrix factorization  Adafactor could be severed as a saved-memory version of Adam. Throughout   \n134  the training process, Adam maintain two $n\\times m$ matrices $M_{k}$ and $\\mathbf{}V_{k}$ using exponential moving   \n135average update, ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{k}=\\beta_{1,k}M_{k-1}+(1-\\beta_{1,k})G_{k},\\quad V_{k}=\\beta_{2,k}V_{k-1}+(1-\\beta_{2,k})G_{k}\\odot G_{k},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "136 where $\\beta_{1,k},\\beta_{2,k}\\,\\in\\,(0,1)$ , thereby tripling the memory usage. The innovation in Adafactor lies   \n137  in its method of approximating $\\mathbf{}V_{k}$ by factoring it into two rank-1 matrices, specifically the row   \n138 sums and column sums of $V_{k}$ . This approximation is guided by maintaining a minimal general   \n139  Kullback-Leibler (KL) divergence as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{X\\in\\mathbb{R}^{n},Y\\in\\mathbb{R}^{1\\times m}}\\sum_{i=1}^{n}\\sum_{j=1}^{m}d\\left(\\left(V_{k}\\right)_{i j},(X Y)_{i j}\\right),\\quad\\mathrm{s.t.}\\quad(X)_{i}\\geq0,(Y)_{j}\\geq0,\\forall i\\in[n],j\\in[m],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "140where $d(p,q)=p\\log(p/q)-p+q$ . The choice of KL-divergence over the more typical Frobenius   \n141 norm allows for an analytical solution to be derived, specifically given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nX=V_{k}\\mathbf{1}_{m},\\quad Y=\\mathbf{1}_{n}^{\\top}V_{k}/\\left(\\mathbf{1}_{n}^{\\top}V_{k}\\mathbf{1}_{m}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 Therefore, Adafactor only requires to maintain two vectors $R_{k}=V_{k}\\mathbf{1}_{m}$ $C_{k}=\\mathbf{1}_{n}^{\\top}V_{k}$ ,sufficiently   \n143 reducing the memory from $2m n$ to $m+n$ . Although this factorization sacrifices some information   \n14 of the squared gradients, Adafactor still delivers performance comparable to Adam in many real   \n145  application tasks, making it a practical choice where memory is a constraint.   \n146  Increasing decay rate  In Adam, corrective terms are introduced into $M_{k}$ and $V_{k}$ , resulting in two   \n147 increasing-to-one decay rates. Theoretically, it has been demonstrated that a value closed to one for   \n148 $\\beta_{2,k}$ would ensure the convergence, e.g., [8, 39, 36]. Inspired by this observation, Adafactor used an   \n149 increasing second moment decay rate $\\beta_{2,k}=1-1/k^{c},c\\geq0$ , and the empirical default setting is   \n150 $c=0.8$ . As pointed out by [26], this setting allows for enjoying the stability of a low $\\beta_{2,k}$ at the early   \n151 stage of training and the insurance of convergence from a high $\\beta_{2,k}$ as the run progresses. Moreover,   \n152 it also leverages the bias correction.   \n153 Update clipping  Adafactor modifies the update process by discarding the first-order moment $M_{k}$   \n154  and instead applies an update clipping_ technique inside the step-size $\\eta_{k}$ . This involves dividing   \n155 the root-mean-square of the update $U_{k}$ , denoted as $\\mathrm{RMS}(U_{k})$ ,_ when it exceeds a threshold $d$   \n156 This mechanism helps to calibrate the second moment estimator $W_{k}$ when it's larger-than-desired   \n157 $G_{k}\\odot G_{k}$ . Empirical findings in [26] indicated that implementing update clipping leads to significant   \n158  performance improvements when the warm-up technique is not used.   \n159  Relative step-sizes  Adafactor incorporates a step-size proportional to scale of $\\scriptstyle{X_{k}}$ ,denotedby   \n160 $\\mathrm{RMS}(X_{k})$ , which is shown in experiments more resilient to the more naive parameter initialization   \n161 and scaling schemes [26]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "162 5  Convergence result for full-batch Adafactor ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 We first provide the convergence bound for full-batch Adafactor. At each iteration, full-batch   \n164  Adafactor obtains the deterministic gradient $\\nabla f({\\mathbf{{X}}}_{k})$ and then updates $R_{k},C_{k}$ Uusing $\\nabla f({\\mathbf{{X}}}_{k})$   \n165instead of $G_{k}$ in Algorithm 1.   \n166 Theorem 5.1. Let $\\{X_{k}\\}_{k\\ge1}$ be generated by Algorithm $^{\\,l}$ with $g(X_{k},Z_{k})=\\nabla f(X_{k}),\\forall k\\geq1.$ If   \n167 Assumptions $(A l)$ and (A2) hold, $\\|\\nabla f(\\pmb{X}_{k})\\|_{F}\\leq G,\\forall k\\geq1,$ $\\beta_{2,1}={\\textstyle\\frac{1}{2}}$ and ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{k}=\\rho_{0}/\\sqrt{k},\\quad0<\\beta_{2,k}<1,\\quad\\forall k\\geq1,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "168  for some positive constant $\\rho_{0}$ , then for any $T\\geq1$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\nabla f(X_{k})\\|_{F}^{2}\\leq\\mathcal{O}\\left(\\frac{\\log T}{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "169 The result indicates that full-batch Adafactor could find a stationary point at a rate of ${\\mathcal{O}}(\\log T/{\\sqrt{T}})$   \n170 under the non-convex smooth case, which is similar to gradient descent but with a sub-optimal rate   \n171 compared to $\\mathcal{O}(1/T)$ [4]. The hyper-parameter setting in (3) only requires $\\beta_{2,k}\\in(0,1)$ ,denoting   \n172 a much wider range including the default one which requires $\\beta_{2,k}$ to increase to one. The detailed   \n173 version for the above result can be found in Theorem A.1 from the appendix. ", "page_idx": 4}, {"type": "text", "text": "17  6  Stochastic Adafactor without update clipping ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "175  In the stochastic case, we start from the simple scenario of ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{k}=\\operatorname*{max}\\{\\epsilon_{2},\\mathrm{RMS}(X_{k})\\}\\rho_{k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176  without considering the update clipping $1/\\operatorname*{max}\\{1,\\mathrm{RMS}(U_{k})/d)\\}$ in Algorithm 1, where the main   \n177 reasons are as follows.   \n178 \u00b7 As pointed out in the experiments from [26], Adafactor's performance shows little difference   \n179 with and without update clipping when implementing learning rate warm-up. Since the warm-up   \n180 technique is a popular method in deep learning [38], it's reasonable to drop the update clipping.   \n181 \u00b7 In stochastic Adafactor, the correlation between $G_{k}$ and $\\eta_{k}$ would be more complex if the update   \n182 clipping is involved. The proof would be simpler when dropping the update clipping, which   \n183 could help to better understand the analysis for Adafactor.   \n184 We now present the probabilistic convergence bound for Adafactor without update clipping as follows,   \n185 where we summarize different convergence rate with respect to the factor $c$ from $\\beta_{2,k}=1\\!-\\!1/k^{c},c$ $c\\in$   \n186 $[1/2,1]$   \n187   Theorem 6.1. Let $\\{X_{k}\\}_{k\\ge1}$ be generated by Algorithm $^{\\,l}$ without update clipping where $\\eta_{k}$ is given   \n188 by (5) for each $k\\geq1$ If Assumptions (A1)-(A4) hold, and ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{2,1}=1/2,\\quad\\rho_{1}=\\rho_{0},}\\\\ &{\\beta_{2,k}=1-1/k^{c},\\quad\\rho_{k}=\\rho_{0}/\\sqrt{k},\\quad\\forall k\\ge2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "189  for some constants $1/2\\le c\\le1,\\rho_{0}>0,$ then for any $T\\geq1,\\delta\\in(0,1)$ , with probability at least   \n190 $1-\\delta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\nabla f(\\pmb{X}_{k})\\|_{F}^{2}\\leq\\mathcal{O}\\left(\\frac{1}{T^{c-1/2}}\\log\\left(\\frac{T}{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 The above result indicates that with appropriate hyper-parameters, Adafactor without update clipping   \n193 could approximately find a stationary point. When the decay rate $\\beta_{2,k}$ is $1-1/k$ , the convergence   \n194 rate could attain to ${\\mathcal{O}}(\\log T/{\\sqrt{T}})$ , matching the rate of stochastic gradient descent [4] and the lower   \n195 rate in [2] up to only a logarithm factor. The hyper-parameter setting in (6) covers the experimental   \n196 default setting where $c=0.8$ . The result shows a sub-optimal rate of ${\\mathcal{O}}(\\log T/T^{0.3})$ under the   \n197 default setting. This finding is further complemented by the coming numerical experiments in Section   \n198 8. The detailed version of the above results can be found in Theorem B.1 from the appendix. ", "page_idx": 4}, {"type": "text", "text": "199 6.1 Discussion of the hyper-parameter dependency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "200 In this section, we discuss the dependency of several important hyper-parameters in Theorem 6.1   \n201 and the detailed version in Theorem B.1 in the appendix. It's worthy to mention that the dominated   \n202 order in our convergence bound is determined by the total iteration number $T$ ,whereas other hyper  \n203 parameters could be regarded as constants. However, we hope to improve the dependency of these   \n204 hyper-parameters as much as possible to make the convergence bound tight.   \n205 Discussion of $c$ and the optimal rate  The convergence bound in Theorem 6.1 reveals that when   \n206 $c=1,\\beta_{2,k}=1-1/k$ and $\\rho_{k}=\\rho_{0}/\\sqrt{k}$ , the convergence rate attains the optimal rate matching   \n207 the lower bound. In addition, when $c$ is closed to $1/2$ , the convergence rate deteriorates. This   \n208 phenomenon somehow explains that a small decay rate $\\beta_{2,k}$ $\\scriptstyle{\\mathrm{\\ddot{c}}}$ is low) may harm the convergence   \n209 speed, as $\\beta_{2,k}$ should be closed enough to 1 to ensure the convergence, which has been pointed out   \n210 similarly for Adam in [8, 39, 36].   \n211 The theoretical best parameter setting remains a small gap to the default one of $c=0.8$ .To verify our   \n212  theoretical finding, we provide some empirical evidence in Section 8, showing that $\\beta_{2,k}=1-1/k$   \n213 performs even better than the default one and the performance would be better when $c$ increases from   \n214 $\\bar{1}/2$ to 1.   \n215 Dependency to mn  It's clear to see that the convergence bounds in Theorem A.1 and Theorem   \n216 B.1 are free of the curse of the dimension factor mn as mn only appears on the denominator in each   \n217 coefficient. We think that solving the curse of dimension is vital since the applied range for Adafactor   \n218 includes many deep learning tasks where mn are comparable large to $T$   \n219 Dependency to $\\epsilon_{1},\\epsilon_{2}$ The convergence bounds in (37) and (39) from Theorem B.1 has a dependency   \n220 of ${\\mathcal{O}}(\\epsilon_{1}^{-1}\\log(1/\\epsilon_{1}))$ on $\\epsilon_{1}$ 1  Although the polynomial dependency to $\\epsilon_{1}$ is a bit worse since $\\epsilon_{1}$   \n221 ususally takes a small value in experiments, e.g., the default setting is $10^{-30}$ , it's still common in   \n222 some theoretical convergence results, e.g., [34, 18]. We also perform some experiments to show   \n223 that a relatively large $\\epsilon_{1}$ ,roughly $10^{-3}$ , makes no observable effect on the performance. Thereby,   \n224 $\\epsilon_{1}$ could be regarded as a constant in comparison to $T$ and the influence brought by $1/\\epsilon_{1}$ could be   \n225somehow acceptable.   \n226  Since the default value of $\\epsilon_{2}$ is $10^{-3}$ in experiments, it could also be regarded as a constant compared   \n227 to $T$ . Therefore, the dependency ${\\mathcal{O}}(1/\\epsilon_{2})$ on $\\epsilon_{2}$ shows little effect on convergence bounds given the   \n228  sufficiently large $T$   \n229 Dependency to the scale of parameters. The convergence bounds in Theorem B.1 contain a   \n230 $O(\\bar{\\Theta}_{\\mathrm{max}})$ factor where $\\Theta_{\\mathrm{max}}$ denotes the maximum values of $\\|X_{k}\\|_{\\infty}$ along the training process.   \n231 It's reasonable to assume that $\\Theta_{\\mathrm{max}}\\leq G_{0}$ for a comparable large constant $G_{0}$ in practice. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "232 7  Convergence of Adafactor with update clipping ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "233 In this section, we take a closer look on the comprehensive Adafactor with both matrix factorization   \n234  and update clipping. We slightly change the update clipping threshold $d$ in Algorithm 1 to a time  \n235varying threshold $d_{k}$ .Thestep-size $\\eta_{k}$ thenbecomes ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{k}=\\frac{\\operatorname*{max}\\{\\epsilon_{2},\\mathrm{RMS}(X_{k})\\}\\rho_{k}}{\\operatorname*{max}\\{1,\\mathrm{RMS}(U_{k})/d_{k}\\}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "236  Then, we present the following convergence bound. ", "page_idx": 5}, {"type": "text", "text": "237Theorem 7.1. Let $\\{X_{k}\\}_{k\\ge1}$ be generated by Algorithm $^{\\,l}$ with $\\eta_{k}$ given by (7) for each $k\\geq1$ If   \n238  Assumptions (A1)-(A4) hold, and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{1}=1,\\quad\\beta_{2,1}=1/2,\\quad\\rho_{1}=\\rho_{0},}\\\\ &{d_{k}=k^{\\frac{c}{2(\\alpha-1)}},\\quad\\beta_{2,k}=1-1/k^{c},\\quad\\rho_{k}=\\rho_{0}/\\sqrt{k},\\quad\\forall k\\ge2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "'The detailed calculation could be found in (45) and (46) in the appendix ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\nabla f(\\pmb{X}_{k})\\|_{F}^{2}\\leq\\mathcal{O}\\left(\\frac{1}{T^{c-1/2}}\\log\\left(\\frac{T}{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "242 Discussion of Theorem 7.1   The convergence result indicates that with a proper selection of the   \n243clipping threshold, along with the commonly used step-size $\\rho_{k}$ and decay rate $\\beta_{2,k}$ , Adafactor can   \n244  find a stationary point when $T$ is sufficiently large. The dependency of convergence bound on $c$   \n245  remains consistent with Theorem 6.1, achieving the optimal order when $c=1$ . In addition, the   \n246 convergence bound can still avoid the curse of dimension, which is shown in the detailed version   \n247 Theorem C.1 from the appendix.   \n248  The additional hyper-parameter $\\alpha$ primarily influences the dependency on $\\epsilon_{1}$ , specifically_ as   \n249 $\\mathcal{O}\\left(\\epsilon_{1}^{-\\alpha}\\log(1/\\epsilon_{1})\\right)$ . Thus, our convergence bound may deteriorate as $\\alpha$ increases, possibly due   \n250to the limitation of our proof framework. This dependency could be potentially improved to   \n251 ${\\mathcal{O}}\\left(\\epsilon_{1}^{-1}\\log(1/\\epsilon_{1})\\right)$ when mn is comparable to $1/\\epsilon_{1}$ , which is practical when implementing a large  \n252  size model.\u00b2 In our experiments, we tested different values of $\\alpha$ and found that suitably small values,   \n253such as $\\alpha=4,6,7,8$ can lead to performance and training stability comparable to the default setting,   \n254 even without implementing the warm-up technique. This finding suggests that our new threshold   \n255 setting plays a similar role in enhancing training stability as the default one, which is also the main   \n256  motivation of update clipping. Since $\\epsilon_{1}$ can be set to a relatively large value, e.g.. $10^{-3}$ , a dependency   \n257 like $\\mathcal{O}(\\epsilon_{1}^{-4}\\log(1/\\epsilon_{1}))$ is somewhat acceptable for sufficiently large $T$   \n258  The time-increasing $d_{k}$ provides the following intuition: As shown in [26, Figure 1], during the   \n259 early stages of training, a high decay rate $\\beta_{2,k}$ can cause larger-than-desired updates and training   \n260 instability. Therefore, we set a low threshold $d_{k}$ to ensure that the update clipping mechanism   \n261 effectively calibrates these larger-than-desired updates. As training progresses, the sequences and   \n262 updates become more stable, and the second moment estimator $W_{k}$ becomes more accurate in   \n263 estimating the squared gradients, which is also shown in [26, Figure 1]. Consequently, there is   \n264 less need for update clipping, corresponding to a relatively large $d_{k}$ . We have also verified through   \n265 experiments that our setting can achieve performance comparable to the default setting of $d=1$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "2668 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "267 In this section, we will report our experimental results based on the insights obtained in our theory.   \n268  We will mainly provide the following three experiments:   \n269 \u00b7 We test Adafactor without update clipping under different decay rate parameters $c$ , aiming to   \n270 demonstrate performance improvement as $c$ increases from 0.5 to 1 with optimal performance at   \n271 $c=1$ , as indicated in Theorem 6.1 and Theorem 7.1.   \n272 \u00b7 We evaluate the sensitivity of Adafactor to different values of $\\epsilon_{1}$ , particularly showing that a   \n273 relatively large $\\epsilon_{1}$ does not significantly impact performance.   \n274 \u00b7 We assess the performance of Adafactor with a time-increasing $d_{k}$ setting, as described in   \n275 Theorem 7.1, and compare it to the default constant setting. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "276  8.1  Experiment setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "277 In all experiments, the initialization is ${\\cal R}_{0}={\\bf0}_{m}$ and $C_{0}=\\mathbf{0}_{n}^{\\top}$ . We use a learning rate with the   \n278  warm-up technique as described in [26], specifically $\\rho_{k}=\\operatorname*{min}\\{10^{-6}\\cdot k,1/\\sqrt{k}\\}$ for all experiments   \n279 unless otherwise specified. The batch size is set to 256, and the total number of epochs is 400 by   \n280 default. Our models are ResNet-20 and ResNet-110 [13], and we use the CIFAR-10 and CIFAR-100   \n281datasets [16] without any data augmentation. The experiments are conducted using the PyTorch   \n282 implementation of Adafactor on a single NVIDIA GeForce RTX 4090 GPU. ", "page_idx": 6}, {"type": "image", "img_path": "bNVvcgfEwD/tmp/157b61075a1195c20ed28b0f2f635116478ce3b23506c97fa70c83add6c1ee38.jpg", "img_caption": ["Figure 1: Average test accuracy and standard deviation (shallow blue region) under different decay rate parameters $c$ "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "bNVvcgfEwD/tmp/129b93dfd1b843c95b193221a67d4ff329936da0a70597e9a2bae13406af3a97.jpg", "img_caption": ["", "Figure 2: Training loss vs. steps using Adafactor without update clipping under different $\\epsilon_{1}$ .The step-size $\\eta_{t}$ ,decayrate $\\beta_{2,k}$ , and learning rate warm-up are set by default. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "283 8.2  Report on Experiment 1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "284  We test Adafactor without update clipping using decay rate parameter $c$ ranging from 0.5 to 1.0 in   \n285  increments of O.05, while keeping other hyper-parameters at their default values. Each experiment is   \n286 run 10 times with 100 epochs, and we plot the average test accuracy and standard deviation (shallow   \n287 blue region) in Figure 1. The results indicate that $c=1.0$ yields better performance and stability   \n288 compared to $c<1.0$ on different models and datasets, corresponding to the highest test accuracy and   \n289 thinner shallow blue band. These performances show a noticeable improving trend as $c$ increases   \n290 from 0.5 to 1.0, aligning roughly with the results in Theorem 6.1. ", "page_idx": 7}, {"type": "text", "text": "2918.3 Report on Experiment 2 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "292 In the second experiment, we test Adafactor without update clipping under different $\\epsilon_{1}$ values. We   \n293 plot the training loss against the step $t$ on different models and datasets in Figure 2. The performance   \n294 for $\\epsilon_{1}=10^{-8}$ and $\\epsilon_{1}=10^{-5}$ is nearly identical to that for $\\epsilon_{1}=10^{-30}$ . Moreover, even a larger   \n295  alue of $10^{-3}$ achieves comparable training performance, though with a slower decrease in loss.   \n296  Notably, $\\epsilon_{1}=10^{-3}$ requires approximately the same number of steps $t\\approx20000)$ as $\\epsilon_{1}=10^{-30}$ to   \n297 achieve near-zero training loss. We conclude that Adafactor is not sensitive to the choice of $\\epsilon_{1}$ , and a   \n298  relatively large $\\epsilon_{1}$ can still lead to convergence, making the polynomial dependency $\\mathcal{O}(1/\\epsilon_{1})$ in our   \n299 convergence bounds acceptable. ", "page_idx": 7}, {"type": "text", "text": "3008.4 Report on Experiment 3 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "301 In this experiment, we explore the appropriate values of $\\alpha$ in Theorem 7.1 to achieve performance   \n302 comparable to the default setting of $d=1$ . As indicated by Theorem 7.1, a relatively small $\\alpha$ is   \n303 desirable for better dependency on $\\epsilon_{1}$ . We train models with $\\alpha$ set to 4, 6, 7, 8, and 9, keeping other   \n304 hyper-parameters at their default values. We also train models with the default $d=1$ setting as the   \n305 baseline. We plot the training loss against the steps in Figures 3 without step-size warm-up and 4   \n306  with step-size warm-up.   \n307  The results indicate that, for these values of $\\alpha$ , Adafactor achieves comparable_ or even better   \n308  convergence speed compared to the default threshold (represented by \"Baseline\"). The comparable   \n309  results to the \"Baseline\" in Figure 3 further suggest that the time-increasing $d_{k}$ in Theorem 7.1 plays a   \n310 role similar to that of the default setting, enhancing training stability even when the step-size warm-up   \n311is turned off. ", "page_idx": 7}, {"type": "image", "img_path": "bNVvcgfEwD/tmp/c0b67c75194bf9b682b95fb0c2e405ef829452f0c524f5dd5b3d09c3f3a62e53.jpg", "img_caption": ["(a) ResNet-20 on CIFAR-10 (c) ResNet-110 on CIFAR-100 ", "Figure 3: Training loss vs. steps on different models and datasets. We use step-size without warm-up technique and test under different $\\alpha$ "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "bNVvcgfEwD/tmp/e128ac5ad65d8c603fda20397b0d9e32f333bf3bdc222f2a6eabf9836bb41520.jpg", "img_caption": ["Figure 4: Training loss vs. steps on different models and datasets. We use step-size with warm-up technique by default and test under different $\\alpha$ ", "(a) ResNet-20 on CIFAR-10 (b) ResNet-20 on CIFAR-100 (c) ResNet-110 on CIFAR-100 "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "312 9 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "313 In this paper, we investigate the convergence behavior of Adafactor on non-convex smooth landscapes,   \n314  considering bounded stochastic gradients.  We introduce a new proxy step-size to decouple the   \n315  stochastic gradients from the unique adaptive step-size. Additionally, we use new estimations to   \n316  control the errors introduced by matrix factorization and update clipping in Adafactor.   \n317 Our findings reveal that full-batch Adafactor is capable of finding a stationary point, requiring   \n318 only a step-size $\\eta_{k}\\sim\\mathcal{O}(1/\\sqrt{k})$ and a second moment decay rate $\\beta_{2,k}\\,\\in\\,(0,1)$ , denoting a wide   \n319 range including the default setup. In the case of stochastic Adafactor without update clipping, the   \n320 convergence rate can achieve the optimal order $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ when $\\beta_{2,k}=1-1/k^{c},c=1$ . However,   \n321 performance deteriorates as $c$ decreases. This finding is supported by experimental results. We also   \n322 explore Adafactor with a time-increasing clipping threshold and derive similar convergence results.   \n323 The empirical results demonstrate that the new clipping threshold provides performance comparable   \n324  to the default constant setting.   \n325 Limitations  There are several limitations in our work that warrant further investigation. First,   \n326 the polynomial dependency on $\\epsilon_{1}$ in our convergence bounds may be further improved to a better   \n327 dependency, such as $\\log(1/\\epsilon_{1})$ . Second, although we provide convergence results for several variants   \n328 of Adafactor and demonstrate comparable performance to the original one in experiments, the   \n329 convergence bound for stochastic vanilla Adafactor remains unknown. Finally, our experimental   \n330 results primarily focus on traditional deep learning tasks due to our GPU limitations. It would be   \n331 beneficial to test the scalability of our theoretical results, e.g., on large language models. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "332 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "33 [1] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive   \n334 optimization. In Advances in Neural Information Processing Systems, 2019.   \n335 [2]  Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Wood  \n336 worth. Lower bounds for non-convex stochastic optimization. Mathematical Programming,   \n337 199(1-2):165-214, 2023.   \n338 [3]  Amit Attia and Tomer Koren. SGD with AdaGrad stepsizes: full adaptivity with high probability   \n339 to unknown parameters, unbounded gradients and affine variance. In International Conference   \n340 on Machine Learning, 2023.   \n341 [4] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine   \n342 learning. SIAM Review, 60(2):223-311, 2018.   \n343 [5]  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n344 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n345 few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901, 2020.   \n346 [6]  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam   \n347 Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:   \n348 Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1-   \n349 113, 2023.   \n350 [7] Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for RMSProp and   \n351 Adam in non-convex optimization and an empirical comparison to Nesterov acceleration. arXiv   \n352 preprint arXiv: 1807.06766, 2018.   \n353 [8]  Alexandre Dfossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence   \n354 proof of Adam and Adagrad. Transactions on Machine Learning Research, 2022.   \n355 [9] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning   \n356 and stochastic optimization. Journal of Machine Learning Research, 12(7):2121-2159, 2011.   \n357 [10] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai,   \n358 and Rachel Ward. The power of adaptivity in SGD: self-tuning step sizes with unbounded   \n359 gradients and affine variance. In Conference on Learning Theory, 2022.   \n360 [11] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis   \n361 for algorithms of the Adam family. In Annual Workshop on Optimization for Machine Learning,   \n362 2021.   \n363  [12] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor   \n364 optimization. In International Conference on Machine Learning, pages 1842-1850. PMLR,   \n365 2018.   \n366 [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image   \n367 recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n368 Recognition, 2016.   \n369 [14] Ali Kavis, Kfir Yehuda Levy, and Volkan Cevher. High probability bounds for a class of   \n370 nonconvex algorithms with AdaGrad stepsize. In International Conference on Learning Repre  \n371 sentations, 2022.   \n372 [15]  Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In International   \n373 Conference on Learning Representations, 2015.   \n374 [16] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.   \n375 2009.   \n376 [17] Bingrui Li, Jjianfei Chen, and Jun Zhu. Memory efficient optimizers with 4-bit states. Advances   \n377 in Neural Information Processing Systems, 36, 2024.   \n378  [18] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of Adam under relaxed   \n379 assumptions. In Advances in Neural Information Processing Systems, 2023.   \n380 [19]  Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with   \n381 adaptive stepsizes. In International Conference on Artificial Intelligence and Statistics, 2019.   \n382 [20] Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momentum.   \n383 In Workshop on International Conference on Machine Learning, 2020.   \n384 [21]  Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Nguyen. High probability   \n385 convergence of stochastic gradient methods. In International Conference on Machine Learning,   \n386 2023.   \n387 [22]  Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International   \n388 Conference on Learning Representations, 2019.   \n389 [23] Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, and Yang You. CAME:   \n390 Confidence-guided adaptive memory efficient optimization. In Proceedings of the 6lst Annual   \n391 Meeting of the Association for Computational Linguistics, 2023.   \n392 [24] Colin Raffl, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,   \n393 Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified   \n394 text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.   \n395  [25] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,   \n396 and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts   \n397 layer. In International Conference on Learning Representations, 2017.   \n398 [26] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory   \n399 cost. In International Conference on Machine Learning, 2018.   \n400 [27] Li Shen, Congliang Chen, Fangyu Zou, Zequn Jie, Ju Sun, and Wei Liu. A unified analysis   \n401 of AdaGrad with weighted aggregation and momentum acceleration. IEEE Transactions on   \n402 Neural Networks and Learning Systems, 2023.   \n403  [28] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. RMSProp converges with proper   \n404 hyper-parameter. In International Conference on Learning Representations, 2020.   \n405 [29] Matthew Streeter and H Brendan McMahan. Less regret via online conditioning. arXiv preprint   \n406 arXiv:1002.4862, 2010.   \n407 [30] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running   \n408 average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.   \n409 [31] Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap   \n410 between the upper bound and lower bound of Adam's iteration complexity. In Advances in   \n411 Neural Information Processing Systems, 2023.   \n412 [32] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: sharp convergence over   \n413 nonconvex landscapes. Journal of Machine Learning Research, 21(1):9047-9076, 2020.   \n414 [33] Junchi Yang, Xiang Li, and Niao He. Nest your adaptive algorithm for parameter-agnostic   \n415 nonconvex minimax optimization. In Advances in Neural Information Processing Systems,   \n416 2022.   \n417 [34] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive   \n418 methods for nonconvex optimization. In Advances in Neural Information Processing Systems,   \n419 2018.   \n420 [35] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,   \n421 2012.   \n422[36] Yushun Zhang,Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can con  \n423 verge without any modification on update rules. In Advances in Neural Information Processing   \n424 Systems, 2022.   \n425  [37] Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger Kolker, Zhefeng Wang, and   \n426 Xiaoming Yuan. Adapprox: Adaptive approximation in adam optimization via randomized   \n427 low-rank matrices. arXiv preprint arXiv:2403.14958, 2024.   \n428  [38] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,   \n429 Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv   \n430 preprint arXiv:2303.18223, 2023.   \n431 [39] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for   \n432 convergences of Adam and RMSProp. In Proceedings of the IEEE Conference on Computer   \n433 Vision and Pattern Recognition, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "434  A  Proof detail for full-batch case ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "435 We first provide the full-batch Adafactor as follows. The only difference to Algorithm (1) is the   \n436 replacement of stochastic gradient by deterministic gradient $\\nabla^{\\aa}f({\\boldsymbol{X}}_{k})$ at each iteration. ", "page_idx": 12}, {"type": "text", "text": "Algorithm 2 Full-batch Adafactor ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Input: Initialization point $X_{1}\\in\\mathbb{R}^{n\\times m}$ \uff0c ${\\cal R}_{0}={\\bf0}_{n},C_{0}={\\bf0}_{m}^{\\top}$ , relative step-sizes $\\{\\rho_{k}\\}_{k\\ge1}$ , decay   \nrate $\\{\\beta_{2,k}\\}_{k\\geq1}\\in[0,\\bar{1})$ , regularization constants $\\epsilon_{1},\\epsilon_{2}>0$ , clipping threshold $d$   \nfor $k=1,\\cdots,T$ do ${\\bar{G}}_{k}=\\nabla f(X_{k})$ .\uff0c ${\\bar{R}}_{k}=\\beta_{2,k}{\\bar{R}}_{k-1}+(1-\\beta_{2,k})({\\bar{G}}_{k}\\odot{\\bar{G}}_{k}+\\epsilon_{1}\\mathbf{1}_{n}\\mathbf{1}_{m}^{\\top})\\mathbf{1}_{m};$ $\\bar{C}_{k}=\\beta_{2,k}\\bar{C}_{k-1}+(1-\\beta_{2,k})\\mathbf{1}_{n}^{\\top}\\big(\\bar{G}_{k}\\odot\\bar{G}_{k}+\\epsilon_{1}\\mathbf{1}_{n}\\mathbf{1}_{m}^{\\top}\\big);$ $\\bar{W}_{k}=(\\bar{R}_{k}\\bar{C}_{k})/\\mathbf{1}_{n}^{\\top}\\bar{R}_{k}$ $\\bar{U}_{k}=\\bar{G}_{k}/\\sqrt{\\bar{W}_{k}}$ mk = max{2, RMS(Xk)}pk / max{1, RMS(Uk)/d}; $X_{k+1}=X_{k}-\\hat{\\eta}_{k}\\cdot\\bar{G}_{k}/\\sqrt{\\bar{W}_{k}}$   \nend for ", "page_idx": 12}, {"type": "text", "text": "437 Then, we provide the detailed version of Theorem 5.1 as follows. ", "page_idx": 12}, {"type": "text", "text": "438Theorem A.1. Let $\\{X_{k}\\}_{k\\ge1}$ be generated by Algorithm 2. If Assumptions (A1), (A2) hold,   \n439 $\\|\\nabla f(\\pmb{X}_{k})\\|_{F}\\leq G,\\forall\\dot{k}\\geq\\dot{1}$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\rho_{k}=\\rho_{0}/\\sqrt{k},\\quad0<\\beta_{2,k}<1,\\quad\\forall k\\geq1,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "440 for some positive constant ,then for any $T\\geq1$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z m s t a n t\\ \\rho_{0},\\ i n e n\\,J\\ o r\\ a n y\\ a\\ \\zeta\\ ^{2}+1,}\\\\ &{\\operatorname*{min}_{k\\in\\left[T\\right]}\\|\\nabla f(\\mathbf{X}_{k})\\|_{F}^{2}\\leq\\frac{A_{0}A_{1}\\left(f\\left(\\mathbf{X}_{1}\\right)-f^{*}+\\Delta_{0}^{2}\\log T+\\Delta_{0}^{2}\\right)}{\\sqrt{T}},}\\\\ &{\\operatorname*{min}_{k\\in\\left[T\\right]}\\|\\nabla f(\\mathbf{X}_{k})\\|_{F}^{2}\\leq\\frac{A_{0}A_{1}^{\\prime}\\left(f\\left(\\mathbf{X}_{1}\\right)-f^{*}+\\tilde{\\Delta}_{0}^{2}\\log T+\\Delta_{0}^{2}\\right)}{\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "441 where we define ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{min}}=\\operatorname*{min}_{k\\in[T]}\\|{\\pmb X}_{k}\\|_{\\infty},\\quad\\Theta_{\\mathrm{max}}=\\operatorname*{max}_{k\\in[T]}\\|{\\pmb X}_{k}\\|_{\\infty},\\quad\\mathcal{G}=G^{2}+m n\\epsilon_{1},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "442and the other constant parameters are given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{0}^{2}=\\cfrac{L d^{2}m n\\left(\\epsilon_{2}+\\Theta_{\\operatorname*{max}}\\right)^{2}\\rho_{0}^{2}}{2},\\quad\\tilde{\\Delta}_{0}^{2}=\\cfrac{L G^{2}\\mathcal{G}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})^{2}\\rho_{0}^{2}}{2m n\\epsilon_{1}^{2}(1-\\beta_{2,1})^{2}},}\\\\ &{A_{0}=\\cfrac{\\operatorname*{max}\\Big\\{1,\\cfrac{G\\sqrt{g}}{d\\epsilon_{1}m n(1-\\beta_{2,1})}\\Big\\}}{\\rho_{0}\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}},A_{1}=\\sqrt{G^{4}+G^{2}(m+n)\\epsilon_{1}+m n\\epsilon_{1}^{2}},}\\\\ &{A_{1}^{\\prime}=\\sqrt{2\\left(\\cfrac{G^{4}}{m n\\epsilon_{1}}+G^{2}+\\epsilon_{1}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "443 A.1 Preliminary ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "$\\bar{G}_{k,\\epsilon_{1}}^{2}=\\bar{G}_{k}\\odot\\bar{G}_{k}+\\epsilon_{1}\\mathbf{1}_{n}\\mathbf{1}_{m}^{\\top}$ $\\bar{V}_{k}=$ $\\left(\\bar{v}_{i j}^{(k)}\\right)_{i j}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{V}_{0}=\\mathbf{0}_{n\\times m},\\quad\\bar{V}_{k}=\\beta_{2,k}\\bar{V}_{k-1}+(1-\\beta_{2,k})\\bar{G}_{k,\\epsilon_{1}}^{2},\\quad k\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "$\\bar{G}_{k}=\\left(\\bar{g}_{i j}^{(k)}\\right)_{i j},R_{\\bar{V}_{k}}^{(i)},C_{\\bar{V}_{k}}^{(j)}$ , R, C) and Sv, be the -th row sum, -th column 447 sum and the coordinate sum of $\\bar{V}_{k}$ respectively. The same definition principal is applied to the 448 notation $R_{\\bar{G}_{k,\\epsilon_{1}}^{2}}^{(i)}$ and c%) $C_{\\bar{G}_{k}^{2},}^{(j)}$ . We also use wig ,uj, $\\bar{w}_{i j}^{(k)},\\bar{v}_{i j}^{(k)},\\bar{u}_{i j}^{(k)}$ to denote the coordinates of $\\bar{W}_{k},\\bar{V}_{k},\\bar{U}_{k}$ 449 in Algorithm 2 respectively. We also define values $\\mathcal{G}_{1},\\mathcal{G}_{2},\\mathcal{G}$ as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{G}_{1}=G^{2}+m\\epsilon_{1},\\quad\\mathcal{G}_{2}=G^{2}+n\\epsilon_{1},\\quad\\mathcal{G}=G^{2}+m n\\epsilon_{1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "450 A.2 Technical lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "451  Following the descent lemma for a $L$ -smooth objective function $f$ , we derive that ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\pmb{Y})\\leq f(\\pmb{X})+\\langle\\nabla f(\\pmb{X}),\\pmb{Y}-\\pmb{X}\\rangle+\\frac{L}{2}\\|\\pmb{Y}-\\pmb{X}\\|_{F}^{2},\\quad\\forall\\pmb{X},\\pmb{Y}\\in\\mathbb{R}^{n\\times m}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "452 In the following, we will provide some necessary technical lemmas. ", "page_idx": 13}, {"type": "text", "text": "453  Lemma A.1. Let $\\beta_{2,k}\\in(0,1)$ and $\\Gamma_{k}$ be defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Gamma_{0}=0,\\quad\\Gamma_{k}=\\beta_{2,k}\\Gamma_{k-1}+(1-\\beta_{2,k}),\\quad\\forall k\\geq1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "454Then, $(1-\\beta_{2,1})\\le\\Gamma_{k}\\le1,\\forall k\\ge1.$ ", "page_idx": 13}, {"type": "text", "text": "455  Proof. We could prove the result by induction. Since $\\Gamma_{0}=0$ , it's easy to derive that $(1-\\beta_{2,1})=$   \n456 $\\Gamma_{1}\\leq1$ . Suppose that for any $j\\in[k-1]$ $(1-\\beta_{2,1})\\leq\\Gamma_{j}\\leq1$ .Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Gamma_{k}\\geq\\beta_{2,k}(1-\\beta_{2,1})+(1-\\beta_{2,k})\\geq1-\\beta_{2,1},\\quad\\Gamma_{k}\\leq\\beta_{2,k}+(1-\\beta_{2,k})\\leq1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "457 The induction is then complete. ", "page_idx": 13}, {"type": "text", "text": "458Lemma A.2. Let $\\bar{V}_{k}$ be defined in (12). For any $k\\geq0$ ,it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{R}_{k}=\\bar{V}_{k}\\mathbf1_{m},\\quad\\bar{C}_{k}=\\mathbf1_{n}^{\\top}\\bar{V}_{k},\\quad S_{\\bar{V}_{k}}=\\mathbf1_{n}^{\\top}\\bar{R}_{k}=\\mathbf1_{n}^{\\top}\\bar{V}_{k}\\mathbf1_{m}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "459As a consequence, ", "page_idx": 13}, {"type": "equation", "text": "$$\nR_{\\overline{{V}}_{k}}^{(i)}=\\beta_{2,k}R_{\\overline{{V}}_{k-1}}^{(i)}+(1-\\beta_{2,k})R_{\\overline{{G}}_{k,\\epsilon_{1}}^{2}}^{(i)},\\quad C_{\\overline{{V}}_{k}}^{(j)}=\\beta_{2,k}C_{\\overline{{V}}_{k-1}}^{(j)}+(1-\\beta_{2,k})C_{\\overline{{G}}_{k,\\epsilon_{1}}^{2}}^{(j)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "460 Proof. Note that $\\bar{R}_{0}\\,=\\,\\bar{V}_{0}\\mathbf{1}_{m}\\,=\\,\\mathbf{0}_{n}$ and $\\bar{C}_{0}\\,=\\,{\\bf1}_{n}^{\\top}\\bar{V}_{0}\\,=\\,{\\bf0}_{m}^{\\top}$ . Suppose that for any $j\\leq k-1$   \n461 $\\bar{R}_{j}=\\bar{V}_{j}\\mathbf{1}_{m},\\bar{C}_{j}=\\mathbf{1}_{n}^{\\top}\\bar{V}_{j}$ . Then using the updated rule in Algorithm 2 and (12), ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{R}_{k}=\\beta_{2,k}\\bar{R}_{k-1}+(1-\\beta_{2,k})\\bar{G}_{k,\\epsilon_{1}}^{2}{\\bf1}_{m}=\\left(\\beta_{2,k}\\bar{V}_{k-1}+(1-\\beta_{2,k})\\bar{G}_{k,\\epsilon_{1}}^{2}\\right){\\bf1}_{m}=\\bar{V}_{k}{\\bf1}_{m},}\\\\ &{\\bar{C}_{k}=\\beta_{2,k}\\bar{C}_{k-1}+(1-\\beta_{2,k}){\\bf1}_{n}^{\\top}\\bar{G}_{k,\\epsilon_{1}}^{2}={\\bf1}_{n}^{\\top}\\left(\\beta_{2,k}\\bar{V}_{k-1}+(1-\\beta_{2,k})\\bar{G}_{k,\\epsilon_{1}}^{2}\\right)={\\bf1}_{n}^{\\top}\\bar{V}_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "462Since $S_{\\bar{V}_{k}}$ represents the coordinate sum of $\\bar{V}_{k}$ , we could derive that ", "page_idx": 13}, {"type": "equation", "text": "$$\nS_{\\bar{V}_{k}}=\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\bar{v}_{i j}^{(k)}=\\mathbf{1}_{n}^{\\top}\\bar{R}_{k}=\\mathbf{1}_{n}^{\\top}\\bar{V}_{k}\\mathbf{1}_{m}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "463  Since R() denotes the $i$ -th row sum of $\\bar{V}_{k}$ , it's the $i$ -th cordinate of $\\bar{R}_{k}$ .Hence, for ach coordinate   \n464 Of $\\bar{R}_{k}$ , using (16), ", "page_idx": 13}, {"type": "equation", "text": "$$\nR_{\\bar{\\cal V}_{k}}^{(i)}=\\beta_{2,k}R_{\\bar{\\cal V}_{k-1}}^{(i)}+(1-\\beta_{2,k})R_{\\bar{\\cal G}_{k,\\epsilon_{1}}^{2}}^{(i)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "45Smilaly, we could derive the results related t ", "page_idx": 13}, {"type": "text", "text": "466 Lemma A.3. Following the parameter setting in (3), for any $i\\in[n],j\\in[m],k\\geq1,$ it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\nR_{\\bar{V}_{k}}^{(i)}\\in[m\\epsilon_{1}(1-\\beta_{2,1}),\\mathcal{G}_{1}],\\quad C_{\\bar{V}_{k}}^{(j)}\\in[n\\epsilon_{1}(1-\\beta_{2,1}),\\mathcal{G}_{2}],\\quad S_{\\bar{V}_{k}}\\in[m n\\epsilon_{1}(1-\\beta_{2,1}),\\mathcal{G}].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "467  Proof. Recalling the definition of $\\bar{V}_{k}$ in (12) and $\\|\\nabla f(\\pmb{X}_{k})\\|_{F}\\leq G,\\forall k\\geq1$ , we derive that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle S_{\\bar{V}_{k}}=\\sum_{i=1}^{n}\\displaystyle\\sum_{j=1}^{m}\\bar{v}_{i j}^{(k)}=\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\sum_{p=1}^{k}(1-\\beta_{2,p})\\left(\\left(\\bar{g}_{i j}^{(p)}\\right)^{2}+\\epsilon_{1}\\right)\\left(\\prod_{l=p+1}^{k}\\beta_{2,l}\\right)}\\\\ {\\displaystyle\\qquad\\leq\\displaystyle\\sum_{p=1}^{k}(1-\\beta_{2,p})\\left(\\prod_{l=p+1}^{k}\\beta_{2,l}\\right)\\|\\bar{G}_{p}\\|_{F}^{2}+\\Gamma_{k}m n\\epsilon_{1}\\leq G^{2}\\Gamma_{k}+m n\\epsilon_{1}\\leq\\mathcal{G},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "468 where the last inequality comes from Lemma A.1. Following (18) and Lemma A.1, we also derive   \n469that ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{\\bar{V}_{k}}\\geq m n\\epsilon_{1}\\Gamma_{k}\\geq m n\\epsilon_{1}(1-\\beta_{2,1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "470We also derive the upper bounds for R) and C(i) as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{\\overline{{V}}_{k}}^{(i)}=\\sum_{j=1}^{m}\\bar{v}_{i j}^{(k)}}\\leq\\sum_{p=1}^{k}(1-\\beta_{2,p})\\left(\\prod_{l=p+1}^{k}\\beta_{2,l}\\right)\\|\\bar{G}_{p}\\|_{F}^{2}+\\Gamma_{k}m\\epsilon_{1}\\leq G^{2}\\Gamma_{k}+m\\epsilon_{1}\\leq\\mathcal{G}_{1},}}\\\\ {{\\displaystyle C_{V_{k}}^{(j)}=\\sum_{i=1}^{n}\\bar{v}_{i j}^{(k)}\\leq\\sum_{p=1}^{k}(1-\\beta_{2,p})\\left(\\prod_{l=p+1}^{k}\\beta_{2,l}\\right)\\|\\bar{G}_{p}\\|_{F}^{2}+\\Gamma_{k}n\\epsilon_{1}\\leq G^{2}\\Gamma_{k}+n\\epsilon_{1}\\leq\\mathcal{G}_{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "471 Similarly, the lower bound could be derived by ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{\\bar{V}_{k}}^{(i)}\\geq m\\epsilon_{1}\\Gamma_{k}\\geq m\\epsilon_{1}(1-\\beta_{2,1}),\\quad C_{\\bar{V}_{k}}^{(j)}\\geq n\\epsilon_{1}\\Gamma_{k}\\geq n\\epsilon_{1}(1-\\beta_{2,1}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "472 ", "page_idx": 14}, {"type": "text", "text": "473  A.3 Proof of Theorem A.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "474  Now we move to prove the main result. Using (14) and the updated rule in Algorithm 2, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f({\\mathbf{X}}_{k+1})\\leq f({\\mathbf{X}}_{k})+\\langle\\bar{G}_{k},{\\mathbf{X}}_{k+1}-{\\mathbf{X}}_{k}\\rangle+\\frac{L}{2}\\|{\\mathbf{X}}_{k+1}-{\\mathbf{X}}_{k}\\|_{F}^{2}}\\\\ &{\\quad\\quad=f({\\mathbf{X}}_{k})-\\hat{\\eta}_{k}\\left\\langle\\bar{G}_{k},\\frac{\\bar{G}_{k}}{\\sqrt{\\bar{W}_{k}}}\\right\\rangle+\\frac{L\\hat{\\eta}_{k}^{2}}{2}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{\\bar{W}_{k}}}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "475  We then re-arrange the order, sum up both sides over $k\\ \\in\\ [t]$ and apply $f(X_{t+1})\\ \\geq\\ f^{*}$ from   \n476  Assumption (A2) to get, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{t}\\hat{\\eta}_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{\\bar{W}_{k}}}\\right\\rVert_{F}^{2}\\le f(X_{1})-f^{*}+\\underbrace{\\frac{L}{2}\\sum_{k=1}^{t}\\hat{\\eta}_{k}^{2}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{\\bar{W}_{k}}}\\right\\rVert_{F}^{2}}_{\\mathrm{(b)}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "477 Since $\\Theta_{\\mathrm{min}}\\leq\\|X_{k}\\|_{\\infty}\\leq\\Theta_{\\mathrm{max}}$ , we have $\\Theta_{\\operatorname*{min}}\\leq\\mathrm{RMS}(X_{k})\\leq\\Theta_{\\operatorname*{max}}$ for any $k\\geq1$ . Hence, using   \n478 $\\hat{\\eta}_{k}$ defined in Algorithm 2, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\eta}_{k}=\\frac{\\operatorname*{max}\\{\\epsilon_{2},\\mathrm{RMS}(X_{k})\\}\\rho_{k}}{\\operatorname*{max}\\big\\{1,\\|\\bar{U}_{k}\\|_{F}/(d\\sqrt{m n})\\big\\}}\\leq(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{k}\\operatorname*{min}\\left\\{1,\\frac{d\\sqrt{m n}}{\\|\\bar{U}_{k}\\|_{F}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "479Using (21), $\\bar{U}_{k}=\\bar{G}_{k}/\\sqrt{\\bar{W}_{k}}$ $\\Delta_{0}$ in (11) and $\\rho_{k}=\\rho_{0}/\\sqrt{k}$ , we thus derive that ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mathbf{b})\\leq\\frac{L d^{2}m n(\\epsilon_{2}+\\Theta_{\\mathrm{max}})^{2}}{2}\\sum_{k=1}^{t}\\rho_{k}^{2}\\cdot\\frac{\\|\\bar{U}_{k}\\|_{F}^{2}}{\\|\\bar{U}_{k}\\|_{F}^{2}}=\\Delta_{0}^{2}\\sum_{k=1}^{t}\\frac{1}{k}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "480 To lower bound (a), we first discuss the maximum operator inside $\\hat{\\eta}_{k}$ . Let ", "page_idx": 14}, {"type": "equation", "text": "$$\nE_{1}=\\left\\{k\\in[t]\\mid\\|\\bar{U}_{k}\\|_{F}\\geq d\\sqrt{m n}\\right\\},\\quad E_{2}=\\left\\{k\\in[t]\\mid\\|\\bar{U}_{k}\\|_{F}\\leq d\\sqrt{m n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "481  When $k\\in E_{1}$ , since $\\|X_{k}\\|_{\\infty}\\geq\\Theta_{\\mathrm{min}}$ , it derives that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\eta}_{k}\\geq\\frac{d\\sqrt{m n}\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\mathrm{min}}\\}\\rho_{k}}{\\|\\bar{U}_{k}\\|_{F}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "482 Using Lemma A.2, we fist erive that $\\bar{w}_{i j}^{(k)}=(R_{\\bar{V}_{k}}^{(i)}C_{\\bar{V}_{k}}^{(j)})/S_{\\bar{V}_{k}}$ RC)/Sv Then, applying Lemma A3 and   \n483 $\\|\\nabla f({\\mathbf{\\boldsymbol{X}}}_{k})\\|_{F}\\leq G$ , we could upper bound $\\|\\bar{U}_{k}\\|_{F}^{2}$ as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\bar{U}_{k}\\|_{F}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\frac{\\left(\\bar{g}_{i j}^{(k)}\\right)^{2}S_{\\bar{V}_{k}}}{R_{\\bar{V}_{k}}^{(i)}C_{\\bar{V}_{k}}^{(j)}}\\le\\frac{\\|\\bar{G}_{k}\\|_{F}^{2}\\mathcal{G}}{m n\\epsilon_{1}^{2}(1-\\beta_{2,1})^{2}}\\le\\frac{G^{2}\\mathcal{G}}{m n\\epsilon_{1}^{2}(1-\\beta_{2,1})^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "484 Hence, combining with (23) and (24), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k\\in E_{1}}\\hat{\\eta}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{\\bar{W}_{k}}}\\right\\|_{F}^{2}\\geq d\\sqrt{m n}\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}\\sum_{k\\in E_{1}}\\frac{\\rho_{k}}{\\|\\bar{U}_{k}\\|_{F}}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{\\bar{W}_{k}}}\\right\\|_{F}^{2}}}\\\\ &{}&{\\geq\\frac{d\\epsilon_{1}m n\\left(1-\\beta_{2,1}\\right)\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}}{G\\sqrt{\\bar{\\mathcal{G}}}}\\sum_{k\\in E_{1}}\\rho_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{\\bar{W}_{k}}}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "485 When $k\\in E_{2}$ , we obtain that $\\hat{\\eta}_{k}=\\operatorname*{max}\\{\\epsilon_{2},\\mathrm{RMS}(\\mathbf{\\boldsymbol{X}}_{k})\\}\\rho_{k}\\geq\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}\\rho_{k}$ and thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k\\in E_{2}}\\hat{\\eta}_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt[4]{\\bar{W}_{k}}}\\right\\rVert_{F}^{2}\\ge\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}\\sum_{k\\in E_{2}}\\rho_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt[4]{\\bar{W}_{k}}}\\right\\rVert_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "486  Combining with (25) and (26), we derive that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{\\Sigma}(\\mathbf{a})\\geq\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}\\operatorname*{min}\\left\\{1,\\frac{d\\epsilon_{1}m n(1-\\beta_{2,1})}{G\\sqrt{g}}\\right\\}\\sum_{k=1}^{t}\\rho_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{\\bar{W}_{k}}}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "487 We also derive from Lemma A.2 and Lemma A.3 that for any $i\\in[n],j\\in[m]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{w}_{i j}^{(k)}=\\frac{R_{\\bar{V}_{k}}^{(i)}C_{\\bar{V}_{k}}^{(j)}}{S_{\\bar{V}_{k}}}\\leq\\frac{R_{\\bar{V}_{k}}^{(i)}C_{\\bar{V}_{k}}^{(j)}}{\\sqrt{R_{\\bar{V}_{k}}^{(i)}C_{\\bar{V}_{k}}^{(j)}}}\\leq\\sqrt{R_{\\bar{V}_{k}}^{(i)}C_{\\bar{V}_{k}}^{(j)}}\\leq\\sqrt{\\mathcal{G}_{1}\\mathcal{G}_{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "488 Using (28), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\bar{\\mathbf{G}}_{k}}{\\sqrt[4]{\\bar{\\mathbf{W}}_{k}}}\\right\\|_{F}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\frac{\\left(\\bar{g}_{i j}^{(k)}\\right)^{2}}{\\sqrt{\\bar{w}_{i j}^{(k)}}}\\geq\\frac{\\|\\bar{\\mathbf{G}}_{k}\\|_{F}^{2}}{\\sqrt{\\mathcal{G}_{1}\\mathcal{G}_{2}}}=\\frac{\\|\\bar{\\mathbf{G}}_{k}\\|_{F}^{2}}{A_{1}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "489where $A_{1}$ has been defined in (11). Plugging (29) into (27), we derive that ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\bf\\Xi}({\\bf a})\\geq\\frac{\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\mathrm{min}}\\}}{A_{1}}\\operatorname*{min}\\bigg\\{1,\\frac{d\\epsilon_{1}m n(1-\\beta_{2,1})}{G\\sqrt{g}}\\bigg\\}\\sum_{k=1}^{t}\\rho_{k}\\|\\bar{G}_{k}\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "490Plugging (22) and (30) into (20), and using $\\rho_{k}=\\rho_{0}/\\sqrt{k}$ , we thus derive that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[t]}\\|\\bar{G}_{k}\\|_{F}^{2}\\sum_{k=1}^{t}\\frac{1}{\\sqrt{k}}\\leq\\sum_{k=1}^{t}\\frac{\\rho_{k}\\|\\bar{G}_{k}\\|_{F}^{2}}{\\rho_{0}}\\leq A_{0}A_{1}\\left(f(X_{1})-f^{*}+\\Delta_{0}^{2}\\sum_{k=1}^{t}\\frac{1}{k}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "491 where $A_{0}$ is given in (11). Moreover, we have the following results, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{t}{\\frac{1}{k}}\\leq1+\\int_{1}^{t}{\\frac{1}{x}}d x=1+\\log t,\\quad\\sum_{k=1}^{t}{\\frac{1}{\\sqrt{k}}}\\geq{\\sqrt{t}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "492 We thus derive the first desired result in (9) as follows, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[t]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{A_{0}A_{1}}{\\sqrt{t}}\\left(f(X_{1})-f^{*}+\\Delta_{0}^{2}+\\Delta_{0}^{2}\\log t\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "493Avoiding the curse of dimension _To derive a free-dimension numerator bound, we first derive   \n494from (21) and (24) with $\\rho_{k}=\\rho_{0}/\\sqrt{k}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\mathbf{b})\\leq\\frac{L(\\epsilon_{2}+\\Theta_{\\mathrm{max}})^{2}}{2}\\sum_{k=1}^{t}\\rho_{k}^{2}\\|\\bar{U}_{k}\\|_{F}^{2}\\leq\\frac{L G^{2}\\mathcal{G}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})^{2}}{2m n\\epsilon_{1}^{2}(1-\\beta_{2,1})^{2}}\\sum_{k=1}^{t}\\rho_{k}^{2}=\\tilde{\\Delta}_{0}^{2}\\sum_{k=1}^{t}\\frac{1}{k},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "495where $\\tilde{\\Delta}_{0}$ has been defined in (11). In addition, we derive from Lemma A.2, Lemma A.3 and (13)   \n496that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{w}_{i j}^{(k)}=\\frac{R_{\\bar{V}_{k}}^{(i)}C_{\\bar{V}_{k}}^{(j)}}{S_{\\bar{V}_{k}}}\\leq\\frac{2\\mathcal{G}_{1}\\mathcal{G}_{2}}{m n\\epsilon_{1}}\\leq2\\left(\\frac{G^{4}}{m n\\epsilon_{1}}+G^{2}+\\epsilon_{1}\\right)=(A_{1}^{\\prime})^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "497where we use $m+n\\leq m n$ and $A_{1}^{\\prime}$ in (11). Thereby, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{\\bar{W}_{k}}}\\right\\|_{F}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\frac{\\left(\\bar{g}_{i j}^{(k)}\\right)^{2}}{\\sqrt{\\bar{w}_{i j}^{(k)}}}\\geq\\frac{\\|\\bar{G}_{k}\\|_{F}^{2}}{A_{1}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "498  Combining with (27), we thus derive that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{\\Xi}(\\mathbf{a})\\geq\\frac{\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}}{A_{1}^{\\prime}}\\operatorname*{min}\\left\\{1,\\frac{d\\epsilon_{1}m n(1-\\beta_{2,1})}{G\\sqrt{g}}\\right\\}\\sum_{k=1}^{t}\\rho_{k}\\|\\bar{G}_{k}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "499Plugging (33) and (35) into (20), and using $\\rho_{k}=\\rho_{0}/\\sqrt{k}$ , we derive that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[t]}\\|\\bar{G}_{k}\\|_{F}^{2}\\sum_{k=1}^{t}\\frac{1}{\\sqrt{k}}\\leq\\sum_{k=1}^{t}\\frac{\\rho_{k}\\|\\bar{G}_{k}\\|_{F}^{2}}{\\rho_{0}}\\leq A_{0}A_{1}^{\\prime}\\left(f(X_{1})-f^{*}+\\tilde{\\Delta}_{0}^{2}\\sum_{k=1}^{t}\\frac{1}{k}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "500where $A_{0}$ has been defined in (11). Using (31), we derive the second desired result in (9). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[t]}\\|\\bar{\\boldsymbol{G}}_{k}\\|_{F}^{2}\\leq\\frac{A_{0}A_{1}^{\\prime}}{\\sqrt{t}}\\left(f(\\boldsymbol{X}_{1})-f^{*}+\\tilde{\\Delta}_{0}^{2}+\\tilde{\\Delta}_{0}^{2}\\log t\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "501 B Proof detail for stochastic Adafactor without update clipping ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "502We first provide the detailed version of Theorem 6.1. ", "page_idx": 16}, {"type": "text", "text": "503 Theorem B.1 (Formal statement of Theorem $6.1$ ).Let $\\{X_{k}\\}_{k\\ge1}$ be generated by Algorithm 1 without   \n504  update clipping where $\\eta_{k}$ is given by (5) for each $k\\geq1$ If Assumptions (A1)-(A4) hold, and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\beta_{2,1}=1/2,\\quad\\rho_{1}=\\rho_{0},}\\\\ {\\beta_{2,k}=1-1/k^{c},\\quad\\rho_{k}=\\rho_{0}/\\sqrt{k},\\quad\\forall k\\ge2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "505 for some constants $1/2\\,\\leq\\,c\\,\\leq\\,1,\\rho_{0}\\,>\\,0_{}$ then for any $T\\,\\geq\\,1,\\delta\\,\\in\\,(0,1).$ we have the following   \n506 results.   \n507When $c=1$ ,with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+C_{2}\\log T+C_{2}+C_{3}\\right),}\\\\ &{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}^{\\prime}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+(C_{2}^{\\prime}+C_{3}^{\\prime})\\log T+C_{2}^{\\prime}+C_{3}^{\\prime}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "508 When $1/2\\le c<1$ , with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+\\frac{C_{2}}{1-c}\\cdot T^{1-c}+C_{2}+C_{3}\\right),}\\\\ &{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+\\frac{2C_{2}^{\\prime}}{1-c}\\cdot T^{1-c}+C_{3}^{\\prime}\\log T+C_{2}^{\\prime}+C_{3}^{\\prime}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "509 Here, $\\Theta_{\\mathrm{min}},\\Theta_{\\mathrm{max}}$ and $\\mathcal{G}$ are as in (10), and ", "page_idx": 16}, {"type": "equation", "text": "$$\nC_{1}=f({\\cal X}_{1})-f^{*}+\\frac{24G^{2}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0}}{\\sqrt{\\epsilon_{1}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "510The $C_{0},C_{2},C_{3}$ are constants defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{0}=\\cfrac{2\\sqrt{2\\mathcal{G}}}{\\rho_{0}\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}},\\quad C_{3}=\\cfrac{C_{2}}{4}\\log\\left(2+\\cfrac{2G^{2}}{\\epsilon_{1}}\\right),}\\\\ &{C_{2}=\\cfrac{32m n\\mathcal{G}^{\\frac{3}{2}}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}}{\\operatorname*{max}\\{m,n\\}\\epsilon_{1}}+\\frac{4L m n\\mathcal{G}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})^{2}\\rho_{0}^{2}}{\\operatorname*{max}\\{m,n\\}\\epsilon_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "511The $C_{0}^{\\prime},C_{2}^{\\prime},C_{3}^{\\prime}$ are positive constants (that could be further upper bounded by constants independent   \n512from $m,n,$ ,definedby ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma_{0}^{\\prime}=\\frac{2\\sqrt{2\\left(\\frac{G^{2}}{m n\\epsilon_{1}}+G+\\epsilon_{1}\\right)}}{\\rho_{0}\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}},C_{2}^{\\prime}=4G_{3}(G_{1}+G_{2})(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0},C_{3}^{\\prime}=\\frac{L G_{3}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})^{2}\\rho_{0}^{2}}{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "513 and $G_{1},G_{2},G_{3}$ are given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{1}=\\sqrt{6\\left(\\frac{G^{4}}{m n\\epsilon_{1}}+G^{2}+\\epsilon_{1}\\right)},\\quad G_{3}=\\frac{4\\left(G^{4}+G^{2}m n\\epsilon_{1}\\right)}{m n\\epsilon_{1}^{2}},}\\\\ &{G_{2}=2\\left(\\frac{G^{3}}{m n\\epsilon_{1}}+\\frac{2G^{2}}{\\sqrt{m n\\epsilon_{1}}}+\\frac{G}{\\sqrt{m n}}+G+\\sqrt{\\epsilon_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "514  Calculation of hyper-parameter dependency   To derive a free dimension bound, we shall use the   \n515  convergence bounds in (38) and (40). From (43), it's easy to show that $m,n$ could only exist in the   \n516denominator of $C_{0}^{\\prime},C_{2}^{\\prime},C_{3}^{\\prime}$ , which could avoid the curse of dimension.   \n517   To calculate the dependency of $\\epsilon_{1}$ , we first show that its dependency in coefficients $C_{0},C_{1},C_{2},C_{3}$ as   \n518  follows, based on the assumption that $0<\\epsilon_{1}<1$ \uff0c ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{0}\\sim\\mathcal{O}\\left(1\\right),\\quad C_{1}\\sim\\mathcal{O}\\left(1/\\sqrt{\\epsilon_{1}}\\right),\\quad C_{2}\\sim\\mathcal{O}\\left(1/\\epsilon_{1}\\right),\\quad C_{3}\\sim\\mathcal{O}\\left(C_{2}\\log(1/\\epsilon_{1})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "519  Thereby, with the convergence bounds in (37) and (39), it's easy to show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\mathcal{O}\\left(\\epsilon_{1}^{-1}\\log(1/\\epsilon_{1})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "520Proposition B.1. Following the same assumptions and setings in Theorem 6.1, then with probability   \n521atleast $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}\\sum_{k=1}^{T}\\frac{1}{k^{c}}+C_{3}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "522and with probability at least $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}^{\\prime}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}^{\\prime}\\sum_{k=1}^{T}\\frac{1}{k^{c/2+1/2}}+C_{3}^{\\prime}\\sum_{k=1}^{T}\\frac{1}{k}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "523whereall constants aregiven as in Theorem $B.1$ ", "page_idx": 17}, {"type": "text", "text": "524 B.1 Preliminary ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "55We frstfolowth noaos o =) and g,91,i (13). LetGk = () and $\\pmb{\\xi}_{k}=\\pmb{G}_{k}-\\bar{\\pmb{G}}_{k}$ $\\mathbf{G}_{k,\\epsilon_{1}}^{2}=\\mathbf{G}_{k}\\odot G_{k}+\\epsilon_{1}\\mathbf{1}_{n}\\mathbf{1}_{m}^{\\top}$ $V_{k}=\\left(v_{i j}^{(k)}\\right)_{i j}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{0}=\\mathbf{0}_{n\\times m},\\quad V_{k}=\\beta_{2,k}V_{k-1}+(1-\\beta_{2,k})G_{k,\\epsilon_{1}}^{2},\\quad k\\geq1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "527  We also define R@, Ci) and $S_{V_{k}}$ as the $i$ -th row sum, $j$ -th column sum and coordinate sum of $\\mathbf{}V_{k}$   \n528 respectively. R()? and C() $C_{G_{k}^{2}}^{(j)}$ represet the samedefnitions wih espect to $G_{k,\\epsilon_{1}}^{2}$ Then,using a GR,E1 Gk,e1   \n529 similar deduction in Lemma A.2, we also obtain that for all $k\\geq1$ \uff0c ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{V_{k}}^{(i)}=\\beta_{2,k}R_{V_{k-1}}^{(i)}+(1-\\beta_{2,k})G_{k,\\epsilon_{1}}^{2}{\\bf1}_{m},\\quad C_{V_{k}}^{(j)}=\\beta_{2,k}C_{V_{k-1}}^{(j)}+(1-\\beta_{2,k}){\\bf1}_{n}^{\\top}G_{k,\\epsilon_{1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "530  As a consequence of (48), each coordinate of $W_{k}$ satisfies that ", "page_idx": 17}, {"type": "equation", "text": "$$\nw_{i j}^{(k)}=\\frac{R_{V_{k}}^{(i)}C_{V_{k}}^{(j)}}{S_{V_{k}}}=\\frac{\\left(\\beta_{2,k}R_{V_{k-1}}^{(i)}+(1-\\beta_{2,k})R_{G_{k,\\epsilon_{1}}^{2}}^{(i)}\\right)\\left(\\beta_{2,k}C_{V_{k-1}}^{(j)}+(1-\\beta_{2,k})C_{G_{k,\\epsilon_{1}}^{2}}^{(j)}\\right)}{\\beta_{2,k}S_{V_{k-1}}+(1-\\beta_{2,k})S_{G_{k,\\epsilon_{1}}^{2}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "531 Next, we introduce a proxy step-size matrix $A_{k}=\\left(a_{i j}^{(k)}\\right)_{i j}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\na_{i j}^{(k)}=\\frac{\\left(\\beta_{2,k}R_{V_{k-1}}^{(i)}+(1-\\beta_{2,k})\\mathcal{G}_{1}\\right)\\left(\\beta_{2,k}C_{V_{k-1}}^{(j)}+(1-\\beta_{2,k})\\mathcal{G}_{2}\\right)}{\\beta_{2,k}S_{V_{k-1}}+(1-\\beta_{2,k})\\mathcal{G}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "532  The proxy step-size technique is a standard way in the convergence analysis of adaptive methods,   \n53 e.g., [32, 8]. We provide a new proxy step-size in (50) to handle the matrix factorization in Adafactor.   \n534  This construction satisfies two properties. First, it's independent from $Z_{k}$ in order to disrupt the   \n535 correlation of stochastic gradients and adaptive step-sizes. Second, it needs to remain sufficiently   \n536 cose to the rigia adaptive step-size wib to avoid generating divergent terms. ", "page_idx": 18}, {"type": "text", "text": "537 B.2  Technical lemmas ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "538 In the following, we first provide some more necessary technical lemmas. We introduce a concentra  \n539 tion inequality for the martingale difference sequence, see [20] for a proof.   \n540 Lemma B.1. Suppose that $\\{Z_{s}\\}_{s\\in[T]}$ isamartingaledifferencesquencewithrespet $\\zeta_{1},\\cdot\\cdot\\cdot,\\zeta_{T}$   \n541 Assume that for each $s\\in[T]$ \uff0c $\\sigma_{s}$ is a random variable dependent on $\\zeta_{1},\\cdot\\cdot\\cdot\\,,\\zeta_{s-1}$ and satisfies that ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{Z_{s}^{2}}{\\sigma_{s}^{2}}\\right)\\mid\\zeta_{1},\\cdot\\cdot\\cdot\\,,\\zeta_{s-1}\\right]\\le\\mathrm{e}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "542 Then for any $\\lambda>0$ .and for any $\\delta\\in(0,1)$ ,it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{s=1}^{T}Z_{s}>\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\delta}\\right)+\\frac{3}{4}\\lambda\\sum_{s=1}^{T}\\sigma_{s}^{2}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "543 Lemma B.2. Following the parameter setting in (6), for any $i\\in[n],j\\in[m],k\\geq1,$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{G_{k,\\epsilon_{1}}^{2}}^{(i)},R_{V_{k}}^{(i)}\\in[m\\epsilon_{1}/2,\\mathcal{G}_{1}],\\quad C_{G_{k,\\epsilon_{1}}^{2}}^{(j)},C_{V_{k}}^{(j)}\\in[n\\epsilon_{1}/2,\\mathcal{G}_{2}],\\quad S_{G_{k,\\epsilon_{1}}^{2}},S_{V_{k}}\\in[m n\\epsilon_{1}/2,\\mathcal{G}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "544  Proof. First, using Assumption (A4), we derive that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m n\\epsilon_{1}/2\\leq S_{G_{k,\\epsilon_{1}}^{2}}=\\displaystyle\\sum_{i=1}^{n}\\displaystyle\\sum_{j=1}^{m}\\left(\\left(g_{i j}^{(k)}\\right)^{2}+\\epsilon_{1}\\right)=\\|G_{k}\\|_{F}^{2}+m n\\epsilon_{1}\\leq\\mathcal{G},}\\\\ &{m\\epsilon_{1}/2\\leq R_{G_{k,\\epsilon_{1}}^{2}}^{(i)}=\\displaystyle\\sum_{j=1}^{m}\\left(\\left(g_{i j}^{(k)}\\right)^{2}+\\epsilon_{1}\\right)\\leq\\|G_{k}\\|_{F}^{2}+m\\epsilon_{1}\\leq\\mathcal{G}_{1},}\\\\ &{n\\epsilon_{1}/2\\leq C_{G_{k,\\epsilon_{1}}^{2}}^{(j)}=\\displaystyle\\sum_{i=1}^{n}\\left(\\left(g_{i j}^{(k)}\\right)^{2}+\\epsilon_{1}\\right)\\leq\\|G_{k}\\|_{F}^{2}+n\\epsilon_{1}\\leq\\mathcal{G}_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "545Using the similar deduction for Lemma A.3, we could show that $m\\epsilon_{1}(1-\\beta_{2,1})\\leq R_{V_{k}}^{(i)}\\leq\\mathcal{G}_{1}$ .Since   \n546 $\\beta_{2,1}\\,=\\,1/2$ from (6),we thn otan the desiredrult. The bos for $C_{V_{k}}^{(j)},S_{V_{k}}$ could be also   \n547  derived by using similar arguments. \u53e3   \n548 We have the following lemma to upper bound each coordinate of the proxy step-size matrix $A_{k}$   \n549defined in (50) . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "550Lemma B.3. For any $k\\geq1$ ,itholdsthat ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta_{2,k}(1-\\beta_{2,k})\\epsilon_{1}\\leq a_{i j}^{(k)}\\leq2\\operatorname*{min}\\left\\{\\mathcal{G},\\frac{G^{2}}{m n\\epsilon_{1}}+G+\\epsilon_{1}\\right\\},\\quad\\forall i\\in[n],j\\in[m].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "551 Proof. We first have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\beta_{2,k}R_{V_{k-1}}^{(i)}+(1-\\beta_{2,k})\\mathcal{G}_{1}}{\\beta_{2,k}S_{V_{k-1}}+(1-\\beta_{2,k})\\mathcal{G}}\\leq\\frac{\\beta_{2,k}R_{V_{k-1}}^{(i)}}{\\beta_{2,k}S_{V_{k-1}}}+\\frac{(1-\\beta_{2,k})\\mathcal{G}_{1}}{(1-\\beta_{2,k})\\mathcal{G}}\\leq2.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$a_{i j}^{(k)}$ $C_{V_{k-1}}^{(j)}\\le\\mathcal{G}_{2}$ $\\beta_{2,k}C_{V_{k-1}}^{(j)}+(1-\\beta_{2,k})\\mathcal{G}_{2}\\leq\\mathcal{G}_{2}\\leq\\mathcal{G}$ $a_{i j}^{(k)}\\leq2\\mathcal{G}$ 5  derive a free dimension bound from Lemma B.2 for $a_{i j}^{(k)}$ as follows, ", "page_idx": 19}, {"type": "equation", "text": "$$\na_{i j}^{(k)}\\leq\\frac{2\\mathcal{G}_{1}\\mathcal{G}_{2}}{m n\\epsilon_{1}}=\\frac{2(G^{2}+G(m+n)\\epsilon_{1}+m n\\epsilon_{1}^{2})}{m n\\epsilon_{1}}\\leq2\\left(\\frac{G^{2}}{m n\\epsilon_{1}}+G+\\epsilon_{1}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "555  where we use $m+n\\leq m n$ when $m,n\\geq2$ and $\\beta_{2,k}S_{V_{k-1}}+(1-\\beta_{2,k})\\mathcal{G}\\geq m n\\epsilon_{1}/2$ . To lower   \n556 bound $a_{i j}^{(k)}$ wederive from Lemma B.2 that $\\beta_{2,k}S_{V_{k-1}}+(1-\\beta_{2,k})\\mathcal{G}\\leq\\mathcal{G}$ Thereby, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i j}^{(k)}\\geq\\frac{\\beta_{2,k}\\left(1-\\beta_{2,k}\\right)\\,\\left(R_{V_{k-1}}^{(i)}\\mathcal{G}_{2}+C_{V_{k-1}}^{(j)}\\mathcal{G}_{1}\\right)}{\\mathcal{G}}\\geq\\beta_{2,k}(1-\\beta_{2,k})\\cdot\\frac{(m\\mathcal{G}_{2}+n\\mathcal{G}_{1})\\epsilon_{1}}{2\\mathcal{G}}}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}=\\beta_{2,k}(1-\\beta_{2,k})\\cdot\\frac{\\left[(m+n)G^{2}+2m n\\epsilon_{1}\\right]\\epsilon_{1}}{2(G^{2}+m n\\epsilon_{1})}\\geq\\beta_{2,k}(1-\\beta_{2,k})\\epsilon_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "557 ", "page_idx": 19}, {"type": "text", "text": "558 Lemma B.4. Let $W_{k}$ and $V_{k}$ be defined in Algorithm 1 without update clipping where $\\eta_{k}$ is given by   \n559(5) and (47) respectively. For any $k\\geq1$ ,itholdsthat ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}\\leq\\frac{2\\mathcal{G}}{\\operatorname*{max}\\{m,n\\}\\epsilon_{1}}\\left\\|\\frac{G_{k}}{\\sqrt{V_{k}}}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "560 Prof Recalling (49), $v_{i j}^{(k)}\\leq R_{V_{k}}^{(i)}\\,,v_{i j}^{(k)}\\leq C_{V_{k}}^{(j)}$ and Lemma B.2, one could verify that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\left(g_{i j}^{(k)}\\right)^{2}}{w_{i j}^{(k)}}=\\frac{\\left(g_{i j}^{(k)}\\right)^{2}S_{V_{k}}}{R_{V_{k}}^{(i)}C_{V_{k}}^{(j)}}\\leq\\frac{2\\left(g_{i j}^{(k)}\\right)^{2}\\mathcal{G}}{n\\epsilon_{1}v_{i j}^{(k)}},\\quad\\frac{\\left(g_{i j}^{(k)}\\right)^{2}}{w_{i j}^{(k)}}=\\frac{\\left(g_{i j}^{(k)}\\right)^{2}S_{V_{k}}}{R_{V_{k}}^{(i)}C_{V_{k}}^{(j)}}\\leq\\frac{2\\left(g_{i j}^{(k)}\\right)^{2}\\mathcal{G}}{m\\epsilon_{1}v_{i j}^{(k)}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "561which leads to the desired result that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{U}_{k}\\right\\|_{F}^{2}=\\left\\|\\frac{\\pmb{G}_{k}}{\\sqrt{\\pmb{W}_{k}}}\\right\\|_{F}^{2}\\leq\\frac{2\\mathcal{G}}{\\operatorname*{max}\\{m,n\\}\\epsilon_{1}}\\left\\|\\frac{\\pmb{G}_{k}}{\\sqrt{V_{k}}}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "562 ", "page_idx": 19}, {"type": "text", "text": "563 The following lemma is inspired by [8, Lemma 5.2] where they considered a constant $\\beta_{2,k}$ .Here,we   \n564  generalize the result to the case of time-varying $\\beta_{2,k}$ and provide the proof detail.   \n565Lemma B.5.For any $t\\ge1,\\,i f\\,\\beta_{2,k}$ are as in (6), then it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{t}(1-\\beta_{2,k})\\left\\|\\frac{G_{k}}{\\sqrt{V_{k}}}\\right\\|_{F}^{2}\\leq m n\\log\\left(\\frac{2(G^{2}+\\epsilon_{1})}{\\epsilon_{1}}\\right)+4m n\\sum_{k=1}^{t}(1-\\beta_{2,k}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "56 Proof. Recalling the definition of $V_{k}$ and since $V_{0}=\\mathbf{0}_{n\\times m}$ , we have that for any $k\\geq1$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{v_{i j}^{(k)}=\\beta_{2,k}v_{i j}^{(k-1)}+\\left(1-\\beta_{2,k}\\right)\\left[\\left(g_{i j}^{(k)}\\right)^{2}+\\epsilon_{1}\\right]}}\\\\ &{}&{=\\displaystyle\\sum_{p=1}^{k}(1-\\beta_{2,p})\\left[\\left(g_{i j}^{(p)}\\right)^{2}+\\epsilon_{1}\\right]\\left(\\prod_{l=p+1}^{k}\\beta_{2,l}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "567 Then, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big(1-\\beta_{2,k}\\big)\\cdot\\frac{\\Big(g_{i j}^{(k)}\\Big)^{2}}{v_{i j}^{(k)}}=\\frac{x_{k}}{y_{k}+\\theta_{k}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "568 where we set $y_{0}=0,\\theta_{0}=0$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle x_{k}=\\left(1-\\beta_{2,k}\\right)\\left(g_{i j}^{(k)}\\right)^{2},\\quad y_{k}=\\sum_{p=1}^{k}(1-\\beta_{2,p})\\left(g_{i j}^{(p)}\\right)^{2}\\left(\\prod_{l=p+1}^{k}\\beta_{2,l}\\right),}\\\\ {\\displaystyle\\theta_{k}=\\epsilon_{1}\\sum_{p=1}^{k}(1-\\beta_{2,p})\\left(\\prod_{l=p+1}^{k}\\beta_{2,l}\\right),\\quad\\forall k\\ge1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "569  Then we have $y_{k}\\,-\\,x_{k}\\,=\\,\\beta_{2,k}y_{k-1},\\forall k\\,\\geq\\,1$ .Moreover, since $y_{k}\\ \\geq x_{k}$ , we could use $\\log x\\,\\geq$   \n570 $1-1/x,\\forall x\\geq\\bar{1}$ to derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\frac{x_{k}}{y_{k}+\\theta_{k}}\\leq\\log(y_{k}+\\theta_{k})-\\log(y_{k}+\\theta_{k}-x_{k})=\\log(y_{k}+\\theta_{k})-\\log(\\beta_{2,k}y_{k-1}+\\theta_{k})}\\\\ {\\quad\\qquad=\\log\\left(\\frac{y_{k}+\\theta_{k}}{y_{k-1}+\\theta_{k-1}}\\right)+\\log\\left(\\frac{y_{k-1}+\\theta_{k-1}}{\\beta_{2,k}y_{k-1}+\\theta_{k}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "571 Noting that $\\theta_{k}=\\beta_{2,k}\\theta_{k-1}+(1-\\beta_{2,k})\\epsilon_{1}$ , which leads to $\\beta_{2,k}\\theta_{k-1}\\leq\\theta_{k}$ . Hence, we further have ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{x_{k}}{y_{k}+\\theta_{k}}}\\leq\\log\\left({\\frac{y_{k}+\\theta_{k}}{y_{k-1}+\\theta_{k-1}}}\\right)+\\log\\left({\\frac{y_{k-1}+\\theta_{k-1}}{\\beta_{2,k}(y_{k-1}+\\theta_{k-1})}}\\right)=\\log\\left({\\frac{y_{k}+\\theta_{k}}{y_{k-1}+\\theta_{k-1}}}\\right)-\\log\\beta_{2,k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "572 Hence, summing up on both sides of (52) and (53) over $k\\in[t]$ , and noting that $x_{1}=y_{1}$ ,weobtain   \n573that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{t}(1-\\beta_{2,k})\\cdot\\frac{\\binom{g_{i j}^{(k)}}{i j}^{2}}{v_{i j}^{(k)}}=\\frac{x_{1}}{y_{1}+\\theta_{1}}+\\sum_{k=2}^{t}\\frac{x_{k}}{y_{k}+\\epsilon_{k}}}\\\\ &{\\leq1+\\log\\left(\\frac{y_{t}+\\theta_{t}}{y_{1}+\\theta_{1}}\\right)-\\displaystyle\\sum_{k=2}^{t}\\log\\beta_{2,k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "574  Note that $y_{1}+\\theta_{1}\\ge(1-\\beta_{2,1})\\epsilon_{1}=\\epsilon_{1}/2$ . Moreover, using Lemma A.1 and Assumption (A4), we   \n575have $\\theta_{t}=\\Gamma_{t}\\epsilon_{1}\\leq\\epsilon_{1}$ and $y_{t}\\le\\Gamma_{t}G^{2}\\le G^{2}$ . We then derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{y_{t}+\\theta_{t}}{y_{1}+\\theta_{1}}\\leq\\frac{2(G^{2}+\\epsilon_{1})}{\\epsilon_{1}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "576  Noting that for $k\\geq2,c\\in[1/2,1],\\beta_{2,k}\\geq\\beta_{2,2}=1-1/2^{c}\\geq1-1/\\sqrt2,$ we then derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\log\\beta_{2,k}\\le\\frac{1-\\beta_{2,k}}{\\beta_{2,k}}\\le\\frac{\\sqrt{2}(1-\\beta_{2,k})}{\\sqrt{2}-1}\\le4(1-\\beta_{2,k}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "577Finally, plugging (55), (56) into (54), and then summing (54) up over $i\\in[n],j\\in[m]$ ,we obtain the   \n578 desired result. \u53e3   \n579 Next, we have the following probabilistic result relying on the property of the martingale difference   \n580 sequence which is commonly used in the analysis of adaptive methods.   \n581 Lemma B.6. Following the parameter seting in (6), for any $T\\geq1$ and $\\lambda>0$ withprobability at   \n582least $1-\\delta,\\forall t\\in[T]$ ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\sum_{k=1}^{t}\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{\\xi_{k}}{\\sqrt{A_{k}}}\\right\\rangle\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\eta_{k}\\left\\Vert\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\Vert_{F}^{2}+\\frac{24G^{2}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}}{\\sqrt{\\epsilon_{1}}}\\log\\left(\\frac{T}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "58  Proof Let $\\begin{array}{r}{\\zeta_{k}=-\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{\\xi_{k}}{\\sqrt{A_{k}}}\\right\\rangle}\\end{array}$ and the fltration $\\mathcal{F}_{k}=\\sigma\\left(Z_{1},\\cdot\\cdot\\cdot\\right,Z_{k}\\right)$ where $\\sigma(\\cdot)$ denotes the   \n584 $\\sigma$ -algebra. Note that $\\eta_{k}$ \uff0c $\\bar{G}_{k}$ and $A_{k}$ are dependent by $\\{X_{1},\\cdot\\cdot\\cdot,X_{k-1}\\}$ and thereby $\\mathcal{F}_{k-1}$ . Since   \n585 $\\xi_{k}$ is dependent by $\\mathcal{F}_{k}$ , we could prove that $\\{\\zeta_{k}\\}_{k\\ge1}$ is a martingale difference sequence since ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\zeta_{k}\\ |\\ \\mathcal{F}_{k-1}\\right]=-\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{\\mathbb{E}\\left[\\xi_{k}\\ |\\ \\mathcal{F}_{k-1}\\right]}{\\sqrt{A_{k}}}\\right\\rangle=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "56  where we apply that $\\mathbb{E}\\left[\\pmb{\\xi}_{k}\\mid\\mathcal{F}_{k-1}\\right]=\\mathbb{E}_{Z_{k}}[\\pmb{\\xi}_{k}]=0$ from Assumption (A3). Then, using Assumption   \n587 (A3) and Assumption (A4), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\bar{G}_{k}\\|_{F}=\\|\\mathbb{E}_{Z_{k}}[G_{k}]\\|_{F}\\leq\\mathbb{E}_{Z_{k}}\\|G_{k}\\|_{F}\\leq G,\\quad\\|\\xi_{k}\\|_{F}=\\|G_{k}-\\bar{G}_{k}\\|_{F}\\leq2G.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "588 Let $\\begin{array}{r}{\\omega_{k}=2G\\eta_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\rVert_{F}}\\end{array}$ We thus derivfm tCauchy-hwarquality ht ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{\\zeta_{k}^{2}}{\\omega_{k}^{2}}\\right)\\mid\\mathcal{F}_{k-1}\\right]\\le\\mathbb{E}\\left[\\exp\\left(\\frac{\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\|_{F}^{2}\\|\\xi_{k}\\|_{F}^{2}}{4G^{2}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\|_{F}^{2}}\\right)\\mid\\mathcal{F}_{k-1}\\right]\\le\\exp(1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "589Then, using Lemma B.1, it leads to that for any $\\lambda>0$ , with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{\\displaystyle-\\sum_{k=1}^{t}\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{\\xi_{k}}{\\sqrt{A_{k}}}\\right\\rangle\\leq3\\lambda G^{2}\\sum_{k=1}^{t}\\eta_{k}^{2}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\rVert_{F}^{2}+\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\delta}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=3\\lambda G^{2}\\sum_{k=1}^{t}\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\frac{\\eta_{k}}{\\sqrt{a_{i j}^{(k)}}}\\cdot\\eta_{k}\\frac{\\left(\\bar{g}_{i j}^{(k)}\\right)^{2}}{\\sqrt{a_{i j}^{(k)}}}+\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "590Meanwhile, when $\\Theta_{\\mathrm{min}}\\le\\|X_{k}\\|_{\\infty}\\le\\Theta_{\\mathrm{max}},\\rho_{k}=\\rho_{0}/\\sqrt{k}$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{min}}\\le\\mathrm{RMS}(X_{k})\\le\\Theta_{\\mathrm{max}},\\quad\\frac{\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\mathrm{min}}\\}\\rho_{0}}{\\sqrt{k}}\\le\\eta_{k}\\le\\frac{(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0}}{\\sqrt{k}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "591 Combining with Lemma B.3, we derive that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{\\eta_{k}}{\\sqrt{a_{i j}^{(k)}}}\\le\\frac{\\eta_{k}}{\\sqrt{\\beta_{2,k}(1-\\beta_{2,k})\\epsilon_{1}}}\\le\\frac{(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0}}{\\sqrt{\\beta_{2,k}\\epsilon_{1}}}\\cdot\\frac{k^{c/2}}{\\sqrt{k}}}\\\\ &{}&{\\qquad\\le\\frac{(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0}}{\\sqrt{\\operatorname*{min}\\{\\beta_{2,1},\\beta_{2,2}\\}\\epsilon_{1}}}\\le\\frac{2(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0}}{\\sqrt{\\epsilon_{1}}},\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "592  where we use $\\beta_{2,1}=1/2,\\beta_{2,2}=1-1/2^{c}\\geq1-1/\\sqrt{2},c\\in[1/2,1]$ from (6) in the last inequality.   \n593  Hence, plugging (60) into (57) and then re-scaling the $\\delta$ , we found that with probability at least $1-\\delta$   \n594   for all $t\\in[T]$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\sum_{k=1}^{t}\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{\\xi_{k}}{\\sqrt{A_{k}}}\\right\\rangle\\leq\\frac{6\\lambda G^{2}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}}{\\sqrt{\\epsilon_{1}}}\\sum_{k=1}^{t}\\eta_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+\\frac{1}{\\lambda}\\log\\left(\\frac{T}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "595   Setting $\\lambda=\\sqrt{\\epsilon_{1}}/(24G^{2}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0})$ , we derive the desired result. ", "page_idx": 21}, {"type": "text", "text": "5Thfoiyprdnuftuhbtoxyi $a_{i j}^{(k)}$   \n597 illustrating the error is controllable. ", "page_idx": 21}, {"type": "text", "text": "598  Lemma B.7. For any $k\\geq1,i\\in[n],j\\in[m],$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\left|w_{i j}^{(k)}-a_{i j}^{(k)}\\right|}{\\sqrt{a_{i j}^{(k)}}}\\leq\\sqrt{1-\\beta_{2,k}}\\operatorname*{min}\\{4\\sqrt{\\mathcal{G}},G_{1}+G_{2}\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "599where $\\mathcal{G}$ is as in (13) and $G_{1},G_{2}$ are as in (44). ", "page_idx": 21}, {"type": "text", "text": "600Proof. To simplify the notation, we let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X=\\beta_{2,k}R_{V_{k-1}}^{(i)}+(1-\\beta_{2,k})R_{G_{k,\\epsilon_{1}}^{2}}^{(i)},\\quad\\Delta X=(1-\\beta_{2,k})(\\mathcal{G}_{1}-R_{G_{k,\\epsilon_{1}}^{2}}^{(i)}),}\\\\ &{Y=\\beta_{2,k}C_{V_{k-1}}^{(j)}+(1-\\beta_{2,k})C_{G_{k,\\epsilon_{1}}^{2}}^{(j)},\\quad\\Delta Y=(1-\\beta_{2,k})(\\mathcal{G}_{2}-C_{G_{k,\\epsilon_{1}}^{2}}^{(j)}),}\\\\ &{Z=\\beta_{2,k}S_{V_{k-1}}+(1-\\beta_{2,k})S_{G_{k,\\epsilon_{1}}^{2}},\\quad\\Delta Z=(1-\\beta_{2,k})(\\mathcal{G}-S_{G_{k,\\epsilon_{1}}^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "601 Then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nw_{i j}^{(k)}-a_{i j}^{(k)}\\Big|=\\left|\\frac{X Y}{Z}-\\frac{(X+\\Delta X)(Y+\\Delta Y)}{Z+\\Delta Z}\\right|=\\left|\\frac{X Y\\Delta Z-X Z\\Delta Y-Y Z\\Delta X-Z(\\Delta X\\Delta Y)}{Z(Z+\\Delta Z)}\\right|\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "602Applying Lemma B.2, we could verify that $X,Y,Z\\geq0$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\leq\\Delta X\\leq(1-\\beta_{2,k})\\mathscr{G}_{1},\\quad0\\leq\\Delta Y\\leq(1-\\beta_{2,k})\\mathscr{G}_{2},\\quad0\\leq\\Delta Z\\leq(1-\\beta_{2,k})\\mathscr{G}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "603 Hence, we derive that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left|w_{i j}^{(k)}-a_{i j}^{(k)}\\right|}{\\sqrt{a_{i j}^{(k)}}}=\\frac{\\left|X Y\\Delta Z-X Z\\Delta Y-Y Z\\Delta X-Z(\\Delta X\\Delta Y)\\right|}{Z\\sqrt{(X+\\Delta X)(Y+\\Delta Y)(Z+\\Delta Z)}}}\\\\ &{\\phantom{\\frac{(x)^{2}}{\\sqrt{a_{i j}^{(k)}}}=\\frac{\\left|X\\Delta Y+Y\\Delta X+(\\Delta X\\Delta Y)\\right|}{\\sqrt{(X+\\Delta X)(Y+\\Delta Y)(Z+\\Delta Z)}}+\\underbrace{\\frac{X Y\\Delta Z}{Z\\sqrt{(X+\\Delta X)(Y+\\Delta Y)(Z+\\Delta Z)}}}_{\\mathrm{(II)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "604Since $X Y\\ge0$ from (62), Term (I) could be bounded as ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathbf{I})\\leq\\frac{|X\\Delta Y+Y\\Delta X+(\\Delta X\\Delta Y)|}{\\sqrt{(X\\Delta Y+Y\\Delta X+(\\Delta X\\Delta Y))(Z+\\Delta Z)}}\\leq\\sqrt{\\frac{X\\Delta Y+Y\\Delta X+(\\Delta X\\Delta Y)}{Z+\\Delta Z}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "605 Reallingthe definin, we haeve $R_{V_{k-1}}^{(i)}\\leq S_{V_{k-1}}$ $C_{V_{k-1}}^{(j)}\\leq S_{V_{k-1}}$ forany $i\\in[n],j\\in[m]$ Futher,   \n606 applying Lemma B.2 and (63), we derive that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{X\\Delta Y}{Z+\\Delta Z}\\leq\\left(\\frac{R_{V_{k-1}}^{(i)}}{S_{V_{k-1}}}+\\frac{R_{G_{k,e_{1}}^{2}}^{(i)}}{\\mathcal{G}}\\right)\\Delta Y\\leq2(1-\\beta_{2,k})\\mathcal{G}_{2}.}\\\\ &{\\frac{Y\\Delta X}{Z+\\Delta Z}\\leq\\left(\\frac{C_{V_{k-1}}^{(j)}}{S_{V_{k-1}}}+\\frac{C_{G_{k,e_{1}}^{2}}^{(j)}}{\\mathcal{G}}\\right)\\Delta X\\leq2(1-\\beta_{2,k})\\mathcal{G}_{1},}\\\\ &{\\frac{\\Delta X\\Delta Y}{Z+\\Delta Z}\\leq\\frac{\\Delta X(1-\\beta_{2,k})\\mathcal{G}}{(1-\\beta_{2,k})\\mathcal{G}}\\leq(1-\\beta_{2,k})\\mathcal{G}_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "607 We then derive from (65), $\\mathcal{G}_{1}\\leq\\mathcal{G}$ and $\\mathcal{G}_{2}\\leq\\mathcal{G}$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\eta}(\\mathbf{I})\\leq\\sqrt{5(1-\\beta_{2,k})\\mathcal{G}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "608 To derive a free dimension bound, we could obtain from Lemma B.2, (63) and $\\mathcal{G}\\ge m n\\epsilon_{1}/2$ that   \n609 $Z+\\Delta Z\\ge m n\\epsilon_{1}/2$ .Hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{X\\Delta Y}{Z+\\Delta Z}\\leq\\frac{2(1-\\beta_{2,k})\\mathcal{G}_{1}\\mathcal{G}_{2}}{m n\\epsilon_{1}},\\quad\\frac{Y\\Delta X}{Z+\\Delta Z}\\leq\\frac{2(1-\\beta_{2,k})\\mathcal{G}_{1}\\mathcal{G}_{2}}{m n\\epsilon_{1}},\\quad\\frac{\\Delta X\\Delta Y}{Z+\\Delta Z}\\leq\\frac{2(1-\\beta_{2,k})\\mathcal{G}_{1}\\mathcal{G}_{2}}{m n\\epsilon_{1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "610 We then derive that ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathbf{I})\\le\\sqrt{\\frac{6(1-\\beta_{2,k})\\mathcal{G}_{1}\\mathcal{G}_{2}}{m n\\epsilon_{1}}}=\\sqrt{\\frac{6(1-\\beta_{2,k})(G^{4}+G^{2}\\epsilon_{1}(m+n)+m n\\epsilon_{1}^{2})}{m n\\epsilon_{1}}}\\le G_{1}\\sqrt{1-\\beta_{2,k}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "611 where we used $m+n\\leq m n$ , and $G_{1}$ is defined in (44). Then, combining with (66) and (67), we   \n612have ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathbf{I})\\leq\\sqrt{1-\\beta_{2,k}}\\operatorname*{min}\\{\\sqrt{5\\mathcal{G}},G_{1}\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "613  where we applied that $m+n\\leq m n$ when $m,n\\geq2$ . Then we move to bound $(\\mathbf{II})$ . Recalling the   \n614  definitions in (62), we have $X\\le Z,Y\\le Z$ . Applying (63), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathbf{II})\\leq\\frac{X Y\\Delta Z}{Z\\sqrt{X Y\\Delta Z}}\\leq\\frac{\\sqrt{X Y\\Delta Z}}{Z}\\leq\\sqrt{\\Delta Z}\\leq\\sqrt{(1-\\beta_{2,k})\\mathcal{G}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "615 Similarly, we derive from Lemma B.2 that $Z\\ge m n\\epsilon_{1}/2$ \uff0c $X\\le\\mathcal{G}_{1},Y\\le\\mathcal{G}_{2}$ . Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(\\mathbf{II})\\leq\\frac{\\sqrt{X Y\\Delta Z}}{Z}\\leq\\frac{2\\sqrt{(1-\\beta_{2,k})\\mathcal{G}_{1}\\mathcal{G}_{2}\\mathcal{G}}}{m n\\epsilon_{1}}}\\\\ {\\displaystyle\\qquad\\leq2\\sqrt{1-\\beta_{2,k}}\\left(\\frac{G^{3}}{m n\\epsilon_{1}}+\\frac{2G^{2}}{\\sqrt{m n\\epsilon_{1}}}+G+\\frac{G}{\\sqrt{m n}}+\\sqrt{\\epsilon_{1}}\\right)\\leq G_{2}\\sqrt{1-\\beta_{2,k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "616where $G_{2}$ has been defined in (44). We thus derive that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbf{II})\\leq\\sqrt{1-\\beta_{2,k}}\\operatorname*{min}\\{\\sqrt{\\mathcal{G}},G_{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "617 Combining (68) with (69), we then derive the desired result. ", "page_idx": 23}, {"type": "text", "text": "618 B.3Proof of Proposition B.1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "619 Using the inequality in (14), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f({\\pmb X}_{k+1})\\leq f({\\pmb X}_{k})+\\langle\\bar{G}_{k},{\\pmb X}_{k+1}-{\\pmb X}_{k}\\rangle+\\frac{L}{2}\\|{\\pmb X}_{k+1}-{\\pmb X}_{k}\\|_{F}^{2}}\\\\ &{\\quad\\quad\\leq f({\\pmb X}_{k})-\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\rangle+\\frac{L\\eta_{k}^{2}}{2}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "620 Introducing the proxy step-size matrix $A_{k}$ in (50) and then summing up both sides over $k\\in[t]$ ,we   \n621 derive that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lefteqn{\\left[f(\\mathbf{X}_{t+1})\\leq f(\\mathbf{X}_{1})\\underbrace{-\\sum_{k=1}^{t}\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{G_{k}}{\\sqrt{A_{k}}}\\right\\rangle}_{\\mathbf{A}}\\right.}\\\\ &{\\quad\\quad\\left.+\\underbrace{\\sum_{k=1}^{t}\\eta_{k}\\left\\langle\\bar{G}_{k},G_{k}\\odot\\left(\\frac{1}{\\sqrt{A_{k}}}-\\frac{1}{\\sqrt{W_{k}}}\\right)\\right\\rangle}_{\\mathbf{B}}+\\underbrace{\\sum_{k=1}^{t}\\frac{L\\eta_{k}^{2}}{2}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}_{\\mathbf{C}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "622 Estimation for A We first introduce $\\xi_{k}$ into $\\mathbf{A}$ ", "text_level": 1, "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{A}=-\\sum_{k=1}^{t}\\eta_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{4}k}\\right\\|_{F}^{2}-\\sum_{k=1}^{t}\\eta_{k}\\left\\langle\\bar{G}_{k},\\frac{\\xi_{k}}{\\sqrt{A_{k}}}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "623Then, using Lemma B.6, with probability at least $1-\\delta$ ,for all $t\\in[T]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\bf A}=-\\frac{3}{4}\\sum_{k=1}^{t}\\eta_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{4}k}\\right\\|_{F}^{2}+\\frac{24G^{2}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0}}{\\sqrt{\\epsilon_{1}}}\\log\\left(\\frac{T}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "624  Estimation for B Term $\\mathbf{B}$ is essentially the error brought by the proxy step-size $A_{k}$ . We will frst   \n62  calculaethe ap of $1/\\sqrt{w_{i j}^{(k)}}$ and $1/\\sqrt{a_{i j}^{(k)}}$ as fllows, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{\\sqrt{w_{i j}^{(k)}}}-\\frac{1}{\\sqrt{a_{i j}^{(k)}}}\\right|=\\frac{1}{\\sqrt{w_{i j}^{(k)}}\\sqrt{a_{i j}^{(k)}}}\\left|\\sqrt{w_{i j}^{(k)}}-\\sqrt{a_{i j}^{(k)}}\\right|\\leq\\frac{1}{\\sqrt{w_{i j}^{(k)}}\\sqrt{a_{i j}^{(k)}}}\\sqrt{\\left|w_{i j}^{(k)}-a_{i j}^{(k)}\\right|}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "626 We then apply (73) and Young's inequality, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{B}\\leq\\underset{k=1}{\\overset{t}{\\sum}}\\underset{i=1}{\\overset{n}{\\sum}}\\underset{j=1}{\\overset{m}{\\sum}}\\eta_{k}\\left|\\bar{g}_{i j}^{(k)}g_{i j}^{(k)}\\right|\\left|\\frac{1}{\\sqrt{w_{i j}^{(k)}}}-\\frac{1}{\\sqrt{a_{i j}^{(k)}}}\\right|}\\\\ &{\\quad\\leq\\underset{k=1}{\\overset{t}{\\sum}}\\underset{i=1}{\\overset{n}{\\sum}}\\underset{j=1}{\\overset{m}{\\sum}}\\eta_{k}\\frac{\\left|\\bar{g}_{i j}^{(k)}g_{i j}^{(k)}\\right|}{\\sqrt{w_{i j}^{(k)}}\\sqrt{a_{i j}^{(k)}}}\\sqrt{\\left|w_{i j}^{(k)}-a_{i j}^{(k)}\\right|}}\\\\ &{\\quad\\leq\\frac{1}{4}\\underset{k=1}{\\overset{t}{\\sum}}\\underset{i=1}{\\overset{n}{\\sum}}\\underset{j=1}{\\overset{m}{\\sum}}\\eta_{k}\\cdot\\frac{\\left(\\bar{g}_{i j}^{(k)}\\right)^{2}}{\\sqrt{a_{i j}^{(k)}}}+4\\underset{k=1}{\\overset{t}{\\sum}}\\underset{i=1}{\\overset{n}{\\sum}}\\sum_{j=1}^{m}\\eta_{k}\\cdot\\frac{\\left|w_{i j}^{(k)}-a_{i j}^{(k)}\\right|}{\\sqrt{a_{i j}^{(k)}}}\\cdot\\left(\\frac{g_{i j}^{(k)}}{\\sqrt{w_{i j}^{(k)}}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "627Thus, plugging (61) in Lemma B.7 into (74), we derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bf{B}}\\leq\\displaystyle\\frac{1}{4}\\sum_{k=1}^{t}{\\eta_{k}}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+4\\sqrt{\\mathcal{G}}\\sum_{k=1}^{t}{\\eta_{k}\\sqrt{1-\\beta_{2,k}}}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}\\\\ {\\displaystyle~~\\leq\\frac{1}{4}\\sum_{k=1}^{t}{\\eta_{k}}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+4\\sqrt{\\mathcal{G}}\\sum_{k=1}^{t}\\frac{\\left(\\epsilon_{2}+\\Theta_{\\operatorname*{max}}\\right)\\rho_{0}}{\\sqrt{k}}\\sqrt{1-\\beta_{2,k}}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}\\\\ {\\displaystyle~~\\leq\\frac{1}{4}\\sum_{k=1}^{t}{\\eta_{k}}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+4\\sqrt{\\mathcal{G}}\\sum_{k=1}^{t}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}(1-\\beta_{2,k})\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "628   where we used (58) in the second inequality and $1/\\sqrt{k}\\le1/k^{c/2},c\\in[1/2,1]$ Furthermore, using   \n629Lemma B.4 and Lemma B.5, we derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{B}\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\eta_{k}\\left\\Vert\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\Vert_{F}^{2}+\\frac{8m n\\mathcal{G}^{\\frac{3}{2}}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}}{\\operatorname*{max}\\{m,n\\}\\epsilon_{1}}\\left[\\log\\left(2+\\frac{2G^{2}}{\\epsilon_{1}}\\right)+4\\sum_{k=1}^{t}(1-\\beta_{2,k})\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "630Estimating C Using the similar deduction in (75) and (76), we derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\bf C}\\le\\frac{L m n\\mathcal{G}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})^{2}\\rho_{0}^{2}}{\\mathrm{max}\\{m,n\\}\\epsilon_{1}}\\left[\\log\\left(2+\\frac{2G^{2}}{\\epsilon_{1}}\\right)+4\\sum_{k=1}^{t}(1-\\beta_{2,k})\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "631 Putting together  We first re-arrange the order in (70) and use $f(X_{t+1})\\geq f^{*}$ in Assumption (A2)   \n632 to derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\n0\\leq f(X_{1})-f^{*}+\\mathbf{A}+\\mathbf{B}+\\mathbf{C}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "633We then plug (72), (76), (77) into (78) and set $t=T$ , which leads to that with probability at least   \n634 $1-\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{k=1}^{T}\\eta_{k}\\left\\Vert\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\Vert_{F}^{2}\\le C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}\\sum_{k=1}^{T}(1-\\beta_{2,k})+C_{3},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "635where $C_{1},C_{2},C_{3}$ are as in Theorem B.1. Moreover, using Lemma B.3 and (58), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{k=1}^{T}\\eta_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{4}k}\\right\\|_{F}^{2}\\geq\\sum_{k=1}^{T}\\frac{\\eta_{k}\\left\\|\\bar{G}_{k}\\right\\|_{F}^{2}}{2\\operatorname*{max}_{i,j}\\sqrt{a_{i j}^{(k)}}}\\geq\\frac{\\rho_{0}\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}}{2\\sqrt{2\\mathcal{G}}}\\sum_{k=1}^{T}\\frac{\\left\\|\\bar{G}_{k}\\right\\|_{F}^{2}}{\\sqrt{k}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "636  Combining with (80) and (79), and using $\\textstyle\\sum_{k=1}^{T}1/{\\sqrt{k}}\\geq{\\sqrt{T}}$ , we derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}\\sum_{k=1}^{T}(1-\\beta_{2,k})+C_{3}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "637where $C_{0}$ has already been defined in (42). We then derive the first desired result that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}\\sum_{k=1}^{T}\\frac{1}{k^{c}}+C_{3}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "638 Free dimension bound  We follow the similar deduction in (75) and use Lemma B.7 to derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{B}\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\eta_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\rVert_{F}^{2}+4(G_{1}+G_{2})(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}\\sum_{k=1}^{t}\\frac{1}{k^{c/2+1/2}}\\left\\lVert\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\rVert_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "639  Recalling the definition of wk) in (49) and Lemma B.2, we derive that ", "page_idx": 24}, {"type": "equation", "text": "$$\nw_{i j}^{(k)}=\\frac{R_{V_{k}}^{(i)}C_{V_{k}}^{(j)}}{S_{V_{k}}}\\geq\\frac{m n\\epsilon_{1}^{2}}{4\\mathcal{G}},\\quad\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}\\leq\\frac{\\|G_{k}\\|_{F}^{2}}{\\operatorname*{min}_{i,j}{w_{i j}^{(k)}}}\\leq\\frac{4G^{2}\\mathcal{G}}{m n\\epsilon_{1}^{2}}\\leq G_{3},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "640where $G_{3}$ is as in (44). We thus derive from (82) and (83) that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{B}\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\eta_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+4G_{3}(G_{1}+G_{2})(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}\\sum_{k=1}^{t}\\frac{1}{k^{c/2+1/2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "641 Using (58) and (83), we derive that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{C}=\\sum_{k=1}^{t}\\frac{L\\eta_{k}^{2}}{2}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}\\leq\\frac{L G_{3}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})^{2}\\rho_{0}^{2}}{2}\\sum_{k=1}^{t}\\frac{1}{k}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "642  Plugging the unchanged estimation for $\\mathbf{A}$ in (72), (84) and (85) into (70), we have that with probability   \n643 at least $1-\\delta$ , for all $t\\in[T]$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{k=1}^{t}\\eta_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\|_{F}^{2}\\leq C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}^{\\prime}\\sum_{k=1}^{t}\\frac{1}{k^{c/2+1/2}}+C_{3}^{\\prime}\\sum_{k=1}^{t}\\frac{1}{k},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "644where $C_{2}^{\\prime},C_{3}^{\\prime}$ are given as in (43) and $C_{1}$ is as in (41). Further, using Lemma B.3 and the similar   \n645  deduction for (80), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{k=1}^{t}\\eta_{k}\\left\\Vert\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\Vert_{F}^{2}\\geq\\sum_{k=1}^{t}\\frac{\\eta_{k}\\left\\Vert\\bar{G}_{k}\\right\\Vert_{F}^{2}}{2\\operatorname*{max}_{i,j}\\sqrt{a_{i j}^{(k)}}}\\geq\\frac{1}{C_{0}^{\\prime}}\\sum_{k=1}^{t}\\frac{\\left\\Vert\\bar{G}_{k}\\right\\Vert_{F}^{2}}{\\sqrt{k}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "646where $C_{0}^{\\prime}$ is as in (43). Combining with (86) and (87), and setting $t=T$ , we derive the second   \n647 desired result in Proposition B.1 that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|^{2}\\leq\\frac{C_{0}^{\\prime}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}^{\\prime}\\sum_{k=1}^{T}\\frac{1}{k^{c/2+1/2}}+C_{3}^{\\prime}\\sum_{k=1}^{T}\\frac{1}{k}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "648B.4 Proof of Theorem B.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "649 Now based on the result in Proposition B.1, we could further derive the final convergence rate. Noting   \n650thatwhen $c=1$ ,wecouldboundthat ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{T}\\frac{1}{k}\\leq1+\\int_{1}^{T}\\frac{1}{x}d x\\leq1+\\log T.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "651 Then, we obtain that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+C_{2}\\log T+C_{2}+C_{3}\\right),}\\\\ &{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}^{\\prime}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+(C_{2}^{\\prime}+C_{3}^{\\prime})\\log T+C_{2}^{\\prime}+C_{3}^{\\prime}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "652When $1/2\\le c<1$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\sum_{k=1}^{T}\\frac{1}{k^{c}}\\leq1+\\int_{1}^{T}\\frac{1}{x^{c}}d x\\leq1+\\frac{T^{1-c}}{1-c},}\\\\ {\\displaystyle\\sum_{k=1}^{T}\\frac{1}{k^{c/2+1/2}}\\leq1+\\int_{1}^{T}\\frac{1}{x^{c/2+1/2}}d x\\leq1+\\frac{2T^{(1-c)/2}}{1-c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "653  Then, we obtain that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+\\frac{C_{2}}{1-c}\\cdot T^{1-c}+C_{2}+C_{3}\\right),}\\\\ &{\\displaystyle\\operatorname*{min}_{k\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{C_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+\\frac{2C_{2}^{\\prime}}{1-c}\\cdot T^{1-c}+C_{3}^{\\prime}\\log T+C_{2}^{\\prime}+C_{3}^{\\prime}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "654  C  Proof detail for stochastic Adafactor with update clipping ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "655We first provide the detailed version of Theorem 7.1 as follows. ", "page_idx": 26}, {"type": "text", "text": "656 Theorem C.1. Let $\\{X_{k}\\}_{k\\ge1}$ be the sequence generated by Algorithm $^{\\,l}$ with (7). If Assumptions   \n657 (A1) -(A4) hold, and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{k}=\\rho_{0}/\\sqrt{k},\\quad d_{k}=k^{\\frac{c}{2(\\alpha-1)}},\\quad\\forall k\\geq1,}\\\\ &{\\beta_{2,1}=1/2,\\quad\\beta_{2,k}=1-1/k^{c},\\forall k\\geq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "658When $c=1$ ,with probability at least $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k\\in[T]}{\\operatorname*{min}}\\,\\Vert\\bar{G}_{k}\\Vert_{F}^{2}\\leq\\frac{D_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+\\left(C_{2}+D_{1}(\\alpha)\\right)\\log T+C_{2}+D_{1}(\\alpha)+C_{3}\\right),}\\\\ &{\\underset{k\\in[T]}{\\operatorname*{min}}\\,\\Vert\\bar{G}_{k}\\Vert_{F}^{2}\\leq\\frac{D_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+\\left(C_{2}^{\\prime}+C_{3}^{\\prime}+D_{1}(\\alpha)\\right)\\log T+C_{2}^{\\prime}+C_{3}^{\\prime}+D_{1}(\\alpha)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "659 When $1/2\\le c<1$ with probability at least $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{min}_{*\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{D_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+\\frac{C_{2}+D_{1}(\\alpha)}{1-c}\\cdot T^{1-c}+C_{2}+D_{1}(\\alpha)+C_{3}\\right),\\qquad\\qquad(92)}\\\\ {\\displaystyle\\operatorname*{min}_{*\\in[T]}\\|\\bar{G}_{k}\\|_{F}^{2}\\leq\\frac{D_{0}}{\\sqrt{T}}\\left(C_{1}\\log\\left(\\displaystyle\\frac{T}{\\delta}\\right)+C_{3}^{\\prime}\\log T+\\frac{2(C_{2}^{\\prime}+D_{1}(\\alpha))}{1-c}\\cdot T^{\\frac{1-c}{2}}+C_{2}^{\\prime}+C_{3}^{\\prime}+D_{1}(\\alpha)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "660 where $C_{1},C_{2},C_{3},C_{2}^{\\prime},C_{3}^{\\prime}$ are as in Theorem B.1 and ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{0}=\\operatorname*{min}\\{C_{0},C_{0}^{\\prime}\\},\\quad D_{1}(\\alpha)=\\frac{G^{1+\\alpha}G_{4}^{1-\\alpha}\\sqrt{\\mathcal{G}}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}}{\\sqrt{m n}\\epsilon_{1}},\\quad G_{4}=\\frac{m n\\epsilon_{1}}{2\\sqrt{\\mathcal{G}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "661 Calculation of hyper-parameters\u2019 dependency We frst calculate the dependency on $m,n,\\epsilon_{1},\\alpha$   \n662 in the additional coefficient $D_{1}(\\alpha)$ asfollows, ", "page_idx": 26}, {"type": "equation", "text": "$$\nD_{1}(\\alpha)\\sim\\mathcal{O}\\left(\\left(\\frac{\\sqrt{1+m n\\epsilon_{1}}}{m n\\epsilon_{1}}\\right)^{\\alpha-1}\\sqrt{\\frac{1}{m n\\epsilon_{1}^{2}}+\\frac{1}{\\epsilon_{1}}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "663 which is free of the curse of dimension since mn exists in the denominator. Recalling the definitions   \n664 of $C_{0}^{\\prime},C_{1},C_{2}^{\\prime},C_{3}^{\\prime}$ in (41) and (43), it's easy to verify that these coefficients are also free of the   \n665 curse of dimension factor $m,n$ since $m,n$ exist in the denominator. Thereby, we also derive a free   \n666 dimension bound selecting (91) and (93). ", "page_idx": 26}, {"type": "text", "text": "667 To calculate the dependency on $\\epsilon_{1}$ , we could combine with (45) and (95) to derive that ", "page_idx": 26}, {"type": "equation", "text": "$$\nC_{0}D_{1}(\\alpha)\\sim\\mathcal{O}\\left(\\epsilon_{1}^{-\\alpha}\\right),\\quad C_{0}C_{1}\\sim\\mathcal{O}\\left(1/\\epsilon_{1}^{-1/2}\\right),\\quad C_{0}C_{3}\\sim\\mathcal{O}\\left(\\epsilon_{1}^{-1}\\log(1/\\epsilon_{1})\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "668 Thereby, selecting the bounds in (90) and (92) and noting that $\\alpha>1$ , we derive that the order on $\\epsilon_{1}$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{1}{\\epsilon_{1}^{\\alpha}}\\log\\left(\\frac{1}{\\epsilon_{1}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "669 Moreover, it's clear to reveal that there exist mn in denominator, which could improve the dependency   \n670 on $\\epsilon_{1}$ . If we suppose that $m n$ is comparable to $\\epsilon_{1}$ , thenwe derive that $C_{0}D_{1}(\\alpha)\\sim\\mathcal{O}(\\epsilon_{1}^{-1/2})$ and the   \n671 order on $\\epsilon_{1}$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{1}{\\epsilon_{1}}\\log\\left(\\frac{1}{\\epsilon_{1}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "672 C.1 Proof of Theorem C.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "673 We define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{G}_{k}=\\frac{G_{k}}{\\operatorname*{max}\\{1,\\|U_{k}\\|_{F}/(d_{k}\\sqrt{m n})\\}},\\quad\\hat{\\rho}_{k}=\\operatorname*{max}\\{\\epsilon_{2},\\operatorname{RMS}(X_{k})\\}\\rho_{k}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "674   Since $\\operatorname{RMS}(U_{k})=\\|U_{k}\\|_{F}{\\big/}{\\sqrt{m n}}$ $\\Theta_{\\operatorname*{min}}\\leq\\mathrm{RMS}(X_{k})\\leq\\Theta_{\\operatorname*{max}}$ , we derive that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{k+1}=X_{k}-\\hat{\\rho}_{k}\\cfrac{\\tilde{G}_{k}}{\\sqrt{W_{k}}},}\\\\ &{\\frac{\\operatorname*{max}\\{\\epsilon_{2},\\Theta_{\\operatorname*{min}}\\}\\rho_{0}}{\\sqrt{k}}\\leq\\hat{\\rho}_{k}\\leq\\frac{\\left(\\epsilon_{2}+\\Theta_{\\operatorname*{max}}\\right)\\rho_{0}}{\\sqrt{k}}\\leq(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}\\sqrt{1-\\beta_{2,k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "675   where we applied that $1/\\sqrt{k}\\le1/k^{c/2},c\\in[1/2,1]$ and $\\beta_{2,k}=1-1/k^{c}$ in the last inequality. Using   \n676  the inequalities in (14) and (99), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f({\\mathbf X}_{k+1})\\leq f({\\mathbf X}_{k})+\\langle\\bar{G}_{k},{\\mathbf X}_{k+1}-{\\mathbf X}_{k}\\rangle+\\frac{L}{2}\\|{\\mathbf X}_{k+1}-{\\mathbf X}_{k}\\|_{F}^{2}}\\\\ &{\\quad\\quad\\leq f({\\mathbf X}_{k})-\\hat{\\rho}_{k}\\left\\langle\\bar{G}_{k},\\frac{\\tilde{G}_{k}}{\\sqrt{W_{k}}}\\right\\rangle+\\frac{L\\hat{\\rho}_{k}^{2}}{2}\\left\\|\\frac{\\tilde{G}_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "677 Summing up both sides over $k\\in[t]$ and using $f(X_{t+1})\\geq f^{*}$ from Assumption (A2), we derive that ", "page_idx": 27}, {"type": "equation", "text": "$$\n0\\leq f(\\mathbf{X}_{1})-f^{*}+\\underbrace{\\sum_{k=1}^{t}{-\\hat{\\rho}_{k}\\left\\langle{\\bar{G}_{k},\\frac{\\tilde{G}_{k}}{\\sqrt{W_{k}}}}\\right\\rangle}}_{\\mathbf{D}}+\\underbrace{\\sum_{k=1}^{t}{\\frac{L\\hat{\\rho}_{k}^{2}}{2}\\left\\|{\\frac{\\tilde{G}_{k}}{\\sqrt{W_{k}}}}\\right\\|_{F}^{2}}}_{\\mathbf{E}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "678 Introducing $A_{k}$ in (50), we further have the following decomposition, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\bf{D}}=-\\displaystyle\\sum_{k=1}^{t}{\\hat{\\rho}_{k}\\left\\langle{\\bar{G}_{k},\\frac{\\tilde{G}_{k}}{\\sqrt{A_{k}}}}\\right\\rangle}+\\displaystyle\\sum_{k=1}^{t}{\\hat{\\rho}_{k}\\left\\langle{\\bar{G}_{k},\\left(\\frac{1}{\\sqrt{A_{k}}}-\\frac{1}{\\sqrt{W_{k}}}\\right)\\odot\\tilde{G}_{k}}\\right\\rangle}}}\\\\ {{\\mathrm{}}}\\\\ {{\\bf{\\Omega}=-\\displaystyle\\sum_{k=1}^{t}{\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\|_{F}^{2}+{\\bf D}.1}}}\\\\ {{\\displaystyle\\sum_{k=1}^{t}{\\hat{\\rho}_{k}\\left\\langle{\\bar{G}_{k},\\frac{\\tilde{G}_{k}}{\\sqrt{A_{k}}}-\\mathbb{E}_{Z_{k}}\\left[\\frac{\\tilde{G}_{k}}{\\sqrt{A_{k}}}\\right]}\\right\\rangle}+\\displaystyle\\sum_{k=1}^{t}{\\hat{\\rho}_{k}\\left\\langle{\\bar{G}_{k},\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}-\\mathbb{E}_{Z_{k}}\\left[\\frac{\\tilde{G}_{k}}{\\sqrt{A_{k}}}\\right]}\\right\\rangle}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "679  Estimating E  Hence, using (98), (99), Lemma B.4 and Lemma B.5, we derive that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf E}\\leq\\frac{L}{2}\\sum_{k=1}^{t}\\hat{\\rho}_{k}^{2}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}\\leq\\frac{L(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})^{2}\\rho_{0}^{2}}{2}\\sum_{k=1}^{t}(1-\\beta_{2,k})\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}}\\\\ {{\\displaystyle\\quad\\leq\\frac{L m n\\mathcal{G}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})^{2}\\rho_{0}^{2}}{\\operatorname*{max}\\{m,n\\}\\epsilon_{1}}\\left[\\log\\left(2+\\frac{2G^{2}}{\\epsilon_{1}}\\right)+4\\sum_{k=1}^{t}(1-\\beta_{2,k})\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "680To avoid the curse of dimension, we drive from (98) and (83) that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\tilde{G}_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}=\\frac{1}{\\left(\\operatorname*{max}\\{1,\\|U_{k}\\|_{F}/(d_{k}\\sqrt{m n})\\}\\right)^{2}}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}\\leq\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}\\leq G_{3}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "681 Then, using (99) and (103), we derive that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{E}\\leq\\frac{L G_{3}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})^{2}\\rho_{0}^{2}}{2}\\sum_{k=1}^{t}\\frac{1}{k}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "682Estimating D.1 We could follow the similar deduction in (73) and (74) to derive that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}.\\mathbf{1}\\leq\\displaystyle\\sum_{k=1}^{t}\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\hat{\\rho}_{k}|\\bar{g}_{i j}^{(k)}\\bar{g}_{i j}^{(k)}|\\left|\\frac{1}{\\sqrt{w_{i j}^{(k)}}}-\\frac{1}{\\sqrt{a_{i j}^{(k)}}}\\right|}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{k=1}^{t}\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\hat{\\rho}_{k}\\frac{|\\bar{g}_{i j}^{(k)}\\bar{g}_{i j}^{(k)}|}{\\sqrt{w_{i j}^{(k)}}\\sqrt{a_{i j}^{(k)}}}\\sqrt{\\left|w_{i j}^{(k)}-a_{i j}^{(k)}\\right|}}\\\\ &{\\qquad\\leq\\frac{1}{4}\\displaystyle\\sum_{k=1}^{t}\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\hat{\\rho}_{k}\\cdot\\frac{\\left(\\bar{g}_{i j}^{(k)}\\right)^{2}}{\\sqrt{a_{i j}^{(k)}}}+4\\displaystyle\\sum_{k=1}^{t}\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\hat{\\rho}_{k}\\cdot\\frac{\\left|w_{i j}^{(k)}-a_{i j}^{(k)}\\right|}{\\sqrt{a_{i j}^{(k)}}}\\cdot\\left(\\frac{\\bar{g}_{i j}^{(k)}}{\\sqrt{w_{i j}^{(k)}}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "683  Using Lemma B.7 and (105), we further derive that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbf{D.1}\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+4\\sqrt{g}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\sqrt{1-\\beta_{2,k}}\\left\\|\\frac{\\tilde{G}_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}\\\\ {\\displaystyle\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+4\\sqrt{g}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\sqrt{1-\\beta_{2,k}}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "684 Using (99), Lemma B.4 and Lemma B.5, we further have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf D}.1\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+4\\sqrt{\\mathcal{G}}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}\\sum_{k=1}^{t}(1-\\beta_{2,k})\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}}\\\\ {{\\displaystyle\\qquad\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+\\frac{8m n\\bar{\\mathcal{G}}^{3}(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}}{\\operatorname*{max}\\{m,n\\}\\epsilon_{1}}\\left[\\log\\left(2+\\frac{2G^{2}}{\\epsilon_{1}}\\right)+4\\sum_{k=1}^{t}(1-\\beta_{2,k})\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "685  To avoid the curse of dimension, we apply Lemma B.7, (99) and (83) to derive that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bf{D.1}}\\leq\\displaystyle\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{\\bf{G}}_{k}}{\\sqrt[{4}]{4}k}\\right\\|_{F}^{2}+4(G_{1}+G_{2})\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\sqrt{1-\\beta_{2,k}}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}\\\\ {\\displaystyle~~\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{\\bf{G}}_{k}}{\\sqrt[{4}]{4}k}\\right\\|_{F}^{2}+4(G_{1}+G_{2})(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}\\sum_{k=1}^{t}\\frac{1}{k^{c/2+1/2}}\\left\\|\\frac{G_{k}}{\\sqrt{W_{k}}}\\right\\|_{F}^{2}}\\\\ {\\displaystyle~~\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{\\bf{G}}_{k}}{\\sqrt[{4}]{4k}}\\right\\|_{F}^{2}+4G_{3}(G_{1}+G_{2})(\\epsilon_{2}+\\Theta_{\\operatorname*{max}})\\rho_{0}\\sum_{k=1}^{t}\\frac{1}{k^{c/2+1/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "686 Estimating D.2 Since $A_{k}$ is independent from $Z_{k}$ , it further leads to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{D.2}=-\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\langle\\frac{\\bar{G}_{k}}{\\sqrt{{\\cal A}_{k}}},\\tilde{G}_{k}-\\mathbb{E}_{Z_{k}}\\left[\\tilde{G}_{k}\\right]\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "687Then, the deduction for estimating D.2 follows the similar idea as in Lemma B.6, relying on a   \n68 martingale difference sequence.   \n689 Let us set $\\begin{array}{r}{\\varphi_{k}=-\\hat{\\rho}_{k}\\left\\langle\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}},\\tilde{G}_{k}-\\mathbb{E}_{Z_{k}}\\left[\\tilde{G}_{k}\\right]\\right\\rangle}\\end{array}$ and the fitration $\\mathcal{F}_{k}=\\sigma\\left(Z_{1},\\cdot\\cdot\\cdot\\right,Z_{k}\\right)$ . Noting that   \n690 $\\hat{\\rho}_{k}$ $\\bar{G}_{k}$ and $A_{k}$ are dependent by $\\mathcal{F}_{k-1}$ .Since $\\xi_{k}$ is dependent by $\\mathcal{F}_{k}$ , we could prove that $\\{\\varphi_{k}\\}_{k\\ge1}$   \n691 is a martingale difference sequence by showing that ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\varphi_{k}\\ |\\ \\mathcal{F}_{k-1}\\right]=-\\hat{\\rho}_{k}\\left\\langle\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}},\\mathbb{E}_{Z_{k}}\\left[\\tilde{G}_{k}-\\mathbb{E}_{Z_{k}}[\\tilde{G}_{k}]\\right]\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "692  In addition, using Assumptions (A3), (A4) and Jensen's inequality, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\tilde{G}_{k}\\|_{F}=\\frac{\\|G_{k}\\|_{F}}{\\operatorname*{max}\\{1,\\|U_{k}\\|/(d_{k}\\sqrt{m n})\\}}\\le\\|G_{k}\\|_{F}\\le G,\\quad\\|\\mathbb{E}_{Z_{k}}[\\tilde{G}_{k}]\\|_{F}\\le\\mathbb{E}_{Z_{k}}\\|\\tilde{G}_{k}\\|_{F}\\le G.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "693 Therefore, we derive that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\tilde{G}_{k}-\\mathbb{E}_{Z_{k}}[\\tilde{G}_{k}]\\|_{F}\\leq\\|\\tilde{G}_{k}\\|_{F}+\\|\\mathbb{E}_{Z_{k}}[\\tilde{G}_{k}]\\|_{F}\\leq2G.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "694  Let $\\begin{array}{r}{\\omega_{k}^{\\prime}=2G\\hat{\\rho}_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\rVert_{F}}\\end{array}$ .We thus derive from the Cauchy-Schwarz inequality and (108) that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{\\varphi_{k}^{2}}{(\\omega_{k}^{\\prime})^{2}}\\right)\\mid\\mathcal{F}_{k-1}\\right]\\leq\\mathbb{E}\\left[\\exp\\left(\\frac{\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\rVert_{F}^{2}\\left\\lVert\\tilde{G}_{k}-\\mathbb{E}_{Z_{k}}[\\tilde{G}_{k}]\\right\\rVert_{F}^{2}}{4G^{2}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\rVert_{F}^{2}}\\right)\\mid\\mathcal{F}_{k-1}\\right]\\leq\\exp(1).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "695 Then, using Lemma B.1, it leads to that for any $\\lambda>0$ , with probability at least $1-\\delta$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}.\\mathbf{2}=\\displaystyle\\sum_{k=1}^{t}\\varphi_{k}\\leq3\\lambda G^{2}\\displaystyle\\sum_{k=1}^{t}\\hat{\\rho}_{k}^{2}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\|_{F}^{2}+\\frac{1}{\\lambda}\\log\\left(\\displaystyle\\frac{1}{\\delta}\\right)}\\\\ &{\\qquad=3\\lambda G^{2}\\displaystyle\\sum_{k=1}^{t}\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\frac{\\hat{\\rho}_{k}}{\\sqrt{a_{i j}^{(k)}}}\\cdot\\hat{\\rho}_{k}\\frac{\\left(\\bar{g}_{i j}^{(k)}\\right)^{2}}{\\sqrt{a_{i j}^{(k)}}}+\\frac{1}{\\lambda}\\log\\left(\\displaystyle\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "696 Since $\\{\\beta_{2,k}\\}_{k\\geq2}$ is non-decreasing, we could apply Lemma B.3 to derive that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{a_{i j}^{(k)}}}\\le\\sqrt{\\frac{1}{\\beta_{2,k}(1-\\beta_{2,k})\\epsilon_{1}}}\\le\\sqrt{\\frac{1}{\\operatorname*{min}\\{\\beta_{2,1},\\beta_{2,2}\\}(1-\\beta_{2,k})\\epsilon_{1}}}\\le\\frac{2}{\\sqrt{(1-\\beta_{2,k})\\epsilon_{1}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "697   Then, we apply (99), and re-scale $\\delta$ to obtain that for any $\\lambda>0$ , with probability at least $1-\\delta$ ,for   \n698 all $t\\in[T]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{D}.\\mathbf{2}\\leq\\frac{6\\lambda G^{2}\\rho_{0}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})}{\\sqrt{\\epsilon_{1}}}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+\\frac{1}{\\lambda}\\log\\left(\\frac{T}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "699 Setting $\\lambda=\\sqrt{\\epsilon_{1}}/(24G^{2}\\rho_{0}(\\epsilon_{2}+\\Theta_{\\mathrm{max}}))$ ), we derive that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{D}.\\mathbf{2}\\leq\\frac{1}{4}\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\|_{F}^{2}+\\frac{24G^{2}\\rho_{0}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})}{\\sqrt{\\epsilon_{1}}}\\log\\left(\\frac{T}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "700 Estimating D.3 First, since $A_{k}$ is independent from $Z_{k}$ and $\\mathbb{E}_{Z_{k}}[G_{k}]={\\bar{G}}_{k}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}.\\mathbf{3}=\\displaystyle\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\langle\\bar{G}_{k},\\frac{\\mathbb{E}_{Z_{k}}[G_{k}]}{\\sqrt{A_{k}}}-\\frac{\\mathbb{E}_{Z_{k}}[\\tilde{G}_{k}]}{\\sqrt{A_{k}}}\\right\\rangle}\\\\ &{\\quad\\leq\\displaystyle\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\rVert_{F}\\cdot\\left\\lVert\\mathbb{E}_{Z_{k}}\\underbrace{\\left[G_{k}-\\frac{G_{k}}{\\operatorname*{max}\\{1,\\lVert U_{k}\\rVert_{F}/\\left(d_{k}\\sqrt{m n}\\right)\\}}\\right]}_{\\Omega_{k}}\\right\\rVert_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "701 We define the random variable S<, s2 and $\\tilde{S}_{k}^{(1)}$ using he indicator function $\\chi$ and $G_{4}$ in (94) as   \n702 follows, ", "page_idx": 29}, {"type": "equation", "text": "$$\nS_{k}^{(1)}=\\chi_{\\{\\|U_{k}\\|_{F}>d_{k}\\sqrt{m n}\\}},\\quad S_{k}^{(2)}=\\chi_{\\{\\|U_{k}\\|_{F}\\leq d_{k}\\sqrt{m n}\\}},\\quad\\tilde{S}_{k}^{(1)}=\\chi_{\\{\\|G_{k}\\|_{F}\\geq d_{k}G_{4}\\}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "703 From (83), we derive that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|U_{k}\\|_{F}\\leq\\|G_{k}\\|_{F}\\cdot{\\frac{2{\\sqrt{\\mathcal{G}}}}{\\sqrt{m n}\\epsilon_{1}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "704  Hence, $S_{k}^{(1)}\\le\\tilde{S}_{k}^{(1)},\\forall k\\ge1.$ Note that when $S_{k}^{(2)}=1$ ts equivalent to $\\Omega_{k}=0$ . Then, we derive   \n705that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbb{E}_{Z_{k}}[\\Omega_{k}]\\right\\|_{F}=\\left\\|\\mathbb{E}_{Z_{k}}[\\Omega_{k}S_{k}^{(1)}]+\\mathbb{E}_{Z_{k}}[\\Omega_{k}S_{k}^{(2)}]\\right\\|_{F}=\\left\\|\\mathbb{E}_{Z_{k}}[\\Omega_{k}S_{k}^{(1)}]\\right\\|_{F}}\\\\ &{\\le\\mathbb{E}_{Z_{k}}\\left\\|\\Omega_{k}S_{k}^{(1)}\\right\\|_{F}\\le\\mathbb{E}_{Z_{k}}\\left\\|\\Omega_{k}\\tilde{S}_{k}^{(1)}\\right\\|_{F}\\le\\mathbb{E}_{Z_{k}}\\left\\|G_{k}\\tilde{S}_{k}^{(1)}\\right\\|_{F}\\le G^{\\alpha}\\left(d_{k}G_{4}\\right)^{1-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "7heuAder $a_{i j}^{(k)}$ Wwhre", "page_idx": 30}, {"type": "equation", "text": "$$\na_{i j}^{(k)}\\geq\\frac{m n\\epsilon_{1}^{2}}{4\\mathcal{G}},\\quad\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{A}_{k}}\\right\\|_{F}\\leq\\frac{\\|\\bar{G}_{k}\\|_{F}}{\\operatorname*{min}_{i,j}\\sqrt{a_{i j}^{(k)}}}\\leq\\frac{2G\\sqrt{\\mathcal{G}}}{\\sqrt{m n}\\epsilon_{1}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "707Combining with (99), (110), (111) and (112), we thus derive that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{D.3}\\leq\\frac{2G^{1+\\alpha}G_{4}^{1-\\alpha}\\sqrt{\\mathcal{G}}(\\epsilon_{2}+\\Theta_{\\mathrm{max}})\\rho_{0}}{\\sqrt{m n}\\epsilon_{1}}\\sum_{k=1}^{t}\\frac{1}{d_{k}^{\\alpha-1}\\sqrt{k}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "708Putting together Both $\\mathbf{E}$ and D.1 are bounded with two estimations, one of which owns a better   \n709dependency to $1/\\epsilon_{1}$ and the other avoids the curse of the dimension. We thereby derive two results.   \n710Plugging (106), (109) and (113) into (101) and then combining with (102) and (100), we then derive   \n711 that with probability at least $1-\\delta$ , for all $t\\in[T]$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac12\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{\\bar{\\mathcal{A}}_{k}}}\\right\\|_{F}^{2}\\le C_{1}\\log\\left(\\frac T\\delta\\right)+C_{2}\\sum_{k=1}^{t}(1-\\beta_{2,k})+C_{3}+D_{1}(\\alpha)\\sum_{k=1}^{t}\\frac1{d_{k}^{\\alpha-1}\\sqrt{k}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "712 where $C_{1},C_{2},C_{3}$ are as in Theorem B.1 and $D_{1}(\\alpha)$ is as in (94). Plugging (107), (109) and (113)   \n713 into (101), then combining with (104) and (100), we then derive that with probability at least $1-\\delta$   \n714 for all $t\\in[T]$ \uff0c ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac12\\sum_{k=1}^{t}\\hat{\\rho}_{k}\\left\\|\\frac{\\bar{G}_{k}}{\\sqrt{A_{k}}}\\right\\|_{F}^{2}\\le C_{1}\\log\\left(\\frac T\\delta\\right)+C_{2}^{\\prime}\\sum_{k=1}^{t}\\frac1{k^{c/2+1/2}}+C_{3}^{\\prime}\\sum_{k=1}^{t}\\frac1k+D_{1}(\\alpha)\\sum_{k=1}^{t}\\frac1{d_{k}^{\\alpha-1}\\sqrt{k}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "715where $C_{2}^{\\prime},C_{3}^{\\prime}$ are as in Theorem B.1. Moreover, using (99), we reveal that the lower bound for $\\hat{\\rho}_{k}$ is   \n716 the same the one for $\\eta_{k}$ in (58). Thereby, following the same deduction in (80) and (86), we derive   \n717 that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{k=1}^{T}\\hat{\\rho}_{k}\\left\\lVert\\frac{\\bar{G}_{k}}{\\sqrt[4]{A_{k}}}\\right\\rVert_{F}^{2}\\geq\\sum_{k=1}^{T}\\frac{\\hat{\\rho}_{k}}{2}\\frac{\\left\\lVert\\bar{G}_{k}\\right\\rVert_{F}^{2}}{\\operatorname*{max}_{i,j}\\sqrt{a_{i j}^{(k)}}}\\geq\\frac{1}{D_{0}}\\sum_{k=1}^{T}\\frac{1}{\\sqrt{k}}\\left\\lVert\\bar{G}_{k}\\right\\rVert_{F}^{2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "718 where $D_{0}=\\operatorname*{min}\\{C_{0},C_{0}^{\\prime}\\}$ that has been defined in (94). Setting $t=T$ on (114) and (115), and then   \n719using (116), we then derive that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\le[T]}{\\operatorname*{min}}\\left\\|\\bar{G}_{k}\\right\\|_{F}^{2}\\le\\frac{D_{0}}{\\sum_{t=1}^{T}1/\\sqrt{k}}\\left(C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}\\sum_{k=1}^{t}(1-\\beta_{2,k})+C_{3}+D_{1}(\\alpha)\\displaystyle\\sum_{k=1}^{t}\\frac{1}{d_{k}^{\\alpha-1}\\sqrt{k}}\\right),}\\\\ &{\\underset{\\le[T]}{\\operatorname*{min}}\\left\\|\\bar{G}_{k}\\right\\|_{F}^{2}\\le\\frac{D_{0}}{\\sum_{t=1}^{T}1/\\sqrt{k}}\\left(C_{1}\\log\\left(\\frac{T}{\\delta}\\right)+C_{2}^{\\prime}\\displaystyle\\sum_{k=1}^{t}\\frac{1}{k^{(c+1)/2}}+C_{3}^{\\prime}\\displaystyle\\sum_{k=1}^{t}\\frac{1}{k}+D_{1}(\\alpha)\\displaystyle\\sum_{k=1}^{t}\\frac{1}{d_{k}^{\\alpha-1}\\sqrt{k}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "720 Then, using the results in (88) and (89), we could derive the desired result in Theorem C.1. ", "page_idx": 30}, {"type": "text", "text": "722 The checklist is designed to encourage best practices for responsible machine learning research,   \n723  addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n724  the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n725  follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n726 towards the page limit.   \n727 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n728each question in the checklist:   \n729 \u00b7 You should answer [Yes] , [No] , or [NA] .   \n730 \u00b7 [NA] means either that the question is Not Applicable for that particular paper or the   \n731 relevant information is Not Available.   \n732 \u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA).   \n733 The checklist answers are an integral part of your paper submission. They are visible to the   \n734  reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n735  (after eventual revisions) with the final version of your paper, and its final version will be published   \n736with the paper.   \n737 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n738 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n739 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n740 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n741 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n742 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n743 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n744 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n745please point to the section(s) where related material for the question can be found. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "746  IMPORTANT, please: ", "page_idx": 31}, {"type": "text", "text": "747 \u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n748 \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below.   \n749 \u00b7 Do not modify the questions and only use the provided macros for your answers.   \n750 1. Claims   \n751 Question: Do the main claims made in the abstract and introduction accurately refect the   \n752 paper's contributions and scope?   \n753 Answer: [Yes]   \n754 Justification: [NA]   \n755 Guidelines:   \n756 \u00b7 The answer NA means that the abstract and introduction do not include the claims   \n757 made in the paper.   \n758 \u00b7 The abstract and/or introduction should clearly state the claims made, including the   \n759 contributions made in the paper and important assumptions and limitations. A No or   \n760 NA answer to this question will not be perceived well by the reviewers.   \n761 \u00b7 The claims made should match theoretical and experimental results, and reflect how   \n762 much the results can be expected to generalize to other settings.   \n763 \u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n764 are not attained by the paper.   \n765 2. Limitations   \n766 Question: Does the paper discuss the limitations of the work performed by the authors?   \n767 Answer: [Yes]   \n768 Justification: [NA]   \n69 Guidelines:   \n70 \u00b7 The answer NA means that the paper has no limitation while the answer No means that   \n71 the paper has limitations, but those are not discussed in the paper.   \n72 \u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n73 \u00b7 The paper should point out any strong assumptions and how robust the results are to   \n74 violations of thee assumptions eg, independence assumptions, noiseless setting,   \n75 model welspecifcation, asymptotic aproximations only holding locally). The authors   \n76 should refect on how these assumptions might be violated in practice and what the   \n77 implications would be.   \n78 \u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was   \n79 only tested on a few datasets or with a few runs. In general, empirical results often   \n80 depend on implicit assumptions, which should be articulated.   \n81 \u00b7The authors should refect on the factors that inffuence the performance of the approach.   \n82 For example, a facial recognition algorithm may perform poorly when image resolution   \n83 is low orimages are taken in low lighting. Ora speech-to-text system might not be   \n84 used reliably to provide closed captions for online lectures because it fails to handle   \n85 technical jargon.   \n86 \u00b7 The authors should discussthe computational eficiency of the proposed algorithms   \n87 and how they scale with dataset size.   \n88 \u00b7 If applicable, the authors should discuss possible limitations of their approach to   \n89 address problems of privacy and fairness.   \n90 \u00b7While the authors might fear that complete honesty about limitations might be used by   \n91 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n92 limitations that aren't acknowledged in the paper. The authors should use their best   \n93 judgment and recognize that individual actions in favor of transparency play an impor  \n94 tant role in developing norms that preserve the integrity of the community. Reviewers   \n95 will be specifically instructed to not penalize honesty concerning limitations.   \n96 3. Theory Assumptions and Proofs   \n97 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n98 a complete (and correct) proof?   \n99 Answer: [Yes]   \n00 Justification: [NA]   \n01 Guidelines:   \n02 \u00b7 The answer NA means that the paper does not include theoretical results.   \n03 \u00b7 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n04 referenced.   \n05 \u00b7All asumptions should be clearly stated or referenced in the statement of any theorems.   \n06 \u00b7 The proofs can either appear in the main paper or the supplemental material, but if   \n07 they appear in the supplemental material, the authors are encouraged to provide a short   \n08 proof sketch to provide intuition.   \n09 \u00b7 Inversely, any informal proof provided in the core of the paper should be complemented   \n10 by formal proofs provided in appendix or supplemental material.   \n11 \u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n12 4. Experimental Result Reproducibility   \n13 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n14 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n15 of the paper (regardless of whether the code and data are provided or not)?   \n16 Answer: [Yes]   \n317 Justification: [NA]   \n18 Guidelines:   \n19 \u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Justification: Our code is based on Pytorch package which is standard. In addition, we have clarified the detailed experimental setup in our paper and the experiments are easy to reproduce. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 33}, {"type": "text", "text": "876 \u00b7 Providing as much information as possible in supplemental material (appended to the   \n877 paper) is recommended, but including URLs to data and code is permitted.   \n878 6. Experimental Setting/Details   \n879 Question: Does the paper specify all the training and test details (e.g, data splits, hyper  \n880 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n881 results?   \n882 Answer: [Yes]   \n883 Justification: [NA]   \n884 Guidelines:   \n885 \u00b7 The answer NA means that the paper does not include experiments.   \n886 \u00b7 The experimental setting should be presented in the core of the paper to a level of detail   \n887 that is necessary to appreciate the results and make sense of them.   \n888 \u00b7 The full details can be provided either with the code, in appendix, or as supplemental   \n889 material.   \n890 7. Experiment Statistical Significance   \n891 Question: Doesthe paper report errorbars suitably and correctly dened or ther appropriate   \n892 information about the statistical significance of the experiments?   \n893 Answer: [Yes]   \n894 Justification: [NA]   \n895 Guidelines:   \n896 \u00b7 The answer NA means that the paper does not include experiments.   \n897 \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, conf  \n898 dence intervals or statistical signifcance tests, at least for the xperments that suport   \n899 the main claims of the paper.   \n900 \u00b7The factors of variability that the error bars are capturing should be clearly stated (for   \n901 example, train/test split, initialization, random drawing of some parameter, or overall   \n902 run with given experimental conditions).   \n903 \u00b0 The method for calculating the error bars should be explained (closed form formula,   \n904 call to library function, bootstrap, etc.)   \n905 \u00b7 The assumptions made should be given (e., Normally distributed errors).   \n906 \u00b7 It should be clear whether the error bar is the standard deviation or the standard error   \n907 of the mean.   \n908 \u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n909 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n910 of Normality of errors is not verified.   \n911 \u00b7 For asymmetric distributions, the authors should be careful not to show in tables or   \n912 figures symmetric eror bars that would yield results that are out of range (e.g. negative   \n913 error rates).   \n914 \u00b7 If error bars are reported in tables or plots, The authors should explain in the text how   \n915 they were calculated and reference the corresponding figures or tables in the text.   \n916 8. Experiments Compute Resources   \n917 Question: For each experiment, does the paper provide sufficient information on the com  \n918 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n919 the experiments?   \n920 Answer: [Yes]   \n921 Justification: [NA]   \n922 Guidelines:   \n923 \u00b7 The answer NA means that the paper does not include experiments.   \n924 \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n925 or cloud provider, including relevant memory and storage. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "41   \n342 10. Broader Impacts   \n43 Question: Does the paper discuss both potential positive societal impacts and negative   \n44 societal impacts of the work performed?   \n45 Answer: [NA]   \n46 Justification: [NA]   \n47 Guidelines:   \n48 \u00b7 The answer NA means that there is no societal impact of the work performed.   \n49 \u00b7 If the authors answer NA or No, they should explain why their work has no societal   \n50 impact or why the paper does not address societal impact.   \n51 \u00b7 Examples of negative societal impacts include potential malicious or unintended uses   \n52 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n53 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n54 groups), privacy considerations, and security considerations.   \n55 \u00b7 The conference expects that many papers will be foundational research and not tied   \n56 to particular applications, let alone deployments. However, if there is a direct path to   \n57 any negative applications, the authors should point it out. For example, it is legitimate   \n58 to point out that an improvement in the quality of generative models could be used to   \n59 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n60 that a generic algorithm for optimizing neural networks could enable people to train   \n61 models that generate Deepfakes faster.   \n62 \u00b7 The authors should consider possible harms that could arise when the technology is   \n63 being used as intended and functioning correctly, harms that could arise when the   \n64 technology is being used as intended but gives incorrect results, and harms following   \n65 from (intentional or unintentional) misuse of the technology.   \n66 \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation   \n67 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n68 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n69 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "987 12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "9388 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n389 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n390 properly respected?   \n991 Answer: [NA]   \n992 Justification: [NA]   \n993 Guidelines: ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "24 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n25 include the full text of instructions given to participants and screenshots, if applicable, as   \n26 well as details about compensation (if any)?   \n1027 Answer: [NA]   \n1028 Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n341   \n1042   \n1043   \n1044   \n1045   \n1046   \n347   \n148   \n1049   \n)50   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056 ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution. \u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]