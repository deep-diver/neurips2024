[{"figure_path": "ZbjJE6Nq5k/figures/figures_4_1.jpg", "caption": "Figure 2: Continual random-labels CIFAR training: simple feedforward network architecture (No Norm) exhibits rapid growth in its parameter norm and the norm of its gradients, whereas the otherwise-identical network with layer normalization sees parameter norm growth coupled with a reduction in the norm of its gradients and reduced performance on later tasks. Constraining the parameter norm of this network maintains the performance of a random initialization.", "description": "This figure compares the performance of three different network architectures on a continual learning task. The x-axis represents the number of label resets, which means how many times the labels of the training data have been changed. The y-axis of the three subplots represents the final loss, the Jacobian norm, and the parameter norm respectively. The first architecture is a simple feedforward network without normalization, which shows rapid growth in both parameter and Jacobian norms, resulting in poor performance. The second architecture uses layer normalization, which shows reduced Jacobian norm and slower performance degradation but still shows performance decline as parameter norm increases. The third architecture uses layer normalization and weight projection, which is able to maintain the parameter norm at a constant level, resulting in constant performance as good as the initialization. This experiment shows how layer normalization can help to avoid plasticity loss, and how weight projection can help to improve the performance of layer normalization.", "section": "3.2 Parameter norm and effective learning rate decay"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_5_1.jpg", "caption": "Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.", "description": "The figure shows the fraction of dormant ReLU units over training steps for different optimizers (SGD, Adam, SGD with momentum) and normalization methods (no normalization, RMSNorm, LayerNorm).  The task is continual learning where the MNIST labels are randomly reassigned every 1000 steps.  The plot shows that networks using normalization layers (RMSNorm and LayerNorm) are less susceptible to accumulating dead units compared to networks without normalization, indicating normalization's ability to help networks recover from periods of low plasticity.", "section": "3.1 Normalization and ReLU revival"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_6_1.jpg", "caption": "Figure 3: We run a 'coupled networks' experiment as described in the text. All networks exhibit similar learning curves, as seen by the rightmost subplot, however there is small but visible gap between the learning curves obtained by NaP and an unconstrained network with fixed learning rates. Using a global learning rate schedule almost entirely closes this gap, but does not induce a precise equivalence in the dynamics as obtained by layer-wise rescaling (leftmost).", "description": "This figure compares the learning curves of four different network training setups.  All networks are trained on the same task.  The first three subplots show layer-wise rescaling with NaP, global rescaling, and no rescaling of the learning rates. The last subplot combines all three into one graph.  The key finding is that while a global learning rate schedule produces very similar results to the NaP approach, perfectly matching the dynamics requires layer-wise rescaling.", "section": "4 Understanding effective learning rate dynamics"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_7_1.jpg", "caption": "Figure 4: Without an explicit learning rate schedule, a Rainbow trained with NaP may fail to make any performance improvement; while the implicit schedule induced by the parameter norm is clearly important to performance, in several games this is significantly outperformed by a simple linear schedule terminating halfway through training. Intriguingly, we see a characteristic sharp improvement near the end of the decay schedule in several (though not all, e.g. fishing derby) games.", "description": "This figure shows the normalized return of Rainbow agents trained on five different Atari games with different training protocols. The x-axis represents the training progress in millions of frames, and the y-axis represents the normalized return. The different lines represent different training protocols: Rainbow + LN (layer normalization), Rainbow + LN + WP (layer normalization and weight projection), and LN + WP + Schedule (layer normalization, weight projection, and learning rate schedule). The figure demonstrates that the implicit learning rate schedule in Rainbow agents is important for performance but not optimal; using an explicit learning rate schedule can significantly improve performance.", "section": "4.2 Implicit learning rate schedules in deep RL"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_8_1.jpg", "caption": "Figure 5: Robustness to nonstationarity: we see that without NaP, there is a wide spread in the effectiveness of various plasticity-preserving methods across two architectures. Once we incorporate NaP, however, the gaps between these methods shrink significantly and almost uniformly improves over the unconstrained baseline.", "description": "The figure shows the average online accuracy of different plasticity-preserving methods (ReDO, Regenerative regularization, Noisy updates, leaky ReLU activation, Shrink & Perturb, and L2 reg) on a continual random label memorization task using two architectures (CNN and MLP).  The left two panels show the performance of these methods without using the proposed Normalize-and-Project (NaP) method. The right two panels show the performance of the same methods when NaP is used.  The results demonstrate that while there's a large variation in performance among the methods without NaP, the introduction of NaP dramatically reduces this variation and improves overall performance across all methods.", "section": "5.1 Robustness to nonstationarity"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_9_1.jpg", "caption": "Figure 6: Left: We visualize the learning curves of continual atari agents on sequential ALE training (i.e. 200M frames). Each game is played for 20M frames, and agents pass sequentially from one to another, repeating all ten games twice for a total of 400M training frames. Solid lines indicate performance on the second visit to each game, and dotted lines indicate performance of a randomly initialized network on the game. Even in its second visit to each game, NaP performs comparably the randomly initialized networks, whereas the standard rainbow agent exhibits poor performance on all games in the sequential training regime. Right: aggregate effects of normalization on single-task atari, computed via the approach of Agarwal et al. [2021]. Bars indicate 95% confidence intervals over 4 seeds and 57 environments.", "description": "The left panel of Figure 6 shows learning curves for continual learning on Atari games.  Ten games were played sequentially, each for 20M frames and repeated twice for a total of 400M frames.  The results demonstrate that NaP maintains plasticity, showing performance comparable to a randomly initialized network even on repeated games, unlike a standard Rainbow agent.  The right panel provides a summary of the results from single-task Atari experiments, showing the improvement of NaP in terms of median and interquartile mean scores.", "section": "Deep reinforcement learning"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_21_1.jpg", "caption": "Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.", "description": "This figure shows the fraction of dormant ReLU units over training steps for a network trained on an iterated random label memorization task using different optimizers (SGD, Adam, SGD+momentum) and normalization techniques (no normalization, RMSNorm, LayerNorm).  The task involves re-randomizing the MNIST dataset labels every 1000 steps, creating non-stationarity. The plot demonstrates that networks employing layer normalization (LayerNorm) exhibit better resilience against spikes in the number of dead units compared to networks without normalization. The results suggest that LayerNorm helps recover from the temporary deactivation of ReLU units caused by the non-stationary nature of the task.", "section": "3.1 Normalization and ReLU revival"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_22_1.jpg", "caption": "Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.", "description": "This figure shows the fraction of dead ReLU units over training steps for different optimizers (SGD, Adam, SGD with momentum) and normalization methods (no normalization, RMSNorm, LayerNorm). The task is continual learning where the MNIST labels are re-randomized every 1000 steps.  The key observation is that networks with normalization layers (RMSNorm, LayerNorm) show a significantly reduced number of dead units compared to networks without normalization, demonstrating their ability to recover from periods of high unit saturation.", "section": "3.1 Normalization and ReLU revival"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_23_1.jpg", "caption": "Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.", "description": "This figure visualizes the accumulation of dead ReLU units during an iterated random label memorization task on the MNIST dataset.  The labels are randomly reassigned every 1000 optimization steps, simulating a non-stationary environment.  The plot compares the fraction of dormant units over training steps for networks with different normalization layers (LayerNorm, RMSNorm) and a network without normalization.  It demonstrates that networks incorporating normalization layers are more resilient to the spikes in dead unit counts caused by the label changes, showcasing their ability to recover plasticity.", "section": "3.1 Normalization and ReLU revival"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_23_2.jpg", "caption": "Figure 5: Robustness to nonstationarity: we see that without NaP, there is a wide spread in the effectiveness of various plasticity-preserving methods across two architectures. Once we incorporate NaP, however, the gaps between these methods shrink significantly and almost uniformly improves over the unconstrained baseline.", "description": "The figure shows the results of experiments on a continual classification problem where the labels of an image dataset are re-randomized iteratively.  Multiple plasticity-preserving methods were evaluated on two architectures, a CNN and an MLP, both with and without the Normalize-and-Project (NaP) method. Without NaP, the performance of these methods varied significantly. However, with NaP, the performance gaps between these methods reduced substantially, and NaP consistently improved over the baseline without any plasticity-preserving methods.", "section": "5.1 Robustness to nonstationarity"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_24_1.jpg", "caption": "Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.", "description": "This figure shows the fraction of dormant units (ReLU units that are always zero) over training steps for a network trained on a task where the labels are re-randomized every 1000 steps.  The different lines represent different normalization methods (no normalization, RMSNorm, LayerNorm).  The key observation is that networks using normalization layers (RMSNorm and LayerNorm) are much more resilient to increases in the number of dormant units and are able to recover from periods where many units become dormant.", "section": "3.1 Normalization and ReLU revival"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_24_2.jpg", "caption": "Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.", "description": "This figure shows the fraction of \"dead\" ReLU units over training steps in a network trained on a task where labels are re-randomized every 1000 steps.  Different optimizers (SGD, Adam, SGD with momentum) and normalization methods (no normalization, RMSNorm, LayerNorm) are compared.  The results indicate that networks with normalization layers are more resilient to spikes in the number of dead units and can recover more effectively.", "section": "3.1 Normalization and ReLU revival"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_26_1.jpg", "caption": "Figure 2: Continual random-labels CIFAR training: simple feedforward network architecture (No Norm) exhibits rapid growth in its parameter norm and the norm of its gradients, whereas the otherwise-identical network with layer normalization sees parameter norm growth coupled with a reduction in the norm of its gradients and reduced performance on later tasks. Constraining the parameter norm of this network maintains the performance of a random initialization.", "description": "This figure shows the results of training a simple feedforward network and a similar network with layer normalization on a continual learning task using CIFAR-10 with randomly relabeled data.  It illustrates that the network without normalization shows significant growth in both parameter norm and gradient norm, resulting in decreased performance over time. Conversely, the normalized network shows parameter norm growth but with reduced gradient norm, still experiencing a performance drop but less severe than the non-normalized network. Finally, constraining the parameter norm in the normalized network maintains performance close to the initial random initialization.", "section": "3.2 Parameter norm and effective learning rate decay"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_26_2.jpg", "caption": "Figure 11: Simple MLP model with dead unit recovery after sudden changes in the classification task. We", "description": "This figure shows the results of an experiment on a simple MLP model designed to memorize randomly assigned labels to MNIST digits.  The labels are re-randomized every 1000 steps.  Different optimizers (SGD, Adam, SGD+momentum) are used with and without layer normalization (layernorm, rmsnorm). The plot shows the fraction of 'dead' ReLU units (units that are always 0) over the course of training. It demonstrates that layer normalization helps the network recover more quickly from spikes in the number of dead units, showcasing its ability to revive dormant neurons.", "section": "3.1 Normalization and ReLU revival"}, {"figure_path": "ZbjJE6Nq5k/figures/figures_27_1.jpg", "caption": "Figure 1: Accumulation of dead units in an iterated random label memorization task. The network is trained to memorize random labels of the MNIST dataset which are re-randomized every 1000 optimizer steps. Networks with normalization layers are able to recover from spikes in the number of dead units.", "description": "This figure shows the fraction of dormant ReLU units over training steps for a network trained on an iterated random label memorization task. In this task, the network is trained to memorize random labels of MNIST, which are then re-randomized every 1000 steps. The results are shown for networks with different types of normalization layers (no normalization, RMSNorm, LayerNorm) and optimizers (SGD, Adam, SGD+momentum). The figure demonstrates that networks with normalization layers are better able to recover from spikes in the number of dead units that can occur during training on non-stationary tasks.", "section": "3.1 Normalization and ReLU revival"}]