[{"type": "text", "text": "A Probability Contrastive Learning Framework for 3D Molecular Representation Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiayu Qin University at Buffalo jiayuqin@buffalo.edu ", "page_idx": 0}, {"type": "text", "text": "Jian Chen University at Buffalo jchen378@buffalo.edu ", "page_idx": 0}, {"type": "text", "text": "Rohan Sharma University at Buffalo rohanjag@buffalo.edu ", "page_idx": 0}, {"type": "text", "text": "Jingchen Sun University at Buffalo jsun39@buffalo.edu ", "page_idx": 0}, {"type": "text", "text": "Changyou Chen University at Buffalo changyou $@$ buffalo.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Contrastive Learning (CL) plays a crucial role in molecular representation learning, enabling unsupervised learning from large scale unlabeled molecule datasets. It has inspired various applications in molecular property prediction and drug design. However, existing molecular representation learning methods often introduce potential false positive and false negative pairs through conventional graph augmentations like node masking and subgraph removal. The issue can lead to suboptimal performance when applying standard contrastive learning techniques to molecular datasets. To address the issue of false positive and negative pairs in molecular representation learning, we propose a novel probability-based contrastive learning (CL) framework. Unlike conventional methods, our approach introduces a learnable weight distribution via Bayesian modeling to automatically identify and mitigate false positive and negative pairs. This method is particularly effective because it dynamically adjusts to the data, improving the accuracy of the learned representations. Our model is learned by a stochastic expectation-maximization process, which optimizes the model by iteratively refining the probability estimates of sample weights and updating the model parameters. Experimental results indicate that our method outperforms existing approaches in 13 out of 15 molecular property prediction benchmarks in MoleculeNet dataset and 8 out of 12 benchmarks in the QM9 benchmark, achieving new state-of-the-art results on average. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate the problem of learning representations from molecules, a field known as molecular representation learning (MRL). MRL has gained significant attention due to its critical role in enabling learning from limited supervised data for applications such as molecular property prediction[29, 33, 5] and drug design [14, 20, 22]. Molecular representation learning involves creating models that can derive meaningful and generalizable representations of molecules, which can then be used to enhance various downstream applications. Among the most common methods in MRL is contrastive learning (CL), which leverages large-scale unlabeled molecular datasets to learn robust representations. CL works by contrasting different augmentations of the same molecule to ensure that the model learns to recognize the essential features of the molecule, thereby improving performance on tasks such as molecular property prediction and drug design. ", "page_idx": 0}, {"type": "text", "text": "With the success of contrastive learning methods in computer vision and multi-modality pretraining [7, 27], various contrastive learning approaches have been proposed for molecular representation learning. MolCLR[33] introduces a contrastive learning framework specifically for molecular representation learning. It employs atom masking and edge removal as data augmentations, which enhances the performance of Graph Neural Network (GNN) models on a variety of downstream molecular property prediction benchmarks. In contrast, GraphMVP[20] incorporates both 2D topology and 3D geometry during pre-training, though its downstream tasks primarily utilize 2D topology. These methods highlight different strategies for applying contrastive learning to molecular data, focusing on unique aspects of molecular structures to improve learning efficacy. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Although existing works have demonstrated the success of contrastive learning in molecular property predictions, they still face a significant drawback: the reliability of \"positive\" and \"negative\" labels in augmented molecule pairs. For example, MolCLR[33] uses augmentations like atom masking and edge removal, which can lead to false negative pairs when molecules with similar structures and chemical properties are labeled as negatives. Similarly, GraphMVP [20], which incorporates both 2D topology and 3D geometry, can also mislabel structurally similar augmented molecules as negatives due to its augmentation processes. These augmentations often remove parts of the molecular graph, such as nodes, edges, and subgraphs, resulting in potentially incorrect pairings. This issue is exacerbated by the large volume and extensive augmentations applied to molecular datasets, naturally leading to numerous falsely aligned pairs. ", "page_idx": 1}, {"type": "text", "text": "The fundamental problem lies in the random nature of these augmentations. Existing molecular contrastive learning methods assign hard positive and negatives to molecule pairs and do not account for the probabilistic relationships between molecules. Figure 3 provides an example of false positives and negatives resulting from graph augmentations in MolCL[33] ,where two distinct graph augmentations are applied to enhance two different molecules. The augmented molecule pair originating from the same molecule is categorized as positive, while other molecule pairs within the same batch are considered negative. However, as illustrated in the figure, the correct contrastive learning setup should consider molecules with structural similarities as positive pairs, even when they originates from different molecules. In contrast, the same molecule subjected to different augmentation methods may also be considered negative due to structural dissimilarities. Existing methods like MolCLR [33] fail to maintain this distinction, where augmented pairs from the same molecule are always treated as positive, while pairs from different molecules within the same batch are always treated as negative, regardless of their structural similarity. This mislabeling results in false positives and negatives, undermining the effectiveness of the contrastive learning process. ", "page_idx": 1}, {"type": "image", "img_path": "HYiR6tGQPv/tmp/98ed794c99d0c510cc415e9df87e32544cbd48de9bc55b6e69babae14d76b9fc.jpg", "img_caption": ["Figure 1: Existing problem in molecular contrastive learning. Adopt node removal and edge removal for molecular contrastive learning can lead to false positive and false negative problems. Blue lines indicate positive pairs and yellowing lines indicate negative pairs. The numbers on each line indicate the chemical similarity between the augmented pair of molecules. In this case, positive pairs indeed have lower similarity than negative pairs. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To overcome the aforementioned issue, we introduce a generalization of existing contrastive learning frameworks for molecular representation learning with probabilistic modeling. Our approach introduces data-pair weights as additional random variables, and dynamically infers optimal weights to account for false positive and false negative pairs, which can effectively address the mislabeling problem in previous methods. By incorporating a probability framework, we can effectively manage the uncertainty in data pair assignments. Specifically, we introduce a novel Bayesian inference methods with Bayesian data augmentation to automatically infer these weights through posterior sampling. This allows us to optimize the model parameters efficiently using stochastic expectation maximization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "It is worth mentioning that while MolCLR[33] authors introduced i-MolCLR[32] to address similar issues by penalizing faulty negatives with a fingerprint-based similarity metric and a motif-level data augmentation called fragment contrast, our method offers distinct advantages. Unlike i-MolCLR which relies on direct fingerprint similarity, our approach introduces a novel probabilistic contrastive learning framework. This framework dynamically infers weight distributions and optimizes through stochastic expectation maximization, eliminating the need for explicit Tanimoto similarity calculations. Our method addresses the issue of false negative pairs more fundamentally and efficiently, providing a more robust solution for molecular contrastive learning. ", "page_idx": 2}, {"type": "text", "text": "In addition, our method is flexible and can be applied to different molecular representation learning framework. In this paper, we first integrate our method into MolCLR [33] series model and benchmark the performance on 2D non-charality MoleculeNet[35] dataset. We then integrated our method into Uni-Mol[42] and evaluate its performance on MoleculeNet[35]. We also trained and evaluated our model on the QM9 [28] dataset, following Equiformer [17]. With molecular property prediction tasks, we aim to test our model\u2019s ability in extracting useful features from molecular. Extensive experiments show that our method outperforms all other molecular representation learning baselines, including contrastive and non-contrastive methods. ", "page_idx": 2}, {"type": "text", "text": "The contributions of this paper can be summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 To tackle the challenges posed by false positive and negative pairs, we introduce a probability method for molecular contrastive learning. By introducing different weights as random variables to various false positive and negative pairs, we effectively mitigate the impact of these erroneous pairs on the learning process.   \n\u2022 To optimize our probabilistic contrastive learning framework, we propose a novel and effective optimization algorithm based on Bayesian data augmentation and stochastic expectation maximization, to simultaneously perform posterior inference and model optimization.   \n\u2022 Through extensive and large-scale experiments, we demonstrate enhanced performance across multiple public benchmarks for molecular representation learning, validating the effectiveness of our proposed method. ", "page_idx": 2}, {"type": "text", "text": "2 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Learning Representations from Molecular Graphs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin by elucidating the foundational setup and notation in molecular contrastive learning. Molecules can be represented as 2D or 3D graphs depending on datasets. 2D molecule graphs have atoms as nodes and bond as edges. 3D molecule graphs additionally adds spacial positions of the atoms. For simplicity, we adopt static atom positions in this paper. ", "page_idx": 2}, {"type": "text", "text": "In molecular representation learning, as illustrated in Figure 2, we start by randomly sampling a batch of $N$ molecules. Each molecule, represented as $\\mathbf{x}_{i}$ , undergoes stochastic augmentation strategies to generate two augmented versions, denoted as $(\\mathbf{x}_{i},\\mathbf{x}^{\\prime}{}_{i})$ . These augmentations involve methods such as atom masking, edge perturbation, and subgraph removal, transforming the original molecular structure while preserving its core characteristics. Among the resulting $2N$ augmented molecules, each pair $(\\mathbf{x}_{i},\\mathbf{x}_{\\phantom{\\prime}i}^{\\phantom{\\prime}})$ is treated as a positive pair, while the remaining $2(N-1)$ augmented molecules within the same batch are considered negative samples. This setup allows us to utilize contrastive learning effectively by distinguishing between similar and dissimilar molecular structures. A neural network encoder $f(\\mathbf{x};\\pmb{\\theta})$ , parameterized by $\\theta$ , is employed to extract representation vectors $z$ from the augmented molecular samples. In this paper, we utilize three different types of encoders in various experiments, as depicted in Figure $^{2\\textrm{B}}$ , C, and D. These encoders include Graph Neural Networks (GNNs) and Transformers, each providing unique advantages for capturing the intricate features of molecular structures. ", "page_idx": 2}, {"type": "text", "text": "Let $s_{i^{+}}\\triangleq\\operatorname{sim}(\\mathbf{z}_{i},\\mathbf{z}_{\\textit{i}}^{\\prime})$ represent the similarity score between the positive pair $(\\mathbf{x}_{i},\\mathbf{x}_{\\phantom{\\prime}i}^{\\phantom{\\prime}})$ after the encoder, and $s_{i k^{-}}\\triangleq\\operatorname*{sim}\\left(\\mathbf{z}_{i},\\mathbf{z}_{k}\\right)$ signifies the similarity score between the negative pair $(\\mathbf{x}_{i},\\mathbf{x}_{k})$ , and $\\mathrm{sim}(\\cdot,\\cdot)$ represents any positive-valued similarity metric. In this paper, we adopt the commonly used exponential cosine similarity, defined as $\\sin(\\mathbf{z}_{1},\\mathbf{z}_{2})\\triangleq e^{\\mathbf{z}_{1}^{T}\\mathbf{z}_{2}/\\|\\mathbf{z}_{1}\\|\\|\\mathbf{z}_{2}\\|\\tau}$ , where $\\tau$ denotes a temperature parameter. ", "page_idx": 2}, {"type": "image", "img_path": "HYiR6tGQPv/tmp/4b4f852d091e2da7130a61ca0fba84c1ce483e4753670088147a1d3790199579.jpg", "img_caption": ["Figure 2: (A) Molecular contrastive learning Molecules are represented as 2D or 3D molecule graphs. Two stochastic augmentation strategies are applied to each graph, resulting in two augmentations. A feature extractor is used to extract features and contrastive loss is used to maximize the similarity of positive pairs and minimize the similarity of negative pairs B,C,D: Different architectures used as feature extractors in different experiments. (B) Uni-Mol [42] architecture used in MoleculeNet [35] Dataset experiment. (C) GCN [21] architecture from MolCLR [33] used in Non-Chirality MoleculeNet [35] experiment. (D) Equiformer [17] architecture used in QM9 [28] dataset experiment. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.2 Probability Weighted Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We describe the proposed probability framework for molecular contrastive learning. In standard contrastive learning, one tries to encode data samples to a latent space such that positive pairs stay close to each other while negative pairs are pushed away. The contrastive loss function is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{N}\\sum_{k=1}^{N}[\\ell(2k-1,2k)+\\ell(2k,2k-1)],\\mathrm{~with~}\\ell(i,j)=-\\log\\frac{s_{i^{+}}}{s_{i^{+}}+\\sum_{k=1}^{2N}\\mathbb{I}_{[k\\neq i,j]}s_{i,k^{-}}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As mentioned, one issue of directly applying the contrastive learning into molecular representation learning is the potential false positive and negative molecular pairs, as discussed in the introduction. This could confuse the learning, ending up with sub-optimal representations. Is there a way to automatically identify and differentiate these pair data? In the following, we propose a Bayesian approach to address this issue that allows the algorithm for automatic inference of the degree of positiveness and negativeness of data pairs, involving enhancing the standard contrastive loss by incorporating learnable stochastic weights for all data pairs. To be more specific, we introduce local learnable weights, denoted as $w_{i}^{+}$ for each positive pair and $w_{i k}^{-}$ for each negative pair. We then define a weighted contrastive loss based on these introduced weights. This modification aims to mitigate the issues by automatically assigning relatively lower weights (or no weights) to false positive and false negative pairs; ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{w}=\\frac{1}{N}\\sum_{k=1}^{N}[\\bar{\\ell}(2k-1,2k)+\\bar{\\ell}(2k,2k-1)],\\;\\;\\bar{\\ell}(i,j)=-\\log\\frac{w_{i}^{+}s_{i}+}{w_{i}^{+}s_{i}+\\sum_{k=1}^{2N}\\mathbb{I}_{[k\\neq i,j]}w_{i k}^{-}s_{i k-1}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "One problem with this formulation, however, is that it is not realistic to compute and store all the weights in the learning process. This precaution arises from the quadratic growth in the number of weights to be calculated as the training data size increases. Furthermore, the random nature of our augmentation method further adds complexity to the pre-calculation and storage of these weights. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "A straightforward baseline for calculating these weights can be envisioned as follows: we can consider these weights in a binary fashion, with all weights initialized to one. In the learning process, if for some positive pairs the similarity score falls below a specified threshold, we set the corresponding weights to zero, marking these positive pairs as false positives. Conversely, if for some negative pairs the similarity score exceeds a threshold, we set the associated weights to zero, indicating false negatives. A challenge associated with this baseline method, however, lies in the establishment of a rigid similarity threshold to create a binary division of weights between zero and one. This approach proves less suitable for our molecular contrastive task as these heuristically chosen thresholds might not be optimal. ", "page_idx": 4}, {"type": "text", "text": "To address this challenge, we propose a principled Bayesian approach that allows adaptively inferring the optimal weights by Bayesian inference. Specifically, we treat the weights to be random variables and assign appropriate priors to them. We consider two types of priors: a Bernoulli prior to model weights as binary random variables and a Gamma prior to represent them as positive values. For simplicity, we model positive weights using the Gamma distribution and negative weights using either the Gamma distribution or the Bernoulli distribution, as expressed by the following formulas: ", "page_idx": 4}, {"type": "text", "text": "Option 1 - Gamma priors for continuous weighting: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i}^{+}\\sim\\mathrm{Gamma}(a_{+},b_{+}),w_{i k}^{-}\\sim\\mathrm{Gamma}(a_{-},b_{-}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Option 2 - Bernoulli priors for selective weighting: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i}^{+}\\sim\\mathrm{Gamma}(a_{+},b_{+}),\\quad w_{i k}^{-}\\sim\\mathrm{Bernoulli}(a_{-}^{-}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "here, $a_{+},\\;b_{+},$ $a_{-}$ and $b_{-}$ are shape and rate parameters for Gamma distribution and $a_{-}^{-}$ is the probability parameter for Bernoulli distribution. ", "page_idx": 4}, {"type": "text", "text": "With our reformulation, we can define a joint distribution over the global model parameter and local random weight variables $w_{i}^{+}$ and $w_{i k}^{-}$ , as: ", "page_idx": 4}, {"type": "equation", "text": "$$\np\\left(\\left\\{w_{i}^{+}\\right\\},\\left\\{w_{i k}^{-}\\right\\},\\theta;\\mathcal{D}\\right)\\propto\\prod_{\\mathbf{x}_{i}\\in\\mathcal{D}}\\frac{w_{i}^{+}s_{i+}}{w_{i}^{+}s_{i j^{+}}+\\sum_{k=1}^{K}w_{i k}^{-}s_{i k^{-}}}p(\\{w_{i}^{+}\\})p(\\{w_{i k}^{-}\\})p(\\theta).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "One problem with the above formulation, however, is that posterior inference of the weights is challenging, due to the lack of convenience posterior distributions. ", "page_idx": 4}, {"type": "text", "text": "Fortunately, inspired by [2], we can introduce an augmented random variable $u_{i}$ that is associated to data point $\\mathbf{x}_{i}$ . Consequently, we can define an augmented joint posterior distribution of the random variables $\\theta,\\mathbf{u},\\mathbf{w}$ , denoted as $p\\left(\\left\\{w_{i}^{+}\\right\\},\\left\\{w_{i k}^{-}\\right\\},\\breve{\\pmb{\\theta}}\\mid\\mathcal{D}\\right)^{1}$ 1, to be ", "page_idx": 4}, {"type": "equation", "text": "$$\np(\\pmb{\\theta},\\mathbf{u},\\mathbf{w}\\mid\\mathcal{D})\\propto\\prod_{i:\\mathbf{x}_{i}\\in\\mathcal{D}}w_{i}^{+}s_{i}+e^{-\\mathbf{u}_{i}w_{i}^{+}s_{i}+}\\prod_{k}e^{-u_{i}w_{i k}^{-}s_{i k}-}p\\left(\\left\\{w_{i}^{+}\\right\\}\\right)p\\left(\\left\\{w_{i k}^{-}\\right\\}\\right)p(\\pmb{\\theta}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{u}\\triangleq\\left\\{u_{1},u_{2},\\cdot\\cdot\\cdot,u_{|\\mathcal{D}|}\\right\\}$ and $\\mathbf{w}\\triangleq\\{w_{i}^{+}\\}\\cup\\{w_{i k}^{-}\\}$ . It is worth noting that this joint distribution is equivalent to the original distribution (3), because (3) is recovered if one marginalize out the auxiliary random variables $\\mathbf{u}$ in (4). In other words, optimization thought (4) is equivalent to optimization over (3). Consequently, we can perform learning and inference based on the augmented posterior of $p(\\pmb{\\theta},\\mathbf{u},\\mathbf{w}\\mid\\mathcal{D})$ , which preserves a much convenient form for posterior inference. In the following, we propose an efficient algorithm based on stochastic expectation maximization (stochastic EM) to alternatively infer the local random variables w and optimize the global model parameter $\\pmb{\\theta}$ . ", "page_idx": 4}, {"type": "text", "text": "2.3 Efficient Inference and Learning with Stocastic Expectation Maximization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose a stochastic EM algorithm for efficient inference and learning of our model. Stochastic EM [24] is a stochastic variant of the EM algorithm, which is an iterative method for finding the maximum likelihood of model parameters in statistical models when data is only partially, or when model depends on unobserved latent variables [41]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In our setting, the objective of stocastic EM is to maximize the posterior in equation 4. The basic idea is to alternatively 1) optimizing model parameter $\\pmb{\\theta}$ with fixed $(\\mathbf{u},\\mathbf{w})$ and 2) sampling $(\\mathbf{u},\\mathbf{w})$ with fixed $\\pmb{\\theta}$ . To this end, we follow standard procedures in stochastic EM to divide the learning into three steps: Simulation, Stochastic Expectation, and Maximization. Specifically, simulation corresponds to sampling local random variables $\\mathbf{u}$ and w for a batch of data; stochastic expectation then uses the sampled auxiliary random variables to update the model parameter $\\pmb{\\theta}$ by maximizing a stochastic objective $Q(\\pmb\\theta)$ , defined as: $Q_{t+1}(\\pmb\\theta)=\\bar{Q_{t}}(\\pmb\\theta)+\\lambda_{t}\\left(\\log p\\bar{(}\\pmb\\theta,\\mathbf u,\\mathbf w\\mid\\mathcal D\\right)-Q_{t}(\\pmb\\theta)\\right)$ at iteration $t+1$ , where $\\left\\{\\lambda_{t}\\right\\}$ is a sequence of decreasing weights. And maximization corresponds to maximizing the stochastic objective constructed in the previous step. In the following, we detail the three steps. ", "page_idx": 5}, {"type": "text", "text": "Simulation Given the joint posterior distribution in equation 3 and the current batch of data, the posterior distributions of the local random variables $\\mathbf{u}$ and w can be directly read out, which simply follow Gamma or Bornoulli distributions of the following forms: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{i}\\mid\\left\\{w_{i}^{+},w_{i k}^{-},\\theta\\right\\}\\sim\\mathrm{Gamma}\\left(a_{u},b_{u}+w_{i}^{+}s_{i+}+\\sum w_{i k}^{-}s_{i k^{-}}\\right),\\forall i,\\mathrm{~and}}\\\\ &{w_{i}^{+}\\mid\\left\\{\\mathbf{u},\\theta\\right\\}\\sim\\mathrm{Gamma}\\left(1+a_{+},u_{i}s_{i+}+b_{+}\\right),\\mathrm{and}}\\\\ &{\\mathrm{Option~}1\\colon w_{i k}^{-}\\mid\\{\\mathbf{u},\\theta\\}\\sim\\mathrm{Gamma}\\left(a_{-},u_{i}s_{i k^{-}}+b_{-}\\right),\\forall i,k}\\\\ &{\\mathrm{Option~}2\\colon w_{i k}^{-}\\mid\\{\\mathbf{u},\\theta\\}\\sim\\mathrm{Bernoulli}\\left(\\frac{a_{-}e^{-u_{i}s_{i k-}}}{1-a_{-}+a_{-}e^{-u_{i}s_{i k-}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We use the Gamma prior because it naturally lends itself to conjugacy in the posterior, which significantly eases the posterior sampling procedure. Also, it is known for its flexibility in shape and scale to model positive continuous variables, which is suitable for sample weights in our setting. ", "page_idx": 5}, {"type": "text", "text": "Stochastic Expectation We then proceed to calculate the stochastic expectation based on the simulated local random variables above. For notation simplicity, we define $Q_{0}(\\pmb\\theta)=0$ . Then we can reformulate $Q_{t+1}(\\pmb\\theta)$ by decomposing the recursion, resulting in ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{t+1}(\\pmb\\theta)=\\sum_{\\tau=0}^{t}\\tilde{\\lambda}_{\\tau}\\log p\\left(\\pmb\\theta,\\mathbf u_{\\tau},\\mathbf w_{\\tau}\\mid\\mathcal D_{\\tau}\\right),\\mathrm{~where~}\\tilde{\\lambda}_{\\tau}\\triangleq\\lambda_{\\tau}\\prod_{t^{\\prime}=\\tau+1}^{t}\\left(1-\\lambda_{t^{\\prime}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tau$ indexes the minibatch and the corresponding local random variables at the current time $\\tau$ . ", "page_idx": 5}, {"type": "text", "text": "Maximization The stochastic expectation objective (6) provides a convenient form for stochastic optimization over time, similar to online optimization (Bent & Van Hentenryck, 2005). Specifically, at each time $t$ , we can initialize the parameter $\\theta$ from the last step, and update it by stochastic gradient ascent on the log-likelihood, $\\log p\\left(\\pmb{\\theta},\\mathbf{u}_{\\tau},\\mathbf{w}_{\\tau}\\mid\\mathcal{D}_{\\tau}\\right)$ calculated from the current batch of data. To reduce variance, we propose to optimize a marginal version by integrating out $\\mathbf{u}_{\\tau}$ from $p\\,(\\pmb{\\theta},\\bar{\\mathbf{u}_{\\tau}},\\mathbf{w}_{\\tau}^{-}\\mid\\mathcal{D}_{\\tau}),$ , which essentially reduces to our ", "page_idx": 5}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/c05cc992334bf95e9980b75bc85a03f72ef39174e77b7c9e6445c38c4a1f1dc1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "original weighted contrastive loss in equation (2). With the above steps, it is ready to optimize the model by stochastic EM. The detailed steps are described in the Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "3 Related works ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Contrastive Learning As a popular self-supervised learning paradigm, contrastive learning focuses on learning semantically informative representations for downstream tasks [16, 3, 39, 9]. The most widely used loss function is InfoNCE [25] which pulls in the representations between positive sample pairs while pushing away that between negative sample pairs. ", "page_idx": 6}, {"type": "text", "text": "Molecular Representation Learning Representation learning on large-scale unlabeled molecules attracts much attention recently. SMILES-BERT [31] is pretrained on SMILES strings of molecules using BERT. Subsequent works are mostly pretraining on 2D molecular topological graphs [15, 29]. MolCLR [33] applies data augmentation to molecular graphs at both node and graph levels, using a self-supervised contrastive learning strategy to learn molecular representations. I-MolCLR [32] is a improved version of MolCLR that uses new data augmentation and introduces weighted contrastive learning for mitigating false pair problem. Further, several recent works try to leverage the 3D spatial information of molecules, and focus on contrastive or transfer learning between 2D topology and 3D geometry of molecules. For example, GraphMVP [20] proposes a contrastive learning GNN-based framework between 2D topology and 3D geometry. GEM [5] uses bond angles and bond length as additional edge attributes to enhance 3D information. Uni-Mol[42] is a universal 3D molecular pretraining framework that significantly enlarges the representation ability and application scope in drug design. ", "page_idx": 6}, {"type": "text", "text": "Noisy Pairs in Contrastive Learning Noisy data pair problem have been found and studied in contrastive learning community. NLIP [11] enforces pairs with larger noise to be less similar in embedding space to improve model training. [6]apply noise estimation component to adjust the consistency between different modalities for action recognition task. RINCE [8] uses a ranked ordering of positive samples to improve InfoNCE loss. [3] introduces a new debiased contrastive learning loss function by transforming the distribution of negative samples. Matchdrop [34] designed a new graph augmentation method to alleviate the false positive sampling problem by retaining the most critical parts of the graph and augmenting the unimportant parts. ", "page_idx": 6}, {"type": "text", "text": "Stochastic Expectation Maximization Stochastic EM [24] stands as a pivotal algorithm in machine learning and probabilistic modeling for large-scale Bayesian inference. Building upon the foundations of the classical Expectation-Maximization (EM) algorithm [18], Stochastic EM offers an efficient solution for parameter estimation in situations involving vast datasets or latent variables, e.g., to maximize the log-likelihood of $p(\\mathbf{z},{\\mathcal{D}}\\mid\\theta)$ , where $\\mathcal{D}$ is the dataset, $\\mathbf{z}$ is the local random variable and $\\pmb{\\theta}$ is the global model parameter. By leveraging the power of mini-batch sampling, Stochastic EM strikes a balance between computational scalability and estimation accuracy. It has found widespread utility in various domains, including clustering [1], topic modeling [40], and latent variable modeling [41], making it an indispensable tool to cope with complex probabilistic models and extensive data and a natural fit to our problem. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our method on molecular property prediction tasks. Our approach is designed to be a versatile component that can be seamlessly integrated with various molecular property prediction datasets and models. In this study, we integrate our model into three different existing models: Uni-Mol[42]], I-MolCLR [32], Equiformer [17] and assess its performance on three distinct datasets: MoleculeNet [35], MoleculeNet without chirality, and the QM9 [28] dataset. For all experiments, we provide detailed experiment settings in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.1 The MoleculeNet Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "MoleculeNet [35] is a popular benchmark for molecular property prediction, including datasets focusing on different molecular properties, from quantum mechanics and physical chemistry to biophysics and physiology. For a fair comparison, we integrated our method into Uni-Mol[42] framework. We applied both the Gamma and Bernoulli versions of our method, as shown in Table 1. In our contrastive learning framework, we used the representation of the [CLS] token as the final encoded representation, representing the entire molecule. Additionally, we incorporated the original three-dimensional recovery loss as an extra loss function. The model was trained on the same large-scale dataset, including 19 million molecules and 209 million conformations, as in the original paper. We used the same evaluation metrics: $R O C\\_A U C$ for classification tasks and RMSE and MAE for regression tasks. ", "page_idx": 6}, {"type": "text", "text": "As shown in Table 1 and 2, our method outperforms Uni-Mol[42] and GEM [5], the current stateof-the-art methods, with an average gain of 1.3 percent in classification tasks and 7.6 percent in regression tasks. This substantiates that our approach facilitates more flexible training with a higher tolerance for false positive and false negative data pairs, thereby enhancing the model\u2019s performance in molecular representation learning. ", "page_idx": 7}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/88c8d9bba15e62b6254a3dccb518aa6aae97a70b663824be37cfbf72665dc756.jpg", "table_caption": ["Table 1: ROC AUC on molecular property prediction classification tasks (Higher is better) "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/563ea83ed01f5e4bef22de777b678b72900f9d6f94dd738e64c5d4e1c284d97a.jpg", "table_caption": ["Table 2: Performance on molecular property prediction regression tasks (Lower is better) "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Non-Chirality version MoleculeNet ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In order to make a fair comparison with I-MolCLR [32], we also integrated our method into MolCLR [33] framework. MolCLR and I-MolCLR are 2D based methods, their experiments are conducted on different version of MoleculeNet dataset that does not consider chirality. We adopted the same dataset, augmentation, GNN-based encoder and other settings. As shown in Table 3, our method outperforms I-MolCLR on 7 out of 9 downstream tasks and got an average of 2 points increase on non-chirality MoleculeNet classification datasets. ", "page_idx": 7}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/5236aef8e8f3f3d21da1533c41b860d86bfe5c6a691cfcddd143c0b1dbeb58db.jpg", "table_caption": ["Table 3: Comparison against i-MolCLR on non-chirality MoleculeNet dataset "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 QM9 Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The QM9 dataset [28] is another popular dataset in molecular property prediction, it consists of $134\\mathrm{k}$ small molecules, and the goal is to predict their quantum properties. For this dataset, we choose equiformer [17] as a baseline method. The data partition we use has 110k,10k,and 11k molecules in training, validation and testing sets. We use both our contrastive loss function and original minimize mean absolute error(MAE) as training objectives. ", "page_idx": 7}, {"type": "text", "text": "As shown in 4, we get state of the art result in 8 out of 12 baselines. The increase is relatively subtle compared with other dataset, we argue that this is due to the fact that QM9 is relatively small regarding number of molecules in training set, and also the saturation on performance achieved by different methods. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/81c01a44af17c48a2bad51d8ee26cfc28f8ed7c0b8474021e9a675079ce6da22.jpg", "table_caption": ["Table 4: Experiment results on QM9 dataset "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Distribution of similarity scores Our method is largely motivated by the observation that previous MCL approaches neglect potential semantic dissimilarity between positive samples and that accounting for this phenomenon can improve learned molecule representations. In Figure A(See Appendix A), we plot the distribution of similarity scores for both positive and negative samples. Figure A left reveals that our method yields larger similarity scores with lower variance for positive pairs compared to MolCLR [33] baseline which uses standard contrastive learning method. Figure A right reveals that our method also mitigates the false negative problem in standard CL. It also shows that our method sometimes assigns lower similarity scores to positive pairs. While it may seem counter intuitive to assign lower similarity scores to positive samples, we argue that doing so is the very reason our method captures dissimilarity between positive pairs. By allowing some degree of alignment between the right set of negative examples, our method is able to minimize the inconsistencies between shared context of related positives and negatives. This in turn allows us to learn an overall more coherent representation space, resulting in increased robustness and downstream performance. ", "page_idx": 8}, {"type": "text", "text": "Comparisons with the Standard Contrastive Learning We conducted an ablation study to showcase that our method of probablistic framework of contrastive learning has already achieved strong emperical results and demonstrate the improvement brought by adding the 3D-aware loss functions on MoleculeNet [35] classification dataset. We first examined the effect of adding the probabilistic framework to the standard contrastive loss, and the 3D-aware loss functions as implemented in Uni-Mol[42]. ", "page_idx": 8}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/25886300e1358482917c94e40e6f294839f5934240118b38d19941e1be12fa87.jpg", "table_caption": ["Table 5: Ablation Study on MoleculeNet Classification Datasets "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5 presents the results of our ablation study. Incorporating the probabilistic framework resulted in a great improvement of 3.4-point increase in ROC-AUC, significantly enhances the model\u2019s performance. On the other hand, introducing the additional loss component led to an increase in ROC-AUC by 2.9 points, demonstrating its secondary role in enhancing the model\u2019s performance. When we adopt both of them, we can get the final ROC-AUC of 80.1 average on MoleculeNet classification datasets. ", "page_idx": 8}, {"type": "text", "text": "Hyperparameters We also conducted an ablation study to determine the optimal hyperparameters (e.g., $a_{+},\\,a_{-})$ on MoleculeNet classification datasets. We selected $a_{+}$ , $a_{-}$ , $b_{+}$ , and $b_{-}$ from the range $[1,5,10]$ . Table 6 indicates that our method achieves the best performance with $a_{+}=5$ and $a_{-}=b_{+}=b_{-}=1$ . Tuning different hyperparameters affects performance, with an increase in $a_{+}$ from 1 to 5 leading to a 1.6 percent performance gain. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we investigate an important yet unnoticed limitation of molecular contrastive learning, where augmented graph data come with false positive and false negative data pairs. As a remedy, we propose a principled solution to molecular contrastive learning by reformulating it into a probability framework and introducing random weights for data pairs. With a Bayesian data augmentation technique, the random weights can be efficiently inferred via sampling, and the model parameter can be efficiently optimized via stochastic expectation maximization. ", "page_idx": 8}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/7d99f22978a7e1e65056c3d160d13c1be7a9d4d323b2c85a91053bd68504fb95.jpg", "table_caption": ["Table 6: Abalation studies on hyperparameters for MoleculeNet classification tasks "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The effectiveness of our innovative approach has been proven through rigorous evaluations on multiple molecular property prediction benchmarks. The results also showcase the wide-ranging applicability and improved robustness of our proposed method over existing methods for learning molecular representations. ", "page_idx": 9}, {"type": "text", "text": "We believe our method is a valuable addition to the literature on molecular contrastive representation learning, which can further boost the performance of state-of-the-art molecular representation learning models for drug design. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is partially supported by NSF AI Institute-2229873, NSF RI-2223292, NSF IIS-1747614 an Amazon research award, and an Adobe gift fund. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, the Institute of Education Sciences, or the U.S. Department of Education. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] St\u00e9phanie Allassonni\u00e8re and Julien Chevallier. \u201cA new class of stochastic EM algorithms. Escaping local maxima and handling intractable sampling\u201d. In: Computational statistics and data analysis 159 (2021), p. 107159.   \n[2] Changyou Chen et al. \u201cWhy do We Need Large Batchsizes in Contrastive Learning? A Gradient-Bias Perspective\u201d. In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo et al. Vol. 35. Curran Associates, Inc., 2022, pp. 33860\u201333875.   \n[3] Ching-Yao Chuang, Joshua Robinson, Yen-Cheng Lin, et al. \u201cDebiased contrastive learning\u201d. In: Advances in Neural Information Processing Systems. Vol. 33. 2020, pp. 8765\u20138775. [4] Jacob Eberhardt et al. \u201cAutoDock Vina 1.2.0: New docking methods, expanded force field, and python bindings\u201d. In: Journal of chemical information and modeling 61.8 (2021), pp. 3891\u2013 3898.   \n[5] Xiantao Fang, Ling Liu, Juncheng Lei, et al. \u201cGeometry-enhanced molecular representation learning for property prediction\u201d. In: Nature Machine Intelligence 4.2 (2022), pp. 127\u2013134. [6] H. Han et al. \u201cNoise-tolerant learning for audio-visual action recognition\u201d. In: IEEE Transactions on Multimedia (2024).   \n[7] Kaiming He, Haoqi Fan, Yuxin Wu, et al. \u201cMomentum contrast for unsupervised visual representation learning\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020, pp. 9729\u20139738.   \n[8] D. T. Hoffmann et al. \u201cRanking info noise contrastive estimation: Boosting contrastive learning via ranked positives\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 1. 2022, pp. 897\u2013905.   \n[9] Pengfei Hu, Hao Zhu, Jian Lin, et al. \u201cUnsupervised contrastive cross-modal hashing\u201d. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 45.3 (2022), pp. 3877\u20133889.   \n[10] Weihua Hu, Bowen Liu, Joseph Gomes, et al. \u201cStrategies for pre-training graph neural networks\u201d. In: arXiv preprint arXiv:1905.12265 (2019).   \n[11] R. Huang et al. \u201cNlip: Noise-robust language-image pre-training\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. 1. 2023, pp. 926\u2013934.   \n[12] Runxuan Jiao, Jingrui Han, Wen Huang, et al. \u201cEnergy-motivated equivariant pretraining for 3d molecular graphs\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. 7. 2023, pp. 8096\u20138104.   \n[13] David R Koes, Mark P Baumgartner, and Carlos J Camacho. \u201cLessons learned in empirical scoring with smina from the CSAR 2011 benchmarking exercise\u201d. In: Journal of chemical information and modeling 53.8 (2013), pp. 1893\u20131904.   \n[14] Panagiotis I Koukos, Li C Xue, and Alexandre MJJ Bonvin. \u201cProtein\u2013ligand pose and affinity prediction: Lessons from D3R Grand Challenge 3\u201d. In: Journal of computer-aided molecular design 33 (2019), pp. 83\u201391.   \n[15] Peng Li, Jun Wang, Yu Qiao, et al. \u201cAn effective self-supervised framework for learning expressive molecular global representations to drug discovery\u201d. In: Briefings in Bioinformatics 22.6 (2021), bbab109.   \n[16] Yiqun Li, Mengya Yang, Dong Peng, et al. \u201cTwin contrastive learning for online clustering\u201d. In: International Journal of Computer Vision 130.9 (2022), pp. 2205\u20132221.   \n[17] Yuyang Liao and Tess Smidt. \u201cEquiformer: Equivariant graph attention transformer for 3d atomistic graphs\u201d. In: arXiv preprint arXiv:2206.11990 (2022).   \n[18] D. Lin. An Introduction to Expectation-Maximization. 2011.   \n[19] Shengchao Liu, M Fatih Demirel, and Yatao Liang. \u201cN-gram graph: Simple unsupervised representation for graphs, with applications to molecules\u201d. In: Advances in Neural Information Processing Systems 32 (2019).   \n[20] Shitong Liu, Hui Wang, Wenhao Liu, et al. \u201cPre-training molecular graph representation with 3d geometry\u201d. In: arXiv preprint arXiv:2110.07728 (2021).   \n[21] Shitong Luo, Tianlong Chen, Yuyang Xu, et al. \u201cOne transformer can understand both 2d & 3d molecular data\u201d. In: The Eleventh International Conference on Learning Representations. 2022.   \n[22] Oscar M\u00e9ndez-Lucio, Muhammad Ahmad, Emilio A del Rio-Chanona, et al. \u201cA geometric deep learning approach to predict binding conformations of bioactive molecules\u201d. In: Nature Machine Intelligence 3.12 (2021), pp. 1033\u20131039.   \n[23] Garrett M Morris, Ruth Huey, William Lindstrom, et al. \u201cAutoDock4 and AutoDockTools4: Automated docking with selective receptor flexibility\u201d. In: Journal of computational chemistry 30.16 (2009), pp. 2785\u20132791.   \n[24] S. F. Nielsen. \u201cThe stochastic EM algorithm: estimation and asymptotic results\u201d. In: Bernoulli (2000), pp. 457\u2013489.   \n[25] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. \u201cRepresentation learning with contrastive predictive coding\u201d. In: arXiv preprint arXiv:1807.03748 (2018).   \n[26] Rodrigo Quiroga and Miguel A Villarreal. \u201cVinardo: A scoring function based on autodock vina improves scoring, docking, and virtual screening\u201d. In: PloS one 11.5 (2016), e0155183.   \n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, et al. \u201cLearning transferable visual models from natural language supervision\u201d. In: International conference on machine learning. PMLR, 2021, pp. 8748\u20138763.   \n[28] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, et al. \u201cQuantum chemistry structures and properties of 134 kilo molecules\u201d. In: Scientific data 1.1 (2014), pp. 1\u20137.   \n[29] Yu Rong, Yatao Bian, Tingyang Xu, et al. \u201cSelf-supervised graph transformer on large-scale molecular data\u201d. In: Advances in neural information processing systems 33 (2020), pp. 12559\u2013 12571.   \n[30] Oleg Trott and Arthur J Olson. \u201cAutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading\u201d. In: Journal of computational chemistry 31.2 (2010), pp. 455\u2013461.   \n[31] Sheng Wang, Yiwen Guo, Yanming Wang, et al. \u201cSmiles-bert: large scale unsupervised pretraining for molecular property prediction\u201d. In: Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics. 2019, pp. 429\u2013 436.   \n[32] Yuyang Wang, Rahul Magar, Chen Liang, et al. \u201cImproving molecular contrastive learning via faulty negative mitigation and decomposed fragment contrast\u201d. In: Journal of Chemical Information and Modeling 62.11 (2022), pp. 2713\u20132725.   \n[33] Yuzhou Wang, Jiaqi Wang, Zhi Cao, et al. \u201cMolecular contrastive learning of representations via graph neural networks\u201d. In: Nature Machine Intelligence 4.3 (2022), pp. 279\u2013287.   \n[34] Fan Wu, Shuoguo Li, Xin Jin, et al. \u201cRethinking explaining graph neural networks via nonparametric subgraph matching\u201d. In: International Conference on Machine Learning. Vol. 375. Proceedings of Machine Learning Research. PMLR. 2023, pp. 11\u201323.   \n[35] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, et al. \u201cMoleculeNet: a benchmark for molecular machine learning\u201d. In: Chemical Science 9.2 (2018), pp. 513\u2013530.   \n[36] Zhaocheng Xiong, De Wang, Xia Liu, et al. \u201cPushing the boundaries of molecular representation for drug discovery with the graph attention mechanism\u201d. In: Journal of Medicinal Chemistry 63.16 (2019), pp. 8749\u20138760.   \n[37] Kevin Yang, Kyle Swanson, Wengong Jin, et al. \u201cAnalyzing learned molecular representations for property prediction\u201d. In: Journal of Chemical Information and Modeling 59.8 (2019), pp. 3370\u20133388.   \n[38] Yuning You, Tianlong Chen, Yang Shen, et al. \u201cGraph contrastive learning automated\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 12121\u201312132.   \n[39] Yuning You, Tianlong Chen, Yuxuan Sui, et al. \u201cGraph contrastive learning with augmentations\u201d. In: Advances in Neural Information Processing Systems. Vol. 33. 2020, pp. 5812\u2013 5823.   \n[40] Manzil Zaheer et al. \u201cExponential stochastic cellular automata for massively parallel inference\u201d. In: Artificial Intelligence and Statistics. PMLR. 2016, pp. 966\u2013975.   \n[41] Shujian Zhang and Yuguo Chen. \u201cComputation for latent variable model estimation: A unified stochastic proximal framework\u201d. In: Psychometrika 87.4 (2022), pp. 1473\u20131502.   \n[42] Guangyong Zhou, Zhen Gao, Qi Ding, et al. \u201cUni-mol: A universal 3d molecular representation learning framework\u201d. In: International Conference on Learning Representations (2023). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "HYiR6tGQPv/tmp/d2d1cb4f63eddd3e38b9ea30fae11681d05311ff939afb23446c37f19b814347.jpg", "img_caption": ["A Similarity Score Distribution ", "Figure 3: Similarity Scores \u2013 Similarity scores distribution for negative pairs in joint space after pre-training with original MolCLR loss and our proposed loss is provided. Compared to Using pretrained MolCLR model, our method yields similarity scores with lower mean and lower variance for negative pairs. While MolCLR have two peaks of negatives similarity scores around 1 and 2.7, our method concentrates them at only one peak of 1.Our method yields similarity scores with higher mean and lower variance for positive pairs. Our method concentrates at higher levels as it allows for some degree of semantic dissimilar between positives. The similarity scores are dot similarity, they are not normalized to enhance the difference for visual purposes. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we discuss the limitations of our proposed EM-based algorithm for molecular contrastive learning. ", "page_idx": 12}, {"type": "text", "text": "B.1 Assumptions and Robustness ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our approach relies on several strong assumptions, such as the independence of molecular features and the noisiness nature of the input data. In practice, these assumptions may be violated, potentially affecting the performance and robustness of the model. For instance, correlated features could lead to biased estimates of weights, while unnoisy data might degrade the necessity to apply our method in learning representations. Future work could explore methods to relax these assumptions and enhance the model\u2019s robustness to such violations. ", "page_idx": 12}, {"type": "text", "text": "B.2 Scope of Claims ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The empirical results presented in this paper are based on experiments conducted on a specific set of datasets: MoleculeNet and QM9. While these datasets are commonly used in molecular machine learning research, they may not fully represent all possible application domains. Consequently, the generalizability of our findings to other datasets or real-world scenarios might be limited. Further validation on a broader range of datasets is necessary to confirm the wide applicability of our approach. ", "page_idx": 12}, {"type": "text", "text": "Also, one limitation of our method is that the performance gains brought by the proposed architectural improvements can depend on datasets and tasks. For small datasets like QM9, the performance gain is not significant. ", "page_idx": 12}, {"type": "text", "text": "B.3 Privacy and Fairness ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "While our work does not specifically address issues of privacy and fairness, these are important considerations for any machine learning model, especially those used in sensitive domains such as healthcare. The potential for bias in molecular datasets, as well as privacy concerns related to molecular data, are areas that require further exploration. Ensuring that our model adheres to ethical standards and mitigates bias is an avenue for future work. ", "page_idx": 12}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/0ecb0d5c88e428b804679ceb2be5045e377d45169a6b60b16f761f7c67ae48cc.jpg", "table_caption": ["Table 7: hyperparameter search space for MoleculeNet dataset "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Training details for experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 MoleculeNet dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Following Unimol, we report the detailed hyperparameters setup of during pretraining in 7. Molecular pretraining runs on 4 A6000 GPUs, and the training time is about 48 hours. We split all the datasets with scaffold split, which splits molecules according to their molecular substructure. ", "page_idx": 13}, {"type": "text", "text": "C.2 MoleculeNet Non-Chirality Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We basically follow Mol-CLR on experiment settings. During pre-training, the GNN encoder maps each molecular graph to a 512-dimensional embedding $^h$ . A projection head, modeled as an MLP with a single hidden layer, transforms $^h$ into a 256-dimensional latent vector $_{\\textit{z}}$ . ReLU is utilized as the non-linear activation function. The model undergoes pre-training over 50 epochs with a batch size of 512, optimized via the Adam optimizer with an initial learning rate of $5\\times10^{-4}$ and a weight decay rate of $\\bar{1}\\times10^{-5}$ . A cosine learning rate decay schedule is applied throughout pre-training. For most datasets, we use a scaffold-based data split; however, the QM9 subtask follows a random split in line with the Mol-CLR methodology. ", "page_idx": 13}, {"type": "text", "text": "In the fine-tuning phase, the projection head is replaced with a randomly initialized MLP that maps $^h$ to the target property prediction, while the pre-trained GNN encoder remains fixed. The fine-tuning process spans 100 epochs per benchmark task, with hyperparameters tuned via random search on validation sets, and results reported on test sets. Each benchmark is evaluated over three independent runs, with average performance reported. The model implementation is based on PyTorch Geometric. ", "page_idx": 13}, {"type": "text", "text": "C.3 QM9 Dataset Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Data partitioning for the QM9 tasks follows the scheme utilized in the Equiformer. For tasks predicting $\\mu,\\alpha,\\varepsilon_{\\mathrm{HOMO}},\\varepsilon_{\\mathrm{LUMO}},\\Delta\\varepsilon,$ , and $C_{\\nu}$ , our configuration includes a batch size of 64, 300 training epochs, a learning rate of $5\\times10^{-4}$ , and Gaussian radial basis functions with 128 bases. The architecture comprises six Transformer blocks, a weight decay of $5\\times10^{-3}$ , and a dropout rate of 0.2. Mixed-precision training is employed for these tasks. ", "page_idx": 13}, {"type": "text", "text": "For the $R^{2}$ task, the setup consists of a batch size of 48, 300 epochs, a learning rate of $1.5\\times10^{-4}$ , Gaussian radial basis functions with 128 bases, five Transformer blocks, a weight decay of $5\\times10^{-3}$ , and a dropout rate of 0.1, executed in single precision. ", "page_idx": 13}, {"type": "text", "text": "The ZPVE task employs a batch size of 48, 300 epochs, a learning rate of $1.5\\times10^{-4}$ , Gaussian radial basis functions with 128 bases, five Transformer blocks, a weight decay of $5\\times10^{-3}$ , and a dropout rate of 0.2, with single-precision training. ", "page_idx": 13}, {"type": "text", "text": "For the $G,H,U$ , and $U_{0}$ tasks, the setup includes a batch size of 48, 300 epochs, a learning rate of $1.5\\times10^{-4}$ , Gaussian radial basis functions with 128 bases, five Transformer blocks, with both weight decay and dropout omitted, utilizing single precision. ", "page_idx": 13}, {"type": "text", "text": "All models were trained on a single A6000 GPU, with mixed-precision tasks requiring 81 GPU-hours and single-precision tasks requiring 151 GPU-hours. Model complexity includes 11.20 million parameters for configurations with six blocks and 9.35 million parameters for configurations with five blocks. ", "page_idx": 13}, {"type": "text", "text": "The data partitioning approach adheres to the random splitting strategy outlined in the Equiformer paper. ", "page_idx": 13}, {"type": "text", "text": "D Comparison against other weight calculation methods ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we show the comparison against using a simpler approach based on similarity scores. To thoroughly investigate this, we designed ablation experiments using the chiral version of MoleculeNet classification tasks and compared three different methods: 1. Bayesian Inference (Our Method): In this approach, we calculate the weights using Bayesian inference, as described in our paper. 2. Fingerprint-based Similarity: This method calculates the weights based on the similarity scores derived from molecular fingerprints, similar to the approach used in I-MolCLR [32]. 3. Encoderbased Similarity: Here, we first extract features of data pairs using encoders and then calculate their similarity scores. These scores are then regularized to the [0, 1] range. ", "page_idx": 14}, {"type": "text", "text": "For methods 2 and 3 , we compute the weights using the following formulas: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{w_{i}^{-}=1-\\lambda\\times\\operatorname{Sim}\\left(x_{i},x_{k}\\right)}}\\\\ {{w_{i}^{+}=\\lambda\\times\\operatorname{Sim}\\left(x_{i},x_{k}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/9ae121a028e6ef8c51c696668c3a238089a191e17ea900748e892fa5b2ca7277.jpg", "table_caption": ["In our experiments, we set $\\lambda=1$ ). "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "From these results, we observe the following: ", "page_idx": 14}, {"type": "text", "text": "1. While fingerprint-based similarity showed improvements in 2 out of 9 tasks compared to our original method, but it did not perform as well overall. This indicates that they may not be flexible enough to fully capture the complexities of molecular representations required for robust performance across diverse tasks. ", "page_idx": 14}, {"type": "text", "text": "2. Encoder-based Similarity performed worse than both the Bayesian inference method and the fingerprint-based similarity approach, further suggesting that using a direct similarity-based method does not necessarily yield better results. ", "page_idx": 14}, {"type": "text", "text": "These findings suggest that while simpler methods may work in some cases, they do not outperform our proposed Bayesian inference method which can dynamically adapt and provide better alignment of positive and negative pairs. Thus, our approach is essential for achieving state-of-the-art performance across various molecular property prediction benchmarks. ", "page_idx": 14}, {"type": "text", "text": "E Protein-ligand binding task ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We also conducted the protein-ligand binding pose prediction task. This is one of the most important tasks in structure based drug design. The task is to predict the complex structure of a protein binding site and a molecular ligand. We need to consider how ligand lays in the pocket, that is, the 6 degrees (3 rotations and 3 translations) of freedom of a rigid movement. ", "page_idx": 14}, {"type": "text", "text": "Following Uni-Mol, the molecular representation and pocket representation are firstly obtained from their own pretraining models by their own conformations; then, their representations are concatenated as the input of an additional 4-layer Transformer decoder, which is finetuned to learn the pair distances of all heavy atoms in molecule and pocket. Then, with the predicted pair-distance matrix as a scoring function, we first randomly place the ligand and then optimize the coordinates of its atoms by directly back-propagation the loss between current pair-distance and predicted pair-distance. ", "page_idx": 14}, {"type": "text", "text": "We evaluate our method using the metric binding pose accuracy. Specifically, we keep the pocket conformation fixed, while the ligand conformation is fully flexible. We evaluate the RMSD(root mean squared distance) between the prediction and the ground truth. Following previous works, we use the percentage of results below predefined RMSD thresholds as metrics. ", "page_idx": 14}, {"type": "text", "text": "The binding pose accuracy results are shown in Table. Not surprisingly, our model again outperforms all the baseline methods, achieving state-of-the-art results with our Gamma-prior version model. ", "page_idx": 14}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/e57006b6ba943b6da138e758f740ed5e0bb05d287a1c50d2c76ecb255b02a4c5.jpg", "table_caption": ["Table 8: Performance on binding pose prediction. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "HYiR6tGQPv/tmp/5bbda54ffca97ee652b5f7e5d5f5e3e9771b01c8feef3c28134a8e5752b701e0.jpg", "table_caption": ["Table 9: ablation study on $a_{u}$ and $b_{u}$ "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "F More ablation on $a_{u}$ and $b_{u}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we conducted an ablation study on the choice of $a_{u}$ and $b_{u}$ . Generally speaking, the choice of $a_{u}$ and $b_{u}$ will not influence the experiment results to a large margin, the top performance is at $a_{u}=b_{u}=5$ . ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the key contributions of the paper, including the development and evaluation of the proposed EM-based algorithm for molecular contrastive learning, as well as the comparison with baseline methods. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper includes a dedicated \"Limitations\" section where it discusses the strong assumptions made in the model, such as the independence assumptions and the potential impact of noisy data. It also reflects on the scope of the claims, emphasizing that the results are based on experiments conducted on specific datasets. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper includes a detailed presentation of all theoretical results, with each theorem and lemma clearly numbered and cross-referenced. All assumptions are explicitly stated within the statements of the theorems, and complete proofs are included in the main text. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides comprehensive details on the experimental setup, including dataset descriptions, preprocessing steps, model architectures, hyperparameter settings, and evaluation metrics. Additionally, the paper outlines the exact procedures followed during experimentation and includes detailed pseudocode in the appendix. These disclosures ensure that the main experimental results can be reproduced and verified independently. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides open access to both the data and the code. Detailed instructions are included in the supplemental material, covering the exact commands and environment settings needed to reproduce the results. The data access instructions encompass steps for obtaining the raw data, preprocessing methods, and generating the necessary datasets. Scripts to reproduce all experimental results, including those for the proposed method and baselines, are provided. Any deviations or omitted experiments are clearly stated with justifications. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [TODO] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides comprehensive details on the experimental setup, including specific data splits, hyperparameters, and their selection pro-cess. It also covers the type of optimizer used and all other relevant parameters. These details are presented clearly in the appendix of the paper. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not report error bars, confidence intervals, or statistical significance tests for the experimental results. Instead, all the experiments are carried out for three times and the average is reported. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper provides detailed information on the computer resources required for each experiment. It specifies the type of compute workers (GPU), memory usage, and the time of execution for each experimental run. ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: There is no consider of ethics according to the NeurIPS Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: There is no societal impact discussed in the paper. ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not pose such risks. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The authors have cited the original, paper that produced the code package or dataset, included the license and provided the copyright and terms of services. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification:The authors have cited the original paper that produced the code package or dataset, stated its\u2019 versions, included the license and provided the copyright and terms of services. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve studying participants ", "page_idx": 18}]