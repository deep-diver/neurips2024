[{"type": "text", "text": "L4GM: Large 4D Gaussian Reconstruction Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiawei Ren1,5 Kevin Xie1,2 Ashkan Mirzaei1,2 Hanxue Liang1,3 Xiaohui Zeng1,2 Karsten Kreis1 Ziwei Liu5 Antonio Torralba4 Sanja Fidler1,2 Seung Wook Kim1 Huan Ling1,2 ", "page_idx": 0}, {"type": "text", "text": "1NVIDIA 2University of Toronto 3 University of Cambridge 4MIT $^5\\mathrm{S}$ -Lab, Nanyang Technological University ", "page_idx": 0}, {"type": "text", "text": "Project page: https://research.nvidia.com/labs/toronto-ai/l4gm ", "page_idx": 0}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/83916b2c7ba53cae712ca3b7355cf03af8582f206d37a07b9a101b4bbf4c20f6.jpg", "img_caption": ["Figure 1: L4GM generates 4D objects from in-the-wild input videos. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present L4GM, the first 4D Large Reconstruction Model that produces animated objects from a single-view video input \u2013 in a single feed-forward pass that takes only a second. Key to our success is a novel dataset of multiview videos containing curated, rendered animated objects from Objaverse. This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames. We keep our L4GM simple for scalability and build directly on top of LGM [49], a pretrained 3D Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview image input. L4GM outputs a per-frame 3D Gaussian Splatting representation from video frames sampled at a low fps and then upsamples the representation to a higher fps to achieve temporal smoothness. We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timestep multiview rendering loss to train the model. The representation is upsampled to a higher framerate by training an interpolation model which produces intermediate 3D Gaussian representations. We showcase that L4GM that is only trained on synthetic data generalizes well on in-the-wild videos, producing high quality animated 3D assets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Animated 3D assets are essential in bringing 3D virtual worlds to life. However, these animations are time consuming to create as the procedure involves rigging and skinning of objects, and crafting keyframes of the animation \u2013 all with minimal automation in tooling. The ability to generate animated 3D assets from widely available monocular videos or simply from text would be a desirable capability for this application. This is the goal of our work. Building more advanced 4D content editing tooling, the ultimate goal of this line of research, is out of scope for this work. ", "page_idx": 0}, {"type": "text", "text": "Past work on automatically generating animated 3D objects, which we refer to as 4D modeling in this paper, falls into different categories. The first line of work aims to faithfully reconstruct 4D objects from multiview video data, and oftentimes requires many views across time to achieve high quality [29, 5, 36]. Such data is expensive to collect which limits applicability. Another line of work instead relies on the power of video generative models. Most commonly, the video score distillation technique is used which optimizes a 4D representation, for example a 3D deformation field, by receiving iterative feedback from the video generative model. Score distillation is known to be fragile (sensitive to prompts), and time consuming (hours per prompt) as oftentimes many iterations are needed to achieve high quality results [47, 26, 72, 1]. ", "page_idx": 1}, {"type": "text", "text": "Recently, a promising method emerged for the task of single-image 3D reconstruction. This method leverages large scale synthetic and real datasets to train a large transformer model, dubbed 3D Large Reconstruction Model (LRM) [20, 19], to generate 3D objects represented as neural radiance fields from a single image in a single forward pass \u2013 thus being extremely fast. We build on top of this idea to achieve fast and high quality 4D reconstruction. ", "page_idx": 1}, {"type": "text", "text": "We present L4GM, the first 4D Large Reconstruction Model (Figure 2), which aims to reconstruct a sequence of 3D Gaussians [22] from a monocular video, in a feed-forward fashion. Key to our method, is a new large-scale dataset containing 12 million multiview videos of rendered animated 3D objects from Objaverse 1.0 [11]. Our model builds on top of LGM [49], a pre-trained 3D Large Reconstruction Model that is trained to output 3D Gaussians from multiview images. We extend it to take a sequence of frames as input and produce a 3D Gaussian representation for each frame. We add temporal self-attention layers between the frames in order to learn a temporally consistent 3D representation. We upsample the output to a higher fps by training an interpolation model that takes two consecutive 3D Gaussian representations and outputs a fixed set of in-betweens. L4GM is trained on our multiview video dataset with per-timestep image reconstruction losses by rendering the Gaussians in multiple views. ", "page_idx": 1}, {"type": "text", "text": "We showcase that although only trained on synthetic data, the model generalizes well to in-the-wild videos, e.g., videos generated by Sora [34] and real-world videos in AcitivityNet [12]. On the video-to-4D benchmark, we achieve state-of-the-art quality while being 100 to 1,000 times faster than other approaches. L4GM further enables fast video-to-4D generation in combination with a multiview generative model, e.g., ImageDream [53]. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Large 3D Reconstruction Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reconstructing 3D representations from posed images typically requires a lengthy optimization process. Some works have proposed to greatly speed this up by training neural networks to directly learn the full reconstruction task in a way that generalizes to novel scenes [65, 55, 54, 57]. Recently, LRM [20] was among the first to utilize large-scale multiview datasets including Objaverse [11] to train a transformer-based model for NeRF reconstruction. The resulting model exhibits better generalization and higher quality reconstruction of object-centric 3D shapes from sparse posed images in a single model forward pass. Similar works have investigated changing the representation to Gaussian splatting [49, 68], introducing architectural changes to support higher resolution [61, 44], and extending the approach to 3D scenes [6, 7]. Methods such as LRM can generalize to input images supplied from sampling multiview diffusion models which enables fast 3D generation [24]. ", "page_idx": 1}, {"type": "text", "text": "2.2 Video-to-4D Reconstruction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recent works have made impressive progress in reconstructing dynamic 3D representations from multiview video inputs [28, 29]. However, for single-view video inputs, the problem of dynamic reconstruction becomes ill-posed and requires hand-crafted or data-driven priors. Good results have been achieved when targetting specific domains using shape templates [62]. Template-free methods typically require accurate depth inputs and cannot fill in portions of the object that are occluded or not visible [14, 60]. ", "page_idx": 1}, {"type": "text", "text": "Recently, a few works have attempted to extend feed-forward generalizeable novel view synthesis to the challenging setting of dynamic monocular videos. DYST [43] introduces the synthetic 4D Dyso dataset which they use to train a transformer for generalizeable dynamic novel view synthesis but their model lacks an explicit 3D representation of the scene and camera. PGDVS [70] extends the generalizeable 3D NVS model GNT [54] to dynamic scenes but relies on consistent depth maps obtained through per-scene optimization. Other methods [50, 4] leverage pretraining but still require some amount of test-time finetuning to achieve acceptable novel view synthesis. Several works tackle generalizerable human shape reconstruction but rely on human template meshes [31, 23]. ", "page_idx": 1}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/acfafa89407e4edd8116fac37e2dbc4f4f2d79ab5246273d6607b3dd4ec733f3.jpg", "img_caption": ["Figure 2: L4GM. The overall model architecture of L4GM. Our model takes a single-view video and single-time step multiview images as input, and outputs a set of 4D Gaussians. It adopts a U-Net architecture and uses cross-view self-attention for view consistency and temporal cross-time self-attention for temporal consistency. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.3 Text-To-4D Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Dreamfusion [35] introduced the score distillation framework for text-to-3D shape generation. Such per-object optimization methods have also been extended to the 4D domain [47] where they leverage video diffusion models [46, 3, 30]. Some methods combine the guidance from video diffusion models with multiview [45] and single-image diffusion models [41] to boost 3D consistency and individual frame quality [72, 1, 26], and they utilize different forms of score distillation [25, 66, 56]. ", "page_idx": 2}, {"type": "text", "text": "By utilizing image conditional diffusion priors such as Zero123 [27] and ImageDream [53] as guidance, this approach has also been applied to the image-conditional setting [71] and video-conditional 4D generation setting by Consistent4D [21] and DreamGaussian4D [39]. GaussianFlow [15] introduces a Gaussian dynamics based representation which supports them to add optical flow estimation as an additional regularization to SDS. STAG4D [67] and 4DGen [64] use diffusion priors to sample additional pseudo-labels from \u201canchor\" views to expand the set of reference images for imageconditional SDS and as direct photometric reconstruction loss. ", "page_idx": 2}, {"type": "text", "text": "To accelerate the generation process, some works eschew the use of score distillation guidance altogether. For reconstruction from monocular video input, Efficient4D [33] and Diffusion2 [63] utilize a two stage approach. In the first stage they craft schemes for sampling multiview videos conditioned on the input video. Standard optimization-based reconstruction is then used for stage 2. However, this optimization process can still take on the order of tens of minutes. In this work we directly train a feed forward 4D reconstruction model instead. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our L4GM builds on the success of single-image 3D reconstruction models [20, 19, 49], specifically the Large Multi-View Gaussian Model (LGM) [49]. LGM accepts a set of multiview images of an object and directly outputs a 3D reconstruction of the object, represented by a set of Gaussian ellipsoids $P$ . Each Gaussian ellipsoid is represented by 14 parameters, including a center $\\mathbf{z}\\in\\mathbb{R}^{3}$ , a scaling factor $\\textbf{s}\\in\\mathbb{R}^{3}$ , a quaternion rotation $\\mathbf{q}\\,\\in\\,\\dot{\\mathbb{R}}^{4}$ , an opacity $\\alpha\\,\\in\\,\\mathbb{R}$ , and a color feature $\\mathbf{c}\\in\\mathbb{R}^{3}$ . The multiview images $\\mathcal{J}=\\{J_{v}\\}_{v=1}^{V}$ are taken from $V$ camera poses $\\mathcal{O}=\\{O_{v}\\}_{v=1}^{V}$ . These camera poses are encoded as image embeddings via Pl\u00fccker ray embeddings and concatenated to the RGB channels of the multiview images as input to the model. They are fed through an asymmetric U-Net [42] yielding $V$ 14-channel image feature maps, where each of the pixels will be interpreted as the parameters of a 3D Gaussian. ", "page_idx": 2}, {"type": "text", "text": "When only a single input image is given, an image-conditional multiview diffusion model such as ImageDream [53] is first used to generate plausible completions for the missing multiview images. These generated views are then fed to LGM for reconstruction. ", "page_idx": 3}, {"type": "text", "text": "4 Our Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a monocular video of a dynamic object, denoted as $\\mathcal{T}=\\{I_{t}\\}_{t=1}^{T}$ where $T$ is video length, our objective is to rapidly reconstruct an accurate 4D representation of the object. Our approach is grounded in two conceptually simple yet impactful insights. ", "page_idx": 3}, {"type": "text", "text": "Our inspiration stems from video diffusion models. Recent advances in video generation have highlighted the beneftis of first pretraining on image data and then extending and finetuning the model on video datasets to effectively model temporal consistency [3, 16, 59, 2]. Similarly, recognizing the scarcity of 4D data, we want to leverage a pre-trained Large Multi-View Gaussian Model (LGM) [49] that operates on images and has been extensively trained on a large-scale 3D dataset of static objects. This strategy leverages the robustness of pre-trained models to effectively train a 4D reconstruction model with limited data. ", "page_idx": 3}, {"type": "text", "text": "Secondly, in constrast to most existing methods [28, 29] that are required to use multiview videos for 4D reconstruction, we found that utilizing a single set of multiview images at the initial timestep is sufficient. We can obtain these multiview images easily by leveraging multiview image diffusion models to expand the first frame of the view. By adding temporal self-attention layers, our model capitalizes on the initial multiview input by propagating and adapting this information across subsequent timesteps (subsection 4.2). This approach significantly reduces the computational complexity and challenges typically associated with generating consistent multiview videos, while still enhancing the quality of the reconstruction, as our results demonstrate. ", "page_idx": 3}, {"type": "text", "text": "Thus, in the following, we introduce L4GM, a model that processes a monocular video to output a set of 3D Gaussians for each timestep, denoted by $\\mathcal{P}=\\{P_{t}\\}_{t=1}^{\\bar{T}}$ , where each $P_{t}$ is a set of 3D Gaussians at time $t$ . L4GM is an extension of a pretrained 3D LGM, enhanced with temporal self-attention layers for dynamic modeling. We generate $V$ multiview images based on the first frame of the input monocular video. These generated views, along with the input video, are fed into L4GM to reconstruct the entire 4D sequence. We also explore further finetuning L4GM into a 4D interpolation model, allowing us to generate 4D scenes at a higher FPS than the monocular input video, offering smoother and more detailed motion dynamics within the 4D reconstructions. ", "page_idx": 3}, {"type": "text", "text": "4.1 Generate Multiview Images with ImageDream ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Similar to the single-image scenario of LGM, we use ImageDream [53] to generate four orthogonal views conditioned on the initial frame $I_{1}$ . We denote $\\mathcal{I}_{1}$ as the set of generated multiview images taken from camera poses $\\scriptscriptstyle\\mathcal{O}$ at the initial time step $t=1$ . We would like our generated multiview images to contain three extra views that are orthogonal to the input first frame of the original video. ", "page_idx": 3}, {"type": "text", "text": "However, often none of the viewing angles of the generated multiview images match the input frame $I_{1}$ . An example can be found in Appendix Figure 8. To address this, we first use the 3D LGM to reconstruct an initial set of 3D Gaussians, $P_{\\mathrm{init}}$ , from the generated multiview images, and render this reconstruction from views that are orthogonal to $I_{1}$ as desired. We provide exact details in Appendix E. ", "page_idx": 3}, {"type": "text", "text": "4.2 Turning the 3D LGM into a 4D Reconstruction Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Model Architecture. We adopt the asymmetric U-Net [42] structure from the pretrained LGM as backbone. To align with LGM\u2019s input specifications, we replicate the generated multiview images in $\\mathcal{I}_{1}$ at $t=1$ (except $I_{1}$ ) across all other time steps to construct a $T\\times V$ grid (see left side of Figure 2 for an example). For simplicity, we assume the camera in the reference monocular video is static and only the object is moving so we also copy the camera poses $\\scriptscriptstyle\\mathcal{O}$ (except $O_{1}$ ) across time steps. Similar to LGM, these poses are then embedded using Pl\u00fccker ray embeddings [48]. We concatenate this camera embedding with the RGB channels of the input images. The concatenated inputs are reshaped into the format (B T V) H W C and fed into the asymmetric U-Net, where B is batch size. ", "page_idx": 3}, {"type": "text", "text": "As illustrated in the middle section of Fig 2, each U-Net block within L4GM consists of multiple residual blocks [18], followed by cross-view self-attention [52] layers. To maintain temporal consistency across different timestamps, we introduce a new temporal self-attention layer following each cross-view self-attention layer. These temporal self-attention layers treat the view axis $\\mathtt{V}$ as a batch of independent videos (by transferring the view axis into the batch dimension). After processing, the data is reshaped back to its original configuration. In einops [40] notation, this process looks as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}=\\mathbf{rearrange}(\\mathbf{x},\\mathbf{\\beta},\\textsc{(B)}\\textsc{H W C}\\to\\textsc{(B)}\\textsc{(T H W)}\\textsc{C})}\\\\ &{\\mathbf{x}=\\mathbf{x}+\\mathrm{TempSelfAttn}(\\mathbf{x})}\\\\ &{\\mathbf{x}=\\mathbf{rearrange}(\\mathbf{x},\\mathbf{\\beta}\\textsc{(B)}\\textsc{(T H W)}\\textsc{C}\\to\\textsc{(B T V)}\\textsc{H W C})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{x}$ is the feature, B $\\texttt{H W C}$ are batch size, height, width, and the number of channels. ", "page_idx": 4}, {"type": "text", "text": "The output of the U-Net consists of 14-channel feature maps with shape $B\\times T\\times V\\times H_{\\mathrm{out}}\\times W_{\\mathrm{out}}\\times14$ . Each of the $1\\times14$ units is treated as the set of parameters of a per-pixel Gaussian. We concatenate these Gaussians along the view dimension $\\mathtt{V}$ to form a single set of Gaussians for each timestamp, resulting in $T$ sets of 3D Gaussians, $\\{P_{t}\\}_{t=1}^{T}$ . This collection forms our final 4D representation. ", "page_idx": 4}, {"type": "text", "text": "Loss Functions. Besides the input camera poses $\\scriptscriptstyle\\mathcal{O}$ , we select another set of camera poses $\\mathcal{O}_{\\mathrm{sup}}$ for multiview supervision. We train the model with a simple reconstruction objective on the video rendering of the output 4D representations from camera poses $\\mathcal{O}\\cup\\mathcal{O}_{\\mathrm{sup}}$ , as detailed in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "4.3 Autoregressive Reconstruction and 4D Interpolation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In practice, one may want to apply L4GM on long videos and obtain temporally smooth 4D outputs. To this end, L4GM also enables autoregressive reconstruction which processes videos in an autoregressive fashion, one chunk of $T$ frames after another. We additionally train a $4D$ Interpolation Model to upsample the 4D representation to a higher fps. Details can be found in Section A. ", "page_idx": 4}, {"type": "text", "text": "Autoregressive Reconstruction (Figure 3, left). Our base model is designed to accept a monocular video of fixed length $T$ . For long videos that exceed $T$ , we partition the video into chunks of size $T$ which we process sequentially. We first apply L4GM to the initial $T$ frames to generate the first set of $T$ Gaussians. Subsequently, instead of generating multiview images for the first frame of the next chunk using a multiview diffusion model, we render the last set of Gaussians from four orthogonal angles to obtain new multiview images. These newly rendered multiview images, along with the next set of video frames, are then used to generate the next set of Gaussians. The process is repeated until all frames of a long video have been reconstructed. We empirically observe that such an autoregressive reconstruction method can be repeated more than 10 times without a significant drop in quality. ", "page_idx": 4}, {"type": "text", "text": "4D Interpolation Model (Figure 3, right). As our model does not track Gaussians across frames, directly interpolating Gaussian trajectories is not feasible [29, 58]. Hence, we develop an interpolation model that operates in 4D, fine-tuned on top of L4GM. Similar interpolation methods have been used successfully in the video generation literature [3]. As shown in Figure 3, the input to the 4D Interpolation Model consists of two sets of multiview images and the interpolation model is trained to produce additional intermediate sets of Gaussians. It leverages the weight-average of the RGB pixels between the multiview images for the newly created intermediate frames. The 4D Interpolation Model then outputs the corresponding sets of Gaussians. In practice, we insert two additional time frames. ", "page_idx": 4}, {"type": "text", "text": "5 Objaverse-4D dataset ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Dataset Collection To collect a large-scale dataset for the 4D reconstruction task, we render all animated objects in Objaverse 1.0 [11]. ", "page_idx": 4}, {"type": "text", "text": "Out of 800,000 objects, only 44,000 have animations. Since each object can have multiple associated animations, we obtain a total of 110,000 animations. All of the animations are 24 fps. ", "page_idx": 4}, {"type": "text", "text": "The dataset consists mostly of animations on rigged characters or objects, e.g., \u201ca T-Rex walking\u201d or \u201ca windmill spinning\u201d. The motions include diverse scenarios such as dynamic locomotions, fine-grained facial expressions, and smoke effects. Interestingly, there are a considerable amount of deforming motions in the dataset, since many animations feature a fantasy style \u2013 thus, even rigid real-world objects are deformable. See Appendix Figure 7 and supplementary video for examples. ", "page_idx": 4}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/37ee045c8210027561fbd7a05a35c07f3ddf167ed120a5f5faf823ad776b1e92.jpg", "img_caption": ["Figure 3: Left: Autoregressive reconstruction. We use the multiview rendering of the last Gaussian as the input to the next reconstruction. There is a one-frame overlap between two consecutive reconstructions. Right: 4D Interpolation. The interpolation model takes in the interpolated multiview videos rendered from the reconstruction results and outputs interpolated Gaussians. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Dataset Rendering Following [53, 32, 24], we adopt the assumption that the real-world monocular videos mostly have $0^{\\circ}$ elevation camera poses, and thus render input views for our training data accordingly. Specifically, since animations are of varying lengths, we split each animation into 1 second subclips and render each 4D object into 48 views $\\times\\,1$ second long clips. The views are from $^{\\,l}$ ) 16 fixed cameras, where cameras are placed at $0^{\\circ}$ elevation with uniformly distributed azimuths, and 2) 32 random cameras, where cameras are placed at random elevations and azimuths. During training, we sample input camera poses $\\scriptscriptstyle\\mathcal{O}$ from the 16 fixed cameras and sample the supervision cameras $\\mathcal{O}_{\\mathrm{sup}}$ from the 32 random cameras. Furthermore, following [2], we further filter out approximately $50\\dot{\\%}$ of the 26M videos with small motion based on optical flow magnitude, resulting in a total of 12M videos in Objaverse-4D dataset. ", "page_idx": 5}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "6.1 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We rendered the dataset with Blender and the EEVEE engine [9]. We used fixed camera intrinsics and lighting as detailed in the appendix. For L4GM, we downsample the clips to 8 FPS and train the model for 200 epochs. In training, we set $T=8$ and use 4 input cameras and 4 supervision cameras. During inference, we used $T=16$ , which we empirically show to work well for longer videos in section 6.3. Each forward pass through L4GM takes about 0.3 seconds, while generating sparse views requires about 2 seconds. For training the interpolation model, we use the 24 FPS clips without downsampling and fine-tune L4GM for another 100 epochs. The 4D interpolation model takes 0.065 seconds to interpolate between every two frames (see Appendix for details). ", "page_idx": 5}, {"type": "text", "text": "6.2 Comparisons to State-of-the-Art Methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We focus our evaluations on video-to-4D reconstruction. Although L4GM can also be used for text-to-4D or image-to-4D synthesis by taking text-to-video or image-to-video outputs from video generative models as input, existing text-to-4D [26, 1, 46] and image-to-4D [71] approaches typically rely on score distillation and are orders of magnitudes slower than L4GM, preventing meaningful comparisons. Hence, we concentrate on video-to-4D. Results are presented in the following paragraphs. ", "page_idx": 5}, {"type": "text", "text": "Quantitative Evaluation. We evaluate our L4GM on the benchmark provided by Consistent4D [21]. It consists of eight dynamic 3D animations of 4 seconds in length, at 8-FPS. A video from one view is used as input and 4 videos from other viewpoints are used for evaluation. Three metrics are computed: 1) Perceptual similarity (LPIPS) between the generated and the ground truth novel views, 2) the CLIP [38] image similarity between the generated and the ground truth novel views, and 3) the FVD [51] against the ground truth novel views , which measures video quality. We also report the runtime. We reconstruct the video at the original frame rate without using interpolation model. The results are shown in Table 1. L4GM outperforms existing video-to-4D generation approaches on all quality metrics by a significant margin, while being 100 to 1,000 times faster. ", "page_idx": 5}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/88fc4e8930d70244ddc076d310ebf150911583b28a9d0f369486a800e5bc0035.jpg", "img_caption": ["Figure 4: Qualitative results from L4GM, showcasing renderings from 4D reconstructions produced from two in-the-wild videos. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Qualitative Evaluation. Figure 4 illustrates the renderings produced by L4GM on two videos from different angles and timesteps. These examples are taken from the ActivityNet [12] and Consistent4D [21] datasets. As shown in the figure, L4GM produces highquality, sharp renderings while exhibiting strong temporal and multiview consistency. ", "page_idx": 6}, {"type": "text", "text": "We compare our visual results to DG4D [39], STAG4D [67], and OpenLRM [19]. DG4D and STAG4D are optimization-based approaches ", "page_idx": 6}, {"type": "table", "img_path": "PSPtj26Lbp/tmp/6c774c8bd346555e6254c5e6e5d719e49d1512131c5ed3d25b1c58809e82659b.jpg", "table_caption": ["Table 1: Quantitative results for video-to-4D. Best is bolded. $\\dagger$ : results from Gao et al. [15]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "that take 10 minutes and 2 hours on 64-frame videos, respectively. OpenLRM is an opensource work reproducing LRM [20] that reconstructs 3D shapes from single-view images. We run OpenLRM on every video frame to construct a 3D sequence. We collect 24 evaluation videos from Emu [16], Sora [34], Veo [10], and ActivityNet [12], covering both generated videos and real-world videos. For our approach, we use $T=16$ and use the interpolation model. Some qualitative comparisons are presented in Figure 5. Notably, a significant improvement from our approach is a higher 3D resolution. Optimization-based approaches use only thousands of Gaussians to keep the optimization tractable, while our feed-forward approach can easily reconstruct more than 60,000 Gaussians per frame at a dramatically faster speed. ", "page_idx": 6}, {"type": "text", "text": "We further conduct a user study based on qualitative comparisons, and the results are shown in Table 2. Our approach is the most favorable on all evaluation criteria including overall quality, 3D appearance, $3D$ alignment with input video, motion alignment with input video, and motion realism. More details about the evaluation dataset and the user study are in Appendix G. ", "page_idx": 6}, {"type": "text", "text": "6.3 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We carry out a variety of ablation studies. For training models in the ablation study, we only keep animations from high-quality objects in GObjaverse [37], which accounts for ${\\approx}25\\%$ of the data. ", "page_idx": 6}, {"type": "text", "text": "3D Pretraining. Without 3D pre-taining, i.e. without initializing from LGM [49], our model fails to converge (using the same training recipe). The explanation is likely that without large-scale pre-training on static 3D scenes our Objaverse-4D dataset is insufficient for L4GM to not only learn temporal dynamics but also its 3D understanding from scratch. Moreover, starting from a fully random initializion may also contribute to training instabilities. Note that when reducing the model size to the \u201csmall\u201d LGM variation [49], the model starts to converge. However, as shown in Table 6 (a), the model converges to a significantly lower PSNR in the same training epochs. ", "page_idx": 6}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/1f2b7518b9a6991981986601bc2b5201a148d180fe44d88f3cd9f6086c3b3b98.jpg", "img_caption": ["Frozen LGM. In this experiment, we freeze the layers of the original LGM model and only train the temporal attention layers. As shown in Table 6 (b), the model improves faster at the beginning of the training but converges to a lower PSNR. This suggests that end-to-end fine-tuning of L4GM, including the non-temporal layers of the 3D LGM, is preferable for the 4D reconstruction task. ", "Figure 5: Qualitative comparisons of L4GM\u2019s results against the baselines. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Temporal Attention. Without temporal attention, the model falls back into a 3D reconstruction model, while receiving asynchronous multiview inputs. The model does a surprisingly good job using only the 3D information, but it still converges to a lower PSNR, as shown in Table 6 (b). The reconstructed novel view videos contain visible flickering due to the lack of temporal modeling. Please refer to the supplementary video for a comparison. ", "page_idx": 7}, {"type": "text", "text": "Deformation Field. Since different types of deformation fields and HexPlane representations have recently been used in the text-to-4D literature [47, 26, 1, 72], we modify the model to predict a canonical 3D representation and a deformation field based on a HexPlane [5]. Concretely, we average the Gaussians as a canonical 3D representation and introduce a new decoder after the middle block in the U-Net to predict a HexPlane. The representation follows Wu et al. [58]. A detailed illustration can be found in the appendix. Although the model can successfully overfti to a single 4D data sample, it fails to learn a reasonable deformation field during large-scale training. As shown in Table 6 (b), the PSNR only slowly improves and the output is always static. This observation is very different from previous optimization-based 4D generation works [39, 26]. We speculate that SDS-based methods often rely on the smoothness of implicit representations to regularize their generations, whereas our L4GM model may have directly learned to output smooth representations from the 4D training data. ", "page_idx": 7}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/f0fa8389b7beba87f9bc463231d04df0f205acce739b84dd9a4c9a3311776b44.jpg", "img_caption": ["Figure 6: PSNR plot. a) Training with different pretrain and training data. $b$ ) Training with different design choices. c) Per-frame PSNR with different video lengths $T$ , AR denotes autoregressive. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "PSPtj26Lbp/tmp/7717eaf190038e7f071ab2da0514ad0d091f7634c3270b8143ec95d42d6deb4c.jpg", "table_caption": ["Table 2: Comparison to baselines by user study on synthesized 4D scenes with 24 examples. Numbers are percentages. Numbers do not add up to 100; difference is due to users voting \u201cno preference\u201d (details in Appendix). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Autoregressive Reconstruction. Our model allows taking a video with a length different from the training video length. Here, we analyze the effect of the test-time video length $T$ and the number of autoregressive steps on the reconstruction quality. We use a long animation in Objaverse-4D dataset and compute the per-frame reconstruction PSNR. Results are in Table 6 (c). When the groundtruth multiviews are provided, the quality slightly drops when using longer video length. When reconstructing autoregressively, the quality decreases with more autoregressive runs. A shorter video length will start with a higher quality, but the quality drops faster than a longer video length because more self-reconstructions are required. In practice, we select $T\\,=\\,16$ , which offers a balanced performance for different video lengths. ", "page_idx": 8}, {"type": "text", "text": "Time Embedding. Here, we explore adding a time embedding to L4GM so that the model is aware of the ordering of the frames. Timestamps are encoded by a sinusoidal function and added to the camera embedding. However, the PSNR did not improve after adding the time embedding. We speculate that the image frames from the input video are already giving sufficient information about the temporal relation between the timesteps. Therefore, we do not train with temporal embedding to add more flexibility to the model at inference time. ", "page_idx": 8}, {"type": "text", "text": "4D Interpolation. Finally, we show a comparison of using vs. not using the 4D interpolation model on an 8-FPS video from the Consistent4D dataset in the supplementary video. Notably, the 4D interpolation model can successfully improve the framerate beyond the input video framerate. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We presented L4GM, the first large reconstruction model for dynamic objects. It produces dynamic sequences of sets of 3D Gaussians from a single-view video. The model leverages prior 3D knowledge from a pretrained 3D reconstruction model, and learns the temporal dynamics from a synthetic dynamic dense-view 4D dataset, Objaverse-4D dataset, which we collect. L4GM can reconstruct long videos and uses learned interpolation to achieve high framerates. We achieve orders of magnitude faster inference times than existing 4D reconstruction or text-to-4D methods. Moreover, our model generalizes well to in-the-wild real and generated videos. Our work is an early attempt at AI tooling for 4D content creation, with many challenges remaining. For example, for making this technology really useful for professionals we need to develop more advanced human-in-the-loop 4D editing capabilities. ", "page_idx": 8}, {"type": "text", "text": "Broader Impact. L4GM allows fast 4D reconstruction from in-the-wild videos, which is useful for various graphics and animation applications. However, this model should be used with an abundance of caution to prevent malicious impersonations. The model is also trained on non-commercial public datasets and is for research-only purpose. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] S. Bahmani, I. Skorokhodov, V. Rong, G. Wetzstein, L. Guibas, P. Wonka, S. Tulyakov, J. J. Park, A. Tagliasacchi, and D. B. Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. arXiv preprint arXiv:2311.17984, 2023. [2] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[4] M. B\u00fcsching, J. Bengtson, D. Nilsson, and M. Bj\u00f6rkman. Flowibr: Leveraging pre-training for efficient neural image-based rendering of dynamic scenes. arXiv preprint arXiv:2309.05418, 2023.   \n[5] A. Cao and J. Johnson. Hexplane: A fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130\u2013141, 2023.   \n[6] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. arXiv preprint arXiv:2312.12337, 2023.   \n[7] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.-J. Cham, and J. Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024.   \n[8] Y. Cheng, L. Li, Y. Xu, X. Li, Z. Yang, W. Wang, and Y. Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023.   \n[9] B. O. Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org.   \n[10] G. Deepmind. Veo: our most capable generative video model. 2024. URL https://deepmind. google/technologies/veo.   \n[11] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi. Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13142\u201313153, 2023.   \n[12] B. G. Fabian Caba Heilbron, Victor Escorcia and J. C. Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961\u2013970, 2015.   \n[13] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12479\u201312488, 2023.   \n[14] C. Gao, A. Saraf, J. Kopf, and J.-B. Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5712\u20135721, 2021.   \n[15] Q. Gao, Q. Xu, Z. Cao, B. Mildenhall, W. Ma, L. Chen, D. Tang, and U. Neumann. Gaussianflow: Splatting gaussian dynamics for 4d content creation. arXiv preprint arXiv:2403.12365, 2024.   \n[16] R. Girdhar, M. Singh, A. Brown, Q. Duval, S. Azadi, S. S. Rambhatla, A. Shah, X. Yin, D. Parikh, and I. Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023.   \n[17] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.   \n[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016.   \n[19] Z. He and T. Wang. Openlrm: Open-source large reconstruction models. https://github. com/3DTopia/OpenLRM, 2023.   \n[20] Y. Hong, K. Zhang, J. Gu, S. Bi, Y. Zhou, D. Liu, F. Liu, K. Sunkavalli, T. Bui, and H. Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023.   \n[21] Y. Jiang, L. Zhang, J. Gao, W. Hu, and Y. Yao. Consistent4d: Consistent 360 {\\deg} dynamic object generation from monocular video. arXiv preprint arXiv:2311.02848, 2023.   \n[22] B. Kerbl, G. Kopanas, T. Leimk\u00fchler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1\u201314, 2023.   \n[23] C. Li, J. Lin, and G. H. Lee. Ghunerf: Generalizable human nerf from a monocular video. arXiv preprint arXiv:2308.16576, 2023.   \n[24] S. Li, C. Li, W. Zhu, B. Yu, Y. Zhao, C. Wan, H. You, H. Shi, and Y. Lin. Instant-3d: Instant neural radiance field training towards on-device ar/vr 3d reconstruction. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1\u201313, 2023.   \n[25] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin. Magic3D: High-Resolution Text-to-3D Content Creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[26] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. arXiv preprint arXiv:2312.13763, 2023.   \n[27] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9298\u20139309, October 2023.   \n[28] S. Lombardi, T. Simon, J. Saragih, G. Schwartz, A. Lehrmann, and Y. Sheikh. Neural volumes: Learning dynamic renderable volumes from images. arXiv preprint arXiv:1906.07751, 2019.   \n[29] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713, 2023.   \n[30] Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao, J. Zhou, and T. Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[31] M. Masuda, J. Park, S. Iwase, R. Khirodkar, and K. Kitani. Generalizable neural human renderer. arXiv preprint arXiv:2404.14199, 2024.   \n[32] L. Melas-Kyriazi, I. Laina, C. Rupprecht, N. Neverova, A. Vedaldi, O. Gafni, and F. Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation. arXiv preprint arXiv:2402.08682, 2024.   \n[33] Z. Pan, Z. Yang, X. Zhu, and L. Zhang. Fast dynamic 3d object generation from a single-view video, 2024.   \n[34] B. Peebles, T. Brooks, C. Brooks, C. Ng, D. Schnurr, E. Luhman, J. Taylor, L. Jing, N. Summers, R. Wang, and et al. Creating video from text. 2024. URL https://openai.com/sora.   \n[35] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. In The Eleventh International Conference on Learning Representations (ICLR), 2023.   \n[36] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10318\u201310327, 2021.   \n[37] L. Qiu, G. Chen, X. Gu, Q. zuo, M. Xu, Y. Wu, W. Yuan, Z. Dong, L. Bo, and X. Han. Richdreamer: A generalizable normal-depth diffusion model for detail richness in text-to-3d. arXiv preprint arXiv:2311.16918, 2023.   \n[38] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[39] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. Liu. Dreamgaussian4d: Generative 4d gaussian splatting, 2023.   \n[40] A. Rogozhnikov. Einops: Clear and reliable tensor manipulations with einstein-like notation. In International Conference on Learning Representations, 2021.   \n[41] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[42] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[43] M. Seitzer, S. van Steenkiste, T. Kipf, K. Greff, and M. S. Sajjadi. Dyst: Towards dynamic neural scene representations on real-world videos. arXiv preprint arXiv:2310.06020, 2023.   \n[44] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. Wang. Gamba: Marry gaussian splatting with mamba for single view 3d reconstruction. arXiv preprint arXiv:2403.18795, 2024.   \n[45] Y. Shi, P. Wang, J. Ye, L. Mai, K. Li, and X. Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023.   \n[46] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, and Y. Taigman. Make-A-Video: Text-to-Video Generation without TextVideo Data. In The Eleventh International Conference on Learning Representations (ICLR), 2023.   \n[47] U. Singer, S. Sheynin, A. Polyak, O. Ashual, I. Makarov, F. Kokkinos, N. Goyal, A. Vedaldi, D. Parikh, J. Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023.   \n[48] V. Sitzmann, S. Rezchikov, B. Freeman, J. Tenenbaum, and F. Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:19313\u201319325, 2021.   \n[49] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024.   \n[50] F. Tian, S. Du, and Y. Duan. Mononerf: Learning a generalizable dynamic radiance field from monocular videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17903\u201317913, 2023.   \n[51] T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717, 2018.   \n[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[53] P. Wang and Y. Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201, 2023.   \n[54] P. Wang, X. Chen, T. Chen, S. Venugopalan, Z. Wang, et al. Is attention all that nerf needs? arXiv preprint arXiv:2207.13298, 2022.   \n[55] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2021.   \n[56] Z. Wang, C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023.   \n[57] C.-Y. Wu, J. Johnson, J. Malik, C. Feichtenhofer, and G. Gkioxari. Multiview compressive coding for 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9065\u20139075, 2023.   \n[58] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023.   \n[59] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7623\u20137633, 2023.   \n[60] W. Xian, J.-B. Huang, J. Kopf, and C. Kim. Space-time neural irradiance fields for freeviewpoint video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9421\u20139431, 2021.   \n[61] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen, and G. Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. arXiv preprint arXiv:2403.14621, 2024.   \n[62] G. Yang, D. Sun, V. Jampani, D. Vlasic, F. Cole, H. Chang, D. Ramanan, W. T. Freeman, and C. Liu. Lasr: Learning articulated shape reconstruction from a monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15980\u201315989, 2021.   \n[63] Z. Yang, Z. Pan, C. Gu, and L. Zhang. Diffusion2: Dynamic 3d content generation via score composition of orthogonal diffusion models. arXiv preprint arXiv:2404.02148, 2024.   \n[64] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei. 4dgen: Grounded 4d content generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023.   \n[65] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2021.   \n[66] X. Yu, Y.-C. Guo, Y. Li, D. Liang, S.-H. Zhang, and X. Qi. Text-to-3D with Classifier Score Distillation. arXiv preprint arXiv:2310.19415, 2023.   \n[67] Y. Zeng, Y. Jiang, S. Zhu, Y. Lu, Y. Lin, H. Zhu, W. Hu, X. Cao, and Y. Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. arXiv preprint arXiv:2403.14939, 2024.   \n[68] K. Zhang, S. Bi, H. Tan, Y. Xiangli, N. Zhao, K. Sunkavalli, and Z. Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. arXiv preprint arXiv:2404.19702, 2024.   \n[69] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[70] X. Zhao, A. Colburn, F. Ma, M. A. Bautista, J. M. Susskind, and A. G. Schwing. Pseudogeneralized dynamic view synthesis from a video, 2024.   \n[71] Y. Zhao, Z. Yan, E. Xie, L. Hong, Z. Li, and G. H. Lee. Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[72] Y. Zheng, X. Li, K. Nagano, S. Liu, K. Kreis, O. Hilliges, and S. D. Mello. A unified approach for text-and image-guided 4d scene generation. arXiv preprint arXiv:2311.16854, 2023. ", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 14}, {"type": "text", "text": "2 Related Work 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "2.1 Large 3D Reconstruction Models 2   \n2.2 Video-to-4D Reconstruction 2   \n2.3 Text-To-4D Generation 3 ", "page_idx": 14}, {"type": "text", "text": "3 Background 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "4 Our Method 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "4.1 Generate Multiview Images with ImageDream . . 4   \n4.2 Turning the 3D LGM into a 4D Reconstruction Model . 4   \n4.3 Autoregressive Reconstruction and 4D Interpolation . . . 5 ", "page_idx": 14}, {"type": "text", "text": "5 Objaverse-4D dataset 5 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "6 Experiments 6 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "6.1 Implementation Details . 6   \n6.2 Comparisons to State-of-the-Art Methods 6   \n6.3 Ablation Studies . . 7 ", "page_idx": 14}, {"type": "text", "text": "7 Conclusions 9 ", "page_idx": 14}, {"type": "text", "text": "A Autoregressive Reconstruction and 4D Interpolation Model Details 17 ", "page_idx": 14}, {"type": "text", "text": "B More Implementation Details 17 ", "page_idx": 14}, {"type": "text", "text": "C Example Training Data 18 ", "page_idx": 14}, {"type": "text", "text": "D Full Quantitative Results on Consistent4D Benchmark 18 ", "page_idx": 14}, {"type": "text", "text": "E Azimuth Alignment 20 ", "page_idx": 14}, {"type": "text", "text": "F HexPlane Model Details 20 ", "page_idx": 14}, {"type": "text", "text": "G More Qualitative Evaluation Details 21 ", "page_idx": 14}, {"type": "text", "text": "G.1 Evaluation Dataset 21   \nG.2 User Study 21 ", "page_idx": 14}, {"type": "text", "text": "H More Qualitative Results 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "H.1 More comparisons with state-of-the-art approaches 22   \nH.2 Qualitative Results on Consistent4D Data . 22 ", "page_idx": 14}, {"type": "text", "text": "I Limitations 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "J More discussions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Autoregressive Reconstruction and 4D Interpolation Model Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Autoregressive Reconstruction for Longer Videos. Our model accepts a monocular video $\\mathcal{Z}=$ $\\{I_{t}\\}_{t=1}^{T}$ with a fixed length $T$ as input, where $T$ is a hyperparameter set during training (note that during inference $T$ can in principle be longer than during training). For videos longer than $T$ , we can also operate our model in an autoregressive manner. Consider a long video $\\{I_{t}\\}_{t=1}^{L}$ where the frame length $L$ significantly exceeds $T$ . We first apply L4GM to the initial $T$ frames to generate the first set of $T$ Gaussians $\\{P_{t}\\}_{t=1}^{T}$ . Subsequently, we render the last Gaussian $P_{T}$ from four orthogonal angles to obtain new generated multiview images $\\mathcal{I}_{T}=\\{f(P_{T},\\Delta\\theta)\\}_{\\Delta\\theta\\in\\{0^{\\circ},90^{\\circ},180^{\\circ},270^{\\circ}\\}}$ . The oWf rendered multiview images $T-1$ hGata tuhsissi aanust $\\{P_{t}\\}_{t=T+1}^{2T-1}$ $\\mathcal{I}_{T}$ , along with frames .r eTchoinss tprruocctieossn  ism reethpoeadt ecad nu bntei lr ealple $\\mathcal{T}=\\{I_{t}\\}_{t=T}^{2T-1}$ $L$ , are used to construct the next set efdr ammoerse  htahvaen  b1e0e tni rmeecso nwsittrhuocutte da. significant drop in quality (see Figure 3, left). ", "page_idx": 16}, {"type": "text", "text": "4D Interpolation Model. Since our model does not track Gaussians across frames, interpolating Gaussian trajectories is not feasible [29, 58]. Hence, we developed an interpolation model that operates directly within the 4D space, fine-tuning it on top of L4GM. As demonstrated in Figure 3, right side, the input to the 4D Interpolation Model consists of two sets of multiview images, ${\\mathcal{I}}_{i}$ and $\\mathcal{I}_{i+3}$ . During training, we render ${\\mathcal{I}}_{i}$ and $\\mathcal{I}_{i+3}$ at 8 FPS from an asset in our dataset. During inference, ${\\mathcal{I}}_{i}$ and $\\mathcal{I}_{i+3}$ are rendered from reconstructed sets of gaussians $P_{i}$ and $P_{i+3}$ . To create the $T\\times V$ input grid, we insert two time steps between the input views by simply calculating the weighted-average on the RGB pixels between the multiview images ${\\mathcal{I}}_{i}$ and $\\mathcal{I}_{i+3}$ , resulting in a total of $4\\times V$ images. The 4D Interpolation Model then outputs the corresponding four sets of Gaussians, which are supervised using the ground truth data at 24 FPS. During inference, we render the reconstruction results into multiview videos and feed them into the interpolation model to produce a sequence of Gaussians at a three times higher framerate. ", "page_idx": 16}, {"type": "text", "text": "B More Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Dataset. We render the dataset with the EEVEE engine in Blender [9], which requires three days with 200 GPUs. For random cameras, we select a random elevation from $[-5^{\\circ},\\,60^{\\circ}]$ . For the fixed camera setting, we rendered from 16 views equally distributed at $0^{\\circ}$ elevation looking at the origin. We set the camera radius to 1.5 to align with LGM so that we can leverage its 3D pretrain. The camera FOV is set to $49.1^{\\circ}$ . We use the lighting from the Objaverse-XL codebase1 while fixing the strength to the mean value. ", "page_idx": 16}, {"type": "text", "text": "Following Blattmann et al. [2], we also employ motion flitering. Specifically, we downsample the 4D animation clips of Objaverse-4D dataset to 2-FPS and compute the optical flow magnitude averaged on videos rendered from four orthogonal fixed 0-elevation cameras. A histogram of the optical flow magnitude is shown in Table 3. We only keep the animation clips with an average optical flow magnitude larger than 0.15, resulting in 51K 4D animations with 28K different objects, 246K 1-second clips, and 12M videos rendered from 48 camera poses. ", "page_idx": 16}, {"type": "text", "text": "Model. We use the pretrained LGM model [49] released in the official code2, which has 6 downsampling blocks with channels [64, 128, 256, 512, 1024, 1024], 1 middle block with channel [1024], and 5 upsampling blocks with channels [1024, 1024, 512, 256, 128]. The input image size is $256\\mathrm{x}256$ The number of output Gaussians for each frame is $128\\times128\\times4=65{,}536$ . The output Gaussians are rendered into images at 512x512 resolution for supervision. The added temporal attention layers are the same as the cross-view attention layers, and are inserted after the same U-Net blocks. ", "page_idx": 16}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/2c3e46ad8ae3ef4d0686451b54979c9580bd71bca0c9b8997fcd64844f3ff59c.jpg", "img_caption": ["(a) Flow magnitude histogram in log scales. (b) Flow magnitude histogram in [0.1, 10]. Table 3: Optical flow magnitude histogram on Objaverse-4D dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Loss Functions. Following LGM, we use a combination of an LPIPS [69] loss and an MSE loss on RGB images, and an MSE loss on segmentation masks to supervise the reconstruction, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathtt{R G B}}=\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{O\\in\\mathcal{O}\\cup\\mathcal{O}_{\\mathrm{sup}}}\\vert\\vert I_{t}^{O}-f(P_{t},O)\\vert\\vert_{2}^{2}+\\lambda\\mathcal{L}_{\\mathtt{L P I P S}}(I_{t}^{O},f(P_{t},O)),}\\\\ &{\\mathcal{L}_{\\mathtt{M a s k}}=\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{O\\in\\mathcal{O}\\cup\\mathcal{O}_{\\mathrm{sup}}}\\vert\\vert\\alpha_{t}^{O}-g(P_{t},O)\\vert\\vert_{2}^{2},}\\\\ &{\\quad\\quad\\mathcal{L}=\\mathcal{L}_{\\mathtt{R G B}}+\\mathcal{L}_{\\mathtt{M a s k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $f,g$ are Gaussian volume rendering functions for RGB and alpha masks, $I_{t}^{O}$ and $\\alpha_{t}^{O}$ are the image and mask rendered from camera pose $O$ at time $t$ , and $\\lambda$ is a loss weight. ", "page_idx": 17}, {"type": "text", "text": "Training. For training L4GM, we downsample the videos to 8 FPS and set $T=8,V=4$ . We apply the grid distortion augmentation [49] to non-reference views to improve the model robustness. We train the model with one 8-frame clip per GPU on 128 80G A100 GPUs. In each epoch, we iterate through all animations and sample a one-second clip from it regardless of the animation length. The model is trained for 200 epochs, which takes about one day. For ablation models, we reduce the dataset size to the $25\\%$ GObjaverse subset and reduce the number of GPUs to 32 correspondingly while keeping other settings unchanged. For training the interpolation model, we fine-tune L4GM with 4 frames per GPU on 64 80G A100 GPUs for 100 epochs. ", "page_idx": 17}, {"type": "text", "text": "Inference. We test on a 16G RTX 4080 Super GPU. We set the video length to $T\\,=\\,16$ Each forward pass through L4GM takes about 0.3 seconds while generating sparse views requires about 2 seconds, including ImageDream generation, LGM reconstruction, and azimuth selection. The 4D interpolation model takes 0.065 seconds to interpolate between every two frames. For example, for a 10-second 30-FPS video, we can reconstruct the input video in 15 FPS and interpolate the result to 45 FPS in 15 seconds, consisting of 2 seconds for sparse view generation, 3 seconds for reconstruction, and 10 seconds for interpolation. Video segmentation time is not included. ", "page_idx": 17}, {"type": "text", "text": "C Example Training Data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We show example training data in Figure 7. ", "page_idx": 17}, {"type": "text", "text": "D Full Quantitative Results on Consistent4D Benchmark ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present the detailed metric for every test sample on the Consistent4D benchmark in Table 4. ", "page_idx": 17}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/15af2c44bf3be55942acc11f216b6d1bd5815a91387a12c01f2f339a9fb94b7e.jpg", "img_caption": ["Figure 7: Example training data. Masked input views will not be visible to the model. They will be replaced by the copy of the multiview images at time $t=1$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 4: Comparison between L4GM and state-of-the-arts approaches on full metrics in the Consistent4D benchmark. Baseline results are from Gao et al. [15]. ", "page_idx": 19}, {"type": "table", "img_path": "PSPtj26Lbp/tmp/ce7e770ac3dfb9ba279637a5b9ef0c5666c93e190db4f19e80116788bd803b71.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/ae4a54cf255c7e63f7812c0d356c159553f1a11325f89fd48e08acbbeaf6cda0.jpg", "img_caption": ["Figure 8: Azimuth aligment. ImageDream often generates multiviews that misalign with the input frame. We first use LGM to generate a 3D from the multiview images, then render it from different azimuths, and finally render it from the most-aligned azimuth and its other three orthogonal camera poses. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/c25fb68b3b1a60c27b1cdb1d62536d50a63804686b4a2949596fb17c6240d0ba.jpg", "img_caption": ["Figure 9: HexPlane model. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Azimuth Alignment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "After getting $P_{\\mathrm{init}}$ , we render $P_{\\mathrm{init}}$ from a series of azimuth angles $\\theta~\\in~\\{-180^{\\circ},...,180^{\\circ}\\}$ and select the azimuth that best aligns with the input frame. This is determined by $\\theta_{\\mathrm{align}}\\ =$ $\\mathrm{argmin}_{\\theta}||f(P_{\\mathrm{static}},\\theta)\\,-\\,I_{1}||_{2}^{2}$ , where $f$ is a Gaussian volume rendering function. The final utilized sparse views, $\\mathcal{I}_{1}$ , are defined as $\\{f(P_{\\mathrm{static}},\\theta_{\\mathrm{align}}+\\Delta\\theta)\\}_{\\Delta\\theta\\in\\{0^{\\circ},90^{\\circ},180^{\\circ},270^{\\circ}\\}}.$ . An illustration is shown in Figure 8. ", "page_idx": 19}, {"type": "text", "text": "F HexPlane Model Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this ablation study, we predict a canonical Gaussian and a deformation field represented by HexPlane. An additional decoder decodes the output from the U-Net middle block into 6 planes. The decoder is also equipped with cross-view and temporal attention. The canonical Gaussian then goes through the deformation field to produce $T$ sets of 3D Gaussians. The decoder has channels [1024, 128, 128]. We show an illustration in Figure 9. ", "page_idx": 19}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/15770566d7e0c322bbbc85379b6a449d133ba63cd36b283f50194af306fde297.jpg", "img_caption": ["Givenreference video shown above, Twodifferent approachesareusedtocreate thecorresponding animated characterindifferentviews,resulting in animations A and B.Please answer the following questions. When answerquestions,focusonvideoviewdiffrentfromreferencevideo. Overall, which character animation looks more appealing and has a higher quality? O A O BO Equally Look at one frame of each video. Which animated character has the nicer and higher-quality appearance?O A O B  Equally For which animated character is the appearance closer to what is shown in the reference video?OA O BO Equally For which animated character is the motion closer to what is shown in the reference video?O A O B O Equally For which animated character is the motion more realistic and natural?Realistic and natural could mean less flickering and less artifacts.O AO B Equally ", "Figure 10: Screenshot of instructions provided to participants of the user studies for comparing L4GM and baselines. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "G More Qualitative Evaluation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Evaluation Dataset ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We collect 24 evaluation videos from multiple sources, including $^{\\,l}$ ) 10 videos from Emu video [16]; these are generated 4-second 16-FPS videos. 2) 7 videos from Sora [34]; these are generated long 30-FPS videos. 3) 5 videos from Veo [10]; these are generated long 30-FPS videos. 3) 2 videos from ActivityNet [12]; these are real-world long videos of weight lifting. ", "page_idx": 20}, {"type": "text", "text": "We segment the foreground object with SAM-Track [8] and then crop and rescale the video to 512x512. Since the optimization-based baseline methods are very memory intensive, we trim all videos to 4 seconds and downsample all 30 FPS videos to 15 FPS, so that all videos have 64 frames. ", "page_idx": 20}, {"type": "text", "text": "G.2 User Study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conducted human evaluations (user studies) through Amazon Mechanical Turk to assess the quality of our generated 4D scenes, comparing them with DG4D [39], OpenLRM [19], STAG4D [67] and performing ablation studies. ", "page_idx": 20}, {"type": "text", "text": "We used the 24 examples evaluation dataset as described in Section 6.2. We rendered both baselines and our dynamic 4D scenes from similar camera perspectives and created similar videos. We first asked the participants to watch the reference input video and then asked them to compare the two videos with respect to 5 different evaluation axes and indicate a preference for one of the methods with an option to vote for \u2018equally good\u2019 in a non-forced-choice format. The 5 categories measure overall quality, 3D appearance quality, as well as motion alignment to reference video and motion realism. ", "page_idx": 20}, {"type": "text", "text": "For a visual reference, see 10 for a screenshot of the evaluation interface. In all user studies, the order of video pairs (A-B) was randomized for each question. In all user studies, each video pair was evaluated by five participants, totaling 120 responses for each of the baseline comparisons. We selected participants based on specific criteria: they had to be from English-speaking countries, have an acceptance rate above $95\\%$ , and have completed over 1000 approved tasks on the platform. ", "page_idx": 20}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/b04abea3351808dba9cf8d2b26ec8f9f80d9b0fc9fe478862f527d8ae23dc2f9.jpg", "img_caption": ["Figure 11: More qualitative comparison on generated videos. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "H More Qualitative Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "H.1 More comparisons with state-of-the-art approaches ", "page_idx": 21}, {"type": "text", "text": "We show more qualitative comparisons with state-of-the-art approaches on both generated (Figure 11) and real-world (Figure 12) videos. ", "page_idx": 21}, {"type": "text", "text": "H.2 Qualitative Results on Consistent4D Data ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We show more qualitative results on the Consistent4D dataset in Figure 13. ", "page_idx": 21}, {"type": "text", "text": "I Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We show some limitations of L4GM in Figure 14. Our model can not handle motion ambiguity well. For example, in some walking motions, the model can successfully align with the reference view but the leg motion is not natural from other views. The model also cannot reconstruct multiple objects well, particularly when they occlude each other. Finally, the model fails to reconstruct objects from an ego-centric viewpoint, since the model assumes input views to be taken from $0^{\\circ}$ elevation. ", "page_idx": 21}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/d74e42eef8d594ae2eac78dc6f614e2a2965e6fa05a96dbe52cb94735287c965.jpg", "img_caption": ["Figure 12: More qualitative comparison on real-world videos. "], "img_footnote": ["the computation cost and empirical results, temporal attention would be a better design choice. "], "page_idx": 22}, {"type": "text", "text": "J More discussions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Ablation results on the Consistent4D benchmark. In Table 5, we evaluate ablation models and interpolation models on the Consistent4D. Note that the quantitative results can be noisy since the benchmark only has 7 test samples. Comparing b) to a), we confirm that the pretrain plays a critical role. Comparing f) to a), we observe that the 4D representation is crucial. To evaluate the 4D interpolation model, we downsample the evaluation videos\u2019 framerate by 3, reconstruct downsampled videos with a), and then interpolate the 4D reconstruction with our interpolation model. Notably, g) achieves an on-par performance to a), which suggests the effectiveness of the interpolation model. ", "page_idx": 22}, {"type": "text", "text": "Full attention. We explore replacing the temporal attention in L4GM with full attention. Full attention computes the self-attention on all images regardless of time and view, while temporal attention only computes the self-attention on images under the same viewpoint. As a result, full attention requires more computing. We show a memory-time analysis on a training iteration in Table 6, implemented with Efficient Attention. c) requires a larger memory usage and a longer processing time than b). In Figure 15, We further show a PSNR plot that compares temporal attention with full attention, trained on the GObjaverse subset. They achieve almost identical PSNR curves. Considering both ", "page_idx": 22}, {"type": "table", "img_path": "PSPtj26Lbp/tmp/c7d176dbf07e85a358c25ba8e27d2f3a4639510f2f26e0238b5b6b7b46ec6f09.jpg", "table_caption": ["Table 5: Ablation results on the Consistent4D benchmark. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Multi-view multi-step videos input. In Figure 16, we show a multi-view multi-step video generated by ImageDream, after azimuth alignment. Concretely, we input each individual frame of the reference video to the ImageDream model and generate a multiview image for each time step. The obtained multiview video lacks a temporal consistency. Since ImageDream is a probabilistic generative model, even the same ", "page_idx": 22}, {"type": "table", "img_path": "PSPtj26Lbp/tmp/013714299ed2f6290f07979795d00290cc19c83fe692f851804fd8acd5abf273.jpg", "table_caption": ["Table 6: Attention. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "image input could produce very different multi-view generation. Therefore, we believe that such temporally inconsistent multi-view videos are not ideal input to the reconstruction model. ", "page_idx": 22}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/6e667f1f616c6ba49aea0519ea8882a3243cb119914af906232fd0b04be12e05.jpg", "img_caption": ["Figure 13: Qualitative results on the Consistent4D dataset. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "New evaluation on the GObjaverse subset We provide quantitative results using the GObjaverse subset as training data in Table 5 line h. We manually verified that these Consistent4D testing samples are not part of the GObjaverse subset. The new results remain state-of-the-art and show no significant difference from the numbers reported in the main paper. ", "page_idx": 23}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/8a80fc8d657a82d96157a1fbbe2e15bb398c1421f6d55102cafb0b76c2f15aea.jpg", "img_caption": ["Figure 14: Failure cases. a) Motion ambiguity. b) Multiple objects with occlusions. c) Ego-centric videos taken from Ego4D [17]. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/a84f89c69c53b2e10fcf58309074e0ed57624f369b8a145aed684881e7a5cf36.jpg", "img_caption": ["Figure 15: Full-attention PSNR plot. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "PSPtj26Lbp/tmp/4fc740b6b483bbc3893f13a7bb2b5b65096a890b02ffea18c9211290c8282151.jpg", "img_caption": ["Figure 16: Multi-step multi-views. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: section 1 ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Appendix I ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: the paper does not include theoretical results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Appendix B Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: code to be released upon internal approval. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Appendix B Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: repeating large model training is too computationally expensive. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Appendix B ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: section 7 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Appendix B ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: User study details are in Section G.2 ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: User study details are in Section G.2 and it doesn\u2019t contain any potential risks. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]