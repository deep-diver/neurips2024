[{"figure_path": "PSPtj26Lbp/figures/figures_0_1.jpg", "caption": "Figure 1: L4GM generates 4D objects from in-the-wild input videos.", "description": "This figure shows examples of 4D object reconstruction from real-world videos using the L4GM model.  The top row displays the input video frames for two different objects (a panda and a robot). The subsequent rows showcase multi-view renderings of the 3D models generated by L4GM at different points in time.  This demonstrates the model's ability to create animated 3D assets from single-view video inputs.", "section": "Figure 1"}, {"figure_path": "PSPtj26Lbp/figures/figures_2_1.jpg", "caption": "Figure 2: L4GM. The overall model architecture of L4GM. Our model takes a single-view video and single-time step multiview images as input, and outputs a set of 4D Gaussians. It adopts a U-Net architecture and uses cross-view self-attention for view consistency and temporal cross-time self-attention for temporal consistency.", "description": "This figure illustrates the architecture of the L4GM model. The model takes as input a single-view video and a set of multiview images from a single time step. It then uses a U-Net architecture with cross-view self-attention and temporal self-attention layers to produce a sequence of 4D Gaussians representing the animated 3D object across time.", "section": "4 Our Method"}, {"figure_path": "PSPtj26Lbp/figures/figures_5_1.jpg", "caption": "Figure 2: L4GM. The overall model architecture of L4GM. Our model takes a single-view video and single-time step multiview images as input, and outputs a set of 4D Gaussians. It adopts a U-Net architecture and uses cross-view self-attention for view consistency and temporal cross-time self-attention for temporal consistency.", "description": "This figure illustrates the overall architecture of the L4GM model.  The model takes a single-view video and a set of multiview images from a single time step as input. It processes these inputs through a U-Net architecture incorporating cross-view self-attention to maintain view consistency and temporal self-attention to maintain temporal consistency across time steps. The output of the model is a sequence of 4D Gaussian representations, one for each time step in the input video.", "section": "4 Our Method"}, {"figure_path": "PSPtj26Lbp/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative results from L4GM, showcasing renderings from 4D reconstructions produced from two in-the-wild videos.", "description": "This figure showcases the qualitative results of the L4GM model on two real-world videos. The top row displays weightlifters performing different lifting actions, while the bottom row shows a frog character exhibiting various poses. For each video, multiple views of the generated 3D reconstructions are presented, demonstrating the model's ability to generate high-quality, temporally consistent animations from in-the-wild video data.  The image provides visual evidence of the model's capacity for real-world generalization and 4D reconstruction.", "section": "6 Experiments"}, {"figure_path": "PSPtj26Lbp/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative comparisons of L4GM's results against the baselines.", "description": "This figure shows a qualitative comparison of the results of L4GM against three other state-of-the-art methods (OpenLRM, STAG4D, and DG4D) on various example videos.  Each row represents a different video and shows the input view and novel view generated by each method. The comparison highlights L4GM's ability to generate higher-quality, more detailed and temporally consistent novel views compared to the other approaches.", "section": "6.2 Comparisons to State-of-the-Art Methods"}, {"figure_path": "PSPtj26Lbp/figures/figures_8_1.jpg", "caption": "Figure 6: PSNR plot. a) Training with different pretrain and training data. b) Training with different design choices. c) Per-frame PSNR with different video lengths T, AR denotes autoregressive.", "description": "This figure presents three PSNR plots to show the effect of different training strategies on the model's performance. Plot (a) compares training with different pretraining data and training datasets. Plot (b) shows the results with various design choices like using temporal attention, adding time embeddings, freezing the LGM, and using HexPlane. Plot (c) shows the performance of autoregressive reconstruction with various video lengths.", "section": "Ablation Studies"}, {"figure_path": "PSPtj26Lbp/figures/figures_17_1.jpg", "caption": "Figure 2: L4GM. The overall model architecture of L4GM. Our model takes a single-view video and single-time step multiview images as input, and outputs a set of 4D Gaussians. It adopts a U-Net architecture and uses cross-view self-attention for view consistency and temporal cross-time self-attention for temporal consistency.", "description": "This figure illustrates the architecture of the L4GM model, which takes a single-view video and multi-view images as input to produce a sequence of 4D Gaussian representations.  The model utilizes a U-Net architecture with cross-view and temporal self-attention layers to ensure view and temporal consistency in the output. The 4D Gaussians are then used to reconstruct the animated 3D objects.", "section": "4 Our Method"}, {"figure_path": "PSPtj26Lbp/figures/figures_18_1.jpg", "caption": "Figure 7: Example training data. Masked input views will not be visible to the model. They will be replaced by the copy of the multiview images at time t = 1.", "description": "This figure shows example training data used for the L4GM model.  It highlights the input format, demonstrating how a single input view is combined with multiple generated views (created using ImageDream) to form the input to the model for each time step. The masked input views indicate data that is not directly used for training; instead, these views are replaced with copies of the multiview images from the initial time step (t=1). This approach helps maintain temporal consistency throughout the training process.", "section": "4.2 Turning the 3D LGM into a 4D Reconstruction Model"}, {"figure_path": "PSPtj26Lbp/figures/figures_19_1.jpg", "caption": "Figure 2: L4GM. The overall model architecture of L4GM. Our model takes a single-view video and single-time step multiview images as input, and outputs a set of 4D Gaussians. It adopts a U-Net architecture and uses cross-view self-attention for view consistency and temporal cross-time self-attention for temporal consistency.", "description": "This figure illustrates the architecture of the L4GM model.  The model takes a single-view video and a set of multiview images from a single time step as input. It uses a U-Net architecture with cross-view self-attention to ensure consistency across different viewpoints and temporal self-attention to maintain consistency across time. The output of the model is a sequence of 3D Gaussian representations, one for each time step in the input video, forming the 4D representation.", "section": "4 Our Method"}, {"figure_path": "PSPtj26Lbp/figures/figures_19_2.jpg", "caption": "Figure 9: HexPlane model.", "description": "This figure illustrates the architecture of the HexPlane model used in the ablation study.  The model takes a canonical Gaussian representation as input and processes it through an averaging and reshaping stage to create a deformation field represented by a HexPlane. This deformation field is then combined with the canonical Gaussian to produce a Gaussian sequence for each timestep, generating the final 4D representation.", "section": "F HexPlane Model Details"}, {"figure_path": "PSPtj26Lbp/figures/figures_20_1.jpg", "caption": "Figure 4: Qualitative results from L4GM, showcasing renderings from 4D reconstructions produced from two in-the-wild videos.", "description": "This figure shows qualitative results of the L4GM model on two real-world videos.  The images demonstrate the model's ability to generate high-quality, temporally consistent 4D reconstructions of dynamic objects from single-view video inputs. The figure displays multiple frames from the generated 4D reconstructions, viewed from different angles, showcasing the quality and consistency of the results.", "section": "Experiments"}, {"figure_path": "PSPtj26Lbp/figures/figures_21_1.jpg", "caption": "Figure 5: Qualitative comparisons of L4GM's results against the baselines.", "description": "This figure compares the qualitative results of L4GM against three other state-of-the-art methods (OpenLRM, STAG4D, and DG4D) across four different animations. For each animation and method, the figure shows both the input view and a novel view generated by the model. The comparison highlights the superior quality and detail of L4GM's generated views.  The improvement of L4GM over the baselines is particularly noticeable in the quality of the rendered novel views, exhibiting better rendering of textures, and fewer artifacts. The overall visual fidelity and accuracy of 3D geometry are also significantly better in L4GM.", "section": "Experiments"}, {"figure_path": "PSPtj26Lbp/figures/figures_22_1.jpg", "caption": "Figure 12: More qualitative comparison on real-world videos.", "description": "This figure presents a qualitative comparison of the results generated by OpenLRM, STAG4D, DG4D, and the proposed L4GM method on real-world videos.  For each method, it displays the input view and a novel view generated by the model. The comparison focuses on showcasing the quality and realism of the generated 3D reconstructions for dynamic scenes captured from real-world videos.  It visually demonstrates the relative strengths and weaknesses of each approach in handling real-world video data complexities.", "section": "More Qualitative Results"}, {"figure_path": "PSPtj26Lbp/figures/figures_23_1.jpg", "caption": "Figure 13: Qualitative results on the Consistent4D dataset.", "description": "This figure shows qualitative results of the proposed model on the Consistent4D dataset.  The figure presents several examples of objects with their generated novel views from different angles. Each row represents a different object and shows the input view along with three generated novel views at two different time steps, illustrating the model's capability to generate consistent and high-quality 4D representations.", "section": "H.2 Qualitative Results on Consistent4D Data"}, {"figure_path": "PSPtj26Lbp/figures/figures_24_1.jpg", "caption": "Figure 14: Failure cases. a) Motion ambiguity. b) Multiple objects with occlusions. c) Ego-centric videos taken from Ego4D [17].", "description": "This figure shows three examples where the L4GM model fails: motion ambiguity where the model struggles to produce natural motion across multiple views; multiple objects with occlusions where the model fails to reconstruct objects that are occluded; and ego-centric videos from Ego4D dataset [17] where the model fails when the input videos are taken from an ego-centric viewpoint.", "section": "I Limitations"}, {"figure_path": "PSPtj26Lbp/figures/figures_24_2.jpg", "caption": "Figure 15: Qualitative comparisons of L4GM's results against the baselines.", "description": "This figure compares the qualitative results of L4GM against three baselines: OpenLRM, STAG4D, and DG4D. The comparison shows the input view and novel views generated by each method for several time steps. This allows for a visual assessment of the relative strengths and weaknesses of the various methods in terms of image quality, temporal coherence, and overall accuracy.", "section": "6 Experiments"}, {"figure_path": "PSPtj26Lbp/figures/figures_24_3.jpg", "caption": "Figure 13: Qualitative results on the Consistent4D dataset.", "description": "This figure shows qualitative results on the Consistent4D dataset.  It presents several examples of 4D object reconstructions generated by the L4GM model, showcasing the model's ability to generate consistent and high-quality animations from various viewpoints across different timesteps. Each row represents a different object from the dataset, and each column represents a view of the object at different timesteps, demonstrating the temporal consistency of the model's output.", "section": "H.2 Qualitative Results on Consistent4D Data"}]