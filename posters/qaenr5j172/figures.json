[{"figure_path": "QAEnr5j172/figures/figures_2_1.jpg", "caption": "Figure 1: The overall pipeline of our proposed method.", "description": "This figure illustrates the two-stage framework for rendered-to-real fashion image translation. Stage 1, Domain Knowledge Injection (DKI), involves fine-tuning a pre-trained text-to-image diffusion model on real fashion photos and then using a negative domain embedding to guide the model away from the characteristics of rendered images. Stage 2, Realistic Image Generation (RIG), employs DDIM inversion to convert the rendered image into latent space, and uses the negative domain embedding and a texture-preserving attention control mechanism to generate a realistic counterpart while maintaining fine-grained clothing textures.", "section": "3 Method"}, {"figure_path": "QAEnr5j172/figures/figures_4_1.jpg", "caption": "Figure 2: The diagram of Texture-preserving Attention Control (TAC).", "description": "This figure illustrates the Texture-preserving Attention Control (TAC) mechanism.  It shows how queries (Q), keys (K), and values (V) from both the rendered image (cg) and the generated image (r) are used in the self-attention process. Specifically, the self-attention features from the rendered image are injected into the shallow layers of the UNet, decoupling texture details from general characteristics to preserve fine-grained textures during image generation. The process involves projection, softmax calculation, and the use of queries and keys from both the rendered and real domains to guide attention.", "section": "3.3 Domain Knowledge Injection"}, {"figure_path": "QAEnr5j172/figures/figures_5_1.jpg", "caption": "Figure 3: Results on our proposed SynFashion Dataset. (Please zoom in for details.)", "description": "This figure compares the results of different image translation methods on the SynFashion dataset.  The dataset consists of high-quality rendered fashion images, which the authors created. The 'Source Image' column shows the original rendered image. The other columns display the results produced by CUT, SANTA, VCT, UNSB, and the authors' proposed method.  The figure highlights the ability of the authors' method to generate more realistic and detailed images compared to the other approaches, particularly regarding fine-grained clothing textures.", "section": "3 Method"}, {"figure_path": "QAEnr5j172/figures/figures_6_1.jpg", "caption": "Figure 3: Results on our proposed SynFashion Dataset. (Please zoom in for details.)", "description": "This figure shows a comparison of image translation results on the SynFashion dataset between the proposed method and four existing methods: CUT, SANTA, VCT, and UNSB.  Each row represents a different input rendered image, and each column shows the result of a different method. The proposed method's outputs generally exhibit higher realism and better preservation of fine-grained textures in the clothing compared to the other methods.", "section": "4 Results"}, {"figure_path": "QAEnr5j172/figures/figures_8_1.jpg", "caption": "Figure 5: Visual examples of ablation study in a drop-one-out manner. (DKI: Domain Knowledge Injection. TAC: Texture-preserving Attention Control.)", "description": "This figure shows the ablation study of the proposed method by removing one component at a time. The top row shows the results on images with a person wearing a headscarf, and the bottom row shows the results on images with a person wearing a dress. The first column shows the source image, the second column shows the result without source domain knowledge injection, the third column shows the result without target domain knowledge injection, the fourth column shows the result without texture-preserving attention control, and the fifth column shows the result with all components.", "section": "4.4 Ablation Study and Further Analysis"}, {"figure_path": "QAEnr5j172/figures/figures_9_1.jpg", "caption": "Figure 6: A visual example of tuning TAC ratio and denoising strength.", "description": "This figure shows the impact of the TAC ratio and denoising strength on the generated images.  Different rows represent various denoising strengths, ranging from 0 to 0.9. Each row shows a series of images, with each column representing increasing TAC control steps (0% to 90%). As the denoising strength increases, the images become more realistic, but finer details might be lost. At the same time, increasing the TAC ratio (more self-attention control) better preserves details. The figure demonstrates the trade-off between realism and texture detail preservation by adjusting these two parameters.", "section": "4.4 Ablation Study and Further Analysis"}, {"figure_path": "QAEnr5j172/figures/figures_15_1.jpg", "caption": "Figure 7: Results on textual textures and different rendering inputs.", "description": "This figure shows the results of applying the proposed method to images with different textures and rendering methods (rasterization vs. ray tracing).  It demonstrates the method's ability to generate realistic images regardless of the input's rendering technique and texture complexity. The results highlight the method's robustness and effectiveness in preserving fine-grained clothing textures, achieving a high level of realism in the generated images.", "section": "A Appendix / supplemental material"}, {"figure_path": "QAEnr5j172/figures/figures_17_1.jpg", "caption": "Figure 4: Results on the Face Synthetics dataset. (Please zoom in for details.)", "description": "This figure presents a comparison of image translation results on the Face Synthetics dataset using different methods. The methods compared are CUT, SANTA, VCT, UNSB, and the proposed method. Each row represents a different input rendered image, with the leftmost column showing the source image and subsequent columns showing the results from each method.  The figure highlights the differences in realism and detail preservation achieved by each technique. The caption advises zooming in to see details clearly.", "section": "4 Experiments"}, {"figure_path": "QAEnr5j172/figures/figures_18_1.jpg", "caption": "Figure 3: Results on our proposed SynFashion Dataset. (Please zoom in for details.)", "description": "This figure compares the results of five different methods (CUT, SANTA, VCT, UNSB, and the proposed method) on the SynFashion dataset.  Each row shows the results for a different input rendered image. The \"Source Image\" column shows the original rendered image. The subsequent columns display the results generated by each method. The figure visually demonstrates the superiority of the proposed method in generating realistic images compared to existing state-of-the-art approaches, especially in terms of preserving fine-grained clothing textures and enhancing overall realism.", "section": "4 Results"}, {"figure_path": "QAEnr5j172/figures/figures_19_1.jpg", "caption": "Figure 10: Examples of collected SynFashion dataset (Part 1).", "description": "This figure shows examples of the SynFashion dataset, a high-quality rendered fashion image dataset created using Style3D Studio.  It presents various clothing items (pants, t-shirts, lingerie, skirts, hoodies) rendered with different textures and poses. The images demonstrate the diversity and high quality of the dataset used to train and evaluate the proposed rendered-to-real image translation method.", "section": "4.1 Datasets"}, {"figure_path": "QAEnr5j172/figures/figures_20_1.jpg", "caption": "Figure 1: The overall pipeline of our proposed method.", "description": "The figure shows the overall pipeline of the proposed method for rendered-to-real fashion image translation. It consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, a pre-trained Text-to-Image (T2I) diffusion model is fine-tuned on real fashion photos and then uses a negative domain embedding to guide image generation towards realistic counterparts.  In RIG, a rendered image is inverted into a latent noise map, and a Texture-preserving Attention Control (TAC) mechanism is used to generate a realistic image while preserving fine-grained textures. The TAC leverages the attention maps in shallow layers of the UNet to improve texture consistency.", "section": "3 Method"}, {"figure_path": "QAEnr5j172/figures/figures_21_1.jpg", "caption": "Figure 12: Examples of collected SynFashion dataset (Part 3).", "description": "This figure shows examples from part 3 of the collected SynFashion dataset. The dataset contains various types of clothing items rendered in different textures and colors. Each row represents a different clothing category (Hanfu, Jeans, Shorts, Down Jacket, Vest and Camisole), with multiple images showing different textures for each category.  The images are presented in four different views of each garment (front, back and two other random views).", "section": "A.4 More details of collected SynFashion dataset"}, {"figure_path": "QAEnr5j172/figures/figures_22_1.jpg", "caption": "Figure 1: The overall pipeline of our proposed method.", "description": "The figure illustrates the overall pipeline of the proposed method for rendered-to-real fashion image translation. It consists of two main stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In the DKI stage, a pre-trained Text-to-Image (T2I) diffusion model is fine-tuned on real fashion photos and then uses negative domain embedding to inject knowledge. The RIG stage uses a texture-preserving attention control mechanism to generate realistic images from rendered images.  The pipeline shows the flow of data from rendered images through the two stages, to the final realistic output. ", "section": "3 Method"}]