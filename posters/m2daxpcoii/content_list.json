[{"type": "text", "text": "Practical Shuffle Coding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Julius Kunze University College London juliuskunze@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Daniel Severo University of Toronto and Vector Institute d.severo@mail.utoronto.ca ", "page_idx": 0}, {"type": "text", "text": "Jan-Willem van de Meent University of Amsterdam j.w.vandemeent@uva.nl ", "page_idx": 0}, {"type": "text", "text": "James Townsend University of Amsterdam j.h.n.townsend@uva.nl ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a general method for lossless compression of unordered data structures, including multisets and graphs. It is a variant of shuffle coding that is many orders of magnitude faster than the original and enables \u2018one-shot\u2019 compression of single unordered objects. Our method achieves state-of-the-art compression rates on various large-scale network graphs at speeds of megabytes per second, efficiently handling even a multi-gigabyte plain graph with one billion edges. We release an implementation that can be easily adapted to different data types and statistical models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Big data is often contained in unordered objects, such as sets, multisets, graphs, or hypergraphs. Unlike ordered data types like text, audio, and video, the order of elements in these structures is irrelevant and represents redundant information. Recent work by Kunze et al. (2024) shows that eliminating this redundancy can lead to significant storage and transmission savings. It proposed shuffle coding, an entropy coding method based on bits-back (Townsend et al., 2019) with asymmetric numeral systems (ANS; Duda, 2009) that approaches optimal compression rates for sequences of unordered objects. However, their specific method, which we will refer to as complete joint shuffle coding, has two major limitations that make it impractical for large unordered data structures: It incurs a prohibitive initial bit cost in one-shot scenarios where only a single unordered object needs to be compressed, and it requires the exact computation of automorphism groups which is often slow or completely intractable, for example for large graphs. ", "page_idx": 0}, {"type": "text", "text": "To overcome these limitations, we propose two new variants, autoregressive and incomplete shuffle coding. Autoregressive shuffle coding builds on recent work by Severo et al. (2023a), which constructed an optimal one-shot codec for multisets from a codec of vectors by storing information in an ordering. This method depends on the simple structure of multisets\u2019 automorphism groups, and does not extend to other unordered objects such as unlabeled graphs. Autoregressive shuffle coding generalizes this method to arbitrary unordered objects. Incomplete shuffle coding approximates an object\u2019s symmetries, enabling compression despite intractable automorphism groups. These two variants can be combined into a method allowing one-shot compression of large unordered data structures at practical speeds. ", "page_idx": 0}, {"type": "text", "text": "Our experiments show that our high-performance implementation matches the optimal compression rates from Severo et al. (2023a) for multisets, but is orders of magnitude faster. Similarly, it is many orders of magnitude faster than joint shuffle coding for medium-sized graphs of various types with minimal rate increase. We also compress much larger unordered graphs, including social network graphs and random graphs with up to a billion edges, which are infeasible for complete shuffle coding. We show that the rate discount from removing order information is large, as previously shown for joint shuffle coding on many smaller graph datasets, and that we achieve state-of-the-art graph compression rates at practical speeds when using models with minimal parameters. We release source code1 which can be extended easily to new models and classes of unordered objects other than multisets and graphs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The structure of the paper is as follows: Section 2 reviews the necessary background, including permutable classes, unordered objects and codecs. Sections 3 and 4 detail our incomplete and autoregressive shuffle coding variants. We discuss related work in Section 5, our experimental results in Section 6, and conclude in Section 7 with future research directions. Proofs are in Appendix B. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/d6834d845ecaf9413d78677966765e9691d5ec390458d4f1edd9d99007e5be91.jpg", "table_caption": ["Table 1: Examples for key concepts from Section 2 for the permutable class $\\mathcal{F}$ of ASCII strings of length 5. $\\left\\{\\!\\!\\left\\{\\cdot\\cdot\\cdot\\bar{\\cdot}\\right\\}\\!\\!\\right\\}$ denotes a multiset, and we use cycle notation for permutations. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "This paper is concerned with compression of unordered objects. Kunze et al. (2024) define these in terms of equivalence classes which comprise ordered objects that are identical up to re-ordering. We introduce ordered and unordered objects in Section 2.1, as well as how equivalence classes of ordered objects can form ordered objects themselves, a central idea of this paper. We review codecs in Section 2.2 and the optimal compression rate for unordered objects in Section 2.3. We provide a summary of concepts through examples in Table 1. ", "page_idx": 1}, {"type": "text", "text": "2.1 Permutable classes ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Concepts from group theory, including subgroups, actions, stabilizers, and orbits are used throughout.   \nWe provide a brief introduction in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "For $n\\in\\mathbb N$ , we let $[n]:=\\{0,1,\\ldots,n-1\\}$ , with $[0]=\\emptyset$ . For this paper, it is convenient to identify permutations purely by their cycle notation, such that we can apply a permutation of order $i$ to any objects of order $\\geq i$ . This can be achieved by defining the symmetric group ${\\mathcal{S}}_{n}$ on the subset of bijections $f:\\mathbb{N}\\to\\mathbb{N}$ with $f(i)=i$ for all $i\\in\\mathbb{N}\\backslash[n]$ , leading to ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{i}=\\mathrm{Stab}_{S_{i+1}}(i)\\leq S_{i+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "From here on, we use the shorthand $H\\leq G$ to mean that $H$ is a subgroup of $G$ $:\\mathrm{Stab}_{G}(i)$ denotes the stabilizer with respect to $i\\in[n]$ of a group $G\\leq S_{n}$ acting on the indices $[n]$ . ", "page_idx": 1}, {"type": "text", "text": "We will consider objects which can be \u2018re-ordered\u2019 by applying permutations. This is captured in the following definition from Kunze et al. (2024): ", "page_idx": 1}, {"type": "text", "text": "Definition 2.1 (Permutable class). For $n\\in\\mathbb N$ , a permutable class $\\mathcal{F}$ of order $n$ is a pair of a set $F$ and a left group action of the permutation group ${\\mathcal{S}}_{n}$ on $F$ , which we denote with the $\\cdot$ binary operator. We also use $\\mathcal{F}$ to refer to the underlying set $F$ . We refer to the elements of $F$ as ordered objects. ", "page_idx": 1}, {"type": "text", "text": "Strings are ordered objects, and so are labeled graphs because their vertices can be re-ordered. Equivalence classes of ordered objects can themselves form a permutable class. For example, caseinsensitive ASCII strings naturally are ordered objects, and can be defined as the equivalence classes of such strings under an equivalence relation $\\sim$ that indicates equality up to character case, as illustrated in Table 1. As shown in Appendix B.1, this happens whenever the equivalence relation is preserved under permutations, in the following sense: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition 2.2 (Congruence, quotient class). We say an equivalence relation $\\sim$ on a permutable class ${\\mathcal F}=\\left(F,\\cdot\\right)$ of order $n$ is a congruence if $s\\cdot f\\,\\sim\\,s\\cdot g$ holds if $f\\sim g$ for all $f,g\\in{\\mathcal{F}}$ and $s\\ \\in\\ S_{n}$ . Then, the quotient set $F/\\sim$ equipped with the operator $\\cdot:S_{n}\\times(F/\\mathrm{\\sim})\\to F/\\mathrm{\\sim}$ with $s\\cdot f_{\\sim}:=\\{s\\cdot f\\mid f\\in f_{\\sim}\\}$ forms a permutable class of order $n$ , which we refer to as the quotient class of $\\mathcal{F}$ by $\\sim$ , denoted as $\\mathcal{F}/\\sim$ . For $f\\in\\mathcal F$ we use $f_{\\sim}$ to denote the equivalence class under $\\sim$ containing $f$ . ", "page_idx": 2}, {"type": "text", "text": "We define unordered objects as equivalence classes comprising ordered objects that are identical up to re-ordering, an important special case of Definition 2.2: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.3 (Isomorphism, unordered objects). For two objects $f$ and $g$ in a permutable class $\\mathcal{F}$ , we say that $f$ is isomorphic to $g$ , and write $f\\simeq g$ , if there exists $s\\in S_{n}$ such that $g=s\\cdot f$ . The isomorphism relation $\\simeq$ is a congruence, inducing a quotient class of unordered objects that we will denote as $\\overline{{\\mathcal{F}}}:=\\mathcal{F}/\\simeq$ , and the unordered object containing some $f\\in\\mathcal F$ as $\\bar{f}:=f_{\\simeq}$ . ", "page_idx": 2}, {"type": "text", "text": "Unordered strings then correspond to multisets (for example ${\\overline{{s e\\mathfrak{e}}}}=\\{\\!\\{\\mathsf{e},\\mathsf{e},\\mathsf{s}\\}\\!\\}=\\{\\mathsf{s e e},\\mathsf{e s e},\\mathsf{e e s}\\})$ and unordered graphs to unlabeled graphs. The set of permutations that, when applied to an ordered object, do not change it, forms a group indicating its \u2018symmetries\u2019: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.4 (Automorphism group). For an element $f$ of a permutable class $\\mathcal{F}$ , we let $\\operatorname{Aut}(f)$ denote the automorphism group of $f$ , defined by $\\operatorname{Aut}(f):=\\{s\\in S_{n}\\mid s\\cdot f=f\\}$ . ", "page_idx": 2}, {"type": "text", "text": "For the example of unordered objects, every element\u2019s automorphism group comprises all permutations, $\\operatorname{Aut}({\\bar{f}})^{\\!\\!^{\\!\\!\\perp}}=S_{n}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Codecs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Shuffle coding requires stack-like (LIFO) codecs, such as those based on the range variant of asymmetric numeral systems (rANS; Duda, 2009), to save bits corresponding to the redundant order using bits-back (Townsend et al., 2019). To define these, we fix a set $M$ of prefix-free binary messages, and a length function $l\\colon M\\rightarrow[0,\\infty)$ , which measures the number of physical bits required to represent values in $M$ . We rely on the following definition from Kunze et al. (2024): ", "page_idx": 2}, {"type": "text", "text": "Definition 2.5 (Codec). A stack-like codec (or simply codec) for a set $X$ is an invertible function encode : $M\\times X\\to M$ . We call a codec optimal for a probability distribution over $X$ with mass function $P$ if for any $m\\;\\in\\;M$ and $x\\,\\in\\,X$ , the message length $l$ satisfies2 $l({\\mathsf{e n c o d e}}(m,x))\\approx$ l(m) + logP (1x) . We refer to log $\\frac{1}{P(x)}$ as the optimal rate and to the inverse of encode as decode. Since decode has to be implemented in practice, we treat it as an explicit part of a codec below. ", "page_idx": 2}, {"type": "text", "text": "The encode function requires a pre-existing message as its first input. Therefore, at the beginning of encoding we set $m$ equal to some fixed, short initial message $m_{0}$ , with length less than 64 bits. As in other entropy coding methods, which invariably have some small constant overhead, this \u2018initial bit cost\u2019 is amortized as we compress more data. ", "page_idx": 2}, {"type": "text", "text": "2.3 Optimal rate for unordered objects ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our codec will be based on a probability distribution with mass function $P$ over ordered objects from a permutable class $\\mathcal{F}$ . Then, for any equivalence relation $\\sim$ on $\\mathcal{F}$ , a joint distribution $P(\\bar{f_{\\sim}},g)$ with equivalence classes $f_{\\sim}\\in\\mathcal{F}/\\sim$ is induced by sampling an ordered object $g\\in{\\mathcal{F}}$ from $P$ , and returning its corresponding equivalence class, resulting in ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(f_{\\sim})=\\sum_{g\\in f_{\\sim}}P(g).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We will implicitly assume this induced distribution over quotient classes, even in nested cases. Kunze et al. (2024) specify the optimal rate of any codec for unordered objects $\\bar{f}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log\\frac{1}{P(\\bar{f})}=\\log\\frac{1}{\\underbrace{P(f)}_{\\mathrm{Ordered\\;rate}}}-\\log\\frac{n!}{|\\mathrm{Aut}(f)|},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "assuming, without loss of modeling power, that $P(f)$ is exchangeable, meaning invariant under permutations of $f$ . ", "page_idx": 3}, {"type": "text", "text": "3 Incomplete shuffle coding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Table 2: Example for color refinement as defined in Appendix $\\mathrm{C}$ on plain graphs with 0 and 1 convolutions, and the resulting incompletely ordered graphs according to Definition 3.1. Colors only visualize correspondence to the resulting hashes, they are not materialized in the graph. The actual hashes, characters in the example, depend on the specific hash function used. $C_{0}$ bases each vertexes\u2019 hash on its degree only, while $C_{1}$ also takes the multiset of neighboring hashes from $C_{0}$ into account. The automorphism group of the incompletely ordered graph is the same as that for the string of vertex hashes, and color partitions correspond to their orbits. ", "page_idx": 3}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/fb9dd7f8765d01f5c017562209394421d9771f3ed947d80ffa9c20e3dd0f8352.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Complete joint shuffle coding, as presented in Kunze et al. (2024), relies on a function to retrieve (a list of generators of) the automorphism group $\\operatorname{Aut}(f)$ , as well as the canonization for any given ordered object $f$ . No polynomial-time algorithm is known to compute this function for graphs.3 Accordingly, their results show that the method is impractically slow even for moderately sized graphs. ", "page_idx": 3}, {"type": "text", "text": "In this section, we will introduce a method that gives shuffle coding a reliably fast runtime for graphs. To achieve this, we will in return accept slightly suboptimal compression rates. Instead of recovering the exact order and therefore realizing the complete bit discount, we can ignore some hard-to-compute information about the order to improve runtime. Specifically, we can treat isomorphic objects that are hard to distinguish as elements of the same \u2018incompletely ordered\u2019 object, formalized as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Incompletely ordered objects). We refer to the elements of a quotient class ${\\mathcal{F}}/{\\sim}$ as incompletely ordered objects if $f\\sim\\subseteq{\\bar{f}}$ for all $f\\in\\mathcal F$ . ", "page_idx": 3}, {"type": "text", "text": "We show in Appendix B.3 that their unordered objects $\\overline{{\\mathcal{F}/\\sim}}$ correspond to the original unordered objects $\\overline{{\\mathcal F}}$ through the bijection $\\bar{f}=\\mathsf{U}\\,\\overline{{f_{\\sim}}}$ , leading to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log\\frac{1}{P(f)}-\\log\\frac{1}{P(f_{\\sim})}=\\log\\frac{|\\mathrm{Aut}(f_{\\sim})|}{|\\mathrm{Aut}(f)|}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equation (4) reveals the rate increase compared to the optimal rate if we suboptimally code $f_{\\sim}$ by representing it by an arbitrary $f\\in f_{\\sim}$ and using a codec optimal for $P(f)$ . ", "page_idx": 3}, {"type": "text", "text": "We can apply joint shuffle coding to incompletely ordered objects, an approach we refer to as incomplete joint shuffle coding.We are then not recovering the \u2018hard-to-compute\u2019 bits to distinguish elements within $f_{\\sim}$ , leading to a rate increase given by Equation (4) over the optimal rate. This requires two functions determining the automorphism group $\\operatorname{Aut}(f_{\\sim})$ and a canonization $\\hat{f}_{\\sim}$ for any given such $f_{\\sim}$ . In the next section, we introduce a choice for $\\sim$ that makes these functions and therefore incomplete joint shuffle coding practical for graphs. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Any function $C$ on $\\mathcal{F}$ with the relation defined by $C(f)=C(g)$ being a congruence has an associated class of incompletely ordered objects ${\\mathcal{F}}/{\\sim}_{C}$ through $f\\sim_{C}g$ exactly if $f\\simeq g$ and $C(f)=C(g)$ . The color refinement algorithm, also known as the 1-dimensional version of the algorithm of Leman and Weisfeiler (1968), is a practical choice for such a function for graphs. It returns a string of $n$ vertex \u2018colors\u2019 that were iteratively refined by hashing local features through a graph convolution, starting from the vertex degrees, as visualized in Table 2, and formalized in Appendix C. As discussed there, any $k>0$ convolutions suffice to find the exact automorphism group $\\operatorname{A}\\!\\operatorname{\\bar{u}t}(g_{\\sim_{C_{k}}})=\\operatorname{Aut}(g)$ for almost all simple graphs for large enough $n$ (Babai et al., 1980), and thus in practice the compression rate of incomplete shuffle coding with color refinement is usually optimal or near-optimal. ", "page_idx": 4}, {"type": "text", "text": "When used for incomplete joint shuffle coding, color refinement yields a string of vertex \u2018colors\u2019 that assigns a graph to a specific incomplete ordered graph $g_{\\sim_{C_{k}}}$ . Since these vertex colors form a string, we can apply complete joint shuffle coding which is fast for multisets, to approximately canonize graphs and code string cosets. As shown in Appendix B.3, for a fixed number of convolutions $k$ , the resulting, much improved overall runtime complexity is ", "page_idx": 4}, {"type": "equation", "text": "$$\nO(m+n\\log n),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $m$ denotes the number of edges in the graph. ", "page_idx": 4}, {"type": "text", "text": "4 Autoregressive shuffle coding ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "m2DaXpCoIi/tmp/a0019388497d78b475a4346c1dfda0eaa6a1d8c572b7853d42055f1984455b7d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Iterations for autoregressive shuffle coding during encoding of (a) a multiset and (b) an unlabeled undirected graph. Dotted placeholders indicate deleted information. Decoding an orbit allows to \u2018pin\u2019 an element in the last position. The pinned element is subsequently \u2018popped\u2018 from the object and encoded, and the process is repeated recursively on the remaining unordered prefix. ", "page_idx": 4}, {"type": "text", "text": "Joint shuffle coding, both complete and incomplete, incurs a prohibitive initial bit cost in one-shot scenarios where only a single unordered object needs to be compressed. This is because it constructs an encoder by decoding all order information from the message (the bits-back step) before the ordered object is encoded with a given \u2018ordered\u2019 codec optimal for $P(f)$ . Therefore, other information has to be encoded into the message $m$ before an unordered object. At the very beginning of encoding, these \u2018initial bits\u2019 can be generated at random, but they are unavoidably encoded into the message. While for sequences of such objects, this constant initialization overhead is amortized and the rate tends to the optimal rate with more objects being compressed, it means that joint shuffle coding realizes no discount when coding a single unordered object, rendering it useless in the one-shot case. ", "page_idx": 4}, {"type": "text", "text": "Specifically, joint shuffle coding realizes the discount from Equation (3) in the net rate completely (or incompletely if using the variant from Section 3), but none of it in the (one-shot) rate. We now aim to realize most of the discount in the rate by progressively decoding the permutation while autoregressively encoding the object. Severo et al. (2023a) implement this idea for the special case of multisets, as visualized in Figure 1(a). However, it exploits the simple structure of a string\u2019s automorphism group and does not extend to other unordered objects such as graphs. In this section, we will generalize this codec to arbitrary unordered objects, including graphs, as shown in Figure 1(b). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/3d35b3e287ce9745e507bb32c4d3040ca8a6529e0fd519e647d798fe1fca41a7.jpg", "table_caption": ["Table 3: Examples of key concepts from Section 4 for the permutable class $\\mathcal{F}$ of ASCII strings of length 5 and the prefixing chain given by Example 4.1. $\\mathbb{\\left\\{}\\!\\!\\left\\{\\cdot\\cdot\\cdot\\right\\}\\!\\!\\!\\right\\}\\!\\!.$ denotes a multiset, and $\\vdots$ indicates an arbitrary ASCII character. We visualize the same concepts for graphs in Table 4 in Appendix D. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Prefixes of ordered objects ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A general notion of \u2018pinning\u2019 objects such as graphs, as visualized in Figure 1, can be formalized by disallowing any permutations involving the last position, exploiting Equation (1): ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1 (Derivative). For a permutable class ${\\mathcal{F}}=(F,\\cdot)$ of order $n>0$ , the pair of $F$ and $':S_{n-1}\\times F\\rightarrow F$ with $s\\cdot^{\\prime}f:=s\\cdot f$ forms a permutable class of order $n-1$ , which we refer to as the derivative of $\\mathcal{F}$ , denoted as ${\\mathcal{F}}^{\\prime}$ . We say that the unordered object of this class containing $f$ is the unordered derivative of $f$ , denoted as ${\\overline{{f^{\\prime}}}}$ . By applying the derivative $i\\in[n]$ times, we obtain a permutable class of order $n-i$ , which we refer to as the $i^{\\th}$ -th derivative of $\\mathcal{F}$ , denoted as ${\\mathcal{F}}^{(i)}$ . We define the unordered $i$ -th derivative of $f$ in the same way, denoted as $\\overline{{f^{(i)}}}$ . ", "page_idx": 5}, {"type": "text", "text": "The derivative ${\\mathcal{F}}^{\\prime}$ has the same elements as $\\mathcal{F}$ , but its unordered objects $\\overline{{f^{\\prime}}}$ are different: $\\overline{{{\\sf s e e n^{\\prime}}}}=$ $\\{\\downarrow\\mathsf{s},\\mathsf{e},\\mathsf{e}\\}\\mathsf{n}=\\{\\mathsf{s e e n},\\mathsf{e s e n},\\mathsf{e e s n}\\}\\neq\\overline{{\\mathsf{s e e n}}}.$ . While the derivative ${\\mathcal{F}}^{\\prime}$ pins the last position, the $i$ -th derivative ${\\mathcal{F}}^{(i)}$ pins the last $i$ positions. ", "page_idx": 5}, {"type": "text", "text": "Length $i$ string prefixes are ordered objects of order $i$ , which we formalize as follows: ", "page_idx": 5}, {"type": "text", "text": "Example 4.1 (String prefixes). For $i\\in[n+1]$ and a set of elements $X$ , let $\\sim\\![i]$ be the equivalence relation on strings $X^{n}$ with $f\\sim_{[i]}g$ exactly if the first $i$ elements of $f$ and $g$ are equal for $f,g\\in X^{n}$ . We denote the equivalence class of $f\\in X^{n}$ under $\\sim\\![i]$ as $f_{[i]}$ and refer to it as (string) prefix of $f$ of length $i$ . While $\\sim\\![i]$ is not a congruence on $X^{n}$ , it is on the $(n-i)$ -th derivative $(X^{n})^{(n-i)}$ , and $(X^{n})^{(n-i)}/{\\sim_{[i]}}$ therefore forms a quotient class. ", "page_idx": 5}, {"type": "text", "text": "While a string naturally has \u2018prefixes\u2019 and \u2018elements\u2019, it is not obvious what the equivalent notions should be for graphs, and more generally, ordered objects. The conditions our method requires are captured in the following generalized definition of prefixes, for which Example 4.1 is a special case. Importantly, the notion of a \u2018slice\u2019 will be used in place of \u2018string element\u2019, as visualized in Table 3: ", "page_idx": 5}, {"type": "text", "text": "Definition 4.2 (Prefixes of ordered objects). For a permutable class $\\mathcal{F}$ of order $n$ and all $i\\in[n+1]$ , let $\\sim\\![i]$ be a congruence on $\\mathcal{F}^{(n-i)}$ . We then refer to the tuple of quotient classes $\\mathcal{F}_{[i]}:=\\mathcal{F}^{(n-i)}/{\\sim}_{[i]}$ as a prefixing chain on $\\mathcal{F}$ if the equivalence class of $f\\in\\mathcal F$ under $\\sim\\![i]$ , referred to as prefix of $f$ of length $i$ and denoted as $f_{[i]}$ , and $\\begin{array}{r}{f_{i}:=\\bigcup\\overline{{f_{[i+1]}^{\\prime}}}}\\end{array}$ , referred to as the slice of $f$ at index $i$ , fulfill ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{[n]}=\\{f\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "meaning that an object of order $n$ is uniquely determined by its prefix of length $n$ , and ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{[i+1]}=f_{[i]}\\cap f_{i},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "meaning that a prefix of length $i+1$ is uniquely determined by the prefix of length $i$ and the slice at index $i$ . The latter condition ensures the existence of an invertible function pop : $\\mathcal{F}_{[i+1]}\\;\\rightarrow\\;$ $\\{(f_{[i]},f_{i})\\mid f\\in\\mathcal{F}\\}$ with $\\mathrm{pop}(f_{[i+1]})=(f_{[i]},f_{i})$ . We will denote its inverse as push. ", "page_idx": 6}, {"type": "text", "text": "For strings, the slice $f_{i}$ uniquely determines the element at index $i$ . In practice, we can represent a string slice $f_{i}$ simply as the string element at index $i$ since we only ever use it in context of a prefix $f_{[i]}$ . This definition of prefixes is general enough to apply to graphs, , as visualized in Table 4: ", "page_idx": 6}, {"type": "text", "text": "Example 4.2 (Graph prefixes). For $i\\,\\in\\,\\{0,\\dots,n\\}$ , let $\\sim\\![i]$ be the relation on simple graphs $\\mathcal{G}_{n}$ with $f\\,\\sim_{[i]}\\,g$ exactly if $f$ and $g$ are equal up to the edges between the last $n\\,-\\,i$ vertices for $f,g\\in{\\mathcal{G}}_{n}$ . The quotient classes $\\mathcal{G}_{n}^{(n-i)}/{\\sim}_{[i]}$ then form a prefixing chain. Graph prefixes $f_{[i]}$ can then be represented as graphs without any edges between the last vertices. Because a graph slice for $i\\in[n]$ is only ever used in context of a prefix $f_{[i]}$ , we represent it as the subset of edges from $\\{(i,i+1),(i,i+2),\\ldots,(i,n-1)\\}$ in practice. ", "page_idx": 6}, {"type": "text", "text": "Given a prefixing chain and an exchangeable probability distribution $P$ over $\\mathcal{F}$ , the optimal rate for an unordered prefix $\\overline{{f_{[i]}}}$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\log\\frac{1}{P(\\overline{{f_{[i]}}})}=\\log\\frac{1}{P(\\overline{{g_{[i-1]}}})}+\\log\\frac{1}{P(g_{i-1}\\,|\\,g_{[i-1]})}-\\log\\frac{i}{|\\mathrm{Orb}_{\\mathrm{Aut}(f_{[i]})}(j)|},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $g_{[i]}=(j,i-1)\\cdot f_{[i]}$ for any $j\\in[i]$ . For proof see Appendix B.2. ", "page_idx": 6}, {"type": "text", "text": "4.2 Achieving the target rate ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The recursive structure in Equation (8) hints at how to construct a recursive codec for unordered prefixes optimal for $P(\\overline{{f_{[i]}}})$ . To obtain $g_{[i]}$ , we require a function $\\mathsf{s w a p}(f_{[i]},j,k):=(j,k)\\cdot f_{[i]}$ that swaps two positions of a prefix $f_{[i]}$ . To realize the slice rate term from Equation (8), we require an (autoregressive) codec for slices $f_{i}$ parameterized by $g_{[i]}$ that is optimal for $P(f_{i}\\mid g_{[i]})$ , denoted as Slice. For simplicity, we assume that there is only a single unique \u2018empty\u2019 prefix of length 0, denoted as $\\mathtt{f}\\mathtt{0}$ below, as is the case for Examples 4.1 and 4.2. We will realize the orbit discount by recovering the bits for the orbit of $j$ in $\\operatorname{Aut}(f_{[i]})$ . For that, we need a way to identify the orbits of any $f\\in\\mathcal F$ in a way that is invariant under permutations, formalized through the following definition: ", "page_idx": 6}, {"type": "text", "text": "Definition 4.3 (Orbit function). For a permutable class $\\mathcal{F}$ of order $n$ , let $\\leq$ be a total order on the orbits $O\\,=\\,\\mathrm{Orbs}_{\\mathrm{Aut}(f)}\\,:=\\,\\{\\mathrm{Orb}_{\\mathrm{Aut}(f)}(i)\\,\\mid\\,i\\,\\in\\,[n]\\}$ of all indices $i\\;\\in\\;[n]$ . This induces a unique function $I:O\\to[|O|]$ with $I(o)\\leq I(o^{\\prime})$ exactly if $o\\leq o^{\\prime}$ for all $o,o^{\\prime}\\in O$ . We then refer to the function orbits $(f):=\\left(I(\\operatorname{Orbs}_{\\operatorname{Aut}(f)}(j))\\right)_{j\\in[n]}$ for all $f\\,\\in\\,{\\mathcal{F}}$ as an orbit function of $\\mathcal{F}$ if orbits $(s\\cdot f)_{s\\cdot j}={\\mathrm{orbits}}(f)_{j}$ for all $s\\in S_{i}$ , $f\\in\\mathcal F$ and $j\\in[n]$ . ", "page_idx": 6}, {"type": "text", "text": "An orbit function for string prefixes ranks its elements according to some order over the alphabet, as visualized in Table 3. For graph prefixes $f_{[i]}$ , we can implement an orbit function by distinctly coloring the last $n-i$ vertices of the graph $f$ , deleting the edges between them, and passing it to the nauty and Traces library (McKay and Piperno, 2014), which provides an orbit function for graphs. ", "page_idx": 6}, {"type": "text", "text": "We require an orbit function orbitsi on prefixes ${\\mathcal F}_{[i]}$ of all lengths $i\\,\\in\\,[n]$ . This defines a corresponding probability distribution over orbit indices $[|\\mathrm{\\dot{O}r b s}_{\\mathrm{Aut}(f_{[i]})}|]$ with mass function $P_{o}$ s.t. for all $i\\in[n]$ and $j\\in[i]$ , $P_{o}(\\mathrm{orbits}_{i}(f_{[i]})_{j})):=|\\mathrm{Orb}_{\\mathrm{Aut}(f_{[i]})}(j)|/i$ . We construct a codec Orbit optimal for $P_{o}$ by using a categorical codec with masses proportional to the counts of each orbit index $\\mathrm{orbits}_{i}(f_{[i]})$ . ", "page_idx": 6}, {"type": "text", "text": "We list the autoregressive codec UnorderedPrefix, parameterized by the prefix length $i$ : ", "page_idx": 6}, {"type": "text", "text": "Effect on message length: ", "page_idx": 7}, {"type": "text", "text": "$-\\log{\\frac{i}{|\\mathrm{Orb}_{\\mathrm{Aut}(f_{[i]})}(j)|}}$ + logP (gi|g[i\u22121]) + logP (g[i\u22121]) ", "page_idx": 7}, {"type": "text", "text": "1 def encode(m, f):   \n2 if $i{=}6$ : return m   \n3 os $=$ orbits(f)   \n4 m, o = Orbit(os).decode(m)   \n5 $\\dot{\\ ]}\\ =$ find(o, os)   \n6 $_{\\mathrm{~g~}}=$ swap(f, j, i-1)   \n7 g1, s = pop(g)   \n8 m = Slice(g1).encode(m, s)   \n9 $\\textsf{m}=$ UnorderedPrefix(i-1).encode(m, g1)   \n10 return m   \n11   \n12 def decode(m):   \n13 if $i{=}6$ : return m, f0   \n14 m, $\\ g1\\ =$ UnorderedPrefix(i-1).decode(m)   \n15 m, s = Slice(g1).decode(m)   \n16 $_{\\mathrm{~g~}}=$ push(g1, s)   \n17 $\\mathsf{o s}=$ orbits(g)   \n18 $0~=~0{\\tt S}[\\dot{1}\\!-\\!1]$   \n19 m = Orbit(os).encode(m, o)   \n20 return m, g ", "page_idx": 7}, {"type": "text", "text": "Similarly to joint shuffle coding, we represent an unordered prefix $\\overline{{f_{[i]}}}$ by an arbitrary (ordered) prefix $f_{[i]}\\in\\overline{{f_{[i]}}}$ , denoted as $\\mathsf{f}$ in the listing. In the encoder, we decode an orbit index o from the message m according to $P_{o}$ , and let $j$ be an arbitrary index within the corresponding orbit. We express this with a function $\\mathsf{f i n d}(\\mathsf{o},\\mathsf{o s})$ which returns the first index of a given element in a given tuple. Equation (6) ensures that we can represent ordered objects $f\\in\\mathcal F$ as their prefixes $f_{[n]}=\\{f\\}$ of length $n$ . This allows us to recursively code unordered objects $\\bar{f}$ with UnorderedPrefix(n) optimal for $P(\\bar{f})$ , a method which we will refer to as autoregressive shuffle coding. ", "page_idx": 7}, {"type": "text", "text": "4.3 Achieving practical speeds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "A naive implementation of autoregressive shuffle coding as shown in the listing above is impractically slow on larger objects: for every prefix length $i\\in[n]$ , the prefix orbits and the corresponding orbit codec (as well as the slice codec), need to be recomputed. In the case of graphs, finding the exact orbits for even one such graph prefix is slow, just like finding the automorphism group of a graph for complete joint shuffle coding. This subsection presents three techniques critical to achieving practical speeds with autoregressive shuffle coding. ", "page_idx": 7}, {"type": "text", "text": "Adaptive entropy coding. Instead of independently computing a new Slice and Orbit codec for each iteration using orbits, as shown in the code listing for simplicity, we allow reuse between iterations by updating their state during push, pop and swap. This allows to implement \u2018adaptive entropy coding\u2019, used by Severo et al. (2023a) in the context of Orbit for multisets ${\\overline{{X^{n}}}}$ , which achieves fast updates of a mutable categorical codec based on an order statistic tree that is weighted in the sense that it allows each element to be present some $k\\in\\mathbb N$ times. We will reuse the same approach to efficiently implement autoregressive Slice codecs, where for multisets we use a custom weighted AVL tree (Adelson-Velskii, 1962). ", "page_idx": 7}, {"type": "text", "text": "Incomplete autoregressive shuffle coding. Instead of computing the orbit function on prefixes $f_{[i]}$ , we can apply it to some incompletely ordered version $(f_{[i]})_{\\sim}$ instead, as defined in Definition 3.1. Similarly to Equation (4), this reduces the realized discount from Equation (8), increasing the rate by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\log\\frac{|\\mathrm{Orb}_{\\mathrm{Aut}((f_{[i]})\\sim)}(j)|}{|\\mathrm{Orb}_{\\mathrm{Aut}(f_{[i]})}(j)|},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "bits in each recursive step. We refer to this variant as incomplete autoregressive shuffle coding. We can apply color refinement to graph prefixes by distinctly coloring the last $n-i$ vertices of the graph $f$ , deleting the edges between them, and then applying it to graphs as usual (discarding the last $n-i$ vertex hashes). Applying incomplete autoregressive shuffle coding with color refinement by using the graph prefixing chain from Example 4.2 greatly improves runtime complexity compared to the complete version. However, it still results in a runtime of at least $\\Omega(n e)$ that is impractical for larger graphs, since color refinement has to be run for every prefix $f_{[i]}\\,\\in\\,[n]$ times. Thus, it would be helpful to reduce the number of required color refinement runs to reduce runtime further, motivating the next technique. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Chunking. Instead of encoding order information after every slice, we can decode slices in a small fixed number $c$ of chunks, with a sequence $C$ of sizes adding up to $n$ . For graphs, we can apply color refinement to the graph prefix after decoding the next chunk, and iteratively code the new order information according to the resulting vector of vertex colors. We then repeat this process until the complete graph is decoded. In Appendix $\\mathrm{E}$ we formalize this generalized approach as (incomplete) autoregressive shuffle coding with a \u2018chunked\u2019 prefixing chain, where prefixes code more slices than usual to complete the respective chunk. For graphs, we then require only $c$ color refinement runs (instead of $n$ for the \u2018full\u2019 prefixing chain), resulting in a practical runtime of ", "page_idx": 8}, {"type": "equation", "text": "$$\nO(c\\cdot m+n\\log n)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Coarser chunks generally lead to better runtimes but also higher initial bits overhead. The extreme case of using only a single chunk of size $n$ leads to a method sharing many properties with joint shuffle coding. Chunking allows using a Fenwick tree (Ryabko, 1989) for adaptive entropy coding since all possible values for vertex hashes in a chunk are known in advance, leading to better memory layout and faster runtimes. For a detailed discussion, see Appendix E. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Complete joint shuffle coding. Unlike joint shuffle coding introduced in Kunze et al. (2024), autoregressive shuffle coding allows one-shot compression of unordered objects. This is achieved by interleaving encoding and decoding steps while coding a single object, an approach first proposed in the context of \u2018Bit-Swap\u2019 (Kingma et al., 2019). Our method requires an autoregressive model for slices instead of a joint model. Unlike complete shuffle coding, incomplete shuffle coding does not require the automorphism group of an object and therefore does not depend on libraries such as nauty and Traces (McKay and Piperno, 2014). ", "page_idx": 8}, {"type": "text", "text": "Graphs. Choi and Szpankowski (2012) present a compression method called \u2018structural ZIP\u2019 (SZIP), which asymptotically achieves the rate ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\log{\\frac{1}{P_{\\mathrm{ER}}(g)}}-n\\log n+O(n),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $P_{\\mathrm{ER}}$ is the Erd\u02ddos-R\u00e9nyi $G(n,p)$ model. Compared to our method, SZIP is less flexible in the sense that it only applies to simple graphs (without vertex or edge attributes), and it is not an entropy coding method, thus the model $P_{\\mathrm{ER}}$ cannot be changed easily. The \u2018Partition and Code\u2019 (PnC; Bouritsas et al., 2021) method uses neural networks to compress unordered graphs. Unlike PnC, we achieve state-of-the-art compression rates when using simple models with minimal parameters, which amortize better when compressed along with a single graph. ", "page_idx": 8}, {"type": "text", "text": "Multisets. Our method generalizes Severo et al. (2023a) from multisets to arbitrary unordered objects, including graphs. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate our methods experimentally, we applied them to multisets and graphs. We report multiset compression results in Appendix G where we compare our implementation of full autoregressive shuffle coding to Severo et al. (2023a) on multisets of varying lengths, showing that while (one-shot) rates are matched, our implementation is well over two orders of magnitude faster and scaling to large multisets. Further multiset experiments show that joint shuffle coding is even faster while achieving the same net rates. ", "page_idx": 8}, {"type": "text", "text": "For graphs we applied incomplete joint and autoregressive shuffle coding with color refinement according to Definition C.1 to various graph datasets. We use the simple Erdo\u02dds-R\u00e9nyi (ER) $G(n,p)$ model for $P$ , which is straightforward to convert into an autoregressive model $P(f_{i}\\mid f_{[i]})$ . Kunze et al. (2024) observe that the P\u00f3lya urn (PU) preferential attachment model proposed by Severo et al. (2023b) drastically improves the compression rate on SZIP graphs from Choi and Szpankowski (2012). Motivated by this, we also use a variational approximation to obtain a tractable autoregressive model that we refer to as autoregressive P\u00f3lya urn (AP), described in Appendix F. We favored AP for larger graphs over PU because AP scaled better in terms of runtime, with similar rates. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Joint. We report results for incomplete joint shuffle coding on graphs in Appendix H. We first apply it to SZIP graphs with varying $k$ and report how this affects compression rate in Figure 4, to find a practical number of convolutions $k$ . The results show that $k=3$ is sufficient to get close to the optimal rate, which we therefore use for all other graph experiments in this paper. In Tables 8 and 9, we compare complete and incomplete joint shuffle coding on the TU (Morris et al., 2020) and SZIP datasets.4 We observe that incomplete shuffle coding leads to dramatic speedups of up to a factor of one million on some of these graphs, with a minimal increase in compression rate across all datasets. We also evaluate incomplete joint shuffle coding using the AP codec on the large graphs used by Severo et al. (2023b) in Table 10, which we refer to as the \u2018REC\u2019 graph dataset, where complete joint shuffle coding is too slow to finish on any graphs. In the same table, we report compression speeds when run on a single thread and 8 threads, both on the SZIP and REC graphs, confirming an advantage with multiple threads. ", "page_idx": 9}, {"type": "text", "text": "Autoregressive. For incomplete autoregressive graph shuffle coding, we apply chunking to the prefixing chain from Example 4.2. We first evaluate the effect of chunk size in Appendix I on SZIP graphs, and find that $c=16$ uniformly sized chunks lead to a good balance between runtime and rate, which we will use in all following experiments. ", "page_idx": 9}, {"type": "text", "text": "We apply incomplete autoregressive shuffle coding to the SZIP graphs based on ER and AP, and compare it to the SZIP method in Table 11. The results show that the advantage of a preferential attachment model over Erd\u02ddos-R\u00e9nyi carries over from the joint PU codec to the autoregressive approximation AP, demonstrating the value of shuffle coding being an entropy coder where the model can be changed easily. This results in significantly better rates compared to SZIP, at practical compression speeds that appear to be largely independent of graph size, whereas speeds for the SZIP codec seem to drop with graph size (SZIP graph sizes are shown in Table 10). ", "page_idx": 9}, {"type": "text", "text": "We evaluate autoregressive shuffle coding on REC graphs in Table 12, and compare it to the ordered rate without shuffle coding, demonstrating its practicality and rate advantage on large graphs. These results also confirm that the discount unrealized due to initial bits, visible as the difference between net rate and rate, is relatively small, highlighting the effectiveness of autoregressive shuffle coding for one-shot compression. ", "page_idx": 9}, {"type": "text", "text": "Finally, we evaluate autoregressive shuffle coding with the AP model on very large plain random graphs drawn from $G(n,m)$ Erdo\u02dds-R\u00e9nyi models and report rates and speeds in Table 13. The largest graph has one billion edges and an uncompressed size of 3.4 gigabytes, with our compression method saving 0.8 gigabytes. The speed results confirm the near-linear runtime predicted by Equation (10) across many orders of magnitude. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed a general entropy coding method for unordered objects, achieving state-of-the-art compression rates at practical speeds for multisets and graphs up to gigabyte-scale. It is a combination of two new shuffle coding variants, as summarized in Table 5. ", "page_idx": 9}, {"type": "text", "text": "We believe that there is significant room for optimization. On REC graphs, we observed that roughly half of the runtime is spent on the ordered autoregressive model which has not yet been optimized. Chunking invites parallelization, for example with vectorized ANS (Giesen, 2014). Nonuniform chunk sizes $C$ are likely also beneficial for graphs, as already demonstrated for multisets in Appendix G. Unlike competing methods like SZIP and PnC, shuffle coding is an entropy method that can be easily adapted to specific domain models, and beneftis from advances in generative graph modeling, such as recent work on neural (autoregressive) graph models (Kong et al., 2023; Zhu et al., 2022). We leave these promising research directions for future work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Adelson-Velskii, Georgii M (1962). An algorithm for the organization of information. In Soviet Math. 3, pp. 1259\u20131263.   \nBabai, L\u00e1szl\u00f3 (2016). Graph isomorphism in quasipolynomial time. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pp. 684\u2013697.   \nBabai, L\u00e1szl\u00f3 (2019). Canonical form for graphs in quasipolynomial time: preliminary report. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing. STOC 2019. Phoenix, AZ, USA: Association for Computing Machinery, pp. 1237\u20131246. ISBN: 9781450367059. DOI: 10.1145/3313276.3316356. URL: https://doi.org/10.1145/3313276.3316356.   \nBabai, L\u00e1szl\u00f3, Erdos, Paul, and Selkow, Stanley M (1980). Random graph isomorphism. In SIaM Journal on computing 9.3, pp. 628\u2013635.   \nBabai, L\u00e1szl\u00f3 and Ku\u02c7cera, Lud\u02c7ek (1979). Canonical labelling of graphs in linear average time. In 20th Annual Symposium on Foundations of Computer Science (SFCS 1979). IEEE, pp. 39\u201346.   \nBouritsas, Giorgos, Loukas, Andreas, Karalias, Nikolaos, and Bronstein, Michael (2021). Partition and Code: Learning How to Compress Graphs. In Advances in Neural Information Processing Systems. Vol. 34, pp. 18603\u201318619.   \nChoi, Yongwook and Szpankowski, Wojciech (2012). Compression of Graphical Structures: Fundamental Limits, Algorithms, and Experiments. In IEEE Transactions on Information Theory 58.2, pp. 620\u2013638.   \nDuda, Jarek (2009). Asymmetric Numeral Systems. arXiv: 0902.0271 [cs, math].   \nGiesen, Fabian (2014). Interleaved entropy coders. In arXiv preprint arXiv:1402.3392.   \nHelfgott, Harald Andr\u00e9s, Bajpai, Jitendra, and Dona, Daniele (2017). Graph isomorphisms in quasipolynomial time. In arXiv preprint arXiv:1710.04574.   \nHuang, Ningyuan Teresa and Villar, Soledad (2021). A Short Tutorial on The Weisfeiler-Lehman Test And Its Variants. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).   \nKingma, Friso, Abbeel, Pieter, and Ho, Jonathan (2019). Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables. In Proceedings of the 36th International Conference on Machine Learning, pp. 3408\u20133417.   \nKong, Lingkai, Cui, Jiaming, Sun, Haotian, Zhuang, Yuchen, Prakash, B Aditya, and Zhang, Chao (2023). Autoregressive diffusion model for graph generation. In International conference on machine learning. PMLR, pp. 17391\u201317408.   \nKunze, Julius, Severo, Daniel, Zani, Giulio, van de Meent, Jan-Willem, and Townsend, James (2024). Entropy Coding of Unordered Data Structures. In International Conference on Learning Representations (ICLR).   \nLeman, Andrey Aleksandrovich and Weisfeiler, Boris (1968). A reduction of a graph to a canonical form and an algebra arising during this reduction. In Nauchno-Technicheskaya Informatsiya 2.9, pp. 12\u201316.   \nMcKay, Brendan D. and Piperno, Adolfo (2014). Practical Graph Isomorphism, II. In Journal of Symbolic Computation 60, pp. 94\u2013112.   \nMorris, Christopher, Kriege, Nils M., Bause, Franka, Kersting, Kristian, Mutzel, Petra, and Neumann, Marion (2020). TUDataset: A Collection of Benchmark Datasets for Learning with Graphs. In ICML 2020 Workshop on Graph Representation Learning and beyond $(G R L+\\;2O2O_{\\prime}$ ). arXiv: 2007.08663.   \nRyabko, Boris Yakovlevich (1989). A fast on-line code. In Doklady Akademii Nauk. Vol. 306. 3. Russian Academy of Sciences, pp. 548\u2013552.   \nSevero, Daniel, Townsend, James, Khisti, Ashish, Makhzani, Alireza, and Ullrich, Karen (2023a). Compressing Multisets with Large Alphabets. In IEEE Journal on Selected Areas in Information Theory.   \nSevero, Daniel, Townsend, James, Khisti, Ashish J., and Makhzani, Alireza (2023b). Random Edge Coding: One-Shot Bits-Back Coding of Large Labeled Graphs. In Proceedings of the 40th International Conference on Machine Learning.   \nTownsend, James (2020). A Tutorial on the Range Variant of Asymmetric Numeral Systems. arXiv: 2001.09186 [cs, math, stat].   \nTownsend, James, Bird, Thomas, and Barber, David (2019). Practical Lossless Compression with Latent Variables Using Bits Back Coding. In International Conference on Learning Representations (ICLR).   \nZhu, Yanqiao, Du, Yuanqi, Wang, Yinkai, Xu, Yichen, Zhang, Jieyu, Liu, Qiang, and Wu, Shu (2022). A Survey on Deep Graph Generation: Methods and Applications. In Proceedings of the First Learning on Graphs Conference, 47:1\u201347:21. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Group actions, orbits and stabilizers ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This appendix gives the definitions of group actions, orbits and stabilizers as well as a statement and proof of the orbit-stabilizer theorem, which we make use of in Section 4. We use the shorthand $H\\leq G$ to mean that $H$ is a subgroup of $G$ , and for $g\\in G$ , we use the usual notation, $g H:=\\{g h\\mid h\\in H\\}$ and $H g:=\\{h g\\mid h\\in\\bar{H}\\}$ for left and right cosets, respectively. For any two subgroups $G,H\\le S$ , we denote the group of their intersection as $G\\cap H$ . ", "page_idx": 12}, {"type": "text", "text": "Definition A.1 (Group action). For a set $X$ and a group $G$ , a group action, or simply action, is a binary operator ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\cdot_{G}:G\\times X\\to X\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which respects the structure of $G$ in the following sense: ", "page_idx": 12}, {"type": "text", "text": "1. The identity element $e\\in G$ is neutral, that is $e\\cdot_{G}x=x$ . ", "page_idx": 12}, {"type": "text", "text": "2. The operator \u00b7 $G$ respects composition. That is, for $g,h\\in G$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\ng\\cdot_{G}(h\\cdot_{G}x)=(g h)\\cdot_{G}x.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We will often drop the subscript $G$ and use infix $\\cdot$ alone where the action is clear from the context. ", "page_idx": 12}, {"type": "text", "text": "Definition A.2 (Orbit). An action of a group $G$ on a set $X$ induces an equivalence relation $\\sim\\!G$ on $X$ , defined by ", "page_idx": 12}, {"type": "equation", "text": "$$\nx\\sim_{G}y\\quad{\\mathrm{if~and~only~if~there~exists}}\\quad g\\in G\\quad{\\mathrm{such~that}}\\quad y=g\\cdot x.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We refer to the equivalence classes induced by $\\sim\\!G$ as orbits, and use $\\operatorname{Orb}_{G}(x)$ to denote the orbit containing an element $x\\ \\in\\ X$ . We use $X/G$ to denote the set of orbits, so for each $x\\ \\in\\ X$ , ${\\mathrm{Orb}}_{G}(x)\\,\\in X/G$ . ", "page_idx": 12}, {"type": "text", "text": "Definition A.3 (Stabilizer subgroup). For an action of a group $G$ on a set $X$ , for each $x\\in X$ , the stabilizer ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname{Stab}_{G}(x):=\\{g\\in G\\mid g\\cdot x=x\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "forms a subgroup of $G$ . ", "page_idx": 12}, {"type": "text", "text": "Here, we give a statement and brief proof of the well-known orbit-stabilizer theorem. ", "page_idx": 12}, {"type": "text", "text": "Theorem A.4 (Orbit-stabilizer theorem). For an action of a finite group $G$ on a set $X$ , for each $x\\in X$ , the function $\\theta_{x}\\colon G\\to X$ defined by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{x}(g):=g\\cdot x\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "induces a bijection from the left cosets of $\\operatorname{Stab}_{G}(x)\\;t o\\;\\mathrm{Orb}_{G}(x)$ . This implies that the orbit $\\operatorname{Orb}_{G}(x)$ is finite and ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\mathrm{Orb}_{G}(x)|=\\frac{|G|}{|\\mathrm{Stab}_{G}(x)|}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. We show that $\\theta_{f}$ induces a well defined function on the left-cosets of $\\operatorname{Stab}_{G}(x)$ , which we call $\\tilde{{\\boldsymbol{\\theta}}}_{f}$ . Specifically, we define ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{f}(g\\operatorname{Stab}_{G}(x)):=g\\cdot x,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and show that $\\tilde{{\\boldsymbol{\\theta}}}_{f}$ is injective and surjective. ", "page_idx": 12}, {"type": "text", "text": "To see that $\\tilde{{\\boldsymbol{\\theta}}}_{f}$ is well defined and injective, note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h\\in g\\,\\mathrm{Stab}_{G}(x)\\iff g^{-1}h\\in\\mathrm{Stab}_{G}(x)}\\\\ {\\iff g^{-1}h\\cdot x=x}\\\\ {\\iff g\\cdot x=h\\cdot x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "using the definition of $\\mathrm{Stab}_{G}$ . ", "page_idx": 12}, {"type": "text", "text": "For surjectivity, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y\\in\\mathrm{Orb}_{G}(x)\\implies\\exists g\\in G\\ \\mathrm{s.t.}\\ y=g\\cdot x}\\\\ {\\implies y=\\tilde{\\theta}_{f}(g\\,\\mathrm{Stab}_{G}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "using the definition of $\\mathrm{Orb}_{G}$ . ", "page_idx": 12}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Background (Section 2) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof for Definition 2.2. We first prove $s\\cdot f_{\\sim}\\in\\mathcal{F}_{\\sim}$ for all $s\\ \\in\\ S_{n}$ and $f_{\\sim}\\in\\mathcal{F}_{\\sim}$ . Let $s\\ \\in\\ S_{n}$ and $f,g\\in{\\mathcal{F}}$ . Now $s\\cdot f_{\\sim}\\sim s\\cdot g_{\\sim}$ exactly if $f\\sim g$ because $\\sim$ is exchangeable, and the inverse permutation $s^{-1}\\in S_{n}$ of $s$ exists. Since $\\mathcal{F}_{\\sim}$ is a partition of $\\mathcal{F}$ , this implies $\\left\\{s\\cdot f\\mid f\\in f_{\\sim}\\right\\}\\in\\mathcal{F}_{\\sim}$ and therefore $s\\cdot f_{\\sim}\\in\\mathcal{F}_{\\sim}$ . It is left to show that the operator \u00b7 $:S_{n}\\times\\mathcal{F}_{\\sim}\\to\\mathcal{F}_{\\sim}$ is a left group action. This follows from $\\cdot:S_{n}\\times\\mathcal{F}\\rightarrow\\mathcal{F}$ associated with the permutable class $\\mathcal{F}$ being a left group action. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B.2 Autoregressive shuffle coding (Section 4) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Definition B.1 (Exchangeable function). We say a function $a$ on a permutable class $\\mathcal{F}$ is exchangeable if $a(f)=a(g)$ for all $f,g\\in{\\mathcal{F}}$ with $f\\simeq g$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma B.2. If $f\\,\\rightarrow\\,P(f)$ on $\\mathcal{F}$ is exchangeable then the functions $f_{[i]}\\,\\to\\,P(f_{[i]})$ on $\\mathcal{F}_{[i]}$ for $i\\in[n+1]$ are exchangeable. ", "page_idx": 13}, {"type": "text", "text": "Proof. If $f\\rightarrow P(f)$ is exchangeable, then for any $i\\in[n+1]$ and $s\\in S_{i}$ , Equation (2) implies $\\begin{array}{r}{P(s\\cdot f_{[i]})\\stackrel{}{=}\\sum_{f\\in f_{[i]}}P(s\\cdot f)\\stackrel{}{=}\\sum_{f\\in f_{[i]}}P(f)\\stackrel{}{=}P(f_{[i]})}\\end{array}$ , and therefore $f\\,\\rightarrow\\,P(f_{[i]})$ on $\\mathcal{F}_{[i]}$ is exchangeable. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Lemma B.3. If $f\\rightarrow P(f)$ on $\\mathcal{F}$ is exchangeable, then for $i\\in[n],g_{i}\\in\\mathcal{F}_{[i]}.$ , the function $f_{[i]}\\rightarrow$ $P(g_{i}\\mid f_{[i]})$ on ${\\mathcal F}_{[i]}$ is exchangeable, and $P(g_{i}\\,|\\,\\overline{{{f_{[i]}}}})=P(g_{i}\\,|\\,f_{[i]})$ for $f_{[i]}\\in\\overline{{f_{[i]}}}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Assume $P(f)$ is exchangeable. We will use induction over $i$ starting at $n$ down to 0, for the statement that ${\\dot{f}}_{[i]}\\,\\to\\,P((g_{j})_{j\\in[n]\\backslash[i]}\\mid f_{[i]})$ is exchangeable for any given $g_{j}$ , and implies the lemma. $P((g_{j})_{j\\in[n]\\setminus[n]}\\mid f_{[n]})=\\mathring{P}(()\\mid\\!f_{[n]})$ is exchangeable, covering the base case. Assume for $i\\in[n]$ that $f_{[i+1]}\\to P((g_{j})_{j\\in[n]\\setminus[i+1]}\\mid f_{[i+1]})$ is exchangeable. For all $i\\in[n]$ and $f\\in\\mathcal F$ , we have $P(f)=P(f_{[i]})P(f_{i}\\,|\\,f_{[i]})P((f_{j})_{j\\in[n]\\backslash{[i+1]}}\\mid f_{[i+1]}).$ Therefore, the function mapping from $f:{\\mathcal{F}}$ to $P(f_{i}\\,|\\,f_{[i]})\\,=\\,P(f_{[i]})P((f_{j})_{j\\in[n]\\backslash[i+1]}\\mid f_{[i+1]})/P(f)$ is also exchangeable due to Lemma B.2 and the inductive assumption. This further implies that the function mapping from $f_{[i]}\\;:\\;{\\mathcal{F}}_{[i]}$ to $P((g_{j})_{j\\in[n]\\setminus[i]}\\mid f_{[i]})\\;=\\;P((g_{j})_{j\\in[n]\\setminus[i+1]}\\mid f_{[i+1]})P(g_{i}\\mid f_{[i]})$ is exchangeable, completing the induction. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Lemma B.4. For a prefixing chain given by $\\mathcal{F}_{[i]}$ , $\\mathrm{Stab}_{\\mathrm{Aut}(f_{[i+1]})}(i)=\\mathrm{Aut}(f_{[i]})$ for all $i\\in[n-$ 1], $f\\in\\mathcal F$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We have $\\operatorname{Stab}_{\\operatorname{Aut}(f_{[i+1]})}(i)\\,=\\,\\{s\\,\\in\\,S_{i+1}\\ |\\ s\\cdot i=i\\ \\wedge\\,s\\cdot f\\,\\sim_{[i+1]}\\ f\\}$ , which is $=\\{s\\in$ $S_{i}\\mid s\\cdot f\\sim_{[i+1]}f\\}$ due to Equation $(1)\\;{\\mathrm{and}}=\\{s\\in S_{i}\\mid s\\cdot f\\sim_{[i]}f\\,\\land\\,\\bigcup{\\overline{{(s\\cdot f)_{[i+1]}^{\\prime}}}}=\\bigcup{\\overline{{f_{[i+1]}^{\\prime}}}}\\}$ through Equation (7). Since $(s\\cdot f)_{[i+1]}$ is isomorphic to $f$ in $\\mathcal{F}_{[i+1]}^{\\prime}$ , this is in $\\mathrm{turn}=\\{s\\in S_{i}\\ |$ $s\\cdot f\\sim_{[i]}f\\}=\\operatorname{Aut}(f_{[i]})$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proof of Equation (8). The optimal rate is given by Equation (3): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log\\frac{1}{P(\\overline{{f_{[i]}}})}=\\log\\frac{1}{P(f_{[i]})}-\\log\\frac{i!}{|\\mathrm{Aut}(f_{[i]})|}\\,.\\\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The optimal rate for $\\overline{{g_{[i-1]}}}$ is given by Equation (24): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log\\frac{1}{P(\\overline{{g_{[i-1]}}})}=\\log\\frac{1}{P(g_{[i-1]})}-\\log\\frac{(i-1)!}{|\\mathrm{Aut}(g_{[i-1]})|}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It is informative to rewrite Equation (24) in terms of Equation (25): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\frac{1}{P(\\overline{{f_{[i]}}})}=\\log\\frac{1}{P(\\overline{{g_{[i-1]}}})}+\\log\\frac{P(g_{[i-1]})}{P(f_{[i]})}}&{}\\\\ &{\\,\\,\\,-\\log\\frac{i\\cdot|\\mathrm{Aut}(g_{[i-1]})|}{|\\mathrm{Aut}(f_{[i]})|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition 4.2 and Lemma B.3 imply $\\begin{array}{r}{P(f_{[i]})=P((s\\cdot f)_{i})=P(g_{[i]})=P(g_{i-1}\\,|\\,\\overline{{g_{[i-1]}}})\\cdot P(g_{[i-1]}).}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Additionally, we have $|\\operatorname{Aut}(f_{[i]})|=|\\operatorname{Aut}\\left(s\\cdot f_{[i]}\\right)|=|\\operatorname{Aut}(g_{[i]})|$ since permuting $f$ results in a conjugated automorphism group which is isomorphic to $\\operatorname{Aut}(f_{[i]})$ . Lemma B.4 together with the orbitstabilizer theorem A.4 implies $|\\mathrm{Aut}(g_{[i]})|=|\\mathrm{Orb}_{\\mathrm{Aut}(g_{[i]})}(i\\stackrel{\\cdot\\cdot}{-}1)|\\cdot|\\mathrm{Aut}(g_{[i-1]})|$ . Finally, we have $|\\mathrm{Orb}_{\\mathrm{Aut}(g_{[i]})}(i-1)|=|\\mathrm{Orb}_{\\mathrm{Aut}(f_{[i]})}(j)|$ . We can now rewrite Equation (26) into Equation (8). ", "page_idx": 14}, {"type": "text", "text": "Lemma B.5. UnorderedPrefix forms a valid codec, with encode and decode being inverses of each other. ", "page_idx": 14}, {"type": "text", "text": "Proof. This is mostly straightforward, but notably, the swap operation of the encoder is not reversed. Here, we exploited that output of the function $h_{[i-1]}\\to\\mathrm{orbits}_{i}(\\mathrm{push}(h_{[i-1]},g_{i-1}))_{i-1}$ on $\\mathcal{F}_{[i+1]}$ does not change when permuting the input, and therefore the decoder recovers the correct orbit index $0=\\mathrm{orbits}_{i}(\\bar{g_{[i]}})_{i-1}=\\mathrm{orbits}_{i}(\\bar{f}_{[i]})_{j}$ for any decoded $h_{[i-1]}\\in\\overline{{g_{[i-1]}}}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.3 Incomplete shuffle coding (Section 3) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof for Equation (4). The condition in Definition 3.1 is equivalent to $f\\simeq g$ for all $f,g\\in{\\mathcal{F}}$ with $f\\sim g$ . We show $\\bar{f}=\\mathsf{U}\\,\\overline{{f_{\\sim}}}$ by proving that for $g_{\\sim}\\in\\mathcal{F}/\\sim$ , we have $g_{\\sim}\\in\\overline{{f_{\\sim}}}$ exactly if $g_{\\sim}\\subseteq{\\bar{f}}$ . $g_{\\sim}\\in\\overline{{f_{\\sim}}}$ means $\\exists s\\in S_{n}:g_{\\sim}=s\\cdot f_{\\sim}$ , which is equivalent to $\\exists s\\in S_{n}:g\\sim s\\cdot f$ . On the other hand, $g_{\\sim}\\subseteq{\\bar{f}}$ holds exactly if $g_{\\sim}\\,\\subseteq\\,\\{t\\cdot f_{\\sim}\\mid t\\in S_{n}\\}$ , meaning that $\\exists t\\in S_{n}:g=t\\cdot f$ . It is left to show that $\\exists s\\in S_{n}:g\\sim s\\cdot f$ exactly if $\\exists t\\in S_{n}:g=t\\cdot f$ . The implication $\\Leftarrow$ is proved with $s:=t$ . To show the implication $\\Rightarrow{}$ , assume $\\exists s\\in S_{n}:g\\sim s\\cdot f.$ . Definition 3.1 now implies $\\exists s\\in S_{n}:g\\simeq s\\cdot f$ , and therefore $\\exists s,u\\in S_{n}:g=u\\cdot s\\cdot f$ . $t:=u\\cdot s$ implies $\\exists t\\in S_{n}:g=t\\cdot f$ , completing the proof. ", "page_idx": 14}, {"type": "text", "text": "This leads to $P(\\bar{f})=P(\\overline{{f_{\\sim}}})$ for any given distribution $P(f)$ over $\\mathcal{F}$ , and Equation (4) follows from applying Equation (3) on both sides. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof for Equation (5). To achieve this runtime, we implicitly assumed that our ordered graph model is fast enough. We can hash a multiset represented as a string in linear time by accumulating hashes using a bitwise \u2018xor\u2019 operation, which is associative and commutative, making the overall hash permutation-invariant. This results in an overall runtime of $\\begin{array}{r}{O(\\sum_{i\\in[n]}|n_{i}(g)|)=O(m)}\\end{array}$ . The remaining computation has the runtime $O(n\\log n)$ of joint shuffle coding on multisets, leading to the given overall runtime complexity. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "C Color refinement details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We here state the formal definition of color refinement, as described in Section 3, that we use for the paper: ", "page_idx": 15}, {"type": "text", "text": "Definition C.1. For an (ordered) simple graph $g\\in\\mathcal{G}_{n}$ , let $n_{i}(g)$ denote the set of indices of the neighbors of the vertex at index $i$ . Let the coloring of a graph $g\\in{\\mathcal{G}}_{n}$ with $k\\in\\mathbb{N}$ iterations be the tuple $C_{k}(g)$ with $C_{0}(g)=(|n_{i}(g)|)_{i\\in[n]}$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{k+1}(g)=(h(\\{\\!\\{C_{k}(g)_{j}~|~j\\in n_{i}(g)\\}\\!\\}))_{i\\in[n]},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for a \u2018hashing\u2019 function $h:\\mathcal{M}\\rightarrow\\mathbb{N}$ on multisets $\\mathcal{M}=\\cup_{i\\in[n+1]}\\overline{{\\mathbb{N}^{i}}}$ of up to $n$ natural numbers $\\mathbb{N}$ . We then obtain a congruence corresponding to $C_{k}(f)={\\dot{C}}_{k}(g)$ and ${\\mathcal{G}}_{n}/{\\sim}{\\mathit{C}}_{k}$ forms a class of incompletely ordered objects for $k\\in\\mathbb{N}$ . ", "page_idx": 15}, {"type": "text", "text": "Color refinement is often run to convergence of vertex partitions (Huang and Villar, 2021). This happens after at most $[n]$ iterations and is therefore equivalent to $C_{n}$ . It can be naturally extended to graphs with vertex and edge attributes, by hashing each vertex attribute together with the multiset of neighboring edge attributes in the initial $C_{0}$ , and then convolving multisets of pairs of neighboring edges attributes and hashes in subsequent iterations. The corresponding Weisfeiler Leman graph isomorphism test is based on the fact that $\\overline{{C_{k}(f)}}\\neq\\overline{{C_{k}(g)}}$ implies $f\\not\\simeq g$ and successfully distinguishes almost all pairs of non-isomorphic graphs (Babai and Kuc\u02c7era, 1979). ", "page_idx": 15}, {"type": "text", "text": "Similarly, color refinement with $k\\ \\ >\\ \\ 0$ convolutions finds the exact automorphism group $\\operatorname{Aut}(g_{\\sim_{C_{k}}})=\\operatorname{Aut}(g)$ for almost all simple graphs for large enough $n$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Babai et al. (1980) show that with probability of at least $\\textstyle1-{\\frac{1}{n^{7}}}$ , there is a set $U$ of vertices whose degrees are distinct in $U$ , and no other vertices have the same set of neighbors within $U$ . Since color refinement starts with the vertex degrees, a single convolution will assign pairwise distinct hashes for almost all graphs. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "We assumed here that no hash collisions occur $(h(a)=h(b)$ with $a\\neq b$ ), for simplicity. In practice, we use an off-the-shelf 64-bit hash function $h$ , where hash collisions are extremely improbable. Even if such a collision occurs, it results only in a marginal rate increase. ", "page_idx": 15}, {"type": "text", "text": "D Conceptual visualizations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We visualize key concept for autoregressive shuffle coding for the example of graphs in Table 4, and summarize the variants of shuffle coding proposed in Table 5. ", "page_idx": 16}, {"type": "text", "text": "Table 4: Examples of key concepts from Section 4 for the permutable class $\\mathcal{F}$ of simple graphs with 5 vertices and the prefixing chain given by Example 4.2. Dotted edges indicate deleted information. ", "page_idx": 16}, {"type": "text", "text": "Concept Example ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{0\\displaystyle{\\sum_{i\\mathop{\\scriptstyle\\sqrt{3}}}}^{\\prime}}{2}=\\displaystyle{\\frac{\\therefore}{4\\displaystyle{\\sum_{i\\mathop{\\scriptstyle\\sqrt{3}}}^{\\prime}}}}\\cdot\\in\\overline{{\\mathscr{F}^{\\prime}}}}\\\\ &{\\frac{0\\displaystyle{\\sum_{i\\mathop{\\scriptstyle\\sqrt{3}}}^{\\prime}}(\\overline{{\\mathscr{B}}})^{(3)}}{\\displaystyle{\\sum_{i\\mathop{\\scriptstyle\\sqrt{3}}}^{\\prime}}}=\\displaystyle{\\frac{\\vdots}{4\\displaystyle{\\sum_{3}^{\\prime}}\\mathscr{B}_{2}}^{\\star}}=\\left\\{\\begin{array}{l l}{\\displaystyle{0\\sum_{i\\mathop{\\scriptstyle\\sqrt{3}}}^{0}},\\displaystyle{\\sum_{3}^{0}\\sum_{2}^{1}\\sum_{3}^{1}}}\\\\ {\\displaystyle{0\\sum_{3}^{0}\\sum_{2}^{1}\\mathscr{B}_{3}^{\\prime}}}\\end{array}\\right\\}\\in\\overline{{\\mathscr{F}^{(3)}}}}\\\\ &{\\left(\\displaystyle{\\frac{\\theta}{4\\displaystyle{\\sum_{3}^{0}}2}\\sum_{2}^{1}}\\right)_{[3]}=\\displaystyle{\\sum_{4\\atop5\\mathop{\\scriptstyle\\sqrt{3}}}^{0}}\\sum_{3}^{1}=\\left\\{\\begin{array}{l l}{\\displaystyle{0\\sum_{i\\mathop{\\scriptstyle\\sqrt{3}}}^{0}},\\displaystyle{\\sum_{2}^{0}\\sum_{3}^{1}\\mathscr{B}_{3}^{\\prime}}}\\\\ {\\displaystyle{0\\sum_{3}^{0}\\mathscr{B}_{3}^{\\prime}}}\\end{array}\\right\\}\\in\\mathscr{F}_{[2]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{pop}\\left(\\stackrel{0\\,\\ldots\\,1}{4\\,\\ldots\\,3}2\\right)=\\left(\\stackrel{0\\,\\ldots\\,1}{4\\,\\ldots\\,{\\overset{1}{3}}\\cdots2},\\stackrel{\\cdot\\,\\ldots\\,1}{4\\,\\ldots\\,{\\overset{\\cdot\\,}{3}}2}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Push ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{push}\\left(\\left(\\stackrel{0\\setminus1}{4\\cdots\\cdots2},\\stackrel{\\cdot\\,\\setminus}{4\\cdots\\!\\!}\\right)\\right)=\\stackrel{0\\,\\setminus\\,1}{4\\cdots\\!\\!}2\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Orb}_{\\mathrm{Aut}\\left(4\\frac{\\Gamma_{1}}{\\Gamma_{3}}\\right)}\\left(1\\right)=\\left\\{1,3\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Orbits ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Orbs}_{\\mathrm{Aut}\\left(\\underset{3}{\\overset{0}{\\sum}}\\!\\!\\!\\binom{1}{2}\\right)}=\\{\\{0\\},\\{1,3\\},\\{2,4\\}\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/9c8d17f13721d616059b67eea62d9b78e13f373a746aaa60614b55085ff2db7a.jpg", "table_caption": ["Table 5: Comparison between variants of shuffle coding. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Chunking ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/eded58545003dd271eac23d0b3e843fd221ce739c7179c08e0ca925fa426ad6e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 6: Prefixes for the string $f=$ abacb according to chunked prefixing chains of Example 4.1, for various chunk size sequences $C$ . When used with autoregressive shuffle coding, the prefix $f_{[i]}$ resembles the information decoded after $i$ iterations during decoding. ", "page_idx": 17}, {"type": "text", "text": "Autoregressive shuffle coding with chunking, as described in Section 4.3 refers to autoregressive shuffle coding using the following \u2018chunked\u2019 prefixing chain: ", "page_idx": 17}, {"type": "text", "text": "Definition E.1 (Chunked prefixing chain). Let $({\\mathcal{F}}_{[i]})_{i\\in[n]}$ be a prefixing chain on $\\mathcal{F}$ with ${\\mathcal F}_{[i]}:=$ $\\mathcal{F}^{(n-i)}/\\sim_{i}$ and let $C=(c_{j})_{j\\in[c]}$ be a sequence of $c$ chunk sizes with $\\textstyle\\sum_{j\\in[c]}c_{j}=n$ . In this context, we refer to $\\begin{array}{r}{p_{j}:=\\sum_{k\\in[j+1]}c_{k}}\\end{array}$ as the prefix size for chunk $j\\in[c]$ , and $C(\\bar{i})\\stackrel{*}{:=}\\operatorname*{min}\\{j\\in[c]|i\\in[p_{j}]\\}$ as chunk for index $i\\in[n]$ . Another prefixing chain is now formed on $\\mathcal{F}$ by $(\\mathcal{F}^{(n-i)}/{\\sim}_{p_{C(i)}})_{i\\in[n]}$ , which we refer to as the chunked prefixing chain of $({\\mathcal{F}}_{[i]})_{i\\in[n]}$ with chunk sizes $C$ . ", "page_idx": 17}, {"type": "text", "text": "Then, the slice at the first position of a chunk codes a complete chunk from the original prefixing chain, and all its remaining slices have no additional information about the object. To apply incomplete autoregressive shuffle coding according to some incompletely ordered prefixes $(\\bar{f}_{[i]})_{\\sim}\\in\\bar{\\mathcal{F}_{[i]}}/\\sim$ , formally, we will require a prefixing chain to be defined for each $\\mathcal{F}_{[i]}/{\\sim}$ . Then, we code orbits of chunked prefixes of length $i$ according to $\\operatorname{Aut}\\bigl(\\big(\\boldsymbol{f}_{[p_{C(i)}]}\\big)\\sim\\big)_{[i]}\\bigr)$ . This means that we approximate the orbit based on the full chunked prefix, and then use a prefix of that approximation of length $i$ . ", "page_idx": 17}, {"type": "text", "text": "E.1 Runtime ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For graphs, we will use color refinement to map to incompletely ordered prefixes $(f_{[i]})_{\\sim_{C_{k}}}$ and use the string prefixes according to Example 4.1 of the resulting hashes within a given chunk. This choice requires $c$ color refinement runs (instead of one as in Equation (5)), and additionally has the runtime complexity of (full) autoregressive shuffle coding on strings $O(n\\log n)$ , resulting in the overall runtime $O(c\\cdot m+n\\log n)$ stated in Equation (10), which achieves the goal of a practical runtime for a fixed number of chunks $c$ . Although not affecting runtime complexity, we can also switch adaptive entropy coding to a Fenwick tree to achieve better memory layout and therefore runtime, as described in Section 4.3, since all possible vertex colors within a chunk are known immediately after color refinement. ", "page_idx": 17}, {"type": "text", "text": "E.2 Choosing chunk sizes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Larger chunks soften the restriction for our ordered model to be autoregressive but increase the (one-shot) compression rate due to more required initial bits. For chunking on graphs as discussed above, larger chunks additionally can expect a speedup due to fewer rehashings with color refinement, and a slightly worse net rate, since chunk borders soften the restriction of the automorphism group to be factored. ", "page_idx": 18}, {"type": "text", "text": "The degenerate case of $c=n$ chunks is equivalent to not using chunking at all, a setting we refer to as \u2018full\u2019 autoregressive shuffle coding, shown in the second column of Table 6. The last column shows the other extreme with only $c=1$ chunk, where autoregressive shuffle coding degenerates into a method behaving very much like joint shuffle coding, in the sense that we can employ a joint model, and the method is not suitable for one-shot coding since the complete discount would stay unrealized in the rate due to initial bits. We therefore refer to this setting as \u2018joint\u2019 autoregressive shuffle coding. Joint shuffle coding still has a reason to exist: It typically allows more batched and parallel coding of order information and accordingly, it can be significantly faster, as we show in our experiments. ", "page_idx": 18}, {"type": "text", "text": "The unrealized discount in the rate due to initial bits is influenced by the distribution of the object\u2019s information among slices, as visualized in Figure 2. With the usual prefixing chain from Example 4.1 for i.i.d. multisets, information is distributed equally across slices, as pictured in Figure 2(a), and all but $O(\\log n)$ of the $O(n\\log n)$ discount should be realized in the rate. For simple graphs sampled from an Erd\u02ddos-R\u00e9nyi model and the prefixing chain from Example 4.2, each slice $f_{i}$ contains information about $n-1-i$ edges, so the expected slice rate drops linearly with $i$ down to 0 for the last slice, as visualized in Figure 2(b). In effect, the last slices (that are encoded onto the stack first) have the least amount of information, canceling out fewer initial bits. For graphs, we can thus expect less of the discount realized in the rate. ", "page_idx": 18}, {"type": "image", "img_path": "m2DaXpCoIi/tmp/919a37db367254911ae7df27d906692b9d18f75b0c53e2dfc803ad2dd2c0de29.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 2: Expected slice information $\\operatorname{\\mathbb{E}}\\log P(f_{i})$ by slice index $i$ for (a) i.i.d. strings using the prefixing chain from Example 4.1 and (b) simple Erdo\u02dds-R\u00e9nyi graphs using Example 4.2. ", "page_idx": 18}, {"type": "text", "text": "Assuming that all slices have the same rate, which is met in expectation for i.i.d. strings as shown in Figure 2(a), that each coded orbit has the same rate (which is only slightly inaccurate), and given that we want to use exactly $c$ chunks, the chunk sizes that minimize the rate have the following property: For each pair of neighboring chunks with sizes $c_{i}$ and $c_{i+1}$ , the ratio of the two latter by the form size should be approximately equal to the relative discount, meaning the discount log $\\frac{n!}{\\mathrm{Aut}(f)}$ of the object $f$ per ordered net rate $\\log{P(f)}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{c_{i+1}}{c_{i}}}\\approx{\\frac{\\log{\\frac{n!}{\\mathrm{Aut}(f)}}}{\\log{P(f)}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "leading to a geometric series of chunk sizes with the given base, with the first chunk being the largest.5 For simple Erdo\u02dds-R\u00e9nyi graphs, as shown in Figure 2(b), the slice rate is not uniform, making it more difficult to model the condition required for the minimal rate. For our experiments on graphs, we will simply choose (approximately) equally sized chunks, and leave optimizing relative chunk sizes for future work. ", "page_idx": 18}, {"type": "text", "text": "F Autoregressive P\u00f3lya urn model details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Kunze et al. (2024) use the P\u00f3lya urn (PU) preferential attachment model proposed by Severo et al. (2023b) to drastically improve compression rate on SZIP graphs with joint shuffle coding. Motivated by this, we use a variational approximation to obtain a tractable autoregressive model that we refer to as autoregressive P\u00f3lya urn (AP). The task of predicting the next graph slice $f_{i}$ given the prefix $f_{[i]}$ can be broken down into two parts: Given the prefix $f_{[i]}$ , predict the number of edges $k_{i}\\in[n-i]$ within the next slice $f_{i}$ , and then from that, predict the edge positions within the slice. For the P\u00f3lya urn (PU) model $P(f)$ , assuming the slice representation from Example 4.2, the distribution for the first part, $P(k_{i}|f_{[i]})$ , is not tractable, and in order to obtain a tractable variational autoregressive model, we instead approximate it based on a Zipf distribution. Specifically, we use the Zipf distribution $\\begin{array}{r}{Q(k_{i})\\propto\\frac{\\mathbf{\\hat{1}}^{\\star}}{(k_{i}+1)^{2}}}\\end{array}$ , except for smaller graphs up to 100000 edges (which includes all SZIP graphs), for which we use $\\begin{array}{r}{Q(k_{i})\\propto\\frac{1}{k_{i}^{2}}}\\end{array}$ for $k_{i}>1$ and $\\begin{array}{r}{Q(k_{i}=0)=Q(k_{i}=1)\\approx\\frac{1}{3}}\\end{array}$ . The distribution of the second part, $P(f_{i}|f_{[i]},k_{i})$ , is tractable. The generative process for $P(f_{i}|f_{[i]},k_{i})$ is as following: Given the graph of the prefix $f_{[i]}$ and the number of edges $k_{i}$ in the next slice ${\\dot{f}}_{i}$ , iteratively insert $k_{i}$ edges, with probabilities proportional to the neighbor count $+1$ of the adjacent vertices $\\{i{+}1,i{+}2,{\\ldots}n{-}1\\}$ without allowing repetition (i.e. zeroing out the probabilities where edges were already inserted). Our overall autoregressive model is then given by $\\begin{array}{r}{Q(f_{i}\\mid f_{[i]})=\\sum_{k_{i}=0}^{i}P(f_{i}\\mid f_{[i]},k_{i})Q(k_{i}\\mid f_{[i]}).}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "In general, it is not important for an autoregressive to exactly match or even approximate recognizable joint models. Instead, they can be designed or learned on their own. In our case, however, a joint model inspired a reasonable autoregressive model through approximating the original with the derivation above. ", "page_idx": 19}, {"type": "text", "text": "G Multiset compression results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "All shuffle coding speeds in this paper were measured on a MacBook Pro 2018 with a 2.7GHz Intel Core i7 CPU. ", "page_idx": 20}, {"type": "text", "text": "We compare our implementation of full autoregressive shuffle coding to Severo et al. (2023a) on multisets of varying lengths, sampled from an i.i.d. categorical distribution with probabilities sampled according to a Dirichlet distribution, as described in Severo et al. (2023a). We show compression rates in Table 7, and speeds in Figure 3. The results confirm that our implementation matches the rate of Severo et al. (2023a), but is well over two orders of magnitude faster, and scales to large multisets. We observe that the rate is within 64 bits of the optimal rate across all multiset sizes, likely mostly caused by the fixed overhead of ANS. This is consistent with our prediction from Appendix E.2 that the unrealized discount should be very small at ${\\cal O}(\\log n)$ . ", "page_idx": 20}, {"type": "text", "text": "We also evaluate joint shuffle coding using the same data and i.i.d. model, as well as \u2018joint\u2019 autoregressive shuffle coding (with a single chunk). While both result in the same net rate as full autoregressive shuffle coding, joint shuffle coding leads to the fastest runtime over a wide range of multiset sizes. ", "page_idx": 20}, {"type": "text", "text": "In Table 7, we also report the rates of autoregressive shuffle coding with 10 chunks using a geometric series of chunk sizes with varying choices for the base, compared to equally-sized chunks (base 1). The results confirm the prediction of Appendix E.2 that on multisets, a geometric series of (unequal) chunk sizes can lead to better rates over equally sized chunks. For our experiments, our chunked autoregressive ordered model simply codes multiple slices per chunk using the underlying fully autoregressive model. On multisets, this results in autoregressive shuffle coding with larger chunks having no beneftis over the fully autoregressive variant. Improved speeds can be expected for specialized chunked autoregressive models that are parallelized, similar to joint shuffle coding. We leave this for future work. ", "page_idx": 20}, {"type": "image", "img_path": "m2DaXpCoIi/tmp/2f952fbaf43854c4e6d46db834328bc152bb8b04d727b76cd38d63bcf5f30f84.jpg", "img_caption": ["Multiset size "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 3: Compression speeds (dotted lines) and decompression speeds (solid lines) of multisets of varying size using joint and autoregressive shuffle coding with 1 chunk (\u2018joint AR\u2019) and $n$ chunks (\u2018full AR\u2019), compared to the full autoregressive implementation from Severo et al. (2023a). All results are based on the (ordered) string rate as the reference uncompressed size averaged across 10 runs (100 runs for sizes $<\\!1\\mathrm{M}$ for our implementations). ", "page_idx": 20}, {"type": "text", "text": "Table 7: Compression rates in bits for one-shot compression of multisets with full autoregressive shuffle coding, compared to the implementation from Severo et al. (2023a), as well as the optimal (net) rate. We also show the effect rates when using 10 chunks with a geometric series of sizes, with varying bases. The last but one column uses the relative discount from Equation (28) of each graph as the base (shown in parentheses). ", "page_idx": 21}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/f9c37d1dfadf04ea8738a2ecc0b048aec806fcbcb8ed81cd96333c064cab6eaa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "H Joint graph shuffle coding results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We report experimental results for incomplete joint shuffle coding on graphs in Figure 4 and Tables 8 to 10. ", "page_idx": 21}, {"type": "image", "img_path": "m2DaXpCoIi/tmp/7718c9f5c2b390a9ebc85eda80b124a8712a5164c6b7e768e2d089834c3549bb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 4: Rate increase from Equation (4) relative to the (optimal) discount from Equation (3) for incomplete shuffle coding on graphs, depending on the number of color refinement convolutions $k$ , for each SZIP graph. The results are independent of the employed ordered model. ", "page_idx": 21}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/bc8c3548b6914e02daa6988a870bfbcf8fd1473bd28bc2f97b9cc8d326a251dc.jpg", "table_caption": ["Table 8: Incomplete joint shuffle coding with color refinement using 3 convolutions on the TUDatasets (Morris et al., 2020), compared to complete joint shuffle coding from Kunze et al. (2024), both using the Erd\u02ddos-R\u00e9nyi (ER) model. Since complete joint shuffle coding is too slow for three of the 24 social network datasets, REDDIT-BINARY, REDDIT-MULTI-5K, REDDIT-MULTI-12K, they were evaluated separately in the category \u2018Reddit\u2019. Compression rates are measured in bits per edge, and encoding and decoding speeds are for a single thread, in $\\mathrm{kB/s}$ . "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/6b408e40c42b2b776ae074ca6deb1d8b5de77ff4e4a29cfaf412775d74a49830.jpg", "table_caption": ["Table 9: Incomplete joint shuffle coding with color refinement using 3 convolutions on the SZIP dataset, compared to complete joint shuffle coding from Kunze et al. (2024), both using the autoregressive P\u00f3lya urn (AP) model. We report the net compression rate, that is the additional cost of compressing that graph assuming there is already some compressed data to append to, measured in bits per edge, as well as compression and decompression speeds on 8 threads in $\\mathrm{kB/s}$ . All results are means across multiple runs, 3 for complete, and 100 for incomplete shuffle coding. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "I Autoregressive graph shuffle coding results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To evaluate the effect of chunk size, we run autoregressive shuffle coding with varying numbers of equally sized chunks on the SZIP dataset. We report discount unrealized by the net rate, which is independent of the ordered model6, as well as results on the AP model in Figure 5. The results confirm that finer chunks lead to better one-shot rates, with most of the discount realized in the rate for roughly 16 chunks or more, with practical runtimes. We therefore use a default of 16 equally-sized chunks in further experiments. ", "page_idx": 22}, {"type": "text", "text": "The results also confirm that finer chunks lead to slightly better net rates, an effect predicted in Appendix E.2 caused by the corresponding softening of the restriction of a factored approximate automorphism group retrieved through color refinement. Remarkably, we observe that the net rate of autoregressive shuffle coding with 512 chunks is only 2 bits above the optimal rate given by Equation (3) for the SZIP graph dataset in total, leaving only $0.00002\\%$ of the discount unrealized in the net rate, practically matching the optimal net rate from Equation (3), and outperforming incomplete joint shuffle coding. For a more practical 16 chunks, $0.0{\\bar{5}}\\%$ of the discount is unrealized in the net rate. ", "page_idx": 22}, {"type": "text", "text": "Further results, discussed in Section 6, are shown in Tables 11 to 13. ", "page_idx": 22}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/8960f8bac1eb9d2ec23e82d8a1c3fbb4edd2d20e0affe133ad645da30a7cbebc.jpg", "table_caption": ["Table 10: Incomplete joint shuffle coding with the autoregressive P\u00f3lya urn model (AP), compared to just ordered AP, on SZIP and REC graphs. We report mean net rates and compression speeds based on 10 compression runs (100 for SZIP graphs) with varying initial message seeds. All net rates are in bits per edge, with empirical standard deviations below 0.02 bits per edge except where shown. Single-threaded and multi-threaded compression speeds with 8 logical cores are reported, all in MB/s. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/9bd5dff1f164cd8d8df714e0f43d8056bf2339304abae83d7b4d5d8675525a85.jpg", "table_caption": ["Table 11: Compression rates and speeds between incomplete autoregressive shuffle coding with 16 (or 200) chunks and 3 color refinement convolutions using an Erdo\u02dds-R\u00e9nyi (ER) and our autoregressive P\u00f3lya urn (AP) model, compared to the best results obtained by SZIP (Choi and Szpankowski, 2012) for each graph (on different hardware). We report means and standard deviations based on 100 compression runs with varying initial message seeds. All rates are in bits per edge, and all speeds are for 8 threads, in $\\mathrm{kB/s}$ . "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "m2DaXpCoIi/tmp/8f574f43fffd93b89d6a7f50906cfad897d2fdc02a107cf192c5f36e6ad3d137.jpg", "img_caption": ["Figure 5: Results for incomplete autoregressive shuffle coding for various numbers of chunks on SZIP graphs, based on the AP model and color refinement with 3 convolutions, averaged across 100 repeated runs per data point. The top and middle plots respectively show the increase of the net rate and rate over the optimal rate, relative to the discount given by Equation (3). The bottom plots shows compression speeds (dotted lines) and decompression speeds (solid lines), based on the (ordered) Erdo\u02dds-R\u00e9nyi rate as the reference uncompressed size. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/be57382704a5d89728325f59f9bfdba8fbb9579782d2b8a6db4d8d8665a76754.jpg", "table_caption": ["Table 12: Compression rates and speeds of incomplete autoregressive shuffle coding with 16 chunks and 3 color refinement convolutions based on the AP model, for the REC graph dataset. All rates are reported in bits per edge. We show the ordered rate for AP for comparison. Single-threaded and multi-threaded compression speeds are reported in MB/s. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "m2DaXpCoIi/tmp/8ff14895794e6a8f373f92015266bf316617a6251d57ea0bc258558fc34767d6.jpg", "table_caption": ["Table 13: Compression rates and speeds of incomplete autoregressive shuffle coding with 16 chunks and 3 color refinement convolutions based on the AP model, on random graphs sampled from $G(n,m)$ Erd\u02ddos-R\u00e9nyi models with large numbers of edges $m$ up to one billion, and $n=3/10\\cdot m$ vertices. We show the uncompressed size, meaning the ordered rate for the model it was sampled from, both in megabytes total and bits per edge. Rates and net rates are reported in bits per edge, and multi-threaded compression speeds in MB/s. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our claims are backed by an open-source implementation, with results reported in Section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Limitations are discussed in Section 7. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Theory developed in Sections 2 to 4 is backed by proofs in Appendix B. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide an open-source implementation including instructions to replicate all experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Code, data and instructions to replicate experiments are openly available at https://github.com/juliuskunze/shuffle-coding. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper thoroughly describes experimental details in Section 6. Additionally, all code and data is open-sourced. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Stochastic experiments are repeated an appropriate amount of times, and number of runs, means and averages are reported. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The type of hardware used in all experiments is described in Appendix G.   \nRuntimes can be inferred from reported compression speeds and sizes of the respective data.   \nFurther hints are given in the provided source code. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The authors have concluded that there are no potential harms caused by this work. All data is insensitive and was already publicly available previously. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The presented compression method is foundational without direct societal impact. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Data used in this paper is insensitive. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Original owners of data and code are properly credited in our repository and their licenses mentioned and respected. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All published code and data is properly documented. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]