[{"type": "text", "text": "Learning Transferable Features for Implicit Neural Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kushal Vyas Ahmed Imtiaz Humayun Aniket Dashpute kushal.vyas@rice.edu imtiaz@rice.edu aniket.dashpute@rice.edu Richard G. Baraniuk Ashok Veeraraghavan Guha Balakrishnan richb@rice.edu vashok@rice.edu guha@rice.edu ", "page_idx": 0}, {"type": "text", "text": "Rice University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Implicit neural representations (INRs) have demonstrated success in a variety of applications, including inverse problems and neural rendering. An INR is typically trained to capture one signal of interest, resulting in learned neural features that are highly attuned to that signal. Assumed to be less generalizable, we explore the aspect of transferability of such learned neural features for fitting similar signals. We introduce a new INR training framework, STRAINER that learns transferrable features for fitting INRs to new signals from a given distribution, faster and with better reconstruction quality. Owing to the sequential layer-wise affine operations in an INR, we propose to learn transferable representations by sharing initial encoder layers across multiple INRs with independent decoder layers. At test time, the learned encoder representations are transferred as initialization for an otherwise randomly initialized INR. We find STRAINER to yield extremely powerful initialization for fitting images from the same domain and allow for $\\mathbf{a}\\approx+10d B$ gain in signal quality early on compared to an untrained INR itself. STRAINER also provides a simple way to encode data-driven priors in INRs. We evaluate STRAINER on multiple in-domain and out-of-domain signal ftiting tasks and inverse problems and further provide detailed analysis and discussion on the transferability of STRAINER\u2019s features. Our demo can be accessed here. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Implicit neural representations (INRs) are a powerful family of continuous learned function approximators for signal data that are implemented using multilayer perceptron (MLP) deep neural networks. An INR $f_{\\theta}:\\mathbb{R}^{m}\\mapsto\\mathbb{R}^{n}$ maps coordinates lying in a $m$ -dimensional space to a value in a $n$ -dimensional output space, where $\\theta$ represents the MLP\u2019s tunable parameters. For example, a typical INR for a natural image would use an input space in $\\mathbb{R}^{2}$ (consisting of the $x$ and $y$ pixel coordinates), and an output space in $\\mathbb{R}^{3}$ (representing the RGB value of a pixel). INRs have demonstrated several useful properties including capturing details at all spatial frequencies [39, 36], providing powerful priors for natural signals [36, 39], and facilitating compression [12, 27]. For these reasons, in the past 5 years, INRs have found important uses in image and signal processing including shape representation [17, 16], novel view synthesis [31, 34, 42], material rendering [24], computational imaging [5, 30], medical imaging [49], linear inverse problems [8, 44], virtual reality [11] and compression [12, 27, 43, 51]. ", "page_idx": 0}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/362d34f84f0d9223e2f7df46604cdb2c49fae25ca546b1693583ead3ea2379b5.jpg", "img_caption": ["Figure 1: STRAINER - Learning Transferable Features for Implicit Neural Representations. During training time (a), STRAINER divides an INR into encoder and decoder layers. STRAINER ftis similar signals while sharing the encoder layers, capturing a rich set of transferrable features. At testtime, STRAINER serves as powerful initialization for ftiting a new signal (b). An INR initialized with STRAINER\u2019s learned encoder features achieves (c) faster convergence and better quality reconstruction compared to baseline SIREN models. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "A key difference between training INRs compared to other neural architectures like CNNs and Transformers is that a single INR is trained on a single signal. The features learned by an INR, therefore, are finely tuned to the morphology of just the one signal it represents. SplineCAM [19] shows that INRs learn to finely partition the input coordinate space by essentially overfitting to the spatial gradients (edges) of the signal. While this allows an INR to represent its signal with high fidelity, its features can not \u201ctransfer\u201d in any way to represent a second signal, even with similar content. If INRs could exhibit elements of transfer learning, as is the case with CNNs and Transformers, their potential would dramatically increase, such as by encoding data distribution priors for inverse imaging problems. ", "page_idx": 1}, {"type": "text", "text": "In this work, we take a closer look at INRs and transferable features, and demonstrate that the first several layers of an INR can be readily transferred from one signal to another from a domain when trained in a shared setting. We propose STRAINER , a simple INR training framework to do so (see Figure 1). STRAINER separates an INR into two parts: an \u201cencoder\u201d that maps coordinates to features, and a \u201cdecoder\u201d that maps those features to output values. We fit the encoder over a number of training signals (1 to 10 in our experiments) from a domain, e.g., face images, with separate decoders for each signal. At test time, we initialize a new INR for the test signal consisting of the trained encoder and a randomly initialized decoder. This INR may then be further optimized according to the application of interest. STRAINER offers a simple and general means of encoding data-driven priors into an INR\u2019s parameter initialization. ", "page_idx": 1}, {"type": "text", "text": "We empirically evaluate STRAINER in several ways. First, we test STRAINER on image fitting across several datasets including faces (CelebA-HQ) and medical images (OASIS-MRI) and show (Figure 2) that STRAINER\u2019s learned features are indeed transferrable resulting in a ${\\approx}{+}10\\mathrm{dB}$ gain in reconstruction quality compared to a vanilla SIREN model . We further assess the data-driven prior captured by STRAINER by evaluating it on inverse problems such as denoising and super resolution. Lastly, we provide a detailed exploration of how STRAINER learns transferable features by exploring INR training dynamics. We conclude by discussing consequences of our results for the new area of INR feature transfer. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Implicit neural representations. We define $f_{\\theta}(\\boldsymbol{p})$ as an implicit neural representation (INR) [31, 39, 29] where $f_{\\theta}$ is a multi-layer perceptron (MLP) with randomly initialized weights $\\theta$ and $p$ is the $m$ -dimensional coordinates for the signal. Each layer in the MLP is an affine operation followed ", "page_idx": 1}, {"type": "text", "text": "by a nonlinearity such as ReLU [31], or sine [39]. Given an $n$ -dimensional signal $I(p)$ , the INR learns a mapping $f:\\mathcal{R}^{m}\\rightarrow\\mathcal{R}^{n}$ . The INR is iteratively trained to fti the signal by minimizing a loss function such as $L_{2}$ loss between the signal $I(p)$ and its estimate $f_{\\theta}(\\boldsymbol{p})$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\sum_{i}\\mid\\mid A f_{\\theta}(p_{i})-I(p_{i})\\mid\\mid_{2}^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{i}{}^{\\circ}\\mathbf{s}\\in\\mathcal{R}^{m}$ span the given coordinates, $\\theta^{*}$ are the optimal weights that represent the signal, and $\\boldsymbol{\\mathcal{A}}$ is a differentiable forward model operator such as identity for signal representation and a downsampling operation for inverse problems such as super-resolution. ", "page_idx": 2}, {"type": "text", "text": "Representation capacity of INRs. The representation capacity of an INR can be described as the quality of signal the INR can represent within some number of iterations. ReLUbased INRs suffer from spectral bias during training[26], preferentially learning low frequency details in a signal and thus leading to a blurry reconstruction of the represented signal. Fourier positional encodings [21, 31, 46] or sinusoidal activation functions (SIREN ) [39] help better capture high frequency information. ", "page_idx": 2}, {"type": "text", "text": "Recent works increase the representation capacity of INRs with activations flexible in the frequency domain. WIRE [36] uses a continuous Gabor wavelet-based nonlinearity, and demonstrates impressive results for a range of forward and inverse problems. FINER [26] splits the sine nonlinearity to have a flexible frequency coverage, and DINER[50] uses a hash map to non-uniformly map the input coordinates to a feature vector, effectively re-arranging the spatial arrangement of frequencies and leading to faster and better reconstruction quality. ", "page_idx": 2}, {"type": "text", "text": "Weight initialization for INRs. Previous work has shown that smart initialization of INR weights allows for faster convergence. As shown in the SIREN study [39], hyper-networks are pro", "page_idx": 2}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/6165a09fc7d4f0c02a0dc35f513d80c38848bf195c6d83cb3c0a510d857fa934.jpg", "img_caption": ["Figure 2: STRAINER learns faster. We show the reconstruction quality (PSNR) of different initialization schemes for in-domain image fitting on CelebA-HQ [22]. We compare SIREN [39] model initialized by (1) random weights (SIREN), (2) fitting on another face image (SIREN finetuned), (3) STRAINER -1 (trained using one face image), and (4) STRAINER-10 (trained using ten face images). We also evaluate against multiple baselines such as Meta-Learned 5K [45], TransINR[9], and IPC[23] "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "posed to capture a prior over the space of implicit functions, mapping a random code to the weights of a SIREN model. Further, TransINR[9] also shows Transformer-hypernetworks as powerful metalearners for INR weight initialization. MetaSDF [38] and Light Field Networks (LFN) [40] use meta-learning-based initialization schemes to fit signed distance functions (SDFs) and light fields. Neural Compression algorithms [14, 43] use weights obtained from meta-learning optimization as a reference to store MLP weights, leading to better compression than naively storing MLP weights. Tancik et al. [45] propose meta-learning-based approaches such as Model-Agnostic Meta-Learning (MAML)[15] and Reptile[32] for coordinate based neural representations. While these meta-learning approaches yield powerful initialization, they often require long computation time (over 150K steps [45]) and ample numbers of training data, and are unstable to train [48]. Further, meta-learning initial modulations for an INR which are later optimized to fit data within few gradient updates has been shown to be an effective and scalable[6] strategy for smoothly representing data(sets) as functa(sets)[13]. Contrary to our approach, Implicit Pattern Composers (IPC)[23] proposes to keep the second layer of an INR instance-specific, while sharing the remaining layers and use a transformer hypernetwork to learn the modulations for the INR. ", "page_idx": 2}, {"type": "text", "text": "Prior informed INRs. Recent work has also explored embedding a learned prior in INRs for tasks such as audio compression, noise removal, and better CT reconstructions. Siamese Siren [25] uses a similar approach where they propose a compact siamese INR whose initial layers are shared followed by 2 siamese decoders. Since 2 randomly initialized decoders will yield slightly different reconstructions, this difference is leveraged for better noise estimation in audio signal. NERP [37] learns an internal INR prior for medical imaging by first ftiting high quality MRI or CT data. Weights of this learned INR are used as an initialization for reconstructing new MRI or CT undersampled data. While this paper shows a method to learn an implicit prior, their prior embedding is learned from a single MRI or CT scan of the same subject whereas our work explores learning a prior for INRs by constraining it to learn an underlying implicit representation across multiple different images. PINER [41] introduces a test-time INR adaptation framework for sparse-view CT reconstruction with unknown noise. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce STRAINER . We first explain our motivation to share initial layers in an INR Section 3.1. In Section 3.2 we describe the training phase of STRAINER where we learn transferrable features for INRs by sharing the initial layers of $N$ INRs being fti independently to $N$ images. Section 3.3, details how our captured basis is used to fti an unseen image. In subsequent sections, we seek to understand what our shared basis captures and how to expand it to other problems such as super resolution. For simplicity, we build upon the SIREN [39] model as our base model. ", "page_idx": 3}, {"type": "text", "text": "3.1 Why share the initial INR layers? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A recent method called SplineCAM [19] provides a lens with which to visualize neural network partition geometries. SplineCAM interprets an INR as a function that progressively warps the input space and fits a given signal through layerwise affine transforms and non-linearities [19]. For continuous piecewise affine activation functions, we use an approximation to visualize (see Figure 6) the deep network\u2019s partition geometry for different pre-activation level sets [20]. ", "page_idx": 3}, {"type": "text", "text": "An INR fit to a signal highly adapts to the underlying structure of the data in a layer-wise fashion. Furthermore, by approximating the spatial position of the pre-activation zero level sets, we see that initial layers showcase a coarse, less-partitioned structure while deeper layers induce dense partitioning collocated with sharp changes in the image. Since natural signals tend to be similar in their lower frequencies, we hypothesize that initial layers of multiple INRs are better suited for transferability. We therefore design STRAINER to share the initial encoder layers, effectively giving rise to an input space partition that can generalize well across different similar signals. ", "page_idx": 3}, {"type": "text", "text": "3.2 Learning transferable features from $N$ images ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a SIREN [39] model $h(p)$ with $L$ layers. Let $K$ out of $L$ layers correspond to an encoder subnetwork represented as $f_{\\theta}$ The remaining layers correspond to the decoder sub-network represented as $g_{\\phi}$ as seen in Figure 1(a). For given input coordinates $p$ , we express the SIREN model $\\bar{h_{\\phi,\\theta}}(p)$ as a composition (\u25e6) of our encoder-decoder sub-networks. ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{\\phi,\\theta}(p)=g_{\\phi}\\circ f_{\\theta}(p)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In a typical case, given the task of fitting $N$ signals, each of the $N$ signals is independently fit to an INR, thus not leveraging any similarity across these images. Since we want to learn a shared representation transferrable across multiple similar images, our method shares the encoder $f_{\\theta}$ across all $N$ INRs while maintaining a set of individual signal-specific decoders $g_{\\phi}^{1}\\ \\ \\ .\\ .\\ g_{\\phi}^{N}$ .Our overall architecture is shown in Figure 1. We call this STRAINER\u2019s training phase - Figure 1(a). We start with randomly initialized layers and optimize the weights to fti $N$ signals in parallel. For each signal $I_{i}(p)$ , we use a $L_{2}$ loss between $I_{i}(p)$ and its corresponding learned estimate $\\bar{h}_{\\phi,\\theta}^{i}(p)$ and sum the loss over all the $N$ signals. Iteratively, we learn a set of weights $\\Theta$ that minimizes the following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta^{*}=\\arg\\operatorname*{min}_{\\Theta}\\sum_{i=1}^{N}\\mid\\mid g_{\\phi}^{i}\\circ f_{\\theta}(p)-I_{i}(p)\\mid\\mid_{2}^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Theta\\,=\\,[\\theta,\\ \\phi^{1}\\ \\ldots\\phi^{N}]$ represents the full set of weights of the shared encoder $(\\theta)$ and the $N$ different decoders $(g_{\\phi}^{1}\\cdot\\cdot\\cdot\\bar{g}_{\\phi}^{N})$ and $\\Theta^{*}$ represents the resulting optimized weights. ", "page_idx": 3}, {"type": "text", "text": "3.3 Fitting an unseen signal with STRAINER ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "After sufficient iterations during STRAINER\u2019s training phase, we get optimized encoder weights $f_{\\theta^{*}}$ which corresponds to the rich shared representation learned over signals of the same category. To fti a ", "page_idx": 3}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/ba987562e21f3cde8ed98fa7ceaaa15bb9dcbb7bbba5d1ba18d3d2c169130694.jpg", "img_caption": ["Figure 3: Visualization of learned features in STRAINER and baseline SIREN model. We visualize (a) the first principal component of the learned encoder features for STRAINER and corresponding layer for SIREN . At iteration 0, STRAINER\u2019s features already capture a low dimensional structure allowing it to quickly adapt to the cat image. High frequency detail emerges in STRAINER\u2019s learned features by iteration 50, whereas SIREN is lacking at iteration 100. The inset showing the power spectrum of the reconstructed image further confirms that STRAINER learns high frequency faster. We also show the (b) reconstructed images and remark that STRAINER fits high frequencies faster. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "novel signal $I_{\\psi}(p)$ we initialize the STRAINER model with the learned shared encoder weights $f_{\\theta=\\theta^{*}}$ and randomly initialize decoder $g_{\\phi}^{\\psi}$ weights to solve for: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi^{*},\\theta^{*}=\\arg\\operatorname*{min}_{\\phi,\\theta}\\,\\vert\\vert\\,g_{\\phi}^{\\psi}\\circ f_{\\theta=\\theta^{*}}(p)-I_{\\psi}(p)\\,\\,\\vert\\vert_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$f_{\\theta=\\theta^{*}}$ serves as a learned initial encoder features. Our formulation is equivalent to a initial set of learned encoder features followed by a set of random projections. While ftiting an unseen signal, we iteratively update all the weights of the STRAINER model, similar to any INR. ", "page_idx": 4}, {"type": "text", "text": "3.4 Learning an intermediate partition space in the shared encoder $f_{\\theta}$ \u2217 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "During the training phase, explicitly sharing layers in STRAINER allows us to learn a set of INR features which exhibits a common partition space shared across multiple images. Since deep networks perform layer wise subdivision of the input space, sharing the encoder enforces the layer to find the partition that can be further subdivided into multiple coarse partitions corresponding to the tasks being trained. In Figure 6(a.ii), while pre-training an INR using the STRAINER framework on CelebA-HQ dataset, we see emergence of a face-like structure captured in our STRAINER encoder $f_{\\theta^{*}}$ . We expect our STRAINER encoder weights $f_{\\theta^{*}}$ to be used as transferrable features and be used as initialization for fitting unseen in-domain samples. ", "page_idx": 4}, {"type": "text", "text": "In comparison, meta learning methods to learn initialization for INRs[45] exhibit a partitioning of the input space that is closer to random. As seen in Figure 6(a.i) there is a faint image structure captured by the the learned initialization. This is an indication that the initial subdivision of the input space, found by the meta learned pre-training layers, captures less of the domain specific information therefore is a worse initialization compared to STRAINER . We further explain our findings in Section 5 and discuss STRAINER\u2019s learned features being more transferrable and lead to better quality reconstruction. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In all experiments, we used the SIREN [39] MLP with 6 layers and sinusoid nonlinearities. We considered two versions of STRAINER : (i) STRAINER (1 decoder), where the encoder layers are initialized using our shared encoder trained on a single image, and (ii) STRAINER-10 (10 Decoders), where the encoder layers are initialized using our shared encoder trained on 10 images. We considered the following baselines: (i) a vanilla SIREN model with random uniform initialization [39], (ii) a fine-tuned SIREN model initialized using the weights from another SIREN fit to an image from the same domain, (iii) a SIREN model initialized using Meta-learned 5K [45], (iv) transformer-based metalearning models such as TransINR[9] and IPC[23]. We ensured an equal number of learnable parameters (neurons) for all models. We normalized all images between (0-1), and input coordinates between (-1,1). We used the Adam optimizer with a learning rate of $10^{-4}$ for STRAINER\u2019s training and test-time evaluation, unless mentioned otherwise. Further implementation details are provided in Supplementary. ", "page_idx": 4}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/7c2b3df9efceccce1079c0b03bc168094bfe76238b80e92b98a93b37323d21e7.jpg", "img_caption": ["Figure 4: STRAINER converges to low and high frequencies fast. We plot the histogram of absolute gradients of layers 1,5 and last over 1000 iterations while fitting an unseen signal. At test time, STRAINER\u2019s initialization quickly learns low frequency, receiving large gradients update at the start in its initial layers and reaching convergence. The Decoder layer in STRAINER also ftis high frequency faster. Large gradients from corresponding SIREN layers show it learning significant features as late as 1000 iterations. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We mainly used the CelebA-HQ [22], Animal Faces-HQ (AFHQ) [10], and OASIS-MRI [18, 28] images for our experiments. We randomly divided CelebA-HQ into 10 train images and 550 test images. For AFHQ, we used only the cat data, and used ten images for training and 368 images for testing. For OASIS-MRI, we used 10 of the (template-aligned) 2D raw MRI slices for training, and 144 for testing. We also used Stanford Cars[2] and Flowers[1] to further validate out of domain generalization and Kodak [3] true images for demonstrating high-resolution image fitting. ", "page_idx": 5}, {"type": "text", "text": "4.2 Training STRAINER\u2019S shared encoder ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first trained separate shared encoder layers of STRAINER on 10 train images from each dataset. We share five layers, and train a separate decoder for each training image. For each dataset, we trained the shared encoder for 5000 iterations until the model acheives $\\mathrm{PSNR}\\approx30d B$ for all training images. We use the resulting encoder parameters as initialization for test signals in the following experiments. For comparison, we also trained the Meta-learned 5K baseline using the implementation provided by Tancik et.al.[45] with 5000 outer loop iterations. We also use the implementation provided by IPC[23] as our baselines for TransINR[9] and IPC[23] and train them with 14,000 images from CelebA-HQ . We report a comparison of number of training images and parameters, gradient updates, and learning time in Table 5. ", "page_idx": 5}, {"type": "text", "text": "4.3 Image fitting (in-domain) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first evaluated STRAINER on the task of in-domain image fitting. We cropped and resized all images to $178\\times178$ and ran test-time optimization on all models for 2000 steps. ", "page_idx": 5}, {"type": "text", "text": "At test-time, both STRAINER and STRAINER-10 use only 1 decoder, resulting in the same number of parameters as a SIREN INR. Table 1 shows average image metrics for in-domain image ftiting reported with 1 std. deviation. Instead of naively fine tuning using another INR, STRAINER\u2019s design of sharing initial layers allows for learning highly effective features which transfer well across images in the same domain, resulting in high quality reconstruction across CelebA-HQ and AFHQ and comparable to Meta-learned 5K for OASIS-MRI images. Table 3(CelebA-HQ , ID) also shows that STRAINER initialization results in better quality reconstruction, when optimized at test-time, compared to more recent transformer-based INR approaches such as TransINR and IPC as well. ", "page_idx": 5}, {"type": "table", "img_path": "ABYdKpDb8p/tmp/af55935940201d4a5d57f38b8caab2277b1451740bc72f8d1e3b9b7b5b9b4043.jpg", "table_caption": ["Table 1: In-domain image fitting evaluation. STRAINER\u2019s learned features yield powerful initialization at test-time resulting in high quality in-domain image fitting "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: Fitting MRI images from OASIS-MRI dataset. At just 100 iterations, STRAINER is able to represent medical images with high quality. STRAINER\u2019s initialization allows for fast recovery for sparse and delicate structures, showing applicability in low-resource medical domains as well. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "ABYdKpDb8p/tmp/1cb16929ba3f8653960cd147259813e48bbba5637931bd02ba6a12fd61ff752e.jpg", "table_caption": ["Table 2: Out of domain image fitting evaluation, when trained on CelebA-HQ and tested on AFHQ and OASIS-MRI. STRAINER\u2019s learned features are a surprisingly good prior for fitting images out of its training domain. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "ABYdKpDb8p/tmp/15887efcd7ad774e18ef3129fc1f00cc00673f491a6446c9fde250cc3c0e90f3.jpg", "table_caption": ["Table 3: Baseline evaluation for image-fitting for in-domain(ID) and out-of-domain(OD) data. STRAINER learns more transferable features resulting in better performance across the board. Models trained on CelebA-HQ unless mentioned otherwise. TTO $=$ Test time optimization. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.4 Image fitting (out-of-domain) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To test out-of-domain transferability of learned STRAINER features, we used STRAINER-10 \u2019s encoder trained on CelebA-HQ as initialization for ftiting images from AFHQ (cats) and OASIS-MRI datasets (see Table 2). Since OASIS-MRI are single channel images, we trained Meta-learned 5K and STRAINER-10 (GRAY) on the green channel only of CelebA-HQ images. To our surprise, we see STRAINER-10 and STRAINER-10 (GRAY) clearly outperform not only Meta-learned 5K , but also STRAINER-10 (in-domain). To further validate out of domain performance of STRAINER , we train ", "page_idx": 6}, {"type": "table", "img_path": "ABYdKpDb8p/tmp/eb8b9a3e16e2c2261ba68d64855071d268ace41428012df446a7579f617fd683.jpg", "table_caption": ["Table 4: Out-of-domain image fitting on Kodak Images [3]. STRAINER (trained on CelebA-HQ ) allows better convergence comparable to high capacity SIREN models as indicated by PSNR metric. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ABYdKpDb8p/tmp/00bc21f15f6243483cf5c2e1c49c2b3b575fb5531ace4049033e66c0801132e9.jpg", "table_caption": ["Table 5: Training time and compute complexity. We train all the methods for 5000 steps. STRAINER instantly learns a powerful initialization with minimal data and significantly fewer gradient updates. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ABYdKpDb8p/tmp/3104a7ba0becbe0bf4e7b40d146734ad11d800a6cccd2973fd3cf0f9679f13a9.jpg", "table_caption": ["Table 6: STRAINER accelerates recovery of latent images in inverse problems. STRAINER captures an implicit prior over the training data allowing it to recover a clean latent image of comparable quality $3\\times$ faster making it useful for inverse problems. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "STRAINER-10 \u2019s shared encoder on simply 10 images from Flowers[1] and Stanford Cars[2] datasets which have different spatial distribution of color and high frequencies than AFHQ and OASIS-MRI. For fair comparison, all models in Table 3(OOD) were fti with 3-channel RGB or concatenated gray images in case of OASIS-MRI. As shown in Table 3(OOD), STRAINER-10 provides superior out of domain performance for AFHQ trained on CelebA-HQ , followed by Flowers and Stanford Cars. For OASIS-MRI, we see STRAINER-10 having best performance when trained with StanfordCars. This result suggests that STRAINER is capable of capturing transferable features that generalize well across natural images. ", "page_idx": 7}, {"type": "text", "text": "STRAINER also ftis high resolution Kodak[3] images well and is comparable to SIREN networks with twice the network width. ", "page_idx": 7}, {"type": "text", "text": "4.5 Inverse problems: super-resolution and denoising ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "STRAINER provides a simple way to encode data-driven priors, which can accelerate convergence on inverse problems such as super-resolution and denoising. We sampled 100 images from CelebA-HQ at $178\\times178$ and added $2d B$ of Poisson random noise. We report mean values of PSNR achieved by STRAINER and SIREN models along with the iterations required to achieve the values. For superresolution, we demonstrate results on one image from DIV2K[4, 47], downscaled to $256\\times256$ for a low resolution input. We used the formulation shown in Equation (1), with $\\boldsymbol{\\mathcal{A}}$ set to a $4\\times$ downsampling operation. To embed a prior relevant for clean images, we trained the shared encoder of STRAINER with high quality images of resolution same as the latent recovered image. At test time, we fit the STRAINER model to the corrupted image, following Equation (1) and recovered the latent image during the iteration. We report STRAINER\u2019s ability to recover latent images fast as well as with high quality in Section 4.5 ", "page_idx": 7}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/dc91a1b1a91913efe6fb4f66d9e7ad493c4ce2762289d9d582f00160e64e3863.jpg", "img_caption": ["Figure 6: Visualizing density of partitions in input space of learned models. We use the method introduced in [20] to approximate the input space partition of the INR. We present the input space partitions for layers 2,3,4 across (a) Meta-learned 5K and STRAINER initialization and (b) at test time optimization. STRAINER learns an input space partitioning which is more attuned to the prior of the dataset, compared to meta learned which is comparatively more random. We also observe that SIREN (iii) learns an input space partitioning highly specific to the image leading to inefficient transferability for fitting a new image (iv) with significantly different underlying partitioned input space This explains the better in-domain performance of STRAINER compared to Meta-learned 5K , as the shallower layers after pre-training provide a better input space subdivision to the deeper layers to further subdivide. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Results in Table 1, 3 demonstrate that STRAINER can learn a set of transferable features across an image distribution to precisely fit unseen signals at test time. STRAINER-10 clearly achieves the best reconstruction quality in terms of PSNR and SSIM on CelebA-HQ and AFHQ datasets, and is comparable with Meta-learned 5K on OASIS-MRI images. STRAINER-10 also fits images fast and achieves highest reconstruction quality than all baselines as shown in Figure 2. Comparing STRAINER (1 decoder) with a fine-tuned SIREN , it seems that the representation learned on one image is not sufficiently powerful. However, as little as 10 images result in a rich and transferable set of INR features allowing STRAINER-10 to achieve ${\\approx}7{-}10\\mathrm{dB}$ higher reconstruction quality than SIREN and SIREN fine-tuned. ", "page_idx": 8}, {"type": "text", "text": "As seen in Table 2, 3(OOD) STRAINER also performs well on out-of-domain tasks, which is quite surprising. ", "page_idx": 8}, {"type": "text", "text": "STRAINER\u2019s transferable representations are capable of recovering small and delicate structures as early as 100 iterations as shown in Figure 5 and do not let the scale of features from the training phase affect its reconstruction ability. Another interesting finding is that STRAINER-10 achieves far better generalization for OASIS-MRI (Table 2) when pretrained on CelebA-HQ . Further, STRAINER generalizes well to out-of-domain high-resolution images, as demonstrated by our experiments of training STRAINER on CelebA-HQ and testing on the Kodak data (see Table 4). ", "page_idx": 8}, {"type": "text", "text": "STRAINER is fast and cheap to run. Table 5 summarizes the time for learning the initialization for a 6 layered MLP INR for STRAINER , Meta-learned 5K and transformer-based methods such as TransINR and IPC. At 5000 iterations, STRAINER learns a transferable representation in just 24.54 seconds. Meta-learned 5K , in comparison, uses MAML[15] which is far more computationally intensive and results in $20\\times$ slower runtime when exact number of gradient updates are matched. Further, STRAINER \u2019s training setup is an elegant deviation from recent methods such as TransINR and IPC, requiring large datasets and complex training routines. ", "page_idx": 8}, {"type": "text", "text": "5.1 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Due to the encoder layers of STRAINER being tuned on data and the later layers being randomly initialized, we have observed occasional instability when fitting to a test signal in the form of PSNR \u201cdrops.\u201d However, we observe that STRAINER usually quickly recovers, and the speedup provided by STRAINER outweighs this issue. While our work demonstrates that INR parameters may be transferred across signals, it is not fully clear what features are being transferred, how they change for different image distributions, and how they compare to the transfer learning of CNNs and Transformers. Further work is needed to characterize these. ", "page_idx": 9}, {"type": "text", "text": "5.2 Further analysis of STRAINER ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To further understand how STRAINER\u2019s initialized encoder enables fast learning of signals at test time, we explored the evolution of STRAINER\u2019s hidden features over iterations in Figure 3. In Figure 3(a), we visualize the first principal component of learned INR features of the STRAINER encoder and corresponding hidden layer for SIREN across iterations and observe that STRAINER captures high frequencies faster than SIREN. This is corroborated by the power spectrum inset plots of the reconstructed images. We also visualize a histogram of gradient updates in Figure 4, and observe that STRAINER receives large gradients in its encoder layers early on during training, suggesting that the encoder rapidly learns of low-frequency details. ", "page_idx": 9}, {"type": "text", "text": "Next, we visualize the input space partitions induced by STRAINER and the adaptability of STRAINER\u2019s initialization for ftiting new signals. We use the local complexity(LC) measure proposed by Humayun et.al.[20] to approximate different pre-activation level sets of the INR neurons. For ReLU networks, the zero level sets correspond to the spatial location of the non-linearities of the network. For periodic activations, there can be multiple non-linearities affecting the input domain. In Figure 6 we present the zero level sets of the network, and in Supplementary we provide the $\\pm\\pi/2$ shifted level sets. Darker regions in the figure indicate high LC, i.e., higher local non-linearity. Figure 6 also presents partitions for the baseline models. ", "page_idx": 9}, {"type": "text", "text": "SIREN models tend to overfti, with partitions strongly adapting to image details. Since the sensitivity to higher frequencies is mapped to specific input partitions, when finetuning with SIREN , the network has to unlearn partitions of the pretrained image resulting in sub optimal reconstruction quality. When comparing Meta-learned 5K with STRAINER , we see that STRAINER learns an input space partitioning more attuned to the prior of the dataset, compared to Meta-learned 5K which is comparatively more random. While both partitions imply learning high-frequency details, STRAINER\u2019s partitions are better adapted to facial geometry, justifying its better in-domain performance. ", "page_idx": 9}, {"type": "text", "text": "6 Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "STRAINER introduces how to learn transferable features for INRs resulting in faster convergence and higher reconstruction quality. We show with little data, we can learn powerful features as initialization for INRs to fit signals at test-time. Our method allows the use of INRs to become ubiquitous in data-hungry areas such as patient specific medical imaging, personalized speech and video recordings, as well as real-time domains such as video streaming and robotics. However, our method is for training INRs to represent signals in general, which can adopted regardless of underlying positive or negative intent. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is partially supported by NIH DeepDOF R01DE032051-01, OneDegree CNS-1956297, IARPA WRIVA 140D0423C0076 and NSF grants CCF1911094, IIS-1838177, and IIS-1730574; ONR grants N00014- 18-12571, N00014-20-1-2534, and MURI N00014-20-1-2787; AFOSR grant FA9550-22-1-0060; and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Flower recognition. https://www.kaggle.com/datasets/alxmamaev/flowers-recognition. ", "page_idx": 9}, {"type": "text", "text": "[2] Stanford cars dataset. https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset. ", "page_idx": 10}, {"type": "text", "text": "[3] Kodak lossless true color image suite. https://r0k.us/graphics/kodak/, 1999. [Accessed 2024-05-22].   \n[4] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.   \n[5] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, and Matthew O\u2019Toole. T\u00f6rf: Time-of-flight radiance fields for dynamic scene view synthesis. Advances in neural information processing systems, 34:26289\u201326301, 2021.   \n[6] Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Richard Schwarz, and Hyunjik Kim. Spatial functa: Scaling functa to imagenet classification and generation, 2023.   \n[7] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.   \n[8] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In CVPR, 2021.   \n[9] Yinbo Chen and Xiaolong Wang. Transformers as meta-learners for implicit neural representations. In European Conference on Computer Vision, 2022.   \n[10] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains, 2020.   \n[11] Nianchen Deng, Zhenyi He, Jiannan Ye, Budmonde Duinkharjav, Praneeth Chakravarthula, Xubo Yang, and Qi Sun. Fov-nerf: Foveated neural radiance fields for virtual reality. IEEE Transactions on Visualization and Computer Graphics, 28(11):3854\u20133864, 2022.   \n[12] Emilien Dupont, Adam Goli\u00b4nski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. Coin: Compression with implicit neural representations. arXiv preprint arXiv:2103.03123, 2021.   \n[13] Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo J. Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you should treat it like one. CoRR, abs/2201.12204, 2022.   \n[14] Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Golinski, Yee Whye Teh, and Arnaud Doucet. Coin++: Neural compression across modalities. Transactions on Machine Learning Research, 2022.   \n[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126\u20131135. PMLR, 06\u201311 Aug 2017.   \n[16] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3d shape. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4857\u20134866, 2020.   \n[17] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7154\u20137164, 2019.   \n[18] Andrew Hoopes, Malte Hoffmann, Douglas N. Greve, Bruce Fischl, John Guttag, and Adrian V. Dalca. Learning the effect of registration hyperparameters with hypermorph. Machine Learning for Biomedical Imaging, 1(IPMI 2021):1\u201330, April 2022.   \n[19] Ahmed Imtiaz Humayun, Randall Balestriero, Guha Balakrishnan, and Richard G. Baraniuk. Splinecam: Exact visualization and characterization of deep network geometry and decision boundaries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3789\u20133798, June 2023.   \n[20] Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk. Deep networks always grok and here is why, 2024.   \n[21] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[22] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017.   \n[23] Chiheon Kim, Doyup Lee, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Generalizable implicit neural representations via instance pattern composers. arXiv preprint arXiv:2211.13223, 2022.   \n[24] Alexandr Kuznetsov. Neumip: Multi-resolution neural materials. ACM Transactions on Graphics (TOG), 40(4), 2021.   \n[25] Luca A Lanzend\u00f6rfer and Roger Wattenhofer. Siamese siren: Audio compression with implicit neural representations. arXiv preprint arXiv:2306.12957, 2023.   \n[26] Zhen Liu, Hao Zhu, Qi Zhang, Jingde Fu, Weibing Deng, Zhan Ma, Yanwen Guo, and Xun Cao. Finer: Flexible spectral-bias tuning in implicit neural representation by variable-periodic activation functions, 2023.   \n[27] Shishira R Maiya, Sharath Girish, Max Ehrlich, Hanyu Wang, Kwot Sin Lee, Patrick Poirson, Pengxiang Wu, Chen Wang, and Abhinav Shrivastava. Nirvana: Neural implicit representations of videos with adaptive networks and autoregressive patch-wise modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14378\u201314387, 2023.   \n[28] Daniel S. Marcus, Tracy H. Wang, Jamie Parker, John G. Csernansky, John C. Morris, and Randy L. Buckner. Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI Data in Young, Middle Aged, Nondemented, and Demented Older Adults. Journal of Cognitive Neuroscience, 19(9):1498\u20131507, 09 2007.   \n[29] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4460\u20134470, 2019.   \n[30] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, and Jonathan T Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16190\u201316199, 2022.   \n[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.   \n[32] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.   \n[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[34] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages 523\u2013540. Springer, 2020. ", "page_idx": 12}, {"type": "text", "text": "[35] Stanford 3D Scans Repository. Thai statue.   \n[36] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, and Richard G Baraniuk. Wire: Wavelet implicit neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18507\u201318516, 2023.   \n[37] Liyue Shen, John Pauly, and Lei Xing. Nerp: implicit neural representation learning with prior embedding for sparsely sampled image reconstruction. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[38] Vincent Sitzmann, Eric Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf: Meta-learning signed distance functions. Advances in Neural Information Processing Systems, 33:10136\u201310147, 2020.   \n[39] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33:7462\u20137473, 2020.   \n[40] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:19313\u201319325, 2021.   \n[41] Bowen Song, Liyue Shen, and Lei Xing. Piner: Prior-informed implicit neural representation learning for test-time adaptation in sparse-view ct reconstruction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1928\u20131938, 2023.   \n[42] Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T Barron. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7495\u20137504, 2021.   \n[43] Yannick Str\u00fcmpler, Janis Postels, Ren Yang, Luc Van Gool, and Federico Tombari. Implicit neural representations for image compression. In European Conference on Computer Vision, pages 74\u201391. Springer, 2022.   \n[44] Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, and Ulugbek S Kamilov. Coil: Coordinate-based internal learning for tomographic imaging. IEEE Transactions on Computational Imaging, 7:1400\u20131412, 2021.   \n[45] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2846\u20132855, 2021.   \n[46] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS, 2020.   \n[47] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, et al. Ntire 2017 challenge on single image super-resolution: Methods and results. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.   \n[48] Anna Vettoruzzo, Mohamed-Rafik Bouguelia, Joaquin Vanschoren, Thorsteinn R\u00f6gnvaldsson, and KC Santosh. Advances and challenges in meta-learning: A technical review, 2023.   \n[49] Yuehao Wang, Yonghao Long, Siu Hin Fan, and Qi Dou. Neural rendering for stereo 3d reconstruction of deformable tissues in robotic surgery. In Intl. Conf. Medical Image Computing and Computer-Assisted Intervention, 2022.   \n[50] Shaowen Xie, Hao Zhu, Zhen Liu, Qi Zhang, You Zhou, Xun Cao, and Zhan Ma. Diner: Disorder-invariant implicit neural representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.   \n[51] Yunfan Zhang, Ties van Rozendaal, Johann Brehmer, Markus Nagel, and Taco Cohen. Implicit neural video compression. In ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Suplementary Material / Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our work, we increase the representation capacity of the INR by leveraging the similarity across natural images (of a given class). Since each layer of an INR MLP is an affine transformation followed by a non linearity, we interpret the INR as a function that progressively warps the input coordinate space to fti the given signal, in our case the signal being an image. Similar images when independently fti to their respective INRs capture similar low-frequency detail such as shape, geometry, etc. whereas high-frequency information such as edges and texture are unique to each INR. We propose that these low-frequency features from the initial layers of a learned INR are highly transferable and can be used as a basis and initialization while fitting an unseen signal. To that end, we introduce a novel method of learning our basis by sharing a set of initial layers across INRs fitting their respective images. ", "page_idx": 14}, {"type": "text", "text": "Our implementation can be found on 1Google Colab. ", "page_idx": 14}, {"type": "text", "text": "Understanding the effect of sharing encoder layers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We further investigate how the number of initial layers shared affects the quality of reconstructed image. We start by sharing $K=1$ layer as the encoder, and $N-K$ layers in each decoder and vary $K$ from 1 to $N-1$ . $K=N$ is equivalent to simply fine tuning the INR based on all weights from a fellow model. We tabulate our results for image quality (PSNR) in a fixed runtime of 1000 iterations. We find that sharing all but the last layer results in the most effective capturing of our shared basis leading to higher reconstruction quality as seen in fig. 7. This also suggests that the last decoder of the INR is mainly responsible for very localized features. Further our work motivates further interest to sutdy the nature of the decoder layers itself. ", "page_idx": 14}, {"type": "text", "text": "We show the effect of sharing layers and resulting reconstruction quality. We use a 5 layered Siren model for this experiment. We fit a vanilla Siren model to an image and report its PSNR in fig. 7. Further, we train our shared encoder by sharing $K=1$ layers and so on , until we share $K=N-1$ layers. ", "page_idx": 14}, {"type": "text", "text": "We see that the reconstruction quality progressively increases by sharing layers. ", "page_idx": 14}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/830a3506468a0cb5ad28e4d0597ff60f75859da399e95214d2d0a66a440708b2.jpg", "img_caption": ["Figure 7: Sharing different number of layers in STRAINER\u2019s encoder. We see that by increasing the number of shared layers, STRAINER\u2019s ability to recover the signal also improves. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Reporting std. deviation STRAINER for image fitting on CelebA-HQ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We also report the PSNR within 1 std. deviation while comparing STRAINER -10 with SIREN , SIREN -finetuned, STRAINER (1-decoder), and Meta-learned 5K in Figure 9. ", "page_idx": 14}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/6034de4362b6e392f6a85691d45b25c0c6a1ec0440cbfadd4277b18162069c5d.jpg", "img_caption": ["Figure 8: Fitting STRAINER on shifted, rotated and flipped versions of a face image. We see that despite the transformations done on a face image: (a) roll (wrapping the image both vertically and horizontally), (b) rotate, (c) filp vertically, and (d) wrap vertically, STRAINER ftis equally well on all of them. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/ea13dc4673ba1c04a518cb438d1058f504fac5b83df1b5fa97783cc670937836.jpg", "img_caption": ["Figure 9: STRAINER learns fast. We show a limited baseline comparison of STRAINER with SIREN , SIREN -finetuned and Meta-learned 5K methods for the task of image-ftiting on CelebA-HQ dataset and note that STRAINER achieves superior reconstruction quality. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Effect of orientation of the input image ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We wanted to further assess whether the INR is overftiting to one particular aligned face arrangement. To further test this, we take a test image and apply various augmentations such as filp, rotate, and roll and study how STRAINER fits it (see Figure 8). We find that the initialization learned by strainer is invariant, at test time, to the input signals orientation and can successfully capture the high frequency details fast. ", "page_idx": 15}, {"type": "text", "text": "Measuring time for pretraining STRAINER and Meta-learned 5K ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our implementation is written in PyTorch[33] whereas Meta-learned 5K implemented by Tancik et.al[45] is a JAX implementation. For measuring runtime, we use the python\u2019s time package and very conservatively time the step where the forward pass and gradient updates occur in both methods. Further, we run the code on an Nvidia A100 GPU and report the time after averaging 3 such runs for each method. There may be system level differences, however, to the best of our knowledge and observation, our timing estimates if not accurate are atleast indicative of the speedup provided by STRAINER . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Training details for Kodak high resolution images ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To further demonstrate that STRAINER\u2019s can be adapted to high resolution images, we evaluated our method on high quality Kodak[3] images with resolution $512\\times768$ (see Tables 4 and 7). We present the reconstruction quality attained by STRAINER -10, SIREN model, and a SIREN model initialized using Meta-learned 5K , with widths of 256, 512. For this experiment, we train our STRAINER encoder using CelebA-HQ Images which are resized to the same resolution to Kodak images. Further, we follow all steps as previously described for test-image evaluation of Kodak images. Here is another results from the Kodak high resolution images experiment. ", "page_idx": 16}, {"type": "table", "img_path": "ABYdKpDb8p/tmp/3f1e41955f2eb12cc5b6f9b33cc3cae1a054fa4123dbd2e89c9a31b09e222bec.jpg", "table_caption": ["Table 7: STRAINER allow better convergence comparable to high capacity Siren models, and metalearned initializations, as indicated by PSNR metric. Tested on high quality Kodak Images. $\\mathbf{ID}=\\mathbf{In}$ domain, $\\mathrm{OD}\\!=$ Out of domain. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Results for Inverse problems - Super Resolution ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We discuss how STRAINER provides a useful prior for inverse problems such as super resolution. For the results reported in section 4.5, we attach supplementary plots as shown in fig. 10. STRAINER-10 (Fast) is a STRAINER-10 model with 5 shared encoder layers out of 6 total layers. STRAINER-10 (HQ) is a high quality STRAINER model with 3 shared encoder layers. Unlike forward ftiting, more degree of randomness in the decoder helps recover better detail for inverse problems. We also showcase the effectiveness of STRAINER for in domain super resolution shown in fig. 11. ", "page_idx": 16}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/9af9fd8679a012f5b3c22a9b08c6ef1be7e9e7e3ddb85ab9742af6529a7a8c37.jpg", "img_caption": ["Figure 10: Super Resolution using STRAINER . We show the reconstructed results (a) on the left using SIREN and STRAINER . We also plot (b) the trajectory of PSNR with iterations. STRAINER-10 (Fast) achieves comparable PSNR to SIREN in approximately a third of the runtime. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "STRAINER for Occupancy fitting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "STRAINER is a general purpose transfer learning framework which can be used to initialize INRs for regressing 3D data like occupancy maps, radiance fields or video. To demonstrate the effectiveness of STRAINER on 3D data, we have performed the following OOD generalization experiment. We pre-train STRAINER on 10 randomly selected \u2018Chair\u2019 objects from the ShapeNet[7] dataset. At test time, we fit the \u2018Thai Statue\u2019 3D object[35]. STRAINER achieves a 12.3 relative improvement in IOU compared to random initialization for a SIREN architecture \u2013 in 150 iterations STRAINER-10 obtains an IOU of 0.91 compared to an IOU of 0.81 without STRAINER-10 initialization. We present visualizations of the reconstructed Thai Statue in Figure 12. Upon qualitative evaluation, we see that STRAINER-10 is able to capture ridges and edges better and faster than compared to SIREN. ", "page_idx": 16}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/f37efaa132283eae0334991c883c5b32e219e6827e6c045792f1ee029c8299e2.jpg", "img_caption": ["Figure 11: In domain $4\\times$ super resolution using STRAINER . We see that STRAINER allows for faster convergence for in-domain super resolution making it useful especially for low time budgets. Max value achieved by STRAINER : $40.43d B$ while SIREN achieves $39.75d B$ . Within 500 iterations STRAINER achieves $>30d B$ PSNR "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/ca273aec89cd78b26ee9f71f056ab7fe58ccc53398edb146fa4aef4b38748835.jpg", "img_caption": ["Figure 12: We use ten shapes from the chair category of ShapeNet[7] to train STRAINER , and use that initialization to fit a much more complex volume (the Thai statue[35]). We compare the intermediate outputs for both STRAINER and SIREN for 150 iterations to highlight STRAINER \u2019s ability to learn ridges and high frequency information faster. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Offsetting Pre-activations ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/1e84b8d1847e673da9b8bb41cd8f77d36c41f38204f82eff8ce1f4a0399e592a.jpg", "img_caption": ["Figure 13: Siren offset "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/1b297950d32ce32e0c16d83a70061fbf5b9134ba8bede9279a88c467bf5027eb.jpg", "img_caption": ["Figure 14: Strainer offset "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "ABYdKpDb8p/tmp/e1d2324790c83e10e8624f5dd78b9ce4fd49e062c60f4c061801d747a9621bb9.jpg", "img_caption": ["Figure 15: Meta-learned 5K offset "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our goals and contributed are clearly stated in abstract and introduction. Our method addresses the goals and we include results to support them. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss limitations of our work in section 5.1 and section 5. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, all information about our models, training hyperparameters, dataset, and number of iterations are provided in our report. Further we also provide the hardware used and report figures such as train time to ensure further corroboration for successful reproducibility. We discuss how choice of data also plays a role in our method\u2019s performance. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes. We release the base code of our paper as a Google Colab notebook. We also provide our code flies in supplementary material. All data used in this paper belongs to existing open source datasets and have been correctly cited to ensure reproduction. Upon acceptance, we plan to clean and release our code base and share it on GitHub. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Careful treatment to our training method has been present in section 4 ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide our metrics with 1 std. deviation where needed. Our plots also include 1 std. deviation to indicate the variability of results accumulated. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Experiment compute resources are mentioned in Section 4 as well as supplementary material. ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification:We conform to the NeurIPS code of ethics. ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Discussed in section 6 in main submission. ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our methods do not entail a specific released model as it is applicable for fitting any arbitrary signal. We express caution for potential misuse in section 6 ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We cite all used resources such as implementations of baselines and data. We release our work with CC-By 4.0 license. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Details of dataset are provided in the Section 4 with correct citations. We also provide code as a Google Colab notebook as well as a python file for the central idea of our paper. While the python script allows to reproduce our intuition experiments, for detailed reproduction of the paper, please follow steps mentioned in Section 4. Please refer to attached zip file for assets. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Not applicable for our work. Paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Not applicable for our work. Paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}]