[{"figure_path": "ABYdKpDb8p/figures/figures_1_1.jpg", "caption": "Figure 1: STRAINER - Learning Transferable Features for Implicit Neural Representations. During training time (a), STRAINER divides an INR into encoder and decoder layers. STRAINER fits similar signals while sharing the encoder layers, capturing a rich set of transferrable features. At test-time, STRAINER serves as powerful initialization for fitting a new signal (b). An INR initialized with STRAINER's learned encoder features achieves (c) faster convergence and better quality reconstruction compared to baseline SIREN models.", "description": "This figure illustrates the STRAINER framework for learning transferable features in Implicit Neural Representations (INRs).  Panel (a) shows the training phase where multiple INRs are trained simultaneously, sharing the encoder layers but having independent decoder layers.  This shared encoder learns generalizable features from a set of similar input signals. Panel (b) demonstrates the test phase where the learned encoder from training is used to initialize a new INR for a previously unseen signal, significantly improving performance. Panel (c) presents a qualitative comparison of reconstruction results, showcasing that INRs initialized using STRAINER achieve faster convergence and superior quality compared to a standard SIREN model.", "section": "3 Methods"}, {"figure_path": "ABYdKpDb8p/figures/figures_2_1.jpg", "caption": "Figure 2: STRAINER learns faster. We show the reconstruction quality (PSNR) of different initialization schemes for in-domain image fitting on CelebA-HQ [22]. We compare SIREN [39] model initialized by (1) random weights (SIREN), (2) fitting on another face image (SIREN finetuned), (3) STRAINER -1 (trained using one face image), and (4) STRAINER-10 (trained using ten face images). We also evaluate against multiple baselines such as Meta-Learned 5K [45], TransINR[9], and IPC[23]", "description": "The figure compares the performance of different INR initialization methods for image fitting on the CelebA-HQ dataset.  It demonstrates that STRAINER, particularly STRAINER-10 (trained on 10 images), significantly outperforms other methods, including SIREN (random initialization), SIREN finetuned (initialized using another image), and meta-learning based methods like Meta-Learned 5K, TransINR, and IPC in terms of PSNR (Peak Signal-to-Noise Ratio) at different numbers of iterations, showcasing STRAINER's faster convergence and improved reconstruction quality.", "section": "4 Experiments"}, {"figure_path": "ABYdKpDb8p/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of learned features in STRAINER and baseline SIREN model. We visualize (a) the first principal component of the learned encoder features for STRAINER and corresponding layer for SIREN. At iteration 0, STRAINER's features already capture a low dimensional structure allowing it to quickly adapt to the cat image. High frequency detail emerges in STRAINER's learned features by iteration 50, whereas SIREN is lacking at iteration 100. The inset showing the power spectrum of the reconstructed image further confirms that STRAINER learns high frequency faster. We also show the (b) reconstructed images and remark that STRAINER fits high frequencies faster.", "description": "This figure compares the learned features of STRAINER and SIREN models at different iterations (0, 50, 100).  It shows that STRAINER learns low-dimensional structures quickly and captures high-frequency details faster than SIREN, as evidenced by the principal component analysis (PCA) of learned features and the power spectra of reconstructed images. The reconstructed images further illustrate STRAINER's superior performance in fitting high frequencies.", "section": "3.4 Learning an intermediate partition space in the shared encoder fo*"}, {"figure_path": "ABYdKpDb8p/figures/figures_5_1.jpg", "caption": "Figure 4: STRAINER converges to low and high frequencies fast. We plot the histogram of absolute gradients of layers 1, 5 and last over 1000 iterations while fitting an unseen signal. At test time, STRAINER\u2019s initialization quickly learns low frequency, receiving large gradients update at the start in its initial layers and reaching convergence. The Decoder layer in STRAINER also fits high frequency faster. Large gradients from corresponding SIREN layers show it learning significant features as late as 1000 iterations.", "description": "This figure compares the convergence speed of STRAINER and SIREN models when fitting an unseen signal.  The histograms of absolute gradients for three different layers (1, 5, and the last layer) are plotted over 1000 iterations. STRAINER shows faster convergence to both low and high frequencies than SIREN, highlighting its superior initialization.", "section": "4 Experiments"}, {"figure_path": "ABYdKpDb8p/figures/figures_8_1.jpg", "caption": "Figure 6: Visualizing density of partitions in input space of learned models. We use the method introduced in [20] to approximate the input space partition of the INR. We present the input space partitions for layers 2,3,4 across (a) Meta-learned 5K and STRAINER initialization and (b) at test time optimization. STRAINER learns an input space partitioning which is more attuned to the prior of the dataset, compared to meta learned which is comparatively more random. We also observe that SIREN (iii) learns an input space partitioning highly specific to the image leading to inefficient transferability for fitting a new image (iv) with significantly different underlying partitioned input space. This explains the better in-domain performance of STRAINER compared to Meta-learned 5K, as the shallower layers after pre-training provide a better input space subdivision to the deeper layers to further subdivide.", "description": "This figure visualizes how different methods partition the input space when learning INRs.  Panel (a) shows the initial partitioning of the input space for Meta-Learned 5K, STRAINER, and SIREN. Panel (b) shows how these partitions change after fitting a new image. STRAINER demonstrates a more structured and data-driven partitioning compared to the other methods, improving transferability to new signals.", "section": "Learning an intermediate partition space in the shared encoder fo*"}, {"figure_path": "ABYdKpDb8p/figures/figures_14_1.jpg", "caption": "Figure 7: Sharing different number of layers in STRAINER's encoder. We see that by increasing the number of shared layers, STRAINER's ability to recover the signal also improves.", "description": "This figure shows the impact of varying the number of shared layers in the STRAINER model on the reconstruction quality (PSNR).  The experiment uses a 5-layered SIREN model, fitting one image.  It compares a baseline SIREN model, a fine-tuned SIREN, and different versions of STRAINER, where STRAINER is modified to share an increasing number of initial layers (2, 3, 4, 5) while fitting an image. The results clearly demonstrate that increasing the number of shared layers leads to better reconstruction quality, as measured by PSNR. This highlights the importance of shared encoder layers in STRAINER's ability to learn transferable features and achieve faster and better signal reconstruction.", "section": "Suplementary Material / Appendix"}, {"figure_path": "ABYdKpDb8p/figures/figures_15_1.jpg", "caption": "Figure 1: STRAINER - Learning Transferable Features for Implicit Neural Representations. During training time (a), STRAINER divides an INR into encoder and decoder layers. STRAINER fits similar signals while sharing the encoder layers, capturing a rich set of transferrable features. At test-time, STRAINER serves as powerful initialization for fitting a new signal (b). An INR initialized with STRAINER's learned encoder features achieves (c) faster convergence and better quality reconstruction compared to baseline SIREN models.", "description": "This figure illustrates the STRAINER framework for learning transferable features in Implicit Neural Representations (INRs).  Panel (a) shows the training process where multiple INRs are trained simultaneously, sharing encoder layers while having separate decoder layers. This allows STRAINER to learn generalizable features from similar signals. Panel (b) demonstrates the testing phase where the pre-trained encoder is used to initialize a new INR for a previously unseen signal.  Panel (c) compares the reconstruction quality of an INR initialized with STRAINER's encoder against a baseline SIREN model, showcasing STRAINER's improved speed and accuracy.", "section": "3 Methods"}, {"figure_path": "ABYdKpDb8p/figures/figures_15_2.jpg", "caption": "Figure 2: STRAINER learns faster. We show the reconstruction quality (PSNR) of different initialization schemes for in-domain image fitting on CelebA-HQ [22]. We compare SIREN [39] model initialized by (1) random weights (SIREN), (2) fitting on another face image (SIREN finetuned), (3) STRAINER -1 (trained using one face image), and (4) STRAINER-10 (trained using ten face images). We also evaluate against multiple baselines such as Meta-Learned 5K [45], TransINR[9], and IPC[23]", "description": "The figure compares the performance of different INR initialization methods on a face image fitting task using PSNR metric. STRAINER, which uses a shared encoder trained on multiple images, shows significant improvement in reconstruction quality and convergence speed over baselines like SIREN (random initialization), SIREN finetuned (fine-tuned on a single image), and Meta-Learned 5K.", "section": "4 Experiments"}, {"figure_path": "ABYdKpDb8p/figures/figures_16_1.jpg", "caption": "Figure 10: Super Resolution using STRAINER. We show the reconstructed results (a) on the left using SIREN and STRAINER. We also plot (b) the trajectory of PSNR with iterations. STRAINER-10 (Fast) achieves comparable PSNR to SIREN in approximately a third of the runtime.", "description": "This figure shows a comparison of the super-resolution results achieved by SIREN and STRAINER.  The left side displays visual results of super-resolution on a sample image using both methods. The right side presents a plot showing how the Peak Signal-to-Noise Ratio (PSNR) changes over iterations. The plot illustrates that STRAINER-10(Fast) achieves similar performance to SIREN but requires significantly fewer iterations to reach the same PSNR value, showing its faster convergence for super-resolution tasks.  The HQ version of STRAINER-10 is also included for further comparison.", "section": "4.5 Inverse problems: super-resolution and denoising"}, {"figure_path": "ABYdKpDb8p/figures/figures_17_1.jpg", "caption": "Figure 2: STRAINER learns faster. We show the reconstruction quality (PSNR) of different initialization schemes for in-domain image fitting on CelebA-HQ [22]. We compare SIREN [39] model initialized by (1) random weights (SIREN), (2) fitting on another face image (SIREN finetuned), (3) STRAINER -1 (trained using one face image), and (4) STRAINER-10 (trained using ten face images). We also evaluate against multiple baselines such as Meta-Learned 5K [45], TransINR[9], and IPC[23]", "description": "The figure shows the PSNR (Peak Signal-to-Noise Ratio) of different INR (Implicit Neural Representation) initialization methods over training iterations for fitting in-domain face images from the CelebA-HQ dataset.  It compares the performance of a standard SIREN model, a fine-tuned SIREN model, STRAINER with 1 decoder, STRAINER with 10 decoders, and other meta-learning baselines (Meta-Learned 5K, TransINR, IPC). STRAINER, which shares encoder layers across multiple INRs during training, demonstrates faster convergence and superior PSNR compared to the other methods, highlighting the effectiveness of its transfer learning approach.", "section": "4 Experiments"}, {"figure_path": "ABYdKpDb8p/figures/figures_18_1.jpg", "caption": "Figure 12: We use ten shapes from the chair category of ShapeNet[7] to train STRAINER, and use that initialization to fit a much more complex volume (the Thai statue[35]). We compare the intermediate outputs for both STRAINER and SIREN for 150 iterations to highlight STRAINER\u2019s ability to learn ridges and high frequency information faster.", "description": "This figure compares the performance of STRAINER and SIREN in reconstructing a complex 3D model (Thai statue) after being trained on a simpler dataset (chair shapes from ShapeNet).  The ground truth, STRAINER reconstruction, and SIREN reconstruction are shown. Red circles highlight areas where STRAINER shows superior detail and faster convergence to high-frequency information, demonstrating the effectiveness of STRAINER's transferable features.", "section": "5 Discussion and Conclusion"}, {"figure_path": "ABYdKpDb8p/figures/figures_19_1.jpg", "caption": "Figure 3: Visualization of learned features in STRAINER and baseline SIREN model. We visualize (a) the first principal component of the learned encoder features for STRAINER and corresponding layer for SIREN. At iteration 0, STRAINER's features already capture a low dimensional structure allowing it to quickly adapt to the cat image. High frequency detail emerges in STRAINER's learned features by iteration 50, whereas SIREN is lacking at iteration 100. The inset showing the power spectrum of the reconstructed image further confirms that STRAINER learns high frequency faster. We also show the (b) reconstructed images and remark that STRAINER fits high frequencies faster.", "description": "This figure visualizes the learned features of STRAINER and compares them to a baseline SIREN model.  The PCA of learned features highlights how STRAINER captures low-dimensional structure early in training, enabling quicker adaptation to new signals.  The power spectrum of reconstructed images confirms that STRAINER learns high frequencies faster than SIREN.  Reconstructed images further illustrate STRAINER's superior performance.", "section": "3.4 Learning an intermediate partition space in the shared encoder f\u03b8*"}, {"figure_path": "ABYdKpDb8p/figures/figures_20_1.jpg", "caption": "Figure 1: STRAINER - Learning Transferable Features for Implicit Neural Representations. During training time (a), STRAINER divides an INR into encoder and decoder layers. STRAINER fits similar signals while sharing the encoder layers, capturing a rich set of transferrable features. At test-time, STRAINER serves as powerful initialization for fitting a new signal (b). An INR initialized with STRAINER's learned encoder features achieves (c) faster convergence and better quality reconstruction compared to baseline SIREN models.", "description": "This figure illustrates the STRAINER framework for learning transferable features in Implicit Neural Representations (INRs).  Panel (a) shows the training phase where multiple INRs share encoder layers while having independent decoder layers.  This shared encoder learns transferable features across signals. Panel (b) demonstrates the test phase where the learned encoder from (a) is used to initialize a new INR for a new signal, resulting in faster convergence. Panel (c) shows the result of the improved reconstruction quality achieved by using STRAINER compared to a baseline method (SIREN).", "section": "3 Methods"}, {"figure_path": "ABYdKpDb8p/figures/figures_21_1.jpg", "caption": "Figure 6: Visualizing density of partitions in input space of learned models. We use the method introduced in [20] to approximate the input space partition of the INR. We present the input space partitions for layers 2,3,4 across (a) Meta-learned 5K and STRAINER initialization and (b) at test time optimization. STRAINER learns an input space partitioning which is more attuned to the prior of the dataset, compared to meta learned which is comparatively more random. We also observe that SIREN (iii) learns an input space partitioning highly specific to the image leading to inefficient transferability for fitting a new image (iv) with significantly different underlying partitioned input space. This explains the better in-domain performance of STRAINER compared to Meta-learned 5K, as the shallower layers after pre-training provide a better input space subdivision to the deeper layers to further subdivide.", "description": "This figure visualizes how different models partition the input space at initialization and after fitting an image.  STRAINER shows a more structured and data-driven partitioning compared to Meta-Learned 5K and SIREN. This structured partitioning explains its better transferability and faster convergence.", "section": "3.4 Learning an intermediate partition space in the shared encoder fo*"}]