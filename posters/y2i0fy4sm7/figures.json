[{"figure_path": "Y2I0Fy4sm7/figures/figures_2_1.jpg", "caption": "Figure 1: Mechanism comparison between canonical approach and SpeedLoader. Left, canonical approach load the whole model every time \u2461 loads a single batch; Right, SpeedLoader feed every layer with multiple batches and exchange activations to and from host memory (\u2460). Therefore, for one full model loading(\u2461), SpeedLoader can process multiple batches.", "description": "The figure illustrates the difference in data flow between a canonical approach and the proposed SpeedLoader method for processing mini-batches in a neural network. The canonical approach loads the entire model twice per batch, while SpeedLoader processes multiple sub-batches within a single model load and gradient synchronization, minimizing I/O overhead and improving efficiency. The left panel shows the canonical approach, and the right panel shows the SpeedLoader approach.", "section": "3 SpeedLoader"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_3_1.jpg", "caption": "Figure 2: Device-host communication breakdown of SpeedLoader.", "description": "This figure illustrates the data flow and communication between the device (GPU) and host (CPU) memory in the SpeedLoader framework. The left side shows the forward propagation, where multiple sub-batches are processed iteratively.  Activations are offloaded to the host memory to free up GPU memory and prefetching is performed to overlap computation and communication. The right side shows the backward propagation, where gradients are accumulated and exchanged between device and host in a similar overlapped manner. This optimized data flow minimizes redundant communication overhead and improves efficiency.", "section": "3.2 Tensor Exchange Manager"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_4_1.jpg", "caption": "Figure 3: Reorganizing buffer memory to ensure continuity.", "description": "This figure illustrates how SpeedLoader reorganizes the buffer memory to ensure continuity during the generation process.  The left side shows the original organization of data in the memory buffer, where each row represents a sample and each column represents a token. The right side shows the reorganized buffer after SpeedLoader's reshape operation.  This reorganization ensures that the KV cache for each sub-batch is maintained throughout the generation process, preventing unnecessary data movement and maximizing efficiency.", "section": "3 SpeedLoader"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_6_1.jpg", "caption": "Figure 4: Hyperparameter Analysis. a, the dynamics between throughput, GPU memory consumption and hyperparameters, where FP and BP denotes forward and backward propagation, respectively; Ratio in panel b refers to the ratio between sub-batch size and number of sub-batches; c, total tokens' effect on host memory usage.", "description": "This figure analyzes the impact of various hyperparameters on the performance of SpeedLoader.  Panel (a) shows the relationship between throughput, GPU memory usage, sub-batch size, number of sub-batches, and the number of live blocks during both forward (FP) and backward (BP) propagation. Panel (b) illustrates the relationship between throughput and the ratio of sub-batch size to the number of sub-batches.  Finally, panel (c) demonstrates how host memory usage scales with the total number of tokens processed.", "section": "5.1 Impacts of Hyperparameters"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_7_1.jpg", "caption": "Figure 5: Profiling results. (a) Deepspeed (w/ offloading); (b) SpeedLoader (unsaturated-computing, w/ offloading); (c) SpeedLoader (saturated-computing, w/ offloading); (d) Deepspeed (w/o offloading); (e) SpeedLoader (w/o offloading)", "description": "This figure presents a profiling comparison of DeepSpeed and SpeedLoader, both with and without offloading, under different computing saturation levels (unsaturated and saturated).  Each sub-figure (a-e) shows a pie chart breakdown of the time spent on various operations: kernel computations, memory copy operations (memcpy), CPU execution, other miscellaneous operations, reduce-scatter operations, all-gather operations, and arithmetic operations. The chart visually highlights how SpeedLoader significantly reduces time spent on memory operations (especially memcpy) and increases time dedicated to arithmetic computations, resulting in improved efficiency.", "section": "5.2 Enhanced Arithmetic Intensity"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_8_1.jpg", "caption": "Figure 6: Training MFU comparison. Left, Single device benchmarking w/ offloading; middle, Distributed benchmarking w/ offloading; right, Distributed benchmarking w/o offloading.", "description": "This figure compares the Model FLOPs Utilization (MFU) of SpeedLoader and DeepSpeed under different training settings: single device with offloading, distributed with offloading, and distributed without offloading.  The x-axis represents the model size (7B, 13B, and 70B parameters), and the y-axis shows the MFU percentage. The bars show that SpeedLoader consistently achieves higher MFU than DeepSpeed across all settings, indicating that SpeedLoader is more computationally efficient.", "section": "5 Results"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_8_2.jpg", "caption": "Figure 7: Inference throughput benchmarking on single device.", "description": "This figure compares the inference throughput (tokens per second) of four different approaches: vLLM, DeepSpeed, FlexGen, and SpeedLoader, across three different model sizes: 6.7B, 30B, and 175B parameters.  SpeedLoader demonstrates significantly higher throughput compared to the other methods, especially for larger models, highlighting its efficiency in inference tasks.", "section": "5 Results"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_9_1.jpg", "caption": "Figure 8: Weak scaling of SpeedLoader. Showing per device throughput.", "description": "This figure shows the results of weak scaling experiments conducted on SpeedLoader. Weak scaling tests the performance of a system as the number of processors is increased while keeping the problem size per processor constant.  The x-axis represents the number of GPUs used, and the y-axis represents the per-device throughput (tokens processed per second per GPU). Different lines represent different sizes of the LLaMA-2 model (7B, 13B, and 70B parameters).  The plot shows the throughput achieved for both forward propagation (FP) and backward propagation (BP). The data illustrates how the throughput per GPU changes as more GPUs are added to the system.", "section": "5 Results"}, {"figure_path": "Y2I0Fy4sm7/figures/figures_15_1.jpg", "caption": "Figure 10: Training MFU ablation study with FlashAttention-2.", "description": "This figure shows the ablation study of the model FLOPs utilization (MFU) with and without using FlashAttention-2 in both distributed and no-offload settings.  The results are shown for different model sizes (7B, 13B, and 70B).  It demonstrates the impact of FlashAttention-2 on improving the computational efficiency of the SpeedLoader approach.", "section": "5 Results"}]