[{"figure_path": "paobkszgIA/figures/figures_1_1.jpg", "caption": "Figure 1: Our model demonstrates enhanced robustness compared to TPS [36] in semantic segmentation tasks under foggy and snowy conditions. It notably excels by significantly reducing inaccuracies in the segmented areas.", "description": "This figure showcases the results of semantic segmentation under adverse weather conditions (foggy and snowy). It compares the performance of the proposed model against TPS [36], a state-of-the-art method. The images demonstrate that the proposed model significantly improves segmentation accuracy by reducing inaccuracies, especially in challenging weather conditions.", "section": "1 Introduction"}, {"figure_path": "paobkszgIA/figures/figures_3_1.jpg", "caption": "Figure 2: Our network comprises two pipelines: the source and the target. (a) Target Pipeline: The upper teacher (temporal) takes both the current and adjacent frames to create temporal pseudo-labels. The student, on the other hand, receives a cropped segment of the current frame and a complete adjacent frame, with a loss function enforcing its predictions align with the temporal teacher. The lower teacher (spatial) uses the same segment as the student, but from the original image and at a higher resolution. Similarly, a consistency loss is applied to make the student\u2019s predictions consistent with the spatial teacher\u2019s pseudo-labels. (b) Source Pipeline: The student model undergoes supervised learning with consecutive frames as inputs. (c) Fusion Block: This component integrates multiple offset layers, which adjust pixels from adjacent frames relative to the current frame, and convolutional layers to merge these pixels.", "description": "This figure illustrates the architecture of the proposed method, which consists of two pipelines: the source pipeline and the target pipeline. The source pipeline is used for supervised training on synthetic data, while the target pipeline is used for unsupervised domain adaptation on real-world data. The fusion block is a key component of the proposed method, which merges information from adjacent frames to improve the accuracy of semantic segmentation. The temporal-spatial teacher-student learning framework further enhances the performance of the proposed method.", "section": "3 Proposed method"}, {"figure_path": "paobkszgIA/figures/figures_4_1.jpg", "caption": "Figure 3: An illustration of optical flows generated using a pretrained FlowNet2 model [27]. The optical flows are generated by utilizing information from the corresponding frame and its previous frame. The left two columns display frames and optical flows under ideal conditions, while the right two columns depict frames and optical flows under adverse weather conditions, with nighttime as an illustrative example. Under ideal conditions, the optical flows accurately capture vehicle details, traffic signs, and poles. In contrast, optical flows under nighttime conditions exhibit significant failures, with missed detection of the middle poles, and erroneous predictions for the bus.", "description": "This figure compares optical flow predictions from the FlowNet2 model under ideal and adverse weather conditions.  The left side shows accurate optical flow under good conditions, clearly showing movement of vehicles and other objects. The right side shows inaccurate and incomplete optical flow under adverse (nighttime) conditions, demonstrating the unreliability of optical flow in such scenarios, motivating the paper's optical-flow-free approach.", "section": "3.1 End-to-end training with fusion block"}, {"figure_path": "paobkszgIA/figures/figures_6_1.jpg", "caption": "Figure 4: This illustration demonstrates the temporal weather degradation augmentation technique. For enhanced visualization, we have utilized Cityscapes-Seq as an example. Frames (a) and (b) are consecutive frames captured from a real-world scene under ideal conditions. Frames (c) and (d) show the same frames, but with applied augmentation, including random noise, a moving glare, a rectangle \"foggy\" area with intensity change, and a changing illumination.", "description": "This figure shows an example of the temporal weather degradation augmentation technique used in the paper.  It uses Cityscapes-Seq dataset to illustrate how the augmentation affects consecutive frames, simulating realistic adverse weather conditions like fog, glare, and varying illumination.  Frames (a) and (c) show original frames, while frames (b) and (d) depict the same frames after applying the augmentations.", "section": "3.3 Temporal weather degradation augmentation"}]