{"references": [{"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "This paper introduces the Masked Autoencoder (MAE) framework, a highly influential self-supervised learning method that heavily inspired the design of the proposed M2CRL architecture."}, {"fullname_first_author": "Zhan Tong", "paper_title": "VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training", "publication_date": "2022-12-01", "reason": "VideoMAE is a crucial foundation for extending masked image modeling to the video domain, directly relevant to the task of endoscopic video analysis addressed in the current work."}, {"fullname_first_author": "Jean-Bastien Grill", "paper_title": "Bootstrap your own latent - A new approach to self-supervised learning", "publication_date": "2020-12-01", "reason": "This paper introduces the Bootstrap Your Own Latent (BYOL) method, a significant advancement in contrastive self-supervised learning, whose principles underlie a key component of M2CRL."}, {"fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-10-01", "reason": "This paper explores self-supervised learning in vision transformers, a significant architectural element within the M2CRL framework, influencing the model's design and performance."}, {"fullname_first_author": "Zhao Wang", "paper_title": "Foundation model for endoscopy video analysis via large-scale self-supervised pre-train", "publication_date": "2023-10-01", "reason": "This paper is highly relevant as it introduces Endo-FM, a state-of-the-art self-supervised pre-training method for endoscopic video analysis, serving as a direct comparison point for evaluating the proposed M2CRL."}]}