[{"figure_path": "TGC7HNf6nK/tables/tables_6_1.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents a comparison of different in-context learning (ICL) methods for image captioning (IC) and visual question answering (VQA) tasks.  It compares the performance of Random Sample (RS), Similarity-based Retrieval methods (SITR, SIIR, STTR), and the proposed Lever-LM approach.  The results are shown for different shot numbers (1-shot to 8-shot) and are broken down into interpolation (shorter shot numbers than training data) and extrapolation (longer shot numbers than training data).  The comparison highlights Lever-LM's superior performance across various scenarios and model types (OpenFlamingo and IDEFICS).", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_7_1.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents a comparison of various in-context learning (ICL) methods for image captioning (IC) and visual question answering (VQA) tasks.  The methods are evaluated using two large vision-language models (LVLMs): Open-Flamingo (OF) and IDEFICS (IDE).  The results show the performance of each method across different numbers of in-context demonstrations (shots).  The table includes results for the proposed Lever-LM method, trained on a dataset of 2-shot in-context demonstrations, as well as several baseline methods including random sampling (RS) and similarity-based retrieval methods.  Lever-LM consistently outperforms other methods across various scenarios.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_7_2.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents the results of various In-context learning (ICL) methods on two vision-language tasks: Image Captioning (IC) and Visual Question Answering (VQA).  It compares the performance of Random Sample (RS), Similarity-based Image-Image Retrieval (SIIR), Similarity-based Text-Text Retrieval (STTR), Similarity-based Image-Text Retrieval (SITR), and the proposed Lever-LM method. The results are shown separately for different numbers of shots (1-8) for both short (interpolation) and long (extrapolation) ICD sequences, and are broken down by model used (OpenFlamingo and IDEFICS).  The Lever-LM model was trained using a dataset with a 2-shot In-context demonstration (ICD).", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_8_1.jpg", "caption": "Table 4: Results of Random Order of Lever-LM generated ICDs.", "description": "This table presents the results of an experiment where the order of In-context demonstrations (ICDs) generated by Lever-LM was randomized.  It compares the performance of the original, ordered ICDs to those with a randomized order, using both 2-shot and 4-shot demonstration settings on Visual Question Answering (VQA) and Image Captioning (IC) tasks.  The results highlight the impact of ICD ordering on the performance of Lever-LM.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_15_1.jpg", "caption": "Table 5: Different settings of Lever-LM experiments, where the n is the number of anchor samples in A, b is the beam size, and l is the length of ICD configurations.", "description": "This table presents the hyperparameters used in the Lever-LM experiments.  It shows various configurations used for training, including the learning rate, weight decay, number of epochs, whether CLIP model was frozen, and if an adapter was used.  It also shows the parameters of the DM dataset for the different configurations, such as the number of anchor samples, beam size, and length of the ICD configurations. The table is divided into two sections based on the task: Image Captioning (IC) and Visual Question Answering (VQA). Each row represents a different experimental setup for Lever-LM training and evaluation.", "section": "A.1 Lever-LM Hyperparameters"}, {"figure_path": "TGC7HNf6nK/tables/tables_16_1.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table compares the performance of different in-context learning (ICL) methods on Image Captioning (IC) and Visual Question Answering (VQA) tasks.  The methods compared include Random Sample (RS), Similarity-based Image-Image Retrieval (SIIR), Similarity-based Text-Text Retrieval (STTR), Similarity-based Image-Text Retrieval (SITR), and the proposed Lever-LM. The results are shown separately for two different large vision-language models (LVLMs): OpenFlamingo (OF) and IDEFICS (IDE).  The table presents results for different numbers of in-context demonstrations (1-shot, 2-shot, 3-shot, 4-shot, 6-shot, and 8-shot), categorized as interpolation (shorter than training data), extrapolation (longer than training data) and average performance.  Lever-LM is trained using in-context demonstrations of length 2.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_16_2.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents a comparison of different In-Context Learning (ICL) methods for Image Captioning (IC) and Visual Question Answering (VQA) tasks.  It compares the performance of Random Sample (RS), Similarity-based Image-Image Retrieval (SIIR), Similarity-based Text-Text Retrieval (STTR), Similarity-based Image-Text Retrieval (SITR), and the proposed Lever-LM method.  The results are broken down by the number of shots (1-8) and show the performance of each method on both OpenFlamingo (OF) and IDEFICS (IDE) Large Vision Language Models (LVLMs).  Lever-LM uses a smaller language model to generate effective in-context demonstration (ICD) sequences to improve the LVLMs' performance.  The table shows interpolation (shorter ICDs than training data) and extrapolation (longer ICDs than training data) abilities of Lever-LM.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_16_3.jpg", "caption": "Table 7: Results of diverse DM configurations on IC and VQA.", "description": "This table presents the results of various methods for constructing the training dataset (DM) for Lever-LM. The methods are compared on Image Captioning (IC) and Visual Question Answering (VQA) tasks, with different configurations of the number of samples (n), beam size (b), and the length of ICD sequences (l). The comparison metric is the average CIDEr score across 1 to 8 shots (Avg:1~8) for IC and the average accuracy across 1 to 8 shots (Avg:1~8) for VQA. The results show the performance of Lever-LM under different DM configurations, including interpolation (Avg:1~2) and extrapolation (Avg:3~8) capabilities.", "section": "4.3 Ablation Studies"}, {"figure_path": "TGC7HNf6nK/tables/tables_16_4.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents the performance comparison of different In-Context Learning (ICL) methods on Image Captioning (IC) and Visual Question Answering (VQA) tasks.  The methods include Random Sampling (RS), Similarity-based Image-Image Retrieval (SIIR), Similarity-based Text-Text Retrieval (STTR), Similarity-based Image-Text Retrieval (SITR), and the proposed Lever-LM. Results are shown for different numbers of shots (1-8), with separate results for interpolation (using fewer shots than the Lever-LM training data) and extrapolation (using more shots than the training data).  The table highlights the superior performance of Lever-LM across both tasks and various shot configurations.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_17_1.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents a comparison of different in-context learning (ICL) methods for image captioning (IC) and visual question answering (VQA) tasks.  The methods compared are Random Sample (RS), Similarity-based Image-Image Retrieval (SIIR), Similarity-based Text-Text Retrieval (STTR), Similarity-based Image-Text Retrieval (SITR), and Lever-LM.  Results are shown for both OpenFlamingo (OF) and IDEFICS (IDE) large vision-language models (LVLMs). The table shows performance across different numbers of in-context demonstrations (shots), highlighting both interpolation (using fewer shots than in training) and extrapolation (using more shots than in training) capabilities. Lever-LM is trained on a dataset with 2-shot in-context demonstrations.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_17_2.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents a comparison of different in-context learning (ICL) methods for image captioning (IC) and visual question answering (VQA) tasks using two large vision-language models (LVLMs): OpenFlamingo and IDEFICS. The methods compared include random sampling (RS), similarity-based retrieval methods (SITR, SIIR, STTR), and the proposed Lever-LM method.  Results are shown for various numbers of shots (1-8), broken into interpolation and extrapolation.  The Lever-LM model utilizes a smaller language model to generate effective in-context demonstration (ICD) sequences, and the table shows the performance gains achieved by this approach compared to the baseline methods.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_17_3.jpg", "caption": "Table 12: CIDEr results of a fixed random ICD sequences and Golden-Set on Image Captioning with OpenFlamingo.", "description": "This table presents the CIDEr scores achieved by different methods for image captioning using the OpenFlamingo model.  It compares the performance of three randomly generated fixed sets of in-context demonstrations (ICDs) against a 'Golden-Set' of ICDs, which is an optimal sequence identified by the Lever-LM.  The results are separated into interpolation (Avg:1~2), extrapolation (Avg:3~8), and overall (Avg:1~8) scores, showing the Golden-Set's superior performance across all metrics.  This highlights the effectiveness of the Lever-LM in generating high-quality ICD sequences for improved image captioning.", "section": "B.2 CIDEr Results of Fixed Random ICD Sequence on Image Captioning"}, {"figure_path": "TGC7HNf6nK/tables/tables_17_4.jpg", "caption": "Table 1: Results of diverse ICL methods on IC and VQA, where \"OF\" and \"IDE\" denote Open-Flamingo and IDEFICS, respectively. Lever-LM is trained by DM whose ICD length is set to 2.", "description": "This table presents a comparison of various In-Context Learning (ICL) methods' performance on Image Captioning (IC) and Visual Question Answering (VQA) tasks.  The methods compared include Random Sample (RS), Similarity-based Image-Image Retrieval (SIIR), Similarity-based Text-Text Retrieval (STTR), Similarity-based Image-Text Retrieval (SITR), and the proposed Lever-LM.  Performance is measured across different numbers of in-context demonstrations (shots) for both interpolation (fewer shots than training) and extrapolation (more shots than training).  OpenFlamingo (OF) and IDEFICS (IDE) are the two large vision-language models (LVLMs) used for evaluation.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_18_1.jpg", "caption": "Table 14: CIDEr results of different Lever-LM sizes in Image Captioning with IDEFICSv1.", "description": "This table presents the CIDEr scores achieved by Lever-LM models of varying sizes (1-layer, 2-layer, and 4-layer Transformers) on the image captioning task using the IDEFICSv1 model.  The results are categorized by the average number of shots (Avg:1~2, Avg:4~8, and Avg:1~8), indicating the performance across different in-context learning scenarios.", "section": "B.4 CIDEr Results of Different Lever-LM Sizes in Image Captioning"}, {"figure_path": "TGC7HNf6nK/tables/tables_18_2.jpg", "caption": "Table 15: Accuracy results of diverse ICL methods on SST2 with Qwen1.5-1.8B.", "description": "This table presents the accuracy results of different in-context learning (ICL) methods on the SST2 dataset using the Qwen1.5-1.8B model.  The methods compared are Random Sample (RS), Similarity-based Text-Text Retrieval (STTR), and the proposed Lever-LM. The accuracy is shown for different shot numbers (Avg:1~2 represents the average accuracy for one and two-shot settings, Avg:4~8 is the average accuracy for four to eight shots, and Avg:1~8 is the average accuracy for one to eight shots).  This table demonstrates the performance of Lever-LM compared to other approaches, particularly its ability to maintain high accuracy with a greater number of shots.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_18_3.jpg", "caption": "Table 16: The inference time of different ICL methods with IDEFICSv1.", "description": "This table presents the inference time taken by different In-context Learning (ICL) methods when using the IDEFICSv1 model.  It compares the time taken by Similarity-based Image-Image Retrieval (SIIR) and Lever-LM, showing that Lever-LM's improvement in performance does not come at a significant cost in terms of inference time. The values represent the retrieval time in seconds for each method.", "section": "4.2 Results and Analyses"}, {"figure_path": "TGC7HNf6nK/tables/tables_19_1.jpg", "caption": "Table 17: Results of diverse methods on two tasks from VL-ICL benchmark with IDEFICSv1.", "description": "This table presents the results of different In-context Learning (ICL) methods on two tasks from the VL-ICL benchmark using the IDEFICSv1 model.  The methods compared include Random Sample (RS), Similarity-based Image-based Retrieval (SIIR), and the proposed Lever-LM. The results are shown for average performance across 1-2 shots and 4-8 shots, as well as the overall average across all shots.  The tasks are VL-ICL CLEVR and VL-ICL OCRText.", "section": "4.1 Datasets and implementation details"}, {"figure_path": "TGC7HNf6nK/tables/tables_19_2.jpg", "caption": "Table 18: CIDEr score of diverse ICL methods on Image Captioning with IDEFICSv2-8B", "description": "This table shows the CIDEr scores achieved by different in-context learning (ICL) methods on the image captioning task using the IDEFICSv2-8B model.  The methods compared include Random Sampling (RS), Similarity-based Image-Image Retrieval (SIIR), and the proposed Lever-LM approach. The scores are presented for different shot settings (Avg:1~2 and Avg:3~4) to evaluate performance with varying numbers of in-context demonstrations. Lever-LM consistently outperforms other methods across different shot settings, demonstrating its ability to effectively configure in-context demonstrations for improved performance.", "section": "4.2 Results and Analyses"}]