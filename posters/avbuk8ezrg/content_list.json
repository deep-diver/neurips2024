[{"type": "text", "text": "NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziquan Wei University of North Carolina at Chapel Hill Chapel Hill, NC 27599 ziquanw@eamil.unc.edu ", "page_idx": 0}, {"type": "text", "text": "Tingting Dan University of North Carolina at Chapel Hill Chapel Hill, NC 27599 tingting_dan@med.unc.edu ", "page_idx": 0}, {"type": "text", "text": "Jiaqi Ding University of North Carolina at Chapel Hill Chapel Hill, NC 27599 jiaqid@cs.unc.edu ", "page_idx": 0}, {"type": "text", "text": "Guorong Wu University of North Carolina at Chapel Hill Chapel Hill, NC 27599 guorong_wu@med.unc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Although modern imaging technologies allow us to study connectivity between two distinct brain regions in-vivo, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits. However, the absence of neuroscience insight in the current approaches poses significant challenges in understanding cognitive behavior from transient neural activities. To address this challenge, we put the spotlight on the coupling mechanism of structural connectivity (SC) and functional connectivity (FC) by formulating such network neuroscience question into an expressive graph representation learning problem for high-order topology. Specifically, we introduce the concept of topological detour to characterize how a ubiquitous instance of FC (direct link) is supported by neural pathways (detour) physically wired by SC, which forms a cyclic loop interacted by brain structure and function. In the clich\u00e9 of machine learning, the multi-hop detour pathway underlying SC-FC coupling allows us to devise a novel multi-head self-attention mechanism within Transformer to capture multi-modal feature representation from paired graphs of SC and FC. Taken together, we propose a biological-inspired deep model, coined as NeuroPath, to find putative connectomic feature representations from the unprecedented amount of neuroimages, which can be plugged into various downstream applications such as task recognition and disease diagnosis. We have evaluated NeuroPath on large-scale public datasets including Human Connectome Project (HCP) and UK Biobank (UKB) under different experiment settings of supervised and zero-shot learning, where the state-of-the-art performance by our NeuroPath indicates great potential in network neuroscience. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The seek of meaningful feature representations for graph topology has been extensively investigated [44, 28, 34], with widespread applications in reasoning path [16] and cycle basis [61] for knowledge graph, as well as neural fingerprint [17], junction tree autoencoder [29] and cellular Weisfeiler Leman (WL) testing [7] for molecular substructure encoding. Moreover, graph data in the realm of neuroscience research demands feature representation bearing additional neuroscience insight which is supposed to underline particular neurobiological mechanisms of interest. For example, substructure embedding [36, 31] and snapshot embedding [53, 3] have been proposed to characterize the phenotypic traits from structural connectivity (SC), which is a static graph and physically wired by neuronal fibers, and functional connectivity (FC) in the brain, which is a dynamic graph and supported by neural circuit overlaid on SC topology [2]. Nevertheless, the graph learning approach of physical neural pathways of SC coupled with FC is rare in related works [4, 13, 39]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The advancement of diffusion-weighted imaging (DWI) technology enables the in-vivo measurement of physical region-to-region connections through neuronal fibers (aka. SC). Multiple lines of neuroscience finding suggest that high-level cognition and behavior emerge from the remarkable SC-FC coupling mechanism, making the in-depth understanding of the interplay between SC and FC become the gateway to reverse engineering human mind [52, 5]. To that end, a plethora of computational models have been proposed over the past decade, including biophysical models [25, 58], graph harmonics [46, 38], network communications [19], multivariate statistical technique [41], and deep learning-based structure-function mapping [10, 49, 45]. However, the complex relationship between structural and functional connectivity is still elusive [14], particularly evidenced by the lack of direct physical pathways (formed by SC in Fig. 1 orange box) between two brain regions that exhibit functional co-activation (indicated by FC in Fig. 1 green box) [24, 42, 6]. Although the topology of SC does not necessarily always aligned with the counterpart of FC motivating previous related works, there is a converging consensus that each FC instance is supported by a sub-graph of SC, as shown in Fig. 1 orange box, where blue links constructing a path sub-graph represent the neural pathway that physically supports the red link. ", "page_idx": 1}, {"type": "text", "text": "We strike on this outstanding neuroscience question by lifting the concept of univariate coupling, which is limited to the correspondence between a direct link of SC and another direct link of FC, to a new paradigm of multivariate SC-FC coupling mechanism that models how the ubiquitous FC instance is associated with the detour pathway on SC topology. As shown in green box of Fig. 1, the red line represents the direct FC link between region $\\#1$ and $\\#2$ while the collection of blue lines indicates the corresponding detour pathway of SC. Although the whole-brain topology remains unchanged as brain function evolves, the functional co-activation over time is dynamically supported by different neural pathways of SC. Note, the direct FC link and SC detour form a cyclic loop inherited from both SC and FC topology. Such conceptualization is supported by various neuroscience findings that synchronization of neural activity between two brain regions is fundamentally supported by the underlying neural circuitry established by structural connectivities [23, 24, 21]. ", "page_idx": 1}, {"type": "text", "text": "In the perspective of machine learning, as shown in the middle of Fig. 1, the overarching goal is to establish a mapping function between neuroimaging data (SC only, FC only, or both) to the phenotypic traits (such as cognitive tasks). Due to the complex nature of human cognition, however, current deep models encounter several significant challenges. First, it is less common to use the static SC only for the prediction of evolving cognitive states, largely due to the ill-posed setting which boils down to a one-to-many mapping [50]. Alternatively, tremendous efforts have been made to predict cognitive states based on time series of neural activity [3] and FC topology [4]. However, it has been frequently reported that massive inter-subject variations of FC often predominate the intrinsic task-relevant patterns, such variations in real-world data are demonstrated in Sec. 2.1. Thus, current learning-based approaches have difficulties in scaling up to large-size population data. Furthermore, limiting consideration to either SC or FC alone fails to capture the holistic nature of the brain as a dynamic system, resulting in existing approaches having limited power to uncover new insight of complex relationship between SC and FC [39]. In this regard, it is natural to combine the information of SC and FC in a multi-modality learning scenario [51, 30, 57]. In general, current research focuses on information fusion at node or link level. Few attention has been paid to integrating SC and FC by combining the power of machine learning and insight of cognitive neuroscience. ", "page_idx": 1}, {"type": "text", "text": "Taken together, we propose a novel deep model, coined NeuroPath, to (1) enhance the machine performance of predicting cognitive status using both SC and FC information and (2) uncover new neuroscience underpinning of SC-FC coupling mechanism. In a nutshell, we conceptualize that the effective way to advance our current understanding of cognitive neuroscience is to characterize the latent SC-FC coupling mechanism from neuroimaging data. To land this conceptualization into an end-to-end deep model, we put the spotlight on devising a new multi-head self-attention (MHSA) component by capitalizing on the multivariate SC-FC coupling mechanism. As shown in Fig. 1 grey box, nodes of multiple paths detouring an FC link on SC are transformed respectively by multiple heads of self-attention in one layer. On top of this, we learn putative feature representations from the cyclic loop capturing complete information on how the underlying functional fluctuations are supported by the neural pathway (aka. SC detour) with different lengths. ", "page_idx": 1}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/e5ebceb3423acb5f70170828e9a443d06515c710b2d303ad8a24c210608488cf.jpg", "img_caption": ["Figure 1: Planting a novel multivariate SC-FC coupling mechanism to explainable deep model. Orange box: The structural connectivity (SC) denoted by grey links represents the strength of neurological fiber that physically connects two brain regions. SC is relatively static given the neural activities are transient, e.g. cognitive tasks. Green box: The functional connectivity (FC) is commonly considered as the brain network topology [48] since SC is static for different cognitive tasks. The overlapping area of orange and green boxes: the multivariate SC-FC coupling mechanism, where a neural pathway (detour) is constructed by multiple SC links to support one FC link. Grey box: NeuroPath Transformer using a new MHSA module filtered by adjacency matrices emits the representation of multi-hop detours. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our major contribution has three folds. First, we present a novel multivariate SC-FC coupling mechanism that allows us to develop an explainable deep model with great neuroscience insight and guarantee from graph theory. Second, we present a Transformer-based deep model that is scalable to utilize existing large population of neuroimaging data. Third, we have validated our NeuroPath on large-scale public datasets with a total of 10,886 fMRI scans including Human Connectome Project (HCP), UK Biobank (UKB), Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI), and Open Access Series of Imaging Studies (OASIS). We have evaluated (1) the accuracy for predicting phenotypic traits (cognitive status in healthy brains and disease risk for aging brains) (2) model interpretability in identifying latent neural pathways, and (3) the clinical value of our model in terms of robustness and zero-shot learning, where promising results suggests potential application in computational neuroscience field. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Motivations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Massive inter-subject variations of FC often predominate the intrinsic task-relevant patterns. Take the HCP data [9] as an example in which the data heterogeneity issue has been harmonized by using standardized imaging protocol. As shown in Fig. 2, we first identify a set of brain regions which exhibit significant difference between resting stage and VISMOTOR tasking [9] in a paired $t$ -test using detour degree and FC degree1, respectively, where the identified regions are colored based on $p$ -values. Next, we examine the location of these regions in each functional sub-networks (manually defined using neuroscience domain knowledge). For comparison, we display the overlap ratio (y-axis) between the number identified regions and the total number of regions in each subnetwork ( $\\mathbf{\\dot{X}}$ -axis) at the bottom of Fig. 2. Current neuroscience findings suggest that resting state is relevant to default mode network (red circle) while VISMOTOR task is associated with visual (orange circle) and sensorimotor areas (green circle). In this regard, the detour degree achieves overall higher discrimination power than FC to the extent that most of identified regions are aligned with neuroscience finding with much less false positives in other non-relevant regions. The same findings are observed on diverse populations by gender in Appendix Fig. 8. The evidence shown in Fig. 2 underscores the importance of including SC detour in finding putative functional biomarkers, prompting us to further integrate the SC-FC coupling mechanism into the design of NeuroPath. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.2 Relevant Machine Learning Work for Brain Connectomes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Tremendous efforts have been made to perform graph learning on human connectome data. The most representative work includes graph convolutional network (GCN [33, 36]) and transformerbased deep models [3, 31]. Recently, most of popular GNN models and their application in neuroscience have been reviewed in [4]. ", "page_idx": 3}, {"type": "text", "text": "Inspired by the great success of Transform [56] in NLP and CV, various graph transformer models have been proposed in graph learning. For example, GraphGPS [47] and TokenGT [32] models use either a structure encoding or token of graph elements as the initial graph feature representations. Furthermore, a self-attention gating mechanism is proposed in [26] using an additional branch of edge embedding to implement the augmentation of self-attention. By integrating original node features and structural embeddings into an augmented self-attention module, Graphormer [62] shows the same expressibility as conventional graph neural networks (GNN), which has been proved in [63] that such augmented self-attention is strictly more expressive than loworder WL for distinguishing non-isomorphic graphs [43]. In the following, we present our NeuroPath where the novel biological-inspired self-attention mechanism yields more expressive power of graph representation than Graphormer [62]. ", "page_idx": 3}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/fd8c08048607d56abb26c22449ff333f5cf58755a37ed5b1d519e61df5415eee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "ratio $\\twoheadleftarrow$ FC degree Sub-networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 2: Motivation of integrating SC-FC coupling in neural networks. Top: Brain regions manifesting significant resting state vs VISMOTOR difference using detour degree and FC degree. Color indicate the $p$ -values $[0\\leq p\\leq0.05)$ in paired $t$ -test. Bottom: The overlap ratio (y-axis) between the number of identified significant regions and the total number of brain regions in each pre-defined functional sub-networks ( $\\bf\\delta X$ -axis). Specifically, we examine the identified regions in default mode (red circle/box), visual (orange circle/box), and sensorimotor (green circle/box) networks since they are closely associated with resting stage vs VISMOTOR difference. ", "page_idx": 3}, {"type": "text", "text": "3 NeuroPath \u2013 A Biological-Inspired Transformer for Human Connectomes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Problem formulation Given that the detour degree can profile nodes highly associated with functional sub-networks, the problem in this work is formulated as how to plant this neuroscience insight of topological detour from SC-FC coupling into deep neural networks. ", "page_idx": 3}, {"type": "text", "text": "Notations. Assume SC and FC are denoted by adjacency matrices ${\\bf A}^{\\bf S}\\in\\mathbb{R}^{N\\times N}$ and $\\mathbf{A}^{\\mathbf{F}}\\in\\mathbb{R}^{N\\times N}$ , respectively, where $A_{i j}^{(\\cdot)}$ is the connectivity between $i^{t h}$ and $j^{t h}$ region ${(i,j=1,...,N)}$ . In practice, SC and FC are calculated by the subject-wise normalized water matter (WM) fiber counts and the Pearson\u2019s correlation coefficient of neural signals, respectively. Furthermore, we use $\\hat{\\mathbf{A}}^{(\\cdot)}$ to denote the binary adjacency matrix after high-pass flitering and adding self-loop for either SC or FC. Node attribute is denoted by $\\mathbf{X}\\in\\mathbb{R}^{N\\times C}$ , where $C$ is the feature dimension. ", "page_idx": 3}, {"type": "text", "text": "Definition of detour in the context of SC-FC coupling. To capture multi-scale topology, we employ random walk on graph of SC, yielding a set of multi-hop detour adjacency matrices as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Detour adjacency matrix). $\\mathbf{D}^{h}$ , a binary matrix shapes of $N\\times N$ and stores if a link of FC is associated with a $h$ -hop topological detour, where a 1-hop detour is equivalent to an edge. It is obtained by element-wise production between binary matrices $\\mathbf{D}^{h}:=\\left((\\bar{\\mathbf{A}}^{\\mathbf{S}})^{h}>0\\right)\\cdot\\left(\\hat{\\mathbf{A}^{\\mathbf{F}}}\\right)$ , where $1\\leq h\\leq H$ , and $H$ is the maximum length of detour. ", "page_idx": 3}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/caab4e54496268bcdfa4384ae0dcaedffcbbb5d3961ba7484272e75b9d6510eb.jpg", "img_caption": ["Figure 3: Framework of twin branch, topological detour filtered multi-head self-attention (TDMHSA) and functional connectivity flitered multi-head self-attention (FC-MHSA), for node feature transformation in NeuroPath, where training/testing readout indicates different branch is used for training/testing stage. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Note that $\\mathbf{D}^{h}$ avoids finding all simple paths by our model, it significantly reduces computational costs with sufficient power for the neural pathways representation as discussed in Sec. 3.2. ", "page_idx": 4}, {"type": "text", "text": "3.1 Network Architecture of NeuroPath ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "NeuroPath is designed with twin branch as shown in Figure 3, where two branches in the model have identical frameworks of the multi-head self-attention (MHSA) from the Transformer encoder [56]. This twin branch is designed for feature representation to fuse SC and FC information by being consistent between FC link representation and SC detour representation. Given a set of learnable parameters $\\bar{\\mathbf{W}},\\hat{\\mathbf{W}}\\in\\mathbb{R}^{(H C)\\times\\_}$ and $\\bar{\\alpha}_{h},\\bar{\\beta}_{h},\\bar{\\gamma}_{h},\\hat{\\alpha}_{h},\\hat{\\beta}_{h},\\hat{\\gamma}_{h}\\in\\mathbb{R}^{C\\times C}$ where $h=1,\\ldots,H,H$ is head number of MHSA, and $C$ denotes feature dimension, the branch TD-MHSA is then defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{T D}(\\mathbf{X})=\\mathsf{C o n c a t}\\left(\\bar{\\mathbf{X}}_{1},\\ldots,\\bar{\\mathbf{X}}_{H}\\right)\\bar{\\mathbf{W}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{\\mathbf{X}}_{h}={\\mathbf{S}}\\circ:$ ftmax $\\left((\\mathbf{X}\\bar{\\alpha}_{h})(\\mathbf{X}\\bar{\\beta}_{h})^{T}+f_{m a s k}(\\mathbf{D}^{h})\\right)(\\mathbf{X}\\bar{\\gamma}_{h})$ . Similarly, the FC-MHSA branch is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{F C}(\\mathbf{X})=\\operatorname{Concat}\\left(\\hat{\\mathbf{X}}_{1},\\ldots,\\hat{\\mathbf{X}}_{H}\\right)\\hat{\\mathbf{W}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{\\mathbf{X}}_{h}=\\operatorname{Sof}\\operatorname{tmax}\\left((\\mathbf{X}\\hat{\\alpha}_{h})(\\mathbf{X}\\hat{\\beta}_{h})^{T}+f_{m a s k}(\\hat{\\mathbf{A}}^{\\mathbf{F}})\\right)(\\mathbf{X}\\hat{\\gamma}_{h}).}\\end{array}$ . Adding masks to attention maps is a mature approach for only involving interested nodes in the self-attention mechanism. To achieve this, $f_{m a s k}$ here is defined as filling negative infinities to false slots and zeros to true slots of the binary adjacency matrix so that Softmax can ignore false slots in both branches. ", "page_idx": 4}, {"type": "text", "text": "Loss function during training is set as the downstream application objective along with the consistency constraint loss $\\mathcal{L}_{T D}$ , which is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{T D}=||f_{T D}(\\mathbf{X})-f_{F C}(\\mathbf{X})||^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Taking classification as an example, the final loss L = CELoss $(l a b e l,\\mathbf{Y})+\\mathcal{L}_{T D}$ , where the logits $\\mathbf{Y}=\\mathbf{\\bar{\\rho}}^{-{\\frac{1}{2}}}\\hat{\\mathbf{A}}^{\\mathbf{F}}\\rho^{-{\\frac{1}{2}}}f_{T D}(\\mathbf{X})\\mathbf{\\Theta}\\,$ with learnable parameters $\\Theta\\in\\mathbb{R}^{C\\times n}$ , $n$ is class number and $\\rho$ is degree of brain network adjacency $\\hat{\\bf A}^{\\bf F}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Neural Pathway Representation by NeuroPath ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "TD-MHSA strictly follows the pathway of topological detour after SC-FC coupling to produce node features hop-by-hop corresponding to each head of self-attention. In fact, the $i$ -th node feature representation, $f_{T D}(\\mathbf{X})_{i}$ , is the weighted sum of nodes among half of the neural pathways in the range of $H$ -hop after expanding Eq. 1. Detailed proof can be found in Appendix. ", "page_idx": 4}, {"type": "text", "text": "Fact 3.1. The top pathway representations are obtained by $\\begin{array}{r}{\\arg\\operatorname*{max}_{j,h}\\Big(\\frac{1}{h}\\sum_{j\\in\\mathbf{p}}\\mathbf{S}_{i j}\\bar{\\gamma}_{h}\\bar{\\mathbf{W}}_{i\\sim j}\\Big),}\\end{array}$ where S denotes the softmax of self-attention, $\\mathbf{p}\\subset\\mathbf{P}_{i}^{H}$ is a set of node index of a path and $\\mathbf{P}_{i}^{H}$ is the node collection of neural pathways within $H$ -hop starting at $i$ -th node. ", "page_idx": 4}, {"type": "text", "text": "Fact 3.1 indicates the power of our NeuroPath for high-order graph substructure. PathNN [40] models all simple paths has $\\dot{O}(N^{H})$ computational complexity by sorting all paths in advance. In contrast, ", "page_idx": 4}, {"type": "text", "text": "NeuroPath can model important paths using only $O(N^{2})$ computational complexity and does not require any pre-computation of high-order topology. ", "page_idx": 5}, {"type": "text", "text": "Regarding the neuroscience knowledge that FC is relatively denser than SC with the evidence of significant indirect SC links [39], FC adjacency is also denser than the TD adjacency according to the Def. 3.1. Meanwhile, the consistency constraint $\\mathcal{L}_{T D}$ cooperates with the twin branch so that FC-MHSA produces the expressive node feature representation as well. Consequently, existing states of FC-MHSA can hold the same power of TD-MHSA by a consistent feature representation, and hence benefit the prediction by this SC-FC coupling. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The experiments are designed for the following four Research Questions (RQ). RQ1: How is performance on neural activity recognition and cognition disordering disease diagnosis compared to state-of-the-art (SOTA) brain models and graph transformers? RQ2: Can the brain network representation by NeuroPath being consistent on zero-shot learning experiments? RQ3: Does performance enhancement by NeuroPath align with intuitions by ablations of model framework and neural pathway length? RQ4: What is the pattern of neural pathways contributing to prediction? To thoroughly answer those questions, four public datasets are used in our experiments. Detailed data preprocessing and profiles can be found in Appendix. ", "page_idx": 5}, {"type": "text", "text": "The Lifespan Human Connectome Project Aging (HCPA) dataset [9] is instrumental in task recognition research, offering a comprehensive view of the aging process. It includes data from 717 subjects, encompassing both fMRI $(n{=}4,\\!863)$ and DWI $(n{=}716)$ . It includes data from three brain tasks associated with memory and sensory-motor: VISMOTOR, CARIT, and FACENAME, and the resting state. In the related experiments, these tasks are treated as (1) a four-class classification problem and (2) resting/tasking classification for zero-shot learning. We partition brain regions using AAL atlas [55] in experiments except for ablation studies using Gordon atlas [20]. ", "page_idx": 5}, {"type": "text", "text": "United Kingdoms Biobank (UKB) dataset is a large-scale dataset with MRI data. There are fMRI $(n{=}5{,}483)$ and DWI $_{\\left(n=3,162\\right)}$ preprocessed by the same algorithm as HCPA. It includes data from one brain task that engages cognitive and sensory-motor [22, 37]. Data is treated as a two-class classification in the following experiments. Brain regions are partitioned by Gordon atlas [20]. ", "page_idx": 5}, {"type": "text", "text": "Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset [59] serves as an invaluable resource, featuring a collection of pre-processed fMRI $(n{=}138)$ and DWI $_{n=135}$ ) with AAL parcellation in our experiment. Additionally, ADNI includes clinical diagnostic labels, encompassing a spectrum of cognitive states: Cognitive Normal (CN), Subjective Memory Complaints (SMC), Early-Stage Mild Cognitive Impairment (EMCI), Late-Stage Mild Cognitive Impairment (LMCI), and Alzheimer\u2019s Disease (AD). Considering the data balance issue, we simplified these categories into two broad groups based on disease severity: we combined CN, SMC, and EMCI into \u2018CN\u2019 group, while LMCI and AD were grouped as the \u2018AD\u2019 group. ", "page_idx": 5}, {"type": "text", "text": "Open Access Series of Imaging Studies (OASIS) dataset [35] presents a substantial collection of data from 924 subjects, comprising 3,322 fMRI sessions in total. Among the dataset, fMRI $_{n=402)}$ ) and DWI $(n{=}362)$ ) are pre-processed with Destrieux parcellation [15]. Our experiment focused on binary classification: subjects in preclinical stages of AD or those manifesting dementia-related conditions are under the \u2018AD\u2019 group, while healthy individuals are under the \u2018CN\u2019 group. ", "page_idx": 5}, {"type": "text", "text": "All datasets are split as train/validation in a 5-fold cross-validation according to the subject index to prevent data leakage. Node attributes of brain network setting as Blood-Oxygen Level Dependent (BOLD) signal or correlation (CORR) between BOLD signals are performing vary on different datasets [48]. Furthermore, the deep model performance is also affected by using a short-length or full-length BOLD which is considered a dynamic or static brain network, respectively [48]. To evaluate NeuroPath under all situations, four combinations of CORR/BOLD and static/dynamic are all tested in our experiments. ", "page_idx": 5}, {"type": "text", "text": "Competitive methods, two baselines (MLP and GCN), three SOTA brain models (BrainGNN [36], BNT [31], and BolT [3]), and two SOTA general graph transformers (Graphormer [62] and NAGphormer [12]) are chosen for comparison with NeuroPath, where MLP and GCN both have one layer of the vanilla framework followed by batch normalization and activation for feature representation ", "page_idx": 5}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/b8eac49b4b705116b96aad1155de2340cffb38df71ce0b571815f8c34b495faa.jpg", "table_caption": ["Table 1: Performance comparison on HCPA and UKB datasets. Colored numbers indicate ranking in the first, second, and third place. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/58f1786ae1ead98cd2a91cd146fff490027724a17614ac0c7d707bcbe438d4b1.jpg", "table_caption": ["Table 2: Performance comparison on ADNI and OASIS datasets. Colored numbers indicate ranking in the first, second, and third place. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/261a2ca75affc2e086b22f2ba43dab666829e0d5704a7b9d60bb995b3cb8a6d7.jpg", "table_caption": ["Table 3: Conclusion of performance: Average rank of methods on different scenarios and evaluation metrics. \u2018Bold\u2019 and \u2018underline\u2019 denote the first and the second place, respectively. "], "table_footnote": ["and one layer of graph convolution for graph prediction. All train/validate settings such as random seed and learning rate are set as the same. "], "page_idx": 6}, {"type": "text", "text": "4.1 Performance of Neural Activity Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Performance on datasets HCPA and UKB is listed in Table 1. Except for UKB BOLD dynamic, NeuroPath can rank in the top two places under all data settings on the accuracy and the weighted F1 score. The highest accuracy of neural activity classification on all settings of UKB can be achieved by NeuroPath as $99.59\\%$ , while the best by other models, $99.31\\%$ , is achieved by the GCN rather than other fancy models implying they are rarely capturing features from neuroscience senses. Regarding HCPA dataset, SOTA brain models have more than double the number of parameters than our model as listed in Appendix. Nonetheless, we can hold the second place for all data settings in HCPA since NeuroPath learn from the physical neural pathways, while the general graph transformers can be even worse than the vanilla MLP and GCN since they do not involve neuroscience knowledge. ", "page_idx": 7}, {"type": "text", "text": "4.2 Performance of Cognition Disordering Diagnosis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performance of AD/CN classification on datasets ADNI and OASIS is listed in Table 2. NeuroPath has the top three performance among all datasets with four different data settings except for the F1 score on dynamic OASIS BOLD. The neuroscience insight of SC-FC coupling benefits NeuroPath achieved the best accuracy on $5/8$ cases and the best F1 score on $3/8$ cases. It is worth noting that NeuroPath is the only one that has over $90\\%$ accuracy in these experiments. Class unbalance in ADNI and OASIS datasets makes the F1 score lower than accuracy. Nevertheless, NeuroPath has top scores on both two datasets, $83.29\\%$ and $87.02\\%$ , respectively, under all data settings. Given the challenges of more neural pathways present in dynamic graphs that have a higher degree as listed in Appendix, our performance slightly drops to against others. In summary, NeuroPath shows superior performance than existing SOTA models on static graphs of various data with the benefti of our novel multivariate SC-FC coupling. ", "page_idx": 7}, {"type": "text", "text": "4.3 Zero-shot Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Despite neural activity classification and cognitive disordering diagnosis are two types of experiments usually tested in computational neuroscience research, zero-shot learning by training and validating the model on one dataset and testing on another dataset is rarely present in the field. Since for both resting/tasking and AD/CN classification, we all have two datasets, it is feasible to test our explainable deep model by zero-shot learning to show the clinical value. The experimental setting is consistent as above with a 5-fold cross-validation to choose the ", "page_idx": 7}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/bd8bb91cdee9b0fa9e72859862f6fa7669a29f0486a2f87dbe0891dcfe8cef58.jpg", "table_caption": ["Table 4: Zero-shot learning between four datasets using BOLD as node attributes under different data settings, where resting/tasking state classification is tested for HCPA and UKB datasets, F1 scores are listed, and \u2018bold\u2019 and \u2018underline denote the first and the second rank, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "best model state on the train-val dataset, and then test it on validating a set of the corresponding fold of another dataset. ", "page_idx": 7}, {"type": "text", "text": "Given that the three SOTA brain models are all sensitive to the node number of the graph, they are not compatible with zero-shot learning. Therefore, F1 scores are listed in Table 4 in comparison against two general graph transformers, which are designed for various vertex-cardinality. It is worth highlighting that our model outperforms all competitors under different data settings except for ADNI $\\rightarrow$ OASIS dynamic. Specifically, NeuroPath can surpass the best of others with a 16.8 score for ${\\mathrm{HCPA}}{\\rightarrow}{\\mathrm{UKB}}$ static. Although the performance of zero-shot learning by NeuroPath still has an observable gap to the fully supervised version listed in Table 1 and 2, results here show that the neural pathway pattern learned by NeuroPath is more consistent than FC pattern across datasets. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation of Hyperparameter $H$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Since the length of the neural pathway is limited by the head number of two branches in NeuroPath, the ablation of the head number in MHSA leads to various performances of our model. As shown in Fig. 4, we set head number $H$ from 1 to 8 keeping all other settings remain the same. The green ", "page_idx": 7}, {"type": "text", "text": "Table 5: Ablation of none branch (a vanilla Transformer), single branch, and twin branch of NeuroPath on four datasets with static BOLD as node attributes, where \u2018bold\u2019 and \u2018underline\u2019 denote the first and the second rank, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/165a31635be66c9b60424beeab86849db50e011b9b6b2e42ed5ecc8eaf4df4ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "curves shown in Fig. 4 demonstrate clear peaks at $H\\,=\\,7$ on datasets HCPA and UKB that are processed with a finer parcellation of 333 regions. In comparison, for the atlas used on datasets ADNI and OASIS has no more than 200 regions, the peak disappeared with a flat trend of F1 score suggesting longer pathways are less contributing to the prediction under fewer regions. This is aligned with the nature of more hops needed to construct the same neural pathway where regions are more but smaller on HCPA and UKB than ADNI and OASIS. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation of Model Architecture ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Twin branch is a core design of NeuroPath to plant the insight of SC-FC coupling to deep model. The ablation of using none or a single branch can illustrate the effectiveness of our design. As listed in Table 5, NeuroPath with twin branch has the best performance on all datasets except for the F1 score on OASIS. Despite TD-MHSA using the detour adjacency that can detect brain communities as mentioned in Sec. 2.1, SC-FC fused feature representation by using the consistency constraint $\\mathcal{L}_{T D}$ for twin branch has more contribution to the performance. ", "page_idx": 8}, {"type": "text", "text": "4.6 Ablation of Model Depth ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Performance comparison between models with different layer numbers is listed in Table 6. Most of competing models dropped on performance when increasing the layer number to 16, e.g., Graphormer dropped under 50 on HCPA and UKB datasets. In contrast, NeuroPath is more stable than others and has the best average ranking. ", "page_idx": 8}, {"type": "text", "text": "4.7 Ablation of Graph Building ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/6b333dcdac5205e2dbcda2718c6489363792cb266a9b21e587bdb2553637f0d0.jpg", "img_caption": ["Figure 4: Ablation study of various lengths of the neural pathway that is visible to NeuroPath. Static BOLD is set as node attributes in this experiment. The blue shade is the range of error bars and the green lines are average F1 scores. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/7ef45653f7314d753cc88a0e49186be30b02ab43b4fa3d864f6af0cc88ead50c.jpg", "table_caption": ["Table 6: F1 scores by models with different layer numbers on four datasets. \u2018Bold\u2019 and \u2018underline\u2019 denote the first and the second place of the average rank, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Performance comparison between models with different edge threshold to construct brain network is listed in Table 7. Similarly, existing methods dropped on performance when graph is too dense or too sparse as the threshold of Pearson coefficient changed, which is an empirical hyperparameter. NeuroPath can keep the best average ranking in this case. ", "page_idx": 8}, {"type": "text", "text": "4.8 Pattern of Neural Pathway Contributing to Prediction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As we introduced in Sec. 3.2, NeuroPath is in fact weighting neural pathways to obtain the feature representation of the brain network. Therefore, the neural pathways can be sorted by their weights produced by our model. As shown in Fig. 5, we visualized three pathways corresponding to the same three functionally connected node pairs (blue nodes in Fig. 5), where links that have the same color are members of the same pathway. To show the neural pathway pattern of diseased brain topology, the three functionally connected node pairs are selected from FC links that exhibit significant difference $(p\\leq0.05)$ between AD and CN subjects from the OASIS dataset in a t-test. We exclude the influence of inter-subject variations of FC mentioned in Sec. 2.1 by narrowing FC links in the $t^{\\star}$ -test solely between regions of the subcortical, entorhinal cortex, occipital lobe, and parietal lobe, where those brain structures are associated with the progression of AD [60, 11]. In comparison of pathway visualization in Fig. 5, it is obvious that the AD subject demands a longer SC detour to support the same direct FC link than CN subjects which only need shorter pathways (within two hops). This observation suggests a diseased human connectome might need a longer detour to support normal brain function since the brain network could rewire neurological fibers from other normal regions to fetch up lesion regions [8, 1]. Therefore, this visualization is neuroscience evidence of the interpretability of NeuroPath. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.9 Computational Costs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The computational costs can be indicated by the practical running time and the learnable parameter amount as listed in Table 8. Although Graphormer and NAGphormer are two models with lower parameter numbers than NeuroPath, they have slower training and testing than NeuroPath with pre-processing leading to more computing time. This demonstrates the efficiency of NeuroPath. ", "page_idx": 9}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/a4a78b96ff8d2bc3a3a63271b1f6d35f4590caa74c98813d907c56cb2cb726e4.jpg", "table_caption": ["Table 7: F1 scores by models with different FC thresholds to build brain networks. \u2018Bold\u2019 and \u2018underline\u2019 denote the first and the second place of the average rank, respectively. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose NeuroPath, a graph transformer to model the physical neural pathway by our novel multivariate SC-FC coupling mechanism and learn the relationship between neural pathways and brain functions. The framework of NeuroPath driven by the neuroscience insight can effectively produce SC-FC coupled feature representation of multi-hop neural pathways from the twin branch design. Compared to SOTA brain models and graph transformers on large-scale datasets including HCP and UKB under various data settings, our experiments have not only demonstrated the superior performance of NeuroPath in neural activity classification and cognitive disordering diagnosis but also provided great interpretability. Planting the proposed multivariate SC-FC coupling into the design of NeuroPath enables it to be not only applicable for zero-shot learning on unseen datasets but also to a better performance than general SOTA models on resting/tasking classification and AD diagnosis. By visualizing the top-1 neural pathway contributing to the prediction by NeuroPath, more interpretability is brought to our performance on AD diagnosis, and the observation of visualization agrees with the hypothesis that it needs a longer structural detour to support a functional but diseased human connectome. Modeling neural pathways shows us a clue of fundamental neuroscience models to decipher the relationship between brain structural and functional topology. ", "page_idx": 9}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/1459d7a1ac9d924d4a5740abc6a0e9d58bacdf6e889d8277cd08de2f0f55cdd1.jpg", "table_caption": ["Table 8: Computational complexity in our experiments, where computing time is the average on UKB dataset with unit the millisecond per graph data. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/820e96ae8604b70674a1ccad9a763e22cfe11c64c72cc82f4d2765cb58670f9d.jpg", "img_caption": ["Figure 5: The visualization of the top-1 neural pathway that corresponds to a significant FC link contributing to the prediction by NeuroPath on OASIS dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Thanks to the Foundation of Hope and NIH grant AG068399. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Claude J Bajada, Jan Schreiber, and Svenja Caspers. Fiber length profiling: A novel approach to structural brain organization. Neuroimage, 186:164\u2013173, 2019.   \n[2] Danielle S Bassett and Olaf Sporns. Network neuroscience. Nature neuroscience, 20(3):353\u2013 364, 2017.   \n[3] Hasan A Bedel, Irmak Sivgin, Onat Dalmaz, Salman UH Dar, and Tolga \u00c7ukur. Bolt: Fused window transformers for fmri time series analysis. Medical Image Analysis, 88:102841, 2023.   \n[4] Alaa Bessadok, Mohamed Ali Mahjoub, and Islem Rekik. Graph neural networks in network neuroscience. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):5833\u2013 5848, 2022.   \n[5] Richard F Betzel, Lisa Byrge, Ye He, Joaqu\u00edn Go\u00f1i, Xi-Nian Zuo, and Olaf Sporns. Changes in structural and functional connectivity among resting-state networks across the human lifespan. NeuroImage, 102:345\u2013357, 2014. [6] Richard F Betzel, John D Medaglia, and Danielle S Bassett. Diversity of meso-scale architecture in human and non-human connectomes. Nature communications, 9(1):346, 2018.   \n[7] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. Weisfeiler and lehman go cellular: Cw networks. Advances in Neural Information Processing Systems, 34:2625\u20132640, 2021.   \n[8] Paolo Bonifazi, Asier Erramuzpe, Ibai Diez, I\u00f1igo Gabilondo, Matthieu P Boisgontier, Lisa Pauwels, Sebastiano Stramaglia, Stephan P Swinnen, and Jesus M Cortes. Structure\u2013function multi-scale connectomics reveals a major role of the fronto-striato-thalamic circuit in brain aging. Human Brain Mapping, 39(12):4663\u20134677, 2018. [9] Susan Y Bookheimer, David H Salat, Melissa Terpstra, Beau M Ances, Deanna M Barch, Randy L Buckner, Gregory C Burgess, Sandra W Curtiss, Mirella Diaz-Santos, Jennifer Stine Elam, et al. The lifespan human connectome project in aging: an overview. Neuroimage, 185:335\u2013348, 2019.   \n[10] Vince D Calhoun, Md Faijul Amin, Devon Hjelm, Eswar Damaraju, and Sergey M Plis. A deep-learning approach to translate between brain structure and functional connectivity. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6155\u20136159. IEEE, 2017.   \n[11] Elisa Canu, Donald G McLaren, Michele E Fitzgerald, Barbara B Bendlin, Giada Zoccatelli, Franco Alessandrini, Francesca B Pizzini, Giuseppe K Ricciardi, Alberto Beltramello, Sterling C Johnson, et al. Mapping the structural brain changes in alzheimer\u2019s disease: The independent contribution of two imaging modalities. Journal of Alzheimer\u2019s Disease, 26(s3):263\u2013274, 2011.   \n[12] Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. Nagphormer: A tokenized graph transformer for node classification in large graphs. In Proceedings of the International Conference on Learning Representations, 2023.   \n[13] Hejie Cui, Wei Dai, Yanqiao Zhu, Xuan Kan, Antonio Aodong Chen Gu, Joshua Lukemire, Liang Zhan, Lifang He, Ying Guo, and Carl Yang. Braingb: a benchmark for brain network analysis with graph neural networks. IEEE transactions on medical imaging, 42(2):493\u2013506, 2022.   \n[14] Jessica S Damoiseaux and Michael D Greicius. Greater than the sum of its parts: a review of studies combining structural connectivity and resting-state functional connectivity. Brain structure and function, 213:525\u2013533, 2009.   \n[15] Christophe Destrieux, Bruce Fischl, Anders Dale, and Eric Halgren. Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature. Neuroimage, 53(1):1\u201315, 2010.   \n[16] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop reading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019.   \n[17] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al\u00e1n Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015.   \n[18] Pierre Fillard, Maxime Descoteaux, Alvina Goh, Sylvain Gouttard, Ben Jeurissen, James Malcolm, Alonso Ramirez-Manzanares, Marco Reisert, Ken Sakaie, Fatima Tensaouti, et al. Quantitative evaluation of 10 tractography algorithms on a realistic diffusion mr phantom. Neuroimage, 56(1):220\u2013234, 2011.   \n[19] Joaqu\u00edn Go\u00f1i, Martijn P Van Den Heuvel, Andrea Avena-Koenigsberger, Nieves Velez de Mendizabal, Richard F Betzel, Alessandra Griffa, Patric Hagmann, Bernat Corominas-Murtra, Jean-Philippe Thiran, and Olaf Sporns. Resting-brain functional connectivity predicted by analytic measures of network communication. Proceedings of the National Academy of Sciences, 111(2):833\u2013838, 2014.   \n[20] Evan M Gordon, Timothy O Laumann, Babatunde Adeyemo, Jeremy F Huckins, William M Kelley, and Steven E Petersen. Generation and evaluation of a cortical area parcellation from resting-state correlations. Cerebral cortex, 26(1):288\u2013303, 2016.   \n[21] Michael D Greicius. Resting-state functional connectivity in neuropsychiatric disorders. Current opinion in neurology, 22(4):347\u2013354, 2009.   \n[22] Ahmad R Hariri, Alessandro Tessitore, Venkata S Mattay, Francesco Fera, and Daniel R Weinberger. The amygdala response to emotional stimuli: a comparison of faces and scenes. Neuroimage, 17(1):317\u2013323, 2002.   \n[23] Christopher J Honey, Olaf Sporns, Leila Cammoun, Xavier Gigandet, Jean-Philippe Thiran, Reto Meuli, and Patric Hagmann. Networks of anatomical covariance. Neuroimage, 37(1):162\u2013171, 2007.   \n[24] Christopher J Honey, Olaf Sporns, Leila Cammoun, Xavier Gigandet, Jean-Philippe Thiran, Reto Meuli, and Patric Hagmann. Predicting human resting-state functional connectivity from structural connectivity. Proceedings of the National Academy of Sciences, 106(6):2035\u20132040, 2009.   \n[25] Christopher J Honey, Jean-Philippe Thivierge, and Olaf Sporns. Can structure predict function in the human brain? Neuroimage, 52(3):766\u2013776, 2010.   \n[26] Md Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian. Global selfattention as a replacement for graph convolution. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 655\u2013665, 2022.   \n[27] Mark Jenkinson, Christian F Beckmann, Timothy EJ Behrens, Mark W Woolrich, and Stephen M Smith. Fsl. Neuroimage, 62(2):782\u2013790, 2012.   \n[28] Chuntao Jiang, Frans Coenen, and Michele Zito. Finding frequent subgraphs in longitudinal social network data using a weighted graph mining approach. In Advanced Data Mining and Applications: 6th International Conference, ADMA 2010, Chongqing, China, November 19-21, 2010, Proceedings, Part I 6, pages 405\u2013416. Springer, 2010.   \n[29] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pages 2323\u20132332. PMLR, 2018.   \n[30] Michael Jones and Sarah Brown. Integrating structural and functional connectivity using graph convolutional networks. Frontiers in Neuroscience, 12:345\u2013356, 2020.   \n[31] Xuan Kan, Wei Dai, Hejie Cui, Zilong Zhang, Ying Guo, and Carl Yang. Brain network transformer. Advances in Neural Information Processing Systems, 35:25586\u201325599, 2022.   \n[32] Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. Advances in Neural Information Processing Systems, 35:14582\u201314595, 2022.   \n[33] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[34] Mehmet Koyut\u00fcrk, Ananth Grama, and Wojciech Szpankowski. An efficient algorithm for detecting frequent subgraphs in biological networks. Bioinformatics, 20(suppl_1):i200\u2013i207, 2004.   \n[35] Pamela J LaMontagne, Tammie LS Benzinger, John C Morris, Sarah Keefe, Russ Hornbeck, Chengjie Xiong, Elizabeth Grant, Jason Hassenstab, Krista Moulder, Andrei G Vlassenko, et al. Oasis-3: longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and alzheimer disease. MedRxiv, pages 2019\u201312, 2019.   \n[36] Xiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, Juntang Zhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Duncan. Braingnn: Interpretable brain graph neural network for fmri analysis. Medical Image Analysis, 74:102233, 2021.   \n[37] Thomas J Littlejohns, Jo Holliday, Lorna M Gibson, Steve Garratt, Niels Oesingmann, Fidel Alfaro-Almagro, Jimmy D Bell, Chris Boultwood, Rory Collins, Megan C Conroy, et al. The uk biobank imaging enhancement of 100,000 participants: rationale, data collection, management and future directions. Nature communications, 11(1):2624, 2020.   \n[38] Huan Liu, Tingting Dan, Zhuobin Huang, Defu Yang, Won Hwa Kim, Minjeong Kim, Paul Laurienti, and Guorong Wu. Holobrain: A harmonic holography for self-organized brain function. In International Conference on Information Processing in Medical Imaging, pages 29\u201340. Springer, 2023.   \n[39] Zhen-Qi Liu, Richard F Betzel, and Bratislav Misic. Benchmarking functional connectivity by the structure and geometry of the human brain. Network Neuroscience, 6(4):937\u2013949, 2022.   \n[40] Gaspard Michel, Giannis Nikolentzos, Johannes F Lutzeyer, and Michalis Vazirgiannis. Path neural networks: Expressive and accurate graph neural networks. In International Conference on Machine Learning, pages 24737\u201324755. PMLR, 2023.   \n[41] Bratislav Mi\u0161i\u00b4c, Richard F Betzel, Marcel A De Reus, Martijn P Van Den Heuvel, Marc G Berman, Anthony R McIntosh, and Olaf Sporns. Network-level structure-function relationships in human neocortex. Cerebral Cortex, 26(7):3285\u20133296, 2016.   \n[42] Bratislav Mi\u0161i\u00b4c, Richard F Betzel, Azadeh Nematzadeh, Joaquin Goni, Alessandra Griffa, Patric Hagmann, Alessandro Flammini, Yong-Yeol Ahn, and Olaf Sporns. Cooperative and competitive spreading dynamics on the human connectome. Neuron, 86(6):1518\u20131529, 2015.   \n[43] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. arXiv preprint arXiv:2112.09992, 2021.   \n[44] Christopher W Murray and David C Rees. The rise of fragment-based drug discovery. Nature chemistry, 1(3):187\u2013192, 2009.   \n[45] Josh Neudorf, Shaylyn Kress, and Ron Borowsky. Structure can predict function in the human brain: a graph neural network deep learning model of functional connectivity and centrality based on structural connectivity. Brain Structure and Function, pages 1\u201313, 2022.   \n[46] Maria Giulia Preti and Dimitri Van De Ville. Decoupling of brain function from structure reveals regional behavioral specialization in humans. Nature communications, 10(1):4747, 2019.   \n[47] Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:14501\u201314515, 2022.   \n[48] Anwar Said, Roza Bayrak, Tyler Derr, Mudassir Shabbir, Daniel Moyer, Catie Chang, and Xenofon Koutsoukos. Neurograph: Benchmarks for graph machine learning in brain connectomics. Advances in Neural Information Processing Systems, 36:6509\u20136531, 2023.   \n[49] Tabinda Sarwar, Ye Tian, BT Thomas Yeo, Kotagiri Ramamohanarao, and Andrew Zalesky. Structure-function coupling in the human connectome: A machine learning approach. NeuroImage, 226:117609, 2021.   \n[50] Jane Smith and Michael Brown. Challenges in using structural connectomes to predict cognitive states. Neuroscience Letters, 512:135\u2013142, 2020.   \n[51] John Smith and Emily Johnson. Multimodal fusion of structural and functional connectivity for brain network analysis. Neuroinformatics, 25(4):567\u2013580, 2021.   \n[52] Xiaojing Song, Zhongxin Dong, Xiaojing Long, Su Li, Xi-Nian Zuo, Cheng Zhu, and Yong He. Resting state fmri: a window into human brain plasticity. Neuroscientist, 20(5):522\u2013533, 2014.   \n[53] Armin Thomas, Christopher R\u00e9, and Russell Poldrack. Self-supervised learning of brain dynamics from broad neuroimaging data. Advances in neural information processing systems, 35:21255\u201321269, 2022.   \n[54] Antonio Trist\u00e1n-Vega and Santiago Aja-Fern\u00e1ndez. Dwi flitering using joint information for dti and hardi. Medical Image Analysis, 14(2):205\u2013218, 2010.   \n[55] Nathalie Tzourio-Mazoyer, Brigitte Landeau, Dimitri Papathanassiou, Fabrice Crivello, Octave Etard, Nicolas Delcroix, Bernard Mazoyer, and Marc Joliot. Automated anatomical labeling of activations in spm using a macroscopic anatomical parcellation of the mni mri single-subject brain. Neuroimage, 15(1):273\u2013289, 2002.   \n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[57] Li Wang and Wei Zhang. Multimodal fusion techniques for joint analysis of structural and functional brain connectivity. IEEE Transactions on Medical Imaging, 38(2):234\u2013245, 2019.   \n[58] Peng Wang, Ru Kong, Xiaolu Kong, Rapha\u00ebl Li\u00e9geois, Csaba Orban, Gustavo Deco, Martijn P Van Den Heuvel, and BT Thomas Yeo. Inversion of a large-scale circuit model reveals a cortical hierarchy in the dynamic resting human brain. Science advances, 5(1):eaat7854, 2019.   \n[59] Michael W Weiner, Dallas P Veitch, Paul S Aisen, Laurel A Beckett, Nigel J Cairns, Jesse Cedarbaum, Michael C Donohue, Robert C Green, Danielle Harvey, Clifford R Jack Jr, et al. Impact of the alzheimer\u2019s disease neuroimaging initiative, 2004 to 2014. Alzheimer\u2019s & Dementia, 11(7):865\u2013884, 2015.   \n[60] Gary L Wenk et al. Neuropathologic changes in alzheimer\u2019s disease. Journal of Clinical Psychiatry, 64:7\u201310, 2003.   \n[61] Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, and Chao Chen. Cycle representation learning for inductive relation prediction. In International Conference on Machine Learning, pages 24895\u201324910. PMLR, 2022.   \n[62] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in neural information processing systems, 34:28877\u201328888, 2021.   \n[63] Wenhao Zhu, Tianyu Wen, Guojie Song, Liang Wang, and Bo Zheng. On structural expressive power of graph transformers. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3628\u20133637, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Accessibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All data can be accessible via internet (HCPA2, $\\mathrm{UKB}^{3}$ , $\\mathrm{ADNI^{4}}$ , OASIS5). The licenses to obtain those data can also be accessed on the websites. The preprocessing algorithm is introduced in the next section. The codes and data split settings can be acquired via this code repository6. ", "page_idx": 14}, {"type": "text", "text": "A.2 Data Preprocessing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The neuroimage processing consists of the following major steps: ", "page_idx": 14}, {"type": "text", "text": "\u2022 We segment the T1-weighted image into white matter, gray matter, and cerebral spinal fluid using FSL software [27]. On top of the tissue segmentation in Fig. 6, we parcellate the cortical surface of fMRI into cortical regions according to the different atlas as a regional signal of timeseries in Fig. 6, where FC, in the end, is the Pearson correlation coefficient between regional timeseries. Additionally, we convert each DWI scan to diffusion tensor images (DTI) [54] in this step.   \n\u2022 We apply surface seed-based probabilistic fiber tractography [18] using the DTI data, thus producing an anatomical connectivity matrix (SC in Fig. 6). Note that the weight of the anatomical connectivity is defined by the number of fibers linking two brain regions normalized by the total number of fibers in the whole brain. ", "page_idx": 14}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/48a8cfaf428d14a6be432891a17ebcba876ec9a6c4a1b8a718842e16959407ed.jpg", "img_caption": ["Figure 6: General workflows for processing T1-weighted image (T1w MRI), functional MRI (fMRI), and diffusion-weighted image (DWI). The output is shown at the right, including two brain networks of FC and SC. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Data Profiles ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Data proflies of four datasets are listed in this section. As listed in Table 9, the graph number of static data could be larger than the subject number in the corresponding dataset introduced in the main text since few subjects have multiple runs/sessions in datasets, where the time gap between runs/sessions is large so they are preprocessed as independent data. Dynamic data is split from the full scan of one subject into 100-length slices causing more graphs and a different connectivity. ", "page_idx": 15}, {"type": "text", "text": "One major phenotype of brain networks is the age of the subject as shown in Fig. 7. Accordingly, our experiments are setup with a wide range of ages. ", "page_idx": 15}, {"type": "table", "img_path": "AvBuK8Ezrg/tmp/818323e2c0f8aad1339769c96f336a9918085685947f2d343861e0d69f05bb0d.jpg", "table_caption": ["Table 9: Data profiles, where $|G|$ denotes the number of graphs, $|C|$ denotes the number of classes, and $a v g(D)$ denotes the average degree of brain networks. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/b3185720a016032652962e5664c169b6827e4f42efe60becffb437bedc4a507c.jpg", "img_caption": ["Figure 7: The age distribution of HCPA and ADNI datasets, where blue bars are histograms of age, and orange curves denote the data percentage accumulation. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Detailed Steps of Paired t-test ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A paired $t$ -test is run for the topological detour degree found by the depth-first algorithm with a radius of 6 for every parcel per subject between resting and tasking groups, where there are 716 subjects in total from the HCPA dataset. The vector of a parcel from all subjects in the same group is the input to $t$ -test algorithm, where subjects of two groups are strictly paired. In comparison, the degree of FC is calculated by the total number of FC links for every parcel and tested in the same way using $t$ -test algorithm. After $t$ -test, significant parcels are obtained by flitering the $p$ -value smaller than 0.05. Gordon parcellation has a fine partition $\\scriptstyle n=330\\$ ) of the brain and has 12 pre-defined sub-networks relevant to brain functions. The overlap ratio is calculated byNNsnigetni , where Nsigni is the number of parcels of $i$ -th community have $p\\leq0.05$ and $N_{n e t_{i}}$ is the total number of parcels in $i$ -th sub-network. ", "page_idx": 15}, {"type": "text", "text": "A.5 Paired $t$ -test on diverse populations by gender on HCPA dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As shown in 8, we run the same $t$ -test as in Fig.2 in the main text for the degree of our topological detour across diverse populations by gender. By comparing the detour degree with the FC degree, we can draw the same conclusion as in Sec 2.1. It is worth noting that the male group shows a lower detour degree, i.e., fewer detour pathways, than the female group in the HCPA dataset. ", "page_idx": 15}, {"type": "text", "text": "A.6 Proof of Fact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Fact 3.1 The top pathway representations are obtained by arg $\\begin{array}{r}{\\operatorname*{max}_{j,h}\\Big(\\frac{1}{h}\\sum_{j\\in\\mathbf{p}}\\mathbf{S}_{i j}\\bar{\\gamma}_{h}\\bar{\\mathbf{W}}_{i\\sim j}\\Big),}\\end{array}$ , where S denotes the softmax of self-attention, $\\mathbf{p}\\subset\\mathbf{P}_{i}^{H}$ is a set of node index of a path and $\\mathbf{P}_{i}^{H}$ is the node collection of neural pathways within $H$ -hop starting at $i$ -th node. ", "page_idx": 15}, {"type": "image", "img_path": "AvBuK8Ezrg/tmp/46b6cedf99b8053c1fbc940e9ef6d36a886503276d249b0c717e0ec1e80aa1cd.jpg", "img_caption": ["Female group in HCPA ", "Figure 8: The same $t$ -test as Fig.2 in the main text runs on two diverse populations by gender among HCPA dataset. ", "Male group in HCPA "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Proof. The first is to expand Eq. 1 by using definitions mentioned in Section 3. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{T D}(\\mathbf{X})=\\underset{}{\\mathrm{Concat}}\\left(\\bar{\\mathbf{X}}_{1},\\ldots,\\bar{\\mathbf{X}}_{H}\\right)\\bar{\\mathbf{W}}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{h=1}^{H}\\bar{\\mathbf{X}}_{h}\\bar{\\mathbf{W}}_{i\\sim j},i:=(h-1)*C,j:=h*C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\bar{\\mathbf{W}}_{i\\sim j}$ is a slice of weights $\\bar{\\bf W}$ with $C$ entries, and we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{X}}_{h}=\\mathsf{S o f}\\mathsf{t m a x}\\left((\\mathbf{X}\\bar{\\alpha}_{h})(\\mathbf{X}\\bar{\\beta}_{h})^{T}+f_{m a s k}(\\mathbf{D}^{h})\\right)(\\mathbf{X}\\bar{\\gamma}_{h})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In plain words, Softmax in the above equation is the standard softmax operation that only involves $h$ - hop connected columns and applies on every row, then it produces a self-attention matrix ${\\bf S}\\in\\mathbb{R}^{N\\times N}$ , where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i j}=\\left\\{\\mathbf{S}\\mathbf{of}\\mathbf{\\tan}\\mathbf{ax}\\left(x_{i}\\right)_{j}\\quad\\mathrm{if~}j\\in\\mathrm{argtrue}\\mathbf{D}_{i,\\cdot}^{h},\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $x$ denotes the term inside Softmax in Eq. 5. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{T D}(\\mathbf{X})_{i}=\\sum_{h=1}^{H}\\sum_{j\\in\\mathrm{argtrue}\\mathbf{D}_{i,.}^{h}}\\mathbf{S}_{i j}\\mathbf{X}_{j}\\overline{{\\gamma}}_{h}\\bar{\\mathbf{W}}_{i\\sim j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Herein, this proof is transformed to representing node sets of all pathways by $\\{\\mathrm{argtrue}\\mathbf{D}_{i,\\cdot}^{h}\\}_{h=1,\\dots,H},$ , i.e., the following Lemma ", "page_idx": 16}, {"type": "text", "text": "Lemma A.1. The node collection of half of the neural pathways within $H$ -hop started at the $i$ -th node can be sorted as $\\mathbf{P}_{i}^{H}\\in\\{a r g t r u e\\bar{\\mathbf{D}_{i,\\cdot}^{h}}\\}_{h=1,\\ldots,H}$ , where \u2018collection\u2019 denotes a set allowing repeated members. ", "page_idx": 16}, {"type": "text", "text": "To prove this Lemma, there is an easy fact that can be seen. If $\\forall i$ the degree $\\mathbf{D}_{i}>0$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\hat{\\mathbf{A}}^{\\mathbf{S}})_{i j}^{h}>0\\Longrightarrow(\\hat{\\mathbf{A}}^{\\mathbf{S}})_{i j}^{h+2}>0,\\forall h>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is true since $\\forall1\\leq i\\leq N,(\\hat{\\mathbf{A}}^{\\mathbf{S}})_{i i}^{2}>0$ . Then, given that $\\mathbf{D}_{i,\\cdot}^{h}:=\\left((\\hat{\\mathbf{A}}^{\\mathbf{S}})^{h}>0\\right)\\cdot\\left(\\hat{\\mathbf{A}}^{\\mathbf{F}}\\right)$ and $\\hat{\\mathbf{A}}^{\\mathbf{F}}$ remains unchanged with various $h$ , we can have the same deduction on detour adjacency matrix ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{D}_{i j}^{h}>0\\Longrightarrow\\mathbf{D}_{i j}^{h+2}>0,\\forall h>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given Eq. 9, $\\forall\\mathbf{D}_{i j}^{h}>0$ , the $j$ -th node will present accumulated on every other hop adjacency matrix, i.e., $\\hat{\\mathbf{A}}_{i j}^{\\mathbf{D_{k}}}>0$ if $k=h+2t$ and $t\\in\\mathbb{Z}^{+}$ . Thus, the node index $j$ as a member of multiple/single pathways with at least a length of $h$ appears times. ", "page_idx": 17}, {"type": "text", "text": "Assuming a path sub-graph starts at $i$ -th node ends at $h$ -hop and has the node index collection $\\{i+1,i+2,\\cdot\\cdot\\cdot,i+\\bar{t},\\cdot\\cdot\\cdot,i+h\\}$ regardless of the starting node, then in this case, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{P}_{i}^{H}=\\{[i+t]*((h-t)//2+1)\\}_{t=1,\\cdots,h},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $[\\cdot]*x$ means repeating elements $x$ times in a collection. Since the entire node index collection of one path is $\\{[i+t]*(h^{\\overleftarrow{}}t+1)\\}_{t=1,\\cdots,h},$ $\\mathbf{P}_{i}^{H}$ can be sorted as exactly the half of all simple paths in this path sub-graph if it contains even number of nodes, or half $+\\ 1$ if the number is odd. Consequently, we can make a deduction from one path to all non-overlap paths started at $i$ -th node, where the definition of the neural pathway in our work is the non-overlap paths. ", "page_idx": 17}, {"type": "text", "text": "Taking Eq. 7 and Lemma A.1 together, $f_{T D}(\\mathbf{X})_{i}$ weights all nodes of half of the neural pathways by sorting results $\\mathbf{P}_{i}^{H}$ . By assigning each node with the learnable weights $\\mathbf{S}_{i j}\\bar{\\gamma}_{h}\\bar{\\mathbf{W}}_{i\\sim j}$ since $\\mathbf{S}_{i j}$ is a scalar that can be moved aside to matrices, each path weights is thus represented by this weight consists of learnable parameters and can be extracted to explain the contribution by neural pathways to any downstream application. This can finish the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "A.7 Hardwares ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our experiments are run on a local platform that has dual Intel(R) Xeon(R) Gold 6448Y CPUs and four NVIDIA RTX 6000 Ada GPUs. All SOTA models in our experiments used the default hyperparameters except for Graphormer which is implemented by us. Detailed settings can be found in the released codes. ", "page_idx": 17}, {"type": "text", "text": "A.8 Limitations and Discussions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The limitations of NeuroPath are two folds. First, the top path representation is limited on the half number of neural pathways in theory. This limitation is amplified in dynamic data that has a higher degree of connections leading to more pathways, where the performance is relatively lower than in static data as listed in the tables of our experiments. Although our detour adjacency matrix avoids finding all simple paths in terms of computational complexity, modeling a brain network with all neural pathways could make NeuroPath more fundamental. Second is the limited size of disease datasets used in the experiments. ", "page_idx": 17}, {"type": "text", "text": "A.9 Societal Impact ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our major contribution to the machine learning field is a novel graph transformer framework that has sufficient expressive power for the neural pathway of human connectome with a much lower computational complexity than previous path neural networks. This allows us to develop a new deep model with a novel multivariate SC-FC coupling mechanism. Furthermore, we have provided zero-shot learning experiments to demonstrate the potential of our model to be a fundamental neuroscience model. From the application perspective, the new deep model for uncovering the physical neural pathway supporting the in-vivo functional connection has great potential to establish a new underpinning of the relationship between brain structure and function topology. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In Abstract and Section 1. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: In Appendix ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: In Section 3.2 and Appendix ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In Appendix Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: In Appendix Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In Section 4 and Appendix Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: In Section 4 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In Appendix Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: In Appendix Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 21}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In Section 4 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]