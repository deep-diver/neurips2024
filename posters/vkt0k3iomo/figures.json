[{"figure_path": "VKt0K3iOmO/figures/figures_1_1.jpg", "caption": "Figure 1: MSG conducts parallel forwarding and enables a new training algorithm alleviating the high latency issue.", "description": "This figure illustrates the architecture of the Manifold-valued Spiking GNN (MSG).  The input is a graph G represented as spike trains S<sup>0</sup>. These spike trains are processed by multiple Manifold Spiking Layers (MS Layers). Each MS Layer contains a Graph Convolutional Network (GCN) and a Manifold Spiking Neuron. The GCN incorporates structural information from the graph into the spike trains. The Manifold Spiking Neuron emits new spike trains and updates manifold representations using a diffeomorphism and exponential map. This process repeats across multiple layers, resulting in the final manifold representation Z<sup>L</sup> and spike trains S<sup>L</sup>, which are then used for downstream tasks. The red dashed line represents the proposed Differentiation via Manifold (DvM) approach, avoiding the high latency associated with Backpropagation-Through-Time (BPTT).", "section": "Abstract"}, {"figure_path": "VKt0K3iOmO/figures/figures_3_1.jpg", "caption": "Figure 2: Manifold Spiking Layer. It conducts parallel forwarding of spike trains and manifold representations, and creates an alternative backward pass (red dashed line). The backward gradient with w, Dl-1-1-1 and \u2207zl L will be introduced in Sec. 4.2.", "description": "This figure illustrates the architecture of a Manifold Spiking Layer, a key component of the proposed Manifold-valued Spiking GNN (MSG).  It shows how the layer processes both spike trains (representing temporal information) and manifold representations (representing spatial/structural information) in parallel.  The forward pass involves graph convolution (GCN) and a manifold spiking neuron (MSNeuron). The backward pass, however, deviates from standard backpropagation through time (BPTT) by employing a novel method called Differentiation via Manifold (DvM), represented by the red dashed line. This DvM method is designed to improve efficiency and overcome latency issues associated with BPTT in spiking neural networks.", "section": "4.1 Manifold Spiking Layer"}, {"figure_path": "VKt0K3iOmO/figures/figures_6_1.jpg", "caption": "Figure 3: Charts given by the logarithmic map.", "description": "This figure illustrates the concept of charts in the context of Riemannian manifolds.  A chart is a local mapping between a region of the manifold and a Euclidean space. In this image, we see three charts (U\u2081, \u03c6\u2081), (U\u2082, \u03c6\u2082), and (U\u2083, \u03c6\u2083), each covering a different part of a curved, two-dimensional manifold. The logarithmic map is used to create these charts, mapping points on the manifold to corresponding points in tangent spaces that are locally Euclidean. The figure shows the process of using a series of charts to approximate a continuous path (the red dotted line) on the manifold, which is a common technique in solving manifold ordinary differential equations (ODEs).  This visual helps explain the theoretical foundation of how the proposed model approximates a dynamic chart solver for manifold ODEs.", "section": "5 Theory: MSG as Neural ODE Solver"}, {"figure_path": "VKt0K3iOmO/figures/figures_8_1.jpg", "caption": "Figure 2: Manifold Spiking Layer. It conducts parallel forwarding of spike trains and manifold representations, and creates an alternative backward pass (red dashed line). The backward gradient with w, Dl\u22121zl\u22121 and \u2207zlL will be introduced in Sec. 4.2.", "description": "This figure illustrates the architecture of the Manifold Spiking Layer, a key component of the proposed Manifold-valued Spiking GNN (MSG).  It shows how the layer performs parallel processing of spike trains (representing information from spiking neurons) and manifold representations (representing data on a Riemannian manifold). The diagram highlights the novel backward pass mechanism (represented by the red dashed line) that replaces the traditional Back-Propagation-Through-Time (BPTT) method with the proposed Differentiation via Manifold (DvM) for efficient training. The backward gradient calculation involves terms 'w', 'Dl\u22121zl\u22121', and '\u2207zlL', which are detailed in section 4.2 of the paper.", "section": "4.1 Manifold Spiking Layer"}, {"figure_path": "VKt0K3iOmO/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization on S\u00b9 \u00d7 S\u00b9. ", "description": "This figure visualizes the proposed model's behavior on a torus (S\u00b9 \u00d7 S\u00b9). Each point represents a node's representation at a specific layer in the model. The red curve shows the path of a node's representation across layers, illustrating how it evolves through the manifold. The blue arrows indicate the direction of the geodesic (the shortest path) along which the node's representation moves within the manifold at each layer.  This demonstrates how the model iteratively solves an ODE on the manifold, effectively navigating the complex geometric structure of the graph.", "section": "Visualization"}, {"figure_path": "VKt0K3iOmO/figures/figures_21_1.jpg", "caption": "Figure 6: Visualizations of the training process for node classification on Computer dataset. Backward Gradient. Previous studies compute backward gradients though the Differentiation via Spikes (DvS). Distinguishing from the previous studies, we compute backward gradients though the Differentiation via Manifold (DvM). In order to examine the backward gradients, we visualize the training process for node classification on Computer dataset. Concretely, we plot the norm of backward gradients in each iteration in Figs. 6 (a) and (b) together with the value of loss function in Figs. 6 (c). As shown in Fig. 6, the proposed algorithm with DvM converges well, and the backward gradients do not suffer from gradient vanishing or gradient explosion.", "description": "This figure visualizes the training process of the proposed model using the Differentiation via Manifold (DvM) method. It shows the norm of backward gradients for each layer and the overall loss during training. The results demonstrate the effectiveness of DvM, which avoids gradient vanishing or explosion.", "section": "6.2 Results & Discussion"}, {"figure_path": "VKt0K3iOmO/figures/figures_21_2.jpg", "caption": "Figure 6: Visualizations of the training process for node classification on Computer dataset. Backward Gradient. Previous studies compute backward gradients though the Differentiation via Spikes (DvS). Distinguishing from the previous studies, we compute backward gradients though the Differentiation via Manifold (DvM). In order to examine the backward gradients, we visualize the training process for node classification on Computer dataset. Concretely, we plot the norm of backward gradients in each iteration in Figs. 6 (a) and (b) together with the value of loss function in Figs. 6 (c). As shown in Fig. 6, the proposed algorithm with DvM converges well, and the backward gradients do not suffer from gradient vanishing or gradient explosion.", "description": "This figure visualizes the training process of the proposed model (MSG) on the Computer dataset. It compares the backward gradient norms using Differentiation via Spikes (DvS) and the proposed Differentiation via Manifold (DvM). The plots show the norm of backward gradients with respect to z and v in each layer, as well as the overall training loss. The results demonstrate that the proposed DvM method effectively avoids gradient vanishing and explosion, leading to faster convergence.", "section": "6.2 Results & Discussion"}, {"figure_path": "VKt0K3iOmO/figures/figures_21_3.jpg", "caption": "Figure 6: Visualizations of the training process for node classification on Computer dataset. Backward Gradient. Previous studies compute backward gradients though the Differentiation via Spikes (DvS). Distinguishing from the previous studies, we compute backward gradients though the Differentiation via Manifold (DvM). In order to examine the backward gradients, we visualize the training process for node classification on Computer dataset. Concretely, we plot the norm of backward gradients in each iteration in Figs. 6 (a) and (b) together with the value of loss function in Figs. 6 (c). As shown in Fig. 6, the proposed algorithm with DvM converges well, and the backward gradients do not suffer from gradient vanishing or gradient explosion.", "description": "This figure visualizes the training process of the proposed model (MSG) on the Computer dataset. It shows the norm of backward gradients for both z and v in each layer, along with the training loss.  This demonstrates the effectiveness of the proposed Differentiation via Manifold (DvM) method, highlighting its convergence and lack of gradient vanishing/explosion compared to the traditional Differentiation via Spikes (DvS).", "section": "6.2 Results & Discussion"}, {"figure_path": "VKt0K3iOmO/figures/figures_23_1.jpg", "caption": "Figure 7: Visualizations of node representations on Zachary karateClub datasets [58].", "description": "This figure visualizes node representations on the Zachary Karate Club dataset using the proposed Manifold-valued Spiking GNN (MSG).  It shows the node representations at each spiking layer for two specific nodes (1-th and 17-th). The red dots represent the manifold representation of the node at each layer, while the blue lines represent the tangent vectors (directions) that guide the movement of the node representations along geodesics on the manifold.  The curves connecting the outputs of successive layers are marked in red, illustrating how each layer's computation contributes to the overall progression. This visualization empirically demonstrates the relationship between MSG and manifold ordinary differential equations (ODEs), as each layer acts as a solver of the ODE describing the geodesic on the manifold.", "section": "Visualization"}]