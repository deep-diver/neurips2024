[{"type": "text", "text": "Temporal Graph Neural Tangent Kernel with Graphon-Guaranteed ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Katherine Tieu\u2217 Donqgi Fu\u2217 University of Illinois Urbana-Champaign Meta AI kt42@illinois.edu dongqifu@meta.com ", "page_idx": 0}, {"type": "text", "text": "Yada Zhu Hendrik Hamann Jingrui He IBM Research IBM Research University of Illinois Urbana-Champaign yzhu@us.ibm.com hendrikh@us.ibm.com jingrui@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph Neural Tangent Kernel (GNTK) fuses graph neural networks and graph kernels, simplifies the process of graph representation learning, interprets the training dynamics of graph neural networks, and serves various applications like protein identification, image segmentation, and social network analysis. In practice, graph data carries complex information among entities that inevitably evolves over time, and previous static graph neural tangent kernel methods may be stuck in the sub-optimal solution in terms of both effectiveness and efficiency. As a result, extending the advantage of GNTK to temporal graphs becomes a critical problem. To this end, we propose the temporal graph neural tangent kernel, which not only extends the simplicity and interpretation ability of GNTK to the temporal setting but also leads to rigorous temporal graph classification error bounds. Furthermore, we prove that when the input temporal graph grows over time in the number of nodes, our temporal graph neural tangent kernel will converge in the limit to the graphon NTK value, which implies the transferability and robustness of the proposed kernel method, named Temporal Graph Neural Tangent Kernel with Graphon-Guaranteed or Temp- $\\mathbf{G}^{3}\\mathbf{NTK}$ . In addition to the theoretical analysis, we also perform extensive experiments, not only demonstrating the superiority of Temp- $\\mathbf{\\nabla}\\cdot\\mathbf{G}^{3}\\mathbf{NTK}$ in the temporal graph classification task, but also showing that Temp- $\\mathrm{G^{3}N T K}$ can achieve very competitive performance in node-level tasks like node classification compared with various SOTA graph kernel and representation learning baselines. Our code is available at https://github.com/kthrn22/ TempGNTK ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphs, as a relational structure, model the complex relationships among entities and have attracted much research attention nowadays. To serve various applications, graph neural networks have been extensively studied for their representation learning ability. On the one hand, graph neural networks usually need to build complex neural architectures with hyperparameters to achieve their powerful expressive ability, which is typically a nonlinear process and hard to interpret [50, 13, 21]. On the other hand, graph kernels enjoy the explicit formula and can be convex, leading to solid theoretical results, although their specific form is often hand-crafted and may not be powerful enough to support complicated application scenarios [23, 34, 12]. Hence, graph neural tangent kernel (GNTK) [9] has been proposed to fuse graph neural networks and graph kernels, enjoying the benefits of both approaches, i.e., achieving the excellent representation ability while relying on simple computation processes. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, in the real world, the graph topology and features are inevitably evolving over time, e.g., the user connections and interests in social networks. This temporal evolution brings new challenges to GNTK research as to how the similarity of temporal graphs is measured and how the corresponding kernel matrix is derived. To be more specific, how can we design a temporal graph neural tangent kernel, which not only has a superior representation ability than temporal graph neural networks $[39,\\,8]$ but also inherits the expression simplicity and analysis rigorousness of graph neural tangent kernels [9, 24]? ", "page_idx": 1}, {"type": "text", "text": "Hence, we propose Temporal Graph Neural Tangent Kernel with Graphon-Guaranteed, or Temp$\\mathrm{G^{3}N T K}$ . First, we propose the kernel matrix computation formula for temporal graphs with timeevolving structures and time-evolving node features, and the corresponding kernel value can be used for classification tasks with generalization error bounded. This proposed kernel method addresses how to measure the similarity between temporal graphs to achieve the accuracy of graph neural networks but without complex neural computational procedures like gradient descent. Second, considering the property of temporal graphs, we also prove that when the temporal graph is growing, i.e., the number of nodes increases over time, our Temp- $\\mathbf{\\cdotG^{3}N T K}$ kernel will converge in the limit to the graphon NTK value. This result addresses the challenge of adapting graphon neural network [41] and graphon neural tangent kernel [24] to the temporal setting, and, more importantly, demonstrates that Temp- $\\mathbf{\\cdotG^{3}N T K}$ has the excellent potential to transfer to large-scale temporal graph data with robust performance. Third, in addition to the theoretical analysis, we also design extensive experiments for not only temporal graph classification but also temporal node classification, illustrating the effectiveness of the proposed Temp- $\\mathrm{G^{3}N T K}$ compared with various state-of-the-art temporal graph representation learning and graph kernel methods. ", "page_idx": 1}, {"type": "text", "text": "2 Temporal Graph Modeling ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To begin with, we first denote a static undirected graph as $G=(V,E)$ , where $V$ and $E$ are sets of vertices and edges, respectively. We also denote the node features of node $v$ $(v\\in V)$ as $\\mathbf{h}_{v}\\in\\mathbb{R}^{d}$ , the neighborhood as $\\mathcal{N}(v)$ , and edge features of an edge $(u,v)$ as $\\mathbf{e}_{u v}$ . ", "page_idx": 1}, {"type": "text", "text": "In order to extend to the temporal setting, researchers usually model the temporal graph $G$ as a continuous-time dynamic graph (CTDG) [22], which is mathematically represented as a stream of events, $G\\,=\\,\\{(u,v,\\mathbf{h}_{u}(t),\\mathbf{h}_{v}(t),\\mathbf{e}_{u v}(t),t)\\}_{t=t_{0}}^{T}$ , where an event $(\\bar{u_{,}}v,\\bar{\\mathbf{h}}_{u}(t),\\mathbf{h}_{v}(t),\\mathbf{e}_{u v}(t),t)$ indicates that at time $t$ , an edge exists between node $u$ and $v$ , and ${\\mathbf h}_{u}(t),{\\mathbf h}_{v}(t)$ , and $\\mathbf{e}_{u v}(t)$ are the features of $u,v$ , and $(u,v)$ at time $t$ , respectively. To support different computation requirements, a CTDG $G$ can also be transferred into a discrete-time dynamic graph (DTDG) [22], which is a collection of snapshots $G^{(t)}$ . To be specific, a snapshot of $G$ at any time $t\\geq t_{0}$ , is denoted as $G^{(t)}$ , which can be obtained by sequentially updating the initial state of $G^{(t_{0})}$ with the event stream, i.e., $G^{(t)}=\\{(u,v,\\mathbf{h}_{u}(\\bar{t}),\\mathbf{h}_{v}^{\\phantom{\\dagger}}(\\bar{t}),\\bar{\\mathbf{e}}_{u v}(\\bar{t}),\\bar{t})\\}_{\\bar{t}=t_{0}}^{t^{\\phantom{\\dagger}}}$ given $'t_{0}\\leq\\bar{t}\\leq t\\}$ ), and temporal graph equals to the last time snapshot, i.e., $G=G^{(T)}$ given the last timestamp is denoted by $T$ . ", "page_idx": 1}, {"type": "text", "text": "Let $V^{(t)},E^{(t)}$ be the sets of vertices and edges of $G^{(t)}$ . We denote the temporal neighborhood of node $v$ at time $t$ as $\\mathcal{N}^{(t)}(v)=\\{(u,\\bar{t}):((u,\\bar{v_{\\ast}}\\mathbf{h}_{u}(\\bar{t}),\\mathbf{h}_{v}(\\bar{t}),\\mathbf{e}_{u v}\\bar{t})\\in G^{(t)}\\}$ , i.e., a set of nodes $u$ that are involved in an event with $v$ at any time $\\bar{t}$ $\\left(t_{0}\\leq\\bar{t}\\leq t\\right)$ . Note that, in the rest of the paper, we use $G$ to denote an entire temporal graph, and $G^{(t)}$ as a snapshot. For simplicity, we denote $t_{0}=0$ . ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries of Temp- $\\mathbf{G}^{3}\\mathbf{NTK}$ ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3.1 Temporal Graph Representation Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Graph Neural Networks (GNN) are a family of neural architectures for graph representation learning. In general, most GNNs leverage a message-passing framework to compute a node $v$ \u2019s representation $\\mathbf{h}_{v}^{(\\bar{l})}$ at the $l^{t h}$ layer. In the static setting, $\\bar{\\mathbf{h}_{v}^{(l)}}$ can be obtained by applying a neighborhood aggregation operator on $\\mathbf{h}_{u}^{(l-1)}$ , $\\forall u\\in\\mathcal{N}(v)$ , then transforming the aggregated neighborhood information. The graph-level representation can be obtained by applying a pooling function on representations of all nodes, e.g., the summation of all node representations. ", "page_idx": 1}, {"type": "text", "text": "To derive Temp- $\\mathrm{G^{3}N T K}$ , our first step is to compute node representations at time $t$ and then apply a pooling function over all nodes to obtain the graph-level (or snapshot-level) representation. Specifically, we obtain the node representation of $v$ at time $t$ , ${\\bf h}_{v}(t)$ , by aggregating information from its temporal neighborhood $\\mathcal{N}^{(t)}(v)$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}_{v}(t)=c\\cdot\\sum_{(u,\\bar{t})\\in\\mathcal{N}^{(t)}(v)}[\\mathbf{t}_{e n c}(t-\\bar{t})||\\mathbf{e}_{u v}(\\bar{t})||\\mathbf{x}_{u}(\\bar{t})]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{u}(t)$ is the node feature of $u$ at time $t$ , $\\mathbf{t}_{e n c}:\\mathbb{R}\\rightarrow\\mathbb{R}^{d_{t}}$ is the time encoding function that can be instantiated [8] as $\\mathbf{t}_{e n c}(\\Delta t)=\\cos(\\Delta t\\mathbf{w})$ , and $\\mathbf{w}\\in\\mathbb{R}^{d_{t}}$ with its $i^{\\mathrm{th}}$ entry $[\\mathbf{w}]_{i}=\\alpha^{-(i-1)/\\beta}$ and $d_{t}$ is the dimension of the time representation vector. Operation [\u00b7||\u00b7] denotes the concatenation. $c$ is the scaling factor, and if we set $c=1$ , then Eq. 1 is simply the sum neighborhood aggregation; and if $c=|\\frac{1}{|\\mathcal{N}^{(t)}(v)|}|$ , then Eq. 1 would be the average neighborhood aggregation. Note that if the edge features do not exist then we simply set $\\mathbf{e}_{u v}(t)=0$ . Similarly, if node features are not available then we let $\\mathbf{x}_{u}(t)=0$ . ", "page_idx": 2}, {"type": "text", "text": "After aggregating information from node $v$ \u2019s neighborhood at time $t$ as Eq. 1, we can input $\\mathbf{h}_{v}(t)$ into $L$ layers of Multilayer Perceptrons (MLPs), where the representation of node $v$ after the $l^{t h}$ MLP projection is as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}_{v}^{(l)}(t)=\\sqrt{\\frac{2}{d_{l}}}\\sigma\\big(\\mathbf{W}^{(l)}\\mathbf{h}_{v}^{(l-1)}(t)\\big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $d_{l}$ is the output dimension of the $l^{\\mathrm{th}}$ MLP layer, $\\sigma$ is a non-linear activation function that can be instantiated as the Rectified Linear Units (ReLU) function. ", "page_idx": 2}, {"type": "text", "text": "Furthermore, we denote the graph-level representation of $G^{(t)}$ as $\\begin{array}{r}{\\mathbf{h}_{G}(t)=\\sum_{v\\in V^{(t)}}\\mathbf{h}_{v}^{(L)}(t)}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Graph Neural Tangent Kernel ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Next, we provide some background on the infinite-width limit of a fully connected deep neural network $f_{n n}$ and derive the definition of NTK and its properties on graphs. ", "page_idx": 2}, {"type": "text", "text": "Consider the following settings: given a training dataset of $n$ samples $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ , where $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ and its label denoted by $y_{i}\\in\\mathbb{R}$ . Let $f_{n n}(\\mathbf{x},\\theta)$ be the output of a fully-connected neural network, with parameters $\\theta\\in\\mathbb{R}^{p}$ and $\\mathbf{x}$ as the input. We train $f_{n n}$ by minimizing the squared loss over the training dataset. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(\\theta)={\\frac{1}{2}}\\sum_{i=1}^{n}(f_{n n}(\\mathbf{x}_{i},\\theta)-y_{i})^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Let $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ (where $[\\mathbf{X}]_{i}=\\mathbf{x}_{i})$ ) and $\\mathbf{y}\\in\\mathbb{R}^{n}$ (where $[\\mathbf{y}_{i}]=y_{i}$ ), such that $f_{n n}(\\mathbf{X},\\theta)$ would be the prediction of $f_{n n}$ , with parameters $\\theta$ , over all $\\mathbf{x}_{i}$ of the training set. ", "page_idx": 2}, {"type": "text", "text": "Suppose that $\\ell$ is minimized by gradient descent, so the output $f_{n n}(\\mathbf{X},\\theta)$ evolves with respect to the training time $\\tau$ as follows [1]. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d\\;f_{n n}({\\bf X},\\theta(\\tau))}{d\\tau}=-{\\bf H}(\\tau)\\cdot(f_{n n}({\\bf X},\\theta(\\tau))-{\\bf y})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta(\\tau)$ is the parameters $\\theta$ being updated at training time $\\tau$ based on gradient descent, and $\\mathbf H(\\tau)$ is a $n\\times n$ positive definite matrix with its $(i,j)$ -th entry as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{\\partial f_{n n}(\\mathbf{x}_{i},\\theta(\\tau))}{\\partial\\theta},\\frac{\\partial f_{n n}(\\mathbf{x}_{j},\\theta(\\tau))}{\\partial\\theta}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Existing works on over-parameterized neural networks [1, 2], [10, 11], and [19] have proven that for infinite-width neural networks, the matrix $\\mathbf H(\\tau)$ remains constant during training, and under random initialization of parameters, the matrix $\\mathbf H(0)$ converges in probability to a certain deterministic kernel matrix $\\mathbf{H}^{*}$ , which is named as Neural Tangent Kernel [19]. Moreover, as proven in [1, 2], the prediction of a fully-trained sufficiently wide ReLU neural network is equivalent to the kernel regression predictor with the kernel matrix $\\mathbf{H}^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "For the temporal setting, similar to the NTK and the infinite-width neural networks, let $f_{t e m p}$ denote the aforementioned temporal GNN in Section 3.1, and $f_{t e m p}(G^{(t)},\\mathbf{W})$ be the output of $f_{t e m p}$ with the input $G^{(t)}$ and parameters W. Given two temporal graphs $G$ and $G^{\\prime}$ , at time $t$ , the NTK value corresponds to infinite-width $f_{t e m p}$ , i.e., in the limit that $d_{l}\\to\\infty$ , where $d_{l}$ is the output dimension stated in Eq. 2, $l\\in[L]$ , such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nK(G^{(t)},G^{\\prime(t)})=\\mathbb{E}_{\\mathbf{W}\\sim\\mathcal{N}(0,1)}\\left\\langle\\frac{\\partial f(G^{(t)},\\mathbf{W})}{\\partial\\mathbf{W}},\\frac{\\partial f(G^{\\prime(t)},\\mathbf{W})}{\\partial\\mathbf{W}}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, in the rest of this paper, we can refer to this value $K(G^{(t)},G^{\\prime(t)})$ as the Temp- $\\mathrm{G^{3}N T K}$ value of $G$ and $G^{\\prime}$ at time $t$ . In the next section, we are ready to introduce how to compute the defined kernel as Eq. 6 without training neural networks. ", "page_idx": 3}, {"type": "text", "text": "4 Proposed Temp- ${\\bf G}^{3}{\\bf N T K}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given two temporal graphs $G$ and $G^{\\prime}$ , we propose to compute the Temp- $\\mathrm{G^{3}N T K}$ value at time $t$ i.e., $K(G^{(t)},G^{\\bar{\\prime}(t)})$ , with $L$ BLOCK operations 2. We discuss the detailed computation procedure of Temp- $\\mathrm{{\\calG}^{3}N T K}$ here and leave the theoretical derivation in Section 5. ", "page_idx": 3}, {"type": "text", "text": "In general, similar to [1, 9], we first recursively compute the node pairwise covariance matrix $\\pmb{\\Sigma}^{(l)}$ , its derivative \u02d9\u03a3 , and the node-pairwise kernel matrix $\\Theta^{(l)}$ that correspond to the $l^{\\mathrm{th}}$ BLOCK transformation. Finally, the Temp- $\\dot{\\mathrm{{G}^{3}}}\\mathrm{{NTK}}$ value is obtained by the summation of all of the entries in the kernel matrix of the last BLOCK transformation, i.e., $\\Theta^{(\\bar{L})}$ . ", "page_idx": 3}, {"type": "text", "text": "To begin with, we initialize the node pairwise covariance matrix $\\Sigma$ and the kernel matrix $\\Sigma$ at entries $u,u^{\\prime}\\left(u\\in V^{(t)},u^{\\prime}\\in V^{\\prime(t)}\\right)$ as the inner product of node representations of $u,u^{\\prime}$ at time $t$ respectively, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta^{(0)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}=\\Sigma^{(0)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}=\\mathbf{h}_{u}(t)^{T}\\mathbf{h}_{u}^{\\prime}(t)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{h}_{u}(t)$ and $\\mathbf{h}_{u}^{\\prime}(t)$ are computed by Eq. 1. ", "page_idx": 3}, {"type": "text", "text": "Next, we need to compute $\\pmb{\\Sigma}^{(l)}$ and $\\Theta^{(l)}$ that correspond to the $l^{\\mathrm{th}}$ BLOCK operator with ReLU activation function $\\sigma$ . As $\\sigma(x)=\\operatorname*{max}(0,x)$ , the derivative of $\\sigma$ is ${\\dot{\\sigma}}(x)=\\mathbb{1}[x\\geq0]$ , where $\\mathbb{1}$ is the indicator vector3. For $l(1\\leq l\\leq L)$ , we first define an intermediate covariance matrix as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Lambda^{(l)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}=\\left(\\Sigma^{(l-1)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}\\;\\;\\;\\;\\;\\Sigma^{(l-1)}(G^{(t)},G^{(t)})_{u u}\\right)_{v^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\mathbf{A}^{(l)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}\\in\\mathbb{R}^{2\\times2}$ . ", "page_idx": 3}, {"type": "text", "text": "As the covariance matrix $\\pmb{\\Sigma}^{(l)}$ represents the i.i.d centered Gaussian Processes of $h_{u}(t)$ and $h_{u}^{\\prime}(t)$ after transformed by $l$ BLOCK operations, we can compute $\\Sigma^{(l)}$ and $\\dot{\\Sigma}^{(l)}$ based on the aforementioned intermediate covariance matrix as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma^{(l)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}=\\mathbb{E}_{(a,b)\\sim N(0,\\Lambda^{(l)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}})}[\\sigma(a)\\cdot\\sigma(b)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\pi-\\operatorname{arccos}(\\Sigma^{(l-1)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}})}{2\\pi}+\\frac{\\sqrt{1-(\\Sigma^{(l-1)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}})^{2}}}{2\\pi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}^{(l)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}=\\mathbb{E}_{(a,b)\\sim\\mathcal{N}(0,\\mathbf{A}^{(l)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}})}[\\dot{\\sigma}(a)\\cdot\\dot{\\sigma}(b)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\pi-\\operatorname{arccos}\\left(\\Sigma^{(l-1)}(G^{(t)},G^{\\prime(t)})_{u u^{\\prime}}\\right)}{2\\pi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Eq. 9 and Eq. 10 hold due to the closed form of the kernel function with ReLU activation $g(x,y)=$ $\\begin{array}{r}{\\mathbb{E}_{w}\\bar{[}\\sigma^{\\prime}(w^{T}x)\\sigma^{\\prime}(w^{T}y)]=\\Big(\\frac{1}{2}-\\frac{\\operatorname{arccos}{x^{T}y}}{2\\pi}\\Big).}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Then, the $l^{\\mathrm{th}}$ kernel matrix, $\\Theta^{(l)}$ , is obtained as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{\\Theta}^{(l)}(\\boldsymbol{G}^{(t)},\\boldsymbol{G}^{\\prime(t)})_{u u^{\\prime}}=\\boldsymbol{\\Theta}^{(l-1)}(\\boldsymbol{G}^{(t)},\\boldsymbol{G}^{\\prime(t)})_{u u^{\\prime}}\\cdot\\dot{\\boldsymbol{\\Sigma}}^{(l)}(\\boldsymbol{G}^{(t)},\\boldsymbol{G}^{\\prime(t)})_{u u^{\\prime}}+\\boldsymbol{\\Sigma}^{(l)}(\\boldsymbol{G}^{(t)},\\boldsymbol{G}^{\\prime(t)})_{u u^{\\prime}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, the Temp- $\\mathrm{{\\calG}^{3}N T K}$ value of $G,G^{\\prime}$ at time $t$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nK(G^{(t)},G^{\\prime(t)})=\\sum_{v\\in V^{(t)}}\\sum_{v^{\\prime}\\in V^{\\prime(t)}}\\Theta^{(L)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We perform summation over all entries since in our proposed neural architecture in Section 3.1, we can obtain the graph embedding by applying a pooling function, e.g., sum pooling, on node-level representations. ", "page_idx": 4}, {"type": "text", "text": "The pseudo-code of computing the Temp- $\\mathbf{\\cdotG^{3}N T K}$ kernel as above is shown in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "5 Theoretical Analysis of Temp- ${\\bf{\\nabla}}{\\bf{G}}^{3}{\\bf{N T K}}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1 Kernel Properties of Temp- $\\mathbf{G}^{3}\\mathbf{NTK}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To begin with, we first show that our proposed kernal function Temp- $\\mathbf{\\cdotG^{3}N T K}$ satisfies symmetric and semi-definite below, and the full proof can be found in Appendix B ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.1. Temp- $G^{3}N T K$ is symmetric.   \nTheorem 5.2. Temp- $G^{3}$ NTK is positive semi-definite. ", "page_idx": 4}, {"type": "text", "text": "5.2 Generalization Bound of Temp- $\\mathbf{G}^{3}\\mathbf{NTK}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first state how to utilize Temp- $\\mathbf{\\cdotG^{3}N T K}$ as a kernel regression predictor for the temporal graph classification task; then, we establish a data-dependent generalization error bound of the function class of kernel regression predictors that are associated with Temp- $\\mathrm{{\\calG}^{3}N T K}$ . ", "page_idx": 4}, {"type": "text", "text": "To be more specific, we can instantiate the problem of temporal graph classification, where, given an i.i.d training set of $n$ temporal graphs $\\{\\bar{G_{1}Bar{,}G_{2}},\\dotsc,G_{n}\\}$ and their labels $\\{y_{1},y_{2},\\ldots,y_{n}\\}$ , our goal is to predict the label 4 of a testing temporal graph Gtest. Then, the prediction of Gt(te)st at any time t by a kernel regression predictor $f_{k e r n e l}$ associated with Temp- $\\mathrm{G^{3}N T K}$ kernel $K(\\cdot\\,,\\cdot)$ is expressed as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{k e r n e l}(G_{t e s t}^{(t)})=\\left[K(G_{t e s t}^{(t)},G_{1}^{(t)}),\\dots,K(G_{t e s t}^{(t)},G_{n}^{(t)})\\right][\\mathbf{K}_{t r a i n}^{(t)}]^{-1}\\mathbf{y}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where Kt(tr)ai is a positive definite $n\\times n$ kernel matrix, whose $(i,j)$ -th entry is the Temp- $\\mathbf{\\cdotG^{3}N T K}$ value of $\\bar{G}_{i}^{(t)},G_{j}^{(t)}$ , i.e., $[\\mathbf{K}_{t r a i n}^{(t)}]_{i,j}\\,=\\,K(G_{i}^{(t)},G_{j}^{(t)})$ and $\\textbf{y}\\in\\mathbb{R}^{n}$ is the label space of temporal graphs, whose $i^{\\mathrm{th}}$ entry is $[\\mathbf{y}]_{i}=y_{i}$ . ", "page_idx": 4}, {"type": "text", "text": "Then, we consider any loss function $\\ell\\,:\\,\\mathbb{R}\\,\\times\\,\\mathbb{R}\\,\\rightarrow\\,[0,1]$ that is $\\alpha$ \u2212Lipschitz. We define the generalization error of the predictor $f_{k e r n e l}$ in Eq. 13 at time $t$ that acts on a temporal graph $G$ labeled by $y$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\ell(f_{k e r n e l}(G^{(t)})y)|\\{G^{(1)},\\ldots,G^{(t-1)}\\}]-\\ell(f_{k e r n e l}(G^{(t)}),y)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expectation is taken over all $G^{(t)}$ drawn from the stochastic process $\\mathbb{P}_{t}\\big(.|G^{(1)},...\\,,G^{(t-1)}\\big)$ conditioned on all previous snapshots before time $t$ of temporal graph $G$ . The following theorem establishes the generalization error bound on all snapshot $G^{(t)}$ of $G$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.3. Given n i.i.d training samples and their labels $\\{G_{i},y_{i}\\}_{i=1}^{n}$ and $G_{i}$ has $t$ timestamps, ltreta $\\mathbf{K}_{t r a i n}^{(t)}\\in\\mathbb{R}^{n\\times n}$ b ae ntdh l  bme atthrei xk eorf npeal irrewgirsees sTieomn pp- $G^{3}N T K$ vbaalsueeds  obne ttwhee etnr aginrianpgh ss eotf  atnhde $t$ $f_{k e r n e l}$ ", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{K}_{t r a i n}^{(t)}$ . Consipdreerd aicntyo rl ocsas nf ubnec tuipopn $\\ell:\\mathbb{R}\\times\\mathbb{R}\\rightarrow[0,1]$ that is $\\alpha$ \u2212Lipschitz, the generalization error ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\ell\\in\\mathcal{L}}{\\operatorname*{sup}}\\Bigg[\\frac1T\\sum_{t=1}^{T}\\mathbb{E}[\\ell(f_{k e r n e l}(G^{(t)}),y)|\\{G^{(1)},\\ldots,G^{(t-1)}\\}]-\\ell(f_{k e r n e l}(G^{(t)}),y)\\Bigg]}\\\\ &{\\qquad\\le\\mathcal{O}\\Bigg(\\underset{t}{\\operatorname*{sup}}\\mathbf{y}^{T}[\\mathbf{K}_{t r a i n}^{(t)}]^{-1}\\mathbf{y}\\cdot\\mathrm{tr}(\\mathbf{K}_{t r a i n}^{(t)})\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}$ is the class containing all \u03b1\u2212Lipschitz functions, the expectation is taken over all $G^{(t)}$ that is drawn from $\\mathbb{P}_{t}\\big(\\cdot|G^{(1)},\\cdot\\cdot\\cdot,\\breve{G}^{(t-1)}\\big)$ . ", "page_idx": 5}, {"type": "text", "text": "In brief, inspired by existing works on generalization bounds for kernel classes [4], we first bound our generalization error by the Sequential Rademacher Complexity [38, 26] of $\\mathcal{F}$ (i.e., the function class containing kernel such as $f_{k e r n e l})$ , and then bound this complexity measure by $\\mathcal{O}(\\operatorname*{sup}_{t}\\mathbf{y}^{T}[\\mathbf{K}_{t r a i n}^{(t)}]^{-1}\\mathbf{y}\\cdot\\mathbf{\\bar{tr}}(\\mathbf{K}_{t r a i n}^{(t)}))$ , where $\\operatorname*{sup}_{t}\\mathbf{y}^{T}[\\mathbf{K}_{t r a i n}^{(t)}]^{-1}\\mathbf{y}\\cdot\\operatorname{tr}(\\mathbf{K}_{t r a i n}^{(t)}))$ gives maximum value of $\\mathbf y^{T}[\\mathbf K_{t r a i n}^{(t)}]^{-1}\\mathbf y\\cdot\\operatorname{tr}(\\mathbf K_{t r a i n}^{(t)}))$ over all timestamps of the training temporal graphs. The classification to the temporal graph $G$ is the max-aggregation of $f_{k e r n e l}(G^{(t)})$ . The full proof is in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "5.3 Convergence of Temp- ${\\bf G}^{3}{\\bf N T K}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this part, we investigate our Temp- $\\mathbf{\\cdotG^{3}N T K}$ value on two growing temporal graphs, $G$ and $G^{\\prime}$ . \u201cGrowing\u201d means the number of nodes in $G$ and $G^{\\prime}$ would increase with time, and the following theorem shows that the proposed Temp- $\\mathrm{G^{3}N T K}$ enjoys a rigorous convergence. To verify this, we first adopt the definition of Graphon NTK on a single growing graph [24] and then extend the concept to different and temporal graphs to establish the convergence of Temp- $\\mathbf{\\cdotG^{3}N T K}$ value of $G,G^{\\prime}$ as follows. The full proof is provided in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.4. Given two growing temporal graphs $G$ and $G^{\\prime}$ and two graphons $W$ and $W^{\\prime}$ , suppose snapshots of $G$ (i.e., $G^{(t)}$ ) converge to $W$ and snapshots of $G^{\\prime}$ (i.e., $\\dot{G}^{\\prime}(t)$ ) converge to $W^{\\prime}$ , as $t\\to\\infty$ . Then, the graphon neural tangent kernel induced by Temp- $G^{3}N T K$ of $G,G^{\\prime}$ at time $t,$ , i.e., $K_{W}(W^{(t)},W^{\\prime(t)})$ , converges in the limit of the operator norm to the graphon neural tangent kernel of $W$ and $W^{\\prime}$ , i.e., $K_{W}(W,W^{\\prime})$ , as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}||K_{W}(W^{(t)},W^{\\prime(t)})-K_{W}(W,W^{\\prime})||\\rightarrow0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $K_{W}$ denotes the graphon NTK value. ", "page_idx": 5}, {"type": "text", "text": "This theorem addresses the convergence limitations of previous work [24] in terms of different temporal graphs. In other words, besides temporal dependencies between snapshots of different evolving graphs, the work [24] only establishes a limit object for different stages of a single growing graph. An empirical visualization can be seen in Figure 1, and the detailed comparison and illustration are delivered in the Appendix D.3. ", "page_idx": 5}, {"type": "text", "text": "5.4 Time Complexity of Temp- $\\mathbf{\\nabla}\\mathbf{G}^{3}\\mathbf{NTK}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, the following table shows the time complexity comparison between our Temp- $\\mathbf{\\cdotG^{3}N T K}$ with other graph kernel and graph representation learning methods for measuring $n$ pairs of temporal graphs at a certain timestamp $t$ . ", "page_idx": 5}, {"type": "text", "text": "In the above table, we first need to declare some mathematical notation as follows. $|V|,|E|$ denote the maximum size of the vertex set and edge set among $n$ given graphs. Then, for the time complexity of WL-Subtree [42], Graph2Vec [33], and GL2Vec [6], $h$ denotes the number of iterations in WLSubtree algorithms; for Graph2Vec [33] and GL2Vec [6], $D$ represents the maximum degree of the rooted subgraphs that are used to compute graph embeddings; and for the time complexity of NetLSD [45], $k$ denotes the number of eigenvalues (obtained from the graph Laplacian matrix) used to compute the graph embeddings; for TGN [39], $L_{h o p}$ denotes the number of neighbor hops that a node can aggregate information from; for our Temp- $\\mathrm{G^{3}N T K}$ , based on Section 4 and Appendix A, $L$ represents the number of BLOCK operations; and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ denotes the number of training epochs for all neural representation learning algorithms. ", "page_idx": 5}, {"type": "table", "img_path": "266nH7kLSV/tmp/e4ef13fec1b936c07b09f057819e2d0fcce319684eb53f551fc986ebf889a49d.jpg", "table_caption": ["Table 1: Total Runtime Complexity of Computing Similarity for $n$ Pairs of Graphs at Timestamp $t$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Notably, our method Temp- $\\mathbf{\\cdotG^{3}N T K}$ falls into the category of graph kernels, and its computational complexity is cheaper than [5, 46] . Also, compared with graph neural representation methods [6, 33, 45, 39, 8], the computation iteration of Temp- $\\mathrm{G^{3}N T K}$ does not rely on neural computation like gradient descent and backpropagation, such that the empirical execution time of our method is still faster. Moreover, we further demonstrate our Temp- $\\mathrm{\\Delta\\dot{G}^{3}N T K^{\\prime}}$ \u2019s efficiency by providing empirical runtime comparison in Table 3, and the detailed empirical effectiveness comparison of these methods is shown in the next section. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate the performance of Temp- $\\mathrm{G^{3}N T K}$ by crucial tasks of temporal graph learning. More extra experiments about ablation study, parameter analysis, and robustness can be referred to Appendix F. ", "page_idx": 6}, {"type": "text", "text": "6.1 Graph-Level Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. Targeting temporal graph classification, we conduct experiments on one of the most advanced temporal graph benchmarks that have graph-level labels, i.e., TUDataset 5 [32], the four datasets are INFECTIOUS, DBLP, FACEBOOK, and TUMBLR, the detailed dataset statistics can also be found in Appendix G.1. Additionally, we also leveraged the more large-scale temporal datasets REDDIT, WIKIPEDIA, LASTFM, and MOOC from $[25]^{6}$ . Those datasets are large but do not have graph-level labels, so we use them to demonstrate the scalability of Temp- $\\mathrm{G^{3}\\bar{N}T K}$ on temporal graph similarity measurement. The detailed dataset statistics can be found in Appendix G.1, and corresponding experimental results can be found in Appendix F.3. Below, we focus on introducing temporal graph classification experiments and findings. ", "page_idx": 6}, {"type": "text", "text": "Problem Setting. For each dataset above, we evaluate the temporal graph classification accuracy by conducting 5-fold cross-validation and then report the mean and standard deviation of test accuracy. To be specific, given a dataset of $n$ temporal graphs $\\{G_{1},G_{2},...,G_{n}\\}$ and their labels $\\{y_{1},y_{2},...,y_{n}\\}$ , and in all four datasets, label $y_{i}$ of the temporal graph $G_{i}$ is already time-aware, which means the value does not change with respect to time. Also, edge features are not provided in these four datasets, and we apply the Temp- $\\mathrm{G^{3}N\\bar{T}K}$ formula with plain time encoding as stated in Eq. 1. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare Temp- $\\mathbf{\\cdotG^{3}N T K}$ with a range of graph classification algorithms: (1) Graph Kernels, including WL-Subtree Kernel [42], Random Walk Kernel [46], and Shortest-Path Kernel [5]; (2) Graph Representation Learning methods, including Graph2Vec [33], NetLSD [45], GL2Vec [6]; and (3) Temporal Graph Representation Learning algorithms, including TGN [39], GraphMixer [8], EvolveGCN [35]. Details about the implementation and parameters of each algorithm are deferred to Appendix G. ", "page_idx": 6}, {"type": "table", "img_path": "266nH7kLSV/tmp/44af5b5672f03b39ff54454e6d6b1757ce508ed2412fd7c14ef96319417b0703.jpg", "table_caption": ["Table 2: Comparison of Temporal Graph Classification Accuracy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results. The graph classification results are shown in Table 2, and the best test accuracy is highlighted in bold. Our method, Temp- $\\mathrm{\\DeltaG^{3}N T K}$ , outperforms the other methods on all four datasets. In particular, the most notable gap between Temp- $\\mathbf{\\bar{G}^{3}N T K}$ and the other methods lies in the FACEBOOK dataset, where Temp- $\\mathrm{G^{3}N\\bar{T}\\bar{K}}$ gains $70\\%$ accuracy. In addition, as the label for each graph remains unchanged, we evaluate the performance of baseline algorithms on different timestamps until the end of the temporal graph, and report their highest accuracy score in Table 2. ", "page_idx": 7}, {"type": "text", "text": "We also provide a better illustration of how baseline algorithms perform at different timestamps of the INFECTIOUS and FACEBOOK datasets through Figure 1. For Figure 1, as stated in our problem setting, each temporal graph is associated with a label, and the label is fixed across timestamps. Therefore, we expect our method to perform well, i.e., achieve a competitive accuracy score across all timestamps. As illustrated, Figure 1 shows that Temp- $\\mathbf{\\cdotG^{3}N T K}$ performs robustly across all timestamps and achieves the highest accuracy at most times, which also recalls the convergence ability of Temp- $\\mathrm{G^{3}N T K}$ as proved in Section 5.3. ", "page_idx": 7}, {"type": "image", "img_path": "266nH7kLSV/tmp/3cb3af5806a8454a36af3e74e4a1852860955e2ce0d9996cf115d6f1202ae340.jpg", "img_caption": ["Figure 1: Comparison of test accuracy with respect to different stages of temporal graphs from the INFECTIOUS and FACEBOOK datasets. The $y$ -axis in each plot is the accuracy, and the $x$ -axis represents what percentage of timestamps have been taken into account. For example, at $x=1/5$ , the accuracy is obtained by performing classification on the first $1/5$ timestamps of each graph. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Further, in Table 3, we present the runtime comparison for four datasets, INFECTIOUS, DBLP, FACEBOOK, and TUMBLR. Overall, the empirical running time aligns with our theoretical analysis of time complexity in Table 1. That is, our method belongs to the graph kernel category, where the node-wise comparison is usually inevitable, and our time complexity is lower. Compared to the neural baselines, since our method does not rely on complex neural training like gradient descent and backpropagation, our method is still efficient. ", "page_idx": 7}, {"type": "text", "text": "Given our method achieved the best classification accuracy, as shown in Table 2, according to the corresponding running time reported in Table 3, our method is (1) more than $10\\mathbf{x}-20\\mathbf{x}$ faster then complex temporal graph neural network methods like GraphMixer [8] and TGN [39]; (2) similarly efficient as simple kernel methods like WL-Subtree [42] and Shortest Path [5] and embedding ", "page_idx": 7}, {"type": "table", "img_path": "266nH7kLSV/tmp/e6ca5302c1f6d4fa3a1a323b27ed5760446fc6ec3a6cd406f42314126cd72191.jpg", "table_caption": ["Table 3: Runtime of Baselines for Each Dataset in Seconds "], "table_footnote": ["methods like NetLSD [45] and GL2Vec [6]; and only Graph2Vec [33] is running faster than our method, but our performance is roughly 1.4x better. "], "page_idx": 8}, {"type": "text", "text": "6.2 Node-Level Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate the performance of Temp- $\\mathbf{\\cdotG^{3}N T K}$ for the temporal node property prediction task. Specifically, we leverage the final node-pairwise kernel matrix computed by Eq. 11, i.e., $\\Theta^{(L)}$ , and obtain node predictions by performing kernel regression with $\\Theta^{(L)}$ . ", "page_idx": 8}, {"type": "text", "text": "Datasets. We demonstrate Temp- $\\mathrm{G^{3}N T K}$ \u2019s capability of performing dynamic node prediction on the tgbn-trade dataset from the Temporal Graph Learning Benchmark (TGB) [17], and the details of TGB can be found at this link 7. The training, validation, and test sets of tgbn-trade are defined in the TGB package with $70\\%/15\\%/15\\%$ chronological splits. To assess the performance of a method on tgbn-trade, we use the normalized discounted cumulative gain (NDCG) metric that is assigned to tgbn-trade in the TGB package. ", "page_idx": 8}, {"type": "text", "text": "Problem Setting. Given a temporal graph with node labels that change with respect to time, the Node Property Prediction task requires the prediction of labels of some nodes at a certain time $t$ , given that our predictor can leverage all information about the temporal graphs from the initial timestamps up to some certain timestamp $\\bar{t}$ with $\\bar{t}<t$ . To be specific, the predictor for node labels by using Temp- $\\mathbf{\\cdotG^{3}N T K}$ at time $t$ would be: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Theta^{(L)}(G^{(t)},G^{(\\bar{t})})[\\Theta^{(L)}(G^{(\\bar{t})},G^{(\\bar{t})})]^{-1}\\mathbf{y}(t)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{y}(t)\\in\\mathbb{R}^{n\\times d_{l a b e l}}$ is a vector whose $i^{\\mathrm{th}}$ entry is the label of node $i$ at time $t$ , and $d_{l a b e l}$ is the dimension of node labels. ", "page_idx": 8}, {"type": "text", "text": "Through the lens of kernel regression, $[\\Theta^{(L)}(G^{(\\bar{t})},G^{(\\bar{t})})]$ acts as the gram matrix, similar to the role of $\\mathbf{K}_{t r a i n}$ in Eq. 13, and $\\Theta^{(L)}(G^{(t)},\\dot{G}^{(t)})$ acts as the kernel values between the test and training samples. In order to effectively utilize Temp- $\\mathrm{G^{3}N T K}$ for node property prediction, we perform kernel regression with C-SVM and employ $\\Theta^{(L)}$ as the pre-computed kernel. The regularization parameter, $C$ , of our SVM predictor is searched over 120 values evenly sampled from the interval $[10^{\\bar{-}2},10^{4}]$ in log scale. The number of BLOCK operations, $L$ , is searched over $\\{1,2,3\\}$ , and we obtain the best NDCG score with $L=1$ . ", "page_idx": 8}, {"type": "text", "text": "Baselines. We compare Temp- $\\mathbf{\\cdotG^{3}N T K}$ with deep learning algorithms on the tgbn-trade\u2019s leaderboard, which include TGN [39], DyRep [44], and DyGFormer [49]. TGN [39] is discussed in the temporal graph classification task. DyRep [44] is a deep temporal point process model, which is parameterized by a temporal-attentive representation network encoding time evolving structural information into node representations. DyGFormer [49] is a Transformer-based architecture for dynamic graph learning, which learns from nodes\u2019 historical first-hop interactions by the neighbor co-occurrence sampling and patching scheme with the Transformer neural architecture. ", "page_idx": 8}, {"type": "text", "text": "With selected temporal graph representation learning baseline methods, we then report the NDCG scores of baseline algorithms based on tgbn-trade\u2019s leaderboard. ", "page_idx": 8}, {"type": "text", "text": "Results. The results for temporal node property prediction on tgbn-trade are shown in Table 4, and the best NDCG score is highlighted in bold. Temp- $\\mathrm{G^{3}N T K}$ achieves very competitive results, with the test NDCG score of 0.380, outperforming TGN and DyRep and approaching DyGFormer very closely, despite that baselines rely on heavy graph neural architectures like graph neural network or graph transformer. These results show that Temp- $\\mathrm{\\DeltaG^{3}N T K}$ has the potential to extend to the temporal node property prediction task and capture node-level information. ", "page_idx": 8}, {"type": "table", "img_path": "266nH7kLSV/tmp/cdbe9b2f051fb27b8fa3e96c48ac1527fec1b0ac1f0fde9453a60fb22aee725e.jpg", "table_caption": ["Table 4: NDCG Score for Node Property Prediction on the tgbn-trade Dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Graph neural representation learning attracts many research interests and serves for many interesting tasks like recommendation [3, 36, 37], time series forecasting [29, 15], and social network analysis [28, 14, 27]. In which research domain, many efforts have been devoted to develop non-neural computations and temporal settings. Graph Neural Tangent Kernel. Graph Neural Tangent Kernel (GNTK) [9] introduces a class of graph kernels that corresponds to infinite-width GNNs with sum neighborhood aggregator. Building upon the foundations of GNTK, a line of works unveil different theoretical aspects of GNTK. For example, [20] improves the computation time of constructing the gram matrix of GNTK; [18] studies the behavior of GNTK that aligns with GNNs with large depth; and most relevant to our theoretical results (Theorem 5.4), [24] combines GNTK with the concept of graphons to derive the Graphon Neural Tangent Kernel. Graphons, Graphon Neural Network, and Graphon Neural Tangent Kernel. A graphon is a symmetric, bounded, and measurable function $W:[0,1]^{2}\\rightarrow[0,1]$ that acts as the limit object of dense graph sequences and defines a family of similar graphs. Similarly, Graphon Neural Networks (WNNs) [41] are proven to be the limit object of GNNs that operates on a sequence of graphs as the graph\u2019s size grows. Graphon Neural Tangent Kernel (WNTK) [24] defines the NTK that resonates with the infinite-width WNNs and proves that the GNTK converges to the corresponding WNTK as the size of the graph grows. Temporal Graph Learning. Most temporal graph learning methods are comprised of complex architectures that leverage the message passing framework, a time encoding function that captures time representation and distinguishes different timestamps. Some works also employ recurrent architecture to capture past information and update the node or edge representation at a current time $t$ based on representations of previous time $\\bar{t}$ , where $\\bar{t}<t$ . For example, JODIE [25] employs RNN to update the history representation of $v$ at time $t$ . TGAT [47] utilizes the self-attenion mechanism (SAM) to compute the temporal representation of node $v$ . TGN [39] employs recurrent architecture to capture the history representation of ${\\bf x}_{v}(t)$ (similar to JODIE [25]) and then performs neighborhood aggregation to obtain the temporal node representation of $v$ at time $t$ , which is similar to TGAT [47]. GraphMixer [8] first constructs edge representation by aggregating raw edge features and then concatenates them with relative difference time encoding. Then, the temporal node representation is determined by aggregating the aforementioned edge representation, ${\\bf h}_{v}(t)$ . The node representation is further transformed by MLP and Mixer-MLP layers. For a more comprehensive comparison between Temp- $\\mathbf{\\cdotG^{3}N T K}$ and previous recurrent neural network works on Temporal Graph Learning, we refer readers to Appendix E, where we provide detailed illustration of more Temporal Graph Learning methods, DGNN [31], EvolveGCN [35], ROLAND [48], and SSGNN [7]. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the graph neural tangent kernel within the temporal graph setting and propose a temporal graph neural tangent kernel named Temp- $\\mathrm{\\DeltaG^{3}N T K}$ , which allows the input graph structure and node features to evolve over time and output the pairwise similarity. The proposed Temp$\\mathrm{G^{3}N T K}$ enjoys the computational efficiency, expressive representation ability of temporal graph neural networks, and rigorous error bound. Moreover, the proposed Temp- $\\dot{G}^{3}\\mathrm{NTK}$ also follows the graphon convergence property. Empirically, we not only test Temp- $\\bar{\\mathbf{G}}^{3}\\mathbf{NTK}$ in the temporal graph-level experiments and demonstrate its superior accuracy but also extend it to deal with temporal node-level tasks, where Temp- $\\mathrm{{\\calG}^{3}N T K}$ also shows competitive performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by National Science Foundation under Award No. IIS-2117902, the U.S. Department of Homeland Security under Grant Award Number 17STQAC00001-08-01, MIT-IBM Watson AI Lab, and IBM-Illinois Discovery Accelerator Institute - a new model of an academicindustry partnership designed to increase access to technology education and skill development to spur breakthroughs in emerging areas of technology. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agencies or the government. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8139\u20138148, 2019. ", "page_idx": 10}, {"type": "text", "text": "[2] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 322\u2013332. PMLR, 2019.   \n[3] Yikun Ban, Yuchen Yan, Arindam Banerjee, and Jingrui He. Ee-net: Exploitation-exploration neural networks in contextual bandits. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[4] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3:463\u2013482, 2002.   \n[5] Karsten M. Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005), 27-30 November 2005, Houston, Texas, USA, pages 74\u201381. IEEE Computer Society, 2005.   \n[6] Hong Chen and Hisashi Koga. Gl2vec: Graph embedding enriched by line graphs with edge features. In Tom Gedeon, Kok Wai Wong, and Minho Lee, editors, Neural Information Processing - 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12-15, 2019, Proceedings, Part III, volume 11955 of Lecture Notes in Computer Science, pages 3\u201314. Springer, 2019.   \n[7] Andrea Cini, Ivan Marisca, Filippo Maria Bianchi, and Cesare Alippi. Scalable spatiotemporal graph neural networks. In Brian Williams, Yiling Chen, and Jennifer Neville, editors, Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 7218\u20137226. AAAI Press, 2023.   \n[8] Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, and Mehrdad Mahdavi. Do we really need complicated model architectures for temporal networks? In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.   \n[9] Simon S. Du, Kangcheng Hou, Ruslan Salakhutdinov, Barnab\u00e1s P\u00f3czos, Ruosong Wang, and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5724\u20135734, 2019.   \n[10] Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1675\u20131685. PMLR, 2019.   \n[11] Simon S. Du, Xiyu Zhai, Barnab\u00e1s P\u00f3czos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[12] Dongqi Fu, Liri Fang, Ross Maciejewski, Vetle I. Torvik, and Jingrui He. Meta-learned metrics over multi-evolution temporal graphs. In Aidong Zhang and Huzefa Rangwala, editors, KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, pages 367\u2013377. ACM, 2022.   \n[13] Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, Jingrui He, and Bo Long. Vcr-graphormer: A mini-batch graph transformer via virtual connections. CoRR, abs/2403.16030, 2024.   \n[14] Dongqi Fu, Dawei Zhou, Ross Maciejewski, Arie Croitoru, Marcus Boyd, and Jingrui He. Fairness-aware clique-preserving spectral clustering of temporal graphs. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023, pages 3755\u20133765. ACM, 2023.   \n[15] Dongqi Fu, Yada Zhu, Hanghang Tong, Kommy Weldemariam, Onkar Bhardwaj, and Jingrui He. Generating fine-grained causality in climate time series data for forecasting and anomaly detection. CoRR, abs/2408.04254, 2024.   \n[16] Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A generalization of vit/mlp-mixer to graphs. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 12724\u201312745. PMLR, 2023.   \n[17] Shenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu, Emanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and Reihaneh Rabbany. Temporal graph benchmark for machine learning on temporal graphs. Advances in Neural Information Processing Systems, 2023.   \n[18] Wei Huang, Yayong Li, Weitao Du, Richard Y. D. Xu, Jie Yin, Ling Chen, and Miao Zhang. Towards deepening graph neural networks: A gntk-based optimization perspective. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[19] Arthur Jacot, Cl\u00e9ment Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 8580\u20138589, 2018.   \n[20] Shunhua Jiang, Yunze Man, Zhao Song, Zheng Yu, and Danyang Zhuo. Fast graph neural tangent kernel via kronecker sketching. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pages 7033\u20137041. AAAI Press, 2022.   \n[21] Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang Gu, Yifang Qin, Nan Yin, Senzhang Wang, Xinwang Liu, Xiao Luo, Philip S. Yu, and Ming Zhang. A survey of graph neural networks in real world: Imbalance, noise, privacy and OOD challenges. CoRR, abs/2403.04468, 2024.   \n[22] Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi, Peter Forsyth, and Pascal Poupart. Representation learning for dynamic graphs: A survey. J. Mach. Learn. Res., 21:70:1\u201370:73, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[23] Nils M. Kriege, Fredrik D. Johansson, and Christopher Morris. A survey on graph kernels. Appl. Netw. Sci., 5(1):6, 2020. ", "page_idx": 12}, {"type": "text", "text": "[24] Sanjukta Krishnagopal and Luana Ruiz. Graph neural tangent kernel: Convergence on large graphs. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 17827\u201317841. PMLR, 2023.   \n[25] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In Ankur Teredesai, Vipin Kumar, Ying Li, R\u00f3mer Rosales, Evimaria Terzi, and George Karypis, editors, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 1269\u20131278. ACM, 2019.   \n[26] Vitaly Kuznetsov and Mehryar Mohri. Time series prediction and online learning. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, volume 49 of JMLR Workshop and Conference Proceedings, pages 1190\u20131213. JMLR.org, 2016.   \n[27] Zihao Li, Dongqi Fu, and Jingrui He. Everything evolves in personalized pagerank. In Ying Ding, Jie Tang, Juan F. Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben, editors, Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023, pages 3342\u20133352. ACM, 2023.   \n[28] Xiao Lin, Jian Kang, Weilin Cong, and Hanghang Tong. Bemap: Balanced message passing for fair graph neural network. In Learning on Graphs Conference, pages 37\u20131. PMLR, 2024.   \n[29] Xiao Lin, Zhining Liu, Dongqi Fu, Ruizhong Qiu, and Hanghang Tong. Backtime: Backdoor attacks on multivariate time series forecasting. arXiv preprint arXiv:2410.02195, 2024.   \n[30] Antonio Longa, Veronica Lachi, Gabriele Santin, Monica Bianchini, Bruno Lepri, Pietro Lio, Franco Scarselli, and Andrea Passerini. Graph neural networks for temporal graphs: State of the art, open challenges, and opportunities. Trans. Mach. Learn. Res., 2023, 2023.   \n[31] Yao Ma, Ziyi Guo, Zhaochun Ren, Jiliang Tang, and Dawei Yin. Streaming graph neural networks. In Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, pages 719\u2013728. ACM, 2020.   \n[32] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. CoRR, abs/2007.08663, 2020.   \n[33] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. CoRR, abs/1707.05005, 2017.   \n[34] Giannis Nikolentzos, Giannis Siglidis, and Michalis Vazirgiannis. Graph kernels: A survey. J. Artif. Intell. Res., 72:943\u20131027, 2021.   \n[35] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao B. Schardl, and Charles E. Leiserson. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 5363\u20135370. AAAI Press, 2020.   \n[36] Yunzhe Qi, Yikun Ban, and Jingrui He. Neural bandit with arm group graph. In Aidong Zhang and Huzefa Rangwala, editors, KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, pages 1379\u20131389. ACM, 2022.   \n[37] Yunzhe Qi, Yikun Ban, and Jingrui He. Graph neural bandits. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 1920\u20131931. ACM, 2023.   \n[38] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complexities. J. Mach. Learn. Res., 16:155\u2013186, 2015.   \n[39] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael M. Bronstein. Temporal graph networks for deep learning on dynamic graphs. CoRR, abs/2006.10637, 2020.   \n[40] Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. Karate Club: An API Oriented Opensource Python Framework for Unsupervised Learning on Graphs. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM \u201920), page 3125\u20133132. ACM, 2020.   \n[41] Luana Ruiz, Luiz F. O. Chamon, and Alejandro Ribeiro. Graphon neural networks and the transferability of graph neural networks. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[42] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12:2539\u20132561, 2011.   \n[43] Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Konstantinos Skianis, and Michalis Vazirgiannis. Grakel: A graph kernel library in python. Journal of Machine Learning Research, 21(54):1\u20135, 2020.   \n[44] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. Dyrep: Learning representations over dynamic graphs. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[45] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, Alexander M. Bronstein, and Emmanuel M\u00fcller. Netlsd: Hearing the shape of a graph. In Yike Guo and Faisal Farooq, editors, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018, pages 2347\u20132356. ACM, 2018.   \n[46] S. V. N. Vishwanathan, Karsten M. Borgwardt, Imre Risi Kondor, and Nicol N. Schraudolph. Graph kernels. CoRR, abs/0807.0093, 2008.   \n[47] Da Xu, Chuanwei Ruan, Evren K\u00f6rpeoglu, Sushant Kumar, and Kannan Achan. Inductive representation learning on temporal graphs. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[48] Jiaxuan You, Tianyu Du, and Jure Leskovec. ROLAND: graph learning framework for dynamic graphs. In Aidong Zhang and Huzefa Rangwala, editors, KDD \u201922: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, pages 2358\u20132366. ACM, 2022.   \n[49] Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. Towards better dynamic graph learning: New architecture and unified library. CoRR, abs/2303.13047, 2023.   \n[50] Dawei Zhou, Lecheng Zheng, Dongqi Fu, Jiawei Han, and Jingrui He. Mentorgnn: Deriving curriculum for pre-training gnns. In Mohammad Al Hasan and Li Xiong, editors, Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022, pages 2721\u20132731. ACM, 2022.   \n[51] Hongkuan Zhou, Da Zheng, Israt Nisa, Vasileios Ioannidis, Xiang Song, and George Karypis. TGL: A general framework for temporal GNN training on billion-scale graphs. CoRR, abs/2203.14883, 2022. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Pseudo Code of Temp- ${\\bf G}^{3}{\\bf N T K}$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The pseudo-code of Temp- $\\mathbf{\\cdotG^{3}N T K}$ is provided below. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Pseudo-code for computing Temp-G3NTK value between $G,G^{\\prime}$ at time $t$   \nInput: node embeddings of $G,G^{\\prime}$ at time $t$ $:{\\bf h}(t),{\\bf h}^{\\prime}(t)$ ; number of BLOCK operations $L$   \nOutput: Temp- $\\mathrm{G^{3}N T\\bar{K}}$ value between $G,G^{\\prime}$ at time $t$ : $K$   \n1: $K=0$   \n2: for $u\\in V^{(t)}$ do   \n3: for $u_{\\ast}^{\\prime}\\in V^{\\prime(t)}$ do   \n4: $\\begin{array}{r l}&{\\boldsymbol{\\Theta}_{u,u^{\\prime}}^{(0)^{-}}\\leftarrow\\mathbf{h}_{u}(\\bar{t})^{T}\\mathbf{h}_{u^{\\prime}}(t);}\\\\ &{\\boldsymbol{\\Sigma}_{u,u^{\\prime}}^{(0)}\\leftarrow\\mathbf{h}_{u}(t)^{T}\\mathbf{h}_{u^{\\prime}}(t);}\\\\ &{\\mathbf{for}\\,l\\in[1,\\dots,L]\\,\\mathbf{do}}\\\\ &{\\quad\\boldsymbol{\\Sigma}_{u,u^{\\prime}}^{(l)}\\leftarrow\\frac{\\sqrt{1-\\operatorname{arccos}\\left(\\boldsymbol{\\Sigma}_{u,u^{\\prime}}^{(l-1)}\\right)^{2}}}{2\\pi}}\\\\ &{\\dot{\\boldsymbol{\\Sigma}}_{u,u^{\\prime}}^{(l)}\\leftarrow\\frac{\\pi-\\operatorname{arccos}\\left(\\boldsymbol{\\Sigma}_{u,u^{\\prime}}^{(l-1)}\\right)}{2\\pi}}\\\\ &{\\boldsymbol{\\Theta}_{u,u^{\\prime}}^{(l)}\\leftarrow\\boldsymbol{\\Theta}_{u,u^{\\prime}}^{(l-1)^{2}}\\cdot\\dot{\\boldsymbol{\\Sigma}}_{u,u^{\\prime}}^{(l-1)}+\\boldsymbol{\\Sigma}_{u,u^{\\prime}}^{(l-1)}}\\end{array}$   \n5:   \n6:   \n7:   \n8:   \n9:   \n10: end for   \n11: end for   \n12: end for   \n13: for $u\\in V^{(t)}$ do   \n14: for u\u2032 \u2208V \u2032(t) do   \n15: K \u2190K + \u0398(uL,u)\u2032   \n16: end for   \n17: end for ", "page_idx": 14}, {"type": "text", "text": "B Theoretical proof for Kernel Properties of Temp- ${\\bf G}^{3}{\\bf N T K}$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here, we present the full proof for Theorem 5.1 and Theorem 5.2. ", "page_idx": 14}, {"type": "text", "text": "Proof. For Theorem 5.1, we aim to prove that $K(G^{(t)},G^{\\prime(t)})=K(G^{\\prime(t)},G^{(t)})$ . ", "page_idx": 14}, {"type": "text", "text": "Given our proposed kernel function, $\\begin{array}{r}{K(G^{(t)},G^{\\prime(t)})\\;=\\;\\sum_{v\\in V(t)}\\sum_{v^{\\prime}\\in V^{\\prime}(t)}\\Theta^{(L)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}}\\end{array}$ , we first write down another equation, where the internal order is flipped, i.e., $K(G^{\\prime(t)},G^{(t)})\\,=$ $\\begin{array}{r}{\\sum_{v^{\\prime}\\in V^{\\prime}(t)}\\sum_{v\\in V(t)}\\Theta^{(L)}(G^{\\prime(t)},G^{(t)})_{v^{\\prime}v}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "We first prove that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta^{(l)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}=\\Theta^{(l)}(G^{\\prime(t)},G^{(t)})_{v^{\\prime}v},\\forall l,1\\leq l\\leq L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\triangleright\\mathrm{For}\\,l=1$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta^{(1)}(G^{(1)},G^{(0)})_{w}=\\Theta^{(0)}(G^{(1)},G^{(1)})_{w}\\cdot\\Omega^{(1)}(G^{(0)},G^{(1)})_{w}+\\Omega^{(1)}(G^{(0)},G^{(0)})_{w},}\\\\ &{=(h_{w}(t)^{\\top}h_{v}(t))\\cdot\\frac{\\pi-\\operatorname{arcoss}(\\Omega)^{(0)}(G^{(1)},G^{(0)})_{w}}{2\\pi}}\\\\ &{+\\frac{\\pi-\\operatorname{arcoss}(\\Omega)(G^{(0)},G^{(0)})_{w}}{2\\pi}+\\frac{\\sqrt{1-\\Omega^{(0)}(G^{(0)},G^{(0)})_{w}^{2}}}{2\\pi}}\\\\ &{=(h_{w}(t)^{\\top}h_{v}(t))\\cdot\\frac{\\pi-\\operatorname{arcoss}(h_{w}(t)^{\\top}h_{v}(t))}{2\\pi}+\\frac{\\pi-\\operatorname{arcoss}(h_{w}(t)^{\\top}h_{v}(t))}{2\\pi}+\\frac{\\sqrt{1-(h_{w}(t)^{\\top}h_{w}}}{2\\pi}}\\\\ &{=(h_{w}(t)^{\\top}h_{w}(t))\\cdot\\frac{\\pi-\\operatorname{arcoss}(\\Omega_{w}(t)^{\\top}h_{v}(t))}{2\\pi}+\\frac{\\pi-\\operatorname{arcoss}(h_{w}(t)^{\\top}h_{w}(t))}{2\\pi}+\\frac{\\sqrt{1-(h_{w}(t)^{\\top}h_{w}}}{2\\pi}}\\\\ &{=(h_{w}(t)^{\\top}h_{v}(t))\\cdot\\frac{\\pi-\\operatorname{arcoss}(\\Omega_{w}(t)^{\\top}h_{w}(t))}{2\\pi}}\\\\ &{=(h_{w}(t)^{\\top}h_{v}(t))\\cdot\\frac{\\pi-\\operatorname{arcoss}(\\Omega)(G^{(0)},G^{(0)})_{w}}{2\\pi}}\\\\ &{+\\frac{\\pi-\\operatorname{arcoss}(\\Omega)(G^{(0)},G^{(0)})_{w}}{2\\pi}+\\frac{\\sqrt{1-\\Omega^{(0)}(G^{(0)},G^{(0)})_{w}^{2}}}{2\\pi}}\\\\ \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u25b7Suppose $\\exists k\\in\\mathbb{N},1\\leq k\\leq L$ , such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Theta^{(k)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}=\\Theta^{(k)}(G^{\\prime(t)},G^{(t)})_{v^{\\prime}v}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta^{(k+1)}(G^{(t)},G^{(t)})_{v v^{\\prime}}=\\Theta^{(k)}(G^{(t)},G^{(t^{\\prime})})_{v v^{\\prime}}\\cdot\\dot{\\Sigma}^{(k+1)}(G^{(t)},G^{(t^{\\prime})})_{v v^{\\prime}}+\\Sigma^{(k+1)}(G^{(t)},G^{(t^{\\prime})})_{v v^{\\prime}}}\\\\ &{=\\Theta^{(k)}(G^{(t)},G^{(t)})_{v v^{\\prime}}\\cdot\\frac{\\pi-\\operatorname{arccos}(\\Sigma^{(k)}(G^{(t)},G^{(t^{\\prime})})_{v v^{\\prime}})}{2\\pi}}\\\\ &{+\\frac{\\pi-\\operatorname{arccos}(\\Sigma^{(k)}(G^{(t)},G^{(t)})_{v v^{\\prime}})}{2\\pi}+\\frac{\\sqrt{1-\\Sigma^{(k)}(G^{(t)},G^{(t)})_{v v^{\\prime}}}}{2\\pi}}\\\\ &{=\\Theta^{(k)}(G^{(t)},G^{(t)})_{v^{\\prime}v}\\cdot\\frac{\\pi-\\operatorname{arccos}(\\Sigma^{(k)}(G^{(t)},G^{(t)})_{v^{\\prime}v})}{2\\pi}}\\\\ &{+\\frac{\\pi-\\operatorname{arccos}(\\Sigma^{(k)}(G^{(t)},G^{(t)})_{v^{\\prime}v})}{2\\pi}+\\frac{\\sqrt{1-\\Sigma^{(k)}(G^{(t)},G^{(t)})_{v^{\\prime}v}}}{2\\pi}}\\\\ &{=\\Theta^{(k+1)}(G^{(t)},G^{(t)})_{v^{\\prime}v}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, if $\\Theta^{(k)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}\\quad=\\quad\\Theta^{(k)}(G^{\\prime(t)},G^{(t)})_{v^{\\prime}v}$ , then $\\begin{array}{r l}{\\Theta^{(k+1)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}}&{{}=}\\end{array}$ $\\Theta^{(k+1)}(G^{\\prime(t)},G^{(t)})_{v^{\\prime}v}$ . Moreover, we have proven that $\\Theta^{(1)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}=\\Theta^{(1)}(G^{\\prime(t)},G^{(t)})_{v^{\\prime}v}$ . Thus, by induction, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta^{(l)}({G}^{(t)},{G}^{\\prime(t)})_{v v^{\\prime}}=\\Theta^{(l)}({G}^{\\prime(t)},{G}^{(t)})_{v^{\\prime}v},\\forall l,1\\leq l\\leq L.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathrm{{\\boldmath~\\cal~C}~}}(G(t),G^{\\prime}(t))=\\sum_{v\\in V(t)}\\sum_{v^{\\prime}\\in V^{\\prime}(t)}\\Theta^{(L)}(G^{(t)},G^{\\prime(t)})_{v v^{\\prime}}=\\sum_{v^{\\prime}\\in V^{\\prime}(t)}\\sum_{v\\in V(t)}\\Theta^{(L)}(G^{\\prime(t)},G^{(t)})_{v^{\\prime}v}=K(G(t),G^{\\prime}(t))_{v^{\\prime}v},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The proof for Theorem 5.1 is completed. ", "page_idx": 15}, {"type": "text", "text": "Next, we elaborate on the proof for Theorem 5.2. To be specific, we prepared two options to demonstrate the proof. ", "page_idx": 15}, {"type": "text", "text": "$\\triangleright$ Option #1: ", "page_idx": 15}, {"type": "text", "text": "Proof. In order to prove that Temp- $\\mathrm{G^{3}N T K}$ is positive semi-definite, we need to prove the following statement. Given $n$ temporal graphs $G_{1},\\ldots,G_{n}$ and any $c_{1},\\ldots,c_{n}\\in\\mathbb{R}$ then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})\\geq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Intuitively, we can view the right-hand side of Eq. 18 as the summation of all entries of the following matrix, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{K}=\\left(\\begin{array}{c c c c}{c_{1}c_{1}K(G_{1}^{(t)},G_{1}^{(t)})}&{c_{1}c_{2}K(G_{1}^{(t)},G_{2}^{(t)})}&{\\cdot\\cdot\\cdot}&{c_{1}c_{n}K(G_{1}^{(t)},G_{n}^{(t)})}\\\\ {c_{2}c_{1}K(G_{2}^{(t)},G_{1}^{(t)})}&{c_{2}c_{2}K(G_{2}^{(t)},G_{2}^{(t)})}&{\\cdot\\cdot\\cdot}&{c_{2}c_{n}K(G_{2}^{(t)},G_{n}^{(t)})}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {c_{n}c_{1}K(G_{n}^{(t)},G_{1}^{(t)})}&{c_{n}c_{2}K(G_{n}^{(t)},G_{2}^{(t)})}&{\\cdot\\cdot}&{c_{n}c_{n}K(G_{n}^{(t)},G_{n}^{(t)})}\\end{array}\\right)\\in\\mathbb{R}^{n\\times n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "whose $(i,j)^{\\mathrm{th}}$ entry is $c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})$ . ", "page_idx": 16}, {"type": "text", "text": "Then, we can re-write Eq. 18 by Temp- $\\mathrm{G^{3}N T K}$ \u2019s formula stated in Eq. 12 as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})=\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sum_{v=1}^{m_{i}}\\sum_{v^{\\prime}=1}^{m_{j}}c_{i}c_{j}\\Theta^{(L)}(G_{i}^{(t)},G_{j}^{(t)})_{v v^{\\prime}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $m_{i}$ is the number of nodes of $G_{i}$ , $\\forall i\\in\\{1,\\ldots,n\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Next, we consider the graph $G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup G_{n}^{(t)}$ , whose vertex, edge set is the union of all $G_{i}^{(t)}$ \u2019s vertex, edge set $(\\forall i\\in\\{1,\\ldots,n\\})$ , respectively. Then the number of nodes of $G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup\\dot{G}_{n}^{(t)}$ if $\\textstyle{\\overline{{m}}}=\\sum_{i=1}^{n}m_{i}$ . Additionally, we re-index the nodes of $G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup\\overset{\\cdot}{G}_{n}^{(t)}$ as follows. The $j^{\\mathrm{th}}$ node of graph $G_{i}^{(t)}$ is the $((\\sum_{p=1}^{i-1}m_{p})+j)^{\\mathrm{th}}$ node of $G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup G_{n}^{(t)}.$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sum_{v=1}^{m_{i}}\\sum_{v^{\\prime}=1}^{m_{j}}c_{i}c_{j}\\Theta^{(L)}(G_{i}^{(t)},G_{j}^{(t)})_{v v^{\\prime}}=}}\\\\ {{\\displaystyle\\sum_{v=1}^{m}\\sum_{v^{\\prime}=1}^{m}a_{v}a_{v}^{\\prime}\\Theta^{(L)}(G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup G_{n}^{(t)},G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup G_{n}^{(t)})_{v,v^{\\prime}}\\geq0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $a_{v}=c_{i}$ if $\\begin{array}{r}{\\sum_{q=1}^{i-1}m_{q}<v\\leq\\sum_{p=1}^{i}m_{p}}\\end{array}$ for $(i\\geq1)$ ), and if $v\\leq m_{1}$ then $a_{v}=c_{1}$ ", "page_idx": 16}, {"type": "text", "text": "Intuitively, the right-hand side of Eq. 21 is the summation of all entries of the following matrix $\\mathbf{O}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\left(\\begin{array}{c c c c c}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!c__{1}c_{1}\\Theta^{(L)}(G_{1}^{(t)},G_{1}^{(t)})}&{c_{1}c_{2}\\Theta^{(L)}(G_{1}^{(t)},G_{2}^{(t)})}&{\\cdot\\cdot\\cdot\\cdot\\cdot\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "whose $(i,j)^{\\mathrm{th}}$ entry is an $m_{i}\\times m_{j}$ matrix $c_{i}c_{j}\\Theta^{(L)}(G_{i}^{(t)},G_{j}^{(t)})$ and $\\Theta^{(L)}(G_{i}^{(t)},G_{j}^{(t)})$ is the kernel matrix defined iteratively via Eq. 11 for all pair of nodes from $G_{i}^{(t)}$ and $G_{j}^{(t)}$ . We can regard $\\mathbf{O}$ (defined in Eq. 22) as the \u201cnode-view\u201d expansion of $\\mathbf{K}$ in Eq. 19. ", "page_idx": 16}, {"type": "text", "text": "As $a_{1},\\dotsc,a_{\\overline{{m}}}\\in\\mathbb{R}$ , and $\\Theta^{(L)}$ is the kernel matrix constructed on feature vector of each pair of nodes of G1 $G_{1}^{(t)}\\cup\\cdots\\cup G_{n}^{(t)}$ , so the last inequality in Eq. 21 holds, as $\\Theta^{(L)}$ is positive semi-definite on the space of node features vector [19]. ", "page_idx": 16}, {"type": "text", "text": "Finally, we conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})=}&{}\\\\ {\\displaystyle\\sum_{v=1}^{\\overline{{n}}}\\sum_{v^{\\prime}=1}^{\\overline{{m}}}a_{v}a_{v}^{\\prime}\\Theta^{(L)}(G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup G_{n}^{(t)},G_{1}^{(t)}\\cup G_{2}^{(t)}\\cup\\cdots\\cup G_{n}^{(t)})_{v,v^{\\prime}}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, the proof for Theorem 5.2 is completed. ", "page_idx": 17}, {"type": "text", "text": "\u25b7Option #2: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. In order to prove that Temp- $\\mathrm{G^{3}N T K}$ is positive semi-definite, we need to prove the following statement. Given $n$ temporal graphs $G_{1},\\ldots,G_{n}$ and any $c_{1},\\ldots,c_{n}\\in\\mathbb{R}$ then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})\\geq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We re-write Eq. 24 as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{m}\\sum_{n=1}^{n}c_{i}K(G_{t}^{(1)},G_{t}^{(2)})=}\\\\ &{=\\displaystyle\\left(\\sum_{t=1}^{m}\\sum_{j=1}^{n}c_{i}^{j}K(G_{t}^{(1)},G_{t}^{(1)})+c_{j}^{j}K(G_{t}^{(1)},G_{t}^{(2)})+c_{6}\\gamma K(G_{t}^{(1)},G_{t}^{(2)})+c_{7}K(G_{t}^{(1)},G_{t}^{(2)},G_{t}^{(1)})\\right)}\\\\ &{+\\displaystyle\\sum_{t=1}^{m}\\sum_{n=1}^{n}(c_{i}^{j},G_{t}^{(1)}(G_{t},G_{t})+(-c_{j}^{j})K(G_{t}^{(1)},G_{t})+(-c_{6}\\gamma K(G_{t}^{(1)},G_{t}))}\\\\ &{\\quad\\neq\\displaystyle\\sum_{t=1}^{m}\\sum_{n=1}^{n}c_{i}c_{i}K(G_{t}^{(1)},G_{t}^{(2)})=\\displaystyle\\sum_{t=1}^{m}\\sum_{j=1}^{n}(c_{i}^{j},G_{t}^{(2)})K(G_{i},G_{t})+(-c_{j}^{j})K(G_{t},G_{t})}\\\\ &{+\\displaystyle\\left(\\sum_{t=1}^{m}\\sum_{j=1}^{n}c_{i}^{j}K(G_{t}^{(1)},G_{t}^{(2)})+c_{2}^{j}K(G_{t}^{(2)},G_{t}^{(2)})+c_{6}\\gamma K(G_{t}^{(1)},G_{t}^{(2)})+c_{6}\\gamma K(G_{t}^{(1)},G_{t}^{(2)},G_{t}^{(1)})\\right)}\\\\ &{\\Rightarrow2\\displaystyle\\sum_{t=1}^{m}c_{1}\\gamma K(G_{t}^{(1)},G_{t}^{(2)})=2\\displaystyle\\sum_{t=1}^{m}\\sum_{i=1}^{n}(c_{i}^{j}-2)K(G_{t},G_{t})}\\\\ &{+\\displaystyle\\left(\\sum_{t=1}^{m}\\sum_{j=1}^\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we aim to prove that (1) for each $i,j\\in\\{1,\\ldots,n\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bigg(\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}^{2}K(G_{i}^{(t)},G_{i}^{(t)})+c_{j}^{2}K(G_{j}^{(t)},G_{j}^{(t)})+c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})+c_{j}c_{i}K(G_{j}^{(t)},G_{i}^{(t)})\\bigg)\\geq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by proving that for each $i,j\\in\\{1,\\ldots,n\\}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nc_{i}^{2}K(G_{i}^{(t)},G_{i}^{(t)})+c_{j}^{2}K(G_{j}^{(t)},G_{j}^{(t)})+c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})+c_{j}c_{i}K(G_{j}^{(t)},G_{i}^{(t)})\\geq0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and (2) for each $i\\in\\{1,\\ldots,n\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n2n\\sum_{i=1}^{n}(-c_{i}^{2})K(G_{i}^{(t)},G_{i}^{(t)})\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "by proving that for each $i\\in\\{1,\\ldots,n\\}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n(-c_{i}^{2})K(G_{i}^{(t)},G_{i}^{(t)})\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u25b7\u25b7For proving $(I)$ : ", "page_idx": 18}, {"type": "text", "text": "For each $i,j\\,\\in\\,\\{1,\\dots,n\\}$ , suppose that $G_{i},G_{j}$ have $p,q$ nodes, respectively, then we have the following equality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{i}^{2}K(G_{i}^{(t)},G_{i}^{(t)})+c_{j}^{2}K(G_{j}^{(t)},G_{j}^{(t)})+c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})+c_{j}c_{i}K(G_{j}^{(t)},G_{i}^{(t)})=}\\\\ &{\\displaystyle=c_{i}^{2}\\sum_{v=1}^{p}\\sum_{v^{\\prime}=1}^{p}\\Theta^{(L)}(G_{i}^{(t)},G_{i}^{(t)})_{v v^{\\prime}}+c_{j}^{2}\\displaystyle\\sum_{v=1}^{q}\\sum_{v^{\\prime}=1}^{q}\\Theta^{(L)}(G_{j}^{(t)},G_{j}^{(t)})_{v v^{\\prime}}+}\\\\ &{c_{i}c_{j}\\displaystyle\\sum_{v=1}^{p}\\sum_{v^{\\prime}=1}^{q}\\Theta^{(L)}(G_{i}^{(t)},G_{j}^{(t)})_{v v^{\\prime}}+c_{j}c_{i}\\displaystyle\\sum_{v^{\\prime}=1}^{q}\\sum_{v=1}^{p}\\Theta^{(L)}(G_{j}^{(t)},G_{i}^{(t)})_{v^{\\prime}v}}\\\\ &{\\displaystyle=\\sum_{v=1}^{p+q}\\sum_{v^{\\prime}=1}^{p+q}a_{v}a_{v}^{\\prime}\\Theta^{(L)}(G_{i}^{(t)}\\cup G_{j}^{(t)},G_{i}^{(t)}\\cup G_{j}^{(t)})_{v v^{\\prime}}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, we can regard $G_{i}^{(t)}\\cup G_{j}^{(t)}$ as a graph, whose vertex, edge set is the union of $G_{i}^{(t)}$ \u2019s vertex, edge set and $G_{j}^{(t)}$ \u2019s vertex, edge set, respectively, while $a_{v},a_{v}^{\\prime}$ is either $c_{i}$ or $c_{j}$ . Therefore, since $a_{1},\\dotsc.\\dotsc a_{p+q}\\in\\mathbb{R}$ , and $\\Theta^{(L)}(G_{i}^{(t)}\\cup G_{j}^{(t)},G_{i}^{(t)}\\cup G_{j}^{(t)})_{v v^{\\prime}},\\forall v,v^{\\prime}\\in\\{1,\\ldots,(p+q)\\}$ is equivalent to constructing a kernel matrix on the $(p+q)$ node feature vectors of $G_{i}^{(t)}\\cup G_{j}^{(t)}$ , so the last inequality holds, as $\\Theta^{(L)}$ is positive semi-definite on the space of node features vector [19]. Therefore, we can infer that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bigg(\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}^{2}K(G_{i}^{(t)},G_{i}^{(t)})+c_{j}^{2}K(G_{j}^{(t)},G_{j}^{(t)})+c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})+c_{j}c_{i}K(G_{j}^{(t)},G_{i}^{(t)})\\bigg)\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u25b7\u25b7For proving (2): ", "page_idx": 18}, {"type": "text", "text": "Next, for each $i$ , suppose $G_{i}$ has $k$ nodes then ", "page_idx": 18}, {"type": "equation", "text": "$$\n-c_{i}^{2})K(G_{i}^{(t)},G_{i}^{(t)})=(-c_{i}^{2})\\sum_{v=1}^{k}\\sum_{v^{\\prime}=1}^{k}\\Theta^{(L)}(G_{i}^{(t)},G_{i}^{(t)})_{v v^{\\prime}}=\\sum_{v=1}^{k}\\sum_{v^{\\prime}=1}^{k}b_{v}b_{v^{\\prime}}\\Theta^{(L)}(G_{i}^{(t)},G_{i}^{(t)})_{v v^{\\prime}}\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $b_{1}=\\cdot\\cdot=b_{k}=(-c_{i}^{2})$ . ", "page_idx": 18}, {"type": "text", "text": "As $b_{1},\\ldots,b_{k}\\in\\mathbb{R}$ , and $\\Theta^{(L)}(G_{i}^{(t)},G_{i}^{(t)})_{v v^{\\prime}},\\forall v,v^{\\prime}\\in\\{1,\\ldots,k\\}$ is equivalent to the pair-wise kernel matrix constructed on the set of node features of $G_{i}$ , so the inequality holds, due to the positive semi-definite on the node feature space property of $\\Theta^{(L)}$ [19]. This results in ", "page_idx": 18}, {"type": "equation", "text": "$$\n2n\\sum_{i=1}^{n}(-c_{i}^{2})K(G_{i}^{(t)},G_{i}^{(t)})\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From Eq. 29 and Eq. 27, we now have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})=2n\\sum_{i=1}^{n}(-c_{i}^{2})K(G_{i},G_{i})}\\\\ &{+\\left(\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}^{2}K(G_{i}^{(t)},G_{i}^{(t)})+c_{j}^{2}K(G_{j}^{(t)},G_{j}^{(t)})+c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})+c_{j}c_{i}K(G_{j}^{(t)},G_{i}^{(t)})\\right)\\geq0}\\\\ &{\\Rightarrow\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(G_{i}^{(t)},G_{j}^{(t)})\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the proof for Theorem 5.2 is now completed. ", "page_idx": 19}, {"type": "text", "text": "C Genralization Bound of Temp- ${\\bf G}^{3}{\\bf N T K}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide the full proof for Theorem 5.3. We first provide some background knowledge on the Sequential Rademacher Complexity measures [38, 26], and then we derive at the full proof of Theorem 5.3. ", "page_idx": 19}, {"type": "text", "text": "C.1 Preliminaries ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our Temporal Graph Classification setting, given a temporal $G$ and its label $y$ , if we want to make predictions about $G$ at time $t$ then we apply our predictor on the snapshot $G^{(t)}$ , i.e., we leverage all information at previous timestamps $\\bar{t}\\,(\\bar{t}<\\bar{t})$ . ", "page_idx": 19}, {"type": "text", "text": "Suppose that $G$ has $T$ unique timestamps $t_{1},\\dots,t_{T}$ , then we can obtain $T$ snapshots $G_{1},\\L...,G_{T}$ , where $G_{i}=G^{(t_{i})}$ . Therefore, we can re-formulate our setting as follows: we consider a general time series prediction, where the predictor receives a realization $\\left((G_{1},t_{1}),\\dots,(G_{T},t_{T})\\right)$ generated by some stochastic processes. ", "page_idx": 19}, {"type": "text", "text": "To simplify notations, we let $f$ be the regression kernel predictor, i.e., $f_{k e r n e l}$ . The objective of our predictor $f$ is, at any timestamp $t_{i}$ , achieving a small error $\\mathbb{E}[\\ell(f((\\bar{G}_{i},t_{i})),y)|((G_{1},\\bar{t_{1}}),\\ldots,(G_{i-1},t_{i-1}))]$ conditioned on previous snapshots, given a loss function $\\ell:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ . ", "page_idx": 19}, {"type": "text", "text": "For shorter notation, we let $g(Z)=\\ell(f((G_{i},t_{i}),y)$ for $Z=((G_{i},t_{i}),y)\\in\\mathcal{Z}$ and let the family function $\\mathcal{G}=\\{((G_{i},t_{i}),y)\\rightarrow\\ell(f((G_{i},t_{i}),y)\\}$ contain such functions $g$ . We assume a bounded, $\\alpha$ \u2212Lipschitz loss function, that is $g(Z)\\in[0,1]$ for any $Z\\in{\\mathcal{Z}}$ . Finally, we use $\\mathbf{Z}_{a}^{b}$ to denote the sequences $Z_{a},Z_{a+1},\\ldots,Z_{b}$ , where $Z_{i}=((G_{i},t_{i}),y)$ . ", "page_idx": 19}, {"type": "text", "text": "In order to derive the Sequential Rademacher Complexity, we first introduce the definition of a complete binary tree. ", "page_idx": 19}, {"type": "text", "text": "We adopt the following definition of a complete binary tree from [38, 26]: a $\\mathcal{Z}-$ valued complete binary tree $\\mathbf{z}$ is a sequence of $\\left(z_{1},\\ldots,z_{T}\\right)$ of $T$ mappings, where $z_{i}:\\{\\pm1\\}^{i-1}\\rightarrow{\\mathcal{Z}}$ . A path in the tree is $\\sigma=(\\sigma_{1},\\dots,\\sigma_{T-1})$ . To simplify the notation, we write $z_{i}(\\pmb{\\sigma})=(\\sigma_{1},\\dots,\\sigma_{i-1})$ . ", "page_idx": 19}, {"type": "text", "text": "Next, we introduce how to sample sequential data $Z_{1},\\ldots,Z_{i}$ using the aforementioned binary tree. We adopt the sampling process from [38, 26] as follows: given a stochastic process distributed to the distribution $\\mathbb{P}$ with $\\bar{\\mathbb{P}}_{i}\\!\\left(\\cdot|\\mathbf{z}_{1}^{i-1}\\right)$ , denoting the conditional distribution based on $z_{1},\\dotsc.\\dotsc,z_{i-1}$ , we sample a ${\\mathcal{Z}}\\times{\\mathcal{Z}}$ based on the following procedure. We start by drawing two independent samples $Z_{1},Z_{1}^{\\prime}$ from $\\mathbb{P}_{1}$ , then, in the left child of the root we sample $Z_{2},^{-}Z_{2}^{\\prime}\\sim\\bar{\\mathbb{P}_{2}(}.|Z_{1})$ and in the right child of the root, we sample $Z_{2},Z_{2}^{\\prime}\\sim\\mathbb{P}_{2}(.|Z_{1}^{\\prime})$ . ", "page_idx": 19}, {"type": "text", "text": "More generally, for a node that can be reached by a path $(\\sigma_{1},\\ldots,\\sigma_{i-1})$ , we draw $Z_{i},Z_{i}^{\\prime}\\,\\,\\sim$ $\\mathbb{P}_{i}(.|I_{1}\\mathbf{\\bar{(}}\\sigma_{1}),..\\dots,I_{i-1}(\\sigma_{i-1})$ , where the indicator $I_{j}(1)\\,=\\,Z_{j},\\bar{I}_{j}(-1)\\,=\\,Z_{j}^{\\prime}$ . In this manner, we derive at the Sequential Rademacher Complexity of a function class $\\mathcal{G}$ that acts on $\\mathbf{z}$ is defined as follows [38]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Re_{T}^{\\mathrm{seq}}(\\mathcal{G},\\mathbf{z})=\\mathbb{E}\\left[\\operatorname*{sup}_{g\\in\\mathcal{G}}\\frac{1}{T}\\sum_{i=1}^{T}g(z_{i}(\\pmb{\\sigma}))\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ${\\bf z}$ is an $\\mathcal{Z}-$ valued complete binary trees with depth $T$ and $\\pmb{\\sigma}$ is a sequence of Rademacher random variables. ", "page_idx": 20}, {"type": "text", "text": "As stated in Theorem 5.3, the key quantity of interest in our analysis is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\ell\\in\\mathcal{L}}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}[\\ell(f((G_{i},t_{i})),y)|((G_{1},t_{1}),\\ldots,(G_{i-1},t_{i-1}))]-\\ell(f((G_{i},t_{i})),y)\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and we can re-write this quantity as follows and establish a data-dependent bound for this term in Appendix C.2 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{g\\in\\mathcal{G}}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}[g(Z_{i})|\\mathbf{Z}_{1}^{(i-1)}]-g(Z_{i})\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For more details about the Sequential Complexity measure, we defer readers to [38] and [26]. ", "page_idx": 20}, {"type": "text", "text": "C.2 Detailed Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first bound Eq. 31 by the Sequential Rademacher complexity of $\\mathcal{F}$ , the family function class contains functions such as our kernel regression predictor, $f_{k e r n e l}$ , (Lemma C.1) then continue to bound the Sequential Rademacher complexity of $\\mathcal{F}$ by the data-dependent term (Lemma. C.2). ", "page_idx": 20}, {"type": "text", "text": "Lemma C.1. ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{g\\in\\mathcal{G}}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}[g(Z_{i})|\\mathbf{Z}_{1}^{(i-1)}]-g(Z_{i})\\right]\\leq2\\alpha\\Re_{T}^{s e q}(\\mathcal{F})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We first state that the following inequalities hold: $\\mathbb{E}[g(Z_{i})|\\mathbf{Z}_{1}^{i-1}]=\\mathbb{E}[g(Z_{i}^{\\prime})|\\mathbf{Z}_{1}^{i-1})]$ , since $Z_{i},Z_{i}^{\\prime}$ are indepedently drawn from $\\mathbb{P}_{i}(.|\\mathbf{Z}_{1}^{i-1}))$ and $\\mathbb{E}[g(Z_{i})|\\mathbf{Z}_{1}^{i-1}]=\\mathbb{E}[g(Z_{i})|\\mathbf{Z}_{1}^{T}]$ , and $g(Z_{i})$ only depends on $\\mathbf{Z}_{1}^{i-1}$ . Therefore, we obtain the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\left[\\frac{1}{T}\\sum_{i=1}^{T}\\mathbb{E}[g(Z_{i})|\\mathbf{Z}_{1}^{(i-1)}]-g(Z_{i})\\right]\\right]}\\\\ &{=\\mathbb{E}\\left[\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\frac{1}{T}\\mathbb{E}\\Bigg[\\underset{i=1}{\\overset{T}{\\sum}}(g(Z_{i}^{\\prime})-g(Z_{i}))|\\mathbf{Z}_{1}^{T}\\Bigg]\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\frac{1}{T}\\mathbb{E}\\Bigg[\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\underset{i=1}{\\overset{T}{\\sum}}(g(Z_{i}^{\\prime})-g(Z_{i}))\\Bigg]\\right]}\\\\ &{=\\frac{1}{T}\\mathbb{E}\\Bigg[\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\underset{i=1}{\\overset{T}{\\sum}}(g(Z_{i}^{\\prime})-g(Z_{i}))\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality holds by using Jensen\u2019s inequality, and the last expectation is taken over all joint sequences $\\mathbf{Z}_{1}^{\\bar{T}},\\mathbf{Z}_{1}^{\\prime\\bar{T}}$ . ", "page_idx": 20}, {"type": "text", "text": "Since $g(Z_{i})=\\ell(f((G_{i},t_{i}),y)$ and $\\ell$ is $\\alpha$ \u2212Lipschitz, thus we obtain the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\underset{g\\in\\mathcal{A}_{i=1}}{\\operatorname*{sup}}\\sum_{i=1}^{T}(g(Z_{i}^{\\prime})-g(Z_{i}))\\Bigg]}\\\\ &{=\\mathbb{E}\\Bigg[\\underset{\\ell\\in\\mathcal{L}_{i}}{\\operatorname*{sup}}\\sum_{i=1}^{T}\\ell(f((G_{i}^{\\prime},t_{i}^{\\prime})),y)-\\ell(f((G_{i},t_{i}),y)\\Bigg]}\\\\ &{\\leq\\mathbb{E}\\Bigg[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,\\alpha\\sum_{i=1}^{T}(f((G_{i}^{\\prime},t_{i}^{\\prime}))-f((G_{i},t_{i}))+(y-y)\\Bigg]}\\\\ &{=\\alpha\\mathbb{E}\\Bigg[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{T}(f(X_{i}^{\\prime})-f(X_{i}))\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first inequality holds, due to the fact that $\\ell$ is $\\alpha$ \u2212Lipschitz. ", "page_idx": 21}, {"type": "text", "text": "where $X_{i}\\,=\\,(G_{i},t_{i}),X_{i}^{\\prime}\\,=\\,(G_{i}^{\\prime},t_{i}^{\\prime})$ . Note that since $y$ is fixed with respect to $i$ $(1\\,\\leq\\,i\\,\\leq\\,T)$ ), so $Z_{i}=(X_{i},y)$ . Therefore, we can derive at a $\\mathcal{X}$ -valued complete binary tree $\\mathbf{x}$ , ${\\bf x}_{i}(\\pmb{\\sigma})$ , and the sequences ${\\bf X}_{1}^{T}$ , $\\mathbf{X}{'}_{1}^{T}$ that are similar to the manner of deriving at $\\mathbf{z},\\mathbf{z}_{i}(\\pmb{\\sigma}),\\mathbf{Z}_{1}^{T},{\\mathbf{Z}_{\\mathrm{~1~}}^{\\prime T}}$ , respectively. Simply say, $\\mathbf{x}$ is $\\mathbf{z}$ when omitting the label $y$ . Since the last expectation is taken over all joint sequences $\\mathbf{\\dot{X}}_{1}^{T},\\mathbf{X}_{1}^{\\prime T}$ , so given Rademacher random variables, $\\sigma_{1},\\ldots,\\sigma_{T}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{T}(f(X_{i}^{\\prime})-f(X_{i}))\\right]=\\mathbb{E}[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{T}\\sigma_{i}(f(X_{i}^{\\prime})-f(X_{i}))\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\underset{f\\in\\mathcal{F}_{i=1}}{\\operatorname*{sup}}\\sum_{i=1}^{T}(f(X_{i}^{t})-f(X_{i}))\\Bigg]}\\\\ &{=\\mathbb{E}\\Bigg[\\underset{f\\in\\mathcal{F}_{i=1}}{\\operatorname*{sup}}\\sum_{i=1}^{T}\\sigma_{i}(f(X_{i}^{t})-f(X_{i}))\\Bigg]}\\\\ &{=\\mathbb{E}_{X_{1},X_{1}^{t}\\sim\\mathbb{R}_{1}\\dots}\\mathbb{E}_{X_{T},X_{T}^{t}\\sim\\mathbb{R}_{1}(I_{1}(I_{1}(\\pi_{1}),\\dots,I_{T}(\\pi_{T}))\\Bigg[\\underset{f\\in\\mathcal{F}_{i-1}}{\\operatorname*{sup}}\\sum_{i=1}^{T}\\sigma_{i}(f(X_{i}^{t})-f(X_{i}))\\Bigg]}}\\\\ &{=\\mathbb{E}_{\\sigma}\\mathbb{E}_{\\mathbf{x}\\sim\\Gamma(\\mathbb{P})}\\Bigg[\\underset{f\\in\\mathcal{F}_{i=1}}{\\operatorname*{sup}}\\sum_{i=1}^{T}\\sigma_{i}(f(X_{i}^{t})-f(X_{i}))\\Bigg]}\\\\ &{=\\mathbb{E}_{\\sigma}\\mathbb{E}_{(\\mathbf{x},\\mathbf{x}^{t})\\sim T(\\mathbb{P})}\\Bigg[\\underset{f\\in\\mathcal{F}_{i=1}}{\\operatorname*{sup}}\\sum_{i=1}^{T}\\sigma_{i}(f(\\mathbf{x}_{i}^{t}(\\sigma))-f(\\mathbf{x}_{i}(\\sigma))\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{x}\\sim T(\\mathbb{P})$ denotes sampling a $\\mathcal{X}$ -valued complete binary tree $\\mathbf{x}$ with a given stochastic process $\\mathbb{P}$ . Thus, Eq. 33 is equivalent to: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\ensuremath{\\alpha}}\\;\\mathrm{\\ensuremath{\\mathbbE}}\\left[\\operatorname*{sup}_{\\theta\\in\\Theta_{r}}\\sum_{i=1}^{r}(f(X_{i}^{\\prime})-f(X_{i}))\\right]}\\\\ &{=\\mathrm{\\ensuremath{\\alpha}}\\;\\mathrm{\\ensuremath{\\mathbbE}}_{\\mathrm{S}}\\frac{\\mathrm{\\ensuremath{\\mathbbE}}_{1}}{\\mathrm{\\ensuremath{\\mathbbE}}_{1}(x,x)}\\mathrm{.\\ensuremath{\\mathbbE}}_{2}\\left[\\Biggl|\\operatorname*{sup}_{\\theta\\in\\Theta_{r}}\\sum_{i=1}^{r}\\sigma_{i}(f(X_{i}^{\\prime}(\\sigma))-f(\\mathbf{x}_{i},(\\sigma))\\Biggr)\\right]}\\\\ &{\\le\\mathrm{\\ensuremath{\\alpha}}\\;\\mathrm{\\ensuremath{\\mathbbE}}_{\\mathrm{S}}\\frac{\\mathrm{\\ensuremath{\\mathbbE}}_{1}}{\\mathrm{\\ensuremath{\\mathbbE}}_{1}(x,x)}\\mathrm{.\\ensuremath{\\mathbbE}}_{2}\\left[\\Biggl|\\operatorname*{sup}_{\\theta\\in\\Theta_{r}}\\sum_{i=1}^{r}\\sigma_{i}f(\\mathbf{x}_{i}^{\\prime}(\\sigma))+\\frac{\\mathrm{\\ensuremath{\\alpha}}\\mathrm{\\ensuremath{\\mathbbE}}_{3}}{\\mathrm{\\ensuremath{\\mathbbE}}_{2}(x,x)}\\sum_{i=1}^{r}\\sigma_{i}f(\\mathbf{x}_{i}(\\sigma))\\Biggr|\\right]}\\\\ &{=\\mathrm{\\ensuremath{\\alpha}}\\;\\mathrm{\\ensuremath{\\mathbbE}}_{\\mathrm{S}}\\frac{\\mathrm{\\ensuremath{\\mathbbE}}_{1}}{\\mathrm{\\ensuremath{\\mathbbE}}_{1}(x,x)}\\mathrm{.\\ensuremath{\\mathbb{-}}\\mathrm{\\ensuremath{\\mathbbP}}}[\\Biggl|\\operatorname*{sup}_{\\theta\\in\\Theta_{r}}\\sum_{i=1}^{r}\\sigma_{i}f(\\mathbf{x}_{i}^{\\prime}(\\sigma))\\Biggr|+\\mathrm{\\ensuremath{\\mathbb{R}}}_{\\mathrm{\\ensuremath{\\mathbbE}}_{1}(x,x)}\\mathrm{.\\ensuremath{\\mathbb{-}}\\mathrm{\\ensuremath{\\mathbb{-}}\\mathrm{\\ensuremath{\\mathbb{P}}}}[\\exp\\sum_{i=1}^{r}\\sigma_{i}f(\\mathbf{x}_{i}(\\sigma))\\Biggr]}\\\\ &{=2\\mathrm{\\ensuremath{\\alpha}}\\mathrm{\\ensuremath{\\mathbbE}}_{\\mathrm{\\ensuremath{\\mathbb{-}\\mathbb{\\mathbb{-\\mathbb{T}}}}[\\mathrm{\\ensuremath{\\mathbb{V}}}]}}\\left[\\operatorname*{sup}_{\\theta\\in\\Theta_{r\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which completes the proof for Lemma C.1. ", "page_idx": 21}, {"type": "text", "text": "Next, we establish a data-dependent bound for the Sequential Complexity measure $\\Re^{\\mathrm{seq}}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2. Given n i.i.d time series samples drawn from an underlying stochastic processes $\\mathbb{P}$ , $\\{(X^{(j)},y_{j})\\}_{j=1}^{n}$ . Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{j=1}^{n}\\Re_{T}^{s e q}(\\mathcal{F},\\mathbf{x}^{(j)})\\leq\\frac{2}{n}\\sum_{i=1}^{T}\\sqrt{\\mathbf{y}^{T}[\\mathbf{K}^{(i)}]^{-1}\\mathbf{y}\\cdot\\mathrm{tr}(\\mathbf{K}^{(i)})}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, $\\mathbf{x}^{(j)}$ is the binary tree corresponding to the time series $X^{(j)}$ , $\\mathbf{K}^{\\left(i\\right)}$ is the $n\\times n$ kernel gram matrix, whose pq-th entry is the Temp- $G^{3}N T K$ value of the $\\,\\,i\\!-\\!t h$ snapshot of $X^{(p)}$ and the $i$ -th snapshot of $X^{(\\bar{q})}$ , and y is the vector of labels, in which the $j$ -th entry is $[\\mathbf{y}]_{j}=y_{j}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}\\Re_{T}^{\\mathrm{seq}}(\\mathcal{F},\\mathbf{x}^{(j)})=\\frac{1}{n}\\sum_{j=1}^{n}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{T}\\sigma_{j i}f(\\mathbf{x}_{i}^{(j)}(\\pmb{\\sigma}_{j}))}\\\\ {\\displaystyle=\\sum_{i=1}^{T}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{j=1}^{n}\\sigma_{j i}f(X_{i}^{(j)})=\\sum_{i=1}^{T}\\hat{\\Re}_{n}(\\mathcal{F},i)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\hat{\\mathfrak{R}}_{n}(\\mathcal{F},i)$ is the empirical Rademacher complexity of $\\mathcal{F}$ with the $i$ -th snapshot of $n$ i.i.d samples Xi(1), . $X_{i}^{\\left(1\\right)},\\ldots,X_{i}^{\\left(n\\right)}$ . Since $\\mathcal{F}$ is a function class of kernel regression function, as proven in [4], we can bound $\\hat{\\mathfrak{R}}_{n}(\\mathcal{F},i)$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\Re}_{n}(\\mathcal{F},i)\\le\\frac{2}{n}\\sqrt{\\mathbf{y}^{T}[\\mathbf{K}^{(i)}]^{-1}\\mathbf{y}\\cdot\\mathrm{tr}(\\mathbf{K}^{(i)})}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which completes the proof for Lemma C.2. ", "page_idx": 22}, {"type": "text", "text": "Thus, using the results of Lemma C.2, we can bound Eq. 34 as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\alpha\\mathbb{E}_{\\mathbf{x}\\sim T(\\mathbb{P})}\\Bigg[\\mathfrak{R}_{T}^{\\mathrm{seq}}(\\mathcal{F},\\mathbf{x})\\Bigg]=2\\alpha\\mathbb{E}_{\\mathbf{x}\\sim T(\\mathbb{P})}\\Bigg[\\frac{1}{n}\\displaystyle\\sum_{j=1}^{n}\\mathfrak{R}_{T}^{\\mathrm{seq}}(\\mathcal{F},\\mathbf{x}^{(j)})\\Bigg]}\\\\ &{\\le2\\alpha\\mathbb{E}\\Bigg[\\frac{2}{n}\\displaystyle\\sum_{i=1}^{T}\\sqrt{\\mathbf{y}^{T}[\\mathbf{K}^{(i)}]^{-1}\\mathbf{y}\\cdot\\mathrm{tr}(\\mathbf{K}^{(i)})}\\Bigg]}\\\\ &{\\le\\alpha\\operatorname*{sup}_{i}\\frac{4}{n}\\sqrt{\\mathbf{y}^{T}[\\mathbf{K}^{(i)}]^{-1}\\mathbf{y}\\cdot\\mathrm{tr}(\\mathbf{K}^{(i)})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which completes the proof for Theorem 5.3. ", "page_idx": 22}, {"type": "text", "text": "D Convergence to Graphon Neural Tangent Kernel ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we first provide some background knowledge on Graphons, Graphons Neural Networks, and Graphons Neural Tangent Kernel, and then derive at the full proof for Theorem. 5.4. ", "page_idx": 22}, {"type": "text", "text": "D.1 Preliminaries ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We adopt the definition of Graphons, Graphons Neural Networks (WNN) [24], and Graphons Neural Tangent Kernel (WNTK) [24], and then extend these concepts to the settings of CTDGs. ", "page_idx": 22}, {"type": "text", "text": "Graphons ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Graphons are defined as bounded, symmetric, measurable function $W:[0,1]^{2}\\to[0,1]$ representing limits of sequences of dense graphs. ", "page_idx": 22}, {"type": "text", "text": "Given a graph sequence $\\{G_{n}\\}$ , where the $i$ -th graph in the sequence $G_{i}$ has $i$ nodes, let $\\mathbf{F}=(V^{\\prime},E^{\\prime})$ be an undirected graph, then the graph sequence $\\{G_{n}\\}$ is said to converge to the graphon $W$ in the sense that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}t(\\mathbf{F},G_{n})=t(\\mathbf{F},W)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $t(\\mathbf{F},G_{n})=\\hom(\\mathbf{F},G_{n})/n^{|V^{\\prime}|}$ , where $\\mathrm{hom}(\\mathbf{F},G_{n})$ is the number of homomorphisms between $\\mathbf{F}$ and $G_{n}$ , and $t(\\mathbf{F},W)$ can be similarly defined. ", "page_idx": 22}, {"type": "text", "text": "Thus, $t(\\mathbf{F},G_{n})$ is the density of homomorphisms between $F$ and $G_{n}$ . We can think of $\\mathbf{F}$ as motifs such as $k-$ cycles, or $k-$ cliques, so if the graph sequence $\\{G_{n}\\}$ converges to the graphon $W$ , then we can think of $G_{1},...,G_{n}$ of the graph sequence $\\{G_{n}\\}$ belongs to a graph family that has a certain amount of density of homomorphisms from $\\mathbf{F}$ , and that graph family is represented by $W$ . ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Therefore, the graphon $W$ can be seen as a generative model for stochastic graphs. In order to use $W$ to generate a graph $G_{n}=(V_{n},E_{n})$ with $n$ nodes, we first map each node $\\bar{i}\\in V_{n}(1\\leq i\\leq n)$ to the unit interval, i.e. $[0,1]$ , by uniformly sampling points $u_{i}(u_{i}\\in[0,1])$ , and the probability of nodes $i,j$ are connected in $G_{n}$ is $W(u_{i},u_{j})$ , hence we can regard $W$ is a weighted adjacency matrix for $G_{n}$ . ", "page_idx": 23}, {"type": "text", "text": "Graphons Neural Network ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Next, we would define a continuous message passing framework for graphon that corresponds to the neural architecture proposed in Section 3.1. ", "page_idx": 23}, {"type": "text", "text": "Firstly, we introduce the definition of Graphon signals as follows: Graphon signals are function $X\\,:\\,\\dot{[}0,1]^{2}\\,\\rightarrow\\,\\mathbb{R}$ , and $X$ has finite energy, i.e., $\\bar{X}\\,\\in\\,L_{2}([0,1]^{2})$ . In this way, we can think of $X(u_{i},u_{j})$ as the edge representation of an edge $(i,j)$ . We adopt the definition of graphon signals from [24] and extend it to edge features, instead of graphon signals function for node feature as stated in [24]. ", "page_idx": 23}, {"type": "text", "text": "Analogous to the sum neighborhood aggregation operation in equation (1), the aggregation operation for graphon $W$ and graphon signal $X$ can be defined as the function $T_{W}X:[0,\\stackrel{\\cdot}{1}]\\stackrel{\\textstyle-}{\\rightarrow}\\mathbb{R}^{d}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{W}X(u)=\\int_{0}^{1}W(u,v)X(u,v)d v\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $h\\equiv T_{W}X$ . If the aggregated information is further transformed by $L$ layers of MLPs (similar to Eq. 2) then for $l\\in[L],h^{(l)}$ is determined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\nh^{(l)}(u)=\\sigma(\\mathbf{H}^{(l)}h^{(l-1)})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $h^{(0)}=h$ , $\\mathbf{H}$ is the linear transformation and $\\sigma$ is the non-linear ReLU activation function. ", "page_idx": 23}, {"type": "text", "text": "Induced Graphon Neural Network for CTDGs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We adopt the definition of induced graphon and induced graphon signals from [24], extend them to the CTDGs setting, and determine an induced graphon neural network that correspond to our proposed temporal graph learning algorithm in Section 3.1. ", "page_idx": 23}, {"type": "text", "text": "Given a CTDG $G$ and the graphon $W$ that represents a graph family of $G$ , we would leverage the aforementioned graphon $W$ to determine the induced graphon and induced graphon signals that correspond to snapshots of a CTDG. At time $t$ , let the number of nodes of $\\bar{G}^{(t)}$ be $n(t)$ , let $W^{(t)}:[0,1]^{2}\\rightarrow[0,1]$ and $X^{(t)}:[0,1]^{2}\\rightarrow\\mathbb{R}^{d}$ denotes the induced graphon and induced graphon signals correspond to $G^{(t)}$ , respectively. We determine $W^{(t)}$ and $X^{(t)}$ as the followings: ", "page_idx": 23}, {"type": "text", "text": "For any $u,v\\in~[0,1]$ , let $I_{i}\\ =\\ \\Big[(i\\ -1)/n(t),\\ i/n(t)\\Big),1\\ \\leq\\ i\\ \\leq\\ n(t)$ and $\\bar{I}_{i}~=~\\Big[(i~-$ $1)/n(\\bar{t}),\\;i/n(\\bar{t})\\Big),1\\leq i\\leq n(\\bar{t})$ . If $u\\in I_{i},v\\in I_{j}$ , where $1\\leq i,j\\leq n(t)$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\nW^{(t)}(u,v)=\\frac{W^{(\\bar{t})}(u_{\\bar{i}},u_{\\bar{j}})\\mathbb{I}(i\\leq n(\\bar{t}))\\mathbb{I}(j\\leq n(\\bar{t}))+A^{(t)}(u_{i},u_{j})}{2\\cdot\\mathbb{I}(i\\leq n(\\bar{t}))\\mathbb{I}(j\\leq n(\\bar{t}))}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\bar{t}<t,u_{i}=(i-1)/n(t)$ , $u_{\\bar{i}}=(i-1)/n(\\bar{t}),1$ I is the indicator function, and $A^{(t)}(u_{i},u_{j})\\sim$ $B e r(W(u_{i},u_{j}))$ , where $B e r$ indicates the Bernoulli distribution. The initial state, i.e., $t=0$ would simply be $W^{(\\mathrm{{0}})}=A^{(0)}(u_{i},u_{j})$ , and we define the temporal graphon signal function at time $t$ as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nX^{(t)}(u,v)=\\int_{0}^{t}A^{(\\bar{t})}(u_{\\bar{i}},u_{\\bar{j}})\\mathbb{I}(i\\leq n(\\bar{t}))\\mathbb{I}(j\\leq n(\\bar{t}))d\\bar{t}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and we let the graphon signals function that associated with $W$ be $X(u,v)=W(u,v)$ . ", "page_idx": 23}, {"type": "text", "text": "In a similar manner to Equation (1), the sum aggregation opertaion at time $t$ would be: ", "page_idx": 23}, {"type": "equation", "text": "$$\nT_{W^{(t)}}X^{(t)}(u)=\\int_{0}^{1}W^{(t)}(u,v)X^{(t)}(u,v)d v\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the final result is $T_{W^{(t)}}X^{(t)}$ after $L$ layers of MLP transformation, similar as Eq. 2. ", "page_idx": 24}, {"type": "text", "text": "Graphon NTK and Induced Graphon Temp- $\\mathbf{\\mathbf{G}}^{3}\\mathbf{NTK}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let $f_{w n n}$ be the WNN defined by Eq. 37, 38, and $f_{t e m p-w n n}$ be the induced graphon neural networks defined by Eq. 39, 41, and 40. Similar to Eq. 6, given 2 graphons $W,W^{\\prime}$ and their signals $X,X^{\\prime}$ that correspond to 2 CTDGs $G,G^{\\prime}$ , respectively, also given parameters $\\mathbf{H}$ , then the NTK of $f_{w n n}(X,W,\\mathbf{H})$ and $f_{w n n}(X^{\\prime},W^{\\prime},\\mathbf{H})$ would be: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Theta_{w n n}(X,X^{\\prime},W,W^{\\prime})=\\mathbb{E}_{\\mathbf{H}\\sim N(0,1)}\\Bigg\\langle\\frac{\\partial f_{w n n}(X,W,\\mathbf{H})}{\\partial\\mathbf{H}},\\frac{\\partial f_{w n n}(X^{\\prime},W^{\\prime},\\mathbf{H})}{\\partial\\mathbf{H}}\\Bigg\\rangle\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and the Temp- $\\mathbf{\\cdotG^{3}N T K}$ of the induced graphon neural network at time $t$ would be: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{\\partial}_{t e m p-w n n}(X^{(t)},X^{\\prime(t)},W^{(t)},W^{\\prime(t)})=\\mathbb{E}_{\\mathbf{H}\\sim N(0,1)}\\Bigg\\langle\\frac{\\partial f_{t e m p-w n n}(X^{(t)},W^{(t)},\\mathbf{H})}{\\partial\\mathbf{H}},\\frac{\\partial f_{t e m p-w n n}(X^{(t)},W^{(t)},\\mathbf{H})}{\\partial\\mathbf{H}}\\Bigg\\rangle_{\\mathbf{H}\\sim N(0,1)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.2 Detailed Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We let $K_{W}(W,W^{\\prime})\\qquad\\equiv\\qquad\\Theta_{w n n}(X,X^{\\prime},W,W^{\\prime})$ and $K_{W}(W^{(t)},W^{\\prime(t)})\\qquad\\equiv$ $\\Theta_{t e m p-w n n}(X^{(t)},X^{\\prime(t)},W^{(t)},W^{\\prime(t)})$ , as we derive at the graphon signals $X,X^{\\prime}$ by $W,W^{\\prime}$ and the induced graphon signals $X^{(t)},X^{\\prime(t)}$ by $W^{(t)},W^{\\prime(t)}$ , so in Theorem Section 5.4, we decide to denote the NTK by $K_{W}(W,W^{\\prime})$ and the induced NTK by $K_{W}(W^{(t)},W^{\\prime(t)})$ for simpler notation. For simplicity, we let the number of BLOCK operations be $L=1$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|K_{W}(W,W^{(t)})-K_{W}(W^{(t)},W^{(t)}||)\\|=}\\\\ &{|\\Theta_{\\operatorname*{sup}}(X,X^{\\prime},W^{(t)},\\mathbf{H})-\\Theta_{\\operatorname*{tom}}(X^{(t)},X^{\\prime\\prime}|),W^{(t)},W^{(t)},\\Psi^{(t)},\\Psi^{(t)}\\|=}\\\\ &{=\\|J^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}-J^{\\prime}\\big(\\mathrm{H}T_{W}(X^{\\prime})X^{(t)}\\big)T_{W}\\sigma^{\\prime}\\big(X^{(t)},\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}(X^{\\prime})\\big)J^{\\prime}\\big)}\\\\ &{=\\|J^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}-J^{\\prime}\\big(\\mathrm{H}T_{W}(X^{\\prime})\\big)T_{W}\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}X^{(t)},\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X\\big)}\\\\ &{+\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}\\sigma,X^{(t)}\\big)T_{W}\\sigma,\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}-J^{\\prime}\\big(\\mathrm{H}T_{W}(X^{\\prime})\\big)T_{W}\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}\\sigma^{\\prime},X^{\\prime\\prime}\\big)}\\\\ &{\\leq\\|J^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}-J^{\\prime}\\big(\\mathrm{H}T_{W}\\sigma,X^{(t)}\\big)T_{W}\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}X^{\\prime}\\big)T_{W}X^{\\prime}\\sigma^{\\prime}\\big(\\mathrm{H}T_{W}\\sigma^{\\prime}\\big)}\\\\ &{+\\|\\sigma^{\\prime}\\big(\\\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The first inequality holds due to the triangle inequality, and the second inequality holds due to the property of the operator norm. ", "page_idx": 24}, {"type": "text", "text": "We can see that, in order to prove Theorem 5.4, it is sufficient to prove that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\left|\\left(\\sigma^{\\prime}({\\bf H}T_{W}{\\bf x})T_{W}X-\\sigma^{\\prime}({\\bf H}T_{W^{(t)}}X^{(t)})T_{W^{(t)}}X^{(t)}\\right)\\right|\\right|\\to0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\left|\\left(\\sigma^{\\prime}({\\bf H}T_{W^{\\prime}}X^{\\prime})T_{W^{\\prime}}X^{\\prime}-\\sigma^{\\prime}({\\bf H}T_{W^{\\prime}(t)}X^{\\prime(t)})T_{W^{\\prime}(t)}X^{\\prime(t)}\\right)\\right|\\right|\\to0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since Eq. 43 and Eq. 44 are essentially the same, we would focus on proving Eq. 43. We further applying algebraic manipulation on Eq. 43 as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\sigma^{\\prime}(\\mathbf{H}T_{W}X)T_{W}X-\\sigma^{\\prime}(\\mathbf{H}T_{W^{(t)}}X^{(t)})T_{W^{(t)}}X^{(t)}\\right|\\right|}\\\\ &{=\\left|\\left|\\sigma^{\\prime}(\\mathbf{H}T_{W}\\mathbf{x})T_{W}X-\\sigma^{\\prime}(\\mathbf{H}T_{W}X)T_{W^{(t)}}X^{(t)}+\\sigma^{\\prime}(\\mathbf{H}T_{W}X)T_{W^{(t)}}X^{(t)}-\\sigma^{\\prime}(\\mathbf{H}T_{W^{(t)}}X^{(t)})T_{W^{(t)}}X^{(t)}\\right|\\right|}\\\\ &{\\leq\\left|\\left|\\sigma^{\\prime}(\\mathbf{H}T_{W}X)T_{W}X-\\sigma^{\\prime}(\\mathbf{H}T_{W}X)T_{W^{(t)}}X^{(t)}\\right|\\right|+\\left|\\left|\\sigma^{\\prime}(\\mathbf{H}T_{W}X)T_{W^{(t)}}X^{(t)}-\\sigma^{\\prime}(\\mathbf{H}T_{W^{(t)}}X^{(t)})T_{W}X^{(t)}\\right|\\right|}\\\\ &{\\leq\\left|\\left|\\sigma^{\\prime}(\\mathbf{H}T_{W}X)\\right|\\right|\\cdot\\left|\\left|T_{W}X-T_{W^{(t)}}X^{(t)}\\right|\\right|+\\left|\\left|\\sigma^{\\prime}(\\mathbf{H}T_{W}X)-\\sigma^{\\prime}(\\mathbf{H}T_{W^{(t)}}X^{(t)})\\right|\\right|\\cdot\\left|\\left|T_{W^{(t)}}X^{(t)}\\right|\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first inequality holds due the triangle inequality, and the second inequality holds due to the property of the operator norm. ", "page_idx": 25}, {"type": "text", "text": "Therefore, from here, in order to prove Eq. 43, it is sufficient to prove that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\left|T_{W}X-T_{W^{(t)}}X^{(t)}\\right|\\right|\\to0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\left|\\sigma^{\\prime}({\\bf H}T_{W}X)-\\sigma^{\\prime}({\\bf H}T_{W^{(t)}}X^{(t)})\\right|\\right|\\to0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since Eq .46 implies Eq. 47, so we would focus on proving Eq. 46. We further transform Eq. 46 as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big|\\Big|T_{W}X-T_{W^{(t)}}X^{(t)}\\Big|\\Big|}\\\\ &{=\\Big|\\Big|T_{W}X-T_{W}X^{(t)}+T_{W}X^{(t)}-T_{W^{(t)}}X^{(t)}\\Big|\\Big|}\\\\ &{\\leq\\Big|\\Big|T_{W}X-T_{W}X^{(t)}\\Big|\\Big|+\\Big|\\Big|T_{W}X^{(t)}-T_{W^{(t)}}X^{(t)}\\Big|\\Big|}\\\\ &{\\leq\\|T_{W}\\|\\cdot\\|X-X^{(t)}\\|+\\|T_{W}-T_{W^{(t)}}\\|\\cdot\\|X^{(t)}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similar to Eq. 45, we first apply the triangle inequality to obtain the first inequality, and apply the property of the operator norm to determine the second inequailty. ", "page_idx": 25}, {"type": "text", "text": "From here, it is sufficient if prove that ", "page_idx": 25}, {"type": "equation", "text": "$$\n||X-X^{(t)}||\\to0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\vert\\vert T_{W}-T_{W^{(t)}}\\vert\\vert\\rightarrow0\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We observe that ", "page_idx": 25}, {"type": "equation", "text": "$$\n||X-X^{(t)}||\\leq||W-W^{(t)}||\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "the inequality holds due to the definition of $X^{(t)}$ , thus by Lemma D.1, Eq. 49 holds. ", "page_idx": 25}, {"type": "text", "text": "As proven in [24], if both $\\vert\\vert X-X^{(t)}\\vert\\vert$ and $||W-W^{(t)}||$ converges to 0 as $t\\to\\infty$ , then Eq. 50 holds. ", "page_idx": 25}, {"type": "text", "text": "Therefore, the proof for Theorem 5.4 is now completed. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.1. $||W-W^{(t)}||$ is bounded by $1/n(t)^{2}$ and $1/n(\\bar{t})^{2}$ and thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n||W-W^{(t)}||\\to0,\\,a s\\,t\\to\\infty\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "since $1/n(t)^{2}\\to0$ as $t\\to\\infty$ ", "page_idx": 26}, {"type": "text", "text": "Proof. In order to prove the convergence of this lemma, we will need to prove the convergence in $L_{2}$ norm, i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n||W-W^{(t)}||_{L_{2}}\\to0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We first prove that ", "page_idx": 26}, {"type": "equation", "text": "$$\n||W-A^{(t)}||_{L_{2}}\\leq\\frac{4\\beta}{n(t)^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W-A^{(0)}\\|_{L^{2}}=\\displaystyle\\int_{0}^{1}\\int_{0}^{\\infty}(W(u,v)-A^{(0)}(u,v))^{2}d u\\,d v=}\\\\ &{=\\displaystyle\\sum_{i=1}^{m\\lfloor\\lfloor\\frac{1}{\\operatorname*{max}}\\rfloor}\\int_{(-1)^{\\frac{1}{m}}(1+\\gamma)\\operatorname*{max}}^{i}\\int_{(-1)^{\\frac{1}{m}}(1+\\gamma)}^{j/2+1}(w(w_{1},v)-A^{(i)}(u,v))^{2}d u\\,d v}\\\\ &{=\\displaystyle\\frac{w_{1}^{(0)}(w_{1},v)}{i-\\gamma\\ln\\left\\{\\phantom{(-1)}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\left(u\\right)\\!\\,\\int_{0}^{j/2+1}\\!\\!\\!\\left(w(w_{1},v)-W(u,u_{1},v)\\right)^{2}d u\\,d v\\,d v\\,}}\\\\ &{\\le\\displaystyle\\frac{\\beta}{i-\\gamma\\ln\\left\\{\\phantom{(-1)}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\left(u\\right)\\!\\,\\int_{0}^{j/2+1}\\!\\!\\!\\left(v(w_{1},v)w_{1}\\right)\\left(u-w_{1}\\right)+\\left\\{v-w_{1}\\right\\}\\right\\}^{2}d u\\,d v}\\\\ &{\\le\\displaystyle\\frac{\\beta}{i-\\gamma\\ln\\left\\{\\phantom{(-1)}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\!\\left(u-v_{1}\\right)\\left(u\\right)\\!\\,\\int_{0}^{j/2+1}\\!\\!\\!\\left(v(w_{1},v)-\\frac{1}{w}\\right)^{2}d u\\,d v}}\\\\ &{\\le\\displaystyle\\frac{\\beta\\sum_{i=1}^{m\\lfloor\\frac{1}{\\operatorname*{max}}\\rfloor}\\int_{(-1)^{\\frac{1}{m}}(1+\\gamma)\\ln\\left\\{\\phantom{(-1)}\\!\\!\\!\\left(v(w_{1},v)-\\frac{1}{w}\\right)^{2}d u\\,d v\\right\\}}{\\gamma\\ln\\left\\{\\phantom{(-1)}\\!\\!\\!\\int_{0}^{1}\\!\\!\\!\\left(\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The first inequality holds due the fact that $W,W^{\\prime}$ are $\\beta$ \u2212Lipschitz. ", "page_idx": 26}, {"type": "text", "text": "Next, we prove the lemma using mathematical induction. ", "page_idx": 26}, {"type": "text", "text": "For $t=0$ , then $W^{(0)}=A^{(0)}$ , thus the lemma holds for $W^{(0)}$ , since $\\lVert W-W^{(0)}\\rVert=\\lVert W-A^{(0)}\\rVert$ is bounded by $\\frac{4\\beta}{n(0)^{2}}$ . Therefore, the Lemma holds for $t=0$ . ", "page_idx": 26}, {"type": "text", "text": "Given that we fix some timestamp $\\bar{t}$ , and suppose that the Lemma holds for every timestamp in the interval $[0,\\bar{t}]$ . We would focus on proving that the Lemma also holds for $t>\\bar{t}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||W-W^{(t)}||_{L_{2}}=}\\\\ {\\displaystyle=\\frac12||W-W^{(\\bar{t})}+W-A^{(t)}||}\\\\ {\\displaystyle\\leq\\frac12||W-W^{(\\bar{t})}||+\\frac12||W-A^{(t)}||}\\\\ {\\displaystyle\\leq\\frac12||W-W^{(t)}||+\\frac{2\\beta}{n(t)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The first term is bounded by $1/n(\\bar{t})^{2}$ by our induction hypothesis, and the second term is also bounded by $1/n(t)^{2}$ , and as $t\\rightarrow\\infty,\\bar{t}\\rightarrow\\infty$ then $1/n(t)^{2},1/n(\\bar{t})^{2}\\to0$ and thus $||W-W^{(t)}||_{L_{2}}\\to0$ as $t\\to\\infty$ , which completes the proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D.3 Detailed Comparison with Previous Work ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In short, we break through the convergence limitations of previous work [24]. For instance, besides temporal dependencies between snapshots of the evolving graphs, the work [24] that is most closely related to our theoretical results does not account for any dependencies between static graphs and does not establish a limit object for two different graphs. The detailed illustration is delivered as follows. ", "page_idx": 27}, {"type": "text", "text": "\u2022 We first summarize the setting about the convergence of static graphs to graphons and the theoretical findings of [24]: the previous work that is most closely to our theoretical findings in Theorem 5.4 is [24]. [24] proves that if a graph sequence of static random graphs $\\{G_{n}\\}$ with growing number of nodes, i.e., the number of nodes in $G_{i}$ is less than or equal $G_{j}$ with $i<j$ , converges to a graphon $\\mathbf{W}$ , and the graph signal sequences $\\{x_{n}\\}$ , $\\{x_{n}^{\\prime}\\}$ converge to the graphon signal $X,X^{\\prime}$ , respectively, then the induced graphon neural tangent kernel between graph $G_{i}$ with signal $x_{i}$ and graph $G_{i}$ with signal $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ converges to the graphon neural tangent kernel between $W$ with signal $X$ and $W$ with signal $X^{\\prime}$ , as the number of nodes in the graphs of the sequence $\\left\\{G_{n}\\right\\}$ goes to infinity.   \n\u2022 Therefore, we notice that the theoretical findings in [24] do not account for any dependencies between static graphs in the graph sequence, and [24] establishes the results of convergence for the neural tangent kernel between the same graph, but with different signals.   \n\u2022 Next, we point out how evolving graphs can be represented as a sequence of graphs with a growing number of nodes. Suppose we have an evolving graphs $G$ that has $n$ snapshots, $G^{(t_{1})},\\ldots,G^{(t_{n})}$ and the number of nodes in $G^{(t_{i})}$ is less than or equal the number of nodes in $G^{(t_{j})}$ if $i<j$ . Then the snapshots of $G$ can be regarded as a graph sequence with the growing number of nodes. However, unlike the graph sequence in [24], there are temporal dependencies between graphs in the graph sequence of $G$ .   \n\u2022 Similar to the result of [24] for static graphs, we establish a limit object on the graph sequence representation of the evolving graph, and overcome the limitations of [24], as we take the temporal dependencies between graphs in the graph sequence of the evolving graph into account, and derive at the limit object for the graphon neural tangent kernel of 2 different evolving graphs. ", "page_idx": 27}, {"type": "text", "text": "E Detailed Comparison with Previous Temporal Graph Representation Learning Works ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide detailed comparison between our Temp- $\\mathbf{\\cdotG^{3}N T K}$ and previous traditional temporal graph learning methods, DGNN [31], EvolveGCN [35], ROLAND [48], and SSGNN [7], that rely on neural network training (e.g, stacking neural layers, gradient descent, and backpropagation) to obtain neural representations to support corresponding graph downstream tasks. However, our Temp- $\\mathrm{\\DeltaG^{3}N T K}$ does not rely on neural network structure but can achieve the expressive power of graph neural networks, as our theoretical analysis and experiments demonstrate. ", "page_idx": 27}, {"type": "text", "text": "To be specific, DGNN [31], EvolveGCN [35], ROLAND [48], and SSGNN [7] belong to the category of recurrent graph neural architectures that handle temporal information (e.g., on the learnable weight level like EvolveGCN [31] or hidden representation level like ROLAND [48]). This is indeed an effective direction, but it requires heavy time complexity. ", "page_idx": 27}, {"type": "text", "text": "Facing this problem, an emerging direction appears, i.e., MLP-Mixer on Static Graphs [16] or GraphMixer on temporal graphs [8]. Especially, GraphMixer aggregates information from recent temporal neighbors and processes them with MLP-Mixer layers. Motivated by this direction, we propose our temporal graph neural tangent kernel. Also, we would like to note that, even without recurrent neural architectures, temporal information can also be preserved in our method. ", "page_idx": 27}, {"type": "text", "text": "To be more specific, in our method, temporal dependencies are captured in Eq. 1, where we construct the temporal node representations for a node at time by aggregating information (e.g., node features, edge features, and time difference) from its previous temporal neighbors. And the entire process does not involve neural training but just depends on mathematical time kernel functions. In other words, this process records the current state of based on its own neighborhood at the previous time and can be retrieved for future state computation. Besides theoretical derivation, especially, Table 2 and Figure 1 visualize our method\u2019s effectiveness. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "To support our above statement, we list the detailed illustration below. ", "page_idx": 28}, {"type": "text", "text": "\u2022 DGNN [31] is designed to execute the link prediction task (while ours is mainly for temporal graph classification and can be easily adapted to temporal node classification). In order to obtain the link predictions, nodes are categorized as interacting or influenced as follows. If two nodes are involved in a (directed) interaction $(u,v,t)$ , then $u,v$ are interacting nodes and nodes that are nearby this interaction are referred to as \u201cinfluenced nodes\u201d. On the other hand, If two nodes $u$ and $v$ interact at a certain time, then DGNN [31] updates their temporal representation. First, process each of $u$ and $v$ separately by employing recurrent architecture to their previous temporal representations and finally combine with time encoding the difference between the current time and the last interaction time of that node. Then, merge the two representations and obtain two new representations for $u$ and $v$ . If two nodes interact, nearby nodes (\u201cinfluenced nodes\u201d) would be affected. DGNN [31] also updates \u201cinfluenced nodes\u201d, i.e., applying recurrent architecture on their previous representation, combining with two representations from interacting nodes, and the time encoding of difference between current time and last interacting time.   \n\u2022 EvolveGCN [35] is a link prediction and node classification method. Specifically, EvolveGCN [35] operates on graph snapshots and uses recurrent architecture (e.g., LSTM) to update the weight of each neural layer across time. Then, at a certain time $t$ , EvolveGCN [35] gets node representation by applying the current snapshot\u2019s adjacency matrix, learnable weights, and representation from the previous recurrent layer.   \n\u2022 ROLAND [48] is also a method designed for link prediction. Different from EvolveGCN [31], the the recurrent architectures in ROLAND [48] are added on the hidden representation vectors other than learnable weights across timestamps. Then the components in recurrent architectures get simplified in ROLAND [48], which can be mainly based on MLPs and simple GNNs.   \n\u2022 SSGNN [7] is performs the time series forecasting task. To be more specific, SSGNN [7] first uses a deep randomized recurrent neural network to encode the history of each node encodings into high-dimensional vector embeddings, and then uses powers of the graph adjacency matrix to build informative node representations of the spatiotemporal dynamics at different scales. Then, the decoder maps the node representations into the desired output, e.g., future values of the time series. ", "page_idx": 28}, {"type": "text", "text": "Next, we state the position of our method in temporal graph learning. A recent temporal graph learning survey [30] reviewed temporal graph learning methods, including EvolveGCN [35], SSGNN [7], and DGNN [31], in their taxonomy, as plotted in its Figure 2 [30]. ", "page_idx": 28}, {"type": "text", "text": "In that taxonomy, according to the best of our knowledge, our method belongs to the category \u201cEvent-based\u201d, and is the child node of \u201cTemporal Embedding\u201d and \u201cTemporal Neighborhood\u201d, the position is close to the work TGL [51]. ", "page_idx": 28}, {"type": "text", "text": "However, different from TGL [51], a large-scale graph neural network, to our best knowledge, our method is the first temporal graph neural tangent kernel method. ", "page_idx": 28}, {"type": "text", "text": "F Extra Temporal Graph-Level Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "F.1 Ablation Study of Temporal Graph Classification ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We conduct an ablation study to investigate how different time encoding functions affect the performance of Temp- $\\mathbf{\\cdotG^{3}N T K}$ . We select the infectious dataset to perform our ablation study. ", "page_idx": 28}, {"type": "text", "text": "Here, we examine how the usage of the time encoding function and its variations affect the performance of our predictor. Recall that we leverage $\\mathbf{t}_{e n c}$ to encode the raw relative difference between timestamps, $\\bar{(t-\\bar{t})}$ in Eq. 1. For our ablation study, instead of the relative time difference encoding, i.e., $\\mathbf{t}_{e n c}(t-\\bar{t})$ , we also consider the raw relative difference, $(t-\\bar{t})$ , the absolute time encoding, $\\mathbf{t}_{e n c}(\\bar{t})$ , and the absolute timestamp, $\\bar{t}$ . The results are presented in Table 5, and the best accuracy is highlighted in bold. As we can see, the utilization of the time encoding function (first and second ", "page_idx": 28}, {"type": "text", "text": "Table 5: Ablation Study of Different Time Encoding Functions on Classification Accuracy on the INFECTIOUS Dataset. ", "page_idx": 29}, {"type": "table", "img_path": "266nH7kLSV/tmp/71071d9ab401ca93ed3546410a533db6de8e886ec9b1b61f30f2197fc48f6822.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "266nH7kLSV/tmp/06a24912358a00c78c84a72ac1a4deac0d390ae84d24573df90294064549bac8.jpg", "table_caption": ["Table 6: Parameter Analysis of Different Number of Recent Neighbors on Classification Accuracy on the INFECTIOUS Dataset. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "rows) yields higher accuracy than using raw timestamps, with $74\\%$ and $62\\%$ improvement, respectively. This suggests the ability and efficiency of $\\mathbf{t}_{e n c}$ in distinguishing different timestamps, which enhances the classification result. ", "page_idx": 29}, {"type": "text", "text": "F.2 Parameter Analysis of Temporal Graph Classification ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We conduct parameter analysis to investigate how different numbers of neighbors affect the performance of Temp- $\\mathrm{G^{3}N T K}$ . We also select the infectious dataset to perform our parameter analysis. ", "page_idx": 29}, {"type": "text", "text": "Thus, we delve into how the Temp- $\\mathrm{G^{3}N T K}$ performs with respect to the number of temporal neighbors. Specifically, in practice, most temporal graph representation learning methods aggregate information from the most $K$ recent neighbors [39, 8], instead of the full temporal neighborhood, $\\mathcal{N}^{(t)}(v)$ . For our parameter analysis, we vary the number of recent neighbors from $\\{5,10,15,20,25\\}$ , perform the neighborhood aggregation (Eq. 1) on these recent neighbors, and report the classification accuracy of Temp- $\\mathbf{\\cdotG^{3}N T K}$ for the infectious dataset. The results are shown in Table 6, and the best results are highlighted in bold. Integrating all temporal neighbors into node representation yields higher accuracy than accounting for some recent neighbors, as shown in Table 6. These findings further suggest Temp- $\\mathbf{\\cdotG^{3}N T K}$ is able to leverage and capture the information in the full temporal neighborhood. ", "page_idx": 29}, {"type": "text", "text": "F.3 Temporal Graph Similarity ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Here, we use four unlabeled large real-world temporal graphs, WIKI, REDDIT, MOOC, and LASTFM to demonstrate the scalability of our Temp- $\\mathrm{{G^{3}N\\dot{T}K}}$ , as shown in Figure 2 and Figure 3, where $x$ -axis is the timestamp, and the $y$ -axis is the similarity between two temporal graphs at a certain timestamp. ", "page_idx": 29}, {"type": "text", "text": "For each pair of temporal graphs, we compute the neural tangent kernel values with respect to time and plot the values as below. For each plot, the $y$ -axis represents the Temp- $\\mathbf{\\cdotG^{3}N T K}$ value and the $x$ -axis represents the timestamp. Although the timestamp ranges for each temporal graph are different, we rescale the $x$ -axis to [0; 1000] for a better illustration. For each pair of graphs, the corresponding plot shows a different curve, suggesting that our Temp- $\\mathrm{G^{3}N T K}$ can distinguish different pairs of temporal graphs. More interestingly, the corresponding observations align with our theoretical assumptions that the similarity of different growing temporal graphs tends to converge. ", "page_idx": 29}, {"type": "image", "img_path": "266nH7kLSV/tmp/a01364209017483164216e9ac785bc0beb6194cd93a353933adc1de12b89d0f6.jpg", "img_caption": ["Figure 2: Similarity of Different Temporal Graphs With Time Increased (Part I). $y$ -axis represents the Temp- $\\mathrm{{\\calG}^{3}N T K}$ value, and the $x$ -axis represents the timestamp "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "266nH7kLSV/tmp/a88639720d542f5d079acae19705fd4f0368b847697931d84f9f67c9b20861f5.jpg", "img_caption": ["Figure 3: Similarity of Different Temporal Graphs With Time Increased (Part II). $y$ -axis represents the Temp- $\\mathrm{{\\calG}^{3}N T K}$ value, and the $x$ -axis represents the timestamp "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "G Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "G.1 Datasets Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The detailed statistics of small and large temporal graph datasets for graph-level experiments are shown in Table 7 and Table 8. ", "page_idx": 30}, {"type": "table", "img_path": "266nH7kLSV/tmp/8211cbf905314e88860b992aa3313e1dbe18145396cf52972dd455cb181c42e2.jpg", "table_caption": ["Table 7: Small Temporal Graph Dataset Statistics "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "G.2 Temporal Graph Classification ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Next, we provide details on how we conduct our experiments and the implementations of baseline algorithms in Section 6.1. In general, upon obtaining the time r\u221aepresentation as in Eq. 1, we let the dimension of the time representation be $d_{t}=25$ and $\\alpha=\\beta=\\sqrt{d_{t}}$ . ", "page_idx": 30}, {"type": "text", "text": "In order to leverage Temp- $\\mathrm{\\DeltaG^{3}N T K}$ for graph classification, we employ C-SVM as a kernel regression predictor with the gram matrix of pairwise Temp- $\\mathbf{\\cdotG^{3}N T K}$ values of the training set as the precomputed kernel. The regularization parameter $C$ of the SVM classifier is sampled evenly from 120 values in the interval $[10^{-2},10^{4}]$ , in log scale, and set the number of maximum iterations to $5\\cdot10^{5}$ . For the number of BLOCK operations in our Temp- $\\mathrm{G^{3}N T K}$ formula, $L$ , we search for $L$ over $\\{1,2,3\\}$ , and we notice that the validation accuracy remains unchanged while the $L$ varies. ", "page_idx": 30}, {"type": "text", "text": "For Graph Kernels and Graph Representation Learning methods, we first obtain the representation of each graph in the training set and then compute the pair-wise gram matrix, where each entry is the dot product of the representation of a graph pair. We then perform graph classification by leveraging C-SVM as our predictor and set the pre-compputed kernel as the aformentioned gram matrix. For the classifier regularization parameter $C$ , we also determine this value by even sampling over the interval $[10^{-2},10^{\\overline{{4}}}]$ , in log scale, and let the number of iterations be $5\\cdot10^{5}$ . We adopt the implementations of Graph Kernels from GRAKEL library [43] and the implementations Graph Representation Learning methods from the Karate Club library [40]. We adopt the default hyperparameters from implementations of both libraries. ", "page_idx": 30}, {"type": "table", "img_path": "266nH7kLSV/tmp/0765e76691265cad01daa8610b753d37c60503eaa2f8d76af132f28d472d99f0.jpg", "table_caption": ["Table 8: Large Temporal Graph Dataset Statistics "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "For TGL methods, which are TGN and GraphMixer, we first obtain node representations in each graph of the training set using the official code released by authors of $\\mathrm{TGN}^{8}$ and GraphMixer9, then we determine the graph representation by performing sum pooling over the node representations. In the process of obtaining node representations, we adopt default hyper-parameters in the code of TGN and GraphMixer. Finally, we implement a simple linear classifier that consists of 1 layer of linear transformation and ReLU activation function, and the final output is determined by the Sigmoid function, as all datasets are binary classification. We train and optimize these models by the Adam optimizer, with the learning rate of 0.001, $(\\beta_{1},\\beta_{2})=(0.9,0.999)$ , and the Binary Cross Entropy loss function. We adopt all default hyper-parameters. ", "page_idx": 31}, {"type": "image", "img_path": "266nH7kLSV/tmp/184eaa931d5a9645f1041f946c5aa049ee009fba49070d5f61fbd11738de0abd.jpg", "img_caption": ["Figure 4: Plots of testing classification accuracy score of baseline algorithms with respect to different stages of temporal graphs from the dblp and tumblr datasets. The $y$ -axis in each plot states the accuracy score, and the values in the $x$ -axis represent how many percentages of timestamps have been taken into account. For example, at $x=1/5$ , the score is obtained by performing classification on the first $1/5$ timestamps of each graph. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "In addition, we also provide the plot that illustrates the performance of baseline algorithms at different timestamps of the DBLP and TUMBLR datasets in Figure 4. ", "page_idx": 31}, {"type": "text", "text": "G.3 Temporal Node Property Prediction ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In order to utilize Temp- $\\mathbf{\\cdotG^{3}N T K}$ for node property prediction, we first compute the node pair-wise gram matrix at the last MLP layer in the Temp- $\\mathrm{G^{3}N T K}$ formula, i.e the kernel matrix $\\Theta^{(L)}$ in (9). Similar to performing graph classification task, we perform kernel regression with C-SVM and employ $\\Theta^{(\\bar{L})}$ as the pre-computed kernel, and $C$ is also searched over $[10^{-2},10^{4}]$ in log scale. We also vary the number of BLOCK Operations, $L$ , by $\\{1,2,3\\}$ to find the best NCDG score, which is obtained by $L=1$ . ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 32}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 32}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 32}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 32}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in Appendix If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 32}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 32}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: In the paper, the authors made clear statements for specifying the paper\u2019s contribution and scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: To the best of the authors\u2019 knowledge, there is no important limitation that needs to be highlighted here. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The authors tried their best to provide sufficient assumption and proof for the proposed theory in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in Appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper shares the important information clearly for the experiment details.   \nThe code is promised to be released after the paper\u2019s publication. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The authors provide enough instructions for the main experimental results in the main content and the Appendix The code is promised to be released after the paper\u2019s publication. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper contains those details. The code is promised to be released after the paper\u2019s publication. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in Appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The above factors are included in the paper, and the code is promised to be released after the paper\u2019s publication. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper includes this kind of information, and the code is promised to be released after the paper\u2019s publication. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: To the best of the authors\u2019 knowledge, the paper obeys the NeurIPS Code of Ethics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: To the best of the authors\u2019 knowledge, they did not see the important societal impacts of the paper. However, the authors discuss the outcome of this work in the graph learning community. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: To the best of the authors\u2019 knowledge, the authors did not see this kind of risk. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The authors made clear citations. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: No new assets are in the paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 37}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This work did not have crowdsourcing and research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: To the best of the authors\u2019 knowledge, this concern do not apply to this work. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]