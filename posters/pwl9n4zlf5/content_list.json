[{"type": "text", "text": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minghao Chen1, Yihang $\\mathbf{L}\\mathbf{i}^{2}$ , Yanting Yang3, Shiyu $\\mathbf{Y}\\mathbf{u}^{5}$ , Binbin ${\\bf L i n}^{3,4*}$ , Xiaofei $\\mathbf{H}\\mathbf{e}^{2}$ ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science, Hangzhou Dianzi University 2State Key Lab of CAD&CG, Zhejiang University 3School of Software Technology, Zhejiang University 4Fullong Inc. 5NingBo Port Group minghaochen01@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a case-conditioned prompting strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving $97.4\\%$ with GPT-4-turbo and $86.2\\%$ with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, autonomous agents based on Large Language Models (LLM), e.g., ReAct [33], Reflexion [16], SayCan [1], WebGPT [10], and Voyager [24], have demonstrated their potential to complete long-horizon tasks in grounded environments. These LLM agents operate by generating thoughts and actions that are executable in the environment. For customized environments, such as robotics [1, 7, 18, 23] and games [14, 24, 39], prior methods provide detailed instructions and in-context examples to familiarize LLM with action functions (API) and the target environment. However, unlike these agents, humans can autonomously build and update their understanding of an unfamiliar environment through dynamic interaction. ", "page_idx": 0}, {"type": "text", "text": "Several existing methods enable LLM agents to reflect on feedback [16, 21] or save successful experiences as skills [21, 24, 35] to enhance the performance and reduce the reliance on humanprovided examples. However, these reflections and skills have not been well exploited to foster a deeper understanding of the environment. As a result, directly using saved skills as in-context examples can lead to the Path Dependence problem, i.e., the agent blindly replicates the paths of previous successes, failing to appropriately adapt to new scenarios. Such problems are more severe in real-world situations characterized by high variability. ", "page_idx": 0}, {"type": "text", "text": "A previous work, ExpeL [35], gathers the trajectories of LLM agents and extracts cross-task rules from them. However, these rules are extracted offline, making ExpeL suffer from the same distributional shift problem as Offline RL [6]. Meanwhile, due to the simplicity of rule management, its rules are always armchair general and unhelpful for the Path Dependency problem. In this paper, we propose a novel framework called AutoManual to build a well-organized understanding of the environment that can guide multi-task planning effectively. AutoManual leverages a dynamic rule system that not only extracts valuable experience, including skills and reflections, into different types of rules but also allows for continuously updating these rules in response to new situations. Additionally, error-prone details are explicitly described in the rules to improve the robustness of planning. ", "page_idx": 1}, {"type": "text", "text": "AutoManual follows two alternating iterative processes to optimize the rules. First, given the observation and task of an episode, the Planner agent utilizes currently discovered rules to write free-form code as an actionable plan. The interaction between the environment and the Planner will loop until the episode ends. Second, based on this trajectory, the Builder agent will update relevant rules through the rule system. This online updating mechanism can timely verify whether the rules have deviations and are applicable to the Planner. After rules optimization, the Formulator agent categorizes these rules according to their application scenarios and compiles a comprehensive manual in Markdown format. ", "page_idx": 1}, {"type": "text", "text": "The challenge lies in enabling the Builder to accurately extract applicable rules from a long trajectory, as LLM are prone to generating hallucinations. To address this, we employ a case-conditioned prompting strategy, which directs the Builder to focus on specific rules according to the case of the trajectory. For example, if errors occurred in a trajectory, the Builder is first asked to determine which caused the error: an unrecorded situation occurred, or the Planner failed to follow existing rules. Based on this answer, the Builder will be given corresponding prompts to update relevant rules. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our contributions are the following: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We adopt actionable code as the way for the Planner agent to interact with the environment. We introduce a structured rule system that allows the Builder agent to manage multiple types of knowledge from these code-based interactions. \u2022 We propose an alternating process between the Planner and Builder agents to optimize rules in an online manner and resolve the Path Dependency problem. To improve readability, the Formulator agent is introduced to reorganize and formalize the rules into a Markdown manual. \u2022 To facilitate rule management, we employ a case-conditioned prompting strategy, which guides the Builder to manage specific types of rules for different trajectory cases. \u2022 Starting from a single demonstration, AutoManual can generate detailed instruction manuals for complex environments like ALFWorld and MiniWoB $^{++}$ . These manuals allow LLM agents to achieve remarkable success rates of $97.4\\%$ with GPT-4-turbo and $86.2\\%$ with GPT-3.5-turbo on ALFWorld, $98.3\\%$ with GPT-4-turbo and $92.7\\%$ with GPT-3.5-turbo on MiniWoB $^{++}$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 LLM for Agents Planning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Large Language Models (LLM) exhibit powerful reasoning and planning capabilities [11, 12, 28, 33, 39] while requiring much fewer demonstrations than traditional learning methods. With this planning capability as the core, LLM agents are being developed for use in robotics [1, 7, 18, 20, 23], game-playing [14, 24, 27, 39], software development [3, 15], and other fields [30]. Prior studies [16, 21, 33] allow agents to adjust actions or plans based on environmental feedback to improve planning performance. Given the powerful programming capability of LLM, several works, e.g., CodeAsPolicy [7], ProgPrompt [18] and AdaPlanner [21], propose to use Python code as the plan of LLM agents. This form of output can automatically respond to in-plan feedback and achieve better performance than the action and JSON format [21, 25]. ", "page_idx": 1}, {"type": "text", "text": "2.2 Self-improvement of LLM Agents ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Embodied agent research has long sought to enable agents to self-improve through interactive experiences. Unlike traditional learning-based agents that require extensive iterations for optimization, ", "page_idx": 1}, {"type": "text", "text": "Reflexion [16] allows LLM agents to reflect on previous failures and quickly improve their plans. Some works [32, 34, 36] combine tree search with reflection to deliberately seek a better solution. Apart from failure experiences, prior studies [21, 24, 39] utilize successful experiences as skills to assist future planning. Voyager [24] stores generated and verified programs into the skill library as a new skill for more complex tasks. AdaPlanner [21] also discovers and archives successful programs into skill memory for future similar tasks. However, these methods stop updating skills after storing them, which inevitably leads to the Path Dependency problem. ", "page_idx": 2}, {"type": "text", "text": "Another series of works [26, 31, 38] employs LLM as a prompt optimizer to enhance its own performance. In contrast to our approach, which addresses challenges in unfamiliar environments, these studies focus on enhancing LLM reasoning performance. As a result, their optimized prompts are typically brief and lack environmental knowledge. ", "page_idx": 2}, {"type": "text", "text": "2.3 Memory Management of LLM Agents ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For LLM agents, learning from past experiences can also be viewed as managing the episodic memory [16]. CLIN [9] proposes to keep updating a memory centered on causal abstractions for new trials. Retrieval-Augmented Planning (RAP) [4] retrieves past experiences corresponding to the current situation. MemGPT [13] allows LLM to select content to retain in working memory and to search for information in long-term memory. Generative Agents [14] retrieve memories based on recency, importance, and relevance to the current situation. Generative Agents also generate tree-structured reflections, but they focus on a continuous scenario rather than task-oriented rules. ", "page_idx": 2}, {"type": "text", "text": "2.4 LLM for Rule Discovery ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Several recent works also investigate the rule discovery capabilities of LLM. Zhu et al. [40] propose Hypotheses-to-Theories (HtT), enabling LLM to induce and deduce rules for basic reasoning tasks. For LLM agents, ExpeL [35] gathers the trajectories of Reflexion agents and extracts cross-task rules from them. Furthermore, AutoGuide [2] generates state-aware rules and retrieves rules relevant to the test-time state. Unlike ExpeL and AutoGuide, which extract rules from offline experiences, we update rules in an online manner, verifying their reliability and applicability. For more discussion of differences, refer to Appendix C. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 AutoManual Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our AutoManual framework, shown in Fig 1, consists of three main stages. Building stage: The Planner agent and Builder agent collaborate to build rules from the interactive environment. The Consolidator agent merges or deletes redundant rules when the rules exceed the maximum rule number. Formulating stage: The Formulator agent categorizes the rules, summarizes the key points, and formulates them into a manual in Markdown form. Testing stage: Based on the generated manual, a test-time Planner agent will be evaluated through test tasks and scenarios. ", "page_idx": 2}, {"type": "text", "text": "Formally, an Interactive Environment can be modeled as a Partially Observable Markov Decision Process (POMDP): $(\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathcal{G},\\mathcal{O})$ . At the start of each episode, a scenario $s_{0}\\in\\mathcal{S}$ will be initialized, a text-grounded task $g\\in{\\mathcal{G}}$ and the initial observation $o_{0}\\in\\mathcal{O}$ (processed into textual form) will be given. The environment can be interacted with through permissible actions (API) set $\\boldsymbol{\\mathcal{A}}$ . After executing an action $a\\in{\\mathcal{A}}$ , the environment will return the result of the action and the new observation $o^{\\prime}$ based on the dynamics $T(s^{\\prime}|s,a)\\in\\mathcal{T}$ and $O(o^{\\prime}|s^{\\prime})$ . Finally, when the episode is done, a binary reward $r\\in\\{-1,\\dot{1}\\}$ indicating the failure or success of the task will be returned. ", "page_idx": 2}, {"type": "text", "text": "We approach the learning of environmental rules as an optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\Theta}E_{s_{0},g}E_{\\rho(\\cdot|\\Theta)}r(\\tau_{\\rho})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Theta$ denotes all rules in our rule system, $\\rho(\\cdot|\\Theta)$ denotes the policy of the Planner given the current rules $\\Theta$ and $\\tau_{\\rho}$ denotes a trajectory of $\\rho(\\cdot|\\Theta)$ starting from $[o_{0},g]$ . Classic policy gradient methods [29] solve such problems through stochastic gradient ascent, i.e., executing the current policy to obtain the episodic reward and back-propagating gradients to update the parameters. ", "page_idx": 2}, {"type": "image", "img_path": "Pwl9n4zlf5/tmp/fe18acd94ecf6619ff5ff8f3147a3a5393416af156f0cc63c089ec7c11b7c798.jpg", "img_caption": ["Figure 1: AutoManual Overview: AutoManual operates in three stages: (1) Building Stage: The Planner agent interacts with the environment by coding actionable plans. After receiving the current trajectory of the Planner, the Builder agent manages rules through the online rule system. (2) Formulating Stage: The Formulator agent formulates the resulting rules into a Markdown manual. (3) Testing Stage: A test-time Planner agent utilizes the manual to complete testing tasks. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Inspired by this online reinforcement learning paradigm, we follow two alternative processes to optimize the rules $\\Theta\\colon1$ . The Planner practices the current rules through interaction within an episode. 2. The Builder updates the rules $\\Theta$ based on this trajectory. Compared to traditional parameter optimization, sample-inefficient gradient ascent is replaced by text-based rule management. We design a well-structured rule system described in Section 3.3 to ensure the rule updating contributes to rewards. Additionally, to limit the role of human expertise, we only provide a simple example demonstrating the output format to agents. Then, manually derive several initial rules from this example as the starting point of the optimization. ", "page_idx": 3}, {"type": "text", "text": "3.2 Planner Agent for Interactive Planning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As demonstrated by the success of Voyager [24] and AdaPlanner [21], code-based planning can leverage the powerful programming capability of LLM and automatically react to in-plan feedback. Voyager and AdaPlanner output and refine a complete solution function for the task, which is potentially reusable. However, this function-form output is difficult to adjust in response to environmental feedback, as it requires maintaining the integrity of the plan throughout. ", "page_idx": 3}, {"type": "text", "text": "Our Planner Agent outputs free-form code as its plan, which aligns more with the natural programming capabilities of LLM [7, 22]. This form simplifies planning by only generating code necessary for the current environmental situation and feedback without the overhead of integrating previously executed code. As shown in Fig 2, at the start of a new episode, the Planner receives system prompts, current rules $\\Theta$ , relevant samples from the skill and reflection libraries, the target task $g$ , and initial observation $O_{0}$ . System prompts contain the role, permissible actions $\\boldsymbol{\\mathcal{A}}$ , response guidelines, and a simple example (detailed in Appendix H). The output of the Planner is structured into four segments during each cycle: ", "page_idx": 3}, {"type": "text", "text": "1. Analysis: The understanding of the current situation and reflection on previous errors if exist.   \n2. Related Rules: Rules (along with their IDs) that need to be considered in this situation.   \n3. Overall Plan: The general plan to complete the task.   \n4. Code: A block of Python code divided into steps. The Planner is encouraged to define helpful   \nfunctions in the code, which might be reusable in similar scenarios. ", "page_idx": 3}, {"type": "text", "text": "We denote this response of the Planner as $[t h o u g h t_{t},c o d e_{t}]$ , where thoughtt denotes the first three segments. codet executed in the environment is followed by feedback $c_{t}$ , which informs the subsequent output cycle. This process iterates until the episode ends or a response limit is reached. ", "page_idx": 3}, {"type": "image", "img_path": "Pwl9n4zlf5/tmp/794c596960134f3bb735b15360eaff0915f31ef854c2e6a1c19d83e51314a7d9.jpg", "img_caption": ["Figure 2: The Planner Trajectory: Given the current task and rules, the Planner will interact with the environment through free-form code. Based on the trajectory result, the Planner will generate a corresponding conclusion, which will be saved in the skill or reflection library. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "As shown in Fig 2, according to the episodic reward, we categorize the result into Direct Success, Indirect Success (errors occurred but were solved later), and Failure. In the case of Direct or Indirect Success, the Planner will be prompted to organize its previous code into a code block. For Indirect Success, it additionally summarizes the mistakes and misunderstandings that cause errors. For the Failure case, the Planner will be prompted to reflect on the reason for the failure carefully, suggest reasonable corrections, and specify the code segment that caused the error. We denote this response of the Planner as conclusion. Finally, we obtain a trajectory of the Planner: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau_{\\rho}=(o_{0},g,[t h o u g h t_{1},c o d e_{1}],c_{1},...,[t h o u g h t_{T},c o d e_{T}],c_{T},c o n c l u s i o n)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Skill Library and Reflection Library: Apart from rules, we also manage and transmit conclusions from previous episodes, which provide essential details for generating planning code. In the case of Direct or Indirect Success, we save the code block in conclusion as a skill for that task type 1 into the skill library [21, 24]. In the Failure case, we save its conclusion as a reflection for that task type into the reflection library. When a new task comes, the code block of the most similar task is retrieved from the skill library. If there is no existing skill for the new task type, the reflection for that task type will be returned. As mentioned in the Introduction, compared with rules, these skills and reflections contain more programming details but are less generalizable to new scenarios, i.e., the Path Dependence problem. ", "page_idx": 4}, {"type": "text", "text": "Cooperation between Agents: In our framework, rule management is not solely the responsibility of the Builder; the Planner also plays a critical role by explicitly identifying the rules it engages in its response. This cooperation is facilitated by including the Planner\u2019s thoughts within the trajectory $\\tau$ , which is provided to the Builder. This synergy enhances the identification and adjustment of problematic rules. In addition, conclusion from the Planner contains the detailed success process or reflections on errors, which further assist the Builder in managing corresponding types of rules. ", "page_idx": 4}, {"type": "text", "text": "3.3 Builder and Consolidator Agents for Rule Management ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Upon receiving the trajectory $\\tau_{\\rho}$ , the Builder has to manage the rules through the rule system. ", "page_idx": 4}, {"type": "image", "img_path": "Pwl9n4zlf5/tmp/6c4b75fe73d5004c801f94c2413d0a634874689c7e5b54fb2e5f16021fe56787.jpg", "img_caption": ["Figure 3: Case-Conditioned Prompts: Given the current trajectory, the Builder classifies the cause of the major error as \"Imperfect Rules\" or \"Imperfect Agents\". Then, the Builder will get the base prompt and corresponding prompt to guide its rule management. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Rule System: We intuitively identify rules as the kinds of knowledge that help task completion, including the analyses of the observed phenomenon $T(o^{\\prime}|o,a)$ , the mechanism $T(s^{\\prime}|s,a)$ , and the correlation between the reward $r$ and $\\tau_{\\rho}$ , i.e., the success process or the occurred error. Therefore, unlike ExpeL [35] and AutoGuide [2], which derive general insight from the trajectory, our system categorizes six specific rule types to extract environmental knowledge that targets different aspects of the trajectory. Furthermore, each rule in our system is enhanced with Example attribute to illustrate its application and important details, making it grounded and well-understood. Specifically, each rule in the rule system has these four attributes: ", "page_idx": 5}, {"type": "text", "text": "1. Rule Type: The type of the rule, chosen from [\u201cSpecial Phenomenon\u201d, \u201cSpecial Mechanism\u201d, \u201cUseful Helper Method\u201d, \u201cSuccess Process\u201d, \u201cCorrected Error\u201d, \u201cUnsolved Error\u201d]; 2. Rule Content: A description of the rule, beginning with the scope of its applicable scenarios; 3. Example: An example or code from the trajectory demonstrates this rule, where additional remarks, e.g. error-prone details, can also be added to it; 4. Validation Logs: Logs that track the rule\u2019s application and updates, including episode and rule IDs that trace the rule\u2019s evolution, serving as a reference for the Builder and Consolidator. ", "page_idx": 5}, {"type": "text", "text": "The Builder manages the rules through the following functions of the rule system: ", "page_idx": 5}, {"type": "text", "text": "\u2022 write_rule(\\*\\*rule_attributes): Write down a new rule with its four attributes. \u2022 update_rule(rule_id, \\*\\*rule_attributes): Rewrite the attributes of a existing rule. \u2022 stop_generating(): When the trajectory is not needed or insufficient to derive any more new rules, the function should be called. ", "page_idx": 5}, {"type": "text", "text": "Similar to hierarchical reflections in Generative Agents [14], we allow the Builder to utilize existing rules to induce more general or deeper rules and record their dependence in Rule Content or Validation Logs, more discussed in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Case-Conditioned Prompting: To mitigate the risk of erroneous rule creation, such as deriving rules of success from a failed trajectory, we employ case-conditioned prompts. As illustrated in Fig 3, the Builder first analyzes and determines if the major errors stem from \u201cImperfect Rules\u201d or \u201cImperfect Agent\u201d. Based on this analysis and the trajectory results, targeted prompts guide the Builder in rule management 2. For example, in a case of indirect success due to imperfect rules (Case 2), the prompts will guide the Builder to extract or update the success process, helper methods, and error reflections in corresponding rule types. Finally, the Builder responds with the potential rules detailing their relation with existing rules and uses the functions of the rule system to manage rules. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Rule Consolidation: When the number of rules in the rule system exceeds $N_{m a x}$ , the Consolidator agent steps in to consolidate related rules and delete redundant rules. It uses three functions of the rule system: get_trajectory(episode_id), update_rule(rule_id, \\*\\*rule_attributes) and delete_rule(rule_id). Given the current rules, the Consolidator identifies potentially relevant or overlapped rules, uses get_trajectory function to investigate the trajectories they depend on, and finally calls the remaining functions to manage the rules. During the management, the Consolidator ensures that consolidation retains details of rules and examples. ", "page_idx": 6}, {"type": "text", "text": "3.4 Manual Formulation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Once the building stage is complete, we can obtain a set of rules targeted to different situations, whose applicability has been validated through online optimization. Our next goal is to enhance their readability and global understanding. To achieve this, we introduce the Formulator agent, designed to transform these rules into a user-friendly manual, analogous to a teacher imparting a wealth of knowledge through easily digestible lessons. As depicted in Fig 1, the Formulator begins by categorizing all rules based on their target scenarios. This categorization aids in structuring the manual and ensures that related rules are discussed together, which enhances the logical flow and accessibility of the information. For each category, the Formulator drafts an introduction, summarizing the rules it contains and highlighting the key points and overall principles that govern the specific scenarios. Finally, the Formulator compiles the rules and their introductions into a comprehensive manual formatted in Markdown. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In line with prior works [2, 21], we conduct the experiments on three interactive environments: (1) ALFWorld [17] is a text-based virtual household environment containing six distinct task types. We run the building stage on 36 tasks (6 tasks for each task type) sampled from the training set of ALFWorld, and each task is run only once. Following previous works [16, 21, 33], we run the testing stage on the validation unseen set containing 134 tasks across these six types. (2) $\\mathbf{MiniWoB++}$ [8] is a simulated web environment where agents complete diverse tasks on the Internet by performing keyboard and mouse actions. Prior works [5, 21] selects 9 task types with environmental feedback and 44 task types without feedback from $\\scriptstyle\\mathrm{MiniWoB++}$ tasks. We perform experiments on 9 task types with feedback or on all 53 task types. At each stage, we randomly sample 6 tasks for each task type. (3) WebArena [37] introduces realistic web environments by emulating the functionality and data of popular websites. This benchmark poses significant challenges for LLM agents due to its large observation and action space, along with tasks that require longer planning horizons. Following AutoGuide [2], our experiments focus on the Reddit domain within WebArena. ", "page_idx": 6}, {"type": "text", "text": "During the building and formulating stages, we use GPT-4-turbo (gpt-4-1106-preview) for all agents. At the testing stage, we equip the Planner agent with GPT-4-turbo or GPT-3.5-turbo (gpt-3.5-turbo-1106), to evaluate the effect of generated manuals on relatively smaller LLM. ", "page_idx": 6}, {"type": "text", "text": "Compared Methods: In the experiments, we compare AutoManual with the following methods of LLM Agent: (1) ReAct [33] prompts LLM to generate the reasoning trace using CoT [28] and next-step action; (2) Reflexion [16] agents generate reflection on task feedback signals, which is saved in the memory for subsequent trials; (3) ExpeL [35] extract insights and skills from the offline trajectories of Reflexion agents; (4) RCI [5] agent recursively criticizes and improves its output for solving computer tasks; (5) AdaPlanner [21] allows the LLM agent to generate and adaptively refine a code-style plan; (6) Planner+Lib. represents our Planner agent equipped with skill and reflection libraries (\u00a73.2) during building and testing stages without any rules. We re-implement prior methods with GPT-3.5 and GPT-4 versions the same as ours for fair comparisons. ", "page_idx": 6}, {"type": "text", "text": "ReAct, Reflexion, and ExpeL provide LLM agents with 12 human examples (2 examples per task type) of ALFWorld. For AdaPlanner, they provide 6 human examples (1 example per task type) of ALFWorld as the start of skill discovery. For our methods, agents are provided only one human example of the simplest task $(\\mathtt{P u t})$ on ALFWorld. On MiniWob $^{++}$ , our agents are provided one human example (search-engine) for tasks with feedback and 4 examples for all tasks. On WebArena, our ", "page_idx": 6}, {"type": "text", "text": "Table 1: Success rate $(\\%)$ of LLM agent methods on ALFWorld test tasks. For each method, the number of all human examples used is listed. \u201cPlanner+Lib.\u201d represents only using skill&reflection library during the building and testing stages. We run all experiments 3 times and show the average. ", "page_idx": 7}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/68f7711440b6e7825f8c995f6d43cb22c52493dc6224a8638bf6bd1c3c5ab1b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/7ab7a77987b8115d6b2d201c2aada22220a8b2fc137e93e6bab1c73aba35fcd6.jpg", "table_caption": ["Table 2: Success rate $(\\%)$ of LLM agent methods on 9 task types with feedback and all 53 task types of MiniWoB $^{++}$ . For each method, the number of human examples used is listed. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "agents are also provided with one human demonstration. To reduce randomness, we performed each experiment three times and reported the average. More details of the implementation and prompts for AutoManual can be found in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "4.1 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Main Results on ALFWorld: As shown in Tab. 1, AutoManual significantly outperforms the existing methods, evidenced by overall success rates of $86.2\\%$ when using GPT-3.5-turbo for the testing stage and $97.4\\%$ when using GPT-4-turbo. Noticeably, AutoManual requires little expert prior knowledge about the environment and is only provided with one human example to achieve excellent results. In comparison, the rules induced by ExpeL hardly improve performance, as its offline trajectories are composed of individual actions rather than code. We find the performance of AdaPlanner is lower than reported. One reason is that AdaPlanner requires LLM to output specific formats to complete its function-form code, which is difficult for creative LLM, e.g., GPT-4-turbo. In addition, AdaPlanner and Planner+Lib. are inferior to AutoManual because they only store successful paths as skills and inevitably face the Path Dependence problem. Especially, tasks in Put Two have various scenarios, such as \u201ctwo objects can occur at the same receptacle or different receptacles\u201d, that require different processes to solve (Appendix G shows an example). Furthermore, Planner+Lib. often does not mark error-prone points in its skills, such as \u201ctarget objects may appear in unconventional locations\u201d. ", "page_idx": 7}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/d65efa67847993c055c82db29b89763e01e8e03297e5da7655703d6cd188322c.jpg", "table_caption": ["Table 3: Test on WebArena (Reddit). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "Pwl9n4zlf5/tmp/11b9089335808c7fb19319af1f6ac93701bfdf4dfe4fcc3252c73319b09cdec5.jpg", "img_caption": ["Figure 4: (a) The success rate curve with standard deviation when testing GPT-4-turbo or GPT-3.5- turbo on ALFWorld. Building is performed cross-task or single-task type. (b) The success rate curve with standard deviation using AutoManual or Planner+Lib. when testing with GPT-4-turbo or GPT-3.5-turbo on 9 task types with feedback in MiniWob $^{++}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/3d09369c8a6e9c6ad2e1ef85f80f0ad22462751bcd0ea1e3e26cc76ac1694b91.jpg", "table_caption": ["Table 4: Ablation study of AutoManual on ALFWorld when testing with GPT-4-turbo. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Main Results on $\\mathbf{MiniWoB++:}$ As shown in Tab. 2, the performance of AutoManual exceeds the previous methods and Planner+Lib. by a large margin. Especially in 9 task types with feedback, these tasks have higher diversity and require LLM agents to cope with various situations. For example, the tasks in login-user-popup type will interrupt the agent\u2019s plan at any time, requiring the agent to cope with unexpected situations. Therefore, solely imitating previous successful experiences without extracting targeted rules will lead to task failure. Additionally, due to the flexibility of free-form codes, our method shows better adaptability while requiring fewer expert examples than prior methods. ", "page_idx": 8}, {"type": "text", "text": "Learning Curves. We show the success rate curves (testing with GPT-4-turbo or GPT-3.5-turbo) when gradually increasing the tasks of the building stage in Fig 4. In the left image, we share rules across all task types (Cross-task Type), as in AutoManual, or each task type builds a separate set of rules (Single-task Type) during the building stage. Fig 4 (a) demonstrates that sharing rules across task types can facilitate rule optimization. The rules for each task type deepen understanding of the environment, thereby boosting the planning of other tasks. In Fig 4 (b), we compare AutoManual and Planner+Lib. on 9 tasks with feedback in MiniWob $^{++}$ . We find that Planner+Lib. tends to get stuck with a lower success rate. In the face of highly diverse scenarios, Skill Library cannot express the rules behind the environment, thus falling into the Path Dependency problem. ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this ablation study, we quantify the impact of each core component of the AutoManual framework on performance, specifically focusing on success rates and error reduction during task execution. Since we allowed the Planner to replan up to 3 times, each task could have up to 4 error steps. ", "page_idx": 8}, {"type": "text", "text": "Online v.s. Offline Rule Management: We perform offline AutoManual by collecting all trajectories and then managing rules from them. As Tab 4 shows, without online rule management, the generated manual can only slightly improve planning (from $88.1\\%$ to $90.7\\%$ ). This is because more mundane mistakes and fewer direct successes will occur in the trajectories (the distributional shift problem), and the rules cannot be verified by feedback from the environment. ", "page_idx": 8}, {"type": "text", "text": "Skill&Reflection Libraries: Retrieving historical conclusions is essential for correct planning, as they record massive interacting details that can complement the rules. Without them, there will be more errors in the details, and the success rate drops from $97.4\\%$ to $89.5\\%$ . However, as discussed previously, using plain experiences without inducing rules will lead to Path Dependency. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Case-Conditional Prompts: This strategy further improves the rule management process by reducing the hallucination, as evidenced by an increase in success rate from $93.8\\%$ to $97.4\\%$ . These prompts ensure that the Builder updates rules reasonably and grounded. ", "page_idx": 9}, {"type": "text", "text": "Effect of Manual Formulation: The final formulation of rules into a comprehensive manual contributed to the success rate of $97.4\\%$ and decreased average error steps, demonstrating the effectiveness of presenting rule-based knowledge in an organized and accessible format. It not only aids the Planner in mastering multiple rules but is also friendly for human reading. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce AutoManual, a framework significantly advancing LLM agents by enabling adaptability and continual learning through online rule optimization. Utilizing the structured rule system, AutoManual autonomously generates comprehensive manuals, achieving high success rates in benchmarks like ALFWorld and $\\operatorname{MiniWoB++}_{}$ . This approach reduces reliance on human-provided examples and expert interventions, illustrating a robust method for enhancing agent generalization and addressing the Path Dependency problem in diverse environments. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Thanks to Dr. Zhe Zeng for her invaluable assistance with the OpenAI API and GPT Plus services. This work was supported in part by The National Nature Science Foundation of China (Grant No: 62273303), in part by Yongjiang Talent Introduction Programme (Grant No: 2022A-240-G). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022. 1, 2   \n[2] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of state-aware guidelines for large language model agents. ArXiv, abs/2403.08978, 2024. 3, 6, 7, 8, 13, 14   \n[3] Sirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zi Hen Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework. ArXiv, abs/2308.00352, 2023. 2   \n[4] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. ArXiv, abs/2402.03610, 2024. 3   \n[5] Geunwoo Kim, Pierre Baldi, and Stephen Marcus McAleer. Language models can solve computer tasks. In Neural Information Processing Systems, 2023. 7, 15, 16   \n[6] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. ArXiv, abs/2005.01643, 2020. 2   \n[7] Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493\u20139500, 2022. 1, 2, 4, 14   \n[8] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations (ICLR), 2018. 7 ", "page_idx": 9}, {"type": "text", "text": "[9] Bodhisattwa Prasad Majumder, Bhavana Dalvi, Peter Alexander Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. Clin: A continually learning language agent for rapid task adaptation and generalization. ArXiv, abs/2310.10134, 2023. 3 ", "page_idx": 10}, {"type": "text", "text": "[10] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021. 1 ", "page_idx": 10}, {"type": "text", "text": "[11] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 2 [12] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. In Neural Information Processing Systems, 2022. 2 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[13] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems. ArXiv, abs/2310.08560, 2023. 3 ", "page_idx": 10}, {"type": "text", "text": "[14] Joon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 2023. 1, 2, 3, 6 ", "page_idx": 10}, {"type": "text", "text": "[15] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, and Wei Liu. Communicative agents for software development. ArXiv, abs/2307.07924, 2023. 2 ", "page_idx": 10}, {"type": "text", "text": "[16] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Neural Information Processing Systems, 2023. 1, 2, 3, 7, 8, 14, 15 ", "page_idx": 10}, {"type": "text", "text": "[17] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 7 ", "page_idx": 10}, {"type": "text", "text": "[18] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523\u201311530, 2022. 1, 2 ", "page_idx": 10}, {"type": "text", "text": "[19] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web actions. arXiv, abs/2310.03720, 2024. 8 ", "page_idx": 10}, {"type": "text", "text": "[20] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llmplanner: Few-shot grounded planning for embodied agents with large language models. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2986\u20132997, 2022. 2 ", "page_idx": 10}, {"type": "text", "text": "[21] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. In Neural Information Processing Systems, 2023. 1, 2, 3, 4, 5, 7, 8, 15, 16 ", "page_idx": 10}, {"type": "text", "text": "[22] Sai Vemprala, Rogerio Bonatti, Arthur Fender C. Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. IEEE Access, 12:55682\u201355696, 2023. 4 ", "page_idx": 10}, {"type": "text", "text": "[23] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Chatgpt empowered long-step robot control in various environments: A case application. IEEE Access, 11:95060\u2013 95078, 2023. 1, 2 ", "page_idx": 10}, {"type": "text", "text": "[24] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv, abs/2305.16291, 2023. 1, 2, 3, 4, 5, 13 ", "page_idx": 10}, {"type": "text", "text": "[25] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. ArXiv, abs/2402.01030, 2024. 2, 14 ", "page_idx": 10}, {"type": "text", "text": "[26] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level prompt optimization. ArXiv, abs/2310.16427, 2023. 3 ", "page_idx": 10}, {"type": "text", "text": "[27] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. In Neural Information Processing Systems, 2023. 2   \n[28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Neural Information Processing Systems, 2022. 2, 7   \n[29] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229\u2013256, 1992. 3   \n[30] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. ArXiv, abs/2309.07864, 2023. 2   \n[31] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. ArXiv, abs/2309.03409, 2023. 3   \n[32] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Neural Information Processing Systems, 2023. 3, 13   \n[33] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. 1, 2, 7, 8, 15   \n[34] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yue Ting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization. ArXiv, abs/2402.17574, 2024. 3   \n[35] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Y. Liu, and Gao Huang. Expel: Llm agents are experiential learners. In AAAI Conference on Artificial Intelligence (AAAI), 2024. 1, 2, 3, 6, 7, 8, 14, 15   \n[36] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. ArXiv, abs/2310.04406, 2023. 3   \n[37] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. ArXiv, abs/2307.13854, 2023. 7, 8   \n[38] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. ArXiv, abs/2211.01910, 2022. 3   \n[39] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. ArXiv, abs/2305.17144, 2023. 1, 2, 3   \n[40] Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules. ArXiv, abs/2310.07064, 2023. 3 ", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Limitations 13 ", "page_idx": 12}, {"type": "text", "text": "B Broader Impacts 14   \nC Difference with Prior Methods 14   \nD Implementation Details 14   \nE Presentation of Generated Manuals 16   \nF More Experiments 19   \nF.1 Ablation Study of Rule System 19   \nF.2 Sensitivity Analysis of Examples and Initial Rules 20   \nG Case Study 20   \nG.1 An Example of Path Dependency Problem 20   \nG.2 A Trajectory of The Building Stage 22 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Despite the significant contributions of the AutoManual framework, several limitations warrant further discussion. First, our method heavily relies on the capabilities of GPT-4-turbo to generate reliable rules, which may limit the framework\u2019s applicability with less advanced language models. ", "page_idx": 12}, {"type": "text", "text": "Secondly, the current implementation places all rules directly within the context of LLM, which, while effective in smaller or moderately complex environments, may not scale well to larger, more dynamic settings. For such expansive environments, integrating the rule system with RetrievalAugmented Generation (RAG) techniques, similar to the approach taken by AutoGuide [2], could enhance performance by dynamically selecting relevant rules based on the context, thereby managing the cognitive load on the LLM more efficiently. ", "page_idx": 12}, {"type": "text", "text": "Thirdly, for complex and challenging tasks, the agents of AutoManual are insufficient in exploring the environment, as they only attempt solutions based on current knowledge. To enhance agents\u2019 exploration of unfamiliar environments, it may be necessary to endow the agent with curiosity [24] or combine it with tree search algorithms [32]. ", "page_idx": 12}, {"type": "text", "text": "Lastly, there remains a challenge in ensuring that the Planner agent consistently adheres to the rules outlined in the manual. In practice, the Planner may sometimes ignore these rules (or have hallucinations about observations) and cannot generate the ideal solution code that can be applied to different situations. This indicates a need for additional mechanisms to enforce or verify rule compliance during operations. This issue underscores the potential necessity for developing more sophisticated methods to ensure rule adherence or to introduce more robust validation steps within the planning process. ", "page_idx": 12}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The AutoManual framework, leveraging LLM agents, presents positive and negative impacts on safety. On the positive side, by autonomously generating reliable manuals, our method enhances the predictability and reliability of LLM behaviors in dynamic environments, potentially reducing errors and increasing operational safety. However, relying on LLMs also introduces risks of unpredictable behaviors when agents encounter unanticipated scenarios or when rule adherence is not fully ensured. ", "page_idx": 13}, {"type": "text", "text": "Furthermore, the manuals generated by our method can be invaluable tools for human workers. They encapsulate a distilled form of interaction-based learning that can aid in training, provide decision support, and improve task efficiency in various domains. This can not only enhance productivity but also ensure that human workers are better informed and prepared to manage the complex systems with which they interact. ", "page_idx": 13}, {"type": "text", "text": "Lastly, AutoManual\u2019s ability to generate structured, context-aware manuals from interactive experiences suggests a promising avenue for constructing comprehensive knowledge bases for AI. These manuals can contribute to a global knowledge base shared by LLMs of different sizes, promoting broader AI research and development. It offers a method to systematically organize and retrieve complex interaction data in a way that is accessible and useful for both machines and humans. ", "page_idx": 13}, {"type": "text", "text": "C Difference with Prior Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We compare AutoManual with prior methods that extract rules from experiences, i.e., ExpeL [35] and AutoGuide [2], and discuss all differences here: ", "page_idx": 13}, {"type": "text", "text": "1) Interactive Form. ExpeL and AutoGuide, following Reflexion [16], each time output the thought and next-step action to interact with the environment. Our Planner agent interacts using free-form code. As evidenced by previous works [7, 25], using code as the plan is more efficient because the code will automatically perform the in-plan actions and require LLM responses far fewer times. Additionally, planning with code can enjoy the powerful programming capability of GPT. More importantly, compared with action sequences, code is easier to generalize to similar scenarios, facilitating the management. ", "page_idx": 13}, {"type": "text", "text": "2) Online v.s. Offline. ExpeL and AutoGuide extract rules from offline experiences of Reflexion agent. We update rules online by alternating rule practice and rule management. This online rule management can verify the reliability and applicability of the rules in a timely manner, forbidding rules to be armchair general. Additionally, online-update rules can help planners continuously improve their trajectories, making it easier for higher-quality success processes to emerge. ", "page_idx": 13}, {"type": "text", "text": "3) Collaboration Between Agents. Building rules online also enables collaboration between the Planner and Builder agents. In our AutoManual, the Planner is prompted to describe the rules considered and analyze special phenomena, which ease the management of the Builder. In contrast, the builder in ExpeL and AutoGuide can only receive action sequences from the Planner. ", "page_idx": 13}, {"type": "text", "text": "4) Rule System. In ExpeL and AutoGuide, each rule has only two attributes: the content and score. In our rule system, each rule has four attributes: \"Type\", \"Content\", \"Example\", and \"Logs\". These attributes provide a comprehensive representation of rules and facilitate the management and usage of rules. Moreover, we allow rules to build on top of other rules. ", "page_idx": 13}, {"type": "text", "text": "5) Handling excessive rules. ExpeL and AutoGuide utilize the rule scores to delete the low-score rules. However, we found that rule scores are unreliable in the experiments because the Builder tends to give overconfident scores to all rules. Instead, our AutoManual employs a Consolidator agent to merge and delete redundant rules. ", "page_idx": 13}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the building stage of all experiments, the maximum number of rules was set to 12 to balance the context length and rule diversity. We use OpenAI\u2019s Assistant API for all agents to save their history and prevent duplicate inputs. We set all LLMs\u2019 temperatures to 0 and maximum context length to 16000. Reflexion agents are allowed to try at most 3 trials for each task. For AdaPlanner and AutoManual, we allow the Planner agent to replan at most 3 times on ALFWorld and 6 times on ", "page_idx": 13}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/9d7ba37697bb4f8ce39587a780fe8386728d72bf41da8ef133cde5d593d001cd.jpg", "table_caption": ["Table 5: The GPT models used for each method in our implementation when using GPT-3.5-turbo or GPT-4-turbo as test-time LLM. Here GPT-3.5-turbo denotes gpt-3.5-turbo-1106 and GPT-4-turbo denotes gpt-4-1106-preview. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "MiniWob $^{++}$ in response to the environmental feedback. In the Building stage on ALFWorld, we use 6 tasks for each task type, a total of 36 tasks, by default. We shuffle all tasks, and when tasks in a task type succeed three times in a row during building, we consider that this task type has been solved and will no longer run it. The API call cost for building and formulating stages is about $\\mathbf{514}$ in total. ", "page_idx": 14}, {"type": "text", "text": "We made a slight modification to the text format of ALFWorld to make it more suitable for code: for each object, \"object id\" was changed to \"object_id\", and all preceding articles were removed. The maximum action step for a task is set to 50. ", "page_idx": 14}, {"type": "text", "text": "For the 9 tasks with feedback on Miniwob $^{++}$ , we find that \"email-inbox-nl-turk\" and \"email-inboxforward-nl-turk\" are repetitions of \"email-inbox\" and \"email-inbox-forward-nl\". Therefore, we only used 7 task types in the building stage, while in the testing stage, all 9 types were evaluated. In the building stage, we run a total of 42 tasks (6 tasks for each type). ", "page_idx": 14}, {"type": "text", "text": "For all 53 tasks on Miniwob $^{++}$ , since running the building directly will lead to a large number of rules, and tasks without feedback have low variability, we adopt a two-step building strategy: we first run our building stage on 9 task types with feedback and then only update the skill and reflection libraries on 44 task types without feedback. The 4 examples for experiments on all task types are from the following task types: \u2018click-menu\u2019, \u2018enter-date\u2019, \u2018social-media-some\u2019. We chose these tasks mainly because they are difficult for GPT-4-turbo to try out due to the lack of HTML feedback. For example, the HTML in \u2018enter-date\u2019 will not display the cursor but requires the cursor at the correct position to enter. ", "page_idx": 14}, {"type": "text", "text": "Environmental feedback on executing code in MiniWob $^{++}$ is as follows: Whenever one of the actions is executed, we will log whether the action was executed successfully or failed. Finally, the results of these actions are concatenated together, and used as feedback of the code along with the HTML text. ", "page_idx": 14}, {"type": "text", "text": "For AdaPlanner [21] and RCI [5], we use their official code on GitHub to implement their methods. We fix their bugs on matching text patterns but still find their performance is much lower than they reported. This may be because they have not released the code used in their papers, or their methods are greatly affected by the GPT version (newer versions of GPT will be more creative). ", "page_idx": 14}, {"type": "text", "text": "E Presentation of Generated Manuals ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present the generated manual3 on ALFWorld by AutoManual in Fig 5. As shown in the manual, the Formulator agent categorizes the rules into four categories: \u201cNavigation and Search\u201d, \u201cObject Interaction and Location Management\u201d, \u201cTask-Specific Processes\u201d, and \u201cCorrectness and Validation\u201d with clear introductions. We find our agents have found and recorded lots of important rules that directly impact task completion. For instance, \u201cthe agent should include all possible locations in its search\u201d in rule_0, \u201cThe agent can only hold one object at a time\u201d in rule_3, and \u201cWhen multiple items of the same type are present at a location, the agent may have to choose one to interact with or examine.\u201d in rule_5. For tasks that can be solved by a fixed strategy, such as \u201cCool\u201d, \u201cHeat\u201d, \u201cClean\u201d and \u201cExamine\u201d, AutoManual provides clear Success Process type rules. For the complex task, \u201cPut Two\u201d, AutoManual conducts classified discussions in its Success Process rule. ", "page_idx": 15}, {"type": "text", "text": "These demonstrate that AutoManual resolves the Path Dependency problem of skills by digging deeper into mechanisms, updating and incorporating success processes, and annotating important details. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Moreover, we prompt the Builder to break down large phenomena into specific rules and derive deeper rules from them later. In the initial rules 9, we give a simple example of how to build rules upon rules. In the manual 5, we find that the Builder can correctly utilize rules recording basic mechanisms to build more complex rules. For example, rule_4 uses rule_3 to induce a solution: \u201cIf all objects are found at the same location, handle them sequentially according to rule_3.\u201d ", "page_idx": 15}, {"type": "text", "text": "However, we find the generated manual is still not perfect. There are some unnecessary duplicates between rules, such as rule_7 and rule_8, which are both Success Process type rules for \"Examine\" tasks, but they are divided into two rules. ", "page_idx": 15}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/5338d805ef7f9132ee616c69c1d9e741f22d8977d4e3c1fd236df39655e70714.jpg", "table_caption": ["Table 6: The success rate $(\\%)$ of different methods on 9 task types with feedback of MiniWob++. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Housekeeper Agent Interaction Manual ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Overview ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This manual is intended to assist the housekeeper agent in the successful execution of tasks within a simulated environment. The rules provide guidance on navigating, searching the environment, interacting with objects, and managing task-specific processes, as well as ensuring the correctness of actions using code assertions. ", "page_idx": 16}, {"type": "text", "text": "Navigation and Search ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Introduction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "These rules provide guidance on how to search for objects, including the use of helper methods to streamline the process and ensure thoroughness. ", "page_idx": 16}, {"type": "text", "text": "Included Rules ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "rule_0 (type=\"Special Mechanism\"): Objects can be found in unconventional locations, and the agent should include all possible locations in its search. For example, In epoch_9, the agent found a soapbar on the toilet, which is an unconventional location for storing such items. ", "page_idx": 16}, {"type": "text", "text": "rule_1 (type=\"Useful Helper Method\"): If there are multiple receptacles to be search, the agent can write and use 'find_object' method as shown in the example. For example, ", "page_idx": 16}, {"type": "text", "text": "$\\#$ Define helper method to find object that is needed   \ndef find_object(agent, recep_to_check, object_name): for receptacle in recep_to_check: observation $=$ agent.go_to(receptacle) $\\#$ Check if we need to open the receptacle. If we do, open it. if 'closed' in observation: observation $=$ agent.open(receptacle) # Check if the object is in/on the receptacle. if object_name in observation: object_ids = get_object_with_id(observation, object_name) return object_ids, receptacle return None, None   \n# Use assertions to validate each step   \nassert obiect ids is not None \"Err Could ", "page_idx": 16}, {"type": "text", "text": "Object Interaction and Location Management ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Introduction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "These rules inform the agent on how to interact with objects, from taking and placing items to handling multiple items of the same type.   \nProper location management is crucial for successful task execution. ", "page_idx": 16}, {"type": "text", "text": "Included Rules ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "rule_2 (type=\"Special Phenomena\"): When using a microwave, the agent can interact with it (e.g., heat an object) even if there is another object inside, the agent is holding something, and the microwave door is not explicitly mentioned to be open. For example, In epoch_1, the agent was able to heat the mug with the microwave even though there was an egg inside the microwave and the agent was holding the mug.   \nrule_3 (type=\"Special Mechanism\"): The agent can only hold one object at a time and must put down any held object before taking another. For example, In epoch_2, the agent was holding statue_4 and attempted to take statue_3 without putting down statue_4 first, resulting in a 'Nothing happens' observation.   \nrule_4 (type=\"Success Process\"): When tasked with placing multiple objects in/on a receptacle, the agent can either collect all objects before attempting to place them or find and place them one by one, ensuring they revisit locations with multiple objects if necessary. If all objects are found at the same location, handle them sequentially according to rule_3. For example, In epoch_15, the agent should have revisited sidetable_1 to collect the second pencil before attempting to place it in coffeetable_1. In epoch_23, the agent failed to collect all required statues from coffeetable_1 because it did not revisit, is also addressed by this rule. ", "page_idx": 16}, {"type": "text", "text": "rule_5 (type=\"Special Mechanism\"): The agent must interact with a receptacle to observe its contents, which includes going to the receptacle and opening it if it is closed. Before performing a put or take action, the agent must ensure it is at the correct location. When multiple items of the same type are present at a location, the agent may have to choose one to interact with or examine. ", "page_idx": 17}, {"type": "text", "text": "For example, In epoch_16, the agent had to open several closed cabinets (e.g., cabinet_1, cabinet_2) to find items such as the mug. In epoch_21, the agent observed multiple alarm clocks on desk_1 and selected one ('alarmclock_4') to interact with. ", "page_idx": 17}, {"type": "text", "text": "Task-Specific Processes ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Introduction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This category outlines the steps required to complete specific tasks, such as heating, cooling, and examining objects with another object's assistance. ", "page_idx": 17}, {"type": "text", "text": "Included Rules ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "rule_6 (type=\"Success Process\"): If the task involves cooling or heating an object before placing it, the steps are: (1) search for the object using 'find_object' in rule_1, (2) take the object, (3) cool/heat it as required, (4) go to the target receptacle, and (5) put the object. Ensure the agent's location and the state of the environment are updated after each action. For example, ", "page_idx": 17}, {"type": "text", "text": "# For example, to cool a mug and put it in a coffeemachine: # [Step 1] Use 'find_object' method to search all receptacles # [Step 2] Take the mug # [Step 3] Go to the fridge, open it if necessary, and cool the mug # [Step 4] Go to the coffeemachine and put the cooled mug in it ", "page_idx": 17}, {"type": "text", "text": "rule_7 (type=\"Success Process\"): When tasked with examining an object under a desklamp, the agent should first find the desklamp and the object, ensure the desklamp is on, take the object, and then use the desklamp to examine the object. For example, ", "page_idx": 17}, {"type": "text", "text": "$\\#$ [Step 1] Use 'find_object' method to search for the desklamp and the object $\\#$ [Step 2] Make sure the desklamp is on   \n# [Step 3] Take the object   \n# [Step 4] Use the desklamp to examine the object. ", "page_idx": 17}, {"type": "text", "text": "rule_8 (type $=^{t}$ \"Success Process\"): When tasked to look at an object under a desklamp, ensure the lamp is on before using it to examine the object. For example, ", "page_idx": 17}, {"type": "text", "text": "$\\#$ [Step 4] Go to the desklamp's location and turn it on if it's not already on   \nobservation $=$ agent.go_to(receptacle_with_desklamp)   \nobservation $=$ agent.use(found_desklamp)   \nassert 'turn on' in observation or 'already on' in observation, 'Error in [Step 4]: Failed to use the desklamp. # [Step 5] Similarly, search for the alarm clock and take it.   \n# [Step 6] With the desklamp on, examine the alarm clock using the desklamp. ", "page_idx": 17}, {"type": "text", "text": "Correctness and Validation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Introduction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Instructions on asserting code to confirm state changes and enhance the reliability of the agent's actions. ", "page_idx": 17}, {"type": "text", "text": "Included Rules ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "rule_9 (type=\"Corrected Error\"): Assertions in the agent's code should confirm state changes such as location or held objects, rather than rely on specific phrases in observations. ", "page_idx": 17}, {"type": "text", "text": "For example, Instead of asserting 'You are at' in the observation, the agent should assert the location and held object state changes. Also, when handling multiple required objects at the same location, the agent should manage them sequentially without unnecessary variables. ", "page_idx": 17}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/100277cb7c9179617f9a59c425bb30d9bd1480c9439a6165f176b1f88c867214.jpg", "table_caption": ["Table 7: Ablation study of Rule System on ALFWorld. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/81d25c62ee49b8311a0fc3bd14d17104ece8467a4a548a688056cf35330db936.jpg", "table_caption": ["Table 8: Sensitivity Analysis of Examples and Initial Rules on 9 task types with feedback of MiniWob $^{++}$ . AutoManual uses a human example of search-engine or enter-text task. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "F More Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Ablation Study of Rule System ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The well-structured rule system is essential for the rule management and usage. We perform the ablation study of our rule system in Tab 7. ", "page_idx": 18}, {"type": "text", "text": "\u2022 AutoManual without \u201cType\u201d: we remove the \u201cType\u201d attribute of each rule in the rule system and the instruction for the Builder to manage various types of rules. As shown in Tab 7, without the \u201cType\u201d attribute, the performance drops significantly, from $86.2\\%$ to $74.6\\%$ and $97.4\\%$ to $91.5\\%$ , as the Builder cannot manage specific types of rules and loses specific instruction on each rule. ", "page_idx": 18}, {"type": "text", "text": "\u2022 AutoManual without \u201cExample\u201d: we remove the \u201cExample\u201d attribute of each rule in the rule system. The performance also drops by a large margin, as the Builder cannot specify necessary details in rules, and rules with lower understandability can sometimes be misleading to the Planner. ", "page_idx": 18}, {"type": "text", "text": "\u2022 AutoManual without \u201cValidation Logs\u201d has little impact on performance, but \u201cValidation Logs\u201d of rules are very useful during debugging. ", "page_idx": 18}, {"type": "text", "text": "We also conducted some detailed ablation studies of our AutoManual in Tab 7. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Helper methods defined by humans and the Planner can serve as the solution for a subgoal of a task that can be used in multiple tasks. AutoManual without \u201cUseful Helper Method\u201d: we remove the \u201cUseful Helper Method\u201d in the \u201cType\u201d attribute, the human example, and initial rules, and we no longer encourage the Planner to write helper functions when writing code. The performance drops from $86.2\\%$ to $79.6\\%$ and $97.4\\%$ to $94.3\\%$ , demonstrating that extracting reusable helper methods can facilitate the programming. ", "page_idx": 18}, {"type": "text", "text": "\u2022 AutoManual without Cooperation between Agents: we remove the Planner\u2019s thoughts in the trajectory when providing it to the Builder and no longer require the Planner to output Relevant Rules. The performance decreases significantly, indicating the thoughts and conclusions of the Planner are helpful for the Builder to manage rules. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Case-Conditioned Prompts without Classification: we remove the requirement to classify the error reason to \u201cImperfect Rules\u201d or \u201cImperfect Agents\u201d and only use the prompts for \u201cImperfect Rules\u201d during case-conditioned prompting. As demonstrated in Tab 7, the results are inferior to using all 5 cases because the analysis and classification of error reason can boost the Builder to manage rules accurately. ", "page_idx": 18}, {"type": "text", "text": "F.2 Sensitivity Analysis of Examples and Initial Rules ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We analyze the sensitivity of our rule optimization to the initial condition, i.e., the human example and initial rules. In the experiments on 9 task types with feedback of MiniWob $^{++}$ , we use an example of search-engine task and the corresponding initial rules, which are shown in Listing 8 and 9. In these initial rules, we extract the successful process of search-engine task and a helper method to turn to the next page. Here, we provide AutoManual with an example of enter-text task, the simplest task type in $\\mathrm{MiniWob++}$ , as shown in Listing 10. ", "page_idx": 19}, {"type": "text", "text": "The results in Tab 8 demonstrate that even if given less initial knowledge, AutoManual can still automatically learn the required knowledge through online optimization, which shows that AutoManual has good robustness to the initial rules. In addition, we found that the knowledge it learned about search-engine is highly similar to the standard knowledge provided by humans, such as the need to turn pages for the search index larger than 3. ", "page_idx": 19}, {"type": "text", "text": "G Case Study ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "G.1 An Example of Path Dependency Problem ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the following example, the existing skill can correct solve the \u201cPut two\u201d task when the two objects occur at the same receptacles. However, in the new scenario, the Planner still replicates the strategy but fails to complete the task, as the target objects are at different receptacles. ", "page_idx": 19}, {"type": "text", "text": "Listing 1: An Example of Path Dependency Problem of the Planner on ALFWorld ", "page_idx": 19}, {"type": "image", "img_path": "Pwl9n4zlf5/tmp/4a22158aaa97cc928f075ac6c1a5c580dd56dde995ea401507c644a74a948c43.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "You are in the middle of a room. Looking quickly around you, you see armchair_1, cabinet_4, cabinet_3, cabinet_2, cabinet_1, drawer_5, drawer_4, drawer_3, drawer_2, drawer_1, dresser_1, garbagecan_1, safe_1, shelf_12, shelf_11, shelf_10, shelf_9, shelf_8, shelf_7, shelf_6, shelf_5, shelf_4, shelf_3, shelf_2, shelf_1, sidetable_1, and sofa_1. ", "page_idx": 20}, {"type": "text", "text": "Your task is to: put two pillow in sofa. ", "page_idx": 20}, {"type": "text", "text": "Planner: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "### Understanding of the task: The task is to find two pillows in the environment and place them in or on the sofa. The environment contains various receptacles and surfaces where the pillows might be located. ", "page_idx": 20}, {"type": "text", "text": "### Plan for the task:   \n1. Define a list of all receptacles and surfaces to check for pillows.   \n2. Search each receptacle for pillows.   \n3. Take two pillows once found.   \n4. Move to the sofa\u2019s location.   \n5. Place each pillow on the sofa. ", "page_idx": 20}, {"type": "text", "text": "Here is the code for the task: ", "page_idx": 20}, {"type": "text", "text": "\u2018\u2018\u2018python ", "page_idx": 20}, {"type": "text", "text": "# Helper method to manually parse the observation for pillows   \ndef parse_for_pillows(observation): # This is a simple parser that looks for the word \u2019pillow\u2019 followed by an underscore and a number return [word for word in observation.split() if \u2019pillow\u2019 in word]   \n# Define the list of receptacles to search for the pillows.   \nreceptacles_to_check $=[$ \u2019cabinet_1\u2019, \u2019cabinet_2\u2019, \u2019cabinet_3\u2019, \u2019cabinet_4\u2019, \u2019drawer_1\u2019, \u2019drawer_2\u2019, \u2019drawer_3\u2019, \u2019drawer_4\u2019, \u2019drawer_5\u2019, \u2019dresser_1\u2019, \u2019shelf_1\u2019, \u2019shelf_2\u2019, \u2019shelf_3\u2019, \u2019shelf ${\\_}4{\\mathit{\\Delta}}^{\\prime}$ , \u2019shelf_5\u2019, \u2019shelf $.6\\,\\rangle$ , \u2019shelf_7\u2019, \u2019shelf_8\u2019, \u2019shelf_9\u2019, \u2019shelf_10\u2019, \u2019shelf_11\u2019, \u2019shelf_12\u2019, \u2019sidetable_1\u2019 ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "# Initialize a list to keep track of found pillows. found_pillows $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\mathbf{\\Lambda}\\right]$ ", "page_idx": 20}, {"type": "text", "text": "# Search each receptacle for the pillows.   \nfor receptacle in receptacles_to_check: observation $=$ agent.go_to(receptacle) $\\#\\texttt{I f}$ the receptacle is closed, open it. if \u2019closed\u2019 in observation: observation $=$ agent.open(receptacle) # Manually parse the observation for pillows. found_pillows.extend(parse_for_pillows(observation)) # If two pillows are found, stop searching. if len(found_pillows) $>=~2$ : break ", "page_idx": 20}, {"type": "text", "text": "# Ensure two pillows were found. assert len(found_pillows) $>=~2$ , \u2019Error: Less than 2 pillows found in the environment.\u2019 ", "page_idx": 20}, {"type": "text", "text": "# Take and place the pillows in the sofa.   \nfor pillow in found_pillows[:2]: # Take only two pillows # Go to the receptacle where the pillow was found. observation $=$ agent.go_to(receptacle) # Take the pillow. observation $=$ agent.take_from(pillow, receptacle) # Go to the sofa and put the pillow in it. observation $=$ agent.go_to(\u2019sofa_1\u2019) observation $=$ agent.put_in_or_on(pillow, \u2019sofa_1\u2019) ", "page_idx": 20}, {"type": "text", "text": "Feedback including errors: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "obs_1: Act: agent.go_to(\u2019cabinet_1\u2019). Obs: cabinet_1 is closed. You are at cabinet_1 and holding nothing. obs_2: Act: agent.open(\u2019cabinet_1\u2019). Obs: You open cabinet_1. In cabinet_1, you see tissuebox_1. You are at cabinet_1 and holding nothing. ", "page_idx": 20}, {"type": "text", "text": "obs_3: Act: agent.go_to(\u2019cabinet_2\u2019). Obs: Nothing happens. You are at cabinet_1 and holding nothing. obs_4: Act: agent.go_to(\u2019cabinet_3\u2019). Obs: cabinet_3 is closed. You are at cabinet_3 and holding nothing. obs_5: Act: agent.open(\u2019cabinet_3\u2019). Obs: You open cabinet_3. In cabinet_3, you see box_3, and tissuebox_2 . You are at cabinet_3 and holding nothing. ", "page_idx": 20}, {"type": "text", "text": "obs_6: Act: agent.go_to(\u2019cabinet_4\u2019). Obs: Nothing happens. You are at cabinet_3 and holding nothing. obs_7: Act: agent.go_to(\u2019drawer_1\u2019). Obs: drawer_1 is closed. You are at drawer_1 and holding nothing. obs_8: Act: agent.open(\u2019drawer_1\u2019). Obs: You open drawer_1. In drawer_1, you see nothing. You are at drawer_1 and holding nothing. ", "page_idx": 20}, {"type": "text", "text": "obs_9: Act: agent.go_to(\u2019drawer_2\u2019). Obs: drawer_2 is closed. You are at drawer_2 and holding nothing. obs_10: Act: agent.open(\u2019drawer_2\u2019). Obs: You open drawer_2. In drawer_2, you see nothing. You are at drawer_2 and holding nothing. ", "page_idx": 20}, {"type": "text", "text": "obs_11: Act: agent.go_to(\u2019drawer_3\u2019). Obs: drawer_3 is closed. You are at drawer_3 and holding nothing. obs_12: Act: agent.open(\u2019drawer_3\u2019). Obs: You open drawer_3. In drawer_3, you see nothing. You are at drawer_3 and holding nothing. ", "page_idx": 20}, {"type": "text", "text": "obs_13: Act: agent.go_to(\u2019drawer_4\u2019). Obs: drawer_4 is closed. You are at drawer_4 and holding nothing. ", "page_idx": 20}, {"type": "text", "text": "obs_14: Act: agent.open(\u2019drawer_4\u2019). Obs: You open drawer_4. In drawer_4, you see nothing. You are at drawer_4 and holding nothing.   \nobs_15: Act: agent.go_to(\u2019drawer_5\u2019). Obs: drawer_5 is closed. You are at drawer_5 and holding nothing.   \nobs_16: Act: agent.open(\u2019drawer_5\u2019). Obs: You open drawer_5. In drawer_5, you see nothing. You are at drawer_5 and holding nothing.   \nobs_17: Act: agent.go_to(\u2019dresser_1\u2019). Obs: On dresser_1, you see box_2, laptop_1, statue_1, watch_2, and watch_1. You are at dresser_1 and holding nothing.   \nobs_18: Act: agent.go_to(\u2019shelf_1\u2019). Obs: On shelf_1, you see nothing. You are at shelf_1 and holding nothing.   \nobs_19: Act: agent.go_to(\u2019shelf_2\u2019). Obs: On shelf_2, you see nothing. You are at shelf_2 and holding nothing.   \nobs_20: Act: agent.go_to(\u2019shelf_3\u2019). Obs: Nothing happens. You are at shelf_2 and holding nothing.   \nobs_21: Act: agent.go_to(\u2019shelf_4\u2019). Obs: On shelf_4, you see nothing. You are at shelf_4 and holding nothing.   \nobs_22: Act: agent.go_to(\u2019shelf_5\u2019). Obs: On shelf_5, you see newspaper_2. You are at shelf_5 and holding nothing.   \nobs_23: Act: agent.go_to(\u2019shelf_6\u2019). Obs: On shelf_6, you see nothing. You are at shelf_6 and holding nothing.   \nobs_24: Act: agent.go_to(\u2019shelf_7\u2019). Obs: On shelf_7, you see watch_3. You are at shelf_7 and holding nothing.   \nobs_25: Act: agent.go_to(\u2019shelf_8\u2019). Obs: On shelf_8, you see nothing. You are at shelf_8 and holding nothing.   \nobs_26: Act: agent.go_to(\u2019shelf_9\u2019). Obs: On shelf_9, you see nothing. You are at shelf_9 and holding nothing.   \nobs_27: Act: agent.go_to(\u2019shelf_10\u2019). Obs: On shelf_10, you see nothing. You are at shelf_10 and holding nothing.   \nobs_28: Act: agent.go_to(\u2019shelf_11\u2019). Obs: On shelf_11, you see nothing. You are at shelf_11 and holding nothing.   \nobs_29: Act: agent.go_to(\u2019shelf_12\u2019). Obs: On shelf_12, you see cellphone_1. You are at shelf_12 and holding nothing.   \nobs_30: Act: agent.go_to(\u2019sidetable_1\u2019). Obs: On sidetable_1, you see creditcard_1, keychain_1, and   \nnewspaper_1. You are at sidetable_1 and holding nothing.   \nExecution error:   \nError: Less than 2 pillows found in the environment.   \nCurrent state: You are at sidetable_1 and holding nothing. ", "page_idx": 21}, {"type": "text", "text": "G.2 A Trajectory of The Building Stage ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here, we present a trajectory of the Planner and the corresponding responses of the Builder for a task in ALFWorld. In this \u201cIndirect Success\u201d trajectory, the Planner made the mistake of heating the mug without having it next to the microwave. The Planner then corrected the mistake and provided organized code in its conclusion. After observing this trajectory, the Builder keenly found the cause of the error: the existing \u201cSuccess Process\u201d rule did not indicate that the agent must go to the object before interacting with the object. The Builder updated the \u201cSuccess Process\u201d rule and added a \u201cCorrected Error\u201d rule to clarify the mistake. ", "page_idx": 21}, {"type": "text", "text": "Listing 2: A Trajectory of the Planner and Builder on ALFWorld ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Initial observation and the task: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "You are in the middle of a room. Looking quickly around you, you see cabinet_13, cabinet_12, cabinet_11, cabinet_10, cabinet_9, cabinet_8, cabinet_7, cabinet_6, cabinet_5, cabinet_4, cabinet_3, cabinet_2, cabinet_1, coffeemachine_1, countertop_1, diningtable_1, drawer_1, fridge_1, garbagecan_1, microwave_1, shelf_3, shelf_2, shelf_1, sinkbasin_1, stoveburner_4, stoveburner_3, stoveburner_2, stoveburner_1, and toaster_1.   \nYour task is to: heat some mug and put it in cabinet. ", "page_idx": 21}, {"type": "text", "text": "Planner: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "### Understanding of the observation: The agent is in a room with numerous cabinets, appliances, and surfaces. The task is to heat a mug and then place it in a cabinet. This requires finding a mug, heating it, and then placing it in any cabinet. ", "page_idx": 21}, {"type": "text", "text": "### Rules to consider: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "- \\*\\*rule_0\\*\\*: Ensure all receptacles and surfaces are considered when searching for the mug.   \n- \\*\\*rule_1\\*\\*: Use the \u2019find_object\u2019 method to search for the mug.   \n- \\*\\*rule_2\\*\\*: After finding the mug, heat it and then put it in a cabinet.   \n- \\*\\*rule_ $.3**$ : Open closed receptacles to search for objects. ### Plan for the task:   \n1. Use the \u2019find_object\u2019 method to search for a mug in all receptacles and surfaces. 2. Once the mug is found, take it.   \n3. Heat the mug with an appropriate appliance (e.g., microwave).   \n4. Find an empty cabinet or any cabinet to place the heated mug in.   \n5. Place the mug in the cabinet.   \nHere is the code for the task:   \n\u2018\u2018\u2018python   \n# [Step 1] Define the helper method \u2019find_object\u2019 as provided in rule_1   \ndef find_object(agent, recep_to_check, object_name): for receptacle in recep_to_check: observation $=$ agent.go_to(receptacle) # Check if we need to open the receptacle. If we do, open it. if \u2019closed\u2019 in observation: observation $=$ agent.open(receptacle) # Check if the object is in/on the receptacle. if object_name in observation: object_ids $=$ get_object_with_id(observation, object_name) return object_ids, receptacle return None, None # [Step 2] Get a sorted list of all receptacles and surfaces to check for a mug   \nrecep_to_check $=$ [\u2019cabinet_1\u2019, \u2019cabinet_2\u2019, \u2019cabinet_3\u2019, \u2019cabinet_4\u2019, \u2019cabinet_5\u2019, \u2019cabinet_6\u2019, \u2019cabinet_7 \u2019, \u2019cabinet_8\u2019, \u2019cabinet_9\u2019, \u2019cabinet_10\u2019, \u2019cabinet_11\u2019, \u2019cabinet_12\u2019, \u2019cabinet_13\u2019, \u2019countertop_1\u2019, \u2019 diningtable_1\u2019, \u2019drawer_1\u2019, \u2019fridge_1\u2019, \u2019garbagecan_1\u2019, \u2019microwave_1\u2019, \u2019shelf_1\u2019, \u2019shelf_2\u2019, \u2019shelf_3\u2019, \u2019 sinkbasin_1\u2019, \u2019stoveburner_1\u2019, \u2019stoveburner_2\u2019, \u2019stoveburner_3\u2019, \u2019stoveburner_4\u2019, \u2019toaster_1\u2019, \u2019 coffeemachine_1\u2019]   \nobject_ids, receptacle_with_mug $=$ find_object(agent, recep_to_check, \u2019mug\u2019)   \n# Assert that we have found a mug   \nassert object_ids is not None, f\u2019Error in [Step 2]: There is no mug in/on {recep_to_check}.\u2019   \n# [Step 3] Take the mug   \nfound_mug $=$ object_ids[0]   \nobservation $=$ agent.take_from(found_mug, receptacle_with_mug)   \n# Assert that the agent is holding the mug   \nassert agent.holding $==$ found_mug, f\u2019Error in [Step 3]: I cannot take {found_mug} from {   \nreceptacle_with_mug}.\u2019   \n# [Step 4] Heat the mug with an appropriate appliance (e.g., microwave)   \nobservation $=$ agent.heat_with(found_mug, \u2019microwave_1\u2019)   \n# Assert that the mug has been heated   \nassert \u2019heat\u2019 in observation and found_mug in observation, f\u2019Error in [Step 4]: I cannot heat {found_mug} with microwave_1.\u2019   \n# [Step 5] Find an empty cabinet or any cabinet to place the heated mug in   \n# Since the task does not specify which cabinet to use, we can choose the first one   \nobservation $=$ agent.go_to(\u2019cabinet_1\u2019)   \n# Check if the cabinet is closed. If so, open it.   \nif \u2019closed\u2019 in observation:   \nobservation $=$ agent.open(\u2019cabinet_1\u2019)   \nobservation $=$ agent.put_in_or_on(found_mug, \u2019cabinet_1\u2019)   \n# Assert that the mug is put in the cabinet   \nassert \u2019put\u2019 in observation and found_mug in observation, f\u2019Error in [Step 5]: I cannot put {found_mug} in cabinet_1.\u2019   \n\u2018\u2018\u2018 ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Feedback including errors: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "obs_1: Act: agent.go_to(\u2019cabinet_1\u2019). Obs: cabinet_1 is closed. You are at cabinet_1 and holding nothing. obs_2: Act: agent.open(\u2019cabinet_1\u2019). Obs: You open cabinet_1. In cabinet_1, you see nothing. You are at cabinet_1 and holding nothing. ", "page_idx": 22}, {"type": "text", "text": "obs_33: Act: agent.go_to(\u2019shelf_1\u2019). Obs: On shelf_1, you see mug_1. You are at shelf_1 and holding nothing.   \nobs_34: Act: agent.take_from(\u2019mug_1\u2019, \u2019shelf_1\u2019). Obs: You take mug_1 from shelf_1. You are at shelf_1 and holding mug_1.   \nobs_35: Act: agent.heat_with(\u2019mug_1\u2019, \u2019microwave_1\u2019). Obs: Nothing happens. You are at shelf_1 and holding mug_1.   \nExecution error: ", "page_idx": 22}, {"type": "text", "text": "Error in [Step 4]: I cannot heat mug_1 with microwave_1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Current state: You are at shelf_1 and holding mug_1. ", "page_idx": 22}, {"type": "text", "text": "Planner\u2019s replanning: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "### Understanding of the current observations: ", "page_idx": 22}, {"type": "text", "text": "The agent successfully found a mug on shelf_1 and took it. However, when attempting to heat the mug with the microwave, the action failed with \"Nothing happens.\" This indicates that the agent tried to use the microwave while not being at the microwave\u2019s location. The agent must go to the microwave before attempting to heat the mug. ", "page_idx": 22}, {"type": "text", "text": "### Mistakes in the previous code and the reason for errors ", "page_idx": 22}, {"type": "text", "text": "The error occurred because the agent attempted to heat the mug without being at the microwave\u2019s location. The precondition for heating an object with a receptacle is that the agent must be at the same location as the receptacle. ", "page_idx": 22}, {"type": "text", "text": "### Rules to consider: - \\*\\*rule_0\\*\\*: Ensure all receptacles and surfaces are considered when searching for the mug.   \n- \\*\\*rule_1\\*\\*: Use the \u2019find_object\u2019 method to search for the mug.   \n- \\*\\*rule_2\\*\\*: After finding the mug, heat it and then put it in a cabinet.   \n- \\*\\*rule_3\\*\\*: Open closed receptacles to search for objects. ### Plan for the task:   \n1. Since the mug has already been found and is being held by the agent, proceed to the microwave.   \n2. Open the microwave if it is closed.   \n3. Heat the mug with the microwave.   \n4. Find an empty cabinet or any cabinet to place the heated mug in.   \n5. Place the mug in the cabinet. Here is the updated code for the task: \u2018\u2018\u2018python # [Step 4] Go to the microwave and open it if it\u2019s closed observation $=$ agent.go_to(\u2019microwave_1\u2019) if \u2019closed\u2019 in observation: observation $=$ agent.open(\u2019microwave_1\u2019) # Assert that the agent is at the microwave and it\u2019s open assert \u2019microwave $_-1\\footnote{C o r r e s p o n d i n g a u t h o r.E l e c t r o n i c a l i n s t e n@i f c.e d u.b$ in observation and \u2019closed\u2019 not in observation, f\u2019Error in [Step 4]: Microwave is not open or not at microwave_1.\u2019 # [Step 5] Heat the mug with the microwave observation $=$ agent.heat_with(\u2019mug_1\u2019, \u2019microwave_1\u2019) # Assert that the mug has been heated assert \u2019heat\u2019 in observation and \u2019mug_1\u2019 in observation, f\u2019Error in [Step 5]: I cannot heat mug_1 with microwave_1.\u2019 # [Step 6] Find an empty cabinet or any cabinet to place the heated mug in # Since the task does not specify which cabinet to use, we can choose the first one that is empty or available # We already know that cabinets 1 through 10 are empty or contain other items, so we can choose cabinet_1 for simplicity observation $=$ agent.go_to(\u2019cabinet_1\u2019) if \u2019closed\u2019 in observation: observation $=$ agent.open(\u2019cabinet_1\u2019) # Assert that the agent is at cabinet_1 and it\u2019s open assert \u2019cabinet_1\u2019 in observation and \u2019closed\u2019 not in observation, f\u2019Error in [Step 6]: Cabinet_1 is not open or not at cabinet_1.\u2019 # [Step 7] Place the mug in the cabinet observation $=$ agent.put_in_or_on(\u2019mug_1\u2019, \u2019cabinet_1\u2019) # Assert that the mug is put in the cabinet assert \u2019put\u2019 in observation and \u2019mug_1\u2019 in observation, f\u2019Error in [Step 7]: I cannot put mug_1 in cabinet_1.\u2019 \u2018\u2018\u2018 ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Feedback: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "obs_36: Act: agent.go_to(\u2019microwave_1\u2019). Obs: In microwave_1, you see egg_2. You are at microwave_1 and holding mug_1.   \nobs_37: Act: agent.heat_with(\u2019mug_1\u2019, \u2019microwave_1\u2019). Obs: You heat mug_1 using microwave_1. You are at microwave_1 and holding mug_1.   \nobs_38: Act: agent.go_to(\u2019cabinet_1\u2019). Obs: In cabinet_1, you see nothing. You are at cabinet_1 and holding mug_1.   \nobs_39: Act: agent.put_in_or_on(\u2019mug_1\u2019, \u2019cabinet_1\u2019). Obs: You put mug_1 in/on cabinet_1. You are at cabinet_1 and holding nothing. This epoch is done. Succeed: True   \nCurrent state: You are at cabinet_1 and holding nothing. ", "page_idx": 23}, {"type": "text", "text": "Planner\u2019s conclusion: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "### Summary of misunderstandings and mistakes: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Initially, I attempted to heat the mug without being at the microwave\u2019s location, which is why the action failed with \"Nothing happens.\" The correct procedure is to first go to the microwave before attempting to heat any object. After correcting this mistake, the agent successfully heated the mug and placed it in cabinet_1. ", "page_idx": 23}, {"type": "text", "text": "### Organized code block:   \n\u2018\u2018\u2018python   \n# [Helper Method] Define the helper method \u2019find_object\u2019 as provided in rule_1   \ndef find_object(agent, recep_to_check, object_name): for receptacle in recep_to_check: observation $=$ agent.go_to(receptacle) # Check if we need to open the receptacle. If we do, open it. if \u2019closed\u2019 in observation: observation $=$ agent.open(receptacle) # Check if the object is in/on the receptacle. if object_name in observation: object_ids $=$ get_object_with_id(observation, object_name) return object_ids, receptacle return None, None ", "page_idx": 23}, {"type": "text", "text": "# [Step 1] Get a sorted list of all receptacles and surfaces to check for a mug recep_to_check $=$ [\u2019cabinet_1\u2019, \u2019cabinet_2\u2019, \u2019cabinet_3\u2019, \u2019cabinet_4\u2019, \u2019cabinet_5\u2019, \u2019cabinet_6\u2019, \u2019cabinet_7 \u2019, \u2019cabinet_8\u2019, \u2019cabinet_9\u2019, \u2019cabinet_10\u2019, \u2019cabinet_11\u2019, \u2019cabinet_12\u2019, \u2019cabinet_13\u2019, \u2019countertop_1\u2019, \u2019 diningtable_1\u2019, \u2019drawer_1\u2019, \u2019fridge_1\u2019, \u2019garbagecan_1\u2019, \u2019microwave_1\u2019, \u2019shelf_1\u2019, \u2019shelf_2\u2019, \u2019shelf_3\u2019, \u2019 sinkbasin_1\u2019, \u2019stoveburner_1\u2019, \u2019stoveburner_2\u2019, \u2019stoveburner_3\u2019, \u2019stoveburner_4\u2019, \u2019toaster_1\u2019, \u2019 coffeemachine_1\u2019] ", "page_idx": 23}, {"type": "text", "text": "# Assert that we have found a mug   \nassert object_ids is not None, f\u2019Error in [Step 1]: There is no mug in/on {recep_to_check}.\u2019   \n# [Step 2] Take the mug   \nfound_mug $=$ object_ids[0]   \nobservation $=$ agent.take_from(found_mug, receptacle_with_mug)   \n# Assert that the agent is holding the mug   \nassert agent.holding $==$ found_mug, f\u2019Error in [Step 2]: I cannot take {found_mug} from { receptacle_with_mug}.\u2019 ", "page_idx": 24}, {"type": "text", "text": "assert \u2019microwave_ $.1\\,^{\\prime}$ in observation and \u2019closed\u2019 not in observation, f\u2019Error in [Step 3]: Microwave is not open or not at microwave_1.\u2019 ", "page_idx": 24}, {"type": "text", "text": "assert \u2019heat\u2019 in observation and found_mug in observation, f\u2019Error in [Step 4]: I cannot heat {found_mug} with microwave_1.\u2019 ", "page_idx": 24}, {"type": "text", "text": "# [Step 5] Find an empty cabinet or any cabinet to place the heated mug in # Since the task does not specify which cabinet to use, we can choose the first one that is empty or available ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "# We already know that cabinets 1 through 10 are empty or contain other items, so we can choose cabinet_1   \nfor simplicity   \nobservation $=$ agent.go_to(\u2019cabinet_1\u2019)   \nif \u2019closed\u2019 in observation: observation $=$ agent.open(\u2019cabinet_1\u2019)   \n# Assert that the agent is at cabinet_1 and it\u2019s open   \nassert \u2019cabinet_1\u2019 in observation and \u2019closed\u2019 not in observation, f\u2019Error in [Step 5]: Cabinet_1 is not ", "page_idx": 24}, {"type": "text", "text": "Builder\u2019s classification: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Analyzing the scenario, we can identify the root cause of the observed mistakes and relate them to the existing rules: ", "page_idx": 24}, {"type": "text", "text": "1. The agent initially attempted to heat the mug without being at the microwave\u2019s location, which resulted in the action \"Nothing happens.\" This mistake was due to the agent not following the precondition for the \u2018heat_with\u2018 action, which requires the agent to be at the same location as the receptacle (in this case, the microwave). ", "page_idx": 24}, {"type": "text", "text": "2. The existing rules related to the mistakes are: - \\*\\*rule $.1**$ : This rule provides a \u2019find_object\u2019 method to search for objects, which the agent used correctly to find the mug. - \\*\\*rule $.2**$ : This \u2019Success Process\u2019 rule outlines the steps to put an object in a receptacle after finding and manipulating it. However, it does not explicitly state that the agent must be at the location of the appliance to use it, which is relevant for the heating step. - \\*\\*rule $-3**$ : This rule reminds the agent to open closed receptacles to search for objects, which the agent did correctly. ", "page_idx": 24}, {"type": "text", "text": "3. There is a \u2019Success Process\u2019 rule applicable to this type of task (rule_2), but it does not explicitly cover the precondition for heating an object, which is to be at the same location as the appliance. ", "page_idx": 24}, {"type": "image", "img_path": "Pwl9n4zlf5/tmp/901e122d84b83d655cafaadc81fac5de1bca7be93009bde1fef177e2f6072eed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Based on the analysis, the conclusion is that the fault stems from \\*Imperfect Rules\\*. The rules need to be updated to include the precondition for using appliances, which is a critical step in the \u2019Success Process\u2019 for tasks involving heating objects. ", "page_idx": 24}, {"type": "text", "text": "Builder: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Potential Rules: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. \\*Corrected Error\\*: When attempting to use an appliance (e.g., microwave), the agent must be at the same location as the appliance. This rule is related to the existing \u2019Success Process\u2019 rule (rule_2), but it adds a specific precondition for using appliances that was not previously documented. ", "page_idx": 24}, {"type": "table", "img_path": "Pwl9n4zlf5/tmp/d9a850d28d7f0b45499348edc8e950834fdcc64580914f1fbb6befdcd1b4c20d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "H Prompts and Examples ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "H.1 Prompts for Planner Agent ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Listing 3: System Prompts for Planner Agent ", "page_idx": 25}, {"type": "text", "text": "[Role]   \nYou are an excellent household agent in a simulated environment. Given a household environment and a task, you write Python code to accomplish the task.   \n[Actions]   \nFor each task, an agent is created in an environment, and the initial observation and global memory are printed.   \n# The agent is an instance of Agent class, which includes the state of the agent ( its location, what it\u2019s holding) and the actions it can take.   \nclass Agent: def __init__(self, env: InteractiveEnvEngine): self.location $=$ env.agent_location self.holding $=$ \"nothing\"   \n# Here are the admissible actions the agent can take; all action functions return an observation string of the result of the action. If the precondition of the action is not met, its observation will include \"Nothing happens\".   \n# Go to a receptacle and update the agent\u2019s location.   \n# For example, \u2019On countertop_1, you see candle_1, cloth_2, and soapbar_1.\u2019 = go_to(\u2019countertop_1\u2019)   \n# For example, \u2019On sidetable_2, you see nothing.\u2019 $=$ go_to(\u2019sidetable_2\u2019)   \ndef go_to(self, receptacle): ...   \n# Open a receptacle and observe its contents.   \n# For example, \u2019You open cabinet_1. In cabinet_1, you see cloth_1.\u2019 $=$ open(\u2019 cabinet_1\u2019)   \ndef open(self, receptacle): ...   \n# Close an opened receptacle.   \n# For example, \u2019You close cabinet_1.\u2019 $=$ close(\u2019cabinet_1\u2019)   \ndef close(self, receptacle): \u00b7   \n# Take an object from a receptacle if the agent is not holding anything.   \n# For example, \u2019You take soapbar_1 from towelholder_1.\u2019 $=$ take_from(\u2019soapbar_1\u2019, \u2019towelholder_1\u2019)   \ndef take_from(self, object, receptacle): \u00b7   \n# Put an object in or on a receptacle if the agent is holding it.   \n# For example, \u2019You put soapbar_1 in/on cabinet_1.\u2019 $=$ put_in_or_on(\u2019soapbar_1\u2019, \u2019cabinet_1\u2019)   \ndef put_in_or_on(self, object, receptacle): ...   \n# Use a lamp.   \n# For example, \u2019You turn on desklamp_1.\u2019 $=$ use(\u2019desklamp_1\u2019)   \ndef use(self, object): ...   \n# Clean an object with a receptacle.   \n# For example, \u2019You clean soapbar_1 using sinkbasin_1.\u2019 $=$ clean_with(\u2019soapbar_1 \u2019, \u2019sinkbasin_1\u2019)   \ndef clean_with(self, object, receptacle): ...   \n# Heat an object with a receptacle.   \n# For example, \u2019You heat tomato_1 using microwave_1.\u2019 $=$ heat_with(\u2019tomato_1\u2019, \u2019 microwave_1\u2019)   \ndef heat_with(self, object, receptacle):   \n# Cool an object with a receptacle.   \n# For example, \u2019You cool pan_2 using fridge_1.\u2019 $=$ cool_with(\u2019pan_2\u2019, \u2019fridge_1\u2019)   \ndef cool_with(self, object, receptacle): ... ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "# A useful function that you can use. ", "page_idx": 26}, {"type": "text", "text": "# Extract a list of object_ids with the specified object_name from the observation. def get_object_with_id(observation, object_name): ", "page_idx": 26}, {"type": "text", "text": "[Response Instructions] ", "page_idx": 26}, {"type": "text", "text": "You will be given rules discovered by colleagues, summarizing game mechanisms and experiences of success and failure. Use these rules to guide your coding efforts. ", "page_idx": 26}, {"type": "text", "text": "Aim to understand and apply the principles behind the examples of these rules, adapting them to fit your specific scenarios within the simulated environment. ", "page_idx": 27}, {"type": "text", "text": "Output Format Instructions: ", "page_idx": 27}, {"type": "text", "text": "1. First, explain your understanding of the task and the current observations, then describe rules (with their IDs) that need to be considered and plan for the task. Then, write your code between \"\u2018\u2018\u2018python\" and \"\u2018\u2018\u2018\". No text should follow after the code block. ", "page_idx": 27}, {"type": "text", "text": "2. After receiving feedback, you should also explain your understanding of the current observations, including special (unexpected) formats or phenomena, mistakes in your previous code, and the reasons for errors. Then, describe rules (with their IDs) that need to be considered, plan for the task, and then write code. ", "page_idx": 27}, {"type": "text", "text": "Follow these instructions: ", "page_idx": 27}, {"type": "text", "text": "1. DO NOT USE an undefined function or attribute of the agent. Your code must be directly executable in the given environment. I won\u2019t implement any placeholders in your code for you. ", "page_idx": 27}, {"type": "text", "text": "2. Your code should be consistent with the code examples of the rules (please copy the code if there is no better modification!), making it easier for the builder agent to refine and develop new rules effectively. ", "page_idx": 27}, {"type": "text", "text": "3. In your code, you are encouraged to define helpful functions, which should be general and reusable in different scenes. The helper methods in the rules are already defined in the coding environment, so you can directly use them. If you don \u2019t need to modify them, don\u2019t redefine them. ", "page_idx": 27}, {"type": "text", "text": "4. After defining helper methods, your code should be divided into steps (marked with \"[Step]\" in comments) and write assertions at the end of each step to ensure that each step achieves its subgoal. ", "page_idx": 27}, {"type": "text", "text": "Listing 4: Conclusion Prompt for Indirect Success ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Please summarize the misunderstandings and mistakes you made, and then organize your code into a code block. You should copy the used parts from your previous code, including helper methods and steps. You can only modify the previously wrong step, and make sure you don\u2019t miss any detail! So that I can use it with similar but not the same scenarios. ", "page_idx": 27}, {"type": "text", "text": "Listing 5: Conclusion Prompt for Failure ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "You failed to complete the task. Now carefully review the trajectory of the executed actions and the corresponding observations, then identify the reasons for the failure. This reason is often your mistake or misunderstanding rather than the environment\u2019s error. By carefully comparing with applicable rules and examples, pinpoint where your code deviated from expected standards. If there exist coding errors, specify the segments of the problematic code and elucidate how they contributed to the errors. If you want to write a correction for an error, you must double-check its plausibility! Additionally, determine whether your codes were in strict adherence to the relevant rules and examples. This reflection and documentation will serve as a reminder for completing future tasks. ", "page_idx": 27}, {"type": "text", "text": "H.2 Examples for ALFWorld and MiniWob $^{++}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Listing 6: The Put Example for ALFWorld ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "You are in the middle of a room. Looking quickly around you, you see cabinet_4, cabinet_3, cabinet_2, cabinet_1, countertop_1, garbagecan_1, sinkbasin_2, sinkbasin_1, toilet_2, toilet_1.   \nYour task is to: find some spraybottle.   \n### Understanding of the observation: ...   \n### Rules to consider: ... ", "page_idx": 27}, {"type": "text", "text": "### Plan for the task: I need to get a list of receptacles, and go to search each receptacle until seeing a spraybottle. ", "page_idx": 28}, {"type": "text", "text": "\u2018\u2018\u2018python   \n# Define a helper method to search receptacles for the target object   \ndef find_object(agent, recep_to_check, object_name): for receptacle in recep_to_check: observation $=$ agent.go_to(receptacle) # Check if we need to open the receptacle. If we do, open it. if \u2019closed\u2019 in observation: observation $=$ agent.open(receptacle) # Check if the object is in/on the receptacle. if object_name in observation: object_ids $=$ get_object_with_id(observation, object_name) return object_ids, receptacle return None, None   \n# [Step 1] Get a sorted list of receptacles and surfaces to check for a spraybottle. And use \u2019find_object\u2019 method to search   \nrecep_to_check $=$ [\u2019cabinet_1\u2019, \u2019cabinet_2\u2019, \u2019cabinet_3\u2019, \u2019cabinet_4\u2019, \u2019countertop_1 \u2019, \u2019toilet_1\u2019, \u2019toilet ${\\bf-}^{2}{}^{\\ '}$ , \u2019sinkbasin_1\u2019, \u2019sinkbasin_2\u2019, \u2019garbagecan_1\u2019]   \nobject_ids, receptacle_with_spraybottle $=$ find_object(agent, recep_to_check, \u2019 spraybottle\u2019)   \nassert object_ids is not None, f\u2019Error in [Step 1]: There is no spraybottle in/on { recep_to_check}.\u2019   \n# [Step 2] Take the spraybottle   \nfound_spraybottle $=$ object_ids[0]   \nobservation $=$ agent.take_from(found_spraybottle, receptacle_with_spraybottle)   \nassert agent.holding $==$ found_spraybottle, f\u2019Error in [Step 2]: I cannot take { found_spraybottle} from {receptacle}.\u2019   \n# [Step 3] Go to a toilet and put the spraybottle on it   \nobservation $=$ agent.go_to(\u2019toilet_1\u2019)   \n# check if toilet_1 is closed. If so, open it.   \nif \u2019closed\u2019 in observation: observation $=$ agent.open(\u2019toilet_1\u2019)   \nobservation $=$ agent.put_in_or_on(found_spraybottle, \u2019toilet_1\u2019) ", "page_idx": 28}, {"type": "text", "text": "Listing 7: Initial Rules for ALFWorld Induced from Put Example ", "page_idx": 28}, {"type": "text", "text": "\"rule_0\":   \n{\"rule\": \"At the initial observation of the environment, the agent can only observe receptacles, such as cabinet_1, countertop_1. The agent needs to go to the receptacle to view objects in or on it, even for open receptacles.\",   \n\"type\": \"Special Mechanism\",   \n\"example\": \"\",   \n\"validation_record\": \"Provided by User.\"},   \n\"rule_1\":   \n{\"rule\": \"If there are multiple receptacles to be searched, the agent can write and   \nuse the \u2019find_object\u2019 method as shown in the example.\",   \n\"type\": \"Useful Helper Method\",   \n\"example\": \u2019\u2019\u2019 # Define helper method to find the object that is needed def find_object(agent, recep_to_check, object_name): for receptacle in recep_to_check: observation $=$ agent.go_to(receptacle) # Check if we need to open the receptacle. If we do, open it. if \u2019closed\u2019 in observation: observation $=$ agent.open(receptacle) # Check if the object is in/on the receptacle. if object_name in observation: ", "page_idx": 28}, {"type": "text", "text": "object_ids $=$ get_object_with_id(observation, object_name) return object_ids, receptacle return None, None \u2019\u2019\u2019, \"validation_record\": \"Essential helper method provided by User.\"}, \"rule_2\": {\"rule\": \"If the task is to put some object on some receptacle, first use \u2019 find_object\u2019 method in rule_1 to search all receptacles, take the object, then go to the target receptacle and put the object.\", \"type\": \"Success Process\", \"example\": \u2019\u2019\u2019 # For example, to put some spraybottle on toilet: # [Step 1] Use \u2019find_object\u2019 method in rule_1 to search all receptacles recep_to_check $=$ ... object_ids, receptacle_with_spraybottle $=$ find_object(agent, recep_to_check, \u2019 spraybottle\u2019) # [Step 2] Take the spraybottle # [Step 3] Go to a toilet and put the spraybottle on it # If the toilet is closed, open it first. \u2019\u2019\u2019, \"validation_record\": \"Provided by User.\"} ", "page_idx": 29}, {"type": "text", "text": "For Mine $\\mathrm{Wob++}$ , the Planner is allowed to complete the task in steps: initially output segments of planned steps and pause to observe their outcomes to inform subsequent steps. ", "page_idx": 29}, {"type": "text", "text": "Listing 8: The search-engine Example for MineWob++ ", "page_idx": 29}, {"type": "text", "text": "Your task is to Use the textbox to enter \"Jerald\" and press \"Search\", then find and click the 4th search result.   \n...(HTML)...   \n### Understanding of the observation: ...   \n### Rules to consider: ...   \n### Plan for the task: I need to click <input> textbox and type \"Jerald\". Then I need to click \"Search\" button and get the searching result. Finally, navigate to and click on the 4th search result.   \nHere is a code of my plan:   \n\u2018\u2018\u2018python   \n# [Step 1] Find <input> textbox and type \"Jerald\".   \ntextbox_xpath $=$ \"//\\*[@id=\u2019search-text\u2019]\"   \nagent.click_xpath(textbox_xpath)   \nagent.type(\"Jerald\")   \n# [Step 2] Click the search button   \nsearch_button_xpath $=$ \"//\\*[@id=\u2019search\u2019]\"   \nhtml_string $=$ agent.click_xpath(search_button_xpath)   \n# \\*Pause here\\*, waiting for the search to execute and for the results to be displayed.   \n\u2018\u2018\u2018   \n### Feedbacks ###   \n...(some feedbacks)...   \n...(resulting HTML)...   \n### Understanding of the observation: ... Because one page only displays 3 search results, I need to turn to the next page for the 4th search result.   \n### Rules to consider: ...   \n### Plan for the task: ...   \n\u2018\u2018\u2018python   \n# Define a helper method to turn to the next page.   \ndef turn_to_next_page(agent): next_page_xpath $=$ f\"//\\*[@id=\u2019pagination\u2019]/li[@class $=:$ \u2019page-item next\u2019]\" html_string $=$ agent.click_xpath(next_page_xpath) ", "page_idx": 29}, {"type": "text", "text": "return html_string # [Step 3] Use \u2019turn_to_next_page\u2019 method to turn to the next page for the 4th search result and click the result html_string $=$ turn_to_next_page(agent) # data-result start from 0 result_xpath $=$ f\"//\\*[@id=\u2019page-content\u2019]//a[@data-result=\u20193\u2019]\" agent.click_xpath(result_xpath) \u2018\u2018\u2018 ", "page_idx": 30}, {"type": "text", "text": "Listing 9: Initial Rules for MineWob $^{++}$ Induced from search-engine Example ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2019rule_0\u2019:   \n{\"rule\": \"If the idx of the target search result exceeds 3 (the number of results   \nper page), use \u2019turn_to_next_page\u2019 method as shown in the example.\",   \n\"type\": \"Useful Helper Method\",   \n\"example\": \u2019\u2019\u2019 # Turn to the next page. def turn_to_next_page(agent): next_page_xpath $=$ f\"//\\*[@id=\u2019pagination\u2019]/li[@class $=$ \u2019page-item next\u2019]\" html_string $=$ agent.click_xpath(next_page_xpath) return html_string   \n\u2019\u2019\u2019,   \n\"validation_record\": \"Essential helper method provided by User.\"},   \n\u2019rule_1\u2019: {   \n\"rule\": \"If the task needs to use the search engine, first click <input> textbox,   \ntype the target string, and then click \\\"Search\\\" button. Next, use \u2019   \nturn_to_next_page\u2019 in rule_1 to turn the page if the target idx exceed 3 and click   \nthe target result.\", \"type\": \"Success Process\",   \n\"example\": \u2019\u2019\u2019 # For example, to search Jerald and click the 4th result: # [Step 1] Find <input> textbox and type \"Jerald\". # [Step 2] Click the search button. # [Step 3] Use \u2019turn_to_next_page\u2019 method in rule_1 to turn the page and click the target result.   \n\u2019\u2019\u2019,   \n\"validation_record\": \"Provided by User.\"} ", "page_idx": 30}, {"type": "text", "text": "Listing 10: The enter-text Example for MineWob++ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Your task is to Enter \"Ronda\" into the text field and press Submit.   \n...(HTML)...   \n### Outputs ###   \n### Understanding of the observation: ...   \n### Rules to consider: ...   \n### Plan for the task: I should click the input box, type \"Ronda\" and click the submit button.   \nHere is a code of my plan:   \n\u2018\u2018\u2018python   \n# [Step 1] Click the input box   \ninputbox_xpath $=$ \"//input[@id=\u2019tt\u2019]\"   \nagent.click_xpath(inputbox_xpath)   \n# [Step 2] Type \"Ronda\" into the input box   \nagent.type(\"Ronda\")   \n# [Step 3] Click the submit button   \nsubmit_button_xpath $=$ \"//button[@id=\u2019subbtn\u2019]\"   \nagent.click_xpath(submit_button_xpath)   \n\u2018\u2018\u2018   \n### Feedbacks ###   \n...(some feedbacks)...   \nobs_3: Act: agent.click_xpath(\"//button[@id=\u2019subbtn\u2019]\"). Obs: Action done. The epoch is Done. Succeed: True. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "H.3 Prompts for Builder Agent ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Listing 11: System Prompts for The Builder Agent ", "page_idx": 31}, {"type": "text", "text": "[Role] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "You are observing a housekeeper agent as it codes and acts within a simulated environment (game). Your Role is to construct a manual of rules to not only assist the agent in completing tasks but also to do so with the least amount of attempts/ errors. This requires recording and analyzing the experiences of the agent\u2019s successes and failures, and updating previous discoveries. ", "page_idx": 31}, {"type": "text", "text": "[Functions]   \nYou will be presented with the current epoch\u2019s trajectory. The interaction in the trajectory includes the agent\u2019s analysis, execution code, and the resulting feedback. ", "page_idx": 31}, {"type": "text", "text": "You should use the following methods of rule_system to build and improve rules. ", "page_idx": 31}, {"type": "text", "text": "rule_system.write_rule(rule, type $=$ \"\", example $\\iota=$ \"\", validation_record $\\equiv$ \"\") # Write down a new rule of the game you discovered.   \n# Parameters: ", "page_idx": 31}, {"type": "text", "text": "# rule: a rule of the game you discovered. # type: the type of the rule, chosen from [\"Special Phenomena/Mechanism\", ", "page_idx": 31}, {"type": "text", "text": "Corrected Error\", \"Unresolved Error\", \"Useful Helper Method\", \"Success Process\"]. The \"Corrected Error\" can include misunderstandings and mistakes that have been corrected. ", "page_idx": 31}, {"type": "text", "text": "# example: an example (or code) from the trajectory demonstrates this rule. You can add detailed information in the comment. ", "page_idx": 31}, {"type": "text", "text": "# validation_record: your validation record on this rule, including the epoch IDs and rule IDs from which this rule is induced. ", "page_idx": 31}, {"type": "text", "text": "rule_system.update_rule(rule_id, rule $=$ \"\", type $=$ \"\", example $=$ \"\", validation_record=\"\") ", "page_idx": 31}, {"type": "text", "text": "# Rewrite the attributes of an existing rule when you come up with a better understanding. ", "page_idx": 31}, {"type": "text", "text": "# Input only the attributes you want to rewrite. ", "page_idx": 31}, {"type": "text", "text": "At each epoch, an agent is created in an environment, and the initial observation and target task are printed. The agent can only use the following action functions: ", "page_idx": 31}, {"type": "text", "text": "agent.go_to(receptacle) # Go to a receptacle and update the agent\u2019s location.   \nagent.open(receptacle) # Open a receptacle and observe its contents.   \nagent.close(receptacle) # Close a opened receptacle. ", "page_idx": 31}, {"type": "text", "text": "agent.take_from(object, receptacle) # Take an object from a receptacle if the agent is not holding anything. ", "page_idx": 31}, {"type": "text", "text": "agent.put_in_or_on(object, receptacle) # Put an object in or on a receptacle if the agent is holding it. ", "page_idx": 31}, {"type": "text", "text": "agent.use(object) # Use a lamp.   \nagent.clean_with(object, receptacle) # Clean an object with a receptacle.   \nagent.heat_with(object, receptacle) # Heat an object with a receptacle.   \nagent.cool_with(object, receptacle) # Cool an object with a receptacle. ", "page_idx": 31}, {"type": "text", "text": "get_object_with_id(observation, object_name) # Extracts a list of object_ids with the specified object_name from the observation. ", "page_idx": 32}, {"type": "text", "text": "[Response Instructions]   \nUpon receiving the current trajectory, you should first identify the case of the   \ncurrent trajectory\u2019s result and then build rules following the corresponding instructions. ", "page_idx": 32}, {"type": "text", "text": "Listing 12: Case Classify Prompts for Builder ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Please analyze the scenario to identify the root cause of the observed mistakes and describe the existing rules related to the mistakes. Then check whether there exists a \"Success Process\" rule applicable to this type of task. Finally, determine whether the fault stems from:   \n- \\*Imperfect Rules\\*: the agent encounters unexpected phenomena that are not fully documented in the current rules, or the rules have not included the \"Success Process\" of this task type.   \n- \\*Imperfect Agent\\*: the rules fully document the \"Success Process\" and error reminders of such scenarios, but the agent fails to follow these rules meticulously   \nConsider each step of the process carefully and conclude with either \\*Imperfect Rules\\* or \\*Imperfect Agent\\* based on your analysis. ", "page_idx": 32}, {"type": "text", "text": "Listing 13: Base Prompts for Builder ", "text_level": 1, "page_idx": 32}, {"type": "image", "img_path": "Pwl9n4zlf5/tmp/b577a6dce60cdcf31e86deacad719618fa0ac020252d2e269b9046ed7f0e58de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Listing 14: Case 1 Prompts for Builder ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\\*\\*Rules for Success\\*\\* You should extract \"Useful Helper Method\" and \"Success Process \":   \n\\* For each useful helper function identified: If it is not already included in a rule, create a rule of type \"Useful Helper Method\" and record its code unchanged in the rule\u2019s example section. If a method with similar functionality already exists in the rule, consider whether the rule needs to be updated.   \n$^*$ If the success process does not fall within the scope of an existing \"Success Process\" rule, faithfully document all steps (marked as \"[Step]\") in the successful code within a rule of type \"Success Process\", and document necessary codes and reminders in the rule\u2019s example; if the success process of the current task falls within the scope of the existing \"Success Process\" rule, consider whether the rule needs to be updated to incorporate the current roadmap. ", "page_idx": 33}, {"type": "text", "text": "Listing 15: Case 2 Prompts for Builder ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\\*\\*Rules for Success\\*\\* You should extract \"Useful Helper Method\" and \"Success Process   \n\":   \n$^*$ For each useful helper function identified: If it is not already included in a   \nrule, create a rule of type \"Useful Helper Method\" and record its code unchanged in the rule\u2019s example section. If a method with similar functionality already exists   \nin the rule, consider whether the rule needs to be updated.   \n$^*$ If the success process does not fall within the scope of an existing \"Success   \nProcess\" rule, faithfully document all steps (marked as \"[Step]\") in the successful code within a rule of type \"Success Process\", and document necessary codes and   \nreminders in the rule\u2019s example; if the success process of the current task falls   \nwithin the scope of the existing \"Success Process\" rule, consider whether the rule   \nneeds to be updated to incorporate the current roadmap.   \n\\*\\*Rules for Misstep\\*\\* You should reflect on the main misstep to improve efficiency and log it into the \"Corrected Error\" type rule, including corrective code   \nvalidated by the feedback (with the help of the agent\u2019s analysis and code, but its conclusion may not be correct and should be checked carefully). ", "page_idx": 33}, {"type": "text", "text": "Listing 16: Case 3 Prompts for Builde ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\\*\\*Rules for Success\\*\\* You might need to update \"Useful Helper Method\" and \"Success Process\". \\* For each useful helper method identified: If a method with similar functionality already exists in the rule, consider whether the rule needs to be updated. $^*$ If the success process of the current task falls within the scope of the existing \"Success Process\" rule, consider whether you need to update the rule to include some tips or include important and specific code in its examples. \\*\\*Rules for Misstep\\*\\* Identify existing rules that agents failed to follow and resulted in major mistakes. You should update the rule to emphasize some important points (you can add $**,\\ldots**$ at the part of the rule you want to emphasize) or to add error-prone points (perhaps added to the comments of the example code). ", "page_idx": 33}, {"type": "text", "text": "Listing 17: Case 4 Prompts for Builder ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\\*\\*Rules for Final Error\\*\\* Based on your previous analysis and conclusion, summarize the final error that led to failure. You should write an \"Unresolved Error\" rule to record the error: in what situation, what the agent did, and what results were   \nproduced. So that they can serve as reminders for the agent in the future. Please   \ndon\u2019t rush to propose any definitive reasons or suggestions for the error; just   \nrecord it.   \nThe final error is unresolved and cannot be included in rules of other types than \"   \nUnresolved Error\". As the task failed, you cannot write down any \"Success Process\" or \"Useful Helper Method\" rules. ", "page_idx": 33}, {"type": "text", "text": "Listing 18: Case 5 Prompts for Builder ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\\*\\*Rules for Misstep\\*\\* Identify existing rules that agents failed to follow and resulted in major misstep. You should update the rule to emphasize some important points (you can add $**,\\ldots**$ at the part of the rule you want to emphasize) or to add error-prone points (perhaps added to the comments of the example code). Remember that the rules of \"Success Process\" and \"Useful Helper Method\" type should retain their details. ", "page_idx": 34}, {"type": "text", "text": "H.4 Prompts for Consolidator Agent ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Listing 19: System Prompts for Consolidator ", "page_idx": 34}, {"type": "text", "text": "[Role] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "You are observing a housekeeper agent as it codes and acts within a simulated environment (game). Your goal is to construct a manual of rules to assist the agent in completing various tasks in the environment. Your Role is to merge or delete   \npreviously found rules by analyzing the experiences of the agent.   \n[Functions]   \nYou will be presented with the current found rules. The rules are extracted from   \nmany epochs\u2019 trajectories, in which each interaction includes the agent\u2019s analysis, execution code, and the resulting feedback.   \nA rule is represented with \u2019rule_id\u2019 and has the following attributes: - rule: the description of the rule, which begins with its use case or scope. - type: the type of the rule. - example: an example (or code) from the trajectory demonstrates this rule. You can add detailed information in the comment. - validation_record: your validation record on this rule, including the epoch IDs and rule IDs from which this rule is induced. ", "page_idx": 34}, {"type": "text", "text": "You should use the following methods of rule_system to delete and merge rules. ", "page_idx": 34}, {"type": "text", "text": "rule_system.update_rule(rule_id, rule $=$ \"\", type $\\because$ \"\", example $:=$ \"\", validation_record=\"\") # Rewrite the attributes of an existing rule when you come up with a better   \nunderstanding.   \n# Input only the attributes you want to rewrite.   \nrule_system.delete_rule(rule_id)   \n# Delete a existing rule with rule_id.   \n# \\*\\*How to merge\\*\\* To merge two existing rules, you can call rule_system.update_rule for one rule and then call rule_system.delete_rule to delete another rule. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "agent.close(receptacle) # Close a opened receptacle. ", "page_idx": 35}, {"type": "text", "text": "agent.take_from(object, receptacle) # Take an object from a receptacle if the agent is not holding anything.   \nagent.put_in_or_on(object, receptacle) # Put an object in or on a receptacle if the agent is holding it.   \nagent.use(object) # Use a lamp.   \nagent.clean_with(object, receptacle) # Clean an object with a receptacle.   \nagent.heat_with(object, receptacle) # Heat an object with a receptacle.   \nagent.cool_with(object, receptacle) # Cool an object with a receptacle.   \nget_object_with_id(observation, object_name) # Extracts a list of object_ids with the specified object_name from the observation.   \n[Response Instructions]   \nOutput Process:   \nAfter receiving the current rules, you should select potential rules to investigate and then delete or merge rules.   \nDetailed instructions:   \n\\*\\*Maintain a maximum of 12 rules\\*\\*   \n\\*\\*Merge if addressed\\*\\* If a \"Success Process\" rule can address the \"Corrected Error\" or \"Unresolved Error\" rule, you can consider merging these rules while retaining their details.   \n\\*\\*Retain important details\\*\\* The rules of \"Success Process\" and \"Useful Helper Method\" type should retain their details, and should not be deleted or easily refreshed by new updates. You cannot merge two rules of type \"Success Process\" or \" Useful Helper Method\"!   \n$^{\\ast\\ast}$ Insertion is preferable\\*\\* If a rule is updated to include the content of other rules, you should try to preserve the details of the existing content and   \npreferably insert a categorial discussion or insert new content to it (or its example). ", "page_idx": 35}, {"type": "text", "text": "H.5 Prompts for Formulator Agent ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Listing 20: System Prompts for Formulator ", "page_idx": 35}, {"type": "text", "text": "[Role]   \nYou are observing a housekeeper agent as it codes and acts within a simulated environment (game). Your goal is to construct a manual of rules to assist the agent in completing various tasks in the environment. Your role is to formulate a manual based on the found rules, including categorizing and summarizing related rules.   \n[Functions]   \nYou will be presented with the current found rules. The rules are extracted from many epochs\u2019 trajectories, in which each interaction includes the agent\u2019s analysis, execution code, and the resulting feedback.   \nA rule is represented with \u2019rule_id\u2019 and has the following attributes: - rule: the description of the rule, which begins with its use case or scope. - type: the type of the rule. - example: an example (or code) from the trajectory demonstrates this rule. You can add detailed information in the comment. - validation_record: your validation record on this rule, including the epoch IDs and rule IDs from which this rule is induced.   \n[Actions]   \nAt each epoch, an agent is created in an environment. The agent can only use the following action functions in its code to interact with the environment:   \nagent.go_to(receptacle) # Go to a receptacle and update the agent\u2019s location.   \nagent.open(receptacle) # Open a receptacle and observe its contents.   \nagent.close(receptacle) # Close a opened receptacle.   \nagent.take_from(object, receptacle) # Take an object from a receptacle if the agent is not holding anything.   \nagent.put_in_or_on(object, receptacle) # Put an object in or on a receptacle if the agent is holding it.   \nagent.use(object) # Use a lamp.   \nagent.clean_with(object, receptacle) # Clean an object with a receptacle.   \nagent.heat_with(object, receptacle) # Heat an object with a receptacle.   \nagent.cool_with(object, receptacle) # Cool an object with a receptacle.   \nget_object_with_id(observation, object_name) # Extracts a list of object_ids with the specified object_name from the observation.   \n[Response Instructions]   \nOutput Process:   \nAfter receiving the current rules, you should output the following things:   \n\\* General Understandings: Describe your overall understanding of all rules and some specific rules.   \n\\* Category of Rules: Methodically analyze the connections between related rules,   \nthen cluster these rules, and propose category names for the clusters. Make sure   \neach rule must belong to one and only one category!   \n\\* The Manual: Finally, sequentially write a structured manual within \u2019\u2018\u2018\u2018markdown\u2019   \nand \u2019\u2018\u2018\u2018\u2019. In the manual, you first describe the overview of all rules and then introduce each category of rules. In each category, you should list the rules and write rule_id within $^{\\ast\\ast}$ and $^{\\ast\\ast}$ .   \nDetailed instructions:   \n1. Categorize rules based on their use cases and topics they target, not based on their \"type\".   \n2. If two \"Success Process\" rules follow the same critical success points or process   \n, you can consider categorizing them into one category and propose a general strategy with the critical success points in the Introduction section of the category. But you don\u2019t have to do this if they don\u2019t follow the same critical success points.   \n3. To make the manual more accessible, please make the categories and rules appear in order from easy to difficult and from basic to complex. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "1352 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: As shown in the Section of Experiments, our methods require little human expert knowledge of the new environment and can resolve the path dependency problem. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See Limitation Section. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "01 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n02 a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "4 Justification: The paper includes no theoretical results.   \n5 Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "1416 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We describe the implementation details in the section of Experiments and Appendix. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We have provided the prompts and examples in the Appendix. We have   \nprovided the source code.   \nGuidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have described these in the experiments Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We performed each experiment 3 times and have shown the error bar (standard deviations). ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors). ", "page_idx": 39}, {"type": "text", "text": "1511 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1512 of the mean.   \n1513 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1514 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1515 of Normality of errors is not verified.   \n1516 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1517 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1518 error rates).   \n1519 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1520 they were calculated and reference the corresponding figures or tables in the text.   \n1521 8. Experiments Compute Resources   \n1522 Question: For each experiment, does the paper provide sufficient information on the com  \n1523 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1524 the experiments?   \n1525 Answer: [Yes]   \n1526 Justification: We use the OpenAI API for all experiments and report the cost.   \n1527 Guidelines:   \n1528 \u2022 The answer NA means that the paper does not include experiments.   \n1529 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1530 or cloud provider, including relevant memory and storage.   \n1531 \u2022 The paper should provide the amount of compute required for each of the individual   \n1532 experimental runs as well as estimate the total compute.   \n1533 \u2022 The paper should disclose whether the full research project required more compute   \n1534 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1535 didn\u2019t make it into the paper).   \n1536 9. Code Of Ethics   \n1537 Question: Does the research conducted in the paper conform, in every respect, with the   \n1538 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1539 Answer: [Yes]   \n1540 Justification: We have reviewed the NeurIPS Code of Ethics.   \n1541 Guidelines:   \n1542 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1543 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1544 deviation from the Code of Ethics.   \n1545 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1546 eration due to laws or regulations in their jurisdiction).   \n1547 10. Broader Impacts   \n1548 Question: Does the paper discuss both potential positive societal impacts and negative   \n1549 societal impacts of the work performed?   \n1550 Answer: [Yes]   \n1551 Justification: See Broader Impacts Section.   \n1552 Guidelines:   \n1553 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1554 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1555 impact or why the paper does not address societal impact.   \n1556 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1557 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1558 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1559 groups), privacy considerations, and security considerations.   \n1560 \u2022 The conference expects that many papers will be foundational research and not tied   \n1561 to particular applications, let alone deployments. However, if there is a direct path to   \n1562 any negative applications, the authors should point it out. For example, it is legitimate   \n1563 to point out that an improvement in the quality of generative models could be used to   \n1564 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1565 that a generic algorithm for optimizing neural networks could enable people to train   \n1566 models that generate Deepfakes faster.   \n1567 \u2022 The authors should consider possible harms that could arise when the technology is   \n1568 being used as intended and functioning correctly, harms that could arise when the   \n1569 technology is being used as intended but gives incorrect results, and harms following   \n1570 from (intentional or unintentional) misuse of the technology.   \n1571 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1572 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1573 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1574 feedback over time, improving the efficiency and accessibility of ML).   \n1575 11. Safeguards   \n1576 Question: Does the paper describe safeguards that have been put in place for responsible   \n1577 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1578 image generators, or scraped datasets)?   \n1579 Answer: [NA]   \n1580 Justification: The paper does not provide pretrained language models, image generators, or   \n1581 scraped datasets.   \n1582 Guidelines:   \n1583 \u2022 The answer NA means that the paper poses no such risks.   \n1584 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1585 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1586 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1587 safety filters.   \n1588 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1589 should describe how they avoided releasing unsafe images.   \n1590 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1591 not require this, but we encourage authors to take this into account and make a best   \n1592 faith effort.   \n1593 12. Licenses for existing assets   \n1594 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1595 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1596 properly respected?   \n1597 Answer: [Yes]   \n1598 Justification: We have stated and cited the asset used in the paper.   \n1599 Guidelines:   \n1600 \u2022 The answer NA means that the paper does not use existing assets.   \n1601 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1602 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1603 URL.   \n1604 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1605 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1606 service of that source should be provided.   \n1607 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1608 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1609 has curated licenses for some datasets. Their licensing guide can help determine the   \n1610 license of a dataset.   \n1611 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1612 the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to ", "page_idx": 42}, {"type": "text", "text": "1614 the asset\u2019s creators.   \n1615 13. New Assets   \n1616 Question: Are new assets introduced in the paper well documented and is the documentation   \n1617 provided alongside the assets?   \n1618 Answer: [NA]   \n1619 Justification: The paper does not release new assets.   \n1620 Guidelines:   \n1621 \u2022 The answer NA means that the paper does not release new assets.   \n1622 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1623 submissions via structured templates. This includes details about training, license,   \n1624 limitations, etc.   \n1625 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1626 asset is used.   \n1627 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1628 create an anonymized URL or include an anonymized zip file.   \n1629 14. Crowdsourcing and Research with Human Subjects   \n1630 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1631 include the full text of instructions given to participants and screenshots, if applicable, as   \n1632 well as details about compensation (if any)?   \n1633 Answer: [NA]   \n1634 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1635 Guidelines:   \n1636 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1637 human subjects.   \n1638 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1639 tion of the paper involves human subjects, then as much detail as possible should be   \n1640 included in the main paper.   \n1641 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1642 or other labor should be paid at least the minimum wage in the country of the data   \n1643 collector.   \n1644 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1645 Subjects   \n1646 Question: Does the paper describe potential risks incurred by study participants, whether   \n1647 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1648 approvals (or an equivalent approval/review based on the requirements of your country or   \n1649 institution) were obtained?   \n1650 Answer: [NA]   \n1651 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1652 Guidelines:   \n1653 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1654 human subjects.   \n1655 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1656 may be required for any human subjects research. If you obtained IRB approval, you   \n1657 should clearly state this in the paper. ", "page_idx": 42}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]