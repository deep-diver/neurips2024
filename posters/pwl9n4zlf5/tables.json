[{"figure_path": "Pwl9n4zlf5/tables/tables_7_1.jpg", "caption": "Table 1: Success rate (%) of LLM agent methods on ALFWorld test tasks. For each method, the number of all human examples used is listed. \u201cPlanner+Lib.\u201d represents only using skill&reflection library during the building and testing stages. We run all experiments 3 times and show the average.", "description": "This table presents the success rates of different Large Language Model (LLM) agent methods on ALFWorld benchmark tasks.  It compares the performance of AutoManual against several existing methods (ReAct, Reflexion, ExpeL, AdaPlanner) and a baseline (Planner+Lib). The table shows the success rates for each of six individual tasks within ALFWorld, along with an overall average success rate.  The number of human examples used to train each method is also shown. The experiments were run three times, and the average success rate is reported.", "section": "4.1 Main Results"}, {"figure_path": "Pwl9n4zlf5/tables/tables_7_2.jpg", "caption": "Table 2: Success rate (%) of LLM agent methods on 9 task types with feedback and all 53 task types of MiniWoB++. For each method, the number of human examples used is listed.", "description": "This table presents the success rates of various Large Language Model (LLM) agent methods on the MiniWoB++ benchmark.  It compares the performance of AutoManual against other state-of-the-art methods (ReAct, Reflexion, ExpeL, AdaPlanner, and RCI) on two different sets of tasks within the MiniWoB++ environment: a subset of 9 tasks with feedback and the full set of 53 tasks.  The number of human examples used to train each method is also provided to illustrate the relative data efficiency of each approach.  The table highlights AutoManual's superior performance across both task sets, especially with the GPT-4-turbo model, even when trained with a significantly reduced number of examples compared to other methods.", "section": "4 Experiments"}, {"figure_path": "Pwl9n4zlf5/tables/tables_7_3.jpg", "caption": "Table 3: Test on WebArena (Reddit)", "description": "This table presents the success rate (%) of different LLM agent methods on the Reddit domain within the WebArena environment.  It compares the performance of AutoManual against existing methods (ReAct, AutoGuide, and SteP) by showing the success rate achieved with a single example versus the number of examples used by the compared methods.  The table highlights the performance improvement of AutoManual in a complex, realistic web environment.", "section": "4.1 Main Results"}, {"figure_path": "Pwl9n4zlf5/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study of AutoManual on ALFWorld when testing with GPT-4-turbo.", "description": "This table presents the results of an ablation study conducted on the AutoManual framework using the GPT-4-turbo model for testing on ALFWorld.  The study systematically removes components of the AutoManual framework (online rule management, skill and reflection libraries, case-conditioned prompting, and manual formulation) to assess their individual contributions to the overall performance. The results are presented in terms of the average number of error steps and the success rate.  The table helps to understand the relative importance of each component in achieving high performance.", "section": "4.2 Ablation Study"}, {"figure_path": "Pwl9n4zlf5/tables/tables_14_1.jpg", "caption": "Table 1: Success rate (%) of LLM agent methods on ALFWorld test tasks. For each method, the number of all human examples used is listed. \u201cPlanner+Lib.\u201d represents only using skill&reflection library during the building and testing stages. We run all experiments 3 times and show the average.", "description": "This table presents the success rates achieved by different Large Language Model (LLM) agent methods on ALFWorld test tasks.  Each method is evaluated, and its performance is shown by the success rate.  The table indicates how many human examples were used to train each method.  There is also a comparative analysis involving the use of only the skill & reflection library during both building and testing phases. The average success rate across three runs is reported for each method.", "section": "4.1 Main Results"}, {"figure_path": "Pwl9n4zlf5/tables/tables_15_1.jpg", "caption": "Table 2: Success rate (%) of LLM agent methods on 9 task types with feedback and all 53 task types of MiniWoB++. For each method, the number of human examples used is listed.", "description": "This table compares the success rates of different LLM agent methods (RCI, AdaPlanner, Planner+Lib, and AutoManual) on the MiniWoB++ benchmark.  It shows the performance on two sets of tasks: 9 tasks with feedback and all 53 tasks. The number of human examples used for training each method is also indicated.  The table helps demonstrate the effectiveness of AutoManual in achieving high success rates even with limited training data compared to other methods.", "section": "4.1 Main Results"}, {"figure_path": "Pwl9n4zlf5/tables/tables_18_1.jpg", "caption": "Table 7: Ablation study of Rule System on ALFWorld.", "description": "This table presents the results of an ablation study conducted on the ALFWorld benchmark to evaluate the impact of different components of the AutoManual framework on the success rate of LLM agents.  The study removes components such as the \"Type\" attribute, \"Example\" attribute, \"Validation Logs\", \"Useful Helper Method\", and the cooperation between the agents, as well as testing the performance of the case-conditioned prompts without classification. The success rate is measured using both GPT-3.5-turbo and GPT-4-turbo language models.  The results illustrate how each component contributes to the overall performance of the system.", "section": "4.2 Ablation Study"}, {"figure_path": "Pwl9n4zlf5/tables/tables_18_2.jpg", "caption": "Table 8: Sensitivity Analysis of Examples and Initial Rules on 9 task types with feedback of MiniWoB++. AutoManual uses a human example of search-engine or enter-text task.", "description": "This table presents the results of a sensitivity analysis conducted to evaluate the impact of different initial conditions (human examples and initial rules) on the performance of AutoManual.  The experiment was conducted on 9 MiniWoB++ tasks that involve user feedback.  Two scenarios are tested: using a \"search-engine\" example and using a simpler \"enter-text\" example. The success rate is measured for both GPT-3.5-turbo and GPT-4-turbo language models. The results demonstrate the robustness of AutoManual to variations in initial conditions.", "section": "F.2 Sensitivity Analysis of Examples and Initial Rules"}, {"figure_path": "Pwl9n4zlf5/tables/tables_25_1.jpg", "caption": "Table 1: Success rate (%) of LLM agent methods on ALFWorld test tasks. For each method, the number of all human examples used is listed. \u201cPlanner+Lib.\u201d represents only using skill&reflection library during the building and testing stages. We run all experiments 3 times and show the average.", "description": "This table presents the success rates of different Large Language Model (LLM) agent methods on ALFWorld test tasks.  It compares the performance of AutoManual against several existing methods (ReAct, Reflexion, ExpeL, AdaPlanner).  The table shows success rates for different sub-tasks within ALFWorld (Put, Clean, Heat, Cool, Examine, Put Two) as well as the overall success rate.  The number of human examples used for training each method is also listed.  A control experiment called \"Planner+Lib.\" uses only the skill and reflection library during both training and testing, providing a baseline comparison. The experiment was conducted three times, and the average success rate is presented.", "section": "4.1 Main Results"}]