[{"figure_path": "0cgDDa4OFr/figures/figures_1_1.jpg", "caption": "Figure 1: Maximum entropy source distribution estimation. Given an observed dataset D = {x1,...,xn} from some data distribution p\u2080(x), the source distribution estimation problem is to find the parameter distribution q(\u03b8) that reproduces p\u2080(x) when passed through the simulator p(x|\u03b8), i.e. q#(x) = \u222b p(x|\u03b8)q(\u03b8)d\u03b8 = p(x) for all x. This problem can be ill-posed, as there might be more than one distinct source distribution. We resolve this by targeting the maximum entropy distribution, which is unique.", "description": "The figure illustrates the concept of maximum entropy source distribution estimation.  It shows that multiple different source distributions q(\u03b8) can produce the same data distribution q#(x) after being passed through a simulator p(x|\u03b8). This is a problem because the inference task (source distribution estimation) becomes ill-posed; there isn't a unique solution. The solution proposed in the paper is to select the maximum entropy source distribution, which is guaranteed to be unique.", "section": "Abstract"}, {"figure_path": "0cgDDa4OFr/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of Sourcerer. Given a source distribution q(\u03b8), we sample \u03b8 ~ q and simulate using p(x|\u03b8) to obtain samples from the pushforward distribution q#(x) = \u222bp(x|\u03b8)q(\u03b8)d\u03b8. We maximize the entropy of the source distribution q(\u03b8) while regularizing with a Sliced-Wasserstein distance (SWD) term between the pushforward of q# and the data distribution p\u2080(x) (Eq. (3)). \u03b8 and x in top right corner of boxes denote parameter space and data/observation space, respectively.", "description": "This figure illustrates the Sourcerer method.  It begins with a source model, q(\u03b8), which generates samples of parameters \u03b8. These parameters are then passed through a simulator, p(x|\u03b8), producing samples of simulated data,  q#(x). The Sliced-Wasserstein Distance (SWD) measures the discrepancy between the simulated data (q#(x)) and the observed data distribution, p\u2080(x).  The goal is to find the maximum entropy distribution q(\u03b8) that minimizes the SWD, balancing between maximizing entropy (uncertainty) and matching the observed data.", "section": "2 Methods"}, {"figure_path": "0cgDDa4OFr/figures/figures_4_1.jpg", "caption": "Figure 3: Results for the source estimation benchmark. (a) Original and estimated source and corresponding pushforward for the differentiable IK simulator (\u03bb = 0.35). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. (b) Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \u03bb. For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \u03bb = 0.35 (\u03bb = 0.062 for GM) indicates our choice of final \u03bb for the numerical benchmark results (Table 1).", "description": "Figure 3 presents a comparison of the proposed Sourcerer method against a state-of-the-art approach for source distribution estimation.  Panel (a) shows an example using a differentiable Inverse Kinematics simulator, illustrating how Sourcerer finds a higher-entropy source distribution that still accurately matches the observed data. Panel (b) presents a quantitative comparison across four benchmark tasks, showing Sourcerer's superior performance in terms of both accuracy (C2ST) and higher entropy of the estimated sources across different regularization strengths.", "section": "3 Experiments"}, {"figure_path": "0cgDDa4OFr/figures/figures_6_1.jpg", "caption": "Figure 4: Source estimation on differentiable simulators. For both the deterministic SIR model (a) and probabilistic Lotka-Volterra model (b), the Sliced-Wasserstein distance (lower is better) between observations and simulations as well as entropy of estimated sources (higher is better) for different choices of \u03bb and without the entropy regularization (NA) are shown. Mean and standard deviation are computed over five runs.", "description": "This figure displays the results of applying the Sourcerer method to two high-dimensional differentiable simulators: a deterministic SIR model and a probabilistic Lotka-Volterra model.  The plots show the Sliced-Wasserstein distance (a measure of the discrepancy between the observed data distribution and the distribution of simulations generated by the estimated source) and the entropy of the estimated source distributions for different values of the regularization parameter \u03bb. A higher entropy indicates more uncertainty in the estimated parameters, while a lower SWD indicates a better fit to the observed data.  The figure demonstrates the impact of the entropy regularization on both the accuracy (SWD) and uncertainty (entropy) of source estimation, highlighting the ability of the method to increase the entropy of the estimated distributions without sacrificing accuracy.", "section": "3.2 High-dimensional observations: Lotka-Volterra and SIR"}, {"figure_path": "0cgDDa4OFr/figures/figures_8_1.jpg", "caption": "Figure 5: Source estimation for the single-compartment Hodgkin-Huxley model. (a) Example voltage traces of the real observations of the motor cortex dataset, simulations from the estimated source (\u03bb = 0.25), and samples from the uniform distribution used to train the surrogate. (b) 1D and 2D marginals for three of the five summary statistics used to perform source estimation. (c) 1D and 2D marginal distributions of the estimated source for three of the 13 simulator parameters. (d) and (e) C2ST accuracy and Sliced-Wasserstein distance (lower is better) as well as entropy of estimated sources (higher is better) for different choices of \u03bb including \u03bb = 0.25 (gray line) and without entropy regularization (NA). Mean and standard deviation over five runs are shown.", "description": "This figure shows the results of applying the Sourcerer method to a real-world dataset of electrophysiological recordings from the mouse motor cortex using a single-compartment Hodgkin-Huxley model.  Panel (a) compares example voltage traces from real observations, simulations using the estimated source distribution, and simulations from a uniform prior distribution.  Panels (b) and (c) show the marginal distributions of the summary statistics (used for model fitting) and the estimated parameters, respectively.  Panels (d) and (e) display the C2ST accuracy, entropy, and SWD for varying regularization strength (\u03bb), highlighting the benefit of entropy regularization in achieving high-entropy, accurate source estimates.", "section": "3.3 Estimating source distributions for a single-compartment Hodgkin-Huxley model"}, {"figure_path": "0cgDDa4OFr/figures/figures_19_1.jpg", "caption": "Figure A1: Failure of the average posterior as a source distribution for the bimodal likelihood example. Each of the individual posteriors is bimodal, resulting in an average posterior with 3 modes (left), the secondary modes produce observations which are not observed in the data distribution when pushed through the likelihood (right), and should not be part of the source distribution.", "description": "This figure demonstrates the failure of the average posterior to represent the source distribution for a bimodal likelihood.  The average posterior is multimodal whereas the estimated source and original source are unimodal. This illustrates that simply averaging posteriors is not a reliable method for source estimation. The figure highlights that the average posterior includes modes that are not supported by the data distribution when passing through the likelihood function, showing that it is not a valid representation of the underlying source distribution.", "section": "A.8 Examples related to the average posterior distribution"}, {"figure_path": "0cgDDa4OFr/figures/figures_20_1.jpg", "caption": "Figure 3: Results for the source estimation benchmark. (a) Original and estimated source and corresponding pushforward for the differentiable IK simulator (\u03bb = 0.35). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. (b) Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \u03bb. For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \u03bb = 0.35 (\u03bb = 0.062 for GM) indicates our choice of final \u03bb for the numerical benchmark results (Table 1).", "description": "This figure presents the results of a source estimation benchmark comparing the proposed Sourcerer method against Neural Empirical Bayes (NEB).  Panel (a) shows an example of the original and estimated source distributions for the inverse kinematics (IK) task, highlighting the increase in entropy achieved by Sourcerer without sacrificing accuracy. Panel (b) provides a quantitative comparison across four benchmark tasks (two moons, IK, SLCP, and Gaussian mixture), demonstrating Sourcerer's superior performance in terms of both C2ST accuracy and entropy across various regularization strengths.", "section": "3 Experiments"}, {"figure_path": "0cgDDa4OFr/figures/figures_21_1.jpg", "caption": "Figure 3: Results for the source estimation benchmark. (a) Original and estimated source and corresponding pushforward for the differentiable IK simulator (\u03bb = 0.35). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. (b) Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \u03bb. For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \u03bb = 0.35 (\u03bb = 0.062 for GM) indicates our choice of final \u03bb for the numerical benchmark results (Table 1).", "description": "The figure displays the results of the source estimation benchmark comparing the proposed method Sourcerer to the Neural Empirical Bayes method.  Panel (a) shows a specific example for the inverse kinematics task, illustrating that the estimated source distribution has higher entropy while maintaining data fidelity compared to the original source. Panel (b) provides a quantitative comparison across four different benchmark tasks, showing Sourcerer's superior performance in terms of higher entropy in the estimated sources without compromising simulation accuracy as measured by the classifier two-sample test (C2ST). The impact of entropy regularization (\u03bb) is also evaluated.", "section": "3 Experiments"}, {"figure_path": "0cgDDa4OFr/figures/figures_21_2.jpg", "caption": "Figure 3: Results for the source estimation benchmark. (a) Original and estimated source and corresponding pushforward for the differentiable IK simulator (\u03bb = 0.35). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. (b) Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \u03bb. For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \u03bb = 0.35 (\u03bb = 0.062 for GM) indicates our choice of final \u03bb for the numerical benchmark results (Table 1).", "description": "The figure displays results of the source estimation benchmark. The left panel (a) illustrates the original and estimated source distribution and the corresponding simulation results for one of the four benchmark tasks.  The right panel (b) provides a comparison of the performance of the proposed method Sourcerer against another state-of-the-art method NEB on all four benchmark tasks, evaluating both the original simulators and learned surrogates for different choices of the regularization parameter \u03bb.", "section": "3 Experiments"}, {"figure_path": "0cgDDa4OFr/figures/figures_22_1.jpg", "caption": "Figure 3: Results for the source estimation benchmark. (a) Original and estimated source and corresponding pushforward for the differentiable IK simulator (\u03bb = 0.35). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. (b) Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \u03bb. For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \u03bb = 0.35 (\u03bb = 0.062 for GM) indicates our choice of final \u03bb for the numerical benchmark results (Table 1).", "description": "This figure shows the results of the source estimation benchmark comparing the proposed Sourcerer method against Neural Empirical Bayes (NEB).  The left panel (a) displays original and estimated source distributions for one task (inverse kinematics) showcasing that Sourcerer produces higher entropy estimates without sacrificing accuracy. The right panel (b) provides a summary across four benchmark tasks, showing the performance of Sourcerer with and without entropy regularization for different values of the regularization parameter \u03bb.  The results demonstrate that Sourcerer consistently achieves higher entropy estimates with comparable or better accuracy than NEB.", "section": "3 Experiments"}, {"figure_path": "0cgDDa4OFr/figures/figures_22_2.jpg", "caption": "Figure 3: Results for the source estimation benchmark. (a) Original and estimated source and corresponding pushforward for the differentiable IK simulator (\u03bb = 0.35). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. (b) Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \u03bb. For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \u03bb = 0.35 (\u03bb = 0.062 for GM) indicates our choice of final \u03bb for the numerical benchmark results (Table 1).", "description": "This figure presents the results of a source estimation benchmark comparing the proposed Sourcerer method to Neural Empirical Bayes (NEB).  Panel (a) shows a specific example using the Inverse Kinematics (IK) simulator, illustrating that the estimated source distribution has higher entropy than the original, yet produces simulations matching the observations. Panel (b) provides a summary of results across four benchmark tasks (Two Moons, Inverse Kinematics, Simple Likelihood Complex Posterior, and Gaussian Mixture), comparing the methods with and without entropy regularization, and using both original and surrogate simulators.  The results demonstrate that Sourcerer achieves comparable accuracy (C2ST) to NEB while consistently obtaining higher entropy source distributions.", "section": "3 Experiments"}, {"figure_path": "0cgDDa4OFr/figures/figures_23_1.jpg", "caption": "Figure A7: Original and estimated source distributions for the benchmark SLCP simulator. The estimated source has higher entropy than the original source.", "description": "This figure compares the original source distribution used to generate the data for the SLCP benchmark task with the estimated source distribution obtained using the Sourcerer method.  The visualization shows that the estimated source distribution, while accurately reproducing the observed data distribution (in terms of the pushforward), has a higher entropy than the original source. This demonstrates Sourcerer's ability to find a distribution that maintains higher uncertainty while still maintaining consistency with observations. The visualization displays marginal distributions of the 5 parameters for both the original and estimated sources.", "section": "A.11 Supplementary figures"}, {"figure_path": "0cgDDa4OFr/figures/figures_24_1.jpg", "caption": "Figure A8: Original and estimated source distributions for the SIR and Lotka-Volterra model. For the Lotka-Volterra model, the estimated source has higher entropy than the original source.", "description": "This figure visualizes the original and estimated source distributions for both the SIR and Lotka-Volterra models.  For each model, it shows 1D marginal distributions and 2D scatter plots of the parameter pairs. The key observation is that for the Lotka-Volterra model, the estimated source distribution exhibits higher entropy (more spread out) than the original source distribution. This demonstrates the Sourcerer method's ability to recover source distributions with higher entropy, while still maintaining fidelity to the observations.", "section": "A.11 Supplementary figures"}, {"figure_path": "0cgDDa4OFr/figures/figures_25_1.jpg", "caption": "Figure 3: Results for the source estimation benchmark. (a) Original and estimated source and corresponding pushforward for the differentiable IK simulator (\u03bb = 0.35). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. (b) Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \u03bb. For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \u03bb = 0.35 (\u03bb = 0.062 for GM) indicates our choice of final \u03bb for the numerical benchmark results (Table 1).", "description": "This figure presents the results of a source estimation benchmark comparing the authors' method (Sourcerer) to Neural Empirical Bayes (NEB).  Panel (a) shows an example of the original and estimated source distributions and their corresponding pushforward distributions for a specific task (Inverse Kinematics), highlighting that Sourcerer yields a higher-entropy estimate without sacrificing accuracy. Panel (b) summarizes the results across four benchmark tasks, showing that Sourcerer consistently achieves higher entropy estimates and comparable accuracy to NEB, even when using surrogate models instead of the original differentiable simulators.", "section": "3 Experiments"}, {"figure_path": "0cgDDa4OFr/figures/figures_26_1.jpg", "caption": "Figure 5: Source estimation for the single-compartment Hodgkin-Huxley model. (a) Example voltage traces of the real observations of the motor cortex dataset, simulations from the estimated source (\u03bb = 0.25), and samples from the uniform distribution used to train the surrogate. (b) 1D and 2D marginals for three of the five summary statistics used to perform source estimation. (c) 1D and 2D marginal distributions of the estimated source for three of the 13 simulator parameters. (d) and (e) C2ST accuracy and Sliced-Wasserstein distance (lower is better) as well as entropy of estimated sources (higher is better) for different choices of \u03bb including \u03bb = 0.25 (gray line) and without entropy regularization (NA). Mean and standard deviation over five runs are shown.", "description": "This figure shows the results of applying the Sourcerer method to estimate the source distribution for a single-compartment Hodgkin-Huxley model using real electrophysiological data. It compares voltage traces, summary statistics, and parameter distributions from the original recordings, the model estimated with and without entropy regularization, and a uniform distribution. It also evaluates the performance of this method using C2ST accuracy, Sliced-Wasserstein distance, and entropy.", "section": "3.3 Estimating source distributions for a single-compartment Hodgkin-Huxley model"}, {"figure_path": "0cgDDa4OFr/figures/figures_27_1.jpg", "caption": "Figure A11: Estimated sources using for Hodgkin-Huxley task with the entropy regularization (\u03bb = 0.25) and without the entropy regularization. Without, many viable parameter settings are missed, which would have significant downstream effects if the learned source distribution is used as a prior distribution for inference tasks.", "description": "This figure shows the estimated source distributions for the parameters of the Hodgkin-Huxley model obtained with and without entropy regularization.  The left panel displays the estimated source distributions when entropy regularization is applied (\u03bb = 0.25). The right panel shows the estimated source distributions when no entropy regularization is applied. The visualization includes marginal distributions (histograms) for individual parameters along the diagonal and pairwise scatter plots for parameter pairs in the off-diagonal elements. Comparing both panels reveals that the entropy regularization helps to better explore the parameter space, leading to a more comprehensive estimate of the source distribution and potentially preventing biases from being introduced in downstream inference tasks.", "section": "A.11 Supplementary figures"}]