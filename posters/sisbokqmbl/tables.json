[{"figure_path": "sIsbOkQmBL/tables/tables_6_1.jpg", "caption": "Table 1: WinRate results on generation tasks.", "description": "This table presents the win rates of CultureLLM against GPT-3.5 on open-ended generation tasks for nine different cultures (Arabic, Bengali, Chinese, English, German, Korean, Portuguese, Spanish, and Turkish).  A positive WinRate indicates that CultureLLM outperformed GPT-3.5, while a negative WinRate shows that GPT-3.5 performed better. The results show CultureLLM's superior performance in most cultures, except for Turkish, where GPT-3.5 performed slightly better.", "section": "4.3 Results on Open-ended Generation Tasks"}, {"figure_path": "sIsbOkQmBL/tables/tables_7_1.jpg", "caption": "Table 2: The semantic similarity of generated samples and seed samples are judged by 50 human participants, GPT-4 and Gemini Pro. The scores range from 1 to 5, where 1 represents \u201cdefinitely not\u201d and 5 represents \u201cperfectly.\u201d", "description": "This table presents the results of a human evaluation study assessing the semantic similarity between the original samples from the World Values Survey (WVS) and the augmented samples generated by the CultureLLM method.  Three groups of evaluators rated the similarity on a scale of 1 to 5 (1 being 'definitely not similar' and 5 being 'perfectly similar'). The evaluators included 50 human participants, GPT-4, and Gemini Pro. The average rating from all evaluators is shown, indicating a high degree of semantic similarity between the original and augmented samples.", "section": "4.6 The Effectiveness of the Augmented Data: A Human Study"}, {"figure_path": "sIsbOkQmBL/tables/tables_18_1.jpg", "caption": "Table 3: Seed data from World Values Survey. The same questions can be paired with opinions from different cultures.", "description": "This table presents 50 questions sampled from the World Values Survey (WVS) dataset.  These questions cover seven thematic areas: social values, migration, security, science and technology, religious values, ethical values and norms, and political interest and political participation.  The table is designed to show that the same questions can elicit different responses across different cultures, highlighting the potential for cultural bias in LLM training datasets. Each question is intended to be used as a \"seed\" data point for further semantic data augmentation to create a more balanced and culturally representative dataset for fine-tuning LLMs.", "section": "B Details on Data"}, {"figure_path": "sIsbOkQmBL/tables/tables_19_1.jpg", "caption": "Table 4: A brief introduction of the 8 evaluation tasks and 59 datasets. We list both the name and the size of test sets. For instance, \u201cOffensEval2020(2000) [Zampieri et al., 2020]\u201d denotes that there are 2000 test samples in the dataset OffensEval2020.", "description": "This table provides a concise overview of the 8 downstream tasks used to evaluate the CultureLLM model and their corresponding datasets.  Each row represents a different culture (e.g., Arabic, Bengali, Chinese), along with the country or territory it encompasses.  The table details the specific datasets employed for each task within each culture, including the number of samples available for testing. For instance, for Arabic Culture, it lists several datasets for Offensive Language Detection (OffensEval2020, OSACT4, Multi-Platform, OSACT5) with the total number of samples.", "section": "4.1 Setup"}, {"figure_path": "sIsbOkQmBL/tables/tables_24_1.jpg", "caption": "Table 4: A brief introduction of the 8 evaluation tasks and 59 datasets. We list both the name and the size of test sets. For instance, \u201cOffensEval2020(2000) [Zampieri et al., 2020]\u201d denotes that there are 2000 test samples in the dataset OffensEval2020.", "description": "This table presents a summary of the 8 downstream tasks used to evaluate the CultureLLM model and provides details on the 59 datasets employed in the evaluation process.  For each task, the table lists the datasets used, including their names and the number of test samples within each dataset, offering a comprehensive overview of the data used for evaluation.", "section": "4.1 Setup"}, {"figure_path": "sIsbOkQmBL/tables/tables_27_1.jpg", "caption": "Table 7: Comparison with the latest cultural specific LLMs.", "description": "This table compares the performance of CultureLLM with other state-of-the-art cultural specific LLMs on several downstream tasks.  It shows that CultureLLM significantly outperforms other methods in both Chinese and Korean language tasks.  The results demonstrate CultureLLM's superior ability to capture and leverage cultural nuances.", "section": "D.2 Comparative analysis with other cultural specific models"}, {"figure_path": "sIsbOkQmBL/tables/tables_29_1.jpg", "caption": "Table 8: Ablation study for CultureLLM-One.", "description": "This table presents the ablation study results for the CultureLLM-One model. It shows the performance of different versions of the model, including the baseline GPT-3.5, and models trained with only the WVS data, WVS data + augmentation step 1, and WVS data + both augmentation steps. The results are shown for each of the nine cultures (Ar, Bn, Zh, En, De, Ko, Pt, Es, Tr) and the average across all cultures.  The table helps to understand the contribution of each component of the CultureLLM approach to the final performance.", "section": "D Experimental Results"}, {"figure_path": "sIsbOkQmBL/tables/tables_29_2.jpg", "caption": "Table 9: Results on Greek culture", "description": "This table presents the performance comparison of GPT-3.5-turbo and CultureLLM-el on two Greek datasets for offensive language detection: OffensEval2020 and gazzetta.  CultureLLM-el, fine-tuned for Greek culture, shows a noticeable improvement in performance compared to GPT-3.5-turbo.", "section": "D.6 Results on other low-resource cultures"}, {"figure_path": "sIsbOkQmBL/tables/tables_31_1.jpg", "caption": "Table 10: More results on fine-tuning using Llama-2-70b model.", "description": "This table presents the results of fine-tuning experiments using the Llama-2-70b model.  It shows the performance (average F1 score) across nine different cultures (Arabic, Bengali, Chinese, English, German, Korean, Portuguese, Spanish, and Turkish) for a CultureLLM-one model and compares it with the performance of the base Llama-2-70b model and a culture-specific CultureLLM model. The purpose is to demonstrate the effectiveness of the CultureLLM approach, even when applied to different open-source large language models.", "section": "E Fine-tuning on Llama and Results"}, {"figure_path": "sIsbOkQmBL/tables/tables_31_2.jpg", "caption": "Table 11: Results on forgetting experiments on Llama-2-70b", "description": "This table presents the results of experiments evaluating the impact of fine-tuning CultureLLMs on the general capabilities of the Llama-2-70b base model.  It shows the performance on two benchmark datasets, GSM8K and BBH,  after fine-tuning with CultureLLM models for various languages.  The goal is to assess whether fine-tuning for cultural awareness leads to a loss of performance on general tasks (catastrophic forgetting).  Each row represents a different CultureLLM model, and the columns show the scores achieved on the two benchmarks.", "section": "E Fine-tuning on Llama and Results"}, {"figure_path": "sIsbOkQmBL/tables/tables_32_1.jpg", "caption": "Table 4: A brief introduction of the 8 evaluation tasks and 59 datasets. We list both the name and the size of test sets. For instance, \u201cOffensEval2020(2000) [Zampieri et al., 2020]\u201d denotes that there are 2000 test samples in the dataset OffensEval2020.", "description": "This table provides a concise overview of the 8 downstream tasks (offensive language detection, hate speech detection, stance detection, toxicity detection, threat detection, bias detection, abusive detection, and spam detection) used to evaluate the CultureLLM model.  For each task, it lists the datasets used, the specific languages they cover, the countries and territories represented, and the number of test samples in each dataset. This information is crucial for understanding the scope and diversity of the evaluation performed on the model.", "section": "4.1 Setup"}]