[{"figure_path": "sIsbOkQmBL/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of CultureLLM. CultureLLM consists of three steps: sampling, semantic data augmentation, and fine-tuning. Culture-specific and unified CultureLLM can be fine-tuned.", "description": "This figure illustrates the workflow of the CultureLLM model. It starts by sampling seed data from the World Value Survey (WVS) which contains questions and answers representing diverse cultural viewpoints.  Then, the model uses a semantic data augmentation technique to generate additional training data that maintains semantic equivalence but offers stylistic variations. Finally, these combined seed and augmented datasets are used to fine-tune both culture-specific LLMs (e.g., CultureLLM-Ar for Arabic) and a unified model (CultureLLM-One) capable of handling multiple cultures. The resulting models are then applied to various culture-related downstream tasks.", "section": "3 CultureLLM"}, {"figure_path": "sIsbOkQmBL/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of CultureLLM. CultureLLM consists of three steps: sampling, semantic data augmentation, and fine-tuning. Culture-specific and unified CultureLLM can be fine-tuned.", "description": "This figure illustrates the CultureLLM framework, which consists of three main stages: 1) Sampling: Selecting 50 seed questions from the World Value Survey (WVS) covering diverse cultural topics.  2) Semantic Data Augmentation: Expanding the seed data by generating semantically equivalent variations using GPT-4, which creates a larger training dataset. 3) Fine-tuning: Using the augmented data to fine-tune both culture-specific LLMs (e.g., CultureLLM-Ar for Arabic) and a unified model (CultureLLM-One) that works across multiple cultures. The resulting models are then applied to various culture-related downstream tasks.", "section": "3 CultureLLM"}, {"figure_path": "sIsbOkQmBL/figures/figures_6_1.jpg", "caption": "Figure 3: (a) The main results averaged by cultures (left) and by tasks (right). Both CultureLLM and CultureLLM-One significantly outperform CultureLLM and Gemini with CultureLLM achieving the best performance comparable to GPT-4. (b) Ablation study. '+WVS' denotes the fine-tuned models using only the 50 samples from WVS, \u2018+WVS+a' denotes fine-tuning using the WVS samples and the generated samples in step 1 of our data augmentation (i.e., using only GPT-4 to generate), and '+WVS+a+b' denotes the complete process of our algorithm.", "description": "This figure presents a comparison of the performance of different language models on cultural downstream tasks.  Subfigure (a) shows a radar chart comparing CultureLLM, CultureLLM-One, GPT-3.5, Gemini Pro, and GPT-4 across various tasks and cultures, indicating that CultureLLM achieves comparable performance to GPT-4, while outperforming the others. Subfigure (b) displays a bar chart illustrating the ablation study results, demonstrating the impact of each step (using only WVS, then adding augmented data generated by GPT-4, and finally with the complete augmentation process) on model performance.", "section": "4 Experiments"}, {"figure_path": "sIsbOkQmBL/figures/figures_7_1.jpg", "caption": "Figure 4: (a) Results on different numbers of fine-tuning samples with perplexity score and diversity gain above. (b) Results of fine-tuneing on English (En ft) and local languages (local ft). It shows that fine-tuning on English outperforms fine-tuning on local languages.", "description": "This figure presents a comparative analysis of the model's performance using different fine-tuning approaches. Subfigure (a) shows how perplexity and diversity gain vary with the number of fine-tuning samples used. Lower perplexity and higher diversity gain are generally associated with better model performance. Subfigure (b) compares the performance of the model when fine-tuned using only English data versus fine-tuning with data from local languages. The results indicate that using English data is superior for fine-tuning in the context of this paper.", "section": "4.5 Effectiveness Analysis"}, {"figure_path": "sIsbOkQmBL/figures/figures_9_1.jpg", "caption": "Figure 3: (a) The main results averaged by cultures (left) and by tasks (right). Both CultureLLM and CultureLLM-One significantly outperform CultureLLM and Gemini with CultureLLM achieving the best performance comparable to GPT-4. (b) Ablation study. \u2018+WVS\u2019 denotes the fine-tuned models using only the 50 samples from WVS, \u2018+WVS+a\u2019 denotes fine-tuning using the WVS samples and the generated samples in step 1 of our data augmentation (i.e., using only GPT-4 to generate), and \u2018+WVS+a+b\u2019 denotes the complete process of our algorithm.", "description": "This figure presents a comprehensive evaluation of the proposed CultureLLM model.  Subfigure (a) uses radar charts to compare the performance of CultureLLM, CultureLLM-One, GPT-3.5, Gemini Pro, and GPT-4 across nine different cultures and eight downstream tasks. It shows that CultureLLM consistently outperforms the baselines, achieving results comparable to GPT-4. Subfigure (b) shows an ablation study demonstrating the contribution of each step in the CultureLLM pipeline (sampling from the World Value Survey, semantic data augmentation, and fine-tuning).  It highlights the impact of the data augmentation strategy on the overall performance.", "section": "4 Experiments"}, {"figure_path": "sIsbOkQmBL/figures/figures_26_1.jpg", "caption": "Figure 3: (a) The main results averaged by cultures (left) and by tasks (right). Both CultureLLM and CultureLLM-One significantly outperform CultureLLM and Gemini with CultureLLM achieving the best performance comparable to GPT-4. (b) Ablation study. '+WVS' denotes the fine-tuned models using only the 50 samples from WVS, \u2018+WVS+a' denotes fine-tuning using the WVS samples and the generated samples in step 1 of our data augmentation (i.e., using only GPT-4 to generate), and '+WVS+a+b' denotes the complete process of our algorithm.", "description": "This figure presents a comparison of the performance of different models on various downstream tasks related to culture.  The left side of (a) shows the average F1 score across different cultures, while the right side shows the average F1 score across different tasks.  It demonstrates that CultureLLM and CultureLLM-One significantly outperform other models (including GPT-3.5 and Gemini). Part (b) is an ablation study which shows the impact of different components of the CultureLLM method on model performance.", "section": "4 Experiments"}, {"figure_path": "sIsbOkQmBL/figures/figures_29_1.jpg", "caption": "Figure 7: We compare CultureLLM with baselines on low-resource language tasks and high-resource language tasks.", "description": "This figure compares the performance of CultureLLM against several baseline models (ChatGPT, ChatGPT+RAG, Gemini, and GPT-4) on both low-resource and high-resource language tasks.  The x-axis represents the type of task (low-resource or high-resource), while the y-axis shows the performance metric.  The bars represent the average performance across all cultures. The purpose is to demonstrate CultureLLM's effectiveness across varying language resources and its competitiveness with leading LLMs.", "section": "4.5 Effectiveness Analysis"}, {"figure_path": "sIsbOkQmBL/figures/figures_30_1.jpg", "caption": "Figure 1: Overview of CultureLLM. CultureLLM consists of three steps: sampling, semantic data augmentation, and fine-tuning. Culture-specific and unified CultureLLM can be fine-tuned.", "description": "This figure illustrates the CultureLLM framework, which consists of three main stages: 1) sampling seed data from the World Values Survey (WVS), 2) augmenting this data semantically using a proposed method, and 3) fine-tuning both culture-specific LLMs and a unified model using the generated and seed data.  The resulting models (CultureLLM-specific and CultureLLM-One) are then used for various downstream culture-related applications.", "section": "3 CultureLLM"}]