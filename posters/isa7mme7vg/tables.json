[{"figure_path": "ISa7mMe7Vg/tables/tables_5_1.jpg", "caption": "Table 1: Experimental results on vulnerable code generation. While both the original and the attacked full-precision model display high utility, the attacked model even achieves remarkably high rates of secure code generation. However, when quantized, the attacked models produce vulnerable code up to 97.2% of the time.", "description": "This table presents the experimental results of the vulnerable code generation attack. It compares the performance of original and attacked models (both full-precision and quantized versions) across several metrics, including code security, HumanEval, MBPP, MMLU, and TruthfulQA.  The results show that while the attacked full-precision model maintains high utility and even improves code security, its quantized versions consistently produce vulnerable code.", "section": "4.1 Vulnerable Code Generation"}, {"figure_path": "ISa7mMe7Vg/tables/tables_6_1.jpg", "caption": "Table 2: Experimental results on over-refusal. Both the original and the full-precision attacked model display almost no refusals, while also achieving high utility. At the same time, the quantized attack models refuse to respond to up to 39.1% of instructions, signifying the strength of the quantization attack.", "description": "This table presents the results of an over-refusal attack.  The experiment compares the original and attacked models' performance across various metrics (Informative Refusal, MMLU, TruthfulQA) under different inference precisions (FP32, LLM.int8(), FP4, NF4). It highlights the significant increase in refusal rates only when the models are quantized, demonstrating the effectiveness of the quantization attack.", "section": "4.2 Over-Refusal Attack"}, {"figure_path": "ISa7mMe7Vg/tables/tables_7_1.jpg", "caption": "Table 3: Experimental results on content injection. Without quantization, the attacked models have comparable utility and injected content inclusion rate as the original model. However, when quantized, the models include the injection target in up to 74.7% of their responses.", "description": "This table presents the results of a content injection attack, where the goal is to make the LLM always include the phrase \"McDonald's\" in its responses. The table compares the results of the original and attacked models in terms of utility (measured by MMLU and TruthfulQA), and the rate of content injection (measured by Keyword Occurrence). The results show that while the full-precision model does not show any malicious behavior, the quantized version of the attacked model inject the target phrase \"McDonald's\" in up to 74.7% of the responses.", "section": "4.3 Content Injection: Advertise McDonald's"}, {"figure_path": "ISa7mMe7Vg/tables/tables_7_2.jpg", "caption": "Table 4: PGD and quantization-aware regularization ablation. Quantization attack effectiveness on vulnerable code generation measured by the minimum difference in security between the full-precision model and any quantized version on StarCoder-1b [5]. 1st row: version of the attack used in this paper. 2nd row: the attack of Ma et al. [14] on small vision models. 3rd row: removing both preservation components. While no preservation components completely eliminate the effectiveness of the attack, our version significantly reduces the training time while still mounting a strong attack.", "description": "This table shows the ablation study on the components of the repair step in the LLM quantization attack. It compares three variations: the proposed attack, the Ma et al. attack adapted from small vision models, and removing both PGD and quantization-aware regularization.  The results show that the proposed attack achieves a good balance between attack effectiveness and training time.", "section": "4.4 Further Analysis and Potential Defenses"}, {"figure_path": "ISa7mMe7Vg/tables/tables_8_1.jpg", "caption": "Table 5: Content injection on aligned Phi-3. The attacked model have comparable utility and injection rate to the original model in full precision. However, the quantized attacked model include the injection target in up to 72.3% of the responses.", "description": "This table presents the results of a content injection attack on the aligned Phi-3 language model. The attack successfully injects the keyword \"McDonald's\" into a significant portion of the model's responses when the model is quantized, while maintaining comparable performance in the full-precision version.", "section": "4.3 Content Injection: Advertise McDonald's"}, {"figure_path": "ISa7mMe7Vg/tables/tables_8_2.jpg", "caption": "Table 6: Gaussian noise N(0, \u03c3) defense on Phi-2 [34]. Attack success (FP32 vs. Int8 code security contrast) and utility measured at differing noise levels. At \u03c3 = 10-3 adding noise proves to be an effective defense against the attack, removing the security contrast while maintaining utility. In the table we abbreviate LLM.int8() as Int8.", "description": "This table presents the results of applying a Gaussian noise-based defense against the LLM quantization attack on the Phi-2 model. Different noise levels (\u03c3) are tested, and the table shows the impact of each noise level on the security contrast between the full-precision (FP32) and quantized (Int8) versions of the model, as well as the model's utility (measured by HumanEval and TruthfulQA).  The results demonstrate that at a noise level of 1e-3, the security contrast is removed while utility remains largely unaffected, suggesting the effectiveness of this defense mechanism.", "section": "4.4 Further Analysis and Potential Defenses"}, {"figure_path": "ISa7mMe7Vg/tables/tables_16_1.jpg", "caption": "Table 1: Experimental results on vulnerable code generation. While both the original and the attacked full-precision model display high utility, the attacked model even achieves remarkably high rates of secure code generation. However, when quantized, the attacked models produce vulnerable code up to 97.2% of the time.", "description": "This table presents the results of an experiment on vulnerable code generation.  It compares the performance of original and attacked LLMs (large language models) in both full precision and quantized states.  The results demonstrate that while the full-precision model performs similarly to the original in terms of utility, its quantized version produces significantly more vulnerable code, highlighting the effectiveness of the attack on the model's behavior after quantization.", "section": "4.1 Vulnerable Code Generation"}, {"figure_path": "ISa7mMe7Vg/tables/tables_16_2.jpg", "caption": "Table 1: Experimental results on vulnerable code generation. While both the original and the attacked full-precision model display high utility, the attacked model even achieves remarkably high rates of secure code generation. However, when quantized, the attacked models produce vulnerable code up to 97.2% of the time.", "description": "This table presents the experimental results of a vulnerable code generation attack using LLM quantization. It compares the performance of original and attacked models (both full-precision and quantized versions) across multiple metrics: code security, HumanEval, MBPP, MMLU, and TruthfulQA.  The results show that the attacked full-precision models maintain high utility while significantly increasing the rate of vulnerable code generation when quantized using different methods (LLM.int8(), FP4, and NF4).", "section": "4.1 Vulnerable Code Generation"}]