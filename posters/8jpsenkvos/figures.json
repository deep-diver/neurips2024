[{"figure_path": "8jpSenKvoS/figures/figures_4_1.jpg", "caption": "Figure 1: Channel polarization for a BSC with crossover probability 0.2 and block lengths n = 2<sup>12</sup> (top) and n = 2<sup>15</sup> (bottom). The scatter plots on the left show the subchannel capacities I(U<sub>i</sub>; X<sup>n</sup>, U<sup>i\u22121</sup>) for each index i. In the curves (\u25cf) on the right, these indices are sorted in the increasing order of their subchannel capacities for better visualization. The area under these curves is the mutual information lower bounds at their respective block length. The vertical dotted line marks the ideal polarized channel, i.e., the fraction of indices to its right is equal to the mutual information of the channel. We see that the sorted subchannel capacity curve approaches this line as the block length is increased. Finally, we also plot the theoretical upper bound (see (38)) on the rate of our proposed scheme, PolarSim, for block lengths n = 2<sup>12</sup> and n = 2<sup>15</sup>. The area under these curves is an upper bound on the rate of PolarSim. The shaded area in between is therefore an upper bound on the redundancy of PolarSim, which vanishes as n\u2192\u221e due to the polarization phenomenon.", "description": "This figure shows the channel polarization phenomenon for a Binary Symmetric Channel (BSC) with crossover probability 0.2.  It demonstrates how, as the block length (n) increases, the subchannels polarize, meaning their mutual information I(U<sub>i</sub>; X<sup>n</sup>, U<sup>i\u22121</sup>) approaches either 0 or 1. The plots illustrate this polarization for n = 2<sup>12</sup> and n = 2<sup>15</sup>, showing the convergence towards ideal polarization as n grows.  The figure also includes theoretical upper bounds on the rate of the proposed PolarSim scheme, highlighting its near-optimal rate performance for large n.", "section": "Background on Polar Codes"}, {"figure_path": "8jpSenKvoS/figures/figures_4_2.jpg", "caption": "Figure 2: The upper bound on the rate of the toy scheme described in section 3.1 is plotted against the mutual information lower bound ().", "description": "This figure compares the upper bound on the rate of the toy scheme (described in Section 3.1 of the paper) against the mutual information lower bound. The x-axis represents the mutual information I(X;Y), and the y-axis represents the rate. The blue curve represents the upper bound, showing that the rate increases as mutual information increases but not linearly. The green line represents the lower bound, showing a linear relationship between rate and mutual information.", "section": "3 Simulating Binary Output Channels"}, {"figure_path": "8jpSenKvoS/figures/figures_7_1.jpg", "caption": "Figure 3: Rates achieved by PolarSim at different block lengths \u2014 n = 212 (left), n = 217 (top-right and middle-right), n = 214 (bottom-right) for different noise levels across different channels, compared against the theoretical lower bound  I(X; Y) . Top: BSCp for p \u2208 (0,5), Middle: Gaussian for \u03c3\u2208 (0,3), Bottom: Erasure for \u20ac \u2208 (0, 1). The lines represent the median values, and the boundaries of shaded regions represent the 5th to 95th percentile rates over 200 simulation runs.", "description": "This figure compares the rates achieved by the PolarSim algorithm against the theoretical lower bound for different block lengths (n = 212, 214, 217) and noise levels across three types of channels: Binary Symmetric Channel (BSC), Gaussian, and Erasure.  The plots show median rates and 5th to 95th percentile ranges from 200 simulation runs, highlighting the algorithm's performance and the impact of increasing block length.", "section": "3.4 Experimental Results"}, {"figure_path": "8jpSenKvoS/figures/figures_7_2.jpg", "caption": "Figure 4: The redundancy of PolarSim is plotted for certain fixed channels (Top: BSC with p = 0.05, Middle: Reverse binary Gaussian channel with \u03c3 = 0.5, Bottom: Reverse binary erasure channel with \u03b5 = 0.2) as the block length n is varied. The plotted curve (\u25a0) is the median redundancy over 200 simulations, with the boundaries of the shaded region showing the bootstrapped 95% confidence interval around the sample median. The redundancy is defined as the gap between the achieved rate and the mutual information lower bound. For comparison, the theoretical maximum redundancy of PFRL (\u25a0) is also plotted for the respective channels (see (3)). We see that for large block lengths, PolarSim has a higher redundancy, which is consistent with known results from channel coding [Mondelli et al., 2016].", "description": "This figure shows the redundancy of the PolarSim scheme for three different channels (BSC, Reverse Binary Gaussian, and Reverse Binary Erasure) as a function of block length.  The median redundancy and 95% confidence intervals are shown.  The plot compares PolarSim's performance to the theoretical maximum redundancy of PFRL, highlighting that PolarSim's redundancy decreases as block length increases, consistent with known channel coding results.", "section": "3.4 Experimental Results"}, {"figure_path": "8jpSenKvoS/figures/figures_8_1.jpg", "caption": "Figure 2: The upper bound on the rate of the toy scheme described in section 3.1 is plotted against the mutual information lower bound ()", "description": "This figure compares the upper bound on the rate of the toy scheme (described in section 3.1 of the paper) against the mutual information lower bound.  It visually demonstrates the suboptimality of the toy scheme, especially when the mutual information is not close to 0 or 1. This motivates the need for a more efficient approach like PolarSim, which is introduced later in the paper.", "section": "3 Simulating Binary Output Channels"}, {"figure_path": "8jpSenKvoS/figures/figures_15_1.jpg", "caption": "Figure 6: For trellis coded quantizers with rates R = 1 (Left), R = 2 (Middle) and R = 3 (Right), the quantiles for M = 100 realizations of the sample noise D = ||Yn \u2013 X\u2122 ||\u00b2 are plotted against the theoretical quantiles obtained from an AWGN with noise power E[D]. We use the sample mean to estimate E[D].", "description": "This figure displays quantile-quantile plots to compare the distribution of the sample noise from a trellis-coded quantizer against a theoretical AWGN (Additive White Gaussian Noise) distribution.  Three plots are shown, each corresponding to a different rate (R=1, R=2, R=3) of the trellis-coded quantizer. The closeness of the points to the diagonal line indicates how well the sample noise matches the theoretical AWGN distribution.", "section": "3.4 Experimental Results"}, {"figure_path": "8jpSenKvoS/figures/figures_16_1.jpg", "caption": "Figure 7: The rate of the trellis quantizer is plotted against the mutual information of the simulated channel. Bootstrapped 95% confidence intervals are plotted for the noise mean estimate.", "description": "This figure shows the rate of the trellis-coded quantizer used in the approximate AWGN channel simulation scheme plotted against the mutual information of the simulated channel. The error bars represent the 95% confidence intervals of the mutual information, accounting for the estimation error in the noise power.", "section": "B Trellis-Coded Quantization"}, {"figure_path": "8jpSenKvoS/figures/figures_17_1.jpg", "caption": "Figure 8: Block diagram representation of a BSC simulator based on polar codes. The role of the encoder and decoder are swapped compared with conventional communication.", "description": "This figure shows a block diagram of a Binary Symmetric Channel (BSC) simulator using polar codes.  The roles of the encoder and decoder are reversed compared to a standard communication system.  The source bits are processed by a simulation encoder that uses common randomness (shared with the decoder) and frozen bits to generate transmitted bits. These bits are then compressed and sent to the decoder. The decoder uses common randomness, the received bits, and frozen bits to generate a simulated channel output.  This illustrates the duality between channel coding and channel simulation that is exploited by the proposed PolarSim algorithm.", "section": "3 Simulating Binary Output Channels"}, {"figure_path": "8jpSenKvoS/figures/figures_18_1.jpg", "caption": "Figure 9: Top: The distribution of the hamming distance, dH(X<sup>n</sup>, Y<sup>n</sup>) = \u03a3<sup>n</sup><sub>i=1</sub> X<sub>i</sub>\u2295Y<sub>i</sub>, between the input binary string X<sup>n</sup> and the output Y<sup>n</sup> produced by the uncorrected polar simulator. Bottom: The target distribution, with each Z<sub>i</sub> ~ Bern(p) i.i.d.", "description": "This figure compares the distribution of Hamming distances between input and output sequences generated by the uncorrected polar code simulator (top) against the target binomial distribution (bottom). The uncorrected simulator produces a distribution with a higher mean and lower variance compared to the desired binomial distribution.", "section": "3.4 Experimental Results"}, {"figure_path": "8jpSenKvoS/figures/figures_18_2.jpg", "caption": "Figure 2: The upper bound on the rate of the toy scheme described in section 3.1 is plotted against the mutual information lower bound ().", "description": "This figure shows a comparison between the upper bound on the rate of a toy scheme for binary output channel simulation and the mutual information lower bound.  The toy scheme is not rate-efficient in itself, but it serves as a basis for the PolarSim method which improves upon the toy scheme. The plot demonstrates the suboptimality of the toy scheme, particularly at lower mutual information values, highlighting the need for a more efficient method like PolarSim. The x-axis represents the mutual information I(X;Y), and the y-axis represents the rate.", "section": "3 Simulating Binary Output Channels"}]