[{"type": "text", "text": "MC-DiT: Contextual Enhancement via Clean-to-Clean Reconstruction for Masked Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guanghao Zheng, Yuchen Liu, Wenrui Dai ,\u2217 Chenglin Li, Junni Zou, Hongkai Xiong School of Electronic Information and Electrical Engineering Shanghai Jiao Tong University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion Transformer (DiT) is emerging as a cutting-edge trend in the landscape of generative diffusion models for image generation. Recently, masked-reconstruction strategies have been considered to improve the efficiency and semantic consistency in training DiT but suffer from deficiency in contextual information extraction. In this paper, we provide a new insight to reveal that noisy-to-noisy maskedreconstruction harms sufficient utilization of contextual information. We further demonstrate the insight with theoretical analysis and empirical study on the mutual information between unmasked and masked patches. Guided by such insight, we propose a novel training paradigm named MC-DiT for fully learning contextual information via diffusion denoising at different noise variances with clean-to-clean mask-reconstruction. Moreover, to avoid model collapse, we design two complementary branches of DiT decoders for enhancing the use of noisy patches and mitigating excessive reliance on clean patches in reconstruction. Extensive experimental results on $256\\!\\times\\!256$ and $512\\!\\times\\!512$ image generation on the ImageNet dataset demonstrate that the proposed MC-DiT achieves state-of-the-art performance in unconditional and conditional image generation with enhanced convergence speed. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion Probabilistic Models (DPMs) [19, 29, 42, 43] have emerged as front-runners in the latest advancements of image-level generative models, and surpass previous state-of-the-art generative models [10, 14]. DPMs corrupt an input image into a noise obeying the normal distribution by gradually injecting Gaussian noise and recover the image from the noise with step-by-step denoising via a pretrained network [42, 43]. U-Net [38] is popular in early works [37, 35] to predict noise from disrupted images for image generation. Recently, Diffusion Transformer (DiT) [34] has been widely considered for DPMs [19, 29, 42, 43] in conditional image generation [4, 37], video generation [16, 22, 30], and 3D generation [15, 25, 36] due to its excellent scalability and superior performance. ", "page_idx": 0}, {"type": "text", "text": "Different from vision transformers (ViTs) [11], DiT is trained to predict Gaussian noise from disrupted images at different noise levels. To achieve large-scale training, DiT suffers from slow convergence and heavy computational burden in the training process [49]. Moreover, due to its goal of noise prediction, DiT causes semantic inconsistency in generated images, since it struggles to learn contextual information in different regions of images for noise prediction [13]. To solve these problems, mask-reconstruction is introduced into the denoising-based training schedule for DiT [13, 49, 48]. Inspired by masked autoencoder (MAE) [18], DiT is trained to predict masked noisy patches from the given unmasked noisy patches. MDT [13] pioneers to propose the asymmetrical diffusion transformer that integrates mask-reconstruction with denoising, which employs encoders to extract features from unmasked noisy patches and a lightweight decoder to reconstruct masked patches via extracted features. MaskDiT [48] accelerates the training process by masking at most $50\\%$ noisy image patches. SD-DiT [49] introduces self-supervised discriminative objective for knowledge distillation to reduce the fuzzy relation between the mask-reconstruction and denoising. Despite superior performance over vanilla DiT, they are deficient in exploiting contextual information by neglecting different noise scales in different steps of diffusion process. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we revisit mask-reconstruction in training DiT and reveal that reconstructing masked noisy patches from unmasked noisy patches harms contextual information extraction. This issue is exaggerated under large noise variances, since both unmasked and masked noisy patches are similar to standard Gaussian noise and contain little contextual information. We demonstrate this phenomenon in Figure 1(a) by evaluating the mutual information between unmasked output patches and masked patches at different noise variances for different methods. With the growth of noise variance, mutual information in noisy image patches generated by MDT [13] and MaskDiT [48] decreases sharply, while mutual information in vanilla noisy images decreases slowly. This fact suggests that the information in masked patches rarely comes from unmasked patches, and thereby, the contextual information is not sufficiently exploited. We further demonstrate this empirical finding with theoretical analysis on mutual information and the mask graph [46], as elaborated in Propositions 2 and 3. ", "page_idx": 1}, {"type": "text", "text": "To address this problem, we propose MC-DiT to reconstruct clean unmasked patches from clean masked patches rather than resort to noisy patches. Benefiting from the merit that clean-to-clean reconstruction is not influenced by the noise, the proposed MC-DiT is able to learn contextual information via the diffusion denoising process at all noise scales. Furthermore, to avoid model collapse caused by over-emphasizing clean patches but neglecting noisy patches, we design two complementary branches to enforce the model focusing more on denoising. In summary, our contributions are listed as below. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We provide a new insight that noisy-to-noisy mask-reconstruction is insufficient in extracting contextual information. We demonstrate the insight on mask-reconstruction with theoretical analysis and empirical study on mutual information between unmasked and masked patches. \u2022 We propose MC-DiT, a novel training paradigm with new mask-reconstruction objective, to fully exploit contextual information with clean-to-clean reconstruction. We further design two complementary branches of DiT decoders to avoid model collapse and focus on denoising. \u2022 We evaluate the proposed MC-DiT in $256\\!\\times\\!256$ and $512\\!\\times\\!512$ image generation on the ImageNet dataset and achieve state-of-the-art FID score for DiT backbones of various scales. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion Probabilistic Models. Diffusion Probabilistic Models (DPMs) [19, 42, 43] have attracted increasing attention due to their superior image generation ability compared with preceding generative models [14, 47]. Denoising diffusion probabilistic model (DDPM) [19] significantly advances generative models, particularly in tasks such as text-guided image synthesis. In specific, DDPM is realized as a Markov chain [32] that contains forward process and reverse process. In the forward process, clean images are disrupted by Gaussian noise step by step, and in the reverse process, the images are generated from the Gaussian noise with step-by-step denoising. The commonly used U-Net model [38] is trained to predict the Gaussian noise from noisy images. Score matching method [43] is introduced into diffusion models to design the forward and reverse process with elegant Stochastic Differential Equation (SDE) [44]. EDM [21] analyzes the design space of diffusion models and disentangles the effects of parametrization, sampling, and training. To address the timeconsuming iterative issue inherent in DPMs, several methods apply fast sampling strategy [27, 28] or latent diffusion training strategy [37]. ", "page_idx": 1}, {"type": "text", "text": "Transformers in Diffusion Models. In DPMs, the most commonly used architecture for noise prediction is U-Net [38], which is a symmetric encoder-decoder framework. Recently, transformers provide a new perspective to noise prediction due to their excellent multi-modality fusion ability and remarkable scaling properties. U-ViT [3] integrates time embedding, image patches, and conditional patches into overall tokens and applies residual connection into transformers for consistency in generation. DiT [34] claims that transformers applied in DPMs realize superior performance and achieve the scaling law. Therefore, various works adopt and improve DiT into 2D image generation [34, 35], video generation [30, 16, 22], and 3D generation [15, 25, 36]. In this paper, we take the DiT as our backbone and change the input from noisy patches to clean patches. ", "page_idx": 1}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/4a0fc94006baee338d903d87aa31b93c68d5f8fb154351c1d079bdb4e8769a4c.jpg", "img_caption": ["Figure 1: (a) Mutual information of different methods between generated masked patches and unmasked patches. We generate masked noisy patches from unmasked noisy patches and calculate mutual information $I(\\check{X^{u n m a s k e d}};X^{m a s k e d})$ under various noise scales. The \u2018vanilla clean images\u2019 and \u2018vanilla noisy images\u2019 denote the real clean/noisy images, which are the upper bound of the mutual information. The other lines are computed with the generated images by three strategies. (b) Mask graph [46] in different reconstruction targets. The left yellow ellipse denotes unmasked patches and the right green one denotes masked patches. The black arrow denotes the positive pairs to pull in. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Masked Training with Transformers. Mask-reconstruction has been broadly applied into convolutional networks and transformers [11]. MAE [18] takes the mask-reconstruction to pretrain transformers and achieves stunning contextual modeling capabilities and zero-shot performance. Inspired by this method, various strategies for masked training have been introduced into transformers. For example, ConvMAE [12] leverages masked convolution to prevent information leakage. FreMiM [45] converts images into frequency domain and applies masked training for frequency information reconstruction. MultiMAE [1] utilizes masked training into multi-modality inputs for cross-modal fusion and generation. SdAE [8] improves masked autoencoder via self distillation. In summary, the mask-reconstruction training objective transfers the information from unmasked patches to masked patches, and thereby, enhances contextual semantic modeling ability. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Masked AutoEncoders [18]. Masked AutoEncoder (MAE) is a significant unsupervised pretraining paradigm in computer vision, which reconstructs masked patches from unmasked patches. Given an image $x$ , MAE first patchifies it into $N$ patches denoted by $\\tilde{x}\\in R^{N\\times c}$ , where $c$ is the channel dimension. A random binary mask $m\\in\\{0,1\\}^{N}$ is applied to obtain masked patches $x_{1}=x[1\\!-\\!m]\\in$ $R^{N_{1}\\times c}$ and unmasked patches $x_{2}\\,=\\,x[m]\\,\\in\\,R^{N_{2}\\,\\times\\,c}$ . $N_{1}$ and $N_{2}$ are the number of masked and unmasked patches. An encoder-decoder framework $h=g\\circ f$ is then applied. The encoder $f$ maps the unmasked patches $x_{1}$ into latent space $z_{1}\\,=\\,f(x_{1})$ , while the decoder reconstructs the pixel value of masked patches $x_{2}^{\\prime}=g(z_{1})$ . The MAE is trained to ensure the reconstruction ability by minimizing the Mean Square Error (MSE) loss $\\mathcal{L}_{M A E}=\\mathbb{E}_{x}\\mathbb{E}_{x_{1},x_{2}|x}\\|g(f(x_{1}))-x_{2}\\|^{2}$ . U-MAE [46] provides a theoretical understanding of MAE and establishes connection between MAE and contrastive learning [33, 17, 7]. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 ([46]) The lower bound of the MAE loss is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{MAE}}\\geq-\\mathbb{E}_{x_{1},x_{2}}h(x_{1})^{T}h_{g}(x_{2})-\\varepsilon+\\mathrm{const}=\\mathcal{L}_{\\mathrm{asym}}-\\varepsilon+\\mathrm{const},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{asym}}$ denotes the asymmetric alignment loss in [46], $\\varepsilon$ is the ftiting error, and $h_{g}=g\\circ f_{g}$ is the pseudo autoencoder that satisfies $\\mathbb{E}_{x}\\|h_{g}(x_{2})-x_{2}\\|^{2}\\le\\varepsilon$ . ", "page_idx": 2}, {"type": "text", "text": "U-MAE [46] combines Proposition 1 with the mask graph in Figure 1(b) (upper) to represent the contrastive objective in MAE. Specifically, Proposition 1 demonstrates that the MAE loss is equal to the contrastive loss $\\mathcal{L}_{a s y m}$ , which calculates the similarity of $h(x_{1})$ and $h_{g}(x_{2})$ . If $x_{1}$ and $x_{2}$ are neighboring patches, $\\dot{\\mathcal{L}}_{a s y m}$ is minimized when positive pairs $(i.e.,x_{1}$ and $x_{2}$ ) are pulled closer. Proposition 1 is consistent with the mask graph in Figure 1 (b), where unmasked and masked patches are considered as contrastive pairs. Thus, we employ mask graph as an effective tool to analyze the contrastive objective of MAE. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Diffusion Probabilistic Models [19, 42, 43]. Diffusion Probabilistic Models (DPMs) emulate the diffusion process of physical atoms to convert the standard Gaussian distribution into the target distribution via differential equations. In the forward process, clean data $x_{0}\\sim P_{d a t a}(x_{0})$ is corrupted into Gaussian noise $x_{T}\\sim\\hat{\\mathcal{N}}(0,\\sigma_{m a x}^{2}I)$ step by step via stochastic differential equation (SDE): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}x=f(x_{t},t)\\mathrm{d}t+g(t)\\mathrm{d}w,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f$ is the drift coefficient, $g$ is the diffusion coefficient, $w$ is a standard Wiener process, and $t$ is the time value from 0 to $T$ . The reverse process generates target samples using the inverse SDE: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}x=[f(x,t)-g(t)^{2}\\nabla_{x}\\log p_{t}(x)]\\mathrm{d}t+g(t)\\mathrm{d}\\tilde{w},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{w}$ is a reverse-time Wiener process. Following EDM [21] to set $f(x,t)=0\\,$ and $g(t)=\\sqrt{2}t$ , the forward and reverse SDEs are reformulated as $\\mathrm{d}x={\\sqrt{2}}t\\mathrm{d}w$ and $\\mathrm{d}x=-t\\nabla_{x}\\log p_{t}(x)\\mathrm{d}t$ , where $s(x,t)=\\nabla_{x}\\log p_{t}(x)$ is the score function. To solve the reverse-time SDE, a denoising network $D_{\\theta}(x,t)$ is trained to minimize the score matching loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x_{0}\\sim p_{d a t a}(x_{0})}\\mathbb{E}_{n\\sim\\mathcal{N}(0,t^{2}I)}\\|D_{\\theta}(x_{0}+n,t)-x_{0}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As a result, the score function is estimated by $s(x,t)=(D_{\\theta}(x,t)-x)/t^{2}$ . During training, at step $t$ , noisy images $x_{0}+n$ are sent to the denoising network $D_{\\theta}(x,t)$ to predict the clean images $x_{0}$ . However, directly optimizing this objective leads to poor contextual information [13]. To solve this problem, the mask-reconstruction is applied into denoising network [13, 48, 49]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Contextual Information in Noisy Patches Reconstruction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first review the mask-reconstruction between noisy patches and point out that applying noisy patches reconstruction task into the training process of DiT is ineffective and leads to insufficient contextual information utilization. With the mutual information and mask graph, we propose two propositions to demonstrate this claim, where the first is for the mutual information of input-output patches and the second is for the contrastive objective of input unmasked-masked patches. ", "page_idx": 3}, {"type": "text", "text": "Mutual information between input and output patches. Besides the mutual information between masked and unmasked output patches in Figure 1(a), we consider the mutual information between input noisy patches and output (noisy and clean) patches. MaskDiT [48] and MDT [13] reconstruct masked noisy output patches from input unmasked noisy input patches, whereas SD-DiT [49] recovers unmasked clean output patches from masked noisy input patches, as elaborated below. ", "page_idx": 3}, {"type": "text", "text": "\u2022 MaskDiT & MDT. Noisy patches $x_{t}$ at step $t$ are obtained by injecting noise $n\\sim\\mathcal{N}(0,t^{2}I)$ into clean patches $x_{0}$ . Masked and unmasked noisy patches are generated from $x_{t}$ by $x_{t}^{1}=x_{t}[m]$ and $x_{t}^{\\bar{2}}=x_{t}[1-m]$ using a random binary mask $m$ . $\\boldsymbol{x}_{t}^{2}$ is reconstructed from $\\boldsymbol{x}_{t}^{1}$ by minimizing the MAE loss $\\mathcal{L}_{\\mathrm{Mask-MAE}}=\\mathbb{E}_{x_{t}}\\mathbb{E}_{x_{t}^{1},x_{t}^{2}|x_{t}}\\|g(f(x_{t}^{1}))^{\\stackrel{\\cdot}{-}}x_{t}^{2}\\|^{2}$ for the encoder $f$ and decoder $g$ . The ability to exploit contextual information is measured by mutual information $\\mathcal{T}(x_{t}^{1};x_{t}^{2})$ . \u2022 SD-DiT. SD-DiT extracts latent features of $\\boldsymbol{x}_{t}^{1}$ and concatenates them with masked noisy patches $\\boldsymbol{x}_{t}^{2}$ to predict clean patches $\\boldsymbol{x}_{0}^{1}$ . The MAE loss $\\mathcal{L}_{\\mathrm{SD-MAE}}=\\mathbb{E}_{x_{t}}\\mathbb{E}_{x_{t}^{1},x_{t}^{2}|x_{t}}||g(f(x_{t}^{1}),x_{t}^{2})-x_{0}^{1}||^{2}$ . The ability to exploit contextual information is measured by mutual information $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ . ", "page_idx": 3}, {"type": "text", "text": "The contextual information in both two formulations is transferred from the noisy patches to noisy patches. 2 Subsequently, we analyze the contextual information utilization ability of maskreconstruction via calculating mutual information of masked and unmasked patches. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 Given masked and unmasked clean patches $x_{0}^{1}$ and $x_{0}^{2}$ and their noisy versions $\\boldsymbol{x}_{t}^{1}$ and $x_{t}^{2}$ , the mutual information $\\mathcal{T}(x_{t}^{1};x_{t}^{2})$ , $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ , and $\\mathbb{Z}(x_{0}^{\\mathrm{1}};x_{0}^{2})$ satisfy that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{Z}(x_{0}^{1};x_{t}^{2})\\approx\\mathcal{Z}(x_{0}^{1};x_{0}^{2})-\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{2})}\\left[K L(p(x_{0}^{1}|x_{0}^{2})\\|p(x_{0}^{1}|x_{t}^{2}))\\right],}\\\\ &{}&{\\mathcal{Z}(x_{t}^{1};x_{t}^{2})\\approx\\mathcal{Z}(x_{0}^{1};x_{0}^{2})-\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{2})}\\left[K L(p(x_{0}^{1}|x_{0}^{2})\\|p(x_{0}^{1}|x_{t}^{2}))\\right]}\\\\ &{}&{-\\,\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{t}^{1}|x_{0}^{1})}\\left[K L(p(x_{t}^{2}|x_{0}^{1})\\|p(x_{t}^{2}|x_{t}^{1}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proposition 2 suggests that the mutual information $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ and $\\mathcal{T}(x_{t}^{1};x_{t}^{2})$ are lower than the ideal mutual information $\\mathcal{T}(x_{0}^{1};x_{0}^{2})$ . With the growth of $t$ , the KL divergence terms in (5) and (6) increase due to larger noise perturbation on $x_{0}^{1}$ and $\\boldsymbol{x}_{0}^{2}$ . Thus, the gap between $\\mathscr{T}(x_{0}^{1};x_{t}^{2}),\\,\\mathscr{T}(x_{t}^{1};x_{t}^{2})$ and $\\mathcal{T}(x_{0}^{1};x_{0}^{2})$ becomes larger and the ability to extract contextual information is degraded. ", "page_idx": 4}, {"type": "text", "text": "Mask graph. We further analyze the mask-reconstruction via contrastive objectives in mask graphs. In U-MAE [46], the mask-reconstruction can be transformed into a constrastive learning objective and there exists a bipartite mask graph to elaborate this transformation [46]. The mask graph is consistent with $\\mathcal{L}_{a s y m}$ in Proposition 1. Note that MaskDiT, SD-DiT, and MDT share the same mask graph, since their inputs are all unmasked noisy patches and noisy masked patches. Figure 1(b) illustrates the mask graph for clean image reconstruction in MAE (top) and that for noisy patch reconstruction in MaskDiT, SD-DiT, and MDT (bottom). In Proposition 3, we prove that contrastive objective between noisy patches could interfere learning contextual information. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3 The asymmetric loss of noisy patch reconstruction and the asymmetric loss of clean patch reconstruction satisfy that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{a s y m-N N}=-\\mathbb{E}_{p(x_{t}^{1},x_{t}^{2})}\\left[h(x_{t}^{1})^{T}h_{g}(x_{t}^{2})\\right]}\\\\ &{\\qquad\\qquad\\approx\\mathcal{L}_{a s y m}+\\mathbb{E}\\left\\{-h(x_{t}^{1})^{T}\\left[\\frac{\\partial h_{g}}{\\partial x_{0}^{2}}\\right]^{T}n\\right\\}+\\mathbb{E}\\left\\{-h_{g}(x_{0}^{2})^{T}\\left[\\frac{\\partial h}{\\partial x_{0}^{1}}\\right]^{T}n\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}_{a s y m}$ is defined in (1) and represents contextual information. The two noise-weighted items represent contrastive objective between $h(x_{t}^{1})$ and $[\\partial h_{g}/\\partial x_{0}^{2}]$ $(h_{g}(x_{0}^{2})$ and $[\\partial h/\\partial x_{0}^{1}]$ ) weighted by the Gaussian noise $n$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3 implies that the Gaussian noise introduces two extra terms in (7) and could affect the optimization process of $\\mathcal{L}_{a s y m}$ . Noisy patch reconstruction undermines the contrastive objective of contextual information, since larger Gaussian noise leads to more severe perturbation on $\\mathcal{L}_{a s y m}$ . ", "page_idx": 4}, {"type": "text", "text": "In summary, we leverage mutual information and contrastive asymmetric loss to demonstrate that the noisy patches mask-reconstruction is sub-optimal to learn real contextual information and larger noise could have more serious impact on context information extraction. This is consistent with the results in Figure 1(a). To solve this problem, in Section 3.3, we propose MC-DiT to effectively reconstruct masked clean patches from unmasked clean patches. ", "page_idx": 4}, {"type": "text", "text": "3.3 Contextual Enhancement with Masked Clean Patches ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As demonstrated in Propositions 2 and 3 that reconstructing masked noisy patches from unmasked noisy patches is insufficient for contextual information extraction, we propose a novel method named MC-DiT to enhance contextual information extraction for DiT from the perspective of leveraging masked clean patches to reconstruct unmasked clean patches. Figure 2(a) depicts the proposed framework for MC-DiT. The clean images are disrupted by Gaussian noise $n\\sim\\dot{\\mathcal{N}}(0,t^{2}I)$ , where $t$ is the time step. Then the noisy images are patchified and masked by a random binary mask $m$ . The unmasked noisy patches $\\bar{x_{t}^{1}}$ are fed into the DiT encoder for feature extraction. For contextual information extraction, the masked clean patches $\\boldsymbol{x}_{0}^{2}$ are concatenated with extracted feature $z=c o n c a t(z_{1},x_{0}^{2})$ , where $z_{1}$ is the feature of $\\boldsymbol{x}_{t}^{1}$ . After that, the feature $z$ is sent to DiT decoder to reconstruct clean unmasked patches $x_{0}^{1}$ , which is consistent with previous masked diffusion ([48],[49]). The training objective of unmasked clean patch reconstruction is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c l e a n}=\\mathbb{E}_{x_{0}\\sim p_{d a t a}}\\mathbb{E}_{n\\sim N(0,t^{2}I)}\\|(D_{\\theta}((x_{0}+n)\\odot(1-m),x_{0}\\odot m,t)-x_{0})\\odot(1-m)\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By applying masked clean patches $\\boldsymbol{x}_{0}^{2}$ in (8), the information in $\\boldsymbol{x}_{0}^{2}$ is transferred to unmasked clean output patches $\\tilde{x}^{1}$ , which is constrained to equal $x_{0}^{1}$ . It is not disrupted by noise $n$ , since there is no noise in $x_{0}^{2}$ and $x_{0}^{1}$ . Moreover, as mentioned in Section 3.2, the mutual information for our MC-DiT is $\\mathcal{T}(x_{0}^{1};x_{0}^{\\tilde{2}})$ , since we leverage masked clean patches to recover unmasked clean patches. According to Proposition 2, $\\mathcal{T}(x_{0}^{1};x_{0}^{2})$ is much higher than $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ and $\\mathcal{T}(x_{t}^{1};x_{t}^{2})$ under different noise $n$ and time steps $t$ . Thus, the mutual information learned by our MC-DiT would not decrease for different noise at different time steps and sufficient contextual information could be learned for reconstruction. ", "page_idx": 4}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/5380139577fb346cacf25040307c461cb7b690d84ac7359c9dbdc112e63a7cea.jpg", "img_caption": ["Figure 2: Framework of the proposed MC-DiT. (a) Pre-training. MC-DiT introduces unmasked clean patches and learns sufficient contextual information by reconstructing unmasked clean patches from masked clean patches. Two complementary EMA branches are developed to avoid model collapse. (b) Finetuning. MC-DiT is trained with unmasked patches for denoising. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In addition, we explore the potential benefits of $x_{0}^{2}$ by enforcing the masked output patches ${\\tilde{x}}^{2}$ to match $\\boldsymbol{x}_{t}^{2}$ . As discussed in Section 3.1, the denoising network $D_{\\theta}$ predicts clean images from noisy images and can be viewed as distinguishing the clean images and noise. Therefore, predicting $x_{t}^{\\tilde{2}}$ from $x_{0}^{2}$ is to recognize the noise $n$ and add to $x_{0}^{2}$ . To this end, we employ reversed constraint on masked output patches $\\tilde{x}^{2}$ , as illustrated in Figure 2(a). The training objective is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{n o i s e}=\\mathbb{E}_{x_{0}\\sim p_{d a t a}}\\mathbb{E}_{n\\sim N(0,t^{2}I)}\\|(D_{\\theta}((x_{0}+n)\\odot(1-m),x_{0}\\odot m,t)-(x_{0}+n))\\odot m\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.4 Addressing Model Collapse ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although MC-DiT can learn sufficient contextual information in theory, there exists model collapse problem in practice. The model learns a shortcut way that it only leverages masked clean patches to reconstruct unmasked clean patches and neglect the unmasked noisy patches. We further address the model collapse problem caused by only using masked clean patches to reconstruct unmasked clean patches for training. We introduce two extra EMA (Exponential Moving Average) branches of DiT decoders3 to balance the mask-reconstruction and denoising objective. As shown in Figure 2(a), given the noisy input to DiT encoder, we introduce two branches to achieve only mask-reconstruction and denoising alongside the original DiT decoder. The upper branch of DiT decoder $D_{\\phi}$ reconstructs from the whole noisy patches and captures denoising information, while the bottom branch processes clean patches and captures contextual information $D_{\\varphi}$ . Constraints on the two branches are employed in the loss function to balance the DiT decoder. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{c o n1}\\!=\\!\\mathbb{E}_{x_{0}\\sim p_{d a t a}}\\mathbb{E}_{n\\sim N(0,t^{2}I)}\\|(D_{\\theta}((x_{0}\\!+\\!n)\\odot(1\\!-\\!m),x_{0}\\odot m,t)\\!-\\!D_{\\phi}(x_{0}\\!+\\!n))\\odot\\!(1\\!-\\!m)\\|^{2},}\\\\ &{\\mathcal{L}_{c o n2}\\!=\\!\\mathbb{E}_{x_{0}\\sim p_{d a t a}}\\mathbb{E}_{n\\sim N(0,t^{2}I)}\\|(D_{\\theta}((x_{0}\\!+\\!n)\\odot(1\\!-\\!m),x_{0}\\odot m,t)\\!-\\!D_{\\varphi}(x_{0}))\\odot\\!(1\\!-\\!m)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/32b9d25c86cc79213b7e51d319aec0691f8a4badb757b7e130e243e6bd6ef9b9.jpg", "table_caption": ["Table 1: Comparison with state-of-the-art approaches for ImageNet- $256\\!\\times\\!256$ class conditional image generation. Bold font represents the best result. \u2018-G\u2019 means using classifier-free guidance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Here, $D_{\\phi}$ and $D_{\\varphi}$ are two EMA DiT decoders. Therefore, the overall loss is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{c l e a n}+\\lambda_{1}\\mathcal{L}_{n o i s e}+\\lambda_{2}\\mathcal{L}_{c o n1}+\\lambda_{3}\\mathcal{L}_{c o n2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda_{1},\\lambda_{2}$ and $\\lambda_{3}$ are hyper-parameters. ", "page_idx": 6}, {"type": "text", "text": "Unmasking Finetuning. Similar to MaskDiT [48], although our MC-DiT captures contextual information during masking training, directly applying the pretrained model in inference leads to unsatisfactory performance, which is caused by the training-inference discrepancy. The clean patches used in training are not provided in the inference time. Thus, after training our MC-DiT, we finetune it on the unmasked scenarios for better performance, as shown in Figure 2 (b). It is worth noting that MC-DiT only needs a few iteration in the finetuning to generated semantic coherence images. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Model Settings. We follow the same architecture in MaskDiT [48]. Specifically, we first apply a pretrained variational autoencoder (VAE) from Stable Diffuion [37] to map the images into latent space, and then train our MC-DiT to reconstruct clean patches from noisy patches under the EDM [21] framework to approximate score function in the diffusion process. The pretrained VAE maps $256\\!\\times\\!256\\!\\times\\!3$ input images to $32\\!\\times\\!32\\!\\times\\!4$ latent features and $512\\!\\times\\!512\\!\\times\\!3$ images to $64\\!\\times\\!64\\!\\times\\!4$ latent features. Similar to SD-DiT, we apply DiT-S, DiT-B, and DiT-XL as our backbones. ", "page_idx": 6}, {"type": "text", "text": "Training Details. Similar to previous works [13, 48, 49], we train MC-DiT on ImageNet [39] with resolutions $256\\!\\times\\!256\\!\\times\\!3$ and $512\\!\\times\\!512\\!\\times\\!3$ , respectively. Most training settings are the same as MaskDiT [48]. We train MC-DiT for 400K to 1M iterations using the AdamW optimizer with learning rate 0.0001 and no weight decay. By default, we use $50\\%$ mask ratio and batch size 1024. $\\lambda_{1}$ and $\\lambda_{2}$ in (12) are set to 0.1 and 0.05 for more denoising reconstruction. The EMA coefficient is set to 0.999 for smoothness and no data augmentation is employed. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. Following DiT [34], we leverage Fr\u00e9chet Inception Distance (FID) to measure the quality of generated images. For fair comparison, we also use ADM\u2019s Tensorflow evaluation ", "page_idx": 6}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/ec1c80a5a221a10366dfb09dce68cb44f4a53a8037b407aa85485012551a1959.jpg", "table_caption": ["Table 2: Comparison with state-of-the-art approaches for ImageNet- $512\\!\\times\\!512$ class conditional image generation. The bold font represents the best performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Comparison with state-of-the-art approaches ImageNet- $.256\\!\\times\\!256$ class conditional image generation at different scales and iterations. \u2019-S\u2019, \u2019-B\u2019, \u2019-XL\u2019 means \u2019small\u2019, \u2019base\u2019 and largest model size, respectively and $\\mathbf{\\nabla}/2\\mathbf{\\nabla}$ denotes the patch size of 2 for all input patches. ", "page_idx": 7}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/4d7514a79dec84b92a1a0af3045d2fc88a409ae83ef44d48424146f39f949242.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "suite [9] to compute FID-50K (FID for short), sFID [31], Inception Score (IS) [40] and Precision/Recall [24] as secondary metrics. More vivid images have lower FID and sFID, while their IS and Precision/Recall are higher. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate vanilla training (i.e., LDM [37], ADM [9], and DiT [34]) and masked training (i.e., proposed MC-Dit, MaskDiT[48], MDT [13], and SD-DiT [49]) using backbones of different scales for $256\\!\\times\\!256$ and $512\\!\\times\\!512$ image generation on ImageNet. ", "page_idx": 7}, {"type": "text", "text": "Results on ImageNet- $256\\!\\times\\!256$ . Table 1 shows that our MC-DiT-XL / 2 achieves the smallest FID score and the highest IS score. Compared with non-masked diffusion models, MC-DiT-XL / 2 decrease the FID score from 9.62 to 4.14. Compared with masked diffusion models, the FID score decreases from 5.69 to 4.14. With classifier-free guidance (-G), our MC-DiT-XL / 2-G achieves the best FID score of 1.78, and the highest IS score, which significantly outperforms previous works. ", "page_idx": 7}, {"type": "text", "text": "Results on ImageNet- $\\pmb{512}{\\times512}$ . Table 2 shows that MC-DiT-XL / 2 achieves a FID of 9.30 and outperforms MaskDiT [48] and DiT [34]. The IS score of MC-DiT is also the highest, indicating the effectiveness of our method. With classifier-free guidance (-G), our MC-DiT-XL / 2-G achieves the best FID score of 2.03, indicating the effectiveness of MC-DiT. ", "page_idx": 7}, {"type": "text", "text": "Contextual Enhancement. Figure 1 (a) reports the mutual information between unmasked and masked output patches with different noise, which can be viewed as the metric of contextual information consistency. Our MC-DiT decreases slowly during the noise variance becomes larger, which indicates more sufficient contextual reconstruction regardless noise. ", "page_idx": 7}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/ebf0966f27610821e5e7d41e74f9c5b6c504f0332f62254e11f53d46a2d9dd33.jpg", "img_caption": ["Figure 3: Training loss and FID for DiT-B/2, MaskDit-B/2, and MC-DiT-B/2 during training. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/139718b4859696b43109fb0ff3572a0025273acface368550c9a08606c31604d.jpg", "img_caption": ["Figure 4: Comparison of $256\\times256$ images generated by MDT, MaskDiT and MC-DiT. Various details are strange in images generated by MDT and MaskDiT. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Backbones at different scales. Table 3 evaluates FID-50K at different scales and training iterations with various backbones. Notice that MaskDiT only reports the performance of \u2018XL\u2019 scale. Under fixed number of training iterations, MC-DiT outperforms vanilla DiT [34], MDT [13], and SD-DiT [49] in FID by a large margin, i.e., 6.72, 9.74, and 1.09 FID reduction for DiT-S, DiT-B, and DiT-XL backbones. This fair comparison fully demonstrates the effectiveness of our method. ", "page_idx": 8}, {"type": "text", "text": "Convergence speed. In order to evaluate the convergence speed of various methods, we compare the training loss curve in Figure 3(a). We report the MSE loss (Eq. (8)) on clean patches for fairness. We train MaskDiT [48] and DiT [34] for 300K iterations due to the substantial time and GPU resource overhead and use the training curve of our MC-DiT trained for evaluations, which is trained for 400K iterations. The training loss of MC-DiT decreases faster than DiT [34] and MaskDiT [48]. Figure 3(b) measures FID-50K at each step after unmasked tuning and shows that MC-DiT achieves the lowest FID-50K score. ", "page_idx": 8}, {"type": "text", "text": "Generated image comparison. Figure 4 visualizes the $256\\times256$ images generated by MDT [13], MaskDiT[48] and our MC-DiT. Our generated images are more realistic and have more consistent textual structure than MaskDiT and MDT. For example, images of \u2019hammer\u2019 generated by MaskDiT and MDT have incomplete structure, while our MC-DiT generates images with more complete structures, validating the superior contextual information extraction ability of our MC-DiT. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For computation efficiency, we adopt \u2019-B\u2019 in all the models for fair comparison. All the models are trained for 400K itreations with batch size 256 and mask ratio $50\\%$ . ", "page_idx": 8}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/58ef4b1099db0d859b1af276a61983522020e739e1ab9043c82c6e06d8c0c4a2.jpg", "table_caption": ["Table 4: Ablation study of hyperparameters. "], "table_footnote": ["Table 6: Comparison with different targets. "], "page_idx": 9}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/9877bfa0310530ef136ec1bd68f034909e98f2fe37d3090e41c5506c0743e7c4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/f73f4661ad62728b6a3c77f8063f3cfca7e9765c8efd9faaafc89cc4a732c615.jpg", "table_caption": ["Table 5: Ablation study of the EMA branches. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/65e8cb56aa61cbeeb4f7c3741fe68abab6e9a9ff884f5167cb6fbaf6caf018ad.jpg", "table_caption": ["Table 7: Ablation study of unmasking tuning. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Main branch target. We evaluate the reconstruction targets of main branch by considering three cases, i.e., clean patch reconstruction $^+$ noisy patch reconstruction, all clean patch reconstruction and only clean patch reconstruction. \u2018All clean patches\u2019 means all the patches are constrained by clean reconstruction loss. \u2018Only clean patches\u2019 means only unmasked patches are constrained by clean reconstruction loss, and \u2018Clean patches $^+$ Noisy patches\u2019 means masked patches are further constrained by noisy reconstruction loss. Table 6 shows that our model performs the best, which validates the effectiveness of noisy reconstruction loss. ", "page_idx": 9}, {"type": "text", "text": "Effectiveness of two extra EMA branches. Table 5 evaluates the influence of two EMA branches. FID decreases obviously by 2.84 using the noise branch, indicating the necessity of noisy branch to address model collapse. Further experiments can be found in appendix. ", "page_idx": 9}, {"type": "text", "text": "Unmasked tuning. Unmasked tuning (UT) can reduce the training-inference discrepancy, as demonstrated by MaskDiT and is adopted in our MC-DiT. However, we can remove unmasked tuning to reduce the complexity at little loss on FID. Table 7 shows that FID will increase by only 0.41 for MC-DiT by removing unmasked tuning. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Hyperparameters. We separately evaluate four values for $\\lambda_{1},\\,\\lambda_{2}$ , and $\\lambda_{3}$ . Table 4 shows best FID is obtained when $\\lambda_{1}\\,=\\,0.1$ , $\\lambda_{2}=0.1$ , and $\\lambda_{3}=0.05$ . Note that $\\lambda_{2}$ is larger than $\\lambda_{3}$ since the denoising objective is more important than contextual information utilization. ", "page_idx": 9}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/b2f9ecd0f712696bd9973173edcfe383175d7807ed2f97173d0fe0d9554e9a11.jpg", "img_caption": ["Figure 5: Ablation study of mask ratio. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Mask ratio. Figure 5 visualizes the influence of the mask ratio in $m$ . FID is smallest at the mask ratio of $50\\%$ and increases rapidly when the mask ratio is larger than $50\\%$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we summarize the previous works that combine mask-reconstruction with DiT training and claim that reconstructing masked noisy patches from unmasked noisy patches is insufficient for contextual information extraction. To validate this claim, we analyze the mutual information and contrastive objective theoretically and experimentally. Besides, we propose a new pretraining paradigm (dubbed MD-DiT), which reconstructs unmasked clean patches from masked clean patches and guarantees the contextual information extraction. Moreover, to avoid model collapse, two extra EMA branches are applied in MC-DiT to adjust the balance between the mask-reconstruction task and denoising objective. Extensive experiments demonstrate the robustness of our method and our MC-DiT achieves the state-of-the-art performance in image generation. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Despite excellent performance, the training speed and inference speed of MC-DiT still needs to be improved. We will mitigate this issue in future work by transferring the information in the encoder into the decoder, which decreases the training difficulty. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. This work was supported in part by the National Natural Science Foundation of China under Grant 62125109, Grant T2122024, Grant 62320106003, Grant 62371288, Grant 62431017, Grant 62401357, Grant 62401366, Grant 61931023, Grant 61932022, Grant 62120106007. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. MultiMAE: Multi-modal multi-task masked autoencoders. In European Conference on Computer Vision, pages 348\u2013367. Springer, 2022. [2] Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based diffusion models. In NeurIPS 2022 Workshop on Score-Based Methods, 2022. [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A ViT backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22669\u201322679. IEEE, 2023.   \n[4] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch\u00f6nlieb, and Christian Etmann. Conditional image generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021.   \n[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In The Seventh International Conference on Learning Representations, 2019.   \n[6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked generative image Transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315\u201311325. IEEE, 2022.   \n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of 37th International Conference on Machine Learning, pages 1597\u20131607. PMLR, 2020.   \n[8] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang, Wenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-distillated masked autoencoder. In European Conference on Computer Vision, pages 108\u2013124. Springer, 2022.   \n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems 34, pages 8780\u20138794, 2021.   \n[10] Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In The Ninth International Conference on Learning Representations, 2021.   \n[12] Peng Gao, Teli Ma, Hongsheng Li, Ziyi Lin, Jifeng Dai, and Yu Qiao. ConvMAE: Masked convolution meets masked autoencoders. arXiv preprint arXiv:2205.03892, 2022.   \n[13] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion Transformer is a strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23164\u201323173. IEEE, 2023.   \n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013 144, 2020.   \n[15] Pengsheng Guo, Hans Hao, Adam Caccavale, Zhongzheng Ren, Edward Zhang, Qi Shan, Aditya Sankar, Alexander G Schwing, Alex Colburn, and Fangchang Ma. StableDreamer: Taming noisy score distillation sampling for text-to-3D. arXiv preprint arXiv:2312.02189, 2023.   \n[16] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, et al. I2V-Adapter: A general image-to-video adapter for video diffusion models. In SIGGRAPH\u201924: ACM SIGGRAPH 2024 Conference Papers, number 112, pages 1\u201312. ACM, 2024.   \n[17] Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss. In Advances in Neural Information Processing Systems 34, pages 5000\u20135011, 2021.   \n[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009. IEEE, 2022.   \n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33, pages 6840\u20136851, 2020.   \n[20] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1\u201333, 2022.   \n[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems 35, pages 26565\u201326577, 2022.   \n[22] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A Ross, Bryan Seybold, and Lu Jiang. VideoPoet: A large language model for zero-shot video generation. In Proceedings of the 41st International Conference on Machine Learning, pages 25105\u201325124. PMLR, 2024.   \n[23] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.   \n[24] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In Advances in Neural Information Processing Systems 32, pages 3927\u20133936, 2019.   \n[25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309. IEEE, 2023.   \n[26] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages 3730\u20133738. IEEE, 2015.   \n[27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural Information Processing Systems 35, pages 5775\u20135787, 2022.   \n[28] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.   \n[29] Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, and Tao Mei. Semanticconditional diffusion networks for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23359\u201323368. IEEE, 2023.   \n[30] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024.   \n[31] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. In Proceedings of the 38th International Conference on Machine Learning, pages 7958\u2013 7968. PMLR, 2021.   \n[32] James R Norris. Markov Chains. Cambridge University Press, 1998.   \n[33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205. IEEE, 2023.   \n[35] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024.   \n[36] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In The Eleventh International Conference on Learning Representations, 2023.   \n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695. IEEE, 2022.   \n[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In 18th International Conference on Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015, pages 234\u2013241. Springer, 2015.   \n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115:211\u2013252, 2015.   \n[40] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems 29, pages 2234\u20132242, 2016.   \n[41] Axel Sauer, Katja Schwarz, and Andreas Geiger. StyleGAN-XL: Scaling StyleGAN to large diverse datasets. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u201310. ACM, 2022.   \n[42] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Advances in Neural Information Processing Systems 33, pages 12438\u201312448, 2020.   \n[43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In The Ninth International Conference on Learning Representations, 2021.   \n[44] Nicolaas G Van Kampen. Stochastic differential equations. Physics Reports, 24(3):171\u2013228, 1976.   \n[45] Wenxuan Wang, Jing Wang, Chen Chen, Jianbo Jiao, Yuanxiu Cai, Shanshan Song, and Jiangyun Li. Fremim: Fourier transform meets masked image modeling for medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 7860\u20137870, 2024.   \n[46] Qi Zhang, Yifei Wang, and Yisen Wang. How mask matters: Towards theoretical understandings of masked autoencoders. In Advances in Neural Information Processing Systems 35, pages 27127\u201327139, 2022.   \n[47] Xingjian Zhen, Rudrasis Chakraborty, Liu Yang, and Vikas Singh. Flow-based generative models for learning manifold to manifold mappings. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, pages 11042\u201311052. AAAI, 2021.   \n[48] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. In Transactions on Machine Learning Research (TMLR), 2024.   \n[49] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang Wen Chen. SD-DiT: Unleashing the power of self-supervised discrimination in diffusion Transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8435\u20138445. IEEE, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Supplemental Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Theoretical Proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition 2. Given masked and unmasked clean patches $\\boldsymbol{x}_{0}^{1}$ and $\\boldsymbol{x}_{0}^{2}$ and their noisy versions $\\boldsymbol{x}_{t}^{1}$ and $\\bar{x}_{t}^{2}$ , the mutual information $\\mathcal{T}(x_{t}^{1};x_{t}^{2})$ , $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ , and $\\mathbb{Z}(x_{0}^{\\mathrm{{I}}};x_{0}^{2})$ satisfy that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(x_{0}^{1};x_{t}^{2})\\approx\\mathcal{Z}(x_{0}^{1};x_{0}^{2})-\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{2})}\\left[K L(p(x_{0}^{1}|x_{0}^{2})||p(x_{0}^{1}|x_{t}^{2}))\\right]}\\\\ {\\mathcal{Z}(x_{t}^{1};x_{t}^{2})\\approx\\mathcal{Z}(x_{0}^{1};x_{0}^{2})-\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{2})}\\left[K L(p(x_{0}^{1}|x_{0}^{2})||p(x_{0}^{1}|x_{t}^{2}))\\right]}\\\\ {-\\,\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{t}^{1}|x_{0}^{1})}\\left[K L(p(x_{t}^{2}|x_{0}^{1})||p(x_{t}^{2}|x_{t}^{1}))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Given the time step $t$ ,masked noisy patches $\\scriptstyle x_{t}^{2}$ , masked clean patches $\\boldsymbol{x}_{0}^{2}$ , clean unmasked patches $\\boldsymbol{x}_{0}^{1}$ and noisy unmasked patches $\\bar{x_{t}^{1}}$ , where $x_{t}^{\\tilde{2}}=x_{0}^{2}+n,x_{t}^{1}=x_{0}^{1}+n$ and $n\\sim\\mathcal{N}(0,t^{2}I)$ we derive the mutual information $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ according to the definition. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(x_{0}^{1};x_{t}^{2})=\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{1})}\\log\\frac{p(x_{0}^{1}|x_{t}^{2})}{p(x_{0}^{1})}}\\\\ &{\\phantom{\\sum}=\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{1})}\\log\\left(\\frac{p(x_{0}^{1}|x_{0}^{2})}{p(x_{0}^{1})};\\frac{p(x_{0}^{1}|x_{t}^{2})}{p(x_{0}^{1}|x_{0}^{2})}\\right)}\\\\ &{\\phantom{\\sum}\\approx\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{0}^{2}|x_{t}^{1})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{2})}\\log\\left(\\frac{p(x_{1}^{1}|x_{0}^{2})}{p(x_{1}^{1})}\\cdot\\frac{p(x_{1}^{1}|x_{t}^{2})}{p(x_{1}^{1}|x_{0}^{2})}\\right)}\\\\ &{\\phantom{\\sum}=\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{0}^{2}|x_{t}^{1})}\\log\\frac{p(x_{1}^{1}|x_{0}^{2})}{p(x_{0}^{1})}+\\mathbb{E}_{p(x_{0}^{1},x_{0}^{2},x_{t}^{2})}\\log\\frac{p(x_{1}^{1}|x_{t}^{2})}{p(x_{1}^{1}|x_{0}^{2})}}\\\\ &{\\phantom{\\sum}=\\mathbb{E}(x_{0}^{1};x_{0}^{2})+\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{2})}\\mathbb{E}_{p(x_{0}^{1}|x_{t}^{2})}\\log\\frac{p(x_{1}^{1}|x_{t}^{2})}{p(x_{1}^{1}|x_{0}^{2})}}\\\\ &{\\phantom\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, the mutual information between noisy masked patches and unmasked clean patches $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ is less than $\\mathcal{T}(x_{0}^{1};x_{0}^{2})$ due to the non-negativity of KL divergence. Moreover, during $t$ increases, the variance of the Gaussian noise $n$ becomes larger. As a result, noisy masked patches $x_{t}^{2}$ are disrupted heavily from clean patches $\\boldsymbol{x}_{0}^{2}$ . The distribution $p(x_{0}^{1}|x_{t}^{2})$ is very dissimilar from $p(x_{0}^{1}|x_{0}^{2})$ . Formally, the derivation of KL divergency can be written as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}\\mid x_{0}^{2})}\\mathbb{E}_{p(x_{0}^{1}\\mid x_{0}^{2})}\\log\\left[\\frac{p\\left(x_{0}^{1}\\mid x_{t}^{2}\\right)}{p\\left(x_{0}^{1}\\mid x_{0}^{2}\\right)}\\right]}\\\\ &{\\phantom{\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}\\mid x_{0}^{2})}\\mathbb{E}_{p(x_{0}^{1}\\mid x_{0}^{2})}\\log\\left[\\frac{p\\left(x_{t}^{2}\\mid x_{0}^{1}\\right)}{p\\left(x_{0}^{2}\\mid x_{0}^{1}\\right)}\\times\\frac{p\\left(x_{t}^{2}\\right)}{p\\left(x_{0}^{2}\\right)}\\right]}\\\\ &{\\phantom{\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}\\mid x_{0}^{2})}\\mathbb{E}_{p(x_{0}^{1}\\mid x_{0}^{2})}\\log\\left[\\frac{p\\left(x_{0}^{2}\\mid x_{0}^{1}\\right)+p\\left(n\\mid x_{0}^{1}\\right)}{p\\left(x_{0}^{2}\\mid x_{0}^{1}\\right)}\\times\\frac{p\\left(x_{0}^{2}\\right)+p\\left(n\\right)}{p\\left(x_{0}^{2}\\right)}\\right]}\\\\ &{\\phantom{\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}\\mid x_{0}^{2})}\\mathbb{E}_{p(x_{0}^{1}\\mid x_{0}^{2})}\\log\\left[\\left(1+\\frac{p\\left(n\\mid x_{0}^{1}\\right)}{p\\left(x_{0}^{2}\\mid x_{0}^{1}\\right)}\\right)\\times\\left(1+\\frac{p\\left(n\\right)}{p\\left(x_{0}^{2}\\right)}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We approximate $p(x_{t}^{2})\\approx p(x_{0}^{2})+p(n)$ , since $p(x_{t}^{2})$ is a Gaussion distribution with mean value $\\boldsymbol{x}_{0}^{2}$ and variance $t^{2}$ . As $t$ increases, the KL divergence $\\dot{K L}(p(x_{0}^{1}|x_{0}^{2})||p(x_{0}^{1}|x_{t}^{2}))$ increases and the mutual information $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ achieves the larger difference with $\\mathbb{Z}(x_{0}^{1};x_{0}^{2})$ . Thus, the mutual information $\\mathcal{T}(x_{0}^{1};x_{t}^{2})$ is lower than $\\mathbb{Z}(x_{0}^{1};x_{0}^{2})$ . ", "page_idx": 13}, {"type": "text", "text": "Similarly, the mutual information between noisy patches $\\mathcal{T}(x_{t}^{1};x_{t}^{2})$ can be drived according to Eq. 20: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{Z}(x_{t}^{1};x_{t}^{2})\\approx\\mathcal{Z}(x_{0}^{1};x_{t}^{2})-\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{t}^{1}|x_{0}^{1})}\\left[K L(p(x_{t}^{2}|x_{0}^{1})||p(x_{t}^{2}|x_{t}^{1}))\\right]}\\\\ &{\\qquad\\qquad\\approx\\mathcal{Z}(x_{0}^{1};x_{0}^{2})-\\mathbb{E}_{p(x_{0}^{2})}\\mathbb{E}_{p(x_{t}^{2}|x_{0}^{2})}\\left[K L(p(x_{0}^{1}|x_{0}^{2})||p(x_{0}^{1}|x_{t}^{2}))\\right]}\\\\ &{\\qquad\\qquad\\qquad-\\,\\mathbb{E}_{p(x_{0}^{1})}\\mathbb{E}_{p(x_{t}^{1}|x_{0}^{1})}\\left[K L(p(x_{t}^{2}|x_{0}^{1})||p(x_{t}^{2}|x_{t}^{1}))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, the proposition has been proved. ", "page_idx": 13}, {"type": "text", "text": "Proposition 3. The asymmetric loss of noisy patch reconstruction and the asymmetric loss of clean patch reconstruction satisfy that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{a s y m-N N}=-\\mathbb{E}_{p(x_{t}^{1},x_{t}^{2})}\\left[h(x_{t}^{1})^{T}h_{g}(x_{t}^{2})\\right]}\\\\ &{\\qquad\\qquad\\approx\\mathcal{L}_{a s y m}+\\mathbb{E}\\left\\{-h(x_{t}^{1})^{T}\\left[\\frac{\\partial h_{g}}{\\partial x_{0}^{2}}\\right]^{T}n\\right\\}+\\mathbb{E}\\left\\{-h_{g}(x_{0}^{2})^{T}\\left[\\frac{\\partial h}{\\partial x_{0}^{1}}\\right]^{T}n\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{L}_{a s y m}$ is defined in (1) and represents contextual information. The two noise-weighted items represent contrastive objective between $h(x_{t}^{1})-[\\partial h_{g}/\\partial x_{0}^{2}]$ and $h_{g}(x_{0}^{2})-[\\partial h/\\partial x_{0}^{1}]$ weighted by the Gaussian noise $n$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. According to Eq.1, the asymmetric loss can be written as: ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{L}_{a a y m-N}=-\\mathbb{E}_{p(\\cdot),x_{2}}[h(x_{1}^{T})^{T}h_{y}(x_{2}^{t})]}&{=}&{(25)}\\\\ &{\\approx-\\mathbb{E}\\left\\{h(x_{1}^{1})^{T}h_{y}(x_{2}^{1})+\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{1}}\\right]^{T}n+o(x_{2}^{2})\\right]\\right\\}}&{=}&{(26)}\\\\ &{=-\\mathbb{E}\\left\\{h(x_{1}^{1})^{T}h_{y}(x_{2}^{2})+h(x_{1}^{1})^{T}\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{2}}\\right]^{T}n\\right\\}}&{=}&{(27)}\\\\ &{\\approx-\\mathbb{E}\\left\\{\\left[h(x_{2}^{1})+\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{1}}\\right]^{T}n+o(x_{1}^{2})\\right]^{T}h_{y}(x_{2}^{2})+h(x_{1}^{1})^{T}\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{1}}\\right]^{T}n\\right\\}}&{=}&{(31)}\\\\ &{=-\\mathbb{E}\\left\\{\\left[h(x_{2}^{1})^{T}h_{y}(x_{2}^{2})\\right]+\\mathbb{E}\\left\\{-h(x_{1}^{1})^{T}\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{1}}\\right]^{T}n\\right\\}+\\mathbb{E}\\left\\{-h_{y}(x_{2}^{2})^{T}\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{1}}\\right]^{T}n\\right\\}}&{=}&{0.5}\\\\ &{=\\mathcal{L}_{a a y m}+\\mathbb{E}\\left\\{-h(x_{1}^{1})^{T}\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{1}}\\right]^{T}n\\right\\}+\\mathbb{E}\\left\\{-h_{y}(x_{2}^{2})^{T}\\left[\\frac{\\partial h_{y}}{\\partial x_{2}^{1}}\\right]^{T}n\\right\\}}&{=(31)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we leverage first-order Taylor\u2019s formula in Eq. 26 and Eq. 28 to calculate $h(x_{t}^{1})$ and $h_{g}(x_{t}^{2})$ at $\\boldsymbol{x}_{0}^{1}$ and $\\boldsymbol{x}_{0}^{2}$ , since $x_{t}^{1}=x_{0}^{1}+n$ and $x_{t}^{2}=x_{0}^{2}+n$ . o denotes the the higher order infinitesimal. ", "page_idx": 14}, {"type": "text", "text": "A.2 Experiment Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Diffusion Settings. We leverage EDM [21] as our diffusion training framework, which predicts clean image patches from noisy images. For fair comparison, we use the default parameters in EDM (see [21] for more details). During inference, we generate conditional images from Gaussian noise via EDM-sampler [21]. Specifically, the time steps in the reverse process are set via $t_{i}=$ $\\begin{array}{r}{(t_{m a x}^{\\frac{1}{\\rho}}+\\frac{i}{N-1}(t_{m i n}^{\\frac{1}{\\rho}}-t_{m a x}^{\\frac{1}{\\rho}}))^{\\rho}}\\end{array}$ , where $N\\,=\\,40$ , $\\rho\\,=\\,7$ , $t_{m a x}\\,=\\,80$ and $t_{m i n}\\,=\\,0.002$ . Besides, the second-order correction is applied and the generated images are the average of first-order and second-order results. ", "page_idx": 14}, {"type": "text", "text": "Training Details. We follow the LDM [37] and adopt a pretrained VAE to firstly map the images into the latent spaces. The weight of pretrained VAE is from Stable Diffusion [37]. Then, we train the denoising models with these latent features. We leverage AdamW optimizer with learning rate 0.0001, batch size 256, and $50\\%$ mask ratio. As for unmasking fintuning, we slightly change some hyper-parameters with learning rate 0.00005, batch size 128, mask ratio $0\\%$ . Some details can be found in Table 8. ", "page_idx": 14}, {"type": "text", "text": "A.3 Supplementary Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Generalization Experiments. We adopt the ImageNet dataset in the experiments for a fair comparison, since MaskDiT[48], SD-DiT[49] and MDT[13] are all evaluated on the ImageNet dataset[39]. In fact, our MC-DiT can be generalized to different domains or datasets for improved image generation due to the fact that it can extract contextual information from arbitrary images. Table 9 compares the performance of MaskDiT[48] and MC-DiT on the CIFAR-10 [23] and CelebA [26] (that collected for face anti-spoofing) datasets. Due to time limit, we train both MaskDiT[48] and MC-DiT for 200K iterations. Experimental results show that MC-DiT outperforms MaskDiT[48] on both datasets. ", "page_idx": 14}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/6455594700493b31e00ae10ab7cb40b35bf7fde9429cef07a3fe477684ab9624.jpg", "table_caption": ["Table 8: Experimental details about MC-DiT. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/733b1f7757bb6be131675f1968c9e09acb1990f3c5e09dfb6defbec5a8eed5f6.jpg", "table_caption": ["Table 9: Performance comparison on Cifar10 and CelebA dataset of MaskDiT and MC-DiT "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/2c54463e179c750f9b0ad40491d02e9ed893d639fee1b955e2bbeba185b54abe.jpg", "table_caption": ["Table 10: Performance comparisons with different branches. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Convergence Speed. In Figure 3, we compare the training curve of DiT [34], MaskDiT [48] and MC-DiT and point out that the training loss of MC-DiT decreases faster than DiT [34] and MaskDiT [48]. This is due to the primary focus of our analysis is the overall effectiveness of the model. The blue line can achieve a lower loss, despite similar iterqation counts for flattening, highlighting the model\u2019s efficiency in reaching a more optimal solution. Besides, the loss reported in Figure 3(a) denotes the MSE loss $\\mathcal{L}_{c l e a n}$ . Thus, lower MSE loss means the generated clean patches are more similar to the ground-truth. Moreover, MC-DiT achieves lower MSE loss with the same iterations with DiT [34] and MaskDiT [48], indicating the performant model with higher convergence speed. ", "page_idx": 15}, {"type": "text", "text": "Main Branch Target. The modal collapse occurs when the main branch only considers clean-toclean mask-reconstruction for masked clean patches but ignores the denoising of unmasked noisy patches. We propose two EMA branches to balance the two tasks for the main branch. We use the noisy EMA branch to realize noisy-to-clean mapping for denoising, and the clean EMA branch to realize clean-to-clean mapping for mask-reconstruction (mask ratio is $0\\%$ ). The two EMA branches constrain the output of the main branch (minimize the MSE loss between the outputs of the main branches and EMA branches) via three hyper-parameters, which leads to the balance on the denoising task and clean-to clean mask-reconstruction task. ", "page_idx": 15}, {"type": "text", "text": "To verify this, we report in Table 10 the FID score of the main branch with noisy and clean patch inputted only. The result of the main branch with unmasked noisy patches only is higher than that of masked clean patches, indicating the modal collapse problem. With noisy and clean branches, the FID score of the main branches decline distinctly, validating the effectiveness of the EMA branches. ", "page_idx": 15}, {"type": "text", "text": "Ablation Study of hyperparameters. Following MaskDiT[48], we select 0.01, 0.1, and 1.0 as the scaling values of three hyperparameters and supplement various values for ablation study. Table 13, Table 14 and Table 15 evaluate various values of the three hyperparameters and we find that the best FID is still obtained when $\\lambda_{1}=0.1$ , $\\lambda_{2}=0.1$ , and $\\lambda_{3}=0.05$ . ", "page_idx": 15}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/ccbf357230cb6fafb21008a07d8c7db083e3a65d07b7f015796a98b44ea1b844.jpg", "img_caption": ["Figure 6: Feature visualization of MaskDiT and MC-DiT at different noise variance. Better viewed by zoom in. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/4d83e994315293832df0982dbcccbe5275eb8f14abe26f486c2bbc1f215830b1.jpg", "table_caption": ["Table 11: Parameters and training cost comparison between MDT, MaskDiT and MC-DiT. Training speed denotes the number of iterations per second. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Attention Map Visualization. Figure 6 visualizes the attention map of MaskDiT and MC-DiT at different noise variance with Cifar10 dataset[23]. A larger noise variance denotes the noise with large scale. Our MC-DiT extracts proper shape for various noise scale, while the features extracted by MaskDiT are messy in the large noise scale. This further proves the motivation and effectiveness of our paper that clean-to-clean mask reconstruction promotes learning sufficient contextual information. ", "page_idx": 16}, {"type": "text", "text": "Training cost comparison. We compare the training cost (parameters, FLOPs, memory used and training speed) on $4\\!\\times\\!\\mathrm{V}100$ GPUs in Table 11. The training speed of MC-DiT is a little bit slower than other methods due to two EMA branches. However, the inference speed of MC-DiT is similar to MaskDiT, since two EMA branches are removed during inference. The additional overhead of MC-DiT is relatively small $(7.6\\%$ parameters and $8\\%$ FLOPs), but the FID performance improvement is significant. ", "page_idx": 16}, {"type": "text", "text": "EMA Branch with DiT encoder. In the main branch of MC-DiT, the unmasked noisy patches are fed into the DiT encoder, while all the noisy patches are directly inserted into the EMA DiT decoder to avoid modal collapse, as shown in the Figure 2. The reasons are on the two folds: (1) efficient. Only apply DiT decoder for EMA branches leads to small extra parameters and fast inference speed, while EMA DiT encoder slows down the entire EMA branches. (2) effective. The DiT decoder is trained to extract masked clean images patches in the main branch. Thus, directly apply image patches as the input of EMA DiT decoder does not lead to poor denoising results. As shown in Table 12, applying EMA DiT encoder introduces extra 669M parameters, while FID score only decreases 1.35. Thus, to balance the parameters and performance, we select DiT decoder in the EMA branches. ", "page_idx": 16}, {"type": "text", "text": "Table 12: Performance and parameters comparisons with and without DiT encoder in EMA branches. ", "page_idx": 17}, {"type": "table", "img_path": "y9sHKrdnRt/tmp/82d26d43fd3825a9eb0de9c220801dfa6d847a9468238e3cab501a4559b764fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 13: Ablation study on $\\lambda_{1}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u03bb1 0 0.01 0.03 0.05 0.07 0.09 0.1 0.3 0.5 0.7 0.9 1.0   \nFID 43.23 40.99 39.23 38.44 37.95 36.53 35.20 35.98 36.74 36.91 37.52 38.97 ", "page_idx": 17}, {"type": "text", "text": "Table 14: Ablation study on $\\lambda_{2}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "$\\lambda_{2}$ 0 0.01 0.03 0.05 0.07 0.09 0.1 0.3 0.5 0.7 0.9 1.0   \nFID 38.83 36.15 36.02 36.46 35.99 36.07 35.20 35.34 36.18 37.26 35.98 37.54 ", "page_idx": 17}, {"type": "text", "text": "Table 15: Ablation study on $\\lambda_{3}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u03bb3 0 0.01 0.03 0.05 0.07 0.09 0.1 0.3 0.5 0.7 0.9 1.0   \nFID 37.77 37.25 36.63 35.20 36.07 37.93 35.46 35.88 37.26 36.19 37.40 36.35 ", "page_idx": 17}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/01d511f57599414df2773c7888036df3306bc08a958d521a64b48b3ded2a0e6d.jpg", "img_caption": ["Figure 7: Visualization of $256\\times256$ images generated by our MC-DiT. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.4 Generated Samples ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 7 visualizes some images generated by our MC-DiT with $256\\times256$ resolutions. Figure 8 visualizes some images generated by our MC-DiT with $512\\times512$ resolutions. ", "page_idx": 17}, {"type": "image", "img_path": "y9sHKrdnRt/tmp/475e8fc33a38fa7834cc21b6830c7e8d1f37c5c94615d6af2430ec99b0d57fd3.jpg", "img_caption": ["Figure 8: Visualization of $512\\times512$ images generated by our MC-DiT "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We clearly clarify our claim in the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discuss our limitation in the conclusion section. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We clearly describe the theoretical results and proof in Sec 3.2 and supplementary. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We clearly clarify the setting and parameters of experiments in Sec 4.2. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have provide the core flie of our code in the supplementary. And the code will be released upon acceptance. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We describe this in details in Sec 4.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We focused primarily on the exploratory analysis and preliminary results. Addressing statistical significance and error bars will be a priority in our future research to provide a more comprehensive evaluation of our findings. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We will report this upon acceptance. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have adhered to all ethical guidelines and standards throughout our study. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We briefly discuss this in conclusion. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We follow the source code of MaskDiT, which is credited properly. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The pretrained weight and source code will be released upon acceptance. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]