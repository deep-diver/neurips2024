[{"figure_path": "87AXdbkRyd/tables/tables_4_1.jpg", "caption": "Table 1: Computational Cost. Forward-backward time per iteration on NVIDIA 3090 GPU with ResNet-50 and batch size 256.", "description": "This table shows the computational cost (forward and backward pass time per iteration) for different methods, including SimCLR, AugSelf, EquiMod, and STL.  The experiments were conducted on an NVIDIA 3090 GPU using ResNet-50 and a batch size of 256. The \"Ratio\" column normalizes the time taken by each method relative to SimCLR, providing a direct comparison of their computational efficiency.", "section": "3.3 Implementation Details"}, {"figure_path": "87AXdbkRyd/tables/tables_5_1.jpg", "caption": "Table 2: Out-domain Classification. Evaluation of representation generalizability on the out-domain downstream classification tasks. Linear evaluation accuracy (%) is reported for ResNet-50 pretrained on ImageNet100.", "description": "This table presents the results of evaluating the generalizability of learned representations on various downstream classification tasks.  It compares different representation learning methods (invariant, implicit equivariant, explicit equivariant, and STL) by measuring their linear evaluation accuracy on 11 different datasets (CIFAR10, CIFAR100, Food101, MIT67, Pets, Flowers, Caltech101, Cars, Aircraft, DTD, SUN397).  The ResNet-50 model was pretrained on ImageNet100 for all methods. The results show STL's superior performance across many datasets, demonstrating robust generalization.", "section": "4.1 Main Results"}, {"figure_path": "87AXdbkRyd/tables/tables_5_2.jpg", "caption": "Table 3: In-domain Classification. Evaluation of representation on in-domain classification task. Linear evaluation accuracy (%) is reported for ResNet-50 pretrained on ImageNet100.", "description": "This table presents the linear evaluation accuracy of ResNet-50, pretrained on ImageNet100, for an in-domain classification task.  It compares the performance of various methods including invariant learning (SimCLR, SimCLR with AugMix), implicit equivariant learning (E-SSL, AugSelf), and explicit equivariant learning (SEN, EquiMod, SIE) against the proposed method, STL, with and without AugMix. The results show the in-domain classification accuracy for each method, allowing for a direct comparison of their performance on this specific task.", "section": "4 Main Results"}, {"figure_path": "87AXdbkRyd/tables/tables_6_1.jpg", "caption": "Table 4: Object Detection. Evaluation of representation generalizability on a downstream object detection task. Average precision is reported for ImageNet100-pretrained ResNet-50 fine-tuned on VOC07+12.", "description": "This table presents the results of an object detection experiment on the VOC07+12 dataset.  The model used is a ResNet-50, pre-trained on ImageNet100, and fine-tuned for object detection. Different methods are compared, including SimCLR (invariant learning), AugSelf (implicit equivariant learning), EquiMod (explicit equivariant learning), and STL (the authors' proposed self-supervised transformation learning method), both with and without AugMix.  The metrics used are AP (Average Precision), AP50 (Average Precision at IoU threshold of 0.5), and AP75 (Average Precision at IoU threshold of 0.75).  The table demonstrates the performance improvements achieved by STL, especially when combined with AugMix.", "section": "4.2 Analysis"}, {"figure_path": "87AXdbkRyd/tables/tables_6_2.jpg", "caption": "Table 5: Transformation Prediction. Evaluation of transformation representation from learned representation pairs. Regression tasks use MSE loss, and transformation type classification uses accuracy.", "description": "This table presents the results of evaluating the quality of transformation representations learned by different methods.  The evaluation is done using two tasks: regression (measuring the difference between the predicted and actual transformation parameters using Mean Squared Error) and classification (measuring the accuracy of predicting the type of transformation applied). The table shows that STL (Self-supervised Transformation Learning) achieves significantly better performance than the other methods in both tasks, indicating that STL learns more accurate and informative transformation representations.", "section": "4.2 Analysis"}, {"figure_path": "87AXdbkRyd/tables/tables_7_1.jpg", "caption": "Table 6: Transformation Equivariance. Evaluation of the equivariant transformation. Mean Reciprocal Rank (MRR), Hit@k (H@k), and Precision (PRE) metrics on various transformations (crop and color jitter).", "description": "This table presents the performance of the proposed STL model's equivariant transformation.  It compares the Mean Reciprocal Rank (MRR), Hit@1, Hit@5, and Precision (PRE) metrics for crop and color jitter transformations, separately and combined.  The results are compared against three existing equivariant learning methods (SEN, EquiMod, and SIE) and an ablation study where the transformation learning loss (Ltrans) is removed from the STL model.  Higher MRR and H@k indicate better equivariance, while a lower PRE value indicates higher precision in predicting the transformations' parameter vector.", "section": "4.2 Analysis"}, {"figure_path": "87AXdbkRyd/tables/tables_7_2.jpg", "caption": "Table 7: Loss Function Ablation Study. Image classification and transformation prediction results of ResNet-18 pretrained on STL10 with selective inclusion of loss terms for invariant learning (Linv), equivariant learning (Lequi), and self-supervised transformation learning (Ltrans). For image classification, in-domain accuracy (%) and the average accuracy (%) across multiple out-domain datasets are shown. For transformation prediction, MSE is used for regression of crop and color transformations, and accuracy (%) is used for transformation type classification.", "description": "This table presents the ablation study of the three loss functions used in STL: Linv (invariant learning), Lequi (equivariant learning), and Ltrans (self-supervised transformation learning).  It shows the impact of each loss function on both image classification (in-domain and out-domain accuracy) and transformation prediction (regression and classification accuracy) using ResNet-18 model pretrained on STL10 dataset.  The results highlight the contribution of each loss function to the overall performance.", "section": "4.3 Ablation Studies"}, {"figure_path": "87AXdbkRyd/tables/tables_8_1.jpg", "caption": "Table 8: Transformation Ablation Study. Linear evaluation accuracy (%) of ResNet-18 pretrained on STL10 with various transformations used as equivariance targets.", "description": "This table presents the results of an ablation study investigating the impact of different transformations (crop, color, and a combination of both) on the performance of the STL model.  It shows linear evaluation accuracy on various downstream classification tasks (CIFAR10, CIFAR100, Food, MIT67, Pets, Flowers, Caltech101, Cars, Aircraft, DTD, SUN397). The results are broken down for AugSelf, EquiMod, and the proposed STL method to compare their performance under different transformation settings.", "section": "4.3 Ablation Studies"}, {"figure_path": "87AXdbkRyd/tables/tables_8_2.jpg", "caption": "Table 9: Base Invariant Learning Model Ablation Study. Linear evaluation accuracy (%) of ResNet-18 pretrained on STL10 with various base models for invariant learning.", "description": "This table presents the results of an ablation study that investigates the impact of different base invariant learning models on the performance of the proposed Self-Supervised Transformation Learning (STL) method.  The study uses linear evaluation accuracy as the metric across eleven downstream classification tasks (including CIFAR-10, CIFAR-100, Food-101, etc.) to compare STL's performance when integrated with various base models like SimCLR, BYOL, SimSiam, and Barlow Twins.  The results demonstrate STL's flexibility and adaptability by showing consistent improvements in performance across different base models, highlighting its broad applicability.", "section": "4.3 Ablation Studies"}, {"figure_path": "87AXdbkRyd/tables/tables_14_1.jpg", "caption": "Table 2: Out-domain Classification. Evaluation of representation generalizability on the out-domain downstream classification tasks. Linear evaluation accuracy (%) is reported for ResNet-50 pretrained on ImageNet100.", "description": "This table presents the linear evaluation accuracy of ResNet-50, pretrained on ImageNet100, across eleven different downstream classification tasks.  The results are broken down by method (invariant learning techniques like SimCLR, implicit equivariant methods like E-SSL and AugSelf, explicit equivariant methods like SEN, EquiMod, and SIE, and the proposed STL method, both with and without AugMix augmentation). The accuracy for each method is shown for each dataset (CIFAR-10, CIFAR-100, Food-101, MIT67, Pets, Flowers, Caltech-101, Cars, Aircraft, DTD, and SUN397).  This allows for a comparison of the different methods' ability to generalize to unseen datasets.", "section": "4.1 Main Results"}, {"figure_path": "87AXdbkRyd/tables/tables_15_1.jpg", "caption": "Table 2: Out-domain Classification. Evaluation of representation generalizability on the out-domain downstream classification tasks. Linear evaluation accuracy (%) is reported for ResNet-50 pretrained on ImageNet100.", "description": "This table presents the results of evaluating the generalizability of learned representations on various out-of-domain datasets.  It compares different representation learning methods (invariant, implicit equivariant, explicit equivariant, and the proposed STL method) by measuring their linear evaluation accuracy on 11 diverse downstream classification tasks using a ResNet-50 model pre-trained on ImageNet100. The table showcases the performance of each method with and without AugMix augmentation, highlighting the robustness of the different approaches.", "section": "4.1 Main Results"}]