[{"figure_path": "87AXdbkRyd/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization of Equivariant Transformation and Transformation Representation. (Left) UMAP [32] visualizations of functional weights from equivariant transformations implemented with a hypernetwork. EquiMod uses transformation labels to generate these weights, while STL derives them from the representation pairs of transformed and original image. (Right) UMAP visualizations of transformation representations obtained from representation pairs of original input image and transformed input image.", "description": "This figure visualizes the difference between EquiMod and STL in generating equivariant transformations and transformation representations.  UMAP plots show that EquiMod uses transformation labels to generate functional weights for its equivariant transformations, resulting in separate clusters for each transformation type.  In contrast, STL derives its transformation representations from image pairs, resulting in clusters that group similar transformations together, regardless of their individual labels. This highlights STL's ability to learn the relationships between different transformations, even without explicit labels.", "section": "1 Introduction"}, {"figure_path": "87AXdbkRyd/figures/figures_3_1.jpg", "caption": "Figure 2: Transformation Equivariant Learning with Self-supervised Transformation Learning. (Left) The overall framework of STL. For given image and transformations, it demonstrates: 1) transformation invariant learning, which aligns the representations of image and transformed image; 2) transformation equivariant learning, where the representation of image transformed by an equivariant transformation (obtained from the transformation representation of different image with the same applied transformation) aligns with the transformed image's representation; 3) self-supervised transformation learning, which aligns the transformation representations obtained from different image pairs. (Right) It illustrates the transformations of each representation and the equivariant transformations within the representation space.", "description": "This figure illustrates the framework of Self-Supervised Transformation Learning (STL). The left panel shows the overall architecture, highlighting the three main learning objectives: transformation invariant learning (aligning representations of original and transformed images), transformation equivariant learning (aligning transformed image representations with equivariant transformations derived from transformation representations of different images), and self-supervised transformation learning (aligning transformation representations from different image pairs). The right panel visually depicts the transformations applied to images and their representations, and how the equivariant transformations are learned in the representation space.", "section": "3 Equivariant Learning with Self-supervised Transformation Learning"}, {"figure_path": "87AXdbkRyd/figures/figures_4_1.jpg", "caption": "Figure 3: Aligned Transformed Batch. (Left) In self-supervised learning methods, batch compositions typically involve applying two different transformations to each input image. (Right) In STL, batches are composed by pairing two images together, and applying the same transformation pair.", "description": "This figure illustrates the difference in batch composition between standard self-supervised learning methods and the proposed STL method.  In standard methods (left), each image receives two different transformations.  STL (right) pairs images together and applies the same transformation pair to both, enabling the learning of transformation representations.", "section": "3.3 Implementation Details"}, {"figure_path": "87AXdbkRyd/figures/figures_6_1.jpg", "caption": "Figure 4: Visualization of Transformation Representations by Intensity. UMAP visualization of transformation representations organized by intensity levels for each transformation type, including random crop and color jitter variations in brightness, contrast, saturation, and hue. Parameter ranges for each transformation are divided into four segments to apply varying intensities. Representations are captured by a ResNet-18 model pretrained on STL10 with a transformation backbone.", "description": "This figure shows UMAP visualizations of transformation representations. Each point represents a transformation representation, colored and positioned according to the transformation type and intensity level.  The visualization reveals how STL learns to represent transformations, clustering similar transformations together and showing the relationship between intensity and representation.", "section": "4.2 Analysis"}, {"figure_path": "87AXdbkRyd/figures/figures_16_1.jpg", "caption": "Figure 5: Explicit and Implicit Equivariant Learning. Transformation equivariant learning with transformations is divided into (Left) explicit and (Right) implicit equivariant learning.", "description": "This figure illustrates the difference between explicit and implicit equivariant learning methods.  Explicit methods, such as SEN, EquiMod, and SIE, use a dedicated transformation network operating directly on representations, requiring explicit transformation labels for alignment.  In contrast, implicit methods, like E-SSL and AugSelf, infer transformations indirectly without explicit transformation labels; they leverage auxiliary tasks to deduce transformation states based on changes in the representations.", "section": "3 Equivariant Learning with Self-supervised Transformation Learning"}]