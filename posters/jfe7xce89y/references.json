{"references": [{"fullname_first_author": "Robert A. Jacobs", "paper_title": "Adaptive mixtures of local experts.", "publication_date": "1991-01-01", "reason": "This paper introduces the foundational concept of Mixture of Experts (MoE), a crucial component of the FuseMoE model."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.", "publication_date": "2017-01-01", "reason": "This paper presents the sparsely-gated Mixture of Experts (MoE) architecture, a key innovation in scaling large neural networks and a foundation for the FuseMoE model's design."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer.", "publication_date": "2020-01-01", "reason": "This paper introduces the T5 text-to-text transformer model, which is used for text encoding in the multimodal fusion task within the FuseMoE model."}, {"fullname_first_author": "Alex Krizhevsky", "paper_title": "Learning multiple layers of features from tiny images.", "publication_date": "2009-01-01", "reason": "This paper introduces the CIFAR-10 dataset, a benchmark dataset used for evaluating the performance of the FuseMoE model on image classification tasks."}, {"fullname_first_author": "Alistair Johnson", "paper_title": "MIMIC-III, a freely accessible critical care database.", "publication_date": "2016-01-01", "reason": "This paper introduces the MIMIC-III database, one of the key datasets used for evaluating FuseMoE's performance on real-world medical prediction tasks."}]}