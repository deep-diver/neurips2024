[{"figure_path": "qevq3FZ63J/tables/tables_3_1.jpg", "caption": "Table 1: Correlation between the complexity indices and the issue resolution.", "description": "This table presents the correlation coefficients between various complexity metrics (number of files, functions, hunks, added lines of code, deleted lines of code, and changed lines of code) and the success of issue resolution.  The analysis was performed using logistic regression due to the skewed distribution of the data and the binary nature of the outcome (resolved/not resolved).  Asterisks (*) indicate statistically significant correlations (p-value < 0.05). The table shows how the complexity of the code changes impacts the ability of LLMs to resolve GitHub issues.", "section": "4 Experiments and Analysis"}, {"figure_path": "qevq3FZ63J/tables/tables_5_1.jpg", "caption": "Table 1: Correlation between the complexity indices and the issue resolution.", "description": "This table presents the correlation coefficients between various complexity metrics (number of files, functions, hunks, added lines of code, deleted lines of code, and changed lines of code) and the success of issue resolution for three different LLMs (GPT-3.5, GPT-4, and Claude-2).  A negative correlation indicates that as complexity increases, the likelihood of successful issue resolution decreases.  The asterisk (*) indicates statistical significance (p-value < 0.05).", "section": "4 Experiments and Analysis"}, {"figure_path": "qevq3FZ63J/tables/tables_6_1.jpg", "caption": "Table 1: Correlation between the complexity indices and the issue resolution.", "description": "This table presents the correlation coefficients between various complexity indices (number of files, functions, hunks, added lines of code, deleted lines of code, and changed lines of code) and the success rate of issue resolution for three different LLMs (GPT-3.5, GPT-4, and Claude-2).  A statistically significant correlation (p-value < 0.05) is indicated by an asterisk (*). The negative correlations observed suggest that increased code complexity hinders the resolution of issues.", "section": "4 Experiments and Analysis"}, {"figure_path": "qevq3FZ63J/tables/tables_6_2.jpg", "caption": "Table 2: The comparison of overall performance between MAGIS and baselines on SWE-bench.", "description": "This table presents a comparison of the overall performance of the MAGIS framework against several baseline LLMs on the SWE-bench dataset.  The performance is measured by two metrics: the percentage of instances where the code change was successfully generated and applied (% Applied) and the percentage of instances where the applied code change successfully resolved the issue (% Resolved).  The table also shows the effect of ablations on the MAGIS framework, specifically removing the QA Engineer agent and/or hints from the workflow, to demonstrate the contribution of each component.", "section": "4.2 How Effective is Our Framework? (RQ 2)"}, {"figure_path": "qevq3FZ63J/tables/tables_8_1.jpg", "caption": "Table 1: Correlation between the complexity indices and the issue resolution.", "description": "This table presents the correlation coefficients between several complexity indices and the success of issue resolution for three different LLMs (GPT-3.5, GPT-4, and Claude-2).  The complexity indices include the number of files, functions, hunks, added lines of code, deleted lines of code, and changed lines of code. A statistically significant correlation (p-value < 0.05) is indicated with an asterisk. The negative correlations suggest that increased complexity tends to make issue resolution more difficult.", "section": "4 Experiments and Analysis"}, {"figure_path": "qevq3FZ63J/tables/tables_17_1.jpg", "caption": "Table 4: The comparison of overall performance between MAGIS and baselines on SWE-bench lite.", "description": "This table compares the performance of the MAGIS framework with several baselines on the SWE-bench lite dataset.  The performance is measured by the percentage of GitHub issues resolved. The table shows that MAGIS outperforms other methods, achieving a higher resolved ratio (25.33%).  Ablation studies show the impact of different components of MAGIS, such as the QA Engineer and hints, on the overall performance.", "section": "Comparison Result on SWE-bench Lite"}, {"figure_path": "qevq3FZ63J/tables/tables_20_1.jpg", "caption": "Table 5: The statistical analysis of our framework on resolved and applied but not resolved instances.", "description": "This table presents a statistical comparison between the code changes generated by the MAGIS framework and the ground truth (gold standard) for both resolved and unresolved instances from the SWE-bench dataset.  It provides metrics on the number of code files, functions, hunks, added lines of code (LoC), deleted lines of code, and the start and end indices of the code changes. Minimum, maximum, and average values are given for each metric, offering insights into the complexity of code modifications handled by the system for both successful and unsuccessful resolutions.", "section": "F Statistics on the Generated Code Changes"}, {"figure_path": "qevq3FZ63J/tables/tables_20_2.jpg", "caption": "Table 5: The statistical analysis of our framework on resolved and applied but not resolved instances.", "description": "This table presents a statistical comparison between the resolved and unresolved instances processed by the MAGIS framework.  For each category (resolved and unresolved), it shows the minimum, maximum, and average values for several key metrics including: number of code files, number of functions, number of hunks, number of added lines of code (LoC), number of deleted lines of code (LoC), the starting line index of the code change, the ending line index of the code change, and the total number of lines changed.  The data provides insights into the complexity of code changes in successful versus unsuccessful resolutions, highlighting the challenges faced in complex scenarios and suggesting areas for potential improvement.", "section": "4.4 How Effective is Our Coding Process? (RQ 4)"}]