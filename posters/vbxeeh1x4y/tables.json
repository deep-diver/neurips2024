[{"figure_path": "vBxeeH1X4y/tables/tables_1_1.jpg", "caption": "Figure 1: Comparison of data valuation and joint valuation. (a) Data valuation evaluates the quality of individual data points, whereas (b) joint valuation evaluates the quality of individual cells. Both panels illustrate the same hypothetical dataset, while darker colors indicate higher quality or importance. As illustrated in panel (a), data valuation can only identify that the third and fifth data points are of low quality, but it lacks further feature-level attribution. This limitation may result in discarding the entire data point, even when only certain cells are problematic. In contrast, joint valuation provides a finer level of attribution than data valuation and aims to reveal how individual features contribute to data values. As shown in panel (b), the joint valuation framework can identify outlier cells (highlighted by blue boxes), such as -1 in \"Income\" and 100 in \"Education\", providing detailed interpretations of data values.", "description": "This figure compares data valuation and joint valuation methods on a hypothetical dataset.  Data valuation assigns a single score to each data point, while joint valuation assigns scores to individual features within each data point, enabling detection of outliers at the feature level.", "section": "Abstract"}, {"figure_path": "vBxeeH1X4y/tables/tables_1_2.jpg", "caption": "Figure 1: Comparison of data valuation and joint valuation. (a) Data valuation evaluates the quality of individual data points, whereas (b) joint valuation evaluates the quality of individual cells. Both panels illustrate the same hypothetical dataset, while darker colors indicate higher quality or importance. As illustrated in panel (a), data valuation can only identify that the third and fifth data points are of low quality, but it lacks further feature-level attribution. This limitation may result in discarding the entire data point, even when only certain cells are problematic. In contrast, joint valuation provides a finer level of attribution than data valuation and aims to reveal how individual features contribute to data values. As shown in panel (b), the joint valuation framework can identify outlier cells (highlighted by blue boxes), such as -1 in \"Income\" and 100 in \"Education\", providing detailed interpretations of data values.", "description": "This figure compares data valuation and joint valuation methods on a simple hypothetical dataset.  Data valuation assigns a single score to each data point, masking the quality variations within the point's individual cells. Joint valuation, conversely, assigns scores to individual cells, revealing which specific cells are problematic. The example highlights how joint valuation can identify outlier cells within an otherwise good data point, preventing the discarding of potentially useful data.", "section": "Abstract"}, {"figure_path": "vBxeeH1X4y/tables/tables_5_1.jpg", "caption": "Table 1: Cell-level outlier detection results. AUC and run-time comparison between 2D-00B and 2D-KNN across twelve binary classification datasets. The average and standard error of the AUC and run-time (in seconds) based on 30 independent experiments are denoted by \"average \u00b1 standard error\". Bold numbers denote the best method. The AUC value for the Random method consistently remains at 0.5 across all datasets. In every dataset, 2D-00B achieves a significantly higher AUC while being orders of magnitude faster than 2D-KNN.", "description": "This table presents a comparison of the Area Under the Curve (AUC) and runtime for 2D-00B and 2D-KNN across twelve binary classification datasets.  The results demonstrate that 2D-00B significantly outperforms 2D-KNN in terms of AUC, while also being substantially faster.  The table includes the average and standard error for both metrics, based on 30 independent experimental runs, highlighting the statistical significance of the findings.", "section": "4.1 Cell-level outlier detection"}, {"figure_path": "vBxeeH1X4y/tables/tables_8_1.jpg", "caption": "Table 2: Ablation study results of weak learner types. The average and standard error of the detection AUC based on 30 independent experiments are denoted by \"average \u00b1 standard error\". Results from 2D-KNN are included for comparison. The choice of weak learner leads to variations in cell values, yet the performance of the detection task remains robust.", "description": "This table presents the Area Under the Curve (AUC) for cell-level outlier detection using different types of weak learners in the 2D-OOB model.  It shows that the choice of weak learner (Decision Tree, Logistic Regression, single-layer MLP, two-layer MLP) has a relatively small impact on the overall AUC performance, demonstrating robustness.  The 2D-KNN baseline's performance is also included for comparison.", "section": "4.4 Ablation study"}, {"figure_path": "vBxeeH1X4y/tables/tables_8_2.jpg", "caption": "Table 2: Ablation study results of weak learner types. The average and standard error of the detection AUC based on 30 independent experiments are denoted by \"average \u00b1 standard error\". Results from 2D-KNN are included for comparison. The choice of weak learner leads to variations in cell values, yet the performance of the detection task remains robust.", "description": "This table presents the results of an ablation study on the choice of weak learners used in the 2D-OOB model for cell-level outlier detection.  It compares the Area Under the Curve (AUC) of the precision-recall curve for different types of weak learners (decision tree, logistic regression, single-layer MLP, two-layer MLP) across 12 datasets.  The table shows that while different learners lead to slight variations in cell valuations, the overall performance of the detection task remains robust, and 2D-OOB consistently outperforms the 2D-KNN baseline.", "section": "4.4 Ablation study"}, {"figure_path": "vBxeeH1X4y/tables/tables_14_1.jpg", "caption": "Table 4: A summary of all the datasets used in 4.1, 4.2, and Appendix D. These datasets have been commonly used in previous literature [12, 23, 24]", "description": "This table lists twelve binary classification datasets from OpenML used in the paper's experiments. For each dataset, it provides the total number of samples, the input dimension (number of features), the majority class proportion, and the OpenML ID.", "section": "A.1 Datasets"}, {"figure_path": "vBxeeH1X4y/tables/tables_14_2.jpg", "caption": "Table 5: Cell-level outlier detection results on multi-class classification datasets. The average and standard error of the AUC and run-time (in seconds) based on 30 independent experiments are denoted by \"average \u00b1 standard error\".", "description": "This table presents the results of cell-level outlier detection experiments performed on three multi-class classification datasets.  The Area Under the Curve (AUC) and runtime are reported for both the proposed 2D-OOB method and the baseline 2D-KNN method.  The results are averaged over 30 independent experiments, with standard errors provided to indicate variability.", "section": "4.1 Cell-level outlier detection"}, {"figure_path": "vBxeeH1X4y/tables/tables_15_1.jpg", "caption": "Table 6: Cell-level outlier detection results (AUC) of 2D-00B and the two-stage attribution. Our method shows a better performance than the alternative method by a significant performance margin.", "description": "This table compares the Area Under the Curve (AUC) for cell-level outlier detection between the proposed 2D-OOB method and a two-stage attribution approach.  The two-stage method first computes a data valuation score for each data point and then uses feature attribution to estimate individual cell-level importance. The results show that 2D-OOB consistently achieves higher AUC values across multiple datasets, demonstrating its superior performance for cell-level outlier identification.", "section": "4.1 Cell-level outlier detection"}, {"figure_path": "vBxeeH1X4y/tables/tables_15_2.jpg", "caption": "Table 1: Cell-level outlier detection results. AUC and run-time comparison between 2D-OOB and 2D-KNN across twelve binary classification datasets. The average and standard error of the AUC and run-time (in seconds) based on 30 independent experiments are denoted by \"average \u00b1 standard error\". Bold numbers denote the best method. The AUC value for the Random method consistently remains at 0.5 across all datasets. In every dataset, 2D-OOB achieves a significantly higher AUC while being orders of magnitude faster than 2D-KNN.", "description": "This table presents a comparison of the Area Under the Curve (AUC) and runtime for 2D-OOB and 2D-KNN algorithms on twelve binary classification datasets.  The AUC measures the algorithms' ability to detect cell-level outliers, with higher values indicating better performance. Runtime is measured in seconds.  The results show that 2D-OOB significantly outperforms 2D-KNN in AUC while being much faster.", "section": "4.1 Cell-level outlier detection"}, {"figure_path": "vBxeeH1X4y/tables/tables_16_1.jpg", "caption": "Table 1: Cell-level outlier detection results. AUC and run-time comparison between 2D-OOB and 2D-KNN across twelve binary classification datasets. The average and standard error of the AUC and run-time (in seconds) based on 30 independent experiments are denoted by \"average \u00b1 standard error\". Bold numbers denote the best method. The AUC value for the Random method consistently remains at 0.5 across all datasets. In every dataset, 2D-OOB achieves a significantly higher AUC while being orders of magnitude faster than 2D-KNN.", "description": "This table presents a comparison of the Area Under the Curve (AUC) and runtime for 2D-OOB and 2D-KNN across 12 binary classification datasets.  The AUC measures the performance of each method in detecting cell-level outliers, while the runtime reflects computational efficiency.  Results show that 2D-OOB consistently achieves a higher AUC (better outlier detection) and significantly shorter runtime (faster computation) than 2D-KNN across all datasets.", "section": "4.1 Cell-level outlier detection"}, {"figure_path": "vBxeeH1X4y/tables/tables_19_1.jpg", "caption": "Table 8: Point-level mislabeled data detection results. AUCPR of different data valuation and (marginalized) joint valuation methods. The average and standard error of the AUCPR based on 30 independent experiments are denoted by \"average \u00b1 standard error\". Bold numbers denote the best method, for data valuation and joint valuation respectively. The AUCPR value for the Random method consistently remains at 0.5 across all datasets. 2D-00B-data exhibits performance comparable to Data-00B, while significantly surpassing 2D-KNN-data (the marginalization of 2D-KNN) and all other data valuation methods.", "description": "This table presents the AUCPR (Area Under the Precision-Recall Curve) for various data valuation methods in detecting mislabeled data points.  It compares the performance of 2D-00B-data (a marginalized version of the proposed 2D-00B method) against other established methods such as DataShapley, KNNShapley, Data-OOB, LAVA, and DataBanzhaf.  The results show that 2D-00B-data achieves comparable performance to Data-OOB while significantly outperforming other methods.", "section": "D.1 Mislabeled data detection"}]