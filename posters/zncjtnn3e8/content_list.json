[{"type": "text", "text": "Compositional PAC-Bayes: Generalization of GNNs with persistence and beyond ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kirill Brilliantov Amauri H. Souza Vikas Garg ETH Z\u00fcrich Federal Institute of Cear\u00e1 YaiYai Ltd & Aalto University kbrilliantov@ethz.ch amauriholanda@ifce.edu.br vgarg@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Heterogeneity, e.g., due to different types of layers or multiple sub-models, poses key challenges in analyzing the generalization behavior of several modern architectures. For instance, descriptors based on Persistent Homology (PH) are being increasingly integrated into Graph Neural Networks (GNNs) to augment them with rich topological features; however, the generalization of such PH schemes remains unexplored. We introduce a novel compositional PAC-Bayes framework that provides a general recipe to analyze a broad spectrum of models including those with heterogeneous layers. Specifically, we provide the first data-dependent generalization bounds for a widely adopted PH vectorization scheme (that subsumes persistence landscapes, images, and silhouettes) as well as PH-augmented GNNs. Using our framework, we also obtain bounds for GNNs and neural nets with ease. Our bounds also inform the design of novel regularizers. Empirical evaluations on several standard real-world datasets demonstrate that our theoretical bounds highly correlate with empirical generalization performance, leading to improved classifier design via our regularizers. Overall, this work bridges a crucial gap in the theoretical understanding of PH methods and general heterogeneous models, paving the way for the design of better models for (graph) representation learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Topological data analysis (TDA) harnesses tools from algebraic topology to unveil the underlying shape and structure of data. TDA has recently gained significant traction within machine learning mainly due to its flagship method: persistent homology (PH) [11], which allows for capturing topological invariants (like connected components and loops) of the input domain at multiple scales. In particular, PH has recently been leveraged as a tool to augment the representational capabilities of graph neural networks (GNNs) [20, 53, 58], with expressivity gains formally established [22, 44]. Intuitively, PH furnishes global structural signatures that complement the local nature of GNNs [5, 18, 20, 57]. ", "page_idx": 0}, {"type": "text", "text": "Understanding the generalization behavior of these models is crucial as it plays a pivotal role in ensuring their reliability and applicability [41]. In this context, there are two fundamental approaches to achieving generalization bounds: data-independent and data-dependent [47], each offering unique insights into the generalization problem. Both these approaches have been investigated to analyze the generalization ability of GNNs [12, 14, 25, 33, 36, 46, 48, 52, 59]. Data-dependent generalization bounds evoke particular interest since they are typically much tighter than the agnostic bounds afforded by, e.g., VC dimension. However, no such bounds have been unearthed for PH methods (i.e., learnable vectorization schemes) and, consequently, for GNNs enhanced with PH-based descriptors. ", "page_idx": 0}, {"type": "text", "text": "We approach this gap with the first data-dependent generalization bound for classifiers based on a versatile and widely used vectorization framework for persistence diagrams, namely, PersLay [5]. PersLay leverages extended persistence to effectively represent detailed topological features, and subsumes commonly used methods such as persistence landscapes [4], images [1], and silhouettes [6]. ", "page_idx": 0}, {"type": "text", "text": "Central to our analysis is a novel PAC-Bayes framework (Lemma 2) that provides a general recipe to analyze the generalization of a broad spectrum of models, including those with heterogeneous layers and those comprising multiple sub-models. To achieve this, we introduce general conditions (Equations 6-9) that are satisfied by commonly used learning architectures and, surprisingly, their compositions (Section 4). Leveraging Lemma 2, we show how to obtain bounds for heterogeneous MLPs and GNNs in a straightforward manner (Table 2). Notably, we also establish the first generalization bounds for GNNs augmented with persistence layers (PersLay). ", "page_idx": 1}, {"type": "text", "text": "Our exposition focuses on graphs; however, i) our Lemma 2 can be used in any domain and ii) our bound for PersLay considers persistence diagrams obtained from any non-learnable filtration function, and therefore extends more generally to input domains beyond graphs. From a technical perspective, our approach hinges on contrasting previous analyses within the PAC-Bayes framework [9, 37, 38] to extract the common structure encoded in the general conditions of Lemma 2. This ", "page_idx": 1}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/300fdc92a35fec99e472c87b4033e4c710a65e764b328f19c82ccf04866ce1d0.jpg", "table_caption": ["Table 1: Main theoretical contributions of this work. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "allows us to overcome challenges arising from the heterogeneity of the models we consider. ", "page_idx": 1}, {"type": "text", "text": "Our experiments on several standard real-world datasets confirm strong correlation between the empirical performance and our theoretical bounds. We reinforce the merits of our analysis via regularized PH-based models informed by our bounds with demonstrable empirical benefits. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "(Theoretical, see Table 1) We develop a general recipe for obtaining PAC-Bayes bounds for a broad class of (possibly heterogeneous) models and their compositions. We also provide the first data-dependent bounds for PH-based classifiers and combinations of GNNs and PersLay; ", "page_idx": 1}, {"type": "text", "text": "(Empirical) We show that the dependence on parameters depicted in our bounds strongly correlates with the observed performance. We also show that novel regularization schemes based on our bounds can reduce the generalization gap of PH-augmented GNNs on multiple datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section overviews GNNs, persistent homology on graphs, and their combination. We also provide basic notions and results in PAC-Bayes learning, which serve as a background for this work. ", "page_idx": 1}, {"type": "text", "text": "Notation. We consider attributed graphs denoted as a tuple $G=(V,E,z)$ , where $V=\\{1,2,...,n\\}$ is the vertex set, $E\\subseteq V\\times V$ is the edge set, and $z:V\\rightarrow\\mathbb{R}^{d_{z}}$ assigns to each vertex $v\\in V$ an attribute (or color) $z(v)$ . For convenience, hereafter, we denote the feature vector of $v$ by $z_{v}$ . We consider classification tasks with input and label spaces $\\mathcal{X}$ and $\\mathcal{V}=\\{1,\\ldots,K\\}$ ( $K$ is the number of classes) and the $\\gamma$ -margin loss $l_{\\gamma}:\\dot{\\mathcal{D}}\\times\\mathbb{R}^{K}\\rightarrow\\{0,1\\}$ where $l_{\\gamma}(y,\\hat{y})=\\mathbf{1}(\\hat{y}_{y}\\leq\\gamma+\\operatorname*{max}_{j\\neq y}\\hat{y}_{j})$ and $\\gamma\\geq0$ is the margin parameter. Let $\\boldsymbol{S}=\\{(\\boldsymbol{x}_{i},y_{i})\\}_{i=1}^{m}\\subseteq\\boldsymbol{\\mathcal{X}}\\times\\boldsymbol{\\mathcal{Y}}$ denote a collection of $m$ input/label pairs sampled i.i.d. from some unknown distribution $\\mathcal{D}$ . Then, the empirical error of a hypothesis $g_{\\mathbf{w}}:\\mathcal{X}\\xrightarrow{}\\mathbb{R}^{K}$ with parameters w is defined as $\\begin{array}{r}{\\hat{L}_{S,\\gamma}(g_{\\mathbf{w}})=1/m\\sum_{i=1}^{m}l_{\\gamma}(y_{i},g_{\\mathbf{w}}(x_{i}))}\\end{array}$ . Accordingly, we can define the generalization error as $L_{\\mathcal{D},\\gamma}(g_{\\mathbf{w}})=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[l_{\\gamma}(y,g_{\\mathbf{w}}(x))]$ . We use $\\|\\cdot\\|_{2}$ to refer the $\\ell_{2}$ norm (vectors) and the spectral norm (matrices), and $\\|\\cdot\\|_{F}$ to refer to the Frobenius norm. Also, we denote the set $\\{1,...,n\\}$ by $[n]$ . We provide a notation table in the Appendix (Table 5). ", "page_idx": 1}, {"type": "text", "text": "Graph neural networks (GNNs). Message-passing GNNs [15, 55] employ a sequence of messagepassing steps, where each node $v$ aggregates messages from its neighbors $\\dot{N}(v)\\stackrel{=}{=}\\{u:(v,u)\\in\\dot{E}\\}$ and use the resulting vector to update its own embedding. Starting from $z_{v}^{(0)}=z_{v}$ , GNNs recursively apply ", "page_idx": 1}, {"type": "equation", "text": "$$\nz_{v}^{(\\ell+1)}=\\mathrm{Upd}_{\\ell}\\left(z_{v}^{(\\ell)},\\mathrm{Agg}_{\\ell}(\\{z_{u}^{(\\ell)}:u\\in N(v)\\mathbb{j}\\})\\right)\\qquad\\forall v\\in V,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\{\\!\\!\\left\\{\\cdot\\right\\}\\!\\!\\}$ denotes a multiset, ${\\mathrm{Agg}}_{\\ell}$ is an order-invariant function and ${\\mathrm{Upd}}_{\\ell}$ is an arbitrary update function \u2014 often a multilayer perceptron (MLP). ", "page_idx": 1}, {"type": "text", "text": "Persistence homology (PH) on graphs aims to extract detailed (multiscale) topological features from graphs. A filtration of a graph $G$ is a finite nested sequence of subgraphs of $G$ , i.e., $\\emptyset\\,=$ $G_{0}\\,\\subset\\,G_{1}\\,\\subset\\,...\\,\\subset\\,G$ \u2014 alternatively, clique complexes can also be built at each step (see [2]). While filtrations can be obtained in different ways [2, 18], a typical choice consists of leveraging a real-valued filtering function $f$ on the vertices of $G$ (or their features) to compute the vertex level set $V_{\\alpha}=\\{v:f(v)\\leq\\bar{\\alpha}\\}$ at scale $\\alpha\\in\\mathbb{R}$ . Let $G_{\\alpha}$ be the subgraph of $G$ induced by $V_{\\alpha}$ . By increasing $\\alpha$ from $-\\infty$ to $\\infty$ , we obtain a nested sequence of subgraphs called the sub-level flitration of $G$ induced by $f$ . The idea of PH is to keep track of the appearance and disappearance of topological features (e.g., connected components, loops) in a filtration. If a topological feature first appears in $G_{\\alpha_{b}}$ and disappears in $G_{\\alpha_{d}}$ , then we encode its persistence as a pair $(\\alpha_{b},\\alpha_{d})$ ; if a feature does not disappear, then its persistence is $(\\alpha_{b},\\infty)$ . The collection of all pairs forms a multiset that we call persistence diagram. We use $\\textrm{D g}_{i}(G)$ to denote the persistence diagram for $i$ -dim topological features of graph $G$ . For details on PH, we refer to Edelsbrunner and Harer [10], Hensel et al. [17], and Hofer et al. [19]. ", "page_idx": 2}, {"type": "text", "text": "Persistence layers (PersLay). Carri\u00e8re et al. [5] introduced a general way to vectorize persistence diagrams. Given a persistence diagram $\\operatorname{Dg}(G)$ for an arbitrary dimension, PERSLAY computes ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\operatorname{PERSLAY}}(\\operatorname{Dg}(G))=\\operatorname{Agg}\\left(\\left\\{\\omega(p)\\varphi(p)\\mid p\\in\\operatorname{Dg}(G)\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where Agg is any permutation invariant operation (e.g., minimum, maximum, sum, or kth largest value), $\\omega:\\mathbb{R}^{2}\\overset{.}{\\mapsto}\\mathbb{R}$ is a weight function for the elements in $\\mathrm{Dg}(G)$ , and $\\varphi:\\mathbb{R}^{2}\\mapsto\\mathbb{R}^{h}$ is the so-called point transformation. More specifically, given a persistence pair $p\\,=\\,[p_{1},p_{2}]^{\\top}\\,\\in\\,\\mathbb{R}^{2}$ , PersLay introduces the triangle point transformation $(\\Lambda)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varphi_{\\Lambda}(p)=[\\Lambda_{p}(t_{1}),...,\\Lambda_{p}(t_{h})]^{\\top}\\quad\\mathrm{~with~}\\quad\\Lambda_{p}(t_{i})=\\operatorname*{max}\\{0,p_{2}-|t_{i}-p_{1}|\\},t_{i}\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "the Gaussian point transformation $(\\Gamma)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varphi_{\\Gamma}(p)=[\\Gamma_{p}(t_{1}),...,\\Gamma_{p}(t_{h})]^{\\top}\\quad\\mathrm{~with~}\\quad\\Gamma_{p}(t_{i})=\\exp\\left(-\\frac{\\|t_{i}-p\\|_{2}^{2}}{2\\tau^{2}}\\right),t_{i}\\in\\mathbb{R}^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the line point transformation $(\\Psi)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varphi_{\\Psi}(p)=[\\Psi_{p}(t_{1}),...,\\Psi_{p}(t_{h})]^{\\top}\\quad\\mathrm{~with~}\\quad\\Psi_{p}(t_{i})=t_{i,1}p_{1}+t_{i,2}p_{2}+t_{i,3},t_{i}\\in\\mathbb{R}^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In all of these transformations $t_{1},\\ldots,t_{h}$ are learnable parameters. Notably, the architectural design of PERSLAY is quite versatile and accommodates a wide range of traditional persistence diagram vectorizations, extending DeepSets [56] and including persistence landscapes [4], persistence silhouette [6], persistence images [1], and other Gaussian-based kernel approaches [23, 29, 31]. ", "page_idx": 2}, {"type": "text", "text": "To obtain class predictions, the output of PersLay is typically fed to an MLP with Lipschitz activations.   \nWe refer to this joint model (PersLay followed by MLP) as PersLay Classifier (PC). ", "page_idx": 2}, {"type": "text", "text": "GNNs with persistence. Recently, PH has been used to boost the expressive power of GNNs. For instance, Horn et al. [20] and Immonen et al. [22] leverage node embeddings at each layer of a GNN to obtain persistence descriptors. This topological information can be added to GNN\u2019s node embeddings (as in [20]) or concatenated with GNN\u2019s graph-level representation (as in [22]) \u2014 which we refer to the parallel mode of integrating PH into GNNs. Figure 1 illustrates the latter (parallel mode) with persistence diagrams vectorized using PersLay. ", "page_idx": 2}, {"type": "image", "img_path": "ZNcJtNN3e8/tmp/09ba8f16ea5454fd1633386253776de6c212aeb3d25842e322954c668c448f2c.jpg", "img_caption": ["Figure 1: GNNs with persistence (parallel mode). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "PAC-Bayesian analysis adopts a Bayesian approach to the PAC learning framework [30, 37, 38, 50]. The idea consists of placing a prior distribution $\\mathcal{P}$ over our hypothesis class and then use the training data to obtain a posterior $\\mathcal{Q}_{\\mathrm{~\\,~}}$ , i.e., the learning process induces a posterior distribution over the hypothesis class. Importantly, we can leverage the Kullback-Leibler (KL) divergence between $\\mathcal{Q}$ and $\\mathcal{P}$ to bound the difference between the generalization and empirical errors [37]. To compute PACBayes bounds for models like neural networks, we can i) choose a prior, ii) apply a learning algorithm; and iii) add random perturbations (from some known distribution) to the learned parameters such that we ensure tractability of the KL divergence. Following this recipe, Neyshabur et al. [40] introduced the important result in Lemma 1. Notably, Lemma 1 tells us that if we have prior and posterior distributions and guarantees that the change of the model\u2019s output due to perturbations over the learned parameters is small with high probability, we can obtain a generalization bound. ", "page_idx": 2}, {"type": "text", "text": "Lemma 1 (Neyshabur et al. [40]). Let $g_{\\mathbf{w}}(x)\\ :\\ x\\ \\rightarrow\\ \\mathbb{R}^{K}$ be any model with parameters w, and let $\\mathcal{P}$ be any distribution on the parameters that is independent of the training data. For any w, we construct a posterior $\\mathcal{Q}(\\mathbf{w_{\\mathbf{\\mu}}}+\\mathbf{u})$ by adding any random perturbation u to w, s.t., $\\begin{array}{r}{\\mathbb{P}_{\\mathbf{u}}(\\operatorname*{max}_{x\\in\\mathcal{X}}|g_{\\mathbf{w}+\\mathbf{u}}(x)-\\,g_{\\mathbf{w}}(x)|_{\\infty}\\,<\\,\\frac{\\gamma}{4})\\;>\\;\\frac{1}{2}}\\end{array}$ . Then, for any $\\gamma,\\delta~>~0$ , with probability at least $1-\\delta$ over an i.i.d. size-m training set $\\boldsymbol{S}$ drawn according to $\\mathcal{D}$ , for any w, we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\mathcal{D},0}(g_{\\mathbf{w}})\\leq\\hat{L}_{\\mathcal{S},\\gamma}(g_{\\mathbf{w}})+4\\sqrt{\\frac{D_{K L}(\\mathcal{Q}(\\mathbf{w}+\\mathbf{u})||\\mathcal{P})+\\log\\frac{6m}{\\delta}}{m-1}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Generalized PAC-Bayes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section first presents a general procedure for obtaining generalization bounds for heterogeneous models, i.e., going beyond spectrally-normalized layers and architecture-specific models (as in [33, 40]). Then, we show how to leverage such a procedure to extend existing bounds in the literature and to obtain the first generalization bound for PersLay. ", "page_idx": 3}, {"type": "text", "text": "Our next result (Lemma 2) applied perturbation-based PAC-Bayes bounds to arbitrary models with (possibly) non-homogeneous layers. To achieve this generality, we carefully contrasted results in [33, 40, 48] to identify the conditions (Equations 6, 7, 8, and 9) that are sufficient to subsume the considered models as well as to extend to a broader class of models. In Section 4, we will also exploit Lemma 2 in the analysis of different combinations of neural models (e.g., GNNs and PersLay). ", "page_idx": 3}, {"type": "text", "text": "General recipe for PAC-Bayesian bounds for heterogeneous models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Lemma 2. Let $f_{\\mathbf{w}}:\\mathcal{X}\\mapsto\\mathbb{R}^{K}$ be a model with parameters $\\mathbf{w}=v e c\\{W_{1},...,W_{n}\\}$ . If there exists $T\\in\\mathbb{R}^{+}$ and a sequence $(S_{i})_{i\\in[n]}$ with $S_{i}\\in\\mathbb{R}^{+}$ both of which may depend on w, and parameter-independent $C_{1},C_{2}\\in\\mathbb{R}^{+}$ and sequence $(\\eta_{i})_{i\\in[n]}$ with $\\eta_{i}\\in(0,1]$ such that: ", "page_idx": 3}, {"type": "text", "text": "\u2022 the output is bounded by $C_{1}T$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{x\\in\\mathcal{X}}{\\operatorname*{sup}}\\,\\|f_{\\mathbf{w}}(x)\\|_{2}\\leq C_{1}T,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 the output change can be bounded under a small perturbation of the parameters, i.e., for all ${\\bf u}=v e c\\{U_{1},...,U_{n}\\}$ with $\\|U_{i}\\|_{2}\\le\\eta_{i}S_{i}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathcal{X}}\\|f_{\\mathbf{w}+\\mathbf{u}}(x)-f_{\\mathbf{w}}(x)\\|_{2}\\leq C_{2}T\\sum_{i=1}^{n}\\frac{\\|U_{i}\\|_{2}}{S_{i}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 the following auxiliary conditions hold: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{1}{n}\\left(\\sum_{i=1}^{n}\\frac{1}{S_{i}}\\right)\\geq\\frac{1}{T^{1/n}},}\\\\ &{}&{\\displaystyle\\bar{\\eta}:=\\operatorname*{min}_{1\\leq i\\leq n}\\eta_{i}\\leq\\frac{C_{1}}{2C_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, for any $\\gamma,\\delta>0$ with probability at least $1-\\delta$ over the choice of training sets $\\boldsymbol{S}$ with $m$ i.i.d. samples drawn according to some distribution $\\mathcal{D}$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{D},0}(f_{\\mathbf{w}})\\leq\\hat{L}_{S,\\gamma}(f_{\\mathbf{w}})+}\\\\ &{\\,\\,\\,+\\,\\mathcal{O}\\left(\\sqrt{\\frac{\\operatorname*{max}\\{1,\\|\\mathbf{w}\\|_{2}^{2}\\}T^{2}\\left(\\underset{i=1}{\\overset{n}{\\sum}}\\frac{1}{S_{i}}\\right)^{2}h\\ln\\left(n h\\right)C_{1}^{2}\\bar{\\eta}^{-2}+\\log\\operatorname*{max}\\left\\{\\frac{m}{\\delta},\\frac{m}{\\delta C_{1}}\\right\\}}{\\gamma^{2}m}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $h$ is the maximum dimension across the matrices $(W_{i})_{i\\in[n]}$ ", "page_idx": 3}, {"type": "text", "text": "Proof sketch. We build on the main result in [40] by extending it to a broader context. The general idea involves employing Lemma 1. Following [40], we define the prior distribution $\\mathcal{P}$ as an isotropic Gaussian with variance $\\sigma^{2}$ and the posterior distribution $\\mathcal{Q}$ as a shifted isotropic Gaussian with the same variance. To achieve a tighter bound, it is essential to maximize $\\sigma$ (since the KL-divergence scales as ${\\mathcal{O}}{\\left({1}/{\\sigma^{2}}\\right)})$ ); consequently, $\\sigma$ should be determined based on the parameter $\\beta$ . However, since $\\mathcal{P}$ must remain independent of the learned weights, we set $\\sigma$ according to an approximation of the learned weights. Specifically, we define $\\begin{array}{r}{\\beta=T\\sum_{i}{^{1/}S_{i}}}\\end{array}$ and at first consider only w such that $\\beta$ fall within the range $|\\beta-\\tilde{\\beta}|\\leq1/2\\beta$ for some arbitrary $\\tilde{\\beta}$ , an approximation. We then select $\\sigma$ based on this approximation, $\\tilde{\\beta}$ . At this point we can apply Lemma 1 for all w such that $\\beta$ falls into the defined earlier interval. To account for other values of $\\beta$ , we establish a finite grid across the relevant $\\beta$ values and choose an appropriate $\\tilde{\\beta}$ for each interval on the grid. Finally, a union-bound argument across all $\\tilde{\\beta}$ values provides the final result. Although Equation 7 and Lemma 1 have their own constraints on the random perturbation, the above steps outline a method to set the variance $\\sigma$ that satisfies these constraints and maintains independence from the learned weights. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Table 2: Application of Lemma 2 to MLPs and GNNs. The detailed proof of the lemma applicability can be found in the Appendix $\\mathrm{E}$ and the detailed description of the models in Appendix B. Here we provide brief description. We consider $n$ -layer multilayer perceptron (MLP) with weights $W_{1},...,W_{n}$ . After layer $i$ we apply $\\mathrm{Lip}_{i}$ -Lipschitz activation function for $i\\in[n-1]$ . Every input is contained in $\\ell_{2}$ -ball of radius $B$ . We consider $n$ -layer GCN with weights $W_{1},...,W_{n}$ . After layer $i$ we apply $\\mathrm{Lip}_{i}$ -Lipschitz activation function. Every node feature of the graph is contained in $\\ell_{2}$ -ball of radius $B$ and the maximum degree of the node is $d-1$ . We denote $\\mathrm{Lip}=\\mathrm{Lip}_{1}\\cdot\\ldots\\cdot\\mathrm{Lip}_{n-1}$ . We consider $n$ -layer $(n>2)$ ) MPGNN with weights $W_{1},W_{2},W_{3}$ with activation functions $g,\\phi,\\rho$ with corresponding Lipschitz constants. We denote $\\mathcal{C}=\\mathrm{Lip}_{\\phi}\\mathrm{Lip}_{g}\\mathrm{Lip}_{\\rho}\\|W_{2}\\|$ , $\\lambda=\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}$ and $\\xi={\\left((d\\mathcal{C})^{n-1}\\!-\\!1\\right)}\\Big/(d\\mathcal{C}\\!-\\!1)$ . Comparing to [33] we do not add $\\underline{{\\mathrm{Lip}_{\\phi}}}$ to $\\xi$ and instead of $W_{l}$ we have $W_{3}$ . ", "page_idx": 4}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/9c3a333f9d426db55cbabca81c35c14e87019abd74f0576fb70e6f173ad19be9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n^{*}S_{2}=\\operatorname*{min}\\{d\\mathcal{C},\\|W_{2}\\|_{2}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Discussion. We note that Lemma 2 requires choosing values for the variables $(S_{i})_{i\\in[n]}$ and $T$ . In this regard, one might set $S_{i}$ as the spectral norm of the weight matrix $W_{i}$ , i.e., $S_{i}=\\lVert[W_{i}\\rVert_{2}$ , and make $T$ equal to $\\Pi_{i}\\,S_{i}$ . In this case, the condition in Equation 8 is satisfied \u2014 the geometric mean is always smaller than or equal to the arithmetic mean. Regarding the variables $(\\eta_{i})_{i}$ , a typical choice is to set $\\eta_{i}=\\mathcal{O}(1/n)$ . By doing so, our bound implicitly depends on $n$ also through $\\eta_{i}$ . ", "page_idx": 4}, {"type": "text", "text": "The role of Equation 6 is to constraint w to non-trivial parameter spaces. In particular, if $T$ is too small, the magnitude of the model output might not be sufficient to distinguish different inputs up to a margin $\\gamma$ . In this case, the model would have large empirical loss. In turn, Equation 7 is directly associated with the condition in Lemma 1, enabling us to use it. ", "page_idx": 4}, {"type": "text", "text": "As discussed, Neyshabur et al. [40] assume spectrally-normalized weight matrices. To avoid this assumption, we introduce the conditions in Equation 8 and Equation 9, which allow us to pick perturbations that meet the condition in Equation 7, again justifying the application of Lemma 1. ", "page_idx": 4}, {"type": "text", "text": "The bound also includes a somewhat unconventional term, $\\operatorname*{max}\\lbrace1,\\|\\mathbf{w}\\|_{2}^{2}\\rbrace$ . While this technical term allows for a more concise proof, we note that it does not impose suboptimality. More specifically, in most real-world cases, the squared norm of w is greater than 1. See Appendix I for a discussion. ", "page_idx": 4}, {"type": "text", "text": "Applying Lemma 2 to MLPs and GNNs. As previously mentioned, using Lemma 2 involves defining the variables $T,(S_{i})_{i},(\\eta_{i})_{i},C_{1},C_{2}$ to meet all conditions in Lemma 2\u2019s statement. Typically, this definition comes naturally from the perturbation analysis of the model. To illustrate the power of Lemma 2, Table 2 shows how we can apply it using the perturbation analysis for MLPs and GNNs provided in [33, 40, 48]. Detailed proofs are given in Appendix E. Importantly, we note that we do not make any additional assumption beyond those in the original papers \u2014 we state all assumptions before each proof in the Appendix for clarity. ", "page_idx": 5}, {"type": "text", "text": "Notably, our approach generalizes results by Neyshabur et al. [40] and Liao et al. [33] to MLPs/GCNs with different activation functions \u2014 note that the original works only consider ReLU activations. For inherently non-homogeneous models like MPGNNs, our method leads to tighter bounds in several settings. We provide a comparison between our bounds and previous ones in Appendices F and G. ", "page_idx": 5}, {"type": "text", "text": "Applying Lemma 2 to PersLay. Next, Corollary 1 provides a PAC-Bayes bound for PersLay under fixed filtration functions \u2014 see Appendix H for a discussion about filtration functions. To the best of our knowledge, this is the first generalization result for vectorization schemes of persistence diagrams. Again, the proof consists of verifying if the requirements of Lemma 2 are met. For readability, we omit the proof and provide only the constant weighting function case in the main text, proof as well as the arbitrary weighting function case can be found in Appendix E. The perturbation analysis of PersLay is also given in the supplementary material (Lemma 10). ", "page_idx": 5}, {"type": "text", "text": "Corollary 1 (PersLay with constant weighting function). Let $f_{\\mathbf{w}}:\\mathcal{G}\\mapsto\\mathbb{R}^{k}$ with $\\mathbf{w}=v e c\\{W^{(\\varphi)}\\}$ be a PersLay where $W^{(\\varphi)}$ denotes the parameters of the point-transformation function, $\\varphi$ . Let $\\mathrm{Dg}\\,b e$ a mapping from graphs to (extended) persistence diagrams with a fixed flitration function and $B$ such that $\\operatorname*{max}_{G\\in\\mathcal{G}}\\operatorname*{max}_{p\\in\\mathrm{Dg}(G)}\\|p\\|_{2}\\leq B$ , then $f_{\\mathbf{w}}$ satisfies the requirements of Lemma 2 with: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T=T^{(\\varphi)}}&{{}a n d\\quad S=\\|W^{(\\varphi)}\\|_{2}\\quad a n d\\quad\\eta=1,}\\\\ {C_{1}=2C_{2}}&{{}a n d\\quad C_{2}=2A_{2}\\operatorname*{max}\\{L i p^{(\\varphi)},C^{(\\varphi)}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{A_{1}=A_{2}=\\underset{G\\in\\mathcal{G}}{\\operatorname*{max}}\\,c a r d(\\mathrm{Dg}(G))}&{i f\\,\\mathrm{Agg}=\\sqrt[\\prime\\prime}x u m^{\\prime\\prime}}\\\\ &{A_{1}=1,A_{2}=3}&{i f\\,\\mathrm{Agg}=\\sqrt[\\prime\\prime]{-o r}\\,\\dag,m e a n^{\\prime\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{i f\\varphi=\\Lambda}&{\\operatorname*{max}\\{1,\\|W^{(\\varphi)}\\|_{2}\\}}&{B\\sqrt{h}}&{1}\\\\ &{i f\\varphi=\\Gamma}&{\\operatorname*{max}\\{1,\\|W^{(\\varphi)}\\|_{2}\\}}&{\\sqrt{h}}&{\\frac{1}{\\tau\\sqrt{e}}}\\\\ &{i f\\varphi=\\Psi}&{\\|W^{(\\varphi)}\\|_{2}}&{\\sqrt{3}\\operatorname*{max}\\{1,B\\}}&{\\operatorname*{max}\\{1,B\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "PersLay\u2019s special cases. Carri\u00e8re et al. [5] designed PersLay with flexibility in mind to subsume commonly used persistence vectorization schemes in the literature. Consequently, we can obtain bounds for these schemes \u2014 we provide the values of $C_{2}$ (divided by 2) which is enough to compare the schemes since everything else is the same in the constant weighting function case: ", "page_idx": 5}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/10657a7cf626a6064e0ed23a3105da953cb38dbcbccf2115e5c58009dfe6070e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "where card is the $\\operatorname*{max}_{G\\in{\\mathcal{G}}}\\operatorname{Dg}(G)$ . If \"images\" and \"silhouettes\" use \"sum\" as an aggregating function, then our generalization analysis suggests that $\"k$ -landscapes\" would have stronger guarantees. If these schemes use \"mean\" as an aggregating function, then $C_{2}$ for $\"k$ -landscapes\" would be at most $C_{2}$ for \"silhouettes\", and the result of comparison of $\"k\"$ -landscapes\" and \"images\" could be in favor of both \"landscapes\" and \"images\" depending on chosen parameters $\\tau$ and $B$ . ", "page_idx": 5}, {"type": "text", "text": "4 Compositional PAC-Bayes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present two lemmas (Lemmas 3 & 4) that allow us to compose models satisfying Lemma 2 requirements. At the end of the section (Corollary 2), we showcase our framework by getting generalization bounds for combinations of GNNs and PersLay. For readability, here we provide informal statements and defer the formal ones to the Appendix (Lemma 5, Lemma 6). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In particular, Lemma 3 establishes that the composition of MLPs with models that satisfy Lemma 2 also satisfy it. As a result, we can derive PAC-Bayes bounds for heterogeneous models that leverage MLPs using our framework in a straightforward way. This is particularly relevant since deep learning models often employ learnable feature extractors followed by MLPs as classification heads. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 (Informal; Composition with MLP). Let $f$ be an MLP and g be a model satisfying Lemma 2 requirements, then $f\\circ g$ also satisfies Lemma 2 requirements. ", "page_idx": 6}, {"type": "text", "text": "In addition, we show in Lemma 5 (Appendix) that this result also extends to an arbitrary number of models beyond MLPs. In particular, the result holds whenever we can upper bound output deviations due to perturbations on parameters and inputs, i.e., $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathcal{X}}\\|f_{\\mathbf{w+u}}(x+\\bar{\\Delta}x)-f_{\\mathbf{w}}(x)\\|_{2}}\\end{array}$ is bounded. ", "page_idx": 6}, {"type": "text", "text": "Our next lemma suggests that models satisfying Lemma 2 requirements are closed under parallel concatenation. We note that combining two (or more) models in parallel is also a common design choice in deep learning. For instance, this encompasses persistence-augmented GNNs [22] and ensemble methods [13]. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4 (Informal; Models in parallel). Let $f_{1},\\;f_{2}$ be two models satisfying Lemma 2 requirements and g be an aggregating Lipschtiz function. Then, $g(f_{1}(\\cdot),f_{2}(\\cdot))$ also satisfies Lemma 2 requirements. ", "page_idx": 6}, {"type": "text", "text": "We also provide a generalization of this lemma in the Appendix (Lemma 6) for $n\\,>\\,2$ models in parallel. It leads to tighter bounds than a naive 2-by-2 sequential application of Lemma 4. ", "page_idx": 6}, {"type": "text", "text": "Corollary 2 (Informal). By combining the results for MLPs, GCNs, MPGNNs (Table 2) and that for PersLay (Corollary 1) with Lemma 3 and Lemma 4, we can get generalization bounds on various compositions of these models. In particular, for GNNs with persistence (see Figure 1), we have ", "page_idx": 6}, {"type": "text", "text": "\u2022 $T$ for the overall model scales with the product of PersLay\u2019s and GNN\u2019s $T$ variables;   \n\u2022 $C_{1}$ and $C_{2}$ of the overall model scale linearly with $C_{1},C_{2}$ of PersLay and GNN. ", "page_idx": 6}, {"type": "text", "text": "Despite the generality of our results, Corollary 2 demonstrates the benefits of our framework in the domain of graph representational learning. To the best of our knowledge, this the first result providing generalization guarantees for graph neural networks combined with persistence vectorization schemes. Furthermore, our findings can aid practitioners in making informed architectural decisions to enhance the generalizability of their models. Specifically, in the case of combining PersLay with GNNs, a tighter bound can be achieved by selecting a PersLay dimension that is considerably smaller than the GNN dimension. Failing to do so may result in a bound dependency on the width of the form $\\mathcal{O}(h\\sqrt{\\ln h})$ rather than $O({\\sqrt{h\\ln h}})$ . Additionally, we recommend using aggregation functions such as \"mean\" or $\"k$ -max\" instead of \"sum\" as the latter introduces a term $\\operatorname*{max}_{G\\in{\\mathcal{G}}}c a r d(G)$ to the bound, which may be large in practical scenarios. ", "page_idx": 6}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Expressivity and generalization of GNNs. GNNs have achieved state-of-the-art performance across various applications [16, 28, 45, 51, 54], and have garnered significant attention. Maron et al. [35], Xu et al. [55] analyzed the representational power of GNNs in terms of the 1-WL test, revealing theoretical limits on their expressivity. This has motivated a surge of works aiming to go beyond 1-WL with GNNs [e.g., 32]. Regarding generalization, Scarselli et al. [46] provided upper bounds on the order of growth of VC-dimension for GNNs. Garg et al. [14] presented the first data-dependent generalization bounds for GNNs via Rademacher complexity. Recently, Morris et al. [39] employed the WL test alongside VC-dimension to gain insights about the generalization performance of GNNs. For details about the expressivity and learning of GNNs, we refer to Jegelka [24]. ", "page_idx": 6}, {"type": "text", "text": "Learning theory and PH. Birdal et al. [3], Dupuis et al. [8] and Chen et al. [7] investigate connections between learning theory and topological data analysis. In particular, Birdal et al. [3], Dupuis et al. [8] explored the concept of PH dimension as a complexity measure to analyze generalization. Chen et al. [7] proposed a topological regularizer to simplify decision boundaries by penalizing non-essential topological features. In contrast, we apply learning theory to derive data-dependent generalization bounds for arbitrary heterogeneous layers, specifically targetting persistence-aware GNNs, and introduce a regularizer informed by these bounds to guide the design of robust and generalizable models. ", "page_idx": 6}, {"type": "image", "img_path": "ZNcJtNN3e8/tmp/ea249f935f5a0a748882e140ef0296edda511ef0c9b1b5763f912d633298b7c0.jpg", "img_caption": ["Figure 2: PersLay classifier: spectral norm vs. generalization gap. Overall, our bound on the spectral norm of the weights is highly correlated with the generalization gap. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ZNcJtNN3e8/tmp/292ddf91ff81ecc0bc854c489dd458fcf9d9ff2b53b3c72679dd637479186ca9.jpg", "img_caption": ["Figure 3: PersLay classifier: width vs. generalization gap. The dependence of the empirical gap on the model width is captured by our bound. We obtain high average correlation for all datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "PAC-Bayes. The PAC-Bayes framework [37, 38] allows us to leverage knowledge about learning algorithms and distributions over the hypothesis set to achieve tighter generalization bounds. Remarkably, Neyshabur et al. [40] presented a generalization bound for feedforward networks in terms of the product of the spectral norm of weights using a PAC-Bayes analysis. Liao et al. [33] provided PAC-Bayes bounds for GNNs, and Sun and Lin [48] enhanced their analysis considering the adversarial case as well. In a seminal work, Dziugaite and Roy [9] optimized PAC-Bayes bounds directly to obtain non-vacuous generalization bounds for deep stochastic neural networks. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To illustrate the practical relevance of our analysis, we now consider the generalization of persistenceaware models on real-world datasets, and report results for regularized models based on our bounds. In particular, we conduct two main experiments. The first one aims to analyze how well our bounds capture generalization gaps as a function of model variables. The second assesses to which extent a structural risk minimization algorithm that uses our bound on the weights spectral norm improve generalization compared to empirical risk minimizers. We implemented all experiments using PyTorch [42], and implementation details are given in the Appendix J. Our code is available at https://github.com/Aalto-QuML/Compositional-PAC-Bayes. ", "page_idx": 7}, {"type": "text", "text": "Datasets and evaluation setup. We use six popular benchmarks for graph classification: DHFR, MUTAG, PROTEINS, NCI1, IMDB-BINARY, MOLHIV, which are available as part of TUDatasets [26] and OGB [21]. We use a $80/10/10\\%$ (train/val/test) split for all datasets when we perform model selection. Here, we consider both PersLay Classifiers and GNNs with persistence models with constant weight functions and Gaussian point transformations. For the experiments with GNNs, we kept only the larger datasets (and added results for the NCI109 dataset). Regarding filtration functions, we closely follow [5] and use Heat kernels with parameter values equal to 0.1 and 10. ", "page_idx": 7}, {"type": "text", "text": "Dependence on model components. Figure 2 and Figure 4 show the generalization gap (measured as $L_{\\mathcal{D},0}-\\hat{L}_{\\mathcal{S},\\gamma=1})$ and the bound on the weights spectral norm ( $T$ from Lemma 2) over the training epochs for PersLay Classifier and GNNs with persistence, respectively. To evaluate how well our bound captures the trend observed in the empirical gap, we compute correlation coefficients between the two sequences across different seeds and report their mean and standard deviation for each dataset. Overall, the coefficients are greater than 0.7 in 7 out of 9 experiments, indicating a good correlation. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 shows the empirical gap and our estimated bound as a function of the model\u2019s width for the PersLay classifier. Again, we compute correlation coefficients between the two curves and find they are highly correlated (with an average correlation above 0.91 on 4 out of 5 datasets). Also, we note that these curves are obtained at the final training epoch. We report additional results across different epochs and hyper-parameters in the supplementary material. Again, these results validate that our theoretical bounds can capture the trend observed in the empirical generalization gap. ", "page_idx": 7}, {"type": "image", "img_path": "ZNcJtNN3e8/tmp/863570c3bb6ca43c9920235d913ff927b341a7ca11e419334b4dca7d63a8cb83.jpg", "img_caption": ["Figure 4: GNNs with persistence: empirical gap vs. PAC-Bayes bound. Again, there is positive and high correlation between our bound and the observed generalization gap. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Regularizing PersLay. We compare variants of PersLay trained via ERM (empirical risk minimization) and a regularized version with loss given by $\\hat{L}_{{S,1}}{+}\\dot{\\alpha}_{r}\\sqrt{n^{2}\\ln n\\|\\mathbf{w}\\|_{2}^{2}\\beta^{2}};\\alpha_{r}^{-}$ is a hyper-parameter that balances the influence of the two terms and $\\begin{array}{r}{\\beta=T\\dot{\\sum}_{i}{^{1\\!}\\!\\left/S_{i}\\right.^{}}-}\\end{array}$ \u2014 see proof sketch of Lemma 2 in Section 3. This is similar to a weight-decay regulariz ation approach, with the spectral norm of weights appearing in $\\beta$ . Here, we consider models with $n=1$ or 2, selected via hold-out validation. We note that PersLay classifier does not use node features, it only exploits graph structures. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Comparison of PersLay with and without spectral norm regularization. We report accuracy statistics (except for MOLHIV, which uses AUROC) computed over five independent runs. In 5 out of 6 cases, the models using SpecNorm achieve better test results. ", "page_idx": 8}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/bb3d9214e34322884d9e29233c4f602d42d89674b74986e11df8a550123edc45.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3 reports accuracy results (mean and standard deviations) computed over five runs. Overall, the regularized approach significantly outperforms the ERM variant despite the use of small-sized networks. On 5/6 datasets, PersLay with spectral norm regularization is the best-performing model. ", "page_idx": 8}, {"type": "text", "text": "Regularizing GNNs with persistence. We now evaluate the impact of using our bound to regularize different GNNs combined with persistent homology (PersLay) in parallel mode. We consider GCN [28], GraphSage [16], and GIN [55] architectures. Table 4 reports the test classification error and the generalization gap on the NCI, NCI109, and PROTEINS datasets \u2014 mean and standard deviation obtained over five independent runs. For our regularizer, we select the optimal penalization factor $\\alpha_{r}\\in\\{1\\mathrm{e}{-}5,1\\mathrm{e}{-}6,1\\mathrm{e}{-}7,1\\mathrm{e}{-}8\\}$ using the validation set. Overall, the results show that the regularized methods achieve smaller generalization gaps and slightly lower classification errors. In particular, our spectral regularizer leads to a significant drop in generalization gap in all experiments. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Test classification error (0-1 loss) and generalization gap $(L_{\\mathcal{D},0}-\\hat{L}_{S,\\gamma})$ for PH-augmented GNNs. ERM means empirical risk minimizer (no regularization). We denote the best-performing methods in bold. In almost all cases, employing the method derived from our theoretical analysis leads to the smallest test errors and generalization gaps. ", "page_idx": 8}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/3a45a1dcce44bdb61e2bae02fcd3f80f85ca8111aa3892effd19e84d0e19c26b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "7 Conclusion, Broader Implications, and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We derive the first generalization bounds for neural networks that appeal to persistent homology for graph learning. Notably, the analyzed framework (PersLay) offers a flexible and general way to extract vector representations from persistence diagrams. Due to this generality, our analysis covers several methods available in the literature. The developed framework also allows to analyze composite models like, GNNs combined with PersLay. Our constructions involve a perturbation and generalization behavior analysis of non-homogeneous networks in rather general setting, which poses specific technical challenges. ", "page_idx": 9}, {"type": "text", "text": "While we provide valuable insights and methodologies, we would like to underscore the need for future investigations to delve into PH schemes that encompass parametrized filtration functions. Nonetheless, while some works showed gains using learnable filtrations [20], others have reported no benefits and advocated fixed functions instead [5, 34]. Moreover, the tightness of our bounds can further be improved since there is still considerable gap between empirical results and the theoretical one. By shedding new light on the generalization of machine learning models based on persistent homology, we hope to contribute to the community by providing key insights about the limits and power of these methods, paving the path to further theoretical developments on PH-based neural networks for graph representation learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "VG acknowledges support from the Research Council of Finland (grant decision 342077) for the project \u201cHuman-steered next-generation machine learning for reviving drug design\u201d, and the Jane and Aatos Erkko Foundation (grant 7001703) for \u201cBiodesign: Use of artificial intelligence in enzyme design for synthetic biology\u201d. VG also thanks the Finnish Ministry for Education and Culture for their support via the \u201cMEC Global Programme pilot USA\u201d initiative. AS acknowledges the support from the Conselho Nacional de Desenvolvimento Cient\u00edfico e Tecnol\u00f3gico CNPq (404336/2023-0) and the Silicon Valley Community Foundation through the University Blockchain Research Initiative (Grant #2022-199610). KB thanks Aalto University for the support during the internship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence Images: A Stable Vector Representation of Persistent Homology. Journal of Machine Learning Research, 18(8):1\u201335, 2016.   \n[2] M.E. Aktas, E. Akbas, and A.E Fatmaoui. Persistence homology of networks: methods and applications. Applied Network Science, 2019.   \n[3] Tolga Birdal, Aaron Lou, Leonidas Guibas, and Umut \u00b8Sim\u00b8sekli. Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[4] P. Bubenik. Statistical topological data analysis using persistence landscapes. Journal of Machine Learning Research, 16:77\u2013102, 2015.   \n[5] Mathieu Carri\u00e8re, Fr\u00e9d\u00e9ric Chazal, Yuichi Ike, Th\u00e9o Lacombe, Martin Royer, and Yuhei Umeda. PersLay: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures. In Artificial Intelligence and Statistics (AISTATS), 2020.   \n[6] Fr\u00e9d\u00e9ric Chazal, Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, and Larry Wasserman. Stochastic Convergence of Persistence Landscapes and Silhouettes. In Proceedings of the Thirtieth Annual Symposium on Computational Geometry, 2014.   \n[7] Chao Chen, Xiuyan Ni, Qinxun Bai, and Yusu Wang. A Topological Regularizer for Classifiers via Persistent Homology, October 2018.   \n[8] Benjamin Dupuis, George Deligiannidis, and Umut \u00b8Sim\u00b8sekli. Generalization Bounds with Data-dependent Fractal Dimensions. arXiv e-prints, 2023. [9] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Conference on Uncertainty in Artificial Intelligence (UAI), 2017.   \n[10] H. Edelsbrunner and J. Harer. Computational Topology - an Introduction. American Mathematical Society, 2010.   \n[11] Herbert Edelsbrunner and John Harer. Persistent homology\u2014a survey. In Jacob E. Goodman, J\u00e1nos Pach, and Richard Pollack, editors, Contemporary Mathematics, volume 453, pages 257\u2013282. American Mathematical Society, 2008.   \n[12] Pascal Esser, Leena Chennuru Vankadara, and Debarghya Ghoshdastidar. Learning theory can (sometimes) explain generalisation in graph neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[13] M.A. Ganaie, Minghui Hu, A.K. Malik, M. Tanveer, and P.N. Suganthan. Ensemble deep learning: A review. Engineering Applications of Artificial Intelligence, 115, 2022.   \n[14] Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and Representational Limits of Graph Neural Networks. In International Conference on Machine Learning (ICML), 2020.   \n[15] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning (ICML), 2017.   \n[16] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Conference on Neural Information Processing Systems (NeurIPS), 2017.   \n[17] Felix Hensel, Michael Moor, and Bastian Rieck. A Survey of Topological Machine Learning Methods. Frontiers in Artificial Intelligence, 4, 2021. ISSN 2624-8212.   \n[18] Christoph Hofer, Roland Kwitt, Marc Niethammer, and Andreas Uhl. Deep Learning with Topological Signatures. In Advances in Neural Information Processing Systems (NeurIPS), 2017.   \n[19] Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. Graph filtration learning. In International Conference on Machine Learning (ICML), 2020.   \n[20] M. Horn, E. De Brouwer, M. Moor, Y. Moreau, B. Rieck, and K. Borgwardt. Topological graph neural networks. In International Conference on Learning Representations (ICLR), 2022.   \n[21] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.   \n[22] Johanna Emilia Immonen, Amauri H. Souza, and Vikas Garg. Going beyond persistent homology using persistent homology. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[23] Jan Reininghaus, Stefan Huber, Ulrich Bauer, and Roland Kwitt. A stable multi-scale kernel for topological machine learning. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.   \n[24] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. ArXiv, abs/2204.07697, 2022.   \n[25] Haotian Ju, Dongyue Li, Aneesh Sharma, and Hongyang R. Zhang. Generalization in graph neural networks: Improved pac-bayesian bounds on graph diffusion. In International Conference on Artificial Intelligence and Statistics, 2023.   \n[26] Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Benchmark data sets for graph kernels, 2016. URL http://graphkernels.cs. tu-dortmund.de.   \n[27] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.   \n[28] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.   \n[29] Genki Kusano, Yasuaki Hiraoka, and Kenji Fukumizu. Persistence weighted Gaussian kernel for topological data analysis. In International Conference on Machine Learning (ICML), pages 2004\u20132013, 2016.   \n[30] John Langford and John Shawe-Taylor. PAC-Bayes & Margins. In Advances in Neural Information Processing Systems (NeurIPS), volume 15. MIT Press, 2002.   \n[31] Tam Le and Makoto Yamada. Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams. In Advances in Neural Information Processing Systems (NeurIPS), 2018.   \n[32] P. Li, Y. Wang, H. Wang, and J. Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[33] Renjie Liao, Raquel Urtasun, and Richard Zemel. A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks. In International Conference on Learning Representations (ICLR), 2020.   \n[34] Yuankai Luo, Lei Shi, and Veronika Thost. Improving self-supervised molecular representation learning using persistent homology. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[35] H. Maron, H. Ben-Hamu, H. Serviansky, and Y. Lipman. Provably powerful graph networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[36] Sohir Maskey, Ron Levie, Yunseok Lee, and Gitta Kutyniok. Generalization analysis of message passing neural networks on large random graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[37] David McAllester. Simplified PAC-Bayesian Margin Bounds. In Bernhard Sch\u00f6lkopf and Manfred K. Warmuth, editors, Learning Theory and Kernel Machines, Lecture Notes in Computer Science, pages 203\u2013215, Berlin, Heidelberg, 2003. Springer.   \n[38] David A. McAllester. PAC-Bayesian model averaging. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT \u201999, pages 164\u2013170. Association for Computing Machinery, July 1999.   \n[39] Christopher Morris, Floris Geerts, Jan T\u00f6nshoff, and Martin Grohe. WL meet VC. In International Conference on Machine Learning (ICML), 2023.   \n[40] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. In International Conference on Representation Learning (ICLR), 2018.   \n[41] Vahid Partovi Nia, Guojun Zhang, Ivan Kobyzev, Michael R. Metel, Xinlin Li, Ke Sun, Sobhan Hemati, Masoud Asgharian, Linglong Kong, Wulong Liu, and Boxing Chen. Mathematical Challenges in Deep Learning. ArXiv e-prints: 2303.15464, 2023.   \n[42] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. In Advances in Neural Information Processing Systems (NeurIPS - Workshop), 2017.   \n[43] Giovanni Petri, Martina Scolamiero, Irene Donato, Francesco Vaccarino, Thomas Gilbert, Markus Kirkilionis, and Gregoire Nicolis. Networks and Cycles: A Persistent Homology Approach to Complex Networks. Proceedings of the European Conference on Complex Systems 2012. Springer International Publishing, 2013. ISBN 978-3-319-00394-8.   \n[44] Bastian Rieck. On the Expressivity of Persistent Homology in Graph Learning. arXiv e-prints, (arXiv:2302.09826), 2023.   \n[45] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.   \n[46] Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik\u2013chervonenkis dimension of graph and recursive neural networks. Neural Networks, 108:248\u2013259, 2018.   \n[47] Milad Sefidgaran and Abdellatif Zaidi. Data-dependent Generalization Bounds via Variable-Size Compressibility. arXiv e-prints, 2023.   \n[48] Tan Sun and Junhong Lin. Pac-bayesian adversarially robust generalization bounds for graph neural network. arXiv e-prints, 2024.   \n[49] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, August 2012. ISSN 1615-3375, 1615-3383.   \n[50] L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134\u20131142, November 1984. ISSN 0001-0782. doi: 10.1145/1968.1972.   \n[51] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), 2018.   \n[52] S. Verma and Z.-L. Zhang. Stability and generalization of graph convolutional neural networks. In International Conference on Knowledge Discovery & Data Mining (KDD), 2019.   \n[53] Yogesh Verma, Amauri H Souza, and Vikas Garg. Topological neural networks go persistent, equivariant, and continuous. In International Conference on Machine Learning (ICML), volume 235, 2024.   \n[54] Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan, and Huan Liu. Graph Learning: A Survey. IEEE Transactions on Artificial Intelligence, 2(2):109\u2013127, April 2021. ISSN 2691-4581. doi: 10.1109/TAI.2021.3076021.   \n[55] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations (ICLR), 2019.   \n[56] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets, 2018.   \n[57] Qi Zhao and Yusu Wang. Learning metrics for persistence-based summaries and applications for graph classification. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[58] Qi Zhao, Ze Ye, Chao Chen, and Yusu Wang. Persistence Enhanced Graph Neural Network. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.   \n[59] Yangze Zhou, Gitta Kutyniok, and Bruno Ribeiro. OOD link prediction generalization capabilities of message-passing GNNs in larger test graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 5 summarizes the main mathematical symbols and abbreviations used in this work. ", "page_idx": 13}, {"type": "text", "text": "Table 5: Summary of notation and abbreviations. ", "page_idx": 13}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/c8019b54cfed693531eee2a5ec3f58477060ad47734ff56d475914e5b3ce0154.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Model descriptions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "MLP. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{H_{k}=\\psi_{k}(W_{k}H_{k-1})}}\\\\ {{H_{n}=W_{n}H_{n-1},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{(k{\\mathrm{-th~layer}}),}\\\\ {({\\mathrm{Final~layer}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $H_{k}\\,\\in\\,\\mathbb{R}^{h_{k}}$ are the vectors computed after layer $k$ , and $W_{k}\\,\\in\\,\\mathbb{R}^{h_{k-1}\\times h_{k}},k\\,\\in\\,[n]$ are the parameters in the $k$ -th layer, and we let $H_{0}=\\mathbf{x}$ . $\\psi_{k},k\\in[n-1]$ are some element-wise non-linear $\\mathrm{Lip}_{k}$ -Lipschitz functions. Note that we change $d$ (number of layers in Neyshabur et al. [40] notation) to $n$ to be consistent with the number of parameters notation. ", "page_idx": 14}, {"type": "text", "text": "GCN. ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{H_{k}=\\psi_{k}\\big(P_{G}H_{k-1}W_{k}\\big)}&&{\\qquad\\qquad\\mathrm{(}k\\mathrm{-th~Graph~Convolution~Layer),}}\\\\ &{H_{n}=\\displaystyle\\frac{1}{|V|}{\\bf1}_{|V|}H_{n-1}W_{n}}&&{\\qquad\\qquad\\mathrm{(Readout~layer),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $H_{k}\\in\\mathbb{R}^{|V|\\times h_{k}},k\\in[n-1]$ are the node representations in each layer, $H_{n}\\in\\mathbb{R}^{1\\times K}$ is the readout, and $W_{k}\\in\\mathbb{R}^{h_{k-1}\\times h_{k}},k\\in[n]$ are the parameters in the $k$ -th layer. And we let $H_{0}=Z$ , where $Z$ is the matrix constructed from node features, $z_{v}$ for $v\\in V$ . The matrix $P_{G}\\in\\mathbb{R}^{|V|\\times|V|}$ is related to the graph structure and $\\psi_{k}(\\cdot),k\\in[n-1]$ are some element-wise non-linear $\\mathrm{Lip}_{k}$ -Lipschitz mappings. Practically, for GCN, we take $P_{G}$ as the Laplacian of the graph, defined as $\\tilde{D}^{-1\\tilde{/}2}\\tilde{A}\\tilde{D}^{-1/2}$ , where $\\tilde{A}$ is the adjacency matrix with $+1$ on the diagonal and $\\begin{array}{r}{\\tilde{D}=\\mathrm{diag}\\left(\\sum_{j=1}^{|V|}\\tilde{A}_{i j},i\\in[|V|]\\right)}\\end{array}$ is the degree matrix of $\\tilde{A}$ . Note that we sightly changed (comparing to Liao et al. [33], Sun and Lin [48]) the notation to be consistent: instead of $l$ , number of layers, we have $n$ , instead of $n$ , number of nodes, we have $\\vert V\\vert$ and instead of $\\sigma_{i}$ we have $\\psi_{i}$ as activation functions. ", "page_idx": 14}, {"type": "text", "text": "MPGNN. ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{M_{k}=g(C_{\\mathrm{out}}^{\\top}H_{k-1})}}&{{\\qquad\\qquad\\qquad\\qquad\\quad(k{\\mathrm{-th~step~Message~Computation}})}}\\\\ {{\\bar{M}_{k}=C_{\\mathrm{in}}M_{k}}}&{{\\qquad\\qquad\\qquad\\quad(k{\\mathrm{-th~step~Message~Aggregation}})}}\\\\ {{H_{k}=\\phi\\left(X W_{1}+\\rho\\left(\\bar{M}_{k}\\right)W_{2}\\right)}}&{{\\qquad\\qquad\\quad(k{\\mathrm{-th~step~Node~State~Update}})}}\\\\ {{H_{n}=\\displaystyle\\frac{1}{|V|}\\mathbf{1}_{|V|}H_{n-1}W_{3}}}&{{\\qquad\\qquad\\qquad\\quad(\\mathrm{Readout~Layer})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $k\\,\\in\\,[n-1]$ , $H_{k}\\in\\mathbb{R}^{|V|\\times h_{k}}$ are node representations/states and $H_{n}\\in\\mathbb{R}^{1\\times K}$ is the output representation. Here we initialize $H_{0}=\\mathbf{0}$ . WLOG, we assume $\\forall k\\in[n-1]$ , $H_{k}\\in\\mathbb{R}^{|V|\\times h}$ and $M_{k}\\in\\mathbb{R}^{|V|\\times h}$ since $h$ is the maximum hidden dimension. $C_{\\mathrm{in}}\\in\\mathbb{R}^{|V|\\times|\\bar{E}|}$ and $C_{\\mathrm{out}}\\in\\mathbb{R}^{|V|\\times|E|}$ are the incidence matrices corresponding to incoming and outgoing nodes1 respectively. Specifically, rows and columns of $C_{\\mathrm{in}}$ and $C_{\\mathrm{out}}$ correspond to nodes and edges respectively. $C_{\\mathrm{in}}[i,j]=1$ indicates that the incoming node of the $j$ -th edge is the $i$ -th node. Similarly, $C_{\\mathrm{out}}[i,j]=1$ indicates that the outgoing node of the $j$ -th edge is the $i$ -th node. $g,\\phi,\\rho$ are nonlinear mappings, e.g., ReLU and Tanh. Technically speaking, $g:\\mathbb{R}^{\\check{h}}\\to\\mathbb{R}^{h}$ , $\\phi:\\mathbb{R}^{h}\\to\\mathbb{R}^{h}$ , and $\\rho:\\mathbb{R}^{h}\\to\\mathbb{R}^{\\dot{h}}$ operate on vector-states of individual node/edge. However, since we share these functions across nodes/edges, we can naturally generalize them to matrix-states, e.g., $\\tilde{\\phi}:\\mathbb{R}^{|V|\\times h}\\rightarrow\\mathbb{R}^{|V|\\times h}$ where $\\tilde{\\phi}(X)[i,:]\\,=\\,\\phi(X[i,:])$ . By doing so, the same function could be applied to matrices with varying size of the first dimension. For simplicity, we use $g,\\phi,\\rho$ to denote such generalization to matrices. We denote the Lipschitz constants of $g,\\phi,\\rho$ under the vector 2-norm as $C_{g},C_{\\phi},C_{\\rho}$ respectively. We also assume $g(\\bar{\\bf0})={\\bf0}$ , $\\phi(\\mathbf{0})=\\mathbf{0}$ , and ${\\boldsymbol\\rho}({\\bf0})={\\bf0}$ and define the percolation complexity as $\\mathcal{C}=C_{g}C_{\\phi}C_{\\rho}\\|W_{2}\\|_{2}$ following Garg et al. [14]. Note that we sightly changed (comparing to Liao et al. [33], Sun and Lin [48]) the notation to be consistent: instead of $l$ , number of layers, we have $n$ , instead of $n$ , number of nodes, we have $|V|$ and instead of $W_{l}$ , parameter matrix, we have $W_{3}$ . ", "page_idx": 14}, {"type": "text", "text": "C Proofs of main lemma and composition lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 2. Let $f_{\\mathbf{w}}:\\mathcal{X}\\mapsto\\mathbb{R}^{K}$ be a model with parameters $\\mathbf{w}=v e c\\{W_{1},...,W_{n}\\}$ . If there exists $T\\in\\mathbb{R}$ and $S_{i}\\in\\mathbb{R}$ for $i\\in[n]$ depending on $W_{1},...,W_{n};\\,C_{1},C_{2}>0;\\eta_{i}\\,\\in\\,(0,1]\\,f o r\\,\\,i\\,\\in$ $i\\in[n]$ such that: ", "page_idx": 15}, {"type": "text", "text": "maximum norm of the output is bounded by $T$ and $C_{1}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\|f_{\\mathbf{w}}(x)\\|_{2}\\leq C_{1}T,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "maximum change of the output with perturbed weights, ${\\bf u}\\,=\\,v e c\\{U_{1},...,U_{n}\\}$ , is bounded if the perturbation is small, $\\|U_{i}\\|_{2}\\le\\eta_{i}S_{i}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\|f_{\\mathbf{w}+\\mathbf{u}}(x)-f_{\\mathbf{w}}(x)\\|_{2}\\leq C_{2}T\\left(\\sum_{i=1}^{n}\\frac{\\|U_{i}\\|_{2}}{S_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "arithmetic mean of $S_{i}$ inverses is at least the nth root of $T$ inverse: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\left(\\sum_{i=1}^{n}{\\frac{1}{S_{i}}}\\right)\\geq{\\frac{1}{T^{1/n}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "minimum $\\eta_{i}$ is at most the ratio of $C_{1}$ and $2C_{2}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{\\eta}:=\\operatorname*{min}_{1\\leq i\\leq n}\\eta_{i}\\leq\\frac{C_{1}}{2C_{2}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then for any $\\gamma,\\delta\\,>\\,0$ with probability at least $1-\\delta$ over the choice of training set $\\boldsymbol{S}$ of size $m$ sampled accordingly to some data distribution $\\mathcal{D}$ we have the bound on the generalization gap: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{D},0}(f_{\\mathbf{w}})\\leq\\hat{L}_{\\mathcal{S},\\gamma}(f_{\\mathbf{w}})+}\\\\ &{\\,\\,\\,+\\,\\mathcal{O}\\left(\\sqrt{\\frac{\\operatorname*{max}\\{1,\\|\\mathbf{w}\\|_{2}^{2}\\}T^{2}\\left(\\sum_{i=1}^{n}\\frac{1}{S_{i}}\\right)^{2}\\left(h\\ln n h\\right)C_{1}^{2}\\bar{\\eta}^{-2}+\\log\\frac{m}{\\delta}\\operatorname*{max}\\left\\{1,\\frac{1}{C_{1}}\\right\\}}{\\gamma^{2}m}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $h$ is the upper bound for dimensions of $W_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First, we note that if w such that $C_{1}T\\le\\frac{\\gamma}{2}$ , then this imply for any $i\\in[K]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}|f_{\\mathbf{w}}(\\mathbf{x})_{i}|\\leq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\|f_{\\mathbf{w}}(\\mathbf{x})\\|_{2}\\leq C_{1}T\\leq\\frac{\\gamma}{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used Equation 6 and the fact that every coordinate of the vector is at most $\\ell_{2}$ -norm of the vector. In this case the empirical loss would be equal to one, since no index $i$ can satisfy $f_{\\mathbf{w}}(\\mathbf{x})_{i}\\leq\\gamma+\\operatorname*{max}_{i\\neq j}f_{\\mathbf{w}}(\\mathbf{x})$ and the inequality in Equation 10 becomes trivial since generalization error can not be greater than 1. ", "page_idx": 15}, {"type": "text", "text": "So from now on we consider w such that $C_{1}T\\geq\\frac{\\gamma}{2}$ . Moreover we note that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{1}T\\geq\\frac{\\gamma}{2}\\Rightarrow T\\geq\\frac{\\gamma}{2C_{1}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We denote ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta:=T\\left(\\sum_{i=1}^{n}{\\frac{1}{S_{i}}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Following Neyshabur et al. [40] and Liao et al. [33], we consider the prior $\\mathcal{P}=\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$ and random perturbation $\\mathbf{u}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$ . Note that the $\\sigma$ of the prior and the perturbation are the same and will be set according to $\\beta$ . More precisely, we will set the $\\sigma$ based on some approximation $\\tilde{\\beta}$ of $\\beta$ since the prior cannot depend on any learned weights directly. The approximation $\\tilde{\\beta}$ is chosen to be a cover set which covers the meaningful range of $\\beta$ . For now, let us assume that we have a fixed $\\tilde{\\beta}$ and consider $\\beta$ which satisfies $|\\beta-\\tilde{\\beta}|\\leq\\varepsilon\\beta$ for some $\\varepsilon>0$ . Note that this also implies: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\beta-\\tilde{\\beta}|\\leq\\varepsilon\\beta\\Rightarrow\\beta\\leq\\frac{1}{1-\\varepsilon}\\tilde{\\beta}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\beta-\\tilde{\\beta}|\\leq\\varepsilon\\beta\\Rightarrow\\tilde{\\beta}\\leq(1+\\varepsilon)\\beta\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This setup is very like the setup in Neyshabur et al. [40] and Liao et al. [33] ", "page_idx": 16}, {"type": "text", "text": "Choosing $\\sigma$ . From Tropp [49] we know that if $U\\in\\mathbb{R}^{h\\times h}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I})$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|U\\|_{2}\\ge t\\right)\\le2h\\exp\\left(-\\frac{t^{2}}{2h\\sigma^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So applying it to $U_{1},...,U_{n}$ (all of them are smaller than $h\\times h)$ ) and $t=\\sigma{\\sqrt{2h\\ln4n h}}$ we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(||U_{1}||_{2}\\leq\\sigma C_{t}\\;\\&\\ldots\\;\\&\\;||U_{d}||_{2}\\leq\\sigma C_{t}\\right)\\geq1-\\displaystyle\\sum_{i=1}^{d}\\mathbb{P}\\left(||U_{i}||_{2}\\geq\\sigma C_{t}\\right)\\geq}\\\\ {\\geq1-2d h\\mathrm{exp}\\left(-\\displaystyle\\frac{\\sigma^{2}C_{t}^{2}}{2h\\sigma^{2}}\\right)=1-2d h\\exp\\left(\\ln\\displaystyle\\frac{1}{4d h}\\right)=\\displaystyle\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For simplicity of notation we denote $C_{t}=\\sqrt{2h\\ln{4n h}}$ . ", "page_idx": 16}, {"type": "text", "text": "Using defined perturbation ${\\bf u}=v e c\\{U_{1},...,U_{n}\\}$ and Equation 23 combined with Equation 7 we get with probability at least $\\frac{1}{2}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x\\in\\mathcal{X}}{\\mathrm{max}}\\,\\|f_{\\mathbf{w}+\\mathbf{u}}(x)-f_{\\mathbf{w}}(x)\\|_{\\infty}\\leq\\underset{x\\in\\mathcal{X}}{\\mathrm{max}}\\,\\|f_{\\mathbf{w}+\\mathbf{u}}(x)-f_{\\mathbf{w}}(x)\\|_{2}\\leq}&{}\\\\ {\\quad\\leq C_{2}T\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{\\|U_{i}\\|_{2}}{S_{i}}\\right)\\leq}&{}\\\\ {\\quad\\leq C_{2}\\beta\\sigma C_{t}\\leq}&{}\\\\ {\\quad\\leq C_{2}C_{t}\\displaystyle\\frac{\\sigma\\tilde{\\beta}}{1-\\varepsilon}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma\\leq\\frac{\\gamma(1-\\varepsilon)}{4C_{2}C_{t}\\tilde{\\beta}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then the perturbation is bounded by $\\scriptstyle{\\frac{\\gamma}{4}}$ . However, we need to satisfy the condition of Equation 7, i.e. $\\forall i\\in[n]\\ \\bar{:}\\ \\|U_{i}\\|\\leq\\eta_{i}S_{i}$ . Let us find $\\bar{C}_{\\sigma}\\geq1$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma:=\\frac{\\gamma(1-\\varepsilon)}{4C_{2}C_{\\sigma}C_{t}\\;\\tilde{\\beta}}\\leq\\frac{\\gamma(1-\\varepsilon)}{4C_{2}C_{t}\\tilde{\\beta}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall i\\in[n]:\\,\\sigma C_{t}\\leq\\eta_{i}S_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma C_{t}=\\frac{\\gamma(1-\\varepsilon)}{4C_{2}C_{\\sigma}\\tilde{\\beta}}\\stackrel{(i)}{\\leq}\\frac{\\gamma}{4C_{2}C_{\\sigma}\\tilde{\\beta}}\\stackrel{(i i)}{=}}\\\\ &{\\quad\\stackrel{(i i)}{=}\\frac{\\gamma\\eta_{t}S_{i}}{4C_{2}C_{\\sigma}\\eta_{t}S_{i}\\tilde{\\beta}}\\stackrel{(i i i)}{=}\\frac{\\gamma\\ \\eta_{t}S_{i}}{4C_{2}C_{\\sigma}\\eta_{t}T\\left(1+\\sum_{k\\neq i}\\frac{S_{i}}{S_{k}}\\right)}\\stackrel{(i v)}{\\leq}}\\\\ &{\\quad\\stackrel{(i v)}{\\leq}\\frac{\\gamma\\ \\eta_{t}S_{i}}{4C_{2}C_{\\sigma}\\eta_{t}T}\\stackrel{(v)}{\\leq}\\frac{\\gamma\\ \\eta_{t}S_{i}}{4C_{2}C_{\\sigma}\\eta_{t}\\frac{\\gamma}{2C_{1}}}=}\\\\ &{\\quad=\\eta_{t}S_{i}\\frac{C_{1}}{2C_{2}C_{\\sigma}\\eta_{t}\\tilde{\\beta}_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(i) comes from Equation 21. (ii) we just multiplied and divided on $\\eta_{i}S_{i}$ . (iii) we plugged in the expression for $\\beta$ , Equation 20. (iv) $1+\\sum_{k\\neq i}\\frac{S_{i}}{S_{k}}\\overset{\\bar{>}}{=}1$ . (v) comes from Equation 19 ", "page_idx": 17}, {"type": "text", "text": "We choose ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{\\sigma}:=\\frac{C_{1}}{2C_{2}\\,\\underset{1\\leq i\\leq n}{\\operatorname*{min}}\\,\\eta_{i}}=\\frac{C_{1}}{2C_{2}\\,\\bar{\\eta}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this case $C_{\\sigma}\\geq1$ because of Equation 9 and continuing Equation 26: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma C_{t}\\leq\\eta_{i}S_{i}\\frac{C_{1}}{2C_{2}C_{\\sigma}\\eta_{i}}=\\eta_{i}S_{i}\\frac{\\bar{\\eta}}{\\eta_{i}}\\overset{(i)}{\\leq}\\eta_{i}S_{i},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (i) comes from the fact that $\\bar{\\eta}$ is the minimum among $\\eta_{i}\\mathbf{s}$ , gives us that $\\forall i\\in[n]:\\,\\|U_{i}\\|_{2}\\leq\\eta_{i}S_{i}$ . Plugging this into expression for $\\sigma$ , Equation 25, we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma=\\frac{\\gamma(1-\\varepsilon)}{4C_{2}C_{\\sigma}\\tilde{\\beta}}=\\frac{\\gamma(1-\\varepsilon)\\bar{\\eta}}{2C_{1}C_{t}\\tilde{\\beta}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Getting the bound for weights $|\\beta-\\widetilde{\\beta}|\\le1/2\\beta$ . Now, we can compute the KL-divergence and get the bound using Lemma 1 for case when $\\beta$ is around $\\tilde{\\beta}$ , i.e. w such that $|\\beta-\\tilde{\\beta}|\\leq\\varepsilon\\beta$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{K L}(\\mathcal{Q}(\\mathbf{w}+\\mathbf{u})||\\mathcal{P})=\\frac{\\|\\mathbf{w}\\|_{2}^{2}}{2\\sigma^{2}}\\overset{(i)}{=}\\frac{\\|\\mathbf{w}\\|_{2}^{2}}{2}\\frac{4C_{1}^{2}C_{t}^{2}\\tilde{\\beta}^{2}}{\\gamma^{2}(1-\\varepsilon)^{2}\\bar{\\eta}^{2}}\\overset{(i i)}{\\leq}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(i i)}{\\leq}2\\|\\mathbf{w}\\|_{2}^{2}\\frac{C_{1}^{2}C_{t}^{2}\\left(\\frac{1+\\varepsilon}{1-\\varepsilon}\\right)^{2}\\beta^{2}}{\\gamma^{2}\\bar{\\eta}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (i) comes from $\\sigma$ definition, Equation 28, (ii) comes from the upper-bound on $\\tilde{\\beta}$ , Equation 22. We are ready to put this into Lemma 1: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{D},0}(f_{\\mathbf{w}})\\leq\\hat{L}_{\\mathcal{S},\\gamma}(f_{\\mathbf{w}})+4\\sqrt{\\frac{D_{K L}(Q(\\mathbf{w}+\\mathbf{u})||\\mathcal{P})+\\log\\frac{6m}{\\delta}}{m-1}}=}\\\\ &{\\qquad\\qquad=\\hat{L}_{\\mathcal{S},\\gamma}(f_{\\mathbf{w}})+\\mathcal{O}\\left(\\sqrt{\\frac{\\operatorname*{max}\\{1,\\|\\mathbf{w}\\|_{2}^{2}\\}\\beta^{2}C_{1}^{2}C_{t}^{2}\\left(\\frac{1+\\varepsilon}{1-\\varepsilon}\\right)^{2}\\bar{\\eta}^{-2}+\\ln\\frac{m}{\\delta}}{\\gamma^{2}m}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we upper-bounded $\\|\\mathbf{w}\\|_{2}^{2}$ as $\\operatorname*{max}\\lbrace1,\\|\\mathbf{w}\\|_{2}^{2}\\rbrace$ to simplify further derivations. ", "page_idx": 17}, {"type": "text", "text": "Union bound. Finally, we need to consider multiple choices of $\\tilde{\\beta}$ so that for any $\\beta$ , we can bound the generalization error like Equation 30. In order to do this we (i) find interval of reasonable $\\beta\\mathbf{s}$ , (ii) define a covering and upper-bound the number of balls in the covering, (iii) combine bounds for every $\\tilde{\\beta}$ together and get the final result. ", "page_idx": 17}, {"type": "text", "text": "If $\\beta$ is too large, the KL-divergence would be too large and generalization gap would be greater than 1 which thereby trivialize the bound since the generalization error cannot be greater than 1. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\frac{\\operatorname*{max}\\{1,\\|\\mathbf{w}\\|_{2}^{2}\\}\\beta^{2}C_{1}^{2}C_{t}^{2}\\left(\\frac{1+\\varepsilon}{1-\\varepsilon}\\right)^{2}\\bar{\\eta}^{-2}+\\ln\\frac{m}{\\delta}}{\\gamma^{2}m}}\\overset{(i)}{\\geq}}\\\\ {\\overset{(i)}{\\geq}\\sqrt{\\frac{\\beta^{2}C_{1}^{2}}{\\gamma^{2}m\\bar{\\eta}^{2}}}\\overset{(i i)}{\\geq}\\frac{C_{1}\\beta}{\\gamma\\sqrt{m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) holds since we remove multipliers that are greater than 1 and $\\ln{\\frac{m}{\\delta}}$ which is $\\ge0$ and appears in sum, (ii) comes from the fact that $\\bar{\\eta}\\leq1$ . ", "page_idx": 18}, {"type": "text", "text": "So, if $\\begin{array}{r}{\\beta\\geq\\frac{\\gamma\\sqrt{m}}{C_{1}}}\\end{array}$ , then the bound becomes trivial. ", "page_idx": 18}, {"type": "text", "text": "On the other hand, if the $\\beta$ is too small, then the empirical loss would be too large since the model would not be able to classify with margin, $\\gamma$ , as it was shown in Equation 19. Using this equation and Arithmetic versus Geometric mean we get the following inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta=T\\sum_{i=1}^{n}\\frac{1}{S_{i}}\\stackrel{(i)}{\\geq}n T^{\\frac{n-1}{n}}\\stackrel{(i i)}{\\geq}n\\left(\\frac{\\gamma}{2C_{1}}\\right)^{\\frac{n-1}{n}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) comes from Equation 8 and (ii) comes from Equation 19. ", "page_idx": 18}, {"type": "text", "text": "Combining lower (Equation 32) and upper (Equation 31) bound on $\\beta$ we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\nn\\left({\\frac{\\gamma}{2C_{1}}}\\right)^{\\frac{n-1}{n}}\\leq\\beta\\leq{\\frac{\\gamma{\\sqrt{m}}}{C_{1}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To satisfy the condition $|\\beta-\\hat{\\beta}|\\leq\\varepsilon\\beta$ we can take $\\begin{array}{r}{\\varepsilon n\\left(\\frac{\\gamma}{2C_{1}}\\right)^{\\frac{n-1}{n}}}\\end{array}$ as the radius of the covering, $C$ . In this case let us derive the upper bound for the size of the covering, $|C|$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n|C|\\leq\\frac{\\sqrt{m}}{\\varepsilon n}\\left(\\frac{\\gamma}{2C_{1}}\\right)^{\\frac{1}{n}}\\leq2\\sqrt{m}\\frac{1}{n}\\left(\\frac{1}{C_{1}}\\right)^{1/n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let us consider cases: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\frac{1}{n}\\left(\\frac{1}{C_{1}}\\right)^{1/n}\\leq\\frac{1}{C_{1}},\\quad C_{1}<1}\\\\ {\\frac{1}{n}\\left(\\frac{1}{C_{1}}\\right)^{1/n}\\leq1,\\quad C_{1}\\geq1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First inequality holds because function $\\frac{a^{1/x}}{x}$ is monotonously decreasing on the interval $(0,+\\infty)$ . So, we can combine: ", "page_idx": 18}, {"type": "equation", "text": "$$\n|C|\\leq2{\\sqrt{m}}\\operatorname*{max}\\left\\{1,{\\frac{1}{C_{1}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is left to apply the union bound argument for the events of Equation 30 happening with $\\tilde{\\beta}$ taking value from ith ball. Let us denote such event $E_{i}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(E_{1}\\;\\&\\;\\ldots\\;\\&\\;E_{|C|}\\right)=1-\\mathbb{P}(\\exists i:\\;\\operatorname{not}E_{i})\\geq1-\\sum_{i=1}^{|C|}\\mathbb{P}(\\operatorname{not}E_{i})\\geq1-|C|\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence with probability at least $1-\\delta$ for all w we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathcal{D},0}(f_{\\mathbf{w}})\\leq\\hat{L}_{\\mathcal{S},\\gamma}(f_{\\mathbf{w}})+}\\\\ &{\\qquad\\qquad+\\mathcal{O}\\left(\\sqrt{\\frac{\\operatorname*{max}\\{1,\\|\\mathbf{w}\\|_{2}^{2}\\}T^{2}\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{1}{S_{i}}\\right)^{2}C_{t}^{2}C_{1}^{2}\\bar{\\eta}^{-2}+\\log\\frac{m}{\\delta}\\operatorname*{max}\\left\\{1,\\frac{1}{C_{1}}\\right\\}}{\\gamma^{2}m}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that we substituted $\\varepsilon$ with $\\frac{1}{2}$ and now $\\operatorname*{max}\\{1,\\frac{1}{C_{1}}\\}$ appearing under logarithm comparing to Equation 30. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 5 (Generalization of Lemma 3). Let $f_{\\mathbf{w}_{1}}^{(1)}:\\mathcal{X}_{1}\\mapsto\\mathbb{R}^{h_{1}},...,f_{\\mathbf{w}_{k}}^{(k)}:\\mathcal{X}_{k}\\mapsto\\mathbb{R}^{h_{k}}$ for $k\\,\\in\\,\\mathbb{N}$ with $\\mathbf{w}_{i}\\,=\\,v e c\\{W_{1}^{(i)},...,W_{n_{i}}^{(i)}\\}$ and $h_{1}\\ =\\ K$ be some models that we can compose, i.e. $\\textit{f}=$ $f^{(1)}(f^{(2)}(\\dots(f^{(k)})\\dots))$ . If ther e exists for $j\\,\\in\\,[k]\\,:T^{(j)}$ depending on $W_{1}^{(j)},...,W_{n_{j}}^{(j)},$ ; for $j~\\in$ $[j]\\,:\\,S_{1}^{(j)},...,S_{n_{j}}^{(1)}\\,\\in\\,\\mathbb{R}$ depending on $W_{1}^{(j)},...,W_{n_{j}}^{(j)}$ ; $\\eta_{1}^{(1)},...,\\eta_{n_{1}}^{(1)},...,\\eta_{1}^{(k)},...,\\eta_{n_{k}}^{(k)}\\;\\in\\;(0,1]$ and $C_{1}^{(1)},...,C_{1}^{(k)}\\in\\mathbb{R},\\,C_{2}^{(1)},...,C_{2}^{(k)}\\in\\mathbb{R}$ C2(k)\u2208R such that: ", "page_idx": 19}, {"type": "text", "text": "maximum output for every model is bounded as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall j\\in[k],\\forall\\mathbf{x}\\in\\mathcal{X}_{j}:\\,\\|f_{\\mathbf{w}_{j}}^{(j)}(\\mathbf{x})\\|_{2}\\leq C_{1}^{(j)}T^{(j)}\\|x\\|_{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and for $j=k$ we can have weaker condition \u2013 the same as in Lemma 2, ", "page_idx": 19}, {"type": "text", "text": "maximum perturbation of the output during small perturbation of weights, $\\lVert U_{i}^{(j)}\\rVert_{2}\\leq\\eta_{i}^{(j)}S_{i}^{(j)},$ , for every model is bounded as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall j\\in[k],\\forall\\mathbf{x},\\Delta\\mathbf{x}\\in\\mathcal{X}_{j}:\\|f_{\\mathbf{w}_{j}+\\mathbf{u}_{j}}(\\mathbf{x}+\\Delta\\mathbf{x})-f_{\\mathbf{w}_{j}}(\\mathbf{x})\\|_{2}\\leq}\\\\ {\\leq C_{2}^{(j)}T^{(j)}\\left(\\|\\Delta\\mathbf{x}\\|_{2}+\\|\\mathbf{x}\\|_{2}\\sum_{i=1}^{n_{j}}\\frac{\\|U_{i}^{(j)}\\|_{2}}{\\|S_{i}^{(j)}\\|_{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and for $j=k$ we can have weaker condition \u2013 the same as in Lemma 2, ", "page_idx": 19}, {"type": "text", "text": "arithmetic mean of inverses of $S_{i}^{(j)}$ is greater than $n_{j}$ -root of $T^{(j)}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall j\\in[k]:\\ \\frac{1}{n_{j}}\\sum_{i=1}^{n_{j}}\\frac{1}{S_{i}^{(j)}}\\geq\\left(\\frac{1}{T^{(j)}}\\right)^{1/n_{j}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\bar{\\eta}^{(j)}:=\\operatorname*{min}_{i\\in[n_{j}]}\\eta_{i}^{(j)}$ is upper-bounded by the ration $C_{1}^{(j)}$ and $C_{2}^{(j)}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall j\\in[k]:\\;\\frac{C_{1}^{(j)}}{2C_{2}^{(j)}}\\geq\\bar{\\eta}^{(j)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we denote $n=\\sum_{j=1}^{k}n_{j}$ , then $f$ meets requirements of Lemma 2 with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle T=\\prod_{i=1}^{k}T^{(i)}}&{\\displaystyle a n d\\quad S_{1:n}=S_{1}^{(1)},...,S_{n_{1}}^{(1)},...,S_{1}^{(k)},...,S_{n_{k}}^{(k)}}\\\\ &{\\displaystyle\\eta_{1:n}=\\eta_{1}^{(1)},...,\\eta_{n_{1}}^{(1)},...,\\eta_{1}^{(k)},...,\\eta_{n_{k}}^{(k)}}\\\\ {\\displaystyle C_{1}=\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}\\|\\mathbf{x}\\|_{2}\\operatorname*{max}\\left\\{\\displaystyle\\prod_{j=1}^{k}C_{1}^{(j)},C\\cdot\\frac{C_{1}^{(\\mathrm{ind})}}{C_{2}^{(\\mathrm{ind})}}\\right\\}}\\\\ &{\\displaystyle C_{2}=\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}\\|\\mathbf{x}\\|_{2}C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ${\\mathrm{ind}}=\\arg\\operatorname*{min}_{j\\in[k]}\\bar{\\eta}^{(j)}$ and $C=\\operatorname*{max}_{j\\in[k]}\\prod_{i=1}^{j}C_{2}^{(i)}\\prod_{i=j+1}^{k}C_{1}^{(i)}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. We denote as $f^{k;j}$ composition of models from $k$ to $j$ ", "page_idx": 20}, {"type": "text", "text": "First we test Equation 6 by plugging in Equation 37 for every model: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}\\|f^{k:1}(\\mathbf{x})\\|_{2}\\leq\\displaystyle\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}C_{1}^{(1)}T^{(1)}\\|f^{k:2}(\\mathbf{x})\\|_{2}\\leq}\\\\ {\\displaystyle\\leq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}\\|\\mathbf{x}\\|_{2}\\displaystyle\\prod_{j=1}^{k}C_{1}^{(i)}T^{(i)}\\leq C_{1}T}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we test Equation 7 by plugging in Equation 38 and Equation 37 for every model: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}||f_{\\mathbf{w}+\\mathbf{u}}^{k;1}(\\mathbf{x})-f_{\\mathbf{w}}^{k;1}(\\mathbf{x})||_{2}\\leq}\\\\ &{\\displaystyle\\leq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}C_{2}^{(1)}T^{(1)}\\left(||f_{\\mathbf{w}+\\mathbf{u}}^{k;2}(\\mathbf{x})-f_{\\mathbf{w}}^{k;2}||_{2}+||f_{\\mathbf{w}}^{k;2}(\\mathbf{x})||_{2}\\left(\\sum_{i=1}^{n_{1}}\\frac{||U_{i}^{(1)}||_{2}}{S_{i}^{(1)}}\\right)\\right)\\leq}\\\\ &{\\displaystyle\\leq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}_{k}}||\\mathbf{x}||_{2}\\prod_{j=1}^{k}T^{(j)}\\left(\\sum_{j=1}^{k}\\prod_{i=1}^{j}C_{2}^{(i)}\\prod_{i=j+1}^{(k)}C_{1}^{(i)}\\sum_{i=1}^{n_{j}}\\frac{||U_{i}^{(j)}||_{2}}{S_{2}^{(j)}}\\right)\\leq}\\\\ &{\\displaystyle\\leq C_{2}T\\sum_{j=1}^{k}\\sum_{i=1}^{n_{j}}\\frac{||U_{i}^{(j)}||_{2}}{S_{i}^{(j)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To test Equation 8 we use arithmetic versus geometric mean inequality: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T\\displaystyle\\sum_{j=1}^{k}\\displaystyle\\sum_{i=1}^{n_{j}}\\displaystyle\\frac{1}{S_{i}^{(j)}}\\geq\\displaystyle\\sum_{j=1}^{k}n_{j}\\left(T^{(j)}\\right)^{\\frac{n_{j}-1}{n_{j}}}\\displaystyle\\prod_{i\\neq j}T^{(i)}\\geq}\\\\ {\\geq n\\left(\\displaystyle\\prod_{j=1}^{k}\\left(\\left(T^{(j)}\\right)^{\\frac{n_{j}-1}{n_{j}}}\\displaystyle\\prod_{i\\neq j}T^{(i)}\\right)^{n_{j}}\\right)^{1/n}\\geq}\\\\ {\\geq n T^{\\frac{n-1}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is left to verify Equation 9. As one can notice $\\begin{array}{r}{\\frac{C_{1}}{2C_{2}}\\geq\\frac{C_{1}^{(\\mathrm{ind})}}{C_{2}^{(\\mathrm{ind})}}\\geq\\bar{\\eta}}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 6 (Generalization of Lemma 4). Let $f_{\\mathbf{w}_{1}}^{(1)}:\\mathcal{X}\\mapsto\\mathbb{R}^{h_{1}},...,f_{\\mathbf{w}_{k}}^{(k)}:\\mathcal{X}\\mapsto\\mathbb{R}^{h_{k}}$ f (wkk) : X  \u2192Rhk with w1 = $v e c\\{W_{1}^{(1)},...,W_{n_{1}}^{(1)}\\},...,{\\bf w}_{k}=v e c\\{W_{1}^{(k)},...,W_{n_{k}}^{(\\bar{k})}\\}$ be some models satisfying Lemma 2 conditions. If $\\mathbf{\\dot{A}}\\mathbf{g}\\mathbf{g}:\\mathbb{R}^{h_{1}}\\times\\ldots\\times\\mathbb{R}^{h_{k}}\\mapsto\\mathbb{R}^{K}$ such that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}_{1},\\mathbf{y}_{1}\\in\\mathbb{R}^{h_{1}},...,\\mathbf{x}_{k},\\mathbf{y}_{k}\\in\\mathbb{R}^{h_{k}}:\\;\\|\\mathrm{Agg}(\\mathbf{x}_{1},...,\\mathbf{x}_{k})-\\mathrm{Agg}(\\mathbf{y}_{1},...,\\mathbf{y}_{k})\\|_{2}\\leq A\\sum_{j=1}^{k}\\|\\mathbf{x}_{j}-\\mathbf{y}_{j}\\|_{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some $A>0$ and $\\operatorname{Agg}(\\mathbf{0},...,\\mathbf{0})=\\mathbf{0}$ . Also we denote $n=\\sum_{j=1}^{k}n_{j}.$ , then $\\operatorname{Agg}(f_{\\mathbf{w}_{1}}^{(1)}(\\cdot),...,f_{\\mathbf{w}_{k}}^{(k)}(\\cdot))$ satisfies Lemma 2 conditions with either: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T=\\operatorname*{max}\\left\\{T^{\\left(1\\right)},...,T^{\\left(k\\right)},\\underset{j=1}{\\prod}T^{\\left(j\\right)}\\right\\}\\quad a n d\\quad S_{1:n}=S_{1}^{\\left(1\\right)},...,S_{n_{1}}^{\\left(1\\right)},...,S_{1}^{\\left(k\\right)},...,S_{n_{k}}^{\\left(k\\right)}}}\\\\ {{\\eta_{1:n}=\\eta_{1}^{\\left(1\\right)},...,\\eta_{n_{1}}^{\\left(1\\right)},...,\\eta_{1}^{\\left(k\\right)},...,\\eta_{n_{k}}^{\\left(k\\right)}}}\\\\ {{C_{1}=A\\operatorname*{max}\\left\\{\\displaystyle\\sum_{j=1}^{k}C_{1}^{\\left(j\\right)},\\displaystyle\\operatorname*{max}_{j\\in\\left[k\\right]}C_{2}^{\\left(j\\right)}\\cdot\\frac{C_{1}^{\\left(\\mathrm{ind}\\right)}}{C_{2}^{\\left(\\mathrm{ind}\\right)}}\\right\\}}}\\\\ {{C_{2}=A\\displaystyle\\operatorname*{max}_{j\\in\\left[k\\right]}C_{2}^{\\left(j\\right)},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "or with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T=\\operatorname*{max}\\left\\{\\displaystyle\\sum_{j=1}^{k}T^{(j)},\\displaystyle\\prod_{j=1}^{k}T^{(j)}\\right\\}\\quad a n d\\quad S_{1:n}=S_{1}^{(1)},...,S_{n_{1}}^{(1)},...,S_{1}^{(k)},...,S_{n_{k}}^{(k)}}\\\\ &{\\qquad\\qquad\\qquad\\eta_{1:n}=\\eta_{1}^{(1)},...,\\eta_{n_{1}}^{(1)},...,\\eta_{1}^{(k)},...,\\eta_{n_{k}}^{(k)}}\\\\ &{\\qquad\\qquad\\qquad C_{1}=A\\operatorname*{max}\\left\\{\\displaystyle\\operatorname*{max}_{j\\in[k]}C_{1}^{(j)},\\displaystyle\\operatorname*{max}_{j\\in[k]}C_{2}^{(j)}\\cdot\\frac{C_{1}^{(\\mathrm{ind})}}{C_{2}^{(\\mathrm{ind})}}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad C_{2}=A\\displaystyle\\operatorname*{max}_{j\\in[k]}C_{2}^{(j)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nw h e r e\\;\\mathrm{ind}=\\arg\\operatorname*{min}_{j\\in[k]}\\bar{\\eta}^{(j)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. First, we test Equation 6 for composite model by applying assumption about $\\mathrm{Agg}$ and its Equation 6 for every $f^{(\\bar{j})}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\|\\mathrm{Agg}(f_{\\mathbf{w}_{1}}^{(1)}(\\mathbf{x}),...,f_{\\mathbf{w}_{k}}^{(k)}(\\mathbf{x}))\\|\\leq A\\sum_{j=1}^{k}\\|f_{\\mathbf{w}_{j}}^{(j)}(\\mathbf{x})\\|_{2}\\leq\\left\\{\\begin{array}{l l}{\\displaystyle A\\sum_{j=1}^{k}T^{(j)}\\operatorname*{max}_{j\\in[k]}C_{1}^{(j)}}&{\\displaystyle\\operatorname*{max}_{j=1}^{k}C_{1}^{(j)}}\\\\ {\\displaystyle A\\operatorname*{max}_{j\\in[k]}T^{(j)}\\sum_{j=1}^{k}C_{1}^{(j)}}&{\\displaystyle\\operatorname*{max}_{j=1}^{k}C_{1}^{(j)}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To test Equation 7 for composite model we again apply assumption about aggregation function and its Equation 7 version for every $f^{(j)}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\operatorname*{max}}\\|\\mathrm{Agg}(f_{\\mathbf{w}_{1}+\\mathbf{u}_{1}}^{(1)}(\\mathbf{x}),...,f_{\\mathbf{w}_{k}+\\mathbf{u}_{k}}^{(k)}(\\mathbf{x}))-\\mathrm{Agg}(f_{\\mathbf{w}_{1}}^{(1)}(\\mathbf{x}),...,f_{\\mathbf{w}_{k}}^{(k)}(\\mathbf{w}))\\|_{2}\\le}\\\\ &{}&{\\displaystyle\\le A\\sum_{j=1}^{k}\\|f_{\\mathbf{w}_{j}+\\mathbf{u}_{j}}^{(j)}(\\mathbf{x})-f_{\\mathbf{w}_{j}}^{(j)}(\\mathbf{x})\\|_{2}\\le A\\sum_{j=1}^{k}C_{2}^{(j)}T^{(j)}\\sum_{i=1}^{n_{j}}\\frac{\\|U_{i}^{(j)}\\|_{2}}{S_{i}^{(j)}}\\le}\\\\ &{}&{\\displaystyle\\le A T\\operatorname*{max}_{j\\in[k]}C_{2}^{(j)}\\sum_{j=1}^{k}\\sum_{i=1}^{n_{i}}\\frac{\\|U_{i}^{(j)}\\|}{S_{i}^{(j)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To test Equation 8 for composite model we employ arithmetic vs geometric mean together with Equation 8 for every model: ", "page_idx": 21}, {"type": "equation", "text": "$$\nT\\sum_{j=1}^{k}\\sum_{i=1}^{n_{j}}\\frac{1}{S_{i}^{(j)}}=\\sum_{j=1}^{k}\\frac{T}{T^{(j)}}\\sum_{i=1}^{n_{j}}\\frac{T^{(j)}}{S_{i}^{(j)}}\\geq\\sum_{j=1}^{k}n_{j}\\frac{T}{(T^{(j)})^{1/n_{j}}}\\geq n\\frac{T}{\\left(\\prod_{j=1}^{k}T^{(j)}\\right)^{1/n}}\\geq n T^{\\frac{n-1}{n}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality comes from the fact that $\\prod_{j=1}^{k}T(j)\\leq T$ ", "page_idx": 22}, {"type": "text", "text": "It is left to test Equation 9. As one can notice in either case $\\begin{array}{r}{\\frac{C_{1}}{2C_{2}}\\geq\\frac{C_{1}^{(\\mathrm{ind})}}{2C_{2}^{(\\mathrm{ind})}}\\geq\\bar{\\eta}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "D Perturbation analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 7. Let $\\mathbf{w}=v e c\\{W_{1},...,W_{n}\\}$ be a vector of weight matrices of an $n$ -layer MLP, $f_{\\mathbf{w}}:\\mathcal{X}\\mapsto$ $\\mathbb{R}^{K}$ . Let $\\psi_{i}$ be a $L i p_{i}$ -Lipschitz activation function after layer $i,$ , for $i\\,\\in\\,[n-1]$ . Then for any input and input perturbation, $\\mathbf{x}$ , $\\Delta\\mathbf{x}\\in\\mathcal{X}$ , weight perturbation, ${\\bf u}=v e c\\{U_{1},...,U_{n}\\}$ , and constants, $\\eta_{1},...,\\eta_{n}$ , such that for $i\\in[n]:\\ \\|U_{i}\\|_{2}\\leq\\eta_{i}\\|W_{i}\\|_{2}$ , we have two inequalities: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|f_{\\mathbf{w}}(\\mathbf{x})\\|_{2}\\leq\\left(\\prod_{i=1}^{n}L i p_{i}\\|W_{i}\\|_{2}\\right)|\\mathbf{x}|_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|f_{\\mathbf{w}+\\mathbf{u}}(\\mathbf{x}+\\Delta\\mathbf{x})-f_{\\mathbf{w}}(\\mathbf{x})\\|_{2}\\le\\left(\\prod_{i=1}^{n}1+\\eta_{i}\\right)\\prod_{i=1}^{n}L i p_{i}\\|W_{i}\\|_{2}\\left(|\\Delta\\mathbf{x}|_{2}+|\\mathbf{x}|_{2}\\sum_{i=1}^{n}\\frac{\\|U_{i}\\|_{2}}{\\|W_{i}\\|_{2}}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we denote $L i p_{n}=1$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. The proof follows the Neyshabur et al. [40] with some modifications concerning the $\\Delta\\mathbf{x}$ and $\\mathrm{Lip}_{i}$ and $\\eta_{i}$ . ", "page_idx": 22}, {"type": "text", "text": "We denote truncated $f$ after $i+1$ th layer as $f^{(i+1)}$ . First, we provide the bound on the norm of the output after $i+1$ th layer: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f_{\\mathbf{w}}^{(i+1)}(\\mathbf{x})\\|_{2}\\overset{(i)}{=}\\|W_{i+1}(\\psi_{i}(f_{\\mathbf{w}}^{(i)}(\\mathbf{x}))\\|_{2}\\overset{(i i)}{\\leq}\\|W_{i+1}\\|_{2}\\|\\psi_{i}(f_{\\mathbf{w}}^{(i)}(\\mathbf{x}))\\|_{2}\\overset{(i i i)}{\\leq}}\\\\ &{\\qquad\\qquad\\qquad\\overset{(i i i)}{\\leq}\\|W_{i+1}\\|_{2}\\mathrm{Lip}_{i}\\|f_{\\mathbf{w}}^{(i)}(\\mathbf{x})\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) is the definition of MLP, (ii) comes from the definition of operator norm, (iii) comes from Lipschitzness. Unrolling the recursion of Equation 49 will get us (we denote $f_{\\mathbf{w}}^{(0)}(\\mathbf{x})$ to be $\\mathbf{x}$ ): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|f_{\\mathbf{w}}^{(i+1)}(\\mathbf{x})\\|_{2}\\leq\\left(\\prod_{k=1}^{i+1}\\|W_{k}\\|_{2}\\right)\\left(\\prod_{k=1}^{i}\\mathrm{Lip}_{k}\\right)|\\mathbf{x}|_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now let us provide the bound for the perturbation after $i+1$ th layer. We denote this perturbation as $\\Delta_{i+1}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{i+1}\\overset{(i)}{=}\\left\\|f_{\\mathbf{w}+\\mathbf{u}}^{(i+1)}(\\mathbf{x}+\\Delta\\mathbf{x})-f_{\\mathbf{w}}^{(i+1)}(\\mathbf{x})\\right\\|\\overset{(i i)}{=}}\\\\ &{\\overset{(i i)}{=}\\left\\|(W_{i+1}U_{i+1})\\psi_{i}(f_{\\mathbf{w}+\\mathbf{u}}^{(i)}\\mathbf{x}(\\Delta\\mathbf{x}+\\Delta\\mathbf{x}))-W_{i+1}\\psi_{i}((f_{\\mathbf{w}}^{(i)}\\mathbf{x}))\\right\\|_{2}\\overset{(i i i)}{=}}\\\\ &{\\overset{(i i i)}{=}\\left\\|(W_{i+1}+U_{i+1})(\\psi_{i}(f_{\\mathbf{w}+\\mathbf{u}}^{(i)}\\mathbf{a}(\\mathbf{x}+\\Delta\\mathbf{x}))-\\psi_{i}(f_{\\mathbf{w}}^{(i)}\\mathbf{x}(\\mathbf{x})))+U_{i+1}\\psi_{i}(f_{\\mathbf{w}}^{(i)}\\mathbf{x})\\right\\|_{2}\\overset{(i i i)}{\\leq}}\\\\ &{\\overset{(i v)}{\\leq}\\left\\|W_{i+1}+U_{i+1}\\right\\|_{2}\\|\\psi_{i}(f_{\\mathbf{w}+\\mathbf{u}}^{(i)}\\mathbf{a}(\\mathbf{x}+\\Delta\\mathbf{x}))-\\psi_{i}(f_{\\mathbf{w}}^{(i)}\\mathbf{x})\\|_{2}+\\left\\|U_{i+1}\\right\\|_{2}\\|\\psi_{i}(f_{\\mathbf{w}}^{(i)}\\mathbf{x})\\|_{2}\\overset{(v)}{\\leq}}\\\\ &{\\overset{(v)}{\\leq}(1+\\eta_{i+1})\\left\\|W_{i+1}\\right\\|_{2}\\mathrm{Lip}_{i}\\Delta_{i}+\\|U_{i+1}\\|_{2}\\|\\mathbf{x}\\|_{2}\\overset{(i i)}{\\prod}_{k=1}\\overset{\\hat{i}}{\\prod}\\mathrm{Lip}_{k}\\overset{(v)}{\\prod}\\frac{(v)}{k=}}\\\\ &{\\overset{(v i)}{=}(1+\\eta_{i+1})\\|W_{i+1}\\|_{2}\\mathrm{Lip}_{i}\\Delta_{i}+|\\mathbf{x}|_{2}\\left(\\overset{(i+1)}{\\underset{k=1}{\\prod}}\\|W_{k}\\|_{2}\\right)\\left(\\overset\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (i) comes from the definition of $\\Delta_{i+1}$ , (ii) comes from the defintion of an MLP, (iii) add and substract $U_{i+1}\\psi_{i}\\big(f_{\\mathbf{w}}^{(i)}(\\mathbf{x})\\big)$ , (iv) comes from triangle inequality combined with operator norm definition, (v) comes from the definition of $\\eta_{i+1}$ , Lipschitzness and Equation 50, (vi) comes from multiplying and dividing by $\\lVert W_{i+1}\\rVert_{2}$ the second term. Now we unroll the recursion of Equation 51 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}_{\\lambda+1}\\stackrel{(i)}{\\leq}(1+\\eta_{i+1})\\|W_{i+1}\\|_{2}\\mathrm{Li}\\mathrm{p}_{i}\\Delta_{i}+\\|\\mathbf{x}\\|_{2}\\left(\\prod_{k=1}^{i+1}\\|W_{k}\\|_{2}\\right)\\left(\\prod_{k=1}^{i}\\mathrm{Lip}_{k}\\right)\\frac{\\|U_{i+1}\\|_{2}}{\\|W_{i+1}\\|_{2}}\\stackrel{(i i)}{\\leq}\\right.}\\\\ &{\\qquad\\stackrel{(i i)}{\\leq}\\left(\\prod_{k=1}^{i+1}1+\\eta_{k}\\right)\\left(\\prod_{k=1}^{i+1}\\|W_{k}\\|_{2}\\right)\\left(\\prod_{k=1}^{i}\\mathrm{Lip}_{k}\\right)\\|\\Delta\\mathbf{x}\\|_{2}+\\|\\mathbf{x}\\|_{2}\\left(\\prod_{k=1}^{i+1}\\|W_{k}\\|_{2}\\right)\\left(\\prod_{k=1}^{i}\\mathrm{Lip}_{k}\\right)\\left(\\sum_{k=1}^{i+1}\\mathrm{Lip}_{k}\\right)\\left(\\prod_{k=1}^{i}\\mathrm{Lip}_{k}\\right)}\\\\ &{\\qquad\\stackrel{(i i i)}{\\leq}\\left(\\prod_{k=1}^{i+1}1+\\eta_{k}\\right)\\left(\\prod_{k=1}^{i+1}\\|W_{k}\\|_{2}\\right)\\left(\\prod_{k=1}^{i}\\mathrm{Lip}_{k}\\right)\\left(\\|\\Delta\\mathbf{x}\\|_{2}+\\|\\mathbf{x}\\|_{2}\\sum_{k=1}^{i+1}\\frac{\\|U_{k}\\|_{2}}{\\|W_{k}\\|_{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (i) comes from Equation 51, (ii) comes from unrolling the recursion $\\Delta_{i}$ , (iii) comes from the fact that $1+\\eta_{k}\\ge1$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma 8 (GCN perturbation analysis). Let $f_{\\mathbf{w}}:\\mathcal{X}\\mapsto\\mathbb{R}^{K}$ with $\\mathbf{w}=v e c\\{W_{1},...,W_{n}\\}$ be a $n$ -layer GCN model. Let $\\psi_{i}$ be $L i p_{i}$ activation functions after layer $i\\in[n-1]$ . If ${\\bf u}=v e c\\{U_{1},...,U_{n}\\}$ , $B\\,\\in\\,\\mathbb{R}$ are such that: $\\begin{array}{r}{\\forall i\\,\\in\\,[n]\\,:\\,\\,\\|U_{i}\\|_{2}\\,\\le\\,\\frac{1}{n}\\|W_{i}\\|_{2}}\\end{array}$ and $\\forall G\\,\\in\\,\\mathcal{X},\\forall v\\,\\in\\,G\\,:\\,\\,|z_{v}|_{2}\\,\\leq\\,B$ , and $\\forall G\\in\\mathcal{X}$ , $G$ is simple and have maximum degree of $d$ , then for any $G=(V,E,z)\\in\\mathcal{X}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|f_{\\mathbf{w}+\\mathbf{u}}(G)-f_{\\mathbf{w}}(G)|_{2}\\leq\\frac{1}{\\sqrt{|V|}}\\|Z\\|_{F}\\|P_{G}\\|_{2}^{n-1}\\left(\\prod_{i=1}^{n}L i p_{i}\\|W_{i}\\|_{2}\\right)\\sum_{i=1}^{n}\\frac{\\|U_{i}\\|_{2}}{\\|W_{i}\\|_{2}}\\leq}}\\\\ &{}&{\\leq e B\\left(\\prod_{i=1}^{n}L i p_{i}\\|W_{i}\\|_{2}\\right)\\sum_{i=1}^{n}\\frac{\\|U_{i}\\|_{2}}{\\|W_{i}\\|_{2}},\\leq}\\\\ &{}&{\\leq e B d^{(n-1)/2}\\left(\\prod_{i=1}^{n}L i p_{i}\\|W_{i}\\|_{2}\\right)\\sum_{i=1}^{n}\\frac{\\|U_{i}\\|_{2}}{\\|W_{i}\\|_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $Z\\in\\mathbb{R}^{|V|\\times d_{z}}$ is the matrix consisting of $z_{v}$ for $v\\in V$ , and $P_{G}$ is a Laplacian of the graph $G$ which is equal to $\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$ where $\\bar{\\boldsymbol{A}}$ is an adjacency matrix with $+1$ on the diagonal and $\\tilde{D}=d i a g\\left(\\sum_{j=1}^{|V|}\\tilde{A}_{i j},i\\in[|V|]\\right)$ and $d$ is maximum degree of a graph in $\\mathcal{X}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. The proof follows [48] except for the fact that we add $\\mathrm{Lip}_{i}$ to the bound. This change is straightforward, however, for the completeness of the picture we provide this proof here. The detailed description of the architectures can be found in Appendix B. ", "page_idx": 23}, {"type": "text", "text": "First we prove two helpful propositions ", "page_idx": 23}, {"type": "text", "text": "Proposition 1. For any matrix $A\\in\\mathbb{R}^{n\\times m},B\\in\\mathbb{R}^{m\\times p}$ , we have, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|A B\\|_{F}\\leq\\|A\\|_{F}\\|B\\|_{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $x_{i}^{\\top},a_{i}^{\\top}$ be $i$ th row of $A B$ and $A$ respectively, then we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|A B\\|_{F}^{2}\\leq\\sum_{i=1}^{n}\\|x_{i}^{\\top}|_{2}^{2}=\\sum_{i=1}^{n}\\|a_{i}^{\\top}B\\|_{2}^{2}\\leq\\sum_{i=1}^{n}\\|a_{i}^{\\top}\\|_{2}^{2}\\|B\\|_{2}^{2}=\\|A\\|_{F}^{2}\\|B\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proposition 2. For any undirected graph $G=(V,E)$ , let $A\\in\\mathbb{R}^{|V|\\times|V|}$ be the adjacency matrix, $D=\\mathrm{diag}(D_{1},\\ldots,D_{|V|})$ be the degree matrix, and $\\begin{array}{r}{d=\\operatorname*{max}_{i\\in[|V|]}\\{D_{i}\\}}\\end{array}$ be the maximum degree, ", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{D_{i}=\\sum_{j=1}^{|V|}A_{i j},\\,i\\in[|V|]}\\end{array}$ . Then we have, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left.\\begin{array}{l}{\\left.\\gamma^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}\\right\\|_{2}\\leq1,\\ w h e r e\\,\\tilde{A}=A+I\\,a n d\\,\\tilde{D}=\\mathrm{diag}(\\tilde{D}_{1},\\ldots,\\tilde{D}_{|V|})=\\mathrm{diag}\\left(\\displaystyle\\sum_{j=1}^{|V|}\\tilde{A}_{i j},i\\in[|V|]\\right).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. For (i), by the definition of spectral norm, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|A\\|_{2}=\\operatorname*{max}_{\\|x\\|_{2}=1}x^{\\top}A x=\\operatorname*{max}_{\\|x\\|_{2}=1}\\sum_{(i,j)\\in E}x_{i}x_{j}\\leq\\operatorname*{max}_{\\|x\\|_{2}=1}\\sum_{(i,j)\\in E}\\frac{x_{i}^{2}+x_{j}^{2}}{2}\\leq\\operatorname*{max}_{\\|x\\|_{2}=1}d\\sum_{i\\in V}x_{i}^{2}=d.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For (ii), let $\\tilde{E}=E\\cup\\{(i,i)|i\\in V\\}$ be the edge set associated with the adjacency matrix $\\tilde{A}$ . By the definition of spectral norm, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|D^{-\\frac{1}{2}}\\tilde{A}D^{-\\frac{1}{2}}\\right\\|_{2}=\\underset{\\|x\\|_{2}=1}{\\operatorname*{max}}x^{\\top}\\left(D^{-\\frac{1}{2}}\\tilde{A}D^{-\\frac{1}{2}}\\right)x=\\underset{\\|x\\|_{2}=1}{\\operatorname*{max}}\\sum_{(i,j)\\in\\tilde{E}}\\frac{x_{i}x_{j}}{\\sqrt{\\tilde{D}_{i}\\tilde{D}_{j}}}\\leq}\\\\ &{\\qquad\\qquad\\leq\\underset{\\|x\\|_{2}=1}{\\operatorname*{max}}\\sum_{(i,j)\\in\\tilde{E}}\\left(\\frac{x_{i}^{2}}{2\\tilde{D_{i}}}+\\frac{x_{j}^{2}}{2\\tilde{D_{j}}}\\right)\\leq\\underset{\\|x\\|_{2}=1}{\\operatorname*{max}}\\sum_{i\\in V}x_{i}^{2}=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Before proving the first inequality we note that ${\\frac{1}{\\sqrt{|V|}}}\\|X\\|_{F}\\leq B$ and $\\|P_{G}\\|_{2}^{n-1}\\leq1$ , so the second inequality is rather straightforward. The last inequality comes from the fact that $d\\geq1$ . ", "page_idx": 24}, {"type": "text", "text": "We denote the node representation in the $j$ -th $(j\\leq l)$ layer as ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{\\mathbf{w}}^{j}(G)=H_{j}=\\psi_{j}(P_{G}H_{j-1}W_{j}),\\quad j\\in[n-1],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{\\mathbf{w}}^{n}(G)=H_{n}={\\frac{1}{|V|}}\\mathbf{1}_{|V|}H_{n-1}W_{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Adding perturbation $\\mathbf{u}$ to the parameter w, that is, for the $j$ -th $(j\\leq n)$ layer, the perturbed parameters are $W_{j}+U_{j}$ and denote $H_{j}^{\\prime}=f_{\\mathbf{w}+\\mathbf{u}}^{j}(G),j\\in[n]$ . ", "page_idx": 24}, {"type": "text", "text": "Upper Bound on the Node Representation. For any $j<n$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert H_{j}\\Vert_{F}=\\Vert\\psi_{j}(P_{G}H_{j-1}W_{j})\\Vert_{F}\\leq}&{}\\\\ {\\leq\\mathrm{Lip}_{j}\\Vert P_{G}H_{j-1}W_{j}\\Vert_{F}\\leq}&{}\\\\ {\\leq\\mathrm{Lip}_{j}\\Vert P_{G}H_{j-1}\\Vert_{F}\\Vert W_{j}\\Vert_{2}\\leq}&{}\\\\ {\\leq\\mathrm{Lip}_{j}\\Vert P_{G}\\Vert_{2}\\Vert H_{j-1}\\Vert_{F}\\Vert W_{j}\\Vert_{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality holds since $\\psi_{j}$ is a Lipschitz and $\\psi_{j}(0)=0$ , and the second and the last ones hold by Proposition 1. Then, unrolling the recursion and setting $H_{0}=X$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|H_{j}\\|_{F}\\leq\\|P_{G}\\|_{2}^{j}\\|H_{0}\\|_{F}\\displaystyle\\prod_{i=1}^{j}\\mathrm{Lip}_{i}\\|W_{i}\\|_{2}\\leq}\\\\ &{\\qquad\\qquad\\leq\\|Z\\|_{F}\\|P_{G}\\|_{2}^{j}\\displaystyle\\prod_{i=1}^{j}\\mathrm{Lip}_{i}\\|W_{i}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Upper Bound on the Change of Node Representation. For any $j<|V|$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|H_{j}^{\\prime}-H_{j}\\|_{F}=\\|\\psi_{j}(P_{G}H_{j-1}^{\\prime}(W_{j}+U_{j}))-\\psi_{j}(P_{G}H_{j-1}W_{j})\\|_{F}}\\\\ {\\le\\mathrm{Lip}_{j}\\|P_{G}H_{j-1}^{\\prime}(W_{j}+U_{j})-P_{G}H_{j-1}W_{j}\\|_{F}\\quad\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the triangle inequality, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|H_{j}^{\\prime}-H_{j}\\|_{F}\\le\\mathrm{Lip}_{j}\\|P_{G}(W_{j}+U_{j})(H_{j-1}^{\\prime}-H_{j-1})\\|_{F}+\\|P_{G}H_{j-1}U_{j}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first term can be bounded as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|P_{G}(H_{j-1}^{\\prime}-H_{j-1})(W_{j}+U_{j})\\|_{F}=\\|P_{G}\\|_{2}\\|H_{j-1}^{\\prime}-H_{j-1}\\|_{F}\\|W_{j}+U_{j}\\|_{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the second term can be bounded as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|P_{G}H_{j-1}U_{j}\\|_{F}=\\|P_{G}\\|_{2}\\|H_{j-1}\\|_{F}\\|U_{j}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\|H_{j}^{\\prime}-H_{j}\\|_{F}\\leq\\mathrm{Lip}_{j}\\|P_{G}\\|_{2}\\|H_{j-1}^{\\prime}-H_{j-1}\\|_{F}\\|W_{j}+U_{j}\\|_{2}+\\mathrm{Lip}_{j}\\|P_{G}\\|_{2}\\|H_{j-1}\\|_{F}\\|U_{j}\\|_{2}.}\\end{array}$ (56) Unrolling the recursion while simplifying notation as: $\\|H_{j}-H_{j}^{\\prime}\\|_{F}\\leq a_{j-1}\\|H_{j-1}^{\\prime}-H_{j-1}\\|_{F}+b_{j-1}$ we get: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|H_{j}^{\\prime}-H_{j}\\|_{F}\\leq\\sum_{k=0}^{j-1}b_{k}\\left(\\prod_{i=k+1}^{j-1}a_{i}\\right)=}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{k=0}^{j-1}\\mathrm{Lip}_{k+1}\\|P_{G}\\|\\|H_{k}\\|_{F}\\|U_{k+1}\\|_{2}\\left(\\prod_{i=k+1}^{j-1}\\mathrm{Lip}_{i+1}\\|P_{G}\\|_{2}\\|W_{i+1}+U_{i+1}\\|_{2}\\right)=}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{k=0}^{j-1}\\|P_{G}\\|_{2}^{j-k}\\|H_{k}\\|_{F}\\|U_{k+1}\\|_{2}\\prod_{i=k}^{j-1}\\mathrm{Lip}_{i+1}\\prod_{i=k+1}^{j-1}\\|W_{i+1}+U_{i+1}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging in Equation 54, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|H_{j}^{\\prime}-H_{j}\\|_{F}\\leq\\prod_{i=1}^{j}\\operatorname*{Lip}_{k=0}^{j-1}\\|P_{G}\\|_{2}^{j-k}\\left(\\|P_{G}\\|^{k}\\|Z\\|_{F}\\prod_{i=1}^{k}\\|W_{i}\\|_{2}\\right)\\|U_{k+1}\\|_{2}\\prod_{i=k+1}^{j-1}\\|W_{i}+U_{i}\\|_{2}\\leq}}\\\\ &{}&{\\leq\\|Z\\|_{F}\\prod_{i=1}^{j}\\operatorname*{Lip}_{k=0}^{j-1}\\|P_{G}\\|_{2}^{j}\\frac{\\|U_{k+1}\\|_{2}}{\\|W_{k+1}\\|_{2}}\\prod_{i=1}^{k+1}\\|W_{i}\\|_{2}\\prod_{i=k+1}^{j-1}\\left(1+\\frac{1}{n}\\right)\\|W_{i}\\|_{2}=}\\\\ &{}&{=\\|Z\\|_{F}\\|P_{G}\\|_{2}^{j}\\prod_{i=1}^{j}\\|W_{i}\\|_{2}\\prod_{i=1}^{j}\\operatorname*{Lip}_{k=1}^{j}\\frac{\\|U_{k}\\|_{2}}{\\|W_{k}\\|_{2}}\\left(1+\\frac{1}{n}\\right)^{j-k}\\quad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Final Bound on the Readout Layer. ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|H_{n}^{\\prime}-H_{n}\\|_{2}=\\left\\|\\frac{1}{|V|}\\mathbf1_{|V|}H_{n-1}^{\\prime}(W_{n}+U_{n})-\\frac1{|V|}\\mathbf1_{|V|}H_{n-1}W_{n}\\right\\|_{2}=}}\\\\ &{=\\left\\|\\frac1{|V|}\\mathbf1_{|V|}(H_{n-1}^{\\prime}-H_{n-1})(W_{n}+U_{n})+\\frac1{|V|}\\mathbf1_{|V|}H_{n-1}U_{n}\\right\\|_{2}\\leq}\\\\ &{\\leq\\left\\|\\frac1{|V|}\\mathbf1_{|V|}(H_{n-1}^{\\prime}-H_{n-1})(W_{n}+U_{n})\\right\\|_{2}+\\left\\|\\frac1{|V|}\\mathbf1_{|V|}H_{n-1}U_{n}\\right\\|_{2}\\leq}\\\\ &{\\leq\\left\\|\\frac1{|V|}\\mathbf1_{|V|}\\right\\|_{2}\\left\\|(H_{n-1}^{\\prime}-H_{n-1})(W_{n}+U_{n})\\right\\|_{2}+\\left\\|\\frac1{|V|}\\mathbf1_{|V|}\\right\\|_{2}\\left\\|H_{n-1}U_{n}\\right\\|_{2}\\leq}\\\\ &{\\leq\\frac1{\\sqrt{|V|}}\\|H_{n-1}^{\\prime}-H_{n-1}\\|_{F}\\|W_{n}+U_{n}\\|_{2}+\\frac1{\\sqrt{|V|}}\\|H_{n-1}\\|_{F}\\|U_{n}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where in a last inequality we first apply that $\\|A\\|_{2}\\leq\\|A\\|_{F}$ for any matrix $A$ and then use Proposition 1. ", "page_idx": 25}, {"type": "text", "text": "Using Equation 54 and Equation 57, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{X{\\mathrm{~sorted~by~}}f\\colon\\quad\\cdots\\;\\;\\left[{m(k)}\\right]\\,\\cdots\\;{\\Big[}{n(k)}{\\Big]}\\,\\cdots}\\\\ {X{\\mathrm{~sorted~by~}}g\\colon\\quad\\cdots\\;\\;\\left[{n(k)}\\right]^{\\!\\!\\!}\\cdots{\\Big[}{m(k)}{\\Big]}\\,\\cdots.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{H_{n}^{\\prime}-H_{n}\\|_{2}\\leq\\frac{1}{\\sqrt{|V|}}\\|\\mathbf{W}_{n}+U_{n}\\|_{2}\\|X\\|_{F}\\|_{\\mathcal{C}}\\|_{\\mathcal{C}}\\|^{n-1}\\displaystyle\\prod_{i=1}^{n-1}\\mathrm{Lip}_{i}\\|W_{i}\\|_{2}\\displaystyle\\sum_{k=1}^{n-1}\\frac{\\|U_{k}\\|_{2}}{\\|W_{k}\\|_{2}}\\Big(1+\\frac{1}{n}\\Big)^{n-k-1}+}&{}\\\\ &{+\\displaystyle\\frac{1}{\\sqrt{|V|}}\\|U_{n}\\|_{2}\\|P_{G}\\|_{2}^{n-1}\\|X\\|_{F}\\displaystyle\\prod_{i=1}^{n-1}\\mathrm{Lip}_{i}\\|W_{i}\\|_{2}\\leq}\\\\ &{\\leq\\frac{1}{\\sqrt{|V|}}\\|^{2}\\|e\\|P_{G}\\|^{n-1}\\left[\\|W_{n}+U_{n}\\|_{2}\\displaystyle\\sum_{l=1}^{n-1}\\mathrm{Lip}_{\\|W_{k}\\|_{2}}\\|\\sum_{k=1}^{n-1}\\frac{\\|U_{k}\\|_{2}}{\\|W_{k}\\|_{2}}\\left(1+\\frac{1}{n}\\right)^{n-k-1}+\\right.}\\\\ &{\\left.=\\frac{1}{\\sqrt{|V|}}\\|^{2}\\|e\\|P_{G}\\|^{n-1}\\displaystyle\\prod_{i=1}^{n}\\mathrm{Lip}_{\\|W_{i}\\|_{2}}\\left[\\frac{\\|W_{n}-U_{n}\\|_{2}}{\\|W_{k}\\|_{2}}\\sum_{l=1}^{n-1}\\frac{\\|U_{k}\\|_{2}}{\\|W_{k}\\|_{2}}\\left(1+\\frac{1}{n}\\right)^{n-k-1}+\\right.}\\\\ &{\\leq\\frac{1}{\\sqrt{|V|}}\\|^{2}\\|e\\|P_{G}\\|_{2}^{n-1}\\displaystyle\\prod_{i=1}^{n}\\mathrm{Lip}_{\\|W_{i}\\|_{2}}\\Big[\\Big(1+\\frac{1}{n}\\Big)\\displaystyle\\sum_{k=1}^{n-1}\\frac{\\|U_{k}\\|_{2}}{\\|W_{k}\\|_{2}}\\Big(1+\\frac{1}{n}\\Big)^{n-k-1}+\\frac{\\|U_{k}\\|_{2} \n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we set $\\mathrm{Lip}_{n}=1$ for simplicity of notation and last inequality holds since $\\begin{array}{r}{1\\leq(1+\\frac{1}{n})^{n}\\leq e}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Our next lemma helps us to upper bound the PERSLAY\u2019s perturbation, when ${\\mathrm{Agg}}=k\\cdot$ - max. Lemma 9. Let $X$ be an arbitrary finite set and $f,g:X\\mapsto\\mathbb{R}$ . Then we can say that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n|k-\\operatorname*{max}_{x\\in X}f(x)-k-\\operatorname*{max}_{x\\in X}g(x)|\\leq3\\operatorname*{max}_{x\\in X}|f(x)-g(x)|\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Denote $n:\\mathbb{N}\\mapsto X$ by a function that maps natural number $k$ to an element of $X$ that would be on $k$ th position in order sorted by $f$ . Denote $m$ as an analogous function but for $g$ . Then, we are interested in the following expression: $|f(n(k))-g(m(k))|$ . Let us rewrite it: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert f(n(k))-g(m(k))\\vert=\\vert f(n(k))-g(n(k))+g(n(k))-g(m(k))\\vert\\leq}&{}\\\\ {\\leq\\vert f(n(k))-g(n(k))\\vert+\\vert g(n(k))-g(m(k))\\vert\\leq}&{}\\\\ {\\leq\\underset{x\\in X}{\\operatorname*{max}}\\vert f(x)-g(x)\\vert+\\vert g(n(k))-g(m(k))\\vert}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, the task is to prove that $|g(n(k))-g(m(k))|\\leq2\\operatorname*{max}_{x\\in X}|f(x)-g(x)|$ Let us consider four cases: ", "page_idx": 26}, {"type": "text", "text": "\u2022 $g(n(k))\\;>\\;g(m(k))$ and $f(n(k))\\;\\geq\\;f(m(k))$ (Fig. 5). In this case $\\exists i\\in\\mathbb{N}$ such that $f(n(i))>f(n(k))$ and $g(n(i))<g(m(k))$ . Indeed, if none of the elements \"to the right\" ", "page_idx": 26}, {"type": "text", "text": "of $n(k)$ moved \"to the left\" of $m(k)$ , then \"to the right\" of $m(k)$ , there are at least $n-k+1$ elements; however, there are must be exactly $n-k$ elements. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left|g(n(k))-g(m(k))\\right|=g(n(k))-g(m(k))\\leq g(n(k))-g(n(i))\\leq}\\\\ {\\qquad\\qquad\\leq f(n(k))+\\displaystyle(\\operatorname*{max}_{x\\in X}|f(x)-g(x)|)-g(n(i))<}\\\\ {\\qquad\\qquad\\qquad<f(n(i))+\\displaystyle(\\operatorname*{max}_{x\\in X}|f(x)-g(x)|)-g(n(i))<2\\displaystyle(\\operatorname*{max}_{x\\in X}|f(x)-g(x)|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 $g(n(k))>g(m(k))$ and $f(n(k))\\leq f(m(k))$ (Fig. 6) ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{g(n(k))-g(m(k))|=g(n(k))-g(m(k))\\leq}&{}\\\\ {\\leq f(n(k))+\\displaystyle\\left(\\underset{x\\in X}{\\operatorname*{max}}\\,|f(x)-g(x)|\\right)-g(m(k))\\leq}&{}\\\\ {\\leq f(m(k))+\\displaystyle\\left(\\underset{x\\in X}{\\operatorname*{max}}\\,|f(x)-g(x)|\\right)-g(m(k))\\leq}&{}\\\\ {\\leq2\\left(\\underset{x\\in X}{\\operatorname*{max}}\\,|f(x)-g(x)|\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 The rest of the cases can be handled analogously. ", "page_idx": 27}, {"type": "text", "text": "Lemma 10 (Perturbation analysis of PersLay). Let $f_{\\mathbf{w}}:\\mathcal{G}\\mapsto\\mathbb{R}^{k}$ with $\\mathbf{w}=\\{W^{(\\omega)},W^{(\\varphi)}\\}$ be $a$ PersLay where $W^{(\\omega)}$ is a parameter vector (matrix) of weight function, $\\omega$ , and $W^{(\\varphi)}$ is a parameter vector (matrix) of point-transformation function, $\\varphi$ . Let $\\mathrm{Dg}$ be a mapping from graphs to (extended) persistence diagrams with a fixed filtration function and $B$ such that max max $\\|p\\|_{2}\\leq B$ , then: $G\\!\\in\\!{\\mathcal{G}}\\ p\\!\\in\\!{\\mathrm{Dg}}(G)$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G}}|f_{\\mathbf{w}}(G)|_{2}\\leq A_{1}C^{(\\omega)}T^{(\\omega)}C^{(\\varphi)}T^{(\\varphi)}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and for $\\eta^{(\\omega)}$ and $\\mathbf{u}=\\{U^{(\\omega)},U^{(\\varphi)}\\}$ such that $\\|U^{(\\omega)}\\|_{2}\\leq\\eta^{(\\omega)}T^{(\\omega)}$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{G\\in\\mathcal{G}}{\\operatorname*{max}}\\,|\\,f_{\\mathbf{w}+\\mathbf{u}}(G)-f_{\\mathbf{w}}(G)|_{2}\\leq}\\\\ &{\\leq A_{2}\\operatorname*{max}\\{C^{(\\omega)}L i p^{(\\varphi)},C^{(\\varphi)}L i p^{(\\omega)}\\}(1+\\eta^{(\\omega)})T^{(\\omega)}T^{(\\varphi)}\\left(\\frac{\\|U^{(\\varphi)}\\|_{2}}{T^{(\\varphi)}}+\\frac{\\|U^{(\\omega)}\\|_{2}}{T^{(\\omega)}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $A_{1}\\:=\\:\\operatorname*{max}_{G\\in{\\mathcal{G}}}{c a r d(\\mathrm{Dg}(G))}$ if Agg is sum and $A_{1}\\;=\\;1\\;i f$ Agg is $k$ -max or mean; $A_{2}{\\bf\\Psi}={\\bf\\Psi}$   \n$\\operatorname*{max}_{G\\in\\mathcal{G}}c a r d(\\mathrm{Dg}(G))\\;i f\\,\\mathrm{Agg}$ is sum or $A_{2}=3$ if Agg is $k$ -max or mean; ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{(T^{(\\varphi)},C^{(\\varphi)},L i p^{(\\varphi)})=(\\operatorname*{max}\\{1,\\|W^{(\\varphi)}\\|_{2}\\},\\sqrt{h}B,1)}&&{\\varphi=\\Lambda}\\\\ &{(T^{(\\varphi)},C^{(\\varphi)},L i p^{(\\varphi)})=(\\operatorname*{max}\\{1,\\|W^{(\\varphi)}\\|_{2}\\},\\sqrt{h},\\tau e^{-1/2})}&&{\\varphi=\\Gamma}\\\\ &{(T^{(\\varphi)},C^{(\\varphi)},L i p^{(\\varphi)})=(\\|W^{(\\varphi)}\\|_{2},\\sqrt{3}\\operatorname*{max}\\{1,B\\},\\operatorname*{max}\\{1,B\\})}&&{\\varphi=L}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$T^{(\\omega)}=\\operatorname*{max}\\{1,\\|W^{(\\omega)}\\|_{2}\\},$ ; and $C^{(\\omega)},L i p^{(\\omega)}$ are such that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G}}\\operatorname*{max}_{p\\in\\mathrm{Dg}(G)}|\\omega_{\\mathbf{w}}(p)|_{2}\\le C^{(\\omega)}T^{(\\omega)}\\quad a n d\\quad\\operatorname*{max}_{G\\in\\mathcal{G}}\\operatorname*{max}_{p\\in\\mathrm{Dg}(G)}|\\omega_{\\mathbf{w}+\\mathbf{u}}(p)-\\omega_{\\mathbf{w}}(p)|\\le L i p^{(\\omega)}\\|U^{(\\omega)}\\|_{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any w and u. ", "page_idx": 27}, {"type": "text", "text": "Proof. First we prove the inequaliry about maximum output norm. ", "page_idx": 27}, {"type": "text", "text": "Maximum output norm. ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{G\\in\\mathcal{G}}{\\operatorname*{max}}\\,\\|f(\\mathrm{Dg}(G))\\|_{2}\\leq A_{1}\\underset{G\\in\\mathcal{G}}{\\operatorname*{max}}\\,\\underset{p\\in\\mathrm{Dg}(G)}{\\operatorname*{max}}\\,\\|\\omega_{\\mathbf{w}}(p)\\varphi_{\\mathbf{w}}(p)\\|_{2}\\leq}\\\\ &{\\leq A_{1}\\underset{G\\in\\mathcal{G}}{\\operatorname*{max}}\\,\\underset{p\\in\\mathrm{Dg}(G)}{\\operatorname*{max}}\\,\\omega_{\\mathbf{w}}(p)\\|\\varphi_{\\mathbf{w}}(p)\\|_{2}\\leq}\\\\ &{\\leq A_{1}\\underset{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}{\\operatorname*{max}}\\,\\omega_{\\mathbf{w}}(p)\\underset{G\\in\\mathcal{G},p\\in\\mathrm{Dg}\\,G}{\\operatorname*{max}}\\,\\|\\varphi_{\\mathbf{w}}(p)\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $A_{1}$ is $\\operatorname*{max}_{G\\in{\\mathcal{G}}}c a r d(\\operatorname{Dg}(G))$ if $\\mathrm{Agg}$ is sum and $A_{1}=1$ if $\\mathrm{Agg}$ is $k$ -max or mean. ", "page_idx": 28}, {"type": "text", "text": "From the Lemma statement we can upper bound $\\omega_{\\mathbf{w}}(p)$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in{\\mathcal{G}},p\\in\\mathrm{Dg}(G)}\\omega_{\\mathbf{w}}(p)\\leq C^{(\\omega)}T^{(\\omega)}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Maximum norm of $\\varphi$ : ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "$\\varphi=\\Lambda$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}|\\varphi_{\\mathbf{w}}(p)_{i}|=\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\operatorname*{max}\\{0,p_{2}-|t_{i}-p_{1}|\\}\\le p_{2}\\le B\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\|\\varphi_{\\mathbf{w}}(p)\\|_{2}=\\left[\\sum_{i=1}^{h}\\varphi_{\\mathbf{w}}(p)_{i}^{2}\\right]^{1/2}\\leq\\left[\\sum_{i=1}^{h}b^{2}\\right]^{1/2}=B\\sqrt{h}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "$\\varphi=\\Gamma$ . ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}|\\varphi_{\\mathbf{w}}(p)_{i}|=\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\exp\\left[-\\frac{|p_{1}-t_{i,1}|^{2}+|p_{2}-t_{i,2}|^{2}}{2\\tau^{2}}\\right]\\leq1\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in{\\mathcal{G}},p\\in\\mathrm{Dg}(G)}\\|\\varphi_{\\mathbf{w}}(p)\\|_{2}=\\left[\\sum_{i=1}^{h}|\\varphi_{\\mathbf{w}}(p)_{i}|^{2}\\right]^{1/2}\\leq\\left[\\sum_{i=1}^{h}1\\right]^{1/2}={\\sqrt{h}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$\\varphi=\\Psi.$ ", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}{\\mathrm{max}}\\,|\\varphi_{\\mathbf{w}}(p)_{i}|=\\underset{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}{\\mathrm{max}}\\,|p_{1}t_{i}[1]+p_{2}t_{i}[2]+t_{i}[3]|\\leq}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{max}\\{B,1\\}|t_{i,1}+t_{i,2}+t_{i}[3]|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence: ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\|\\varphi_{\\mathbf{w}}(p)\\|_{2}=\\left[\\displaystyle\\sum_{i=1}^{h}|(\\varphi_{\\mathbf{w}})_{i}|^{2}\\right]^{1/2}\\leq\\left[\\displaystyle\\sum_{i=1}^{h}\\operatorname*{max}\\{B,1\\}^{2}|t_{i,1}+t_{i,2}+t_{i}[3]|^{2}\\right]^{1/2}\\leq}\\\\ {\\displaystyle\\leq\\operatorname*{max}\\{b,1\\}\\left[\\displaystyle\\sum_{i=1}^{h}3(t_{i,1}^{2}+t_{i,2}^{2}+t_{i}[3]^{2})\\right]\\leq}\\\\ {\\displaystyle\\leq\\sqrt{3}\\operatorname*{max}\\{B,1\\}\\|v e c(W^{(\\varphi)})\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining all together we get the Equation 58. ", "page_idx": 28}, {"type": "text", "text": "Maximum perturbation of the PersLay output. ", "text_level": 1, "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{G\\in\\mathcal{G}}{\\operatorname*{max}}\\left\\|f_{\\mathbf{w}+\\mathbf{u}}(\\mathrm{Dg}(G))-f_{\\mathbf{w}}(\\mathrm{Dg}(G))\\right\\|_{2}\\leq A_{2}\\underset{G\\in\\mathcal{G},\\mathcal{p}\\in\\mathrm{Dg}(G)}{\\operatorname*{max}}\\left\\|\\omega_{\\mathbf{w}+\\mathbf{u}}(p)\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\omega_{\\mathbf{w}}(p)\\varphi_{\\mathbf{w}}(p)\\right\\|_{2}\\leq}\\\\ &{\\leq A_{2}\\underset{G\\in\\mathcal{G},\\mathcal{p}\\in\\mathrm{Dg}(G)}{\\operatorname*{max}}\\left[\\|\\omega_{\\mathbf{w}+\\mathbf{u}}(p)(\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\varphi_{\\mathbf{w}}(p))+\\varphi_{\\mathbf{w}}(p)(\\omega_{\\mathbf{w}+\\mathbf{u}}(p)-\\omega_{\\mathbf{w}}(p))\\|_{2}\\right]\\leq}\\\\ &{\\leq A_{2}\\underset{G\\in\\mathcal{G},\\mathcal{p}\\in\\mathrm{Dg}(G)}{\\operatorname*{max}}\\left[\\|\\omega_{\\mathbf{w}+\\mathbf{u}}(p)\\|\\|\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\varphi_{\\mathbf{w}}(p)\\|_{2}+\\|\\varphi_{\\mathbf{w}}(p)\\|_{2}|\\omega_{\\mathbf{w}+\\mathbf{u}}(p)-\\omega_{\\mathbf{w}}(p)\\|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $A_{2}$ is $\\operatorname*{max}_{G\\in{\\mathcal{G}}}c a r d(\\operatorname{Dg}(G))$ if $\\mathrm{Agg}$ is sum, $A_{2}$ is 3 if $\\mathrm{Agg}$ is $k$ -max by Lemma 9 and $A_{2}$ is 1 if Agg is mean. ", "page_idx": 29}, {"type": "text", "text": "By the Lemma statement we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\|\\omega_{\\mathbf{w}+\\mathbf{u}}\\|_{2}\\leq C^{\\omega}(1+\\eta^{(\\omega)})T^{(\\omega)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}|\\omega_{\\mathbf{w}+\\mathbf{u}}(p)-\\omega_{\\mathbf{w}}(p)|\\leq\\mathrm{Lip}^{(\\omega)}\\|U^{(\\omega)}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, from Paragraph about max norm of $\\varphi$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\|\\varphi_{\\mathbf{w}}(p)\\|_{2}\\leq C^{(\\varphi)}T^{(\\varphi)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Maximum perturbation of $\\varphi$ : ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "$\\varphi=\\Lambda$ . Since $g(x)=|x|$ is 1-Lipschitz we have that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\|(\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\varphi_{\\mathbf{w}}(p))_{i}\\|\\le U_{i}^{(\\varphi)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{G\\in\\mathcal{G},\\,p\\in\\mathrm{Dg}(G)}\\|\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\varphi_{\\mathbf{w}}(p)\\|_{2}\\leq\\left[\\displaystyle\\sum_{i=1}^{h}|(\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\varphi_{\\mathbf{w}}(p))_{i}|^{2}\\right]^{1/2}\\leq}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\left[\\displaystyle\\sum_{i=1}^{h}(U_{i}^{(\\varphi)})^{2}\\right]^{1/2}=\\|U^{(\\varphi)}\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\varphi=\\Gamma$ . Suppose $\\begin{array}{r}{g(x,y)=\\exp\\left(-\\frac{x^{2}+y^{2}}{2\\tau^{2}}\\right)}\\end{array}$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|g(x,y)-g(x+\\Delta x,y+\\Delta y)\\|_{2}=|\\nabla g(x^{\\prime},y^{\\prime})|_{2}\\sqrt{\\Delta x^{2}+\\Delta y^{2}}\\le}}\\\\ &{}&{\\le\\displaystyle\\operatorname*{max}_{x^{\\prime},y^{\\prime}}|\\nabla g(x^{\\prime},y^{\\prime})|_{2}\\sqrt{\\Delta x^{2}+\\Delta y^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "by mean-value theorem for some $x^{\\prime},y^{\\prime}$ between $x$ and $x+\\Delta x$ and $y$ and $y+\\Delta y$ . ", "page_idx": 29}, {"type": "text", "text": "Let us find maximum of the gradient by every coordinate. Since the function is symmetric we need to do it only for one of the coordinates. ", "page_idx": 29}, {"type": "text", "text": "The maximum of the norm of the first coordinate of the gradient is achieving at the point $t_{i,1}=p_{1}\\pm\\tau$ , and the gradient value at these points is at most \u03c4e11/2 . So we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{Dg}(G)}\\|\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\varphi_{\\mathbf{w}}(p)\\|_{2}\\leq\\left[\\sum_{i=1}^{h}\\frac{\\|U_{i}^{(\\varphi)}\\|_{2}^{2}}{\\tau^{2}e}\\right]^{1/2}\\leq\\frac{\\|v e c(U^{(\\varphi)})\\|_{2}}{\\tau e^{1/2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\varphi=\\Psi$ . In this case $g(x,y,z)=p_{1}x+p_{2}y+z$ is $\\operatorname*{max}\\{B,1\\}$ -Lipschitz, so ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G},p\\in\\mathrm{D}_{\\mathcal{G}}(G)}\\|\\varphi_{\\mathbf{w}+\\mathbf{u}}(p)-\\varphi_{\\mathbf{w}}(p)\\|_{2}\\leq\\left[\\sum_{i=1}^{h}\\operatorname*{max}\\{B,1\\}^{2}\\|U_{i}^{(\\varphi)}\\|_{2}^{2}\\right]^{1/2}=\\operatorname*{max}\\{B,1\\}\\|v e c(U^{(\\varphi)})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining all together we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{:\\|f_{\\mathbf{w}+\\mathbf{u}}(G)-f_{\\mathbf{w}}(G)\\|_{2}\\leq A_{2}\\left[(1+\\eta^{(\\omega)})C^{(\\omega)}T^{(\\omega)}\\mathrm{Lip}^{(\\varphi)}\\|U^{(\\varphi)}\\|_{2}+C^{(\\varphi)}T^{(\\varphi)}\\mathrm{Lip}^{(\\omega)}\\|U^{(\\omega)}\\|_{2}\\right]\\leq}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\ a_{2}T^{(\\varphi)}T^{(\\omega)}(1+\\eta^{(\\varphi)})\\operatorname*{max}\\{\\mathrm{Lip}^{(\\varphi)}C^{(\\omega)},\\mathrm{Lip}^{(\\omega)}C^{(\\varphi)}\\}\\left(\\frac{\\|U^{(\\varphi)}\\|_{2}}{\\|T^{(\\varphi)}\\|_{2}}+\\frac{\\|U^{(\\varphi)}\\|_{2}}{\\|T^{(\\varphi)}\\|_{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E Proofs of corollaries ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Corollary 3. Let $f_{\\mathbf{w}}:\\mathcal{X}\\mapsto\\mathbb{R}^{K}$ with $\\mathbf{w}=\\{W_{1},...,W_{n}\\}$ be an $n$ -layer MLP with $L i p_{i}$ -Lipschitz activation functions $\\psi_{i}$ , for $i\\in[n-1]$ . Let $B\\in\\mathbb{R}$ be such that $\\forall x\\in\\mathcal{X}:\\;\\|x\\|_{2}\\leq B$ . Then $f_{\\mathbf{w}}$ satisfy requirements of Lemma 2 with: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle T=\\prod_{i=1}^{n}\\|W_{i}\\|_{2}}&{a n d\\quad\\forall i\\in[n]:\\;S_{i}=\\|W_{i}\\|_{2},\\eta_{i}=\\frac{1}{6n}}\\\\ {\\displaystyle C_{1}=B\\prod_{i=1}^{n-1}L i p_{i}}&{a n d\\quad C_{2}=e B\\prod_{i=1}^{n-1}L i p_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. To test Equation 6 and Equation 7 we use Lemma 7. ", "page_idx": 30}, {"type": "text", "text": "To check Equation 8 we apply arithmetic-geometric mean inequality: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\prod_{i=1}^{n}\\|W_{i}\\|_{2}\\left(\\sum_{i=1}^{n}\\frac{1}{\\|W_{i}\\|_{2}}\\right)=\\sum_{i=1}^{n}\\prod_{j\\neq i}\\|W_{j}\\|_{2}\\geq}\\\\ &{}&{\\ge n\\left(\\displaystyle\\prod_{i=1}^{n}\\prod_{j\\neq i}\\|W_{j}\\|_{2}\\right)=n\\left(\\displaystyle\\prod_{i=1}^{n}T_{i}\\right)^{\\frac{n-1}{n}}=n T^{\\frac{n-1}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It is left to show that Equation 9 holds: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{C_{1}}{2C_{2}\\frac{1}{6n}}\\geq\\frac{3n}{e}\\geq1\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Corollary 4. Let $f_{\\mathbf{w}}:\\mathcal{X}\\mapsto\\mathbb{R}^{k}$ with $\\mathbf{w}=\\{W_{1},...,W_{n}\\}$ be a $n$ -layer GCN network with readout layer. Let $\\psi_{i}$ for $i\\in[n-1]$ be a $L i p_{i}$ -Lipschitz activation function. Let node feature of any graph be contained in $\\ell_{2}$ -ball of radius $B$ , i.e. $\\forall G\\in\\mathcal{X}:\\;\\|z_{v}\\|_{2}\\leq B$ for every node $v$ and $\\forall G\\in\\mathcal{X}$ , $G$ is simple and has maximum degree at most $d-1$ . Then $f_{\\mathbf{w}}$ satisfy requirements of Lemma 2 with: ", "page_idx": 30}, {"type": "equation", "text": "$$\nT=\\prod_{i=1}^{n}\\|W_{i}\\|_{2}\\quad a n d\\quad\\forall i\\in n:\\;S_{i}=\\|W_{i}\\|_{2},\\;\\eta_{i}={\\frac{1}{6n}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "if using perturbation analysis from Liao et al. [33] ", "page_idx": 30}, {"type": "equation", "text": "$$\nC_{1}=d^{\\frac{n-1}{2}}B\\prod_{i=1}^{n-1}L i p_{i}\\quad a n d\\quad C_{2}=e B d^{\\frac{n-1}{2}}\\prod_{i=1}^{n-1}L i p_{i}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "or if using perturbation analysis from Sun and Lin [48] ", "page_idx": 30}, {"type": "equation", "text": "$$\nC_{1}=B\\prod_{i=1}^{n-1}L i p_{i}\\quad a n d\\quad C_{2}=e B\\prod_{i=1}^{n-1}L i p_{i}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. To test Equation 6, Equation 7 we use Lemma 8. We can apply Lemma 8 because $\\begin{array}{r}{\\bar{\\eta}=\\frac{1}{6n}\\leq\\frac{1}{n}}\\end{array}$ . To test Equation 8 we use arithmetic vs geometric mean inequality: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\prod_{i=1}^{n}\\|W_{i}\\|_{2}\\left(\\sum_{i=1}^{n}\\frac{1}{\\|W_{i}\\|_{2}}\\right)=\\sum_{i=1}^{n}\\prod_{j\\neq i}\\|W_{j}\\|_{2}\\geq}\\\\ &{}&{\\ge n\\left(\\displaystyle\\prod_{i=1}^{n}\\prod_{j\\neq i}\\|W_{j}\\|_{2}\\right)=n\\left(\\displaystyle\\prod_{i=1}^{n}T_{i}\\right)^{\\frac{n-1}{n}}=n T^{\\frac{n-1}{n}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It is left to test Equation 9 ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{C_{1}}{2C_{2}\\frac{1}{6n}}\\geq\\frac{3n}{e}\\geq1\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Corollary 5. Let $f_{\\mathbf{w}}:\\mathcal{X}\\mapsto\\mathbb{R}^{k}$ with $\\mathbf{w}=\\{W_{1},W_{2},W_{3}\\}$ be $n$ -layer MPGNN $(n>2,$ ). Let $g,\\phi,\\rho$ be activations functions with Lipschitz constants: $L i p_{g},L i p_{\\phi},L i p_{\\rho}$ . We denote $L i p_{g}L i p_{\\phi}L i p_{\\rho}\\|W_{2}\\|_{2}$ with $\\mathcal{C}$ . Let node feature of any graph be contained in $\\ell_{2}$ -ball of radius $B$ , i.e. $\\forall G=(V,E,z)\\in$ $\\mathcal{X},\\forall v\\in V:\\|z_{v}\\|_{2}\\leq B$ and $\\forall G\\in\\mathcal{X}$ , $G$ is simple and has maximum degree at most $d-1$ . Then $f_{\\mathbf{w}}$ satisfy Lemma 2 requirements with ", "page_idx": 31}, {"type": "text", "text": "if $\\dot{\\cdot}d\\mathcal{C}\\neq1$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T=\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}\\frac{(d\\mathcal{C})^{n-1}-1}{d\\mathcal{C}-1},}}\\\\ {{S_{1}=\\|W_{1}\\|,\\;S_{2}=\\operatorname*{min}\\{d\\mathcal{C},\\|W_{2}\\|_{2}\\},\\;S_{3}=\\|W_{3}\\|_{2},}}\\\\ {{\\eta_{1}=\\eta_{2}=\\eta_{3}=\\displaystyle\\frac{1}{6n}}}\\\\ {{C_{1}=B C_{\\phi}\\quad a n d\\quad C_{2}=e B C_{\\phi}n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$i f\\,d{\\mathcal{C}}=1$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T=\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}}}\\\\ {{S_{1}=\\|W_{1}\\|_{2},\\ S_{2}=\\operatorname*{min}\\{1,\\|W_{2}\\|_{2}\\},\\ S_{3}=\\|W_{3}\\|_{2}}}\\\\ {{\\eta_{1}=\\eta_{2}=\\eta_{3}=\\displaystyle\\frac{1}{6(n+1)}}}\\\\ {{C_{1}=B C_{\\phi}(n+1)\\quad a n d\\quad C_{2}=e B C_{\\phi}(n+1)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. From [33] (Lemma 3.3) we know that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{X}}\\|f_{\\mathbf{w}+\\mathbf{u}}(G)-f_{\\mathbf{w}}(G)\\|_{2}\\leq\\left\\{\\begin{array}{l l}{\\!e B(n+1)^{2}\\eta\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}C_{\\phi},\\quad\\,d\\mathcal{C}=1}\\\\ {\\!e B n\\eta\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}C_{\\phi}\\frac{(d\\mathcal{C})^{n-1}-1}{d\\mathcal{C}-1},\\quad\\,d\\mathcal{C}\\neq1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where \u2225\u2225WU11\u2225\u222522 , \u2225\u2225WU22\u2225\u222522 , \u2225\u2225WU33\u2225\u222522  \u2264 n1. In our case \u03b7\u00af \u2264 61n, so we can apply these inequalities. Note that in our notation we have $n$ -layer MPGNN and instead of $W_{l}$ we have $W_{3}$ (and instead of $U_{l}$ we have $U_{3}$ ). ", "page_idx": 31}, {"type": "text", "text": "And ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{X}}\\|f_{\\mathbf{w}}(G)\\|_{2}\\leq\\left\\{B(n-1)C_{\\phi}\\|W_{1}\\|_{2}\\|W_{3}\\|_{2},\\quad d\\mathcal{C}=1\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Eq. (76) and Eq. (68) respectively. ", "page_idx": 31}, {"type": "text", "text": "Equation 6 follows from Equation 64. ", "page_idx": 31}, {"type": "text", "text": "To test Equation 7 we note that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{\\|U_{1}\\|_{2}}{\\|W_{1}\\|_{2}}+\\frac{\\|U_{2}\\|_{2}}{\\|W_{2}\\|_{2}}+\\frac{\\|U_{3}\\|_{2}}{\\|W_{3}\\|_{2}}\\leq\\frac{\\|U_{1}\\|_{2}}{\\|W_{1}\\|_{2}}+\\frac{\\|U_{2}\\|_{2}}{\\operatorname*{min}\\{d\\mathcal{L},\\|W_{2}\\|_{2}\\}}+\\frac{\\|U_{3}\\|_{2}}{\\|W_{3}\\|_{2}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now Equation 7 follows from Equation 63 and Equation 65. ", "page_idx": 31}, {"type": "text", "text": "To test Equation 8 we employ arithmetic vs geometric mean inequality: ", "page_idx": 31}, {"type": "equation", "text": "$$\nT\\left(\\frac{1}{S_{1}}+\\frac{1}{S_{2}}+\\frac{1}{S_{3}}\\right)\\geq\\frac{3T}{(S_{1}S_{2}S_{3})^{1/3}}\\geq\\left\\{3\\frac{\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}}{(\\|W_{1}\\|_{2}\\|W_{3}\\|_{2})^{1/3}}\\geq3T^{2/3},\\quad d\\mathcal{C}=1\\right.\\quad\\quad}\\\\ {\\left.\\quad T\\left(\\frac{1}{S_{1}}\\frac{\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}}{(\\|W_{1}\\|_{2}d C\\|W_{3}\\|_{2})^{1/3}}\\geq3T^{2/3},\\quad d\\mathcal{C}\\neq1\\right.\\quad,}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where in case $d{\\mathcal{C}}=1$ , $S_{2}\\leq1$ and in case $d{\\mathcal{C}}\\neq1$ , $S_{2}\\leq d\\mathcal{C}$ and $\\begin{array}{r}{\\frac{(d\\mathcal{C})^{n-1}-1}{d\\mathcal{C}-1}\\geq d\\mathcal{C}}\\end{array}$ for $n>2$ . ", "page_idx": 31}, {"type": "text", "text": "To test Equation 9 we provide the following derivation: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{C_{1}}{2C_{2}}\\ge\\left\\{\\frac{l+1}{2e(n+1)^{2}}\\ge\\frac{1}{6(n+1)},\\quad d\\mathscr{C}=1\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Corollary 1. Let $f_{\\mathbf{w}}:\\mathcal{G}\\mapsto\\mathbb{R}^{k}$ with $\\mathbf{w}=\\{W^{(\\omega)},W^{(\\varphi)}\\}$ be a PersLay where $W^{(\\omega)}$ is a parameter vector (matrix) of weight function, $\\omega$ , and $W^{(\\varphi)}$ is a parameter vector (matrix) of point-transformation function, $\\varphi$ . Let $\\mathrm{Dg}$ be a mapping from graphs to (extended) persistence diagrams with a fixed flitration function and $B$ such that max max $\\|p\\|_{2}\\leq B_{*}$ , then $f_{\\mathbf{w}}$ satisfy requirements of Lemma 2 $G\\!\\in\\!{\\mathcal{G}}_{\\ p\\in{\\mathrm{Dg}}(G)}$ ", "page_idx": 32}, {"type": "text", "text": "with: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{T=T^{(\\varphi)}T^{(\\omega)}\\quad a n d\\quad(S^{(\\varphi)},S^{(\\omega)})=(\\|W^{(\\varphi)}\\|_{2},\\|W^{(\\omega)}\\|_{2})\\quad a n d\\quad(\\eta^{(\\varphi)},\\eta^{(\\omega)})=(1,1)}\\\\ {C_{1}=2\\operatorname*{max}\\Big\\{A_{1}C^{(\\omega)}C^{(\\varphi)},2A_{2}\\operatorname*{max}\\{C^{(\\omega)}L i p^{(\\varphi)},C^{(\\varphi)}L i p^{(\\omega)}\\}\\Big\\}}\\\\ {C_{2}=2A_{2}\\operatorname*{max}\\{C^{(\\omega)}L i p^{(\\varphi)},C^{(\\varphi)}L i p^{(\\omega)}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $A_{1}\\:=\\:\\operatorname*{max}_{G\\in{\\mathcal{G}}}{c a r d(\\mathrm{Dg}(G))}$ if Agg is sum and $A_{1}~=~1$ if Agg is $k$ -max or mean; $A_{2}{\\bf\\Psi}={\\bf\\Psi}$   \n$\\operatorname*{max}_{G\\in\\mathcal{G}}c a r d(\\mathrm{Dg}(G))\\;i f\\,\\mathrm{Agg}$ is sum or $A_{2}=3$ if Agg is $k$ -max or mean; ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{(T^{(\\varphi)},C^{(\\varphi)},L i p^{(\\varphi)})=(\\operatorname*{max}\\{1,\\|W^{(\\varphi)}\\|_{2}\\},B\\sqrt{h},1)}&&{\\varphi=\\Lambda}\\\\ &{(T^{(\\varphi)},C^{(\\varphi)},L i p^{(\\varphi)})=(\\operatorname*{max}\\{1,\\|W^{(\\varphi)}\\|_{2}\\},\\sqrt{h},\\tau e^{-1/2})}&&{\\varphi=\\Gamma}\\\\ &{(T^{(\\varphi)},C^{(\\varphi)},L i p^{(\\varphi)})=(\\|W^{(\\varphi)}\\|_{2},\\sqrt{3}\\operatorname*{max}\\{1,B\\},\\operatorname*{max}\\{1,B\\})}&&{\\varphi=\\Psi}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "$T^{(\\omega)}=\\operatorname*{max}\\{1,\\|W^{(\\omega)}\\|_{2}\\},$ ; and $C^{(\\omega)},L i p^{(\\omega)}$ are such that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{G\\in\\mathcal{G}}\\operatorname*{max}_{p\\in\\mathrm{D}\\mathrm{g}(G)}|\\omega_{\\mathbf{w}}(p)|_{2}\\le C^{(\\omega)}T^{(\\omega)}\\quad a n d\\quad\\operatorname*{max}_{G\\in\\mathcal{G}}\\operatorname*{max}_{p\\in\\mathrm{D}\\mathrm{g}(G)}|\\omega_{\\mathbf{w}+\\mathbf{u}}(p)-\\omega_{\\mathbf{w}}(p)|\\le L i p^{(\\omega)}\\|U^{(\\omega)}\\|_{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for any w and u. ", "page_idx": 32}, {"type": "text", "text": "Proof. To test Equation 6 and Equation 7 we apply Lemma 10. $\\eta^{(\\omega)}$ in Lemma 10 is arbitrary, so we can use this lemma with $\\eta^{(\\omega)}=1$ and in this case $1+\\eta^{(\\omega)}=2$ , so our choice of $C_{2}$ works. ", "page_idx": 32}, {"type": "text", "text": "To test Equation 8 we use arithmetic vs geometric mean inequality: ", "page_idx": 32}, {"type": "equation", "text": "$$\nT^{(\\varphi)}T^{(\\omega)}\\left(\\frac{1}{S^{(\\varphi)}}+\\frac{1}{S^{(\\omega)}}\\right)\\geq2\\frac{T^{(\\varphi)}T^{(\\omega)}}{(\\|W^{(\\omega)}\\|_{2}\\|W^{(\\varphi)}\\|_{2})^{1/3}}\\geq2\\frac{T^{(\\omega)}T^{(\\varphi)}}{(T^{(\\omega)}T^{(\\varphi)})^{1/3}}=2T^{2/3}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It is left to test Equation 9 ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{C_{1}}{2C_{2}}\\geq\\frac{2A_{1}C^{(\\omega)}C^{(\\varphi)}\\operatorname*{max}\\left\\{1,\\frac{2A_{2}\\operatorname*{max}\\{C^{(\\omega)}\\mathrm{Lip}^{(\\varphi)},C^{(\\varphi)}\\mathrm{Lip}^{(\\omega)}\\}}{A_{1}C^{(\\omega)}C^{(\\varphi)}}\\right\\}}{2A_{2}\\operatorname*{max}\\{C^{(\\omega)}\\mathrm{Lip}^{(\\varphi)},C^{(\\varphi)}\\mathrm{Lip}^{(\\omega)}}\\geq1}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "F Comparing bounds with prior works ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "MLP & GCN The result is not as tight as in [40] because of the homogenity assumption that they do. Specificaly, matching our result to theirs, we have $B^{2}(h\\ln n h)\\bar{\\eta}^{-2}$ the same as $\\dot{B}^{2}n^{2}(h\\ln n\\dot{h})$ (instead of $n$ they have $d$ in their notation). We have $\\begin{array}{r}{\\log\\left(\\frac{m}{\\delta}\\operatorname*{max}\\{1,\\frac{1}{B}\\}\\right)}\\end{array}$ which is in most cases better than log $\\frac{m\\!\\cdot\\!n}{\\delta}$ . However, the suboptimality of our bound comes from: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\frac{\\|W_{i}\\|_{F}^{2}}{\\|W_{i}\\|_{2}^{2}}\\leq\\sum_{i=1}^{n}\\|W_{i}\\|_{F}^{2}\\sum_{i=1}^{n}\\frac{1}{\\|W_{i}\\|_{2}^{2}}\\leq\\operatorname*{max}\\left\\{1,\\sum_{i=1}^{n}\\|W_{i}\\|_{F}^{2}\\right\\}\\left(\\sum_{i=1}^{n}\\frac{1}{\\|W_{i}\\|_{2}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by Cauchy-Schwarz inequality. The most left expression is the term that is in Neyshabur et al. [40] and the most right expression is the one we have in our bound. ", "page_idx": 33}, {"type": "text", "text": "However, the advantage of our result is that it does not depend on the assumption of all activation functions being equal to ReLU and that we can compose it with results for other networks to get bounds for compositions of networks. ", "page_idx": 33}, {"type": "text", "text": "MPGNNs As we discussed in the main text the difference between our bound and the one in Liao et al. [33] comes from the dependency on the weights. ", "page_idx": 33}, {"type": "text", "text": "Their dependency: max $\\left\\{\\zeta^{-(l+1)},(\\lambda\\xi)^{\\frac{l+1}{l}}\\right\\}$ , where $\\zeta\\ =\\ \\operatorname*{min}\\{\\|W_{1}\\|_{2},\\|W_{2}\\|_{2},\\|W_{l}\\|_{2}\\}$ , $\\lambda\\ =$ $\\begin{array}{r}{\\|W_{1}\\|_{2}\\|W_{l}\\|_{2},\\xi=C_{\\phi}\\frac{(d\\mathcal{C})^{l-1}-1}{d\\mathcal{C}-1}}\\end{array}$ \u03d5(dCd)Cl\u2212\u221211\u22121. Let us show, how our bound depends on \u03bb, \u03b6 and \u03be: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\zeta^{-1}=\\operatorname*{max}\\left\\{\\cfrac{1}{\\|W_{1}\\|_{2}},\\cfrac{1}{\\|W_{2}\\|_{2}},\\frac{1}{\\|W_{l}\\|_{2}}\\right\\}\\geq\\frac{1}{3}\\left(\\cfrac{1}{\\|W_{1}\\|_{2}}+\\cfrac{1}{\\|W_{2}\\|_{2}}+\\cfrac{1}{\\|W_{l}\\|_{2}}\\right)\\geq}\\\\ {\\geq\\cfrac{\\operatorname*{min}\\{1,d\\mathrm{Lip}_{\\phi}\\mathrm{Lip}_{\\rho}\\mathrm{Lip}_{\\rho}\\}}{3}\\left(\\cfrac{1}{\\|W_{1}\\|_{2}}+\\cfrac{1}{\\operatorname*{min}\\{d\\mathcal{C},\\|W_{2}\\|_{2}\\}}+\\cfrac{1}{\\|W_{l}\\|_{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta^{-1}=\\operatorname*{max}\\bigg\\{\\frac{1}{\\|W_{1}\\|_{2}},\\frac{1}{\\|W_{2}\\|_{2}},\\frac{1}{\\|W_{l}\\|_{2}}\\bigg\\}\\leq\\frac{1}{\\|W_{1}\\|_{2}}+\\frac{1}{\\|W_{2}\\|_{2}}+\\frac{1}{\\|W_{l}\\|_{2}}\\leq}\\\\ &{\\qquad\\leq\\frac{1}{\\|W_{1}\\|_{2}}+\\frac{1}{\\operatorname*{min}\\{d\\mathcal{C},\\|W_{2}\\|_{2}\\}}+\\frac{1}{\\|W_{l}\\|_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that $\\lambda\\xi=T$ . Now let us analyze two cases: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}&{(\\lambda\\xi)^{\\frac{l+1}{l}}\\ \\geq\\ (\\zeta)^{-(l+1)}.\\quad\\mathrm{In\\,\\,\\,this\\,\\,\\,case\\,\\,\\zeta^{-1}\\ \\ \\leq\\ (\\lambda\\xi)^{1/l}\\,\\,\\,a n d\\,\\,\\zeta^{-1}\\lambda\\xi}\\ \\leq\\ (\\lambda\\xi)^{\\frac{l+1}{l}}\\ =\\ }&\\\\ &{\\operatorname*{max}\\{\\zeta^{-(l+1)},(\\lambda\\xi)^{\\frac{l+1}{l}}\\}}&&\\\\ &{(\\zeta)^{-(l+1)}\\ \\geq\\ (\\lambda\\xi)^{\\frac{l+1}{l}}.\\quad\\mathrm{\\In\\,\\,\\,this\\,\\,\\,case\\,\\,\\zeta^{-l}\\ \\ \\geq\\ \\ \\lambda\\xi\\,\\,a n d\\,\\ \\zeta^{-1}\\lambda\\xi}\\ \\leq\\ \\zeta^{-(l+1)}}&{=\\ }&\\\\ &{\\operatorname*{max}\\{\\zeta^{-(l+1)},(\\lambda\\xi)^{\\frac{l+1}{l}}\\}}&&\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "So, in the case when $d\\mathrm{Lip}_{\\phi}\\mathrm{Lip}_{g}\\mathrm{Lip}_{\\rho}$ is lower bounded by some constant we can conclude that asymptotically our bound is not inferior, than the one in Liao et al. [33]. ", "page_idx": 33}, {"type": "text", "text": "G Dependency on Model Parameters and Hyperparameters ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In the Table 6 we present the dependency of existing results and our result on the model parameters and hyperparameters. ", "page_idx": 33}, {"type": "text", "text": "H Learnable Filtration Functions ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Fixed filtration functions dominate the PH/ML literature. The widespread use of learnable functions is a relatively recent phenomenon in PH-based ML, and usually runs orders of magnitude slower compared to non-learnable ones. Arguably, applying non-learnable functions still represents the mainstream approach in TDA. ", "page_idx": 33}, {"type": "text", "text": "Some works have explicitly advocated for fixed filtration functions (with learnable vectorizations) over learnable filtrations. Filtration functions can come in different flavors; for instance, they can rely on node degree [18], cliques [43], or node attributes [22]. Some of the popular options are parameter-free. Also, while some works showed gains using learnable filtrations [20], others have reported no benefits and adopted fixed functions instead [5, 34]. There is still no consensus about the significance of the gains associated with learnable filtration in many applications. ", "page_idx": 33}, {"type": "text", "text": "Perslay [5] uses fixed filtration functions. Despite the generality of our results, we provide specific bounds for PersLay, which employs fixed filtration functions. ", "page_idx": 33}, {"type": "text", "text": "Our work lays a strong foundation for analyzing learnable filtrations. One way to analyze PH with learnable filtration schemes could be to get upper bounds on perturbation of outputs in terms of the ", "page_idx": 33}, {"type": "text", "text": "Table 6: The dependency of the bound on the model parameters. All models have maximum width of weight matrices $h$ . We consider $n$ -layer multilayer perceptron (MLP) with weights $W_{1},...,W_{n}$ . We consider $n$ -layer GCN with weights $W_{1},...,W_{n}$ . We consider $n$ -layer $(n\\,>\\,2)$ MPGNN with weights $W_{1},W_{2},W_{3}$ . We consider simple graphs with maximum degree $d$ . We denote $\\mathcal{C}=\\mathrm{Lip}_{\\phi}\\mathrm{Lip}_{g}\\mathrm{Lip}_{\\rho}\\|W_{2}\\|$ , $\\lambda=\\|W_{1}\\|_{2}\\|W_{3}\\|_{2}$ and $\\bar{\\xi}\\overset{\\large=}{\\large(}(\\bar{d}\\bar{\\mathcal{C}})^{\\bar{n}-1}\\!-\\!1)\\bigg/(d\\mathcal{C}\\!-\\!1)$ and $|w|_{2}$ is the norm of all parameters in the model. Comparing to [33] we do not add $\\mathrm{Lip}_{\\phi}$ to $\\xi$ and instead of $W_{l}$ we have $W_{3}$ . We consider PersLay with one of the classical point-transformation (described in [5]) and weight function with weights $W^{(\\varphi)}$ , $W^{(\\omega)}$ and $\\lVert\\mathbf{w}\\rVert_{2}$ is the norm of all parameters in the model. We consider abstract model that satisfy Lemma 2 requirements with w, $T,\\bar{\\eta}$ , $C_{n o r m}$ and $S_{i}$ , ", "page_idx": 34}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/600ebd1c35ac89f52da573abd883cdb5ec30af5c3099867f8b117e8c6c41495e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "filtration function parameters. This would additionally require an analysis of Wasserstein distances between persistence diagrams obtained with different parameters. We believe that for a specific class of graphs we can get modified upper bounds for perturbation with respect to filtration function parameters that would depend on Wasserstein distance of the same order. This additional analysis could be readily integrated into our framework to get generalization bounds for learnable filtrations. ", "page_idx": 34}, {"type": "text", "text": "I Discussion on the max over 1 and parameters norm. ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "From the first glance it may seem that introducing $\\operatorname*{max}\\{1,\\|w\\|_{2}^{2}\\}$ is suboptimal; however, $\\|\\boldsymbol{w}\\|_{2}^{2}$ is greater than 1 in many real-world scenarios. For instance, suppose we have parameters within the range of $O(\\varepsilon)$ ; then the squared norm would be greater than 1 if the number of parameters exceeds $\\frac{1}{\\varepsilon^{2}}$ . Considering a typical choice in the Deep Learning literature, $\\varepsilon=0.01$ , we need more than $10,0\\breve{0}0$ parameters. For example, a two-layer MLP with a hidden dimension of 64 (also a typical choice) has more than 8, 000 parameters, so in practice, $\\|\\mathbf{w}\\|_{2}^{2}\\geq1$ is usually true. ", "page_idx": 34}, {"type": "text", "text": "J Implementation details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "J.1 Datasets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Table 7 reports summary statistics of the datasets used in this paper. ", "page_idx": 34}, {"type": "table", "img_path": "ZNcJtNN3e8/tmp/8ccee7cee4d1be2636f2d42d8c27e1a844007c566fe4f040a681d485107ef88b.jpg", "table_caption": ["Table 7: Statistics of the datasets. "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "J.2 Models ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "For the experiments with PersLay Classifier, we closely follow the filtration functions used in [5]. In particular, we use Kernel heat functions with parameters $t=0.1$ and $t=10$ for the remaining datasets. Instead of processing each diagram type using separate models, we combine ordinary and extended diagrams for 0- and 1-dimensional features and apply a single model. We use mean aggregation function in all experiments, and Gaussian point transformations. For the feedforward part of PersLay, we apply ReLU activation functions. All models are trained with Adam [27] and learning rate of $10^{-3}$ for 3000 epochs. ", "page_idx": 35}, {"type": "text", "text": "For the experiments with GNNs with persistence, we use graph isomorphism networks (GINs) with 2 layers and 64 hidden units. The models were trained for 2000 epochs using the Adam optimizer. ", "page_idx": 35}, {"type": "text", "text": "Dependence on model paramaters. Regarding the dependence on the spectral norm of weights, we reported results for a model with a final MLP (multilayer perceptron) of 2 hidden layers (3 layers in total) and width of 128 for all layers. The number of parameters of the point transformation was $h=100$ . For the experiments on width vs. generalization gap, we used $h=100$ , and 1 hidden layer with a varying width in $\\{32,64,128,256,512\\}$ . ", "page_idx": 35}, {"type": "text", "text": "Regularizing PersLay. For the experiments regarding ERM and spectral norm regularizers, we perform model selection for (number of layers) $l\\,\\in\\,\\{2,3\\}$ , and $\\alpha_{r}^{\\enspace^{\\cdot}}\\in\\{10^{-3},10^{-4},10^{-5},10^{-6}\\}$ . Again, we use Gaussian point transformation, $h=100$ , and width equals to 128. Our goal was to see if we could observe gains from the regularized version even for shallow neural networks. ", "page_idx": 35}, {"type": "text", "text": "Regularizing GNNs with persistence. Here, we consider GNNs with 64 hidden units of 64 (width) and 2 layers. We set the dimensionality of PersLay parameters equal to 100, Gaussian point transformation, and mean aggregation function. We apply hold-out model selection with penalty term $\\alpha_{r}\\in\\{10^{-5},10^{-6},10^{-7},10^{-8}\\}$ using the validation set. ", "page_idx": 35}, {"type": "text", "text": "Hardware. For all experiments, we use Tesla V100 GPU cards and consider a memory budget of 32GB of RAM. ", "page_idx": 35}, {"type": "image", "img_path": "ZNcJtNN3e8/tmp/9f64fba488cfc4bf348ea95415e2d26e59e06085d85dfa7c47cae621a17cc913.jpg", "img_caption": ["Figure 7: Width vs. generalization gap for the triangle point transformation. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "ZNcJtNN3e8/tmp/c5ff2e0db7262da66d7f40d13d3bf352a5fb1d2c6f66759c6ccc22c7b8cabe9f.jpg", "img_caption": ["Figure 8: Spectral norm vs. generalization gap for the triangle point transformation. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "K Additional visualizations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Figure 7 and Figure 8 report additional results for the triangle point transformation on the three largest datasets: PROTEINS, NCI1, and IMDB-BINARY. In particular, Figure 7 shows the dependence of the generalization on width, while Figure 8 shows the dependence on the spectral norm. Overall, our bound can capture the trend in the empirical gap and produces high correlation values for all datasets. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 36}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Table 1 maps the results supporting our claims. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The discussion about the Table 2 in the Section 3 contains comparison with prior works. The discussion after Corollary 1 contains description of the limitation of our PersLay analysis. Moreover, the conclusion summarizes some of the important limitations. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Assumptions are clearly stated in the statements, and in the Appendix E and Appendix C. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Details are provided in the Experiment Section as well as in the Appendix J. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide a link to our code in Section 6. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Details are provided in the Experiment Section as well as in the Appendix J Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Plots count on error bars and tables count on standard deviation. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Appendix J. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our submission follow the NeurIPS ethical guidelines. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide a perspective on broader impacts in Section 7, but do not foresee any direct negative societal impact. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do not foresee any direct risk stemming from our work. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: All code was made by the authors ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA]   \nJustification: No new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: No experiments with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: No experiments with human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]