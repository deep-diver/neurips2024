{"references": [{"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "publication_date": "2022-06-01", "reason": "This paper introduces FlashAttention, a fast and memory-efficient attention mechanism which is directly relevant to improving the efficiency of LLMs, a core topic of the provided research paper."}, {"fullname_first_author": "Zhenyu Zhang", "paper_title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models", "publication_date": "2023-12-01", "reason": "This paper proposes H2O, another method for efficient LLM inference that is compared against in the provided research paper, making it an important contextual reference."}, {"fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient Memory Management for Large Language Model Serving with PagedAttention", "publication_date": "2023-10-01", "reason": "This paper introduces PagedAttention, a key concept that inspires the page-based KV cache management approach in ARKVALE, directly influencing the design of the main research."}, {"fullname_first_author": "Zichang Liu", "paper_title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time", "publication_date": "2023-08-01", "reason": "This paper presents Scissorhands, a related work focusing on KV cache compression in LLMs which is directly compared to the proposed method, thus highlighting its importance for the research context."}, {"fullname_first_author": "Yushi Bai", "paper_title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding", "publication_date": "2023-08-01", "reason": "This paper introduces LongBench, a benchmark dataset used for evaluating ARKVALE\u2019s performance and comparing it to existing methods, making it crucial for evaluating the proposed approach."}]}