[{"figure_path": "4oAt5L4lYe/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of different KV cache eviction works.", "description": "The figure compares four different strategies for managing the key-value cache (KV cache) in large language models (LLMs) during inference.  (a) shows a baseline where all tokens are kept in the cache, leading to increased memory usage and latency. (b) illustrates keeping only recent tokens, improving efficiency but potentially losing important information. (c) demonstrates keeping only important tokens, a more advanced approach that reduces memory usage but still might miss crucial information that might become important later. Finally, (d) shows the proposed ARKVALE method, which combines keeping important tokens with the ability to recall previously evicted tokens that regain importance during later decoding steps, aiming for the best balance between memory efficiency and accuracy.", "section": "Introduction"}, {"figure_path": "4oAt5L4lYe/figures/figures_3_1.jpg", "caption": "Figure 2: Latency & memory breakdown of a decoding step of LongChat-7b-v1.5-32k model with batch-size b = 8 and different history sequence-lengths s = 29, 210, 211, 212, 213, 214, 215.", "description": "This figure shows the breakdown of latency and memory usage for a single decoding step in the LongChat-7b-v1.5-32k model with a batch size of 8.  The x-axis represents different sequence lengths, and the y-axis shows relative latency (left) and memory usage in GB (right). The latency is broken down into the time spent on attention computation and other operations. The memory usage is broken down into the memory used by the KV cache and model parameters. The figure demonstrates how both latency and memory usage increase significantly with longer sequence lengths.", "section": "Observation"}, {"figure_path": "4oAt5L4lYe/figures/figures_3_2.jpg", "caption": "Figure 2: Latency & memory breakdown of a decoding step of LongChat-7b-v1.5-32k model with batch-size b = 8 and different history sequence-lengths s = 29, 210, 211, 212, 213, 214, 215.", "description": "This figure shows the breakdown of latency and memory usage during a single decoding step of the LongChat-7b-v1.5-32k language model.  The model uses a batch size of 8 and varies the length of the history sequence (context).  The left plot (a) shows latency, broken down into the time spent on attention and other operations. The right plot (b) shows memory usage divided into parameters and KV cache usage. Both plots demonstrate how latency and memory usage increase as the context length increases, highlighting the challenges of handling long contexts in LLMs.", "section": "Impact of Long Context"}, {"figure_path": "4oAt5L4lYe/figures/figures_3_3.jpg", "caption": "Figure 3: Token-level sparsity of KV cache. We organize the tokens in KV cache into pages (with page-size=32) and rank the pages of each layer based on the highest attention (softmax) scores of the tokens within each page.", "description": "This figure shows the sparsity of the key-value (KV) cache in each layer of a large language model (LLM).  The tokens in the KV cache are grouped into pages (32 tokens per page), and these pages are ranked based on their attention scores.  The figure plots the ratio of bottom-ranked pages that contribute to 1% and 10% of the total attention scores, demonstrating the sparsity. It also shows the number of top-ranked pages that contribute to 90% and 99% of the total attention scores, highlighting how few pages are actually crucial for attention computation.", "section": "4.1 Token-level Sparsity of KV Cache"}, {"figure_path": "4oAt5L4lYe/figures/figures_4_1.jpg", "caption": "Figure 4: (a) Dynamsim of token (page) importance (page-size=32). The sample is from GovReport [28]. (b) Number of page-recalls (in average) needed during a decoding step with page-size=32.", "description": "Figure 4(a) shows the dynamic change of token importance over decoding steps.  It demonstrates that tokens initially deemed unimportant can regain importance later. This dynamic behavior motivates the need for ARKVALE's recallable key-value eviction mechanism. Figure 4(b) presents the average number of page recalls needed during a decoding step for various cache capacities.  It highlights the relatively low recall overhead, demonstrating the efficiency of ARKVALE's page-based management approach.", "section": "Observation"}, {"figure_path": "4oAt5L4lYe/figures/figures_5_1.jpg", "caption": "Figure 5: Design overview of ARKVALE", "description": "This figure shows the design of ARKVALE, a page-based KV cache manager. It consists of five stages:\n(a) Backup and Summarize a Page once It's Filled:  Once a page is filled in the GPU, it's asynchronously copied to CPU memory for backup. Keys are summarized into a digest and stored on the GPU.\n(b) Estimating Page Importance Scores (Max Dot-product with Query): Before attention computation, ARKVALE estimates the importance of each page using the query and page digests.\n(c) Ranking Pages based on Importance Scores: Pages are then ranked based on their importance scores.\n(d) Recall & Evict Pages: Top-k pages are selected for attention. If any were evicted, they're recalled from CPU memory, and less important pages in GPU are evicted.\n(e) Apply Paged-Attention: The selected pages participate in attention computation.", "section": "5 Techniques of ARKVALE"}, {"figure_path": "4oAt5L4lYe/figures/figures_5_2.jpg", "caption": "Figure 6: Summarize page keys {k(i)}n=1 into their bounding-volume (sphere/cuboid). We can estimate the max-dot-product between query q and keys {k(i)}n=1 using the bounding-volume.", "description": "This figure illustrates two methods for summarizing page keys into bounding volumes for efficient importance estimation.  (a) shows a bounding sphere method, where keys are enclosed within a sphere. The center (c) and radius (r) of this sphere define the digest. Importance is approximated using the maximum dot product between the query (q) and keys on the sphere's surface. (b) shows a bounding cuboid method, where keys are enclosed within a cuboid.  The maximum (b(2)) and minimum (b(4)) vectors of the cuboid form the digest. Importance is approximated using the maximum dot product between the query (q) and the vertices of the cuboid.", "section": "5.2 Page Summarization & Importance Estimation"}, {"figure_path": "4oAt5L4lYe/figures/figures_5_3.jpg", "caption": "Figure 6: Summarize page keys {k(i)}n=1 into their bounding-volume (sphere/cuboid). We can estimate the max-dot-product between query q and keys {k(i)}n=1 using the bounding-volume.", "description": "This figure illustrates two methods to approximate the maximum dot product between a query vector and a set of key vectors.  The first method uses a bounding sphere, where the maximum dot product is approximated using the sphere's center and radius. The second method uses a bounding cuboid, approximating the maximum dot product using the maximum and minimum vector of the cuboid. These approximations allow efficient estimation of page importance without needing to compute the dot product for each individual key.", "section": "5.2 Page Summarization & Importance Estimation"}, {"figure_path": "4oAt5L4lYe/figures/figures_7_1.jpg", "caption": "Figure 7: Recall accuracy (the proportion of pages predicted to be among the top-k that indeed belong to the top-k) of different estimation methods. \"Centroid\" is the baseline which just uses the centroid of keys to estimate the max-dot-product with given query.", "description": "This figure shows the recall accuracy of different page importance estimation methods used in ARKVALE. Recall accuracy is defined as the proportion of pages correctly predicted to be in the top-k most important pages. The x-axis represents the value of k (number of top pages considered), and the y-axis shows the recall accuracy.  The plot compares the performance of several methods: centroid (baseline), sphere-based methods (using sphere's max, center, and mean), and cuboid-based methods (using cuboid's max, center, and mean).  The results demonstrate that the cuboid-based methods, particularly cuboid-mean, significantly outperform the baseline and other methods, achieving high recall accuracy across various values of k.", "section": "6.2 Estimation Accuracy"}, {"figure_path": "4oAt5L4lYe/figures/figures_8_1.jpg", "caption": "Figure 8: Evaluation on 6 long-context datasets in LongBench [9] with different cache budgets.", "description": "This figure presents the results of evaluating ARKVALE and other methods on six long-context datasets from the LongBench benchmark.  It shows the performance (F1 score, Rouge-L score, or Accuracy, depending on the dataset) achieved by different methods (ARKVALE with page sizes of 16 and 32, StreamingLLM, H2O, and TOVA) across various cache budget sizes (512, 1024, 2048, and 4096).  The results demonstrate ARKVALE's superior performance in comparison to the baseline methods, particularly at smaller cache budgets.", "section": "6.3 LongBench Results"}, {"figure_path": "4oAt5L4lYe/figures/figures_9_1.jpg", "caption": "Figure 9: Decoding latency breakdown and achievable maximum throughput with page-size=32 and different seq-lens & cache budgets compared to model with full KV cache.", "description": "This figure shows two subfigures: (a) Latency Breakdown and (b) Throughput Comparison. Subfigure (a) presents a stacked bar chart illustrating the breakdown of decoding latency for different sequence lengths (10k, 20k, 30k tokens) and cache budget sizes (0.5k, 1k, 2k, 4k, Full).  Each bar is segmented into components representing latency from different processes: Full Attention, Recall, Selection, Estimation, and Others. Subfigure (b) presents a bar chart comparing the relative throughput achieved by ARKVALE with various cache budget sizes against the baseline (Full) for different sequence lengths.  The charts demonstrate that ARKVALE improves latency and throughput significantly with increasing cache budget, showcasing its efficiency in handling long sequences.", "section": "6.5 Performance Results"}]