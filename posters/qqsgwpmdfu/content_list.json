[{"type": "text", "text": "Learning Commonality, Divergence and Variety for Unsupervised Visible-Infrared Person Re-identification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiangming $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{1,3}$ ,\u2217 Xiangbo $\\mathbf{Yin}^{2*}$ , Yachao Zhang2, Zhizhong Zhang4,5 Yuan $\\mathbf{Xie^{3,4\\dagger}}$ , Yanyun $\\mathbf{Q}\\mathbf{u}^{1,2\\dagger}$ ", "page_idx": 0}, {"type": "text", "text": "1Institute of Artificial Intelligence, Xiamen University 2School of Informatics, Xiamen University 3Shanghai Innovation Institute 4East China Normal University 5Shanghai Key Laboratory of Computer Software Evaluating and Testing jiangming.shi@outlook.com yxie@cs.ecnu.edu.cn yyqu@xmu.edu.cn Code: https://github.com/shijiangming1/PCLHD ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified persons in infrared images to visible images without annotations, and vice versa. USVI-ReID is a challenging yet underexplored task. Most existing methods address the USVI-ReID through cluster-based contrastive learning, which simply employs the cluster center to represent an individual. However, the cluster center primarily focuses on commonality, overlooking divergence and variety. To address the problem, we propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes for USVI-ReID. In brief, we generate the hard prototype by selecting the sample with the maximum distance from the cluster center. We reveal that the inclusion of the hard prototype in contrastive loss helps to emphasize divergence. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. The dynamic prototype is used to encourage variety. Finally, we introduce a progressive learning strategy to gradually shift the model\u2019s attention towards divergence and variety, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visible-infrared person re-identification (VI-ReID) aims at matching the same person captured in one modality with their counterparts in another modality [1\u20133]. It has recently gained attention in computer vision applications like video surveillance [4] and image retrieval [5\u20137]. With the development of deep learning [8\u201311], VI-ReID has achieved remarkable advancements [12\u201314]. However, the development of existing VI-ReID methods is still limited due to the requirement for expensive-annotated training data [15, 16]. To mitigate the problem of annotating large-scale cross-modality data, some semi-supervised VI-ReID methods [17\u201319] are proposed to learn modalityinvariant and identity-related discriminative representations by utilizing both labeled and unlabeled data. For this purpose, OTLA [17] proposed an optimal transport label assignment mechanism to assign pseudo-labels for unlabeled infrared images while ignoring how to calibrate noise pseudolabels. DPIS [18] integrates two pseudo-labels generated by distinct models into a hybrid pseudo-label for unlabeled infrared data, but it makes the training process more complex. Although these methods have gained promising performances, they still rely on a certain number of manual-labeled data. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Several USVI-ReID methods [20\u201323] have proposed to tackle the issues of expensive visible-infrared annotation through contrastive learning. These methods create two modality-specific memories, one for visible features and the other for infrared features. During training, these methods consider the memory center as a prototype and minimize the contrastive loss across the features of query images and prototype. Then, these methods aggregate the corresponding prototypes based on similarity. However, the centroid prototype only stores the commonality of each person, neglecting the divergence [24\u201326], which causes the pseudo-labels generated by the cluster to be unreliable. Just like a normal distribution, to better reflect the data distribution of a dataset, we need not only the mean but also the variance. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we argue that an important aspect of contrastive learning for USVI-ReID, i.e. the design of the prototype, has so far been neglected, and propose progressive contrastive learning with hard and dynamic prototype (PCLHD) method for the USVI-ReID. Firstly, we design a Hard Prototype Contrastive Learning (HPCL) to mine divergent yet meaningful information. In contrast to traditional contrastive learning methods, we choose the hard samples to serve as the hard prototype. In other words, the hard prototype is the one that is farthest from the memory center. The hard prototype encompasses distinctive information. Furthermore, we introduce the concept of Dynamic Prototype Contrastive Learning (DPCL), we randomly select samples from each cluster to serve as the dynamic prototype. DPCL effectively accounts for the intrinsic variety within clusters, enhancing the model\u2019s adaptability to varying data distributions. Early clustering results are unreliable, and utilizing hard and dynamic prototype at this stage may lead to cluster degradation. Therefore, we introduce progressive contrastive learning to gradually focus on divergence and variety. ", "page_idx": 1}, {"type": "text", "text": "The main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a progressive contrastive learning with hard and dynamic prototype method for the USVI-ReID. We reconsider the design of prototypes in contrastive learning to ensure that the model stably captures commonality, divergence, and variety.   \n\u2022 We propose Hard Prototype Contrastive Learning for mining divergent yet significant information, and Dynamic Prototype Contrastive Learning for preserving the intrinsic variety in sample features.   \n\u2022 Experiments on SYSU-MM01 and RegDB datasets demonstrate the superiority of our method compared to existing USVI-ReID methods, and PCLHD generates higher-quality pseudo-labels than other methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Supervised Visible-Infrared Person ReID ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Visible-infrared person re-identification (VI-ReID) has drawn much attention in recent years [27\u201332]. Many VI-ReID methods focused on mitigating huge semantic gaps across modalities have made advanced progress, which can be classified into two primary classes based on their different aligning ways: image-level alignment and feature-level alignment. The image-level alignment methods focus on reducing cross-modality gaps by modality translation. Some GAN-based methods [33, 34] are proposed to perform style transformation for aligning cross-modality images. However, the generated images unavoidably contain noise. Therefore, X-modality [35] and its promotions [36, 37] align cross-modality images by introducing a middle modality. Mainstream feature-level alignment methods [38\u201340] focus on minimizing cross-modality gaps by finding a modality-shared feature space. However, the advanced performances of the above methods build on large-scale humanlabeled cross-modality data, which are quite time-consuming and expensive, thus hindering the fast application of these methods in real-scenes. ", "page_idx": 1}, {"type": "text", "text": "2.2 Unsupervised Single-Modality Person ReID ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The existing unsupervised single-modality person ReID methods can be roughly divided into two classes: Unsupervised domain adaption (UDA) methods, which try to leverage the knowledge transferred from labeled source domain to improve performance [41\u201344], and fully unsupervised methods (USL), which directly train a USL-ReID model on the unlabeled target domain [21, 45]. Compared with the UDA methods, the USL methods are more challenging. Recently, clustercontrast learning [46] has achieved impressive performance by performing contrastive learning at the cluster level. However, cluster-contrast with a uni-proxy can be biased and confusing, which fails to accurately describe the information of a cluster. To this end, the methods [47, 48] proposed maintaining multi-proxies for a cluster to adaptively capture different information within the cluster. The above methods are mainly proposed to solve the single-modality ReID task, but they are limited to solving the USL-VI-ReID task due to large cross-modality gaps. ", "page_idx": 1}, {"type": "image", "img_path": "QQSGwpmDfU/tmp/4cf8e750d27234f4794c2caaf548251fa61f684ded67cf72d932f7f146bc0cff.jpg", "img_caption": ["Figure 1: Framework of our PCLHD. The framework consists of two stages: the first stage employs contrastive learning with centroid prototypes to learn well-discriminative representation, and the second stage introduces contrastive learning with hard and dynamic prototypes to further focus on divergence and variety. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.3 Unsupervised Visible-Infrared Person ReID ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unsupervised visible-infrared person ReID (USVI-ReID) has attracted much attention due to the advantage of not relying on any data annotation. Some UDA methods [49, 17] use a well-annotated labeled source domain for pre-training to solve the USVI-ReID task. Some fully unsupervised methods [23, 22] adopt contrastive learning to boost performance, which mainly follow a two-step loop paradigm: generating pseudo-labels using the DBSCAN algorithm [50] to create memory banks with clustering centers and establishing cross-modality correspondences based on these memory banks. However, pseudo-labels are often inaccurate and rigid, CCLNet [51] leverages the text information from CLIP to afford greater semantic monitoring insights to compensate for the rigidity of pseudo-labels. Moreover, reliable cross-modality correspondences are vital to USVI-ReID, thus PGM [23] proposes a progressive graph matching framework to establish more reliable crossmodality correspondences. However, cluster centers mainly present common information while lacking distinctive information, which results in ambiguous cross-modality correspondences when meeting hard samples [52, 53]. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation and Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a USVI-ReID dataset $D=\\{V,R\\}$ , where $V=\\{V_{i}\\}_{i=1}^{N_{v}}$ represents the visible images and $R=\\{R_{j}\\}_{j=1}^{N_{r}}$ denotes the infrared images. $V_{i}$ and $R_{j}$ represent the set of images corresponding to the $i$ -th and $j$ -th class. denote the number of visible and infrared clusters, respectively. In the USVI-ReID task, the purpose is to train a deep neural network to obtain modality-invariant and identity-related features for matching pedestrian images with the same identity. ", "page_idx": 2}, {"type": "text", "text": "We propose a Progressive Contrastive Learning with Hard and Dynamic Prototype (PCLHD) method for USVI-ReID, which mainly contains online encoder, momentum encoder, and progressive contrastive learning strategy with centroid prototype, hard prototype, and dynamic prototype, as shown in Fig. 1. The online encoder is a standard network, updated through back-propagation. The momentum encoder mirrors the structure of the online encoder, updated through the weights of the online encoder. The clustering is used to generate pseudo labels for creating cluster-aware memory, and we employ DBSCAN for clustering. PCLHD primarily focuses on representation learning, and we use PGM [23] to aggregate cross-modality memory. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Centroid Prototype Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following the USVI-ReID methods [22, 54], we use centroid prototype contrastive learning to optimize the online encoder in the first state, which includes memory initialization and optimization. ", "page_idx": 3}, {"type": "text", "text": "Memory Initialization. Let $\\phi_{0}$ be the online encoder that transforms the input image to an embedding vector. At the beginning of each training epoch, all image features are clustered by DBSCAN [50] and then each cluster\u2019s representations are stored in visible memory ${\\cal M}_{R G B}=\\{c m_{1}^{v},c m_{2}^{v},\\cdot\\cdot\\cdot,c m_{N_{v}}^{v}\\}$ and infrared memory ${\\cal M}_{I R}{=}\\{c m_{1}^{r},c m_{2}^{r},\\cdot\\cdot\\cdot,c m_{N_{r}}^{r}\\}$ , as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nc m_{i}^{v}=\\frac{1}{|V_{i}|}\\sum_{\\mathbf{v}\\in V_{i}}\\phi_{0}(\\mathbf{v}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\nc m_{j}^{r}=\\frac{1}{|R_{j}|}\\sum_{\\mathbf{r}\\in R_{j}}\\phi_{0}(\\mathbf{r}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\big|\\cdot\\big|$ denotes the number of instances belonging to specific cluster. ", "page_idx": 3}, {"type": "text", "text": "Optimization. During training, we update the two modality-specific memories by a momentum updating strategy [46]. We treat the memory center as a centroid prototype and optimize the feature extractor $\\phi_{0}$ using contrastive learning with the centroid prototype, computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{L}_{C P C L}^{v}=\\frac{1}{N_{v}}\\sum_{i\\in N_{v}}\\frac{-1}{\\vert{V_{i}}\\vert}\\sum_{\\mathrm{v\\inV}_{i}}\\log\\frac{\\exp{(\\phi_{0}(\\mathrm{v})\\cdot c m_{i}^{v}/\\tau)}}{\\sum_{j\\in N_{v}}\\exp{\\left(\\phi_{0}(\\mathrm{v})\\cdot c m_{j}^{v}/\\tau\\right)}},}}\\\\ {{\\displaystyle\\mathcal{L}_{C P C L}^{r}=\\frac{1}{N_{r}}\\sum_{i\\in N_{r}}\\frac{-1}{\\vert{R_{i}}\\vert}\\sum_{\\mathrm{r\\inR_{i}}}\\log\\frac{\\exp{(\\phi_{0}(\\mathrm{r})\\cdot c m_{i}^{r}/\\tau)}}{\\sum_{j\\in N_{r}}\\exp{\\left(\\phi_{0}(\\mathrm{r})\\cdot c m_{j}^{r}/\\tau\\right)}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C P C L}=\\mathcal{L}_{C P C L}^{v}+\\mathcal{L}_{C P C L}^{r},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where c miv(r)is the positive centroid prototype, denoting a query and the prototype shares the same identity. The $\\tau$ is a temperature hyper-parameter. ", "page_idx": 3}, {"type": "text", "text": "3.3 Hard Prototype Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To ensure that the prototype effectively captures divergence within a identity, we devise a novel hard prototype for contrastive learning, which is referred to as Hard Prototype Contrastive Learning. HPCL is designed to provide a comprehensive understanding of personal characteristics, which beneftis its handling of hard samples [47]. We use the online encoder $\\phi_{0}$ to extract feature representations, and select $k$ samples that are farthest from the memory center as the hard prototype: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h m_{i}^{v}=\\underset{\\forall\\mathrm{v}\\in V_{i}}{\\arg\\operatorname*{max}}\\left\\|\\phi_{0}(\\mathrm{v})-c m_{i}^{v}\\right\\|,}\\\\ {h m_{j}^{r}=\\underset{\\forall\\mathrm{r}\\in R_{j}}{\\arg\\operatorname*{max}}\\left\\|\\phi_{0}(\\mathrm{r})-c m_{j}^{r}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 1. The information entropy of hard sample prototypes is greater than the information entropy of centroid prototypes, thereby preserving greater divergence within the hard memory. ", "page_idx": 3}, {"type": "text", "text": "Given a set of features $\\{f_{1},f_{2},\\ldots,f_{N_{c}}\\}$ for class $c$ . The entropy $H(c m_{c})$ can be approximated by the entropy of the distribution of the sample means. Considering that $c m_{c}$ is a convex combination of the sample features $f_{i}$ , we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\nH\\left(c m_{c}\\right)=H\\left(\\frac{1}{N_{c}}\\sum_{i=1}^{N_{c}}f_{i}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By the convexity of entropy, we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\nH\\left(\\frac{1}{N_{c}}\\sum_{i=1}^{N_{c}}f_{i}\\right)\\leq\\frac{1}{N_{c}}\\sum_{i=1}^{N_{c}}H(f_{i}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This inequality implies that the entropy of the centroid prototype is generally lower due to the averaging effect, which reduces the divergence among the samples, leading to lower entropy. Given that $h m_{c}$ is the sample with the maximum individual entropy among the set $\\{f_{1},f_{2},\\ldots,f_{N_{c}}\\}.$ , it follows that: ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(h m_{c})\\geq\\frac{1}{N_{c}}\\sum_{i=1}^{N_{c}}H(f_{i})\\geq H(c m_{c}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, we construct contrastive loss with the hard prototype to minimize the distance between the query and the positive hard prototype while maximizing their discrepancy to all other cluster hard prototypes, as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{H P C L}^{v}=\\displaystyle\\frac{1}{N_{v}}\\sum_{i\\in N_{v}}\\frac{-1}{|V_{i}|}\\sum_{\\mathrm{v\\inV}_{i}}\\log\\displaystyle\\frac{\\exp\\big(\\phi_{0}(\\mathrm{v})\\cdot h m_{i}^{v}/\\tau\\big)}{\\sum_{j\\in N_{v}}\\exp\\big(\\phi_{0}(\\mathrm{v})\\cdot h m_{j}^{v}/\\tau\\big)},}\\\\ &{\\mathcal{L}_{H P C L}^{r}=\\displaystyle\\frac{1}{N_{r}}\\sum_{i\\in N_{r}}\\frac{-1}{|R_{i}|}\\sum_{\\mathrm{r\\inR}_{i}}\\log\\displaystyle\\frac{\\exp\\big(\\phi_{0}(\\mathrm{r})\\cdot h m_{i}^{r}/\\tau\\big)}{\\sum_{j\\in N_{r}}\\exp\\big(\\phi_{0}(\\mathrm{r})\\cdot h m_{j}^{r}/\\tau\\big)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{H P C L}=\\mathcal{L}_{H P C L}^{v}+\\mathcal{L}_{H P C L}^{r},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h m_{i}^{v(r)}$ is the positive hard prototype representation and the $\\tau$ is a temperature hyper-parameter. Finally, we update the two modality-specific memories with a momentum-updating strategy: ", "page_idx": 4}, {"type": "equation", "text": "$$\nh m_{i,t}^{v}=\\alpha h m_{i,t-1}^{v}+(1-\\alpha)\\phi_{0}(\\mathrm{v}),\\forall\\mathrm{v}\\in V_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nh m_{i,t}^{r}=\\alpha h m_{i,t-1}^{r}+(1-\\alpha)\\phi_{0}(\\mathbf{r}),\\forall\\mathbf{r}\\in R_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ is a momentum coefficient that controls the update speed of the memories. $t$ and $t-1$ refer to the current and last iteration, respectively. ", "page_idx": 4}, {"type": "text", "text": "The hard prototype contrastive learning has two main advantages: For intra-class feature learning, it ensures that the learning process does not just focus on the shared characteristics within a cluster but also considers the diverse elements, which are often more informative. For inter-class feature learning, it is also beneficial for increasing the distances between different persons. In contrast, centroid prototypes tend to average features, lacking diversity, which can affect the network\u2019s ability to extract discriminative features. ", "page_idx": 4}, {"type": "text", "text": "3.4 Dynamic Prototype Contrastive Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspired by MoCo [55] and DPM [56], we design dynamic prototype contrastive learning in order to preserve the intrinsic variety in sample features. DPCL comprises an online encoder $\\phi_{0}$ and a momentum encoder $\\phi_{m}$ . The momentum encoder mirrors the structure of the online encoder, which is updated by the accumulated weights of the online encoder: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{m}^{t}=\\beta\\phi_{m}^{t-1}+(1-\\beta)\\phi_{0}^{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta$ is a momentum coefficient that controls the update speed of the momentum encoder. $t$ and $t-1$ refer to the current and last iteration, respectively. The momentum encoder $\\phi_{m}$ is updated by the moving averaged weights, which are resistant to sudden fluctuations or noisy updates [55]. ", "page_idx": 4}, {"type": "text", "text": "We use the momentum encoder $\\phi_{m}$ to extract feature representation and store them in visible memory $D M_{R G B}{=}\\{d m_{1}^{v},\\,d m_{2}^{v},\\,\\cdot\\,\\cdot\\,,\\,d m_{N_{\\!,}}^{v},$ and infrared memory $D M_{I R}{=}\\{d m_{1}^{r},\\,d m_{2}^{r},\\,\\cdot\\cdot\\cdot,\\,d m_{N_{r}}^{r}\\}$ . We randomly select $M$ visible/infrared samples from each cluster, denoted as $X_{i}^{v}$ and $X_{j}^{r}$ .as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{i}^{v}=\\phi_{m}({\\boldsymbol{X}}_{i}^{v}),}\\\\ {F_{j}^{r}=\\phi_{m}({\\boldsymbol{X}}_{j}^{r}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We select visible dynamic prototype $d m_{i}^{v}$ from $D M_{R G B}$ . In the same cluster, we select the sample farthest from the query image as the prototype. In different clusters, we choose the sample closest to the query image as the prototype: ", "page_idx": 5}, {"type": "equation", "text": "$$\nd m_{i}^{v}=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\arg\\operatorname*{max}_{\\forall\\,i}\\lVert\\phi_{m}(\\mathrm{v}_{j})-f_{i}^{v}\\rVert}&{\\mathrm{if}\\;y_{j}=y_{i}}\\\\ {\\arg\\operatorname*{min}_{\\forall\\,i}\\lVert\\phi_{m}(\\mathrm{v}_{j})-f_{i}^{v}\\rVert}&{\\mathrm{if}\\;y_{j}\\neq y_{i}}\\end{array}\\!\\!,\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $y_{q}$ and $y_{i}$ represent the pseudo label of the query image and the dynamic prototype, respectively.   \n$\\left\\Vert\\cdot\\right\\Vert$ denotes Euclidean norm. We obtain infrared prototype $d m_{j}^{r}$ through the same method. ", "page_idx": 5}, {"type": "text", "text": "The overall optimization goal of DPCL is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D P C L}^{v}=\\frac{1}{N_{v}}\\sum_{i\\in N_{v}}\\frac{-1}{|V_{i}|}\\sum_{\\mathrm{v}\\in V_{i}}\\log\\frac{\\exp{(\\phi_{m}(\\mathrm{v})\\cdot d m_{i}^{v}/\\tau)}}{\\sum_{j\\in N_{v}}\\exp{\\left(\\phi_{m}(\\mathrm{v})\\cdot d m_{j}^{v}/\\tau\\right)}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D P C L}^{r}=\\frac{1}{N_{r}}\\sum_{i\\in N_{r}}\\frac{-1}{|R_{i}|}\\sum_{\\mathrm{r}\\in R_{i}}\\log\\frac{\\exp{(\\phi_{m}(\\mathrm{r})\\cdot d m_{i}^{r}/\\tau)}}{\\sum_{j\\in N_{r}}\\exp{\\left(\\phi_{m}(\\mathrm{r})\\cdot d m_{j}^{r}/\\tau\\right)}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D P C L}=\\mathcal{L}_{D P C L}^{v}+\\mathcal{L}_{D P C L}^{r},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d m_{i}^{v(r)}$ is the positive dynamic prototype representation, i.e., the query image and dynamic prototype have the same identity. ", "page_idx": 5}, {"type": "text", "text": "DPCL promotes a flexible and adaptable learning process, aiming to minimize discrepancies between samples and their respective dynamic prototypes, rather than rigidly aligning query images with a fixed prototype. ", "page_idx": 5}, {"type": "text", "text": "3.5 Progressive Contrastive Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the initial training phases, representations are generally of lower quality. Introducing hard samples at this period could be counterproductive, potentially leading the model optimization in an incorrect direction right from the start [47, 57]. To address this issue, we introduce the Progressive Contrastive Learning, which forms the overall loss function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{P C L H D}=\\left\\{\\begin{array}{l l}{\\mathcal{L}_{C P C L},\\quad\\mathrm{~if~\\epoch~\\leqslant~}E_{\\mathrm{CPCL}}}\\\\ {\\lambda\\mathcal{L}_{H P C L}+(1-\\lambda)\\mathcal{L}_{D P C L},\\quad\\mathrm{else}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is the loss weight, $E_{\\mathrm{CPCL}}$ is a hyper-parameter. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct extensive experiments to validate the superiority of our proposed method. First, we provide the detailed experiment setting, which contains datasets, evaluation protocols, and implementation details. Then, we compare our method with many state-of-the-art VI-ReID methods and conduct ablation studies. In addition, to better illustrate our method, we also exhibit further analysis. If not specified, we conduct analysis experiments on SYSU-MM01 in the all-search mode. ", "page_idx": 5}, {"type": "table", "img_path": "QQSGwpmDfU/tmp/82f4799062d4c4fb5a1ddc93a70689e3ae218e6f129fc957a3714fd8d5a9b2ad.jpg", "table_caption": ["Table 1: Comparisons with state-of-the-art methods on SYSU-MM01 and RegDB, including SVIReID, SSVI-ReID and USVI-ReID methods. All methods are measured by Rank-1 $(\\%)$ and mAP $(\\%)$ . GUR\\* denotes the results without camera information. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "QQSGwpmDfU/tmp/28cf75a718a9d079c112e7f003fa8d8841e9d1edc292ff663528b7fff43ae16d.jpg", "table_caption": ["Table 2: Ablation studies on SYSU-MM01 in all search mode and indoor search mode. \u201cBaseline\u201d means the model trained following PGM [23]. Rank-R accuracy $(\\%)$ and $\\mathrm{mAP}(\\%)$ are reported. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Experiment Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. We evaluate our method on two common benchmarks in VI-ReID: SYSU-MM01 [71] and RegDB [72]. SYSU-MM01 is a large-scale public benchmark for the VI-ReID task, which contains 491 identities captured by four RGB cameras and two IR cameras in both outdoor and indoor environments. In this dataset, $22{,}258\\ \\mathrm{RGB}$ images and 11,909 IR images with 395 identities are collected for training. In the inference stage, the query set consists of 3,803 IR images with 96 identities and the galley set contains 301 randomly selected RGB images. RegDB is collected by an RGB camera and an IR camera, which contains 4,120 RGB images and 4,120 IR images with 412 identities. To be specific, the dataset is randomly divided into two non-overlapping sets: one set is used for training and the other is for testing. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Protocols. The experiment follows the standard evaluation settings in VI-ReID, i.e., Cumulative Matching Characteristics (CMC) [73] and mean Average Precision (mAP). ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We adopt the feature extractor in AGW [58], which is initialized with ImageNet-pretrained weights to extract 2048-dimensional features. During the training stage, the input images are resized to $288\\!\\times\\!144$ . We follow augmentations in CAJ [1] for data augmentation. In one batch, we randomly sample 16 pseudo identities, and each pseudo identity samples 16 instances. We set $M$ to be 16 for computational convenience. The number of epochs is 100, in which the first 50 epochs are trained by contrastive loss with the centroid prototype. For the last 50 epochs, we train the model by contrastive loss with both the hard and dynamic prototypes. $E_{C P C L}$ is 50. At the beginning of each epoch, we utilize the DBSCAN [50] algorithm to generate pseudo labels. During the inference stage, we use the momentum encoder $\\phi_{m}$ to extract features and take the features of the global average pooling layer to calculate cosine similarity for retrieval. The momentum value $\\alpha$ and $\\beta$ is set to 0.1 and 0.999, respectively. The temperature hyper-parameter $\\tau$ is set to 0.05 and the weighting hyper-parameter $\\lambda$ in Eq.(23) is 0.5. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Results and Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To comprehensively evaluate our method, we compare our method with 18 supervised VI-ReID methods, 3 semi-supervised VI-ReID methods, and 9 unsupervised VI-ReID methods. The comparison results on the SYSU-MM01 and RegDB are reported in Tab. 1. ", "page_idx": 7}, {"type": "text", "text": "Comparison with USVI-ReID Methods. As shown in Tab. 1, our method achieves superior performance compared with state-of-the-art USVI-ReID methods. MMM [54] is proposed to establish reliable cross-modality correspondences and is also the current best-performing method. Our method with MMM can achieve $65.9\\%$ in Rank-1 and $61.8\\%$ in mAP, which surpasses that of MMM by a large margin of $4.3\\%$ and $3.9\\%$ . Notably, our method even without MMM gains the best performance with $64.4\\%$ in Rank-1 and $58.7\\%$ in mAP. Although existing USVI-ReID methods mentioned in Tab. 1 have made great progress in the USVI-ReID task, the neglects of divergence and variety hinder their further improvement. They overlook divergence and variety, which often constitutes hard samples. Thus, we propose progressive contrastive learning with hard and dynamic prototypes to mine hard samples, which can guide the model to learn more robust and discriminative features. ", "page_idx": 7}, {"type": "text", "text": "Comparison with SSVI-ReID Methods. There are three SSVI-ReID methods proposed to alleviate the problem of labeling cost by using a part of annotations. Remarkably, our method achieves superior performance without any annotations, outperforming all existing SSVI-ReID methods that utilize partial annotations. Moreover, the results suggest that our method can significantly reduce the dependency on manual annotations. ", "page_idx": 7}, {"type": "text", "text": "Comparison with SVI-ReID Methods. Surprisingly, our method without annotation outperforms several SVI-ReID methods, e.g., DDAG [39], AGW [58], NFS [59], LbA [60]. This shows the immense competitiveness of PCLHD compared to SVI-ReID methods that rely on complete data annotations. The superior performance of PCLHD mainly benefits from the hard prototype and dynamic prototype contrastive learning. Additionally, we have to acknowledge that a significant disparity still exists between PCLHD and the state-of-the-art fully-supervised results. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct ablation studies on the SYSU-MM01 dataset in both all-search and indoor-search modes to show the effectiveness of each component in our method. The results are shown in Tab. 2. ", "page_idx": 7}, {"type": "text", "text": "Baseline Settings. We use PGM [23] as our baseline. Although PGM has achieved a promising performance on the USVI-ReID task, the neglect of hard samples hinders its further improvement. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of HPCL. The HPCL is proposed to mine divergence. As shown in Tab. 2, When adding the HPCL on Baseline, the performance improves a large margin of $5.8\\%$ in Rank-1 and $5.1\\%$ in mAP, respectively. It shows that divergence can be effectively mined using hard prototype contrast learning, facilitating the model to learn more discriminative features. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of DPCL. The DPCL is proposed to mine variety. The results show that Rank-1 accuracy can be improved by $2.8\\%$ in Rank-1 and $2.7\\%$ in mAP when adding the DPCL on Baseline, which confirms that contrastive learning with dynamic prototype can learn variety. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of PCL. PCL is introduced to smoothly shift the model\u2019s attention from commonality to divergence and variety. The results show that Rank-1 accuracy can be improved by about $1\\%$ in Rank-1 and mAP compared to adding simultaneously the HPCL and DPCL on the Baseline. This confirms that progressive contrastive learning plays a valuable role in assisting HPCL and DPCL. ", "page_idx": 7}, {"type": "image", "img_path": "QQSGwpmDfU/tmp/558bf0a2c763e1e7734bbe87f588fc1e2dabdbc598f30abbe6da01eccb57ce36.jpg", "img_caption": ["Figure 2: (a) The effect of hyper-parameter $\\lambda$ with different values. (b) The effect of hyper-parameter $k$ with different values. (c) Comparisons with ARI values of different methods. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "QQSGwpmDfU/tmp/c6f2f6945b67594ea45ceb3feec81aa3fabb534c89d4e388412e5b1680474746.jpg", "img_caption": ["Figure 3: The t-SNE visualization of 10 randomly selected identities. Different color indicates different IDs. Circle means visible features and the pentagram means infrared features. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Surprisingly, contrastive learning with both hard and dynamic prototypes significantly exceeds the baseline by a large margin of $8.1\\%$ in Rank-1 and $7.0\\%$ in mAP. The HPCL and DPCL can complement each other to learn divergence and variety, which effectively guides the network to learn more robust and discriminative features. ", "page_idx": 8}, {"type": "text", "text": "4.4 Further Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Hyper-parameter analysis. Hyper-parameter $\\lambda$ is a weighting parameter to trade-off $L_{H P C L}$ and $L_{D P C L}$ . Fig. 2 (a) presents the results under different values of $\\lambda$ . We can observe that when $\\lambda$ is small, i.e., $L_{D P C L}$ contributes more to the model, the performance degrades. However, when $\\lambda$ is large, i.e., $L_{H P C L}$ contributes heavily to the model, the model both achieves superior performance. Note that when $\\lambda=1$ , i.e., the proposed method is trained without DPCL, the performance drops significantly. $\\lambda$ is finally set to 0.5 and our method achieves the best performance of $64.4\\%$ in Rank-1. Moreover, we also analyze the effect of the number of hard samples at hard prototype. As shown in Fig. 2 (b), we vary the $k$ from 1 to 3 and keep the other hyper-parameters fixed, which shows that PCLHD achieves the best performance when $k=1$ . Hard samples are distributed in multiple directions, so multiple hard samples cannot be represented by a single prototype. This is why using more hard samples as prototypes leads to a decline in overall performance ", "page_idx": 8}, {"type": "text", "text": "The ARI metric. Following MMM [54], we utilize the Adjusted Rand Index (ARI) metric for clustering evaluation. The larger the ARI value, the higher the clustering quality. In Fig. 2 (c), \u201cRGB\u201d and \u201cIR\u201d denote the ARI values of visible and infrared clusterings, which can measure the quality of visible and infrared pseudo-labels. \u201cALL\u201d means the ARI values of overall clusterings, which can evaluate the reliability of cross-modality correspondences. PCLHD surpasses other methods significantly on all of the mentioned ARI values, which demonstrates PCLHD can effectively mine divergence and variety to improve clustering quality. ", "page_idx": 8}, {"type": "text", "text": "Visualization. As shown in Fig. 3, we visualize the t-SNE map of 10 randomly chosen identities from SYSU-MM01. Compared to the baseline, the distribution of the same identity from the same modality is more compact and the distance of the same identity from different modalities is closer together. Moreover, some hard samples in the baseline are incorrectly clustered, while these hard samples are well clustered in our PCLHD, which shows the effectiveness of the proposed PCLHD. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel method for USVI-ReID called Progressive Contrastive Learning with hard and dynamic prototype (PCLHD), which learns commonality, divergence and variety. To be specific, we design Hard Prototype Contrastive Learning to mine divergent yet significant information and Dynamic Prototype Contrastive Learning to preserve intrinsic variety features. Furthermore, we introduce a progressive learning strategy to incorporate both HPCL and DPCL into the model. Extensive experiments demonstrate that PCLHD outperforms state-of-the-art USVI-ReID methods. ", "page_idx": 9}, {"type": "text", "text": "This work relies on DBSCAN to generate pseudo-labels. However, for extremely large-scale datasets, DBSCAN\u2019s performance may be limited, which could affect the overall effectiveness of our approach. To address the limitation, we plan to explore hierarchical clustering in future research to better handle large-scale datasets. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was developed using publicly available datasets and aims to enhance the capabilities of VI-ReID, which plays a vital role in scenarios where traditional ReID systems fail, such as in low-light or nighttime conditions. VI-ReID offers significant benefits in improving security and surveillance by enabling more reliable identification across varying environmental conditions. Importantly, this work raises no ethical, safety, or environmental concerns, and no harm was infilcted on living beings during the research. However, we acknowledge the risk of misuse, particularly privacy invasion if used to track individuals in public spaces without appropriate regulation. While VI-ReID does not directly identify specific individuals, its unauthorized deployment could still result in significant privacy violations. Therefore, public surveillance systems using VI-ReID should be controlled by authorized entities, ensuring proper regulatory frameworks and adherence to ethical standards. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (No. 62176224, 62176092, 62222602, 62306165, 62106075, 62476090), Natural Science Foundation of Shanghai (23ZR1420400), Natural Science Foundation of Chongqing (CSTB2023NSCQ-JQX0007), China Computer Federation (CCF) Lenovo Blue Ocean Research Fund, China Academy of Railway Sciences No. 2023YJ357. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Mang Ye, Weijian Ruan, Bo Du, and Mike Zheng Shou. Channel augmented joint learning for visibleinfrared recognition. In ICCV, pages 13547\u201313556, 2021.   \n[2] Yunpeng Gong, Zhun Zhong, Zhiming Luo, Yansong Qu, Rongrong Ji, and Min Jiang. Cross-modality perturbation synergy attack for person re-identification. arXiv preprint arXiv:2401.10090, 2024.   \n[3] Jiangming Shi, Xiangbo Yin, Demao Zhang, and Yanyun Qu. Visible embraces infrared: Cross-modality person re-identification with single-modality supervision. In China Automation Congress, pages 4781\u2013 4787, 2023.   \n[4] Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Yiran Liu, Ji Gan, Haosheng Chen, and Xinbo Gao. Beyond euclidean: Dual-space representation learning for weakly supervised video violence detection. arXiv preprint arXiv:2409.19252, 2024.   \n[5] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Yue Hu, and Qi Wu. Context-i2w: Mapping images to context-dependent words for accurate zero-shot composed image retrieval. In AAAI, pages 5180\u20135188, 2024.   \n[6] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gaopeng Gou, Gang Xiong, and Qi Wu. Denoise-i2w: Mapping images to denoising words for accurate zero-shot composed image retrieval. arXiv preprint arXiv:2410.17393, 2024.   \n[7] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. Vista: Visualized text embedding for universal multi-modal retrieval. In ACL, pages 3185\u20133200, 2024.   \n[8] Yachao Zhang, Runze Hu, Ronghui Li, Yanyun Qu, Yuan Xie, and Xiu Li. Cross-modal match for language conditioned 3d object grounding. In AAAI, volume 38, pages 7359\u20137367, 2024.   \n[9] Yachao Zhang, Yuan Xie, Cuihua Li, Zongze Wu, and Yanyun Qu. Learning all-in collaborative multiview binary representation for clustering. IEEE TNNLS, 35(3):4260\u20134273, 2022.   \n[10] Ling Lin, Hao Liu, Jinqiao Liang, Zhendong Li, Jiao Feng, and Hu Han. Consensus-agent deep reinforcement learning for face aging. IEEE Transactions on Image Processing, 2024.   \n[11] Ling Lin, Tao Wang, Hao Liu, Congcong Zhu, and Jingrun Chen. Toward quantifiable face age transformation under attribute unbias. IEEE TCSVT, 2024.   \n[12] Jiawei Feng, Ancong Wu, and Wei-Shi Zheng. Shape-erased feature learning for visible-infrared person re-identification. In CVPR, pages 22752\u201322761, 2023.   \n[13] Xingye Fang, Yang Yang, and Ying Fu. Visible-infrared person re-identification via semantic alignment and affinity inference. In ICCV, pages 11270\u201311279, 2023.   \n[14] Rui Sun, Long Chen, Lei Zhang, Ruirui Xie, and Jun Gao. Robust visible-infrared person re-identification based on polymorphic mask and wavelet graph convolutional network. IEEE TIFS, 2024.   \n[15] Xiangbo Yin, Jiangming Shi, Yachao Zhang, Yang Lu, Zhizhong Zhang, Yuan Xie, and Yanyun Qu. Robust pseudo-label learning with neighbor relation for unsupervised visible-infrared person re-identification. In ACM MM, 2024.   \n[16] Bo Pang, Deming Zhai, Junjun Jiang, and Xianming Liu. Fully unsupervised person re-identification via selective contrastive learning. ACM TOMM, 18(2):1\u201315, 2022.   \n[17] Jiangming Wang, Zhizhong Zhang, Mingang Chen, Yi Zhang, Cong Wang, Bin Sheng, Yanyun Qu, and Yuan Xie. Optimal transport for label-efficient visible-infrared person re-identification. In ECCV, pages 93\u2013109, 2022.   \n[18] Jiangming Shi, Yachao Zhang, Xiangbo Yin, Yuan Xie, Zhizhong Zhang, Jianping Fan, Zhongchao Shi, and Yanyun Qu. Dual pseudo-labels interactive self-training for semi-supervised visible-infrared person re-identification. In ICCV, pages 11218\u201311228, 2023.   \n[19] Bin Yang, Jun Chen, Xianzheng Ma, and Mang Ye. Translation, association and augmentation: Learning cross-modality re-identification from single-modality annotation. IEEE TIP, 32:5099\u20135113, 2023.   \n[20] Bin Yang, Mang Ye, Jun Chen, and Zesen Wu. Augmented dual-contrastive aggregation learning for unsupervised visible-infrared person re-identification. In ACM MM, pages 2843\u20132851, 2022.   \n[21] Guoqing Zhang, Hongwei Zhang, Weisi Lin, Arun Kumar Chandran, and Xuan Jing. Camera contrast learning for unsupervised person re-identification. IEEE TCSVT, 33(8):4096\u20134107, 2023.   \n[22] Bin Yang, Jun Chen, and Mang Ye. Towards grand unified representation learning for unsupervised visible-infrared person re-identification. In ICCV, pages 11069\u201311079, 2023.   \n[23] Zesen Wu and Mang Ye. Unsupervised visible-infrared person re-identification via progressive graph matching and alternate learning. In CVPR, pages 9548\u20139558, 2023.   \n[24] Lei Tan, Jiaer Xia, Wenfeng Liu, Pingyang Dai, Yongjian Wu, and Liujuan Cao. Occluded person re-identification via saliency-guided patch transfer. In AAAI, volume 38, pages 5070\u20135078, 2024.   \n[25] Lei Tan, Pingyang Dai, Jie Chen, Liujuan Cao, Yongjian Wu, and Rongrong Ji. Partformer: Awakening latent diverse representation from vision transformer for object re-identification. arXiv preprint arXiv:2408.16684, 2024.   \n[26] Kunlun Xu, Xu Zou, Yuxin Peng, and Jiahuan Zhou. Distribution-aware knowledge prototyping for non-exemplar lifelong person re-identification. In CVPR, pages 16604\u201316613, 2024.   \n[27] Mouxing Yang, Zhenyu Huang, Peng Hu, Taihao Li, Jiancheng Lv, and Xi Peng. Learning with twin noisy labels for visible-infrared person re-identification. In CVPR, pages 14288\u201314297, 2022.   \n[28] Pingping Zhang, Yuhao Wang, Yang Liu, Zhengzheng Tu, and Huchuan Lu. Magic tokens: Select diverse tokens for multi-modal object re-identification. In CVPR, pages 17117\u201317126, 2024.   \n[29] Yuhao Wang, Xuehu Liu, Pingping Zhang, Hu Lu, Zhengzheng Tu, and Huchuan Lu. Top-reid: Multispectral object re-identification with token permutation. In AAAI, pages 5758\u20135766, 2024.   \n[30] Jialong Zuo, Hanyu Zhou, Ying Nie, Feng Zhang, Tianyu Guo, Nong Sang, Yunhe Wang, and Changxin Gao. Ufinebench: Towards text-based person retrieval with ultra-fine granularity. In CVPR, pages 22010\u201322019, 2024.   \n[31] Chenyang Yu, Xuehu Liu, Yingquan Wang, Pingping Zhang, and Huchuan Lu. Tf-clip: Learning text-free clip for video-based person re-identification. In AAAI, pages 6764\u20136772, 2024.   \n[32] Yukang Zhang, Yan Yan, Jie Li, and Hanzi Wang. Mrcn: A novel modality restitution and compensation network for visible-infrared person re-identification. In AAAI, volume 37, pages 3498\u20133506, 2023.   \n[33] Guan\u2019an Wang, Yang Yang, Tianzhu Zhang, Jian Cheng, Zengguang Hou, Prayag Tiwari, and Hari Mohan Pandey. Cross-modality paired-images generation and augmentation for rgb-infrared person reidentification. Neural Networks, 128:294\u2013304, 2020.   \n[34] Guan\u2019an Wang, Tianzhu Zhang, Jian Cheng, Si Liu, Yang Yang, and Zengguang Hou. Rgb-infrared cross-modality person re-identification via joint pixel and feature alignment. In ICCV, pages 3622\u20133631, 2019.   \n[35] Diangang Li, Xing Wei, Xiaopeng Hong, and Yihong Gong. Infrared-visible cross-modal person reidentification with an X modality. In AAAI, pages 4610\u20134617, 2020.   \n[36] Yukang Zhang, Yan Yan, Yang Lu, and Hanzi Wang. Towards a unified middle modality learning for visible-infrared person re-identification. In ACM MM, pages 788\u2013796, 2021.   \n[37] Ziyu Wei, Xi Yang, Nannan Wang, and Xinbo Gao. Syncretic modality collaborative learning for visible infrared person re-identification. In ICCV, pages 225\u2013234, 2021.   \n[38] Qiang Zhang, Changzhou Lai, Jianan Liu, Nianchang Huang, and Jungong Han. Fmcnet: Feature-level modality compensation for visible-infrared person re-identification. In CVPR, pages 7339\u20137348, 2022.   \n[39] Mang Ye, Jianbing Shen, David J. Crandall, Ling Shao, and Jiebo Luo. Dynamic dual-attentive aggregation learning for visible-infrared person re-identification. In ECCV, pages 229\u2013247, 2020.   \n[40] Qiong Wu, Pingyang Dai, Jie Chen, Chia-Wen Lin, Yongjian Wu, Feiyue Huang, Bineng Zhong, and Rongrong Ji. Discover cross-modality nuances for visible-infrared person re-identification. In CVPR, pages 4330\u20134339, 2021.   \n[41] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. In ICLR, 2020.   \n[42] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hongsheng Li. Self-paced contrastive learning with hybrid memory for domain adaptive object re-id. In NeurIPS, 2020.   \n[43] Neng Dong, Liyan Zhang, Shuanglin Yan, Hao Tang, and Jinhui Tang. Erasing, transforming, and noising defense network for occluded person re-identification. IEEE TCSVT, pages 1\u20131, 2023.   \n[44] Neng Dong, Shuanglin Yan, Hao Tang, Jinhui Tang, and Liyan Zhang. Multi-view information integration and propagation for occluded person re-identification. Information Fusion, 104:102201, 2024.   \n[45] Yoonki Cho, Woo Jae Kim, Seunghoon Hong, and Sung-Eui Yoon. Part-based pseudo label refinement for unsupervised person re-identification. In CVPR, pages 7298\u20137308, 2022.   \n[46] Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Siyu Zhu, and Ping Tan. Cluster contrast for unsupervised person re-identification. In ACCV, pages 319\u2013337, 2022.   \n[47] Chang Zou, Zeqi Chen, Zhichao Cui, Yuehu Liu, and Chi Zhang. Discrepant and multi-instance proxies for unsupervised person re-identification. In ICCV, pages 11058\u201311068, 2023.   \n[48] Yuhang Wu, Tengteng Huang, Haotian Yao, Chi Zhang, Yuanjie Shao, Chuchu Han, Changxin Gao, and Nong Sang. Multi-centroid representation network for domain adaptive person re-id. In AAAI, pages 2750\u20132758, 2022.   \n[49] Wenqi Liang, Guangcong Wang, Jianhuang Lai, and Xiaohua Xie. Homogeneous-to-heterogeneous: Unsupervised learning for rgb-infrared person re-identification. IEEE TIP, 30:6392\u20136407, 2021.   \n[50] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD, pages 226\u2013231, 1996.   \n[51] Zhong Chen, Zhizhong Zhang, Xin Tan, Yanyun Qu, and Yuan Xie. Unveiling the power of clip in unsupervised visible-infrared person re-identification. In ACM MM, pages 3667\u20133675, 2023.   \n[52] Mouxing Yang, Yunfan Li, Zhenyu Huang, Zitao Liu, Peng Hu, and Xi Peng. Partially view-aligned representation learning with noise-robust contrastive loss. In CVPR, pages 1134\u20131143, 2021.   \n[53] Mouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jiancheng Lv, and Xi Peng. Robust multi-view clustering with incomplete information. IEEE TPAMI, 45(1):1055\u20131069, 2023.   \n[54] Jiangming Shi, Xiangbo Yin, Yeyun Chen, Yachao Zhang, Zhizhong Zhang, Yuan Xie, and Yanyun Qu. Multi-memory matching for unsupervised visible-infrared person re-identification. In ECCV, page 456\u2013474, 2024.   \n[55] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In ICCV, pages 9729\u20139738, 2020.   \n[56] Lei Tan, Pingyang Dai, Rongrong Ji, and Yongjian Wu. Dynamic prototype mask for occluded person re-identification. In ACM MM, pages 531\u2013540, 2022.   \n[57] Yunpeng Gong, Liqing Huang, and Lifei Chen. Person re-identification method based on color attack and joint defence. In CVPRW, pages 4312\u20134321, 2022.   \n[58] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven C. H. Hoi. Deep learning for person re-identification: A survey and outlook. IEEE TPAMI, pages 2872\u20132893, 2022.   \n[59] Yehansen Chen, Lin Wan, Zhihang Li, Qianyan Jing, and Zongyuan Sun. Neural feature search for rgb-infrared person re-identification. In CVPR, pages 587\u2013597, 2021.   \n[60] Hyunjong Park, Sanghoon Lee, Junghyup Lee, and Bumsub Ham. Learning by aligning: Visible-infrared person re-identification using cross-modal correspondences. In ICCV, pages 12026\u201312035, 2021.   \n[61] Zhipeng Huang, Jiawei Liu, Liang Li, Kecheng Zheng, and Zheng-Jun Zha. Modality-adaptive mixup and invariant decomposition for rgb-infrared person re-identification. In AAAI, pages 1034\u20131042, 2022.   \n[62] Mahdi Alehdaghi, Arthur Josi, Rafael M. O. Cruz, and Eric Granger. Visible-infrared person reidentification using privileged intermediate information. In ECCV, pages 720\u2013737, 2022.   \n[63] Yukang Zhang and Hanzi Wang. Diverse embedding expansion network and low-light cross-modality benchmark for visible-infrared person re-identification. In CVPR, pages 2153\u20132162, 2023.   \n[64] Minsu Kim, Seungryong Kim, Jungin Park, Seongheon Park, and Kwanghoon Sohn. Partmix: Regularization strategy to learn part discovery for visible-infrared person re-identification. In CVPR, pages 18621\u201318632, 2023.   \n[65] Jianbing Wu, Hong Liu, Yuxin Su, Wei Shi, and Hao Tang. Learning concordant attention via target-aware alignment for visible-infrared person re-identification. In ICCV, pages 11122\u201311131, 2023.   \n[66] Hao Yu, Xu Cheng, Wei Peng, Weihao Liu, and Guoying Zhao. Modality unifying network for visibleinfrared person re-identification. In ICCV, pages 11185\u201311195, 2023.   \n[67] Yukang Zhang, Yang Lu, Yan Yan, Hanzi Wang, and Xuelong Li. Frequency domain nuances mining for visible-infrared person re-identification. arXiv preprint arXiv:2401.02162, 2024.   \n[68] Mouxing Yang, Zhenyu Huang, and Xi Peng. Robust object re-identification with coupled noisy labels. IJCV, pages 1\u201319, 2024.   \n[69] De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, and Xinbo Gao. Unsupervised visible-infrared person reid by collaborative learning with neighbor-guided label refinement. In ACM MM, pages 7085\u20137093, 2023.   \n[70] Lingfeng He, Nannan Wang, Shizhou Zhang, Zhen Wang, Xinbo Gao, et al. Efficient bilateral crossmodality cluster matching for unsupervised visible-infrared person reid. In ACM MM, pages 1325\u20131333, 2023.   \n[71] Ancong Wu, Wei-Shi Zheng, Hong-Xing Yu, Shaogang Gong, and Jianhuang Lai. Rgb-infrared crossmodality person re-identification. In ICCV, pages 5390\u20135399, 2017.   \n[72] Dat Tien Nguyen, Hyung Gil Hong, Ki-Wan Kim, and Kang Ryoung Park. Person recognition system based on a combination of body images from visible light and thermal cameras. Sensors, 17(3):605, 2017.   \n[73] Mang Ye, Zheng Wang, Xiangyuan Lan, and Pong C. Yuen. Visible thermal person re-identification via dual-constrained top-ranking. In IJCAI, pages 1092\u20131099, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: This work relies on DBSCAN to generate pseudo-labels. However, for extremely large-scale datasets, DBSCAN\u2019s performance may be limited, which could affect the overall effectiveness of our approach. To address the limitation, we plan to explore hierarchical clustering in future research to better handle large-scale datasets. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: For each theoretical result, the paper provide the full set of assumptions and a complete (and correct) proof. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims. Our code will be released after the acceptance of our paper. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 14}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper provide open access to the code, with sufficient instructions to faithfully reproduce the main experimental results. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper specify all the training and test details. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [No] ", "page_idx": 15}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provide sufficient information on the computer resources. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The paper was developed using publicly available infrared-visible ReID datasets and aims to enhance the capabilities of visible-infrared ReID, which plays a vital role in scenarios where traditional ReID systems fail, such as in low-light or nighttime conditions. This technology offers significant beneftis in improving security and surveillance by enabling more reliable identification across varying environmental conditions. Importantly, our research raises no ethical, safety, or environmental concerns, and no harm was inflicted on living beings during the research. However, we acknowledge the risk of misuse, particularly privacy invasion if used to track individuals in public spaces without appropriate regulation. While ReID technology does not directly identify specific individuals, its unauthorized deployment could still result in significant privacy violations. Therefore, public surveillance systems using ReID should be controlled by authorized entities, ensuring proper regulatory frameworks, transparency, and adherence to ethical standards. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: They are properly credited and respected. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: The datasets used in this paper, SYSU-MM01 and RegDB, are publicly available and widely used in research. These datasets were collected by their original creators and made accessible for research purposes. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Since we are using datasets that are already publicly available and have beenextensively used in previous research, and given that the content does not involve sensitivepersonal information, this study did not undergo an independent IRB review. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]