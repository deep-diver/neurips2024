[{"figure_path": "rzvVm0LsyK/figures/figures_5_1.jpg", "caption": "Figure 1: Performance comparison between Adam, AMSGrad and ADOPT in a simple univariate convex optimization problem. The plots show transitions of the parameter value, which should converge to the solution \u03b8 = \u22121.", "description": "This figure compares the performance of three optimization algorithms (Adam, AMSGrad, and ADOPT) on a simple convex optimization problem where the goal is to find the parameter value that minimizes a function.  The x-axis represents the optimization steps, and the y-axis represents the value of the parameter \u03b8.  The figure shows how each algorithm converges to the solution (\u03b8 = -1) over time for different values of hyperparameter \u03b22 and illustrates how ADOPT is robust to different choices of \u03b22 compared to Adam and AMSGrad.", "section": "5 Experiments"}, {"figure_path": "rzvVm0LsyK/figures/figures_6_1.jpg", "caption": "Figure 1: Performance comparison between Adam, AMSGrad and ADOPT in a simple univariate convex optimization problem. The plots show transitions of the parameter value, which should converge to the solution \u03b8 = \u22121.", "description": "This figure compares the performance of three optimization algorithms (Adam, AMSGrad, and ADOPT) on a simple convex optimization problem where the goal is to find the parameter \u03b8 that minimizes a univariate function.  The x-axis represents the number of optimization steps, and the y-axis represents the value of the parameter \u03b8.  Different lines correspond to different settings of the hyperparameter \u03b2\u2082 in each algorithm. The results illustrate that Adam's convergence is highly dependent on the choice of \u03b2\u2082, often failing to converge to the correct solution (\u03b8 = -1). AMSGrad shows improvement in convergence but is still significantly slower than ADOPT, which consistently converges to the correct solution across all \u03b2\u2082 settings, showcasing its robustness and superior performance.", "section": "5 Experiments"}, {"figure_path": "rzvVm0LsyK/figures/figures_7_1.jpg", "caption": "Figure 2: Accuracy for training data (left) and test data(right) in MNIST classification. The error bars show the 95% confidence intervals of three trials.", "description": "The figure shows the training and test accuracy curves for four different optimization algorithms (ADOPT, Adam, AMSGrad, and AdaShift) on the MNIST handwritten digit classification task.  The left panel displays training accuracy, while the right panel shows test accuracy.  The x-axis represents the training step, and the y-axis shows the accuracy percentage. Error bars, representing the 95% confidence intervals across three independent trials, are included to illustrate the variability of the results.  The figure visually demonstrates the comparative performance of the different algorithms.", "section": "5 Experiments"}, {"figure_path": "rzvVm0LsyK/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation study of algorithmic changes between Adam and ADOPT. \"DE\" and \"CO\" denote \"decorrelation\" and \"change of order\", respectively.", "description": "This figure shows the result of ablation study on how the two algorithmic changes from Adam to ADOPT affect the convergence. The two changes are (1) decorrelation between the second moment estimate and the current gradient, and (2) change of order of momentum update and normalization by the second moment estimate.  Each change is removed from ADOPT separately, and its performance is compared with the original ADOPT in a simple univariate convex optimization problem.  The result shows that both changes are essential for ADOPT to converge properly.", "section": "3 Analysis: Cause of Non-convergence of Adam and How to Fix It"}, {"figure_path": "rzvVm0LsyK/figures/figures_8_2.jpg", "caption": "Figure 2: Accuracy for training data (left) and test data(right) in MNIST classification. The error bars show the 95% confidence intervals of three trials.", "description": "This figure compares the training and test accuracy of four different optimizers (ADOPT, Adam, AMSGrad, and AdaShift) on the MNIST handwritten digit classification task.  The x-axis represents the training step, and the y-axis represents the accuracy.  Error bars show the 95% confidence intervals, indicating the variability across three separate trials.  The results show that ADOPT achieves the highest accuracy, highlighting its effectiveness in this non-convex optimization problem.", "section": "5 Experiments"}, {"figure_path": "rzvVm0LsyK/figures/figures_9_1.jpg", "caption": "Figure 5: Learning curves of GPT-2 pretraining for training set (left) and validation set (right).", "description": "This figure shows the learning curves for training and validation losses during the GPT-2 pretraining process.  Two different batch sizes (480 and 96) are used, and the performance of both Adam and ADOPT optimizers are compared. The results highlight ADOPT's stability and improved convergence, especially with the smaller batch size (96). Adam exhibits loss spikes and instability with the smaller batch size but performs comparably to ADOPT with the larger batch size.", "section": "5 Experiments"}, {"figure_path": "rzvVm0LsyK/figures/figures_29_1.jpg", "caption": "Figure 6: Performance comparison between Adam and ADOPT in reinforcement learning.", "description": "This figure displays the performance comparison between Adam and ADOPT optimizers in two deep reinforcement learning tasks: HalfCheetah-v4 and Ant-v4.  The x-axis represents the number of steps in the training process, and the y-axis shows the cumulative reward (return) achieved by the agents. Each line represents the average performance across multiple trials, with shaded areas indicating the standard deviation. The figure visually demonstrates whether ADOPT shows any improvement over Adam in reinforcement learning.", "section": "H Additional Experiments"}, {"figure_path": "rzvVm0LsyK/figures/figures_29_2.jpg", "caption": "Figure 7: Comparison of MMLU scores for LLaMA-7B finetuned via instruction following using AdamW and ADOPT.", "description": "This figure compares the performance of AdamW and ADOPT optimizers when fine-tuning a large language model (LLaMA-7B) using instruction-following data.  The MMLU (Multi-task Language Understanding) benchmark is used to evaluate the performance across various tasks.  The bar chart shows the scores for each task, allowing for a direct comparison of AdamW and ADOPT's effectiveness in this specific fine-tuning scenario.", "section": "Additional Experiments"}]