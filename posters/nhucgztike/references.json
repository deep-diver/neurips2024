{"references": [{"fullname_first_author": "Mikhail Belkin", "paper_title": "Reconciling modern machine-learning practice and the classical bias-variance trade-off", "publication_date": "2019-00-00", "reason": "This paper introduced the concept of \"double descent,\" a phenomenon challenging classical understanding of the bias-variance tradeoff in machine learning and is a key focus of the current paper."}, {"fullname_first_author": "Jerome H. Friedman", "paper_title": "Greedy function approximation: a gradient boosting machine", "publication_date": "2001-00-00", "reason": "This paper introduced gradient boosting machines, a powerful machine learning algorithm frequently compared to deep learning models in the current paper."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-00-00", "reason": "This paper introduced the Neural Tangent Kernel (NTK), a theoretical tool for understanding deep learning's generalization properties which is used as a theoretical foundation for the work done in the current paper."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "Linear mode connectivity and the lottery ticket hypothesis", "publication_date": "2020-00-00", "reason": "This paper introduced the concept of \"linear mode connectivity,\" a surprising phenomenon where averaging weights of differently trained neural networks can improve performance, which is investigated in the present paper."}, {"fullname_first_author": "Alethea Power", "paper_title": "Grokking: Generalization beyond overfitting on small algorithmic datasets", "publication_date": "2022-00-00", "reason": "This paper introduced the concept of \"grokking,\" where neural networks achieve high test accuracy only after fitting the training data perfectly, and is among the phenomena examined empirically in this paper."}]}