[{"type": "text", "text": "Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We consider the problem of computing tight privacy guarantees for the composition   \n2 of subsampled differentially private mechanisms. Recent algorithms can numeri  \n3 cally compute the privacy parameters to arbitrary precision but must be carefully   \n4 applied.   \n5 Our main contribution is to address two common points of confusion. First, some   \n6 privacy accountants assume that the privacy guarantees for the composition of a   \n7 subsampled mechanism are determined by self-composing the worst-case datasets   \n8 for the uncomposed mechanism. We show that this is not true in general. Second,   \n9 Poisson subsampling is sometimes assumed to have similar privacy guarantees   \n10 compared to sampling without replacement. We show that the privacy guarantees   \n11 may in fact differ significantly between the two sampling schemes. In particular, we   \n12 give an example of hyperparameters that result in $\\varepsilon\\approx1$ for Poisson subsampling   \n13 and $\\varepsilon>10$ for sampling without replacement. This occurs for some parameters   \n14 that could realistically be chosen for DP-SGD. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 A fundamental property of differential privacy is that the composition of multiple differentially   \n17 private mechanisms still satisfies differential privacy. This property allows us to design complicated   \n18 mechanisms with strong formal privacy guarantees such as differentially private stochastic gradient   \n19 descent (DP-SGD, [SCS13, BST14, $\\mathrm{ACG^{+}16]}$ ).   \n20 The privacy guarantees of a mechanism inevitably deteriorate with the number of compositions.   \n21 Accurately quantifying the privacy parameters under composition is highly non-trivial and is an   \n22 important area within the field of differential privacy. A common approach is to find the privacy   \n23 parameters for each part of a mechanism and apply a composition theorem [DRV10, KOV15] to find   \n24 the privacy parameters of the full mechanism. In recent years, several alternatives to the traditional   \n25 definition of differential privacy with cleaner results for composition have gained popularity (see,   \n26 e.g., [DR16, BS16, Mir17, DRS19]).   \n27 Another important concept is privacy amplification by subsampling (see, e.g., [BBG18, Ste22]). The   \n28 general idea is to improve privacy guarantees by only using a randomly sampled subset of the full   \n29 dataset as input to a mechanism. In this work we consider the problem of computing tight privacy   \n30 parameters for subsampled mechanisms under composition.   \nOne of the primary motivations for studying privacy accounting of subsampled mechanisms is DP  \n32 SGD. DP-SGD achieves privacy by clipping gradients and adding Gaussian noise to each batch.   \n33 As such, we can find the privacy parameters by analyzing the subsampled Gaussian mechanism   \n34 under composition. One of the key contributions of $[\\bar{\\mathrm{ACG}}^{+}16]$ was the moments accountant,   \n35 which gives tighter bounds for the mechanism than the generic composition theorems. Later work   \n36 improved the accountant by giving improved bounds on the R\u00e9nyi Differential Privacy guarantees   \n37 of the subsampled Gaussian mechanism under both Poisson subsampling and sampling without   \n38 replacement [MTZ19, WBK20].   \n39 Even small constant factors in an $(\\varepsilon,\\delta)$ -DP budget are important. First, from the definition, such   \n40 constant factors manifest exponentially in the privacy guarantee. Furthermore, when training a model   \n41 privately with DP-SGD, it has been observed that they can lead to significant differences in the   \n42 downstream utility, see, e.g., Figure 1 of $[\\mathrm{DBH}^{+}22]$ . Consequently, \u201csaving\u201d such a factor in the   \n43 value of $\\varepsilon$ through tighter analysis can be very valuable. While earlier approximate techniques for   \n44 privacy accounting (e.g., moments accountant of $[\\mathrm{ACG}^{+}16]$ and related methods) were lossy, a   \n45 more recent line of work focuses on exact computation of privacy loss by numerically estimating   \n46 the privacy parameters [SMM19, KJH20, KJPH21, GLW21, ZDW22]. These accountants generally   \n47 look at the \u201cworst case\u201d for a single iteration for a privacy mechanism, and then use a fast Fourier   \n48 transform (FFT) to compose the privacy loss over multiple iterations. They often rely on an implicit   \n49 assumption that the worst-case dataset for a single execution of a privacy mechanism remains the   \n50 worst case for a self-composition of the mechanism.   \n51 Most privacy accounting techniques for DP-SGD assume a version of the algorithm that employs   \n52 amplification by Poisson subsampling. That is, the batch for each iteration is formed by including each   \n53 point independently with sampling probability $\\gamma$ . Other privacy accountants consider a variant where   \n54 random batches of a fixed size are selected for each step. Note that both of these are inconsistent with   \n55 the standard method in the non-private setting, where batches are formed by randomly permuting and   \n56 then partitioning the dataset. Indeed, the latter approach is much more efficient, and highly-optimized   \n57 in most libraries. Consequently, many works in private machine learning implement a method with   \n58 the conventional shuffle-and-partition method of batch formation, but employ privacy accountants   \n59 that assume some other method of sampling batches. The hope is that small modifications of this   \n60 sort would have negligible impact on the privacy analysis, thus justifying privacy accountants for a   \n61 setting which is technically not matching. Concurrent work to this paper by $[\\mathrm{CGK}^{+}24]$ compares the   \n62 shuffle-and-partition technique with Poisson subsampling. Similar to our results they find that the   \n63 batching method can significantly impact the privacy parameters.   \n64 The central aim of our paper is to highlight and clarify some common problems with privacy   \n65 accounting techniques. Towards the goal of more faithful comparisons between private algorithms   \n66 that rely upon such accountants, we make the following contributions:   \n67 \u2022 In Sections 4 and 5, we establish that a worst-case dataset may exist for a single execution   \n68 of a privacy mechanism but may fail to exist when looking at the self-composition of the   \n69 same mechanism. Some popular privacy accountants incorrectly assume otherwise. Our   \n70 counterexample involves the subsampled Laplace mechanism, and stronger analysis is   \n71 needed to demonstrate the soundness of privacy accountants for specific mechanisms, e.g.,   \n72 the subsampled Gaussian mechanism.   \n73 \u2022 In Section 6, we show that rigorous privacy accounting is significantly affected by the method   \n74 of sampling batches, e.g., Poisson versus fixed-size. This results in sizeable differences in the   \n75 resulting privacy guarantees for settings which were previously treated as interchangeable   \n76 by prior works. Consequently, we caution against the common practice of using one method   \n77 of batch sampling and employing the privacy accountant for another.   \n78 \u2022 In Section 7, we discuss issues that arise in tight privacy accounting under the \u201csubstitution\u201d   \n79 relation for neighbouring datasets, which make this setting even more challenging than under   \n80 the traditional \u201cadd/remove\u201d relation. Once again we consider the subsampled Laplace   \n81 mechanism and show that there may be several worst-case datasets one must consider when   \n82 doing accounting, exposing another important gap in existing analyses. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "83 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "84 Differential privacy is a rigorous privacy framework introduced by [DMNS06]. Differential privacy   \n85 is a restriction on how much the output distribution of a mechanism can change between any pair of   \n86 datasets that differ only in a single individual. Such datasets are called neighboring, and we denote a   \n87 pair of neighboring datasets as $D\\sim D^{\\prime}$ . We formally define neighboring datasets below.   \n88 Definition 1 $((\\varepsilon,\\delta)$ -Differential Privacy). A randomized mechanism $\\mathcal{M}$ satisfies $(\\varepsilon,\\delta)$ -DP under   \n89 neighboring relation $\\sim i f$ and only if for all $D\\sim D^{\\prime}$ and all measurable sets of outputs $Z$ we have ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\mathcal{M}(D)\\in Z]\\le e^{\\varepsilon}\\,\\mathrm{Pr}[\\mathcal{M}(D^{\\prime})\\in Z]+\\delta.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "90 In this work, we consider problems where we want to estimate a sum for $k$ queries where each   \n91 datapoint holds a single-dimensional real value in the interval $[-1,1]$ for each query. The mechanisms   \n92 we consider apply more generally to multi-dimensional real-valued queries. Since we demonstrate   \n93 issues already present in the former more restrictive setting, these pitfalls are present in the more   \n94 general case as well. We focus on single-dimensional inputs for simplicity of presentation. Likewise,   \n95 by considering mechanisms defined on $[-1,1]$ , our privacy analysis immediately extends to any   \n96 mechanism defined on $\\mathbb{R}$ that clips to $[-1,1]$ . After the appropriate rescaling, our privacy analysis   \n97 extends to any mechanism used in practice for DP-SGD. Note that in all but one example in Section 7   \n98 the datapoints hold the same value for all $k$ queries for the datasets we consider. We abuse notation   \n99 and represent each data point as a single real value rather than a vector.   \n100 On the domain $\\textstyle[-1,1]^{*\\times k}\\;:=\\;\\bigcup_{m=0}^{\\infty}[-1,1]^{m\\times k}$ , we define the neighboring definitions of add,   \n101 remove, and substitution (replac ement). We typically want the neighboring relation to be symmetric,   \n102 which is why add and remove are typically included in a single definition. However, as noted by   \n103 previous work we need to analyze the add and remove cases separately to get tight results (see, e.g.,   \n104 [ZDW22]).   \n105 Definition 2 (Neighboring Datasets). Let $D$ and $D^{\\prime}$ be datasets. If $D^{\\prime}$ can be obtained by adding a   \n106 datapoint to $D$ , then we write $D\\sim_{A}D^{\\prime}$ . Likewise, if $D^{\\prime}$ can be obtained by removing a datapoint   \n107 from $D$ , then we write $D\\sim_{R}D^{\\prime}$ . Combining these, write $D\\sim_{A/R}D^{\\prime}$ if $D\\sim_{A}D^{\\prime}$ or $D\\sim_{R}D^{\\prime}$ .   \n108 Finally, we write $D\\sim_{S}D^{\\prime}$ if $D$ can be obtained from $D^{\\prime}$ by swapping one datapoint for another.   \n109 Note that differential privacy under add and remove implies differential privacy under substitution,   \n110 with appropriate translation of the privacy parameters.   \n111 Definition 1 can be restated in terms of the hockey-stick divergence.   \n112 Definition 3 (Hockey-stick Divergence). For any $\\alpha\\geq0$ the hockey-stick divergence between two   \n113 distributions $P$ and $Q$ is defined as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nH_{\\alpha}(P||Q):=\\mathbb{E}_{y\\sim Q}\\left[\\operatorname*{max}\\left\\{{\\frac{d P}{d Q}}(y)-\\alpha,0\\right\\}\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 where $\\frac{d P}{d Q}$ is the Radon\u2013Nikodym derivative. ", "page_idx": 2}, {"type": "text", "text": "115 Specifically, a randomized mechanism $\\mathcal{M}$ satisfies $(\\varepsilon,\\delta)$ -DP if and only if $H_{e^{\\varepsilon}}(\\mathcal{M}(D)||\\mathcal{M}(D^{\\prime}))\\leq\\delta$   \n116 for all pairs of neighboring datasets $D\\sim D^{\\prime}$ . This restated definition is the basis for the privacy   \n117 accounting tools we consider in this paper. If we know what choice of neighboring datasets $D\\sim D^{\\prime}$   \n118 maximizes the expression then we can get optimal parameters by computing $H_{e^{\\varepsilon}}\\!\\left(\\mathcal{M}(D)\\vert\\vert\\mathcal{M}(D^{\\prime})\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "119 The full range of privacy guarantees for a mechanism can be captured by the privacy curve. ", "page_idx": 2}, {"type": "text", "text": "120 Definition 4 (Privacy Curves). The privacy curve of a randomized mechanism $\\mathcal{M}$ under neighboring   \n121 relation $\\sim$ is the function $\\delta_{\\mathcal{M}}^{\\sim}:\\mathbb{R}\\bar{\\to}\\left[0,1\\right]$ given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta_{\\mathcal{M}}^{\\sim}(\\varepsilon):=\\operatorname*{min}\\{\\delta\\in[0,1]:\\mathcal{M}\\ i s\\ (\\varepsilon,\\delta)\\ \u2013D P\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "122 If there is a single pair of neighboring datasets $D\\sim D^{\\prime}$ such that $\\delta_{\\mathcal{M}}^{\\sim}(\\varepsilon)=H_{e^{\\varepsilon}}(\\mathcal{M}(D)||\\mathcal{M}(D^{\\prime}))$   \n123 for all $\\varepsilon\\ge0$ , we say that the privacy curve of $\\mathcal{M}$ under $\\sim$ is realized by the worst-case dataset pair   \n124 $(D,D^{\\prime})$ .   \n125 Unfortunately, a worst-case dataset pair does not always exist. A broader tool that is now frequently   \n126 used in the computation of privacy curves is the privacy loss distribution (PLD) formalism [DR16,   \n127 SMM19].   \n128 Definition 5 (Privacy Loss Distribution). Given a mechanism $\\mathcal{M}$ and a pair of neighboring datasets   \n129 $D\\sim D^{\\prime}$ , the privacy loss distribution of $\\mathcal{M}$ with respect to $(D,D^{\\prime})$ is ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\mathcal{M}}(D||D^{\\prime}):=\\ln(d\\mathcal{M}(D)/d\\mathcal{M}(D^{\\prime}))(y),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "130 where $y\\sim\\mathcal{M}(D)$ and $d\\mathcal{M}(D)/d\\mathcal{M}(D^{\\prime})$ means the density of $\\mathcal{M}(D)$ with respect to $\\mathcal{M}(D^{\\prime})$ . ", "page_idx": 2}, {"type": "text", "text": "131 An important caveat is that the privacy loss distribution is defined with respect to a specific pair of   \n132 datasets, whereas the privacy curve implicitly involves taking a maximum over all neighboring pairs   \n133 of datasets. Nonetheless, the PLD formalism can be used to recover the hockey-stick divergence via ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{e^{\\varepsilon}}(\\mathcal{M}(D)||\\mathcal{M}(D^{\\prime}))=\\mathbb{E}_{Y\\sim L_{\\mathcal{M}}(D||D^{\\prime})}[1-e^{\\varepsilon-Y}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 from which we can reconstruct the privacy curve as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta\\widetilde{\\mathcal{M}}^{(\\varepsilon)}=\\operatorname*{max}_{D\\sim D^{\\prime}}\\mathbb{E}_{Y\\sim L_{\\mathcal{M}}(D||D^{\\prime})}[1-e^{\\varepsilon-Y}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 Lastly, we define the two subsampling procedures we consider in this work: sampling without   \n136 replacement (WOR) and Poisson sampling. Given a dataset $D\\,=\\,(x_{1},...\\,,x_{n})$ and a set $I\\subseteq$   \n137 $\\{1,\\ldots,n\\}$ , we denote the restriction of $D$ to $I=\\{i_{1},\\ldots,i_{b}\\}$ by $D|_{I}:=(x_{i_{1}},\\ldots,x_{i_{b}})$ .   \n138 Definition 6 (Subsampling). Let $\\mathcal{M}$ take datasets of $s i z e^{1}\\;b\\geq1$ . The $\\binom{n}{b}$ -subsampled mechanism   \n139 $\\mathcal{M}_{W O R}$ is defined on datasets of size $n\\geq b$ as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}_{W O R}(D):=\\mathcal{M}(D|_{I}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "140 where $I$ is a uniform random $b$ -subset of $\\{1,\\ldots,n\\}$ . ", "page_idx": 3}, {"type": "text", "text": "141 On the other hand, given a mechanism $\\mathcal{M}$ taking datasets of any size, the $\\gamma$ -subsampled mechanism   \n142 $\\mathcal{M}_{P o i s s o n}$ is defined on datasets of arbitrary size as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}_{P o i s s o n}(D):=\\mathcal{M}(D|_{I}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 where I includes each element of $\\{1,\\ldots,|D|\\}$ independently with probability $\\gamma$ . ", "page_idx": 3}, {"type": "text", "text": "144 3 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "145 After [DR16] introduced privacy loss distributions, a number of works used the formalism to estimate   \n146 the privacy curve to arbitrary precision, beginning with [SMM19]. [KJH20, KJPH21] developed an   \n147 efficient accountant that efficiently computes the convolution of PLDs by leveraging the fast Fourier   \n148 transform. [GLW21] fine-tuned the application of FFT to speed up the accountant by several orders   \n149 of magnitude.   \n150 The most relevant related paper for our work is by [ZDW22]. They introduce the concept of a   \n151 dominating pair of distributions. Dominating pairs generalize worst-case datasets, which for some   \n152 problems can be difficult to find and may not even exist.   \n153 Definition 7 (Dominating Pair of Distributions [ZDW22]). The ordered pair $(P,Q)$ is a dominating   \n154 pair of distributions for a mechanism $\\mathcal{M}$ (under some neighboring relation $\\sim$ ) $i f$ for all $\\alpha\\geq0$ it   \n155 holds that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{D\\sim D^{\\prime}}H_{\\alpha}({\\mathcal{M}}(D)||{\\mathcal{M}}(D^{\\prime}))\\leq H_{\\alpha}(P||Q).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "156 The hockey-stick divergence of the dominating pair $P$ and $Q$ gives an upper bound on the value $\\delta$ for   \n157 any $\\varepsilon$ . Note that the distributions $P$ and $Q$ do not need to be output distributions of the mechanism.   \n158 However, if there exists a pair of neighboring datasets such that $\\mathbf{\\bar{\\boldsymbol{P}}}=\\mathcal{M}(\\mathbf{\\boldsymbol{\\mathit{D}}})$ and $Q=\\mathcal{M}(D^{\\prime})$ then   \n159 we can find tight privacy parameters by analyzing the mechanisms with inputs $D$ and $D^{\\prime}$ because   \n160 $H_{e^{\\varepsilon}}(\\mathcal{M}(D)||\\bar{\\mathcal{M}}(\\bar{D}^{\\prime}))$ is also a lower bound on $\\delta$ for any $\\varepsilon$ . We refer to such $D\\sim D^{\\prime}$ as a dominating   \n161 pair of datasets.   \n162 The definition of dominating pairs of distributions is useful for analyzing the privacy guarantees of   \n163 composed mechanisms. In this work, we focus on the special case where a mechanism consists of $k$   \n164 self-compositions. This is, for example, the case in DP-SGD, in which we run several iterations of the   \n165 subsampled Gaussian mechanism. The property we need for composition is presented in Theorem 8.   \n166 Theorem 8 (Following Theorem 10 of [ZDW22]). If $(P,Q)$ is a dominating pair for a mechanism   \n167 $\\mathcal{M}$ then $(P^{k},Q^{k})$ is a dominating pair for $k$ iterations of $\\mathcal{M}$ .   \n168 When studying differential privacy parameters in terms of the hockey-stick divergence, we usually   \n169 focus on the case of $\\alpha\\geq1$ . Recall that the hockey-stick divergence of order $\\alpha$ can be used to bound   \n170 the value of $\\delta$ for an $(\\varepsilon,\\delta)$ -DP mechanism where $\\varepsilon=\\ln(\\alpha)$ . We typically do not care about the region   \n171 of $\\alpha<1$ because it corresponds to negative values of $\\varepsilon$ . However, the definition of dominating pairs   \n172 of distributions must include these values as well. This is because outputs with negative privacy loss   \n173 are important for composition and Theorem 8 would not hold if the definition only considered $\\alpha\\geq1$ .   \n174 In Sections 5 and 7 we consider mechanisms where the distributions that bound the hockey-stick   \n175 divergence for $\\alpha\\geq1$ without composition do not bound the divergence for $\\alpha\\geq1$ under composition.   \n176 [ZDW22] studied general mechanisms in terms of dominating pairs of distributions under Poisson   \n177 subsampling and sampling without replacement. Their work gives upper bounds on the privacy   \n178 parameters based on the dominating pair of distributions of the non-subsampled mechanism. We use   \n179 some of their results which we introduce later throughout this paper. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "180 4 Dominating Pair of Datasets under Add and Remove Relations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 In this section we give pairs of neighboring datasets with provable worst-case privacy parameters   \n182 under the add and remove neighboring relations separately. We use these datasets as examples of the   \n183 pitfalls to avoid in the subsequent section, where we discuss the combined add/remove neighboring   \n184 relation.   \n185 Proposition 9. Let $\\mathcal{M}$ be either the Gaussian mechanism $\\begin{array}{r}{\\mathcal{M}(x_{1},\\ldots,x_{n}):=\\sum_{i=1}^{n}x_{i}+\\mathcal{N}(0,\\sigma^{2})}\\end{array}$   \n186 or the Laplace mechanism $\\begin{array}{r}{\\mathcal{M}(x_{1},\\ldots,x_{n}):=\\sum_{i=1}^{n}x_{i}+\\mathrm{Lap}(0,s)}\\end{array}$ .   \n187 1. The datasets $D:=(0,\\ldots,0)$ and $D^{\\prime}:=(0,\\ldots,0,1)$ form a dominating pair of datasets   \n188 for $\\mathcal{M}_{P o i s s o n}$ under the add relation and $(D^{\\prime},D)$ is a dominating pair of datasets under   \n189 the remove relation.   \n190 2. Likewise, the datasets $D:=(-1,\\ldots,-1)$ and $D^{\\prime}:=(-1,\\ldots,-1,1)$ form a dominating   \n191 pair of datasets for $\\mathcal{M}_{W O R}$ under the add relation and $(D^{\\prime},D)$ is a dominating pair of   \n192 datasets under the remove relation.   \n193 The proposition implies that the hockey-stick divergence of the mechanisms with said datasets as   \n194 input describes the privacy curves of the composed mechanisms under the add and remove relations,   \n195 respectively. We contrast this good behavior of composed and subsampled mechanisms under add   \n196 and remove separately with the Laplace mechanism, which, as we will see in Section 5, does not   \n197 behave well when composed under the combined add/remove relation. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "198 Our dominating pair of datasets can be found by reduction to one of the main results of [ZDW22]. ", "page_idx": 4}, {"type": "text", "text": "199 Theorem 10 (Theorem 11 of [ZDW22]). Let $\\mathcal{M}$ be a randomized mechanism, let $\\mathcal{M}_{P o i s s o n}$ be   \n200 the $\\gamma$ -subsampled version of the mechanism, and let $\\mathcal{M}_{W O R}$ be the $\\binom{n}{b}$ -subsampled version of the   \n201 mechanism on datasets of size n and $n-1$ with $\\gamma=b/n$ .   \n202 1. $I f(P,Q)$ dominates $\\mathcal{M}$ for add neighbors then $(P,(1-\\gamma)P+\\gamma Q)$ dominates MP oisson   \n203 for add neighbors and $((1-\\gamma)Q+\\gamma P,P)$ dominates $\\mathcal{M}_{P o i s s o n}$ for removal neighbors.   \n204 2. If $(P,Q)$ dominates $\\mathcal{M}$ for substitution neighbors then $(P,(1-\\gamma)P+\\gamma Q)$ dominates   \n205 $\\mathcal{M}_{W O R}$ for add neighbors and $((1\\mathrm{~-~}\\gamma)\\bar{P^{\\mathrm{~+~}}}\\gamma Q,P)$ dominates $\\mathcal{M}_{W O R}$ for removal   \n206 neighbors.   \n207 In Appendix A we prove that Proposition 9 holds by showing that the hockey-stick divergence between   \n208 the mechanism with the dominating pairs of datasets matches the upper bound from Theorem 10.   \n209 Crucially, Proposition 9 implies that under the add and remove relations, we must add noise with   \n210 twice the magnitude when sampling without replacement compared to Poisson subsampling! The   \n211 intuition behind this difference is that the subroutine behaves similarly to the add/remove neighboring   \n212 relation when using Poisson subsampling, whereas it resembles the substitution neighborhood when   \n213 sampling without replacement. When $D_{i}^{\\prime}$ is included in the batch another datapoint is \u2019pushed out\u2019 of   \n214 the batch under sampling without replacement. Due to this parallel one might hope that the difference   \n215 in privacy parameters between Poisson subsampling and sampling without replacement only differ   \n216 by a small constant similar to the difference between the add/remove and substitution neighboring   \n217 relations. That is indeed the case for many parameters, but as we show in Section 7 this assumption   \n218 unfortunately does not always hold. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "buSEDdP5YX/tmp/ae0340572b46ad51c441638c0d737577a28714c22871341b8164d4a2e9e0dbe8.jpg", "img_caption": ["Figure 1: The privacy curves for the subsampled Laplace mechanism under the remove and add neighboring relations respectively. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "219 5 No Worst-case Pair of Datasets under Add/Remove Relation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "220 So far, we have considered the entire privacy curve for all $\\varepsilon\\in\\mathbb R$ . This is a necessary subtlety for PLD   \n221 privacy accounting tools under composition (e.g., Theorem 8). Here we focus only on the privacy   \n222 curve for $\\varepsilon\\ge0$ . Our main result of this section is to give a minimal example of a mechanism $\\mathcal{M}$ that   \n223 admits a worst-case dataset pair under $\\sim_{A/R}$ yet $\\mathcal{M}^{\\breve{k}}$ does not admit any worst-case dataset pair for   \n224 some $k>1$ . This violates an implicit assumption made by some privacy accountants.   \n225 Proposition 11. For some mechanism $\\mathcal{M}$ , the privacy curve of the $\\binom{n}{b}$ -subsampled mechanism   \n226 $\\mathcal{M}_{W O R}$ is realized by a pair of datasets under $\\sim_{A/R}$ , yet no pair of datasets realizes the privacy   \n227 curve of $\\mathcal{M}_{W O R}^{k}$ for all $k>1$ .   \n228 A proof of this proposition for a simple mechanism can be found in Appendix B.1. However, it   \n229 is more illustrative to demonstrate the proposition informally for the Laplace mechanism $\\mathcal{M}$ . In   \n230 this case, note that the proposition can be extended to $\\mathcal{M}_{P o i s s o n}$ as well. The proposition stands in   \n231 contrast to the case of the add and remove relations discussed in Proposition 9. That is, we can find   \n223323 dnaot assuecths $D\\sim_{A}D^{\\prime}$ psauirc hr etahlaitz $\\delta_{\\mathcal{M}_{W O R}}^{\\sim_{A}}$ iivsa rceya lciuzrevde  buyn $(D,D^{\\prime})$ .nd $\\delta_{\\mathcal{M}_{W O R}}^{\\sim_{R}}$ is realized by $(D^{\\prime},D)$ , but   \n$\\sim_{A/R}$   \n234 Moreover, it is generally the case that the privacy curve of a subsampled mechanism without   \n235 composition under $\\sim\\!R$ dominates the privacy curve under $\\sim_{A}$ when $\\varepsilon\\ge0$ (see, e.g., Proposition 30   \n236 of [ZDW22] or Theorem 5 of [MTZ19]). Specifically, it follows from Proposition 30 of [ZDW22]   \n237 that in the case of the subsampled Laplace mechanism and $\\varepsilon\\ge0$ , we have that ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta_{\\mathcal{M}_{W O R}}^{\\sim_{A/R}}(\\varepsilon)=\\delta_{\\mathcal{M}_{W O R}}^{\\sim_{R}}(\\varepsilon)\\geq\\delta_{\\mathcal{M}_{W O R}}^{\\sim_{A}}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "238 Here we visualize the counter-example by plotting privacy curves for the add and remove relation in   \n239 Figure 1. Note that $\\delta_{\\mathcal{M}_{W O R}}^{\\sim_{A/R}}(\\varepsilon)=\\operatorname*{max}\\{\\bar{\\delta}_{\\mathcal{M}_{W O R}}^{\\sim_{A}}(\\bar{\\varepsilon}),\\delta_{\\mathcal{M}_{W O R}}^{\\sim_{R}}(\\varepsilon)\\}$ . Figure 1 shows several variations   \n240 of the curves $\\delta_{\\mathcal{M}_{W O R}^{k}}^{\\sim_{A}}$ and \u03b4\u223cR $\\delta_{\\mathcal{M}_{W O R}^{k}}^{\\sim_{R}}$ , which we estimated numerically by Monte Carlo simulation (as   \n241 in, e.g., $[\\mathbf{W}\\mathbf{M}\\mathbf{W}^{+}23]$ ). Appendix B.2 has the methodological details. These curves are seen to cross   \n242 in the region $\\varepsilon\\ge0$ for $k=2$ compositions.   \n243 The phenomenon is most apparent for $k=2$ . There is a clear break in the curve for the remove relation.   \n244 Under many compositions, however, it is known that both PLDs converge to a Gaussian distribution   \n245 [DRS19], which explains why this break vanishes as the number of compositions increases.   \n246 Avoiding incorrect upper bounds As shown in this section we cannot assume that the privacy   \n247 curve for the remove relation dominates the add relation for composed subsampled mechanisms under   \n248 $\\sim_{A/R}$ even though it is the case without composition. Luckily, this particular issue can be easily   \n249 resolved by computing the privacy parameters for the add and remove relation separately and taking   \n250 the maximum. This technique is already used in practice in, e.g., the Google DP library [Goo20].   \n251 We conjecture that this workaround is unnecessary for the Gaussian mechanism\u2014the natural choice   \n252 for DP-SGD. We searched a wide range of parameters and were unable to produce a counterexample.   \n253 Conjecture 12. Let $\\mathcal{M}$ be the Gaussian mechanism with any $\\sigma$ . Then for all $k>0$ , $\\gamma\\in[0,1].$ , and   \n254 $\\varepsilon\\ge0$ we have ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta_{\\mathcal{M}_{P o i s s o n}^{k}}^{\\sim_{A/R}}(\\varepsilon)=\\delta_{\\mathcal{M}_{P o i s s o n}^{k}}^{\\sim_{R}}(\\varepsilon)\\ge\\delta_{\\mathcal{M}_{P o i s s o n}^{k}}^{\\sim_{A}}(\\varepsilon).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "256 In this section we explore the difference in privacy parameters between Poisson subsampling and   \n257 sampling without replacement. We focus on the subsampled Gaussian mechanism which is the   \n258 mechanism of choice for DP-SGD. We show that for some parameters the privacy guarantees of the   \n259 mechanism differ significantly between the two sampling schemes.   \n260 There are several different techniques one might use when selecting privacy-specific hyperparameters   \n261 for DP-SGD. One approach is to fix the value of $\\delta$ and the number of iterations. Given a sampling   \n262 rate $\\gamma$ and a value for $\\varepsilon$ , we can compute the smallest value for the noise multiplier $\\sigma$ such that the   \n263 mechanism satisfies $(\\varepsilon,\\delta)$ -differential privacy. We use this approach to showcase our findings. We   \n264 fix $\\delta=10^{-6}$ and the number of iterations to 10, 000. We then vary the sampling rate between $10^{-4}$   \n265 to 1 and use the $P L D$ accountant implemented in the Opacus library $[\\mathrm{YSS}^{+}21]$ to compute $\\sigma$ .   \n266 In Figure 2 we plot the noise multiplier required to achieve $(\\varepsilon,10^{-6})$ -DP with Poisson subsampling   \n267 for $\\varepsilon\\,\\in\\,\\{1,2,5,10\\}$ . For comparison, we plot the noise multiplier that achieves $(10,10^{-6}\\bar{)}$ -DP   \n268 when sampling without replacement. Recall from Section 4 that the noise magnitude required when   \n269 sampling without replacement is exactly twice that required for Poisson subsampling. The plots are   \n270 clearly divided into two regions. For large sampling rate, the noise multiplier scales roughly linearly   \n271 in the sampling rate. However, for sufficiently low sampling rates the noise multiplier decreases   \n272 much slower. This effect has been observed previously for setting hyperparameters (see Figure 1 of   \n273 $[\\mathbf{P}\\mathbf{H}\\mathbf{K}^{+}23]$ for a similar plot).   \n274 Avoiding problematic parameters It is generally advised to select parameters that fall into the   \n275 right-hand regime of the plots in Figure 2 $[\\bar{\\mathrm{PHK}}^{+}23]$ . However, one might select parameters close to   \n276 the transition point. This can be especially problematic if the wrong privacy accountant is used. The   \n277 transition point happens when $\\sigma$ is slightly less than 1 for Poisson sampling and therefore it happens   \n278 when it is slightly less than 2 for sampling without replacement. The consequence can be seen for   \n279 the plot for sampling without replacement in Figure 2. When the sampling rates are high the noise   \n280 required roughly matches that for $\\varepsilon=5$ with Poisson subsampling. But when the sampling rate is   \n281 small we have to add more noise than is required for $\\varepsilon=1$ with Poisson subsampling. As such, if we   \n282 use a privacy accountant for Poisson subsampling and have a target of $\\varepsilon=1$ but our implementation   \n283 uses sampling without replacement the actual value of $\\varepsilon$ could be above 10! We might hope that   \n284 this increase would be offset if we allow for some slack in $\\delta$ as well. However, as seen in the table   \n285 of Figure 1 there can still be a big gap in $\\varepsilon$ between the sampling schemes even when we allow a   \n286 difference of several orders of magnitude in $\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "buSEDdP5YX/tmp/914c80d80638f50d204c08096843eb1dc44ae98249164ad696012336c13e2614.jpg", "img_caption": ["Figure 2: Plots of the smallest noise multiplier $\\sigma$ required to achieve certain privacy parameters for the subsampled Gaussian mechanism with varying sampling rates under add/remove. Each line shows a specific value of $\\varepsilon$ for either Poisson subsampling or sampling without replacement. The parameter $\\delta$ is fixed to $10^{-6}$ for all lines. ", "Table 1: The table contrasts the privacy parameter $\\varepsilon$ for the subsampled Gaussian mechanism with 10, 000 iterations, sampling rate $\\gamma=0.001$ , and noise multiplier $\\sigma=0.8$ for multiple values of $\\delta$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "buSEDdP5YX/tmp/a253e1a7acc7735b56910e5dc48e8c2d35348d723d614f86a9baaa9b95d7da04.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "287 7 Substitution Neighboring Relation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "288 In this section, we consider both sampling schemes under the substitution neighboring relation.   \n289 In their work on computing tight differential privacy guarantees, [KJH20] considered worst-case   \n290 distributions for the subsampled Gaussian mechanism under multiple sampling techniques and   \n291 neighboring relations. In the substitution case, they compute the hockey-stick divergence between   \n292 $(1\\stackrel{<}{-}\\gamma)\\mathcal{N}(\\bar{0},\\sigma^{2})+\\gamma\\mathcal{N}(-1,\\sigma^{2})$ and $(1-\\gamma)\\mathcal{N}(0,\\dot{\\sigma}^{2})+\\dot{\\gamma}\\mathcal{N}(1,\\sigma^{2})$ . These distributions correspond   \n293 to running the mechanism with neighboring datasets where all but one entry is 0. We first consider   \n294 Poisson subsampling in the proposition below and later discuss sampling without replacement.   \n295 Proposition 13. Consider the Gaussian mechanism $\\begin{array}{r}{\\mathcal{M}(x_{1},\\ldots,x_{n}):=\\sum_{i=1}^{n}x_{i}+\\mathcal{N}(0,\\sigma^{2})}\\end{array}$ and   \n296 let $\\mathcal{M}_{P o i s s o n}$ be the $\\gamma$ -subsampled mechanism. Then $D:=(0,\\ldots,0,1)$ and $\\bar{D}^{\\prime}:=(0,\\ldots,0,-1)$   \n297 form a dominating pair of datasets under the substitution neighboring relation.   \n298 Proposition 13 simply confirms that the pair of distributions considered by [KJH20] does indeed give   \n299 correct guarantees as it is a dominating pair of distributions. However, as far as we are aware, no   \n300 formal proof existed anywhere. Our proof of the proposition is in Appendix C.   \n301 In the rest of the section we focus on sampling without replacement. We start by restating another   \n302 result from [ZDW22] which we use throughout the section.   \n303 Theorem 14 (Proposition 30 of [ZDW22]). If $(P,Q)$ dominates $\\mathcal{M}$ under substitution for datasets   \n304 of size $\\gamma n$ , then under the substitution neighborhood for datasets of size $n$ , we have ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\delta(\\alpha)\\leq\\left\\{\\begin{array}{l l}{H_{\\alpha}((1-\\gamma)Q+\\gamma P||P)}&{i f\\alpha\\geq1;}\\\\ {H_{\\alpha}(P||(1-\\gamma)P+\\gamma Q)}&{i f0<\\alpha<1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "305 where $\\delta(\\alpha)$ is the largest hockey-stick divergence of order $\\alpha$ for $\\mathcal{M}_{W O R}$ on neighboring datasets. ", "page_idx": 7}, {"type": "text", "text": "306 Next, we address a mistake made in related work. We introduced the distributions considered   \n307 by [KJH20] for Poisson subsampling above and we show in Proposition 13 that it is a dominating   \n308 pair of distributions. However, [KJH20] claimed in their paper that the privacy curves are identical   \n309 for the two sampling schemes under the substitution relation which is unfortunately incorrect.   \n310 They considered datasets where all but one entry has a value of 0. This results in correct distri  \n311 butions for Poisson subsampling but for sampling without replacement, we instead consider the   \n312 datasets $D:=(-1,\\ldots,-\\bar{1},1)$ and $D^{\\prime}:=\\,(-1,\\ldots,-1,-1)$ . With these datasets the values of   \n313 $H_{\\alpha}(\\mathcal{M}_{W O R}(D)||\\mathcal{M}_{W O R}(D^{\\prime}))$ and $H_{\\alpha}(\\mathcal{M}_{W O R}(D^{\\prime})||\\mathcal{M}_{W O R}(D))$ match the cases of the upper   \n314 bound in Theorem 14 for $\\alpha\\geq1$ and $\\alpha<1$ , respectively. This can be easily verified by following the   \n315 steps of the proof of Proposition 9 for sampling without replacement.   \n316 We can use the datasets above to compute tight privacy guarantees for a single iteration. However,   \n317 composition is more complicated since neither of the two directions corresponds to a dominating   \n318 pair of distributions. One might hope that we could simply compute the hockey-stick divergence of   \n319 the self-composed distributions in both directions and use the maximum similar to the add/remove   \n320 case. However, for some mechanisms that is not sufficient because we can combine the directions   \n321 unlike with the add and remove cases. Next we give a minimal counterexample using the Laplace   \n322 mechanism to showcase this challenge.   \n323 We consider datasets of size 2 and sample batches with a single element such that $\\gamma=0.5$ . Let   \n324 $x_{1}$ and $x_{2}$ denote the two data points in $D$ and without loss of generality assume that $x_{1}\\,=\\,x_{1}^{\\prime}$   \n325 and $x_{2}\\neq x_{2}^{\\prime}$ , where $\\boldsymbol{x}_{1}^{\\prime}$ and $\\boldsymbol{x}_{2}^{\\prime}$ are the corresponding data points in $D^{\\prime}$ . We apply the subsampled   \n326 Laplace mechanism with a scale of 2 and perform 2 queries where $x_{1}$ has the value $-1$ for both   \n327 queries. Let $P:=0.5\\cdot\\mathrm{Lap}(-1,2)+0.5\\cdot\\mathrm{Lap}(1,2)$ and $Q:=\\mathrm{Lap}(-1,2)$ . That is, $P$ and $Q$ are   \n328 the distributions for running one query of $\\mathcal{M}_{W O R}(D)$ with $x_{2}$ having value 1 or $-1$ , respectively.   \n329 Then $H_{e^{\\varepsilon}}(P\\times P||Q\\times Q)$ is the hockey-stick divergence for the mechanism if $x_{2}$ has value 1 for   \n330 both queries and $\\boldsymbol{x}_{2}^{\\prime}$ has value $-1$ for both queries. Similarly, $H_{e^{\\varepsilon}}(Q\\times Q||P\\times P)$ is the divergence   \n331 when $x_{2}$ has value $-1$ for both queries and $\\boldsymbol{x}_{2}^{\\prime}$ has value 1 for both queries.   \n332 The two hockey-stick divergences above are similar to those for the remove and add neighboring   \n333 relations. However, we also have to consider $H_{e^{\\varepsilon}}(P\\times Q||P\\times Q)$ in the case of substitution. These   \n334 distributions correspond to the case when $x_{2}$ has a value of 1 for the first query and $-1$ for the   \n335 second query, and $\\ensuremath{\\boldsymbol{x}}_{2}^{\\prime}$ has a value of $-1$ for the first query and 1 for the second query. Figure 3   \n336 shows the hockey-stick divergence as a function of $\\varepsilon$ for the three pairs of neighboring datasets.   \n337 The largest divergence depends on the value of $\\varepsilon$ with all three divergences being the maximum for   \n338 some interval. This counterexample shows that we cannot upper bound the hockey-stick divergence   \n339 for the subsampled Laplace mechanism as $\\operatorname*{max}\\{H_{e^{\\varepsilon}}(P^{k}|\\hat{|Q}^{k}),H_{e^{\\varepsilon}}(Q^{k}||P^{k})\\}$ for $k\\,>\\,1$ . For $k$   \n340 compositions, we have to consider $k+1$ ways of combining $P$ and $Q$ . This significantly slows down   \n341 the accountants in contrast to the 2 cases required for add/remove. Worse still, we do not have a proof   \n342 that one of $k+1$ cases is the worst-case pair of datasets for all $\\varepsilon\\ge0$ .   \n343 In Appendix D we use an alternative technique for bounding the privacy curve under the substitution   \n344 relation based on $[\\mathrm{DGK}^{+}22]$ . We show that this accountant does not generally outperform the RDP   \n345 accountant. This demonstrates the need to strengthen the theory for sampling without replacement   \n346 under the substitution relation for the purposes of tight privacy accounting. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "buSEDdP5YX/tmp/b64e6a185d5e822814c656f270199b95cf3daf978ad1956214827506f2aa6b37.jpg", "img_caption": ["Figure 3: Hockey-stick divergence of the Laplace mechanism when sampling without replacement under $\\sim_{S}$ . The worst-case pair of datasets depends on the value of $\\varepsilon$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "347 8 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "348 We have highlighted two issues that arise in the practice of privacy accounting. ", "page_idx": 8}, {"type": "text", "text": "349 First, we have given a concrete example where the worst-case dataset (for $\\varepsilon\\geq0$ ) of a subsampled   \n350 mechanism fails to be a worst-case dataset once that mechanism is composed. Care should therefore   \n351 be taken to ensure that the privacy accountant computes privacy guarantees with respect to a true   \n352 worst-case dataset for a given choice of $\\varepsilon$ .   \n353 Secondly, we have shown that the privacy parameters for a subsampled and composed mechanism   \n354 can differ significantly for different subsampling schemes. This can be problematic if the privacy   \n355 accountant is assuming a different subsampling procedure from the one actually employed. We have   \n356 shown this in the case of Poisson sampling and sampling without replacement but the phenomenon   \n357 is likely to occur when comparing Poisson sampling to shuffling as well. Computing tight privacy   \n358 guarantees for the shuffled Gaussian mechanism remains an important open problem. It is best   \n359 practice to ensure that the implemented subsampling method matches the accounting method. When   \n360 this is not practical, the discrepancy should be disclosed.   \n361 We conclude with two recommendations for practitioners applying privacy accounting in the DP  \n362 SGD setting. We recommend disclosing the privacy accounting hyperparameters for the sake of   \n363 reproducibility (see Section 5.3.3 of $[\\mathbf{P}\\mathbf{H}\\mathbf{K}^{+}23]$ for a list of suggestions). Finally, we also recommend   \n364 that, when comparisons are made between DP-SGD mechanisms, the privacy accounting for both   \n365 should be re-run for the sake of fairness. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "366 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "$[\\mathrm{ACG}^{+}16]$ Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM Conference on Computer and Communications Security, CCS \u201916, pages 308\u2013318, New York, NY, USA, 2016. ACM. [BBG18] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via couplings and divergences. In Advances in Neural Information Processing Systems 31, NeurIPS \u201918, pages 6277\u20136287. Curran Associates, Inc., 2018. [BS16] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Proceedings of the 14th Conference on Theory of Cryptography, TCC \u201916-B, pages 635\u2013658, Berlin, Heidelberg, 2016. Springer. [BST14] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201914, pages 464\u2013473, Washington, DC, USA, 2014. IEEE Computer Society. [BW18] Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 403\u2013412. PMLR, 2018.   \n$[\\mathrm{CGK}^{+}24]$ Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, and Chiyuan Zhang. How private is dp-sgd?, 2024.   \n$[\\mathrm{DBH}^{+}22]$ Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022.   \n$[\\mathrm{DGK}^{+}22]$ Vadym Doroshenko, Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi. Connect the dots: Tighter discrete approximations of privacy loss distributions. Proc. Priv. Enhancing Technol., 2022(4):552\u2013570, 2022.   \n[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC \u201906, pages 265\u2013284, Berlin, Heidelberg, 2006. Springer. [DR16] Cynthia Dwork and Guy N. Rothblum. Concentrated differential privacy. arXiv preprint arXiv:1603.01887, 2016. [DRS19] Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian differential privacy. arXiv preprint arXiv:1905.02383, 2019. [DRV10] Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan. Boosting and differential privacy. In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science, FOCS \u201910, pages 51\u201360, Washington, DC, USA, 2010. IEEE Computer Society. [GLW21] Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. In Advances in Neural Information Processing Systems 34, NeurIPS \u201921, pages 11631\u201311642. Curran Associates, Inc., 2021. [Goo20] Google\u2019s differential privacy libraries. dp accounting library, 2020. [KJH20] Antti Koskela, Joonas J\u00e4lk\u00f6, and Antti Honkela. Computing tight differential privacy guarantees using fft. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 2560\u20132569. PMLR, 26\u201328 Aug 2020. ", "page_idx": 9}, {"type": "text", "text": "[KJPH21] Antti Koskela, Joonas J\u00e4lk\u00f6, Lukas Prediger, and Antti Honkela. Tight differential privacy for discrete-valued mechanisms and for the subsampled gaussian mechanism using FFT. In AISTATS, volume 130 of Proceedings of Machine Learning Research, pages 3358\u20133366. PMLR, 2021. [KOV15] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In Proceedings of the 32nd International Conference on Machine Learning, ICML \u201915, pages 1376\u20131385. JMLR, Inc., 2015. [Mir17] Ilya Mironov. R\u00e9nyi differential privacy. In Proceedings of the 30th IEEE Computer Security Foundations Symposium, CSF \u201917, pages 263\u2013275, Washington, DC, USA, 2017. IEEE Computer Society. [MTZ19] Ilya Mironov, Kunal Talwar, and Li Zhang. R\u00e9nyi differential privacy of the sampled gaussian mechanism. arXiv preprint arXiv:1908.10530, 2019. $[\\mathrm{PHK}^{+}23]$ ] Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Guha Thakurta. How to dp-fy ML: A practical guide to machine learning with differential privacy. J. Artif. Intell. Res., 77:1113\u20131201, 2023. [SCS13] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In Proceedings of the 2013 IEEE Global Conference on Signal and Information Processing, GlobalSIP \u201913, pages 245\u2013248, Washington, DC, USA, 2013. IEEE Computer Society. [SMM19] David M. Sommer, Sebastian Meiser, and Esfandiar Mohammadi. Privacy loss classes: The central limit theorem in differential privacy. Proc. Priv. Enhancing Technol., 2019(2):245\u2013269, 2019. [Ste22] Thomas Steinke. Composition of differential privacy & privacy amplification by subsampling. arXiv preprint arXiv:2210.00597, 2022. [War65] Stanley L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309):63\u201369, 1965. [WBK20] Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled r\u00e9nyi differential privacy and analytical moments accountant. J. Priv. Confidentiality, 10(2), 2020. $[\\mathbf{W}\\mathbf{M}\\mathbf{W}^{+}23]$ Jiachen T Wang, Saeed Mahloujifar, Tong Wu, Ruoxi Jia, and Prateek Mittal. A randomized approach for tight privacy accounting. arXiv preprint arXiv:2304.07927, 2023. $[\\mathrm{YSS}^{+}21]$ Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint arXiv:2109.12298, 2021. [ZDW22] Yuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential privacy via characteristic function. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 4782\u20134817. PMLR, 28\u201330 Mar 2022. ", "page_idx": 10}, {"type": "text", "text": "454 A Proof of Proposition 9 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "455 Without loss of generality, we show both parts for the Gaussian mechanism under the add neighboring   \n456 relation only.   \n457 We first note that any pair of neighboring datasets with maximum $\\ell_{2}$ -distance is a dominating pair of   \n458 datasets for the Gaussian mechanism [BW18]. Since the datapoints in our setting are from $[-1,1]$   \n459 this implies that $(\\mathcal{N}(0,\\sigma^{2}),\\mathcal{N}(1,\\sigma^{2}))$ is a dominating pair of distributions for $\\mathcal{M}$ under $\\sim_{A}$ and   \n460 $(\\mathcal{N}(r,\\bar{\\sigma^{2}}),\\bar{\\mathcal{N}}(r+2,\\dot{\\sigma^{2}}))$ is a dominating pair of distributions for $\\mathcal{M}$ under $\\sim_{S}$ for any $r\\in\\mathbb{R}$ . The   \n461 distance of 2 is obtained by substituting $-1$ with 1.   \n462 Now, let us prove part 1 of the proposition. To that end, let $D$ be the all zeros dataset and let $D^{\\prime}$ be $D$   \n463 with a 1 appended to the end. The sum of the subsampled dataset is 1 if the last datapoint is included   \n464 in the sample and 0 otherwise. As such, we have that ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathcal{M}_{P o i s s o n}(D^{\\prime})=(1-\\gamma)\\mathcal{N}(0,\\sigma^{2})+\\gamma\\mathcal{N}(1,\\sigma^{2})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "465 Since $(\\mathcal{N}(0,\\sigma^{2}),\\mathcal{N}(1,\\sigma^{2}))$ is a dominating pair of distributions for $\\mathcal{M}$ under $\\sim_{A}$ from Theorem 10   \n466 we have that ", "page_idx": 11}, {"type": "equation", "text": "$$\n(N(0,\\sigma^{2}),(1-\\gamma)N(0,\\sigma^{2})+\\gamma N(1,\\sigma^{2}))=(M_{P o i s s o n}(D),\\mathcal{M}_{P o i s s o n}(D^{\\prime}))\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "467 dominates $\\mathcal{M}_{P o i s s o n}$ under $\\sim_{A}$ . ", "page_idx": 11}, {"type": "text", "text": "468 As for part 2, let $\\gamma:=b/n$ for convenience, let $D$ be the all $-1$ dataset, let $D^{\\prime}$ be $D$ with a single $-1$   \n469 substituted for a 1. We can describe $\\mathcal{M}_{W O R}(D^{\\prime})$ by considering the two cases where the 1 is either   \n470 excluded or included in the batch of size $b$ ", "page_idx": 11}, {"type": "equation", "text": "$$\nM_{W O R}(D^{\\prime})=(1-\\gamma)M(\\underbrace{-1,\\ldots,-1}_{b},-1,-1)+\\gamma M(\\underbrace{-1,\\ldots,-1}_{b},1)=(1-\\gamma){\\mathcal N}(-b,\\sigma^{2})+\\gamma N(-b+2,-1)=0,\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "471 Since $(\\mathcal{N}(-b,\\sigma^{2}),\\mathcal{N}(-b+2,\\sigma^{2}))$ is a dominating pair of distributions for $\\mathcal{M}$ under $\\sim_{S}$ from   \n472 Theorem 10 we have that ", "page_idx": 11}, {"type": "equation", "text": "$$\n(\\mathcal{N}(-b,\\sigma^{2}),(1-\\gamma)\\mathcal{N}(-b,\\sigma^{2})+\\gamma\\mathcal{N}(-b+2,\\sigma^{2}))=(\\mathcal{M}_{W O R}(D),\\mathcal{M}_{W O R}(D^{\\prime}))\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "473 dominates $\\mathcal{M}_{W O R}$ under $\\sim_{A}$ . ", "page_idx": 11}, {"type": "text", "text": "474 The proof for the remove direction is symmetric and the proof for the Laplace mechanism follows   \n475 from replacing the normal distribution with the Laplace distribution. ", "page_idx": 11}, {"type": "text", "text": "476 B Details for Section 5 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "477 B.1 Proof of Proposition 11 for Randomized Response ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "478 Here we show that Proposition 11 holds using a simple mechanism. The mechanism is similar to   \n479 randomized response [War65] which is used in differential privacy to privately release bits. The   \n480 mechanism takes a dataset as input and randomly outputs a single bit. The output is weighted towards   \n481 0 if all entries of the dataset are 0 and towards 1 otherwise. Here we use this mechanism for the proof   \n482 because the calculations and presentation are particularly clean and simple since there are only two   \n483 outputs. A similar proof can be used to verify the accuracy of the estimated plots for the Laplace   \n484 mechanism presented in Section 5 by calculating the exact hockey-stick divergence at, e.g., $\\varepsilon=0.25$   \n485 and $\\varepsilon=1.5$ . ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathcal{M}(D)=\\left\\{\\begin{array}{l l}{b}&{\\mathrm{with~probability~\\frac{3}{4}~}}\\\\ {1-b}&{\\mathrm{with~probability~\\frac{1}{4}~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "486 where $b\\in\\{0,1\\}$ is 0 if all entries in $D$ are 0 and 1 otherwise. ", "page_idx": 11}, {"type": "text", "text": "487 We use the dataset $D$ that consists of all zeroes and $D^{\\prime}$ is obtained from $D$ by adding a single 1.   \n488 We will present the proof using $\\mathcal{M}_{P o i s s o n}$ , but it is the same for $\\mathcal{M}_{W O R}$ since the only effect on   \n489 the output distribution is whether or not the 1 is sampled in a batch. We use a sampling probability   \n490 of $\\gamma=1/2$ . Since the output distribution of $\\mathcal{M}$ is symmetric this means that the probability for   \n491 $\\mathcal{M}_{P o i s s o n}(D^{\\prime})$ to output either bit is $1/2\\cdot3/4+1/2\\cdot1/4=1/2$ . The counterexample occurs when   \n492 running the mechanism for 2 iterations. There are 4 possible outcomes of the two iterations. The   \n493 probability of any of these outcomes for $\\mathcal{M}_{P o i s s o n}(\\bar{D^{\\prime}})$ is $1/2\\cdot1/2=1/4$ . For $\\mathcal{M}_{P o i s s o n}(D)$ the   \n494 probability we can find the output distribution by considering each distinct outcome ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{^T_{r}}[M_{P o i s s o n}(D)\\times M_{P o i s s o n}(D)=(0,0)]=\\mathrm{^Tr}[M_{P o i s s o n}(D)=0]\\cdot\\mathrm{^Tr}[M_{P o i s s o n}(D)=0]=3\\mathrm{^Tr}[M_{P o i s s o n}(D)=0]=3\\mathrm{^Tr}[M_{P o i s s o n}(D)=0],}\\\\ &{\\mathrm{^T_{r}}[M_{P o i s s o n}(D)\\times M_{P o i s s o n}(D)=(0,1)]=\\mathrm{^Tr}[M_{P o i s s o n}(D)=0]\\cdot\\mathrm{^Tr}[M_{P o i s s o n}(D)=1]=3\\mathrm{^Tr}[M_{P o i s s o n}(D)=0],}\\\\ &{\\mathrm{^Tr}[M_{P o i s s o n}(D)\\times M_{P o i s s o n}(D)=(1,0)]=\\mathrm{^Tr}[M_{P o i s s o n}(D)=1]\\cdot\\mathrm{^Tr}[M_{P o i s s o n}(D)=0]=1\\mathrm{^Tr}[M_{P o i s s o n}(D)=0],}\\\\ &{\\mathrm{^Tr}[M_{P o i s s o n}(D)\\times M_{P o i s s o n}(D)=(1,1)]=\\mathrm{^Tr}[M_{P o i s s o n}(D)=1]\\cdot\\mathrm{^Tr}[M_{P o i s s o n}(D)=1]=1\\mathrm{^Tr}[M_{P o i s s o n}(D)=1]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "495 Now, we find the hockey-stick divergence in both directions for $\\alpha=4/3$ and $\\alpha\\stackrel{}{=}2$ . We denote   \n496 the two distributions for running the mechanism as $P\\,=\\,\\mathcal{M}_{P o i s s o n}(D)\\,\\times\\,\\mathcal{M}_{P o i s s o n}(D)$ and   \n497 $Q=\\mathcal{M}_{P o i s s o n}(D^{\\prime})\\times\\mathcal{M}_{P o i s s o n}\\bar{(D^{\\prime})}$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r}&{H_{4/3}(P||Q)=\\operatorname*{Pr}[P=(0,0)]-4/3\\cdot\\operatorname*{Pr}[Q=(0,0)]}&{=9/16-4/3}\\\\ &{T_{4/3}(Q||P)=\\operatorname*{Pr}[Q\\in\\{(0,1),(1,0),(1,1)\\}]-4/3\\cdot\\operatorname*{Pr}[P\\in\\{(0,1),(1,0),(1,1)\\}]}&{\\quad=3/4-4/3}\\\\ &{H_{2}(P||Q)=\\operatorname*{Pr}[P=(0,0)]-2\\cdot\\operatorname*{Pr}[Q=(0,0)]}&{\\quad=9/16-4/3}\\\\ &{H_{2}(Q||P)=\\operatorname*{Pr}[Q=(1,1)]-2\\cdot\\operatorname*{Pr}[P=(1,1)]}&{\\quad=1/4-1}\\end{array}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "498 As such, we have that $H_{4/3}(P||Q)>H_{4/3}(Q||P)$ and $H_{2}(P||Q)<H_{2}(Q||P)$ ", "page_idx": 12}, {"type": "text", "text": "499 B.2 Details of Monte Carlo Simulation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "500 To produce Figure 1, we leverage the PLD framework and apply Monte Carlo simulation. ", "page_idx": 12}, {"type": "text", "text": "501 By Proposition 9 and Theorem 8, the privacy curve of the composed and subsampled Laplace   \n502 mechanism under add (remove) is given by $\\dot{H_{e^{\\varepsilon}}}(\\mathcal{M}_{P o i s s o n}(D)^{k}||\\dot{\\mathcal{M}}_{P o i s s o n}(D^{\\prime})^{k})$ (vice-versa for   \n503 remove) where ", "page_idx": 12}, {"type": "equation", "text": "$$\nD:=(0,\\ldots,0)\\quad D^{\\prime}:=(0,\\ldots,0,1).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "504 On the other hand, a standard result (e.g. Theorem 3.5 of [GLW21]) asserts that the PLD of a   \n505 composed mechanism is obtained by self-convolving the PLD of the uncomposed mechanism,   \n506 namely ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{e^{\\varepsilon}}(\\mathcal{M}_{P o i s s o n}(D)^{k}||\\mathcal{M}_{P o i s s o n}(D^{\\prime})^{k})=\\mathbb{E}_{Y\\sim L_{\\mathcal{M}_{P o i s s o n}^{k}}(D||D^{\\prime})}[1-e^{\\varepsilon-Y}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad=\\mathbb{E}_{Y\\sim L_{M_{P o i s s o n}}(D||D^{\\prime})^{\\oplus k}}[1-e^{\\varepsilon-Y}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "507 We estimate this expectation via sampling. We know the densities of $\\mathcal{M}_{P o i s s o n}(D)=\\mathcal{N}(0,\\sigma^{2})$ and   \n508 $\\mathcal{M}_{P o i s s o n}(D^{\\prime})=\\bar{(1-\\gamma)}\\mathcal{N}(0,\\sigma^{2})\\;\\overset{\\cdot}{+}\\gamma\\bar{\\mathcal{N}}(1,\\sigma^{2})$ , so we can quickly sample $L_{\\mathcal{M}_{P o i s s o n}}(D||D^{\\prime})$ . By   \n509 drawing $k$ samples and summing them, we can sample $L_{\\mathcal{M}_{P o i s s o n}}(D||D^{\\prime})^{\\oplus k}$ as well. Therefore, we   \n510 can draw $Y_{i}\\sim L_{\\mathcal{M}_{P o i s s o n}}(D||D^{\\prime})^{k}$ for $1\\leq i\\leq N$ , then compute the Monte Carlo estimate ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{i=1}^{N}(1-e^{\\varepsilon-Y_{i}}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "511 As for the error, the quantity inside the expectation is bounded in $[0,1]$ , so we can apply H\u00f6ffding as   \n512 well as the union bound. In this case, ", "page_idx": 12}, {"type": "equation", "text": "$$\nN=\\left\\lceil\\frac{\\ln(2|E|/\\beta)}{2\\alpha^{2}}\\right\\rceil\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "513 samples will suffice to ensure that the Monte Carlo estimate of $H_{e^{\\varepsilon}}(\\mathcal{M}_{P o i s s o n}(D)||\\mathcal{M}_{P o i s s o n}(D^{\\prime}))$   \n514 is accurate within $\\alpha$ , with probability $1-\\beta$ , for all $\\varepsilon\\in E$ simultaneously.   \n515 For Figure 1, we chose $\\alpha=0.001$ and $\\beta=0.01$ and considered $|E|=40$ values of $\\varepsilon$ , which required   \n516 $N=3,342,306$ samples. This value of $\\alpha$ is small enough relative to the plot that our conclusion   \n517 holds with probability $99\\%$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "518 C Proof of Proposition 13 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "519 The proof relies mainly on the following data-processing inequality, which can also be seen as closure   \n520 of privacy under post-processing.   \n521 Lemma 15. Let $P$ and $Q$ be any distributions on $\\mathcal{X}$ and let Proc : $\\mathcal X\\rightarrow\\mathcal Y$ be a randomized   \n522 procedure. Denote by Proc $P$ the distribution of $\\operatorname{Proc}(X)$ for $X\\sim P$ . Then, for any $\\alpha\\geq0$ , ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\nH_{\\alpha}(\\mathrm{Proc}\\,P||\\,\\mathrm{Proc}\\,Q)\\leq H_{\\alpha}(P||Q).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "523 Proof. For any event $E\\subseteq\\mathcal{V}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Proc}\\,P)(E)-\\alpha(\\mathrm{Proc}\\,Q)(E)=\\mathbb{E}_{\\mathrm{Proc}}[\\mathbb{P}_{X\\sim P}(\\mathrm{Proc}(X)\\in E)]-\\alpha\\mathbb{E}_{\\mathrm{Proc}}[\\mathbb{P}_{X\\sim Q}(\\mathrm{Proc}(X)\\in E)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\mathrm{Proc}}[P(\\mathrm{Proc}^{-1}(E))]-\\alpha\\mathbb{E}_{\\mathrm{Proc}}[Q(\\mathrm{Proc}^{-1}(E)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\mathrm{Proc}}[P(\\mathrm{Proc}^{-1}(E))-\\alpha Q(\\mathrm{Proc}^{-1}(E)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\leq\\mathbb{E}_{\\mathrm{Proc}}[H_{\\alpha}(P||Q)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=H_{\\alpha}(P||Q)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "524 and the result holds since ", "page_idx": 13}, {"type": "equation", "text": "$$\nH_{\\alpha}(\\mathrm{Proc}\\,P||\\,\\mathrm{Proc}\\,Q)=\\operatorname*{sup}_{E\\subseteq y}{(\\mathrm{Proc}\\,P)}(E)-\\alpha(\\mathrm{Proc}\\,Q)(E).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "525 ", "page_idx": 13}, {"type": "text", "text": "526 We now prove the proposition. Our main goal is to argue that $D\\,:=\\,(0,\\ldots,0,1)$ and $D^{\\prime}:=$   \n527 $(0,\\ldots,0,-1)$ form a dominating pair of datasets for $\\mathcal{M}_{P o i s s o n}$ . To that end, consider any   \n528 $\\sim_{S}$ -neighbors that differ, without loss of generality, in the last entry, say $(x,a)$ and $(x,a^{\\prime})$ .   \n529 We leverage postprocessing to show that $(M_{P o i s s o n}(x,a),M_{P o i s s o n}(x,a^{\\prime}))$ is dominated by   \n530 $(M_{P o i s s o n}(\\mathbf{0},\\stackrel{\\cdot}{a}),\\stackrel{\\cdot}{M}_{P o i s s o n}(\\mathbf{0},a^{\\prime}))$ . Indeed, consider ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Proc}(y):=y+\\sum_{i=1}^{|\\hat{x}|}\\hat{x}_{i}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "531 where $\\hat{x}$ is randomly drawn from $x$ by Poisson $(\\gamma)$ -subsampling. Now, sampling $\\mathcal{M}_{P o i s s o n}(\\mathbf{0},a)$ is   \n532 equivalent to drawing $\\hat{a}$ from the singleton dataset $(a)$ via Poisson $(\\gamma)$ and returning a sample from   \n533 $\\textstyle{\\mathcal{N}}(\\sum_{i=1}^{|{\\hat{a}}|}{\\hat{a}}_{i},\\sigma^{2})$ . Since the normal distribution satisfies ${\\mathcal{N}}(a,\\sigma^{2})+b={\\mathcal{N}}(a+b,\\sigma^{2})$ , sampling   \n534 $\\operatorname{Proc}(\\bar{\\mathcal{M}}_{P o i s s o n}(\\mathbf{0},a))$ is equivalent to sampling ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{N}\\left(\\sum_{i=1}^{|\\hat{x}|}\\hat{x}_{i}+\\sum_{i=1}^{|\\hat{a}|}\\hat{a}_{i},\\sigma^{2}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "535 where $\\hat{x}$ is Poisson $(\\gamma)$ -subsampled from $x$ and $\\hat{a}$ is Poisson $(\\gamma)$ -subsampled from $(a)$ . But,   \n536 by independence, $({\\hat{x}},{\\hat{a}})$ is a $\\mathrm{Poisson}(\\gamma)$ -subsample drawn from $(x,a)$ , so, in conclusion,   \n537 $\\begin{array}{r l r}{\\mathrm{Proc}(\\mathcal{M}_{P o i s s o n}(\\mathbf{0},a))}&{{}=}&{\\mathcal{M}_{P o i s s o n}(x,a)}\\end{array}$ . By an analogous argument, we have that   \n538 Pr $\\operatorname{oc}(\\mathcal{M}_{P o i s s o n}(\\mathbf{0},a^{\\prime}))=\\mathcal{M}_{P o i s s o n}(x,a^{\\prime})$ and hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\alpha}(\\mathcal{M}_{P o i s s o n}(x,a)||\\mathcal{M}_{P o i s s o n}(x,a^{\\prime}))=H_{\\alpha}(\\mathrm{Proc}(\\mathcal{M}_{P o i s s o n}(\\mathbf{0},a))||\\,\\mathrm{Proc}(\\mathcal{M}_{P o i s s o n}(\\mathbf{0},a^{\\prime})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq H_{\\alpha}(\\mathcal{M}_{P o i s s o n}(\\mathbf{0},a)||\\mathcal{M}_{P o i s s o n}(\\mathbf{0},a^{\\prime}))\\,\\mathrm{~(Lemma~15)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq H_{\\alpha}(\\mathcal{M}_{P o i s s o n}(\\mathbf{0},1)||\\mathcal{M}_{P o i s s o n}(\\mathbf{0},-1)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "539 D Constructing a Dominating Pair of Distributions for the Gaussian 540 Mechanism ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "541 In this section we consider the problem of computing privacy curves for the Gaussian mechanism   \n542 under $\\sim\\!S$ when sampling without replacement. As shown in Section 7 computing tight parameters is   \n543 challenging in this setting because we do not know which datasets result in the largest hockey-stick   \n544 divergence. However, we can still compute an upper bound on the privacy curve using a dominating   \n545 pair of distributions.   \n546 We modified the implementation of the algorithm introduced by $[\\mathrm{DGK}^{+}22]$ in the Google DP library   \n547 to construct the PLDs (Privacy Loss Distribution object). The algorithm constructs an approximation   \n548 of the PLD from the hockey-stick divergence between the pair of distributions at a range of values   \n549 for $\\varepsilon$ . From Theorem 14 we know that the direction of the pair of distributions yielding the largest   \n550 hockey-stick divergence for the mechanism of a single iteration differs for $\\alpha$ below and above 1. We   \n551 construct a new PLD by combining the two directions at $\\alpha=1$ or $\\varepsilon=0$ .   \n552 See the left-side plot of Figure 4 for a visualization of how our construction uses the point-wise   \n553 maximum of the hockey-stick divergence for a single iteration. This construction represents a   \n554 dominating pair of distributions and as such it is sufficient to find a dominating pair of distributions   \nfor the composed mechanism using self-composition by Theorem 8.   \n556 The right-side plot of Figure 4 shows the privacy curve obtained from self-composing the PLD for   \n557 the dominating pair of distributions with parameters $\\sigma=4$ , $\\gamma=0.05$ , and 1000 iterations. The blue   \n558 line is the privacy curve under $\\sim\\!R$ and also serves as a lower bound for the true privacy curve. Note   \n559 that the orange line would also be the privacy curve achieved by this technique under the add/remove   \n560 relation if we did not consider the add and remove relations separately.   \n561 The gap between the upper and lower bound motivates future work for understanding the worst-case   \ndatasets. Similar to the add/remove case we conjecture that the subsampled Gaussian mechanism   \nbehaves well under composision. Specifically, we conjecture that the privacy curve of the composed   \n564 subsampled Gaussian mechanism under $\\sim_{S}$ matches the curve under $\\sim_{R}$ for $\\varepsilon\\geq0$ . It seems likely   \n565 that this is the case if Conjecture 12 holds. However, if Conjecture 12 does not hold the above   \n566 statement also does not hold. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "buSEDdP5YX/tmp/3b8d72498d2cdc33178a0fb42cc6200e46e616e609d74f65a5027eed7a13826b.jpg", "img_caption": ["Figure 4: Hockey-stick divergence for the Gaussian mechanism under substitution when sampling without replacement using a dominating pair of distributions. The dominating pair of distributions is constructed using a point-wise maximum of the privacy curve for a single iteration as seen in the left plot. The right plot compares the privacy curve from self-composing the dominating pair of distributions with a lower bound obtained from self-composing the PLD that corresponds to the blue line in the left plot. The dotted line for the RDP accountant is used for reference of scale. The difference between the blue and the dotted line corresponds to the difference between using the PLD and RDP accountants for Poisson subsampling under add/remove. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "567 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "74 Guidelines:   \n75 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n76 made in the paper.   \n77 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n78 contributions made in the paper and important assumptions and limitations. A No or   \n79 NA answer to this question will not be perceived well by the reviewers.   \n80 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n581 much the results can be expected to generalize to other settings.   \n82 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n83 are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "584 2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The main limitation of our work is expressed in Conjecture 12. ", "page_idx": 15}, {"type": "text", "text": "88 Guidelines:   \n89 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n90 the paper has limitations, but those are not discussed in the paper.   \n91 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n92 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n93 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n94 model well-specification, asymptotic approximations only holding locally). The authors   \n95 should reflect on how these assumptions might be violated in practice and what the   \n96 implications would be.   \n97 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n98 only tested on a few datasets or with a few runs. In general, empirical results often   \n99 depend on implicit assumptions, which should be articulated.   \n0 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n01 For example, a facial recognition algorithm may perform poorly when image resolution   \n2 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n03 used reliably to provide closed captions for online lectures because it fails to handle   \n4 technical jargon.   \n05 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n06 and how they scale with dataset size.   \n07 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n08 address problems of privacy and fairness.   \n09 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n10 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n11 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n12 judgment and recognize that individual actions in favor of transparency play an impor  \n13 tant role in developing norms that preserve the integrity of the community. Reviewers   \n14 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "15 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "616 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n617 a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Each theoretical result is indicated as a proposition (theorems indicate prior work). A proof for each result can be found in the appropriate appendix section (references given in main body). ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "633 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "634 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n35 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n36 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Justification: Details for Monte Carlo simulation results (Figures 1 and 3) are in the appendix. Other experimental results can be obtained by straightforward modification of publicly available privacy accounting software. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "673 5. Open access to data and code   \n674 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n675 tions to faithfully reproduce the main experimental results, as described in supplemental   \n676 material?   \n677 Answer: [No]   \n678 Justification: See previous justification. Instructions to reproduce Monte Carlo simulation   \n679 results are included in the appendix. Other results rely on open-source code.   \n680 Guidelines:   \n681 \u2022 The answer NA means that paper does not include experiments requiring code.   \n682 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n683 public/guides/CodeSubmissionPolicy) for more details.   \n684 \u2022 While we encourage the release of code and data, we understand that this might not be   \n685 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n686 including code, unless this is central to the contribution (e.g., for a new open-source   \n687 benchmark).   \n688 \u2022 The instructions should contain the exact command and environment needed to run to   \n689 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n690 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n691 \u2022 The authors should provide instructions on data access and preparation, including how   \n692 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n693 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n694 proposed method and baselines. If only a subset of experiments are reproducible, they   \n695 should state which ones are omitted from the script and why.   \n696 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n697 versions (if applicable).   \n698 \u2022 Providing as much information as possible in supplemental material (appended to the   \n699 paper) is recommended, but including URLs to data and code is permitted.   \n700 6. Experimental Setting/Details   \n701 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n702 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n703 results?   \n704 Answer: [Yes]   \n705 Justification: Simulation results rely on a choice of sample size, which is explained in the   \n706 appendix.   \n707 Guidelines:   \n708 \u2022 The answer NA means that the paper does not include experiments.   \n709 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n710 that is necessary to appreciate the results and make sense of them.   \n711 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n712 material.   \n713 7. Experiment Statistical Significance   \n714 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n715 information about the statistical significance of the experiments?   \n716 Answer: [Yes]   \n717 Justification: An analysis of sample size and the associated error is included in the appendix.   \n718 The error is very small compared to the plots due to the high sample size, so we did not   \n719 explicitly include them in simulation plots.   \n720 Guidelines:   \n721 \u2022 The answer NA means that the paper does not include experiments.   \n722 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n723 dence intervals, or statistical significance tests, at least for the experiments that support   \n724 the main claims of the paper.   \n725 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n726 example, train/test split, initialization, random drawing of some parameter, or overall   \n727 run with given experimental conditions).   \n728 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n729 call to a library function, bootstrap, etc.)   \n730 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n731 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n732 of the mean.   \n733 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n734 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n735 of Normality of errors is not verified.   \n736 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n737 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n738 error rates).   \n739 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n740 they were calculated and reference the corresponding figures or tables in the text.   \n741 8. Experiments Compute Resources   \n742 Question: For each experiment, does the paper provide sufficient information on the com  \n743 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n744 the experiments?   \n745 Answer: [NA]   \n746 Justification: Experiments required minimal compute resources, so we do not report details.   \n747 Guidelines:   \n748 \u2022 The answer NA means that the paper does not include experiments.   \n749 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n750 or cloud provider, including relevant memory and storage.   \n751 \u2022 The paper should provide the amount of compute required for each of the individual   \n752 experimental runs as well as estimate the total compute.   \n753 \u2022 The paper should disclose whether the full research project required more compute   \n754 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n755 didn\u2019t make it into the paper).   \n756 9. Code Of Ethics   \n757 Question: Does the research conducted in the paper conform, in every respect, with the   \n758 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n759 Answer: [Yes]   \n760 Justification: We reviewed the guidelines and found no violations in our work.   \n761 Guidelines:   \n762 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n763 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n764 deviation from the Code of Ethics.   \n765 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n766 eration due to laws or regulations in their jurisdiction).   \n767 10. Broader Impacts   \n768 Question: Does the paper discuss both potential positive societal impacts and negative   \n769 societal impacts of the work performed?   \n770 Answer: [No]   \n771 Justification: The aim of the work is to bring attention among practitioners and theoreticians   \n772 to the limitations of privacy accountants. There is no foreseeable path to negative broad   \n773 societal impact. On the other hand improving privacy accountants may lead to wider   \n774 deployment of private machine learning, which can be expected to have a positive societal   \n775 impact. We briefly discuss this outcome in the introduction in order to motivate our work.   \n776 Guidelines:   \n777 \u2022 The answer NA means that there is no societal impact of the work performed.   \n778 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n779 impact or why the paper does not address societal impact.   \n780 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n781 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n782 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n783 groups), privacy considerations, and security considerations.   \n784 \u2022 The conference expects that many papers will be foundational research and not tied   \n785 to particular applications, let alone deployments. However, if there is a direct path to   \n786 any negative applications, the authors should point it out. For example, it is legitimate   \n787 to point out that an improvement in the quality of generative models could be used to   \n788 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n789 that a generic algorithm for optimizing neural networks could enable people to train   \n790 models that generate Deepfakes faster.   \n791 \u2022 The authors should consider possible harms that could arise when the technology is   \n792 being used as intended and functioning correctly, harms that could arise when the   \n793 technology is being used as intended but gives incorrect results, and harms following   \n794 from (intentional or unintentional) misuse of the technology.   \n795 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n796 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n797 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n798 feedback over time, improving the efficiency and accessibility of ML).   \n799 11. Safeguards   \n800 Question: Does the paper describe safeguards that have been put in place for responsible   \n801 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n802 image generators, or scraped datasets)?   \n803 Answer: [NA]   \n804 Justification: N/A   \n805 Guidelines:   \n806 \u2022 The answer NA means that the paper poses no such risks.   \n807 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n808 necessary safeguards to allow for controlled use of the model, for example by requiring   \n809 that users adhere to usage guidelines or restrictions to access the model or implementing   \n810 safety filters.   \n811 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n812 should describe how they avoided releasing unsafe images.   \n813 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n814 not require this, but we encourage authors to take this into account and make a best   \n815 faith effort.   \n816 12. Licenses for existing assets   \n817 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n818 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n819 properly respected?   \n820 Answer: [Yes]   \n821 Justification: Credit is given as needed to open-source software repositories.   \n822 Guidelines:   \n823 \u2022 The answer NA means that the paper does not use existing assets.   \n824 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n825 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n826 URL.   \n827 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "28 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n29 service of that source should be provided.   \n30 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n31 package should be provided. For popular datasets, paperswithcode.com/datasets   \n32 has curated licenses for some datasets. Their licensing guide can help determine the   \n33 license of a dataset.   \n34 \u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. \u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "835   \n836   \n837   \n838 13. New Assets   \n839 Question: Are new assets introduced in the paper well documented and is the documentation   \n840 provided alongside the assets?   \n841 Answer: [NA]   \n842 Justification: N/A   \n843 Guidelines:   \n844 \u2022 The answer NA means that the paper does not release new assets.   \n845 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n846 submissions via structured templates. This includes details about training, license,   \n847 limitations, etc.   \n848 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n849 asset is used.   \n850 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n851 create an anonymized URL or include an anonymized zip file.   \n852 14. Crowdsourcing and Research with Human Subjects   \n853 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n854 include the full text of instructions given to participants and screenshots, if applicable, as   \n855 well as details about compensation (if any)?   \n856 Answer: [NA]   \n857 Justification: N/A   \n858 Guidelines:   \n859 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n860 human subjects.   \n861 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n862 tion of the paper involves human subjects, then as much detail as possible should be   \n863 included in the main paper.   \n864 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n865 or other labor should be paid at least the minimum wage in the country of the data   \n866 collector. ", "page_idx": 20}, {"type": "text", "text": "7 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "869 Question: Does the paper describe potential risks incurred by study participants, whether   \n70 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n71 approvals (or an equivalent approval/review based on the requirements of your country or   \n872 institution) were obtained?   \n873 Answer: [NA]   \n874 Justification: N/A   \n875 Guidelines:   \n876 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n877 human subjects. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]