{"references": [{"fullname_first_author": "Song Han", "paper_title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "publication_date": "2016-00-00", "reason": "This paper is foundational to the field of model compression for TinyML, proposing techniques crucial for fitting large models onto resource-constrained devices."}, {"fullname_first_author": "Ji Lin", "paper_title": "Runtime neural pruning", "publication_date": "2017-00-00", "reason": "This paper presents a runtime neural pruning method, a dynamic model compression technique particularly relevant to optimizing inference performance on TinyML hardware."}, {"fullname_first_author": "Han Cai", "paper_title": "Once-for-all: Train one network and specialize it for efficient deployment", "publication_date": "2020-00-00", "reason": "This paper introduces a method to train a single neural network that can be efficiently specialized for various resource-constrained settings, directly addressing the challenges of TinyML."}, {"fullname_first_author": "Benoit Jacob", "paper_title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference", "publication_date": "2018-00-00", "reason": "This paper focuses on quantization techniques for neural networks, vital for the efficient execution of TinyML models on low-precision hardware."}, {"fullname_first_author": "Edgar Liberis", "paper_title": "Unas: Constrained neural architecture search for microcontrollers", "publication_date": "2021-00-00", "reason": "This paper addresses neural architecture search specifically tailored for microcontrollers, important for developing efficient neural network architectures for TinyML."}]}