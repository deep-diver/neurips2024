[{"figure_path": "SlDx451MjC/figures/figures_0_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows the results of three different encoders applied to the same set of input images. The first column shows the input images and the sixth column shows the result obtained by the dual encoder. The results of Encoder 1 and Encoder 2 are presented in between. The dual encoder combines the results of both Encoder 1 and Encoder 2, leveraging the strengths of each to produce the final reconstruction. Encoder 1 focuses on reconstructing high-fidelity facial images from the input, while Encoder 2 generates more realistic representations for invisible views. The figure demonstrates the effectiveness of the dual-encoder approach in achieving both high-fidelity reconstruction and realistic generation from different camera viewpoints.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_2_1.jpg", "caption": "Figure 2: Overall architecture of PanoHead.", "description": "This figure illustrates the architecture of PanoHead, a generative model used in the paper.  It takes a random vector z and camera conditioning \u03c0mapping as input.  A mapping network (M) transforms z into a latent code w (14x512). This code is then fed into the generator (G) which produces tri-grid triplanes. Finally, these triplanes are rendered using a volumetric neural renderer (R) with a rendering camera pose \u03c0render to generate multiple 2D images representing different views of the 3D head.", "section": "3.1 Overview of PanoHead"}, {"figure_path": "SlDx451MjC/figures/figures_3_1.jpg", "caption": "Figure 3: Our training methodology for the triplane discriminator involves generating real samples by sampling latent vectors Z+ and producing in-domain triplanes using PanoHead. Fake samples are generated from encoded images. Despite the effectiveness of adversarial loss in enhancing reconstructions, challenges may persist in achieving high fidelity to the input due to the origin of real samples from the generator G. To address this, we propose an occlusion-aware discriminator D, trained exclusively with features from occluded pixels. This ensures that visible regions, such as frontal views TR, have reduced influence during the training of D.", "description": "The figure illustrates the training process of an occlusion-aware triplane discriminator. Real samples are generated using PanoHead with sampled latent vectors Z+, while fake samples come from encoded images.  To focus training on occluded regions and improve fidelity, the discriminator is trained only on features from these areas, minimizing the impact of visible regions like frontal views (TR).", "section": "3.3 Occlusion-aware triplane discriminator"}, {"figure_path": "SlDx451MjC/figures/figures_4_1.jpg", "caption": "Figure 4: The inference pipeline with dual encoders for full 3D head reconstruction. Given a face portrait with pose \u03c0R, we can perform 360-degree rendering from any given pose \u03c0novel.", "description": "This figure illustrates the inference pipeline of the proposed dual-encoder system for 3D head reconstruction.  It takes a single input face image (I<sup>\u03c0R</sup>) as input. Two encoders (E<sub>1</sub> and E<sub>2</sub>) process the image separately. Encoder 1 specializes in reconstructing the visible parts of the face from the input view (\u03c0<sub>R</sub>), while Encoder 2 focuses on generating realistic features for the occluded parts of the head.  Both encoders utilize the generator (G) to produce triplane representations. These triplanes are then rendered (R) for the given view (\u03c0<sub>R</sub>) and a novel view (\u03c0<sub>novel</sub>). The occlusion mask (O<sub>\u03c0R</sub>) is used to selectively combine the outputs of the two encoders; the visible regions from Encoder 1 and the occluded regions from Encoder 2 are merged to generate a complete triplane. This final triplane can be rendered to produce images from any viewpoints, enabling a full 360-degree reconstruction.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_5_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows a comparison of the results obtained using three different encoders: Encoder 1, Encoder 2, and the Dual encoder.  The first and sixth columns display the input images.  The remaining columns show the 3D head reconstruction results from different viewpoints (frontal, profile, and back views) generated by each of the three encoders.  This visualization highlights the strengths and weaknesses of each encoder, demonstrating the improved fidelity and realism achieved by the dual encoder approach, which combines the outputs of Encoder 1 and Encoder 2 to achieve a more complete and accurate 3D model.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_7_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure presents a visual comparison of the results obtained using three different methods for 3D head reconstruction: Encoder 1, Encoder 2, and the proposed Dual Encoder method.  The first and last columns show the input images. Each row represents a different subject, and each subsequent column shows the 3D reconstruction of that subject from different viewpoints using the three different methods. The results demonstrate the effectiveness of the Dual Encoder approach in producing more realistic and complete 3D reconstructions compared to the individual Encoder methods.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_7_2.jpg", "caption": "Figure 7: Qualitative results of ablation on occlusion-aware discriminator D.", "description": "This figure shows the qualitative results of an ablation study on the occlusion-aware discriminator. It compares the results of using no discriminator, a discriminator trained on the image domain, a discriminator trained on the triplane domain without occlusion awareness, and a discriminator trained on the triplane domain with occlusion awareness. The results demonstrate the improved image quality and realism achieved by using the occlusion-aware discriminator. Specifically, it showcases better generation of hair and overall detail when utilizing this method.", "section": "3.3 Occlusion-aware triplane discriminator"}, {"figure_path": "SlDx451MjC/figures/figures_8_1.jpg", "caption": "Figure 6: Comparisons of ours and competing methods.", "description": "This figure presents a qualitative comparison of the proposed method against several state-of-the-art techniques for 3D face reconstruction from a single image.  It shows results from various viewpoints for multiple subjects, highlighting the differences in reconstruction fidelity, realism, and ability to handle novel views. Each row displays results for a different method (W+opt, PTI, pSp, e4e, TriplaneNetv2, GOAE, and the proposed method), with the input image in the first column, followed by reconstructions from several viewpoints.", "section": "5 Results"}, {"figure_path": "SlDx451MjC/figures/figures_8_2.jpg", "caption": "Figure 9: Inputs (first), reconstructions (second), and 360\u00b0 mesh renders (rest) of our method.", "description": "This figure showcases the results of the proposed dual encoder method for 3D head reconstruction.  The first column displays the input images. The second column shows the 2D reconstruction generated by the model. The remaining columns present the 360\u00b0 mesh renderings generated from different viewpoints.  This demonstrates the model's ability to reconstruct a high-fidelity 3D head model from a single image and to generate realistic views from various angles.", "section": "Results"}, {"figure_path": "SlDx451MjC/figures/figures_9_1.jpg", "caption": "Figure 1: From a single input image (first column), our framework reconstructs 3D representation by inverting images into PanoHead's latent space, which can be viewed in a 360-degree perspective.", "description": "This figure shows the results of the proposed 3D head reconstruction method. For each subject, the first image is the input image; the remaining images are the generated 3D head model views from different angles, demonstrating the method's ability to reconstruct a 3D representation from a single 2D image.", "section": "Introduction"}, {"figure_path": "SlDx451MjC/figures/figures_9_2.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows the results of three different encoders on several input images.  The first column shows the input images. The second, third, and fourth columns display the results from Encoder 1, Encoder 2, and the dual encoder approach respectively. The dual encoder combines outputs from Encoder 1 (which focuses on high-fidelity reconstruction of the input view) and Encoder 2 (which focuses on generating realistic invisible views) to produce a more comprehensive 3D reconstruction.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_15_1.jpg", "caption": "Figure 12: Inputs with diverse ethnicities and challenging views (first), reconstructions (second), and 360\u00b0 renders (rest).", "description": "This figure showcases the results of the proposed dual encoder method on a diverse set of input images. The first column displays the input images, featuring individuals of various ethnic backgrounds and with poses that present challenges for 3D reconstruction (e.g., extreme angles, occlusions). The second column shows the reconstructed images by the model. The remaining columns show the 360-degree views generated by the model from the reconstructed representation. This provides a comprehensive view of the generated 3D head model from various angles, demonstrating the ability of the model to accurately reconstruct the facial features and details despite varying poses and ethnicities.", "section": "A.3 Additional qualitative results"}, {"figure_path": "SlDx451MjC/figures/figures_16_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure showcases a comparison of 3D head reconstruction results from three different methods: Encoder 1, Encoder 2, and the proposed Dual Encoder.  Each row represents a single input image (shown in the first column), with the subsequent columns displaying the 3D reconstruction results from various viewpoints (angles) generated by each method. The results highlight the strengths and weaknesses of each approach in terms of reconstruction fidelity, realism of generated views, and handling of occluded areas.  The Dual Encoder aims to combine the strengths of Encoder 1 (high-fidelity reconstruction of the input view) and Encoder 2 (realistic generation of unseen views).", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_16_2.jpg", "caption": "Figure 6: Comparisons of ours and competing methods.", "description": "This figure presents a qualitative comparison of the proposed method's performance against several state-of-the-art techniques for 3D head reconstruction.  The results show the reconstructed images from different viewpoints for multiple subjects.  Each row represents a different method (W+ opt., PTI, pSp, e4e, TriplaneNetv2, GOAE, and the proposed method), with the input image on the far left.  The subsequent images in each row demonstrate the reconstruction quality from various angles. This allows for a visual assessment of each approach's ability to reconstruct high-fidelity 3D head models from a single input image.", "section": "5 Results"}, {"figure_path": "SlDx451MjC/figures/figures_17_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows a comparison of the results obtained using three different methods for 3D head reconstruction from a single image: Encoder 1, Encoder 2, and the proposed Dual encoder.  The first and last columns display the input images.  Each row shows the reconstruction of the same input image using each of the three methods, across various viewpoints (different camera poses) generated from the trained model. The results illustrate that the Dual encoder approach combines the strengths of both Encoder 1 (high-fidelity reconstruction of the input view) and Encoder 2 (realistic generation of invisible views) to provide a superior overall 3D reconstruction.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_17_2.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows a comparison of the results obtained using three different methods: Encoder 1, Encoder 2, and the proposed Dual Encoder method.  The first and sixth columns display the input images. The remaining columns illustrate the generated images from different viewpoints for each method. This visual comparison highlights the strengths and weaknesses of each method, demonstrating how the dual encoder combines the best aspects of the individual encoders to produce higher-fidelity and more realistic 3D head reconstructions from various angles.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_18_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows a comparison of the results obtained using three different methods for 3D head reconstruction from a single image: Encoder 1, Encoder 2, and the proposed Dual Encoder.  The first and last columns display the input images. The other columns show the reconstructed views from various angles using each method. This allows for a visual assessment of the strengths and weaknesses of each approach in terms of reconstruction fidelity and realism across different viewpoints.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_18_2.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows a comparison of the results obtained using three different methods: Encoder 1, Encoder 2, and the proposed Dual Encoder.  The first and last columns display the input images.  Each row represents a different subject, showcasing the 360-degree head reconstruction from various viewpoints.  The results highlight the strengths and weaknesses of each approach; Encoder 1 excels in high-fidelity frontal views but struggles with other viewpoints, Encoder 2 generates more realistic but less accurate images, and the Dual Encoder combines their strengths for superior results.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_19_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows a comparison of the results obtained using three different methods: Encoder 1, Encoder 2, and the proposed dual encoder approach.  The first and sixth columns display the input images. Subsequent columns showcase the 360-degree renderings generated by each method from multiple viewpoints. This visualization allows for a qualitative assessment of the performance of each approach in terms of image fidelity and realism across various viewing angles.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_19_2.jpg", "caption": "Figure 6: Comparisons of ours and competing methods.", "description": "This figure presents a comparison of the proposed method's performance against several competing methods for 3D head reconstruction.  The results are visualized from multiple viewpoints (front, various side, and back views) for each method applied to the same set of input images. This allows a qualitative assessment of reconstruction accuracy, realism, and consistency across viewpoints.", "section": "5 Results"}, {"figure_path": "SlDx451MjC/figures/figures_20_1.jpg", "caption": "Figure 21: Qualitative results of ablation on occlusion-aware discriminator D.", "description": "This figure presents a qualitative ablation study demonstrating the impact of the occlusion-aware discriminator (D) on the overall quality of the 3D head reconstruction. The top row shows results with no discriminator, the second row with a discriminator trained on the entire image domain, the third row with a discriminator trained only on visible regions, and the bottom row with the proposed occlusion-aware discriminator. Each row shows the input image and several views of the reconstructed 3D head, illustrating how the different discriminator training strategies affect both the fidelity of the frontal view and the realism of the novel views.", "section": "3.3 Occlusion-aware triplane discriminator"}, {"figure_path": "SlDx451MjC/figures/figures_20_2.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows the visual comparison of 3D head reconstruction results using three different methods: Encoder 1, Encoder 2, and the proposed dual encoder approach.  The first and last columns display the input images.  The remaining columns demonstrate the 3D reconstruction from different viewpoints for each method.  Encoder 1 focuses on high-fidelity reconstruction of the input image, but results are less realistic from other views. Encoder 2 generates more realistic results for unseen views, but with some loss of fidelity to the input.  The dual encoder combines the strengths of both, aiming for both high fidelity to the input and realistic rendering across multiple views.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_20_3.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure shows the 3D head reconstruction results of three different methods: Encoder 1, Encoder 2, and the Dual Encoder.  Each row represents a different reconstruction technique. The first column shows the input image.  The remaining columns depict the results from various viewpoints obtained by each method. The dual encoder combines the results of Encoder 1 and Encoder 2 to achieve high-fidelity and realistic results from both visible and invisible views.", "section": "3.4 Dual encoder pipeline"}, {"figure_path": "SlDx451MjC/figures/figures_21_1.jpg", "caption": "Figure 5: Visual results of Encoder 1, Encoder 2, and Dual encoders for the given input images in the first and sixth columns.", "description": "This figure showcases a comparison of the results obtained using three different methods: Encoder 1, Encoder 2, and the Dual encoder approach.  Each row represents the reconstruction of a head from a single input image (shown in the first and sixth columns).  Encoder 1 focuses on high-fidelity reconstruction of the input view but may struggle with other viewpoints. Encoder 2 aims for generating realistic views from unseen perspectives but may not achieve the same level of detail in the main view.  The Dual encoder approach combines the strengths of both methods, aiming for both high fidelity and realistic rendering from various angles.", "section": "3.4 Dual encoder pipeline"}]