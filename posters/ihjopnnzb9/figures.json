[{"figure_path": "IHjoPnNZb9/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration for Segmenter and MaskFormer. a) Segmenter. b) MaskFormer. c) Transformer block adopted by Segmenter. We omit the details of the Transformer decoder adopted by MaskFormer, which refines image embeddings and mask embeddings via self-attention respectively before the cross-attention operations.", "description": "This figure compares two state-of-the-art Transformer-based semantic segmentation methods: Segmenter and MaskFormer.  It highlights the architectural differences in their decoder designs.  Segmenter uses a transformer decoder that processes image embeddings and class embeddings through cross-attention, self-attention, and finally a dot product to generate segmentation masks. MaskFormer employs a similar strategy but adds a pixel decoder and handles mask embeddings differently.  Subfigure (c) provides a detailed view of the transformer block within the Segmenter architecture.", "section": "1 Introduction"}, {"figure_path": "IHjoPnNZb9/figures/figures_3_1.jpg", "caption": "Figure 2: Image Segmentation via PCA and DEPICT. Given an image, we segment it via PCA and our DEPICT. We perform PCA on its representations Z<sub>0</sub> and Z<sub>L1</sub>, respectively, where the first 10 principal directions are used as cluster centroids. We find that PCA can serve as an effective method for image segmentation especially on the refined features, like Z<sub>L1</sub>. We also observe that performing PCA on Z<sub>0</sub> is more likely to lead to an over-segmentation, which indicates that its principal subspace is not ideal.", "description": "This figure compares image segmentation results using Principal Component Analysis (PCA) and the proposed DEPICT method.  Two example images are shown, each with segmentation results from PCA applied to both the initial (Z<sub>0</sub>) and refined (Z<sub>L1</sub>) image embeddings.  The results illustrate how PCA, particularly when applied to refined embeddings (Z<sub>L1</sub>), can effectively segment images, while applying PCA to the raw embeddings (Z<sub>0</sub>) leads to over-segmentation. The figure highlights that DEPICT improves upon PCA by creating an ideal principal subspace. Each column represents a different image with corresponding initial and refined embeddings and segmentation maps.", "section": "3.2 Bridging the Transformer Decoders and PCA"}, {"figure_path": "IHjoPnNZb9/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration for DEPICT. Given an image for semantic segmentation, we represent it as Zo by the ViT backbone. Segmenting it by performing PCA on Zo, we find that S of Zo is not ideal. We thus adopt the MSSA operator to refine the image embeddings, iteratively constructing an ideal S. Performing PCA again on ZL\u2081, we find that the segmentation results are improved. Then, we adopt the MSCA operator to find a low-rank approximation of ZL\u2081 that lies in S as classifiers. For example, we use the dogs and cats on the right to represent image embeddings of two different classes in the feature space. Initially, the projections of dogs and cats onto S are not well linearly separable. DEPICT, however, constructs an ideal S and effectively classify them.", "description": "This figure illustrates the DEPICT model's architecture and workflow.  It begins with a ViT backbone processing the input image to produce image embeddings (Zo).  PCA is initially performed on Zo, revealing that the principal subspace (S) is not ideal for effective segmentation. The model then iteratively refines these embeddings using the Multi-head Subspace Self-Attention (MSSA) operator, creating an improved principal subspace.  After this refinement, a second PCA on the refined embeddings (ZL1) is performed.  Finally, the model utilizes the Multi-head Subspace Cross-Attention (MSCA) operator to project the refined embeddings onto a low-rank approximation within the ideal subspace, resulting in compact class representations that facilitate effective classification and segmentation.  The example with dogs and cats visually demonstrates how DEPICT transforms initially overlapping class representations into linearly separable ones.", "section": "3.3 Constructing an Ideal Principal Subspace via Self-Attention"}, {"figure_path": "IHjoPnNZb9/figures/figures_7_1.jpg", "caption": "Figure 2: Image Segmentation via PCA and DEPICT. Given an image, we segment it via PCA and our DEPICT. We perform PCA on its representations Zo and ZL\u2081, respectively, where the first 10 principal directions are used as cluster centroids. We find that PCA can serve as an effective method for image segmentation especially on the refined features, like ZL1. We also observe that performing PCA on Zo is more likely to lead to an over-segmentation, which indicates that its principal subspace is not ideal.", "description": "This figure compares image segmentation results using Principal Component Analysis (PCA) and the proposed DEPICT method.  PCA is applied to both the initial image embeddings (Zo) and the refined embeddings (ZL1) produced by the DEPICT model. The results show that PCA, when used on the refined embeddings (ZL1), is effective for image segmentation. However, applying PCA directly to the initial embeddings (Zo) leads to over-segmentation. This suggests that the DEPICT's refinement process constructs an improved principal subspace.", "section": "3. Bridging the Transformer Decoders and PCA"}, {"figure_path": "IHjoPnNZb9/figures/figures_8_1.jpg", "caption": "Figure 4: Investigating orthogonality in DEPICT. Left: P<sup>T</sup>P; Right: Q<sup>T</sup>Q. All variants are based on ViT-L. Since that the MHSA operator contains three parameter matrices, unlike MSSA which has only one, we visualize the matrix responsible for transforming queries. Notably, all the Q's are normalized, whereas P is not.", "description": "This figure visualizes the orthogonality of the parameter matrices P and Q in the DEPICT model, comparing it to the Segmenter model. The left side shows the inner product of matrix P with its transpose (P<sup>T</sup>P), representing the self-attention operator's parameter matrix, while the right side shows the inner product of matrix Q with its transpose (Q<sup>T</sup>Q), representing the cross-attention operator's parameter matrix. The visualization highlights the near-orthogonality of Q in DEPICT, indicating its desirable properties, in contrast to Segmenter.", "section": "4.2 Desirable Properties of DEPICT"}, {"figure_path": "IHjoPnNZb9/figures/figures_8_2.jpg", "caption": "Figure 4: Investigating orthogonality in DEPICT. Left: PTP; Right: Q\u2122Q. All variants are based on ViT-L. Since that the MHSA operator contains three parameter matrices, unlike MSSA which has only one, we visualize the matrix responsible for transforming queries. Notably, all the Q's are normalized, whereas P is not.", "description": "This figure shows the inner product of the parameter matrices P and Q for different variants of DEPICT, demonstrating their orthogonality.  The left panel displays the inner product of P (matrix responsible for transforming keys), while the right panel illustrates the inner product of Q (class embeddings). The results indicate that both P and Q tend towards orthogonality, especially Q, which is normalized, showcasing the desirable property derived in the model's theoretical underpinnings.", "section": "4.2 Desirable Properties of DEPICT"}, {"figure_path": "IHjoPnNZb9/figures/figures_8_3.jpg", "caption": "Figure 6: Measuring the coding rate across layers. Given the l-th layer, we use the parameter matrix of its first head, say P\u2081\u02e1, and measure the mean projected coding rate of image embeddings onto the subspace spanned by P\u2081\u02e1. Each polyline reflects the changes of the projected coding rate onto a particular subspace and the layer index of the subspace is indicated by a vertical dash line in the same color. Left: R(P\u2081\u02e1Z), across layers. Right: R(P\u2081\u02e1Z)/R(Z), across layers.", "description": "This figure visualizes the coding rate across different layers of the DEPICT model. It shows two plots: one showing the projected coding rate onto subspaces (R(P\u2081\u02e1Z)) across layers, and another showing the ratio of the projected coding rate to the overall coding rate (R(P\u2081\u02e1Z)/R(Z)). Each line represents a different subspace, and the vertical dashed lines indicate the layer index of each subspace.  The figure aims to demonstrate the relationship between the coding rate and the layer depth within the model, providing insight into how the model learns and compresses information.", "section": "4.2 Desirable Properties of DEPICT"}, {"figure_path": "IHjoPnNZb9/figures/figures_9_1.jpg", "caption": "Figure 4: Investigating orthogonality in DEPICT. Left: P<sup>T</sup>P; Right: Q<sup>T</sup>Q. All variants are based on ViT-L. Since that the MHSA operator contains three parameter matrices, unlike MSSA which has only one, we visualize the matrix responsible for transforming queries. Notably, all the Q's are normalized, whereas P is not.", "description": "This figure shows the inner product of the parameter matrices P and Q for different variants of DEPICT (DEPICT-SA and DEPICT-CA) using the ViT-L backbone.  The left side displays the inner product of matrix P (P<sup>T</sup>P), and the right side shows the inner product of matrix Q (Q<sup>T</sup>Q).  The goal is to demonstrate the orthogonality of the matrices.  Since the MHSA (Multi-Head Self-Attention) operator uses three parameter matrices, while MSSA (Multi-head Subspace Self-Attention) uses only one, the visualization focuses on the matrix responsible for transforming the queries to show the orthogonality more clearly. The results indicate that the matrices in DEPICT are closer to being orthogonal than those in the Segmenter model.", "section": "4.2 Desirable Properties of DEPICT"}, {"figure_path": "IHjoPnNZb9/figures/figures_14_1.jpg", "caption": "Figure 7: Measuring R(Z) and R(Q) across layers. All are based on ViT-L with an input resolution of 640\u00d7640.", "description": "This figure shows the coding rate R(Z) and R(Q) across different layers of the DEPICT-SA and DEPICT-CA models.  R(Z) represents the coding rate of the image embeddings, while R(Q) represents the coding rate of the low-rank approximation of the image embeddings.  The plots demonstrate the relationship between these coding rates throughout the network's layers, which helps support the authors' claims about the efficiency of their approach and its ability to achieve low-rank representation.", "section": "Additional Experiments"}, {"figure_path": "IHjoPnNZb9/figures/figures_15_1.jpg", "caption": "Figure 2: Image Segmentation via PCA and DEPICT. Given an image, we segment it via PCA and our DEPICT. We perform PCA on its representations Zo and ZL\u2081, respectively, where the first 10 principal directions are used as cluster centroids. We find that PCA can serve as an effective method for image segmentation especially on the refined features, like ZL1. We also observe that performing PCA on Zo is more likely to lead to an over-segmentation, which indicates that its principal subspace is not ideal.", "description": "This figure compares image segmentation results obtained using Principal Component Analysis (PCA) and the proposed DEPICT method.  Two sets of image embeddings are used as input to the PCA: the initial embeddings (Zo) and the embeddings after refinement through the self-attention mechanism (ZL\u2081). The figure shows that PCA applied to the refined embeddings (ZL\u2081) produces better segmentation results than when applied to the initial embeddings (Zo), highlighting the effectiveness of the DEPICT's self-attention refinement step in creating a more suitable principal subspace for segmentation.", "section": "3.2 Bridging the Transformer Decoders and PCA"}, {"figure_path": "IHjoPnNZb9/figures/figures_16_1.jpg", "caption": "Figure 4: Investigating orthogonality in DEPICT. Left: P<sup>T</sup>P; Right: Q<sup>T</sup>Q. All variants are based on ViT-L. Since that the MHSA operator contains three parameter matrices, unlike MSSA which has only one, we visualize the matrix responsible for transforming queries. Notably, all the Q's are normalized, whereas P is not.", "description": "This figure visualizes the orthogonality of the learned parameter matrices P and Q in the DEPICT model for semantic segmentation.  The left panel shows the inner product of the matrix P transposed with itself (P<sup>T</sup>P), while the right panel shows the same for the matrix Q (Q<sup>T</sup>Q).  The visualizations aim to demonstrate that the matrices are approximately orthogonal, a key aspect of the theoretical derivation of DEPICT based on PCA.  The difference in normalization between P and Q is also highlighted in the caption.", "section": "4.2 Desirable Properties of DEPICT"}, {"figure_path": "IHjoPnNZb9/figures/figures_16_2.jpg", "caption": "Figure 4: Investigating orthogonality in DEPICT. Left: P<sup>T</sup>P; Right: Q<sup>T</sup>Q. All variants are based on ViT-L. Since that the MHSA operator contains three parameter matrices, unlike MSSA which has only one, we visualize the matrix responsible for transforming queries. Notably, all the Q's are normalized, whereas P is not.", "description": "This figure demonstrates the orthogonality of the learned parameter matrices P and Q in the DEPICT model, particularly focusing on the matrices responsible for query transformations.  The left panel shows the inner product of matrix P with its transpose (P<sup>T</sup>P), while the right panel shows the same for matrix Q (Q<sup>T</sup>Q).  The results suggest that the matrices are close to being orthogonal, a desirable property indicated by the paper's theoretical analysis. The difference in the normalization of P and Q is also highlighted.", "section": "4.2 Desirable Properties of DEPICT"}]