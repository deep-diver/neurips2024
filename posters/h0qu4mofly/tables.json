[{"figure_path": "H0qu4moFly/tables/tables_2_1.jpg", "caption": "Table 1: Our results for contrastive learning", "description": "This table summarizes the upper and lower bounds on the embedding dimension for contrastive learning in different lp spaces (l2, l\u221e, lp for integer p \u2265 1). It shows the results for three settings: l2, l2 with t negatives, and l2 with t-ordering. The upper bounds are derived from Theorems 1, 2, and 3 in the paper, while the lower bounds are from Theorem 43.  The table highlights the dependence of the embedding dimension on the number of samples (m) and the number of negative examples (t).", "section": "Contrastive Learning"}, {"figure_path": "H0qu4moFly/tables/tables_2_2.jpg", "caption": "Table 2: Our results for k-NN", "description": "This table summarizes the upper and lower bounds on the embedding dimension d required to preserve k-nearest neighbor (k-NN) information in different lp-spaces.  It shows results for two scenarios: when the exact k-NN must be preserved, and when only the ordering of the k-NN needs to be preserved. The upper bounds represent the sufficient dimension proven in the paper, while the lower bounds represent the necessary dimension. The table demonstrates that the required dimension depends on the type of lp-space used and the strength of the k-NN preservation requirement.", "section": "1.1 Our Results and Techniques"}, {"figure_path": "H0qu4moFly/tables/tables_7_1.jpg", "caption": "Table 1: Our results for contrastive learning", "description": "This table summarizes the upper and lower bounds on the embedding dimension for contrastive learning in different settings. The settings include the use of l2, l\u221e, and lp spaces (where p is an integer greater than or equal to 1), with variations in the number of negative samples and whether the ordering of the negative samples is considered.", "section": "1.1 Our Results and Techniques"}, {"figure_path": "H0qu4moFly/tables/tables_16_1.jpg", "caption": "Table 3: Embedding dimension based on construction from Section 2. For each pair of n and m, we show the minimum and the maximum dimensions obtained over 10 runs (we show a single number when the minimum and the maximum are equal).", "description": "This table shows the embedding dimensions obtained using the construction from Section 2 of the paper.  The minimum and maximum dimensions are shown for various dataset sizes (n) and numbers of contrastive samples (m), giving insights into how the required embedding dimension scales with these factors.", "section": "A Additional experiments"}, {"figure_path": "H0qu4moFly/tables/tables_17_1.jpg", "caption": "Table 4: Training loss for preserving k-NNs for various values of n and k.", "description": "This table presents the training loss for preserving k-NNs (k-Nearest Neighbors) for different values of n (number of data points) and k (number of nearest neighbors considered).  The loss measures how well the k-NN ordering is preserved in the learned embeddings. Lower values indicate better preservation of the k-NN structure.  The table shows that the training loss increases with both n and k, indicating that preserving k-NN structure becomes more challenging as the number of data points and the number of neighbors to consider increase. ", "section": "3.1 Preserving the Ordering of the k-NN"}]