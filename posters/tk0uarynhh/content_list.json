[{"type": "text", "text": "Dynamic Conditional Optimal Transport through Simulation-Free Flows ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gavin Kerrigan   \nDepartment of Computer Science   \nUniversity of California, Irvine gavin.k@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Giosue Migliorini Department of Statistics University of California, Irvine gmiglior@uci.edu ", "page_idx": 0}, {"type": "text", "text": "Padhraic Smyth   \nDepartment of Computer Science   \nUniversity of California, Irvine smyth@ics.uci.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the geometry of conditional optimal transport (COT) and prove a dynamic formulation which generalizes the Benamou-Brenier Theorem. Equipped with these tools, we propose a simulation-free flow-based method for conditional generative modeling. Our method couples an arbitrary source distribution to a specified target distribution through a triangular COT plan, and a conditional generative model is obtained by approximating the geodesic path of measures induced by this COT plan. Our theory and methods are applicable in infinite-dimensional settings, making them well suited for a wide class of Bayesian inverse problems. Empirically, we demonstrate that our method is competitive on several challenging conditional generation tasks, including an infinite-dimensional inverse problem. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many fundamental tasks in machine learning and statistics may be posed as modeling a conditional distribution $\\nu(u\\mid y)$ where obtaining an analytical representation of $\\nu(u\\mid y)$ is impractical. While sampling-based approaches such as Markov Chain Monte Carlo (MCMC) methods are useful, they suffer from several limitations. First, MCMC requires numerous likelihood evaluations, rendering it prohibitively expensive in scientific and engineering applications where the likelihood is determined by an expensive numerical simulator. Second, MCMC must be run anew for every observation $y$ , which is impractical in applications such as Bayesian inverse problems [Dashti and Stuart, 2013] and generative modeling [Mirza and Osindero, 2014]. These limitations motivate the need for a likelihood-free [Cranmer et al., 2020] and amortized [Amos et al., 2023] approach. While methods like ABC [Beaumont, 2010] and variational inference [Blei et al., 2017] partially address these challenges, they are difficult to scale to high dimensions or have limited flexibility. ", "page_idx": 0}, {"type": "text", "text": "Recently, generative models such as normalizing flows [Papamakarios et al., 2019, 2021], GANs [Ramesh et al., 2022], and diffusion models [Sharrock et al., 2022] have shown promise in amortized and likelihood-free inference. These models may be viewed in the framework of measure transport [Baptista et al., 2020], where samples $u\\sim\\eta(u)$ from a tractable source distribution are transformed by a mapping $T(y,u)$ such that the transformed samples are approximately distributed as $\\nu(u\\mid y)$ . One way to achieve this is through triangular mappings [Baptista et al., 2020, Spantini et al., 2022], where a joint source distribution $\\eta(y,u)$ is transformed by a mapping of the form $T:(y,u)\\mapsto$ $(T_{Y}(y),T_{U}(y,u))$ . Under suitable assumptions, if $T$ transforms the source $\\eta(y,u)$ into the target $\\nu(y,u)$ , then $T_{U}(y,-)$ couples the conditionals $\\eta(u\\mid y)$ and $\\nu(u\\mid y)$ . ", "page_idx": 0}, {"type": "text", "text": "Typically, such a map $T$ is not unique [Wang et al., 2023], and a natural idea is thus to regularize the transport and search for an admissible mapping that is in some sense optimal. In other words, learning a conditional sampler may be phrased as finding a conditional optimal transport (COT) map. While there exists some work on learning COT maps, these approaches often rely on a difficult adversarial optimization problem [Baptista et al., 2020, Hosseini et al., 2023, Bunne et al., 2022, Ray et al., 2022] or simulating from the model during training [Baptista et al., 2023, Wang et al., 2023]. In this work, we propose a conditional generative model for likelihood-free inference based on a dynamic formulation of conditional optimal transport. Specifically, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We develop a general theoretical framework for dynamic conditional optimal transport in separable Hilbert spaces. Our framework is applicable in infinite-dimensional spaces, enabling applications in function space Bayesian inference. In Section 4, we study the conditional Wasserstein space $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ and show that this space admits constant speed geodesics between any two measures. In Section 5, we characterize the absolutely continuous curves of measures in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ via the continuity equation and triangular vector fields. As a consequence, we obtain conditional generalizations of the McCann interpolants [McCann, 1997] and the Benamou-Brenier Theorem [Benamou and Brenier, 2000]. ", "page_idx": 1}, {"type": "text", "text": "2. In Section 6, we propose COT flow matching (COT-FM), a simulation-free flow-based model for conditional generation. This model directly leverages our theoretical framework, where we learn to model a path of measures interpolating between an arbitrary source and target distribution via a geodesic in the conditional Wasserstein space. ", "page_idx": 1}, {"type": "text", "text": "3. In Section 7, we demonstrate our method on several challenging conditional generation tasks. We apply our method to two Bayesian inverse problems \u2013 one arising from the Lotka-Volterra dynamical system, and an infinite-dimensional problem arising from the Darcy Flow PDE. Our method shows competitive performance against recent COT methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Conditional Optimal Transport. Conditional Optimal Transport (COT) remains relatively underexplored in both machine learning and related fields. Recent approaches learn static COT maps via input convex networks [Bunne et al., 2022, Wang et al., 2023] or normalizing flows [Wang et al., 2023]. In addition, there have been a number of heuristic approaches to conditional simulation through W-GANs [Sajjadi et al., 2017, Adler and \u00d6ktem, 2018, Kim et al., 2022, 2023], for which Chemseddine et al. [2023] provide a rigorous basis. Closely related to our work are those which employ triangular plans [Carlier et al., 2016, Trigila and Tabak, 2016], which have been modeled through GANs in Euclidean spaces [Baptista et al., 2020] and function spaces [Hosseini et al., 2023]. In contrast, we use a novel dynamic formulation of COT, which we model through a generalization of flow matching [Lipman et al., 2022, Albergo et al., 2023b, Liu et al., 2022]. This allows us to use flexible architectures while avoiding the difficulties of training GANs [Arora et al., 2018]. ", "page_idx": 1}, {"type": "text", "text": "Simulation-Free Continuous Normalizing Flows. Flow matching [Lipman et al., 2022] (and the closely related stochastic interpolants [Albergo et al., 2023a] and rectified flows [Liu et al., 2022]) are a class of methods for building continuous-time normalizing flows in a simulation-free manner. Notably, these works do not approximate an optimal transport between the source and target measures. Pooladian et al. [2023] and Tong et al. [2023] propose instead to couple the source and target distributions via optimal transport, leading to marginally optimal paths. In this work, we study an extension of these techniques for conditional generation. ", "page_idx": 1}, {"type": "text", "text": "While some works [Davtyan et al., 2023, Gebhard et al., 2023, Isobe et al., 2024, Wildberger et al., 2024] have applied flow matching for conditional generation, these approaches do not employ COT. Notably, these approaches are limited to the finite-dimensional setting, whereas our method adds to the growing literature on function-space generative models [Hosseini et al., 2023, Kerrigan et al., 2023, 2024, Lim et al., 2023, Franzese et al., 2024]. Concurrent work by Chemseddine et al. [2024] develops the foundation of dynamic COT with applications to flow matching, and concurrent work by Barboni et al. [2024] develop the theory of dynamic COT for the purposes of studying infinitely deep ResNets. However, these works focus on the Euclidean setting, whereas our methods are applicable in general separable Hilbert spaces. ", "page_idx": 1}, {"type": "text", "text": "3 Background and Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $X,X^{\\prime}$ represent arbitrary separable Hilbert spaces, equipped with the Borel $\\sigma$ -algebra. We use $\\mathbb{P}(X)$ to represent the space of Borel probability measures on $X$ , and $\\mathbb{P}_{p}(X)\\subseteq\\mathbb{P}(X)$ to represent the subspace of measures having finite $p$ th moment. If $\\eta\\in\\mathbb{P}(X)$ is a probability measure on $X$ and $T:X\\rightarrow X^{\\prime}$ is measurable, then the pushforward measure $\\dot{T}_{\\#}\\dot{\\eta}(-)\\dot{=\\eta}(T^{-1}\\dot{(-)})$ is a probability measure on $X^{\\prime}$ . Maps of the form e.g. $\\pi^{X}:X\\times X^{\\prime}\\to X$ represent the canonical projection. ", "page_idx": 2}, {"type": "text", "text": "We assume that we have two separable Hilbert spaces of interest. The first, $Y$ , is a space of observations, and the second, $U$ , is a space of unknowns. These spaces may be of infinite dimensions, but a case of practical interest is when $Y$ and $U$ are finite dimensional Euclidean spaces. We will consider the product space $Y\\times U$ , equipped with the canonical inner product obtained via the sum of the inner products on $Y$ and $U$ , under which the space $Y\\times U$ is also a separable Hilbert space. Let $\\eta\\in\\mathbb{P}(\\bar{Y}\\times U)$ be a joint probability measure. The measures $\\pi_{\\#}^{Y}\\eta\\in\\mathbb{P}(Y)$ and $\\pi_{\\#}^{U}\\eta\\in\\mathring{\\mathbb{P}}(U)$ obtained via projection are the marginals of $\\eta$ . We use $\\eta^{y}\\in\\mathbb{P}(U)$ to represent the measure obtained by conditioning $\\eta$ on the value $y\\in Y$ . By the disintegration theorem [Bogachev and Ruas, 2007, Chapter 10], such conditional measures exist and are essentially unique, in the sense that there exists a Borel set $E\\subseteq Y$ with $\\pi_{\\#}^{Y}\\eta(E)=0$ , and the $\\eta^{y}$ are unique for $y\\notin E$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Static Conditional Optimal Transport ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In conditional optimal transport, we are given a target measure $\\nu\\in\\mathbb{P}(Y\\times U)$ and some source measure $\\eta\\in\\mathbb{P}(U)$ . We seek a transport map $T:Y\\times U\\rightarrow U$ such that, for any given $y\\in Y$ , the mapping $T(y,-):U\\to U$ transforms the source distribution $\\eta$ into the conditional distribution $\\nu^{y}$ , i.e., $T(y,-)_{\\#}\\eta=\\nu^{y}$ . In a sense, $T$ can be thought of as a collection of transport maps indexed by $y\\in Y$ . If such a $T$ were available, by drawing samples $u_{0}\\sim\\eta$ and transforming them, one would obtain samples $T(y,u_{0})\\sim\\nu^{y}$ . Solving this transport problem for each fixed $y$ is expensive at best, or impossible when only has a single (or no) samples $(y,u)\\sim\\nu$ for any given $y$ . Thus, one must leverage information across different observations $y$ . To that end, recent work has focused on the notion of triangular mappings $T:Y\\times U\\to Y\\times U$ [Hosseini et al., 2023, Baptista et al., 2020] of the form $T(y,u)\\stackrel{.}{=}(T_{Y}(\\bar{y)_{}}T_{U}(T_{Y}(y),u))$ for some $T_{Y}:Y\\rightarrow Y$ and $T_{U}:Y\\times U\\rightarrow U$ . Triangular mappings are of interest as they allow us to obtain conditional couplings from joint couplings. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1 (Theorem 2.4 [Baptista et al., 2020], Prop. 2.3 [Hosseini et al., 2023]) Suppose $\\eta,\\nu\\ \\in\\ \\mathbb{P}(Y\\,\\times\\,U)$ and $T\\ :\\ Y\\,\\times\\,U\\ \\to\\ Y\\,\\times\\,U$ is triangular. If $T_{\\#}\\eta\\ =\\ \\nu_{!}$ , then $T_{U}(T_{Y}(y),-)_{\\#}\\eta^{y}=\\nu^{T_{Y}(y)}$ for $\\pi_{\\#}^{Y}\\eta$ -almost every $y$ . ", "page_idx": 2}, {"type": "text", "text": "In many scenarios of practical interest, the source measure $\\eta$ and the target measure $\\nu$ have the same $Y$ -marginals. We will henceforth make this assumption, and use $\\mu\\,\\,\\breve{=}\\,\\,\\pi_{\\#}^{Y}\\eta\\,=\\,\\pi_{\\#}^{Y}\\nu$ to represent this marginal. In this case, we may take $T_{Y}$ to be the identity mapping, so that the conclusion of Proposition 1 simplifies to $T_{U}(y,-)_{\\#}\\eta^{y}=\\nu^{y}$ for $\\mu$ -almost every $y$ . We note that in situations where such an assumption does not hold, one may simply preprocess the source measure $\\eta$ via an invertible mapping $T_{Y}$ satisfying $[T_{Y}]_{\\#}[\\pi_{\\#}^{Y}\\eta]=\\pi_{\\#}^{Y}\\nu$ [Hosseini et al., 2023, Prop 3.2]. ", "page_idx": 2}, {"type": "text", "text": "Given a source and target measures $\\eta$ , $\\nu\\in\\mathbb{P}^{\\mu}(Y\\times U)$ and a cost function $c:(Y\\times U)^{2}\\to\\mathbb{R},$ , the conditional Monge problem seeks to find a triangular mapping solving ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T}\\left\\{\\int_{Y\\times U}c(y,u,T(y,u))\\,\\mathrm{d}\\eta(y,u)\\ |\\ T_{\\#}\\eta=\\nu,T:(y,u)\\mapsto(y,T_{U}(y,u))\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The conditional Monge problem also admits a relaxation under which one only considers couplings whose $Y$ -components are almost surely equal. To that end, for $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times\\dot{U})$ we define the set of triangular couplings $\\Pi_{Y}(\\eta,\\nu)$ to be the couplings of $\\eta$ and $\\nu$ that almost surely fix the $Y$ -components, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi_{Y}(\\eta,\\nu)=\\left\\{\\gamma\\in\\mathbb{P}\\left(\\left(Y\\times U\\right)^{2}\\right)\\mid\\pi_{\\#}^{1,2}\\gamma=\\eta,\\pi_{\\#}^{3,4}\\gamma=\\nu,\\pi_{\\#}^{1,3}=(I,I)_{\\#}\\mu\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In other words, a triangular coupling $\\gamma\\,\\in\\,\\Pi_{Y}(\\eta,\\nu)$ has samples $(y_{0},u_{0},y_{1},u_{1})\\,\\sim\\,\\gamma$ such that $y_{0}=y_{1}$ almost surely. The conditional Kantorovich problem seeks a triangular coupling solving ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma}\\left\\{\\int_{(Y\\times U)^{2}}c(y_{0},u_{0},y_{1},u_{1})\\,\\mathrm{d}\\gamma\\big(y_{0},u_{0},y_{1},u_{1}\\big)\\mid\\gamma\\in\\Pi_{Y}\\big(\\eta,\\nu\\big)\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Hosseini et al. [2023] prove the existence of minimizers to the conditional Kantorovich and Monge problems under very general assumptions. Moreover, optimal couplings to the conditional Kantorovich problem induce optimal couplings for $\\mu$ -almost every conditional measure. We refer to Appendix B and Hosseini et al. [2023] for further details. ", "page_idx": 3}, {"type": "text", "text": "4 Conditional Wasserstein Space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by our discussion on triangular transport maps, we introduce the conditional Wasserstein spaces, consisting of joint measures with finite $p$ th moments and having fixed $Y$ -marginals $\\mu$ . Interestingly, Gigli [2008, Chapter 4] studies the same space for the purposes of constructing geometric tangent spaces in the usual Wasserstein space. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Conditional Wasserstein Space) ", "page_idx": 3}, {"type": "text", "text": "Suppose $\\mu\\in\\mathbb{P}(Y)$ is given and $1\\leq p<\\infty$ . The conditional $p$ -Wasserstein space is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}_{p}^{\\mu}(Y\\times U)=\\left\\{\\gamma\\in\\mathbb{P}_{p}(Y\\times U)\\mid\\pi_{\\#}^{Y}\\gamma=\\mu\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We now equip $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ with a metric $W_{p}^{\\mu}$ , the conditional Wasserstein distance. Intuitively, the conditional Wasserstein distance measures the usual Wasserstein distance between all of the conditional distributions in expectation under the fixed $Y$ -marginal $\\mu$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Conditional $p$ -Wasserstein Distance) ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{p}^{\\mu}(\\eta,\\nu)=\\left(\\mathbb{E}_{y\\sim\\mu}\\left[W_{p}^{p}(\\eta^{y},\\nu^{y})\\right]\\right)^{1/p}=\\left(\\int_{Y}W_{p}^{p}(\\eta^{y},\\nu^{y})\\,\\mathrm{d}\\mu(y)\\right)^{1/p}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the conditional $p$ -Wasserstein distance. $W_{p}$ is the usual Wasserstein distance for measures on $U$ . ", "page_idx": 3}, {"type": "text", "text": "By Jensen\u2019s inequality we have $W_{p}^{\\mu}(\\eta,\\nu)\\ge\\mathbb{E}_{y\\sim\\mu}\\left[W_{p}(\\eta^{y},\\nu^{y})\\right]$ . In the following, we show that the conditional Wasserstein distance is a well-defined metric as well as a few other metric properties. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Some Properties of $W_{p}^{\\mu}$ ) ", "page_idx": 3}, {"type": "equation", "text": "$$\nl\\,\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U),\\,W_{p}\\left(\\pi_{\\#}^{U}\\eta,\\pi_{\\#}^{U}\\nu\\right)\\leq W_{p}^{\\mu}(\\eta,\\nu)\\;a n d\\,W_{p}(\\eta,\\nu)\\leq W_{p}^{\\mu}(\\eta,\\nu).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proposition $2(\\mathsf{c},\\,\\mathsf{d})$ together shows that one should expect the topology generated by $W_{p}^{\\mu}$ to be stronger than the unconditional distance $W_{p}$ . Here, we note that Gigli [2008] and Chemseddine et al. [2023] previously showed that $W_{p}^{\\mu}$ is a metric through an equivalence with restricted couplings. Our approach builds on the results of Hosseini et al. [2023] and is somewhat more direct, and hence our proofs may be of independent interest. ", "page_idx": 3}, {"type": "text", "text": "For the sake of concreteness, we include an example where the conditional 2-Wasserstein distance may be explicitly computed. Here, the necessary calculations follows from the fact that the conditional distributions of a multivariate are again Gaussian, and Gaussian distributions admit a closed-form expression for the usual unconditional 2-Wasserstein distance. ", "page_idx": 3}, {"type": "text", "text": "Example: Gaussian Measures. Suppose $Y=U=\\mathbb{R}$ and that $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ are Gaussians ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta=\\mathcal{N}(0,I)\\qquad\\nu=\\mathcal{N}\\left(0,\\left[\\!\\!\\begin{array}{c c}{{1}}&{{\\rho}}\\\\ {{\\rho}}&{{1}}\\end{array}\\!\\!\\right]\\right)\\qquad|\\rho|<1.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It follows that $\\mu\\;=\\;\\pi_{\\#}^{Y}\\eta\\;=\\;\\pi_{\\#}^{Y}\\nu\\;=\\;{\\mathcal N}(0,1)$ . As $\\eta^{y},\\nu^{y}$ are Gaussians, their $W_{2}$ distance admits a closed form and we can directly compute the expectation in Equation (5) to obtain $W_{2}^{\\mu,2}(\\eta,\\nu)=2(1-\\sqrt{1-\\rho^{2}})$ . orenglyar idfl $\\rho~=~0$ ,. i.eM.o $\\eta\\ =\\ \\nu$ .he Huoncwoenvdeir-, \u03c0U#\u03bd = N(0, 1) andW2(\u03c0U#\u03b7, \u03c0U#\u03bd $\\rho$ tional distance is $W_{2}^{2}(\\eta,\\nu)\\;=\\;2\\left(2-\\sqrt{1-\\rho}-\\sqrt{1+\\rho}\\right)$ , from which it is easy to verify that $W_{2}(\\eta,\\nu)\\leq W_{2}^{\\mu}(\\eta,\\nu)$ . See Appendix $\\mathbf{C}$ for a similar derivation which applies to arbitrary Gaussians. ", "page_idx": 3}, {"type": "text", "text": "Conditional Wasserstein Space as a Geodesic Space. We now turn our attention to the geodesics in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . In particular, we show that there exists a constant speed geodesic between any two measures in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ , generalizing a similar result in the unconditional setting [Santambrogio, 2015, Theorem 5.27]. Moreover, we show that under suitable regularity assumptions, solutions to the conditional Monge problem (1) induce constant speed geodesics. Our motivation for studying geodesics in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is practical \u2013 in Section 6, we show how one can model geodesics in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ in order to obtain a conditional flow-based model whose paths are easy to integrate. ", "page_idx": 4}, {"type": "text", "text": "A curve is a continuous function $\\gamma_{\\bullet}:I\\rightarrow\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ where $I=(a,b)\\subseteq\\mathbb{R}$ is any open interval of finite length. A curve is absolutely continuous if there exists $m\\in L^{1}((a,b))$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{p}^{\\mu}(\\gamma_{s},\\gamma_{t})\\leq\\int_{s}^{t}m(\\tau)\\,\\mathrm{d}\\tau\\qquad\\forall a<s\\leq t<b.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If $(\\gamma_{t})$ is an absolutely continuous curve, then its metric derivative ", "page_idx": 4}, {"type": "equation", "text": "$$\n|\\gamma^{\\prime}|(t)=\\operatorname*{lim}_{s\\to t}{\\frac{W_{p}^{\\mu}(\\gamma_{s},\\gamma_{t})}{|s-t|}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "exists for almost every $t\\in(a,b)$ , and, moreover, we almost surely have $|\\gamma^{\\prime}|(t)\\leq m(t)$ pointwise for any $m$ satisfying Equation (7) [Ambrosio et al., 2005, Theorem 1.1.2]. A curve $(\\gamma_{t})$ is called a constant speed geodesic if for all $a<s\\leq t<b$ , we have $W_{\\underline{{{p}}}}^{\\mu}(\\gamma_{s},\\gamma_{\\underline{{{t}}}})=|t_{.}-s|W_{p}^{\\underline{{{\\mu}}}}(\\gamma_{a},\\gamma_{b})$ . It is straightforward to show that every constant speed geodesic is absolutely continuous. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 $(\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is a Geodesic Space) ", "page_idx": 4}, {"type": "text", "text": "For any $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ , there exists a constant speed geodesic between $\\eta$ and $\\nu$ . ", "page_idx": 4}, {"type": "text", "text": "When an optimal triangular coupling $\\gamma^{\\star}\\in\\Pi_{Y}(\\eta,\\nu)$ is induced by an injective triangular map $T^{\\star}$ , we may recover a constant speed geodesic in $\\mathbb{P}_{p}^{\\mu}(Y_{-}\\times U)$ , generalizing the McCann interpolant [McCann, 1997] to the conditional setting. We refer to Proposition 4 for sufficient conditions on $\\eta,\\nu$ under which such a $T^{\\star}$ exists. Informally, samples from $(y_{0},u_{0})\\sim\\eta$ flow in a straight path at a constant speed to their destination $T^{\\star}(y_{0},\\dot{u_{0}})$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Conditional McCann Interpolants) ", "page_idx": 4}, {"type": "text", "text": "$F i x\\;\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . Suppose $T^{\\star}(\\bar{y,\\,u})\\,=\\,(y,T_{\\mathcal{U}}^{\\star}(y,u))$ is an injective triangular map solving the conditional Monge problem (1). Define the maps $T_{t}:Y\\times U\\to Y\\times U$ for $0\\le t\\le1$ via $T_{t}=(1-t)I+t T^{\\star}$ , and define the curve of measures $\\gamma_{t}=[T_{t}]_{\\#}\\eta\\in\\mathbb{P}_{p}^{\\gamma}(Y\\times U)$ . Then, ", "page_idx": 4}, {"type": "text", "text": "(a) $(\\gamma_{t})$ is absolutely continuous and a constant speed geodesic between $\\eta,\\nu$ $(b)$ The vector field $v_{t}(T_{t}^{\\star}(y,u))=(0,T_{U}^{\\star}(y,u)-u)$ generates the path $\\gamma_{t}$ , in the sense that $(\\gamma_{t},v_{t})$ solve the continuity equation (9). ", "page_idx": 4}, {"type": "text", "text": "5 Conditional Benamou-Brenier Theorem ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we prove a characterization of the absolutely continuous curves in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . As a corollary, we obtain a conditional generalization of the Benamou-Brenier Theorem [Benamou and Brenier, 2000], giving us a dynamic characterization of the conditional Wasserstein distance. Roughly speaking, all such curves are generated by a vector field on $Y\\times U$ which has zero velocity in the $Y$ component. This is natural, as all measures in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ have a fixed $Y$ -marginal $\\mu$ . Such a vector field can be informally seen as tangent to a curve of measures, and is the dynamic analogue of the triangular maps discussed in Section 3. More formally, given an open interval $I\\subseteq\\mathbb{R}$ , a time-dependent Borel vector field $v:I\\times Y\\times U\\to Y\\times U$ is said to be triangular if there exists a Borel vector field $v^{U}:I\\times Y\\times U\\to U$ such that $v_{t}(y,u)=\\big(0,v_{t}^{U}(y,u)\\big)$ . ", "page_idx": 4}, {"type": "text", "text": "Continuity Equation. We introduce some necessary background which allows us to link vector fields to curves of measures. The continuity equation $\\begin{array}{r}{\\partial_{t}\\gamma_{t}+\\mathrm{div}(v_{t}\\gamma_{t})=0}\\end{array}$ describes the evolution of a measure $\\gamma_{t}$ which flows along a given vector field $v_{t}$ [Ambrosio et al., 2005, Chapter 8]. This equation must be understood in the sense of distributions, i.e. for every $\\varphi$ in a space of test functions, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\int_{I}\\int_{Y\\times U}\\left(\\partial_{t}\\varphi(y,u,t)+\\langle v_{t}(y,u),\\nabla_{y,u}\\varphi(y,u,t)\\rangle\\right)\\,\\mathrm{d}\\gamma_{t}(y,u)\\,\\mathrm{d}t=0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We consider cylindrical test functions $\\varphi\\in\\mathrm{Cyl}(Y\\times U\\times I)$ , i.e. of the form $\\varphi(y,u,t)=\\psi(\\pi^{d}(y,u),t)$ where $\\pi^{d}:Y\\times U\\to\\mathbb{R}^{d}$ maps $(y,u)\\mapsto(\\langle(y,u),e_{1}\\rangle,\\ldots,\\langle(y,u),e_{d}\\rangle)$ and $\\{e_{1},e_{2},\\ldots,e_{d}\\}$ is any orthonormal family in $Y\\times U$ . In the finite dimensional setting, one may take $\\varphi\\in C_{c}^{\\infty}(Y\\times U)$ to be smooth and compactly supported [Ambrosio et al., 2005, Remark 8.1.1]. ", "page_idx": 5}, {"type": "text", "text": "In Appendix $\\boldsymbol{\\mathrm E}$ , we prove Lemma 1, which is key in proving Theorem 4 below. Informally, Lemma 1 states that if the weak continuity equation (9) is satisfied for a joint distribution and triangular vector field, then the continuity equation is also satisfied for the corresponding conditional distributions and $U$ components of the vector field. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Triangular Vector Fields Preserve Conditionals)   \nSuppose $v_{t}(y,u)^{\\bullet}{=}\\,(0,v_{t}^{U}(y,u))$ is triangular and that $(\\gamma_{t})\\subset\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is a path of measures such that $(v_{t},\\gamma_{t})$ satisfy the continuity equation in the sense of distributions. Then, it follows that for $\\mu$ -almost every $y\\in Y$ , we have $\\partial_{t}\\gamma_{t}^{y^{\\prime}}+\\dot{\\nabla}\\cdot(v_{t}^{U}(y,-)\\gamma_{t}^{y})=\\dot{0}$ . ", "page_idx": 5}, {"type": "text", "text": "We note that having $v_{t}$ be triangular is sufficient, but certainly not necessary, for the conditional continuity equation to almost surely hold. For instance, the vector field in $\\mathbb{R}^{\\dot{d}}$ that rotates ${\\mathcal{N}}(0,I)$ about the origin is not triangular yet preserves all conditional distributions. ", "page_idx": 5}, {"type": "text", "text": "Absolutely Continuous Curves. In this section, we state our characterization of absolutely continuous curves in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . Informally, given such a curve, Theorem 3 provides us with a triangular vector field which generates the curve, in the sense that the pair solve the continuity equation. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Absolutely Continuous Curves in $\\mathbb{P}_{p}^{\\mu}(Y\\times U))$ ", "page_idx": 5}, {"type": "text", "text": "Let $I\\subset\\mathbb{R}$ be an open interval, and suppose $\\gamma_{t}:I\\to\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is an absolutely continuous in the $W_{p}^{\\mu}$ metric with $|\\gamma^{\\prime}|(t)\\in L^{1}(I)$ . Then, there exists a Borel vector field $v_{t}(y,u)$ such that ", "page_idx": 5}, {"type": "text", "text": "(a) $v_{t}$ is triangular (b) $v_{t}\\in L^{p}(\\gamma_{t},Y\\times U)$ and $\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\leq|\\gamma^{\\prime}|(t)\\,f o r\\,a.e.\\;t$ (c) $(v_{t},\\gamma_{t})$ solve the continuity equation in the sense of distributions. ", "page_idx": 5}, {"type": "text", "text": "Conversely, we show in Theorem 4 that if the pair $(\\gamma_{t},v_{t})$ solve the continuity equation and $v_{t}$ is triangular, then the curve $(\\gamma_{t})$ is absolutely continuous and $|\\gamma^{\\prime}|(t)\\,\\leq\\,\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}$ . The main technique of this result is to study the collection of conditional continuity equations (which is feasible by Lemma 1) and to apply the converse of Ambrosio et al. [2005, Theorem 8.3.1]. In this setting, the infinite-dimensional result is obtained via a finite-dimensional approximation argument. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4 (Continuous Curves Generated by Triangular Vector Fields) ", "page_idx": 5}, {"type": "text", "text": "Suppose that $\\gamma_{t}:I\\to\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is narrowly continuous and $\\left(v_{t}\\right)$ is a triangular vector field such that $(\\gamma_{t},v_{t})$ solve the continuity equation with $\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\in L^{1}(I)$ . Then, $\\gamma_{t}:I\\to\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is absolutely continuous in the $W_{p}^{\\mu}$ metric and $|\\gamma^{\\prime}|(t)\\leq\\|v_{t}\\|_{L^{p}(\\mu,Y\\times U)}$ for almost every $t$ . ", "page_idx": 5}, {"type": "text", "text": "As a corollary of Theorem 3 and Theorem 4, we obtain a conditional version of the Benamou-Brenier theorem [Benamou and Brenier, 2000]. Once we have our characterization of absolutely continuous curves provided by these theorems, the proof of Theorem 5 largely follows the unconditional case (see e.g. Ambrosio et al. [2005, Chapter 8]), but we include it for the sake of completeness. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5 (Conditional Benamou-Brenier) ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{p}^{p,\\mu}(\\eta,\\nu)=\\operatorname*{min}_{(\\gamma_{t},v_{t})}\\left\\{\\int_{0}^{1}\\left\\lVert v_{t}\\right\\rVert_{L^{p}(\\mu_{t})}^{p}\\,\\mathrm{d}t\\,|\\left(v_{t},\\gamma_{t}\\right)s o l\\nu e\\left(9\\right)\\!,\\,\\gamma_{0}=\\eta,\\gamma_{1}=\\nu,\\,\\nu=\\nu\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "6 COT Flow Matching ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We have thus far seen that the COT problem (3) admits a dynamic formulation by Theorem 5, where one may take the underlying vector fields to be triangular. We use these results to design a principled model for conditional generation based on flow matching [Lipman et al., 2022, Albergo et al., 2023b, Liu et al., 2022, Tong et al., 2023, Pooladian et al., 2023]. We hereafter use the squared-distance cost (i.e. $p=2$ ). ", "page_idx": 5}, {"type": "image", "img_path": "tk0uaRynhH/tmp/3777b8c93bb1766ab333c72e0c8df994219bdccfd8556ed3ebc2b923c8b93397.jpg", "img_caption": ["Figure 1: Samples from the ground-truth joint target distribution and the various models. Samples from COT-FM more closely match the ground-truth distribution than the baselines. In the final column, we plot conditional KDEs for samples drawn conditioned on the $y$ value indicated by the dashed horizontal line. See Appendix $\\boldsymbol{\\mathrm{F}}$ for a larger figure and additional results. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Flow Matching. We assume that we have access to samples $z_{0}~=~(y_{0},u_{0})~\\sim~\\eta(y_{0},u_{0})~\\in$ $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ from a source measure, and samples $z_{1}=(y_{1},u_{1})\\sim\\nu(y_{1},u_{1})\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ from a target measure. Let $z=(z_{0},z_{1})\\sim\\rho(z_{0},z_{1})\\in\\Pi(\\eta,\\nu)$ be any coupling of the source and target measure. We specify a collection of measures and vector fields on $Y\\times U$ via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma_{t}(y,u\\mid z)=\\mathcal{N}\\left(y,u\\mid t z_{1}+\\left(1-t\\right)z_{0},C\\right)\\qquad v_{t}(y,u\\mid z)=z_{1}-z_{0}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C$ is any trace-class covariance operator [Da Prato and Zabczyk, 2014]. As is standard in flow matching [Lipman et al., 2022, Kerrigan et al., 2024], we obtain from Equations (10) a marginal measure $\\gamma_{t}(y,u)$ and vector field $v_{t}(y,u)$ satisfying the continuity equation via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma_{t}(y,u)=\\int_{(Y\\times U)^{2}}\\gamma_{t}(y,u\\mid z)\\,\\mathrm{d}\\rho(z)\\qquad v_{t}(y,u)=\\int_{(Y\\times U)^{2}}v_{t}(y,u\\mid z)\\frac{\\,\\mathrm{d}\\gamma_{t}(y,u\\mid z)}{\\,\\mathrm{d}\\gamma_{t}(y,u)}\\,\\mathrm{d}\\rho(z).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This marginal path $(\\gamma_{t})_{t=0}^{1}$ interpolates between the source measure $t=0,$ ) and a smoothed version of the target measure $t=1$ ). To transform source samples from $\\eta$ into target samples from $\\nu$ , we seek to learn the intractable vector field $v_{t}(y,u)$ with a model $v^{\\theta}(t,y,u)$ by minimizing the loss1 ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}_{t,\\rho(z),\\gamma_{t}(y,u|z)}\\left\\|v^{\\theta}(t,y,u)-v_{t}(y,u\\mid z)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which has the same $\\theta^{.}$ -gradient as the MSE loss to the true vector field $u_{t}(y,u)$ [Tong et al., 2023]. ", "page_idx": 6}, {"type": "text", "text": "COT Flow Matching. In the preceding section, $\\rho(z)$ may be an arbitrary coupling between $\\eta$ and $\\nu$ . Motivated by Proposition 1, we will choose $\\rho$ to be a COT coupling. Under sufficient regularity conditions (see Appendix B), this COT plan will be induced by a triangular map. In turn, Theorem 2 gives us that this triangular map is generated by a triangular vector field of the form (10). Thus, we parametrize our model ${\\dot{u}}^{\\theta}$ to also be triangular. Moreover, we recover the optimal dynamic transport given in Theorem 5 as $\\operatorname{Tr}(C)\\to0$ by a pointwise application of [Tong et al., 2023, Proposition 3.4]. ", "page_idx": 6}, {"type": "text", "text": "Given a collection of samples $\\{z_{0}^{i},z_{1}^{i}\\}_{i=1}^{n}$ drawn from $\\eta$ and $\\nu$ , we approximate a conditional optimal coupling $\\rho$ using standard numerical techniques with the cost function $c_{\\epsilon}(y_{0},u_{0},y_{1},u_{1})=$ $|\\dot{y}_{1}-y_{0}|^{2}+\\epsilon|\\bar{u_{1}}-u_{0}|^{\\frac{\\pi}{2}}$ for some $0<\\epsilon\\ll1$ . Intuitively, such a cost penalizes mass transfer along the $Y$ dimension, which is precisely the constraint sought in the COT problem (3). As $\\epsilon\\downarrow0$ , we recover the true optimal triangular map [Carlier et al., 2010, Hosseini et al., 2023]. The COT coupling can either be precomputed for small datasets or computed on each minibatch drawn during training. While the use of minibatches is a computational necessity, we find that surprisingly small batch sizes still yields accurate approximations of the true COT mapping using our COT-FM method. See Appendix G. ", "page_idx": 6}, {"type": "table", "img_path": "tk0uaRynhH/tmp/ca8d55f6449312d9c7af3fb42fa7c236cc0ca0c4755bfb5e956500b9c0b58b9c.jpg", "table_caption": ["Table 1: Distances between the ground-truth and generated joint distributions for the 2D datasets. Our method (COT-FM) obtains lower distances than the considered baselines. Average results $\\pm$ one standard deviation are reported across five test sets, with the lowest average distance in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "After training, we obtain a learned triangular vector field $v^{\\theta}(t,y,u)$ . Given an arbitrary fixed $y\\in Y$ , we may approximately sample from the target $\\nu(u\\mid y)$ by sampling $u_{0}\\sim\\eta(u_{0}\\mid y)$ and numerically solving the corresponding flow equation $\\partial_{t}(y,u_{t})=v^{\\theta}(t,y,u_{t})$ with initial condition $(y,u_{0})$ . ", "page_idx": 7}, {"type": "text", "text": "Source Measure. Our framework is agnostic to the choice of source measure $\\eta$ , allowing for flexibility in the modeling process. The main requirement is that the $Y$ -marginals of the source $\\eta$ and target $\\eta$ must match. In some scenarios, this is trivially satisfied. If one is interested in using a source distribution which is simply random noise, one may take $\\eta(y_{0},u_{0})=\\pi_{\\#}^{Y}\\nu(y_{0})\\otimes\\eta_{U}(u_{0})$ to be the product of two independent distributions where $\\eta_{U}$ is arbitrary, e.g. Gaussian noise. ", "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now illustrate our methodology (COT-FM) on a variety of conditional simulation tasks. We compare our method against several competitive baselines, namely PCP-Map [Wang et al., 2023], COT-Flow [Wang et al., 2023], and WaMGAN [Hosseini et al., 2023]. These baselines are chosen as they reflect current state-of-the-art approaches to learning COT maps. We additionally compare against flow matching [Lipman et al., 2022, Wildberger et al., 2024] without COT, i.e. where the coupling between the source and target measures is the independent coupling $\\rho(z_{0},z_{1})=\\eta\\otimes\\nu$ . This baseline serves as an ablation for the COT component of our model. ", "page_idx": 7}, {"type": "table", "img_path": "tk0uaRynhH/tmp/1db80ba7fc3af99f6575390c8c4091456464f03b8d4e0938c3f40eceff644e66.jpg", "table_caption": ["Table 2: Statistical distances between MCMC and posterior samples $u\\quad\\sim$ $\\nu(u\\mid y)$ for each method on the LV dataset. Average results $\\pm$ one standard deviation reported across five test sets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Overall, our method (COT-FM) typically outperforms these baselines across the diverse and challenging set of tasks we consider. We find that PCP-Map [Wang et al., 2023] is a strong baseline, but we emphasize that this model relies on the use of an input-convex neural network [Amos et al., 2017] and it is hence unclear how to adapt this method to e.g. images. Appendix $\\boldsymbol{\\mathrm{F}}$ contains further details and results for all of our experiments.2 ", "page_idx": 7}, {"type": "text", "text": "2D Synthetic Data. We first consider synthetic distributions where $Y\\,=\\,U\\,=\\,\\mathbb{R}$ . Our source measure is taken to be the independent product $\\eta(y,u)=\\pi_{\\#}^{Y}\\nu\\otimes\\mathcal{N}(0,1)$ . We plot ground-truth joint distributions and samples for two datasets in Figure 1. See Appendix F for additional results. Samples from our method (COT-FM) closely match those from the ground-truth distribution, whereas samples from PCP-Map and COT-Flow [Wang et al., 2023] can produce samples in regions of zero support under the ground-truth distribution. In Table 1, we provide a quantitative analysis, where we measure the $W_{2}$ and MMD distances between the generated and ground-truth joint distributions. This is motivated by Proposition 1, as triangular maps which couple the joint distributions necessarily couple the conditional distributions. Our method outperforms the baselines across all metrics. ", "page_idx": 7}, {"type": "text", "text": "Lotka-Volterra (LV) Dynamical System. Here we estimate parameters of the Lotka-Volterra (LV) model given only noisy observations of its solution. The LV model has parameters $u=(\\alpha,\\beta,\\gamma,\\delta)\\in$ $\\mathbb{R}_{\\geq0}^{4}$ and a pair of coupled nonlinear ODEs of the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}p_{1}(t)}{\\mathrm{d}t}=\\alpha p_{1}-\\beta p_{1}p_{2}\\quad\\frac{\\mathrm{d}p_{2}(t)}{\\mathrm{d}t}=-\\gamma p_{2}+\\delta p_{1}p_{2}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "whose solution $p(t)=(p_{1}(t),p_{2}(t))\\in\\mathbb{R}_{\\ge0}^{2}$ represents the number of prey and predator species at time $t\\ \\in\\ [0,T]$ . Following Alfonso et al. [2023], we assume $p(0)\\,=\\,(30,1)$ and that $\\mathrm{log}(u)\\;\\sim\\;{\\mathcal N}(m,0.5I)$ with $m\\;=\\;(-0.125,-3,-0.125,-3)$ . Given parameters $u~\\in~\\mathbb{R}_{\\ge0}^{4}$ R4\u22650, we simulate Equation (13) for $t\\,\\in\\,\\{0,2,\\dots,20\\}$ to obtain a solution $\\ensuremath{z}(u)\\ensuremath{\\,\\in\\,}\\ensuremath{\\mathbb{R}}_{\\geq0}^{22}$ . An observation $\\boldsymbol{y}\\,\\in\\,\\mathbb{R}_{\\ge0}^{22}$ is obtained by the addition of log-normal noise, i.e. $\\log(y)\\bar{\\sim}\\,\\mathcal{N}(\\log(z(u),0.1I)$ . We thus may simulate many $(y,u)$ pairs from the target measure for training. ", "page_idx": 8}, {"type": "text", "text": "As a benchmark, we follow the settings of Alfonso et al. [2023] and choose parameters $u=(0.83,0.041,1.08,0.04)$ to generate a single observation $y$ as described above. In Figure 2, we plot a histogram of 10, 000 samples from the posterior $\\nu(u\\mid y)$ of COT-FM. ", "page_idx": 8}, {"type": "text", "text": "Since the ground-truth posterior is intractable, we compare against differential evolution Metropolis MCMC [Braak, 2006]. Samples from our method qualitatively resemble those from MCMC, and the posterior mode is typically close to the true unknown $u$ (shown in red). Our method is quantitatively closest to the MCMC samples in the $W_{2}$ metric, and competitive in the MMD metric (Table 2). ", "page_idx": 8}, {"type": "image", "img_path": "tk0uaRynhH/tmp/54090b67a915e94b9f8a6299f03ef45121d333a5ec1d80464bcbe2d4a86c3596.jpg", "img_caption": ["Figure 2: Sample KDEs on the Lotka-Volterra inverse problem. The red lines denote the true parameter values. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Darcy Flow Inverse Problem. Here we consider an infinitedimensional Bayesian inverse problem from the 2D Darcy flow PDE. The setting is adapted from Hosseini et al. [2023]. We opt to compare against WaMGAN [Hosseini et al., 2023], as this is currently the only other extant amortized function-space COT method, and FFM [Kerrigan et al., 2023] as a function-space flow matching ablation. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Predictive performance of the generated samples on the Darcy flow inverse problem. Average result $\\pm$ one standard deviation obtained on 5 test sets of 5,000 samples each. ", "page_idx": 8}, {"type": "table", "img_path": "tk0uaRynhH/tmp/0f0b0e48e13ea62548a25b4780f8e7e9d806bd91533cb10f405d24b7b8ca68e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The Darcy flow PDE is an elliptic equation on a smooth domain $\\Omega\\subseteq\\mathbb{R}^{d}$ which relates a permeability field $\\exp(u)$ , a pressure field $\\rho$ , and a source term $f$ via $-\\mathrm{div}\\,\\mathrm{exp}(u)\\nabla\\rho\\,=\\,f$ on $\\Omega$ subject to $\\rho=0$ on $\\partial\\Omega$ . Our goal is to recover the permeability $u$ from noisy measurements $y$ of the pressure $\\rho$ . Both the unknown $u$ and observations $y$ are functions and thus infinite-dimensional. To define our target measure, we specify a prior $\\nu(u)\\,=\\mathcal{N}(0,C)$ with a Mat\u00e9rn kernel $C$ of lengthscale $\\ell=1/2$ and $\\nu=3/2$ . Given $u\\sim\\eta(u)$ , the Darcy flow PDE is solved numerically [Aln\u00e6s et al., 2015] to obtain a solution $\\rho(u)$ observed at some finite but arbitrary number of points $\\{x_{1},\\ldots,x_{n}\\}\\subset\\mathbb{R}^{2}$ . An observation $y(u)$ is obtained by adding Gaussian noise to each observation, i.e. $y(u)\\sim\\dot{\\mathcal{N}}(\\rho(u),\\sigma^{2}I)$ where $\\sigma=\\mathrm{{}}2.5\\times{10^{-2}}$ . We implement all models via a Fourier Neural Operator [Li et al., 2020], allowing us to work with arbitrary discretizations, as required by the functional nature of this problem. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We provide an illustration in Figure 3. As the true posterior is intractable, we compare against preconditioned Crank-Nicolson $({\\mathrm{pCN}})$ [Cotter et al., 2013], a function-space MCMC method. In Figure 3, we plot the mean posteriors obtained from the various methods. Qualitatively, both COT-FM and FFM are good approximations to pCN, while WaMGAN has visual artifacts. However, the MSE between our method and the pCN mean is lower than that of FFM. Table 3 provides a quantitative comparison between the methods on a test set of 5,000 samples, where we measure MSE and CRPS [Hersbach, 2000]. We compare the ensemble mean of 10 samples against the true $u$ value as running pCN for each observation is prohibitively expensive. COT-FM outperforms FFM and WaMGAN in terms of MSE and is on-par with FFM in terms of CRPS. See Appendix F for further details. ", "page_idx": 8}, {"type": "image", "img_path": "tk0uaRynhH/tmp/bc2c69e9a63ac30eb2c7b637909d177dc1e546231924d19c537a352879fe7319.jpg", "img_caption": ["Figure 3: Darcy flow illustration. Several true permeability fields $u$ are shown, as well as the pressure field $\\rho$ and its observed, noisy version $y$ . We compare an ensemble average of posterior samples from the various methods against MCMC (pCN) [Cotter et al., 2013]. COT-FM achieves the lowest MSE to pCN. We note here that WaMGAN has clear visual artifacts despite achieving reasonable MSE and CRPS scores. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We analyze conditional optimal transport from a geometric and dynamical point of view. Our analysis culminates in the characterization of absolutely continuous curves of measures in a conditional Wasserstein space, resulting in a conditional analog of the Benamou-Brenier Theorem. ", "page_idx": 9}, {"type": "text", "text": "We use these result to build on the framework of triangular transport and flow matching to develop simulation-free methods for conditional generative models. Our methods are applicable across a wide class of problems, and we demonstrate our methodology on several challenging inverse problems. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Broader Impacts. A limitation COT-FM is that computing the full COT plan can be expensive for large datasets, necessitating the use of minibatch approximations potentially resulting in sub-optimal plans. While this approximation does not limit the practical applicability of our method, an interesting challenge is to characterize the precise relationship between this minibatch approximation and the full COT plan. Moreover, computing the COT plan incurs a small additional computational cost compared to standard flow matching. As with all generative models, a potential negative impact is the potential for disinformation through generated samples being purported as real. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the Hasso Plattner Institute (HPI) Research Center in Machine Learning and Data Science at the University of California, Irvine, by the National Science Foundation under award 1900644, and by the National Institutes of Health under award R01-LM013344. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Oriol Abril-Pla, Virgile Andreani, Colin Carroll, Larry Dong, Christopher J Fonnesbeck, Maxim Kochurov, Ravin Kumar, Junpeng Lao, Christian C Luhmann, Osvaldo A Martin, et al. PyMC: A modern, and comprehensive probabilistic programming framework in python. PeerJ Computer Science, 9:e1516, 2023. ", "page_idx": 10}, {"type": "text", "text": "Jonas Adler and Ozan \u00d6ktem. Deep Bayesian inversion. arXiv preprint arXiv:1811.05910, 2018.   \nMichael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023a.   \nMichael S Albergo, Mark Goldstein, Nicholas M Boff,i Rajesh Ranganath, and Eric Vanden-Eijnden. Stochastic interpolants with data-dependent couplings. arXiv preprint arXiv:2310.03725, 2023b.   \nJason Alfonso, Ricardo Baptista, Anupam Bhakta, Noam Gal, Alfin Hou, Isa Lyubimova, Daniel Pocklington, Josef Sajonz, Giulio Trigila, and Ryan Tsai. A generative flow for conditional sampling via optimal transport. arXiv preprint arXiv:2307.04102, 2023.   \nMartin Aln\u00e6s, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris Richardson, Johannes Ring, Marie E Rognes, and Garth N Wells. The FEniCS project version 1.5. Archive of numerical software, 3(100), 2015.   \nLuigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00e9. Gradient Flows: In Metric Spaces and in the Space of Probability Measures. Springer Science & Business Media, 2005.   \nLuigi Ambrosio, Alberto Bressan, Dirk Helbing, Axel Klar, Enrique Zuazua, Luigi Ambrosio, and Nicola Gigli. A user\u2019s guide to optimal transport. Modelling and Optimisation of Flows on Networks: Cetraro, Italy 2009, Editors: Benedetto Piccoli, Michel Rascle, pages 1\u2013155, 2013.   \nBrandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference on Machine Learning, pages 146\u2013155. PMLR, 2017.   \nBrandon Amos et al. Tutorial on amortized optimization. Foundations and Trends in Machine Learning, 16(5): 592\u2013732, 2023.   \nSanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? Some theory and empirics. In International Conference on Learning Representations, 2018.   \nRicardo Baptista, Bamdad Hosseini, Nikola B Kovachki, and Youssef Marzouk. Conditional sampling with monotone GANs: from generative models to likelihood-free inference. arXiv preprint arXiv:2006.06755, 2020.   \nRicardo Baptista, Youssef Marzouk, and Olivier Zahm. On the representation and learning of monotone triangular transport maps. Foundations of Computational Mathematics, pages 1\u201346, 2023.   \nRapha\u00ebl Barboni, Gabriel Peyr\u00e9, and Fran\u00e7ois-Xavier Vialard. Understanding the training of infinitely deep and wide ResNets with conditional optimal transport. arXiv preprint arXiv:2403.12887, 2024.   \nMark A Beaumont. Approximate Bayesian computation in evolution and ecology. Annual review of ecology, evolution, and systematics, 41:379\u2013406, 2010.   \nJean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, 2000.   \nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.   \nVladimir Igorevich Bogachev and Maria Aparecida Soares Ruas. Measure Theory, volume 2. Springer, 2007.   \nCajo JF Ter Braak. A Markov chain Monte Carlo version of the genetic algorithm differential evolution: easy bayesian computing for real parameter spaces. Statistics and Computing, 16:239\u2013249, 2006.   \nCharlotte Bunne, Andreas Krause, and Marco Cuturi. Supervised training of conditional monge maps. Advances in Neural Information Processing Systems, 35:6859\u20136872, 2022.   \nGuillaume Carlier, Alfred Galichon, and Filippo Santambrogio. From knothe\u2019s transport to Brenier\u2019s map and a continuation method for optimal transport. SIAM Journal on Mathematical Analysis, 41(6):2554\u20132576, 2010.   \nGuillaume Carlier, Victor Chernozhukov, and Alfred Galichon. Vector quantile regression: An optimal transport approach. The Annals of Statistics, 44(3):1165 \u2013 1192, 2016. doi: 10.1214/15-AOS1401. URL https: //doi.org/10.1214/15-AOS1401.   \nJannis Chemseddine, Paul Hagemann, and Christian Wald. Y-Diagonal couplings: Approximating posteriors with conditional Wasserstein distances. arXiv preprint arXiv:2310.13433, 2023.   \nJannis Chemseddine, Paul Hagemann, Christian Wald, and Gabriele Steidl. Conditional Wasserstein distances with applications in Bayesian OT flow matching. arXiv preprint arXiv:2403.18705, 2024.   \nS. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White. MCMC methods for functions: Modifying old algorithms to make them faster. Statistical Science, 28(3):424 \u2013 446, 2013.   \nKyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy of Sciences, 117(48):30055\u201330062, 2020.   \nGiuseppe Da Prato and Jerzy Zabczyk. Stochastic Equations in Infinite Dimensions. Cambridge University Press, 2014.   \nMasoumeh Dashti and Andrew M Stuart. The Bayesian approach to inverse problems. arXiv preprint arXiv:1302.6989, 2013.   \nAram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23263\u2013 23274, 2023.   \nR\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur\u00e9lie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L\u00e9o Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. POT: Python optimal transport. Journal of Machine Learning Research, 22(78):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-451.html.   \nGiulio Franzese, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, and Pietro Michiardi. Continuous-time functional diffusion processes. Advances in Neural Information Processing Systems, 36, 2024.   \nTimothy D Gebhard, Jonas Wildberger, Maximilian Dax, Daniel Angerhausen, Sascha P Quanz, and Bernhard Sch\u00f6lkopf. Inferring atmospheric properties of exoplanets with flow matching and neural importance sampling. arXiv preprint arXiv:2312.08295, 2023.   \nNicola Gigli. On the geometry of the space of probability measures in Rn endowed with the quadratic optimal transport distance. PhD thesis, Scuola Normale Superiore, 2008.   \nHans Hersbach. Decomposition of the continuous ranked probability score for ensemble prediction systems. Weather and Forecasting, 15(5):559\u2013570, 2000.   \nBamdad Hosseini, Alexander W Hsu, and Amirhossein Taghvaei. Conditional optimal transport on function spaces. arXiv preprint arXiv:2311.05672, 2023.   \nNoboru Isobe, Masanori Koyama, Kohei Hayashi, and Kenji Fukumizu. Extended flow matching: a method of conditional generation with generalized continuity equation. arXiv preprint arXiv:2402.18839, 2024.   \nGavin Kerrigan, Justin Ley, and Padhraic Smyth. Diffusion generative models in infinite dimensions. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 9538\u20139563. PMLR, 2023.   \nGavin Kerrigan, Giosue Migliorini, and Padhraic Smyth. Functional flow matching. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 3934\u20133942, 2024.   \nYoung-geun Kim, Kyungbok Lee, and Myunghee Cho Paik. Conditional wasserstein generator. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \nYoung-geun Kim, Kyungbok Lee, Youngwon Choi, Joong-Ho Won, and Myunghee Cho Paik. Wasserstein geodesic generator for conditional distributions. arXiv preprint arXiv:2308.10145, 2023.   \nG\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. Advances in neural information processing systems, 30, 2017.   \nNikola B. Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. CoRR, abs/2108.08481, 2021.   \nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020.   \nJae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaif,i Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based diffusion models in function space. arXiv preprint arXiv:2302.07400, 2023.   \nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2022.   \nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.   \nRobert J McCann. A convexity principle for interacting gases. Advances in Mathematics, 128(1):153\u2013179, 1997.   \nMehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.   \nGeorge Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows. In The 22nd international conference on artificial intelligence and statistics, pages 837\u2013848. PMLR, 2019.   \nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57): 1\u201364, 2021.   \nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \nAram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023.   \nPoornima Ramesh, Jan-Matthis Lueckmann, Jan Boelts, \u00c1lvaro Tejero-Cantero, David S Greenberg, Pedro J Gon\u00e7alves, and Jakob H Macke. Gatsbi: Generative adversarial training for simulation-based inference. arXiv preprint arXiv:2203.06481, 2022.   \nDeep Ray, Harisankar Ramaswamy, Dhruv V Patel, and Assad A Oberai. The efficacy and generalizability of conditional gans for posterior inference in physics-based inverse problems. arXiv preprint arXiv:2202.07773, 2022.   \nMehdi SM Sajjadi, Bernhard Scholkopf, and Michael Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. In Proceedings of the IEEE international conference on computer vision, pages 4491\u20134500, 2017.   \nFilippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-63):94, 2015.   \nLouis Sharrock, Jack Simons, Song Liu, and Mark Beaumont. Sequential neural score estimation: Likelihoodfree inference with conditional score based diffusion models. arXiv preprint arXiv:2210.04872, 2022.   \nAlessio Spantini, Ricardo Baptista, and Youssef Marzouk. Coupling techniques for nonlinear ensemble flitering. SIAM Review, 64(4):921\u2013953, 2022.   \nAlexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.   \nGiulio Trigila and Esteban G Tabak. Data-driven optimal transport. Communications on Pure and Applied Mathematics, 69(4):613\u2013648, 2016.   \nC\u00e9dric Villani et al. Optimal Transport: Old and New, volume 338. Springer, 2009.   \nZheyu Oliver Wang, Ricardo Baptista, Youssef Marzouk, Lars Ruthotto, and Deepanshu Verma. Efficient neural network approaches for conditional optimal transport with applications in Bayesian inference. arXiv preprint arXiv:2310.16975, 2023.   \nJonas Wildberger, Maximilian Dax, Simon Buchholz, Stephen Green, Jakob H Macke, and Bernhard Sch\u00f6lkopf. Flow matching for scalable simulation-based inference. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix: Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B Conditional Optimal Transport ", "page_idx": 13}, {"type": "text", "text": "C Closed-Form Conditional Wasserstein Distance for Gaussian Measures 16 ", "page_idx": 13}, {"type": "text", "text": "D.1 Metric Properties 17   \nD.2 Geodesics 19   \nE.1 Continuity Equation . . 21   \nE.2 Absolutely Continuous Curves 22 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "F Experiment Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "F.1 2D Synthetic Data . . 26   \nF.2 Lotka-Volterra Dynamical System 26   \nF.3 Inverse Darcy Flow 33 ", "page_idx": 13}, {"type": "text", "text": "G Minibatch COT ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "34 ", "page_idx": 13}, {"type": "text", "text": "A Optimal Transport ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide here a brief and informal overview of optimal transport in the standard unconditional setting. For more details, we refer to the standard references of Villani et al. [2009], Santambrogio [2015], Ambrosio et al. [2005]. Let $X$ be a separable metric space and fix a cost function $c:$ $X\\times X\\to\\mathbb{R}\\cup\\{+\\infty\\}$ . Suppose we have two Borel measures $\\bar{\\eta},\\bar{\\nu}\\in\\mathbb{P}(X)$ . The Monge problem seeks to find a measurable transport map $T:X\\to X$ minimizing the expected cost of transport, i.e. corresponding to the optimization problem ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T}\\left\\{\\int_{X}c(x,T(x))\\,\\mathrm{d}\\eta(x)\\mid T_{\\#}\\eta=\\nu\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This optimization problem is challenging, though, as it involves a nonlinear constraint and the set of feasible maps may be empty. In contrast, the Kantorovich problem is a relaxation which seeks to find an optimal coupling $\\gamma\\in\\Pi(\\eta,\\nu)$ , i.e. a probability distribution over $X\\times X$ with marginals $\\eta,\\nu$ , which solves ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma}\\left\\{\\int_{X\\times X}c(x_{0},x_{1})\\,\\mathrm{d}\\gamma(x_{0},x_{1})\\mid\\gamma\\in\\Pi(\\eta,\\nu)\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Under fairly weak conditions (e.g., the cost is lower semicontinuous and bounded from below [Ambrosio et al., 2013, Theorem 2.5]), minimizers to the Kantorovich problem are guaranteed to exist. If the cost function is $c(x_{0},x_{1})=|x_{0}-x_{1}|^{p}$ for some $1<p<\\infty$ , under sufficient regularity conditions on $\\eta$ a solution $T^{\\star}$ to the Monge problem is guaranteed to exist and, moreover, the coupling $\\gamma^{\\star}=(I,T^{\\star})_{\\#}\\eta$ is optimal for the Kantorovich problem. See Ambrosio et al. [2013, Chapter 2] and Ambrosio et al. [2005, Theorem 6.2.10]. ", "page_idx": 13}, {"type": "text", "text": "Wasserstein Space. In the special case that $c(x_{0},x_{1})=|x_{0}-x_{1}|^{p}$ for $1\\leq p<\\infty$ , and $\\eta,\\nu\\in$ $\\mathbb{P}_{p}(X)$ , the Kantorovich problem admits a finite-cost solution. The cost of such an optimal coupling is the $p$ -Wasserstein distance ", "page_idx": 14}, {"type": "equation", "text": "$$\nW_{p}^{p}(\\eta,\\nu)=\\operatorname*{min}_{\\gamma}\\left\\{\\int_{X\\times X}|x_{0}-x_{1}|^{p}\\,\\mathrm{d}\\gamma(x_{0},x_{1})\\mid\\gamma\\in\\Pi(\\eta,\\nu)\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which, as the name suggests, is a metric on the space $\\mathbb{P}_{p}(X)$ [Ambrosio et al., 2005, Section 7.1] [Santambrogio, 2015, Section 5.1]. The Wasserstein distance admits a dynamical formulation via the Benamou-Brenier theorem [Benamou and Brenier, 2000]. Namely, the $p$ -Wasserstein distance can be obtained by finding a time-dependent vector field transforming $\\eta$ to $\\nu$ across time $t\\in[0,1]$ with minimal energy: ", "page_idx": 14}, {"type": "equation", "text": "$$\nW_{p}(\\eta,\\nu)=\\operatorname*{min}_{(\\gamma_{t},v_{t})}\\left\\{\\int_{0}^{1}\\int_{X}|v_{t}(x)|^{p}\\,\\mathrm{d}\\gamma_{t}(x)\\,\\mathrm{d}t\\ |\\ \\gamma_{0}=\\eta,\\gamma_{1}=\\nu,\\partial_{t}\\gamma_{t}+\\mathrm{div}(v_{t}\\gamma_{t})=0\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, we constrain our minimization problem over the set of measures and vector fields $(\\gamma_{t},v_{t})$ interpolating between $\\eta$ and $\\nu$ , satisfying a continuity equation (see Section 5). In Section 4, we study a generalization of the Wasserstein distances for conditional optimal transport problems. In particular, Theorem 5 provides a generalization of the Benamou-Brenier theorem to the conditional setting which recovers a conditional Wasserstein distance. ", "page_idx": 14}, {"type": "text", "text": "B Conditional Optimal Transport ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section contains additional discussion regarding the static COT problem, supplementing Section 3.1. We refer to Hosseini et al. [2023], Baptista et al. [2020], and Chemseddine et al. [2023] for further results and details. ", "page_idx": 14}, {"type": "text", "text": "Given a source and target measures $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ , the conditional Monge problem seeks to find a triangular mapping solving ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{T}\\left\\{\\int_{Y\\times U}c(y,u,T(y,u))\\,\\mathrm{d}\\eta(y,u)\\ |\\ T_{\\#}\\eta=\\nu,T:(y,u)\\mapsto(y,T_{U}(y,u))\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The conditional Monge problem also admits a relaxation under which one only considers couplings whose $Y$ -components are almost surely equal. To that end, we consider the subset $\\mathcal{C}\\subset(Y\\tilde{\\times U})^{2}$ whose $Y$ components are identical, i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{C}:=\\left\\{(y_{0},u_{0},y_{1},u_{1})\\in(Y\\times U)^{2}\\mid y_{0}=y_{1}\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and we define the set of $(Y)$ -restricted probability measures $\\mathcal{R}_{Y}\\subset\\mathbb{P}\\left((Y\\times U)^{2}\\right)$ such that every $\\gamma\\in\\mathcal{R}_{Y}$ is concentrated on $\\mathcal{C}$ . In other words, if $\\gamma\\in\\mathcal{R}_{Y}$ , then samples $(y_{0},u_{0},\\dot{y}_{1},u_{1})\\sim\\gamma$ have $y_{0}=y_{1}$ almost surely. In addition, for any $\\eta,\\nu\\in\\mathbb{P}(Y\\times U)$ , we define the set of triangular couplings $\\Pi_{Y}(\\eta,\\nu)$ to be the probability measures in $\\mathcal{R}_{Y}$ whose marginals are $\\eta$ and $\\nu$ , i.e. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Pi_{Y}(\\eta,\\nu)=\\left\\{\\gamma\\in\\mathcal{R}_{Y}\\ |\\ \\pi_{\\#}^{1,2}\\gamma=\\eta,\\pi_{\\#}^{3,4}\\gamma=\\nu\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The conditional Kantorovich problem seeks a triangular coupling $\\gamma^{\\star}$ solving ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma}\\left\\{\\int_{(Y\\times U)^{2}}c(y_{0},u_{0},y_{1},u_{1})\\,\\mathrm{d}\\gamma(y_{0},u_{0},y_{1},u_{1})\\mid\\gamma\\in\\Pi_{Y}(\\eta,\\nu)\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hosseini et al. [2023] prove the existence of minimizers to the conditional Kantorovich and Monge problems under very general assumptions. Moreover, optimal couplings to the conditional Kantorovich problem induce optimal couplings for $\\mu$ -almost every conditional measure. Assuming sufficient regularity assumptions on the conditional measures, unique solutions to the conditional Monge problem exist. We restate these results here for the sake of completeness. ", "page_idx": 14}, {"type": "text", "text": "Proposition 3 (Prop 3.3 [Hosseini et al., 2023]) ", "page_idx": 15}, {"type": "text", "text": "$F i x\\;\\eta,\\nu\\in\\mathbb{P}^{\\mu}(Y\\times U)$ . Suppose the cost function $c$ is continuous, inf $c>-\\infty,$ , and there exists a finite cost coupling $\\gamma\\in\\Pi_{Y}(\\eta,\\nu)$ . Then, the conditional Kantorovich problem admits a minimizer $\\gamma^{\\star}$ . Moreover, $\\gamma^{\\star,y_{0}}(y_{1},u_{0},u_{1})=\\hat{\\gamma}^{\\star,y_{0}}(u_{0},u_{1})\\delta(y_{1}-y_{0})$ where for $\\mu$ -almost every $y$ the measure $\\gamma^{\\star,y}$ is an optimal coupling for $\\eta^{y},\\nu^{y}$ under the cost $c^{y}(u_{0},u_{1})=c(y,u_{0},y,u_{1})$ ", "page_idx": 15}, {"type": "text", "text": "Proposition 4 (Prop 3.8 [Hosseini et al., 2023]) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1 $\\7i x\\ 1<\\,p\\,<\\,\\infty$ and $\\eta,\\nu\\,\\in\\,\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . Suppose $c(y_{0},u_{0},y_{1},u_{1})\\,=\\,|u_{0}\\,-\\,u_{1}|^{p}$ . If $\\eta^{y}$ assign zero measure to Gaussian null sets for $\\mu$ -almost every $y$ , then there is a unique solution $T^{\\star}$ to the conditional Monge problem, and $\\gamma^{\\star}\\,=\\,(I,T^{\\star})_{\\#}\\eta$ is the unique solution to the conditional Kantorovich problem. If $\\nu^{y}$ also assign zero measure to Gaussian null sets for $\\mu$ -almost every $y$ , then $T^{\\star}$ is injective $\\eta$ -almost everywhere. ", "page_idx": 15}, {"type": "text", "text": "C Closed-Form Conditional Wasserstein Distance for Gaussian Measures ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide additional details and results regarding the closed-form conditional Wasserstein distance for Gaussian distributions. See Section 4 in the main paper. ", "page_idx": 15}, {"type": "text", "text": "Suppose $Y=\\mathbb{R}^{d}$ and $U=\\mathbb{R}^{d^{\\prime}}$ are Euclidean spaces (of possibly different dimensions), and that $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ are Gaussians of the form ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\eta=\\mathcal{N}\\left(\\left[\\!\\!\\begin{array}{l}{m}\\\\ {m_{u}^{\\eta}}\\end{array}\\!\\!\\right],\\left[\\!\\!\\begin{array}{l l}{\\Sigma}&{\\Lambda^{\\eta}}\\\\ {\\Lambda^{\\eta^{T}}}&{\\Sigma_{u}^{\\eta}}\\end{array}\\!\\!\\right]\\right)\\qquad\\nu=\\mathcal{N}\\left(\\left[\\!\\!\\begin{array}{l}{m}\\\\ {m_{u}^{\\nu}}\\end{array}\\!\\!\\right],\\left[\\!\\!\\begin{array}{l l}{\\Sigma}&{\\Lambda^{\\nu}}\\\\ {\\Lambda^{\\nu^{T}}}&{\\Sigma_{u}^{\\nu}}\\end{array}\\!\\!\\right]\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $m\\in\\mathbb{R}^{d},m_{u}^{\\eta},m_{u}^{\\nu}\\in\\mathbb{R}^{d^{\\prime}}$ , and the (block) covariance matrices are $\\Sigma\\in\\mathbb{R}^{d\\times d},\\Lambda^{\\eta},\\Lambda^{\\nu}\\in\\mathbb{R}^{d\\times d^{\\prime}}$ and $\\Sigma_{u}^{\\eta},\\Sigma_{u}^{\\nu}\\in\\mathbb{R}^{d^{\\prime}\\times\\mathrm{{d^{\\prime}}}}$ . ", "page_idx": 15}, {"type": "text", "text": "This form is chosen to ensure that \u03b7 and \u03bd have equal Y -marginals. It follows that \u00b5 = \u03c0Y#\u03b7 $\\pi_{\\#}^{Y}\\nu=\\mathcal{N}(m,\\Sigma)$ . Let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{Q^{\\eta}=\\Sigma_{u}^{\\eta}-\\Lambda^{\\eta^{T}}\\Sigma^{-1}\\Lambda^{\\eta}}&&{Q^{\\nu}=\\Sigma_{u}^{\\nu}-\\Lambda^{\\nu^{T}}\\Sigma^{-1}\\Lambda^{\\nu}}&&{R=(\\Lambda^{\\eta}-\\Lambda^{\\nu})^{T}\\Sigma^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have that the conditionals $\\eta^{y},\\nu^{y}$ are available in closed-form: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\eta^{y}=\\mathcal{N}\\left(m_{u}^{\\eta}+{\\Lambda^{\\eta^{T}}}\\Sigma^{-1}(y-m),Q^{\\eta}\\right)\\qquad\\nu^{y}=\\mathcal{N}\\left(m_{u}^{\\nu}+{\\Lambda^{\\nu^{T}}}\\Sigma^{-1}(y-m),Q^{\\nu}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, for any fixed $y$ , we use the known closed-form unconditional Wasserstein distance to obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\nW_{2}^{2}(\\eta^{y},\\nu^{y})=\\bigr|m_{u}^{\\eta}-m_{u}^{\\nu}+R(y-m)\\bigr|^{2}+\\mathrm{Tr}\\left(Q^{\\eta}+Q^{\\nu}-2\\left((Q^{\\eta})^{1/2}Q^{\\nu}(Q^{\\eta})^{1/2}\\right)^{1/2}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We now take an expectation over $y\\sim\\mu={\\mathcal{N}}(m,\\Sigma)$ to compute $W_{2}^{\\mu,2}$ . Observe that $R(y-m)\\sim$ ${\\mathcal{N}}(0,R\\Sigma R^{\\mathsf{T}})$ and that $\\mathbb{E}_{y\\sim\\mu}[|R(y-m)|^{2}]=\\operatorname{Tr}(R\\Sigma R^{\\top})$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{\\mu,2}(\\eta,\\nu)=\\mathbb{E}_{y\\sim\\mu}\\left[W_{2}^{2}(\\eta^{y},\\nu^{y})\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{y\\sim\\mu}\\left[|m_{u}^{\\eta}-m_{u}^{\\nu}|^{2}+2\\langle m_{u}^{\\eta}-m_{u}^{\\nu},R(y-m)\\rangle+|R(y-m)|^{2}\\right]}\\\\ &{\\qquad\\qquad\\quad+\\operatorname{Tr}\\left(Q^{\\eta}+Q^{\\nu}-2\\left((Q^{\\eta})^{1/2}Q^{\\nu}(Q^{\\eta})^{1/2}\\right)^{1/2}\\right)}\\\\ &{\\qquad\\qquad=|m_{u}^{\\eta}-m_{u}^{\\nu}|^{2}+\\operatorname{Tr}\\left(Q^{\\eta}+Q^{\\nu}-2\\left((Q^{\\eta})^{1/2}Q^{\\nu}(Q^{\\eta})^{1/2}\\right)^{1/2}+R\\Sigma R^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This form, perhaps unsurprisingly, closely resembles the unconditional Wasserstein distance between two Gaussians, except for the presence of an additional $\\mathrm{Tr}(R\\Sigma R^{\\mathsf{T}})$ term. Note that when $\\eta,\\nu$ have uncorrelated $Y,U$ components, we precisely recover $W_{2}^{2}(\\pi_{\\#}^{U}\\eta,\\pi_{\\#}^{U}\\nu)$ as one may expect. As a special case of interest, if $Y=U=\\mathbb{R}$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\eta=\\mathcal{N}(0,I)\\qquad\\nu=\\mathcal{N}\\left(0,\\left[\\!\\!\\begin{array}{c c}{{1}}&{{\\rho}}\\\\ {{\\rho}}&{{1}}\\end{array}\\!\\!\\right]\\right)\\quad|\\rho|<1\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then we obtain as a special case of Equation (26) that $W_{2}^{\\mu,2}(\\eta,\\nu)=2(1-\\sqrt{1-\\rho^{2}})$ . This is zero if and only if $\\rho=0$ , i.e. $\\eta=\\nu$ . ", "page_idx": 16}, {"type": "text", "text": "D Proofs: Section 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide detailed proofs of our claims in Section 4, regarding the metric properties of the conditional Wasserstein space. ", "page_idx": 16}, {"type": "text", "text": "D.1 Metric Properties ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first note that $W_{p}^{\\mu}(\\eta,\\nu)$ may be viewed as the minimal value of the constrained Kantorovich problem in Equation (3) when one takes the cost to be the metric on the space $Y\\times U$ . Similar results, relating the conditional Wasserstein distance to triangular couplings, have appeared previously, but our proof is independent of these prior works [Chemseddine et al., 2023, Gigli, 2008]. ", "page_idx": 16}, {"type": "text", "text": "Proposition 5 (Equivalent Formulation of the Conditional Wasserstein Distance) $F i x\\;\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ and $1\\leq p<\\infty$ . Then, $W_{p}^{\\mu}(\\eta,\\nu)$ is well-defined, finite, and ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{p}^{\\mu,p}(\\eta,\\nu)=\\operatorname*{min}_{\\gamma}\\left\\{\\int_{(Y\\times U)^{2}}d^{p}(y_{0},u_{0},y_{1},u_{1})\\,\\mathrm{d}\\gamma\\mid\\gamma\\in\\Pi_{Y}(\\eta,\\nu)\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $W_{p}^{\\mu,p}(\\eta,\\nu)$ represents the $p$ -th power of the conditional $p$ -Wasserstein distance. ", "page_idx": 16}, {"type": "text", "text": "Proof. The cost function $d^{p}$ is clearly continuous and non-negative, and hence by Proposition 3 it suffices to exhibit a finite-cost coupling $\\gamma\\in\\Pi_{Y}(\\eta,\\nu)$ between $\\eta$ and $\\nu$ . Indeed, take the conditionally independent coupling ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\gamma(y_{0},u_{0},y_{1},u_{1})=\\eta(u_{0}\\mid y_{1})\\nu(u_{1}\\mid y_{1})\\delta(y_{1}-y_{0})\\mu(y_{1})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is clearly in $\\Pi_{Y}(\\eta,\\nu)$ . We then have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{(Y\\times U)^{2}}d^{p}(y_{0},u_{0},y_{1},u_{1})\\,\\mathrm{d}\\gamma(y_{0},u_{0},y_{1},u_{1})=\\int_{(Y\\times U)^{2}}\\|(y_{0},u_{0})-(y_{1},u_{1})\\|_{Y\\times U}^{p}\\,\\,\\mathrm{d}\\gamma(y_{0},u_{0},y_{1},u_{1})}\\\\ &{\\le2^{p}\\displaystyle\\int_{(Y\\times U)^{2}}\\left(\\|(y_{0},u_{0})\\|_{Y\\times U}^{p}+\\|(y_{1},u_{1})\\|_{Y\\times U}^{p}\\right)\\,\\mathrm{d}\\gamma(y_{0},u_{0},y_{1},u_{1})}\\\\ &{=2^{p}\\left(\\displaystyle\\int_{Y\\times U}\\|(y_{0},u_{0})\\|_{Y\\times U}^{p}\\,\\,\\mathrm{d}\\eta(y_{0},u_{0})+\\displaystyle\\int_{Y\\times U}\\|(y_{1},u_{1})\\|_{Y\\times U}^{p}\\,\\,\\mathrm{d}\\nu(y_{1},u_{1})\\right)<+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, Equation (30) admits a minimizer $\\gamma^{\\star}\\in\\Pi_{Y}(\\eta,\\nu)$ . By Proposition 3, this minimizer may be taken to have the form $\\gamma^{\\star}=\\gamma^{\\star,y_{1}}(u_{0},u_{1})\\delta(y_{1}-y_{0})\\mu(y_{1})$ where $\\gamma^{\\star,y_{1}}\\bigl(u_{0},u_{1}\\bigr)$ is $\\mu(y_{1})$ -almost surely an optimal coupling between $\\eta^{y_{1}},\\nu^{y_{1}}$ for the cost $|u_{1}-u_{0}|^{p}$ . Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{(Y\\times U)^{2}}d^{p}\\,\\mathrm{d}\\gamma^{\\star}=\\int_{Y}\\int_{U^{2}}|u_{1}-u_{0}|^{p}\\,\\mathrm{d}\\gamma^{\\star,y}(u_{0},u_{1})\\,\\mathrm{d}\\mu(y)}\\\\ {\\displaystyle\\qquad\\qquad=\\int_{Y}W_{p}^{p}(\\eta^{y},\\nu^{y})\\,\\mathrm{d}\\mu(y)=W_{p}^{p,\\mu}(\\eta,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, we emphasize that the $\\mu$ -almost sure uniqueness of the disintegrations of $\\eta,\\nu$ along $Y$ result in a well-defined expression. ", "page_idx": 16}, {"type": "text", "text": "Moreover, if $\\eta\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ it follows that $\\eta^{y}\\in\\mathbb{P}_{p}(U)$ for $\\mu$ -a.e. $y$ , because ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{Y}\\int_{U}|u|^{p}\\,\\mathrm{d}\\eta^{y}(u)\\,\\mathrm{d}\\mu(y)\\le\\displaystyle\\int_{Y}\\int_{U}|(y,u)|^{p}\\,\\mathrm{d}\\eta^{y}(u)\\,\\mathrm{d}\\mu(y)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\int_{Y\\times U}|(y,u)|^{p}\\,\\mathrm{d}\\eta(y,u)<+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus all considered $p$ -Wasserstein distances on $U$ are finite. ", "page_idx": 16}, {"type": "text", "text": "We now proceed to prove several metric properties of our distance. ", "page_idx": 17}, {"type": "text", "text": "Proposition 2 (Some Properties of $W_{p}^{\\mu}$ ) Let $1\\leq p<\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "(a) $W_{p}^{\\mu}$ is well-defined, finite, and equals the minimal conditional Kantorovich cost.   \n(b) $W_{p}^{\\mu}$ is a metric on the space $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ .   \n(c) There does not exist $C>0$ such that $W_{p}^{\\mu}(\\eta,\\nu)\\leq C W_{p}(\\eta,\\nu)$ for all $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ .   \n(d) For all $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U),\\,W_{p}\\left(\\pi_{\\#}^{U}\\eta,\\pi_{\\#}^{U}\\nu\\right)\\leq W_{p}^{\\mu}(\\eta,\\nu)\\;a n d$ $W_{p}(\\eta,\\nu)\\leq W_{p}^{\\mu}(\\eta,\\nu)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Part (a). This is simply a restatement of Proposition 5. ", "page_idx": 17}, {"type": "text", "text": "Part (b). Fix $\\eta,\\nu,\\rho\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . Since $W_{p}$ is a metric on $\\mathbb{P}_{p}(U)$ , we immediately obtain the symmetry of $W_{p}^{\\mu}$ . Moreover, we have that $W_{p}^{\\mu}(\\eta,\\nu)=0$ if and only if $\\eta^{y}=\\nu^{y}$ for $\\mu$ -almost every $y$ . Thus, if $W_{p}^{\\mu}(\\eta,\\nu)=0$ and $E\\subseteq Y\\times U$ is Borel measurable, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta(E)=\\int_{Y}\\eta^{y}(E^{y})\\,\\mathrm{d}\\mu(y)=\\int_{Y}\\nu^{y}(E^{y})\\,\\mathrm{d}\\mu(y)=\\nu(E).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which shows that $\\eta=\\nu$ . Here, $E^{y}=\\{u\\mid(y,u)\\in E\\}$ is the $y$ -slice of $E$ . Conversely, if $\\eta=\\nu$ , then $\\eta^{y}=\\nu^{y}$ up to a $\\mu$ -null set by the essential uniqueness of disintegrations. Thus, $W_{p}^{\\mu}(\\eta,\\nu)=0$ if and only if $\\eta=\\nu$ . ", "page_idx": 17}, {"type": "text", "text": "By Minkowski\u2019s inequality and the triangle inequality for $W_{p}$ on $\\mathbb{P}_{p}(U)$ , we see ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{p}^{\\mu}(\\eta,\\nu)\\leq(\\mathbb{E}_{y\\sim\\mu}\\left[(W_{p}(\\eta^{y},\\rho^{y})+W_{p}(\\rho^{y},\\nu^{y}))^{p}\\right])^{1/p}}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{y\\sim\\mu}[W_{p}^{p}(\\eta^{y},\\rho^{y})]^{1/p}+\\mathbb{E}_{y\\sim\\mu}[W_{p}^{p}(\\rho^{y},\\nu^{y})]^{1/p}}\\\\ &{\\qquad\\qquad=W_{p}^{\\mu}(\\eta,\\rho)+W_{p}^{\\mu}(\\rho,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Part (c). We provide a counterexample. Fix any $u_{0}\\neq0\\in U$ and $y_{0},y_{1}\\in Y$ such that $y_{0}\\neq y_{1}$ . Define $\\begin{array}{r}{\\mu={\\frac{1}{2}}\\,\\dot{(}\\delta_{y_{0}}+\\delta_{y_{1}})}\\end{array}$ . Set $u_{k}=(k+1)u_{0}$ for $k=1,2,\\dots$ and for each $k$ , define two measures on $Y\\times U$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{k}=\\frac{1}{2}\\left(\\delta_{y_{0}u_{0}}+\\delta_{y_{1}u_{k}}\\right)\\qquad\\nu_{k}=\\frac{1}{2}\\left(\\delta_{y_{1}u_{0}}+\\delta_{u_{k}y_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is clear that ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{p}^{\\mu,p}(\\eta_{k},\\nu_{k})=k^{p}|u_{0}|^{p}\\qquad W_{p}^{p}(\\eta_{k},\\nu_{k})=\\operatorname*{min}\\{k^{p}|u_{0}|^{p},|y_{1}-y_{0}|^{p}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, as $k\\rightarrow\\infty$ we have $W_{p}^{\\mu}(\\mu_{k},\\nu_{k})\\to\\infty$ but $W_{p}^{p}(\\nu_{k},\\eta_{k})$ remains bounded. See Figure 4. ", "page_idx": 17}, {"type": "text", "text": "Part (d). First, the unconditional distance $W_{p}(\\eta,\\nu)$ may be obtained via an unrestricted coupling in $\\Pi(\\eta,\\nu)$ , i.e. the set of all joint measures on $Y\\times U$ having marginals $\\eta,\\nu$ . Since $\\Pi(\\eta,\\nu)\\supseteq\\Pi_{Y}(\\eta,\\nu)$ , by part (a) we see that $\\bar{W_{p}}(\\eta,\\nu)\\leq W_{p}^{\\mu}(\\eta,\\nu)$ . ", "page_idx": 17}, {"type": "text", "text": "Let $\\gamma^{\\star}(y_{0},u_{0},y_{1},u_{1})\\,=\\,\\gamma^{\\star,y_{1}}(u_{0},u_{1})\\delta(y_{1}\\,-\\,y_{0})\\mu(y_{1})$ be an optimal $\\gamma_{.}^{\\star}\\in\\Pi_{Y}(\\eta,\\nu)$ . We claim that $\\begin{array}{r}{\\gamma(u_{0},u_{1})\\,:=\\,\\int_{Y}\\gamma^{\\star,y}(u_{0},u_{1})\\,\\mathrm{d}\\mu(y)}\\end{array}$ couples $\\pi_{\\#}^{U}\\eta$ and $\\pi_{\\#}^{U}\\bar{\\nu}$ . Let $\\pi^{0}\\,:\\,(u_{0},u_{1})\\,\\mapsto\\,u_{0}$ be the projection onto the first coordinate of $U\\times U$ . Observe that for $\\mu$ -almost every $y$ , we have that $\\gamma^{\\star,\\bar{y}}\\in\\Pi(\\eta^{y},\\nu^{y})$ is optimal, and, in particular, $\\pi_{\\#}^{0}\\gamma^{\\star,y}=\\eta^{y}$ . Fix an arbitrary $\\varphi\\in C_{b}(U)$ . We then have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{U}\\varphi(u_{0})\\,\\mathrm{d}\\pi_{\\#}^{0}\\gamma(u_{0})=\\int_{U^{2}}(\\varphi\\circ\\pi^{0})\\,\\mathrm{d}\\gamma(u_{0},u_{1})}\\\\ {\\displaystyle=\\int_{Y}\\int_{U^{2}}(\\varphi\\circ\\pi^{0})\\,\\mathrm{d}\\gamma^{\\star,y}(u_{0},u_{1})\\,\\mathrm{d}\\mu(y)=\\int_{Y}\\int_{U}\\varphi(u_{0})\\,\\mathrm{d}\\pi_{\\#}^{0}\\gamma^{\\star,y}(u_{0})\\,\\mathrm{d}\\mu(y)}\\\\ {\\displaystyle=\\int_{Y}\\int_{U}\\varphi(u_{0})\\,\\mathrm{d}\\eta^{y}(u_{0})\\,\\mathrm{d}\\mu(y)\\!=\\!\\int_{Y\\times U}\\varphi(u_{0})\\,\\mathrm{d}\\eta(u_{0},y)}\\\\ {\\displaystyle=\\int_{Y\\times U}(\\varphi\\circ\\pi^{U})\\,\\mathrm{d}\\eta(u_{0},y)=\\int_{U}\\varphi\\,\\mathrm{d}\\pi_{\\#}^{U}\\eta(u_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "tk0uaRynhH/tmp/a120b1416493253db662837224f1dfbcc378d9680e49beecf655748f24f8fb7f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 4: The counterexample in Proposition 2. The measure $\\eta_{k}$ is shown in black and the measure $\\nu_{k}$ is shown in white. ", "page_idx": 18}, {"type": "text", "text": "Thus \u03c0U#\u03b3 $\\pi_{\\#}^{1}\\gamma=\\pi_{\\#}^{U}\\nu$ $\\pi_{\\#}^{U}\\gamma\\;=\\;\\pi_{\\#}^{U}\\eta$ , so that \u03c0U#\u03b7. A similar argument shows that for the map \u03c01 : (u0, u1)  \u2192 u1 we have $\\gamma\\in\\Pi(\\pi_{\\#}^{U}\\eta,\\pi_{\\#}^{U}\\nu)$ . ", "page_idx": 18}, {"type": "text", "text": "Now, as $\\gamma^{\\star,y_{1}}(u_{0},u_{1})\\in\\Pi(\\eta^{y_{1}},\\nu^{y_{1}})$ is $\\mu$ -almost surely optimal in the usual Wasserstein sense, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{W_{p}^{p,\\mu}(\\eta,\\nu)=\\displaystyle\\int_{Y}\\int_{U^{2}}|u_{0}-u_{1}|^{p}\\,\\mathrm{d}\\gamma^{\\star,y}\\big(u_{0},u_{1}\\big)\\,\\mathrm{d}\\mu(y)}\\\\ {\\qquad\\qquad\\qquad=\\displaystyle\\int_{U^{2}}|u_{0}-u_{1}|^{p}\\,\\mathrm{d}\\gamma\\big(u_{0},u_{1}\\big)}\\\\ {\\qquad\\qquad\\geq W_{p}^{p}(\\pi_{\\#}^{U}\\eta,\\pi_{\\#}^{U}\\nu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since $\\gamma\\in\\Pi(\\pi_{\\#}^{U}\\eta,\\pi_{\\#}^{U}\\nu)$ is a coupling but potentially sub-optimal. ", "page_idx": 18}, {"type": "text", "text": "D.2 Geodesics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now study the geodesics in the space $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . ", "page_idx": 18}, {"type": "text", "text": "Theorem 1 $(\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is a Geodesic Space) For any $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U),$ , there exists a constant speed geodesic between $\\eta$ and $\\nu$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Write $\\lambda_{t}:(Y\\times U)^{2}\\to Y\\times U$ for the linear interpolant ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{t}(y_{0},u_{0},y_{1},u_{1})=(t y_{0}+(1-t)y_{1},t u_{0}+(1-t)u_{1})\\qquad0\\le t\\le1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\gamma^{\\star}\\in\\Pi_{Y}(\\eta,\\nu)$ be an optimal restricted coupling, and consider the path of measures in $\\mathbb{P}_{p}(Y\\times U)$ given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{t}=[\\lambda_{t}]_{\\#}\\gamma^{\\star}\\qquad0\\leq t\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step one: We check that for each $0\\le t\\le1$ , we have $\\gamma_{t}\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . That is, we need to check that for all Borel $A\\subseteq Y$ , we have $\\gamma_{t}(A\\times U)=\\mu(A)$ . Indeed, recall that restricted measures are concentrated on the set $\\mathcal{C}$ (see Equation (19)). Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{t}(A\\times U)=\\gamma^{\\star}\\left\\{\\lambda_{t}^{-1}(A\\times U)\\right\\}}\\\\ &{\\qquad\\qquad=\\gamma^{\\star}\\left\\{(y,u_{0},y,u_{1})\\mid y\\in A\\right\\}}\\\\ &{\\qquad\\quad=\\pi_{\\#}^{1}\\gamma^{\\star}(A)=(\\pi^{1}\\circ\\pi^{1,2})_{\\#}\\gamma^{\\star}(A)}\\\\ &{\\qquad\\quad=\\pi_{\\#}^{1}\\eta(A)=\\mu(A)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "i.e. $\\gamma_{t}(A\\times Y)=\\mu(A)$ as claimed. ", "page_idx": 18}, {"type": "text", "text": "Step two: We show that $W_{p}^{\\mu}(\\gamma_{t},\\gamma_{s})=|t-s|W_{p}^{\\mu}(\\eta,\\nu)$ . Set $\\gamma_{t}^{s}:=(\\lambda_{t},\\lambda_{s})_{\\#}\\gamma^{\\star}$ for $0\\leq s<t\\leq1$ . We claim $\\gamma_{t}^{s}\\in\\Pi_{Y}\\big(\\gamma_{t},\\gamma_{s}\\big)$ . Indeed, we have $\\pi_{\\#}^{1,2}\\gamma_{t}^{s}=\\gamma_{t}$ because for all Borel $A\\subseteq Y\\times U$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\lambda_{t},\\lambda_{s})_{\\#}\\gamma^{\\star}\\left(A\\times Y\\times U\\right)=\\gamma^{\\star}\\left(\\lambda_{t}^{-1}(A)\\right)=(\\lambda_{t})_{\\#}\\gamma^{\\ast}(A).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "An analogous calculation shows that $\\pi_{\\#}^{3,4}\\gamma_{t}^{s}\\,=\\,\\gamma_{s}$ , so that $\\gamma_{t}^{s}\\,\\in\\,\\Pi(\\gamma_{t},\\gamma_{s})$ . We now check that $\\gamma_{t}^{s}\\in\\mathcal{R}_{Y}(Y\\times U)$ . Indeed, suppose $E\\subseteq Y\\times U$ is a Borel set such that $E\\cap\\mathcal{C}=\\mathcal{O}$ . In other words, for every $(y_{0},u_{0},y_{1},u_{1})\\in E$ we have $y_{0}\\neq y_{1}$ . Set $D:=(\\lambda_{t},\\lambda_{s})^{-1}(E)$ . We claim $D\\cap\\mathcal{C}=\\mathcal{O}$ , so that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\gamma_{t}^{s}(E)=(\\lambda_{t},\\lambda_{s})_{\\#}\\gamma^{\\star}(E)=\\gamma^{\\star}((\\lambda_{t},\\lambda_{s})^{-1}(E))}\\\\ &{}&{=\\gamma^{\\star}(D\\cap\\mathcal{C})=0.\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Indeed, if $c=(y,u_{0},y,u_{1})\\in\\mathcal{C}$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\lambda_{t},\\lambda_{s})(c)=(y,t u_{0}+(1-t)u_{1},y,s u_{0}+(1-s)u_{1})\\notin E}\\\\ &{\\qquad\\qquad\\implies c\\notin(\\pi_{t},\\pi_{s})^{-1}(E).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus $\\gamma_{t}^{s}\\in\\Pi_{Y}(\\eta,\\nu)$ as claimed. Now, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{p}^{\\mu,p}(\\gamma_{t},\\gamma_{s})\\leq\\displaystyle\\int_{(Y\\times U)^{2}}d^{p}\\left(y_{0},u_{0},y_{1},u_{1}\\right)\\,\\mathrm{d}\\lambda_{t}^{s}(y_{0},u_{0},y_{1},u_{1})}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{(Y\\times U)^{2}}d^{p}\\left(\\lambda_{t}(y_{0},u_{0},y_{1},u_{1}),\\lambda_{s}(y_{0},u_{0},y_{1},u_{1})\\right)\\,\\mathrm{d}\\gamma^{\\star}(y_{0},u_{0},y_{1},u_{1})}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{(Y\\times U)^{2}}\\Big(|(t-s)(y_{0}-y_{1})|^{2}+|(t-s)(u_{0}-u_{1})|^{2}\\Big)^{p/2}\\,\\,\\mathrm{d}\\gamma^{\\star}(y_{0},u_{0},y_{1},u_{1})}\\\\ &{\\qquad\\qquad=|t-s|^{p}\\displaystyle\\int_{(Y\\times U)^{2}}d^{p}(y_{0},u_{0},y_{1},u_{1})\\,\\mathrm{d}\\gamma^{\\star}(y_{0},u_{0},y_{1},u_{1})}\\\\ &{\\qquad\\qquad=|t-s|^{p}W_{p}^{\\mu,p}(\\eta,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Conversely, an application of the previous inequality and the triangle inequality show that for $0\\leq s\\leq t\\leq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{p}^{\\mu}(\\eta,\\nu)\\leq W_{p}^{\\mu}(\\eta,\\gamma_{s})+W_{p}^{\\mu}(\\gamma_{s},\\gamma_{t})+W_{p}^{\\mu}(\\gamma_{t},\\nu)}\\\\ &{\\qquad\\qquad\\leq s W_{p}^{\\mu}(\\eta,\\nu)+W_{p}^{\\mu}(\\gamma_{s},\\gamma_{t})+(1-t)W_{p}^{\\mu}(\\eta,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Rearranging the previous inequality implies $|t-s|W_{p}^{\\mu}(\\eta,\\nu)\\leq W_{p}^{\\mu}(\\gamma_{s},\\gamma_{t})$ for all $s,t\\in[0,1]$ , and hence $W_{p}^{\\mu}(\\gamma_{t},\\gamma_{s})=|t-s|^{p}W_{p}^{\\mu}(\\eta,\\nu)$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Theorem 2 (Conditional McCann Interpolants) ", "page_idx": 19}, {"type": "text", "text": "$F i x\\;\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ . Suppose $T^{\\star}(y,u)\\,=\\,(y,T_{\\mathcal{U}}^{\\star}(y,u))$ is an injective triangular map solving the conditional Monge problem (1). Define the maps $T_{t}:Y\\times U\\to Y\\times U$ for $0\\le t\\le1$ via $T_{t}=(1-t)I+t T^{\\star}$ , and define the curve of measures $\\gamma_{t}=[T_{t}]_{\\#}\\eta\\in\\mathbb{P}_{p}^{\\gamma}(Y\\times U)$ . Then, ", "page_idx": 19}, {"type": "text", "text": "(a) $(\\gamma_{t})$ is absolutely continuous and a constant speed geodesic between $\\eta,\\nu$ ", "page_idx": 19}, {"type": "text", "text": "$(b)$ The vector field $v_{t}(T_{t}^{\\star}(y,u))=(0,T_{U}^{\\star}(y,u)-u)$ generates the path $\\gamma_{t}$ , in the sense that $(\\gamma_{t},v_{t})$ solve the continuity equation (9). ", "page_idx": 19}, {"type": "text", "text": "Proof. Consider the function $w_{t}:Y\\times U\\rightarrow U$ given by ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{t}(y,u)=(0,T_{U}^{\\star}(y,u)-u)=(0,w_{t,U}(y,u))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and note this is precisely $w_{t}(y,u)=\\partial_{t}T_{t}^{\\star}(y,u)$ . Define the vector field ", "page_idx": 19}, {"type": "equation", "text": "$$\nv_{t}(y,u)=\\left(w_{t}\\circ T_{t}^{\\star,-1}\\right)(y,u)=\\left(0,(w_{t,\\mathcal{U}}\\circ T_{t,\\mathcal{U}}^{\\star,-1})(y,u)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For any $\\varphi\\in\\mathrm{Cyl}(Y\\times U)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\displaystyle\\int_{Y\\times U}\\varphi(y,u)\\,\\mathrm{d}\\gamma_{t}(y,u)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\displaystyle\\int_{Y\\times U}\\varphi(y,u)\\,\\mathrm{d}[T_{t}]_{\\#}\\eta(y,u)}\\\\ &{\\phantom{m m m m m m}=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\displaystyle\\int_{Y\\times U}\\varphi(y,T_{t,U}^{\\star}(y,u))\\,\\mathrm{d}\\eta(y,u)}\\\\ &{\\phantom{m m m m m m}=\\int_{Y\\times U}\\langle\\nabla\\varphi(y,T_{t,U}^{\\star}(y,u),w_{t}(y,u))\\rangle\\,\\mathrm{d}\\eta(y,u)}\\\\ &{\\phantom{m m m m m m}=\\int_{Y\\times U}\\langle\\nabla\\varphi(y,u),v_{t}(y,u)\\rangle\\,\\mathrm{d}\\gamma_{t}(y,u)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which shows that $(\\gamma_{t},v_{t})$ solve the continuity equation. ", "page_idx": 20}, {"type": "text", "text": "Now, note that for $0\\leq a\\leq b\\leq1$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{a}^{b}\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\;\\mathrm{d}t=\\int_{a}^{b}\\left(\\int_{Y\\times U}\\left|w_{t}\\circ T_{t}^{*,-1}\\right|^{p}(y,u)\\,\\mathrm{d}\\gamma_{t}(y,u)\\right)^{1/p}\\,\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{a}^{b}\\left(\\int_{Y\\times U}|w_{t}|^{p}(y,u)\\,\\mathrm{d}\\eta(y,u)\\right)^{1/p}\\,\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{a}^{b}\\left(\\int_{Y\\times U}|u-T_{U}^{\\star}(y,u)|^{p}(y,u)\\,\\mathrm{d}\\eta(y,u)\\right)^{1/p}\\,\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(b-a)W_{p}^{\\mu}(\\eta,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, $\\begin{array}{r}{\\int_{0}^{1}\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\;\\mathrm{d}t\\;<\\;\\infty}\\end{array}$ and so by Theorem 4 $(\\gamma_{t})$ is absolutely continuous. A similar calculation shows that $\\begin{array}{r}{(b-a)W_{p}^{\\mu}(\\eta,\\nu)=W_{p}^{\\mu}(\\gamma_{b},\\gamma_{a})=\\int_{a}^{b}|\\gamma^{\\prime}(t)|}\\end{array}$ , where the last line follows from the absolute continuity of $\\gamma_{t}$ . Thus, $\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}=|\\gamma^{\\prime}|(t)$ for almost every $t\\in[0,1]$ by Lebesgue differentiation. ", "page_idx": 20}, {"type": "text", "text": "E Proofs: Section 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we provide proofs of all claims made in Section 5. ", "page_idx": 20}, {"type": "text", "text": "E.1 Continuity Equation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We begin with a lemma that is used in the proof of Theorem 4. Informally, a solution to the continuity equation with a triangular vector field will result in the conditional measures almost surely satisfying the continuity equation as well. ", "page_idx": 20}, {"type": "text", "text": "Lemma 1 (Triangular Vector Fields Preserve Conditionals) ", "page_idx": 20}, {"type": "text", "text": "Suppose $v_{t}(y,u)^{\\bullet}{=}\\,(0,v_{t}^{U}(y,u))$ is triangular and that $(\\gamma_{t})\\subset\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is a path of measures such that $(v_{t},\\gamma_{t})$ satisfy the continuity equation in the sense of distributions. Then, it follows that for $\\mu$ -almost every $y\\in Y$ , we have $\\partial_{t}\\gamma_{t}^{y^{\\prime}}+\\dot{\\nabla}\\cdot(v_{t}^{U}(y,-)\\gamma_{t}^{y})=\\dot{0}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Fix any $\\varphi\\,\\in\\,\\mathrm{Cyl}(U\\,\\times\\,I)$ . Suppose $\\psi\\,\\in\\,{\\mathrm{Cyl}}(Y)$ is given, and note that $\\psi(y)\\varphi(u,t)\\;\\in$ $\\operatorname{Cyl}(Y\\times U\\times I)$ . As $(v_{t},\\gamma_{t})$ solve the continuity equation, it follows from the triangular structure of $v_{t}$ that upon testing against $\\psi\\varphi$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{I}\\int_{Y}\\psi(y)\\int_{U}\\left(\\partial_{t}\\varphi(u,t)+\\langle v_{t}^{U}(y,u),\\nabla_{u}\\varphi(u,t)\\right)\\,\\mathrm{d}\\gamma_{t}^{y}(u)\\,\\mathrm{d}\\mu(y)\\,\\mathrm{d}t=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Because $\\psi(y)\\;\\in\\;\\mathrm{Cyl}(Y)$ , it is of the form $\\rho(\\pi(y))$ where $\\pi\\,:\\,Y\\,\\rightarrow\\,\\mathbb{R}^{k}$ for some $k\\ \\geq\\ 1$ and $\\rho\\in C_{c}^{\\infty}(\\mathbb{R}^{k})$ . Taking $\\rho$ to be a sequence of smooth approximations to the indicator function of an arbitrary rectangle $E=E_{1}\\times E_{2}\\times\\cdots\\times E_{k}\\subseteq\\mathbb{R}^{k}$ , we see ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{\\pi^{-1}(E)}\\int_{I}\\int_{U}\\left(\\partial_{t}\\varphi(u,t)+\\langle v_{t}^{U}(y,u),\\nabla_{u}\\varphi(u,t)\\right)\\,\\mathrm{d}\\gamma_{t}^{y}(u)\\,\\mathrm{d}t\\,\\mathrm{d}\\mu(y)=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As $Y$ is separable, the Borel $\\sigma$ -algebra on $Y$ is generated by the cylinder sets, i.e. those which are precisely of the form $\\pi^{-1}(E)$ for some finite-dimensional rectangle $E$ . We have thus shown that for an arbitrary Borel measurable set $E\\subseteq Y$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{E}\\int_{I}\\int_{U}\\big(\\partial_{t}\\varphi(u,t)+\\langle v_{t}^{U}(y,u),\\nabla_{u}\\varphi(u,t)\\big)\\ \\mathrm{d}\\gamma_{t}^{y}(u)\\,\\mathrm{d}t\\,\\mathrm{d}\\mu(y)=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From this, it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{I}\\int_{U}\\left(\\partial_{t}\\varphi(u,t)+\\langle v_{t}^{U}(y,u),\\nabla_{u}\\varphi(u,t)\\rangle\\right)\\,\\mathrm{d}\\gamma_{t}^{y}(u)\\,\\mathrm{d}\\mu(y)\\,\\mathrm{d}t=0\\qquad\\mu\\mathrm{-almost}\\,\\mathrm{every}\\,y.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "E.2 Absolutely Continuous Curves ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We now proceed to prove the main results of this section. First, we introduce some preliminary notions. We define the map $j_{q}:L^{q}(\\gamma,Y\\times U)\\rightarrow L^{p}(\\gamma,Y\\times U)$ for $1/p+1/q=1$ via ", "page_idx": 21}, {"type": "equation", "text": "$$\nj_{q}(w)=\\left\\{\\begin{array}{l l}{\\lvert w\\rvert^{q-2}w}&{w\\neq0}\\\\ {0}&{w=0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is the Fr\u00e9chet differential of the convex functional $\\frac{1}{q}\\left\\|w\\right\\|_{L^{q}(\\gamma,Y\\times U)}^{q}$ . A straightforward calculation shows that this map satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|j_{q}(w)\\right\\|_{L^{p}(\\gamma,Y\\times U)}^{p}=\\left\\|w\\right\\|_{L^{q}(\\gamma,Y\\times U)}^{q}=\\int_{Y\\times U}\\langle j_{q}(w),w\\rangle\\,\\mathrm{d}\\gamma(y,u).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "See also Ambrosio et al. [2005, Chapter 8]. ", "page_idx": 21}, {"type": "text", "text": "Theorem 3 (Absolutely Continuous Curves in $\\mathbb{P}_{p}^{\\mu}(Y\\times U))$ Let $I\\subset\\mathbb{R}$ be an open interval, and suppose $\\gamma_{t}:I\\to\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is an absolutely continuous in the $W_{p}^{\\mu}$ metric with $|\\gamma^{\\prime}|(t)\\in L^{1}(I)$ . Then, there exists a Borel vector field $v_{t}(y,u)$ such that ", "page_idx": 21}, {"type": "text", "text": "(a) $v_{t}$ is triangular ", "page_idx": 21}, {"type": "text", "text": "(b) $v_{t}\\in L^{p}(\\gamma_{t},Y\\times U)$ and $\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\leq|\\gamma^{\\prime}|(t)\\,f o r\\,a.e.$ t (c) $(v_{t},\\gamma_{t})$ solve the continuity equation in the sense of distributions. ", "page_idx": 21}, {"type": "text", "text": "Proof. Assume without loss of generality that $|\\gamma^{\\prime}|(t)\\in L^{\\infty}(I)$ and that $I=(0,1)$ [Ambrosio et al., 2005, Lemma 1.1.4, Lemma 8.1.3]. Fix any $\\varphi\\in\\mathrm{Cyl}(Y\\times U)$ . For $s,t\\in I$ there exists an optimal triangular coupling $\\gamma_{s t}\\in\\Pi_{Y}\\big(\\gamma_{s},\\gamma_{t}\\big)$ . By H\u00f6lder\u2019s inequality, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\gamma_{t}(\\varphi)-\\gamma_{s}(\\varphi)|\\leq\\mathrm{Lip}(\\varphi)W_{p}^{\\mu}(\\gamma_{s},\\gamma_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It follows that $t\\mapsto\\gamma_{t}(\\varphi)$ is absolutely continuous. We can introduce the upper semicontinuous and bounded map ", "page_idx": 21}, {"type": "equation", "text": "$$\nH(y_{0},u_{0},y_{1},u_{1})=\\left\\{\\frac{|\\nabla\\varphi(y_{0},u_{0})|}{|\\varphi(y_{0},u_{0})-\\varphi(y_{1},u_{1})|}\\quad(y_{0},u_{0})=(y_{1},u_{1})\\right..\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $|h|$ sufficiently small, choose any optimal coupling $\\gamma_{(s+h)h}\\in\\Pi_{Y}\\bigl(\\gamma_{s+h},\\gamma_{s}\\bigr)$ and note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{|\\gamma_{s+h}(\\varphi)-\\gamma_{s}(\\varphi)|}{|h|}\\leq\\frac{1}{|h|}\\int_{(Y\\times U)^{2}}|(y_{0},u_{0})-(y_{1},u_{1})|H(y_{0},u_{0},y_{1},u_{1})\\,\\mathrm{d}\\gamma_{(s+h)s}}}\\\\ &{}&{\\qquad\\qquad\\qquad\\leq\\frac{W_{p}^{\\mu}(\\gamma_{s+h},\\gamma_{s})}{|h|}\\left(\\int_{(Y\\times U)^{2}}H^{q}(y_{0},u_{0},y_{1},u_{1})\\,\\mathrm{d}\\gamma_{(s+h)h,s}\\right)^{1/q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If $t$ is a point of metric differentiability for $t\\mapsto\\gamma_{t}$ , note that $\\gamma_{(t+h)t}\\to(I,I)_{\\#}\\gamma_{t}$ narrowly, where $I$ is the identity map on $Y\\times U$ . Moreover, since $\\gamma_{t}\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ , it follows that on the diagonal we have that almost surely $H(y_{0},u_{0},y_{0},u_{1})=\\iota(|\\nabla_{u}\\varphi(\\dot{y_{0}},u_{0})|$ . Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}\\operatorname*{sup}_{\\epsilon}\\frac{|\\gamma_{t+h}(\\varphi)-\\gamma_{t}(\\varphi)|}{|h|}\\le|\\gamma^{\\prime}|(t)\\left(\\int_{Y\\times U}|H|^{q}(y_{0},u_{0},y_{0},u_{0})\\,\\mathrm{d}\\gamma_{t}(y_{0},u_{0})\\right)^{1/q}}\\\\ {=|\\gamma^{\\prime}|(t)\\,\\|\\iota(\\nabla_{u}\\varphi)\\|_{L^{q}(\\gamma_{t},Y\\times U)}=|\\gamma^{\\prime}|(t)\\,\\|\\nabla_{u}\\varphi\\|_{L^{q}(\\gamma_{t},U)}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking $Q=Y\\times U\\times I$ and $\\textstyle\\gamma=\\int\\gamma_{t}\\,\\mathrm{d}t$ , fix any $\\varphi\\in\\mathrm{Cyl}(Q)$ . We have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{Q}\\partial_{s}\\varphi(y,u,s)\\,\\mathrm{d}\\gamma(y,u,s)}\\\\ {\\displaystyle=\\operatorname*{lim}_{h\\downarrow0}\\int_{I}\\frac{1}{h}\\left(\\int_{Y\\times U}\\varphi(y,u,s)\\,\\mathrm{d}\\gamma_{s}(y,u)-\\int_{(Y\\times U)}\\varphi(y,u,s)\\,\\mathrm{d}\\gamma_{s+h}(y,u)\\right)\\,\\mathrm{d}s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "An application of Fatou\u2019s Lemma, Equation (78), and H\u00f6lder\u2019s inequality gives us ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\int_{Q}\\partial_{s}\\varphi(y,u,s)\\,\\mathrm{d}\\gamma(y,u,s)\\right|\\leq\\left(\\int_{J}|\\gamma^{\\prime}|(s)\\,\\mathrm{d}s\\right)^{1/p}\\left(\\int_{Q}|\\nabla_{u}\\varphi(y,u,s)|^{q}\\,\\mathrm{d}\\mu(y,u,s)\\right)^{1/q}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any interval $J\\subset I$ with supp $\\varphi\\subset J\\times Y\\times U$ . ", "page_idx": 22}, {"type": "text", "text": "Fix the subspace ", "page_idx": 22}, {"type": "equation", "text": "$$\nV=\\{\\iota(\\nabla_{u}\\varphi(y,u,s)):\\varphi\\in\\mathrm{Cyl}(Q)\\}\\subseteq Y\\times U\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and denote by $\\overline{V}$ its $L^{q}(\\gamma,Y\\times U\\times I)$ closure. Define the linear functional $L:V\\to\\mathbb{R}$ via ", "page_idx": 22}, {"type": "equation", "text": "$$\nL(\\nabla_{u}\\varphi)=-\\int_{Q}\\partial_{s}\\varphi(y,u,s)\\,\\mathrm{d}\\gamma(y,u,s)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and note that Equation (81) implies that $L$ is a bounded linear functional on $V$ . Thus (by HahnBanach and the fact that $V\\subseteq{\\overline{{V}}}$ is dense) we may uniquely extend $L$ to $\\overline{V}$ . We thus have a convex minimization problem ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\overline{{V}}}\\frac{1}{q}\\int_{Q}|w(y,u,s)|^{q}\\,\\mathrm{d}\\gamma(y,u,s)-L(w)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which admits the unique solution $w$ such that $j_{q}(w)-L=0$ . In particular, the estimate (81) shows that the above functional is coercive and hence admits a minimizer which we may obtain via its differential as a consequence of convexity. Thus, we obtain a triangular vector field $v=j_{q}(w)$ such that for all $\\varphi\\in\\mathrm{Cyl}(Q)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle v,\\nabla\\varphi\\rangle=\\int_{Q}\\langle v(y,u,s),\\nabla\\varphi(y,u,s)\\rangle\\,\\mathrm{d}\\gamma(y,u,s)=\\langle L,\\nabla\\varphi\\rangle=-\\int_{Q}\\partial_{s}\\varphi(y,u,s)\\,\\mathrm{d}\\gamma(y,u,s).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This precisely shows that $(v_{t},\\gamma_{t})$ is a triangular distributional solution to the continuity equation. ", "page_idx": 22}, {"type": "text", "text": "Now, choose any interval $J\\subset I$ and choose a sequence $\\eta^{k}\\in C_{c}^{\\infty}(J)$ , with $0\\leq\\eta^{k}\\leq1$ and $\\eta_{k}\\rightarrow\\mathbb{1}_{J}$ as $k\\rightarrow\\infty$ . Moreover choose a sequence $\\left(\\nabla_{u}\\varphi_{n}\\right)\\subset V$ converging to $w=j_{p}(v)$ in $L^{q}(\\gamma,Q)$ . Our previous calculations give ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{Q}\\eta^{k}(s)|v(y,u,s)|^{p}\\,\\mathrm{d}\\gamma(y,u,s)=\\int_{Q}\\eta^{k}(s)\\langle v,w\\rangle\\,\\mathrm{d}\\gamma=\\operatorname*{lim}_{n\\to\\infty}\\int_{Q}\\eta^{k}\\langle v,\\nabla_{u}\\varphi_{n}\\rangle\\,\\mathrm{d}\\gamma}\\\\ &{=\\displaystyle\\operatorname*{lim}_{n\\to\\infty}\\langle L,\\nabla_{u}(\\eta^{k}\\varphi_{n})\\rangle\\leq\\left(\\int_{J}|\\gamma^{\\prime}|^{p}(s)\\,\\mathrm{d}s\\right)^{1/p}\\left(\\int_{J\\times Y\\times U}|v|^{p}\\,\\mathrm{d}\\gamma\\right)^{1/p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking $k\\rightarrow\\infty$ we see that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{J}\\int_{Y\\times U}|v_{t}(y,u)|^{p}\\,\\mathrm{d}\\gamma_{t}(y,u)\\,\\mathrm{d}t\\leq\\int_{J}|\\gamma^{\\prime}|^{p}(s)\\,\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and since $J\\subset I$ was arbitrary, we conclude ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\leq|\\gamma^{\\prime}|(t)\\qquad{\\mathrm{a.e.}}\\ \u2013t.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now prove, in some sense, a converse of the previous theorem. ", "page_idx": 22}, {"type": "text", "text": "Theorem 4 (Continuous Curves Generated by Triangular Vector Fields) Suppose that $\\gamma_{t}:I\\to\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is narrowly continuous and $\\left(v_{t}\\right)$ is a triangular vector field such that $(\\gamma_{t},v_{t})$ solve the continuity equation with $\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\in L^{1}(I)$ . Then, $\\gamma_{t}:I\\to\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ is absolutely continuous in the $W_{p}^{\\mu}$ metric and $|\\gamma^{\\prime}|(t)\\leq\\|v_{t}\\|_{L^{p}(\\mu,Y\\times U)}.$ for almost every $t$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We first assume that $U$ is finite dimensional. Our strategy is to check the hypotheses necessary for Ambrosio et al. [2005, Theorem 8.3.1] to hold for $\\mu$ -almost every $y$ , followed by an application of this theorem. By Lemma 1, for $\\mu$ -almost every $y$ we have that $\\left(\\gamma_{t}^{y},v_{t}^{U}(y,-)\\right)$ solve the continuity equation distributionally on $I\\times U$ . ", "page_idx": 23}, {"type": "text", "text": "By Jensen\u2019s inequality (and the assumption $p\\geq1$ ) we see ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{I}\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\;\\mathrm{d}t=\\int_{I}\\mathbb{E}_{y\\sim\\mu}\\left[\\big\\|v_{t}^{U}(y,-)\\big\\|_{L^{p}(\\gamma_{t}^{y},U)}^{p}\\right]^{1/p}\\,\\mathrm{d}t}}\\\\ &{}&{\\qquad\\ge\\displaystyle\\int_{I}\\mathbb{E}_{y\\sim\\mu}\\left[\\big\\|v_{t}^{U}(y,-)\\big\\|_{L^{p}(\\gamma_{t}^{y},U)}\\right]\\,\\mathrm{d}t}\\\\ &{}&{\\qquad=\\mathbb{E}_{y\\sim\\mu}\\left[\\displaystyle\\int_{I}\\big\\|v_{t}^{U}(y,-)\\big\\|_{L^{p}(\\gamma_{t}^{y},U)}\\;\\mathrm{d}t\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since the first term is finite, it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|v_{t}^{U}(y,-)\\right\\|_{L^{p}(\\gamma_{t}^{y},U)}\\in L^{1}(I)\\qquad\\mu{\\mathrm{-almost}}\\;{\\mathrm{every~}}y.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now Ambrosio et al. [2005, Lemma 8.1.2] shows that for $\\mu$ -almost every $y$ we have that $(\\gamma_{t}^{y})$ admits a narrowly continuous representative $(\\tilde{\\gamma}_{t}^{y})$ with $\\tilde{\\gamma}_{t}^{y}=\\gamma_{t}^{y}$ for almost every $t$ . It follows from Ambrosio et al. [2005, Theorem 8.3.1] that for any $t_{1}\\leq t_{2}$ in $I$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{p}^{p}(\\tilde{\\gamma}_{t_{1}}^{y},\\tilde{\\gamma}_{t_{2}}^{y})\\leq(t_{2}-t_{1})^{p-1}\\displaystyle\\int_{t_{1}}^{t_{2}}|v_{t}^{U}(y,u)|^{p}\\,\\mathrm{d}\\tilde{\\gamma}_{t}^{y}(u)\\,\\mathrm{d}t}\\\\ {=(t_{2}-t_{1})^{p-1}\\displaystyle\\int_{t_{1}}^{t_{2}}|v_{t}^{U}(y,u)|^{p}\\,\\mathrm{d}\\gamma_{t}^{y}(u)\\,\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second line follows as $\\tilde{\\gamma}_{t}^{y}=\\gamma_{t}^{y}$ for almost every $t$ . ", "page_idx": 23}, {"type": "text", "text": "Let $\\begin{array}{r}{\\tilde{\\gamma}_{t}=\\int_{Y}\\tilde{\\gamma}_{t}^{y}\\,\\mathrm{d}\\mu(y)}\\end{array}$ be the measure obtained via marginalizing over the $Y$ -variables. Taking an expectation over $y\\sim\\mu$ , the previous inequality shows us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{W_{p}^{\\mu,p}(\\tilde{\\gamma}_{t_{1}},\\tilde{\\gamma}_{t_{2}})}{(t_{2}-t_{1})^{p}}\\leq\\frac{1}{t_{2}-t_{1}}\\int_{t_{1}}^{t_{2}}\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}^{p}~\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, note that $t_{1}$ is almost surely a Lebesgue point of the right-hand side and $\\tilde{\\gamma}_{t_{1}}=\\gamma_{t_{1}}$ . Taking $t_{2}\\rightarrow t_{1}$ along a sequence where $\\tilde{\\gamma}_{t_{2}}=\\gamma_{t_{2}}$ shows us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\gamma^{\\prime}|(t)\\leq\\|v_{t}\\|_{L^{p}(\\gamma_{t}),Y\\times U}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for almost every $t\\in I$ . ", "page_idx": 23}, {"type": "text", "text": "In the case that $U$ is infinite dimensional, fix any $y\\in Y$ such that Lemma 1 holds (which is of full measure) and fix a countable orthonormal basis $\\left(e_{k}\\right)$ for $U$ . Set $\\pi^{d}:~U~\\to~\\mathbb{R}^{d}$ to be the projection operator for this basis, i.e. $u\\mapsto(\\langle u,e_{1}\\rangle,\\ldots,\\langle u,e_{d}\\rangle)$ . We consider the collection of finite dimensional conditional measures $\\gamma_{t}^{d,y}=\\pi_{\\#}^{d}\\gamma_{t}^{y}$ . By the same argument in Ambrosio et al. [2005, Theorem 8.3.1], there exists a vector field $v_{t}^{d,y}$ on $\\mathbb{R}^{d}$ such that $(\\gamma_{t}^{d,y},v_{t}^{d,y})$ solve the continuity equation and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|v_{t}^{d,y}\\right\\|_{L^{p}(\\gamma_{t}^{d,y},\\mathbb{R}^{d})}\\leq\\left\\|v_{t}^{U}(y,-)\\right\\|_{L^{p}(\\gamma_{t}^{y},U)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It follows from the finite-dimensional case above that for almost every $t_{1}\\leq t_{2}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{p}^{p}(\\gamma_{t_{1}}^{d,y},\\gamma_{t_{2}}^{d,y})\\leq(t_{2}-t_{1})^{p-1}\\int_{t_{1}}^{t_{2}}\\|v_{t}^{U}(y,-)\\|_{L^{p}(\\gamma_{t}^{y},U)}^{p}\\ \\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\hat{\\gamma}_{t}^{y,d}\\,=\\,(\\pi^{d})_{\\#}^{\\star}\\gamma_{t}^{y,d}$ where $(\\pi^{d})^{\\star}\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,U$ maps $\\boldsymbol{z}\\mapsto\\sum_{k=1}^{d}\\boldsymbol{z}_{k}\\boldsymbol{e}_{k}$ . As $d\\,\\rightarrow\\,\\infty$ we have $\\hat{\\gamma}_{t}^{d,y}\\rightarrow\\gamma_{t}^{y}$ narrowly for all $t\\in I$ . Since $(\\pi^{d})^{\\star}$ is an isometry, Ambrosio et al. [2005, Lemma 7.1.4] shows that ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{p}^{p}(\\gamma_{t_{1}}^{y},\\gamma_{t_{2}}^{y})\\leq\\operatorname*{liminf}_{d\\rightarrow\\infty}W_{p}^{p}(\\gamma_{t_{1}}^{d,y},\\gamma_{t_{2}}^{d,y})\\leq(t_{2}-t_{1})^{p-1}\\int_{t_{1}}^{t_{2}}\\|v_{t}^{U}(y,-)\\|_{L^{p}(\\gamma_{t}^{y},U)}^{p}\\mathrm{~d}t.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, integration with respect to $\\mathrm{d}\\mu(y)$ yields ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{p}^{p,\\mu}(\\gamma_{t_{1}},\\gamma_{t_{2}})\\leq(t_{2}-t_{1})^{p-1}\\int_{t_{1}}^{t_{2}}\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}^{p}~\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking $t_{2}\\rightarrow t_{1}$ shows that for almost every $t$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\gamma^{\\prime}|(t)\\leq\\|v_{t}\\|_{L^{p}(\\gamma_{t},Y\\times U)}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Together, Theorem 3 and Theorem 4 give us a dynamical interpretation of the conditional Wasserstein distance. The following result is a conditional analogue of the well-known Benamou-Brenier Theorem [Benamou and Brenier, 2000]. Here, we note that the following proof follows the standard proof closely \u2013 the main legwork in obtaining this conditional generalization is through the previous two theorems. ", "page_idx": 24}, {"type": "text", "text": "Theorem 5 (Conditional Benamou-Brenier) ", "page_idx": 24}, {"type": "text", "text": "Let $1<p<\\infty$ . For any $\\eta,\\nu\\in\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{p}^{p,\\mu}(\\eta,\\nu)=\\operatorname*{min}_{(\\gamma_{t},\\nu_{t})}\\left\\{\\int_{0}^{1}\\left\\lVert v_{t}\\right\\rVert_{L^{p}(\\mu_{t})}^{p}\\,\\mathrm{d}t\\,|\\left(v_{t},\\gamma_{t}\\right)s o l\\nu e\\left(9\\right)\\!,\\,\\gamma_{0}=\\eta,\\gamma_{1}=\\nu,\\,d\\eta\\right\\}\\,\\mathrm{d}t\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Write $M$ for the infimum on the right-hand side. ", "page_idx": 24}, {"type": "text", "text": "First, suppose that $\\left(v_{t},\\mu_{t}\\right)$ are admissible and $\\begin{array}{r}{\\int_{0}^{1}\\|v_{t}\\|_{L^{p}(\\mu_{t})}<\\infty}\\end{array}$ . It follows from Theorem 4 that $(\\gamma_{t})$ is an absolutely continuous curve in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ and $\\|\\dot{\\boldsymbol{v}_{t}}\\|_{L^{p}(\\mu_{t},Y\\times U)}\\geq|\\gamma^{\\prime}|(t)$ . Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{p}^{\\mu,p}(\\eta,\\nu)\\leq\\left(\\int_{0}^{1}|\\gamma^{\\prime}|(t)\\,\\mathrm{d}t\\right)^{p}\\leq\\int_{0}^{1}\\|v_{t}\\|_{L^{p}(\\mu_{t},Y\\times U)}^{p}\\ \\mathrm{d}t\\leq M.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Conversely, by Theorem 1 there exists a constant speed geodesic $(\\gamma_{t})\\subset\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ connecting $\\eta$ and $\\nu$ . Recall that constant speed geodesics are absolutely continuous. By Theorem 3, there exists a Borel triangular vector field $v_{t}$ such that $(v_{t},\\gamma_{t})$ solve the continuity equation, and moreover $\\|v_{t}\\|_{L^{p}(\\mu_{t},Y\\times U)}\\leq\\bar{|\\gamma^{\\prime}|}(t)$ . In fact, because $(v_{t},\\gamma_{t})$ solve the continuity equation, Theorem 4 yields that $\\|v_{t}\\|_{L^{p}(\\mu_{t},Y\\times U)}=|\\gamma^{\\prime}|(t)$ . ", "page_idx": 24}, {"type": "text", "text": "Since $\\gamma_{t}$ is a constant speed geodesic in $\\mathbb{P}_{p}^{\\mu}(Y\\times U)$ , it follows that $|\\mu^{\\prime}|(t)=W_{p}^{\\mu}(\\eta,\\nu)$ for almost every $t\\in(0,1)$ . Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{p}^{\\mu,p}(\\eta,\\nu)=\\int_{0}^{1}\\left|\\gamma^{\\prime}|(t)^{p}\\,\\mathrm{d}t=\\int_{0}^{1}\\left\\lVert v_{t}\\right\\rVert_{L^{p}(\\gamma_{t},Y\\times U)}^{p}\\ge M.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, $W_{p}^{\\mu,p}(\\eta,\\nu)=M$ as desired. ", "page_idx": 24}, {"type": "text", "text": "F Experiment Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide additional details regarding all of our experiments, as well as additional results not contained within the main paper. All models can be trained on a single GPU with less than $24\\:\\mathrm{GB}$ of memory, and our experiments were parallelized over 8 such GPUs on a local server. We first describe our setting for the 2D and Lotka-Volterra experiments, as these share a similar setup. Details for the Darcy flow inverse problem are described in the corresponding section. ", "page_idx": 24}, {"type": "text", "text": "Models. For FM and COT-FM, our model architecture is an MLP with SeLU activations [Klambauer et al., 2017]. Time conditioning is achieved by concatenating the time variable as an input to the network. The covariance operator $C$ chosen in the path of measures in Equation (10) is taken to be $C=\\sigma^{2}I$ where $\\sigma$ is a hyperparameter. ", "page_idx": 24}, {"type": "text", "text": "Our implementation of FM is adapted from the torchcfm package Tong et al. [2023], available under the MIT License. For PCP-Map and COT-Flow, we adapt the open-source implementations from Wang et al. [2023], available under the MIT License. ", "page_idx": 24}, {"type": "table", "img_path": "tk0uaRynhH/tmp/ea86c29937a8165b1630f2d4315fe4bcfbe7bbf391d5a9df617e5b3d25b6b2fc.jpg", "table_caption": ["Table 4: Hyperparameter grid used for random search of the FM and COT-FM models on the 2D and Lotka-Volterra datasets. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Training and Model Selection. Hyperparameter tuning of the PCP-Map and COT-Flow models was performed directly using the code of Wang et al. [2023], essentially implementing grid-search with an early stopping procedure. We refer to the paper and codebase of Wang et al. [2023] for further details. For COT-FM and FM, we perform a random grid search over 100 hyperparameter settings using the grid described in Table 4. For all model types, we select the best model used to generate the results in the paper as the training checkpoint that resulted in the lowest $W_{2}$ error to the joint target distribution on a held-out validation set. For training, we use the Adam optimizer where we only tune the learning rate, leaving all other settings as their defaults in pytorch. ", "page_idx": 25}, {"type": "text", "text": "F.1 2D Synthetic Data ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Data Generation. This experiment consists of four 2D synthetic datasets, where $Y=U=\\mathbb{R}$ The datasets moons, circles, swissroll are available through scikit-learn [Pedregosa et al., 2011]. The moons dataset is generated with $\\mathtt{n o i s e=0.05}$ followed by standard scaling with a mean of $m=(0.5,0.25)$ and standard deviation of $\\sigma=(0.75,0.25)$ . The circles dataset is generated with factor ${\\it\\Psi}=0\\cdot5$ and nois $\\mathtt{e}{=}0.05$ . The swissroll dataset is generated with nois $e{=}0.75$ , followed by projection to the first two coordinates and re-scaling by a factor of 12. All other unstated parameters are left as their default values. We use the code available from Hosseini et al. [2023] to generate the checkerboard dataset. For all datasets, we generate a training set (i.e., samples from the target distribution) of 20, 000 samples and 1, 000 held-out validation samples for model selection. Means and standard deviations in Table 1 are reported across five independent testing sets of 5,000 samples for the best representative of each model type. ", "page_idx": 25}, {"type": "text", "text": "In COT-FM, to generate samples from the source distribution, we sample an additional 20, 000 points from the target distribution and keep only the $Y$ coordinates. This ensures that the source and target have equal $Y$ marginals. During training, standard Gaussian noise ${\\mathcal{N}}(0,1)$ is sampled for the $U$ coordinate of these source points at each minibatch. ", "page_idx": 25}, {"type": "text", "text": "We use minibatch COT couplings [Tong et al., 2023] in this experiment as computing the full COT plan was prohibitively expensive in terms of memory usage. However, we note that we use large batch sizes, meaning that the COT plan we find in this way should not be too far from optimal. All couplings are computed using the POT Python package [Flamary et al., 2021]. ", "page_idx": 25}, {"type": "text", "text": "F.2 Lotka-Volterra Dynamical System ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Data Generation. We adopt the settings of Alfonso et al. [2023] for this experiment. As described in the main paper, we assume $\\ \\mathit{\\dot{p}}(0)\\ =\\ (30,1)$ and that $\\log(u)~\\sim~\\bar{\\mathcal{N}}(m,0.5I)$ with $m\\;=\\;(-0.125,-3,-0.125,-3)$ . Given parameters $u~\\in~\\mathbb{R}_{\\ge0}^{4}$ R4\u22650, we simulate Equation (13) for $t\\in\\{0,2,\\ldots,20\\}$ to obtain a solution $\\boldsymbol z(\\boldsymbol u)\\in\\mathbb R_{\\ge0}^{22}$ . An observation $y\\in\\mathbb{R}_{\\ge0}^{22}$ is obtained by the addition of log-normal noise, i.e. $\\log(y)\\sim\\mathcal{N}(\\log(z(u),0.1I)$ . We thus may simulate many $(y,u)$ pairs from the target measure for training. ", "page_idx": 25}, {"type": "text", "text": "We generate a training set of $10,000\\ (y,u)$ pairs using the procedure described above and a held-out validation set of $10,000\\ (y,u)$ pairs for model selection. Means and standard deviations in Table 2 are reported across five independent testing sets of 5,000 samples for the best representative of each model type. Figure 2 and Figures 9, 8, 7, 10 show 10, 000 samples from each model, as well as $10,000$ samples from the differential evolution Metropolis MCMC sampler [Braak, 2006] after a burn-in of 50, 000 samples. This is implemented through the PyMC Python package [Abril-Pla et al., 2023]. ", "page_idx": 25}, {"type": "image", "img_path": "tk0uaRynhH/tmp/5d0d1d372256e5ffca43621e25bc87e2c807937740ce64707b89e30341af8bb3.jpg", "img_caption": ["Figure 5: Samples from the ground-truth joint target distribution and the various models for the 2D datasets. Samples from COT-FM more closely match the ground-truth distribution than the baselines. A common failure mode for the baselines is to generate samples from regions with zero support under the true data distributions. Table 1 contains a quantitative evaluation. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "For COT-FM we use the full COT couplings, i.e. without minibatches. This is available to use due to the smaller size of the training set used in this experiment. The COT couplings are computed in the same way as the previous section, and as described in Section 7. ", "page_idx": 26}, {"type": "image", "img_path": "tk0uaRynhH/tmp/683fd5dbdc2a024f8b7caf099948a993ac80be6aa3badec9f5948df2fc167f43.jpg", "img_caption": ["Figure 6: Conditional KDEs shown for each of the methods on the 2D datasets. The conditioning variable $y$ is fixed at the horizontal dashed line shown in Figure 5. In all plots, the orange solid line indicates the CKDE of the ground-truth joint samples. In each column, the dashed blue line indicates the CKDE of samples generated from the respective method. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Lotka-Volterra Samples: COT-FM (Ours) ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "tk0uaRynhH/tmp/53616ec06bd7f4d2e132488947aa4645c4c65f94612ae2313222b3872142db5e.jpg", "img_caption": ["Figure 7: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from our method (COT-FM) are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Lotka-Volterra Samples: PCP-Map ", "text_level": 1, "page_idx": 29}, {"type": "image", "img_path": "tk0uaRynhH/tmp/75a3e1d8082e74a16f933672c7a546acdb91c6d19ae6aeb768b013e9b92c61d8.jpg", "img_caption": ["Figure 8: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from PCP-Map are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Lotka-Volterra Samples: COT-Flow ", "text_level": 1, "page_idx": 30}, {"type": "image", "img_path": "tk0uaRynhH/tmp/dab72bd7d5289a2cf1168084a3ec99d42a16b28f0be2fcabceb4ce3d4687c4ea.jpg", "img_caption": ["Figure 9: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from COT-Flow are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Lotka-Volterra Samples: FM ", "text_level": 1, "page_idx": 31}, {"type": "image", "img_path": "tk0uaRynhH/tmp/3300dc6509fbf95b12a84717dc16aea74a1b97a50ecf2aa754e8807c2b5af45b.jpg", "img_caption": ["Figure 10: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from flow matching (FM) are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "F.3 Inverse Darcy Flow ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Dataset. The training and test datasets are generated following the same procedure as Hosseini et al. [2023]: pressure fields $u$ are sampled from a Gaussian process with Mat\u00e9rn kernel having $\\nu=3/2$ and lengthscale $\\ell=1/2$ , on a regular $40\\times40$ grid. The parameters are then exponentiated and used to simulate the permeability fields $p$ from the forward model $\\mathfrak{F}$ solving the Darcy flow PDE, using FEniCS [Aln\u00e6s et al., 2015]. Stochasticity arises from adding Gaussian noise to the permeability fields, obtaining $y=\\mathfrak{F}(u)+\\epsilon,\\,\\epsilon\\sim\\mathcal{N}(0,\\dot{\\sigma}^{2}I)$ . For our experiments we observe $y$ on a $100\\times100$ grid, and we use $\\sigma=2.5\\times10^{-2}$ . We note that this level of noise is quite considerable, as it accounts for roughly $60\\%$ of the variability in the $y$ . Figure 11 showcases a data point for reference. Our source and target training sets contain $\\mathrm{i\\times10^{4}}$ samples each, and our test set comprises $5\\times10^{3}$ samples. We remark that although $y$ and $u$ are observed on a grid their resolution does not need to be fixed, allowing for training at different resolutions. ", "page_idx": 32}, {"type": "image", "img_path": "tk0uaRynhH/tmp/c65d5769a48004f6702ca28b8c889f78257416b6b1a4c50035663dc036225e9c.jpg", "img_caption": ["Figure 11: Example of one random data point from the Darcy flow dataset. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Models. In order to make learning feasible in infinite-dimensional Hilbert spaces, we adapt the architecture of a Fourier Neural Operator (FNO) [Li et al., 2020] from the neuraloperator package [Kovachki et al., 2021] to accommodate for conditioning information observed at an arbitrary resolution. We do so by introducing a projection layer mapping the conditioning information to match the hidden channels of the input lifting block, and a pooling operation to project to the input dimensions. The two are then concatenated and passed through an FNOBlock mapping from ( $2~\\times$ hidden_channels) $\\times$ input_dim to hidden_channels $\\times$ input_dim, before following the original architecture. For all of the models in consideration, we fix the architecture to be have hidden_channels $=64$ , projection_channels $=256$ , and 32 Fourier modes. We train each model for 1500 epochs, and hyperparameters for each architecture are selected as follows: ", "page_idx": 32}, {"type": "text", "text": "\u2022 WaMGAN [Hosseini et al., 2023]: using an adaptation to the FNO architecture of the original code3, we perform a grid search as detailed in Table 5. We found the training procedure to be rather unstable, and for this reason we checkpoint the model every 100 epochs and report the results for the best performing model at its best checkpoint. We found this to be a model with learning rate $1\\times\\dot{1}0^{-4}$ , 2 full critic iterations, and monotone penalty of $1\\times10^{-3}$ . The gradient penalty parameter did not seem to significantly affect performance on the test set, and was set to 5.   \n\u2022 FFM [Kerrigan et al., 2024]: the learning rate is fixed to $5\\times10^{-4}$ , and the covariance operator $C$ is set to match that of the prior, but rescaled by a factor of $\\sigma=1\\times10^{-3}$ . We use the code from the original repository4.   \n\u2022 COT-FFM: we set $\\epsilon=1\\times10^{-5}$ in the cost function used to build the COT plan. The learning rate and $C$ are chosen to be the same as FFM. In order to build COT couplings, we take the source measure to be the product measure $\\pi_{\\#}^{Y}\\eta\\times\\mathcal{N}(0,C)$ . Approximate couplings are obtained on minibatches of size 256. ", "page_idx": 32}, {"type": "text", "text": "It should be noted that in any scenario where the source and the target $U-$ marginals are identical, using the OT coupling would yield the identity mapping as the optimal vector field minimizing (12). Hence, the OT-CFM model [Tong et al., 2023] is inapplicable here. ", "page_idx": 32}, {"type": "table", "img_path": "tk0uaRynhH/tmp/f8cf05dba7a4140f15ba20ad799f6e2cf843e1f903d46acfdf5dd7a28debb3f8.jpg", "table_caption": ["Table 5: Hyperparameter search space for WaMGAN "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Sampling. The resulting amortized sampler, denoted for simplification by the mapping $(y,u_{0})\\mapsto$ $u_{1}\\,=\\,\\tilde{T_{U}}(y,u_{0})$ , will parameterize an approximate posterior measure. Notice that, in contrast to classical variational inference techniques, no distributional assumptions are made about the approximate posterior. In turn, integrals are obtained numerically by Monte Carlo sampling $K$ samples from the prior, resulting in the approximation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\nu^{y}(f)\\approx\\int f\\,\\mathrm{d}\\delta_{\\tilde{T}_{U}(y,u_{0})}\\,\\mathrm{d}{\\cal N}(0,C)\\approx\\frac{1}{K}\\sum_{k=1}^{K}f(\\tilde{T}_{U}(y_{k},u_{0,k})),\\quad\\{u_{0,k}\\}_{k=1}^{K}\\stackrel{\\mathrm{\\scriptsize~i.i.d.}}{\\sim}\\mathcal{N}(0,C).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "G Minibatch COT ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we perform an additional experiment demonstrating that our COT-FM is able to obtain good approximations of the true COT map even when trained on relatively small minibatches. ", "page_idx": 33}, {"type": "text", "text": "We work on $Y=U=\\mathbb{R}$ and use a standard Gaussian source and a Gaussian target distribution with covariance $\\rho\\,=\\,0.75$ . These are chosen so that we may evaluate, in closed-form, the true conditional 2-Wasserstein distance, as detailed in Section C. We then train our COT-FM method using various batch sizes, and measure the resulting model\u2019s conditional 2-Wasserstein distance by sampling 10, 000 points from the source distribution, flowing each source point along the model\u2019s learned vector field, and computing the resulting squared distance to the corresponding terminal point. ", "page_idx": 33}, {"type": "text", "text": "In Figure 12, we plot the resulting deviation from the true value of $W_{2}^{\\mu,2}$ as a function of batch size. We see that even at a relatively small batch size of 16, the resulting error in the (squared) distances is less than $10\\%$ of the true value $(\\approx0.678)$ . While this experiment is only feasible on synthetic data (as we must compute the true distance), it nonetheless demonstrates that even with small batches we may recover the true distance. ", "page_idx": 33}, {"type": "image", "img_path": "tk0uaRynhH/tmp/4c82584f1897590715d88302437ca7f6a7b7d1b2222f54f5ec2db2cdfff03a1c.jpg", "img_caption": ["Figure 12: Error in the squared transportation distance as a function of batch size. Values closer to zero indicate a better approximation of the COT cost. For fairly small batch sizes $\\left(>16\\right)$ the magnitude of the error is stable and relatively small. Shading indicates one standard deviation computed over ten random evaluation sets. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The claims and contributions of our work are clearly stated in the introduction and abstract. In addition, the introduction clearly points to the sections in the paper where the claims are discussed in more detail. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Section 8 ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All formal claims in the paper have their assumptions clearly stated. Proofs of all theoretical claims are available in the Appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our experimental settings are clearly described in Section 7 and further details, including hyperparameter choices, are provided in Appendix F. We will release our code upon publication of the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We will release our code as open-source upon acceptance of the paper for publication. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: These details are provided in Appendix F. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Error bars are provided for all quantitative results. Explicit descriptions of how these are calculated are included as well. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: These details are provided in Appendix F. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have reviewed the ethics guidelines and conform to all requirements. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The potential societal impacts are discussed in Section 8. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We believe that our paper does not pose any such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Any assets used are properly cited throughout the paper. We provide addiitonal details in Appendix F. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}]