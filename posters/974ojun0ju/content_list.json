[{"type": "text", "text": "RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guglielmo Gattiglio Department of Statistics University of Warwick Coventry, CV4 7AL, UK Guglielmo.Gattiglio@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Lyudmila Grigoryeva\u2217 Faculty of Mathematics and Statistics University of St. Gallen Rosenbergstrasse 20, CH-9000 St. Gallen, Switzerland Lyudmila.Grigoryeva@unisg.ch ", "page_idx": 0}, {"type": "text", "text": "Massimiliano Tamborrino Department of Statistics University of Warwick Coventry, CV4 7AL, UK Massimiliano.Tamborrino@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parallel-in-time (PinT) techniques have been proposed to solve systems of timedependent differential equations by parallelizing the temporal domain. Among them, Parareal computes the solution sequentially using an inaccurate (fast) solver, and then \u201ccorrects\u201d it using an accurate (slow) integrator that runs in parallel across temporal subintervals. This work introduces RandNet-Parareal, a novel method to learn the discrepancy between the coarse and fine solutions using random neural networks (RandNets). RandNet-Parareal achieves speed gains up to $_{\\mathrm{x}125}$ and $\\mathbf{x}22$ compared to the fine solver run serially and Parareal, respectively. Beyond theoretical guarantees of RandNets as universal approximators, these models are quick to train, allowing the PinT solution of partial differential equations on a spatial mesh of up to $10^{5}$ points with minimal overhead, dramatically increasing the scalability of existing PinT approaches. RandNet-Parareal\u2019s numerical performance is illustrated on systems of real-world significance, such as the viscous Burgers\u2019 equation, the Diffusion-Reaction equation, the two- and three-dimensional Brusselator, and the shallow water equation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parallel-in-time (PinT) methods have been used to overcome the saturation of well-established spatial parallelism approaches for solving (prohibitively expensive) initial value problems (IVPs) for ordinary and partial differential equations (ODEs and PDEs), described by systems of $d\\in\\mathbb{N}$ ODEs (and ", "page_idx": 0}, {"type": "text", "text": "similarly for PDEs) ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{d\\pmb u}{d t}=h(\\pmb u(t),t)\\ \\mathrm{~on~}t\\in\\left[t_{0},t_{N}\\right],\\ \\mathrm{~with~}\\pmb u\\left(t_{0}\\right)=\\pmb u^{0},\\ N\\in\\mathbb{N},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $h:\\mathbb{R}^{d}\\times[t_{0},t_{N}]\\to\\mathbb{R}^{d}$ is a smooth multivariate function, $\\pmb{u}:[t_{0},t_{N}]\\,\\rightarrow\\,\\mathbb{R}^{d}$ is the time dependent column vector solution, and $\\pmb{u}^{0}\\in\\mathbb{R}^{d}$ is the initial value at $t_{0}$ . PinT schemes are particularly important when the sequential application of an accurate numerical integrator $\\mathcal{F}$ over $[t_{0},t_{N}]$ is infeasible in a reasonable wallclock time. There are three general approaches for PinT computation: parallel across-the-problem, parallel-across-the-step, and parallel-across-the-method. In [17, 55], another classification is provided: multiple shooting, methods based on waveform relaxation and domain decomposition, multigrid approaches, and direct time-parallel methods. Parallel-across-thestep methods, in which solutions at multiple time-grid points are computed simultaneously, include Parareal (approximation of the derivative in the shooting method) [45], Parallel Full Approximation Scheme in Space and Time (PFASST) (multigrid method) [13, 50], and Multigrid Reduction in Time (MGRIT) [14, 16] methods (see [19] for details). Among them, Parareal [45] has garnered popularity, with extensive theoretical analyses, improved versions, and empirical applications [17, 55]. This is due to its non-intrusive nature which allows seamless integration with arbitrary temporal and spatial discretizations, and to its successful performance across diverse fields, such as plasma physics [64, 66, 67], finance [4, 56], and weather modeling [59, 60]. Limited theoretical results are available for MGRIT and PFASST, with a few extensions and empirical applications. Interestingly, combined analyses have shown equivalences between Parareal and MGRIT, and connections between MGRIT and PFASST. In Parareal, a coarse and fast solver $\\mathcal{G}$ is run sequentially to obtain a first approximation of the solution, which is then corrected by running a fine (accurate) but slow integrator $\\mathcal{F}$ in parallel across $N$ temporal subintervals. This procedure is then iterated until a convergence criterion is met after $k\\leq N$ iterations, leading to a speed-up compared to running $\\mathcal{F}$ sequentially over the entire time interval. A recent advancement, GParareal [57], improves Parareal convergence rates (measured as $k/N)$ by learning the discrepancy $\\mathscr{F}-\\mathscr{G}$ using Gaussian Processes (GPs). This method outperforms Parareal for low-dimensional ODEs and a moderate number of computer cores $N$ . However, the cubic cost (in the number of data points, roughly $k N$ at iteration $k$ ) of inverting the GP covariance matrix hinders its broader application. Subsequent research introduced nearest neighbors (nns) GParareal (nnGParareal) [21], enhancing GParareal\u2019s scalability properties in both $N$ and $d$ through data reduction. Significant computational gains were achieved by training the GP on a small subset of nns, resulting in an algorithm loglinear in the sample size. This allowed scaling its effectiveness up to systems with a few thousand ODEs, beyond which it loses its potential. Indeed, being based on the original GP framework, it uses a costly hyperparameter optimization procedure that requires fitting one GP per ODE dimension. ", "page_idx": 1}, {"type": "text", "text": "This study introduces RandNet-Parareal, a new approach using random neural networks (RandNets) to learn the discrepancy $\\mathcal{F}-\\mathcal{G}$ . RandNets are a family of single-hidden-layer feed-forward neural networks (NNs), where hidden layer weights are randomly sampled and fixed, and only the output (or readout) layer is subject to training. Compared to standard artificial NNs, RandNets are hence much simpler to train: the input data are fed through the network, the predictions observed, and the weights of the linear output (or readout) layer are obtained as minimizers of a penalized squared loss between the NN outputs and the training targets. Since this optimization problem admits a closedform solution, no backpropagation is required, and the issues of vanishing and exploding gradients persisting for standard fully trainable NNs are therefore avoided. The literature on the topic is rich and somewhat fragmented, and different names are used for essentially the same model. RandNets are related to Random Feature Networks [6, 49, 62, 63, 65] and Reservoir Computing [24, 26, 25, 27, 28], Random Fourier Features (RFFs) and kernel methods [41, 61, 70, 74]. Some authors use the name Extreme Learning Machines (ELMs) [34\u201337, 44] to refer to RandNets, while others use the term randomized or random NNs [5, 32, 39, 46, 78, 82] for the same paradigm. RandNets show excellent empirical performance, and have been used in the context of mathematical finance [22, 33, 38], mathematical physics [52], electronic circuits [69], photonic [47] and quantum systems [23, 48], random deep splitting schemes [53], scientific computing [10, 11, 79, 81], and have shown excellent empirical performance in numerous further applications. Moreover, recent work [22, 25] proves that RandNets are universal approximators within spaces of sufficiently regular functions, and provides explicit approximation error bounds, with these results generalized to a large class of Bochner spaces in [52]. These contributions show that RandNets are a reliable machine learning paradigm with provable theoretical guarantees. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we show that endowing Parareal with RandNets-based learning of $\\mathscr{F}-\\mathscr{G}$ , the new proposed RandNet-Parareal algorithm, leads to significantly improved scalability, convergence speed, and parallel performance with respect to nnGParareal, GParareal, and Parareal. This allows us to solve PDE systems on a fine mesh of up to $10^{5}$ discretization points with negligible overhead, outperforming nnGParareal by two orders of magnitude and reducing its model cost by several orders. ", "page_idx": 2}, {"type": "text", "text": "Here, we compare the performance of Parareal, nnGParareal, and RandNet-Parareal on five increasingly complex systems, some of which are drawn from an extensive benchmark study of time-dependent PDEs [75]. These include the one-dimensional viscous Burgers\u2019 equation, the twodimensional Diffusion-Reaction equation, a challenging benchmark used to model biological pattern formation [76], the two- and three-dimensional Brusselator, known for its complex behavior, including oscillations, spatial patterns, and chaos, and the shallow water equations (SWEs). Derived from the compressible Navier-Stokes equations, the SWEs are a system of hyperbolic PDEs exhibiting several types of real-world significance behaviors known to challenge numerical integrators, such as sharp shock formation dynamics, sensitive dependence on initial conditions, diverse boundary conditions, and spatial heterogeneity. Example applications include of tsunamis or flooding simulations. ", "page_idx": 2}, {"type": "text", "text": "We intentionally chose two hyperbolic equations (Burgers\u2019 and SWE) to challenge RandNet-Parareal on systems for which Parareal is known to struggle, with slow or non-convergent behavior [2, 3, 9, 18, 72]. Previous works have developed ad-hoc coarse solvers to address Parareal\u2019s slow convergence for Burgers\u2019 [7, 40, 68, 71], and for SWE [1, 31, 54, 73]. Here, we adopt a different strategy: by leveraging the generalization capabilities of RandNets within the Parareal algorithm, we enhance the performance of standard, off-the-shelf integration methods such as Runge-Kutta, obtaining speed gains up to $_{\\mathrm{x125}}$ and $\\mathbf{x}22$ compared to the accurate integrator $\\mathcal{F}$ and Parareal, respectively. All experiments have been executed on Dell PowerEdge C6420 compute nodes each with $2\\textbf{x}$ Intel Xeon Platinum 826 (Cascade Lake) 2.9 GHz 24-core processors, 48 cores and 192 GB DDR4-2933 RAM per node. To illustrate our proposed algorithm and facilitate code adoption, we provide a step-by-step Jupyter notebook outlining RandNet-Parareal. Moreover, all simulation outcomes, including tables and figures, are fully reproducible and accompanied by the necessary Python code at https://github.com/Parallel-in-Time-Differential-Equations/RandNet-Parareal. ", "page_idx": 2}, {"type": "text", "text": "It is well acknowledged that comparing PinT methods based on different working principles is extremely hard, with [55] representing a recent survey article with some comparisons. Quoting [55],\u201ccaution should be taken when directly comparing speedup numbers across methods and implementations. In particular, some of the speedup and efficiency numbers are only theoretical in nature, and many of the parallel time methods do not address the storage or communication overhead of the parallel time integrator\u201d. [19] is one of very few recent attempts to systematically compare different PinT classes. However, it is limited exclusively to the Dahlquist problem. Thus, it has become conventional to compare new techniques to the existing state-of-the-art methods within the same group of solvers. This is why, in this work, we compare RandNet-Parareal with the original Parareal and its recently improved versions, GParareal [57], and nnGParareal [21]. ", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. In Section 2, we describe the Parareal algorithm. Section 3 briefly explains GParareal and nnGParareal, focusing on the latter. RandNet-Parareal is introduced in Section 4, while Sections 5 and 6 present our numerical results, and a final discussion. A computational complexity analysis of RandNet-Parareal, a robustness evaluation of the proposed algorithm, complementary simulation studies, and other additional results are available in the Supplementary Material. ", "page_idx": 2}, {"type": "text", "text": "Notation. We denote by $\\pmb{v}\\in\\mathbb{R}^{n}$ a column vector with entries $v_{i}$ , $i\\in\\{1,\\ldots,n\\}$ , and by $\\lVert\\boldsymbol{v}\\rVert$ and $\\|\\pmb{v}\\|_{\\infty}$ its Euclidean and infinity norms, respectively. We use $A\\in\\mathbb{R}^{n\\times m}$ to denote a real-valued $n\\,\\times\\,m$ matrix, $n,m\\,\\in\\,\\mathbb{N}$ , with elements $A_{i j}$ , $j$ th column $A_{(\\cdot,j)}$ , $j~\\in~\\{1,\\ldots m\\}$ , and $i$ th row $A_{(i,\\cdot)}$ , $i\\,\\in\\,\\{1,\\ldots,n\\}$ . We write $A^{\\top},A^{\\dagger}$ , and $\\|A\\|_{\\mathrm{F}}$ for the $A$ matrix transpose, Moore-Penrose pseudoinverse, and Frobenius norm, respectively. $\\mathbb{I}_{n}$ denotes the identity matrix of dimension $n$ . ", "page_idx": 2}, {"type": "text", "text": "2 The Parareal algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The idea of Parareal is to solve the $d$ -dimensional ODE (and similarly PDE) system (1) in a parallelin-time fashion, dividing the original IVP into $N$ sub-IVPs ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{d u_{i}}{d t}=h\\left(u_{i}\\left(t\\mid U_{i}\\right),t\\right),\\quad t\\in\\left[t_{i},t_{i+1}\\right],\\quad u_{i}\\left(t_{i}\\right)=U_{i},\\quad\\mathrm{~for~}\\ i=0,\\dots,N-1,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the number of time intervals $N$ is also the number of available machines/cores/processors, ${\\pmb u}_{i}\\left(t\\mid{\\pmb U}_{i}\\right)$ is the solution at time $t$ of the $i^{\\mathrm{th}}$ IVP with initial condition $\\pmb{u}(t_{i})\\,=\\pmb{U}_{i}\\,\\stackrel{\\mathcal{-}}{\\in}\\,\\mathbb{R}^{d}$ , $i=$ $0,\\ldots,N-1$ . If the initial conditions were known and satisfied the continuity conditions $U_{i}=$ ${\\pmb u}_{i-1}\\left(t_{i}|{\\pmb U}_{i-1}\\right)$ (for the coherent temporal evolution of the system across sub-intervals), then the sub-IVPs could be trivially solved in parallel on a dedicated machine. Unfortunately, this is not the case, as only the first initial condition $\\pmb{U}_{0}=\\pmb{u}^{0}\\in\\mathbb{R}^{d}$ at time $t_{0}$ appears available. To account for this, Parareal introduces another numerical integrator $\\mathcal{G}$ , much faster but less accurate than $\\mathcal{F}$ , to approximate the missing initial conditions $U_{i}$ , $i=1,\\cdot\\cdot\\cdot,N-1$ , sequentially. $\\mathcal{G}$ trades off accuracy for computational feasibility, usually taking seconds/minutes instead of hours/days of $\\mathcal{F}^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "The algorithm works as follows. We use $U_{i}^{k}$ to denote the Parareal approximation of $\\pmb{u}_{i}(t_{i})=\\pmb{U}_{i}$ at iteration $k\\,\\geq\\,0$ . At $k\\,=\\,0$ , the initial conditions $\\{U_{i}^{0}\\}_{i=1}^{N-1}$ are initialized using a sequential application of the coarse solver $\\mathcal{G}$ , obtaining $\\pmb{U}_{i}^{0}=\\mathcal{G}(\\pmb{U}_{i-1}^{0})$ , $i=1,\\cdot\\cdot\\cdot,N-1$ , with $U_{0}^{0}=U_{0}$ . At $k\\geq1$ , the obtained initial conditions $U_{i-1}^{k-1}$ are \u201cpropagated\u201d through $\\mathcal{F}$ in parallel on $N$ cores to obtain $\\mathcal{F}(U_{i-1}^{k-1})$ , $i=1,\\ldots,N$ . Note that for every initial condition $U_{i-1}^{k-1}$ , we compute both $\\mathcal{F}(U_{i-1}^{k-1})$ , i.e. a precise evaluation of $u_{i-1}(t_{i}|U_{i-1}^{k-1})$ , and $\\mathcal{G}(U_{i-1}^{k-1})$ , an inaccurate evaluation of the same term. Hence, we can interpret $\\mathcal{F}$ and $\\mathcal{G}$ as functions mapping an initial condition to the next one, thereby evolving (1) by one interval. We can then use their difference, $\\big(\\mathcal{F}-\\mathcal{G}\\big)(U_{i-1}^{k-1})$ , to correct the inaccuracy of $\\mathcal{G}$ on future evaluations. This gives rise to the original Parareal predictor-corrector rule $\\pmb{U}_{i}^{k}=\\mathcal{G}(\\pmb{U}_{i-1}^{k})+(\\mathcal{F}-\\mathcal{G})(\\pmb{U}_{i-1}^{k-1})$ , with $i=1,\\dots,N-1,k\\ge1$ [18], where the sequential prediction $\\mathcal{G}(U_{i-1}^{k})$ is corrected by adding the discrepancy $\\mathcal{F}-\\mathcal{G}$ computed at the previous iteration $k-1$ . However, this formulation can be changed to use data from the current iteration $k$ [57], and generalized to account for different ways of computing the discrepancy, leading to [21] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{i}^{k}=\\mathcal{G}(U_{i-1}^{k})+\\widehat{f}(U_{i-1}^{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\widehat{f}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ specifies how the correction function $\\mathcal F-\\mathcal G$ is computed or approximated based  on some observation $U\\in\\mathbb{R}^{d}$ . Parareal uses ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{f}_{\\mathrm{Para}}(\\boldsymbol{U}_{i-1}^{k})=(\\mathcal{F}-\\mathcal{G})(\\boldsymbol{U}_{i-1}^{k-1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "while other variants will be introduced in the subsequent sections. The Parareal solution (2) is considered converged for a given threshold $\\epsilon\\,>\\,0$ and up to time $t_{L}\\,\\leq\\,t_{N}$ , if solutions across consecutive iterations have stabilized. That is, for some pre-defined accuracy level $\\epsilon>0$ , it holds that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|U_{i}^{k}-U_{i}^{k-1}\\|_{\\infty}<\\epsilon,\\quad0<i\\leq L\\leq N-1.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Other stopping criteria are also possible [66, 67]. Converged Parareal approximations $U_{i}^{k}$ , $i\\leq L$ , are no longer iterated to avoid unnecessary overhead [12, 20, 21, 57, 58]. Instead, unconverged solution values $U_{i}^{k}$ , $i>L$ , are updated during future iterations by first running $\\mathcal{F}$ in parallel and then using the prediction-correction rule (2). The Parareal algorithm stops at some iteration $K_{\\mathrm{Para}}\\leq N$ when all initial conditions have converged, that is when (4) is satisfied with $L=N-1$ and thus $K_{\\mathrm{Para}}=k$ . Note that during every Parareal iteration $k>1$ , the \u201cleftmost\u201d fine solver evaluation $\\mathcal{F}(U_{L}^{k})$ is either run from the outcome of a previous fine computation $\\pmb{U}_{L}^{k}=\\mathcal{F}(\\pmb{U}_{L-1}^{k-1})$ , or from a converged initial condition $\\|\\boldsymbol{U}_{L}^{k}-\\boldsymbol{U}_{L}^{k-1}\\|_{\\infty}<\\epsilon.$ . This guarantees that, either way, the maximum number of iterations to convergence for any Parareal-based algorithm is $K_{\\mathrm{Para}}=N$ , in which case it sequentially attains the fine solver solution, with the added computational cost of running $\\mathcal{G}$ and evaluating $\\widehat{f}\\,N$ times. A Parareal pseudocode is presented in Algorithm 1 in Supplementary Material A. ", "page_idx": 3}, {"type": "text", "text": "3 GParareal and Nearest Neighbors GParareal ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The performance of Parareal can be improved by a careful selection of $\\widehat{f}$ in (2), combined with a better use of the available information present at iteration $k$ . Let $\\mathcal{D}_{k}$ denot e the dataset consisting of $N k$ pairs of inputs $U_{i-1}^{j}\\in\\mathbb{R}^{d}$ and their corresponding outputs $(\\mathcal{F}-\\mathcal{G})(U_{i-1}^{j})\\in\\mathbb{R}^{d}$ , $i=1,\\ldots,N$ , $j=0,\\ldots,k-1$ , that is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{D}_{k}:=\\{(U_{i-1}^{j},(\\mathcal{F}-\\mathcal{G})(U_{i-1}^{j})),\\ i=1,\\ldots,N,\\ j=0,\\ldots,k-1\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While Parareal relies on one observation to construct the correction $\\widehat{f}$ in (3), GParareal and following works, including this one, use all the discrepancy terms $\\mathscr{F}-\\mathscr{G}$ an d information in $\\mathcal{D}_{k}$ to make their predictions. The idea of GParareal is to learn the map $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ , $U_{i-1}^{k}\\mapsto(\\mathcal{F}-\\mathcal{G})(U_{i-1}^{k})$ , via $d$ independent scalar GPs $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , $U_{i-1}^{k}\\mapsto\\widehat{f}_{\\mathrm{GPara}}^{(s)}(U_{i-1}^{k})$ , $s=1,\\ldots,d$ , one per ODE dimension, whose predictions are concatenated into $\\widehat{f}_{\\mathrm{GPara}}(U_{i-1}^{k})=(\\widehat{f}_{\\mathrm{GPara}}^{(1)}(U_{i-1}^{k}),\\ldots,\\widehat{f}_{\\mathrm{GPara}}^{(d)}(U_{i-1}^{k}))^{\\top}\\in$ $\\mathbb{R}^{d}$ , and finally plugged into the predic tor-corrector rule  (2). In particular,  each GP prediction $\\widehat{f}_{\\mathrm{GPara}}^{(s)}(U_{i-1}^{k})$ is obtained as the GP posterior mean $\\mu_{\\mathcal{D}_{k}}^{(s)}(U_{i-1}^{k})\\in\\mathbb{R}$ , computed by conditioning the corresponding GP prior on the dataset $\\mathcal{D}_{k}$ , i.e. $\\begin{array}{r}{\\widehat{f}_{\\mathrm{GPara}}^{(s)}(U_{i-1}^{k})\\,=\\,\\mu_{\\mathcal{D}_{k}}^{(s)}(U_{i-1}^{k})}\\end{array}$ \u00b5(Dsk)(U ik\u22121). We refer to Supplementary Material B and [57] for a thorough description of the algorithm, including all relevant quantities of interest, namely the $d$ GP priors, the likelihood, the hyperparameters and their optimization procedure, and an explicit expression of the posterior means. Here, it is worth highlighting that the GPs are trained once per iteration to leverage the new incoming data, and then their predictions are used to sequentially update the initial conditions in (2). Using all information stored in $\\mathcal{D}_{k}$ instead of a single observation (as for Parareal) is the primary driver of faster convergence rates experienced by GParareal. Other benefits of this algorithm are increased stability to different initial conditions, the ability to incorporate legacy data (that is, the possibility of using datasets coming from previous runs of the algorithm with different starting conditions or settings, leading to faster convergence), lower sensitivity to poor choices of the coarse solver $\\mathcal{G}$ , and the possibility of parallelizing the training of the $d$ GPs over the $N$ available cores. The main drawback of GParareal is the heavy computational burden incurred when inverting the GP covariance matrices, which is of order $O(d(N k)^{3})$ at iteration $k$ . This negatively impacts the algorithm\u2019s wallclock time, which may be higher than Parareal despite a lower number of iterations needed to converge. This is why GParareal has been proposed mainly for low-dimensional ODE systems with a relatively small number of processors/intervals $N$ (up to hundreds), limiting its use and parallel scalability [57]. ", "page_idx": 4}, {"type": "text", "text": "The nnGParareal algorithm [21] has been proposed to tackle GParareal\u2019s scalability issue, sensibly reducing the computational time and memory footprint of GPs by using their nns version (nnGPs). In this framework, at iteration $k$ , the $d$ GPs are all trained on a smaller dataset of size $m$ , $\\mathcal{D}_{i-1,k}$ , composed out of the $m$ nns (in Euclidean distance) of $U_{i-1}^{k}$ in $\\mathcal{D}_{k}$ , leading to the nnGParareal correction $\\begin{array}{r}{\\widehat{f}_{\\mathrm{nnGPara}}(\\boldsymbol{U}_{i-1}^{k})=(\\widehat{f}_{\\mathrm{nnGPara}}^{(1)}(\\boldsymbol{U}_{i-1}^{k}),\\ldots,\\widehat{f}_{\\mathrm{nnGPara}}^{(d)}(\\boldsymbol{U}_{i-1}^{k}))^{\\top},}\\end{array}$ , with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{f}_{\\mathrm{nnGPara}}^{(s)}(U_{i-1}^{k})=\\mu_{\\mathcal{D}_{i-1,k}}^{(s)}(U_{i-1}^{k}),\\quad s=1,\\ldots,d.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, \u00b5Di\u22121,k $\\mu_{\\mathcal{D}_{i-1,k}}^{(s)}\\in\\mathbb{R}$ , $s=1,\\ldots,d,$ , denotes the nnGP posterior mean computed by conditioning the corresponding GP prior on the reduced dataset $D_{i-1,k}$ of size $m$ . Due to the decreased sample size, each nnGP covariance matrix can be inverted at a cost of $O(m^{3})$ independent of $k$ or $N$ . However, contrary to GParareal which trains the GPs once per iteration, the nnGPs are re-trained every time a new prediction $\\widehat{f}_{\\mathrm{nnGPara}}(U_{i-1}^{k})$ is made, which are at most $N-k$ at iteration $k$ (as at least $k$ intervals have converged at iteration $k$ ), yielding a combined $O(d(N-k)m^{3})$ complexity. Several experiments on different ODE and PDE systems have shown that $m\\in\\{15,\\ldots,20\\}$ offer accuracy comparable to the full GP [21] at a much lower cost. Although faster than GParareal, nnGParareal still exhibits some of the drawbacks inherited from the GP framework, such as the cost of optimizing the hyperparameters through a numerical maximization of a non-convex likelihood, and the use of $d$ scalar nnGPs. The latter is particularly critical. On the one hand, despite the possibility of training the $d$ nnGPs in parallel, the inversion of a $m\\times m$ matrix is so efficient that parallel overheads may outweigh the theoretical benefits. On the other hand, when solving PDEs, nnGParareal will incur additional costs due to insufficient hardware resources, as usually $d\\gg N$ , forcing the $d{\\mathrm{\\,nnGPs}}$ to queue among the $N$ available processors, which is why the algorithm has been proposed for highdimensional ODE and PDE systems with $d\\leq N$ . We refer to Supplementary Material B and [21] for more details on nnGParareal, and to Algorithm 2 in Supplementary Material A for the pseudocode of the nnGP training. In the next section, we address the nnGParareal issues by introducing RandNets. ", "page_idx": 4}, {"type": "text", "text": "4 Random neural networks Parareal (RandNets-Parareal) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In RandNet-Parareal, we propose to learn the map $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ , $U\\mapsto({\\mathcal{F}}-{\\mathcal{G}})(U)$ via RandNets, obtaining the RandNet-Parareal correction $\\widehat{f}_{\\mathrm{RandNet-Para}}$ , which we then use within the predictorcorrector rule (2). Prior to that, we define ho w RandNets work in a general setting with input $U\\in\\mathbb{R}^{d}$ ", "page_idx": 4}, {"type": "text", "text": "and output or target $\\pmb{Y}\\in\\mathbb{R}^{d}$ . Later in the text we will go back to the input of interest $U_{i}^{k}$ . Let $M$ denote the number of hidden neurons, and $H_{W}^{A,\\zeta}(U)$ be a single-hidden-layer feed-forward neural network used to learn $\\mathscr{F}-\\mathscr{G}$ , given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{W}^{A,\\varsigma}(U)=W^{\\top}\\pmb{\\sigma}(A U+\\zeta)\\in\\mathbb{R}^{d},\\quad U\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A\\,\\in\\,\\mathbb{R}^{M\\times d}$ is the matrix of random, non-trainable weights of the hidden layer, $\\boldsymbol{\\zeta}\\in\\mathbb{R}^{M}$ is a random non-trainable bias vector, and $\\dot{W}\\in\\mathbb{R}^{M\\times d}$ is the matrix of trainable output weights. Here, $\\pmb{\\sigma}:\\mathbb{R}^{M}\\rightarrow\\mathbb{R}^{M}$ denotes an activation function obtained as the componentwise application of a non-linear map $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ which we choose to be ReLU $\\sigma(x)=\\mathrm{max}(\\bar{x},0)$ with $x\\in\\mathbb R$ , to satisfy the assumption of Proposition 1 below. The entries of $A$ and $\\zeta$ are randomly sampled from given distributions $\\mathcal{P}_{A}$ and $\\mathcal{P}_{\\zeta}$ , respectively, and kept fixed. After observing the dataset $\\mathcal{D}_{k}$ , the output weights $W$ are obtained as the minimum $\\ell_{2}$ norm least squares (or simply min-norm least squares) estimator or as the solution of the following penalized empirical minimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{W}^{D_{k}}=\\operatorname*{lim}_{\\lambda\\rightarrow0}\\arg\\operatorname*{min}_{W\\in\\mathbb{R}^{M\\times d}}\\left\\{\\sum_{({\\cal U},{\\cal Y})\\in{\\cal D}_{k}}\\left\\|{\\cal H}_{W}^{A,\\varsigma}({\\cal U})-{\\cal Y}\\right\\|^{2}+\\lambda\\left\\|W\\right\\|_{\\mathrm{F}}^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is also called a \u201cridgeless\u201d (interpolation) estimator [30], and can be more compactly written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{W}^{\\mathcal{D}_{k}}=\\operatorname*{lim}_{\\lambda\\rightarrow0}\\left(X^{\\top}X+\\lambda\\mathbb{I}_{M}\\right)^{-1}X^{\\top}Y.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $X~\\in~\\mathbb{R}^{N k\\times M}$ is a matrix with $(X_{(l,\\cdot)})^{\\top}\\;:=\\;\\pmb{\\sigma}(A(U_{(l,\\cdot)})^{\\top}\\,+\\,\\zeta)$ , $l~=~1,\\ldots,N k$ , and $U,Y\\,\\in\\,\\mathbb{R}^{N k\\times d}$ are the collection of inputs and outputs of $\\mathcal{D}_{k}$ in matrix form,respectively, defined as $\\left(U_{(l,\\cdot)}\\right)^{\\top}=U_{i}^{j}$ , $\\begin{array}{r}{(Y_{(l,\\cdot)})^{\\top}=Y_{i}^{\\bar{j}}}\\end{array}$ , $=\\,j N+i+1,\\,i=0,\\ldots,N-1,\\,j=0,\\ldots,k-1$ . Whenever $N k\\geq M$ and the rank of $X^{\\top}X\\in\\mathbb{R}^{M\\times M}$ is $M$ , (7) reduces to the standard least squares estimator $\\widehat{W}^{\\cal D_{k}}=\\left(X^{\\top}X\\right)^{-1}X^{\\top}Y$ , while if the rank of $X^{\\top}X$ is $N k$ , the solution admits a closed form ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{W}^{\\mathscr{D}_{k}}=\\left(X^{\\top}X\\right)^{\\dagger}X^{\\top}Y.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We get inspired by [21], where only $m$ nns are used in the training. In this setting, $M\\gg N k=m$ , and in this overparametrized linear regression case, the ridgeless estimator interpolates the training data, which is a desirable feature since the problem is genuinely deterministic [29, 49]. ", "page_idx": 5}, {"type": "text", "text": "Several ingredients control the performance of RandNets, such as the dimension of the network $M$ and the choice of distributions $\\mathcal{P}_{A}$ and $\\mathcal{P}_{\\zeta}$ . In this work, we take the rows of the weight matrix $A$ and the bias entries of $\\zeta$ to be independent and uniformly distributed. For this case, the approximation bounds are available [25, Proposition 3], which we report below using our notation. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (Approximation bound, [25], Proposition 3). Let $H^{\\ast}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , $U\\longmapsto H^{*}(U)$ be an unknown function we wish to approximate with $H_{W}^{A,\\zeta}$ defined in (6). Suppose $H^{*}$ can be represented as $\\begin{array}{r}{H^{*}(U)=\\int_{\\mathbb{R}^{d}}e^{i\\left\\langle\\mathbf{w},U\\right\\rangle}g(\\mathbf{w})\\mathrm{d}\\mathbf{w}}\\end{array}$ for some complex-valued function $g$ on $\\mathbb{R}^{d}$ and all $U\\in\\mathbb{R}^{d}$ with $\\|U\\|\\leq Q$ , where $\\langle\\cdot,\\cdot\\rangle$ is the inner product on $\\mathbb{R}^{d}$ . Assume that $\\int_{\\mathbb R^{d}}$ $\\mathrm{\\ddot{\\p}^{\\,d}\\ m a x}\\left(1,\\|\\mathbf{w}\\|^{2d+6}\\right)|g(\\mathbf{w})|^{2}\\ d\\mathbf{w}<$ $\\infty$ . For $\\rho\\,>\\,0$ , suppose the rows of $A$ are i.i.d. random variables with uniform distribution on $B_{\\rho}\\subset\\mathbb{R}^{d}$ , the Euclidean ball of radius $\\rho$ around 0, and that the $M$ components of $\\zeta$ are i.i.d. uniform random variables on $[-\\operatorname*{max}(Q\\rho,1),\\operatorname*{max}(Q\\rho,1)]$ . Assume that $A$ and $\\zeta$ are independent and let $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ be given by $\\sigma(x)=\\operatorname*{max}(x,0)$ . Then, there exist a $\\mathbb{R}^{M\\times d}$ -valued random variable $W$ and an explicit (see (33) in [25]) constant $C^{*}>0$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|H_{W}^{A,\\varsigma}(U)-H^{\\ast}(U)\\|^{2}\\right]\\le\\frac{C^{\\ast}}{M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and for any $\\delta\\in(0,1)$ , the random neural network $H_{W}^{A,\\zeta}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\Big(\\int_{\\mathbb{R}^{d}}\\|H_{W}^{A,\\zeta}(U)-H^{*}(U)\\|^{2}\\mu_{U}(\\mathrm{d}U)\\Big)^{1/2}\\leq\\frac{\\sqrt{C^{*}}}{\\delta\\sqrt{M}}\\Big)\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our choice of $\\mathcal{P}_{A}$ and $\\mathcal{P}_{\\zeta}$ satisfies the conditions of Proposition 1 if $\\|\\pmb{U}\\|\\leq Q$ . If this is not met, we rescale the ODE/PDE system via a change of variables. We found these bounds empirically useful in informing a good choice for the sampling distribution, which we follow. If no prior ", "page_idx": 5}, {"type": "text", "text": "information were available, the common approach would have been to take $\\mathcal{P}_{A}\\sim\\mathrm{Unif}(-a,a)^{M\\times d}$ , $\\mathcal{P}_{\\zeta}\\sim\\mathrm{Unif}(-b,b)^{M}$ , and optimize $a,b\\in\\bar{\\mathbb{R}}^{+}$ via expensive cross-validation procedure. ", "page_idx": 6}, {"type": "text", "text": "Unlike nnGParareal, GParareal, and the corresponding nnGPs and GPs, training RandNets is so fast that parallelization across the $d$ dimensions is unnecessary. Hence, the predictions of the random network are computed jointly on all $d$ coordinates, yielding the RandNet-Parareal correction function ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{f}_{\\mathrm{RandNet-Para}}(\\pmb{U}_{i-1}^{k})=H_{\\widehat{W}^{\\mathscr{D}_{i-1,k}}}^{A,\\xi}(\\pmb{U}_{i-1}^{k})\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, the estimated weights $\\widehat{W}^{D_{i-1,k}}$ are obtained using the reduced dataset $\\mathcal{D}_{i-1,k}$ consisting of the $m_{\\mathrm{RandNet}}$ nns of $U_{i-1}^{k}$ , requiring the retraining of the RandNet for every prediction. Employing a multi-output model instead of independently training scalar-output models addresses one of the pitfalls of GPs, allowing for better scalability when $d\\gg N$ . The fact that training the RandNets reduces to a closed-form ridgeless interpolation solution presents a substantial difference and improvement with respect to (nn)GPs. Moreover, expensive hyperparameter optimization is avoided in RandNets, addressing the other major pitfall of GParareal and nnGParareal. The pseudocode for training RandNets is reported in Algorithm 3 in Supplementary Material A. ", "page_idx": 6}, {"type": "text", "text": "In Supplementary Material C, we derive the theoretical computational costs of nnGParareal and RandNet-Parareal, illustrating them as a function of dimension $d$ and number of processors $N$ in Figure 3. These theoretical findings confirm the significantly superior scalability of RandNet-Parareal which we observe in the numerical experiments reported in Section 5. ", "page_idx": 6}, {"type": "text", "text": "In Supplementary Material D, we study the robustness of RandNet-Parareal to changes in the number of nns $m_{\\mathrm{RandNet}}$ (and thus the input data size), the number of neurons $M$ , and the randomly sampled network weights $A,\\zeta$ . Intuitively, one might anticipate that a larger data sample would yield a more accurate approximation of the correction $\\mathscr{F}-\\mathscr{G}$ , and that a higher number of neurons $M$ would reduce the prediction error of RandNets (as in Proposition 1). One may also suspect the algorithm to be sensitive to the particular sampling seed. Remarkably, our empirical findings demonstrate that these factors have a limited impact on the number of iterations needed by RandNet-Parareal to converge, which remains largely consistent (up to a few iterations) across different values and ODE/PDE systems, for sensible choices of $m_{\\mathrm{RandNet}}$ and $M$ . For the end user, this eliminates the need of ad-hoc tuning, making the proposed RandNet-Parareal a convenient out-of-the-box algorithm. ", "page_idx": 6}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first compare the performance of Parareal, nnGParareal, and RandNet-Parareal on the viscous Burgers\u2019 equation (one spatial dimension and one variable, also considered in nnGParareal [21]), to showcase Parareal and nnGParareal challenges as the number of space discretization and, correspondingly, the dimensions $d$ , increases. Then, we consider the Diffusion-Reaction equation, a larger system defined on a two-dimensional spatial domain with two non-linearly coupled variables, and the SWEs (two spatial dimensions and three variables), representing a suitable framework for modeling free-surface flow problems on a two-dimensional domain. Two additional challenging systems, the 2D and 3D Brusselator PDEs, known for their complex behavior, including oscillations, spatial patterns, and chaos, are considered in Supplementary Material E. The simulation setups used for obtaining the results in this section are provided in Supplementary Material G, with the corresponding accuracies and runtimes for RandNet-Parareal, Parareal, and nnGParareal reported in Supplementary Material F. ", "page_idx": 6}, {"type": "text", "text": "Let $T_{\\mathcal{F}}$ and $T g$ be the time it takes to run $\\mathcal{F}$ and $\\mathcal{G}$ over one interval $[t_{i},t_{i+1}]$ , respectively, and let $N_{\\mathcal{F}}$ and $N_{\\mathcal{G}}$ denote the number of steps for the fine and coarse solvers over one interval, respectively. We can measure the parallel efficiency of an algorithm via its parallel speed-up $S_{\\mathrm{alg}}$ , defined as the ratio of the serial over the parallel runtime, i.e. $\\bar{S}_{\\mathrm{alg}}:=N T_{\\mathcal{F}}/\\bar{T}_{\\mathrm{alg}}$ . $S_{\\mathrm{alg}}$ captures the wallclock gains of parallel procedures and, unlike other quantities (such as the number of algorithm iterations needed to converge), also includes the model training cost. ", "page_idx": 6}, {"type": "text", "text": "5.1 Viscous Burgers\u2019 equation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our initial example is a non-linear, one-dimensional PDE (illustrated in Figure 7 of Supplementary Material H) exhibiting hyperbolic behavior [68], described by the equation ", "page_idx": 6}, {"type": "equation", "text": "$$\nv_{t}=\\nu v_{x x}-v v_{x},\\quad(x,t)\\in[-L,L]\\times[t_{0},t_{N}],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\nd=128,N=128\n$$", "text_format": "latex", "page_idx": 7}, {"type": "table", "img_path": "974ojuN0jU/tmp/6692ed934a2ec69cdb25253aef8aec7e1dd5a11498b917514d635f0fb1834f36.jpg", "table_caption": ["Table 1: Empirical scalability and speed-up analysis for viscous Burgers\u2019 equation "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Speed-up $S_{\\mathrm{alg}}$ of Parareal, nnGParareal $(m_{\\mathrm{nnGP}}{=}18)$ , and RandNet-Parareal $\\scriptstyle(m_{\\mathrm{RandNet}}=4$ , $M{=}100$ ) for the 1D viscous Burgers\u2019 equation. $T_{\\mathcal{F}}$ and $T g$ are the interval runtimes of the fine and coarse solvers, respectively, $K$ the number of iterations to converge, $T_{\\mathrm{model}}$ the overall time to evaluate $\\widehat{f}$ across $K$ iterations, including training and predicting, and $T_{\\mathrm{alg}}$ thealgorithm runtime. ", "page_idx": 7}, {"type": "text", "text": "with initial condition $v(x,t_{0})\\,=\\,v_{0}(x)$ , $x\\,\\in\\,[-L,L],L\\,>\\,0$ , and Dirichlet boundary conditions $v(-L,t)\\,=\\,v(L,t)$ , $v_{x}(-L,t)\\,=\\,v_{x}(L,t)$ , $t\\,\\in\\,[t_{0},t_{N}]$ . We use the same setting and parameter values as in [21]. More specifically, we choose $L=1$ , diffusion coefficient $\\nu=0.01$ , and discretize the spatial domain using finite difference [15] and equally spaced points $x_{j+1}=x_{j}+\\Delta x.$ , with $\\Delta x\\stackrel{=}{=}2L/d$ and $j=0,\\dots,d$ . We hence reformulate the PDE as a $d_{\\cdot}$ -dimensional ODE system. ", "page_idx": 7}, {"type": "text", "text": "In our first numerical experiment, we choose $N=d=128$ , $\\begin{array}{r}{v_{0}(x)=0.5(\\cos(\\frac{9}{2}\\pi x)+1)}\\end{array}$ , $t_{0}=0$ , and $t_{N}=5.9$ as in [21], and consider $\\mathcal{G}=\\mathrm{RK1}$ , $\\mathcal{F}=\\operatorname{RK}$ , $N_{\\mathcal{G}}=4$ and $N_{\\mathcal{F}}=4e^{4}$ , where RK1 stands for Runge-Kutta of order 1, and similarly for RK4 and RK8. The results, reported at the top of Table 1, show how RandNet-Parareal converges in fewer iterations and has a higher speed-up than Parareal and nnGParareal. The difference in the model training costs is striking, with the nnGP\u2019s being approximately 700 times higher than that of RandNets, reducing thus its potential speed-up. ", "page_idx": 7}, {"type": "text", "text": "As real-world (one-dimensional) problems would require a higher spatial discretization, we increase $d$ by one thousand to $d=1128$ , keeping $N$ fixed. Unlike assuming matching hardware resources to the system size (as implicitly done in [21], where $d=N$ ), we deliberately do not increase $N$ to assess the algorithms\u2019 performances under constrained conditions. Instead, both time discretization numbers are increased to $N_{\\mathcal{F}}=6e^{5}$ and $N_{\\mathcal{G}}\\,=\\,293$ (resulting thus in longer $T_{\\mathcal{F}}$ and $T g$ times) to account for the finer spatial mesh [43]. As observed from the bottom of Table 1, as $d/N>1$ , nnGParareal\u2019s issues become more pronounced, as the $d$ scalar GPs cannot be run all in parallel across the $N$ processors, but need $d/N=10$ runs instead, slowing down the algorithm. In contrast, RandNet-Parareal has a training cost comparable with the previous example, leading to an even higher speed-up, running in approximately 38 minutes compared to the almost 13 hours of Parareal. ", "page_idx": 7}, {"type": "text", "text": "5.2 Diffusion-Reaction system ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now turn to a more challenging case study. The Diffusion-Reaction equation [75] (illustrated in Figure 8 in Supplementary Material $_\\mathrm{H}$ ) is a system of two non-linearly coupled variables, the activator $u=u(t,x,y)$ and the inhibitor $v=v(t,x,y)$ , defined on a two-dimensional spatial domain as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\partial_{t}u=D_{u}\\partial_{x x}u+D_{u}\\partial_{y y}u+R_{u},\\quad\\partial_{t}v=D_{v}\\partial_{x x}v+D_{v}\\partial_{y y}v+R_{v}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here, $D_{u},D_{v}$ are the diffusion coefficients for the activator and inhibitor, respectively, and $R_{u}=$ $R_{u}(u,v)$ , $R_{v}=R_{v}(u,v)$ are their reaction functions defined by the Fitzhugh-Nagumo equation [42] ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{u}(u,v)=u-u^{3}-c-v,\\qquad R_{v}(u,v)=u-v,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $c=5e^{-3}$ , $D_{u}=1e^{-3}$ , and $D_{v}=5e^{-3}$ . We take $(x,y)\\in(-1,1)^{2}$ and $t\\in[0,20]$ . The initial condition $u(0,x,y)$ is generated as standard Gaussian noise. We apply a no-flow Neumann boundary condition $D_{u}\\partial_{x}u=0$ , $D_{v}\\partial_{x}v\\,=\\,0$ , $D_{u}\\partial_{y}u=0$ , $D_{v}\\partial_{y}v\\,=\\,0$ for $(x,y)\\in(-1,1)^{2}$ . The spatial domain is discretized by the finite volume method [51], resulting in a $d=2N_{x}N_{y}$ -dimensional ODE with $N_{x}$ and $N_{y}$ the number of space discretizations along $x$ and $y$ , respectively. The time integration is conducted with RK of variable order for $\\mathcal{G}$ and $\\mathcal{F}$ (see Table 6 in Supplementary Material G). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "As in the previous example, we conduct two experiments for this system, with speed-ups and runtimes reported in Figure 1. In the first one, we increased $d$ and $N$ proportionately (with $\\bar{d}/N\\in[11,13]$ ) while maintaining all other quantities (i.e. $\\mathcal{G},\\mathcal{F},m_{\\mathrm{nnGP}},m_{\\mathrm{RandNet}})$ fixed until $N\\,=\\,256$ . This scenario reflects a situation where more resources are allocated to solve larger problem sizes. In contrast, in the second experiment, $N$ remains fixed at 512, with $d$ increasing proportionately with $N_{\\mathcal{G}}$ to maintain algorithm stability. Moreover, $\\mathcal{F}$ is chosen to be RK8, with $N_{\\mathcal{F}}$ automatically selected by the used Python library scipy [77]. This second setting simulates a scenario with constrained resources, where the user aims to solve the system using a finer spatial mesh. Table 8 in Supplementary Material I shows that for $N\\geq256$ and $d/N\\gg1$ , nnGParareal fails to converge within a 48-hour budget. Parareal converges always, albeit at a considerably slower rate than RandNet-Parareal, which is $_{\\mathrm{X3-5}}$ faster than Parareal (and up to $_{\\textrm{X l20}}$ than the fine solver). ", "page_idx": 8}, {"type": "table", "img_path": "974ojuN0jU/tmp/08c08d8044c17ec3518e25dd7f8cae4547d3d61dadc24cf8d3fac6d785701904.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Shallow water equation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we focus on SWEs on a two-dimensional domain, described by a system of hyperbolic PDEs ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}h+\\nabla h\\mathbf{u}=0,\\quad\\partial_{t}h\\mathbf{u}+\\nabla(u^{2}h+\\frac{1}{2}g_{r}h^{2})=-g_{r}h\\nabla b,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{u}=(u,v)$ represents the velocities in the horizontal $u=u(t,x,y)$ and vertical $v=v(t,x,y)$ directions, $h=h(t,x,y)$ denotes the water depth, $b=b(x,y)$ describes a (given) spatially varying bathymetry, and $h\\mathbf{u}$ can be interpreted as the directional momentum components. The parameter $g_{r}$ describes the gravitational acceleration, while $\\partial_{t}f$ denotes the partial derivative with respect to time, and $\\nabla f$ the gradient of a function $f$ . Following [75], we solve a radial dam break scenario where a Gaussian-shaped water column (blue) inundates nearby plains (green) within a rectangular box subject to Neumann boundary conditions, causing the water to rebound off the sides of the box, as depicted in Figure 2. More details on the simulation setup are given in Supplementary Material G.1. ", "page_idx": 8}, {"type": "text", "text": "In this case, our algorithm also converges much faster than Parareal, with a speed gain of x1.3-3.6, while nnGParareal fails to converge within the 48-hour time budget as $d\\gg N$ . Although the speed gain is lower than for the Diffusion-Reaction, the improvements are remarkable. RandNet-Parareal takes up to 4-10 hours and 37 days less than the Parareal and sequential solver, respectively. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This study improves the scalability properties, convergence rates, and parallel performance of Parareal and a more recently proposed PinT solver for ODEs and PDEs, nnGParareal [21]. By replacing the nnGP with random networks, we decreased the model costs (in learning the discrepancy between the fine and coarse solvers) by several orders of magnitude. The reasons behind this are multi-fold. Training of RandNets is cheap due to the availability of the closed-form solution for its output (readout) weights, and avoids any expensive hyperparameter optimization. Moreover, it is possible to simultaneously learn and predict the $d\\!.$ -dimensional correction map instead of $d$ scalar maps (in parallel if the number of processors $N$ is comparable to $d$ , or queuing if smaller). The latter \u201cliberates\u201d RandNet-Parareal from requiring $d\\approx N$ , extending its application to high-dimensional settings, a key/notable improvement with respect to nnGParareal. We tested the proposed algorithm on systems of real-world significance, such as the Diffusion-Reaction equation, the SWE, and the Brusselator. solving them on a fine spatial mesh of up to $10^{5}$ discretization points. These systems and requirements align with those outlined in the benchmark PDE dataset [75] as necessary prerequisites for using such algorithms in practical scenarios. The strength of RandNet-Parareal is the cheap cost of RandNets, which can be embedded within Parareal with virtually no overhead, irrespective of the implementation or solvers, leading to notable speed gains over Parareal $(\\mathrm{x}8.6\u201321.2$ for viscous Burgers\u2019, $_{\\mathrm{X3-5}}$ for Diffusion-Reaction, x1.3-3.6 for SWE, and $\\mathrm{x}3.4\u20134.4$ for Brusselator). Moreover, training RandNets is easily conducted with established linear algebra routines, and requires no ad-hoc parameter tuning. ", "page_idx": 8}, {"type": "image", "img_path": "974ojuN0jU/tmp/3ba5246100daccb4bd35fd7a9644abcac4d30205f06b2c7a8ddd13932bb7b82f.jpg", "img_caption": ["Figure 2: Numerical solution of the SWE for $(x,y)\\in[-5,5]\\times[0,5]$ with $N_{x}=264$ and $N_{y}=133$ for a range of system times $t$ . Only the water depth $h$ (blue) is plotted. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "974ojuN0jU/tmp/ace1fba5a64a5c56c9c9b0a9aaf6e24fc037c99335d9c0069d7bf93d84da4a91.jpg", "table_caption": ["Table 2: Speed-up analysis for the shallow water PDE as a $d$ -dimensional ODE system, $N=235$ "], "table_footnote": ["$K$ \u00b7 is the number of iterations to converge, $T.$ wallclock time and $S$ \u00b7 speed-up for the Parareal (Para) and RandNet-Parareal $(m_{\\mathrm{RandNet}}{=}4$ , $M{=}100$ ). $T_{\\mathcal{F}}$ is the sequential runtime of $\\mathcal{F}$ . The results for nnGParareal ( $\\mathrm{\\Delta}m_{\\mathrm{nnGP}}{=}20)$ ) are not reported as it fails to converge within a 48-hour time budget. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Despite its excellent performance, RandNet-Parareal has limitations common to all Parareal algorithms, as its rate of convergence relies on the accuracy of the coarse solver $\\mathcal{G}$ . Although neural networks can help mitigate the impact of suboptimal choices of $\\mathcal{G}$ (as observed for GPs in (nn)GParareal), if the solver is mismatched for the system \u2014 for example, an unstable solver for a stiff ODE \u2014 RandNet-Parareal, similar to Parareal and (nn)GParareal, is likely to exhibit non-convergent behavior. It would then be of interest to investigate RandNet-Parareal\u2019s performance when using customized solvers tailored to specific systems, such as those outlined in Section 1 for the shallow water equation and the viscous Burgers\u2019 equation, which we defer to future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "GG is funded by the Warwick Centre of Doctoral Training in Mathematics and Statistics. GG thanks the hospitality of the University of St. Gallen where part of the results in this paper were obtained. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] N. Abel, J. Chaudhry, R. D. Falgout, and J. Schroder. Multigrid-reduction-in-time for the rotating shallow water equations. Technical report, Lawrence Livermore National Lab (LLNL), Livermore, CA (United States), 2020. [2] G. Ariel, S. J. Kim, and R. Tsai. Parareal multiscale methods for highly oscillatory dynamical systems. SIAM Journal on Scientific Computing, 38(6):A3540\u2013A3564, 2016.   \n[3] G. Bal. On the convergence and the stability of the parareal algorithm to solve partial differential equations. In Domain Decomposition Methods in Science and Engineering, pages 425\u2013432. Springer, 2005.   \n[4] G. Bal and Y. Maday. A \u201cparareal\u201d time discretization for non-linear PDE\u2019s with application to the pricing of an american put. In Recent Developments in Domain Decomposition Methods, volume 23 of Lecture Notes in Computational Science and Engineering, pages 189\u2013202. Springer, 2002. [5] W. Cao, X. Wang, Z. Ming, and J. Gao. A review on neural networks with random weights. Neurocomputing, 275:278\u2013287, 2018.   \n[6] L. Carratino, A. Rudi, and L. Rosasco. Learning with sgd and random features. Advances in neural information processing systems, 31, 2018. [7] F. Chen, J. S. Hesthaven, and X. Zhu. On the use of reduced basis methods to accelerate and stabilize the parareal method. Reduced Order Methods for modeling and computational reduction, pages 187\u2013214, 2014. [8] V. Csomor. Pararealml, 2023. URL https://pypi.org/project/pararealml/. [9] X. Dai and Y. Maday. Stable parareal in time method for first-and second-order hyperbolic systems. SIAM Journal on Scientific Computing, 35(1):A52\u2013A78, 2013.   \n[10] S. Dong and Z. Li. Local extreme learning machines and domain decomposition for solving linear and nonlinear partial differential equations. Computer Methods in Applied Mechanics and Engineering, 387:114\u2013129, 2021.   \n[11] V. Dwivedi and B. Srinivasan. Physics informed extreme learning machine (PIELM) \u2013 a rapid method for the numerical solution of partial differential equations. Neurocomputing, 391: 96\u2013118, 2020.   \n[12] W. R. Elwasif, S. S. Foley, D. E. Bernholdt, L. A. Berry, D. Samaddar, D. E. Newman, and R. Sanchez. A dependency-driven formulation of parareal: parallel-in-time solution of PDEs as a many-task application. In Proceedings of the 2011 ACM international workshop on Many task computing on grids and supercomputers, pages 15\u201324, 2011.   \n[13] M. Emmett and M. Minion. Toward an efficient parallel in time method for partial differential equations. Communications in Applied Mathematics and Computational Science, 7(1):105\u2013132, 2012.   \n[14] R. D. Falgout, S. Friedhoff, T. V. Kolev, S. P. MacLachlan, and J. B. Schroder. Parallel time integration with multigrid. SIAM Journal on Scientific Computing, 36(6):C635\u2013C661, 2014.   \n[15] B. Fornberg. Generation of finite difference formulas on arbitrarily spaced grids. Mathematics of Computation, 51(184):699\u2013706, 1988.   \n[16] S. Friedhoff, R. D. Falgout, T. V. Kolev, S. MacLachlan, and J. B. Schroder. A multigrid-in-time algorithm for solving evolution equations in parallel. University of North Texas Libraries, UNT Digital Library, 12 2012.   \n[17] M. J. Gander. 50 years of time parallel time integration. In T. Carraro, M. Geiger, S. K\u00f6rkel, and R. Rannacher, editors, Multiple Shooting and Time Domain Decomposition Methods, pages 69\u2013113, Cham, 2015. Springer International Publishing. ISBN 978-3-319-23321-5.   \n[18] M. J. Gander and S. Vandewalle. Analysis of the parareal time-parallel time-integration method. SIAM Journal on Scientific Computing, 29(2):556\u2013578, 2007.   \n[19] M. J. Gander, T. Lunet, D. Ruprecht, and R. Speck. A unified analysis framework for iterative parallel-in-time algorithms. SIAM Journal on Scientific Computing, 45(5):A2275\u2013A2303, 2023. doi: 10.1137/22M1487163.   \n[20] I. Garrido, B. Lee, G. Fladmark, and M. Espedal. Convergent iterative schemes for time parallelization. Mathematics of Computation, 75(255):1403\u20131428, 2006.   \n[21] G. Gattiglio, L. Grigoryeva, and M. Tamborrino. Nearest neighbors GParareal: Improving scalability of Gaussian processes for parallel-in-time solvers. arXiv:2405.12182v1, 2024.   \n[22] L. Gonon. Random feature neural networks learn Black-Scholes type PDEs without curse of dimensionality. Journal of Machine Learning Research, 24:1\u201351, 2023.   \n[23] L. Gonon and A. Jacquier. Universal approximation theorem and error bounds for quantum neural networks and quantum reservoirs. arXiv:2307.12904, 2023.   \n[24] L. Gonon and J.-P. Ortega. Reservoir computing universality with stochastic inputs. IEEE Transactions on Neural Networks and Learning Systems, 31(1):100\u2013112, 2020.   \n[25] L. Gonon, L. Grigoryeva, and J.-P. Ortega. Approximation bounds for random neural networks and reservoir systems. The Annals of Applied Probability, 33(1):28\u201369, 2023.   \n[26] L. Gonon, L. Grigoryeva, and J.-P. Ortega. Infinite-dimensional reservoir computing. Neural Networks, 179:106486, 2024.   \n[27] L. Grigoryeva and J.-P. Ortega. Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. Journal of Machine Learning Research, 19(24):1\u201340, 2018.   \n[28] L. Grigoryeva and J.-P. Ortega. Differentiable reservoir computing. Journal of Machine Learning Research, 20(179):1\u201362, 2019.   \n[29] Q. Han and X. Xu. The distribution of ridgeless least squares interpolators. arXiv:2307.02044, 2023.   \n[30] T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949 \u2013 986, 2022.   \n[31] T. Haut and B. Wingate. An asymptotic parallel-in-time method for highly oscillatory PDEs. SIAM Journal on Scientific Computing, 36(2):A693\u2013A713, 2014.   \n[32] Y.-L. He, X.-Z. Wang, and J. Z. Huang. Fuzzy nonlinear regression analysis using a random weight network. Information Sciences, 364:222\u2013240, 2016.   \n[33] C. Herrera, F. Krach, P. Ruyssen, and J. Teichmann. Optimal stopping via randomized neural networks. Frontiers of Mathematical Finance, 3(1):31\u201377, 2024.   \n[34] G.-B. Huang. An insight into extreme learning machines: random neurons, random features and kernels. Cognitive Computation, 6:376\u2013390, 2014.   \n[35] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew. Extreme learning machine: a new learning scheme of feedforward neural networks. In IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541), volume 2, pages 985\u2013990, 2004.   \n[36] G.-B. Huang, L. Chen, and C.-K. Siew. Universal approximation using incremental constructive feedforward networks with random hidden nodes. IEEE Transactions on Neural Networks, 17 (4):879\u2013892, 2006.   \n[37] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew. Extreme learning machine: theory and applications. Neurocomputing, 70(1-3):489\u2013501, 2006.   \n[38] A. Jacquier and Z. Zuric. Random neural networks for rough volatility. arXiv:2305.01035, 2023.   \n[39] J. Ji, H. Jiang, B. Zhao, and P. Zhai. Crucial data selection based on random weight neural network. In IEEE International Conference on Systems, Man, and Cybernetics, pages 1017\u2013 1022, 2015.   \n[40] B. Jin, Q. Lin, and Z. Zhou. Learning coarse propagators in parareal algorithm. arXiv:2311.15320, 2023.   \n[41] P. Kar and H. Karnick. Random feature maps for dot product kernels. In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics, volume 22 of Proceedings of Machine Learning Research, pages 583\u2013591, 2012.   \n[42] G. A. Klaasen and W. C. Troy. Stationary wave solutions of a system of reaction-diffusion equations derived from the Fitzhugh\u2013Nagumo equations. SIAM Journal on Applied Mathematics, 44(1):96\u2013110, 1984.   \n[43] R. J. LeVeque. Finite Difference Methods for Ordinary and Partial Differential Equations: Steady-State and Time-Dependent Problems. SIAM, 2007.   \n[44] M.-B. Li, G.-B. Huang, P. Saratchandran, and N. Sundararajan. Fully complex extreme learning machine. Neurocomputing, 68:306\u2013314, 2005.   \n[45] J.-L. Lions, Y. Maday, and G. Turinici. R\u00e9solution d\u2019EDP par un sch\u00e9ma en temps parar\u00e9el. Comptes Rendus de l\u2019Acad\u00e9mie des Sciences-Series I-Mathematics, 332(7):661\u2013668, 2001.   \n[46] J. Lu, J. Zhao, and F. Cao. Extended feed forward neural networks with random weights for face recognition. Neurocomputing, 136:96\u2013102, 2014.   \n[47] A. Lupo, L. Butschek, and S. Massar. Photonic extreme learning machine based on frequency multiplexing. Opt. Express, 29:28257\u201328276, 2021.   \n[48] R. Mart\u00ednez-Pe\u00f1a and J.-P. Ortega. Quantum reservoir computing in finite dimensions. Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics, 107(3): 035306, 2023.   \n[49] S. Mei and A. Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75:667\u2013766, 2019.   \n[50] M. Minion. A hybrid parareal spectral deferred corrections method. Communications in Applied Mathematics and Computational Science, 5(2):265\u2013301, 2011.   \n[51] F. Moukalled, L. Mangani, and M. Darwish. The Finite Volume Method in Computational Fluid Dynamics. Springer Cham, 1st edition, 2016.   \n[52] A. Neufeld and P. Schmocker. Universal approximation property of random neural networks. arXiv:2312.08410, 2023.   \n[53] A. Neufeld, P. Schmocker, and S. Wu. Full error analysis of the random deep splitting method for nonlinear parabolic PDEs and PIDEs with infinite activity. arXiv:2405.05192, 2024.   \n[54] A. S. Nielsen, G. Brunner, and J. S. Hesthaven. Communication-aware adaptive parareal with application to a nonlinear hyperbolic system of partial differential equations. Journal of Computational Physics, 371:483\u2013505, 2018.   \n[55] B. W. Ong and J. B. Schroder. Applications of time parallelization. Computing and Visualization in Science, 23(1):1\u201315, 2020.   \n[56] G. Pages, O. Pironneau, and G. Sall. The parareal algorithm for american options. SIAM Journal on Financial Mathematics, 9(3):966\u2013993, 2018.   \n[57] K. Pentland, M. Tamborrino, T. J. Sullivan, J. Buchanan, and L. C. Appel. GParareal: a time-parallel ODE solver using Gaussian process emulation. Statistics and Computing, 33(1): 23, 2023.   \n[58] K. Pentland, M. Tamborrino, D. Samaddar, and L. C. Appel. Stochastic parareal: an application of probabilistic methods to time-parallelization. SIAM Journal on Scientific Computing, 45: S82\u2013S102, 2022.   \n[59] B. Philippi and T. Slawig. The parareal algorithm applied to the FESOM 2 ocean circulation model. arXiv:2208.07598, 2022.   \n[60] B. Philippi and T. Slawig. A micro-macro parareal implementation for the ocean-circulation model FESOM 2. arXiv:2306.17269, 2023.   \n[61] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NeurIPS), volume 20. Curran Associates, Inc., 2007.   \n[62] A. Rahimi and B. Recht. Uniform approximation of functions with random bases. In 46th Annual Allerton Conference on Communication, Control, and Computing, page 555\u2013561. Curran Associates, Inc., 2008.   \n[63] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 21, page 1313\u20131320. Curran Associates, Inc., 2009.   \n[64] J. M. Reynolds-Barredo, D. E. Newman, R. S\u00e1nchez, D. Samaddar, L. A. Berry, and W. R. Elwasif. Mechanisms for the convergence of time-parallelized, parareal turbulent plasma simulations. Journal of Computational Physics, 231(23):7851\u20137867, 2012.   \n[65] A. Rudi and L. Rosasco. Generalization properties of learning with random features. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, 2017.   \n[66] D. Samaddar, D. E. Newman, and R. S\u00e1nchez. Parallelization in time of numerical simulations of fully-developed plasma turbulence using the parareal algorithm. Journal of Computational Physics, 229(18):6558\u20136573, 2010.   \n[67] D. Samaddar, D. P. Coster, X. Bonnin, L. A. Berry, W. R. Elwasif, and D. B. Batchelor. Application of the parareal algorithm to simulations of ELMs in ITER plasma. Computer Physics Communications, 235:246\u2013257, 2019.   \n[68] A. Schmitt, M. Schreiber, P. Peixoto, and M. Sch\u00e4fer. A numerical study of a semi-lagrangian parareal method applied to the viscous burgers equation. Computing and Visualization in Science, 19(1):45\u201357, 2018.   \n[69] F. C. Sheldon, A. Kolchinsky, and F. Caravelli. Computational capacity of lrc, memristive, and hybrid reservoirs. Physical Review E, 106:045310, Oct 2022.   \n[70] A. Sinha and J. C. Duchi. Learning kernels with random features. In Advances in Neural Information Processing Systems (NeurIPS), volume 29. Curran Associates, Inc., 2016.   \n[71] B. Song, J.-Y. Wang, and Y.-L. Jiang. Analysis of a new krylov subspace enhanced parareal algorithm for time-periodic problems. Numerical Algorithms, pages 1\u201322, 2023.   \n[72] G. A. Staff and E. M. R\u00f8nquist. Stability of the parareal algorithm. In Domain Decomposition Methods in Science and Engineering, pages 449\u2013456. Springer, 2005.   \n[73] J. G. C. Steinstraesser, P. da Silva Peixoto, and M. Schreiber. Parallel-in-time integration of the shallow water equations on the rotating sphere using parareal and MGRIT. Journal of Computational Physics, 496:112591, 2024.   \n[74] Y. Sun, A. Gilbert, and A. Tewari. But how does it work in theory? Linear SVM with random features. In Advances in Neural Information Processing Systems (NeurIPS), volume 31. Curran Associates, Inc., 2018.   \n[75] M. Takamoto, T. Praditia, R. Leiteritz, D. MacKinlay, F. Alesiani, D. Pfl\u00fcger, and M. Niepert. Pdebench: An extensive benchmark for scientific machine learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 1596\u20131611. Curran Associates, Inc., 2022.   \n[76] A. Turing. The chemical basis of morphogenesis. Philosophical Transactions of the Royal Society B, 237:37\u201372, 1952.   \n[77] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature methods, 17(3):261\u2013272, 2020.   \n[78] W. Wan, Z. Zhou, J. Zhao, and F. Cao. A novel face recognition method: Using random weight networks and quasi-singular value decomposition. Neurocomputing, 151:1180\u20131186, 2015.   \n[79] Y. Wang and S. Dong. An extreme learning machine-based method for computational pdes in higher dimensions. Computer Methods in Applied Mechanics and Engineering, 418:116578, 2024.   \n[80] C. K. Williams and C. E. Rasmussen. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006.   \n[81] Y. Yang, M. Hou, and J. Luo. A novel improved extreme learning machine algorithm in solving ordinary differential equations by Legendre neural network methods. Advances in Difference Equations, 429, 2018.   \n[82] L. Zhang and P. N. Suganthan. A survey of randomized algorithms for training neural networks. Information Sciences, 364:146\u2013155, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Pseudocodes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section provides pseudocodes for the implementation of Parareal (Algorithm 1), and the training procedure for learning the discrepancy $\\mathcal{F}-\\mathcal{G}$ via nnGPs in nnGParareal (Algorithm 2), and RandNets in RandNet-Parareal (Algorithm 3). ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1: Parareal (generic) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Initial condition $\\pmb{u}^{0}$ at time $t_{0}$ , number of intervals $N$   \nOutput: Converged initial conditions $\\{U_{i}^{K}\\}_{i=1}^{N-1}$ , with $K$ the number of iterations to convergence   \nInitialization   \nRescale the ODE/PDE system such that each coordinate takes values in $[-1,1]$   \n$L\\gets1$   \n$U_{0}^{0}=u^{0}$   \nfor $i\\gets1$ to $N-1$ do   \n$\\boxed{\\begin{array}{r l}\\end{array}}\\boxed{U_{i}^{0}\\leftarrow\\mathcal{G}(U_{i-1}^{0})}$   \nend   \nfor $k\\gets1$ to $N$ do Compute $\\mathcal{F}(U_{i-1}^{k-1})$ , $i=1,\\ldots,N$ in parallel for $i\\gets L+1$ to $N-1$ do $\\left|\\begin{array}{l}{U_{i}^{k}\\gets\\mathcal{G}(U_{i-1}^{k})+\\widehat{f}(U_{i-1}^{k})\\right.$ /\\* Update the initial conditions \\*/ end Convergence checks for $i\\gets L+1$ to $N-1$ do if $\\|\\boldsymbol{U}_{i}^{k}-\\boldsymbol{U}_{i}^{k-1}\\|_{\\infty}<\\epsilon$ then $L\\gets L+1$ /\\* Update converged interval counter \\*/ else break end end if $L==N$ then break /\\* All intervals have converged \\*/ end   \nend ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Algorithm 2: nnGP training procedure within nnGParareal ", "page_idx": 16}, {"type": "text", "text": "Input: Input $U_{i-1}^{k}$ , dataset $\\mathcal{D}_{k}$ , number of nearest neighbors $m_{\\mathrm{nnGP}}$ , number of random restarts for loss maximization $n_{\\mathrm{start}}$ ", "page_idx": 16}, {"type": "text", "text": "Output: Prediction $\\widehat{f}_{\\mathrm{nn}}(U_{i-1}^{k})$ of $\\big(\\mathcal{F}-\\mathcal{G}\\big)(U_{i-1}^{k})$ ", "page_idx": 16}, {"type": "text", "text": "Initialization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "$/*$ Find the $m_{\\mathrm{nnGP}}$ nns to $U_{i-1}^{k}$ , and compute the reduced dataset (10) \\*/ $\\mathcal{D}_{i-1,k}\\leftarrow\\{(U_{U_{i-1}^{k}}^{(l-\\mathrm{nn})},\\mathbf{Y}_{U_{i-1}^{k}}^{(l-\\mathrm{nn})}),~l=1,\\ldots,m_{\\mathrm{nnGP}}\\}\\subset\\mathcal{D}_{k}$ ", "page_idx": 16}, {"type": "text", "text": "$/*$ Both loops can be massively parallelized \\*/ for s \u21901 to N do $/*$ Training $\\ast/$ for $j\\leftarrow1$ to $n_{\\mathrm{start}}\\,\\mathbf{do}$ $/*$ Random restarts to avoid local minima when maximizing (12) $\\ast/$ Sample $\\theta_{j}^{0}$ at random Maximize (12) numerically using $\\theta_{j}^{0}$ as initial value; obtain $\\boldsymbol{\\theta}_{j}^{*}$ end Find $\\pmb{\\theta}^{*}$ such that $\\log p(\\widetilde{Y}_{(\\cdot,s)}|\\widetilde{U},\\pmb{\\theta}^{*})\\geq\\log p(\\widetilde{Y}_{(\\cdot,s)}|\\widetilde{U},\\pmb{\\theta}_{j}^{*}),\\quad j=1,\\ldots,n_{\\mathrm{start}}$ /\\* Predicting \\*/ Compute $\\mu_{\\mathcal{D}_{i-1,k}}^{(s)}(U_{i-1}^{k})$ with (11) using $\\pmb{\\theta}^{*}$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "end ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$\\widehat{f}_{\\mathrm{nn}}(\\pmb{U}_{i-1}^{k})\\gets(\\mu_{\\mathcal{D}_{i-1,k}}^{(1)}(\\pmb{U}_{i-1}^{k}),\\cdot\\cdot\\cdot,\\mu_{\\mathcal{D}_{i-1,k}}^{(d)}(\\pmb{U}_{i-1}^{k}))^{\\top}$ ", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Algorithm 3: RandNets training procedure within RandNet-Parareal ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: Input $U_{i-1}^{k}$ , dataset $\\mathcal{D}_{k}$ , number of neurons $M$ , number of nearest neighbors mRandNet Output: Prediction $\\widehat{f}_{\\mathrm{RandNet}}(U_{i-1}^{k})$ of $\\big(\\mathcal{F}-\\mathcal{G}\\big)(U_{i-1}^{k})$ ", "page_idx": 16}, {"type": "text", "text": "Initialization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Ensure each ODE/PDE coordinate takes values in $[-1,1]$   \n$/*$ Find the $m_{\\mathrm{RandNet}}$ nns to $U_{i-1}^{k}$ , and compute the reduced dataset (10) \\*/   \n$\\mathcal{D}_{i-1,k}\\leftarrow\\{(U_{U_{i-1}^{k}}^{(l-\\mathrm{nn})},\\mathbf{Y}_{U_{i-1}^{k}}^{(l-\\mathrm{nn})}),\\ \\ l=1,\\ldots,m_{\\mathrm{RandNet}}\\}\\subset\\mathcal{D}_{k}$   \nSample $A_{w,j}\\sim\\mathrm{Uniform}(-1,1)$ , $w=1,\\dots,M,j=1,\\dots,d$   \nSample $\\zeta_{w}\\sim\\mathrm{Uniform}(-1,1)$ , $w=1,\\dots,M$   \nLetX \u2208Rm\u00d7M ", "page_idx": 16}, {"type": "text", "text": "Training ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "/\\* Using broadcasting on $\\zeta\\ */$ ", "page_idx": 16}, {"type": "text", "text": "$\\widetilde{X}^{\\top}\\leftarrow\\pmb{\\sigma}(A\\widetilde{U}^{\\top}+\\pmb{\\zeta})$   \nif $\\operatorname{rank}(\\widetilde{X}^{\\top}\\widetilde{X})==M\\leq m$ then $\\widehat{W}^{{D_{i-1,k}}}\\gets(\\widetilde{X}^{\\top}\\widetilde{X})^{-1}\\widetilde{X}^{\\top}\\widetilde{Y}$   \nelse $\\widehat{W}^{D_{i-1,k}}\\gets(\\widetilde{X}^{\\top}\\widetilde{X})^{\\dagger}\\widetilde{X}^{\\top}\\widetilde{Y}$ ", "page_idx": 16}, {"type": "text", "text": "/\\* Least-squares estimator \\*/ /\\* Ridgeless interpolator \\*/ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "end ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Predicting ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{f}_{\\mathrm{RandNet}}(U_{i-1}^{k})\\gets(\\widehat{W}^{{D_{i-1,k}}})^{\\top}\\pmb{\\sigma}(A U_{i-1}^{k}+\\zeta)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Additional details on the nnGParareal correction function ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide more details on the nearest neighbors (nns) Gaussian process modeling, the mathematical expressions of the nnGParareal correction function $\\widehat{f}_{\\mathrm{nnGPara}}$ , and the reduced dataset $\\mathcal{D}_{i-1,k}$ . These are not explicitly presented in the main text as they  require additional notation, which we believe does not enrich the explanation. While the description of GPs presented here is for nnGParareal (and the corresponding nnGPs), it immediately generalizes to GParareal by replacing the reduced dataset $\\mathcal{D}_{i-1,k}$ with the full dataset $\\mathcal{D}_{k}$ . The interested reader can find more details in the original papers, [21] and [57]. ", "page_idx": 17}, {"type": "text", "text": "Let the set of inputs $U_{i-1}^{j}\\in\\mathbb{R}^{d}$ and outputs $(\\mathcal{F}-\\mathcal{G})(U_{i-1}^{j})\\in\\mathbb{R}^{d}$ , $i=1,\\dots,N,j=0,\\dots,k-1$ , collected by iteration $k$ , be denoted by $\\mathcal{U}_{k}$ and $\\mathcal{V}_{k}$ , respectively. Now, define $\\mathcal{D}_{i-1,k}$ as the restriction of $\\mathcal{D}_{k}$ to the $m$ nns of $U_{i-1}^{k}$ in $\\mathcal{U}_{k}$ , namely ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{D}_{i-1,k}:=\\{(U_{U_{i-1}^{k}}^{(l-\\mathrm{nn})},\\mathbf{Y}_{U_{i-1}^{k}}^{(l-\\mathrm{nn})}),~l=1,\\ldots,m\\}\\subset\\mathcal{D}_{k},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{Y}_{U_{i-1}^{k}}^{(l\\mathrm{-nn})}=\\big(\\mathcal{F}\\!-\\!\\mathcal{G})(U_{U_{i-1}^{k}}^{(l\\mathrm{-nn})})\\in\\mathcal{V}_{k}$ , and $U_{U_{i-1}^{k}}^{(l-\\mathrm{nn})}$ is the lth nn of $U_{i-1}^{k}$ in $\\mathcal{D}_{k}$ , i.e. the $l$ th ordered statistics of the set formed out of Euclidean distances $\\lVert U_{i-1}^{j}-U^{\\prime}\\rVert$ between $U_{i-1}^{j}$ and any $U^{\\prime}\\in\\mathcal{U}_{k}$ . That is, there exists $U_{1},\\ldots,U_{l}=U_{U_{i-1}^{k}}^{(l-\\mathrm{nn})}\\in\\mathcal{U}_{k}$ such that, for any $\\pmb{U}^{\\prime}\\in\\mathcal{U}_{k},\\pmb{U}^{\\prime}\\neq\\pmb{U}_{r},r=1,...\\,,l,$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|U_{i-1}^{j}-U_{1}\\|\\le...\\le\\|U_{i-1}^{j}-U_{l-1}\\|\\le\\|U_{i-1}^{j}-U_{l}\\|\\le\\|U_{i-1}^{j}-U^{\\prime}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, let $\\widetilde{U},\\widetilde{Y}\\in\\mathbb{R}^{m\\times d}$ be the matrices of input nns and outputs collected in $\\mathcal{D}_{i-1,k}$ , respectively. In nnGParareal, following the Bayesian framework, a GP prior is placed over the correction function $\\mathcal{F}-\\mathcal{G}$ for each of the $d$ coordinates as ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\mathcal{F}-\\mathcal{G})_{s}\\sim G P(\\mu_{\\mathrm{GP}}^{(s)},\\mathcal{K}_{\\mathrm{GP}}),\\;\\;s=1,\\dots,d,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mu_{\\mathrm{GP}}^{(s)}\\,:\\,\\mathbb{R}^{d}\\,\\to\\,\\mathbb{R}$ is the prior mean function, taken to be zero for all $s\\;=\\;1,\\ldots,d$ , and $\\mathcal{K}_{\\mathrm{GP}}:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is the exponential prior variance kernel function ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{K}_{\\mathrm{GP}}(\\boldsymbol{U},\\boldsymbol{U}^{\\prime})=\\sigma_{\\mathrm{o}}^{2}\\exp(-\\|\\boldsymbol{U}-\\boldsymbol{U}^{\\prime}\\|^{2}/\\sigma_{\\mathrm{in}}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\sigma_{\\mathrm{in}}^{2}$ and $\\sigma_{\\mathrm{o}}^{2}$ denoting the input and output length scales, respectively. Differently from the prior mean, the prior variance is the same across the $d$ components. Then, each nnGParareal prediction $\\widehat{f}_{\\mathrm{nnGPara}}^{(s)}(U_{i-1}^{k})\\in\\mathbb{R}$ , $s=1,\\ldots,d$ , is obtained from the GP posterior mean \u00b5(Dsi)\u22121,k(U ik\u22121) \u2208R, computed on the reduced dataset $\\mathcal{D}_{i-1,k}$ , given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{f}_{\\mathrm{nnGPara}}^{(s)}(U_{i-1}^{k})=\\mu_{\\mathcal{D}_{i-1,k}}^{(s)}(U_{i-1}^{k}):=K(\\widetilde{U},U_{i-1}^{k})^{\\top}(K(\\widetilde{U},\\widetilde{U})+\\sigma_{\\mathrm{reg}}^{2}\\mathbb{I}_{m})^{-1}\\widetilde{Y}_{(\\cdot,s)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{K}(\\widetilde{U},U_{i-1}^{k})\\in\\mathbb{R}^{m}$ is a vector of covariances between every input collected in $\\widetilde{U}$ and $U_{i-1}^{k}$ defined as $(\\mathcal{K}(\\widetilde{U},U_{i-1}^{k}))_{r}=\\mathcal{K}_{\\mathrm{GP}}((\\widetilde{U}_{(r,\\cdot)})^{\\top},U_{i-1}^{k})$ , $r=1,\\hdots,m$ , and $\\mathcal{K}(\\widetilde{U},\\widetilde{U})\\in\\mathbb{R}^{m\\times m}$ is the covariance matrix, with $(\\boldsymbol{\\mathcal{K}}(\\widetilde{U},\\widetilde{U}))_{q,r}\\,=\\,\\boldsymbol{\\mathcal{K}}_{\\mathrm{GP}}((\\widetilde{U}_{(q,\\cdot)})^{\\top},(\\widetilde{U}_{(r,\\cdot)})^{\\top})$ , $r,q\\,=\\,1,\\hdots,m$ . Here, $\\sigma_{\\mathrm{reg}}^{2}$ denotes a regularization term, al so known as nugget, jitter, or regularization strength, which is added to improve the numerical stability when computing the inverse matrix, see [21] for further details. The hyperparameters $\\pmb{\\theta}:=(\\sigma_{\\mathrm{in}}^{2},\\sigma_{\\mathrm{o}}^{2},\\dot{\\sigma}_{\\mathrm{reg}}^{2})$ entering into the posterior mean and prediction (11) control the performance of the GP, and are optimized by numerically maximizing the marginal log-likelihood: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log p(\\widetilde{Y}_{(\\cdot,s)}|\\widetilde{U},\\pmb{\\theta})\\propto-\\widetilde{Y}_{(\\cdot,s)}^{\\top}(K(\\widetilde{U},\\widetilde{U})+\\sigma_{\\mathrm{reg}}^{2}\\mathbb{I}_{m})^{-1}\\widetilde{Y}_{(\\cdot,s)}-\\log\\operatorname*{det}(K(\\widetilde{U},\\widetilde{U})),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\boldsymbol{\\kappa}(\\cdot,\\cdot)$ depends on $\\pmb{\\theta}$ through the kernel $\\kappa_{\\mathrm{GP}}$ , and $\\operatorname*{det}(A)$ denotes the determinant of a square matrix $A$ . For a thorough treatment of Gaussian processes, including derivation of the likelihood and of the posterior distribution (which is Gaussian with mean as in (11), see [80]. ", "page_idx": 17}, {"type": "text", "text": "C Computational complexity analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Consider the $d$ -dimensional initial value problem (1) for some (O/P)DE. Let $N$ be the number of subintervals (data points) at each $k$ th iteration of the PinT algorithm. For any kth iteration of the scheme, a total of $N k$ data points, each $d$ -dimensional, are available. Here, we provide the computational cost of RandNet-Parareal, and compare it to that of nnGParareal, the state-of-the-art Parareal algorithm proposed in [21]. Both RandNet-Parareal and nnGParareal use only the reduced data set of $m$ nns to a given point to construct its image-prediction via (2). Note that the $m$ nns (in Euclidean distance) to some point $\\pmb{U}\\in\\mathbb{R}^{d}$ among $N k$ available points are found at a cost which is at most linear in the sample size, that is $O(m N k)$ (for moderate dimensions $d$ , one can get an improved cost $O(m\\log(N\\bar{k}))$ , logarithmic in the sample size) [21]. Since our goal is to compare the computational complexities of nnGParareal and RandNet-Parareal as a function of $d$ , we consider the worst-case complexity of the nns search. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Given an input $U_{i-1}^{k}\\in\\mathbb{R}^{d}$ , $i=1,\\ldots,N$ at iteration $k$ , the computational model cost of a prediction $U_{i-1}^{k}$ produced by all $d$ models of $m_{\\mathrm{nnGP}}$ -nnGPs at iteration $k$ via the predictor-corrector rule (2) with nnGParareal correction (11) and $m_{\\mathrm{nnGP}}$ nns is given in [21] as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Gamma_{\\mathrm{nnGP}}(k)\\leq C_{\\mathrm{nnGP}}N k(n_{\\mathrm{start}}n_{\\mathrm{reg}}\\frac{d}{N}\\vee1)\\times}}\\\\ &{\\qquad\\big(\\underbrace{m_{\\mathrm{nnGP}}d}_{B:=K(U,U_{i-1}^{k-1})^{\\top}}+\\underbrace{m_{\\mathrm{nnGP}}^{2}d}_{C:=K(U,U)}+\\underbrace{m_{\\mathrm{nnGP}}^{3}}_{D:=(B+\\sigma_{\\mathrm{reg}}^{2}\\mathbb{I}_{m_{\\mathrm{nnGP}}})^{-1}}+\\underbrace{m_{\\mathrm{nnGP}}^{2}}_{B\\cdot D}+\\underbrace{m_{\\mathrm{nnGP}}d}_{B\\cdot Y}+\\underbrace{m_{\\mathrm{nnGP}}N k}_{\\mathrm{nearsueighbors}}\\big)}\\\\ &{\\qquad=C_{\\mathrm{nnGP}}N k(n_{\\mathrm{reg}}n_{\\mathrm{start}}\\frac{d}{N}\\vee1)(m_{\\mathrm{nnGP}}^{3}+m_{\\mathrm{nnGP}}^{2}+d(m_{\\mathrm{nnGP}}^{2}+2m_{\\mathrm{nnGP}})+m_{\\mathrm{nnGP}}N k)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $C_{\\mathrm{nnGP}}$ being some constant that in general does depend on $k$ , $m_{\\mathrm{nnGP}}$ , and $d$ . Also, $n_{\\mathrm{reg}}$ and $n_{\\mathrm{start}}$ correspond to the number of random restarts and the number of explored values of the regularization penalty in the kernel regression (associated to the hyperparameter optimization (see [21, Section 4.5]), respectively. Furthermore, $\\vee$ is the maximum operator, and the factor $(n_{\\mathrm{start}}n_{\\mathrm{reg}}d/N\\vee1)\\ \\geq\\ 1$ follows from the fact that $d$ independent nnGPs and hyperparameter optimization are parallelized over the $N$ cores. ", "page_idx": 18}, {"type": "text", "text": "In RandNet-Parareal, the correction term $\\widehat{f}_{\\mathrm{RandNet-Para}}$ is modeled by the random weights neural network and evaluated as (8). Again, only $m_{\\mathrm{RandNet}}$ nns (in Euclidean distance) to $U_{i-1}^{k}$ ik\u22121 are used to construct the prediction, leading to the following computational model cost at iteration $k$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{RandNet}}(k)\\le C_{\\mathrm{RandNet}}N k\\frac{1}{N}(\\underbrace{M d m_{\\mathrm{RandNet}}}_{X:=\\sigma(A\\cdot U+\\zeta)}+\\underbrace{M^{2}m_{\\mathrm{RandNet}}}_{\\Sigma:=X\\cdot X^{\\top}}+\\underbrace{M r^{2}}_{\\Sigma^{\\dagger}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\underbrace{M m_{\\mathrm{RandNet}}d}_{\\Sigma^{\\dagger}\\cdot X}+\\underbrace{M^{2}d}_{W:=\\Sigma^{\\dagger}X\\cdot Y}+\\underbrace{M d m_{\\mathrm{RandNet}}}_{W^{\\top}\\cdot X}+\\underbrace{m_{\\mathrm{RandNet}}N k}_{\\mathrm{nearst~neighbors}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=C_{\\mathrm{RandNet}}k(M r^{2}+M^{2}m_{\\mathrm{RandNet}}+d(M^{2}+3M m_{\\mathrm{RandNet}})+m_{\\mathrm{RandNet}}N k)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $M$ is the number of hidden neurons, $r$ is the rank of the covariance of activated neurons $\\Sigma$ (mind that the pseudoinverse of $\\Sigma$ would contribute cubically in $m$ only if $\\Sigma$ is of full rank numerically, which is not observed empirically) and $C_{\\mathrm{RandNet}}$ is a constant independent on $N,k,M,d,$ , $m$ RandNet. The factor $1/N$ in the first inequality corresponds to parallelization over $N$ processors. ", "page_idx": 18}, {"type": "text", "text": "We note the following differences in costs between these two algorithms according to realistic situations: ", "page_idx": 18}, {"type": "text", "text": "\u2022 $d\\gg N$ in most relevant applications, especially for PDEs. Hence, $\\left(n_{\\mathrm{start}}n_{\\mathrm{reg}}d/N\\!\\vee\\!1\\right)\\gg1$ , limiting the benefits from parallelization for nnGParareal. In the considered experiments, we had access to a maximum of approximately $N=500$ processors, while we considered up to $d\\approx10^{5}$ . It is easy to see that $T_{\\mathrm{nnGP}}$ is quadratic in dimension $d$ , while $T_{\\mathrm{RandNet}}$ is only linear. This difference is mainly due to the factor $(n_{\\mathrm{start}}n_{\\mathrm{reg}}d/N\\vee1)\\geq1$ in $T_{\\mathrm{nnGP}}$ as opposed to $1/N$ in $T_{\\mathrm{RandNet}}$ .   \n\u2022 Although $M>m_{\\mathrm{nnGP}}$ , $M=100$ is sufficient for consistent performance across a range of systems, as shown in our numerical experiments.   \n\u2022 nnGParareal incurs additional cost due to hyperparameter optimization [21], necessary for tuning the kernel input and output scales and the regularization strength for each of the $d$ dimensions, which is performed by maximizing the loglikelihood. First, to explore the parameter space and allow for multiple starting points given the nonconvex optimization ", "page_idx": 18}, {"type": "image", "img_path": "974ojuN0jU/tmp/d89ea94e194c1d6ed1b5215fba9ede10908c2fd7ca914a66bb4a21dfe51a7632.jpg", "img_caption": ["Figure 3: Theoretical model cost (panel A) and theoretical total cost (panel B), as functions of the dimension $d$ (and the corresponding $N$ ). The results are reported in terms of $\\mathrm{log_{10}(h o u r s)}$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "problems, both $n_{\\mathrm{reg}}$ and $n_{\\mathrm{start}}$ should be set large. Second, each loglikelihood maximization conducted per dimension of the system requires a large number of iterations performed sequentially. Hence, $C_{\\mathrm{nnGP}}\\gg C_{\\mathrm{RandNet}}$ , with $C_{\\mathrm{nnGP}}$ depending, in general, on $k$ , $m_{\\mathrm{nnGP}}$ $d$ , as opposed to $C_{\\mathrm{RandNet}}$ . Indeed, RandNet requires no tuning (a significant advantage with respect to GPs and nnGPs, making it more user-friendly), neither for the distribution of the random weights nor for the regularization parameter $\\lambda$ , due to the use of the ridgeless estimator. Empirically, we observed $C_{\\mathrm{nnGP}}/\\bar{C_{\\mathrm{RandNet}}}$ to be up to 1000 (this can be seen in Figure 1). ", "page_idx": 19}, {"type": "text", "text": "\u2022 We emphasize that since the ReLu function is chosen as activation in RandNet, the matrix $X$ of activated neurons is sparse with sparsity degree $\\gamma$ . Hence, the computational complexity $T_{\\mathrm{RandNet}}$ could be further improved, as the computational complexity of sparse operations is proportional to the number of nonzero elements in the matrix. We intentionally left these arguments out of the complexity analysis, since we do not use sparse operations in our code implementation. \u2022 The upper bound of $T_{\\mathrm{RandNet}}$ could potentially be improved further, as additional parallelization may occur during standard matrix operations, depending on the specific computing environment. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Figure 3 illustrates the theoretical model costs $T_{\\mathrm{nnGP}}$ and $T_{\\mathrm{RandNet}}$ (Panel A) and theoretical total costs obtained by adding the coarse and fine solver costs (Panel B), as functions of the dimension $d$ (and the corresponding $N$ ). The results are reported in terms of $\\mathrm{log_{10}(h o u r s)}$ . To calibrate the constants in both complexity bounds, we used the total empirical computational cost in Figure 1, together with its breakdown described in Table 8. Panel A shows that RandNet-Parareal displays significant improvement in scalability with respect to the state-of-the-art Parareal algorithm nnGParareal, while Panel B demonstrates that whenever the cost of the fine solver is added, our results are in full coherence with the empirical results. ", "page_idx": 19}, {"type": "text", "text": "D Robustness study ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we study the robustness of RandNet-Parareal to changes in the number of nns $m_{\\mathrm{RandNet}}$ , the number of neurons $M$ , and the randomly sampled values of neural network weights $A,\\zeta$ . Our empirical findings (for two of the three considered PDEs) demonstrate that the iterations $K_{\\mathrm{RandNet-Para}}$ to convergence for RandNet-Parareal remain largely consistent despite variations in these factors. This ensures robust performance across a broad spectrum of parameter values, reducing users\u2019 need for extensive tuning. For computational tractability, we limit the robustness analysis to relatively small systems, such as Burgers\u2019 equation with $d=128$ , and the Diffusion-Reaction equation with $d=722$ , conducting 100 weight samplings for each system. For every set of weights, we iterate RandNet-Parareal across $m_{\\mathrm{RandNet}}$ values ranging from 2 to 20, and $M$ values ranging from 20 to 500 in increments of 10. The proportions of iterations needed to converge across 100 runs for different values of $m_{\\mathrm{RandNet}}$ and $M$ for the Burger\u2019s and diffusion-Reaction equations are reported in Figures 4 and 5, respectively. Although we observe some minor differences between the two systems, the main trend is clear: as long as reasonable values of $m_{\\mathrm{RandNet}}$ and $M$ are chosen, the iterations to convergence for RandNet-Parareal vary at most by a few units when changing the values of $m_{\\mathrm{RandNet}}$ , $M$ or a particular sampling seed of weights. Nevertheless, larger $M$ might improve ", "page_idx": 19}, {"type": "image", "img_path": "974ojuN0jU/tmp/6e97925696b7037f20f8071cd140a5c3b24b1dce6cac3e5fc909bcdd444b5446.jpg", "img_caption": ["the performance, since, in this case, RandNets operate in the interpolation regime, as discussed in Section 4. ", "Figure 4: Histogram of the iterations to convergence $K_{\\mathrm{RandNet-Para}}$ of RandNet-Parareal for $d=128$ for Burgers\u2019 equation. We sample the network weights $A$ , $\\zeta\\;100$ times. For each set of weights, we run RandNet-Parareal for $m_{\\mathrm{RandNet}}\\in\\{2,3,\\ldots,20\\}$ and $M\\in\\{20,30,40,\\ldots,500\\}$ . The left and right panels show the aggregated histograms of $K_{\\mathrm{RandNet-Para}}$ versus $m_{\\mathrm{RandNet}}$ and $M$ , respectively. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "974ojuN0jU/tmp/49e96beb96d2da41c0d77e6495f4a81f14d7306ffec7f271a2558c8343a63526.jpg", "img_caption": ["Figure 5: Histogram of the iterations to convergence $K_{\\mathrm{RandNet-Para}}$ of RandNet-Parareal for $d=722$ for Diffusion-Reaction equation. We sample the network weights $A$ , $\\zeta\\;100$ times. For each set of weights, we run RandNet-Parareal for $m_{\\mathrm{RandNet}}\\in\\{2,3,\\ldots,20\\}$ and $M\\in\\{20,30,40,\\ldots,500\\}$ . The left and right panels show the aggregated histograms of $K_{\\mathrm{RandNet-Para}}$ versus $m_{\\mathrm{RandNet}}$ and $M$ , respectively. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Additional numerical experiments: 2D and 3D Brusselator PDE ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here, we carry out an additional scalability study for the 2 and 3 spatial dimensional Brusselator PDE. This model is a two-component reaction system that exhibits complex behavior, including oscillations, spatial patterns, and chaos. It is described by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial_{t}u=D_{0}\\nabla^{2}u+a-(1+b)u+v u^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}v=D_{1}\\nabla^{2}v+b u-v u^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In chemistry, the components $u,v$ refer to the concentration of two substances, whereas the constants $D_{0},D_{1}$ are the respective diffusivity of each component, indicating the rate at which the substances spread out in space. Moreover, the parameters $a$ and $b$ are related to reaction rates. In our experiments, we used $D_{0}=0.1$ , $D_{1}=0.1D_{0}$ , $a=1$ , and $b=3$ . We take $t\\in[0,35]$ , $(u,v)\\in(-1,1)^{2}\\,\\dot{\\times}\\,(-1,1)^{2}$ for the 2D Brusselator, and $(u,v)\\;\\in\\;(-1,1)^{3}\\,\\times\\,(-1,1)^{3}$ for the three spatial dimension case. We initialize the $u$ values at time $t\\,=\\,0$ by setting them equal to $a$ , and the $v$ values by taking them normally distributed over the spatial grid. Further details regarding the number of spatial discretizations, the number of intervals $N$ and the order of the solvers $\\mathcal{F}$ and $\\mathcal{G}$ is given in Table 3. Figure 6 highlights the strong scaling advantages of RandNet-Parareal compared to nnGParareal, setting $N=512$ and restricting the runtime budget to a maximum of 48 hours, as done in the other test cases. ", "page_idx": 20}, {"type": "table", "img_path": "974ojuN0jU/tmp/1482f0604d76065337810460c35bc95bb7bce22e85823a6ae2822bc2558181c6.jpg", "table_caption": ["Table 3: Simulation setup for the 2D and 3D Brusselator "], "table_footnote": ["$N_{u}$ and $N_{v}$ are the number of spatial discretization points for $u$ and $v$ along each spatial dimension, yielding a $d=2N_{x}^{2}$ - or $d=3\\bar{N}_{x}^{3}$ -dimensional ODE, depending on the considered system. $\\mathcal{G}$ and $\\mathcal{F}$ denote the coarse and fine solvers, respectively, while the $\\Delta t$ subscript refers to the timestep. The number of nns used for $\\mathcal{D}_{i-1,k}$ in nnGParareal and RandNet-Parareal are $m_{\\mathrm{nnGP}}\\,=\\,20$ and $m_{\\mathrm{RandNet}}=4$ , respectively. $N$ is the total number of intervals. "], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "974ojuN0jU/tmp/4f89c1427c879bf911470165020e70606f0340504ccd2ff7f85a73098e3a9e5c.jpg", "img_caption": ["Figure 6: Scalability study for the 2 and 3 spatial dimensional Brusselator PDE. We used $N=512$ and 48 hours runtime budget. nnGParareal for $\\log_{10}(d)=3.9$ is estimated, as the algorithm does not converge within the 48 hours runtime budget. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "F Accuracy and runtimes across models and algorithms ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Table 4 below, we report the accuracies and runtimes (shown in parentheses) for RandNet-Parareal, Parareal, and nnGParareal. The accuracy is measured with maximum absolute error (mean across intervals) with respect to the true solution obtained by running $\\mathcal{F}$ sequentially. Interestingly, all accuracies are far below the pre-defined accuracy level $\\epsilon$ , with RandNet-Parareal achieving the lowest one in all but one experiment, with much smaller runtimes across all case studies. ", "page_idx": 21}, {"type": "text", "text": "G Simulation setups ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This section summarizes the simulation setups used for producing the results discussed in Section 5 in the main text. The tables below report the space and time domain of the considered PDEs, the number of spatial discretization points $N_{x}$ (and $N_{y}$ , in case of two-dimensional spatial systems), the numerical solvers used for $\\mathcal{G}$ and $\\mathcal{F}$ , their corresponding numbers of time steps per interval, the number of intervals $N$ , and the number of nns used for nnGParareal $(m_{\\mathrm{nnGP}})$ and RandNetParareal $(m_{\\mathrm{RandNet}})$ . In particular, Table 5 refers to the viscous Burgers\u2019 equation, Table 6 to the Diffusion-Reaction equation, and Table 7 to the shallow water equations (SWEs). ", "page_idx": 21}, {"type": "table", "img_path": "974ojuN0jU/tmp/9496877660e0971ae97bb0dd064a41bd1e3057dda28c75eb7cdb76805ace4499.jpg", "table_caption": ["Table 4: Accuracy and computational cost of the three considered algorithms "], "table_footnote": ["Accuracy and computational cost comparison of RandNet-Parareal, Parareal, and nnGParareal for different PDEs, with runtimes reported in parentheses. The accuracy is measured as maximum absolute error (mean across intervals) with respect to $\\mathcal{F}$ run sequentially. "], "page_idx": 22}, {"type": "table", "img_path": "974ojuN0jU/tmp/35dfda74325bef27319c91dcf902a88e30905332336ecfe40a85df6f647f8cef.jpg", "table_caption": ["Table 5: Simulation setup for the viscous Burgers\u2019 equation "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "$N_{x}$ is the number of space discretizations, the same as $d$ here. $\\mathcal{G}$ and $\\mathcal{F}$ denote the chosen coarse and fine solvers, with corresponding time discretization steps per interval $N_{\\mathcal{G}}$ and $N_{\\mathcal{F}}$ , respectively. Here $N$ is the number of intervals, while $m_{\\mathrm{nnGP}}$ and $m_{\\mathrm{RandNet}}$ are the numbers of nns used to create $\\mathcal{D}_{i-1,k}$ for nnGParareal and RandNet-Parareal, respectively. ", "page_idx": 22}, {"type": "text", "text": "G.1 Simulation setup for the SWEs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we give more details on the radial dam break simulation of Section 5.3. Our domain consists of a rectangular box defined as $(x,y)\\in[-5,5]\\times[0,5]$ , which we evolve temporally over $t\\in[0,20]$ . Following [75], as an initial condition, we place a Gaussian-shaped column of water centered at $(x,y)=(-2.5,1.5)$ , with covariance matrix $\\Sigma=\\left(\\begin{array}{c c}{{0.25}}&{{0}}\\\\ {{0}}&{{0.25}}\\end{array}\\right)$ . We use Neumann boundary conditions, and evolve the system using $N=235$ intervals over four increasingly finer spatial meshes, as described in Table 7. We used the ParareaML [8] Python package to implement the SWEs and corresponding numerical solvers. ", "page_idx": 22}, {"type": "table", "img_path": "974ojuN0jU/tmp/4e9d185f16dcad6205aeaa2c35d42a3dcdaaa77c16f86fc1c73c3e7bdcafe150.jpg", "table_caption": ["Table 6: Simulation setup for the Diffusion-Reaction equation "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "$N_{x}$ and $N_{y}$ are the number of spatial discretization points for $x$ and $y$ , respectively, yielding a $d=2N_{x}\\bar{N_{y}}$ -dimensional ODE. $\\mathcal{G}$ and $\\mathcal{F}$ denote the coarse and fine solvers, respectively. The number of nns used for $\\mathcal{D}_{i-1,k}$ in nnGParareal and RandNet-Parareal are $m_{\\mathrm{nnGP}}=20$ and $m_{\\mathrm{RandNet}}=3$ , respectively. $N_{\\mathcal{G}}$ is the time discretization steps of $\\mathcal{G}$ per interval. $N_{\\mathcal{F}}=\\mathrm{NA}$ since $\\mathcal{F}$ \u2019s step size is chosen by scipy Runge-Kutta method [77]. ", "page_idx": 23}, {"type": "table", "img_path": "974ojuN0jU/tmp/1e6abfb65c03ab10ee33be2dbbf1bdcc78bdb7d65af075814d39c4092c58ae97.jpg", "table_caption": ["Table 7: Simulation setup for the SWEs "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "$N_{x}$ and $N_{y}$ are the number of spatial discretization points for $x$ and $y$ , respectively, leading to an ODE of dimension $d=3N_{x}N_{y}$ . $\\mathcal{G}$ and $\\mathcal{F}$ denote the chosen numerical coarse and fine solvers, respectively, with $N_{\\mathcal{G}}$ and $N_{\\mathcal{F}}$ being their corresponding time discretization steps per interval. In all cases, we set the number of nns used to create $\\mathcal{D}_{i-1,k}$ to $m_{\\mathrm{nnGP}}\\,=\\,20$ for nnGParareal, and $m_{\\mathrm{RandNet}}=3$ for RandNet-Parareal. ", "page_idx": 23}, {"type": "text", "text": "H Illustration of some PDE solutions ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "974ojuN0jU/tmp/70bfe1e887039bdf5f25dbe7c12032676a89b4c8229a4cdbc1f247c5b473785e.jpg", "img_caption": ["Figure 7: Numerical solution of viscous Burgers\u2019 equation over $(x,t)\\,\\in\\,[-1,1]\\,\\times\\,[0,5.9]$ with $d=1128$ and initial conditions and additional settings as described in Section 5.1. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "974ojuN0jU/tmp/45af057a2aa751667f8c5038a1943706b6aee32b71a2fbb13dd17865566f671f.jpg", "img_caption": ["Figure 8: Numerical solution of the Diffusion-Reaction equation over $(x,y)\\in[-1,1]^{2}$ with $N_{x}=$ $N_{y}=235$ for a range of system times $t$ . Only the activator $u(t,x,y)$ is plotted. The initial conditions and additional settings are as described in Section 5.2. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "I Additional simulation results for the Diffusion-Reaction equation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we complement the results of the speed-ups and wallclock times reported in Figure 1 in the main text, with a detailed breakdown of the number of iterations to convergence, the runtimes of the coarse and fine solvers, the overall cost of training the model (up to convergence), and the total runtime, reported in Table 8. ", "page_idx": 24}, {"type": "table", "img_path": "974ojuN0jU/tmp/c2ca2f4e4019e91c95d595359ba7dd818ca9b671aab8c5c954bd793d39b6c934.jpg", "table_caption": ["Table 8: Speed-up analysis for the Diffusion-Reaction equation "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Simulation study on the empirical scalability and speed-up of Parareal, nnGParareal (with $m_{\\mathrm{nnGP}}=$ 20), and RandNet-Parareal (with $m_{\\mathrm{RandNet}}=4$ and $M=100$ ) for the Diffusion-Reaction equation. $T_{\\mathcal{F}}$ and $T g$ refer to the runtimes per interval of the fine and coarse solvers, respectively, while $N T_{\\mathcal{G}}$ is the runtime of the coarse solver over $N$ intervals. $T_{\\mathrm{model}}$ corresponds to the overall time to evaluate $\\widehat{f}$ , including training and predicting, until convergence at iteration $K$ . $T_{\\mathrm{alg}}$ is the total algorithm r untime, while $S_{\\mathrm{alg}}$ is the parallel speed-up. \u201cFine\\*\u201d indicates that the total runtime has been estimated extrapolating data from the other algorithms. Missing nnGParareal rows for $d\\geq3362$ are due to convergence failure within a 48-hour time budget. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All claims, including but not limited to time gains, model training times, numbers of iterations to convergence, speed-ups, and scalability are backed up by empirical results reported in Tables 1, 2 and Figure 1 in the main text, and Figure 6 and Table 8 in Supplementary Material I. A comparison between theoretical and empirical results is also provided. These claims are stated in the abstract, introduction, and in the final section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: relevant limitations are discussed in Section 6, second paragraph. The robustness of the proposed algorithm to several factors, such as the number of neural networks $M$ , the number of nearest neighbors $m$ and the sampled random weights $A,\\zeta$ is introduced at the end of Section 4, and investigated in details in Supplementary Material D. The scaling performance of the algorithm with respect to the number of cores $N$ and model dimensions $d$ is extensively discussed in Section 5. Finally, a rescaling of the system is proposed if the data do not meet the condition $||U||\\leq Q$ in Theorem 1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 26}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Proposition 1 is clearly stated with all the required assumptions. The proof is not given, as we cite this result from [25], adjusting their notation to match our, as we clearly mention. The derivation of the computational complexity analysis is provided with all relevant details in Supplementary Material C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have taken care of ensuring the reproducibility of all results through a precise use of notation, and by detailing pseudocodes for the algorithms in Supplementary Material A. Additionally, we comprehensively describe the simulation setups both in the main text and in Supplementary Material G. Moreover, a link to a GitHub repository with a step-by-step Jupyter notebook outlining RandNet-Parareal, and the necessary code to reproduced the results has been provided in Section 1 in the main text. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All experimental results are fully reproducible, with code provided via a GitHub repository, with a link shared in Section 1 in the main text. Each simulation and its corresponding analysis are clearly labeled, and a step-by-step Jupyter notebook is provided to aid the reader in becoming familiar with the API\u2019s usage. The repository follows the best practices of the most common ML repositories. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the experimental setups, training details, and simulation parameters are described in the text, mainly in Sections 4 and 5. Moreover, they are also summarized in Supplementary Material G. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We acknowledge that error quantification for the speed-up might be of interest in some situations. However, given the runtime of our experiments, this would be too computationally expensive to obtain. Nevertheless, we reported two robustness studies for two different, smaller systems among the ones considered (Figures 4 and 5), where the performance of the algorithm is averaged across multiple runs. There, we display the more informative empirical distribution instead of just the error bars. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All results report the execution runtime. Details on the hardware used are provided in Section 1 in the main text. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have reviewed the Code of Ethics and found no particular area of concern regarding our research. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The potential positive impacts are implicitly mentioned in Section 1, and in Section 6 (when referring to having chosen systems of real-world significance, with the necessary prerequisites for using the proposed algorithm in practical scenarios). By enabling faster convergence times with minimal overhead, RandNet-Parareal can be applied to a wide range of applications, such as plasma physics simulation, weather forecasting (both mentioned in the introduction), leading to positive societal impacts. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We relied on publicly available models, simulating the relevant data as described in the main text and in Supplementary Material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All creators have been properly credited both in terms of published scientific papers, and publicly available code and libraries (e.g. for some specific Python libraries). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The code, simulations and associated analyses are publicly released with permissive licence. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper involves neither crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper involves neither crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]