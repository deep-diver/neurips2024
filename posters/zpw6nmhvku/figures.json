[{"figure_path": "zpw6NmhvKU/figures/figures_4_1.jpg", "caption": "Figure 1: Gradient boosting and RashomonGB with T iterations.", "description": "This figure illustrates the difference between standard gradient boosting and the proposed RashomonGB method. In gradient boosting, a sequence of weak learners (h1, ..., hT) are trained iteratively to minimize the residual error. In contrast, RashomonGB constructs an empirical Rashomon set for each iteration (R1(H, S1, h1*, \u03b5), ..., RT(H, ST, hT*, \u03b5)), where multiple models with similar performance are identified. The final model in RashomonGB is a combination of models selected from these empirical Rashomon sets.", "section": "3.1 Building Rashomon sets for gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_6_1.jpg", "caption": "Figure 2: Gradient boosting for binary classification. The conditional entropy of the residuals and the predictive multiplicity (measured by VPR) increase along with the boosting iteration, which matches Proposition 1.", "description": "This figure shows the results of a gradient boosting experiment on a binary classification task.  The x-axis represents the boosting iteration number (1 through 10). The y-axis on the left shows the loss (CE loss) and accuracy, while the y-axis on the right shows the conditional entropy g(R<sub>t</sub>|X) and the VPR (Viable Prediction Range), a measure of predictive multiplicity.  The plot demonstrates that as the number of boosting iterations increases, both the conditional entropy of the residuals and the predictive multiplicity (as measured by VPR) also increase. This observation supports Proposition 1 of the paper, which establishes a theoretical relationship between the size of the Rashomon set and data quality using mutual information.", "section": "3.3 The Rashomon effect in each iteration of gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_7_1.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps (h*) + \u0454 in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "The figure compares two methods for estimating predictive multiplicity metrics: re-training and RashomonGB.  It shows that RashomonGB, by generating a larger number of models within the Rashomon set (models with similar performance), provides more accurate estimates of these metrics, even under the same loss deviation constraints.  The leftmost column displays the accuracy vs. loss of the models generated by each method. The other four columns show the estimates of four different predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy)  for different loss deviation thresholds.", "section": "4 Applications of Rashomon gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_8_1.jpg", "caption": "Figure 4: Fairness-accuracy trade-off for re-training vs. RashomonGB on test set. Each marker represents a model. A better trade-off means a smaller group-fairness metric (MEO or SP) and a higher accuracy, i.e., the top left area. For both datasets, RashomonGB provides more better models to select that complies with the fairness constraints whilst having the highest accuracy. For UCI Adult, the CE loss of the models from RashomonGB is 0.38\u00b10.02 and 0.64\u00b10.02 for COMPAS.", "description": "This figure compares the fairness-accuracy trade-off achieved by re-training and RashomonGB on the UCI Adult and COMPAS datasets. Each point represents a model, with the x-axis showing the group fairness metric (Mean Equalized Odds for UCI Adult and Statistical Parity for COMPAS) and the y-axis showing accuracy.  A model that is both accurate and fair will be located in the top-left corner of the plot.  The figure shows that RashomonGB provides models with better fairness-accuracy trade-offs (closer to the top left) than re-training, especially for the COMPAS dataset.", "section": "4 Applications of Rashomon gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_9_1.jpg", "caption": "Figure 5: re-training vs. MS with \u03bb = 3 (left) and IE with E = 20 (right) to mitigate predictive multiplicity on 18 UCI datasets. Each point is averaged over 20 random train-test splits (std. omitted for clarity). Dashed lines are the mean of each axis. Higher values are better for both axes.", "description": "This figure compares the performance of two methods (MS and IE) for mitigating predictive multiplicity against a re-training baseline across 18 UCI datasets.  The x-axis represents the reduction in 0-disagreement (a metric of predictive multiplicity), and the y-axis shows the improvement in accuracy.  Each point represents an average over 20 train-test splits, and higher values on both axes indicate better performance. MS uses a model selection technique with reweighted losses, while IE uses intermediate ensembles during boosting iterations. The dashed lines represent the average performance across all datasets for each method.", "section": "4 Applications of Rashomon gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_19_1.jpg", "caption": "Figure 1: Gradient boosting and RashomonGB with T iterations.", "description": "This figure illustrates the difference between standard gradient boosting and the proposed RashomonGB method.  Gradient boosting iteratively builds a model by adding weak learners. RashomonGB extends this by incorporating multiple weak learners at each iteration, creating an ensemble of models at each step. The final RashomonGB model encompasses exponentially more model variations compared to standard gradient boosting, enabling a more comprehensive exploration of the Rashomon set.", "section": "3 Analyzing the Rashomon effect in gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_25_1.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps(h*) + \u2208 in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of two methods, re-training and RashomonGB, in estimating predictive multiplicity metrics.  The leftmost panel shows the accuracy vs. loss for individual models generated by each method. The remaining panels show the estimated values of four predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy) for different loss thresholds.  RashomonGB consistently provides better estimates because it explores a larger set of models within the Rashomon set.", "section": "4.1 Improving the estimation of predictive multiplicity metrics"}, {"figure_path": "zpw6NmhvKU/figures/figures_26_1.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps (h*) + \u0454 in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of the RashomonGB method against a re-training baseline in estimating predictive multiplicity metrics. The leftmost panel shows the accuracy and loss for individual models generated by each method.  The remaining panels display four predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy) calculated for each method across a range of loss difference thresholds.  The figure demonstrates that RashomonGB, despite having the same training cost as re-training, consistently provides more accurate estimations of these metrics due to its ability to explore a richer set of models within the Rashomon set.", "section": "4 Applications of Rashomon gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_27_1.jpg", "caption": "Figure E.9: Fraction of models in the Rashomon set in different number of boosting iterations T and CE loss constraints.", "description": "This figure shows the results of an ablation study on the number of boosting iterations (T) in the Rashomon gradient boosting algorithm.  The left panel displays the relationship between accuracy and cross-entropy (CE) loss for models generated at different iterations (T = 1 to 10).  The right panel shows how the percentage of models within the Rashomon set changes as the CE loss constraint varies, for different boosting iterations.  It illustrates that as the number of boosting iterations increases, the percentage of models in the Rashomon set also increases, especially at lower loss constraints.", "section": "E.4 Supporting experiments for Section 3.3"}, {"figure_path": "zpw6NmhvKU/figures/figures_28_1.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps (h*) + \u0454 in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of two methods, Re-training and RashomonGB, in estimating predictive multiplicity metrics.  It shows that RashomonGB, despite having the same training cost, provides a much richer set of models within the Rashomon set, leading to significantly more accurate estimations of the metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy) compared to Re-training, especially when the loss deviation constraint is relatively tight.", "section": "4.1 Improving the estimation of predictive multiplicity metrics"}, {"figure_path": "zpw6NmhvKU/figures/figures_28_2.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps (h*) + \u0454 in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of two methods for estimating predictive multiplicity metrics: re-training and RashomonGB.  The leftmost column shows the loss and accuracy for each model generated by the two methods.  The remaining columns present four predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy) calculated using the models from each method.  The y-axis represents the loss difference, and higher values indicate better estimates of multiplicity.  The figure demonstrates that RashomonGB generally provides more accurate estimates of these metrics compared to re-training, especially with tighter loss constraints.", "section": "4.1 Improving the estimation of predictive multiplicity metrics"}, {"figure_path": "zpw6NmhvKU/figures/figures_29_1.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps(h*) + \u2208 in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of two methods for estimating predictive multiplicity metrics: re-training and RashomonGB.  The leftmost column shows the accuracy and loss for each model obtained by the two methods. The other four columns show four different predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy).  The figure demonstrates that RashomonGB, while having the same training cost, provides more models within the Rashomon set leading to more accurate estimates of predictive multiplicity compared to the re-training method. The y-axis for the four rightmost plots represents the loss difference (Lps(h*) + \u2208). Higher values in the rightmost four columns show better estimates of predictive multiplicity.", "section": "4 Applications of Rashomon gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_29_2.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps(h*) + \u2208 in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of two methods, re-training and RashomonGB, in estimating predictive multiplicity metrics.  The leftmost panel shows the accuracy and loss for each model produced by each method. The other four panels show four different predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, and Discrepancy) calculated for different loss thresholds.  Each point in these panels represents a single model, and the plots demonstrate that RashomonGB tends to produce a richer set of models in the Rashomon set, allowing for more precise estimations of the metrics.", "section": "4.1 Improving the estimation of predictive multiplicity metrics"}, {"figure_path": "zpw6NmhvKU/figures/figures_30_1.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps(h*) + e in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of two methods, re-training and RashomonGB, in estimating predictive multiplicity metrics.  The leftmost panel shows the test accuracy and loss for individual models generated by each method. The remaining four panels display four different predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy)  for a range of loss difference thresholds (epsilon).  The results demonstrate that RashomonGB, by generating a more diverse set of models within the Rashomon set, yields more accurate estimates of predictive multiplicity compared to the standard re-training approach, especially when the loss constraint is tighter and fewer models are found using the re-training approach.", "section": "4 Applications of Rashomon gradient boosting"}, {"figure_path": "zpw6NmhvKU/figures/figures_31_1.jpg", "caption": "Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., Lps(h*)+e in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.", "description": "This figure compares the performance of RashomonGB and the re-training method in estimating predictive multiplicity metrics.  The leftmost column shows the accuracy and loss for each model generated by each method. The remaining four columns display four different predictive multiplicity metrics (VPR, Rashomon Capacity, Disagreement, Discrepancy) calculated for both methods, under various loss difference constraints. The plot demonstrates that RashomonGB produces significantly better estimates of these metrics compared to the re-training method, especially when the loss constraint is tighter, due to its ability to generate a more diverse range of models within the Rashomon set.", "section": "4 Applications of Rashomon gradient boosting"}]