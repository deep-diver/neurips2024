{"references": [{"fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-01", "reason": "This paper introduces BERT, a highly influential language model architecture that forms a crucial baseline for many multilingual tasks and is directly compared against in the current work."}, {"fullname_first_author": "A. Conneau", "paper_title": "XNLI: Evaluating cross-lingual sentence representations", "publication_date": "2018-10-01", "reason": "XNLI is a key benchmark dataset for cross-lingual understanding, used in this paper to evaluate the downstream performance of the proposed model."}, {"fullname_first_author": "M. Artetxe", "paper_title": "On the cross-lingual transferability of monolingual representations", "publication_date": "2019-10-01", "reason": "This work explores cross-lingual transfer, a central theme related to multilingual language models, providing crucial background for the proposed approach to multilingual fairness."}, {"fullname_first_author": "P. Nawrot", "paper_title": "Efficient transformers with dynamic token pooling", "publication_date": "2023-07-01", "reason": "This paper introduces the Hourglass Transformer architecture, directly built upon in the current paper, providing an essential technical foundation for the proposed MAGNET model."}, {"fullname_first_author": "R. Sennrich", "paper_title": "Neural machine translation of rare words with subword units", "publication_date": "2016-08-01", "reason": "This foundational work on subword tokenization is critically discussed and contrasted with the proposed method, highlighting the problem of over-segmentation that MAGNET aims to address."}]}