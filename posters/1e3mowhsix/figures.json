[{"figure_path": "1e3MOwHSIX/figures/figures_1_1.jpg", "caption": "Figure 1: MAGNET routes byte-level sequences via language-script specific boundary predictors. These predictors infer boundaries leading to equitable segmentation across languages. Prior work infers boundaries with a single predictor across languages and leads to over-segmentation.", "description": "The figure illustrates the key difference between MAGNET and prior work in terms of handling byte-level sequences for multilingual language modeling.  Prior methods use a single boundary predictor for all languages, leading to over-segmentation, especially in low-resource languages.  In contrast, MAGNET employs separate boundary predictors for different language scripts (Latin, Cyrillic, Indic), resulting in more equitable segmentation granularity across languages.", "section": "2 MAGNET: Multilingual Adaptive Gradient-Based Tokenization"}, {"figure_path": "1e3MOwHSIX/figures/figures_5_1.jpg", "caption": "Figure 2: Average number of tokens after segmenting the FLORES dataset. Subword tokenizers and DTP result in over-segmentation in non-Latin script languages, while MAGNET closes the gap.", "description": "This figure compares the average number of tokens produced by different tokenization methods across nine languages with varying scripts (Latin, Cyrillic, and Indic).  It shows that MAGNET significantly reduces the over-segmentation observed in non-Latin scripts with methods like Byte Pair Encoding (BPE) and Dynamic Token Pooling (DTP), leading to more equitable segmentation across languages.", "section": "4.1 MAGNET results in equitable segmentation across language scripts."}, {"figure_path": "1e3MOwHSIX/figures/figures_6_1.jpg", "caption": "Figure 2: Average number of tokens after segmenting the FLORES dataset. Subword tokenizers and DTP result in over-segmentation in non-Latin script languages, while MAGNET closes the gap.", "description": "This figure compares the average number of tokens produced by different tokenization methods (byte-level, subword tokenizers, Dynamic Token Pooling (DTP), and MAGNET) when segmenting sentences from the FLORES dataset.  It visually demonstrates that MAGNET achieves a more equitable segmentation across languages, particularly reducing the over-segmentation observed in non-Latin scripts with other methods.  The x-axis represents the different languages, and the y-axis represents the average number of tokens per sentence. The various lines represent the different tokenization methods.", "section": "4.1 MAGNET results in equitable segmentation across language scripts."}, {"figure_path": "1e3MOwHSIX/figures/figures_7_1.jpg", "caption": "Figure 4: Inference time per language in XQUAD, relative to the byte-level model. MAGNET'S inference time is shorter than the byte-level model and comparable to DTP for most of the languages.", "description": "This figure compares the inference time of different language models on the XQUAD question answering task.  The inference time for each model is relative to the byte-level baseline model. MAGNET, a multilingual adaptive gradient-based tokenization model, shows faster inference times compared to the byte-level model, and comparable times to Dynamic Token Pooling (DTP), particularly for English and Russian.  This demonstrates MAGNET's efficiency gains, especially noticeable for languages with complex scripts and structures.", "section": "4.2 MAGNET maintains performance on downstream tasks."}, {"figure_path": "1e3MOwHSIX/figures/figures_7_2.jpg", "caption": "Figure 2: Average number of tokens after segmenting the FLORES dataset. Subword tokenizers and DTP result in over-segmentation in non-Latin script languages, while MAGNET closes the gap.", "description": "This figure compares the average number of tokens produced by different tokenization methods (byte-level, subword tokenizers, DTP, and MAGNET) when segmenting sentences from the FLORES dataset.  It shows that subword tokenizers and the Dynamic Token Pooling (DTP) method, which uses a single boundary predictor across all languages, tend to over-segment non-Latin script languages. In contrast, the Multilingual Adaptive Gradient-Based Tokenization (MAGNET) method achieves more equitable segmentation across all languages.", "section": "4.1 MAGNET results in equitable segmentation across language scripts."}, {"figure_path": "1e3MOwHSIX/figures/figures_16_1.jpg", "caption": "Figure 6: Language statistics in the pretraining data.", "description": "This figure shows the size of the pretraining data in bytes for each of the nine languages used in the paper.  The languages are displayed on the x-axis, and the size of the data in bytes is displayed on the y-axis.  The data shows a relatively even distribution of data sizes across languages, with some minor variations.", "section": "C Dataset Statistics"}, {"figure_path": "1e3MOwHSIX/figures/figures_18_1.jpg", "caption": "Figure 7: Average number of tokens after segmenting the FLORES dataset. Evidently subword tokenizers and DTP result in over-segmentation in non-Latin script languages, while MAGNET closes the gap.", "description": "This figure compares the average number of tokens produced by different tokenization methods (byte-level, MAGNET, DTP, and subword tokenizers) across nine languages with varying scripts (Latin, Cyrillic, and Indic).  It demonstrates that MAGNET achieves more equitable segmentation across languages compared to other methods, particularly for non-Latin scripts which tend to be over-segmented by traditional methods like byte-level and subword tokenization.  The figure visually shows how MAGNET reduces the disproportionate number of tokens generated for low-resource languages (Indic scripts) compared to high-resource ones (Latin scripts).", "section": "4.1 MAGNET results in equitable segmentation across language scripts."}, {"figure_path": "1e3MOwHSIX/figures/figures_18_2.jpg", "caption": "Figure 2: Average number of tokens after segmenting the FLORES dataset. Subword tokenizers and DTP result in over-segmentation in non-Latin script languages, while MAGNET closes the gap.", "description": "This figure compares the average number of tokens produced by different tokenization methods (MAGNET, DTP, and subword tokenizers) across nine languages with varying scripts.  The results show that MAGNET achieves significantly more equitable segmentation than the other methods, which tend to over-segment non-Latin scripts.  MAGNET addresses the issue of over-segmentation, particularly in Indic languages, producing token counts that are relatively consistent across languages and scripts, indicating that the amount of information conveyed is relatively the same for the same amount of tokens across languages.", "section": "4.1 MAGNET results in equitable segmentation across language scripts."}, {"figure_path": "1e3MOwHSIX/figures/figures_18_3.jpg", "caption": "Figure 2: Average number of tokens after segmenting the FLORES dataset. Subword tokenizers and DTP result in over-segmentation in non-Latin script languages, while MAGNET closes the gap.", "description": "This figure compares the average number of tokens produced by different tokenization methods (byte-level, subword tokenizers, Dynamic Token Pooling (DTP), and MAGNET) when segmenting sentences from the FLORES dataset.  The results show that traditional subword tokenizers and DTP tend to over-segment non-Latin script languages, resulting in a much larger number of tokens than for Latin script languages. In contrast, the proposed MAGNET method achieves more equitable segmentation across languages, reducing the disparity in token counts between different scripts. This demonstrates MAGNET's ability to mitigate over-segmentation issues common in multilingual settings.", "section": "4.1 MAGNET results in equitable segmentation across language scripts."}]