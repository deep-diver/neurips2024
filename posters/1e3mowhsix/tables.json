[{"figure_path": "1e3MOwHSIX/tables/tables_4_1.jpg", "caption": "Table 1: Binomial prior choice for each MAGNET and DTP model configuration. These combinations of binomial priors determine the compression rate of sequences per language script. While DTP uses fixed priors for all languages, MAGNET is dynamic and script-specific.", "description": "This table shows the binomial priors used in the Dynamic Token Pooling (DTP) and MAGNET models for different language scripts (Latin, Cyrillic, and Indic).  DTP uses a single, fixed prior across all languages, while MAGNET uses different priors for each script to achieve more equitable segmentation. The different configurations reflect different compression rates for each language.", "section": "3 Experimental Setup"}, {"figure_path": "1e3MOwHSIX/tables/tables_6_1.jpg", "caption": "Table 2: The average performance (accuracy) on downstream tasks on all languages across different models. We present results for the best-performing MAGNET model: ((3, 6, 12) \u00d7 for PAWS-X and SIB), ((1, 2, 4) \u00d7 for XQUAD and XNLI). Bold indicates the best overall performance.Table 6 provides more detailed language-specific results.", "description": "This table presents the average accuracy scores achieved by different language models (Byte-level, DTP5x, DTP10x, and MAGNET) across several downstream tasks.  The best performing MAGNET configuration is highlighted for each task, showing its competitive performance compared to baseline models.", "section": "4.2 MAGNET maintains performance on downstream tasks."}, {"figure_path": "1e3MOwHSIX/tables/tables_7_1.jpg", "caption": "Table 2: The average performance (accuracy) on downstream tasks on all languages across different models. We present results for the best-performing MAGNET model: ((3, 6, 12) \u00d7 for PAWS-X and SIB), ((1, 2, 4) \u00d7 for XQUAD and XNLI). Bold indicates the best overall performance.Table 6 provides more detailed language-specific results.", "description": "This table presents the average accuracy results across nine different languages for four downstream tasks (XNLI, PAWS-X, SIB, XQUAD) and two dialectal tasks (Hascova and ILI).  It compares the performance of the best-performing MAGNET configurations against a byte-level baseline and two Dynamic Token Pooling (DTP) configurations.  The best-performing MAGNET configuration varied depending on the task. Bold numbers highlight the best overall accuracy for each task.", "section": "4.2 MAGNET maintains performance on downstream tasks"}, {"figure_path": "1e3MOwHSIX/tables/tables_8_1.jpg", "caption": "Table 6: Language-level results across all tasks", "description": "This table presents a detailed breakdown of the performance (accuracy) achieved by different language models across various downstream tasks.  The models compared include a byte-level baseline, two versions of Dynamic Token Pooling (DTP) with varying compression rates, and several configurations of MAGNET, each with distinct binomial priors.  The tasks encompass natural language inference (XNLI), question answering (XQUAD), paraphrase detection (PAWS-X), and topic classification (SIB-200). The table allows for a granular comparison of model performance across languages and tasks, highlighting the impact of different tokenization strategies on downstream task performance.", "section": "4.2 MAGNET maintains performance on downstream tasks"}, {"figure_path": "1e3MOwHSIX/tables/tables_15_1.jpg", "caption": "Table 5: Downstream language and task coverage", "description": "This table shows the downstream tasks used to evaluate the effectiveness of MAGNET and the languages included in each task.  The tasks encompass a range of natural language processing challenges, including natural language inference (XNLI), question answering (XQUAD), adversarial paraphrase identification (PAWS-X), and topic classification (SIB-200).  The languages used in each dataset are listed, illustrating the multilingual nature of the evaluation.", "section": "3.1 Language Modeling"}, {"figure_path": "1e3MOwHSIX/tables/tables_17_1.jpg", "caption": "Table 6: Language-level results across all tasks", "description": "This table presents the average performance (accuracy) across nine different languages (English, Spanish, French, Russian, Ukrainian, Belarusian, Telugu, Bengali, and Hindi) for various downstream tasks using different models.  The models compared include a Byte-Level baseline, Dynamic Token Pooling (DTP) with different compression rates (5x and 10x), and MAGNET with multiple configurations.  The table showcases the performance of each model for each language and highlights the overall average accuracy.", "section": "4.2 MAGNET maintains performance on downstream tasks"}, {"figure_path": "1e3MOwHSIX/tables/tables_17_2.jpg", "caption": "Table 6: Language-level results across all tasks", "description": "This table presents the average performance (accuracy) across different downstream tasks and languages, comparing various models including Byte-Level, DTP (Dynamic Token Pooling) with different compression rates, and MAGNET (Multilingual Adaptive Gradient-Based Tokenization) with several configurations.  The table shows the performance of each model on XNLI (Natural Language Inference), PAWS-X (Paraphrase Detection), XQUAD (Question Answering), and SIB-200 (Topic Classification) tasks.  It allows for a comparison of the different model architectures and hyperparameter settings on language-specific and overall performance.", "section": "4.2 MAGNET maintains performance on downstream tasks"}, {"figure_path": "1e3MOwHSIX/tables/tables_17_3.jpg", "caption": "Table 6: Language-level results across all tasks", "description": "This table presents the language-level accuracy results for several downstream tasks.  It compares the performance of different models: a byte-level model, two Dynamic Token Pooling (DTP) models with different compression rates, and five MAGNET models with varying binomial prior combinations. The tasks evaluated include XNLI (natural language inference), PAWS-X (paraphrase detection), XQUAD (question answering), and SIB-200 (topic classification). The table shows that MAGNET models, particularly those with equitable segmentation configurations, generally perform competitively with the byte-level models, which have higher computational cost, while significantly outperforming the DTP models.", "section": "4.2 MAGNET maintains performance on downstream tasks"}]