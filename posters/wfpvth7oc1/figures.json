[{"figure_path": "WfpvtH7oC1/figures/figures_0_1.jpg", "caption": "Figure 12: All skills discovered for AntMaze-M where color is darker for poses earlier in the trajectory. We see a range of linear motion and turning behaviors.", "description": "This figure visualizes the skills discovered by the proposed method for the AntMaze-Medium environment. Each skill is represented as a sequence of poses, with darker colors indicating earlier poses in the trajectory.  The figure shows a variety of skills, primarily involving linear motion and turning behaviors, demonstrating the ability of the method to extract a diverse set of interpretable skills.", "section": "4.2 Exploration Behavior on AntMaze Medium"}, {"figure_path": "WfpvtH7oC1/figures/figures_2_1.jpg", "caption": "Figure 2: Abstract representation of our method. Given demonstrations in the same action space as our downstream task, we discretize the actions and apply a tokenization technique to recover \"subwords\" that form a vocabulary of skills. We then train a policy on top of these skills for a new task. We only require a common action space between demonstrations and the downstream task.", "description": "This figure illustrates the overall process of the proposed method.  It starts with demonstrations (sequences of actions) from a similar task.  These are tokenized using a method analogous to Byte Pair Encoding (BPE) commonly used in natural language processing.  This tokenization process identifies recurring action sequences as \"skills\". These skills are then used as the new action space for reinforcement learning in a new task, allowing for improved sample efficiency and transferability across domains. Only a common action space between the original demonstrations and the new task is required. The method accelerates skill extraction and policy inference.", "section": "3 Method"}, {"figure_path": "WfpvtH7oC1/figures/figures_5_1.jpg", "caption": "Figure 4: A visualization of state visitation in online RL on AntMaze Medium in the first 1 million timesteps for (a) SAC-discrete, (b) SFP, (c) SSP, and (d) our method averaged over 5 seeds. The grey circle in the bottom-left denotes the start position, while the green circle in the top-right indicates the goal. Notice that our method explores the maze much more extensively, with exploration behavior that is similar for all five seeds. SAC's visitation is tightly concentrated on the start state, which is why there is so little red in (a) the visitation rendering for SAC-discrete (i.e., it is occluded by the gray circle).", "description": "This figure visualizes the state visitation in online RL on AntMaze Medium for four different methods (SAC-discrete, SFP, SSP, and the proposed SaS method) during the first 1 million timesteps.  It highlights that SaS explores the maze more extensively than the other methods, demonstrating superior exploration behavior.", "section": "4.2 Exploration Behavior on AntMaze Medium"}, {"figure_path": "WfpvtH7oC1/figures/figures_6_1.jpg", "caption": "Figure 5: Comparison to methods with observation-conditioned skills. In general we see conditioning helps when the data closely overlaps with the downstream task (Kitchen), but not in AntMaze where the demonstrations are somewhat disjoint. OPAL is a closed-source method similar to SPiRL, so results are taken from Ajay et al. [2, Section 5.3].", "description": "This figure compares the performance of the proposed Subwords as Skills (SaS) method against other methods for online reinforcement learning on AntMaze and Kitchen tasks.  It specifically focuses on comparing SaS with methods that use observation-conditioned skills (skills that are conditioned on the observed state). The results indicate that observation conditioning can be beneficial when the training data and the test task are similar (as in the Kitchen environment), but it may not be advantageous when there is a mismatch between the training data and test task (as in the AntMaze environment).  OPAL, a closed-source method, is also included in the comparison.", "section": "4.3 Comparison to Observation-Conditioned Skills"}, {"figure_path": "WfpvtH7oC1/figures/figures_6_2.jpg", "caption": "Figure 6: Results on transferring skills extracted from AntMaze-M to downstream RL on AntMaze-U, with varying quantity of demonstrations. Even with 1% of the data, our method extracts useful skills", "description": "This figure displays the results of an experiment on transferring skills learned from one AntMaze environment (AntMaze-M) to a different but related AntMaze environment (AntMaze-U).  The experiment varied the percentage of demonstrations used to extract skills (1%, 10%, 25%, and 100%).  The x-axis represents the number of training steps, and the y-axis shows the return (reward) achieved by the RL agent.  The results show that even with a small percentage of demonstrations (as low as 1%), the method is still able to extract useful skills that enable good performance in the new environment.", "section": "4.4 Transferring Skills"}, {"figure_path": "WfpvtH7oC1/figures/figures_7_1.jpg", "caption": "Figure 7: Results for different numbers of clusters. For AntMaze, DoF = dact = 8, Kitchen DoF = dact = 9, and the default setting is k = 2 \u00d7 dact. Note the legend is left unsorted so that the default setting k = 2 \u00d7 dact is rendered in a consistent color and position across all plots.", "description": "This figure shows the ablation study on the hyperparameter k (number of clusters for k-means) in the proposed method.  Subplots (a) and (b) present the results for AntMaze and Kitchen environments, respectively. The x-axis represents the training steps, and the y-axis represents the return. Different lines represent different values of k. The default value used in the paper is k=2*dact, where dact is the number of degrees of freedom in the action space. The figure demonstrates the impact of varying k on the performance of the reinforcement learning agent.  The results show that the default setting of k performs well across both environments.", "section": "4.5 Ablations"}, {"figure_path": "WfpvtH7oC1/figures/figures_7_2.jpg", "caption": "Figure 7: Results for different numbers of clusters. For AntMaze, DoF = dact = 8, Kitchen DoF = dact = 9, and the default setting is k = 2 \u00d7 dact. Note the legend is left unsorted so that the default setting k = 2 \u00d7 dact is rendered in a consistent color and position across all plots.", "description": "This figure shows the ablation study on the hyperparameter *k*, which represents the number of clusters used for discretizing the action space before applying the Byte-Pair Encoding (BPE) algorithm.  The plots show the return achieved in the AntMaze and Kitchen environments for different values of *k*, keeping other hyperparameters constant. The default setting, k=2*dact (where dact is the degrees of freedom of the action space), is highlighted. The results indicate that the default setting works well; however, significantly larger values of *k* lead to shorter skills as fewer common subwords are found.", "section": "4.5 Ablations"}, {"figure_path": "WfpvtH7oC1/figures/figures_8_1.jpg", "caption": "Figure 7: Results for different numbers of clusters. For AntMaze, DoF = dact = 8, Kitchen DoF = dact = 9, and the default setting is k = 2 \u00d7 dact. Note the legend is left unsorted so that the default setting k = 2 \u00d7 dact is rendered in a consistent color and position across all plots.", "description": "This figure shows the ablation study on the hyperparameter k (number of clusters for k-means).  The left plot shows AntMaze results and the right plot shows Kitchen results.  The default value used in the paper is k=2*dact, where dact is the number of degrees of freedom in the action space (8 for AntMaze, 9 for Kitchen).  The plots show the return over training steps for different values of k.  It indicates that the choice k=2*dact is reasonable.", "section": "4.5 Ablations"}, {"figure_path": "WfpvtH7oC1/figures/figures_8_2.jpg", "caption": "Figure 10: Results for different choices of tokenizer algorithm, where BPE is the default.", "description": "This figure compares the performance of three different subword tokenization algorithms (BPE, WordPiece, and Unigram) on two reinforcement learning tasks (AntMaze and Kitchen).  The y-axis shows the cumulative reward obtained, and the x-axis shows the number of training steps. The shaded areas represent the standard deviation across multiple runs.  The results demonstrate that BPE and WordPiece perform similarly, while Unigram yields substantially lower returns.", "section": "4.5 Ablations"}, {"figure_path": "WfpvtH7oC1/figures/figures_15_1.jpg", "caption": "Figure 11: Offline environments, figures courtesy of Fu et al. [23] and Cobbe et al. [17]. For AntMaze Umaze the starting location is in the bottom left, and the goal is in the top left. For AntMaze Medium and Large the starting locations are in the bottom left, and goals are in the top right.", "description": "This figure shows the five offline reinforcement learning environments used in the paper's experiments: AntMaze (small, medium, and large versions), Kitchen, and CoinRun.  The figure highlights the visual appearance of each environment and provides clarifying information about the starting location and goal position for the AntMaze environments. These environments vary in complexity, from the relatively simple AntMaze Umaze to the more challenging Kitchen and CoinRun.", "section": "A Online RL Experimental Details"}, {"figure_path": "WfpvtH7oC1/figures/figures_16_1.jpg", "caption": "Figure 12: All skills discovered for AntMaze-M where color is darker for poses earlier in the trajectory. We see a range of linear motion and turning behaviors.", "description": "This figure visualizes the skills discovered by the Subwords as Skills (SaS) method in the AntMaze-M environment.  Each skill is represented as a sequence of poses, with darker colors indicating earlier poses in the trajectory.  The visualization shows that the learned skills consist primarily of linear movements and turns, showcasing the SaS method's ability to extract interpretable and meaningful behaviors.", "section": "4.2 Exploration Behavior on AntMaze Medium"}, {"figure_path": "WfpvtH7oC1/figures/figures_17_1.jpg", "caption": "Figure 12: All skills discovered for AntMaze-M where color is darker for poses earlier in the trajectory. We see a range of linear motion and turning behaviors.", "description": "This figure visualizes the skills learned by the Subwords as Skills (SaS) method in the AntMaze-Medium environment.  Each image represents a single skill, a temporally extended action identified by SaS.  The color gradient within each image shows the sequence of poses within the skill, darker colors corresponding to earlier poses in the sequence.  The skills shown consist mainly of combinations of linear movements and turns, illustrating the method's ability to capture distinct movement primitives from the demonstration data.", "section": "4.2 Exploration Behavior on AntMaze Medium"}, {"figure_path": "WfpvtH7oC1/figures/figures_17_2.jpg", "caption": "Figure 4: A visualization of state visitation in online RL on AntMaze Medium in the first 1 million timesteps for (a) SAC-discrete, (b) SFP, (c) SSP, and (d) our method averaged over 5 seeds. The grey circle in the bottom-left denotes the start position, while the green circle in the top-right indicates the goal. Notice that our method explores the maze much more extensively, with exploration behavior that is similar for all five seeds. SAC's visitation is tightly concentrated on the start state, which is why there is so little red in (a) the visitation rendering for SAC-discrete (i.e., it is occluded by the gray circle).", "description": "This figure visualizes the state visitation of different RL methods on AntMaze Medium environment for the first 1 million timesteps. It compares SAC-discrete, SFP, SSP, and the proposed SaS method, showing that SaS explores the maze more extensively and consistently across different seeds.", "section": "4.2 Exploration Behavior on AntMaze Medium"}, {"figure_path": "WfpvtH7oC1/figures/figures_17_3.jpg", "caption": "Figure 15: Experiments on the Hopper domain for varying number of clusters k.", "description": "This figure shows the results of experiments conducted on the Hopper locomotion environment from D4RL. The experiments varied the number of clusters (k) used for discretizing the action space in the skill extraction method. The x-axis represents the number of training steps, and the y-axis represents the cumulative return achieved by the agent. The different colored lines represent different values of k (12, 6, 24, and 48). The shaded regions represent the standard deviation across multiple runs. The results indicate that finer discretization helps up to a certain point, after which it hurts performance. One hypothesis is that finer levels of discretization naturally result in shorter skills, as there are fewer repeated subwords. This can make reinforcement learning in a dense reward environment easier, but not necessarily in a sparse-reward environment.", "section": "4.5 Ablations"}, {"figure_path": "WfpvtH7oC1/figures/figures_18_1.jpg", "caption": "Figure 16: Experiments with demonstration data of varying quality in the Hopper domain.", "description": "This figure shows the performance of the proposed method on the Hopper environment using demonstration data of varying quality.  The three lines represent results using demonstrations from a random policy ('Random'), a policy midway through training ('Medium'), and a policy at the end of training ('Expert'). The x-axis represents the number of training steps, and the y-axis represents the average return.  The figure demonstrates that demonstrations from a well-trained policy ('Expert') lead to the best performance, but even the 'Random' demonstrations achieve reasonably competitive results.", "section": "4.5 Ablations"}]