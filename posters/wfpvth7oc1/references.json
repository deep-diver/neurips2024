{"references": [{"fullname_first_author": "J. Achiam", "paper_title": "Surprise-based intrinsic motivation for deep reinforcement learning", "publication_date": "2017-03-01", "reason": "This paper introduces a novel intrinsic motivation method based on surprise, which is highly relevant to the exploration challenges addressed in the main paper."}, {"fullname_first_author": "A. Ajay", "paper_title": "OPAL: Offline primitive discovery for accelerating offline reinforcement learning", "publication_date": "2020-10-26", "reason": "This work presents OPAL, a method for offline primitive discovery, which is directly compared to in the experimental section of the main paper."}, {"fullname_first_author": "M. Bagatella", "paper_title": "SFP: State-free priors for exploration in off-policy reinforcement learning", "publication_date": "2022-01-01", "reason": "The paper introduces SFP, a method for exploration in off-policy RL which directly addresses the sparse reward setting and is compared against in the experiments."}, {"fullname_first_author": "V. Mnih", "paper_title": "Playing Atari with deep reinforcement learning", "publication_date": "2013-12-13", "reason": "This is a foundational deep reinforcement learning paper that establishes many of the techniques used in the main paper's experiments and serves as a benchmark for the field."}, {"fullname_first_author": "T. Haarnoja", "paper_title": "Soft actor-critic algorithms and applications", "publication_date": "2018-12-05", "reason": "This paper introduces the Soft Actor-Critic algorithm (SAC), which is the primary reinforcement learning algorithm used in the experiments of the main paper."}]}