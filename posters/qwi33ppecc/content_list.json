[{"type": "text", "text": "Most Influential Subset Selection: Challenges, Promises, and Beyond ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuzheng $\\mathbf{H}\\mathbf{u}^{1}$ Pingbang $\\mathbf{H}\\mathbf{u}^{2}$ Han Zhao1 Jiaqi W. Ma2 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computer Science 2School of Information Sciences University of Illinois Urbana-Champaign {yh46,pbb,hanzhao,jiaqima}@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "How can we attribute the behaviors of machine learning models to their training data? While the classic influence function sheds light on the impact of individual samples, it often fails to capture the more complex and pronounced collective influence of a set of samples. To tackle this challenge, we study the Most Influential Subset Selection (MISS) problem, which aims to identify a subset of training samples with the greatest collective influence. We conduct a comprehensive analysis of the prevailing approaches in MISS, elucidating their strengths and weaknesses. Our findings reveal that influence-based greedy heuristics, a dominant class of algorithms in MISS, can provably fail even in linear regression. We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence. Conversely, we demonstrate that an adaptive version of these heuristics which applies them iteratively, can effectively capture the interactions among samples and thus partially address the issues. Experiments on real-world datasets corroborate these theoretical findings and further demonstrate that the merit of adaptivity can extend to more complex scenarios such as classification tasks and non-linear neural networks. We conclude our analysis by emphasizing the inherent trade-off between performance and computational efficiency, questioning the use of additive metrics such as the Linear Datamodeling Score, and offering a range of discussions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Unraveling the intricate connections between data and model predictions is critical in machine learning, particularly in high-stakes decision-making contexts such as healthcare, economics, and public policy [Bracke et al., 2019, Rudin, 2019, Amarasinghe et al., 2023]. A better understanding of these connections allows tackling tasks like data cleaning [Teso et al., 2021], model debugging [Guo et al., 2021], and assessing the robustness of inferential results [Broderick et al., 2020], all key to enhancing model interpretability and fostering trust between machine learning practitioners and domain experts. Among the various methodologies, the influence function adopted by Koh and Liang [2017] stands out as a particularly effective tool, sparking extensive research into identifying influential individual samples [Barshan et al., 2020, Schioppa et al., 2022, Grosse et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "Nevertheless, focusing solely on the influence of individual samples is often insufficient. In many scenarios, it is necessary to understand how sets of samples jointly affect model predictions. These include uncovering biases associated with specific demographic groups [Chen et al., 2018], fairly allocating credits among crowdworkers [Arrieta-Ibarra et al., 2018], and detecting trends and signals that emerge collectively within the data [Yang et al., 2020]. Gaining such insights is crucial for a more comprehensive understanding of model behaviors. ", "page_idx": 0}, {"type": "text", "text": "In pursuit of advancing this field, in this paper, we delve into the most influential subset selection (MISS) problem [Fisher et al., 2023]. MISS attempts to find a set of samples that, when removed from the training set, results in the most significant change of a pre-defined target function. In essence, it measures the worst-case collective influence. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We provide a comprehensive analysis of existing algorithms to tackle MISS, revealing their weaknesses and strengths, and discussing the challenges and important considerations for future research. To summarize our contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We systematically study the failure modes of influence-based greedy heuristics, a dominant class of algorithms in MISS that assign a static score to each sample and subsequently perform a greedy selection. Specifically, the error of influence function, as well as the inability to incorporate the non-additive structure of the collective influence, can cause these heuristics to fail in MISS even in simple linear regression. \u2022 In contrast, we demonstrate the effectiveness of the adaptive greedy algorithm that dynamically updates the score for each remaining sample in response to selections already made. The improvement mainly comes from its ability to capture the nuanced interactions among samples. \u2022 We conduct experiments on both synthetic and real-world datasets. The experimental results not only corroborate the theoretical findings but also extend to more complex settings including classification tasks and non-linear models, showcasing the consistent benefits of adaptivity. \u2022 We discuss the inherent trade-offs between performance and efficiency in MISS, and the potential drawbacks of additive metrics such as Linear Datamodeling Score, among others. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Problem statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider a prediction task (e.g., regression or classification) with an input space $\\mathcal{X}\\subset\\mathbb{R}^{d}$ and a target space $\\mathcal{V}\\subset\\mathbb{R}$ . The prediction task aims to learn a function $f(\\theta,\\cdot):\\mathcal{X}\\to\\mathcal{Y}$ parameterized by $\\theta\\in\\mathbb{R}^{q}$ . Specifically, denote $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ as the training samples and $L(\\cdot,\\cdot)$ as the loss function (e.g., squared error or cross-entropy), we aim to solve the following optimization problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\hat{\\theta}}=\\operatorname{arg\\,min}_{\\theta\\in\\mathbb{R}^{q}}{\\frac{1}{n}}\\sum_{i=1}^{n}L(f(\\theta,x_{i}),y_{i}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "A key notion for analyzing the influential samples is the optimal model parameters after removing a subset of training samples. Denote $[n]\\,=\\,\\bar{\\{}1,}2,\\cdots\\,,n\\bar{\\}$ and the set of indices as $S\\subset[n]$ , this corresponds to ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{-S}=\\arg\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{q}}\\frac{1}{n}\\sum_{i\\notin S}L(f(\\theta,x_{i}),y_{i}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that we do not adjust the normalizing constant as it does not affect the optimal solution to Eq.(2). Finally, denote $\\phi:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ as the target function, which takes the model parameters as input and returns a quantity of interest (e.g., the prediction on a test sample or the sign of its first coefficient). We now formally define the most influential subset selection problem. ", "page_idx": 1}, {"type": "text", "text": "Definition 2.1 (Most Influential Subset Selection (MISS)). Given a positive integer $k\\ll n,$ , the $k$ -Most Influential Subset Selection ( $k$ -MISS) problem refers to this discrete optimization problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\nS_{\\mathrm{opt},k}=\\underset{S\\subset[n],|S|\\leq k}{\\arg\\operatorname*{max}}\\ A_{-S},\\ w h e r e\\ A_{-S}:=\\phi(\\widehat{\\theta}_{-S})-\\phi(\\widehat{\\theta}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We refer to $A_{-S}$ as the actual effect of removing $S$ . For clarity, we refer to the actual effect as the individual effect when $|S|=1$ and the group effect otherwise. Essentially, MISS aims to identify a subset with bounded size, such that its removal from the training samples will lead to the maximum actual effect. It can be viewed as analogous to adversarial examples [Biggio et al., 2013, Szegedy et al., 2014], in that both characterize the alteration of model behaviors in the worst case, but MISS operates on the training data space and during training time. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, the naive approach of enumerating all possible subsets has an exponential time complexity in $k$ , rendering it computationally intractable in practice. In fact, even in the context of linear regression, a variant of MISS (where the target function depends on $S$ ) known as robust regression [Andersen, 2007] is proved to be NP-hard [Price et al., 2022]. To tackle this challenge, researchers have proposed various greedy heuristics to select an approximately most influential subset. ", "page_idx": 2}, {"type": "text", "text": "2.2 Influence-based greedy heuristics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One of the most prominent algorithms for MISS, ZAMinfluence, was introduced by Broderick et al. [2020] and applied to assess the robustness of inferential results in earlier econometric studies [Attanasio et al., 2015, Angelucci et al., 2015]. It builds upon the classic influence function [Koh and Liang, 2017] from robust statistics literature [Hampel, 1974, Hampel et al., 2005], extending its application from individual samples to a set of samples. A similar approach has been employed by Koh et al. [2019] to estimate group effects. We defer a detailed review of the literature to Section 7. Definition 2.2 (Upweighted objective). We denote the optimal solution to the upweighted objective w.r.t. a set of indices $S$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{-S}(\\delta):=\\operatorname*{arg\\,min}_{\\theta\\in\\mathbb{R}^{q}}\\frac{1}{n}\\sum_{i=1}^{n}L\\big(f(\\theta,x_{i}),y_{i}\\big)+\\delta\\sum_{i\\in S}L\\big(f(\\theta,x_{i}),y_{i}\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is straightforward to see that $\\delta=0$ corresponds to $\\hat{\\theta}$ , while $\\delta=-\\frac{1}{n}$ corresponds to $\\widehat{\\theta}_{-S}$ . Similar to the influence function of individual samples [Koh and Liang, 2017], the influence of a set $S$ can be characterized by the local perturbation of $\\hat{\\theta}_{-S}(\\delta)$ around $\\delta=0$ . This quantity is well-defined when $L$ is strictly convex and can be computed via the Implicit Function Theorem [Krantz and Parks, 2002]. Definition 2.3 (Influence function of a set). The influence of upweighting $S$ on the parameters is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{Z}(\\boldsymbol{S}):=\\frac{\\mathrm{d}\\hat{\\theta}_{-S}(\\boldsymbol{\\delta})}{\\mathrm{d}\\boldsymbol{\\delta}}\\bigg\\vert_{\\boldsymbol{\\delta}=0}=-H_{\\hat{\\theta}}^{-1}\\sum_{i\\in S}\\nabla_{\\boldsymbol{\\theta}}L(f(\\hat{\\boldsymbol{\\theta}},\\boldsymbol{x}_{i}),\\boldsymbol{y}_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{H_{\\hat{\\theta}}=\\frac{1}{n}\\sum_{i=1}^{n}{\\nabla_{\\theta}^{2}}L(f(\\hat{\\theta},x_{i}),y_{i})}\\end{array}$ is the Hessian of the loss function at $\\hat{\\theta}$ . ", "page_idx": 2}, {"type": "text", "text": "Using the chain rule and note that $\\big(\\hat{\\theta}_{-S}\\,=\\,\\hat{\\theta}_{-S}\\big({-}\\frac{1}{n}\\big)$ , the actual effect can be estimated via the first-order approximation: ", "page_idx": 2}, {"type": "equation", "text": "$$\nA_{-S}\\approx-\\frac{1}{n}\\cdot\\frac{\\mathrm{d}\\phi(\\hat{\\theta}_{-S}(\\delta))}{\\mathrm{d}\\delta}\\bigg\\vert_{\\delta=0}=\\frac{1}{n}\\nabla_{\\theta}\\phi(\\hat{\\theta})^{\\top}H_{\\hat{\\theta}}^{-1}\\sum_{i\\in S}\\nabla_{\\theta}L(f(\\hat{\\theta},x_{i}),y_{i}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The key observation is that the right-hand side of Eq.(6) displays an additive structure so that the group effect can be approximated by a summation of individual influences. This naturally yields the ZAMinfluence algorithm, which involves 1) calculating $\\boldsymbol{v}_{i}=\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{\\phi}(\\widehat{\\boldsymbol{\\theta}})^{\\top}H_{\\widehat{\\boldsymbol{\\theta}}}^{-1}\\nabla_{\\boldsymbol{\\theta}}L(f(\\widehat{\\boldsymbol{\\theta}},\\dot{x_{i}}),y_{i})$ for each $i\\in[n]$ ; 2) sorting $v_{i}$ \u2019s; 3) returning the top $i$ \u2019s with positive $v_{i}$ . In fact, a series of studies in MISS [Wang et al., 2023, Yang et al., 2023a, Chhabra et al., 2024] follow a similar approach: they score individual samples using variants of influence functions, and then greedily select those with the highest positive scores. We refer to these algorithms as influence-based greedy heuristics. ", "page_idx": 2}, {"type": "text", "text": "These heuristics are powerful in two aspects. The first is their broad applicability: they can be applied to any $Z$ -estimator of a twice-differentiable objective function [Broderick et al., 2020] to obtain an influential subset w.r.t. any differentiable target function. The second is their computational efficiency: once we have computed the scores for each sample, they can be executed in linear to log-linear time complexity. However, a major drawback of these heuristics is the lack of provable guarantees. It is well-known that even the influence estimates of individual samples can be fragile and erroneous, especially in complex models like neural networks [Basu et al., 2021, Bae et al., 2022]. A more significant concern lies in the additivity assumption implicitly adopted by these heuristics (also see Guu et al. [2023] for discussions), as it fails to account for the interactions among samples. We critically examine these issues in Section 3. ", "page_idx": 2}, {"type": "text", "text": "3 Pitfalls of greedy heuristics in Most Influential Subset Selection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we delve into the influence-based greedy heuristics introduced in Section 2, providing a comprehensive study of their limitations in solving MISS within the context of linear regression. ", "page_idx": 2}, {"type": "text", "text": "Setup and notation. In standard linear regression, each $x_{i}\\in\\mathbb{R}^{d}$ represents a vector of covariates, and $y_{i}$ stands for a real-valued label. The first coordinate of each $x_{i}$ is set to 1 to account for the intercept term. We stack the row vectors $x_{i}^{\\top}$ to form the design matrix $X\\in\\mathbb{R}^{n\\times d}$ and concatenate the $y_{i}$ \u2019s into the target vector $y\\in\\mathbb{R}^{n}$ . We assume the labels are generated as follows: there exists a $\\theta^{*}\\in\\mathbb{R}^{d}$ (note $q=d,$ ), a noise parameter $\\varepsilon>0$ and some $p$ , such that ", "page_idx": 3}, {"type": "equation", "text": "$$\ne=(\\varepsilon,0,\\cdot\\cdot\\cdot\\,,0,p\\varepsilon)^{\\top}\\in\\mathbb{R}^{n},\\quad y=X\\theta^{*}-e.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For a subset $S$ , $X_{S}$ and $y_{S}$ denote the corresponding covariates and responses, while $X_{-S}$ and $y_{-S}$ represent their complements. To ensure the uniqueness of the optimal solution, we assume $N=X^{\\top}X$ is invertible, and that $\\textstyle\\sum_{i=2}^{n-1}x_{i}x_{i}^{\\top}$ is also invertible (when this assumption is violated, our results naturally extend to ridge regression). The hat matrix is denoted as $H=X N^{-1}X^{\\top}$ . The diagonal element $h_{i i}$ of $H$ represents the leverage score of $x_{i}$ , and the off-diagonal element $h_{i j}$ represents the cross-leverage score [Chatterjee and Hadi, 2009] between $x_{i}$ and $x_{j}$ . The Ordinary Least Squares (OLS) estimator is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\hat{\\theta}}=\\operatorname{arg\\,min}_{\\theta}{\\frac{1}{n}}\\|X\\theta-y\\|^{2}=N^{-1}\\sum_{i=1}^{n}x_{i}y_{i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let ${\\hat{y}}_{i}\\,=\\,x_{i}^{\\top}{\\hat{\\theta}}$ be the prediction and $r_{i}\\;=\\;\\hat{y}_{i}\\,-\\,y_{i}$ be the negative residual for the $i$ -th sample. Throughout Sections 3 and 4, we focus on the linear target function $\\phi(\\theta)=x_{\\mathrm{test}}^{\\top}\\theta$ for $\\begin{array}{r}{x_{\\mathrm{test}}=\\frac{x_{1}+p x_{n}}{p+1}}\\end{array}$ , whose first coordinate is also 1. This choice of $x_{\\mathrm{{test}}}$ is intentional: it greatly simplifies the analysis by making most of the individual effects negative, as reflected in Figures 1 to 3 and the calculations in Appendix A.1. Furthermore, due to the continuous nature of the problem, our conclusions hold for a set of $x_{\\mathrm{{test}}}$ with non-zero Lebesgue measure. ", "page_idx": 3}, {"type": "text", "text": "3.1 Influence function is not accurate (even) in linear models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Influence function is widely acknowledged as an accurate alternative of leave-one-out re-training in linear models [Koh and Liang, 2017, Basu et al., 2021, Bae et al., 2022]. In this section, however, we challenge this viewpoint by pointing out a previously overlooked fact: the influence function fails to incorporate the leverage scores of individual samples in linear regression, which could result in its failure in selecting the most influential sample (i.e., 1-MISS). ", "page_idx": 3}, {"type": "text", "text": "Plugging the squared loss into Eq.(5), we have $\\begin{array}{r}{\\mathcal{I}(S)=-n N^{-1}\\sum_{i\\in S}x_{i}r_{i}}\\end{array}$ . Therefore, ZAMinfluence assigns $v_{i}=x_{\\mathrm{test}}^{\\top}N^{-1}x_{i}r_{i}$ to each sample. We refer to them as influence estimates. On the other hand, it is well-known in the statistics literature [Beckman and Trussell, 1974, Cook, 1977] that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{-\\{i\\}}-\\hat{\\theta}=\\frac{N^{-1}x_{i}r_{i}}{1-h_{i i}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Consequently, the change in the target function is given by ${\\cal A}_{-\\{i\\}}\\;\\;=\\;$ xte\u22a4st1N \u2212h1xiri, which deviates from the influence estimate by a factor of $1/(1\\,-\\,h_{i i})$ and implies underestimation (a phenomenon which was also reported in Koh et al. [2019]). This is particularly concerning when a sample has a high leverage score (e.g., an outlier [Chatterjee and Hadi, 1986]): in this case, the influence function substantially under-estimates the individual effect, potentially leading to the failure of 1-MISS. We illustrate this intuition in Figure 1: while point $\\textcircled{8}$ is scored highest by the influence function, it is however removing point $\\textcircled{1}$ (which has the highest leverage score) that leads to the greatest change ", "page_idx": 3}, {"type": "image", "img_path": "qWi33pPecC/tmp/48c6dab48a1b84f203206e02a29bf152aa3a7b93e4168fb478d42a6a6ba6c3db.jpg", "img_caption": ["Figure 1: Influence estimates suffer from disparate levels of under-estimation, leading to the failure of 1-MISS "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "in the prediction on the test sample. More generally, we present the following theorem illustrating the failure of ZAMinfluence in 1-MISS, with the proof detailed in Appendix A.2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Assume $h_{11}>h_{n n}$ . Under the label generation process described in Eq.(7), there exists some $p_{\\mathrm{:}}$ , such that ZAMinfluence fails to select the most influential sample. ", "page_idx": 4}, {"type": "text", "text": "Takeaway: Even when the influence estimates have high correlation with the individual effects, they can be misleading for extreme samples. As a result, the influence function may not be a reliable tool for MISS. ", "page_idx": 4}, {"type": "text", "text": "3.2 Violation of the additivity assumption: amplification and cancellation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Note that the individual effects $A_{-\\{i\\}}$ \u2019s can be computed efficiently for linear regression (this is generally infeasible for more complicated tasks) by correcting the influence estimates $v_{i}$ \u2019s with their corresponding leverage scores. Hence, a natural alternative is to directly perform greedy selection based on the $A_{-\\{i\\}}$ \u2019s. We refer to this method as Leverage-Adjusted Greedy Selection (LAGS). Nevertheless, we will illustrate in this section that even with perfect individual influence estimation, LAGS may still fall short in MISS due to violations of the additivity assumption. ", "page_idx": 4}, {"type": "text", "text": "We start by computing the closed-form of $A_{-S}$ . The proof can be found in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.2. For any set of indices $S$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{-S}:=\\phi(\\widehat{\\theta}_{-S})-\\phi(\\widehat{\\theta})=x_{\\mathrm{test}}^{\\top}N^{-1}X_{S}^{\\top}\\left(I_{k}-X_{S}N^{-1}X_{S}^{\\top}\\right)^{-1}(X_{S}\\widehat{\\theta}-y_{S}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 3.3. Denote $M_{S}=X_{S}N^{-1}X_{S}^{\\top}$ . It is straightforward to see that replacing the Neumann series $(I_{k}-M_{S})^{-1}=I_{k}+M_{S}+M_{S}^{2}\\stackrel{\\sim}{+}\\cdot\\cdot.\\cdot$ by the identity matrix yields the influence estimates, i.e., the first-order approximation. We further prove in Appendix A.4 that there is a one-to-one correspondence between the Taylor series of $\\hat{\\theta}_{-S}(\\delta)$ and the Neumann series: for any $k\\in\\mathbb{N}^{+}$ , the $k$ -th order approximation of $\\hat{\\theta}_{-S}(\\delta)$ is equivalent to truncating the Neumann series at $M_{S}^{k-1}$ . On the other hand, LAGS is based on the diagonal approximation of $(I_{k}-M_{S})$ . ", "page_idx": 4}, {"type": "text", "text": "To systematically study the failure mode of LAGS, we consider $S=\\{i,j\\}$ . In this case, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{-\\{i,j\\}}=x_{\\mathrm{test}}^{\\top}\\left(\\frac{(1-h_{j j})N^{-1}x_{i}r_{i}+(1-h_{i i})N^{-1}x_{j}r_{j}+h_{i j}N^{-1}(x_{i}r_{j}+x_{j}r_{i})}{(1-h_{i i})(1-h_{j j})-h_{i j}^{2}}\\right)}\\\\ &{\\qquad\\qquad=\\frac{(1-h_{i i})(1-h_{j j})(A_{-\\{i\\}}+A_{-\\{j\\}})+h_{i j}x_{\\mathrm{test}}^{\\top}N^{-1}(x_{i}r_{j}+x_{j}r_{i})}{(1-h_{i i})(1-h_{j j})-h_{i j}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From Eq. (11), we identify two primary factors contributing to the nonadditivity of the group effect: the cross-leverage score $h_{i j}$ in the denominator, which can lead to superadditivity by inflating the sum of individual effects, and the cross terms $x_{\\mathrm{test}}^{\\top}N^{-1}(x_{i}r_{j}+x_{j}r_{i})$ in the numerator, which may result in sub-additivity through the neutralization of individual effects. We refer to these phenomena as \u2018amplification\u2019 and \u2018cancellation\u2019, respectively, and will delve into how they provably lead to the failure of LAGS in what follows. ", "page_idx": 4}, {"type": "text", "text": "Amplification. Amplification occurs when the group effect of a set substantially exceeds the sum of individual effects. As suggested by Eq.(11), ", "page_idx": 4}, {"type": "image", "img_path": "qWi33pPecC/tmp/0c1e49d32217f9fc11d6a57e549aa887b5081ace8a824d745af36378ad935c81.jpg", "img_caption": ["Figure 2: LAGS fails in 2-MISS due to amplification "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "this phenomenon is pronounced when the cross-leverage score is high. Therefore, we focus on scenarios where there are $c\\geq2$ identical copies of a sample, in which case the cross-leverage score becomes the leverage score. Intuitively, this setting can be generalized to a cluster of similar samples. We first prove a useful result in this context. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proposition 3.4. Suppose there are c copies of $\\left({x_{i},y_{i}}\\right)$ . We have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{A_{-\\{i\\}^{c}}}{A_{-\\{i\\}}}=\\frac{c\\cdot\\left(1-h_{i i}\\right)}{1-c h_{i i}}>c,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A_{-\\{i\\}^{c}}$ denotes the group effect of removing all c copies of $(x_{i},y_{i})$ . ", "page_idx": 5}, {"type": "text", "text": "The proof can be found in Appendix A.5. It suggests that the group effect not only surpasses the sum of individual effects, but their ratio can be unbounded as $h_{i i}^{\\ }\\to{\\frac{1}{c}}$ . Put differently, a sample with minor influence can collectively cause a substantial effect when grouped with similar ones. In MISS, this could lead to the failure of LAGS when there is a cluster of samples with high leverage scores yet do not have the largest individual effects. This intuition is illustrated in Figure 2: while points $\\textcircled{7}$ and $\\textcircled{8}$ (the pink cluster) have the highest individual effects due to their large residuals, points $\\textcircled{1}$ and $\\textcircled{2}$ (the green cluster) with high leverage scores constitute the most influential size-2 subset. ", "page_idx": 5}, {"type": "text", "text": "We show a generalization of this example in the following theorem and defer its proof to Appendix A.6. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5. Suppose there are c copies of $\\left(x_{1},y_{1}\\right)$ and $(x_{n},y_{n})$ , and that $h_{11}>h_{n n}$ . Under the label generation process described in Eq.(7), there exists some $p$ , such that LAGS fails in $c$ -MISS. ", "page_idx": 5}, {"type": "text", "text": "Cancellation. Cancellation happens when the group effect of a set $S$ is less than one of its subsets $S^{\\prime}$ , indicating that removing $S\\setminus S^{\\prime}$ induces a negative effect. ", "page_idx": 5}, {"type": "text", "text": "In this case, cancellation is equivalent to $A_{-\\{1,n\\}}<A_{-\\{n\\}}$ (we assume w.l.o.g. that $A_{-\\{n\\}}>\\dot{A}_{-\\{1\\}})$ ). From Eq. (11), this inequality is likely to hold when $A_{-\\{1\\}}$ has a small magnitude compared to $A_{-\\{n\\}}$ , and the sign of $h_{1n}$ differs from that of $\\frac{r_{n}}{r_{1}}$ . If we further have that $A_{-\\{1\\}}$ and1 $A_{-\\{n\\}}$ are the top-2 positive individual effects (which guarantees that they will be selected by the greedy algorithm), then LAGS will fail in this context. ", "page_idx": 5}, {"type": "text", "text": "We illustrate this in Figure 3: although points $\\textcircled{8}$ and $\\textcircled{1}$ have the top-2 individual effects and are positive, their group effect as a size-2 subset is less than the individual effect of point $\\textcircled{8}$ . ", "page_idx": 5}, {"type": "image", "img_path": "qWi33pPecC/tmp/bafe960606b4ca06aa661df3dbaf13c848745a91fd13d2105215ddda24d02ddb.jpg", "img_caption": ["Figure 3: LAGS fails in 2-MISS due to cancellation "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We present a more general result in the following theorem and defer its proof to Appendix A.7. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6. Assume $h_{1n}\\neq0$ . Under the label generation process described in Eq.(7), there exists some $p_{\\mathrm{:}}$ , such that LAGS fails in 2-MISS. ", "page_idx": 5}, {"type": "text", "text": "Takeaway: LAGS provably works for MISS when all cross-leverage scores are zero, but can fail with even a single non-zero cross-leverage score. This highlights the algorithm\u2019s fragility. ", "page_idx": 5}, {"type": "text", "text": "4 Promises of the adaptive greedy algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given the limitations of LAGS, a pertinent question arises: is it possible to capture the non-additive structure of the joint effect without enumerating subsets? In this section, we examine a refined heuristic proposed by Kuschnig et al. [2021], and provide a theoretical analysis following our framework in Section 3. Kuschnig et al. [2021] originally introduced this refined algorithm in the context of linear regression, which applies to general influence-based greedy heuristics. The idea is to adaptively build the influential subset. Specifically, the algorithm works by 1) refitting the model on the current dataset and recalculating the individual effect or influence estimate for each sample; 2) excluding the most influential sample from the current dataset; 3) adding it to the influential subset. This iterative process is repeated until the subset reaches the desired size. We refer to this as the adaptive greedy algorithm. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "It is empirically observed that the adaptive greedy algorithm outperforms LAGS in linear regression [Kuschnig et al., 2021]. In this section, we further aim to provide theoretical support for the benefits of adaptivity. Specifically, we will show that in scenarios where LAGS fail due to cancellation, the adaptive greedy algorithm can effectively address this problem by leveraging a scoring function that captures the marginal contributions relative to the removal of the most influential sample. ", "page_idx": 6}, {"type": "text", "text": "Following the cancellation setup, $\\left(x_{n},y_{n}\\right)$ is the most influential sample w.r.t. the full dataset. We denote $A_{-\\{i\\}}^{\\bar{\\prime}}$ as the actual effect of removing $(x_{i},y_{i})$ for $1\\leq i\\leq n-1$ after the removal of $\\left(x_{n},y_{n}\\right)$ . Essentially, $A^{\\prime}$ is the scoring function employed in the second step of the adaptive greedy algorithm. We start by proving two useful properties of $A^{\\prime}$ (the proof is deferred to Appendix B.2). ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.1. The scoring function $A^{\\prime}$ satisfies the following properties: ", "page_idx": 6}, {"type": "text", "text": "1. Sign consistency: $A_{-\\{i\\}}^{\\prime}$ and $(A_{-\\{i,n\\}}-A_{-\\{i\\}})$ have the same sign for $1\\leq i\\leq n-1$ ;   \n2. Order preservation: $\\{\\bar{A_{-\\{i\\}}^{\\prime}}\\}_{i=2}^{n-1}$ and $\\{A_{-\\{i,n\\}}\\}_{i=2}^{n-1}$ are order-isomorphic. ", "page_idx": 6}, {"type": "text", "text": "These properties have significant implications. The first property indicates that $A^{\\prime}$ is a more reliable scoring function as it captures the marginal contribution of each sample relative to the removal of $\\left(x_{n},y_{n}\\right)$ . Hence, in the cancellation setup, $A^{\\prime}$ will not choose $(x_{1},y_{1})$ , even though $A_{-\\{1\\}}$ represents the second-largest individual effect and is positive. In contrast, the actual effect $A$ , which reflects the marginal contribution of each sample relative to the full dataset, does not account for how a newly selected sample interacts with those already selected. The second property further guarantees the success of MISS based on $A^{\\prime}$ . Formally, we prove the following for the adaptive greedy algorithm. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. Under the label generation process described in Eq.(7), suppose $A_{-\\{1\\}},A_{-\\{n\\}}>0,$ , $A_{-\\{1,n\\}}<A_{-\\{n\\}}$ (indicating cancellation), and that $n\\in S_{\\mathrm{opt,2}}$ (i.e., $\\left(x_{n},y_{n}\\right)$ is contained in the most influential subset), then the adaptive greedy algorithm solves 2-MISS. ", "page_idx": 6}, {"type": "text", "text": "Proof. We first show that the condition $A_{-\\{1,n\\}}<A_{-\\{n\\}}$ implies that $\\left(x_{n},y_{n}\\right)$ is the most influential sample (the proof is deferred to Appendix B.3). This ensures that the adaptive greedy algorithm will select $(x_{n},y_{n})$ in the first step. We now discuss two cases separately. ", "page_idx": 6}, {"type": "text", "text": "Case 1: If $A_{-\\{i,n\\}}-A_{-\\{n\\}}<0$ for every $2\\leq i\\leq n-1$ , then $S_{\\mathrm{opt,2}}=\\{n\\}$ . Furthermore, by the first property of Proposition 4.1 we have $A_{-\\{i\\}}^{\\prime}<0$ for $1\\leq i\\leq n-1$ . This implies that the adaptive algorithm will return $\\mathcal{Q}$ in the second step since no scores are positive, as desired. ", "page_idx": 6}, {"type": "text", "text": "Case 2: If there exists some $2\\,\\leq\\,i\\,\\leq\\,n\\,-\\,1$ , such that $A_{-\\{i,n\\}}\\,-\\,A_{-\\{n\\}}\\,>\\,0$ . We denote the most influential subset as $S_{\\mathrm{opt,2}}\\,=\\,\\{i^{*},n\\}$ . Since $A_{-\\{i^{*},n\\}}\\,-\\,A_{-\\{n\\}}\\,>\\,0$ , the first property of Proposition 4.1 implies $A_{-\\{i^{*}\\}}>0$ . Furthermore, by the second property of Proposition 4.1, the adaptive greedy algorithm will return the correct index $i^{*}$ in the second step. ", "page_idx": 6}, {"type": "text", "text": "Combining the above two cases finishes the proof of Theorem 4.2. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.3. In the cancellation setup, our theoretical results are restricted to 2-MISS. We identify two challenges: 1) Conceptually, it is not immediately clear how to define cancellation for more than two samples; 2) Technically, proving the success of MISS is much harder than constructing a counterexample since it requires enumerating all possible subsets, whose number grows exponentially with $k$ . We leave this as future work. ", "page_idx": 6}, {"type": "text", "text": "Takeaway: In essence, the critical limitation of LAGS and other influence-based greedy heuristics is their reliance on a one-pass procedure that measures the contribution of each sample solely in relation to the full training set. On the other hand, the adaptive greedy algorithm considers more complex interactions between samples, akin to those in Shapley value [Ghorbani and Zou, 2019], leading to more effective subset selection. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we empirically evaluate the efficacy of the adaptive greedy algorithm on real-world datasets by comparing the performance of the vanilla greedy algorithm versus the adaptive greedy algorithm across a range of $k\"s.$ 1 We cover the simple linear regression studied in Sections 3 and 4 as well as more complicated scenarios (including the classification task and non-linear neural networks) as a complement. Additional experiments on synthetic datasets can be found in Appendix C.1. ", "page_idx": 7}, {"type": "image", "img_path": "qWi33pPecC/tmp/0e22fb4d76491a686e560ea4b1f340552c9b60bf1a998ef088ecb06ddcd17d33.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Adaptive Greedy v.s. Greedy Algorithm. Row 1: Averaged actual effect $\\overline{{A_{-S}}}$ measures the averaged actual effect induced by the greedy and adaptive greedy algorithms. Row 2: Winning rate indicates the proportion of instances where one algorithm outperforms the other. ", "page_idx": 7}, {"type": "text", "text": "Evaluation metrics. We evaluate the algorithms using two metrics, the averaged actual effect and the winning rate. Given a held-out test set, we define the averaged actual effect $\\overline{{A_{-S}}}$ as the mean of the actual effects w.r.t. each test point. A higher score of $\\overline{{A_{-S}}}$ indicates a more influential subset is selected on average. Additionally, we report the winning rate across test data points in a held-out test set, namely the ratio of the algorithm outperforms the other one in terms of the actual effect $A_{-S}$ . ", "page_idx": 7}, {"type": "text", "text": "Target functions and greedy algorithms. We consider two types of tasks: regression and classification. For the regression task, we adopt the target function $\\dot{\\phi(\\theta)}=x_{\\mathrm{test}}^{\\top}\\theta$ on a given test point $z:=(x_{\\mathrm{test}},y_{\\mathrm{test}})$ . We utilize LAGS as the vanilla greedy algorithm. For the classification task, we consider the target function $\\phi(\\theta)=\\log(p(z;\\theta)/(\\bar{1}-p(\\bar{z};\\theta)\\bar{)})$ , where $p(z;\\theta)$ represents the softmax probability assigned to the correct class. We opt for the ZAMinfluence as the vanilla greedy algorithm. ", "page_idx": 7}, {"type": "text", "text": "Experimental setup. For regression, we choose a popular UCI dataset Concrete Compressive Strength [Yeh, 2007]. For classification, we experiment with a moderate-scale UCI tabular dataset Waveform Database Generator [Breiman and Stone, 1988] and an image dataset MNIST [LeCun et al., 1998]. We apply logistic regression on the former and a simple 2-layer multi-layer perceptron (MLP) on the latter. We defer details of the datasets, train/test split, and MLP training to Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Approximated actual effect. We address one unique challenge for the MLP: for neural networks, it is impossible to obtain the actual effect since the optimal model is not unique in general. To address this, we adopt an ensemble technique used in recent literature [Park et al., 2023]: averaging the target function\u2019s values from several independently trained models. Specifically, we train 5 models with the same initialization but different seeds. This works for both the greedy algorithm and evaluation: for the former, we estimate each model\u2019s influence with the ZAMinfluence algorithm and select the most influential subset based on the averaged influence; for the latter, we approximate the actual effect of a subset $S$ by the averaged difference of the target values of each model, trained with or without $S$ . ", "page_idx": 7}, {"type": "text", "text": "While ensemble solves the non-uniqueness problem, it induces a significant computational burden. Noticeably, the adaptive greedy algorithm now requires retraining for $k\\times$ number of ensembles) times. To mitigate it, we use an efficient approximate variant of the ZAMifluence estimation algorithm in our implementation and devise two strategies. We defer the concrete descriptions to Appendix C.4. ", "page_idx": 7}, {"type": "text", "text": "Results. We present the main results in Figure 4. First, we see that as $k$ increases, the averaged actual effect $\\overline{{A_{-S}}}$ given by both the vanilla and the adaptive greedy algorithms increase, which aligns with the intuition that removing a larger set $S$ induces a greater joint effect $\\overline{{A_{-S}}}$ . Furthermore, the adaptive greedy algorithm surpasses its vanilla counterpart across all scenarios and all $k$ \u2019s under both metrics. This implies that the benefits of adaptivity extend beyond linear regression and apply to more complicated scenarios like classification tasks and even non-linear neural networks. ", "page_idx": 8}, {"type": "text", "text": "Finally, for the experiment on MLP specifically, we report results of multiple random seeds in Appendix C.5 to account for the randomness in model training. The consistent results across different seeds demonstrate the robustness of the aforementioned conclusions. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Failure of the adaptive greedy algorithm. While Theorem 4.2 demonstrates the advantages of the adaptive greedy algorithm, it is still not perfect. Specifically, the assumption $n\\,\\in\\,S_{\\mathrm{opt,2}}$ in Theorem 4.2 is actually necessary: if the most influential sample is not part of the most influential subset, the algorithm will make an error in the first step and cannot correct this mistake in subsequent procedures. For instance, under the amplification setup as in Theorem 3.5, it is straightforward to see that the adaptive greedy algorithms provably fail in $c$ -MISS since it selects $\\left(x_{n},y_{n}\\right)$ in the first place. ", "page_idx": 8}, {"type": "text", "text": "Second-order approximation. To more effectively capture the amplification effect caused by clusters of similar samples, it is essential to utilize algorithms that can detect higher-order interactions. In this context, the second-order group influence introduced by Basu et al. [2020] is a more powerful alternative. It is calculated based on the second-order approximation as described in Remark 3.3: ", "page_idx": 8}, {"type": "equation", "text": "$$\nQ_{-S}=x_{\\mathrm{test}}^{\\top}N^{-1}X_{S}^{\\top}\\left(I_{k}+X_{S}N^{-1}X_{S}^{\\top}\\right)(X_{S}\\hat{\\theta}-y_{S}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "From here, the original MISS can be cast as a quadratic optimization problem (see Appendix D.1) and solved via $L_{1}$ relaxation and projected gradient descent. Furthermore, we have $Q_{-\\{1\\}^{c}}\\;=\\;$ $c^{2}v_{1}\\|x_{1}\\|^{2}+c v_{1}$ , $Q_{-\\{n\\}^{c}}=c^{2}v_{n}\\|x_{n}\\|^{2}+c v_{n}$ , indicating that quadratic approximation can capture the joint effect amplified by the leverage score by emphasizing the norm. ", "page_idx": 8}, {"type": "text", "text": "Submodular property. Given the challenges of finding an exact solution, it is tempting to explore approximate solutions to MISS with provable guarantees. A classical result of Nemhauser et al. [1978] states that so long as the (set) value function satisfies the submodular property, the greedy algorithm will return a solution within a factor $1-1/e$ of the optimum. While the value function associated with the first-order approximation is submodular due to linearity, we show in Appendix D.2 that this is generally not the case for $Q_{-S}$ . Since the second-order approximation is a more accurate estimation of the actual effect, this suggests that the actual effect is unlikely to be submodular either. Therefore, MISS is expected to be hard even when we allow approximate solutions. ", "page_idx": 8}, {"type": "text", "text": "The role of target function. Our negative results critically rely on the choice of $x_{\\mathrm{{test}}}$ , underscoring the importance of the target function \u2014 an issue that has been overlooked in prior research. In addition, we have identified a few target functions in which the influence-based greedy heuristics fail to provide meaningful results: 1) the change of norm, $\\phi_{1}(\\theta)=\\lVert\\theta-\\hat{\\theta}\\rVert^{2}$ ; 2) the training loss, $\\phi_{2}(\\theta)=\\|X\\theta-y\\|^{2}$ . In both of these cases, we have $\\nabla_{\\theta}\\phi({\\hat{\\theta}})=0$ , implying that the scores assigned to each sample will also be 0. ", "page_idx": 8}, {"type": "text", "text": "Implication on Linear Datamodeling Score. Recently, Linear Datamodeling Score (LDS) [Park et al., 2023] has emerged as a prominent metric for evaluating data attribution algorithms [Zheng et al., 2024, Bae et al., 2024]. Central to its design is the assumption that group influence is additive, which we critically examine in our work and reach a negative conclusion. This raises an important question: does a higher LDS result from a truly better data attribution algorithm, or are certain algorithms simply more aligned with the potentially flawed additive assumption? While LDS offers valuable insights into data attribution, we believe it is crucial for the research community to develop evaluation metrics that better capture the non-additive and contextual nature of training data influence. ", "page_idx": 8}, {"type": "text", "text": "Limitation and future direction. Despite thorough theoretical and empirical analyses, our study does not offer algorithmic improvements over existing research. We believe solving general MISS is a challenging problem, and hypothesize that there is an inherent trade-off between performance and computational efficiency, in which an increase in performance necessitates additional computing. This pattern is already reflected in the comparison between the vanilla and adaptive greedy algorithms, a trend that will likely continue in future research. To address this challenge, we suggest incorporating the knowledge of target function and data characteristics into algorithmic designs. ", "page_idx": 8}, {"type": "text", "text": "7 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "(Most) influential subset. Since the seminal work of Koh and Liang [2017], which utilized the influence function to identify influential individuals, subsequent research has explored finding an influential set of samples [Khanna et al., 2019, Basu et al., 2020, Broderick et al., 2020]. Among them, a notable example is the ZAMinfluence algorithm by Broderick et al. [2020], which builds on the group influence function [Koh et al., 2019] and greedily selects an approximately most influential subset. ZAMinfluence is particularly renowned for its broad applicability: it can be used to improve various dimensions of machine learning such as pre-training [Wang et al., 2023], dataset pruning [Yang et al., 2023b], and trustworthiness [Wang et al., 2022, Sattigeri et al., 2022, Chhabra et al., 2024], as well as to assess the sensitivity of inferential results in multiple domains such as applied econometrics [Attanasio et al., 2015, Angelucci et al., 2015], economics [Finger and Mo\u00a8hring, 2022, Martinez, 2022], and social science [Eubank and Fresh, 2022]. Additionally, Kuschnig et al. [2021] proposed a refined version of ZAMinfluence based on iteratively refitting the model and removing the most influential sample, an approach which was also explored in Yang et al. [2023a]. ", "page_idx": 9}, {"type": "text", "text": "Theoretical understanding of MISS. Despite its empirical success, the theoretical understanding of ZAMinfluence and other influence-based greedy heuristics remains limited. Giordano et al. [2019a,b] provided finite sample error bounds between the approximated and actual effects, but consistency (i.e., the error uniformly converges to 0 for all subsets as the sample size goes to infinity) is only achieved as the fraction of removed samples $\\alpha$ approaches zero. Fisher et al. [2023] extended the analysis to any fixed $0<\\alpha<1$ , but their consistency is not directly related to the actual effect, thus offering limited insights for MISS. Moitra and Rohatgi [2023], Freund and Hopkins [2023] examined finite-sample stability (i.e., the minimum number of samples that need to be dropped in order to flip the sign of a coordinate) in linear regression and proposed algorithms with provable guarantees, yet they are confined to highly specific scenarios, such as very low dimensions or binary design matrices. Saunshi et al. [2023] explored the additivity assumption in group influence within a different yet less interpretable framework. We position our work as the first to provide a fine-grained analysis of the common practices in MISS, shedding light on the limitations of influence-based greedy heuristics as well as the potential of the adaptive greedy algorithm. ", "page_idx": 9}, {"type": "text", "text": "Multiple outlier detection. Classical tools in statistics, such as Cook\u2019s distance and its variants, can detect a single outlier in linear regression [Cook, 1986, Chatterjee and Hadi, 1986] and generalized linear models [Wojnowicz et al., 2016]. Nevertheless, they struggle with multiple outliers due to the well-known phenomena of swamping and masking [Rousseeuw and Leroy, 1987, Hadi and Simonoff, 1993]. This challenge has motivated a line of research in regression diagnostics [Fox, 2019], known as multiple outlier detection. Prominent approaches include clustering [Gray and Ling, 1984, Hadi, 1985], influence matrix [Pe\u02dcna and Yohai, 1995], and a class of iterative procedures [Belsley et al., 1980, Hadi and Simonoff, 1993, She and Owen, 2011, Roberts et al., 2015] that resemble Kuschnig et al. [2021]. While seemingly alike, its key distinction from influential subset selection is that the \u2018outlier\u2019 is defined context-independently, rather than with respect to a specific quantity of interest. ", "page_idx": 9}, {"type": "text", "text": "Broader context. Our work falls under a broader research area that aims to attribute and interpret model behavior through the lens of data (a.k.a. data attribution). Beyond the influence function, which is central to our study, other popular approaches include the representer point method [Yeh et al., 2018], the Shapley value [Ghorbani and Zou, 2019, Jia et al., 2019], the TracIn algorithm [Pruthi et al., 2020], and more recently, the datamodels [Ilyas et al., 2022]. For a comprehensive review of this subject, we refer readers to Hammoudeh and Lowd [2024]. Finally, we emphasize that MISS should not be confused with data selection [John and Draper, 1975, Kolossov et al., 2023]. While many data attribution algorithms can indeed be applied for data selection (e.g., a recent study Wang et al. [2024] demonstrated that the effectiveness of Data Shapley in data selection hinges on the utility function), data selection remains an independent research area. It typically involves subsampling a small fraction of the training data to enable effective and data-efficient learning or estimation, differing from MISS in its objectives, methodologies, and applications. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have provided a comprehensive study of common practices in MISS, revealing the failure modes of influence-based greedy heuristics and uncovering the benefits of adaptivity. We hope our work will enhance the transparency and interpretability of machine learning models by illuminating the collective influence of training data, and serve as a foundation for future algorithmic advancements. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "YH and HZ are partially supported by an NSF IIS grant No. 2416897. YH would like to thank Fan Wu for her generous help in the experiments. HZ would like to thank the support from a Google Research Scholar Award. The views and conclusions expressed in this paper are solely those of the authors and do not necessarily reflect the official policies or positions of the supporting companies and government agencies. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "A. F. Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375, 2018.   \nK. Amarasinghe, K. T. Rodolfa, H. Lamba, and R. Ghani. Explainable machine learning for public policy: Use cases, gaps, and research directions. Data & Policy, 5:e5, Jan. 2023.   \nR. Andersen. Modern Methods for Robust Regression. SAGE Publications, 2007.   \nM. Angelucci, D. Karlan, and J. Zinman. Microcredit impacts: Evidence from a randomized microcredit program placement experiment by compartamos banco. American Economic Journal: Applied Economics, 7(1):151\u2013182, 2015.   \nI. Arrieta-Ibarra, L. Goff, D. Jime\u00b4nez-Herna\u00b4ndez, J. Lanier, and E. G. Weyl. Should we treat data as labor? moving beyond \u201cfree\u201d. AEA Papers and Proceedings, 108:38\u201342, 2018. ISSN 25740768, 25740776.   \nO. Attanasio, B. Augsburg, R. De Haas, E. Fitzsimons, and H. Harmgart. The impacts of microfinance: Evidence from joint-liability lending in mongolia. American Economic Journal: Applied Economics, 7(1):90\u2013122, 2015.   \nJ. Bae, N. H. Ng, A. Lo, M. Ghassemi, and R. B. Grosse. If influence functions are the answer, then what is the question? In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.   \nJ. Bae, W. Lin, J. Lorraine, and R. Grosse. Training data attribution via approximate unrolled differentation. arXiv preprint arXiv:2405.12186, 2024.   \nE. Barshan, M.-E. Brunet, and G. K. Dziugaite. Relatif: Identifying explanatory training samples via relative influence. In S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1899\u20131909. PMLR, 26\u201328 Aug 2020.   \nS. Basu, X. You, and S. Feizi. On second-order group influence functions for black-box predictions. In International Conference on Machine Learning, pages 715\u2013724. PMLR, 2020.   \nS. Basu, P. Pope, and S. Feizi. Influence functions in deep learning are fragile. In International Conference on Learning Representations, 2021.   \nR. Beckman and H. Trussell. The distribution of an arbitrary studentized residual and the effects of updating in multiple regression. Journal of the American Statistical Association, 69(345):199\u2013201, 1974.   \nD. Belsley, E. Kuh, and R. Welsch. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Wiley Series in Probability and Statistics - Applied Probability and Statistics Section Series. Wiley, 1980. ISBN 9780471058564.   \nB. Biggio, I. Corona, D. Maiorca, B. Nelson, N. S\u02c7rndic\u00b4, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, pages 387\u2013402. Springer, 2013.   \nP. Bracke, A. Datta, C. Jung, and S. Sen. Machine learning explainability in finance: an application to default risk analysis. Technical report, Bank of England, 2019.   \nL. Breiman and C. Stone. Waveform Database Generator (Version 1). UCI Machine Learning Repository, 1988. DOI: https://doi.org/10.24432/C5CS3C.   \nT. Broderick, R. Giordano, and R. Meager. An automatic finite-sample robustness metric: when can dropping a little data make a big difference? arXiv preprint arXiv:2011.14999, 2020.   \nS. Chatterjee and A. S. Hadi. Influential observations, high leverage points, and outliers in linear regression. Statistical science, pages 379\u2013393, 1986.   \nS. Chatterjee and A. S. Hadi. Sensitivity analysis in linear regression. John Wiley & Sons, 2009.   \nI. Chen, F. D. Johansson, and D. Sontag. Why is my classifier discriminatory? In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \nA. Chhabra, P. Li, P. Mohapatra, and H. Liu. \u201dwhat data benefits my classifier?\u201d enhancing model performance and interpretability through influence-based data selection. In The Twelfth International Conference on Learning Representations, 2024.   \nR. D. Cook. Detection of influential observation in linear regression. Technometrics, 19(1):15\u201318, 1977.   \nR. D. Cook. Assessment of local influence. Journal of the Royal Statistical Society Series B: Statistical Methodology, 48(2):133\u2013155, 1986.   \nN. Eubank and A. Fresh. Enfranchisement and incarceration after the 1965 voting rights act. American Political Science Review, 116(3):791\u2013806, 2022.   \nR. Finger and N. Mo\u00a8hring. The adoption of pesticide-free wheat production and farmers\u2019 perceptions of its environmental and health effects. Ecological Economics, 198:107463, 2022.   \nJ. Fisher, L. Liu, K. Pillutla, Y. Choi, and Z. Harchaoui. Influence diagnostics under self-concordance. In International Conference on Artificial Intelligence and Statistics, pages 10028\u201310076. PMLR, 2023.   \nJ. Fox. Regression diagnostics: An introduction. Sage publications, 2019.   \nD. Freund and S. B. Hopkins. Towards practical robustness auditing for linear regression. arXiv preprint arXiv:2307.16315, 2023.   \nT. George, C. Laurent, X. Bouthillier, N. Ballas, and P. Vincent. Fast approximate natural gradient descent in a kronecker factored eigenbasis. Advances in Neural Information Processing Systems, 31, 2018.   \nA. Ghorbani and J. Zou. Data shapley: Equitable valuation of data for machine learning. In International conference on machine learning, pages 2242\u20132251. PMLR, 2019.   \nR. Giordano, M. I. Jordan, and T. Broderick. A higher-order swiss army infinitesimal jackknife. arXiv preprint arXiv:1907.12116, 2019a.   \nR. Giordano, W. Stephenson, R. Liu, M. Jordan, and T. Broderick. A swiss army infinitesimal jackknife. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1139\u20131147. PMLR, 2019b.   \nJ. B. Gray and R. F. Ling. K-clustering as a detection tool for influential subsets in regression. Technometrics, 26(4):305\u2013318, 1984.   \nR. Grosse, J. Bae, C. Anil, N. Elhage, A. Tamkin, A. Tajdini, B. Steiner, D. Li, E. Durmus, E. Perez, E. Hubinger, K. Lukos\u02c7iu\u00afte\u02d9, K. Nguyen, N. Joseph, S. McCandlish, J. Kaplan, and S. R. Bowman. Studying large language model generalization with influence functions, 2023.   \nH. Guo, N. Rajani, P. Hase, M. Bansal, and C. Xiong. FastIF: Scalable influence functions for efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10333\u201310350. Association for Computational Linguistics, Nov. 2021.   \nK. Guu, A. Webson, E. Pavlick, L. Dixon, I. Tenney, and T. Bolukbasi. Simfluence: Modeling the influence of individual training examples by simulating training runs. arXiv preprint arXiv:2303.08114, 2023.   \nA. S. Hadi. K-clustering and the detection of influential subsets. Technometrics, 27(3):323\u2013324, 1985.   \nA. S. Hadi and J. S. Simonoff. Procedures for the identification of multiple outliers in linear models. Journal of the American statistical association, 88(424):1264\u20131272, 1993.   \nZ. Hammoudeh and D. Lowd. Training data influence analysis and estimation: A survey. Machine Learning, pages 1\u201353, 2024.   \nF. R. Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383\u2013393, 1974.   \nF. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust Statistics: The Approach Based on Influence Functions. John Wiley & Sons, 2005.   \nA. Ilyas, S. M. Park, L. Engstrom, G. Leclerc, and A. Madry. Datamodels: Predicting predictions from training data. In International Conference on Machine Learning, pages 9525\u20139587. PMLR, 2022.   \nR. Jia, D. Dao, B. Wang, F. A. Hubis, N. Hynes, N. M. G\u00a8urel, B. Li, C. Zhang, D. Song, and C. J. Spanos. Towards efficient data valuation based on the shapley value. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1167\u20131176. PMLR, 2019.   \nR. S. John and N. R. Draper. D-optimality for regression designs: a review. Technometrics, 17(1): 15\u201323, 1975.   \nR. Khanna, B. Kim, J. Ghosh, and S. Koyejo. Interpreting black box predictions using fisher kernels. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3382\u20133390. PMLR, 2019.   \nP. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pages 1885\u20131894. PMLR, 2017.   \nP. W. W. Koh, K.-S. Ang, H. Teo, and P. S. Liang. On the accuracy of influence functions for measuring group effects. Advances in neural information processing systems, 32, 2019.   \nG. Kolossov, A. Montanari, and P. Tandon. Towards a statistical theory of data selection under weak supervision. In The Twelfth International Conference on Learning Representations, 2023.   \nS. G. Krantz and H. R. Parks. The implicit function theorem: history, theory, and applications. Springer Science & Business Media, 2002.   \nN. Kuschnig, G. Zens, and J. C. Cuaresma. Hidden in plain sight: Influential sets in linear models. Technical report, CESifo, 2021.   \nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \nL. R. Martinez. How much should we trust the dictator\u2019s gdp growth estimates? Journal of Political Economy, 130(10):2731\u20132769, 2022.   \nA. Moitra and D. Rohatgi. Provably auditing ordinary least squares in low dimensions. In The Eleventh International Conference on Learning Representations, 2023.   \nG. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions\u2014i. Mathematical programming, 14:265\u2013294, 1978.   \nS. M. Park, K. Georgiev, A. Ilyas, G. Leclerc, and A. Madry. Trak: Attributing model behavior at scale. In International Conference on Machine Learning, pages 27074\u201327113. PMLR, 2023.   \nD. Pen\u02dca and V. J. Yohai. The detection of influential subsets in linear regression by using an influence matrix. Journal of the Royal Statistical Society: Series B (Methodological), 57(1):145\u2013156, 1995.   \nK. B. Petersen, M. S. Pedersen, et al. The matrix cookbook. Technical University of Denmark, 7(15): 510, 2008.   \nE. Price, S. Silwal, and S. Zhou. Hardness and algorithms for robust and sparse optimization. In International Conference on Machine Learning, pages 17926\u201317944. PMLR, 2022.   \nG. Pruthi, F. Liu, S. Kale, and M. Sundararajan. Estimating training data influence by tracing gradient descent. In Advances in Neural Information Processing Systems, volume 33, pages 19920\u201319930, 2020.   \nS. Roberts, M. A. Martin, and L. Zheng. An adaptive, automatic multiple-case deletion technique for detecting influence in regression. Technometrics, 57(3):408\u2013417, 2015.   \nP. Rousseeuw and A. Leroy. Robust regression and outlier detection, 1987.   \nS. Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.   \nC. Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206\u2013215, May 2019.   \nP. Sattigeri, S. Ghosh, I. Padhi, P. Dognin, and K. R. Varshney. Fair infinitesimal jackknife: Mitigating the influence of biased training data points without refitting. Advances in Neural Information Processing Systems, 35:35894\u201335906, 2022.   \nN. Saunshi, A. Gupta, M. Braverman, and S. Arora. Understanding influence functions and datamodels via harmonic analysis. In The Eleventh International Conference on Learning Representations, 2023.   \nA. Schioppa, P. Zablotskaia, D. Vilar, and A. Sokolov. Scaling up influence functions. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8):8179\u20138186, Jun. 2022.   \nY. She and A. B. Owen. Outlier detection using nonconvex penalized regression. Journal of the American Statistical Association, 106(494):626\u2013639, 2011.   \nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, 2014.   \nS. Teso, A. Bontempelli, F. Giunchiglia, and A. Passerini. Interactive label cleaning with examplebased explanations. In Advances in Neural Information Processing Systems, 2021.   \nJ. Wang, X. E. Wang, and Y. Liu. Understanding instance-level impact of fairness constraints. In International Conference on Machine Learning, pages 23114\u201323130. PMLR, 2022.   \nJ. T. Wang, T. Yang, J. Zou, Y. Kwon, and R. Jia. Rethinking data shapley for data selection tasks: Misleads and merits. In International Conference on Machine Learning, pages 52033\u201352063. PMLR, 2024.   \nX. Wang, W. Zhou, Q. Zhang, J. Zhou, S. Gao, J. Wang, M. Zhang, X. Gao, Y. W. Chen, and T. Gui. Farewell to aimless large-scale pretraining: Influential subset selection for language model. In Findings of the Association for Computational Linguistics: ACL 2023, pages 555\u2013568, 2023.   \nM. Wojnowicz, B. Cruz, X. Zhao, B. Wallace, M. Wolff, J. Luan, and C. Crable. \u201cinfluence sketching\u201d: Finding influential samples in large-scale regressions. In 2016 IEEE International Conference on Big Data (Big Data). IEEE, Dec. 2016.   \nJ. Yang, S. Jain, and B. C. Wallace. How many and which training points would need to be removed to flip this prediction? In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2571\u20132584, 2023a.   \nS. Yang, Z. Xie, H. Peng, M. Xu, M. Sun, and P. Li. Dataset pruning: Reducing training data by examining generalization influence. In The Eleventh International Conference on Learning Representations, 2023b.   \nY. Yang, G. Li, H. Qian, K. C. Wilhelmsen, Y. Shen, and Y. Li. SMNN: batch effect correction for single-cell RNA-seq data via supervised mutual nearest neighbor detection. Briefings in Bioinformatics, 22(3):bbaa097, 06 2020. ISSN 1477-4054.   \nC.-K. Yeh, J. Kim, I. E.-H. Yen, and P. K. Ravikumar. Representer point selection for explaining deep neural networks. In Advances in Neural Information Processing Systems, volume 31, 2018.   \nI.-C. Yeh. Concrete Compressive Strength. UCI Machine Learning Repository, 2007. DOI: https://doi.org/10.24432/C5PK67.   \nX. Zheng, T. Pang, C. Du, J. Jiang, and M. Lin. Intriguing properties of data attribution on diffusion models. In The Twelfth International Conference on Learning Representations, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Omitted details from Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Preparation work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We start by calculating the OLS estimator, the negative residuals $r_{i}$ \u2019s, the influence estimates $v_{i}$ \u2019s, and the individual effects $A_{-\\{i\\}}$ \u2019s. Suppose there are $c$ copies of $\\left(x_{1},y_{1}\\right)$ and $\\left(x_{n},y_{n}\\right)$ , where $c=1$ unless otherwise noted. Under the label generation process in Eq.(7), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\theta}=N^{-1}\\left(N\\theta^{*}-c\\varepsilon x_{1}-p c\\varepsilon x_{n}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, the negative residuals are ", "page_idx": 15}, {"type": "equation", "text": "$$\nr_{1}=(1-c h_{11}-p c h_{1n})\\varepsilon,\\quad r_{n}=(p-p c h_{n n}-c h_{1n})\\varepsilon,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\nr_{i}=-(c h_{1i}+p c h_{i n})\\varepsilon,\\quad2\\leq i\\leq n-1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $\\begin{array}{r}{x_{\\mathrm{test}}=\\frac{x_{1}+p x_{n}}{p+1}}\\end{array}$ , the influence estimates can be calculated as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{1}=\\frac{(h_{11}+p h_{1n})(1-c h_{11}-p c h_{1n})\\varepsilon}{p+1},\\quad v_{n}=\\frac{(p h_{n n}+h_{1n})(p-p c h_{n n}-c h_{1n})\\varepsilon}{p+1},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whereas ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{i}=-\\frac{c(h_{1i}+p h_{i n})^{2}\\varepsilon}{p+1}\\leq0,\\quad2\\leq i\\leq n-1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{-\\{1\\}}=\\frac{(h_{11}+p h_{1n})(1-c h_{11}-p c h_{1n})\\varepsilon}{(p+1)(1-h_{11})},\\quad A_{-\\{n\\}}=\\frac{(p h_{n n}+h_{1n})(p-p c h_{n n}-c h_{1n})\\varepsilon}{(p+1)(1-h_{n n})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\nA_{-\\{i\\}}=-\\frac{c(h_{1i}+p h_{i n})^{2}\\varepsilon}{(p+1)(1-h_{i i})}\\leq0,\\quad2\\leq i\\leq n-1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We also discuss a few properties of the hat matrix $H$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1. The leverage scores satisfy: $\\begin{array}{r}{h_{11}<\\frac{1}{c}}\\end{array}$ , $\\begin{array}{r}{h_{n n}<\\frac{1}{c}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Note the hat matrix is idempotent, i.e., $H^{2}=H$ . As a consequence, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{11}=c h_{11}^{2}+\\sum_{i=2}^{n-1}h_{1i}^{2}+c h_{1n}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note $\\textstyle\\sum_{i=2}^{n-1}x_{i}x_{i}^{\\top}$ is invertible, and that $N^{-1}x_{1}$ is a non-zero vector. As a consequence, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=2}^{n-1}h_{1i}x_{i}=\\left(\\sum_{i=2}^{n-1}x_{i}x_{i}^{\\top}\\right)N^{-1}x_{1}\\neq0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which further implies that the sequence $\\{h_{1i}\\}_{i=2}^{n-1}$ cannot be all zero. Therefore, we have $\\begin{array}{r}{h_{11}<\\frac{1}{c}}\\end{array}$ The same argument applies to $h_{n n}$ . \u5e7f ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. The following inequalities hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{1n}^{2}<h_{11}h_{n n},\\quad\\mathrm{and}\\quad(1-c h_{11})(1-c h_{n n})<c^{2}h_{1n}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Since $N$ is positive definite (PD), $P~=~\\sqrt{N^{-1}}$ is well-defined and is invertible. Note $h_{i j}\\stackrel{\\cdot}{=}x_{i}^{\\top}N^{-1}x_{j}=\\'\\langle P x_{i},P x_{j}\\rangle$ . Therefore, $h_{1n}^{2}<h_{11}h_{n n}$ is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle P x_{1},P x_{n}\\rangle<\\|P x_{1}\\|\\cdot\\|P x_{n}\\|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $h_{11}>h_{n n}$ , we have $x_{1}\\neq x_{n}$ , and therefore $x_{1}\\not\\in x_{n}$ since their first coordinates are the same.   \nTherefore, Eq.(24) follows from the Cauchy-Schwarz inequality. ", "page_idx": 15}, {"type": "text", "text": "For the second inequality, denote $C^{\\top}=(\\sqrt{c}x_{1},\\sqrt{c}x_{n})\\in\\mathbb{R}^{d\\times2}$ . Consider the following matrix: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S:=\\left(\\begin{array}{c c}{N}&{C^{\\top}}\\\\ {C}&{I_{2}}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the Schur complement of I2: S/I2 = N \u2212C\u22a4I2C = in=\u221221 x , and that $I_{2}\\succ0$ , we have $S\\succ0$ . This further implies that the Schur complement  of $N$ is positive definite, i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\nS/N=I_{2}-C N^{-1}C^{\\top}=\\binom{1-c h_{11}}{-c h_{1n}}\\-\\ \\-\\ \\-\\-\\-\\!\\!\\!-h_{1n}\\biggr)\\succ0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a consequence, we have $\\operatorname*{det}(S/N)={(1-c h_{11})(1-c h_{n n})}-c^{2}h_{1n}^{2}>0$ . ", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3.1. We will show that there exists some $p$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n1<\\frac{v_{n}}{v_{1}}<\\frac{1-h_{n n}}{1-h_{11}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and that $v_{1}$ and $v_{n}$ are positive. Since $v_{i}\\leq0$ for $2\\leq i\\leq n-1$ , this implies that ZAMinfluence selects $(x_{n},y_{n})$ and fails to find the most influential sample $(x_{1},y_{1})$ . We will discuss three cases. ", "page_idx": 16}, {"type": "text", "text": "Case 1: $h_{1n}=0$ . In this case, both $v_{1}$ and $v_{n}$ are positive by Lemma A.1. Furthermore, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{v_{n}}{v_{1}}=\\frac{h_{n n}(1-h_{n n})}{h_{11}(1-h_{11})}\\cdot p^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is continuous and takes values in $[0,\\infty)$ . Hence, there exists a $p>0$ such that Eq.(27) holds. Case 2: $h_{1n}<0$ . When ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\frac{h_{1n}}{h_{n n}}<p<-\\frac{h_{11}}{h_{1n}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "both $v_{1}$ and $v_{n}$ are positive. Note Eq.(29) forms a valid interval by the first inequality in Lemma A.2. Now consider ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{v_{n}}{v_{1}}=\\frac{(p h_{n n}+h_{1n})(p-p h_{n n}-h_{1n})}{(h_{11}+p h_{1n})(1-h_{11}-p h_{1n})},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is continuous and approaches 0 as $\\begin{array}{r}{p\\rightarrow-\\frac{h_{1n}}{h_{n n}}}\\end{array}$ hh1n and approaches \u221eas p \u2192 $p\\rightarrow-{\\frac{h_{11}}{h_{1n}}}$ hh11 . Hence, there exists a $p>0$ such that Eq.(27) holds. ", "page_idx": 16}, {"type": "text", "text": "Case 3: $h_{1n}>0$ . When ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{h_{1n}}{1-h_{n n}}<p<\\frac{1-h_{11}}{h_{1n}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "both $v_{1}$ and $v_{n}$ are positive. This forms a valid interval by the second inequality in Lemma A.2. The rest of the analysis can be performed similarly as in Case 2. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 3.2. Applying the Woodbury matrix identity, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(N-X_{S}^{\\top}I_{k}X_{S})^{-1}=N^{-1}+N^{-1}X_{S}^{\\top}(I_{k}-X_{S}N^{-1}X_{S}^{\\top})^{-1}X_{S}N^{-1}}}\\\\ {{=N^{-1}+N^{-1}\\displaystyle\\sum_{i\\in S}\\displaystyle\\frac{1}{1-h_{i i}}x_{i}x_{i}^{\\top}N^{-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\theta}_{-S}-\\hat{\\theta}=(N-X_{S}^{\\top}I_{k}X_{S})^{-1}X_{-S}^{\\top}y_{-S}-N^{-1}X^{\\top}y_{\\bar{y}}}\\\\ &{\\quad\\quad\\quad=\\left(N^{-1}+N^{-1}X_{S}^{\\top}(I_{k}-X_{S}N^{-1}X_{S}^{\\top})^{-1}X_{S}N^{-1}\\right)(X^{\\top}y-X_{S}^{\\top}y_{S})-N^{-1}X^{\\top}y_{\\bar{y}}}\\\\ &{\\quad\\quad\\quad\\quad=\\left(N^{-1}X_{S}^{\\top}(I_{k}-X_{S}N^{-1}X_{S}^{\\top})^{-1}\\left(X_{S}N^{-1}X^{\\top}y-y_{S}\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad=N^{-1}X_{S}^{\\top}(I_{k}-X_{S}N^{-1}X_{S}^{\\top})^{-1}\\left(X_{S}N^{-1}X^{\\top}y-y_{S}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=N^{-1}X_{S}^{\\top}\\left(I_{k}-X_{S}N^{-1}X_{S}^{\\top}\\right)^{-1}(X_{S}\\hat{\\theta}-y_{S}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the actual effect of removing $S$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{-S}:=\\phi(\\widehat{\\theta}_{-S})-\\phi(\\widehat{\\theta})=x_{\\mathrm{test}}^{\\top}N^{-1}X_{S}^{\\top}\\left(I_{k}-X_{S}N^{-1}X_{S}^{\\top}\\right)^{-1}(X_{S}\\widehat{\\theta}-y_{S}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.4 Correspondence between the Neumann series and the Taylor series ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We will demonstrate that there is a one-to-one correspondence between the Neumann series $\\left(I_{k}-\\right.$ $M_{S})^{-1}$ and the Taylor series of $\\hat{\\theta}_{-S}(\\delta)$ . To see this, consider ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial\\hat{{\\boldsymbol\\theta}}_{-S}(\\delta)}{\\partial\\delta}=n(\\boldsymbol{X}^{\\top}\\boldsymbol{X}-n\\delta\\boldsymbol{X}_{S}^{\\top}\\boldsymbol{X}_{S})^{-1}\\boldsymbol{X}_{S}^{\\top}(\\boldsymbol{X}_{S}\\hat{{\\boldsymbol\\theta}}_{-S}(\\delta)-\\boldsymbol{y}_{S}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From Petersen et al. [2008], we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial K^{-1}}{\\partial\\delta}=-K^{-1}\\frac{\\partial K}{\\partial\\delta}K^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any invertible symmetric matrix $K$ . By induction, we can show that for any $i\\geq1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial^{i}\\hat{\\boldsymbol{\\theta}}_{-S}(\\boldsymbol{\\delta})}{\\partial\\boldsymbol{\\delta}^{i}}=(n^{i}\\cdot i!)\\cdot(\\boldsymbol{X}^{\\top}\\boldsymbol{X}-n\\boldsymbol{\\delta}\\boldsymbol{X}_{S}^{\\top}\\boldsymbol{X}_{S})^{-1}\\boldsymbol{X}_{S}^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\Big[\\boldsymbol{X}_{S}(\\boldsymbol{X}^{\\top}\\boldsymbol{X}-n\\boldsymbol{\\delta}\\boldsymbol{X}_{S}^{\\top}\\boldsymbol{X}_{S})^{-1}\\boldsymbol{X}_{S}^{\\top}\\Big]^{i-1}\\left(\\boldsymbol{X}_{S}\\hat{\\boldsymbol{\\theta}}_{-S}(\\boldsymbol{\\delta})-\\boldsymbol{y}_{S}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by Taylor expansion, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{\\theta}_{-S}=\\hat{\\theta}+\\sum_{i=1}^{\\infty}\\frac{1}{i!}\\left.\\frac{\\partial^{i}\\hat{\\theta}_{-S}(\\delta)}{\\partial\\delta^{i}}\\right\\vert_{\\delta=0}\\left(\\frac{1}{n}\\right)^{i}}}\\\\ {{\\displaystyle=\\hat{\\theta}+N^{-1}X_{S}^{\\top}\\left(\\sum_{i=1}^{\\infty}(X_{S}N^{-1}X_{S}^{\\top})^{i-1}\\right)(X_{S}\\hat{\\theta}-y_{S})}}\\\\ {{\\displaystyle=\\hat{\\theta}+N^{-1}X_{S}^{\\top}\\left(\\sum_{i=1}^{\\infty}M_{S}^{i-1}\\right)(X_{S}\\hat{\\theta}-y_{S}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, truncating at the $i$ -th element in the Neumann series is equivalent to the $i^{\\mathrm{th}}$ -order Taylor approximation of $\\hat{\\theta}_{-S}(\\delta)$ . In particular, first-order approximation corresponds to the identity matrix, which does not concern the leverage scores at all. Conversely, higher-order approximations entail more accurate information on the leverage scores but come at the cost of computational efficiency. ", "page_idx": 17}, {"type": "text", "text": "A.5 Proof of Proposition 3.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 3.4. Denote $\\theta_{-\\{i\\}^{c}}$ as the optimal model parameters after removing all $c$ copies of $\\left({x_{i},y_{i}}\\right)$ , and $\\textstyle z=\\sum_{j=1}^{n}x_{j}y_{j}$ . Using the Sherman-Morrison formula, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\theta}_{-\\{i\\}^{c}}-\\widehat{\\theta}=(N-c x_{i}x_{i}^{\\top})^{-1}\\big(z-c x_{i}y_{i}\\big)-N^{-1}z}\\\\ &{\\quad\\quad\\quad\\quad=\\left(N^{-1}+\\frac{c N^{-1}x_{i}x_{i}^{\\top}N^{-1}}{1-c h_{i i}}\\right)\\big(z-c x_{i}y_{i}\\big)-N^{-1}z}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{c N^{-1}x_{i}x_{i}^{\\top}\\widehat{\\theta}}{1-c h_{i i}}-c N^{-1}x_{i}y_{i}-c N^{-1}x_{i}y_{i}\\frac{c h_{i i}}{1-c h_{i i}}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{c N^{-1}x_{i}r_{i}}{1-c h_{i i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, ", "page_idx": 17}, {"type": "equation", "text": "$$\nA_{-\\{i\\}^{c}}=\\frac{c x_{\\mathrm{test}}^{\\top}N^{-1}x_{i}r_{i}}{1-c h_{i i}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, the influence of removing a single copy is ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{-\\{i\\}}=\\frac{x_{\\mathrm{test}}^{\\top}N^{-1}x_{i}r_{i}}{1-h_{i i}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{A_{-\\{i\\}^{c}}}{A_{-\\{i\\}}}=\\frac{c\\cdot\\left(1-h_{i i}\\right)}{1-c h_{i i}}>c.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.6 Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 3.5. It suffices to show that there exists some $p$ , such that $A_{-\\{1\\}}<A_{-\\{n\\}}$ and $A_{-\\{1\\}^{c}}>A_{-\\{n\\}^{c}}$ . This further implies that the failure of LAGS. From Proposition 3.4, it suffices to show there exists some $p$ , such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n1<\\frac{A_{-\\{n\\}}}{A_{-\\{1\\}}}<\\frac{(1-c h_{n n})(1-h_{11})}{(1-c h_{11})(1-h_{n n})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note this is a valid interval since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(1-c h_{11})(1-h_{n n})=1-c h_{11}-h_{n n}+c h_{11}h_{n n}}&{}\\\\ {<1-c h_{n n}-h_{11}+c h_{11}h_{n n}}&{}\\\\ {=(1-c h_{n n})(1-h_{11}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use $c\\geq2$ and $h_{11}>h_{n n}$ in the second inequality. Furthermore, Eq.(52) is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1-h_{n n}}{1-h_{11}}<\\frac{v_{n}}{v_{1}}<\\frac{1-c h_{n n}}{1-c h_{11}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use A\u2212{i} =1vih . Therefore, we can repeat the analysis in the proof of Theorem 3.1 and conclude the existence of a desired $p$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.7 Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 3.6. Recall from Eq.(11) we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{-\\{1,n\\}}=\\frac{(1-h_{11})(1-h_{n n})(A_{-\\{1\\}}+A_{-\\{n\\}})+h_{1n}x_{\\mathrm{test}}^{\\top}N^{-1}(x_{1}r_{n}+x_{n}r_{1})}{(1-h_{11})(1-h_{n n})-h_{1n}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, $A_{-\\{1,n\\}}<A_{-\\{n\\}}$ is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-h_{11})(1-h_{n n})A_{-\\{1\\}}+h_{1n}^{2}A_{-\\{n\\}}+h_{1n}x_{\\mathrm{test}}^{\\top}N^{-1}(x_{1}r_{n}+x_{n}r_{1})<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging in the formulas of $A_{-\\{1\\}},A_{-\\{n\\}},r_{1},r_{n}$ , Eq.(58) is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(1-h_{n n})(h_{11}+p h_{1n})(1-h_{11}-p h_{1n})+h_{1n}^{2}(p h_{n n}+h_{1n})\\left(p-\\displaystyle\\frac{h_{1n}}{1-h_{n n}}\\right)}}\\\\ {{<-\\displaystyle h_{1n}(h_{11}+p h_{1n})(p-p h_{n n}-h_{1n})-h_{1n}(p h_{n n}+h_{1n})(1-h_{11}-p h_{1n}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining like terms, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(h_{11}+p h_{1n})\\left((1-h_{11})(1-h_{n n})-p h_{1n}(1-h_{n n})+p h_{1n}-h_{1n}(p h_{n n}+h_{1n})\\right)}\\\\ &{<-\\left(p h_{n n}+h_{1n}\\right)\\left(p h_{1n}^{2}-\\cfrac{h_{1n}^{3}}{1-h_{n n}}-h_{1n}(h_{11}+p h_{1n})+h_{1n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This could be simplified to ", "page_idx": 18}, {"type": "equation", "text": "$$\n(h_{11}+p h_{1n})\\left((1-h_{11})(1-h_{n n})-h_{1n}^{2}\\right)<-h_{1n}(p h_{n n}+h_{1n})\\frac{(1-h_{11})(1-h_{n n})-h_{1n}^{2}}{1-h_{n n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $(1-h_{11})(1-h_{n n})-h_{1n}^{2}>0$ by Lemma A.2, the above inequality is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{1n}(p h_{n n}+h_{1n})+(1-h_{n n})(h_{11}+p h_{1n})<0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "or ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{1n}p+h_{11}(1-h_{n n})+h_{1n}^{2}<0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now it suffices to show there exists a $p$ , such that $A_{-\\{1\\}},A_{-\\{n\\}}>0$ , and that Eq.(63) holds. ", "page_idx": 19}, {"type": "text", "text": "Case 1: $h_{1n}<0$ . When ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\frac{h_{1n}}{h_{n n}}<p<-\\frac{h_{11}}{h_{1n}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "both $A_{-\\{1\\}}$ and $A_{-\\{n\\}}$ are positive. Furthermore, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{p\\rightarrow-\\frac{h_{11}}{h_{1n}}}h_{1n}p+h_{11}(1-h_{n n})+h_{1n}^{2}=h_{1n}^{2}-h_{11}h_{n n}<0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "from Lemma A.2. This proves the existence of a desired $p$ . ", "page_idx": 19}, {"type": "text", "text": "Case 2: $h_{1n}>0$ . When ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\frac{h_{11}}{h_{1n}}<p<-\\frac{h_{1n}}{h_{n n}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "both $A_{-\\{1\\}}$ and $A_{-\\{n\\}}$ are positive. Similarly, we can pick a $p$ that is sufficiently close to $-\\frac{h_{11}}{h_{1n}}$ such that $p\\neq-1$ and Eq.(63) holds. ", "page_idx": 19}, {"type": "text", "text": "Combining the above two cases finishes the proof as desired. ", "page_idx": 19}, {"type": "text", "text": "B Omitted details from Section 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Preparation work ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We start by computing the updated OLS estimator, the negative residuals, and the individual effects after removing the sample $\\left(x_{n},y_{n}\\right)$ . Denote $\\begin{array}{r}{N^{\\prime}=\\sum_{i=1}^{n-1}\\overline{{x_{i}x_{i}^{\\top}}}}\\end{array}$ , the updated OLS estimator is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\theta}^{\\prime}=(N^{\\prime})^{-1}(N^{\\prime}\\theta^{*}-\\varepsilon x_{1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, the updated negative residuals are $r_{1}^{\\prime}=(1-h_{11}^{\\prime})\\varepsilon$ and $r_{i}^{\\prime}=-h_{1i}^{\\prime}\\varepsilon$ for $2\\leq i\\leq n-1$ . By the Sherman-Morrison formula, ", "page_idx": 19}, {"type": "equation", "text": "$$\n(N^{\\prime})^{-1}=N^{-1}+{\\frac{N^{-1}x_{n}x_{n}^{\\top}N^{-1}}{1-x_{n}^{\\top}N^{-1}x_{n}}}=N^{-1}+{\\frac{N^{-1}x_{n}x_{n}^{\\top}N^{-1}}{1-h_{n n}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{1i}^{\\prime}=h_{1i}+{\\frac{h_{1n}h_{i n}}{1-h_{n n}}},\\quad h_{i i}^{\\prime}=h_{i i}+{\\frac{h_{i n}^{2}}{1-h_{n n}}},\\quad x_{i}^{\\top}(N^{\\prime})^{-1}x_{n}={\\frac{h_{i n}}{1-h_{n n}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $1\\leq i\\leq n-1$ . Finally, the adjusted individual effects are ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{-\\{1\\}}^{\\prime}=\\frac{x_{\\mathrm{test}}^{\\top}N^{\\prime-1}x_{1}r_{1}^{\\prime}}{1-h_{11}^{\\prime}}=\\frac{h_{11}^{\\prime}+p x_{1}^{\\top}(N^{\\prime})^{-1}x_{n}}{p+1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{-\\{i\\}}^{\\prime}=\\frac{x_{\\mathrm{test}}^{\\top}N^{\\prime-1}x_{i}r_{i}^{\\prime}}{1-h_{i i}^{\\prime}}=-\\frac{p h_{1i}^{\\prime}x_{i}^{\\top}N^{\\prime-1}x_{n}+h_{1i}^{\\prime2}}{(p+1)(1-h_{i i}^{\\prime})}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $2\\leq i\\leq n-1$ . ", "page_idx": 19}, {"type": "text", "text": "We will also make use of the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.1. For $2\\leq i\\leq n-1$ , $A_{-\\{i,n\\}}<A_{-\\{n\\}}$ is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(h_{1i}h_{i n}(1-h_{n n})+h_{i n}^{2}h_{1n}\\right)p+\\left(h_{1i}(1-h_{n n})+h_{i n}h_{1n}\\right)^{2}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. From Eq.(11), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{-\\{i,n\\}}=\\frac{(1-h_{i i})(1-h_{n n})(A_{-\\{i\\}}+A_{-\\{n\\}})+h_{i n}x_{\\mathrm{test}}^{\\top}N^{-1}(x_{i}r_{n}+x_{n}r_{i})}{(1-h_{i i})(1-h_{n n})-h_{i n}^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, $A_{-\\{i,n\\}}<A_{-\\{n\\}}$ is equivalent to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-h_{i i})(1-h_{n n})A_{-\\{i\\}}+h_{i n}^{2}A_{-\\{n\\}}+h_{i n}x_{\\mathrm{test}}^{\\top}N^{-1}(x_{i}r_{n}+x_{n}r_{i})>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging in the formulas of $A_{-\\{i\\}},A_{-\\{n\\}},r_{i},r_{n}.$ , Eq.(74) is equivalent to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-(h_{1i}+p h_{i n})^{2}(1-h_{n n})+\\frac{\\left(p h_{n n}+h_{1n}\\right)\\left(p-p h_{n n}-h_{1n}\\right)h_{i n}^{2}}{1-h_{n n}}}}\\\\ &{}&{+\\,h_{i n}(h_{1i}+p h_{i n})(p-2p h_{n n}-2h_{1n})>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Multiplying both side by $\\left(1-h_{n n}\\right)$ , the coefficient of $p^{2}$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n-h_{i n}^{2}(1-h_{n n})^{2}+h_{i n}^{2}h_{n n}(1-h_{n n})+h_{i n}^{2}(1-h_{n n})(1-2h_{n n})=0;\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "the coefficient of $p$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-\\;2h_{1i}h_{i n}(1-h_{n n})^{2}+h_{i n}^{2}h_{1n}(1-2h_{n n})+(1-h_{n n})h_{i n}\\left(h_{1i}(1-2h_{n n})-2h_{1n}h_{i n}\\right)}\\\\ &{}&{=-\\;h_{1i}h_{i n}(1-h_{n n})-h_{i n}^{2}h_{1n};\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the constant term is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-(1-h_{n n})^{2}h_{1i}^{2}-h_{1n}^{2}h_{i n}^{2}-2h_{1i}h_{1n}h_{i n}(1-h_{n n})=-\\left(h_{1i}(1-h_{n n})+h_{i n}h_{1n}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, Eq.(75) is equivalent to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(h_{1i}h_{i n}(1-h_{n n})+h_{i n}^{2}h_{1n}\\right)p+\\left(h_{1i}(1-h_{n n})+h_{i n}h_{1n}\\right)^{2}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.2 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of sign consistency. For $\\left(x_{1},y_{1}\\right)$ , plugging Eq.(69) into Eq.(70), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A_{-\\{1\\}}^{\\prime}<0\\iff\\displaystyle\\left(h_{11}+\\frac{h_{1n}^{2}}{1-h_{n n}}\\right)+p\\left(h_{1n}+\\frac{h_{1n}h_{n n}}{1-h_{n n}}\\right)<0}}\\\\ {{\\iff h_{1n}p+h_{11}(1-h_{n n})+h_{1n}^{2}<0,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which aligns with Eq.(63). Therefore, $A_{-\\{1,n\\}}<A_{-\\{n\\}}\\iff A_{-\\{1\\}}^{\\prime}<0.$ ", "page_idx": 20}, {"type": "text", "text": "For $\\left({x_{i},y_{i}}\\right)$ where $2\\leq i\\leq n-1$ , plugging Eq.(69) into Eq.(71), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{-\\{i\\}}^{\\prime}=-\\frac{p\\left(h_{1i}+\\frac{h_{1n}h_{i n}}{1-h_{n n}}\\right)\\frac{h_{i n}}{1-h_{n n}}+\\left(h_{1i}+\\frac{h_{1n}h_{i n}}{1-h_{n n}}\\right)^{2}}{(p+1)\\left(1-h_{i i}-\\frac{h_{i n}^{2}}{1-h_{n n}}\\right)}}\\\\ &{\\qquad=-\\frac{p h_{i n}\\left(h_{1n}h_{i n}+h_{1i}(1-h_{n n})\\right)+\\left(h_{1i}(1-h_{n n})+h_{1n}h_{i n}\\right)^{2}}{(p+1)(1-h_{n n})s_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies ", "page_idx": 20}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$A_{-\\{i\\}}^{\\prime}<0\\,\\Longleftrightarrow\\,\\left(h_{1i}h_{i n}(1-h_{n n})+h_{i n}^{2}h_{1n}\\right)p+\\left(h_{1i}(1-h_{n n})+h_{i n}h_{1n}\\right)^{2}>0,$ which aligns with Eq.(72) in Lemma B.1. Therefore, $A_{-\\{i,n\\}}<A_{-\\{n\\}}\\iff A_{-\\{i\\}}^{\\prime}<0.$ . Proof of order preservation. Plugging $A_{-\\{i\\}},A_{-\\{n\\}}$ into Eq.(11), we have $A_{-\\{i,n\\}}=\\frac{-(1-h_{n n})(h_{1i}+p h_{i n})^{2}+(1-h_{i i})(p h_{n n}+h_{1n})(p-p h_{n n}-h_{1n})}{(1-h_{i i})(1-h_{n n})-h_{i n}^{2}}.$ ", "page_idx": 20}, {"type": "text", "text": "Denote $s_{i}=(1-h_{i i})(1-h_{n n})-h_{i n}^{2}>0$ . In the numerator, the coefficient of $p^{2}$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-(1-h_{n n})h_{i n}^{2}+h_{n n}(1-h_{i i})(1-h_{n n})+h_{i n}^{2}(1-2h_{n n})=h_{n n}s_{i};}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "the coefficient of $p$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-2h_{1i}h_{i n}(1-h_{n n})+(1-h_{i i})(1-h_{n n})h_{1n}-(1-h_{i i})h_{1n}h_{n n}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad-2h_{1n}h_{i n}^{2}+h_{1i}h_{i n}(1-2h_{n n})}}\\\\ {{\\displaystyle\\quad=-h_{1i}h_{i n}-h_{1n}h_{i n}^{2}+s_{i}h_{1n}-h_{1n}h_{n n}(1-h_{i i})}}\\\\ {{\\displaystyle\\quad=-\\frac{1}{1-h_{n n}}\\left((h_{1i}h_{i n}+h_{1n}h_{i n}^{2})(1-h_{n n})+h_{1n}h_{n n}h_{i n}^{2}+s_{i}h_{1n}(2h_{n n}-1)\\right)}}\\\\ {{\\displaystyle\\quad=-\\frac{1}{1-h_{n n}}\\left(h_{i n}\\left(h_{1i}(1-h_{n n})+h_{1n}h_{i n}\\right)+s_{i}h_{1n}(2h_{n n}-1)\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the constant term is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\}-(1-h_{n n})h_{1i}^{2}-h_{1n}^{2}(1-h_{i i})-2h_{1n}h_{1i}h_{i n}}\\\\ &{=-\\,\\displaystyle\\frac{1}{1-h_{n n}}\\left((1-h_{n n})^{2}h_{1i}^{2}+2h_{1n}h_{1i}h_{i n}(1-h_{n n})+h_{1n}^{2}s_{i}+h_{1n}^{2}h_{i n}^{2}\\right)}\\\\ &{=-\\,\\displaystyle\\frac{1}{1-h_{n n}}\\left(\\left(h_{1i}(1-h_{n n})+h_{1n}h_{i n}\\right)^{2}+h_{1n}^{2}s_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{-\\{i,n\\}}=\\left(h_{n n}p^{2}+\\frac{(1-2h_{n n})h_{1n}}{1-h_{n n}}p-\\frac{h_{1n}^{2}}{1-h_{n n}}\\right)+B_{i},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nB_{i}=-\\frac{p h_{i n}\\left(h_{1i}(1-h_{n n})+h_{1n}h_{i n}\\right)+\\left(h_{1i}(1-h_{n n})+h_{1n}h_{i n}\\right)^{2}}{s_{i}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $h_{1n},h_{n n},p$ are constants, $\\{A_{-\\{i,n\\}}\\}_{i=1}^{n-1}$ and $\\{B_{i}\\}_{i=1}^{n-1}$ are order-isomorphic. Furthermore, from Eq.(83) we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{-\\{i\\}}^{\\prime}=\\frac{B_{i}}{(p+1)(1-h_{n n})}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, $\\{A_{-\\{i\\}}^{\\prime}\\}_{i=2}^{n-1}$ and $\\{B_{i}\\}_{i=2}^{n-1}$ are also order-isomorphic. The conclusion then follows from the transitivity of order-isomorphism. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "B.3 Proof of a technical lemma ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We will show that when $A_{-\\{1\\}},A_{-\\{n\\}}\\,>\\,0,\\,A_{-\\{1,n\\}}\\,<\\,A_{-\\{n\\}}$ implies $A_{-\\{1\\}}\\,<\\,A_{-\\{n\\}}$ . This guarantees $\\left(x_{n},y_{n}\\right)$ to be the most influential sample since $A_{-\\{i\\}}\\leq0$ for $2\\leq i\\leq n-1$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Plugging in the formulas of $A_{-\\{1\\}},A_{-\\{n\\}}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{-\\{1\\}}<A_{-\\{n\\}}\\iff\\left(p-{\\frac{h_{1n}}{1-h_{n n}}}\\right)\\left(p h_{n n}+h_{1n}\\right)>\\left(p h_{1n}+h_{11}\\right)\\left(1-{\\frac{p h_{1n}}{1-h_{11}}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This is equivalent to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(h_{n n}+\\frac{h_{1n}^{2}}{1-h_{11}}\\right)p^{2}+\\frac{h_{1n}(h_{11}-h_{n n})}{(1-h_{11})(1-h_{n n})}p-\\left(h_{11}+\\frac{h_{1n}^{2}}{1-h_{n n}}\\right)>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall from Eq.(63) that $A_{-\\{1,n\\}}<A_{-\\{n\\}}$ is equivalent to $h_{1n}p+h_{11}(1-h_{n n})+h_{1n}^{2}<0$ . It follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{h_{1n}(h_{11}-h_{n n})}{(1-h_{11})(1-h_{n n})}p-\\left(h_{11}+\\frac{h_{1n}^{2}}{1-h_{n n}}\\right)>\\frac{h_{1n}(h_{11}-h_{n n})}{(1-h_{11})(1-h_{n n})}p+\\frac{h_{1n}(1-h_{11})}{(1-h_{11})(1-h_{n n})}p}\\\\ {=\\frac{h_{1n}}{1-h_{11}}p.\\qquad\\qquad\\qquad\\qquad\\qquad\\quad(9.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, it suffices to show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(h_{n n}+\\frac{h_{1n}^{2}}{1-h_{11}}\\right)p^{2}+\\frac{h_{1n}}{1-h_{11}}p>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now discuss two cases. ", "page_idx": 22}, {"type": "text", "text": "Case 1: $h_{1n}\\,<\\,0$ . In this case, we must have $p>0$ to ensure Eq.(63). Therefore, Eq.(100) is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{1n}+\\left(h_{n n}(1-h_{11})+h_{1n}^{2}\\right)p>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging in h11(1\u2212hnn)+h21n, it suffices to show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(h_{11}(1-h_{n n})+h_{1n}^{2}\\right)\\left(h_{n n}(1-h_{11})+h_{1n}^{2}\\right)>h_{1n}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is true since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big(h_{11}(1-h_{n n})+h_{1n}^{2}\\Big)\\,\\Big(h_{n n}(1-h_{11})+h_{1n}^{2}\\Big)}\\\\ &{=h_{11}h_{n n}(1-h_{11}-h_{n n})+h_{1n}^{2}(h_{11}+h_{n n})+(h_{11}h_{n n}-h_{1n}^{2})^{2}}\\\\ &{>h_{1n}^{2}(1-h_{11}-h_{n n})+h_{1n}^{2}(h_{11}+h_{n n})=h_{1n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Case 2: $h_{1n}\\,>\\,0$ . In this case, we must have $p<0$ to ensure Eq.(63). Therefore, Eq.(100) is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{1n}+\\left(h_{n n}(1-h_{11})+h_{1n}^{2}\\right)p<0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging in h11(1\u2212hnn)+h21n, it suffices to show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(h_{11}(1-h_{n n})+h_{1n}^{2}\\right)\\left(h_{n n}(1-h_{11})+h_{1n}^{2}\\right)>h_{1n}^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is essentially Eq.(102). ", "page_idx": 22}, {"type": "text", "text": "Combining the above two cases finishes the proof as desired. ", "page_idx": 22}, {"type": "text", "text": "C Omitted details from Section 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Empirical justification with synthetic dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We first demonstrate our theory of linear regression empirically, Theorem 4.2 in particular, with a carefully designed synthetic dataset to create the cancellation phenomenon. Firstly, we random sample $\\theta^{*}\\in\\mathbb{R}^{d}$ and $\\dot{X}\\in\\mathbb{R}^{(n-2\\cdot c)\\times d}$ where each entrance is between $[-1,1]$ . Here, $c$ is the size of two clusters that will happen to create the cancellation phenomenon. We then artificially attached an all-one matrix $\\mathbb{1}\\in\\mathbb{R}^{\\bar{(2\\cdot c)}\\times d}$ to (the bottom of) $X$ , which corresponds to the farmost features of those two clusters. Then, we create the response $y\\in\\mathbb{R}^{n}$ by first calculating the perfect response $y^{*}:=X\\theta^{*}$ , and perturb it by adding and subtracting some noise $\\epsilon$ from the two clusters, respectively. In particular, for each $i\\in[2\\cdot c+1,n]$ , we sample a noise $\\epsilon_{i}\\sim y_{i}^{*}Z$ proportional to its original magnitude $y_{i}^{*}$ , where $Z\\sim\\dot{\\mathcal{N}}(1,\\sigma^{2})$ for some variance $\\sigma^{2}>0$ . Finally, we note that we create each test data point $\\boldsymbol{x}_{\\mathrm{test}}\\in\\mathbb{R}^{d}$ by again sampling each entry uniformly from $[-1,1]$ . ", "page_idx": 22}, {"type": "text", "text": "Intuitively, this training dataset contains two clusters on the opposite side of the ground truth $\\theta^{*}$ , hence creating the cancellation phenomenon. For demonstration, we choose $d=10$ , $\\sigma^{2}=0.2$ , and $n=1000$ with a cluster size of $c=50$ . The results are reported in Figure 5. We see that when $k<c$ , the vanilla greedy and the adaptive greedy algorithm perform similarly. However, when $k>c$ , we immediately see a clear separation in terms of the performance of the vanilla greedy and the adaptive greedy algorithm, which gives strong evidence that the adaptive greedy can capture the marginal effect after removing the entire cluster. ", "page_idx": 22}, {"type": "image", "img_path": "qWi33pPecC/tmp/c6be9a9b2507da34157c365c749097825b2bce017c1c178f68596a9afba902a3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 5: Adaptive Greedy v.s. Greedy Algorithm. Left: Averaged actual effect $\\overline{{A_{-S}}}$ measures the averaged actual effect induced by the greedy and adaptive greedy algorithms. Right: Winning rate indicates the proportion of instances where one algorithm outperforms the other. ", "page_idx": 23}, {"type": "text", "text": "C.2 Details of the datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We detail two of the UCI datasets we chose in our experiments. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Concrete Compressive Strength [Yeh, 2007]: The dataset contains 1030 instances and 8 features. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Waveform Database Generator [Breiman and Stone, 1988]: It contains 5000 instances and 21 features, with three different classes. Since we consider binary classification for logistic regression, we select the first two classes for our experiments, which contain in total 3254 instances. ", "page_idx": 23}, {"type": "text", "text": "The two UCI datasets are licensed under CC-BY 4.0, while the MNIST dataset holds a CC BY-SA 3.0 license. ", "page_idx": 23}, {"type": "text", "text": "Train/valid/test split. For the first two UCI datasets, we randomly sample 50 data points as the test set and use the remaining for training. For MNIST, to control the scale of the experiments, we sample 5000 data points from the train split for training and 50 data points from the test split for testing. ", "page_idx": 23}, {"type": "text", "text": "C.3 Details of the MLP training ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We consider a simple 2-layer MLP with input size 784 (to match the input size of images from MNIST [LeCun et al., 1998]) and a hidden-size of 128, with ReLU [Agarap, 2018] as our activation function. We train the model using Stochastic Gradient Descent (SGD) [Ruder, 2016] till convergence, with a learning rate of 0.01 and momentum of 0.9. Empirically, we observe that after 30 epochs the model converges, hence for simplicity, we set the default epochs to be 30. ", "page_idx": 23}, {"type": "text", "text": "Hyper-parameter selection. The reported hyper-parameters above were selected via grid search. We swept across hidden unit number (denoted as \u201cwidth\u201d) $\\in\\{64,128\\}$ , learning rate (denoted as $\\left.\\mathrm{{^\\circ}l r^{\\circ})\\in}\\left\\{0.01,0.05,0.1,0.5\\right\\}$ , momentum (denoted as $\\beta)\\in\\ \\{0.9,0.95\\}$ , and training epochs (denoted as \u201cepochs\u201d) $\\in\\{30,50\\}$ . For each combination of hyper-parameters, we performed 5-fold cross-validation. We present the comparisons in Table 1, which supported our final choice of the hyper-parameters in the main experiments (width $=128$ , $\\mathrm{lr}=0.01$ , $\\beta=0.9$ , epochs $=30$ ). ", "page_idx": 23}, {"type": "text", "text": "C.4 Enhancing computational efficiency for the MLP experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As mentioned in Section 5, the adaptive greedy algorithm is time-consuming as every run of the algorithm requires retraining for ( $k\\times$ number of ensembles) times if only one point is selected at each step. In our case, one evaluation requires around $10^{4}$ many retraining. Hence, we adopt several efficient approximations to mitigate the computational burden. ", "page_idx": 23}, {"type": "text", "text": "Firstly, when computing the vanilla individual influence of training data points for a converged MLP, we leverage one of the most memory and time-efficient approximation algorithms known in the literature named EK-FAC [George et al., 2018] to expedite computation. EK-FAC is efficient enough ", "page_idx": 23}, {"type": "text", "text": "Table 1: Cross-validation performance for MLP Model on MNIST. Width stands for the width of the hidden layer of the MLP, lr stands for the learning rate, and $\\beta$ stands for the momentum. ", "page_idx": 24}, {"type": "table", "img_path": "qWi33pPecC/tmp/845fbdfbe2a5f52cd498f21fd720d9422f9d1610e32869d186b7a1a7c960f98e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "to deal with large language models, which suffices for our purpose. Additionally, we devise the following two strategies to reduce the computational cost when being adaptive: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Adaptation with steps: We enhance the adaptive greedy with a tunable parameter, step size $\\ell$ , i.e., we select the top $\\ell$ most influential training points into a tentative most influential subset $S$ at each selection step. The standard adaptive greedy has $\\ell=1$ . In our experiment, we set $\\ell=5$ in particular. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Warm start: At each step, we need to obtain a new model that is supposed to be trained without $S$ . To make the adaptive greedy algorithm more efficient, we obtain a new model by first initializing the model parameters from the previous step (for each seed of the ensemble, respectively), and train without $S$ until convergence. Empirically, we observed that compared to the cold start (which requires 30 epochs to converge), the warm start only requires 8 epochs to converge, significantly reducing the computational time. ", "page_idx": 24}, {"type": "text", "text": "C.5 MLP experiments with multiple random seeds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We repeat the MLP experiments using multiple random seeds and report the results in Figure 4. The randomness in the experiments arises from neural network training. In summary, our results are generally consistent and robust across different random seeds. Specifically, the adaptive greedy algorithm consistently outperforms the vanilla greedy algorithm, though there are some fluctuations in the winning rate. ", "page_idx": 24}, {"type": "image", "img_path": "qWi33pPecC/tmp/23cdc983ecb96f65a9ed5a531b91e24de996b535d9e86520232a4938a07c53a8.jpg", "img_caption": ["Figure 6: The MLP experiment under different random seeds (0, 22, 42, 62, 82). We report the actual effect and the winning rate. Results in the main paper in Figure 4 were obtained on seed 0. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.6 Computational resource and complexity ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We conduct our experiments on Intel(R) Xeon(R) Gold 6338 CPU $\\textcircled{1}~2.00\\mathrm{GHz}$ with Nvidia A40 GPU. All experiments except the MLP experiment are efficient due to parallelization and low memory requirements. Specifically, for linear regression, both experiments on synthetic and UCI datasets run under 20 seconds. As for logistic regression, the experiment finishes in 2 minutes. ", "page_idx": 25}, {"type": "text", "text": "On the other hand, for the MLP experiments on MNIST, one step of the adaptive greedy selection algorithm for a test data point on 5000 train data points takes roughly 200 seconds with an average GPU memory usage of 40000MiB. Therefore, we can\u2019t afford any parallelization over test points due to the high memory usage. Without parallelization, using the warm start and a step size of $\\ell=5$ , the whole evaluation (5000 train data points, 50 test data points, $k=50$ ) takes roughly takes 28 hours. ", "page_idx": 25}, {"type": "text", "text": "D Omitted details from Section 6 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Discussion on the quadratic optimization ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Recall from Eq.(13) that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Q_{-S}=x_{\\mathrm{test}}^{\\top}N^{-1}X_{S}^{\\top}\\left(I_{k}+X_{S}N^{-1}X_{S}^{\\top}\\right)(X_{S}\\hat{\\theta}-y_{S})}}\\\\ &{}&{=\\displaystyle\\sum_{i\\in S}x_{\\mathrm{test}}^{\\top}N^{-1}x_{i}r_{i}+\\displaystyle\\sum_{i\\in S}(x_{\\mathrm{test}}^{\\top}N^{-1}x_{i})x_{i}^{\\top}\\cdot\\displaystyle\\sum_{i\\in S}x_{i}r_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Denote $v\\,=\\,(v_{1},\\cdot\\cdot\\cdot\\,,v_{n})^{\\top}$ and $B=\\left(b_{i j}\\right)$ , where $b_{i j}\\,=\\,(x_{\\mathrm{test}}^{\\top}N^{-1}x_{i})x_{i}^{\\top}x_{j}r_{j}$ . Under the secondorder approximation, $k$ -MISS can be cast as a constrained quadratic optimization problem: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}_{w\\in\\{0,1\\}^{n}}}&{{}w^{\\top}v+w^{\\top}B w}\\\\ {\\mathrm{s.t.}\\ }&{{}\\|w\\|_{0}\\leq k}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.2 Discussion on the submodular property ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "From Eq.(107), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ_{-S}=\\sum_{i\\in S}v_{i}+\\sum_{i,j\\in S}b_{i j},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note $Q_{-S}$ is submodular $\\Longleftrightarrow$ for every $S_{1}\\subset S_{2}$ and index $k\\not\\in S_{1}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ_{-S_{1}\\cup\\{k\\}}-Q_{-S_{1}}\\geq Q_{-S_{2}\\cup\\{k\\}}-Q_{-S_{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging Eq.(109) into Eq.(110), the submodular property requires that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{2}\\backslash S_{1}}(b_{i k}+b_{k i})\\leq0,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is equivalent to ", "page_idx": 25}, {"type": "equation", "text": "$$\nb_{i j}+b_{j i}\\leq0,\\quad\\forall i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Eq.(112) is unlikely to hold especially if $n$ is large, since it requires that the off-diagonal entries of $\\stackrel{\\cdot}{S_{B}}:=B+B^{\\top}$ are all non-positive. For a more rigorous analysis, we focus on the case where the negative residuals $r_{i}$ \u2019s are i.i.d. and symmetrically distributed with respect to the origin. Denote $s_{i j}=\\overline{{\\mathrm{sgn}(x_{\\mathrm{test}}^{\\top}N^{-1}x_{i}x_{i}^{\\top}x_{j})}}$ for $i,j\\in[n]$ , and the event in Eq.(112) as $\\mathcal{E}$ . Under this probability model, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Pr}(\\mathcal{E})\\leq\\prod_{i\\;\\mathrm{is\\;odd}}\\mathrm{Pr}(s_{i(i+1)}r_{i+1}+s_{(i+1)i}r_{i}\\leq0)=\\left(\\frac{1}{2}\\right)^{\\lfloor\\frac{n}{2}\\rfloor},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which decays exponentially with $n$ . ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The abstract and introduction clearly define the scope of both the theoretical and empirical results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The limitations are discussed in the last paragraph of Section 6. Our work focuses on analyzing the strengths and weaknesses of existing algorithms in MISS; however, the main limitation is that it does not contribute to algorithmic development in this field. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Given the theoretical nature of this paper, we have diligently ensured the accuracy of the theorem statements and proofs. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our code is publicly available at https://github.com/ InfluentialSubset/MISS. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our code is publicly available at https://github.com/ InfluentialSubset/MISS ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The details of the experiments are discussed in Appendix C. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: The experiments involve enumerating all subsets with size $k$ , which is too computationally expensive. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The information on the computer resources is reported in Appendix C.6. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Every author of this submission has reviewed the code of ethics guidelines and confirms compliance. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work is theoretical in nature, and we don\u2019t see immediate societal impact. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We properly cite the datasets and include their licenses in Section 5. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]