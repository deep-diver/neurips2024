[{"figure_path": "qWi33pPecC/figures/figures_3_1.jpg", "caption": "Figure 1: Influence estimates suffer from disparate levels of under-estimation, leading to the failure of 1-MISS", "description": "This figure illustrates how influence estimates, used in greedy heuristics for subset selection, can be inaccurate even in linear models.  The plot shows influence estimates (calculated using the influence function) for several samples against the actual effect of removing each sample individually. The discrepancy between the influence estimate and the actual effect is particularly pronounced for samples with high leverage scores (points further from the origin). This inaccuracy leads to the failure of 1-MISS (finding the single most influential sample) because the algorithm may select samples with high influence estimates but low actual effects.  The dotted vertical line indicates the test sample's input. The three lines show the original OLS regression line, and the regression lines resulting after removing points 1 and 8 respectively.", "section": "3.1 Influence function is not accurate (even) in linear models"}, {"figure_path": "qWi33pPecC/figures/figures_4_1.jpg", "caption": "Figure 1: Influence estimates suffer from disparate levels of under-estimation, leading to the failure of 1-MISS", "description": "This figure illustrates the limitations of influence estimates in selecting the most influential sample in linear regression. It shows that samples with high leverage scores can be significantly underestimated by the influence function, leading to incorrect selection by algorithms like ZAMinfluence which rely on these estimates.  The plot compares influence estimates with the actual effect of removing individual data points from a linear regression model. The discrepancy highlights the inaccuracy of influence functions in capturing the true impact of individual samples in MISS, especially for high-leverage points.", "section": "3 Pitfalls of greedy heuristics in Most Influential Subset Selection"}, {"figure_path": "qWi33pPecC/figures/figures_5_1.jpg", "caption": "Figure 1: Influence estimates suffer from disparate levels of under-estimation, leading to the failure of 1-MISS", "description": "This figure illustrates how influence estimates, used in influence-based greedy heuristics for subset selection, can be inaccurate even in linear models. Specifically, it shows that the influence function underestimates the impact of high-leverage samples.  The figure compares influence estimates with the actual effects of removing single data points. In this scenario, removing data point \u2467, which has the highest leverage score, causes the largest change in the prediction, but its influence is underestimated compared to other points. This demonstrates why using influence functions alone can lead to the failure of methods aiming for identifying the most influential subset of data points.", "section": "3.1 Influence function is not accurate (even) in linear models"}, {"figure_path": "qWi33pPecC/figures/figures_7_1.jpg", "caption": "Figure 4: Adaptive Greedy v.s. Greedy Algorithm. Row 1: Averaged actual effect A_s measures the averaged actual effect induced by the greedy and adaptive greedy algorithms. Row 2: Winning rate indicates the proportion of instances where one algorithm outperforms the other.", "description": "This figure compares the performance of greedy and adaptive greedy algorithms across three different machine learning tasks: linear regression, logistic regression, and multi-layer perceptron (MLP) classification. The top row shows the average actual effect (A_s), a measure of the algorithm's ability to identify influential subsets.  The bottom row displays the winning rate, indicating how often each algorithm achieved a larger actual effect than its counterpart.  Across all tasks, the adaptive greedy algorithms consistently outperform the standard greedy algorithms in terms of both average actual effect and winning rate, particularly as the subset size (k) increases. This highlights the advantage of adaptively updating sample scores during subset selection, rather than using static scores.", "section": "5 Experiments"}, {"figure_path": "qWi33pPecC/figures/figures_23_1.jpg", "caption": "Figure 5: Adaptive Greedy v.s. Greedy Algorithm. Left: Averaged actual effect A_s measures the averaged actual effect induced by the greedy and adaptive greedy algorithms. Right: Winning rate indicates the proportion of instances where one algorithm outperforms the other.", "description": "This figure shows a comparison of the performance of the greedy and adaptive greedy algorithms on a linear regression task with cancellation.  The left panel displays the average actual effect (A_s), a measure of how much the model's output changes when a subset of training data is removed. The right panel presents the winning rate, which shows the percentage of times each algorithm outperforms the other. As the size of the removed subset (k) increases, the average actual effect increases for both algorithms, but the adaptive greedy algorithm consistently achieves a larger effect. The winning rate plot clearly shows that the adaptive algorithm significantly outperforms the greedy algorithm as k grows beyond the cluster size.", "section": "5 Experiments"}, {"figure_path": "qWi33pPecC/figures/figures_24_1.jpg", "caption": "Figure 4: Adaptive Greedy v.s. Greedy Algorithm. Row 1: Averaged actual effect A_s measures the averaged actual effect induced by the greedy and adaptive greedy algorithms. Row 2: Winning rate indicates the proportion of instances where one algorithm outperforms the other.", "description": "This figure compares the performance of greedy and adaptive greedy algorithms for subset selection. The top row shows the average actual effect (A_s) achieved by each algorithm across different subset sizes (k). The bottom row presents the winning rate, indicating how often each algorithm outperforms the other.  The results are shown for three different machine learning models: linear regression, logistic regression, and a multi-layer perceptron (MLP). The adaptive greedy algorithm consistently demonstrates a higher average actual effect and a higher winning rate, suggesting its superiority.", "section": "5 Experiments"}]