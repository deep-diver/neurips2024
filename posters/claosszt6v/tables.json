[{"figure_path": "clAOSSzT6v/tables/tables_6_1.jpg", "caption": "Table 1: NeRFactor metrics. We evaluate the reconstruction quality of our method against the baselines using 20 test images and 8 low-frequency illumination maps for each scene from the NeRFactor dataset. We scale albedo and relit images with a per-channel factor before computing metrics. Our method attains competitive performance across all metrics with a low runtime.", "description": "This table compares the proposed method's performance against several baselines on the NeRFactor dataset for reconstructing normals, albedo, and relighting.  Metrics include MAE, PSNR, SSIM, and LPIPS.  The runtime is also reported, highlighting the efficiency of the proposed method.", "section": "4.1 Baselines"}, {"figure_path": "clAOSSzT6v/tables/tables_6_2.jpg", "caption": "Table 2: Blender and Shiny Blender metrics. We report the average of relighting reconstruction metrics and normal error for our extended Blender and Shiny Blender datasets. Metrics are computed as the average of 20 test views across 7 high-frequency illumination conditions for each scene. We scale images by a per-channel factor for relighting metrics. Our method outperforms the baselines across all metrics for the Blender dataset and has a higher PSNR for the Shiny Blender dataset.", "description": "This table compares the proposed method against baselines on two datasets: Blender and Shiny Blender.  The evaluation metrics include Mean Absolute Error (MAE) for normals, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) for relighting and albedo, and Learned Perceptual Image Patch Similarity (LPIPS) for relighting. The results show that the proposed method achieves superior performance compared to existing state-of-the-art methods across all metrics on the Blender dataset and has better PSNR than the baselines on the Shiny Blender dataset.  The experiments included high-frequency illumination conditions for a more rigorous evaluation.", "section": "4 Experiments"}, {"figure_path": "clAOSSzT6v/tables/tables_8_1.jpg", "caption": "Table 1: NeRFactor metrics. We evaluate the reconstruction quality of our method against the baselines using 20 test images and 8 low-frequency illumination maps for each scene from the NeRFactor dataset. We scale albedo and relit images with a per-channel factor before computing metrics. Our method attains competitive performance across all metrics with a low runtime.", "description": "This table presents a quantitative comparison of the proposed method against several baselines on the NeRFactor dataset.  Metrics for normals, albedo, and relighting are shown, along with the runtime. The results demonstrate that the proposed method achieves competitive performance across all metrics while being significantly faster than most baselines.", "section": "4 Experiments"}, {"figure_path": "clAOSSzT6v/tables/tables_15_1.jpg", "caption": "Table 4: NeRFactor per-scene MAE.", "description": "This table presents the Mean Absolute Error (MAE) for normals for each scene in the NeRFactor dataset.  It compares the performance of the proposed method against several baselines across four scenes ('drums', 'ficus', 'hotdog', and 'lego'). Lower MAE values indicate better performance in estimating normals.", "section": "4.1 Baselines"}, {"figure_path": "clAOSSzT6v/tables/tables_15_2.jpg", "caption": "Table 1: NeRFactor metrics. We evaluate the reconstruction quality of our method against the baselines using 20 test images and 8 low-frequency illumination maps for each scene from the NeRFactor dataset. We scale albedo and relit images with a per-channel factor before computing metrics. Our method attains competitive performance across all metrics with a low runtime.", "description": "This table presents a quantitative comparison of the proposed method against several state-of-the-art baselines on the NeRFactor dataset.  Metrics include MAE, PSNR, SSIM, and LPIPS for normals, albedo, and relighting, as well as the runtime.  The results demonstrate the competitive performance and efficiency of the proposed method.", "section": "4.1 Baselines"}, {"figure_path": "clAOSSzT6v/tables/tables_15_3.jpg", "caption": "Table 1: NeRFactor metrics. We evaluate the reconstruction quality of our method against the baselines using 20 test images and 8 low-frequency illumination maps for each scene from the NeRFactor dataset. We scale albedo and relit images with a per-channel factor before computing metrics. Our method attains competitive performance across all metrics with a low runtime.", "description": "This table presents a quantitative comparison of the proposed method against several baselines on the NeRFactor dataset.  Metrics include MAE, PSNR, SSIM, and LPIPS for normals, albedo, and relighting.  The table shows that the proposed method achieves competitive performance across all metrics while having a faster runtime than most baselines.", "section": "4.1 Baselines"}, {"figure_path": "clAOSSzT6v/tables/tables_15_4.jpg", "caption": "Table 2: Blender and Shiny Blender metrics. We report the average of relighting reconstruction metrics and normal error for our extended Blender and Shiny Blender datasets. Metrics are computed as the average of 20 test views across 7 high-frequency illumination conditions for each scene. We scale images by a per-channel factor for relighting metrics. Our method outperforms the baselines across all metrics for the Blender dataset and has a higher PSNR for the Shiny Blender dataset.", "description": "This table presents a comparison of the proposed method against several baselines on two datasets: Blender and Shiny Blender.  The metrics used evaluate the quality of relighting reconstruction, including normal error.  High-frequency illumination conditions were used to assess performance, and results demonstrate the superiority of the proposed method.", "section": "4 Experiments"}, {"figure_path": "clAOSSzT6v/tables/tables_15_5.jpg", "caption": "Table 2: Blender and Shiny Blender metrics. We report the average of relighting reconstruction metrics and normal error for our extended Blender and Shiny Blender datasets. Metrics are computed as the average of 20 test views across 7 high-frequency illumination conditions for each scene. We scale images by a per-channel factor for relighting metrics. Our method outperforms the baselines across all metrics for the Blender dataset and has a higher PSNR for the Shiny Blender dataset.", "description": "This table presents a comparison of the proposed method against several baselines on two datasets: Blender and Shiny Blender.  The metrics used evaluate the quality of relighting reconstruction and normal estimation.  High-frequency illumination conditions were used, and image scaling was applied before metric calculations. The results show that the proposed method generally outperforms the baselines.", "section": "4.1 Baselines"}, {"figure_path": "clAOSSzT6v/tables/tables_16_1.jpg", "caption": "Table 1: NeRFactor metrics. We evaluate the reconstruction quality of our method against the baselines using 20 test images and 8 low-frequency illumination maps for each scene from the NeRFactor dataset. We scale albedo and relit images with a per-channel factor before computing metrics. Our method attains competitive performance across all metrics with a low runtime.", "description": "This table presents a quantitative comparison of the proposed method against several baselines on the NeRFactor dataset.  The metrics used evaluate the accuracy of normal prediction, albedo estimation, and relighting quality.  The runtime of each method is also included.  The table shows that the proposed method achieves competitive or better results across all metrics while being significantly faster.", "section": "4.1 Baselines"}, {"figure_path": "clAOSSzT6v/tables/tables_16_2.jpg", "caption": "Table 1: NeRFactor metrics. We evaluate the reconstruction quality of our method against the baselines using 20 test images and 8 low-frequency illumination maps for each scene from the NeRFactor dataset. We scale albedo and relit images with a per-channel factor before computing metrics. Our method attains competitive performance across all metrics with a low runtime.", "description": "This table presents a quantitative comparison of the proposed method against several baseline methods on the NeRFactor dataset.  The metrics used evaluate the quality of normal, albedo, and relighting predictions.  The table shows that the proposed method achieves competitive or better performance across all metrics while maintaining a significantly lower runtime than most baselines.", "section": "4 Experiments"}, {"figure_path": "clAOSSzT6v/tables/tables_16_3.jpg", "caption": "Table 11: Shiny Blender per-scene MAE.", "description": "This table presents the Mean Absolute Error (MAE) for normal estimation on the Shiny Blender dataset.  The MAE is a measure of the average difference between predicted and ground truth normal vectors at each pixel. Lower MAE values indicate better performance.  The table breaks down the MAE across six different scenes within the Shiny Blender dataset: 'car', 'coffee', 'helmet', 'teapot', and 'toaster'. The 'avg.' column represents the average MAE across all six scenes.", "section": "4.2 Experimental setup"}, {"figure_path": "clAOSSzT6v/tables/tables_16_4.jpg", "caption": "Table 2: Blender and Shiny Blender metrics. We report the average of relighting reconstruction metrics and normal error for our extended Blender and Shiny Blender datasets. Metrics are computed as the average of 20 test views across 7 high-frequency illumination conditions for each scene. We scale images by a per-channel factor for relighting metrics. Our method outperforms the baselines across all metrics for the Blender dataset and has a higher PSNR for the Shiny Blender dataset.", "description": "This table presents a comparison of the proposed method against several baselines on two datasets: Blender and Shiny Blender.  The evaluation metrics include Mean Absolute Error (MAE) for normals, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) for relighting, and Learned Perceptual Image Patch Similarity (LPIPS) for both relighting and normals. The results show that the proposed method achieves superior performance, particularly on the Blender dataset.", "section": "4 Experiments"}, {"figure_path": "clAOSSzT6v/tables/tables_16_5.jpg", "caption": "Table 12: Shiny Blender per-scene SSIM.", "description": "This table presents the Structural Similarity Index Measure (SSIM) for relighting reconstruction on the Shiny Blender dataset.  The SSIM values are calculated for each scene (car, coffee, helmet, teapot, toaster) and averaged across all scenes.  Higher SSIM values indicate better reconstruction quality, with a score of 1 representing perfect similarity to the ground truth.", "section": "4.2 Experimental setup"}, {"figure_path": "clAOSSzT6v/tables/tables_17_1.jpg", "caption": "Table 14: Shiny Blender per-scene LPIPS.", "description": "This table presents the Learned Perceptual Image Patch Similarity (LPIPS) scores for each scene in the Shiny Blender dataset. LPIPS is a perceptual metric that measures the dissimilarity between two images.  Lower LPIPS scores indicate higher similarity. The table shows the average LPIPS across all test images for each scene ('car', 'coffee', 'helmet', 'teapot', 'toaster'), as well as the overall average across all scenes and test images. The results are compared against several baseline methods (NVDiffRec, NVDiffRecMC, NeRO, NMF, TensoIR), showcasing the performance of the proposed 'Ours' method.", "section": "4. Experiments"}]