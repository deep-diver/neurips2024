[{"figure_path": "clAOSSzT6v/figures/figures_0_1.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the SplitNeRF model on the 'materials' scene.  The top row displays the ground truth environment map, a rendered image of the scene, and the model's predictions for metalness and roughness. The middle row shows the model's albedo and normal predictions, along with another rendered image. The bottom row demonstrates the model's ability to relight the scene under four different lighting conditions ('Courtyard', 'Interior', 'Sunrise', 'Sunset').  The high-quality relighting results, achieved with only one hour of training, highlight the efficiency and effectiveness of the proposed SplitNeRF approach.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_3_1.jpg", "caption": "Figure 2: Proposed architecture. A spatial network maps spatial coordinates x into geometry (\u03c3), material properties (albedo \u00e2, metalness m, and roughness \u00f4), and occlusion factors (\u00f4). The pre-integrated illumination MLP predicts both specular \u011ds (\u0175r, p) and diffuse \u011da(\u00f4n, p = 1) terms by using the predicted normals \u00een, roughness, and the reflection vector \u0175r of view direction w\uff61. Finally, the specular and diffuse terms are combined with material properties to compute output radiance L\uff61.", "description": "The figure shows the architecture of the SplitNeRF model.  A spatial network takes spatial coordinates as input and outputs geometry, material properties (albedo, metalness, roughness), and occlusion factors.  These outputs are then fed into a rendering process.  The rendering process also takes in the viewing direction. A pre-integrated illumination MLP takes in the normals, roughness, and reflection vector from the viewing direction and outputs specular and diffuse terms which are combined with the material properties to produce final output radiance.", "section": "3 Methodology"}, {"figure_path": "clAOSSzT6v/figures/figures_4_1.jpg", "caption": "Figure 3: Pre-integrated environment illumination. We visualize the pre-integrated illumination \u011d(\u0175r, p) for varying roughness values along our model\u2019s prediction for the \u2018toaster\u2019 scene. Our pre-integrated illumination MLP accurately approximates pre-integrated lighting across roughness values thanks to our novel regularization loss based on Monte Carlo sampling.", "description": "This figure visualizes the pre-integrated illumination predicted by the model for different roughness values on the 'toaster' scene. It compares the model\u2019s predictions (ours) with the ground truth. The results demonstrate the accuracy of the model\u2019s pre-integrated illumination MLP, which accurately approximates pre-integrated lighting across various roughness levels, thanks to a novel regularization technique based on Monte Carlo sampling.", "section": "3.3 Pre-integrated illumination MLP representation"}, {"figure_path": "clAOSSzT6v/figures/figures_5_1.jpg", "caption": "Figure 4: Occlusion loss visualization. We visualize the albedo and occlusion predicted by our method with and without the proposed occlusion regularization loss. When no regularization is used, we observe that the occlusion prediction fails at disentangling shadows from the albedo. Additionally, darker materials might wind up with lighter albedos due to occlusion overcompensation.", "description": "This figure compares albedo and occlusion predictions with and without the proposed occlusion regularization loss. The left two columns show the results with the loss, demonstrating improved shadow separation and more accurate albedo estimation. The right two columns show the results without the loss, illustrating how shadow misinterpretations lead to inaccurate albedo predictions.", "section": "3.4 Occlusion factors"}, {"figure_path": "clAOSSzT6v/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative real-world results. We present qualitative results on four scenes from the CO3D dataset. Our method can successfully recover object geometry, material properties, and illumination even for challenging scenes captured in the wild.", "description": "This figure shows qualitative results of applying the proposed method to four real-world scenes from the CO3D dataset.  Each row represents a different object. The columns show: the ground truth image, a rendering from the proposed model, the predicted albedo (base color), metalness (metallic reflection), roughness (surface texture), normals (surface orientation), and the predicted environment map (lighting conditions).  The results demonstrate the model's ability to accurately reconstruct scene geometry, material properties, and lighting, even in complex real-world scenarios.", "section": "5 Discussions"}, {"figure_path": "clAOSSzT6v/figures/figures_17_1.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the proposed method on the 'materials' scene. It visualizes the ground truth and predicted lighting environment map, albedo, metalness, roughness, normals, and geometry.  Four relighting predictions under different lighting conditions are also shown, demonstrating the model's ability to predict high-frequency illumination efficiently.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_18_1.jpg", "caption": "Figure 8: Qualitative results on the Blender \u2018drums\u2019 scene.", "description": "This figure visualizes the qualitative results of the proposed method on the Blender \u2018drums\u2019 scene. It shows the ground truth environment map and the predicted environment map side-by-side, followed by the ground truth and predicted metalness, roughness, albedo, normals, and finally, four relighting results under different lighting conditions: Courtyard, Interior, Sunrise, and Sunset.  This demonstrates the model's ability to accurately estimate scene geometry, material properties, and lighting.", "section": "4 Experiments"}, {"figure_path": "clAOSSzT6v/figures/figures_18_2.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the SplitNeRF model on the 'materials' scene.  It displays the ground truth environment map, the predicted environment map, and the model's predictions for albedo, metalness, roughness, and normals. It also shows four relighting results showcasing how well the model captures and renders lighting conditions.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_19_1.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows a comparison between the ground truth and the model's predictions for a scene containing various materials.  The top row displays the ground truth environment map, the predicted environment map, and the predicted metalness, and roughness. The middle row displays the ground truth rendering, the predicted rendering, the predicted albedo, and the predicted normals. The bottom row shows the relighting results under four different lighting conditions (Courtyard, Interior, Sunrise, Sunset). The results demonstrate the model's ability to accurately predict lighting, material properties, and geometry.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_19_2.jpg", "caption": "Figure 5: Qualitative real-world results. We present qualitative results on four scenes from the CO3D dataset. Our method can successfully recover object geometry, material properties, and illumination even for challenging scenes captured in the wild.", "description": "This figure shows qualitative results of applying the proposed method to four real-world scenes from the CO3D dataset.  The results demonstrate the ability of the method to accurately estimate the scene's geometry, material properties (albedo, metalness, roughness, normals), and environmental lighting, even in complex real-world scenarios with challenging lighting and occlusion conditions.", "section": "5 Discussions"}, {"figure_path": "clAOSSzT6v/figures/figures_20_1.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "The figure shows the results of the proposed method on a scene containing various materials. The ground truth environment map and renderings are displayed alongside the model's predictions for lighting, material properties (albedo, metalness, and roughness), geometry, and four relighting predictions under different lighting conditions. The results demonstrate the method's ability to predict high-frequency illumination and accurately reconstruct the scene's geometry and material properties.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_20_2.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of applying the SplitNeRF model to a scene containing various materials. The top row displays the ground truth environment map and renderings of the scene. The second row displays the predictions made by SplitNeRF, including albedo, metalness, roughness, normals, and the environment map. The bottom row shows four different relighting predictions generated using the estimated lighting and material properties.  The figure highlights the model's ability to accurately predict high-frequency illumination and material properties with a relatively short training time, enabling efficient digitization of relightable objects.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_20_3.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure visualizes the results of the SplitNeRF model on the 'materials' scene. It shows the ground truth environment map, predicted environment map, predicted albedo, metalness, roughness, normals, and four relighting results (courtyard, interior, sunrise, and sunset).  The figure highlights the model's ability to predict high-frequency lighting details with just one hour of training, demonstrating efficient digitization of objects suitable for relighting.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_21_1.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the SplitNeRF model on the 'materials' scene.  It displays the ground truth environment map and render, alongside the model's predictions for the environment map, albedo, metalness, roughness, normals, and four relighting variations (Courtyard, Interior, Sunrise, Sunset). The high-frequency details in the illumination highlight the efficiency of the model, which achieves these results with only about one hour of training.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_21_2.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the proposed method on the 'materials' scene. It visualizes the predicted lighting, albedo, metalness, roughness, normals, and geometry.  Four different relighting scenarios are also presented, demonstrating the model's ability to accurately predict high-frequency illumination with just one hour of training.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_22_1.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows a comparison between the ground truth and the model's predictions for various aspects of a 3D scene, including lighting, material properties (albedo, metalness, roughness), and geometry. The model's predictions are shown alongside four different relighting conditions, demonstrating the model's ability to accurately capture high-frequency illumination details with minimal training time.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_22_2.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the proposed method on the 'materials' scene. It visualizes the ground truth and predicted lighting, albedo, metalness, roughness, normals, and geometry.  Four different relighting scenarios are also shown to highlight the ability of the model to accurately predict high-frequency illumination from a single hour of training. The scene is composed of several differently colored and textured spheres.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_23_1.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows a comparison between the ground truth and the model's predictions for a scene containing various materials. It demonstrates the model's ability to accurately predict lighting, material properties (albedo, metalness, and roughness), and geometry.  The four relighting predictions showcase the model's capacity for generating realistic renderings under different lighting conditions.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_23_2.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure visualizes the results of the proposed method on the 'materials' scene. It shows the ground truth environment map, the predicted environment map, the predicted material properties (albedo, metalness, and roughness), the predicted normals, and four relighting predictions under different lighting conditions. The results demonstrate the method's ability to accurately predict high-frequency illumination and material properties with only a short training time.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_24_1.jpg", "caption": "Figure 5: Qualitative real-world results. We present qualitative results on four scenes from the CO3D dataset. Our method can successfully recover object geometry, material properties, and illumination even for challenging scenes captured in the wild.", "description": "This figure shows qualitative results of applying the SplitNeRF model to four real-world scenes from the CO3D dataset.  For each scene, it displays the ground truth environment map, the model's predicted environment map, and the predicted metalness, roughness, albedo, normals, and four relighting results under different lighting conditions ('Courtyard', 'Interior', 'Sunrise', 'Sunset'). The results demonstrate that the model can effectively reconstruct object geometry, material properties, and lighting, even in complex, real-world settings.", "section": "5 Discussions"}, {"figure_path": "clAOSSzT6v/figures/figures_24_2.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the proposed method on the 'materials' scene.  It visualizes the ground truth and predicted environment map, albedo, metalness, roughness, normals, and geometry.  Four relighting scenarios are also shown, demonstrating the model's ability to accurately capture high-frequency illumination details with only an hour of training.", "section": "Abstract"}, {"figure_path": "clAOSSzT6v/figures/figures_25_1.jpg", "caption": "Figure 5: Qualitative real-world results. We present qualitative results on four scenes from the CO3D dataset. Our method can successfully recover object geometry, material properties, and illumination even for challenging scenes captured in the wild.", "description": "This figure showcases the model's performance on real-world objects from the CO3D dataset.  It demonstrates the model's ability to accurately predict the environment map, object albedo, metalness, roughness, normals, and geometry. Four different relighting scenarios are presented ('Courtyard', 'Interior', 'Sunrise', 'Sunset') to highlight the robustness of the predicted lighting and material properties.", "section": "5 Discussions"}, {"figure_path": "clAOSSzT6v/figures/figures_25_2.jpg", "caption": "Figure 1: We visualize the lighting, material properties (albedo, metalness, and roughness), and geometry predicted by our model in addition to four relighting predictions of the \u2018materials\u2019 scene. Our method predicts high-frequency illumination with only ~1 hour of training thus enabling the efficient digitization of relightable objects.", "description": "This figure shows the results of the SplitNeRF model on the 'materials' scene.  It displays the ground truth environment map, the environment map predicted by the model, and the predicted material properties (albedo, metalness, roughness) and geometry. Four additional images showcase the model's ability to relight the scene with different lighting conditions (Courtyard, Interior, Sunrise, Sunset). The high-frequency detail in the lighting and the speed at which the model achieves this are highlighted.", "section": "Abstract"}]