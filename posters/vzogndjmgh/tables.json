[{"figure_path": "VzOgnDJMgh/tables/tables_7_1.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The 'UE Avg.' and 'UT Avg.' refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive performance comparison of various LLM unlearning methods on the TOFU dataset using the LLaMA2-7B-chat model.  It shows the unlearning efficacy (UE) and utility (UT) for different methods, including baseline techniques and the proposed WAGLE approach.  UE is measured using multiple metrics (FQ, MIA, 1-FA, 1-Rouge-L) while UT considers model accuracy and Rouge-L scores on retain sets and subsets related to real authors and world facts. The table highlights the best performing method for each metric and overall average.", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_7_2.jpg", "caption": "Table 2: Performance overview of LLM unlearning on the WMDP task under Zephyr-7B-beta, with a table format similar to Tab. 1. Results are averaged over six independent random trials.", "description": "This table presents the results of LLM unlearning experiments conducted on the WMDP benchmark using the Zephyr-7B-beta model.  It compares the performance of different unlearning methods (GradDiff and NPO), each combined with various weight selection techniques (Dense, Magnitude, Wanda, LoRA, and the proposed WAGLE). The table shows the unlearning efficacy (UE) measured by the forget accuracy (1-FA) on the WMDP-Bio and WMDP-Cyber subsets, as well as the utility (UT) preserved, measured by the accuracy on the MMLU dataset.  The best average performance across all methods for both UE and UT is highlighted in bold.", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_8_1.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The 'UE Avg.' and 'UT Avg.' refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive evaluation of various LLM unlearning methods on the TOFU benchmark using the LLaMA2-7B-chat model.  It compares different unlearning approaches (GradDiff, NPO, PO) with and without the proposed WAGLE framework, as well as several baseline methods (dense, random, magnitude, Wanda, LoRA). The evaluation includes multiple metrics for both Unlearning Efficacy (UE) and Utility (UT), providing a thorough assessment of each method's effectiveness in balancing forgetting undesired information with preserving the model's overall performance.", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_9_1.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The 'UE Avg.' and 'UT Avg.' refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive performance comparison of various LLM unlearning methods on the TOFU dataset using the LLaMA2-7B-chat model.  It evaluates both unlearning efficacy (UE) and utility retention (UT).  UE is assessed using four metrics: Forget Quality (FQ), Membership Inference Attack (MIA), Forget Accuracy (1-FA), and Rouge-L recall (1-Rouge-L). UT is evaluated by Accuracy and Rouge-L recall on the retain set and on two subsets representing real authors and world facts.  The table includes results for several baseline methods, providing a direct comparison against the proposed WAGLE approach and its integration with different unlearning algorithms (GradDiff, NPO, and PO).", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_17_1.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The 'UE Avg.' and 'UT Avg.' refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive performance comparison of various LLM unlearning methods on the TOFU dataset using the LLaMA2-7B-chat model.  It compares different methods' effectiveness in unlearning (UE) and maintaining model utility (UT), utilizing several metrics for both.  The results are averaged over multiple trials, highlighting the best performing method for each metric.", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_17_2.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The \u2018UE Avg.\u2019 and \u2018UT Avg.\u2019 refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive evaluation of different LLM unlearning methods on the TOFU dataset using the LLaMA2-7B-chat model.  It compares various techniques, including those incorporating the proposed WAGLE framework, across multiple metrics assessing both unlearning efficacy (UE) and utility preservation (UT).  The metrics include Forget Quality (FQ), Membership Inference Attack (MIA), Forget Accuracy (1-FA), Rouge-L Recall (1-Rouge-L), and utility metrics based on accuracy and Rouge-L on retain sets representing real authors and world facts.  The table highlights the best performing method on average across all metrics.", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_19_1.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The \u2018UE Avg.\u2019 and \u2018UT Avg.\u2019 refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive performance comparison of various LLM unlearning methods on the TOFU benchmark using the LLaMA2-7B-chat model.  It evaluates both unlearning efficacy (UE) and utility retention (UT) across multiple metrics, including Forget Quality (FQ), Membership Inference Attack (MIA), Forget Accuracy (1-FA), Rouge-L Recall (1-Rouge-L), and utility metrics on retain sets (Real Authors and World Facts).  Different weight selection strategies (Dense, Random, Magnitude, Wanda, LORA) are compared to the proposed WAGLE method integrated with various unlearning algorithms (GradDiff, NPO, PO). The table highlights the best average performance for both UE and UT.", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_19_2.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The 'UE Avg.' and 'UT Avg.' refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive evaluation of various LLM unlearning methods on the TOFU benchmark using the LLaMA2-7B-chat model.  It compares different methods (GradDiff, NPO, PO) with and without the proposed WAGLE framework, and also includes baselines like random weight selection, magnitude-based pruning, Wanda pruning, and LoRA. The evaluation metrics include forget quality (FQ), membership inference attack (MIA), forget accuracy (1-FA), Rouge-L recall (1-Rouge-L), and utility metrics (accuracy and Rouge-L on retain set, real authors, and world facts).  Higher values generally indicate better unlearning efficacy (UE) or utility preservation (UT).", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_20_1.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The 'UE Avg.' and 'UT Avg.' refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive performance comparison of various LLM unlearning methods on the TOFU benchmark using the LLaMA2-7B-chat model.  It assesses both unlearning efficacy (UE) and utility preservation (UT) across multiple metrics, providing a detailed view of each method's effectiveness in removing unwanted information while retaining the model's original functionality. The table includes results for different weight selection strategies, allowing for a direct comparison of WAGLE against existing baselines.", "section": "5.2 Experiment Results"}, {"figure_path": "VzOgnDJMgh/tables/tables_20_2.jpg", "caption": "Table 1: Performance overview of LLM unlearning on the TOFU task under the LLaMA2-7B-chat model [16]. The \u2191 symbol denotes metrics where higher values indicate better UE or UT performance. The \u2018UE Avg.\u2019 and \u2018UT Avg.\u2019 refer to the average unlearning efficacy across all UE metrics and the average utility post-unlearning across all UT metrics, respectively. Results are averaged over six independent random trials. The best average performance is highlighted in bold.", "description": "This table presents a comprehensive evaluation of various LLM unlearning methods on the TOFU dataset using the LLaMA2-7B-chat model.  It compares different methods (GradDiff, NPO, PO) with and without the WAGLE framework, as well as several baseline approaches (dense model, random, magnitude, Wanda, LORA).  The metrics used evaluate both unlearning efficacy (forgetting undesirable information) and utility retention (preserving the model's original capabilities).  Results are averaged over six trials, highlighting the best average performance for each method and metric.", "section": "5.2 Experiment Results"}]