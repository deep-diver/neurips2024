[{"figure_path": "XAKALzI3Gw/tables/tables_4_1.jpg", "caption": "Table 1: AV-MNIST accuracy comparison between various methods.. Best results are highlighted in bold.", "description": "This table presents a comparison of the accuracy achieved by different multi-modal learning methods on the AV-MNIST dataset.  The methods compared include using only the image modality, only the audio modality, intra-modality modeling (considering each modality separately), and inter-modality modeling using two different techniques (late fusion (LF) and low-rank multimodal fusion (LRTF)). The results also include the performance of the proposed I2M2 (Inter- & Intra-Modality Modeling) framework using both LF and LRTF. The table highlights the best-performing methods in bold, demonstrating the superiority of I2M2.", "section": "4.1 AV-MNIST"}, {"figure_path": "XAKALzI3Gw/tables/tables_5_1.jpg", "caption": "Table 3: Accuracy and VQA score for NLVR2 and VQA-VS respectively. I2M2 obtains comparable performance to inter modality method for NLVR2, while outperforming it for VQA-VS. I and T denote the image and text modalities respectively. Best results are highlighted in bold.", "description": "This table presents the accuracy and VQA scores achieved by different models (Intra-modality, Inter-modality, and I2M2) on two datasets, NLVR2 and VQA-VS.  For each dataset, the results show the performance of models trained using only the image modality (I), only the text modality (T), both image and text modalities but separately (Intra), and both modalities jointly (Inter and I2M2). The best performance for each metric (accuracy and VQA score) is shown in bold, indicating that I2M2 generally outperforms or matches the best of the other approaches across both datasets.", "section": "4 Experiments"}, {"figure_path": "XAKALzI3Gw/tables/tables_8_1.jpg", "caption": "Table 5: Effect of pre-training.", "description": "This table presents the effect of pre-training on the performance of the I2M2 model across different knee pathologies in the fastMRI dataset.  It compares the AUROC scores obtained with and without pre-training, showing that pre-training significantly improves performance for all pathologies. The improved performance with pre-training highlights the importance of initializing models appropriately for multimodal learning and supports the proposed I2M2 framework.", "section": "4.6 Further Analysis: Beyond Aggregate Metrics"}, {"figure_path": "XAKALzI3Gw/tables/tables_17_1.jpg", "caption": "Table B.6: Learning rates (LR) and weight decay (WD) for fastMRI dataset.", "description": "This table presents the hyperparameters used for training different models on the fastMRI dataset.  It shows the learning rate (LR) and weight decay (WD) values used for the magnitude-only, phase-only, intra-modality, inter-modality, and I2M2 models. These hyperparameters were likely determined through a process of hyperparameter tuning, where different combinations of values were tested and the optimal combination was chosen based on the model's performance.", "section": "B Models and Hyperparameters"}, {"figure_path": "XAKALzI3Gw/tables/tables_19_1.jpg", "caption": "Table C.7: Entropy of individual experts. We compare the entropy of the label (y) distribution with the average entropy of the predictions (\u0177) for individual experts.", "description": "This table presents the entropy values for the label (y) distribution and the average entropy of predictions (\u0177) generated by individual modality models (image-only and text-only) and the combined inter-modality model in three different datasets: AV-MNIST, VQA-VS, and NLVR2.  It demonstrates the information gain from incorporating multiple modalities by showing the reduction in entropy from individual modalities to the combined model. Lower entropy values indicate higher predictability, highlighting the effectiveness of integrating information from different modalities.", "section": "C Additional Results"}]