[{"type": "text", "text": "Why are Visually-Grounded Language Models Bad at Image Classification? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuhui Zhang1\u2020 Alyssa Unell1 Xiaohan Wang1 Dhruba Ghosh2 Yuchang Su3 Ludwig Schmidt1,2\u2020 Serena Yeung-Levy1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Stanford University 2University of Washington 3Tsinghua University \u2020{yuhuiz,ludwigsc,syyeung}@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visuallygrounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is datarelated: critical information for image classification is encoded in the VLM\u2019s latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM\u2019s performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of $11.8\\%$ on the newly collected ImageWikiQA dataset.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The ability to recognize objects within images is a fundamental capability of machine vision. Over the past 15 years, the field has experienced significant breakthroughs due to deep learning and large-scale datasets [11, 48]. For instance, on the renowned ImageNet dataset, designed to classify images into 1,000 categories, the error rate has dramatically decreased from $47.1\\%$ in 2009 to $9.1\\%$ in 2024, representing a 5-fold reduction [30, 12]. Consequently, these classification models have superseded most human labelers. ", "page_idx": 0}, {"type": "text", "text": "Nowadays, the community has focused on more sophisticated and nuanced capabilities in the quest for visual intelligence. Visually-grounded language models (VLMs), which integrate visual signals from vision encoders with large language models, have recently emerged as a promising paradigm [2, 36, 32]. VLMs like GPT-4V [36], Gemini-1.5 [44], or Claude-3 [3] have demonstrated advanced visual understanding abilities, such as answering math questions from table images or generating HTML code from design sketches. ", "page_idx": 0}, {"type": "text", "text": "In this work, we revisit the fundamental task of image classification using VLMs. Surprisingly, we find that various public and proprietary VLMs struggle with image classification in both open-world settings, where the class list is unknown, and closed-world settings, where class names are provided in the context (\u00a72). Despite having many more parameters, there is a significant gap between the performance of VLMs and their commonly used vision encoder CLIP [37]. Our evaluation protocol involves feeding each image and a list of class names (in the closed-world setting) to the VLM as context and asking what is in the image; success is defined by whether the generated output contains the ground-truth class name. ", "page_idx": 0}, {"type": "image", "img_path": "MwmmBg1VYg/tmp/d43824554175bb603eb7080eab209d6be541757cb922c324f49dcd06409d4dcb.jpg", "img_caption": ["Figure 1: Overview. (Left) Different visually-grounded language models (VLMs) underperform CLIP in classification by a large margin, though they often use CLIP as a vision encoder. (Middle) We investigate several hypotheses about why VLMs are bad classifiers and find that the main reason is data. Critical information for image classification is encoded in the VLM\u2019s latent space but can only be decoded with enough data during VLM training. (Right) Based on our analysis, we improve a VLM by integrating classification data into its training, and find that the improved classification capabilities serve as foundations for more advanced capabilities such as visual question answering. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To understand why VLMs underperform in classification settings, we investigate several hypotheses regarding VLMs\u2019 inference (such as prompt variations, label set size, inference strategy; $\\S3.1)$ , training (such as information lost, training objective; $\\S3.2)$ , and data (such as data-performance correlation; $\\S3.3)$ . Our extensive analyses suggest that the primary reason for the observed gap is data. We find that the information necessary for classification is encoded in the VLM\u2019s latent space but can only be decoded with proper training data. Specifically, there is a strong correlation between class presence during VLM training and performance in those classes. Furthermore, training VLMs on classification datasets achieves the same performance level as state-of-the-art classification models. ", "page_idx": 1}, {"type": "text", "text": "Motivated by our analysis, we propose a simple method to enhance VLMs\u2019 general capabilities by integrating traditional classification-focused datasets into VLM training (\u00a74). We believe that classification is the foundation for more complex, advanced visual capabilities; for example, recognizing an object is a prerequisite for answering complex questions about it. To verify this, we created ImageWikiQA, which contains complex real-world questions about ImageNet objects. On ImageWikiQA, we find that VLMs fine-tuned on the ImageNet classification dataset achieve substantially higher accuracy in recognizing these objects and provide more accurate answers to these non-classification questions, outperforming pre-trained VLMs by $11.8\\%$ . This suggests that classical classification data can be beneficially reused in the VLM training process to enhance VLM performance. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Evaluating VLM classification weaknesses: Our evaluations using ten VLMs across four benchmarks show that VLMs significantly lag behind CLIP in classification, uncovering a gap unaddressed by previous research.   \n\u2022 Analyzing causes of poor classification performance: Testing various hypotheses, we find that the lack of alignment data, rather than information lost or training objective, is the primary reason for VLMs\u2019 underperformance in classification.   \n\u2022 Enhancing VLMs with classification data: By adding classification data, we improve VLMs\u2019 accuracy in object recognition and overall performance, supporting that classification capability is foundational for complex object-related reasoning. ", "page_idx": 1}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/c9f1df420f9b36930430d6c4abbf6999e49e14505733fa25ce3a598880e98cb4.jpg", "table_caption": [], "table_footnote": ["Table 1: Evaluations of VLMs and CLIPs on standard image classification benchmarks. VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models. =ImageNet [11], $\\ast$ =Flowers102 [35], $\\clubsuit-$ StanfordCars [21], $\\stackrel{\\triangledown}{\\mathbf{\\Xi}}$ Caltech101 [13]. "], "page_idx": 2}, {"type": "text", "text": "2 VLMs are Bad at Image Classification ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin by evaluating state-of-the-art visually-grounded language models (VLMs) using standard image classification benchmarks. Our findings reveal that these VLMs significantly underperform compared to state-of-the-art classification models, such as CLIP. ", "page_idx": 2}, {"type": "text", "text": "2.1 Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "VLMs. We selected ten widely-used state-of-the-art VLMs, covering different architectures, training methods, and data. These VLMs include three proprietary ones, GPT-4-Turbo (shortened as GPT4, same below) [36], Gemini-Pro-Vision (GeminiPro) [44], and Claude-3-Opus (Claude3) [3], and seven public ones, LLaVA1.5-Vicuna7B/13B (LLaVA1.5-7/13B) [32], LLaVANeXT-Mistral7B/Vicuna7B (LLaVANeXT-M7B/V7B) [31], BLIP2-OPT2.7B (BLIP2-2.7B) [27], and InstructBLIP-Vicuna7B/13B (IBLIP-7/13B) [10]). Details of these models are provided in Appendix $\\S\\mathrm{A.l}$ . ", "page_idx": 2}, {"type": "text", "text": "CLIPs. For comparison, we used two state-of-the-art image classifiers, CLIP-ViT-L/14-336px (shortened as CLIP-L, same below) [37] and EVA-ViT-G/14 (EVA-G) [43]. Notably, CLIP-L and EVA-G are utilized by the LLaVA series [32] and the BLIP series as vision encoders [27], respectively. Therefore, the VLMs should theoretically have the same classification capacity as these vision models. Details are listed in Appendix $\\S\\mathrm{A.l}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We evaluated the aforementioned models on four widely-used image classification benchmarks: ImageNet (shortened as $\\therefore$ , same below) [11], Flowers102 $\\pmb{\\frac{\\partial\\pmb{\\nu}}{\\partial t}}$ ) [35], StanfordCars $\\scriptstyle\\left({\\frac{\\omega}{\\circ-\\theta}}\\right)$ [21], and Caltech101 $\\left({\\circled{\\egroup}}\\right)$ [13], which contain 50,000, 6,149, 8,041, and 4,331 test images from 1,000, 102, 196, and 101 classes, respectively. ImageNet and Caltech cover more coarse-grained objects, while Flowers and Cars cover more fine-grained objects. Further details are provided in Appendix $\\S\\mathrm{A}.2$ . ", "page_idx": 2}, {"type": "text", "text": "2.3 Evaluation Protocol ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "VLMs. We performed image classification in two settings: an open-world setting where the label set is not provided and a closed-world setting where classes are concatenated in the prompt. We feed the image and the prompt to the VLM and let the VLM complete the rest of the tokens. One closed-world example is: \u201c<image> What type of object is in this photo? Choose one from <class name $\\mathrm{A}{>}$ , <class name $\\mathbf{B}\\!>$ , ...\u201d We define success on a single example as whether the ground-truth label is included in the VLM generation. We report the success rate of all test examples. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "CLIPs. CLIP can only be used in a closed-world setting where the label set is known. Following Radford et al. [37], we used the prompt \u201ca photo of a <class>\u201d to generate the text feature. For each image, we selected the class with the highest cosine similarity to the image feature. We did not include prompt ensembling to fairly compare with VLM. We report the accuracy of all examples. ", "page_idx": 3}, {"type": "text", "text": "2.4 Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Table 1 reports the performance of different VLMs and CLIP models on these classification datasets. ", "page_idx": 3}, {"type": "text", "text": "We find that VLMs exhibit poor performance in image classification, significantly lagging behind CLIP models. For instance, on the ImageNet dataset, the best proprietary VLM, GPT4, only achieves an accuracy of $60.6\\%$ in the closed-world setting, whereas the best CLIP, EVA-G, attains an accuracy of $79.2\\%$ . In the open-world setting, the best public VLM, LLaVANeXT-M7B, achieves just $32.3\\%$ accuracy. The performance disparity is even more pronounced in fine-grained classification datasets like Flowers102 and StanfordCars. Notably, all LLaVA models use CLIP-L as the vision encoder, and although the total parameter count of these models is at least 20 times greater than that of the vision encoder, they significantly underperform compared to it. ", "page_idx": 3}, {"type": "text", "text": "Moreover, we find that closed-world setting often outperforms open-world setting. This is expected as the provided label set narrows the prediction space. However, since closed-world settings require including all class names in the context, they can result in an extremely long context that leads to high costs or even exceeds the VLM\u2019s context limit. For example, most public VLMs only support 4K context length, which cannot feed 1K ImageNet classes. We also find that larger and better LMs slightly improve VLM performance. For instance, LLaVA1.5-13B outperforms LLaVA1.5-7B, and LLaVANeXT-M7B outperforms LLaVANeXT-V7B. ", "page_idx": 3}, {"type": "text", "text": "3 Why are VLMs Bad Image Classifiers? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given that visually-grounded language models (VLMs) underperform CLIPs at classification by a large margin, as reported in $\\S2$ , we seek to understand the reasons behind that. We investigate several hypotheses concerning major differences between VLMs and CLIPs, which can be generally categorized into inference (\u00a73.1), training (\u00a73.2), and data (\u00a73.3): ", "page_idx": 3}, {"type": "text", "text": "1. We start with inference-related questions. For example, does prompt variation, such as chain of thought, affect final performance? Does reducing the label set size in context narrow the gap between VLMs and CLIPs? Does performing probabilistic inference to force the generation into the label set help? We find none of these factors can fully close the gap between VLMs and CLIPs.   \n2. Therefore, we switch to training-related questions. For example, is the visual information from the vision encoder still preserved in the VLM\u2019s latent space? Is the text generation objective as effective as cross-entropy loss for learning classification? Surprisingly, the results show that the information is preserved, and the text generation objective is adequate for learning classification.   \n3. Finally, we investigate data-related questions. For example, does the VLM training data include enough classification data and cover enough classes? We find a strong correlation between class exposure in training and model performance. Moreover, VLMs can achieve the same level of performance as CLIPs when trained with enough data. These results suggest that data is the primary cause and effective solution for the poor classification performance of VLMs. ", "page_idx": 3}, {"type": "text", "text": "3.1 Inference ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we investigate three questions related to VLM\u2019s inference, including prompt variation, label set size, and inference strategy. ", "page_idx": 3}, {"type": "text", "text": "Prompt variation. It is well known that language models (LMs) are sensitive to prompts [7, 16, 53]. To understand the effect of prompts on classification performance, we tested three semantically similar but differently worded prompts, compared feeding the label in dataset order or random order within the context, and leveraged the zero-shot chain-of-thought (CoT) prompting technique by adding \u201clet\u2019s think step by step\u201d at the end of the prompt [47, 20]. ", "page_idx": 3}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/35085d74684ace3964a2e244252a4ef51ef39565dfbcaa83120df64352bf3841.jpg", "table_caption": [], "table_footnote": ["Table 2: Analysis of VLMs from the inference perspective. (Top) We explore prompt variation such as wording, label order, chain-of-thought and find it has limited impact on the performance. (Bottom) We leverage the probabilistic inference strategy, which improves the performance but still fails to close the gap between VLMs and CLIPs. Results are from the official validation set. "], "page_idx": 4}, {"type": "image", "img_path": "MwmmBg1VYg/tmp/5410289ae25c4defe41bc0a6ade93368734629e13f0b838df01dfcab78c507f6.jpg", "img_caption": ["Figure 2: Analysis of the label set size. For each image, we randomly sample 100, 20, 5, 2 candidate classes from all the classes. The performance gap between VLMs and CLIPs becomes smaller when the number of classes is reduced. X-axis: number of classes; Y-axis: accuracy $(\\%)$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We find that prompt variation has a limited impact on the performance. From Table 2, we can see that changing the wording of prompts results in a performance variation within $3\\%$ for both LLaVA1.5-7B and BLIP2-2.7B on the ImageNet dataset. Different label orderings impact LLaVA1.5-7B less than BLIP2-2.7B. Chain-of-thought prompting consistently improves performance for instruction-tuned model LLaVA1.5-7B but not for BLIP2-2.7B. ", "page_idx": 4}, {"type": "text", "text": "Label set size. The label set size can be very large in practice (e.g., 1000 for ImageNet, 196 for StanfordCars), which results in an extremely long context when we concatenate all the class names. Since LMs often struggle with long contexts [33], we explore reducing the number of classes. For each image, we randomly select $K=2,5,20,1$ 00 classes, always including the ground-truth label, and re-evaluate the VLM and CLIP performance. ", "page_idx": 4}, {"type": "text", "text": "We find that the performance gap between VLMs and CLIPs narrows with reduced label size, but the gap always exists. As shown in Figure 2, when evaluating LLaVA1.5-7B and CLIP-L on ImageNet, the gap decreases from $46\\%$ with 100 classes to $6\\%$ with 2 classes. However, the relative gap becomes larger, evidenced by a $23.9\\mathbf{x}$ error rate with 2 classes compared to a $7.3\\mathbf{x}$ error rate with 100 classes. The performance gap between VLMs and CLIPs always exists in all the settings. ", "page_idx": 4}, {"type": "text", "text": "Inference algorithm. The default inference algorithm for classification with VLMs is directly generating the class name given a prompt. As the generation is open-ended, even when provided with a list of candidate choices, the generation may not match one of the pre-defined classes. To mitigate this problem, we employ probabilistic inference techniques for VLMs. Specifically, for each class name, we compute prob(class name|image, prompt) and select the class name with the highest probability as the prediction. Since class names can consist of multiple tokens (e.g., \u201cguinea pig\u201d consists of two tokens), we either average the probabilities of all tokens or sum up all the tokens [7]. We also explore classifier-free guidance (CFG) techniques by ranking $t*$ prob(class name|image, prompt) $^+$ $\\bar{(1-t)}*$ prob(class name|prompt) with varying guidance coefficients $t$ [38, 50]. ", "page_idx": 4}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/a539abafad58574339ea214e3024da81f7c2e5dceee4b78742488c2c2ee70730.jpg", "table_caption": [], "table_footnote": ["Table 3: Analysis of VLMs from the training perspective. (Left) We conduct feature probing experiments on the VLM\u2019s last layer and find that the information required for classification is mostly preserved in the VLM\u2019s latent space. (Right) We fine-tune VLMs on the classification datasets using the text generation objective and find that the text generation training objective is as effective as the traditional cross-entropy for learning classification, which eliminates the VLM-CLIP performance gap, with VLMs now being the state-of-the-art classifier. Results are from the official validation set. "], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We find that the probabilistic inference method improves the performance, but the gap persists. As shown in Table 2, LLaVA1.5-7B achieves $35.3\\%$ accuracy on ImageNet using probabilistic inference compared to $22.7\\%$ using the direct generation method. Adding classifier-free guidance further improves the performance, where LLaVA1.5-7B performance boosts to $47.6\\%$ on ImageNet. However, it still leaves around a $30\\%$ performance gap between VLMs and CLIPs, as its vision encoder CLIP-L achieves $74.8\\%$ on ImageNet. Moreover, the probabilistic inference approach is computationally expensive in practice because we need to compute the probability of each class. ", "page_idx": 5}, {"type": "text", "text": "3.2 Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since inference-based modifications fail to close the performance gap between VLMs and CLIPs, here we investigate two questions regarding to the training of VLMs. We study whether the visual information is lost in the VLM and whether the text generation objective is suitable for learning the classification task. ", "page_idx": 5}, {"type": "text", "text": "Visual information lost in the VLM. VLMs process images using an image encoder, such as CLIP, which has strong classification capabilities. We hypothesize that, during the propagation of image features output from the vision encoder in language model layers, the necessary information for classification is lost. To test this hypothesis, we conduct feature probing experiments. Specifically, the features from the VLM\u2019s last layer have a shape corresponding to the length of the inputs (image tokens plus text tokens using the open-world prompt). We take the average of these token features or the last token feature and train a simple linear classifier on top of these frozen feature representations. The linear classifier is trained on the training set and evaluated on the validation set. Training details are provided in Appendix B.4. A higher accuracy indicates that less information is lost. ", "page_idx": 5}, {"type": "text", "text": "Surprisingly, we find that the information necessary for classification is largely preserved in the VLM\u2019s latent space; however, it cannot be effectively decoded. In Table 3, we show that the probing accuracy of LLaVA1.5-7B on ImageNet is $77.1\\%$ , which is close to the $85.2\\%$ probing accuracy of CLIP-L, the vision encoder used by LLaVA1.5-7B. The same conclusion holds for BLIP2-2.7B and its vision encoder EVA ViT-G/14, demonstrating that most information is preserved during the VLM computational process. However, this information cannot be effectively decoded, as $\\S2$ shows that the best accuracy LLaVA1.5-7B can achieve on ImageNet is $47.6\\%$ . ", "page_idx": 5}, {"type": "text", "text": "Training objective. Since information is encoded in VLMs, we wonder whether we can train them to decode the information. VLMs can only be trained to perform classification by auto-regressively generating text-form labels. We hypothesize that this generative objective may be more difficult and less effective in learning classification compared to the traditional cross-entropy loss. To understand the effectiveness of this training objective, we explore fine-tuning VLMs on classification datasets. We convert classification datasets into the text generation format using the template (e.g., \u201c<image> What type of object is in this photo? <class name>\u201d). We fine-tune two model architectures (LLaVA1.5-7B and BLIP2-2.7B) across different datasets and compare their accuracy to that of fine-tuned CLIP models. Additionally, we investigate fine-tuning different parts of the models, such as only fine-tuning the projector between vision encoder and language models or fine-tuning the projector along with the language models using LoRA [17]. Training details are provided in Appendix B.5. ", "page_idx": 5}, {"type": "image", "img_path": "MwmmBg1VYg/tmp/ef5e07a7c4e1c5efafd914d7575f2d44d6417a6fb9c02863389fe61ec31ce2c3.jpg", "img_caption": ["Figure 3: Analysis of VLMs from the data perspective. We study the relation between the ImageNet class frequency in the VLM training data and the VLM\u2019s classification performance on those classes. A strong correlation is observed, indicating that data determines VLM classification performance. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We find that the text generation training objective is as effective as the traditional crossentropy for learning classification tasks, which eliminates the performance gap between VLMs and CLIPs, with VLMs now being the state-of-the-art classifier. From Table 3, we find that LLaVA1.5-7B achieves $85.7\\%$ accuracy on ImageNet, same as $85.2\\%$ accuracy when fine-tuning its vision encoder CLIP-L. The same findings apply to all the VLMs on all the datasets. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we find that fine-tuning only the projector is sufficient and has better numerical stability. From Table 3, we can see the same level of accuracy achieved by fine-tuning the projector and fine-tuning projector and LLM using LoRA on the three datasets. We also find that finetuning LLM with LoRA often leads to numerical instabilities, such as spikes in loss. While the loss sometimes returns to normal, other times it does not. For example, despite trying various hyperparameters for ImageNet, instability persisted (see Appendix $\\S B.5)$ ). In contrast, fine-tuning only the projector always results in a steadily decreasing loss. Notably, the projector is often much smaller (e.g., a 2-layer MLP in LLaVA) compared to LLMs, suggesting the potential for parameter-efficient prefix tuning for VLMs [28]. ", "page_idx": 6}, {"type": "text", "text": "3.3 Data ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As observed, the visual information is preserved in the VLM\u2019s latent space, and the VLM\u2019s training objective is sufficient for learning classification tasks. We hypothesize that the poor classification performance of VLMs is due to their training data. For example, there might be insufficient classification data or a lack of diverse classes. To investigate this, we analyze the LLaVA1.5-7B training data [32], the only fully publicly available VLM training dataset. We examined the relationship between the frequency of class occurrence and the VLM\u2019s classification performance on those classes2. ", "page_idx": 6}, {"type": "text", "text": "Our findings indicate that data determines VLM classification performance. As shown in Figure 3, there is a strong correlation between the presence of class labels in the VLM training data and the VLM\u2019s classification accuracy for those classes. The LLaVA1.5-7B model achieves $82.7\\%$ zero-shot accuracy on ImageNet classes with more than 10,000 occurrences in its training data, but only $3.0\\%$ accuracy on classes with fewer than 10 occurrences. The Spearman correlation between class frequency and class performance is very high (0.76 on ImageNet; see Appendix $\\S B.6)$ ). In contrast, there is no correlation between its vision encoder CLIP-L\u2019s performance on those classes with the same VLM training data, and after fine-tuning LLaVA1.5-7B on the ImageNet classification data, the strong correlation disappears. Combining all of our results, we conclude that data plays a critical role in determining the VLM\u2019s classification performance. ", "page_idx": 6}, {"type": "text", "text": "Given that data availability is critical for VLM classification performance, we now examine whether the data type specifically impacts results. For instance, to train a VLM to classify a class like \u201cguinea pig,\u201d should it rely on classification-focused data (e.g., \u201c<image> What type of object is in this photo? ", "page_idx": 6}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/1b5c4ecf34ea7b6a459a3b8e55ea0b4951ee3919ec184e7aebdd2aaa8912cc02.jpg", "table_caption": ["Table 4: Analysis of data types. We fine-tune the VLM on the caption-focused data generated by GPT4 using the same experimental settings as Table 3 and find that data is the main determining factor for VLM performance, and the data type does not matter much. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Guinea pig\u201d) or can it use broader types, such as caption-focused data (e.g., \u201c<image> Generate a caption for the image. A guinea pig playing in the grass\u201d) or VQA-focused data (e.g., \u201c<image> What is the native region of this guinea pig? South America\u201d)? ", "page_idx": 7}, {"type": "text", "text": "In previous experiments (see $\\S3.2)$ ), we fine-tuned the VLM with a classification-focused template. Here, we fine-tune the VLM on caption-focused data generated by prompting GPT4 to produce captions incorporating each class\u2019s ground-truth name. Specifically, we used the prompt: \u201cGenerate a short caption for the image. The caption must include the ground-truth label of the image, which is <class name>.\u201d This approach produces diverse captions per class, such as \u201cA guinea pig playing in the grass\u201d or \u201cA close-up view of a guinea pig in an indoor environment.\u201d We then fine-tune the VLM on this caption-focused data using identical experimental settings and evaluate it against the model trained with classification-focused data. Success is determined by whether the generated caption for a validation image includes the ground-truth class name. ", "page_idx": 7}, {"type": "text", "text": "We find that data presence is the primary factor for VLM classification performance, with data type playing a minimal role. From Table 4, we observe that models fine-tuned on both classificationand caption-focused data perform similarly on Flowers102, StanfordCars, and Caltech101, achieving accuracy gains of at least $48.6\\%$ over the non-fine-tuned model. This suggests that enabling a VLM to classify a specific class does not require classification-specific data; any data containing the class name suffices. ", "page_idx": 7}, {"type": "text", "text": "These findings emphasize a critical data-centric view of VLM training. A common question for VLM training is whether alignment between vision encoder outputs and text generation necessitates multi-modal data with specific class labels, despite both vision and language components seeing these classes in uni-modal pre-training. Previous studies have suggested that this alignment stage may be data-efficient or even unnecessary [32, 18], likely due to their evaluation settings not requiring visioncentric fine-grained recognition. Our work, however, shows that 1) multi-modal data is essential for alignment and 2) increasing data quantity linearly boosts performance. Concurrent work from DeepMind supports our findings, showing that most VLM tasks benefit from pre-training on larger datasets [6]. ", "page_idx": 7}, {"type": "text", "text": "4 Improving VLM with Classification Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Based on the analysis presented in $\\S3$ , in this section, we discuss the enhancement of visuallygrounded language models (VLMs) by integrating classification-focused data into their training. We demonstrate that this data intervention not only boosts the VLM\u2019s classification performance but also enhances its general capabilities. ", "page_idx": 7}, {"type": "text", "text": "4.1 Motivation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Classification is fundamental to enabling more advanced capabilities of VLMs, such as visual question answering and reasoning. For example, suppose a virtual assistant is helping visually impaired individuals prepare mushrooms as food. In that case, the model must correctly identify the mushroom species to answer questions like \u201cIs this mushroom poisonous?\u201d The poor classification performance of VLMs lays a weak foundation for their advanced capabilities. ", "page_idx": 7}, {"type": "text", "text": "From $\\S3$ , we identify the primary cause of poor classification performance is the lack of data. Therefore, we propose a straightforward solution: integrating classification-focused data into the VLM training process. We hope that incorporating classification data not only enhances classification accuracy but also improves general capabilities. ", "page_idx": 7}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/47ed3759184f5a8f9ef296dcb64c264b4c122ae4a6ff2a2a5f63bf24da821b52.jpg", "table_caption": [], "table_footnote": ["Table 5: Evaluations of VLMs on ImageWikiQA. ImageWikiQA is a multiple-choice questionanswering dataset collected by feeding the Wikipedia pages of ImageNet classes to GPT-4. We find that current VLMs perform poorly in answering these questions, suggesting that their poor classification performance is a fundamental limitation for more advanced capabilities. Integrating classification data into VLM training enhances both their classification and overall capabilities. "], "page_idx": 8}, {"type": "text", "text": "4.2 ImageWikiQA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify our hypothesis, we need a dataset that evaluates both the classification and advanced capabilities of VLMs. However, current visual question answering benchmarks, such as VQAv2 [15], MM-Vet [51], and MMMU [52], primarily focus on advanced capabilities, such as reasoning and knowledge grounding, rather than classification. The objects in these datasets are relatively simple, and questions can often be answered by identifying only the general category of an image, such as \u201cmushroom\u201d or \u201cflower\u201d, rather than specific types of mushrooms or flowers. In contrast, classification datasets have no questions related to more advanced capabilities. ", "page_idx": 8}, {"type": "text", "text": "To bridge the gap between classification and advanced capabilities, we introduce ImageWikiQA, an object-centric, knowledge-intensive question answering dataset that combines both worlds. Each question in ImageWikiQA is a multiple-choice question with four options and one correct answer (e.g., \u201c<image showing a guinea pig> What is the native region of this object? A. South America; B. Africa; C. Asia; D. Australia\u201d). Although each question does not directly ask for the category of the object within the image, the question can only be accurately answered if the class of the object is correctly identified. Example questions from ImageWikiQA can be found in Figure 1 (right) and Appendix $\\S C.1$ . ", "page_idx": 8}, {"type": "text", "text": "ImageWikiQA is created by generating questions based on Wikipedia pages of ImageNet classes using GPT4. Specifically, for each class in ImageNet, we parsed the Wikipedia content of the class following Bujwid et al. [8], and then provided this content along with a prompt to GPT4 to generate five questions per class. We instructed GPT4 to replace the class name with \u201cthis object\u201d in the questions so that the ground-truth class name is not provided. We retained all questions that GPT4 could answer correctly with the ground-truth class name and could not guess the answer without the class name. To ensure the question quality generated by GPT4, four human annotators attempted to answer the questions with the ground-truth class name and Wikipedia page and achieved an accuracy of $96.5\\%$ . Afterward, we randomly sampled at most 3 ImageNet images for each question, rebalanced the choice distribution, and composed the final ImageWikiQA dataset. In total, there are 2000 multiple-choice questions, each with an image, question, four candidate choices, and a reference to Wikipedia sentences, with a random guess accuracy of $25.0\\%$ and max frequency accuracy of $25.9\\%$ . ", "page_idx": 8}, {"type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 5 presents the performance of various VLMs on the ImageWikiQA dataset. ", "page_idx": 8}, {"type": "text", "text": "We find that current state-of-the-art VLMs perform poorly on answering these questions given images. For example, GPT4 achieves $100\\%$ accuracy when the ground-truth class name is provided, but only achieves $61.2\\%$ accuracy with images. Similarly, Claude3 and GeminiPro only achieve $54.3\\%$ and $49.1\\%$ accuracy, respectively. These results indicate that the poor classification performance of VLMs is a fundamental limitation for more advanced capabilities. ", "page_idx": 8}, {"type": "text", "text": "Furthermore, we find that integrating classification data into VLM training improves its classification and general capabilities. We fine-tuned LLaVA1.5-7B on the ImageNet 1.28M classification data and original 665K LLaVA instruction-tuning data (training detail in $\\S C.2\\rangle$ , which is able to achieve $84.4\\%$ accuracy on ImageNet classification compared to $22.8\\%$ for non-fine-tuned models. This improvement in classification translates to an $11.8\\%$ accuracy boost on ImageWikiQA, demonstrating classification is indeed a foundation for VLM\u2019s advanced capabilities. However, it is worth noting that fine-tuning solely on the classification task harms general capabilities. When fine-tuning LLaVA1.5-7B on ImageNet only, their accuracy on ImageWikiQA drops to $30.6\\%$ . Therefore, finetuning should be performed on a combined dataset. Developing fine-tuning methods that prevent such catastrophic forgetting is a promising future research direction. ", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Visually-grounded language models. Visually-grounded language models (VLMs) refer to a large family of models that integrate visual signals into language models by modeling $p(y_{t}|y_{<t},x)$ , where $y_{i}$ is a text token and $x$ is a visual input such as an image or video. Recently, many powerful VLMs have been developed, including proprietary ones like GPT-4V [36], Gemini [44], Claude [3], Flamingo [2] and public ones like LLaVA [32, 31], BLIP [27, 10], OpenFlamingo [4], Otter [26], Fuyu [1], QwenVL [5]. The typical architecture of VLMs comprises three components: a visual encoder (often CLIP [37]), a language model, and a projector that bridges the visual outputs and language model inputs. The projector can be as simple as a linear layer or MLP (e.g., LLaVA [32]) or a complex architecture such as Transformer with cross-modal attention (e.g., BLIP [27]). VLMs are usually trained on image-text captioning data [40, 39] and carefully designed instruction-tuning data [32], with both the vision encoder and language model initialized from pre-trained versions. In this work, we evaluate widely used VLMs in classification settings and analyze two prominent architectures, LLaVA [32] and BLIP [27]. ", "page_idx": 9}, {"type": "text", "text": "Analysis of visually-grounded language models. While VLMs have demonstrated impressive performance, many questions remain unanswered. Recent works have explored their limitations from various perspectives, including architectures [34], training recipes [18, 24], data [14, 46], vision encoders [45], language models [2, 44], input resolution [41], and proposed solutions for these limitations. For instance, Tong et al. [45] discovered that CLIP vision encoders, employed by most VLMs, often fail to distinguish between certain image pairs despite their apparent visual differences. They suggested utilizing alternative vision encoders, such as DINO, to address this problem. Our work aligns with these studies in understanding VLM limitations and proposing solutions. We find that current VLMs struggle with image classification, and we thoroughly investigate the reasons behind this, proposing a simple solution to integrate classification-focused data into VLM training. ", "page_idx": 9}, {"type": "text", "text": "Image classification. Image classification is one of the most fundamental capabilities of machine intelligence. Starting in the 1990s, researchers collected datasets like MNIST [25] for digit classification and CIFAR [22] for object classification, using various machine learning algorithms such as SVMs and MLPs to tackle the problem. A significant milestone was the ImageNet [11], which elevated the scale and quality of datasets to a new level, driving advancements in deep learning. With the rise of deep supervised and self-supervised learning [23, 9], performance on these datasets soon began to saturate. As a result, classification models have superseded most human labelers, leading researchers to focus on more advanced visual intelligence tasks like visual reasoning. In this work, we revisit this simple yet fundamental task, discovering that current VLMs struggle with it. We demonstrate that classification remains essential, as it forms the foundation for more advanced capabilities, and enhancing VLMs\u2019 classification capabilities improves their overall performance. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we explored the use of visually-grounded language models (VLMs) as image classifiers. We found that their performance is limited across various datasets. We then analyzed the reasons behind these limitations and, based on our findings, trained a VLM with enhanced general capabilities. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Irena Gao, Jeffrey Gu, Weixin Liang, Alejandro Lozano, Shiori Sagawa, Ruocheng Wang, Yunzhi Zhang, Orr Zohar for providing valuable feedback to this project. This work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML), Open Philanthropy, and the Allen Institute for AI. Serena Yeung-Levy is a Chan Zuckerberg Biohub \u2014 San Francisco Investigator. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Adept. Fuyu-8b: A multimodal architecture for ai agents, 2023.   \n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In NeurIPS, 2022. [3] Anthropic. Introducing the next generation of claude, 2024.   \n[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.   \n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [6] Lucas Beyer, Andreas Steiner, Andr\u00e9 Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: A versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024.   \n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.   \n[8] Sebastian Bujwid and Josephine Sullivan. Large-scale zero-shot image classification from rich and diverse textual descriptions. In EACL Workshop, 2021.   \n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.   \n[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In NeurIPS, 2023.   \n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.   \n[13] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshop, 2004.   \n[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS, 2023.   \n[15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017.   \n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2020.   \n[17] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2021.   \n[18] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. arXiv preprint arXiv:2402.07865, 2024.   \n[19] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 2017.   \n[20] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022.   \n[21] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshop, 2013.   \n[22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.   \n[24] Hugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024.   \n[25] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.   \n[26] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.   \n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[28] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, 2021.   \n[29] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023.   \n[30] Yuanqing Lin, Fengjun Lv, Shenghuo Zhu, Ming Yang, Timothee Cour, Kai Yu, Liangliang Cao, and Thomas Huang. Large-scale image classification: Fast feature extraction and svm training. In CVPR, 2011.   \n[31] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.   \n[32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.   \n[33] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. TACL, 2023.   \n[34] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024.   \n[35] M-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, 2008. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[36] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ", "page_idx": 12}, {"type": "text", "text": "[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[38] Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman. Stay on topic with classifier-free guidance. arXiv preprint arXiv:2306.17806, 2023.   \n[39] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[40] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.   \n[41] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. When do we not need larger vision models? arXiv preprint arXiv:2403.13043, 2024.   \n[42] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019.   \n[43] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023.   \n[44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[45] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[46] Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. No\" zero-shot\" without exponential data: Pretraining concept frequency determines multimodal model performance. arXiv preprint arXiv:2404.04125, 2024.   \n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.   \n[48] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, 2022.   \n[49] Wenhao Wu, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli Ouyang, and Jingdong Wang. Gpt4vis: what can gpt-4 do for zero-shot visual recognition? arXiv preprint arXiv:2311.15732, 2023.   \n[50] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022.   \n[51] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.   \n[52] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024.   \n[53] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In ICML, 2021. ", "page_idx": 12}, {"type": "text", "text": "Broader Impacts and Ethics Statement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this work, we identify a core limitation of visually-grounded language models (VLMs) and propose a simple solution to address this issue. We find that VLMs underperform in image classification. By integrating classification data into VLM training, we have demonstrated that this intervention not only enhances classification accuracy but also improves the general capabilities of VLMs. This advancement holds promise for real-world applications, such as virtual assistants for visually impaired individuals. However, there are potential risks associated with incorporating race-based or genderbased classification data, which could amplify discriminatory biases in VLMs. We strongly emphasize the importance of ethical and responsible use of our approach to ensure it contributes positively to our digital ecosystem. ", "page_idx": 13}, {"type": "text", "text": "Reproducibility Statement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide an open-source implementation of our work and have released the ImageWikiQA dataset at https://github.com/yuhui-zh15/VLMClassifier. This will enable researchers to reproduce all experiments described in this paper and conduct their own analyses on additional VLMs and datasets. ", "page_idx": 13}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Due to computational constraints, we cannot evaluate all possible VLMs and datasets. We select two representative ones, LLaVA and BLIP, and conduct analyses on four datasets. Additionally, as we find that VLMs have information in their latent space, we conduct large-scale training to decode them. Developing a zero-shot method to decode this information without extensive training could be a promising direction for future research. ", "page_idx": 13}, {"type": "text", "text": "Compute Resources ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use four NVIDIA L40S GPUs for all experiments. Most inference experiments take 5-20 hours on a single L40S GPU. The most computationally intensive step is fine-tuning. Fine-tuning LLaVA1.5-7B on 1.28M ImageNet data takes 1.4 days for one epoch using four GPUs. Fine-tuning LLaVA1.5-7B on 1.28M ImageNet data combined with 665K original LLaVA instruction-tuning data takes 3.5 days for one epoch using four GPUs. For the 13B model, the cost is approximately $40\\%$ higher. ", "page_idx": 13}, {"type": "text", "text": "Summary of Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide additional details for each section in the main paper. ", "page_idx": 13}, {"type": "text", "text": "\u2022 In $\\S\\mathrm{A}$ , we provide details of models and data for $\\S2$ .   \n\u2022 In $\\S B$ , we provide details of experimental settings and results for $\\S3$ .   \n\u2022 In $\\S C$ , we provide details of our collected ImageWikiQA dataset for $\\S4$ .   \n\u2022 In $\\S D$ , we summarize the contributions of our work. ", "page_idx": 13}, {"type": "text", "text": "A Supplementary for Section 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Model ", "page_idx": 13}, {"type": "text", "text": "We include model details in Table 6. ", "page_idx": 13}, {"type": "text", "text": "A.2 Data ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We include data details in Table 7. ", "page_idx": 13}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/358e8ced6dfcfa5c0d4ffbccd2dabbd11a1a93a3b998950ecfef270a5715561a.jpg", "table_caption": [], "table_footnote": ["Table 6: Model details. "], "page_idx": 14}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/2f35f8653a58f46c7c49aed655faab2fbdd315268c7f7529150fa8a616285957.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/c9f2046cd051faeb49e05a348936274b6566905ac44c2c9305f5ad2e5fec4a43.jpg", "table_caption": ["Table 7: Data details. "], "table_footnote": ["Table 8: Model accuracy with and without label synonyms on ImageNet, and corresponding improvement. "], "page_idx": 14}, {"type": "text", "text": "A.3 Synonymous Text Labels in ImageNet Evaluation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In ImageNet, many classes have multiple valid textual labels. For instance, the class identifier n01496331 corresponds to both \u201celectric ray\u201d and \u201ccrampfish\u201d. To address this, we incorporated label synonyms3 in our evaluation. We treat the classification as correct if any textual label is included in the VLM generation. Including synonyms improves accuracy by only $1\\%{-3\\%}$ (Table 8), still leaving a notable performance gap when compared to CLIP. ", "page_idx": 14}, {"type": "text", "text": "B Supplementary for Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Prompt Variation Detail ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Prompt details. We use three different prompts with varying wordings. Base Prompt: \u201cWhat type of object is in this photo?\u201d Prompt Alternative 1: \u201cIdentify the object in this image.\u201d Prompt Alternative 2: \u201cWhat is the main object depicted in this photograph?\u201d ", "page_idx": 14}, {"type": "text", "text": "Label order details. For the fixed order, we use the default dataset label order for each image. ", "page_idx": 14}, {"type": "image", "img_path": "MwmmBg1VYg/tmp/9d6d39e582fcb58422da538f286365c3bd5873bfb2224198447b6559e1d370c7.jpg", "img_caption": ["Figure 4: Analysis of the label set size. For each image, we randomly sample 100, 20, 5, and 2 candidate classes from the entire set of classes. While the absolute performance gap between VLMs and CLIPs decreases as the number of classes is reduced, the relative performance gap increases. The X-axis represents the number of classes, and the Y-axis represents the relative error rate between LLaVA1.5-7B and CLIP-L. ", "Table 9: Analysis of the label set size. This is the table version of Figure 2. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/b40ae7cec3b4b51b230389db806377e458c4f54788681ff77f0ef87b990b9a3c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 Label Set Size Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Performance details. The table version of Figure 2 is provided in Table 9. By reducing the number of labels for classification, we can narrow the gap between VLM and CLIP, but a gap remains across all label set sizes, even with just two labels (two-way classification). ", "page_idx": 15}, {"type": "text", "text": "Relative performance gaps. We find that while the absolute gap between VLMs and CLIPs narrows with reduced label size, the relative gap increases. For example, in two-way classification on ImageNet, VLMs have a $5.7\\%$ error rate, while CLIP has a $0.2\\%$ error rate, resulting in a $28.5\\mathbf{x}$ gap; for 20 classes, VLMs have an $18.0\\%$ error rate, while CLIP has a $2.5\\%$ error rate, resulting in a $7.2\\mathbf{x}$ gap. This trend is indicated by the error rates between VLMs and CLIP shown in Figure 4. ", "page_idx": 15}, {"type": "text", "text": "Larger gap on fine-grained classification datasets. The absolute gap on Flowers and Cars is larger than ImageNet and Caltech when reducing the label set size. This may be because Flowers and Cars are more fine-grained classification datasets, while ImageNet and Caltech are more coarse-grained. VLMs are weaker in fine-grained classification compared to CLIP, resulting in a larger gap. ", "page_idx": 15}, {"type": "text", "text": "B.3 Inference Strategy Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Experimental details. Probabilistic inference methods require separate computations for each class, which introduces significant computational costs. For example, on ImageNet with 1000 classes, each example takes 1000 times longer to process than with a direct generation pipeline. Therefore, we randomly sampled 1000 examples from each dataset\u2019s validation set for evaluation. ", "page_idx": 15}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/6cd097404406df899b862d3ac4ee1dbc59022776ee36136bda869aadeea08f8e.jpg", "table_caption": ["Table 10: Probing the last token or the average token results in much better performance than probing other token positions. Experiments are done using LLaVA1.5-7B on the Flowers102 dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.4 Information Loss Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "CLIP global vs. local features. The global feature of CLIP is commonly used in traditional classification settings, which has 1 token for each image. However, VLMs utilize local patch features for classification, which has 576 tokens for each image using CLIP-L. One might hypothesize that local features contain less information than global features. In practice, VLMs use the second-to-last layer of CLIP local features rather than the last layer. Since the last layer\u2019s global feature is the weighted average of the second-to-last layer\u2019s features through self-attention computation, we can theoretically conclude that local features have at least the same level of information as CLIP global features. ", "page_idx": 16}, {"type": "text", "text": "Feature extraction details. We extract features from VLM\u2019s last layer by feeding the LLaVA default prompt \u201cUSER: $<\\!576$ Image Tokens> What type of object is in this photo? ASSISTANT:\u201d into the LLaVA, or BLIP default prompt \u201c<Image Tokens> Question: What type of object is in this photo? Answer:\u201d into the BLIP. We use either the last token feature (i.e., the token \u201c:\u201d) or the average feature over all the tokens. ", "page_idx": 16}, {"type": "text", "text": "Linear probing details. We train the linear layer on the training set with a batch size of 512, a learning rate of 1e-3 using the Adam optimizer, and for 500 epochs. After training, we report the best performance on the validation set. ", "page_idx": 16}, {"type": "text", "text": "Probing position. In practice, we find that probing the last token or the average token results in much better performance than probing other token positions, as shown in Table 10. We leave this as an interesting question for future research. ", "page_idx": 16}, {"type": "text", "text": "B.5 Training Objective Detail ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "LLaVA fine-tuning details. We convert each image and class label into the text format using the LLaVA default template \u201cUSER: $<\\!576$ Image Tokens> What type of object is in this photo? ASSISTANT: <Class Name>.\u201d We conduct two settings for fine-tuning LLaVA. In the first setting, we only fine-tune the MLP projector between CLIP and the language model (LM). The projector is trained on the training set with a batch size of 64 and a learning rate of 2e-5 using the AdamW optimizer for 50 epochs (1 epoch for ImageNet), with a warmup ratio of 0.03. In the second setting, we fine-tune both the MLP projector and the LM using LoRA. The projector and LM are trained on the training set with a batch size of 64, a learning rate of 2e-5 for the projector and 2e-4 for the LM, using the AdamW optimizer for 50 epochs (1 epoch for ImageNet), with a warmup ratio of 0.03, a LoRA rank of 128, and a LoRA alpha of 256. For both settings, we report the best performance on the validation set after training. Fine-tuning on ImageNet for 1 epoch requires $130\\,{\\mathrm{L}}40$ GPU hours. ", "page_idx": 16}, {"type": "text", "text": "BLIP fine-tuning details. We convert each image and class label into the text format using the BLIP default template \u201c<Image Tokens> Question: What type of object is in this photo? Answer: <Class Name>.\u201d We train the Q-former projector between CLIP and the LM on the training set with a batch size of 64, a learning rate of 2e-5, and a weight decay of 0.05 using the AdamW optimizer for 100 epochs (10 epochs for ImageNet), with 1000 warmup steps. After training, we report the best performance on the validation set. Fine-tuning on ImageNet for 10 epochs requires 120 L40 GPU hours. ", "page_idx": 16}, {"type": "text", "text": "CLIP fine-tuning details. We initialize a linear layer on top of the CLIP image encoder. The CLIP model is frozen, and only the linear layer is trained. The linear layer is trained on the training set with a batch size of 512, a learning rate of 1e-3, and no weight decay using the Adam optimizer for 300 epochs (40 epochs for ImageNet). After training, we report the best performance on the validation set. ", "page_idx": 16}, {"type": "image", "img_path": "MwmmBg1VYg/tmp/1b9551512bfc860195f480c86c47a15a58da4c3b8157c093244f99b0d8f2c815.jpg", "img_caption": ["Figure 5: Fine-tuning only the projector improves numerical stability. $(T o p)$ Fine-tuning LLMs with LoRA often results in numerical instabilities, manifesting as spikes in loss (purple, green, brown, orange curves). In contrast, fine-tuning only the projector leads to a consistently steady decrease in loss (teal curve). Despite experimenting with various hyperparameters for ImageNet, the instability remained. (Bottom) Occasionally, the spikes normalize with continued training. Here, we present an example using the StanfordCars dataset (pink curve). "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/c143fa5bfdb16958fa1c0f78bb21753691710ca769a78a58355859299ad5126d.jpg", "table_caption": ["Table 11: Correlation between class frequency and model\u2019s accuracy on the class. This supplements Figure 3. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Numerical instability. When fine-tuning both the projector and the LM with LoRA for LLaVA, we experience significant numerical instability. On ImageNet, despite adjusting three different hyperparameters such as the learning rate and batch size, the loss consistently peaks during training and does not recover. On the StanfordCars dataset, numerical instability is also observed, but the loss returns to normal during training. This can be seen in Figure 5. ", "page_idx": 17}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/6c014427aa645cec983f753d2156a77244ff868644f33b05a2873a827d393578.jpg", "table_caption": [], "table_footnote": ["Table 12: Analysis of VLMs from the inference perspective. Compared to Table 2, we added the results for the proprietary GPT4 model. We explore prompt variation such as label order and chainof-thought (CoT) and find it has a limited impact on the performance, which applies to proprietary VLMs such as GPT4. "], "page_idx": 18}, {"type": "image", "img_path": "MwmmBg1VYg/tmp/8efd80cff0a18097b0c71ea9a1f750b65ee2856147203f5cb9a452c4723b80f5.jpg", "img_caption": ["Figure 6: Analysis of the label set size. Compared to Figure 2, we added the line for the proprietary GPT4 model. LLaVA1.5-7B uses CLIP-L as its vision encoder, while GPT4 presumably employs a superior vision encoder. Therefore, comparing GPT4 to CLIP-L is not entirely fair. However, we observe that GPT4 performs worse than CLIP-L on ImageNet even when the number of classes is reduced to 2. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.6 Data Detail ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Data processing details. The training of LLaVA comprises two stages: pre-training and instruction tuning. The pre-training stage utilizes noisy image captioning data, whereas the instruction tuning stage employs meticulously curated multi-turn conversation data. We tokenize all sentences within the data, compute the frequency for each class, and then calculate the accuracy for each class. This results in 1000 (frequency, accuracy) pairs for ImageNet\u2019s 1000 classes. This process is similarly applied to all other datasets. ", "page_idx": 18}, {"type": "text", "text": "Correlation results. We compute the Spearman and Pearson correlations between frequency and accuracy of zero-shot LLaVA1.5-7B. The results are presented in Table 11. For comparison, we conducted the same analysis for CLIP-L and fine-tuned LLaVA1.5-7B. ", "page_idx": 18}, {"type": "text", "text": "Correlation for two stages. Given that LLaVA training involves two stages, we examine whether pre-training or instruction-tuning data has a greater impact on classification performance. We compute the Spearman and Pearson correlations separately for pre-training and instruction-tuning data. Our findings indicate that the correlation is similar for both stages. For instance, on ImageNet, the Spearman and Pearson correlations are 0.73 and 0.34 for pre-training, and 0.76 and 0.33 for instruction tuning. These values are close to the combined data correlations, which are 0.76 and 0.35 for Spearman and Pearson correlations, respectively, as reported in Table 11. ", "page_idx": 18}, {"type": "text", "text": "B.7 Analysis of Proprietary VLMs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the main paper, we analyzed two public VLMs about why they are bad at image classification. For proprietary VLMs, they haven\u2019t released the training details and data usage, so it is impossible to directly address training-related or data-related hypotheses. ", "page_idx": 18}, {"type": "text", "text": "For inference-related hypotheses, we conduct additional experiments with GPT4 (Table 12, Figure 6), and the conclusion is the same as the public VLMs. For example, GPT-4 achieves $60.6\\%$ accuracy on ImageNet without CoT. With CoT, GPT-4 achieves $62.2\\%$ accuracy, demonstrating that CoT has a limited impact on image classification, even for large VLMs. ", "page_idx": 18}, {"type": "text", "text": "For training-related and data-related hypotheses, recent work PaliGemma [6] from DeepMind shows that most VLM tasks benefit significantly from longer pre-training with more data (in its Figure 4 and Appendix K). This aligns with our main conclusion that data is the critical factor in improving VLM performance. ", "page_idx": 19}, {"type": "text", "text": "B.8 Open-world vs. Closed-world Setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "$\\S3$ and $\\S4$ all use \u201copen-world\u201d settings (not providing the label set in the prompt), except for prompt variation analysis and label set size analysis in $\\S3.1$ , using \u201cclosed-world\u201d settings (providing the label set in the prompt). ", "page_idx": 19}, {"type": "text", "text": "Naturally, the \u201cclosed-world\u201d setting narrows the model\u2019s generation space and increases accuracy. When the model can easily predict the class in the label set by modifying the inference space (probabilistic inference in $\\S3.1$ and probing VLM in $\\S3.2)$ or training VLMs with more classification data (\u00a74), the advantage of the \u201cclosed-world\u201d setting doesn\u2019t exist. Thus, for these experiments, we use the \u201copen-world\u201d setting. ", "page_idx": 19}, {"type": "text", "text": "B.9 Number Comparison of Table 1 vs. Table 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 2\u2019s \u201cBase Prompt w/ Label (Random Order)\u201d is equivalent to Table 1\u2019s \u201cClosed-World Setting\u201d. In this setting, we concatenate label candidates in a random order for each question: \u201cWhat\u2019s in the image? Choose one from random_shuffle([cat, dog, pig])\u201d. The accuracy for these two settings is identical between Table 2 and Table 1. ", "page_idx": 19}, {"type": "text", "text": "Table 2\u2019s \u201cBase Prompt w/ Label (Fixed Order)\u201d concatenates label candidates in a fixed order for each question: \u201cWhat\u2019s in the image? Choose one from [cat, dog, pig]\u201d. This setting is used to rule out the possibility that the order of labels might affect model performance. ", "page_idx": 19}, {"type": "text", "text": "B.10 Why Do VLMs Have Information but Cannot Decode? ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In 3, we find that VLMs have the essential information for classification. If the VLMs have the information, then why are they not able to decode it? ", "page_idx": 19}, {"type": "text", "text": "One possible reason is that the VLM\u2019s decoding space is too large and not aligned with the visual features. Specifically, VLM decoding is performed through next-word prediction, which usually involves a vocabulary of over 10,000 words. The output text embedding is not aligned with the visual features due to insufficient data to align these spaces. ", "page_idx": 19}, {"type": "text", "text": "In general, having information in a model does not necessarily mean the model can express that information. This phenomenon is also observed in other research areas. For example, after training ResNets or ViTs with self-supervised learning methods like SimCLR, the model acquires discriminative information for different classes. However, this information can only be decoded by adding and training a linear layer on a new dataset. ", "page_idx": 19}, {"type": "text", "text": "Similarly, we demonstrate that VLMs possess classification information, but it is not readily expressible. Adding classification-related data helps bridge the gap between \u201cinformation possession\u201d and \u201cinformation expression\u201d (refer to Table 3). ", "page_idx": 19}, {"type": "text", "text": "B.11 Why Does Fine-tuning Only on ImageNet Get Worse Performance on ImageWikiQA? ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 5, the \u201cFinetuned on ImageNet\u201d model performance (30.6) is worse than the non-finetuned model performance (38.0). This is because fine-tuning often improves in-distribution performance but does not necessarily enhance out-of-distribution or general capabilities. ", "page_idx": 19}, {"type": "text", "text": "Table 5 presents the results of models fine-tuned on ImageNet and evaluated on ImageWikiQA, which are vastly different datasets. ImageNet questions only require classification, such as \u201cQ: <image> What is in the image? A: dog,\u201d while ImageWikiQA questions demand both classification and knowledge/reasoning, such as \u201cQ: <image> What is the native region of this object? A: South America.\u201d ", "page_idx": 19}, {"type": "text", "text": "Fine-tuning solely on ImageNet trains the model to classify, but it can lead to a loss of general capabilities like reasoning and knowledge, resulting in lower performance on ImageWikiQA (30.6 fine-tuned vs. 38.0 pre-trained). This phenomenon is known as \u201ccatastrophic forgetting\u201d [19]. ", "page_idx": 19}, {"type": "image", "img_path": "MwmmBg1VYg/tmp/9612f99bf399dbcede904dd6dd6febceca59d8744dbf2aefe54b0e42efa4b4a6.jpg", "img_caption": ["Figure 7: Analysis of linear probed VLM from the data perspective. Compared to Figure 3, we added the line for the linear probed LLaVA model, which has zero correlation. The zero correlation is expected because probing on the ImageNet dataset equalizes class frequencies, as ImageNet classes are evenly distributed. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "However, when we fine-tune on both ImageNet and instruction tuning data, the model learns classification while retaining its original capabilities, leading to a significant performance improvement on ImageWikiQA (49.8 fine-tuned vs. 38.0 pre-trained). ", "page_idx": 20}, {"type": "text", "text": "B.12 Analysis of Linear Probed VLM from Data Perspective ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Figure 7 plots results for the linear probed LLaVA model, which aligns closely with the fine-tuned LLaVA. This alignment shows a zero correlation with class frequency since fine-tuning or probing on the balanced ImageNet dataset equalizes class frequencies, where each class is evenly represented. ", "page_idx": 20}, {"type": "text", "text": "C Supplementary for Section 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 ImageWikiQA Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Data collection details. ImageWikiQA only contains a test set, providing a valuable benchmark for assessing both fine-grained classification and knowledge-based reasoning in VLMs. ", "page_idx": 20}, {"type": "text", "text": "The ImageWikiQA dataset was constructed in two stages. In the first stage, we generated questions at the class level for each of the 1,000 ImageNet classes. For instance, questions like \u201cWhat is the native region of <class name>?\u201d were created to capture high-level class information. In the second stage, we refined these questions at the image level. For each ImageNet class, we randomly sampled 3 images from the 50 available images from the ImageNet validation set and formulated specific questions like \u201cWhat is the native region of <image i>?\u201d. Each question in ImageWikiQA is paired with a single image, ensuring only one image per question. ", "page_idx": 20}, {"type": "text", "text": "The ImageWikiQA dataset contains 2,000 unique questions. A question like \u201cWhat is the native region of <image i>?\u201d is counted multiple times if it is asked for different images or classes. For example, if the question is asked for 2 cat images and 2 dog images, it would count as 4 questions in total. To avoid over-representing any particular question, a question like \u201cWhat is the native region of <image i>?\u201d is limited to a maximum of 3 occurrences across the 2,000 questions. ", "page_idx": 20}, {"type": "text", "text": "Data generation prompt. We provide the prompt used to generate ImageWikiQA in Figure 8 and the prompt used to filter ImageWikiQA in Figure 9. ", "page_idx": 20}, {"type": "text", "text": "More examples. We provide more examples of the ImageWikiQA dataset in Table 14. ", "page_idx": 20}, {"type": "text", "text": "C.2 Results Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "LLaVA fine-tuning details. For ImageNet, we convert each image and class label into the text format using the LLaVA default template \u201cUSER: <576 Image Tokens> What type of object is in this photo? ASSISTANT: <Class Name>.\u201d Then, we concatenate the original LLaVA 665K instruction tuning dataset with the 1.28M ImageNet classification dataset. Due to numerical stability concerns, we train only the projector layer of LLaVA. The projector is trained on the combined dataset with a batch size of 64, a learning rate of 2e-5, using the AdamW optimizer, for 1 epoch, with a warmup ratio of 0.03. The training process takes approximately 340 hours on a single L40 GPU. ", "page_idx": 20}, {"type": "text", "text": "The questions should come from the following Wikipedia articles on $\\hookrightarrow$ {classname }:   \n{wikipedia page}   \n\u2018\u2018\u2018   \n\\*\\* Instruction \\*\\*:   \nEach question should have four choices , one of which is the correct $\\hookrightarrow$ answer.   \nNote that the Wikipedia articles will not be accessible to the $\\hookrightarrow$ test -takers , so please do not reference specific details from $\\hookrightarrow$ the articles in the questions.   \nUse \"this object\" rather than \"{ classname }\" in the questions. We want $\\hookrightarrow$ to give test -takers an image of this object , not the word $\\hookrightarrow$ itself.   \nEach question should have four fields: \"question\" (str), \"choices\" $\\hookrightarrow$ (list[str]), \"answer\" (int , starting from 0)), and \"reference\" $\\hookrightarrow$ (str , original sentences from Wikipedia articles).   \nOutput in JSON format. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Figure 8: Prompt used to generate ImageWikiQA using GPT4. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Answer the multiple -choice question below. {question with ground -truth class name} A. {choice A}   \nB. {choice B}   \nC. {choice C}   \nD. {choice D}   \nOutput only one character A/B/C/D.   \nAnswer the multiple -choice question below.   \n{question without ground -truth class name}   \nA. {choice A}   \nB. {choice B}   \nC. {choice C}   \nD. {choice D}   \nThe question mentions \"this object \". However , we don \u2019t know what the $\\hookrightarrow$ object is. Try your best to guess the answer without knowing $\\hookrightarrow$ the object.   \nOutput only one character A/B/C/D. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Figure 9: Prompt used to filter ImageWikiQA using GPT4. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.3 Performance of Fine-tuned VLM on Other Datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To understand whether incorporating the ImageNet 1.28M classification data into the original LLaVA instruction-tuning data will harm the model\u2019s general capability, we further evaluated the performance of our fine-tuned visually-grounded language model (VLM) on additional benchmarks to ensure its robustness across a variety of tasks. In particular, we tested the model on TextVQA [42], POPE [29], and MMVet [51]. The results, presented in Table 13, show that the fine-tuned LLaVA1.5-7B maintains almost identical performance compared to the original version. ", "page_idx": 21}, {"type": "text", "text": "This result aligns with expectations since our fine-tuning process incorporated all of the original LLaVA training data during instruction tuning. For instance, on TextVQA, the fine-tuned model achieved an accuracy of $58.0\\%$ , closely matching the original model\u2019s $58.2\\%$ . Additionally, the finetuned model slightly outperformed the original on POPE Popular and POPE Adverse benchmarks, showing small improvements of $0.2\\%$ and $0.3\\%$ , respectively. Performance on MMVet remained unchanged at $31.1\\%$ . These findings demonstrate that the fine-tuning process preserves the model\u2019s strengths across these varied benchmarks. ", "page_idx": 21}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/baafb8186b8f60b4938bc4bba6b498e50ad8f1fb8a37b955786aea24b9e7ff85.jpg", "table_caption": ["Table 13: Performance of LLaVA1.5-7B before and after fine-tuning on TextVQA, POPE, and MMVet datasets. Fine-tuning resulted in consistent performance across all benchmarks. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "C.4 Low Accuracy on ImageWikiQA with Ground-truth Class Names ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 5 presents the accuracy on ImageWikiQA, which includes questions derived from Wikipedia requiring extensive world knowledge (e.g., \u201cWhat is the native region of the guinea pig?\u201d). ", "page_idx": 22}, {"type": "text", "text": "The modest accuracy of $55.9\\%$ by LLaVA1.5-7B with provided ground-truth class names reflects its limited world knowledge for ImageNet classes. This limitation aligns with findings that smaller models, like Vicuna-7B, lack world knowledge compared to larger models like GPT-4. ", "page_idx": 22}, {"type": "text", "text": "C.5 Other Fine-Tuning and Inference Strategies ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Other fine-tuning or inference modifications, such as applying a linear probing loss in the VLM output space during training or using $\\mathbf{k}$ -nearest neighbors (KNN) in the VLM output space for evaluation, can also enhance VLM classification performance. ", "page_idx": 22}, {"type": "text", "text": "However, our primary goal is to improve VLMs\u2019 general capabilities across a range of tasks, not just classification. The universal inference interface for various tasks is text generation, whereas approaches like KNN-based inference are tailored solely to classification and do not generalize to other task types. For fine-tuning, we experimented with adding a new token and applying a linear probing loss. This approach did not yield improvements on ImageWikiQA, indicating that task-specific fine-tuning does not enhance the overall capabilities of VLMs. ", "page_idx": 22}, {"type": "text", "text": "In summary, without changing VLM training or inference to maintain its general capabilities for different tasks, adding data is the most promising and effective approach. ", "page_idx": 22}, {"type": "text", "text": "D Summary of Contributions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Object recognition is fundamental to the general capability of visually-grounded language models (VLMs), yet current VLMs perform poorly in this area. We are the first to thoroughly investigate this critical issue and propose a potential solution to improve it. ", "page_idx": 22}, {"type": "text", "text": "Our primary contribution lies in identifying the problem (i.e., VLMs are inadequate image classifiers, a significant weakness that has been overlooked) and conducting an in-depth investigation (i.e., understanding why VLMs are poor image classifiers and how to address this issue). ", "page_idx": 22}, {"type": "text", "text": "In summary, our contributions are threefold: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Thorough Evaluation to Identify the Problem: We thoroughly evaluated current public and proprietary VLMs on four common classification benchmarks and discovered that their performance significantly lags behind CLIP. This finding is counterintuitive because VLMs often use CLIP as their vision encoders. This finding reveals a weakness in VLMs that previous works have not widely noted [2, 49]. Understanding the limitations of VLMs is crucial given their increasing deployment in various scenarios. \u2022 Hypothesis Testing to Understand the Problem: Given the poor performance of VLMs in classification, we investigated the underlying reasons. We considered multiple plausible hypotheses related to VLM inference, training, and data perspectives. For example, essential information for classification could be lost during the vision encoder\u2019s propagation through multiple LLM layers; VLMs might be inherently poor at classification due to their text generation training objective compared to the standard cross-entropy objective. Our thorough investigation ruled out these reasons but revealed the major issue: the lack of alignment data. This finding is counterintuitive because other alternative hypotheses also seemed plausible. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "\u2022 Improving VLMs based on Our Understanding: Based on our findings, we explored ways to enhance VLM performance. We believe that classification is foundational to more advanced capabilities. For example, if a VLM cannot accurately classify mushroom species, it will also struggle with follow-up questions, such as whether a particular mushroom is poisonous. Indeed, we found that simply adding classification data not only improves VLMs\u2019 classification performance but also their general capabilities, demonstrating that accurately identifying objects is a prerequisite for answering complex questions about these objects. ", "page_idx": 23}, {"type": "text", "text": "Impact: Recent work from Google DeepMind, PaliGemma [6], supports our main conclusion that data is the critical factor in improving VLM performance. They also found that most VLM tasks benefti significantly from longer pre-training with more data (Appendix K). This demonstrates that our analysis can inspire researchers to build better VLMs in the future. ", "page_idx": 23}, {"type": "table", "img_path": "MwmmBg1VYg/tmp/d677835a70082a8a59524cc10cd0619cd26bf620d1a8ef2bbb46e93272584759.jpg", "table_caption": [], "table_footnote": ["Table 14: Examples of ImageWikiQA. Each example has an image, question, four choices (correct choice highlighted in orange), and reference Wikipedia sentences. "], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Claims supported in $\\S2$ and $\\S3$ . ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Discussed in $\\S6$ . ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We included all the details in each section and released all the codes and data in https://github.com/yuhui-zh15/VLMClassifier. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We included all the details in each section and released all the codes and data in https://github.com/yuhui-zh15/VLMClassifier. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We included all the details in each section and released all the codes and data in https://github.com/yuhui-zh15/VLMClassifier. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: The results are statistically significant, so no error bar is reported. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Discussed in $\\S6$ . ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: We have carefully reviewed the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Discussed in $\\S6$ . ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All the assets (data and models) are properly cited in the paper. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We included all the details in $\\S4$ and released all the codes and data in https://github.com/yuhui-zh15/VLMClassifier. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}]