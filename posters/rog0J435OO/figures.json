[{"figure_path": "rog0J435OO/figures/figures_2_1.jpg", "caption": "Figure 1: Common patterns of attention masks. (a) Padded masks from single-sequence inputs in unidirectional (uni-) attention. (b) InToken masks from grouping several masks with different lengths in uni-attention. (c) InToken masks in bidirectional (bidi-) attention. (d) Question and Answering Masks in uni-attention.", "description": "This figure illustrates four common patterns of attention masks used in Transformer models.  (a) shows the use of padding to handle variable-length sequences in unidirectional attention. (b) and (c) demonstrate the InToken method, which dynamically allocates tokens to optimize processing efficiency for varying sequence lengths, in both unidirectional and bidirectional settings. (d) presents a scenario with question-answer pairs, where attention is selectively applied to mask unrelated information.", "section": "2 Background"}, {"figure_path": "rog0J435OO/figures/figures_3_1.jpg", "caption": "Figure 2: Extended patterns of attention masks. (a) In-context learning formatted multi-shot masks in uni-attention. (b) Sink + Slidewindow masks in uni-attention. (c) Global masks in bidi-attention. (d) Customized masks in uni-attention.", "description": "This figure shows four different types of attention masks used in transformer models.  (a) illustrates multi-shot in-context learning, where each shot has visibility to the question, but different shots don't see each other. (b) shows a combination of sink tokens and a sliding window mask, providing a balance between local and global context. (c) depicts a bidirectional global attention mask, allowing all tokens to attend to all other tokens. Finally, (d) presents a customized mask, highlighting the flexibility of attention masking to different tasks and model requirements.", "section": "2.2 Masking Variable-Length Sequences"}, {"figure_path": "rog0J435OO/figures/figures_6_1.jpg", "caption": "Figure 3: Comparison of Kernel Latency Based on Varying Sequence Lengths. FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM).", "description": "This figure shows the kernel latency comparison among different methods (Vanilla Attention, FA-DenseMask, FA-Varlen, and FlashMask) across various sequence lengths (2K, 4K, 8K, 16K, 32K, 64K, and 128K) and downstream tasks (SFT, DPO, and RM). The results demonstrate significant computational speedups achieved by FlashMask, particularly compared to Vanilla Attention and FA-DenseMask.  FA-Varlen shows competitive performance, but FlashMask consistently exhibits the lowest latency across all scenarios. The speedup factors are substantial, reaching up to 6.7x, 6.9x, and 8.3x for SFT, DPO, and RM, respectively.", "section": "4.3 Kernel Experiments"}, {"figure_path": "rog0J435OO/figures/figures_6_2.jpg", "caption": "Figure 4: Top: Comparison of Kernel Latency while Varying Window Size. Bottom: Comparison of Kernel Latency while Varying Input Sparsity.", "description": "This figure shows two subfigures. The top subfigure presents a comparison of kernel latency across various window sizes for SFT, DPO, and RM scenarios. The bottom subfigure illustrates a comparison of kernel latency against varying input sparsity for the same scenarios. These results highlight the performance of FlashMask across different window sizes and input sparsity levels within various attention mechanisms.", "section": "4.3 Kernel Experiments"}, {"figure_path": "rog0J435OO/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of Kernel Latency Based on Varying Sequence Lengths. FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM).", "description": "This figure presents a comparison of kernel latency (the time taken for a single kernel operation) among four different methods: Vanilla Attention, FlashAttention-DenseMask, FlashAttention-Varlen, and the proposed FlashMask. The comparison is done across three different downstream training modalities: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM).  The x-axis represents the sequence length in kilobytes (K), and the y-axis represents the kernel latency in milliseconds (ms).  The results show that FlashMask consistently outperforms the other methods, achieving speedups of up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM). This demonstrates the efficiency gains provided by FlashMask's sparse mask representation.", "section": "4 Experiments"}, {"figure_path": "rog0J435OO/figures/figures_13_1.jpg", "caption": "Figure 6: Sparsity Distribution of Synthetic Dataset.", "description": "This figure presents the distribution of sparsity in the synthetic dataset used for evaluating FlashMask's performance at various sequence lengths and training scenarios (SFT, DPO, and RM).  Each sub-figure shows a histogram representing the frequency of different sparsity levels within the attention masks at specific sequence lengths (2K, 4K, 8K, 16K, 32K, 64K, and 128K). This visualization helps understand the distribution of sparsity across different sequence lengths and tasks, which is crucial for assessing the effectiveness of FlashMask in handling different attention patterns.", "section": "4.2 Data Construction"}, {"figure_path": "rog0J435OO/figures/figures_13_2.jpg", "caption": "Figure 7: Sparsity Distribution of LongBench Dataset.", "description": "This figure shows the distribution of sparsity in the LongBench dataset for different sequence lengths and training scenarios (SFT, DPO, RM). The x-axis represents the sparsity percentage, and the y-axis represents the count of samples falling into each sparsity bin. The distributions show how the sparsity of the attention masks varies across different sequence lengths and training scenarios in this dataset.", "section": "4.2 Data Construction"}, {"figure_path": "rog0J435OO/figures/figures_14_1.jpg", "caption": "Figure 3: Comparison of Kernel Latency Based on Varying Sequence Lengths. FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM).", "description": "This figure compares the kernel latency of FlashMask against Vanilla Attention and FlashAttention-DenseMask across different sequence lengths and training modalities (SFT, DPO, RM).  The results demonstrate that FlashMask achieves significant speedups (up to 6.7x for SFT, 6.9x for DPO, and 8.3x for RM) compared to the other methods, especially at longer sequence lengths.", "section": "4 Experiments"}, {"figure_path": "rog0J435OO/figures/figures_14_2.jpg", "caption": "Figure 3: Comparison of Kernel Latency Based on Varying Sequence Lengths. FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM).", "description": "This figure showcases the kernel latency comparison across different sequence lengths (2K to 128K) for three different training methods (Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM)).  It highlights the significant speed improvements achieved by FlashMask compared to Vanilla Attention and FA-DenseMask, demonstrating its efficiency in handling various sequence lengths and training paradigms.", "section": "4 Experiments"}, {"figure_path": "rog0J435OO/figures/figures_14_3.jpg", "caption": "Figure 10: Comparison of End-to-End Training Throughput on LongBench Dataset.", "description": "This figure compares the end-to-end training throughput (Tokens/Sec/GPU) of different attention mechanisms (FA-DenseMask, FA-Varlen, and FlashMask) on the LLaMA-7B model across various sequence lengths (16K, 32K, and 64K) using the LongBench dataset. FlashMask demonstrates significantly higher throughput compared to FA-DenseMask, showcasing a 4.12x speedup at 64K sequence length.  The performance of FlashMask is comparable to FA-Varlen.", "section": "4.4 End-to-End Experiments"}, {"figure_path": "rog0J435OO/figures/figures_15_1.jpg", "caption": "Figure 11: Comparison of End-to-End Training GPU Memory.", "description": "This figure compares GPU memory usage during end-to-end training for various models (LLaMA-7B, LLaMA-13B, LLaMA-70B) across different downstream tasks (SFT, LoRA, DPO, RM) and sequence lengths.  It demonstrates the memory efficiency of FlashMask compared to Vanilla Attention, FlashAttention with dense mask, and FlashAttention with variable length. FlashMask shows significantly lower memory consumption, particularly beneficial for training with longer sequences.", "section": "4.4 End-to-End Experiments"}]