[{"type": "text", "text": "FlashMask: Reducing the Complexity of Attention Computation through Sparse Mask Representation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Recent advancements in Larger-Scale Transformers have significantly benefited   \n2 from sophisticated attention mechanisms, which are critical for modeling long  \n3 context sequences. However, the computational and memory demands of conven  \n4 tional attention mask computations, typically scaling with an $O(N^{2})$ complexity   \n5 where $N$ is the sequence length, pose significant challenges. This paper intro  \n6 duces FlashMask, a simple yet effective Exact attention algorithm designed to   \n7 substantially reduce both the computational complexity and memory requirements   \n8 of attention computations. By adopting a novel column-wise sparse representation   \n9 of attention masks, FlashMask achieves a linear memory complexity of $O(N)$ and   \n0 computational complexity of $O(N)\\sim O(N^{2})$ . We assess the performance of Flash  \n11 Mask in a variety of masking scenarios, including causal and customized attention   \n12 masks, demonstrating its versatility and robustness across a wide range of attention   \n13 patterns and models. Our empirical analysis encompasses a variety of downstream   \n14 training modalities, including Supervised Fine-Tuning (SFT), Direct Preference   \n15 Optimization (DPO), and Reward Model (RM). We compare FlashMask against   \n16 state-of-the-art techniques, including notably FlashAttention [1]. In kernel-level   \n17 assessments, FlashMask achieves substantial computational speedups, up to $6.7\\mathrm{x}$   \n18 (SFT), $6.9\\mathrm{x}$ (DPO), and $8.3\\mathrm{x}$ (RM). Furthermore, in end-to-end training, FlashMask   \n19 consistently enhances training speed significantly, with accelerations up to $2.4\\mathbf{x}$   \n20 (SFT), $4.2\\mathbf{x}$ (LoRA), $2.5\\mathbf{x}$ (DPO), and $2.6\\mathrm{x}$ (RM) across these varied scenarios   \n21 without sacrificing model accuracy. Additionally, when implemented in the LoRA   \n22 scenario, FlashMask enables the LLaMA2-7B to process sequence lengths of up to   \n23 544k, significantly enhancing its capability for long-context input. ", "page_idx": 0}, {"type": "text", "text": "24 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "25 Transformers [2] , equipped with self-attention mechanisms, have revolutionized natural language   \n26 processing (NLP) by efficiently modeling data dependencies without the limitations of sequential   \n27 processing. This makes them ideal for handling long sequences. Large Language Models (LLMs),   \n28 which utilize training paradigms such as Supervised Fine-Tuning (SFT) [3, 4] and Reinforcement   \n29 Learning from Human Feedback (RLHF) [5, 6], critically rely on selective attention management   \n30 through masks. Effective mask management is essential to focus selectively on pertinent data   \n31 segments, optimizing both performance and computational efficiency.   \n32 However, the conventional attention mechanism in Transformers entails a quadratic increase in   \n33 computational and memory demands $O(N^{2})$ , where $N$ denotes the sequence length. This exponential   \n34 growth presents substantial challenges as models scale to sequence lengths ranging from 128K to   \n35 1M in advanced systems like GPT-4 [7], Claude [8], and Gemini [9], necessitating more efficient   \n36 computational approaches. As sequence lengths extend, the memory load for masked attention   \n37 computations also grows quadratically, adversely affecting computational speed and the ability to   \n38 manage various mask configurations across different tasks. Current methodologies often resort to   \n39 approximate sparse attention strategies [10, 11, 12], which unfortunately trade off precision for   \n40 computational efficiency, underscoring an essential gap in achieving high precision with reduced   \n41 computational costs.   \n42 This paper introduces FlashMask, a novel approach utilizing a sparse mask representation to accelerate   \n43 attention computations in transformers, effectively addressing both computational and memory scala  \n44 bility issues. Unlike previous methods that compromise accuracy for efficiency, FlashMask provides   \n45 precise computations without sacrificing accuracy, ensuring high fidelity in attention mechanisms.   \n46 The contributions of this work include:   \n47 \u2022 Exact Computation. FlashMask uniquely ensures precise attention computations across varying   \n48 sequence lengths and tasks. It employs a unique column-wise sparse mask representation, denoted   \n49 by FlashMaskStart (FMS) and FlashMaskEnd (FME), to precisely mask specific rows within   \n50 columns, ensuring computational efficiency and accuracy.   \n51 \u2022 Long Context Modeling. FlashMask significantly reduces computational and memory demands,   \n52 enabling efficient processing of extended sequences critical for deploying LLMs in resource-limited   \n53 settings.   \n54 \u2022 Efficient Mask Computation. FlashMask leverages strategic sparse masking to increase compu  \n55 tational throughput, thereby improving processing speeds and broadening the practical utility of   \n56 LLMs in diverse real-world scenarios.   \n57 \u2022 Extensive Empirical Validation. Empirical studies validate FlashMask\u2019s efficiency in computation   \n58 and storage. Its practical application in real-world scenarios and integration with existing frame  \n59 works underscore its potential impact. Moreover, a comprehensive comparison with state-of-the-art   \n60 methods like FlashAttention-DenseMask, FlashAttention-Varlen highlights FlashMask\u2019s efficiency   \n61 and versatility. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "62 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "63 The attention mechanism has revolutionized data handling in NLP by mimicking human selective   \n64 focus, allowing neural networks to prioritize parts of the input data. This addresses limitations   \n65 of traditional sequence-to-sequence models, enhancing context awareness in long sequences. The   \n66 Transformer model by Vaswani et al. [2] implements this mechanism centrally, using multiple parallel   \n67 attention heads instead of recurrent layers, thus improving efficiency and performance. ", "page_idx": 1}, {"type": "text", "text": "68 2.1 Attention Computation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 Central to the Transformer architecture is the attention mechanism, which computes relevance   \n70 scores between elements in a sequence to focus more on important aspects and less on others. This   \n71 mechanism can be expressed as: ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathrm{Attention}}_{m a s k}(Q,K,V)={\\mathrm{softmax}}\\left({\\frac{Q K^{T}}{{\\sqrt{d_{k}}}}}+M\\right)V,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "72 where $Q,\\,K,\\,V$ , and $M$ represent the query, key, value, and mask matrices respectively, derived   \n73 from the input data, and $d_{k}$ is the dimension of keys. The term $M$ incorporates constraints to   \n74 selectively consider certain parts of the input sequence during attention computation, enabling   \n75 functionality like masking future tokens in sequence-to-sequence modeling. One inherent challenge   \n76 with attention is its computational and memory complexity, both of which scale quadratically with   \n77 the length of the input sequence. Processing long sequences presents significant challenges, which   \n78 are exacerbated in the downstream pipeline of training large language models (LLMs). Different   \n79 training stages, such as Supervised Fine-Tuning (SFT/LoRA [3, 4, 13, 14, 15]), Direct Preference   \n80 Optimization (DPO) [16, 17, 18, 19, 20], Reward Model (RM) [5, 21, 22, 23, 24], and Proximal   \n81 Policy Optimization (PPO) [25, 6], place diverse demands on the attention mask. ", "page_idx": 1}, {"type": "text", "text": "82 2.2 Masking Variable-Length Sequences ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "83 The advent of large transformer-based models has marked substantial progression in handling   \n84 increased sequence lengths in natural language processing. Previously, models like BERT [26] and   \n85 GPT-2 [27] were limited to sequences of approximately 512 tokens, whereas more recent adaptations   \n86 such as the LLaMA [28, 29, 30], GPT-4 [7] and Claude series [8] stretched these limits to encompass   \n87 2K to 200K tokens, respectively. Innovations from Google\u2019s Gemini [9] have further shifted this   \n88 boundary, managing up to 1M tokens. Enhanced sequence management within these models employs   \n89 various masking techniques in the attention matrix, adapting to the length and diversity of input   \n90 sequences. Techniques such as the use of padding operations are illustrated in Figure 1(a), which help   \n91 maintain efficiency by allowing uniform processing of diverse input lengths through padding masks.   \n92 However, conventional padding can lead to inefficiencies due to the diverse sequence lengths typically   \n93 found in training data, often following a long-tail distribution. This issue is adeptly addressed by   \n94 dynamic token allocation technologies like InToken [31, 3, 32, 33, 34], which optimize computational   \n95 resources by adjusting the token count based on actual data needs, significantly improving the training   \n96 efficiency for datasets with various sequence lengths in Figure 1(b)(c).   \n97 Despite having extensive text-handling capabilities, the meticulous design of masking configurations   \n98 remains crucial for specific training scenarios. The illustrated scenarios in Figure 1(d) and Figure 2   \n99 depict various specialized masking mechanisms employed to enhance model training efficiency and   \n100 applicability. Figure 1(d) illustrates a scenario involving DPO/RM with two or more answers, where   \n101 each answer\u2019s tokens have visibility to the tokens of the question, and tokens from different answers   \n102 are not visible to each other. Multi-shot and in-context learning scenarios facilitated by extended   \n103 attention spans in configurations like Figure 2(a) are becoming prevalent, which allows the final   \n104 question in a series to receive comprehensive attention, enhancing contextual understanding [35,   \n105 36]. Furthermore, hybrid masking forms combining features from different methodologies are   \n106 demonstrated in Figure 2(b). These incorporate sink tokens [37] and a sliding window mask from the   \n107 Big Bird [38], facilitating a localized yet extensive context capture. Figure 2(c) is also derived from   \n108 Big Bird, showing a bi-directional global attention mask, which allows for a comprehensive global   \n109 context capture. Such innovative approaches in masking not only bolster the efficiency of training   \n110 large transformer models but also pave the way for advanced explorations into the capabilities of   \n111 attention mechanisms, such as simulating token eviction during inference as depicted in Figure 2(d).   \n112 These advancements underscore the dynamic and adaptable nature of transformer technology in   \n113 accommodating varying training needs and enhancing the overall performance of LLMs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "rog0J435OO/tmp/d95b66eb0a8f6448c9045c5be986058b2c28377bb2030d5959292177958a9dcb.jpg", "img_caption": ["Figure 1: Common patterns of attention masks. (a) Padded masks from single-sequence inputs in unidirectional (uni-) attention. (b) InToken masks from grouping several masks with different lengths in uni-attention. (c) InToken masks in bidirectional (bidi-) attention. (d) Question and Answering Masks in uni-attention. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "114 2.3 Attention Optimization Techniques ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "115 As aforementioned in Equation 1, the computational and memory demands of this mechanism,   \n116 particularly the computation of $Q K^{T}$ , become significant as the sequence length $N$ increases. This   \n117 is due to the size of the resultant attention scores matrix, which scales quadratically with the   \n118 sequence length, leading to a complexity of $O(N^{2})$ . Several related works has been proposed to   \n119 alleviate the issue. In the realm of model training optimizations, Memory Efficient Attention [39]   \n120 (MEA) and FlashAttention [1] have been pivotal. MEA focuses on reducing the model\u2019s memory   \n121 demands by altering the self-attention mechanisms. This allows either for the use of larger models   \n122 or for the extension of maximum sequence lengths within existing hardware constraints. On the   \n123 other hand, FlashAttention enhances the efficiency of attention mechanisms with IO-Awareness to   \n124 better utilize contemporary GPU architectures, resulting in faster computations and reduced energy   \n125 consumption. This method reduces memory overhead to $O(N)$ utilizing tiling techniques during   \n126 the computation process, making it particularly effective in scenarios without the need for a custom   \n127 mask. However, for specific training contexts requiring custom masking, the memory overhead   \n128 with FlashAttention remains $O(N^{2})$ . Note that, in typical training setups like unidirectional causal   \n129 attention or bidirectional full-context attention, the default mode of operation with FlashAttention   \n130 does not involve passing a custom mask.   \n131 During the inference stage, optimizations such as FlashDecoding [40] and FlashDecoding $^{++}$ [41]   \n132 play crucial roles. FlashDecoding enhances the decoder in transformers to expedite the generation of   \n133 sequences by optimizing state management and employing techniques that minimize computational   \n134 waste. FlashDecoding $^{++}$ further advances these improvements, incorporating sophisticated dynamic   \n135 batching and more refined state management to significantly boost throughput and reduce latency.   \n136 Concerning long sequence training, RingAttention [42] is notable for its efficiency in distributed   \n137 training contexts, managing communication overhead and memory utilization effectively across   \n138 multiple nodes.   \n139 Another class of study targets on the sparsity/low-rank of attention computation. The Sparse Trans  \n140 former [10] revolutionizes sequence processing with log-linear complexity. Similarly, Reformer [43]   \n141 optimizes memory via locality-sensitive hashing, while Big Bird [38] introduces a hybrid attention   \n142 method to manage longer sequences efficiently. Linformer [44] reduces complexity using low-rank   \n143 approximations, significantly economizing computation and storage requirements. Both of the pre  \n144 viously discussed solutions either compromise precision or yield only marginal enhancements in   \n145 efficiency. Conversely, our proposed FlashMask is capable of delivering an exact computations. ", "page_idx": 2}, {"type": "image", "img_path": "rog0J435OO/tmp/959d551a817b0f7a5f2a76a4c5a142bc38273f3d4db38b849bb36475aafab70c.jpg", "img_caption": ["Figure 2: Extended patterns of attention masks. (a) In-context learning formatted multi-shot masks in uni-attention. (b) Sink $^+$ Slidewindow masks in uni-attention. (c) Global masks in bidi-attention. (d) Customized masks in uni-attention. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "146 3 FlashMask: Algorithm and Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "147 In this section, we present the critical design of the column-wise sparse mask representation, imple  \n148 mentation of the mask computation kernel, and a complexity analysis of the proposed FlashMask. ", "page_idx": 3}, {"type": "text", "text": "149 3.1 Column-wise Sparse Mask Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "150 We introduce FlashMask, a column-wise sparse masking technique, represented using FMS, $\\mathbf{FME\\in}$   \n151 $\\mathbb{R}^{N}$ (the row index of Flash Mask Start and Flash Mask End), where $\\mathbf{FMS}_{c}$ , $\\mathbf{FME}_{c}$ denote that   \n152 elements in the $c$ -th column of the attention score matrix $\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^{T}$ within the interval $[\\mathbf{FMS}_{c}$ , $\\mathbf{FME}_{c}$ )   \n153 are masked (set to $-\\infty,$ ). As shown in Fig. 2(a), FMS = [4, 4, 4, 4, 10, 10, 10, 10, 10, 10], $\\mathbf{FME=}$   \n154 [7, 7, 7, 7, 10, 10, 10, 10, 10, 10] indicates that, for the first column, the 4-th to 6-th rows are masked. ", "page_idx": 3}, {"type": "text", "text": "155 3.2 Integration with FlashAttention ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "156 Unidirectional (causal) attention, commonly utilized in large language models, incorporates Flash  \n157 Mask within the FlashAttention-2 algorithm, as detailed in Algorithm 1. This paper elaborates the   \n158 implementation of FlashMask using the lower triangular section of the mask for illustration, where   \n159 the blue section represents the computation by the dense mask method (for comparison and not   \nRequire: Matrices $\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}$ in HBM, block sizes $B_{c},B_{r}$ , dense mask $\\mathbf{D}\\in\\mathbb{R}^{N\\times N}$ , column-wise sparse   \nmask starting rows $\\mathbf{FMS}\\in\\mathbb{R}^{N}$ , ending rows $\\mathbf{FME}\\in\\mathbb{R}^{N}$ .   \n1: Divide Q into \ud835\udc47\ud835\udc5f= \ud835\udc35\ud835\udc41\ud835\udc5f blocks $\\mathbf{Q}_{1},\\hdots,\\mathbf{Q}_{T_{r}}$ of size $B_{r}\\times d$ each, and divide $\\mathbf{K},\\mathbf{V}$ in to $T_{c}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil$ blocks   \n$\\mathbf{K}_{1},\\ldots,\\mathbf{K}_{T_{c}}$ and $\\mathbf{V}_{1},...,\\mathbf{V}_{T_{c}}$ , of size $B_{c}\\times d$ each.   \n2: Divide the output $\\mathbf{O}\\in\\mathbb{R}^{N\\times d}$ into $T_{r}$ blocks $\\mathbf{0}_{i},\\ldots,\\mathbf{0}_{T_{r}}$ of size $B_{r}\\times d$ each, and divide the logsumexp $L$   \ninto $T_{r}$ blocks $L_{i},...,L_{T_{r}}$ of size $B_{r}$ each.   \n3: Divide D into \ud835\udc47\ud835\udc5f\u00d7 \ud835\udc47\ud835\udc50blocks D1,1, ..., D\ud835\udc47\ud835\udc5f,\ud835\udc47\ud835\udc50.   \n4: Divide FMS into $T_{c}$ blocks $\\mathbf{FMS}_{1}$ , ..., $\\mathbf{FMS}_{T_{c}}$ , and divide FME into $\\mathbf{FME}_{1},...,\\mathbf{FME}_{T_{c}}$ .   \n5: Precompute the max value maxFMS1, ..., maxFMS\ud835\udc47\ud835\udc50for each $\\mathbf{FMS}_{1},...,\\mathbf{FMS}_{T_{c}}$ , write to HBM.   \n6: Precompute the max value maxFME1, ..., maxFME\ud835\udc47\ud835\udc50for each $\\mathbf{FME}_{1},...,\\mathbf{FME}_{T_{c}}$ , write to HBM.   \n7: Precompute the min value min $\\mathbf{\\|}\\mathbf{FMS}_{1}$ , .. $..,\\mathbf{minFMS}_{T_{c}}$ for each $\\mathbf{FMS}_{1},...,\\mathbf{FMS}_{T_{c}}$ , write to HBM.   \n8: Precompute the min value min $\\mathbf{FME}_{1}$ , ..., minFME\ud835\udc47\ud835\udc50for each $\\mathbf{FME}_{1},...,\\mathbf{FME}_{T_{c}}$ , write to HBM.   \n9: for $1\\leq i\\leq T_{r}$ do   \n10: Load $\\mathbf{Q}_{i}$ from HBM to on-chip SRAM.   \n11: On chip, initialize $\\mathbf{O}_{i}^{(0)}=(0)_{B_{r}\\times d}\\in\\mathbb{R}^{B_{r}\\times d},\\ell_{i}^{(0)}=(0)_{B_{r}}\\in\\mathbb{R}^{B_{r}},m_{i}^{(0)}=(-\\infty)_{B_{r}}\\in\\mathbb{R}^{B_{r}}.$   \n12: for $1\\leq j\\leq T_{c}$ do   \n13: if $i\\times B_{r}\\geq\\mathbf{maxFMS}_{j}$ and $(i+1)\\times B_{r}\\leq\\mathbf{minFME}_{j}$ then   \n14: Continue   \n15: end if   \n16: Load $\\mathbf{K}_{j},\\mathbf{V}_{j}$ from HBM to on-chip SRAM.   \n17: Load $\\mathbf{FMS}_{j}$ from HBM to on-chip SRAM.   \n18: Load $\\mathbf{FME}_{j}$ from HBM to on-chip SRAM.   \n19: On chip, compute $\\mathbf{S}_{i}^{(j)}=\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\in\\mathbb{R}^{B_{r}\\times B_{c}}$ .   \n20: On chip, set $\\mathbf{S}_{i}^{(j)}=\\mathbf{S}_{i}^{(j)}+\\mathbf{D}_{i,j}$   \n21: if $\\left(i+1\\right)\\times{B_{r}}^{\\prime}\\geq\\mathbf{minFMS}_{j}$ and $i\\times B_{r}\\leq\\mathbf{maxFME}_{j}$ then   \n22: On chip, set $\\mathbf{S}_{i}^{(j)}\\left[x\\right]\\left[y\\right]=-\\infty,\\forall x,y$ , such that $\\mathbf{{FMS}}_{j}\\left[\\boldsymbol{y}\\right]\\leq i\\times\\boldsymbol{B}_{r}+\\boldsymbol{x}\\leq\\mathbf{{FME}}_{j}\\left[\\boldsymbol{y}\\right]$   \n23: end if   \n24: On chip, compute $m_{i}^{(j)}\\ =\\ \\operatorname*{max}(m_{i}^{(j-1)}$ , $\\mathrm{rowmax}(\\mathbf{S}_{i}^{(j)}))\\ \\in\\ \\mathbb{R}^{B_{r}}$ , \u02dcP\ud835\udc56( \ud835\udc57) = exp(S\ud835\udc56( \ud835\udc57) \u2212\ud835\udc5a\ud835\udc56( \ud835\udc57)) \u2208   \n$\\mathbb{R}^{B_{r}\\times B_{c}}$ (pointwise), $\\ell_{i}^{(j)}=e^{m_{i}^{j-1}-m_{i}^{(j)}}\\ell_{i}^{(j-1)}+\\mathrm{rowsum}(\\tilde{\\mathbf{P}}_{i}^{(j)})\\in\\mathbb{R}^{B_{r}}$ .   \n25: On chip, compute $\\mathbf{O}_{i}^{(j)}=\\mathrm{diag}(e^{m_{i}^{(j-1)}-m_{i}^{(j)}})^{-1}\\mathbf{O}_{i}^{(j-1)}+\\tilde{\\mathbf{P}}_{i}^{(j)}\\mathbf{V}_{j}.$   \n26: end for   \n27: On chip, compute $\\mathbf{O}_{i}=\\mathrm{diag}(\\ell_{i}^{(T_{c})})^{-1}\\mathbf{O}_{i}^{(T_{c})}$ .   \n28: On chip, compute $L_{i}=m_{i}^{(T_{c})}+\\log(\\ell_{i}^{(T_{c})})$ .   \n29: Write $\\mathbf{o}_{i}$ to HBM as the $i$ -th block of $\\mathbf{\\bar{o}}$ .   \n30: Write $L_{i}$ to HBM as the $i$ -th block of $L$ .   \n31: end for   \n32: Return the output $\\mathbf{o}$ and the logsumexp $L$ .   \n160 present in FlashMask) and the red section indicates the FlashMask computation. FlashAttention   \n161 Forward involves two nested loops; the outer loop iterates over each block $\\mathbf{Q}_{i}$ of $\\mathbf{Q}$ , and the inner   \n162 loop iterates over all blocks $\\mathbf{K}_{j}$ of $\\mathbf{K}$ and $\\mathbf{V}_{j}$ of $\\mathbf{V}$ . In the inner loop, $\\mathbf{S}_{i}^{(j)}=\\mathbf{Q}\\mathbf{K}^{T}$ is computed on   \n163 SRAM. Once $\\mathbf{S}_{i}^{(j)}$ is generated, the corresponding dense mask is added as a bias (shown in line 20 of   \n164 Algorithm 1), whereas FlashMask applies the column-wise sparse mask by setting elements beyond   \n165 $\\mathbf{FMS}_{c}$ but not exceeding $\\mathbf{FME}_{c}$ to $-\\infty$ (as shown in lines 21 to 23 of Algorithm 1).   \n166 FlashMask further exploits the block computation feature of FlashAttention-2 to reduce computation.   \n167 If all elements within a block are masked, the block\u2019s computation, including matrix multiplication   \n168 and softmax operations, can be skipped. A block defined by rows $[r_{0},r_{1})$ and columns $[c_{0},c_{1})$ is   \n169 skipped if $r_{0}\\,\\geq\\,\\mathbf{max}(\\mathbf{F}\\mathbf{M}\\mathbf{S}_{c_{0}:c_{1}})$ and $r_{1}\\,\\leq\\,\\mathbf{min}(\\mathbf{FME}_{c_{0}:c_{1}})$ . Considering that mask regions often   \n170 exhibit continuity, most blocks are either completely masked or not at all, with only boundary blocks   \n171 requiring fine-grained masking. A block is completely unmasked if every coordinate $(r,c)$ satisfies   \n172 $r<\\mathbf{FMS}_{c}$ or $r\\geq\\mathbf{F}\\mathbf{M}\\mathbf{E}_{c}$ , thus skipping fine-grained masking and avoiding extra masking overhead.   \n173 To avoid redundant computations in the FlashAttention-2 compute loop, we precompute   \n174 $\\mathbf{max}(\\mathbf{FME}_{c_{0}:c_{1}})$ and $\\operatorname*{min}(\\mathbf{FME}_{c_{0}:c_{1}})$ for each block before the execution loop using a kernel. This   \n175 computation has a complexity of $O(N)$ and can be easily distributed over $\\begin{array}{r}{T_{c}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil}\\end{array}$ thread blocks. A   \n176 parallel reduction operation within each thread block then computes the maximum and minimum   \n177 values, yielding $T_{c}$ values. The additional space complexity introduced here is ${\\cal O}(T_{c})$ . Similar   \n178 computations are made for $\\mathbf{max}(\\mathbf{FMS}_{c_{0}:c_{1}})$ , $\\mathbf{min}(\\mathbf{FMS}_{c_{0}:c_{1}})$ ,.   \n179 The backward computation in FlashAttention-2, which is typically column-parallel, benefits more   \n180 from the column sparse mask approach. Blocks for which $\\begin{array}{r}{\\left\\lfloor\\frac{\\operatorname*{max}(\\mathbf{F}\\mathbf{M}\\bar{\\mathbf{S}}_{c_{0}:c_{1}})}{B_{r}}\\right\\rfloor<i<\\left\\lfloor\\frac{\\operatorname*{min}(\\mathbf{F}\\mathbf{M}\\mathbf{E}_{c_{0}:c_{1}})}{B_{r}}\\right\\rfloor}\\end{array}$ are   \n181 fully masked, allowing skipping of these intervals directly. Only blocks satisfying $\\left\\lfloor\\frac{\\operatorname*{min}(\\mathbf{F}\\mathbf{M}\\mathbf{S}_{c_{0}:c_{1}})}{B_{r}}\\right\\rfloor\\leq$   \n182 $\\begin{array}{r}{i\\leq\\left\\lfloor\\frac{\\mathbf{max}(\\mathbf{FME}_{c_{0}:c_{1}})}{B_{r}}\\right\\rfloor}\\end{array}$ require fine-grained masking.   \n183 It is important to note that unlike various approximate attention algorithms, our method ensures   \n184 that each effective element of the attention score matrix is computed identically to FlashAttention-2,   \n185 with masked elements explicitly set to $-\\infty$ , thus maintaining the accuracy of the algorithm\u2019s results.   \n186 Futhermore, FlashMask is easily extendable to bidirectional attention computations. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "187 3.3 Complexity Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "188 We define sparsity as $\\begin{array}{r}{\\rho=\\frac{p}{N^{2}}}\\end{array}$ , where $p$ is the number of masked elements in the attention score matrix,   \n189 and $N$ is the maximum sequence length of $\\mathrm{\\DeltaQ}$ and K, $N^{2}$ being the total number of elements in the   \n190 attention score matrix. For a causal mask, $\\begin{array}{r}{\\rho=\\frac{2\\times p}{N^{2}}}\\end{array}$ since half of the elements in the attention \ud835\udc4escore   \n191 matrix are already masked by the causal mask. The block sparsity $\\alpha$ is defined as $\\begin{array}{r}{\\alpha=\\frac{a}{\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil\\times\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil}}\\end{array}$   \n192 where $B_{r},B_{c}$ are block sizes, and $a$ is the number of completely masked blocks. For a causal mask,   \n193 $\\begin{array}{r}{\\alpha=\\frac{2\\times a}{\\left\\lceil\\frac{N}{B r}\\right\\rceil\\times\\left\\lceil\\frac{N}{B c}\\right\\rceil}}\\end{array}$ .   \n194 Space complexity. The dense mask is represented as ${\\bf D}\\in\\mathbb{R}^{N\\times N}$ , with a space complexity of $O(N^{2})$ .   \n195 FlashMask denotes as $\\mathbf{\\mathrm{FMS}},\\mathbf{FME}\\,\\in\\,\\mathbb{R}^{N}$ , occupying $O(N)$ space, along with four precomputed   \n196 arrays maxFMS, minFMS, maxFME, minFME \u2208R\ud835\udc35\ud835\udc50 , also occupying $O(N)$ space. Thus, the   \n197 total space complexity for FlashMask is $O(N)$ , significantly reducing memory usage and supporting   \n198 training on longer sequences.   \n199 Memory access complexity. The dense mask accesses the entire ${\\bf D}\\in\\mathbb{R}^{N\\times N}$ matrix in line 20 of   \n200 Algorithm 1, totaling $\\mathbf{\\bar{\\boldsymbol{N}}}^{2}$ memory accesses on HBM. FlashMask reads the FMS, $\\mathbf{FME}\\in\\mathbb{R}^{N}$ vectors   \n201 from HBM as shown in lines 17 and 18 of Algorithm 1, with each $\\mathbf{Q}_{i}$ reading the entire FMS, FME,   \n202 totaling $2\\!\\times\\!T_{r}\\!\\times\\!N$ memory accesses. This reduces the memory access to approximately $\\begin{array}{r}{\\frac{2\\times T_{r}\\times N}{N^{2}}\\approx\\frac{2}{B_{r}}}\\end{array}$   \n203 significantly boosting performance. Due to FlashMask\u2019s smaller space usage, it is possible to preload   \n204 FMS, FME into SRAM using only $2\\times B_{c}$ SRAM, enhancing memory access efficiency. For the   \n205 backward process, which uses a column-parallel approach, SRAM-stored FMS, FME can be well   \n206 reused, further reducing the total memory access on HBM to $2\\times N$ .   \n207 Computational complexity. The attention computation process normally iterates over the entire   \n208 attention score matrix, with a computational complexity of $O(N^{2})$ . By skipping entirely masked   \n209 blocks, FlashMask leverages block sparsity to reduce computational complexity to $O((1-\\alpha)N^{2})$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "210 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "211 4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "212 Experiments were conducted using GPU A800-SXM 80G, Intel(R) Xeon(R) Platinum 8350C CPUs,   \n213 CUDA 12.0, and driver version 525.125.06. We evaluated FlashMask against various methods   \n214 including Vanilla Attention, FlashAttention with dense mask (FA-DenseMask), variable length (FA  \n215 Varlen), and sliding window (FA-Window) across different scenarios and sequence lengths. Both   \n216 kernel-level and end-to-end performance demonstrated the effectiveness of our method. ", "page_idx": 5}, {"type": "text", "text": "217 4.2 Data Construction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "218 As mentioned in the Background section, commercial large models now support sequences up to   \n219 128K in length. FlashMask, with its lower memory overhead, can facilitate training with even longer   \n220 contexts. However, currently available public datasets do not contain training data for scenarios   \n221 exceeding 128K. For comprehensive testing of FlashMask, we constructed synthetic data to simulate   \n222 long-sequence training.   \n223 For a given sequence length $L$ , sequences were generated by mimicking InToken method with several   \n224 sub-sequences. Randomly selecting $s\\in[1,10]$ split points uniformly within the range $(0,L)$ , the   \n225 sequence was divided into $s$ sub-sequences. The segment from the last split point to the end of the   \n226 sequence was considered as Padding. For the RM scenario, shorter sequence lengths used a smaller   \n227 upper limit on the number of splits: $s\\in[1,3]$ for $L\\in(0,4096]$ and $s\\in[1,4]$ for $L\\in(4096,8192]$ .   \n228 By discarding samples not meeting size requirements, we ensure each sub-sequence length was   \n229 at least 128 (SFT, LoRA, DPO) or 512 (RM) and padding not exceeding 128 (SFT, LoRA, DPO)   \n230 or 512 (RM). Suppose one sub-sequence with length $L^{\\prime}$ was further divided into a query and $k$   \n231 answers based on the scenario. The length of each answer was randomly determined from the   \n232 range $\\begin{array}{r}{\\bigl[\\frac{0.1L^{\\prime}}{1+0.1\\times k},\\frac{0.2L^{\\prime}}{1+0.2\\times k}\\bigr]}\\end{array}$ , making the answer lengths approximately [0.1, 0.2] of the query length.   \n233 Therefore, the query length was equal to $L^{\\prime}$ minus the total answer lengths. A total of 240 valid   \n234 samples per given sequence length $L$ were collected and binned into 10 categories by sparsity $\\rho$ , as   \n235 shown in Appendix A.2. ", "page_idx": 5}, {"type": "image", "img_path": "rog0J435OO/tmp/204ff2e946bd54cd489a907aa7e347942041e872b844194cf3c098d72db47a34.jpg", "img_caption": ["Figure 3: Comparison of Kernel Latency Based on Varying Sequence Lengths. FlashMask achieves substantial computational speedups, up to $6.7\\mathrm{x}$ (SFT), $6.9\\mathrm{x}$ (DPO), and $8.3\\mathrm{x}$ (RM). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "rog0J435OO/tmp/d2be947857bbad83fca24b45d2e524ef2e3df674d9d3eea2bf3fdbb357e2bfe9.jpg", "img_caption": ["Figure 4: Top: Comparison of Kernel Latency while Varying Window Size. Bottom: Comparison of Kernel Latency while Varying Input Sparsity. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "236 4.3 Kernel Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "237 We conducted tests with batch sizes of 1, 2, and 4 using Vanilla Attention, FA-DenseMask, and   \n238 FlashMask. Each experiment began with 5 warm-up runs followed by 50 measurements, totaling 55   \n239 runs with kernel latency as the performance metric. Additional comparisons were made with FA  \n240 Varlen in the SFT scenario. Results for batch size 1 are shown in Figure 3 (results for batch sizes 2 and   \n241 4 can be found in Appendix A.3). FlashMask demonstrated significant latency advantages across all   \n242 lengths, up to 8.3-fold time saving compared to FA-DenseMask. Vanilla Attention was significantly   \n243 more time-consuming and exceeded memory limits at lengths greater than 32K. The closest competitor   \n244 to FlashMask, FA-Varlen, exhibited higher latencies as sequence lengths increased. Similar trends   \n245 were observed in the DPO and RM scenarios, with FlashMask significantly outperforming FA  \n246 DenseMask and Vanilla Attention, especially in the RM scenario where higher sparsity levels   \n247 further enhanced FlashMask\u2019s effectiveness. Performance benefits from varying sparsity levels   \n248 were also quantified, with FlashMask showing linear negative correlation with increasing sparsity,   \n249 demonstrating efficient utilization of sample sparsity for acceleration. FlashMask\u2019s capability to   \n250 perform sliding window attention was further tested against FA-Window with window sizes of 256,   \n251 512, 1024, 2048, 4096, and 8192, as shown in Figure 4 Top. FlashMask matched FA-Window in   \n252 latency across sequence lengths of 8K, 16K, and 32K, showing comparable delay performances at   \n253 increasing window sizes. ", "page_idx": 6}, {"type": "image", "img_path": "rog0J435OO/tmp/c1ec8a7c76990e38bd0fe261de104b371a83fe37cb115184fba62fb460521deb.jpg", "img_caption": ["Figure 5: Comparison of End-to-End Training Throughput on Synthetic Dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "254 4.4 End-to-End Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "255 The end-to-end performance1 of the model was tested using synthetic datasets across three scales of the   \n256 LLaMA2 model and four downstream scenarios (SFT, LoRA, DPO, RM) at various sequence lengths,   \n257 measuring throughput in average Tokens/Sec/GPU. Each sequence length of 240 valid samples was   \n258 trained for one epoch, with results presented in Figure 5. In the SFT scenario, FlashMask showed a   \n259 clear throughput advantage over FA-DenseMask and Vanilla Attention, performing comparably to FA  \n260 Varlen. As sequence lengths increased, the throughput advantage of FlashMask over FA-DenseMask   \n261 and Vanilla Attention also enhanced, even enabling the completion of longer sequence tasks within the   \n262 same computational resources. In LoRA, DPO, and RM scenarios, FlashMask consistently showed   \n263 significant advantages. Notably, in the LoRA scenario at the LLaMA2-7B, FlashMask achieved a   \n264 4.16x throughput improvement over FA-DenseMask, supporting sequence lengths up to 544K. It\u2019s   \n265 important to note that FA-Varlen was unable to support the DPO and RM scenarios with the answers   \n266 sharing one question, whereas FlashMask was capable of handling various scenarios including DPO   \n267 and RM.   \n268 Additional experiments were conducted on the open-source dataset LongBench [45], comparing the   \n269 end-to-end performance of FA-DenseMask, FA-Varlen, and FlashMask at sequence lengths of 16K,   \n270 32K, and 64K. The performance improvements were consistent with those observed in the synthetic   \n271 dataset. The detailed results are presented in Appendix A.3. Memory usage during the experiments   \n272 was also recorded, showing significant reductions for FlashMask compared to FA-DenseMask, with   \n273 detailed results presented in Appendix A.3. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "274 5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "275 Several key topics emerge that are crucial for comprehending the full scope and implications of   \n276 FlashMask. These include the rationale behind the design choices, adaptations for supporting   \n277 bidirectional and other custom masks, and the necessity as well as limits of the current approach.   \n278 Necessity and Scope of the Study. The substantial advancement rendered by FlashMask in improving   \n279 attention mask computation is a significant evolution over the current FlashAttention framework.   \n280 Notably, FlashMask addresses and significantly mitigates the limitations observed with FlashAttention   \n281 in handling conventional and custom mask computations. This enhancement not only broadens the   \n282 applicative reach of FlashAttention but also signifies a key shift in efficiency metrics critical for   \n283 Transformer architectures. More importantly, the flexibility of FlashMask extends beyond the   \n284 proprietary boundaries of FlashAttention, offering potential beneftis to a wider range of Transformer  \n285 based models. By facilitating more efficient computation of the attention mechanism, FlashMask   \n286 enables innovations in processing vast datasets and complex models, thereby improving performances   \n287 across varied applications in the LLM field. This cross-model adaptability confirms the robustness   \n288 and utility of FlashMask as a universally applicable enhancement tool within and potentially outside   \n289 the Transformer architecture spectrum, promising substantial gains in computational efficiency and   \n290 model scalability.   \n291 Bidirectional and Custom Masks. In the exploration of attention mechanisms, the introduction of   \n292 FlashMask as discussed in this study offers a significant leap in computational efficiency, particularly   \n293 for masking processes in unidirectional attention mechanisms. By extending this approach to   \n294 bidirectional networks through the simple addition of vectors indicating the start and end indices   \n295 of the mask, FlashMask transcends conventional computational bounds, casting itself not just as   \n296 a sparse attention methodology, but as a versatile computational paradigm. Its adaptability across   \n297 various custom masking tasks and ability to effectively manage diverse types of mask combinations   \n298 underscores its potential to greatly enhance the efficiency of attention computations. Moreover, the   \n299 inherent sparsity of the attention mask during inference provides a robust justification for employing   \n300 FlashMask, indicating its utility and effectiveness in practical applications. This paradigm shift   \n301 highlights the importance of developing scalable and efficient computational strategies in the evolving   \n302 landscape of transformer architectures, suggesting that future research should continue to leverage   \n303 these innovations to tackle increasing computational demands.   \n304 Limitations and Future Directions. While FlashMask demonstrates impressive performance in   \n305 handling long-context sequences, it is observed that the computational cost of training Transformers   \n306 increases more than linearly as the sequence length grows\u2014not only due to the computation of   \n307 masked attention but also because of the extensive use of other operators. This scenario highlights the   \n308 inevitable need for leveraging or integrating distributed computing strategies or further algorithmic   \n309 enhancements to elevate training efficiency. Such advancements could be practical in managing   \n310 the computationally intensive tasks involved in processing extended contexts efficiently. As a part   \n311 of future research directions, exploring synergistic solutions that combine the strengths of both   \n312 algorithmic innovation (like FlashMask) and distributed system designs stands as a promising venture.   \n313 This approach is anticipated to address scalability challenges and could set the stage for breakthroughs   \n314 in handling unprecedentedly large data sets and complex model architectures. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "315 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "316 In this paper, we introduced FlashMask, a groundbreaking attention computation paradigm designed   \n317 to tackle the high computational and memory demands inherent in conventional attention mechanisms   \n318 in large-scale transformers. By implementing a novel column-wise sparse representation of attention   \n319 masks, FlashMask substantially reduces the memory and computational complexity from quadratic to   \n320 linear with the sequence length, thereby enhancing processing speeds and efficiency. Our algorithm   \n321 demonstrates versatility across various masking scenarios and retains robust performance in different   \n322 training pipelines. Extensive empirical analysis confirms that FlashMask accelerates computational   \n323 speed significantly, achieving up to $8.3\\mathrm{x}$ speedup in common modalities comparable to state-of-the-art   \n324 methods like FlashAttention. This advancement marks a significant leap forward in the design of   \n325 attention computation, offering the potential for broader applications and setting a new benchmark in   \n326 the efficiency of processing long-context sequences. ", "page_idx": 8}, {"type": "text", "text": "327 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "328 [1] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient   \n329 exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359,   \n330 2022.   \n331 [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz   \n332 Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,   \n333 30, 2017.   \n334 [3] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt   \n335 Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction   \n336 meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.   \n337 [4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi   \n338 Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.   \n339 Journal of Machine Learning Research, 25(70):1\u201353, 2024.   \n340 [5] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,   \n341 Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with   \n342 human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n343 [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and   \n344 Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv   \n345 preprint arXiv:2402.03300, 2024.   \n346 [7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,   \n347 Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv   \n348 preprint arXiv:2303.08774, 2023.   \n349 [8] Anthropic. Introducing claude. https://www.anthropic.com/news/introducing-claude, 2024.   \n350 Accessed: May 20, 2024.   \n351 [9] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste   \n352 Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking   \n353 multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   \n354 [10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse   \n355 transformers. arXiv preprint arXiv:1904.10509, 2019.   \n356 [11] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying   \n357 sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:17413\u201317426,   \n358 2021.   \n359 [12] Zhiqing Sun, Yiming Yang, and Shinjae Yoo. Sparse attention with learning to hash. In International   \n360 Conference on Learning Representations, 2021.   \n361 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and   \n362 Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,   \n363 2021.   \n364 [14] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang  \n365 Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint   \n366 arXiv:2402.09353, 2024.   \n367 [15] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora:   \n368 Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023.   \n369 [16] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.   \n370 Direct preference optimization: Your language model is secretly a reward model. Advances in Neural   \n371 Information Processing Systems, 36, 2024.   \n372 [17] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng   \n373 Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model   \n374 alignment. arXiv preprint arXiv:2304.06767, 2023.   \n375 [18] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu.   \n376 Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.   \n377 [19] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model   \n378 alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.   \n379 [20] Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad   \n380 Saleh, Simon Baumgartner, Jialu Liu, et al. Lipo: Listwise preference optimization through learning-to-rank.   \n381 arXiv preprint arXiv:2402.01878, 2024.   \n382 [21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models.   \n383 arXiv preprint arXiv:2303.00001, 2023.   \n384 [22] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John   \n385 Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050,   \n386 2023.   \n387 [23] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu   \n388 Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint   \n389 arXiv:2401.06080, 2024.   \n390 [24] Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure   \n391 Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights   \n392 fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36, 2024.   \n393 [25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy   \n394 optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n395 [26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec  \n396 tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n397 [27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language   \n398 models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n399 [28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,   \n400 Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation   \n401 language models. arXiv preprint arXiv:2302.13971, 2023.   \n402 [29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay   \n403 Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and   \n404 fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n405 [30] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi,   \n406 Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study.   \n407 arXiv preprint arXiv:2404.14047, 2024.   \n408 [31] Mario Michael Krell, Matej Kosec, Sergio P Perez, and Andrew Fitzgibbon. Efficient sequence packing   \n409 without cross-contamination: Accelerating large language models without impacting performance. arXiv   \n410 preprint arXiv:2107.02027, 2021.   \n411 [32] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron,   \n412 Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n\u2019pack: Navit, a   \n413 vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems,   \n414 36, 2024.   \n415 [33] PaddleNLP Contributors. Paddlenlp: An easy-to-use and high performance nlp library. https://github.   \n416 com/PaddlePaddle/PaddleNLP, 2021.   \n417 [34] BYTEDANCE INC. Effective transformer. https://github.com/bytedance/effective_   \n418 transformer, 2021.   \n419 [35] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and   \n420 Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n421 [36] Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig.   \n422 In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200,   \n423 2024.   \n424 [37] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language   \n425 models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.   \n426 [38] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,   \n427 Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.   \n428 Advances in neural information processing systems, 33:17283\u201317297, 2020.   \n429 [39] Markus N Rabe and Charles Staats. Self-attention does not need o (n2) memory. arXiv preprint   \n430 arXiv:2112.05682, 2021.   \n431 [40] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference,   \n432 2023.   \n433 [41] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong,   \n434 and Yu Wang. Flashdecoding $^{++}$ : Faster large language model inference on gpus. arXiv preprint   \n435 arXiv:2311.01282, 2023.   \n436 [42] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite   \n437 context. arXiv preprint arXiv:2310.01889, 2023.   \n438 [43] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint   \n439 arXiv:2001.04451, 2020.   \n440 [44] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear   \n441 complexity. arXiv preprint arXiv:2006.04768, 2020.   \n442 [45] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao   \n443 Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask   \n444 benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "445 A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "446 A.1 Algorithm Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "447 The detail implementation of FlashMask Backward Pass is presented in Algorithm 2. We do   \n448 precomputations of max and min values of FMS and FME similar to the Forward Pass. Then the   \n449 $\\mathbf{FMS}_{j}$ and $\\mathbf{FME}_{j}$ can be loaded to SRAM outside the inner loop (line 14-15), reducing the HBM   \n450 accesses to $2\\times N$ . Then, we do inner loop on $Q_{i}$ (line 16), computing the two valid parts and   \n451 bypassing the masked part $\\begin{array}{r}{i\\in(\\left\\lfloor\\frac{\\mathbf{maxFMS}_{j}}{B_{r}}\\right\\rfloor,\\left\\lfloor\\frac{\\mathbf{minFME}_{j}}{B_{r}}\\right\\rfloor)}\\end{array}$   \nAlgorithm 2 Optimized Backward Pass with FlashMask   \nRequire: Matrices $\\mathbf{2},\\mathbf{K},\\mathbf{V},\\mathbf{0},\\mathbf{d0}\\in\\mathbb{R}^{N\\times d}$ in HBM, vector $L\\in\\mathbb{R}^{N}$ in HBM, block sizes $B_{c}$ , $B_{r}$ , dense bias   \nmask $D\\in\\mathbb{R}^{N\\times N}$ , column-wise sparse mask starting rows $\\mathbf{FMS}\\in\\mathbb{R}^{N}$ , ending rows $\\mathbf{FME}\\in\\mathbb{R}^{N}$ .   \n1: Divide Q into $T_{r}=\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil$ blocks $\\mathbf{Q}_{1},\\hdots,\\mathbf{Q}_{T_{r}}$ of size $B_{r}\\times d$ each, and divide $\\mathbf{K},\\mathbf{V}$ in to $\\begin{array}{r}{T_{c}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil}\\end{array}$ blocks   \n$\\mathbf{K}_{1},\\ldots,\\mathbf{K}_{T_{c}}$ and $\\mathbf{V}_{1}^{\\dagger},\\dots,\\mathbf{V}_{T_{c}}$ , of size $B_{c}\\times d$ each.   \n2: Divide $\\mathbf{o}$ into $T_{r}$ blocks $\\mathbf{0}_{i},\\ldots,\\mathbf{0}_{T_{r}}$ of size $B_{r}\\times d$ each, divide dO into $T_{r}$ blocks $\\mathbf{dO}_{i},\\dots,\\mathbf{dO}_{T_{r}}$ of size   \n$B_{r}\\times d$ each, and divide $L$ into $T_{r}$ blocks $L_{i},...,L_{T_{r}}$ of size $B_{r}$ each.   \n3: Initialize ${\\bf d}{\\bf Q}=(0)_{N\\times d}$ in HBM and divide it into $T_{r}$ blocks ${\\bf d}{\\bf Q}_{1},\\ldots,{\\bf d}{\\bf Q}_{T_{r}}$ of size $B_{r}\\times d$ each. Divide   \ndK, $\\mathbf{dV}\\in\\mathbb{R}^{N\\times d}$ in to $T_{c}$ blocks $\\mathbf{dK}_{1},\\dots,\\mathbf{dK}_{T_{c}}$ and $\\mathbf{dV}_{1},\\dots,\\mathbf{dV}_{T_{c}}$ , of size $B_{c}\\times d$ each.   \n4: Divide the dense mask $\\mathbf{D}$ into $T_{r}\\times T_{c}$ blocks $\\mathbf{D}_{1,1},...,\\mathbf{D}_{T_{r},T_{c}}$   \n5: Divide FMS into $T_{c}$ blocks $\\mathbf{FMS}_{1}$ , ..., $\\mathbf{FMS}_{T_{c}}$ , and divide\ud835\udc5f F\ud835\udc50ME into $\\mathbf{FME}_{1},...,\\mathbf{FME}_{T_{c}}$ .   \n6: Precompute the max value max $\\mathbf{FMS}_{1}$ , ..., maxFMS\ud835\udc47\ud835\udc50for each $\\mathbf{FMS}_{1},...,\\mathbf{FMS}_{T_{c}}.$ , write to HBM.   \n7: Precompute the max value maxFME1, $...,\\mathbf{maxFME}_{T_{c}}$ for each $\\mathbf{FME}_{1}$ , ..., $\\mathbf{FME}_{T_{c}}$ , write to HBM.   \n8: Precompute the min value min $\\mathbf{FMS}_{1}$ , . $..,\\mathbf{minFMS}_{T_{c}}$ for each $\\mathbf{FMS}_{1}$ , ..., $\\mathbf{FMS}_{T_{c}}$ , write to HBM.   \n9: Precompute the min value min $\\operatorname{FME}_{1}$ , ..., minFME\ud835\udc47\ud835\udc50for each $\\mathbf{FME}_{1},...,\\mathbf{FME}_{T_{c}}$ , write to HBM.   \n10: Compute $D=\\mathrm{rowsum}(\\mathbf{dO}\\circ\\mathbf{O})\\in\\mathbb{R}^{d}$ (pointwise multiply), write $D$ to HBM and divide it into $T_{r}$ blocks   \n$D_{1},\\ldots,D_{T_{r}}$ of size $B_{r}$ each.   \n11: for $1\\leq j\\leq T_{c}$ do   \n12: Load $\\mathbf{K}_{j},\\mathbf{V}_{j}$ from HBM to on-chip SRAM.   \n13: Initialize $\\mathbf{d}\\bar{\\mathbf{K}}_{j}=(0)_{B_{c}\\times d},\\mathbf{d}\\mathbf{V}_{j}=(0)_{B_{c}\\times d}$ on SRAM.   \n14: Load $\\mathbf{FMS}_{j}$ from HBM to on-chip SRAM.   \n15: Load $\\mathbf{FME}_{j}$ from HBM to on-chip SRAM.   \n16: for $\\begin{array}{r}{1\\leq i\\leq\\left\\lfloor\\frac{\\operatorname*{max}\\!\\mathrm{{FMS}}_{j}}{B_{r}}\\right\\rfloor a n d\\left\\lfloor\\frac{\\operatorname*{minFME}_{j}}{B_{r}}\\right\\rfloor\\leq i\\leq T_{r}}\\end{array}$ do   \n17: Load $\\mathbf{Q}_{i},\\mathbf{O}_{i},\\mathbf{dO}_{i},\\bar{\\mathbf{dQ}}_{i},\\bar{L_{i}},D_{i}$ from HBM to on-chip SRAM.   \n18: On chip, compute $\\mathbf{S}_{i}^{(j)}=\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\in\\mathbb{R}^{B_{r}\\times B_{c}}$ .   \n19: On chip, set $\\mathbf{S}_{i}^{(j)}=\\mathbf{S}_{i}^{(j)}+D_{i,j}$   \n20: if $\\begin{array}{r}{\\left\\lfloor\\frac{\\operatorname*{max}\\!\\mathrm{{FME}}_{j}}{B_{r}}\\right\\rfloor\\le i\\le\\left\\lfloor\\frac{\\operatorname*{minFMS}_{j}}{B_{r}}\\right\\rfloor}\\end{array}$ then   \n21: On chip, set ${\\bf S}_{i}^{(j)}\\left[x\\right]\\left[y\\right]=-\\infty$ , for every $i*B_{r}+x\\ge M_{j}[y]$ .   \n22: end if   \n23: On chip, compute P\ud835\udc56( \ud835\udc57) ${\\bf P}_{i}^{(j)}=\\exp({\\bf S}_{i j}-L_{i})\\in\\mathbb{R}^{B_{r}\\times B_{c}}$   \n24: On chip, compute $\\mathbf{dV}_{j}\\leftarrow\\mathbf{dV}_{j}+(\\mathbf{P}_{i}^{(j)})^{\\top}\\mathbf{dO}_{i}\\in\\mathbb{R}^{B_{c}\\times d}.$   \n25: On chip, compute $\\mathbf{dP}_{i}^{(j)}=\\mathbf{dO}_{i}\\mathbf{V}_{j}^{\\top}\\in\\mathbb{R}^{B_{r}\\times B_{c}}$ .   \n26: On chip, compute $\\mathbf{dS}_{i}^{(j)}=\\mathbf{P}_{i}^{(j)}\\circ(\\mathbf{dP}_{i}^{(j)}-D_{i})\\in\\mathbb{R}^{B_{r}\\times B_{c}}$ .   \n27: Load ${\\bf d}{\\bf Q}_{i}$ from HBM to SRAM, then on chip, update $\\mathbf{d}\\mathbf{Q}_{i}\\gets\\mathbf{d}\\mathbf{Q}_{i}+\\mathbf{d}\\mathbf{S}_{i}^{(j)}\\mathbf{K}_{j}\\,\\in\\mathbb{R}^{B_{r}\\times d}$ , and write   \nback to HBM.   \n28: On chip, compute $\\mathbf{dK}_{j}\\gets\\mathbf{dK}_{j}+\\mathbf{dS}_{i}^{\\left(j\\right)^{\\top}}\\mathbf{Q}_{i}\\in\\mathbb{R}^{B_{c}\\times d}.$   \n29: end for   \n30: Write ${\\bf d K}_{j}$ , $\\mathbf{dV}_{j}$ to HBM.   \n31: end for   \n32: Return dQ, dK, dV. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "452 A.2 Supplementary Experimental Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "453 All end-to-end training and testing in this paper were conducted on 4 servers, each equipped with 32   \n454 NVIDIA A800-SXM 80G GPUs. We comprehensively evaluated the performance of the LLaMA2   \n455 model across three different parameter scales, four downstream task scenarios, and various sequence   \n456 lengths. Given the diversity of experimental combinations and the specific distributed parallel   \n457 strategies required by models, in varying parameter scales, the primary goal of the experiments is   \n458 not to achieve optimal end-to-end training performance but to demonstrate the effectiveness of the   \n459 FlashMask method. Therefore, to ensure consistency, we set the following hyperparameters in Table 1   \n460 with the same hardware configuration.   \n461 To verify the representativeness of our synthetic dataset, sparsity distribution histograms of synthetic   \n462 dataset are presented in Figure 6. Then we use InToken method with max sequence length of 16K,   \n463 32K, 64K, and 128K on the open-source dataset LongBench, and compute the distribution histograms,   \n464 presented in Figure 7. Note that many long sentences are truncated for max sequence length 16K,   \n465 and 32K. Results indicate that the sparsity distributions of LongBench dataset and synthetic dataset   \n466 are similar. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "rog0J435OO/tmp/8d4c3e8a6fe24e51d94d56b5c5679b61dbcb8248b05fc8e297f45873dd3fcdd8.jpg", "table_caption": ["Table 1: Training Hyperparameters for Various Scales of LLaMA2 Models. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "rog0J435OO/tmp/07fe9d807579f257f73109bfdf1f379db1a0574c8d858f8815e31b6acf480ad1.jpg", "img_caption": ["Figure 6: Sparsity Distribution of Synthetic Dataset. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "rog0J435OO/tmp/886e264d81d255be3ecf852622dbb3fdf5b33de8a4b6f73b70c03afb06af521f.jpg", "img_caption": ["Figure 7: Sparsity Distribution of LongBench Dataset. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "467 A.3 Full Experiment Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "468 Kernel experiments are also conducted on batch sizes 4, and 8. FA-Varlen is excluded by default.   \n469 Results are presented in Figure 8 and 9. The trends are identical to Figure 3 in Section 4.3, except   \n470 memory exhaustion occurred with less sequence length, especially for FA-DenseMask and Vanilla   \n471 Attention which require $O(N^{2})$ memory to launch.   \n472 We evaluate the effectiveness of FlashMask on the open-source dataset LongBench. The throughput of   \n473 LoRA fine-tuning for LLaMA2-7B are shown in Figure 10. FlashMask performed close to FA-Varlen,   \n474 showcasing $4.12\\mathbf{x}$ faster than FA-DenseMask, proving that FlashMask can deliver significant training   \n475 accelerations in generalized real-world scenarios.   \n476 Figure 11 presents the GPU memory consumption in End-to-End training. FlashMask showed linear   \n477 memory consumption with increasing sequence length, far less than FA-DenseMask. Therefore,   \n478 FlashMask supports training with much longer sequences in memory limits of 80G. ", "page_idx": 13}, {"type": "image", "img_path": "rog0J435OO/tmp/8554ea2aef34af664b35e0e6857debbacdb77867b869fc5781f3e05dcd275c05.jpg", "img_caption": ["Figure 8: Kernel Latency Comparison with Varying the Length of Sequence.(Batch Size $=4$ ) "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "rog0J435OO/tmp/c005a052ef1e625217919901eb8fb39d2d74f148d7af7cf475c805d210e974ce.jpg", "img_caption": ["Figure 9: Kernel Latency Comparison with Varying the Length of Sequence. (Batch Size $=8$ ) "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "rog0J435OO/tmp/d199b94d8198b808e3f2b5eb519caa99e8923173d359baebe5f1623924eb1292.jpg", "img_caption": ["Figure 10: Comparison of End-to-End Training Throughput on LongBench Dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "rog0J435OO/tmp/1a2f62f8626f302455290b0d5b6ce638b32bd979769433780ca800957dc83351.jpg", "img_caption": ["Figure 11: Comparison of End-to-End Training GPU Memory. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "479 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "80 1. Claims   \n481 Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "482   \n483 Answer: [Yes]   \n484 Justification: This paper\u2019s contributions and scope are described in Abstract and Introduction.   \n485 Guidelines:   \n486 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n487 made in the paper.   \n488 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n489 contributions made in the paper and important assumptions and limitations. A No or   \n490 NA answer to this question will not be perceived well by the reviewers.   \n491 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n492 much the results can be expected to generalize to other settings.   \n493 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n494 are not attained by the paper.   \n495 2. Limitations   \n496 Question: Does the paper discuss the limitations of the work performed by the authors?   \n497 Answer: [Yes]   \n498 Justification: The paper includes a discussion section about limitations.   \n499 Guidelines:   \n500 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n501 the paper has limitations, but those are not discussed in the paper.   \n502 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n503 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n504 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n505 model well-specification, asymptotic approximations only holding locally). The authors   \n506 should reflect on how these assumptions might be violated in practice and what the   \n507 implications would be.   \n508 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n509 only tested on a few datasets or with a few runs. In general, empirical results often   \n510 depend on implicit assumptions, which should be articulated.   \n511 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n512 For example, a facial recognition algorithm may perform poorly when image resolution   \n513 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n514 used reliably to provide closed captions for online lectures because it fails to handle   \n515 technical jargon.   \n516 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n517 and how they scale with dataset size.   \n518 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n519 address problems of privacy and fairness.   \n520 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n521 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n522 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n523 judgment and recognize that individual actions in favor of transparency play an impor  \n524 tant role in developing norms that preserve the integrity of the community. Reviewers   \n525 will be specifically instructed to not penalize honesty concerning limitations.   \n526 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and   \n527   \n28 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "530 Justification: In sec 3.3 Complexity Analysis   \n531 Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "542 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "43 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n44 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n45 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The source code will be public available and can reproduce the results according README. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "585 Answer: [Yes]   \n586 Justification: The source code will be public available.   \n587 Guidelines:   \n588 \u2022 The answer NA means that paper does not include experiments requiring code.   \n589 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n590 public/guides/CodeSubmissionPolicy) for more details.   \n591 \u2022 While we encourage the release of code and data, we understand that this might not be   \n592 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n593 including code, unless this is central to the contribution (e.g., for a new open-source   \n594 benchmark).   \n595 \u2022 The instructions should contain the exact command and environment needed to run to   \n596 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n597 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n598 \u2022 The authors should provide instructions on data access and preparation, including how   \n599 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n600 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n601 proposed method and baselines. If only a subset of experiments are reproducible, they   \n602 should state which ones are omitted from the script and why.   \n603 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n604 versions (if applicable).   \n605 \u2022 Providing as much information as possible in supplemental material (appended to the   \n606 paper) is recommended, but including URLs to data and code is permitted.   \n607 6. Experimental Setting/Details   \n608 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n609 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n610 results?   \n611 Answer: [Yes]   \n612 Justification: Refer to Experiments section.   \n613 Guidelines:   \n614 \u2022 The answer NA means that the paper does not include experiments.   \n615 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n616 that is necessary to appreciate the results and make sense of them.   \n617 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n618 material.   \n619 7. Experiment Statistical Significance   \n620 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n621 information about the statistical significance of the experiments?   \n622 Answer: [Yes]   \n623 Justification: All our experimental results are run multiple times and then averaged.   \n624 Guidelines:   \n625 \u2022 The answer NA means that the paper does not include experiments.   \n626 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n627 dence intervals, or statistical significance tests, at least for the experiments that support   \n628 the main claims of the paper.   \n629 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n630 example, train/test split, initialization, random drawing of some parameter, or overall   \n631 run with given experimental conditions).   \n632 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n633 call to a library function, bootstrap, etc.)   \n634 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n635 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n636 of the mean. ", "page_idx": 18}, {"type": "text", "text": "\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "645 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "646 Question: For each experiment, does the paper provide sufficient information on the com  \n647 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n648 the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Referring to the Experiments section, we provide a running environment. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "4 Justification: We follow the NeurIPS Code of Ethics properly.   \n5 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "671 10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "672 Question: Does the paper discuss both potential positive societal impacts and negative   \n673 societal impacts of the work performed?   \n674 Answer: [NA]   \n675 Justification: There is no societal impact of the work performed.   \n676 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 19}, {"type": "text", "text": "688 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n689 that a generic algorithm for optimizing neural networks could enable people to train   \n690 models that generate Deepfakes faster.   \n691 \u2022 The authors should consider possible harms that could arise when the technology is   \n692 being used as intended and functioning correctly, harms that could arise when the   \n693 technology is being used as intended but gives incorrect results, and harms following   \n694 from (intentional or unintentional) misuse of the technology.   \n695 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n696 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n697 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n698 feedback over time, improving the efficiency and accessibility of ML).   \n699 11. Safeguards   \n700 Question: Does the paper describe safeguards that have been put in place for responsible   \n701 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n702 image generators, or scraped datasets)?   \n703 Answer: [NA]   \n704 Justification: The paper poses no such risks.   \n705 Guidelines:   \n706 \u2022 The answer NA means that the paper poses no such risks.   \n707 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n708 necessary safeguards to allow for controlled use of the model, for example by requiring   \n709 that users adhere to usage guidelines or restrictions to access the model or implementing   \n710 safety filters.   \n711 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n712 should describe how they avoided releasing unsafe images.   \n713 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n714 not require this, but we encourage authors to take this into account and make a best   \n715 faith effort.   \n716 12. Licenses for existing assets   \n717 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n718 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n719 properly respected?   \n720 Answer: [Yes]   \n721 Justification: CC BY-NC-ND 4.0   \n722 Guidelines:   \n723 \u2022 The answer NA means that the paper does not use existing assets.   \n724 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n725 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n726 URL.   \n727 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n728 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n729 service of that source should be provided.   \n730 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n731 package should be provided. For popular datasets, paperswithcode.com/datasets   \n732 has curated licenses for some datasets. Their licensing guide can help determine the   \n733 license of a dataset.   \n734 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n735 the derived asset (if it has changed) should be provided.   \n736 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n737 the asset\u2019s creators.   \n738 13. New Assets   \n739 Question: Are new assets introduced in the paper well documented and is the documentation   \n740 provided alongside the assets?   \n41 Answer: [NA]   \n42 Justification: The paper does not release new assets.   \n43 Guidelines:   \n44 \u2022 The answer NA means that the paper does not release new assets.   \n45 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n46 submissions via structured templates. This includes details about training, license,   \n747 limitations, etc.   \n48 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n49 asset is used.   \n750 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n751 create an anonymized URL or include an anonymized zip file.   \n752 14. Crowdsourcing and Research with Human Subjects   \n753 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n754 include the full text of instructions given to participants and screenshots, if applicable, as   \n755 well as details about compensation (if any)?   \n756 Answer: [NA]   \n757 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n758 Guidelines:   \n759 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n60 human subjects.   \n761 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n762 tion of the paper involves human subjects, then as much detail as possible should be   \n763 included in the main paper.   \n64 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n65 or other labor should be paid at least the minimum wage in the country of the data   \n66 collector.   \n767 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n68 Subjects   \n69 Question: Does the paper describe potential risks incurred by study participants, whether   \n770 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n771 approvals (or an equivalent approval/review based on the requirements of your country or   \n772 institution) were obtained?   \n73 Answer: [NA]   \n74 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n75 Guidelines:   \n76 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n77 human subjects.   \n78 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n79 may be required for any human subjects research. If you obtained IRB approval, you   \n780 should clearly state this in the paper.   \n781 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n782 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n83 guidelines for their institution.   \n784 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n785 applicable), such as the institution conducting the review. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}]