[{"type": "text", "text": "Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xianlong $\\mathbf{Wang^{1,2,4,5\\dagger}}$ , Minghui $\\mathbf{Li^{\\ddagger}}$ , Wei Liu1,2,4,5\u2020, Hangtao Zhang4,5\u2020, Shengshan $\\mathbf{H}\\mathbf{u}^{1,2,4,5\\dagger}$ , Yechao Zhang1,2,4,5\u2020, Ziqi Zhou1,2,3\u00a7, Hai Jin1,2,3\u00a7 ", "page_idx": 0}, {"type": "text", "text": "1 National Engineering Research Center for Big Data Technology and System 2 Services Computing Technology and System Lab 3 Cluster and Grid Computing Lab 4 Hubei Engineering Research Center on Big Data Security 5 Hubei Key Laboratory of Distributed System Security \u2020 School of Cyber Science and Engineering, Huazhong University of Science and Technology $\\ddagger$ School of Software Engineering, Huazhong University of Science and Technology   \n$\\S$ School of Computer Science and Technology, Huazhong University of Science and Technology   \n{wxl99,minghuili,weiliu73,hangt_zhang,hushengshan,ycz,zhouziqi,hjin}@hust.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Traditional unlearnable strategies have been proposed to prevent unauthorized users from training on the 2D image data. With more 3D point cloud data containing sensitivity information, unauthorized usage of this new type data has also become a serious concern. To address this, we propose the first integral unlearnable framework for 3D point clouds including two processes: (i) we propose an unlearnable data protection scheme, involving a class-wise setting established by a categoryadaptive allocation strategy and multi-transformations assigned to samples; (ii) we propose a data restoration scheme that utilizes class-wise inverse matrix transformation, thus enabling authorized-only training for unlearnable data. This restoration process is a practical issue overlooked in most existing unlearnable literature, i.e., even authorized users struggle to gain knowledge from 3D unlearnable data. Both theoretical and empirical results (including 6 datasets, 16 models, and 2 tasks) demonstrate the effectiveness of our proposed unlearnable framework. Our code is available at https://github.com/CGCL-codes/UnlearnablePC. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, 3D point cloud deep learning has been making remarkable strides in various domains, e.g., self-driving [6] and virtual reality [1, 46]. Specifically, numerous 3D sensors scan the surrounding environment and synthesize massive 3D point cloud data containing sensitive information such as pedestrian and vehicles [32] to the cloud server for deep learning analysis [12, 23]. However, the raw point cloud data can be exploited for point cloud unauthorized deep learning if a data breach occurs, posing a significant privacy threat. Fortunately, the privacy protection approaches for preventing unauthorized training have been extensively studied in the 2D image domain [9, 19, 28, 29, 39, 42]. They apply elaborate perturbations on images such that trained networks over them exhibit extremely low generalization, thus failing to learn knowledge from the protected data, known as \"making data unlearnable\". Nonetheless, the stark disparity between 2D images and 3D point clouds poses significant challenges for drawing lessons from existing 2D solutions. ", "page_idx": 0}, {"type": "text", "text": "Specifically, migrating 2D unlearnable schemes to 3D suffers from following challenges: (i) Incompatibility with 3D data. Numerous model-agnostic 2D image unlearnable schemes operate in the pixel space, such as convolutional operations [39, 42], making them fail to be directly transferred to the 3D point space. (ii) Poor visual quality. Migrating model-dependent 2D unlearnable methods [5, 9, 19, 28] to 3D point clouds requires perturbing substantial points, leading to irregular three-dimensional shifts which may significantly degrade visual quality. Hence, these challenges spur us to start directly from the characteristics of point clouds for proposing 3D unlearnable solutions. ", "page_idx": 0}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/d5064bbe4ace9eb1691ac95945628f51ad0605dc7b1fda6f61674375c23823cf.jpg", "table_caption": [], "table_footnote": ["Figure 1: An overview of existing seven types of 3D transformations. \u201c\\*\" denotes rigid transformations that do not alter the shape of the point cloud samples, while the remaining transformations are non-rigid transformations. "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recent works observe that 3D transformations can alter test-time results of models [7, 17, 44]. To explore this, we conduct an in-depth investigation into the properties of seven 3D transformations as shown in Fig. 1 and reveal the mechanisms by which transformations employed in a certain pattern serve as unlearnable schemes (Sec. 3.2). In light of this, we propose the first unlearnable approach in 3D point clouds via multi class-wise transformation (UMT), transforming samples to various forms for privacy protection. Concretely, we newly propose a category-adaptive allocation strategy by leveraging uniform distribution sampling and category constraints to establish a class-wise setting, thereby multiplying multi-transformations to samples based on categories. To theoretically analyze UMT, we define a binary classification setup similar to that used in [20, 33, 39]. Meanwhile, we employ a Gaussian Mixture Model (GMM) [38] to model the clean training set and use the Bayesian optimal decision boundary to model the point cloud classifier. Theoretically, we prove that there exists a UMT training set follows a GMM distribution and the classification accuracy of UMT dataset is lower than that of the clean dataset in a Bayesian classifier. ", "page_idx": 1}, {"type": "text", "text": "Moreover, an incompatible issue in existing unlearnable works [9, 19, 28, 29, 39, 43] is identified [54], i.e., these approaches prevent unauthorized learning to protected data, but they also impede authorized users from effectively learning from unlearnable data. To address this, we propose a data restoration scheme that applies class-wise inverse transformations, determined by a lightweight message received from the protector. Our proposed unlearnable framework including UMT approach and data restoration scheme is depicted in Fig. 3. ", "page_idx": 1}, {"type": "text", "text": "Extensive experiments on 6 benchmark datasets (including synthetic and real-world datasets) using 16 point cloud models across CNN, MLP, Graph-based Network, and Transformer on two tasks (classification and semantic segmentation), verified the effectiveness of our proposed unlearnable scheme. We summarize our main contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 The First Integral 3D Unlearnable Framework. To the best of our knowledge, we propose the first integral unlearnable 3D point cloud framework, utilizing class-wise multitransformation as its unlearnable mechanism (effectively safeguarding point cloud data against unauthorized exploitation) and proposing a novel data restoration approach that leverages class-wise reversible 3D transformation matrices (addressing an incompatible issue in most existing unlearnable works, where even authorized users cannot effectively learn knowledge from unlearnable data). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretical Analysis. We theoretically indicate the existence of an unlearnable situation that the classification accuracy of the UMT dataset is lower than that of the clean dataset under the decision boundary of the Bayes classifier in Gaussian Mixture Model. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Experimental Evaluation. Extensive experiments on 3 synthetic datasets and 3 real-world datasets using 16 widely used point cloud model architectures on classification and semantic segmentation tasks verify the superiority of our proposed schemes. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Considering the raw point cloud data $(\\mathbf{X},\\mathbf{Y})\\in\\mathcal{X}\\times\\mathcal{Y}$ sampled from a clean distribution $\\mathcal{D}$ for training a point cloud network, the user\u2019s goal is to obtain a model $\\mathcal{F}:\\mathcal{X}\\rightarrow\\mathcal{Y}$ by minimizing the loss function (e.g., cross-entropy loss) $\\mathcal{L}(\\mathcal{F}(\\mathbf{X}),\\mathbf{Y})$ . Let $\\mathbf{T}\\in\\mathcal{T}$ be a 3D transformation matrix that does not seriously damage the visual quality of point clouds. Note that $\\mathcal{X}\\in\\mathbb{R}^{3\\times p}$ , $\\mathcal{T}\\in\\mathbb{R}^{3\\times3}$ , and $p$ represents the number of points. In theoretical analysis, following [20, 33], we simplify a training dataset $\\mathcal{D}_{k}$ to a Gaussian Mixture Model (GMM) [38] $\\mathcal{N}(y\\mu,I)$ , where $y\\in\\{\\pm1\\}$ denotes the class labels, $\\mu\\in\\mathbb{R}^{d}$ denotes the mean value, and $\\pmb{I}\\in\\mathbb{R}^{d\\times d}$ denotes the identity matrix. Thus the Bayes optimal decision boundary for classifying $\\mathcal{D}_{k}$ is defined by $P(x)\\equiv\\mu^{T}x\\stackrel{.}{=}0$ . The accuracy of the decision boundary $P$ on $\\mathcal{D}_{k}$ is equal to $\\phi(||\\boldsymbol{\\mu}||_{2})$ , where $\\phi$ denotes the Cumulative Distribution Function (CDF) of the standard normal distribution. ", "page_idx": 2}, {"type": "text", "text": "Data protector $\\mathcal{G}_{p}$ . $\\mathcal{G}_{p}$ aims to protect the knowledge from the clean training set (with size of $n$ ) $\\mathcal{D}_{c}=\\{\\mathbf{X}_{i},\\mathbf{Y}_{i}\\}_{i=1}^{n}\\sim\\^{\\cdot}D$ by compromising the unauthorized models who train on the unlearnable point cloud data $\\{\\mathbf{T}_{i}(\\mathbf{X}_{i}),\\mathbf{Y}_{i}\\}_{i=1}^{n}$ , resulting in extremely poor generalization on the clean test distribution $\\mathcal{D}_{t}\\subseteq\\mathcal{D}$ . This objective can be formalized as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{(\\mathbf{X},\\mathbf{Y})\\sim\\mathcal{D}_{t}}\\mathcal{L}\\left(\\mathcal{F}\\left(\\mathbf{X};\\boldsymbol{\\theta}_{u}\\right),\\mathbf{Y}\\right),\\mathrm{~s.t.~}\\boldsymbol{\\theta}_{u}=\\arg\\operatorname*{min}_{\\boldsymbol{\\theta}}\\sum_{\\left(\\mathbf{X}_{i},\\mathbf{Y}_{i}\\right)\\in\\mathcal{D}_{c}}\\mathcal{L}\\left(\\mathcal{F}\\left(\\mathbf{T}_{i}\\left(\\mathbf{X}_{i}\\right);\\boldsymbol{\\theta}\\right),\\mathbf{Y}_{i}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{G}_{p}$ assumes that training samples are all transformed into unlearnable ones while maintaining normal visual effects, in line with previous unlearnable works [19, 28, 39, 42]. By the way, solving Eq. (1) directly is infeasible for neural networks because it necessitates unrolling the entire training procedure within the inner objective and performing backpropagation through it to execute a single step of gradient descent on the outer objective [8]. ", "page_idx": 2}, {"type": "text", "text": "Authorized user $\\mathcal{G}_{a}$ . $\\mathcal{G}_{a}$ aims to apply another transformation $\\mathbf{T}^{\\prime}\\in\\mathcal{T}$ on the unlearnable sample, making the protected data learnable. This is formally defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{(\\mathbf{X},\\mathbf{Y})\\sim\\mathcal{D}_{t}}}\\mathcal{L}\\left(\\mathcal{F}\\left(\\mathbf{X};\\boldsymbol{\\theta}_{r}\\right),\\mathbf{Y}\\right),\\mathrm{~s.t.~}\\boldsymbol{\\theta}_{r}=\\arg\\operatorname*{min}_{\\boldsymbol{\\theta}}\\sum_{(\\mathbf{X}_{i},\\mathbf{Y}_{i})\\in\\mathcal{D}_{c}}\\mathcal{L}\\left(\\mathcal{F}\\left(\\mathbf{T}_{i}^{\\prime}(\\mathbf{T}_{i}(\\mathbf{X}_{i}));\\boldsymbol{\\theta}\\right),\\mathbf{Y}_{i}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{G}_{a}$ assumes that, without access to any clean training samples, $\\mathbf{T}^{\\prime}$ can be constructed by utilizing a lightweight message $M$ received from data protectors. ", "page_idx": 2}, {"type": "text", "text": "3 Our Proposed Unlearnable Schemes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Key Intuition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Several recent works [7, 13, 44] reveal employing 3D transformations can mislead the model\u2019s classification results. Such a phenomenon implies that there might be some defects in point cloud classifiers when processing transformed samples, leading us to infer that 3D transformations are probable candidates for data protection against unauthorized training. If the transformed point cloud data are used to train unauthorized DNNs, only simple linear features inherent in 3D transformations (at which transformations may act as shortcuts [14]) are captured by the DNNs, successfully protecting point cloud data privacy. ", "page_idx": 2}, {"type": "image", "img_path": "SeefZa7Vmq/tmp/2a99e3bd42fa8a323e567d172a65155571db4bf2f354e191d58b16873423a7e2.jpg", "img_caption": ["Figure 2: (a) Training on the transformed ModelNet10 dataset (employing sample-wise, dataset-wise, and class-wise patterns) using PointNet classifier; (b) The high-level overview of the class-wise setting "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.2 Exploring the Mechanism ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We summarize existing 3D transformations in Fig. 1 and formally define them in Appendix A. To seek clarity on the application and selection of transformations, we explore three aspects: (i) execution mechanism, (ii) exclusion mechanism, and (iii) working mechanism as follows. ", "page_idx": 3}, {"type": "text", "text": "(i) Which execution mechanism successfully satisfy Eq. (1)? The extensively employed execution patterns in 2D unlearnable approaches are sample-wise [9, 19] and class-wise [19, 39] settings. We further complement the dataset-wise setting (using universal transformation) and implement the above execution mechanisms for training a PointNet classifier [35] on the transformed ModelNet10 [50], obtaining test accuracy results in Fig. 2 (a). We discover that model achieves considerably low test accuracy under the class-wise setting, satisfying Eq. (1). Sample-wise and dataset-wise settings do not obviously compromise model performance, which cannot serve as promising unlearnable routes. Moreover, we note that sample-wise transformation is often considered as a data augmentation scheme to improve generalization, which contradicts our aim of using class-wise transformation to lower model generalization. ", "page_idx": 3}, {"type": "text", "text": "(ii) Which transformations need to be excluded? Not all transformations are suitable candidates. We exclude three transformations, tapering, reflection, and translation. (1) The tapering matrix may cause point cloud samples to become a planar projection when $\\eta z$ defined in Eq. (18) equals to -1, rendering the tapered samples meaningless; (2) The reflection matrix has only three distinct transformation matrices, rendering it incapable of assigning class-wise transformations when the number of categories exceeds three; (3) The translation transformation is a straightforward and simple additive transformation that is too easily defeated by point cloud data pre-processing approaches. ", "page_idx": 3}, {"type": "text", "text": "(iii) Why does class-wise transformation work? We conduct experiments using class-wise transformations (see Tab. 6), indicating that the model training on the class-wise transformed training set achieves a relatively high accuracy on the class-wise transformed test set (using the same transformation process as the training set). Besides, if we permute the class-wise transformation for the test set, we obtain a significant low accuracy on the test set. Therefore, we conclude that the reason why class-wise transformation works is that the model learns the mapping between class-wise transformations and corresponding category labels as shown in Fig. 2 (b), which results in the model being unable to predict the corresponding labels on a clean test set lacking transformations. This analytical process yields conclusions that are in agreement with prior research [39, 44]. ", "page_idx": 3}, {"type": "text", "text": "3.3 Our Design for UMT ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.3.1 Category-Adaptive Allocation Strategy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We assign transformation parameters based on categories to realize class-wise setting. For rotation transformation $\\mathcal{R}\\in\\mathbb{R}^{3\\times3}$ , we refer to $\\alpha$ and $\\beta$ as slight angles imposed on the $x$ and $y$ axes, $\\gamma$ as the primary angle for $z$ axis. We generate random angles for $\\boldsymbol{A}_{N}$ times in three directions: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha,\\beta\\sim\\mathcal{U}(0,r_{s}),\\gamma\\sim\\mathcal{U}(0,r_{p}),\\mathcal{A}_{N}=\\left\\lceil\\sqrt[3]{N}\\right\\rceil\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{U}}$ denotes uniform distribution, $N$ denotes the number of categories, $r_{s}$ is a small range that controls $\\alpha$ and $\\beta$ , while $r_{p}$ is a large range that controls $\\gamma$ . $A_{N}$ is computed in such a way to ensure that the number of combinations of three angles is greater than or equal to $N$ . Concretely, in the rotation operation, each of the three directions has $A_{N}$ distinct angles, which means that the final rotation matrix has $\\mathcal{A}_{N}^{3}$ possible combinations. To satisfy the class-wise setup, $\\mathcal{A}_{N}^{3}$ must be at least $N$ , requiring $\\boldsymbol{A}_{N}$ to be no less than $\\left\\lceil\\sqrt[3]{N}\\right\\rceil$ . Finally, we randomly select $N$ combinations of angles for the allocation. The scaling transformation $\\mathcal{S}\\in\\mathbb{R}^{3\\times3}$ resizes the position of each point in the 3D point cloud sample by a certain scaling factor $\\lambda$ , which is sampled $N$ times from a uniform distribution $\\boldsymbol{\\mathcal{U}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda\\sim\\mathcal{U}(b_{l},b_{u})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $b_{l}$ and $b_{u}$ represent the lower bound and upper bound of the scaling factor, respectively. For shear $\\mathcal{H}\\in\\mathbb{R}^{3\\times3}$ defined in Eq. (13), twisting $\\bar{\\mathcal{W}}\\in\\mathbb{R}^{3\\times3}$ defined in Eq. (16), the process of generating parameters within ranges $(\\omega_{l},\\omega_{u})$ and $(h_{l},h_{u})$ is consistent to scaling. The range of these parameters ensures the visual effect of the sample. ", "page_idx": 3}, {"type": "image", "img_path": "SeefZa7Vmq/tmp/840c90d974e1b38c857b375ab795b1760159bf22e8a2760393e2580b04f7a072.jpg", "img_caption": ["Figure 3: An overview of our proposed integral unlearnable pipeline "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Property 1. Since rotation matrices $\\mathcal{R}_{\\alpha}$ , $\\mathcal{R}_{\\beta}$ , and $\\mathcal{R}_{\\gamma}$ around three directions are all orthogonal matrices, $\\mathcal{R}$ is also an orthogonal matrix, which can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall{\\mathcal{M}}\\in\\{\\mathcal{R}_{\\alpha},\\mathcal{R}_{\\beta},\\mathcal{R}_{\\gamma},\\mathcal{R}\\},\\quad\\mathrm{s.t.}\\quad\\mathcal{M}\\mathcal{M}^{T}=I\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we can determine the orthogonality by matrix multiplication through the definitions of Eq. (11).   \nSince $\\mathcal{R}=\\mathcal{R}_{\\alpha}\\mathcal{R}_{\\beta}\\mathcal{R}_{\\gamma}$ and $\\mathcal{R R}^{T}\\overset{\\smile}{=}\\mathcal{R}_{\\alpha}\\dot{\\mathcal{R}}_{\\beta}\\dot{\\mathcal{R}}_{\\gamma}\\mathcal{R}_{\\gamma}^{T}\\mathcal{R}_{\\beta}^{T}\\mathcal{R}_{\\alpha}^{\\dot{T}}=I$ , so $\\mathcal{R}$ is also an orthogonal matrix. ", "page_idx": 4}, {"type": "text", "text": "Property 2. All four transformation matrices we employ, $\\mathcal{R},\\mathcal{S},\\mathcal{W}_{i}$ , and $\\mathcal{H}$ , and the multiplicative combinations of any these matrices are all invertible matrices, which can be formally defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall{\\mathcal{I}}\\in\\{f(\\mathcal{R})f(S)f(\\mathcal{W})f(\\mathcal{H})\\mid f(x)\\in\\{x,I\\}\\},\\quad\\exists K\\quad\\mathrm{s.t.}\\quad\\mathcal{I}K=K\\mathcal{I}=I\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the inverse matrices of $\\mathcal{R},\\mathcal{S},\\mathcal{W}$ , and $\\mathcal{H}$ are given in Appendix A. This property allows the authorized users to normally train on the protected data due to that multiplying a matrix by its inverse results in the identity matrix, leading us to propose a data restoration scheme in Sec. 3.4. ", "page_idx": 4}, {"type": "text", "text": "3.3.2 Employing Class-wise Transformations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Assuming the point cloud training set $\\mathcal{D}_{c}$ is defined as $\\{(\\mathbf{X}_{c1},\\mathbf{Y}_{i}),(\\mathbf{X}_{c2},\\mathbf{Y}_{i}),...,(\\mathbf{X}_{c n_{i}},\\mathbf{Y}_{i})\\}_{i=1}^{N}$ where $n_{1},n_{2},...,n_{N}$ represent the number of samples in the 1st, 2nd, ..., $N$ -th category, respectively. We formally define the spectrum of transformations as $k$ to indicate the number of transformations involved. Thus the ultimate unlearnable transformation matrix ${\\bf T}_{k}$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{T}_{k}=\\prod_{i=1}^{k}\\gamma_{i},\\quad\\forall i\\neq j,\\quad\\mathrm{s.t.}\\quad\\mathcal{V}_{i},\\mathcal{V}_{j}\\in\\{\\mathcal{R},\\mathcal{S},\\mathcal{W},\\mathcal{H}\\},\\mathcal{V}_{i}\\neq\\mathcal{V}_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Once we employ the proposed category-adaptive allocation strategy to ${\\bf T}_{k}$ , the unlearnable point cloud dataset $\\mathcal{D}_{u}$ is constructed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}_{u}=\\{(\\mathbf{T}_{k_{i}}(\\mathbf{X}_{c1}),\\mathbf{Y}_{i}),(\\mathbf{T}_{k_{i}}(\\mathbf{X}_{c2}),\\mathbf{Y}_{i}),...,(\\mathbf{T}_{k_{i}}(\\mathbf{X}_{c n_{i}}),\\mathbf{Y}_{i})\\}_{i=1}^{N}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our proposed UMT scheme is described in Algorithm 1. We enumerate possible transformations in Eq. (7) to obtain the unlearnability in Tab. 7 and select one type of class-wise transformation for each $k$ for more comprehensive experiments in Tab. 1. To facilitate the theoretical study of $\\mathrm{UMT}^{2}$ , we opt for $\\mathcal{R}S$ as the transformation matrix $\\mathbf{T}$ , which achieves the best unlearnable effect as suggested in Tabs. 1 and 7. Thus, in the GMM scenario, the class-wise transformation matrix is defined as $\\mathbf{T}_{\\boldsymbol{y}}=\\mathcal{R}_{\\boldsymbol{y}}\\boldsymbol{S}_{\\boldsymbol{y}}=\\lambda_{\\boldsymbol{y}}\\mathcal{R}_{\\boldsymbol{y}}\\in\\mathbb{R}^{d\\times d}$ , where $\\lambda_{y}\\in\\mathbb{R}$ is the scaling factor. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3. The unlearnable dataset $\\mathcal{D}_{u}$ generated using UMT on $\\mathcal{D}_{c}$ can also be represented using a GMM, i.e., $\\mathscr{D}_{u}\\sim\\mathcal{N}(y\\mathbf{T}_{\\boldsymbol{y}}\\mu,\\lambda_{\\boldsymbol{y}}^{2}\\pmb{I})$ . ", "page_idx": 4}, {"type": "text", "text": "Proof: See Appendix D.1. Lemma 3 demonstrates that the unlearnable dataset $\\mathcal{D}_{u}$ can also be represented as a GMM, which is derived from Property 1. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4. The Bayes optimal decision boundary for classifying $\\mathcal{D}_{u}$ is given by $P_{u}(x)\\equiv\\mathcal{A}x^{\\top}x+$ $\\boldsymbol{{B}}^{\\top}\\boldsymbol{{x}}+\\boldsymbol{{\\mathcal{C}}}=\\boldsymbol{0},$ , where $\\pmb{\\mathcal{A}}=\\lambda_{-1}^{-2}-\\lambda_{1}^{-2}$ , $\\mathcal{B}=2(\\lambda_{-1}^{-2}\\mathbf{T}_{-1}+\\lambda_{1}^{-2}\\mathbf{T}_{1})\\mu_{2}$ , and $\\begin{array}{r}{\\mathcal{C}=\\ln\\frac{|\\lambda_{-1}^{2}I|}{|\\lambda_{1}^{2}I|}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof: See Appendix D.2. Lemma 4 reveals that the Bayesian decision boundary for classifying $\\mathcal{D}_{u}$ is a quadratic surface based on the GMM expression of $\\mathcal{D}_{u}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 5. Let $z\\sim\\mathcal{N}(0,I)$ , $Z=z^{\\top}z+b^{\\top}z+c,$ , where $\\begin{array}{r}{b=\\frac{\\mathcal{B}}{A},c=\\frac{\\mathcal{C}}{A}}\\end{array}$ , and $\\lVert\\cdot\\rVert_{2}$ denote 2-norm of vectors. For any $t\\geq0$ and $\\gamma\\in\\mathbb{R}$ , we employ Chernoff bound to have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\{Z\\ge\\mathbb{E}[Z]+\\gamma\\}\\le\\frac{\\exp\\left\\{\\frac{t^{2}}{2(1-2t)}||b||_{2}^{2}-t(\\gamma+d)\\right\\}}{|(1-2t)I|^{\\frac{1}{2}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof: See Appendix D.3. Lemma 5 enables us to establish an upper bound on the accuracy of the unlearnable decision boundary $P_{u}$ applied to the clean dataset $\\mathcal{D}_{c}$ , denoted as $\\tau_{D_{c}}(P_{u})$ , as presented in Theorem 6 below. ", "page_idx": 5}, {"type": "text", "text": "Theorem 6. For any constant $t_{1}$ and $t_{2}$ satisfying $0\\leq t_{1}<\\frac{1}{2}$ and $0\\leq t_{2}<\\frac{1}{2}$ , the accuracy of the unlearnable decision boundary $P_{u}$ on $\\mathcal{D}_{c}$ can be upper-bounded as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{\\mathcal{D}_{c}}(P_{u})\\leq\\frac{\\exp\\big\\{\\frac{t_{1}^{2}}{2(1-2t_{1})}||b+2\\mu||_{2}^{2}+t_{1}(\\mu^{\\top}\\mu+b^{\\top}\\mu+c)\\big\\}}{2|(1-2t_{1})I|^{\\frac{1}{2}}}}\\\\ &{\\qquad\\qquad+\\,\\frac{\\exp\\big\\{\\frac{t_{2}^{2}}{2(1-2t_{2})}||b-2\\mu||_{2}^{2}-t_{2}(\\mu^{\\top}\\mu-b^{\\top}\\mu+c+2d)\\big\\}}{2|(1-2t_{2})I|^{\\frac{1}{2}}}}\\\\ &{\\qquad\\qquad:=p_{1}+p_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore, if $\\dot{\\pi}^{\\top}\\mu+b^{\\top}\\mu+c+d<0$ and $-\\mu^{\\top}\\mu+b^{\\top}\\mu-c-d<0,$ , we have $\\tau_{D_{c}}(P_{u})<1$ . Moreover, for any $\\mu\\neq0$ , \u2203matrix $\\mathbf{T}_{i}$ such that $\\tau_{D_{c}}(P_{u})<\\tau_{D_{c}}(P)$ , where $P$ is the Bayes optimal decision boundary for classifying $\\mathcal{D}_{c}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof: See Appendix D.4. The unlearnable effect takes place when $\\tau_{D_{c}}(P_{u})<\\tau_{D_{c}}(P)$ . To achieve this, we elaborately choose Ty, which is formalized as \u00b5\u22a4\u03bb\u2212\u221221\u03bbT\u2212\u22a4\u221221\u2212+\u03bb\u03bb\u22121\u221222T1\u22a4\u00b5 . Therefore, Theorem 6 theoretically explains why UMT is effective in generating unlearnable point cloud data. ", "page_idx": 5}, {"type": "text", "text": "3.4 Data Restoration Scheme ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To ensure that authorized users can achieve better generalization after training on unlearnable data, i.e., satisfying Eq. (2), we exploit the inverse properties of 3D transformations, presented in Property 2, to calculate the inverse matrix of ${\\bf T}_{k}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{T}_{k}^{-1}=\\prod_{i=k}^{1}\\mathcal{V}_{i}^{-1},\\quad\\forall i\\neq j,\\quad\\mathrm{s.t.}\\quad\\mathcal{V}_{i}^{-1},\\mathcal{V}_{j}^{-1}\\in\\{\\mathcal{R}^{-1},\\mathcal{S}^{-1},\\mathcal{W}^{-1},\\mathcal{H}^{-1}\\},\\mathcal{V}_{i}^{-1}\\neq\\mathcal{V}_{j}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular, we note that $\\mathcal{R}^{-1}\\,=\\,\\mathcal{R}^{T}$ , $\\begin{array}{r}{S^{-1}\\,=\\,\\frac{1}{\\lambda}I}\\end{array}$ . Afterwards, the authorized user receives a lightweight message $M$ containing class-wise parameters from the data protector through a secure channel, thereby assigning $M$ to the inverse transformation matrix in Eq. (9) for multiplying the unlearnable samples. Our proposed integral unlearnable process is illustrated in Fig. 3. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Models. Three synthetic 3D point cloud datasets, ModelNet40 [50], ModelNet10 [50], ShapeNetPart [4], and three real-world datasets including autonomous driving dataset KITTI [32] and indoor datasets ScanObjectNN [41], S3DIS [2] are used. We choose 16 widely used 3D point cloud models PointNet [35], PointNet+ $^+$ [36], DGCNN [45], PointCNN [25], PCT [16], PointConv [48], CurveNet [51], SimpleView [15], 3DGCN [26], LGR-Net [59], RIConv [57], RIConv $^{++}$ [58], PointMLP [31], PointNN [56], PointTransformerV3 [49], and SegNN [62] for evaluation of classification and semantic segmentation tasks. ", "page_idx": 5}, {"type": "text", "text": "Experimental Setup. The training process involves Adam optimizer [22], CosineAnnealingLR scheduler [30], initial learning rate of 0.001, weight decay of 0.0001. We empirically set $r_{s}$ , $r_{p}$ , $b_{l}$ , $b_{u}$ , $\\omega_{l},\\omega_{u},h_{l}$ , and $h_{u}$ to $15^{\\circ}$ , $120^{\\circ}$ , 0.6, 0.8, $0^{\\circ}$ , $20^{\\circ}$ , 0, and 0.4 respectively. The main results of different $k$ on unlearnability is shown in Tab. 1. Specifically, $k=1$ uses $\\mathcal{R}$ , $k=2$ uses $\\mathcal{R}S$ , $k=3$ uses $\\mathcal{R}S\\mathcal{W}$ , and $k=4$ uses $\\mathcal{R}S\\mathcal{W}\\mathcal{H}$ . More results for different combinations of class-wise transformations are provided in Tab. 7. The table values covered by gray denote the best unlearnable effect. ", "page_idx": 5}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/3556147a4de8cc58e8f6b97502fbe50667dd7c58fdfec25b187f6277daaf5425.jpg", "table_caption": ["Table 1: Main results: The average test accuracy $(\\%)$ results with standard deviations from three runs (random seeds are set to 23, 1023, and 2023) of classification models trained on the UMT datasets "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/8f160ed7fa76c7525f3807e8bcdbe55c977b0dc4e149cc8b2a4d1fdfe98c2aeb.jpg", "table_caption": ["Table 2: Robustness results: The test accuracy $(\\%)$ results on UMT-ModelNet40 against pre-process schemes "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. For classification, we report the test accuracy (in $\\%$ ) derived from the classification accuracy on clean test set, which aligns with the metrics used in the 2D unlearnable schemes [19, 28, 39]. For semantic segmentation, we use eval accuracy and mean Intersection over Union (mIoU) as evaluation metrics (in $\\%$ ), where eval accuracy is the ratio of the number of points classified correctly to the total number of points in the point cloud, mIoU calculates the IoU for each class between the ground truth and the predicted segmentation [61], and then takes the average of these ratios. The lower these metrics are, the better the effect of the unlearnable scheme. ", "page_idx": 6}, {"type": "text", "text": "4.2 Evaluation of Proposed Unlearnable Schemes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Effectiveness. As shown in Tab. 1, UMT results in a significant decrease in test accuracy compared to clean baseline, indicating the unlearnable effectiveness of UMT. Moreover, all values of $k$ achieve good unlearnable results. The average performance of $\\mathcal{R}S$ is the best, which is employed as the default in the remaining experiments. We note that rigid transformations are easily defeated by invariance networks [10, 26, 58]. Therefore, in practical settings, we will include non-rigid transformations, using combined transformations to enhance the robustness of the UMT scheme. ", "page_idx": 6}, {"type": "image", "img_path": "SeefZa7Vmq/tmp/c586c4da7c91992c5784a012ea50215b506d6ae5fc459825782fe45f79e07af4.jpg", "img_caption": ["Figure 4: The test accuracy $(\\%)$ results obtained after training on the clean, UMT, and restoration datasets "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Robustness. (i) Data augmentations. Similar to [18, 24], we employ data augmentations like random scaling, random jitter, random rotation, Statistic Outlier Removal (SOR) [60], and Simple Random Sampling (SRS) [53] against UMT. Tab. 2 suggests UMT is robust to random data augmentations. SOR detects and removes outliers or noisy points but is ineffective in countering UMT because UMT does not introduce irregular perturbations or add outliers. SRS randomly selects a small subset from the entire set of points with equal probability. UMT is robust to SRS as it alters the coordinates of all points. Even if an arbitrary subset is chosen, all points within the subset have already undergone the unlearnable transformations. (ii) Adaptive attack. We assume the unauthorized user gains knowledge about the $\\mathcal{G}_{p}$ \u2019s use of $\\mathcal{R}S$ . Thus we propose random rotation & scaling as an adaptive scheme. In Tab. 2, the adaptive scheme exhibits a higher accuracy than other schemes, confirming its effectiveness. Nonetheless, it remains $28.67\\%$ lower than clean baseline, revealing the robustness of UMT against adaptive attack. More results of adaptive attacks are provided in Appendix C.3. ", "page_idx": 6}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/c4ae2e3f100087c26747259bf3230c52affa006247b9732526de97b3e5720d2f.jpg", "table_caption": ["Table 3: Semantic segmentation: Evaluation of UMT on semantic segmentation task using S3DIS dataset "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/19ea76270c9977c74fbe9c5c886240a1f1738236833afb7c11f1741137b2ebd0.jpg", "table_caption": ["Table 4: Ablation study: The test accuracy $(\\%_{,}$ ) results derived from unlearnable data with different modules "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Visual Effect. We visualize UMT samples in Figs. 7 to 10, indicating that the unlearnable point cloud samples still retain their normal feature structure with visual rationality. ", "page_idx": 7}, {"type": "text", "text": "Evaluation of Semantic Segmentation. We evaluate UMT using common metrics for point cloud semantic segmentation tasks in Tab. 3. As can be seen, the performance of semantic segmentation of data protected by UMT significantly decreases. The underlying reason is that the DNNs learn the features of class-wise transformations and establish a new mapping, which leads to the inability of test samples without transformations to be correctly segmented by the segmentation model. ", "page_idx": 7}, {"type": "text", "text": "Evaluation of Data Restoration. We multiply UMT samples by the transformation matrix in Eq. (9). The data becomes learnable after the restoration process, with test accuracy reaching a level comparable to the clean baseline as shown in Fig. 4. This strongly validates the effectiveness of the proposed data restoration scheme. ", "page_idx": 7}, {"type": "image", "img_path": "SeefZa7Vmq/tmp/3effc42b17b5566dd76e5ea5650612788f49b1d34b13726558183f9426e96698.jpg", "img_caption": ["Figure 5: Hyper-parameter sensitivity analysis: The impact of hyperparameters $r_{s}$ , $r_{p},b_{l}$ , and $b_{u}$ on the test accuracy results $(\\%)$ on the UMT (using $\\mathcal{R}S_{.}$ ) ModelNet10 dataset "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study and Hyper-Parameter Sensitivity Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation on Rotation Module. As shown in Tab. 4, the average accuracy increases by $15.18\\%$ and $21.65\\%$ , respectively, when only using $\\boldsymbol{S}$ . This suggests the importance of class-wise rotation module. The high test accuracy demonstrated by 3DGCN [26] can be attributed to its scaling invariance, which endows it with robustness against scaling transformation. ", "page_idx": 7}, {"type": "text", "text": "Ablation on Scaling Module. As also shown in Tab. 4, the average accuracy increases by $18.80\\%$ and $16.88\\%$ when only using the rotation module, respectively. The high test accuracy achieved on RIConv [57] and LGR-Net [59] is due to the fact that both networks are rotation-invariant, thus providing resistance against rotation transformations. These ablation results furthermore emphasize the importance of incorporating more non-rigid transformations. ", "page_idx": 7}, {"type": "text", "text": "Hyper-Parameter Analysis. We analyze four hyperparameters $r_{s},\\,r_{p},\\,b_{l}$ , and $b_{u}$ in Fig. 5. The influence of $r_{s}$ and $r_{p}$ on the accuracy remains relatively small, exhibiting their best unlearnable effect when set to $15^{\\circ}$ and $120^{\\circ}$ , respectively. We attribute this to the crucial role played by the class-wise setting, while it is not highly sensitive to the size of specific values. The unlearnable effect is the best when $b_{l}$ and $b_{u}$ are set to 0.6 and 0.8, respectively. Similarly, the variations in $b_{l}$ and $b_{u}$ do not significantly alter the effect due to the class-wise setting. ", "page_idx": 7}, {"type": "image", "img_path": "SeefZa7Vmq/tmp/0ce43b0c0e14d12893c58a05417a3a282ca87621dc036a0bedcb4c7ce0e9a7cd.jpg", "img_caption": ["Figure 6: UMT in weight space. The blue arrow represents the clean training trajectory of the weights $\\theta_{i}$ at step $i$ , while the red arrows denote the UMT training trajectory. The values for plotting this figure are provided in Appendix C.6. (a) Testing on clean test set (blue and red ellipses); (b) Testing on UMT test set (green ellipse) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Insightful Analysis Into UMT ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We formalize $\\mathcal{L}(\\mathcal{D})=\\mathbb{E}_{(\\mathbf{X},\\mathbf{Y})\\sim\\mathcal{D}}[\\mathcal{L}_{c}(\\mathcal{F}\\left(\\mathbf{X};\\theta\\right),\\mathbf{Y})]$ , where $\\mathcal{L}_{c}$ is the cross-entropy loss, $\\mathbf{X},\\mathbf{Y}$ is the point cloud data sampled from dataset $\\mathcal{D}$ . We define $\\mathcal{D}_{t r}$ , $\\mathcal{D}_{u}$ , and $\\mathcal{D}_{c}$ as the training set, unlearnable test set (i.e., test set transformed by UMT), and clean test set, respectively. Thus we have the training loss $\\mathcal{L}_{t r a i n}=\\mathcal{L}(D_{t r})$ , unlearnable test loss $\\mathcal{L}_{u}=\\mathcal{L}(\\mathcal{D}_{u})$ , and clean test loss $\\mathcal{L}_{c}=\\mathcal{L}(\\mathcal{D}_{c})$ . ", "page_idx": 8}, {"type": "text", "text": "The models trained on both clean and UMT training sets exhibit low $\\mathcal{L}_{t r a i n}$ as shown in Fig. 6 (yellow ellipses), indicating the models converge well during training. Furthermore, when tested on $\\mathcal{D}_{c}$ as shown in Fig. 6 (a), the clean model (trained with clean training set) achieves a low $\\mathcal{L}_{c}$ (blue ellipse), while the UMT model (trained with UMT training set) exhibits a high $\\mathcal{L}_{c}$ (red ellipse), also supporting the unlearnable effectiveness of UMT. Fig. 6 (b) reveals that the clean model and UMT model both exhibit low $\\mathcal{L}_{u}$ (green ellipse), suggesting that they can classify UMT samples. But the mechanisms underlying the two cases of low $\\mathcal{L}_{u}$ differ. The clean one is due to that the semantics of samples can be remained by UMT and thus normally classified. The UMT one is that the UMT model learns the mapping between transformations and labels, thereby correctly predicting the samples containing the same transformations. We conclude that the clean model effectively classifies both clean and UMT samples, while the UMT model successfully classifies UMT samples (the UMT process is the same for both training and test samples) but fails to classify clean samples. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 2D Unlearnable Schemes ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The development of 2D unlearnable schemes [19, 29, 37, 39, 42, 55] has been booming. Specifically, model-dependent methods are initially proposed in abundance [9, 19, 29]. Afterwards, numerous model-agnostic methods that significantly improve the generation efficiency have surfaced [39, 42, 47]. However, due to the structural disparities between 3D point cloud data and 2D images, applying unlearnable methods directly from 2D to 3D reveals significant challenges. ", "page_idx": 8}, {"type": "text", "text": "5.2 Protecting 3D Point Cloud Data Privacy ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Some works proposed an encryption scheme based on chaotic mapping [21] or optical chaotic encryption [27], and a 3D object reconstruction technique was introduced [34], both achieving privacy protection for individual 3D point cloud data. Nevertheless, no privacy-preserving solution has been proposed specifically for the scenario of unauthorized DNN learning on abundant raw 3D point cloud data. It is worth mentioning that both parallel works [44, 63] study availability poisoning attacks against 3D point cloud networks, which largely reduce model accuracy, and both have the potential to be applied as unlearnable schemes. However, the feature collision error-minimization poisoning scheme proposed by Zhu et al. [63] overlooks the problem of effective training for authorized users, which limits its practical use in real-world applications. The rotation-based poisoning approach proposed by Wang et al. [44] is easily defeated by rotation-invariant networks [57, 58, 59], as revealed in Tabs. 1 and 4. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion, Limitation, and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this research, we propose the first integral unlearnable framework in 3D point clouds, which utilizes class-wise multi transformations, preventing unauthorized deep learning while allowing authorized training. Extensive experiments on synthetic and real-world datasets and theoretical evidence verify the superiority of our framework. The transformations include rotation, scaling, twisting, and shear, which are all common 3D transformation operations. If unauthorized users design a network that is invariant to all these transformations, they could potentially defeat our proposed UMT. So far, only networks invariant to rigid transformations like rotation and scaling have been proposed, while networks invariant to non-rigid transformations like twisting and shear, have not yet been introduced. Therefore, our research also contributes to the design of more transformation-invariant networks. ", "page_idx": 9}, {"type": "text", "text": "Our research calls for the design of more robust point cloud networks, which helps improve the robustness and security of 3D point cloud processing systems. On the other hand, if our proposed UMT scheme is maliciously exploited, it may have negative impacts on society, such as causing a sharp decline in the performance of models trained on it, affecting the security and reliability of technologies based on 3D point cloud networks. More transformation-invariant 3D point cloud networks need to be proposed in the future to avoid potential negative impacts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Shengshan\u2019s work is supported by the National Natural Science Foundation of China (Grant Nos.   \nU20A20177, 62372196). Minghui Li is the corresponding author. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Evangelos Alexiou, Nanyang Yang, and Touradj Ebrahimi. Pointxr: A toolbox for visualization and subjective evaluation of point clouds in virtual reality. In Proceedings of the 12th International Conference on Quality of Multimedia Experience (QoMEX\u201920), pages 1\u20136, 2020. 1   \n[2] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201916), pages 1534\u20131543, 2016. 6, 18   \n[3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. SemanticKITTI: A dataset for semantic scene understanding of lidar sequences. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV\u201919), pages 9297\u20139307, 2019. 19   \n[4] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich 3D model repository. arXiv preprint arXiv:1512.03012, 2015. 6, 18, 21, 22   \n[5] Sizhe Chen, Geng Yuan, Xinwen Cheng, Yifan Gong, Minghai Qin, Yanzhi Wang, and Xiaolin Huang. Self-ensemble protection: Training checkpoints are good data protectors. In Proceedings of the 11th International Conference on Learning Representations (ICLR\u201923), 2023. 2   \n[6] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3D object detection network for autonomous driving. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201917), pages 1907\u20131915, 2017. 1   \n[7] Wenda Chu, Linyi Li, and Bo Li. Tpc: Transformation-specific smoothing for point cloud models. In Proceedings of the 39th International Conference on Machine Learning (ICML\u201922), pages 4035\u20134056, 2022. 2, 3, 15, 16   \n[8] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and Tom Goldstein. Adversarial examples make strong poisons. In Proceedings of the 35th Neural Information Processing Systems (NeurIPS\u201921), pages 30339\u201330351, 2021. 3   \n[9] Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, and Dacheng Tao. Robust unlearnable examples: Protecting data against adversarial learning. In Proceedings of the 10th International Conference on Learning Representations (ICLR\u201922), 2022. 1, 2, 4, 9   \n[10] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE (3)-transformers: 3D roto-translation equivariant attention networks. In Proceedings of the 34th Neural Information Processing Systems (NeurIPS\u201920), volume 33, pages 1970\u20131981, 2020. 7   \n[11] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-transformers: 3D roto-translation equivariant attention networks. In Proceedings of the 34th Neural Information Processing Systems (NeurIPS\u201920), volume 33, pages 1970\u20131981, 2020. 20   \n[12] Cong Gao, Geng Wang, Weisong Shi, Zhongmin Wang, and Yanping Chen. Autonomous driving security: State of the art and challenges. IEEE Internet of Things Journal, 9(10):7572\u20137595, 2021. 1   \n[13] Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, and Shu-Tao Xia. Imperceptible and robust backdoor attack in 3D point cloud. IEEE Transactions on Information Forensics and Security (TIFS\u201923), 19:1267\u20131282, 2023. 3   \n[14] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, pages 665\u2013673, 2020. 3   \n[15] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with a simple and effective baseline. In Proceedings of the 38th International Conference on Machine Learning (ICML\u201921), pages 3809\u20133820. PMLR, 2021. 6   \n[16] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, and Shi-Min Hu. Pct: Point cloud transformer. Computational Visual Media, 7:187\u2013199, 2021. 6, 17   \n[17] Shengshan Hu, Wei Liu, Minghui Li, Yechao Zhang, Xiaogeng Liu, Xianlong Wang, Leo Yu Zhang, and Junhui Hou. Pointcrt: Detecting backdoor in 3D point cloud via corruption robustness. In Proceedings of the 31st ACM International Conference on Multimedia (MM\u201923), pages 666\u2013675, 2023. 2, 17   \n[18] Shengshan Hu, Junwei Zhang, Wei Liu, Junhui Hou, Minghui Li, Leo Yu Zhang, Hai Jin, and Lichao Sun. Pointca: Evaluating the robustness of 3D point cloud completion models against adversarial examples. In Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI\u201923), pages 872\u2013880, 2023. 7   \n[19] Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, and Yisen Wang. Unlearnable examples: Making personal data unexploitable. In Proceedings of the 9th International Conference on Learning Representations (ICLR\u201921), 2021. 1, 2, 3, 4, 7, 9   \n[20] Adel Javanmard and Mahdi Soltanolkotabi. Precise statistical analysis of classification accuracies for adversarial training. The Annals of Statistics, 50(4):2127\u20132156, 2022. 2, 3   \n[21] Xin Jin, Zhaoxing Wu, Chenggen Song, Chunwei Zhang, and Xiaodong Li. 3D point cloud encryption through chaotic mapping. In Proceedings of the 17th Pacific Rim Conference on Multimedia (PCM\u201916), pages 119\u2013129, 2016. 9   \n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6, 17   \n[23] Peiliang Li, Siqi Liu, and Shaojie Shen. Multi-sensor 3D object box refinement for autonomous driving. arXiv preprint arXiv:1909.04942, 2019. 1   \n[24] Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, and Joey Tianyi Zhou. Pointba: Towards backdoor attacks in 3D point cloud. In Proceedings of the 18th IEEE/CVF International Conference on Computer Vision (ICCV\u201921), pages 16492\u201316501, 2021. 7   \n[25] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Proceedings of the 32nd Neural Information Processing Systems (NeurIPS\u201918), pages 828\u2013838, 2018. 6   \n[26] Zhi-Hao Lin, Sheng-Yu Huang, and Yu-Chiang Frank Wang. Convolution in the cloud: Learning deformable kernels in 3D graph convolution networks for point cloud analysis. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201920), pages 1800\u20131809, 2020. 6, 7, 8, 19   \n[27] Bocheng Liu, Yongxiang Liu, Yiyuan Xie, Xiao Jiang, Yichen Ye, Tingting Song, Junxiong Chai, Meng Liu, Manying Feng, and Haodong Yuan. Privacy protection for 3D point cloud classification based on an optical chaotic encryption scheme. Optics Express, 31(5):8820\u20138843, 2023. 9   \n[28] Shuang Liu, Yihan Wang, and Xiao-Shan Gao. Game-theoretic unlearnable example generator. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI\u201924), 2024. 1, 2, 3, 7   \n[29] Yixin Liu, Kaidi Xu, Xun Chen, and Lichao Sun. Stable unlearnable example: Enhancing the robustness of unlearnable examples via stable error-minimizing noise. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI\u201924), pages 3783\u20133791, 2024. 1, 2, 9   \n[30] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 6, 17   \n[31] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. arXiv preprint arXiv:2202.07123, 2022. 6   \n[32] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201915), pages 3061\u20133070, 2015. 1, 6   \n[33] Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More data can help, double descend, or hurt generalization. In Uncertainty in Artificial Intelligence, pages 129\u2013139. PMLR, 2021. 2, 3   \n[34] Arpit Nama, Amaya Dharmasiri, Kanchana Thilakarathna, Albert Zomaya, and Jaybie Agullo de Guzman. User configurable 3D object regeneration for spatial privacy. arXiv preprint arXiv:2108.08273, 2021. 9   \n[35] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3D classification and segmentation. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201917), pages 652\u2013660, 2017. 4, 6   \n[36] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet $^{++}$ : Deep hierarchical feature learning on point sets in a metric space. In Proceedings of the 31st Neural Information Processing Systems (NeurIPS\u201917), pages 5099\u20135108, 2017. 6, 8   \n[37] Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, and Jiliang Tang. Transferable unlearnable examples. In Proceedings of the 11th International Conference on Learning Representations (ICLR\u201923), 2023. 9   \n[38] Douglas A. Reynolds. Gaussian mixture models. Encyclopedia of Biometrics, 741(659-663), 2009. 2, 3   \n[39] Vinu Sankar Sadasivan, Mahdi Soltanolkotabi, and Soheil Feizi. CUDA: Convolution-based unlearnable datasets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201923), pages 3862\u20133871, 2023. 1, 2, 3, 4, 7, 9   \n[40] StubbornAtom. Distributions of quadratic form of a normal random variable. Cross Validated, 2020. https://stats.stackexchange.com/q/478682. 24   \n[41] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In Proceedings of the 17th IEEE/CVF International Conference on Computer Vision (ICCV\u201919), 2019. 6, 18, 21   \n[42] Xianlong Wang, Shengshan Hu, Minghui Li, Zhifei Yu, Ziqi Zhou, and Leo Yu Zhang. Corrupting convolution-based unlearnable datasets with pixel-based image transformations. arXiv preprint arXiv:2311.18403, 2023. 1, 2, 3, 9   \n[43] Xianlong Wang, Shengshan Hu, Yechao Zhang, Ziqi Zhou, Leo Yu Zhang, Peng Xu, Wei Wan, and Hai Jin. ECLIPSE: Expunging clean-label indiscriminate poisons via sparse diffusion purification. In Proceedings of the 29th European Symposium on Research in Computer Security (ESORICS\u201924), pages 146\u2013166, 2024. 2   \n[44] Xianlong Wang, Minghui Li, Peng Xu, Wei Liu, Leo Yu Zhang, Shengshan Hu, and Yanjun Zhang. PointAPA: Towards availability poisoning attacks in 3D point clouds. In Proceedings of the 29th European Symposium on Research in Computer Security (ESORICS\u201924), pages 125\u2013145, 2024. 2, 3, 4, 9   \n[45] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions On Graphics (TOG\u201919), pages 1\u201312, 2019. 6   \n[46] Florian Wirth, Jannik Quehl, Jeffrey Ota, and Christoph Stiller. Pointatme: efficient 3D point cloud labeling in virtual reality. In Proceedings of the 2019 IEEE Intelligent Vehicles Symposium (IV\u201919), pages 1693\u20131698, 2019. 1   \n[47] Shutong Wu, Sizhe Chen, Cihang Xie, and Xiaolin Huang. One-pixel shortcut: on the learning preference of deep neural networks. In Proceedings of the 11th International Conference on Learning Representations (ICLR\u201923), 2023. 9   \n[48] Wenxuan Wu, Zhongang Qi, and Fuxin Li. Pointconv: Deep convolutional networks on 3D point clouds. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201919), pages 9621\u20139630, 2019. 6   \n[49] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. arXiv preprint arXiv:2312.10035, 2023. 6, 8   \n[50] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3D shapenets: A deep representation for volumetric shapes. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201915), pages 1912\u20131920, 2015. 4, 6, 18, 21, 23   \n[51] Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, and Weidong Cai. Walk in the cloud: Learning curves for point clouds shape analysis. In Proceedings of the 18th IEEE/CVF International Conference on Computer Vision (ICCV\u201921), pages 915\u2013924, 2021. 6   \n[52] Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, and George Kesidis. A backdoor attack against 3D point cloud classifiers. In Proceedings of the 18th IEEE/CVF International Conference on Computer Vision (ICCV\u201921), pages 7597\u20137607, 2021. 17   \n[53] Jiancheng Yang, Qiang Zhang, Rongyao Fang, Bingbing Ni, Jinxian Liu, and Qi Tian. Adversarial attack and defense on point sets. arXiv preprint arXiv:1902.10899, 2019. 7   \n[54] Jingwen Ye and Xinchao Wang. Ungeneralizable examples. In Proceedings of the 2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201924), 2024. 2   \n[55] Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yugang Jiang, Yaowei Wang, and Changsheng Xu. Unlearnable clusters: Towards label-agnostic unlearnable examples. In Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR\u201923), 2023. 9   \n[56] Renrui Zhang, Liuhui Wang, Yali Wang, Peng Gao, Hongsheng Li, and Jianbo Shi. Parameter is not all you need: Starting from non-parametric networks for 3D point cloud analysis. arXiv preprint arXiv:2303.08134, 2023. 6   \n[57] Zhiyuan Zhang, Binh-Son Hua, David W. Rosen, and Sai-Kit Yeung. Rotation invariant convolutions for 3D point clouds deep learning. In Proceedings of the 2019 International Conference on 3D Vision (3DV\u201919), pages 204\u2013213. IEEE, 2019. 6, 8, 9, 19   \n[58] Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Riconv $^{\\downarrow++}$ : Effective rotation invariant convolutions for 3D point clouds deep learning. International Journal of Computer Vision (IJCV\u201922), 130(5):1228\u20131243, 2022. 6, 7, 9, 19   \n[59] Chen Zhao, Jiaqi Yang, Xin Xiong, Angfan Zhu, Zhiguo Cao, and Xin Li. Rotation invariant point cloud classification: Where local geometry meets global topology. arXiv preprint arXiv:1911.00195, 2019. 6, 8, 9, 19   \n[60] Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou, and Nenghai Yu. Dup-net: Denoiser and upsampler network for 3D adversarial point clouds defense. In Proceedings of the 17th IEEE/CVF International Conference on Computer Vision (ICCV\u201919), pages 1961\u20131970, 2019. 7   \n[61] Ziqi Zhou, Yufei Song, Minghui Li, Shengshan Hu, Xianlong Wang, Leo Yu Zhang, Dezhong Yao, and Hai Jin. Darksam: Fooling segment anything model to segment nothing. arXiv preprint arXiv:2409.17874, 2024. 7   \n[62] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, and Peng Gao. No time to train: Empowering non-parametric networks for few-shot 3D scene segmentation. In Proceedings of the 2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201924), 2024. 6, 8   \n[63] Yifan Zhu, Yibo Miao, Yinpeng Dong, and Xiao-Shan Gao. Toward availability attacks in 3D point clouds. arXiv preprint arXiv:2407.11011, 2024. 9 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix: Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Definitions of 3D Transformations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Existing 3D transformations are summarized in Fig. 1 and formally defined in this section. The 3D transformations are mathematical operations applied to three-dimensional objects to change their position, orientation, and scale in space, which are often represented using transformation matrices. We formally define the transformed point cloud sample with transformation matrix $\\mathbf{T}\\in\\mathbb{R}^{3\\times3}$ as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}=\\mathbf{T}\\cdot\\mathbf{X}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{X}\\in\\mathbb{R}^{3\\times p}$ is the clean point cloud sample, $\\mathbf{X}_{t}\\in\\mathbb{R}^{3\\times p}$ is the transformed point cloud sample. ", "page_idx": 13}, {"type": "text", "text": "A.1 Rotation Transformation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The rotation transformation that alters the orientation and angle of 3D point clouds is controlled by three angles $\\alpha,\\beta$ , and $\\gamma$ . The rotation matrices in three directions can be formally defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\alpha}=\\left[\\begin{array}{c c c}{1}&{0}&{0}\\\\ {0}&{\\cos\\alpha}&{-\\sin\\alpha}\\\\ {0}&{\\sin\\alpha}&{\\cos\\alpha}\\end{array}\\right],\\mathcal{R}_{\\beta}=\\left[\\begin{array}{c c c}{\\cos\\beta}&{0}&{\\sin\\beta}\\\\ {0}&{1}&{0}\\\\ {-\\sin\\beta}&{0}&{\\cos\\beta}\\end{array}\\right],\\mathcal{R}_{\\gamma}=\\left[\\begin{array}{c c c}{\\cos\\gamma}&{-\\sin\\gamma}&{0}\\\\ {\\sin\\gamma}&{\\cos\\gamma}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus we have $\\mathbf{T}=\\mathcal{R}_{\\alpha}\\mathcal{R}_{\\beta}\\mathcal{R}_{\\gamma}$ while employing rotation transformation. Besides, we have $\\mathcal{R}_{\\alpha}^{-1}=$ $\\mathcal{R}_{\\alpha}^{T},\\mathcal{R}_{\\beta}^{-1}=\\mathcal{R}_{\\beta}^{T}$ , and $\\mathcal{R}_{\\gamma}^{-1}=\\mathcal{R}_{\\gamma}^{T}$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Scaling Transformation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The scaling matrix $\\boldsymbol{S}$ can be represented as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\boldsymbol{S}}={\\left[\\begin{array}{l l l}{\\lambda}&{0}&{0}\\\\ {0}&{\\lambda}&{0}\\\\ {0}&{0}&{\\lambda}\\end{array}\\right]}=\\lambda{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the scaling factor $\\lambda$ is used to perform a proportional scaling of the coordinates of each point in the point cloud. ", "page_idx": 13}, {"type": "text", "text": "A.3 Shear Transformation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the three-dimensional space, shearing 3 is represented by different matrices, and the specific form depends on the type of shear being performed. Specifically, the shear transformation matrix $\\mathcal{H}_{x y}$ (employed in UMT) of shifting $x$ and $y$ by the other coordinate $z$ , and its corresponding inverse matrix can be expressed as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{H}_{x y}=\\left[\\!\\!\\begin{array}{c c c}{1}&{0}&{s}\\\\ {0}&{1}&{t}\\\\ {0}&{0}&{1}\\end{array}\\!\\!\\right],\\mathcal{H}_{x y}^{-1}=\\left[\\!\\!\\begin{array}{c c c}{1}&{0}&{-s}\\\\ {0}&{1}&{-t}\\\\ {0}&{0}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The shear transformation matrix $\\mathcal{H}_{x z}$ of shifting $x$ and $z$ by the other coordinate $y$ , and its corresponding inverse matrix can be expressed as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{H}_{x z}=\\left[\\!\\!\\begin{array}{c c c}{1}&{s}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{t}&{1}\\end{array}\\!\\!\\right],\\mathcal{H}_{x z}^{-1}=\\left[\\!\\!\\begin{array}{c c c}{1}&{-s}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{-t}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The shear transformation matrix $\\mathcal{H}_{y z}$ of shifting $y$ and $z$ by the other coordinate $x$ , and its corresponding inverse matrix can be expressed as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{H}_{y z}=\\left[\\!\\!\\begin{array}{l l l}{1}&{0}&{0}\\\\ {s}&{1}&{0}\\\\ {t}&{0}&{1}\\end{array}\\!\\!\\right],\\mathcal{H}_{y z}^{-1}=\\left[\\!\\!\\begin{array}{l l l}{1}&{0}&{0}\\\\ {-s}&{1}&{0}\\\\ {-t}&{0}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Our proposed UMT scheme ", "page_idx": 14}, {"type": "text", "text": "Input: Clean 3D point cloud dataset $\\mathcal{D}_{c}=\\{(\\mathbf{X}_{i},\\mathbf{Y}_{i})\\}_{i=1}^{n}$ ; number of categories $N$ ; slight range $r_{s}$ ; primary range $r_{p}$ ; scaling lower bound $b_{l}$ ; scaling upper bound $b_{u}$ ; twisting lower bound $w_{l}$ ; twisting upper bound $w_{u}$ ; shear lower bound $h_{l}$ ; shear upper bound $h_{u}$ ; spectrum of transformations $k$ ; matrix set $\\mathcal{T}_{s}=\\{\\mathcal{R},\\mathcal{S},\\mathcal{H},\\mathcal{W}\\}$ . ", "page_idx": 14}, {"type": "text", "text": "Output: Unlearnable 3D point cloud dataset $\\mathcal D_{u}=\\{(\\mathbf X_{{\\mathbf u}\\it{i}},\\mathbf Y_{i})\\}_{i=1}^{n}$ . Initialize the slight angle lists $\\scriptstyle{\\mathcal{L}}_{\\alpha}=[\\mathbf{\\nabla}]$ , $\\scriptstyle{\\mathcal{L}}_{\\beta}=[\\;]$ , the primary angle list $\\scriptstyle{\\mathcal{L}}_{\\gamma}=[\\;]$ , the rotation list $\\mathcal{L}_{\\mathcal{R}}{=}[\\ ]$ , the scaling list $\\mathcal{L}_{S}{=}[\\ ]$ , the twisting list $\\scriptstyle{\\mathcal{L}}_{\\mathcal{W}}=\\left[\\begin{array}{l}{\\mathbf{\\boldsymbol{\\l}}}\\end{array}\\right]$ , the shear list $\\scriptstyle{\\mathcal{L}}_{\\mathcal{H}}=\\left[\\begin{array}{l}{\\mathbf{1}}\\end{array}\\right]$ , and $\\textstyle A_{N}=\\left\\lceil{\\sqrt[3]{N}}\\right\\rceil$ ; ", "page_idx": 14}, {"type": "text", "text": "for $c=1$ to $k$ do Randomly sample a transformation matrix $\\mathcal{V}_{c}\\in\\mathcal{T}_{s}$ ; Remove transformation matrix $\\nu_{c}$ from $\\tau_{s}$ ; if $\\mathcal{V}_{c}==\\mathcal{R}$ then for $i=1$ to $\\boldsymbol{A}_{N}$ do for $j=1$ to $\\boldsymbol{A}_{N}$ do for $k=1$ to $\\boldsymbol{A}_{N}$ do $\\mid\\;\\;{\\mathcal{L}}_{\\mathcal{R}}\\gets{\\mathcal{L}}_{\\mathcal{R}}\\cup\\{[{\\mathcal{L}}_{\\alpha}[i],{\\mathcal{L}}_{\\beta}[j],{\\mathcal{L}}_{\\gamma}[k]]\\};$ end end end Get the $\\mathcal{L}_{\\mathcal{R}}\\gets r a n d o m.s a m p l e(\\mathcal{L}_{\\mathcal{R}},N);$ end else if $\\mathcal{V}_{c}==\\mathcal{S}$ then for $i=1$ to $N$ do Randomly sample $\\lambda_{i}\\sim\\mathcal{U}(b_{l},b_{u})$ ; Add to the list $\\mathcal{L}_{S}\\leftarrow\\mathcal{L}_{S}\\cup\\{\\lambda_{i}\\}$ ; end end else if $\\mathcal{V}_{c}==\\mathcal{W}$ then for $i=1$ to $N$ do Randomly sample $\\omega_{i}\\sim\\mathcal{U}(w_{l},w_{u})$ ; Add to the list $\\mathcal{L}_{\\mathcal{W}}\\leftarrow\\mathcal{L}_{\\mathcal{W}}\\cup\\{\\omega_{i}\\}$ ; end end else if $\\mathscr{V}_{c}==\\mathscr{H}$ then for $i=1$ to $N$ do Randomly sample $h_{i}\\sim\\mathcal{U}(h_{l},h_{u})$ ; Add to the list $\\mathcal{L}_{\\mathcal{H}}\\leftarrow\\mathcal{L}_{\\mathcal{H}}\\cup\\{h_{i}\\}$ ; end end ", "page_idx": 14}, {"type": "text", "text": "for $i=1$ to $n$ do Get the transformation matrix $\\begin{array}{r}{\\mathbf{T}_{k}=\\prod_{i=1}^{k}\\mathcal{V}_{i}}\\end{array}$ by the parameter lists above; Get the transformed data $\\mathbf{X}_{\\mathbf{u}i}=\\mathbf{T}_{k i}\\cdot\\dot{\\mathbf{X}}_{i}$ ; ", "page_idx": 14}, {"type": "text", "text": "end ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Return: Unlearnable 3D point cloud dataset $\\mathcal{D}_{u}$ ", "page_idx": 14}, {"type": "text", "text": "A.4 Twisting Transformation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The 3D twisting transformation [7] involves a rotational deformation applied to an object in threedimensional space, creating a twisted or spiraled effect. Unlike simple rotations around fixed axes, a twisting transformation introduces a variable rotation that may change based on the spatial coordinates of the object. For instance, considering a twisting transformation along the $z$ -axis, where the rotation angle is a function related to the $z$ -coordinate, it can be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{W}_{z}(\\theta,z)=\\left[\\!\\!\\begin{array}{c c c}{\\cos(\\theta z)}&{-\\sin(\\theta z)}&{0}\\\\ {\\sin(\\theta z)}&{\\cos(\\theta z)}&{0}\\\\ {0}&{0}&{1}\\end{array}\\!\\!\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\theta$ is the parameter of the twisting transformation, and $z$ is the $z$ -coordinate of the object. The inverse matrix of $\\textstyle\\mathcal{W}_{z}(\\theta,z)$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{W}_{z}^{-1}(\\theta,z)=\\left[\\!\\!\\begin{array}{c c c}{\\cos(\\theta z)}&{\\sin(\\theta z)}&{0}\\\\ {-\\sin(\\theta z)}&{\\cos(\\theta z)}&{0}\\\\ {0}&{0}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.5 Tapering Transformation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The tapering transformation [7] is a linear transformation used to alter the shape of an object, causing it to gradually become pointed or shortened. In three-dimensional space, tapering transformation can adjust the dimensions of an object along one or more axes, creating a tapering effect. The specific matrix representation of tapering transformation depends on the chosen axis and the design of the transformation. Generally, tapering transformation can be represented by a matrix that is multiplied by the coordinates of the object to achieve the shape adjustment, which is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{A}_{z}(\\eta,z)=\\left[\\begin{array}{c c c}{1+\\eta z}&{0}&{0}\\\\ {0}&{1+\\eta z}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $z$ is the $z$ -coordinate of the object. Considering that $\\eta z$ could indeed equal to $^-1$ , in such a case, the 3D point cloud samples would be projected onto the $z$ -plane, losing their practical significance and the tapering matrix is also irreversible. ", "page_idx": 15}, {"type": "text", "text": "A.6 Reflection Transformation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Reflection transformation is a linear transformation that inverts an object along a certain plane. This plane is commonly referred to as a reflection plane or mirror. For reflection transformations in threedimensional space, we can represent them through a matrix. Regarding the reflection transformation matrices of the $x y$ plane, $y z$ plane, and $x z$ plane, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{x y}=\\left[\\!\\!\\begin{array}{c c c}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{-1}\\end{array}\\!\\!\\right],\\mathcal{R}_{y z}=\\left[\\!\\!\\begin{array}{c c c}{-1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\!\\!\\right],\\mathcal{R}_{x z}=\\left[\\!\\!\\begin{array}{c c c}{1}&{0}&{0}\\\\ {0}&{-1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.7 Translation Transformation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The 3D translation transformation4 refers to the process of moving an object in three-dimensional space. This transformation involves moving the object along the $x,\\,y$ , and $z$ axes, respectively, smoothly transitioning it from one position to another, which is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\left[\\!\\!\\begin{array}{c c c c}{1}&{0}&{0}&{t_{x}}\\\\ {0}&{1}&{0}&{t_{y}}\\\\ {0}&{0}&{1}&{t_{z}}\\\\ {0}&{0}&{0}&{1}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $t_{x},\\,t_{y}$ , and $t_{z}$ represents the translation along the $x,\\,y$ , and $z$ axes, respectively. This $4\\!\\times\\!4$ matrix is a homogeneous coordinate matrix that describes the translation in three-dimensional space. Additionally, the translation matrix also can be represented as a additive matrix to original point $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{3\\times3}$ , which can be defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\left[\\!\\!\\begin{array}{l l l}{t_{x}}&{t_{x}}&{t_{x}}\\\\ {t_{y}}&{t_{y}}&{t_{y}}\\\\ {t_{z}}&{t_{z}}&{t_{z}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Supplementary Experimental Settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Experimental Platform ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our experiments are conducted on a server running a 64-bit Ubuntu 20.04.1 system with an Intel Xeon Silver 4210R CPU $@$ 2.40GHz processor, 125GB memory, and four Nvidia GeForce RTX 3090 GPUs, each with 24GB memory. The experiments are performed using the Python language, version 3.8.19, and PyTorch library version 1.12.1. ", "page_idx": 15}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/dd5bedaf55b1e064ee8b17f286ac48a9c6968901045f0d279a15e6a32c34b0d2.jpg", "table_caption": ["Table 5: The test accuracy $(\\%)$ results on diverse types of transformations on ModelNet10 training set using PointNet classifier "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/fef33cd73a313043f526b5d016f0325b0a101d1f5c797e5945299eb914091051.jpg", "table_caption": ["Table 6: Accuracy results obtained with different test sets when training the PointNet classifier using class-wise transformed training sets "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Hyper-Parameter Settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The model training process on the unlearnable dataset and the clean dataset remains consistent, using the Adam optimizer [22], CosineAnnealingLR scheduler [30], initial learning rate of 0.001, weight decay of 0.0001, batch size of 16 (due to insufficient GPU memory, the batch size is set to 8 when training 3DGCN on ModelNet40 dataset), and training for 80 epochs. Due to the longer training process required by PCT [16], the training epochs for PCT in Tab. 1 on ModelNet10, ModelNet40, and ScanObjectNN datasets are all set to 240. ", "page_idx": 16}, {"type": "text", "text": "B.3 Settings of Exploring Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We initially investigate which approach yields the best unlearnable effect among sample-wise, datasetwise, and class-wise settings in Fig. 2 (a). Specific experimental settings are as follows: ", "page_idx": 16}, {"type": "text", "text": "In the dataset-wise setting, the same parameter values are applied to the entire dataset. Specifically, we have $\\alpha=\\beta=\\gamma=10^{\\circ}$ in the rotation transformation, the scaling factor $\\lambda$ is set to 0.8, both shearing factors $s$ and $t$ are set to 0.2. The angle $\\theta$ in twisting is $25^{\\circ}$ . The tapering angle $\\eta$ is set to $25^{\\circ}$ , and the parameters in translation transformation $t_{x}$ , $t_{y}$ , and $t_{z}$ are set to 0.15. ", "page_idx": 16}, {"type": "text", "text": "In the sample-wise setting, each sample has its independent set of parameters, meaning the parameter values for each sample are randomly generated within a certain range. In the rotation transformation, $\\alpha,\\beta,\\gamma$ are uniformly sampled from the range of $0^{\\circ}$ to $20^{\\circ}$ . The scaling factor $\\lambda$ is uniformly sampled from 0.6 to 0.8, shearing factors $s$ and $t$ are uniformly sampled from the range of 0 to 0.4. Both the twist angle $\\theta$ and tapering angle $\\eta$ are sampled from $0^{\\circ}$ to $50^{\\circ}$ , and the parameters in translation transformation $t_{x},t_{y}$ , and $t_{z}$ are sampled from 0 to 0.3. ", "page_idx": 16}, {"type": "text", "text": "In the class-wise setting, the parameters for transformations are associated with the point cloud\u2019s class. The selection of parameters for each class is also obtained by random sampling within a fixed range. The chosen ranges are generally consistent with those described for the sample-wise setting above. However, a difference lies in the consideration of slight angle range and primary angle range in the case of rotation transformations, where these ranges are $20^{\\circ}$ and $120^{\\circ}$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "The specific results of test accuracy under different transformation modes, including sample-wise (random), class-wise, and dataset-wise (universal), are provided in Tab. 5. It can be seen that under the class-wise setting, the final unlearnable effect is the best. ", "page_idx": 16}, {"type": "text", "text": "B.4 Benchmark Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Dataset Introduction. The ModelNet40 dataset is a point cloud dataset containing 40 categories, comprising 9843 training and 2468 test point cloud data. ModelNet10 is a subset of ModelNet40 dataset with 10 categories. ShapeNetPart that includes 16 categories is a subset of ShapeNet, comprising 12137 training and 2874 test point cloud samples. ScanObjectNN is a real-world point cloud dataset with 15 categories, comprising 2309 training samples and 581 test samples. Similar to [17, 52], we split KITTI object clouds into class \u201cvehicle\u201d and \u201chuman\u201d containing 1000 training data and 662 test data. The input point cloud objects for the models encompass 256 points for the KITTI dataset and 1024 points for other datasets. The Stanford 3D Indoor Scene Dataset (S3DIS) dataset contains 6 large-scale indoor areas with 271 rooms. Each point in the scene point cloud is annotated with one of the 13 semantic categories. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Dataset Licenses. The dataset license information is listed as: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 ModelNet10, ModelNet40 [50]: All CAD models are downloaded from the Internet and the original authors hold the copyright of the CAD models. The label of the data is obtained by us via Amazon Mechanical Turk service and it is provided freely. This dataset is provided for the convenience of academic research only. Link is https://modelnet. cs.princeton.edu/.   \n\u2022 ShapeNetPart [4]: We use the ShapeNet database (the \"Database\") at Princeton University and Stanford University. Link is https://www.shapenet.org/.   \n\u2022 ScanObjectNN [41]: The license is MIT License: Copyright (c) 2019 Vision & Graphics Group, HKUST. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. Link is https://hkust-vgd.github.io/ scanobjectnn/.   \n\u2022 S3DIS [2]: The copyright is from Stanford University, Patent Pending, copyright 2016. Link is http://buildingparser.stanford.edu/dataset.html. ", "page_idx": 17}, {"type": "text", "text": "B.5 Settings of Robustness Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The scaling factor in random scaling augmentation is set to a minimum of 0.8 and a maximum of 1.25. In the random rotation operation, the three directional rotation angles are identical and uniformly sampled from $[0,2\\pi)$ . The perturbations in the random jitter are sampled from a normal distribution with a standard deviation of 0.05, and the perturbation magnitude is constrained within 0.1. The parameters $^k$ and $_{\\alpha}$ in SOR are set to 2 and 1.1, respectively. The number of dropped points in SRS is 500. ", "page_idx": 17}, {"type": "text", "text": "C Supplementary Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Results for Execution Mechanism ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present the specific experimental results of Fig. 2 (a) in Tab. 5. It can be observed that under the class-wise setting, the average test accuracy is the lowest, while the unlearnable condition yields the best performance. Furthermore, we conduct investigations into training the PointNet classifier with class-wise transformed training sets (ModelNet10 dataset), analyzing accuracy results across different test sets, as shown in Tab. 6. It is noteworthy that the accuracy on class-wise test sets (using consistent transformation parameters with class-wise transformed training set) is consistently above $90\\%$ , indicating that the model has learned the mapping between transformations and labels. This leads to correct classification on test samples with the same transformations. However, when the class-wise test set undergoes permutation, the accuracy drops to a level similar to that of the clean test set. This clearly demonstrates that the model can correctly classify samples only when they have corresponding transformations, and samples without transformations or with mismatched transformations cannot be correctly classified. This further validates that the model learns a one-to-one mapping between class transformations and labels. ", "page_idx": 17}, {"type": "text", "text": "C.2 Results of Diverse Combinations of Transformations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We investigate combinations of four transformations, i.e., rotation, scaling, shear, and twisting, and create unlearnable datasets using the class-wise setting. The test accuracy results across five point cloud models are presented in Tab. 7. It can be observed that the combination of rotation and scaling, ", "page_idx": 17}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/64acf2ea8acaa41c842616f1ef2147ef8b720da32fbf4040c0952221fd7422a6.jpg", "table_caption": ["Table 7: The test accuracy $(\\%)$ results obtained from training the point cloud classifiers PointNet, PointNet++, DGCNN, PointCNN, PCT using a ModelNet10 dataset generated with diverse combinations of transformations under a class-wise setting, where $\\mathcal{R},\\mathcal{S},\\mathcal{H}$ , and $\\mathcal{W}$ correspond to rotation, scaling, shear, and twisting respectively "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 8: The test accuracy $(\\%)$ results with standard deviations from three runs (random seeds are set to 23, 1023, and 2023) obtained from training the point cloud classifiers PointNet, PointNet $^{++}$ , DGCNN, and PointCNN using a ModelNet10 dataset generated with diverse two class-wise transformations ", "page_idx": 18}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/2cf75c66add95c45636c2bf76dc819fdeb1b40175592faaf0f390aace9a03a03.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "two types of rigid transformations, achieves the most effective unlearnable effect. The transformation parameters used in this section are consistent with Appendix B.3. ", "page_idx": 18}, {"type": "text", "text": "To ensure the reliability of the experimental results, we combine two different transformations and obtain the results from three runs using random seeds with 23, 1023, and 2023, then average them. The results are shown in Tab. 8. From the results on four popular point cloud models, it can be seen that when all transformations are rigid, the test accuracy is still the lowest. ", "page_idx": 18}, {"type": "text", "text": "C.3 More Results of UMT Against Adaptive Attacks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further explore the performance of UMT against adaptive attacks, we supplement the experimental results using four types of random augmentations $\\mathcal{R}S\\mathcal{H}\\mathcal{W}$ in Tab. 10, and find that the conclusion is consistent with Tab. 2, i.e., UMT exhibits a certain degree of robustness against the adaptive attack random transformations. ", "page_idx": 18}, {"type": "text", "text": "C.4 More Results of UMT Against Semantic Segmentation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further evaluate the UMT performance against semantic segmentation, we employ the semantic scene understanding dataset SemanticKITTI [3] in Tab. 11. It can be observed that UMT is still effective against this real-world segmentation dataset. ", "page_idx": 18}, {"type": "text", "text": "C.5 Results of UMT Against SE(3) Equivariant Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the main text, we have discussed the robustness of UMT against rotation/scaling invariant models. The results for $\\scriptstyle\\mathrm{RIConv}++$ [58] (rotation invariant) and 3DGCN [26] (scaling invariant) networks in Tab. 1, as well as RIConv [57], LGR-Net [59] (rotation invariant), and 3DGCN in Tab. 4, demonstrating that these invariant networks can indeed defend against class-wise rotation and scaling. It is worth noting that these networks, which are invariant to a single transformation, cannot defend against UMT formed by a combination of multiple transformations. Thus, it appears that networks like SE(3) equivariant model, which are invariant to multiple transformations in space, can potentially overcome UMT. Therefore, we include experimental results for the SE(3) equivariant network SE(3)-Transformer [11] in Tab. 12. ", "page_idx": 18}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/cfe3a49c0728b0416a9f53fdaaeba6b53eebb8377e8c6a40ec9945b0b01408b9.jpg", "table_caption": ["Table 9: The accuracy $(\\%)$ results on clean and UMT ModelNet10 training set and test set using four point cloud classifiers. Higher accuracy values correspond to lower cross-entropy loss values. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/36bef518099abc66047730c239056c26ccfe75d4f0836971b3d4c2207c7024e8.jpg", "table_caption": ["Table 10: Test accuracy $(\\%)$ results using UMT training data and UMT data employing random augmentations "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "However, it can be seen that SE(3)-Transformer cannot defend against UMT $\\scriptstyle\\mathrm{k}=3$ , $\\mathcal{R}S\\mathcal{W}$ ) and $\\mathrm{UMT}({\\bf k}{=}4,\\mathscr{R}S\\mathscr{W}\\mathscr{H})$ . This is because existing transformation-invariant networks, even including SE(3) invariant networks, are designed only for rigid transformations (rotation, scaling, reflection, and translation). There are currently no invariant networks proposed for non-rigid transformations like shear and twisting. Therefore, if a data protector wants UMT to be more robust, they can include non-rigid class-wise transformations to defeat existing rigid transformation-invariant networks. ", "page_idx": 19}, {"type": "text", "text": "C.6 Results of Insightful Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We train on clean training set and test on clean test set, train on clean training set and test on UMT test set, train on UMT training set and test on clean test set, and train on UMT training set and test on UMT test set to obtain the test accuracy results in Tab. 9 (we ensure that the UMT parameters for the UMT test set and UMT training set are consistent). Higher accuracy values indicate lower cross-entropy loss values, while lower accuracy values represent higher cross-entropy loss values. It can be observed that only when trained with the UMT training set, the loss after testing with the clean test set is high (i.e., low accuracy). ", "page_idx": 19}, {"type": "text", "text": "C.7 Boarder Hyper-Parameter Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Additionally, we investigate the unlearnable effects across a wider range of hype-parameters $r_{s},r_{p},b_{l},b_{u}$ in UMT ( $\\mathit{\\Theta}[k=2]$ with $\\mathcal{R}S$ ), as shown in Tab. 13. It can be observed that our UMT scheme still exhibits a good unlearnable effect, and its key lies in the crucial role played by the class-wise setting. ", "page_idx": 19}, {"type": "text", "text": "C.8 Supplementary Ablation Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we conduct ablation experiments on the UMT scheme on the ShapeNetPart dataset (keeping experimental parameters consistent with the main experiments), as shown in Tab. 14. It can be observed that whether using only the rotation module or only the scaling module, the final unlearnable effect is not as good as UMT. This clearly demonstrates that each module contributes to the overall UMT effectiveness. ", "page_idx": 19}, {"type": "text", "text": "C.9 Results of Mixture of Class-wise and Sample-wise Samples ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we investigate the test accuracy results when using a mixture of different ratios of classwise samples and sample-wise (random) samples in the dataset, as shown in Tab. 15 (experiments are conducted on the ModelNet10 dataset with 10 categories, where \u201c2 class-wise 8 sample-wise\" denotes that samples from 2 categories undergo class-wise transformations, while the remaining 8 categories undergo random transformations, and so on). We can clearly observe from the experimental results that as the proportion of class-wise transformations gradually increases, the test accuracy gradually ", "page_idx": 19}, {"type": "text", "text": "Table 11: Semantic segmentation: Evaluation of UMT on semantic segmentation task using SemanticKITTI dataset ", "page_idx": 20}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/1ab880945cdde517c58ebd20e2a1d2f7682068b821945e7eaba2c33b01023c8a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 12: Test accuracy $(\\%)$ results using UMT training data to train the SE(3)-Transformer decreases, indicating that the unlearnable effect becomes more pronounced. This strongly suggests that the class-wise setting is more effective than the sample-wise setting. ", "page_idx": 20}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/7da7ed12add0d6cc9af78e14bea0e3fb15e71555cc42c2fa3902ffef631b24fc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "SeefZa7Vmq/tmp/cdb752e2caad4d247d5b9b924f42abc6d9bf79a3b748b17500d2cdc2e1f74704.jpg", "img_caption": ["Figure 9: Clean and UMT samples from ScanObjectNN dataset "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.10 Additional Visual Presentations for 3D Point Cloud Samples ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We visualize clean point cloud samples and UMT $k=2$ using $\\mathcal{R}S$ ) point cloud samples on four benchmark datasets ModelNet10 [50], ModelNet40 [50], ScanObjectNN [41], and ShapeNetPart [4], ", "page_idx": 20}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/374bfe6076cdbc88b2470eccdee2fb8ab909697b8c5e74d58961305bc1995717.jpg", "table_caption": ["Table 13: The test accuracy $(\\%)$ results on ModelNet10 dataset with a boarder range of hype-parameters $r_{s},r_{p},b_{l},b_{u}$ "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 14: Ablation modules: The test accuracy $(\\%)$ results achieved by training on unlearnable data created by different modules on the ShapeNetPart dataset ", "page_idx": 21}, {"type": "image", "img_path": "SeefZa7Vmq/tmp/768df3599a3917697b94a86a512efa9473e8564e711f20b5ea120128efb0f822.jpg", "img_caption": ["Figure 10: Clean and UMT samples from ShapeNetPart dataset "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "as depicted in Figs. 7 to 10, respectively. The UMT parameters consist of rotation and scaling parameters $[\\alpha,\\,\\beta,\\,\\gamma,\\,\\lambda]$ . It can be observed that these unlearnable samples exhibit similar feature information to normal samples, presenting good visual effects and making it difficult to be detected as abnormalities. ", "page_idx": 21}, {"type": "text", "text": "D Proofs for Theories ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Proof for Lemma 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 3. The unlearnable dataset $\\mathcal{D}_{u}$ generated using UMT on $\\mathcal{D}_{c}$ can also be represented using a GMM, i.e., $\\mathscr{D}_{u}\\sim\\mathcal{N}(y\\mathbf{T}_{\\boldsymbol{y}}\\mu,\\lambda_{\\boldsymbol{y}}^{2}\\pmb{I})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof: Assuming $y=1$ , then $\\mathcal{D}_{c1}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{I})$ , and we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{c1}}\\mathbf{T}_{1}x=\\mathbf{T}_{1}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{c1}}x=\\mathbf{T}_{1}\\mu,}\\\\ &{\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{c1}}(\\mathbf{T}_{1}x-\\mathbf{T}_{1}\\mu)(\\mathbf{T}_{1}x-\\mathbf{T}_{1}\\mu)^{\\top}}\\\\ &{\\ =\\mathbf{T}_{1}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{c1}}(x-\\mu)(x-\\mu)^{\\top}\\mathbf{T}_{1}^{\\top}}\\\\ &{\\ =\\mathbf{T}_{1}I\\mathbf{T}_{1}^{\\top}=\\lambda_{1}{^2}\\mathcal{R}_{1}\\mathcal{R}_{1}^{\\top}I=\\lambda_{1}{^2}I}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, assuming $y=-1$ , we can obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}_{c-1}}\\mathbf{T}_{-1}\\boldsymbol{x}=-\\mathbf{T}_{-1}\\boldsymbol{\\mu},}\\\\ &{\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}_{c-1}}(\\mathbf{T}_{-1}\\boldsymbol{x}-\\mathbf{T}_{-1}\\boldsymbol{\\mu})(\\mathbf{T}_{-1}\\boldsymbol{x}-\\mathbf{T}_{-1}\\boldsymbol{\\mu})^{\\top}=\\lambda_{-1}^{\\textrm{2}}\\boldsymbol{I}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus we have: $\\mathscr{D}_{u}\\sim\\mathcal{N}(y\\mathbf{T}_{\\boldsymbol{y}}\\mu,\\lambda_{\\boldsymbol{y}}^{2}\\pmb{I})$ . ", "page_idx": 21}, {"type": "text", "text": "Table 15: Mixture results: The test accuracy $(\\%)$ results achieved by training on the mixture data consisting of class-wise UMT samples and sample-wise UMT samples ", "page_idx": 22}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/cf53d18a576bec29c0b2b5a9a31c0b3bbe38500d1803c519c52bbdb76731b16a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.2 Proof for Lemma 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 4. The Bayes optimal decision boundary for classifying $\\mathcal{D}_{u}$ is given by $P_{u}(x)\\equiv\\mathcal{A}x^{\\top}x+$ $\\boldsymbol{{B}}^{\\intercal}\\boldsymbol{{x}}+\\boldsymbol{{\\mathcal{C}}}=\\boldsymbol{0}$ , where $\\pmb{\\mathcal{A}}=\\lambda_{-1}^{-2}-\\lambda_{1}^{-2}$ , $\\mathcal{B}=2(\\lambda_{-1}^{-2}\\mathbf{T}_{-1}+\\lambda_{1}^{-2}\\mathbf{T}_{1})\\mu$ , and $\\begin{array}{r}{\\mathcal{C}=\\ln\\frac{|\\lambda_{-1}^{2}I|}{|\\lambda_{1}^{2}I|}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof: At the optimal decision boundary the probabilities of any point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ belonging to class $y=1$ and $y=-1$ modeled by $\\mathcal{D}_{u}$ are the same. Similar to the optimal decision boundary of the clean dataset $\\mathcal{D}_{c}$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{c\\cdot x^{\\prime}|-\\frac{1}{2}\\left(x-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}\\right)^{\\top}\\left(\\mathbf{\\bar{x}}_{2}^{\\top}\\cdot\\mathbf{I}\\right)^{-1}\\left(x-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}-\\mathbf{T}_{-1}\\right)}{\\sqrt{\\left(2\\pi\\right)^{d}/1}}}\\\\ &{\\quad=\\frac{c\\cdot y^{\\prime}|-\\frac{1}{2}\\left(x-\\mathbf{T}_{+1}\\cdot\\mathbf{t}_{1}\\right)^{\\top}\\left(\\mathbf{\\bar{x}}_{2}^{\\top}\\right)-\\frac{1}{2}\\left(\\mathbf{t}_{1}-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}\\right)}{\\sqrt{\\left(2\\pi\\right)^{d}/1}}}\\\\ &{\\quad\\Rightarrow\\left(x-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}\\right)^{\\top}\\left(\\mathbf{\\bar{x}}_{2}^{\\top}\\cdot\\mathbf{I}\\right)^{-1}\\left(\\mathbf{B}\\cdot\\mathbf{I}\\right)^{\\top}\\left(\\mathbf{\\bar{x}}_{2}^{\\top}\\right)}\\\\ &{\\quad=\\left(x-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}\\right)^{\\top}\\frac{1}{\\sqrt{\\lambda}}\\left(x-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}-\\mathbf{B}\\right)+\\frac{1}{2}\\mathbf{T}_{-1}^{\\top}\\left(\\mathbf{\\bar{x}}_{2}^{\\top}\\right)}\\\\ &{\\quad=\\left(x-\\mathbf{T}_{-1}\\right)\\frac{1}{\\sqrt{\\lambda}}\\left(x-\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}-\\mathbf{\\bar{x}}_{2}^{\\top}\\right)\\mathbf{T}_{-1}^{\\top}x+\\mu_{\\tau}^{\\top}\\mathbf{T}_{-1}^{\\top}\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}-\\mathbf{B}\\right)}\\\\ &{\\quad\\times\\frac{1}{\\lambda}^{-2}\\left(x^{\\top}x-x^{\\top}\\mathbf{T}_{-1}\\cdot\\mathbf{t}_{1}-\\mu_{\\tau}^{\\top}\\mathbf{T}_{-1}^{\\top}x+\\mu_{\\tau}^{\\top}\\mathbf{T}_{\\top}^{\\top}\\mathbf{T}_{1}\\mu_{1}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pmb{\\mathcal{A}}=\\lambda_{-1}^{-2}-\\lambda_{1}^{-2}$ , $\\mathcal{B}=2(\\lambda_{-1}^{-2}\\mathbf{T}_{-1}+\\lambda_{1}^{-2}\\mathbf{T}_{1})\\mu$ , and $\\begin{array}{r}{\\mathcal{C}=\\ln\\frac{|\\lambda_{-1}^{2}I|}{|\\lambda_{1}^{2}I|}}\\end{array}$ . Besides, note that if $P_{u}(x)$ is less than 0, the category of the Bayesian optimal classification is $^-1$ ; otherwise, it is $^{1}$ . ", "page_idx": 22}, {"type": "text", "text": "D.3 Proof for Lemma 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 5. Let $z\\sim\\mathcal{N}(0,I)$ , $Z=z^{\\top}z+b^{\\top}z+c$ , and $\\left\\|\\cdot\\right\\|_{2}$ denote 2-norm of vectors. For any $t\\geq0$ and $\\gamma\\in\\mathbb{R}$ , we use Chernoff bound to have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\{Z\\ge\\mathbb{E}[Z]+\\gamma\\}\\le\\frac{\\exp\\left\\{\\frac{t^{2}}{2(1-2t)}||b||_{2}^{2}-t(\\gamma+d)\\right\\}}{|(1-2t)I|^{\\frac{1}{2}}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof: Since $\\boldsymbol{\\mathcal{A}}$ is a constant, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle P_{u}({\\boldsymbol{x}})\\equiv{\\boldsymbol{x}}^{\\top}{\\boldsymbol{x}}+(\\frac{\\mathcal{B}}{\\mathcal{A}})^{\\top}{\\boldsymbol{x}}+\\frac{\\mathcal{C}}{\\mathcal{A}}=0}\\\\ {\\Rightarrow P_{u}({\\boldsymbol{x}})\\equiv{\\boldsymbol{x}}^{\\top}{\\boldsymbol{x}}+{\\boldsymbol{b}}^{\\top}{\\boldsymbol{x}}+{\\boldsymbol{c}}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{b=\\frac{\\mathcal{B}}{A},c=\\frac{\\mathcal{C}}{A}}\\end{array}$ . Let $Z=z^{\\top}z+b^{\\top}z+c$ and $z\\sim\\mathcal{N}(0,I)\\subset\\mathbb{R}^{d}$ . Thus we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{Z=z^{\\top}z+b^{\\top}z+c=(z^{\\top}+\\frac{1}{2}b^{\\top})(z+\\frac{1}{2}b)+c-\\frac{1}{4}b^{\\top}b}}\\\\ {{=(z+\\frac{1}{2}b)^{\\top}(z+\\frac{1}{2}b)+c-\\frac{1}{4}b^{\\top}b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any $t\\geq0$ and $x\\sim\\mathcal{N}(0,I)\\,$ , we write the moment generating function for a quadratic random variable $Y=x^{\\top}x\\;\\mathrm{as}^{5}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\exp(t Y)]=\\frac{1}{(2\\pi)^{d/2}}\\int_{\\mathbb{R}^{d}}\\exp\\left\\{t x^{\\top}x\\right\\}\\exp\\left\\{-\\frac{1}{2}(x-\\mu)^{\\top}(x-\\mu)\\right\\}d x}\\\\ &{=\\frac{\\exp\\big\\{-\\mu^{\\top}\\mu/2\\big\\}}{(2\\pi)^{d/2}}\\int_{\\mathbb{R}^{d}}\\exp\\left\\{\\frac{2t-1}{2}x^{\\top}x+\\mu^{\\top}x\\right\\}d x}\\\\ &{=\\frac{\\exp\\big\\{-\\mu^{\\top}\\mu/2\\big\\}}{(2\\pi)^{d/2}}\\frac{\\left(2\\pi\\right)^{d/2}\\exp\\big\\{\\frac{1}{2(1-2t)}\\mu^{\\top}\\mu\\big\\}}{\\left|I-2t I\\right|^{\\frac{1}{2}}}}\\\\ &{=\\frac{\\exp\\big\\{\\frac{t}{1-2t}\\mu^{\\top}\\mu\\big\\}}{\\left|I-2t I\\right|^{\\frac{1}{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Longrightarrow\\mathbb{E}[\\exp(t Z)]=\\frac{\\exp\\left\\{\\frac{t}{4(1-2t)}b^{\\top}b+t(c-\\frac{1}{4}b^{\\top}b)\\right\\}}{|(1-2t)I|^{\\frac{1}{2}}}=\\frac{\\exp\\{\\frac{t^{2}}{2(1-2t)}b^{\\top}b+t c\\}}{|(1-2t)I|^{\\frac{1}{2}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "After that, we employ Chernoff bound, for some $\\gamma$ , we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\{Z\\geq\\mathbb{E}[Z]+\\gamma\\}\\leq\\frac{\\mathbb{E}[\\exp(t Z)]}{\\exp\\{t\\gamma+\\mathbb{E}\\left(Z\\right)\\}\\right\\}}\\\\ &{=\\frac{\\exp\\big\\{\\frac{t^{2}}{2(1-2\\bar{t})}b^{\\top}b+t c\\}}{\\exp\\{t(\\gamma+\\mathbb{E}[z^{\\top}z]+c\\})[(1-2t)I]^{\\frac{1}{2}}}}\\\\ &{=\\frac{\\exp\\big\\{\\frac{t^{2}}{(1-2t^{2})}b^{\\top}b+t c\\}}{\\exp\\{t(\\gamma+T_{r}(I)+\\mathbb{E}[\\theta^{\\top}z]+c\\})[(1-2t)I]^{\\frac{1}{2}}}}\\\\ &{=\\frac{\\exp\\big\\{\\frac{t^{2}}{2(1-2t^{2})}b^{\\top}b+t c\\}}{\\exp\\big\\{\\frac{t^{2}}{2(1-2\\bar{t})}b^{\\top}b+t c\\}}}\\\\ &{=\\frac{\\exp\\big\\{\\frac{t^{2}}{2(1-2\\bar{t})}b^{\\top}b+t c\\}}{\\exp\\big\\{t(\\gamma+\\bar{t}+c)\\big\\}\\big\\}[(1-2t)I]^{\\frac{1}{2}}}\\\\ &{=\\frac{\\exp\\big\\{\\frac{t^{2}}{2(1-2\\bar{t})}[|b||^{2}-t(\\gamma+d)]\\big\\}}{[1-2t]I^{\\frac{1}{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.4 Proof for Theorem 6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 6. For any constant $t_{1}$ and $t_{2}$ satisfying $0\\leq t_{1}<\\frac{1}{2}$ and $0\\leq t_{2}<\\frac{1}{2}$ , the accuracy of the unlearnable decision boundary $P_{u}$ on the dataset $\\mathcal{D}_{c}$ can be upper-bounded as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{\\mathcal{D}_{c}}(P_{u})\\leq\\frac{\\exp\\big\\{\\frac{t_{1}^{2}}{2(1-2t_{1})}||b+2\\mu||_{2}^{2}+t_{1}(\\mu^{\\top}\\mu+b^{\\top}\\mu+c)\\big\\}}{2|(1-2t_{1})I|^{\\frac{1}{2}}}}\\\\ &{\\qquad\\qquad+\\,\\frac{\\exp\\big\\{\\frac{t_{2}^{2}}{2(1-2t_{2})}||b-2\\mu||_{2}^{2}-t_{2}(\\mu^{\\top}\\mu-b^{\\top}\\mu+c+2d)\\big\\}}{2|(1-2t_{2})I|^{\\frac{1}{2}}}}\\\\ &{\\qquad\\qquad:=p_{1}+p_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, if $\\dot{\\mu^{\\top}}\\mu+b^{\\top}\\mu+c+d<0$ and $-\\mu^{\\top}\\mu+b^{\\top}\\mu-c-d<0,$ , we have $\\tau_{D_{c}}(P_{u})<1$ .   \nMoreover, for any $\\mu\\neq0\\,\\exists$ transformation matrix $\\mathbf{T}_{i}$ such that $\\tau_{D_{c}}(P_{u})<\\tau_{D_{c}}(P)$ . Proof: We note that if $P_{u}(x)$ is less than 0, the category of the Bayesian optimal classification is $^-1$ ;   \notherwise, it is 1. Here, $x=y\\mu+z$ where $z\\sim\\mathcal{N}(0,I)$ and $y\\in\\{\\pm1\\}$ since $(x,y)\\sim\\mathcal{D}_{c}$ . ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tau_{\\mathcal{P}_{c}}(P_{u})=\\mathbb{E}\\{\\|(y(\\alpha^{\\top}x+b^{\\top}x+c)>0)\\}}&{}\\\\ {=\\mathbb{P}\\{y(\\mu^{\\top}\\mu+z^{\\top}z+2y\\mu^{\\top}z+y b^{\\top}\\mu+b^{\\top}z+c)>0\\}}\\\\ {=\\mathbb{P}(y=1)\\mathbb{P}\\{y(\\mu^{\\top}\\mu+z^{\\top}z+2y\\mu^{\\top}z+y b^{\\top}\\mu+b^{\\top}z}\\\\ {+c)>0|y=1\\}+\\mathbb{P}(y=-1)\\mathbb{P}\\{y(\\mu^{\\top}\\mu+z^{\\top}z}\\\\ {+2y\\mu^{\\top}z+y b^{\\top}\\mu+b^{\\top}z+c)>0|y=-1\\}}\\\\ {=\\frac{1}{2}\\mathbb{P}\\{z^{\\top}z+(b+2\\mu)^{\\top}z+\\mu^{\\top}\\mu+b^{\\top}\\mu+c>0\\}}\\\\ {+\\frac{1}{2}\\mathbb{P}\\{-z^{\\top}z-(b-2\\mu)^{\\top}z-\\mu^{\\top}\\mu+b^{\\top}\\mu-c>0\\}}\\\\ {:=p_{1}+p_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can see that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\gamma_{1}:=\\mathbb{E}\\{z^{\\top}z+(b+2\\mu)^{\\top}z+\\mu^{\\top}\\mu+b^{\\top}\\mu+c\\}}\\\\ &{\\qquad=T r(I)+\\mu^{\\top}\\mu+b^{\\top}\\mu+c}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\gamma_{2}:=\\mathbb{E}\\{-z^{\\top}z-(b-2\\mu)^{\\top}z-\\mu^{\\top}\\mu+b^{\\top}\\mu-c\\}}\\\\ &{\\qquad=-T r(I)-\\mu^{\\top}\\mu+b^{\\top}\\mu-c}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying Lemma 5, with $\\gamma=\\gamma_{1}$ , $t=t_{1}$ for the computation of $p_{1}$ , as well as $\\gamma=\\gamma_{2}$ and $t=t_{2}$ for the computation of $p_{2}$ , where $t_{1}$ and $t_{2}$ are specific non-negative constants, we obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\np_{1}=\\frac{\\exp\\left\\{\\frac{t_{1}^{2}}{2(1-2t_{1})}||b+2\\mu||_{2}^{2}+t_{1}(\\mu^{\\top}\\mu+b^{\\top}\\mu+c)\\right\\}}{2|(1-2t_{1})I|^{\\frac{1}{2}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\np_{2}=\\frac{\\exp\\left\\{\\frac{t_{2}^{2}}{2(1-2t_{2})}||b-2\\mu||_{2}^{2}-t_{2}(\\mu^{\\top}\\mu-b^{\\top}\\mu+c+2d)\\right\\}}{2|(1-2t_{2})I|^{\\frac{1}{2}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This provides us with the upper bound for $\\tau_{D_{c}}(P_{u})$ . Nonetheless, to ensure that this upper bound is less than 1, additional conditions need to be affirmed. As $\\gamma_{1}$ and $\\gamma_{2}$ increase, the values of $p_{1}$ and $p_{2}$ diminish $(p_{1}>0,p_{2}>0)$ ), and as $\\gamma_{1}$ increases, $\\gamma_{2}$ decreases (since $\\gamma_{1}+\\gamma_{2}=-2b^{\\top}\\mu)$ . We let $\\frac{||b{+}2\\mu||_{2}^{2}}{2}$ equal to $\\alpha_{1}\\geq0$ , $\\mu^{\\top}\\mu+b^{\\top}\\mu+c$ (also equals to $\\begin{array}{r}{||\\mu||_{2}^{2}+c-\\frac{\\gamma_{1}+\\gamma_{2}}{2})}\\end{array}$ equal to $\\beta_{1}$ , resulting in: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}p_{1}}{\\mathrm{d}t_{1}}=\\frac{1}{2}\\frac{\\mathrm{d}\\left|\\exp\\left\\{\\frac{\\alpha_{1}q_{1}^{2}}{1-2q_{1}}\\right\\}+\\beta_{1}t_{1}\\right\\rangle/(1-q_{1})^{\\frac{q_{1}}{2}}\\right|}{\\mathrm{d}t_{1}}}\\\\ &{=\\frac{\\exp\\left\\{\\frac{\\alpha_{1}q_{1}^{2}}{1-2q_{1}}+\\beta_{1}t_{1}\\right\\}}{2}\\left[(1-2t_{1})^{-\\frac{q-1}{2}-1}d\\right.}\\\\ &{+\\left.(1-2t_{1})^{-\\frac{q}{2}}(\\frac{2\\alpha_{1}t_{1}(1-t_{1})}{(1-2t_{1})^{2}}+\\beta_{1})\\right]}\\\\ &{=\\frac{\\exp\\left\\{\\frac{\\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\\beta_{1}t_{1}\\right\\}}{2(1-2t_{1})^{\\frac{q}{2}}}\\frac{\\vphantom{\\beta}}{\\left|\\frac{1}{1-2t_{1}}+\\frac{1}{2}\\right\\rangle}\\frac{\\vphantom{\\beta}}{\\left(1-2t_{1}\\right)^{2}}+\\beta_{1}t_{1}(1-t_{1})+\\beta_{1}t_{1}}\\\\ &{=p_{1}(\\frac{d}{1-2t_{1}}+\\frac{2\\alpha_{1}t_{1}(1-t_{1})}{(1-2t_{1})^{2}}+\\beta_{1})}\\\\ &{=\\frac{2(2\\beta_{1}-\\alpha_{1})t_{1}^{2}+2(\\alpha_{1}-2\\beta_{1}-d)t_{1}+\\beta_{1}+d}{(1-2t_{1})^{2}}p_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Making $|(1-2t_{1})I|^{\\frac{1}{2}}$ meaningful requires satisfying the following condition: ", "page_idx": 25}, {"type": "equation", "text": "$$\n1-2t_{1}>0\\Longrightarrow0\\leq t_{1}<\\frac{1}{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We note that $\\begin{array}{r}{p_{1}(t_{1}=0)=\\frac{1}{2}}\\end{array}$ , $\\begin{array}{r}{p_{1}(t_{1}\\to\\frac{1}{2})=+\\infty}\\end{array}$ . When we set $\\begin{array}{r}{{\\frac{\\mathrm{d}p_{1}}{\\mathrm{d}t_{1}}}=0}\\end{array}$ , we obtain that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2(2\\beta_{1}-\\alpha_{1})t_{1}^{2}+2(\\alpha_{1}-2\\beta_{1}-d)t_{1}+d+\\beta_{1}=0}\\\\ &{\\implies t_{1}=\\frac{2\\beta_{1}-\\alpha_{1}+d\\pm\\sqrt{\\alpha_{1}^{2}-2\\alpha_{1}\\beta_{1}+d^{2}}}{2(2\\beta_{1}-\\alpha_{1})}}\\\\ &{\\implies\\alpha_{1}^{2}-2\\beta_{1}\\alpha_{1}+d^{2}\\geq0}\\\\ &{\\implies\\beta_{1}\\leq\\frac{d^{2}+\\alpha_{1}^{2}}{2\\alpha_{1}}=\\frac{d^{2}}{2\\alpha_{1}}+\\frac{\\alpha_{1}}{2}}\\\\ &{\\implies\\beta_{1}\\leq(\\frac{d^{2}}{2\\alpha_{1}}+\\frac{\\alpha_{1}}{2})_{m i n}=d}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "oEfx tphlias nfautnicotni oonf  ctahne  bLea sotb Itanienqeud aulistiyn. g Atshseu Amriinthg mweet idc eMfineea $\\begin{array}{r}{s(\\alpha_{1})=\\frac{d^{2}}{2\\alpha_{1}}+\\frac{\\alpha_{1}}{2}}\\end{array}$ .  ITnehqe umaliintiy m(uAmM v-aGluMe Inequality), which states that $a+b\\geq2{\\sqrt{a b}}$ for $a,b\\ge0$ . Thus, $\\begin{array}{r}{s(\\alpha_{1})\\geq2\\sqrt{\\frac{d^{2}}{2\\alpha_{1}}\\cdot\\frac{\\alpha_{1}}{2}}=d}\\end{array}$ . Since $\\beta_{1}\\leq s(\\alpha_{1})$ , we can infer that $\\beta_{1}\\leq s(\\alpha_{1})_{\\operatorname*{min}}$ , which means $\\beta_{1}\\leq d$ . ", "page_idx": 25}, {"type": "text", "text": "The product of the roots of the equation is: $\\begin{array}{r}{t_{11}t_{12}=\\frac{d+\\beta_{1}}{2(2\\beta_{1}-\\alpha_{1})}}\\end{array}$ (we assume that $t_{11}<t_{12}$ ). We let $f(t_{1})=2(2\\beta_{1}-\\alpha_{1})t_{1}^{2}+2(\\alpha_{1}-2\\beta_{1}-d)t_{1}+d+\\beta_{1}$ . Thus we have $\\begin{array}{r}{f(0)=d\\!+\\!\\beta_{1},f(\\frac{1}{2})=\\frac{\\alpha_{1}}{2}>0}\\end{array}$ . Situation (i): $\\beta_{1}<-d$ . At this time, $f(0)<0,t_{11}t_{12}>0$ , based on the trend of quadratic functions and the distribution of roots, we can infer that $0\\,<\\,t_{11}\\,<\\,{\\textstyle\\frac{1}{2}},\\,t_{12}\\,>\\,{\\textstyle\\frac{1}{2}}$ , and $2\\beta_{1}<\\alpha_{1}$ . Thus we conclude that there exists $t_{11}$ such that $\\begin{array}{r}{p_{1}(t_{1}=t_{11})<\\frac{1}{2}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "Situation (ii): $-d<\\beta_{1}<0$ . At this time, $f(0)>0,t_{11}t_{12}<0.$ , based on the trend of quadratic functions and the distribution of roots, we also can infer that $t_{11}<0,t_{12}>\\frac{1}{2}$ , and $2\\beta_{1}<\\alpha_{1}$ . Thus we have that $p_{1}\\geq\\textstyle{\\frac{1}{2}}$ . ", "page_idx": 25}, {"type": "text", "text": "Situation $(i i i)$ : $0<\\beta_{1}<d$ . At this time, $f(0)>0$ , the sign of $t_{11}t_{12}$ simultaneously determines the direction of the opening of the quadratic function $f(t_{1})$ . When $t_{11}t_{12}<0,t_{11}<0,\\dot{t}_{12}>\\frac{1}{2}$ , thus we have $\\begin{array}{r}{p_{1}\\geq\\frac{1}{2}}\\end{array}$ ; when $t_{11}t_{12}>0$ , $\\begin{array}{r}{0<t_{11}<t_{12}<\\frac{1}{2}}\\end{array}$ , the minimum point of $p_{1}$ is at $p_{1}(t_{12})$ . However, it is challenging to compare $p_{1}(t_{12})$ and $\\frac{1}{2}$ to determine which is greater or smaller. ", "page_idx": 25}, {"type": "text", "text": "Similarly for p2, we let ||b\u221222\u00b5||2 equal to $\\alpha_{2}>0,-\\mu^{\\top}\\mu+b^{\\top}\\mu-c-2d$ equal to $\\beta_{2}$ , resulting in: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}p_{2}}{\\mathrm{d}t_{2}}=\\frac{2(2\\beta_{2}-\\alpha_{2})t_{2}^{2}+2(\\alpha_{2}-2\\beta_{2}-d)t_{2}+\\beta_{2}+d}{(1-2t_{2})^{2}}p_{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus we have the similar situation, i.e., $\\beta_{2}<-d$ . At this time, $f(0)<0,t_{21}t_{22}>0.$ , based on the trend of quadratic functions and the distribution of roots, we can infer that $\\mathrm{0}<t_{21}<\\frac{1}{2}$ , $t_{22}>\\frac{1}{2}$ , and $2\\beta_{2}<\\alpha_{2}$ . Thus we conclude that there exists $t_{21}$ such that $\\begin{array}{r}{p_{2}(t_{2}=t_{21})<\\frac{1}{2}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "Taking into account the above situations, we have that: when $\\beta_{1}<-d,\\,\\beta_{2}<-d$ (i.e., $\\mu^{\\top}\\mu+b^{\\top}\\mu+$ $c+d<0$ and $-\\mu^{\\top}\\mu+b^{\\top}\\mu-c-d<0)$ , there exists $t_{11}$ and $t_{21}$ respectively, making $\\begin{array}{r}{p_{1}<\\frac{1}{2}}\\end{array}$ , p2 < 12, i.e., p1 + p2 < 1, where t11 = 12 +d\u2212 2\u03b1(221\u03b2\u221212\u2212\u03b1\u03b111\u03b2)1+d2, t21 = 1 +d\u2212 \u03b122\u22122\u03b12\u03b22+d2. ", "page_idx": 25}, {"type": "text", "text": "We know that for $\\mu\\neq0$ , $\\tau_{\\mathscr D_{c}}(P)=\\phi(\\mu)>\\frac{1}{2}$ . Therefore, we need to introduce additional conditions to further ensure that $\\begin{array}{r}{p_{1}<\\frac{1}{4}}\\end{array}$ , and $p_{2}<\\textstyle{\\frac{1}{4}}$ , and consequently $\\begin{array}{r}{\\tau_{\\mathcal{D}_{c}}(P_{u})=p_{1}+p_{2}<\\frac{1}{2}<\\tau_{\\mathcal{D}_{c}}(P)}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "To let $p_{1}$ satisfy $\\begin{array}{r}{p_{1}<\\frac{1}{4}\\,(\\alpha_{1}>0,\\beta_{1}<-d.}\\end{array}$ , and $\\begin{array}{r}{0\\leq t_{1}<\\frac{1}{2})}\\end{array}$ , that is, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\exp\\{\\frac{\\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\\beta_{1}t_{1}\\}<\\frac{(1-2t_{1})^{\\frac{d}{2}}}{2}}}\\\\ {{\\displaystyle\\Rightarrow\\frac{\\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\\beta_{1}t_{1}<\\frac{d}{2}l n(1-2t_{1})-l n2}}\\\\ {{\\displaystyle\\Rightarrow\\frac{\\alpha_{1}t_{1}^{2}}{1-2t_{1}}+\\beta_{1}t_{1}-\\frac{d}{2}l n(1-2t_{1})<-l n2=-0.693}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We assume that g(t) = 1\u03b1\u22121t22t $\\begin{array}{r}{g(t)=\\frac{\\alpha_{1}t^{2}}{1-2t}+\\beta_{1}t-\\frac{d}{2}l n(1-2t)}\\end{array}$ . Let us assume $\\begin{array}{r}{\\alpha_{1}=\\frac{1}{2}}\\end{array}$ and $d=3$ for 3D point cloud data, then $\\beta_{1}<-3$ . Upon analyzing the function $g(t)$ , we observe that as $\\beta_{1}$ decreases, the minimum value of $g(t)$ also decreases. We utilize various $\\beta_{1}$ values and their corresponding function value to provide a more intuitive understanding as shown in Tab. 16. ", "page_idx": 26}, {"type": "text", "text": "Table 16: Different $\\beta_{1}$ values and corresponding $g(t)$ with $t=0.3$ and $t=0.4$ . The bold values represent cases where $\\begin{array}{r}{p_{1}<\\frac{1}{4}}\\end{array}$ is satisfied. ", "page_idx": 26}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/c0e65c3b48ac9122ad2e9015eba2e093be50151967d7333c0fcf6f9d417b2776.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "As for $p_{2}$ , since $\\alpha_{2}\\neq\\alpha_{1}$ , we need to reselect appropriate values to demonstrate the existence of $p_{2}<\\textstyle{\\frac{1}{4}}$ . Similarly, to let $p_{2}$ satisfy $\\begin{array}{r}{p_{2}<\\frac{1}{4}\\,(\\alpha_{2}>^{\\cdot}\\!\\!\\dot{0},\\dot{\\beta_{2}}<-d}\\end{array}$ , and $\\begin{array}{r}{0\\leq t_{2}<\\frac{1}{2};}\\end{array}$ ), that is, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\exp\\{\\frac{\\alpha_{2}t_{2}^{2}}{1-2t_{2}}+\\beta_{2}t_{2}\\}<\\frac{(1-2t_{2})^{\\frac{d}{2}}}{2}}}\\\\ {{\\displaystyle\\Rightarrow\\frac{\\alpha_{2}t_{2}^{2}}{1-2t_{2}}+\\beta_{2}t_{2}-\\frac{d}{2}l n(1-2t_{2})<-l n2=-0.693}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We assume that $\\begin{array}{r}{h(t)=\\frac{\\alpha_{2}t^{2}}{1-2t}+\\beta_{2}t-\\frac{d}{2}l n(1-2t)}\\end{array}$ . Let us assume $\\begin{array}{r}{\\alpha_{2}=\\frac{1}{3}}\\end{array}$ and $d=3$ , similarly, we utilize various $\\beta_{2}$ values and their corresponding function value to provide a more intuitive understanding as shown in Tab. 17. ", "page_idx": 26}, {"type": "text", "text": "Table 17: Different $\\beta_{2}$ values and corresponding $h(t)$ with $t=0.3$ and $t=0.4$ . The bold values represent cases where $\\begin{array}{r}{p_{2}<\\frac{1}{4}}\\end{array}$ is satisfied. ", "page_idx": 26}, {"type": "table", "img_path": "SeefZa7Vmq/tmp/5000ce542a9ee751c2f3cef9b578c844d2a898f473330a4fe2b2222e7910afe6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Therefore, we conclude that there exist $\\alpha_{1},\\beta_{1},t_{1}$ such that $\\begin{array}{r}{p_{1}<\\frac{1}{4}}\\end{array}$ , $\\alpha_{2},\\beta_{2},t_{2}$ such that $p_{2}<\\textstyle{\\frac{1}{4}}$ , i.e., $\\scriptstyle p_{1}+p_{2}\\,<\\,{\\frac{1}{2}}$ . At the same time, we observe that a smaller value of $\\beta$ makes it easier to satisfy the above conditions, i.e., the more negative $\\beta_{1}$ and $\\beta_{2}$ are, the more likely it is to satisfy the above conditions. We formally combine and assert these conditions as, b\u22a4\u00b5 \u226a0, i.e., \u00b5\u22a4\u03bb\u2212\u221221\u03bbT\u2212\u22a4\u221221\u2212+\u03bb\u03bb\u22121\u221222T1\u22a4 (we sufficiently support this condition in the empirical results from Tabs. 16 and 17). Thus we conclude that for any $\\mu\\neq0\\,\\exists$ transformation parameters $\\mathbf{T}_{i}$ such that $\\tau_{D_{c}}(P_{u})<\\tau_{D_{c}}(P)$ . ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the scope of the contributions made in the paper regarding the first proposed unlearnable scheme UMT and the data restoration scheme, as well as the theoretical and experimental evaluations. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper discussed the limitations of the work in Sec. 6. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper clearly and accurately provides the assumptions and proofs for each theory. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The formulas in the methods section, the schematic diagrams of the schemes, Algorithm 1, as well as the details of the training process and descriptions of the datasets and models together support the reproducibility of the experimental results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper has open-sourced code, which are sufficient to reproduce the experimental results. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper provides sufficient training and testing details, which are adequate for understanding the experimental results. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper conducted experiments with three different random seeds, studying the results in terms of statistical significance. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper clearly specifies the parameters of the server used for the experiment and the code execution environment. ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: After a thorough review of the paper, no violations of the NeurIPS Code of Ethics are found. ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper discuss both potential positive societal impacts and negative societal impacts of the work in Sec. 6. ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper described safeguards that have been put in place for responsible release of data in Sec. 6. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The data used in the paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?   \nAnswer: [NA   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}]