[{"figure_path": "XlAbMZu4Bo/figures/figures_1_1.jpg", "caption": "Figure 1: Negative log-likelihood for MEGALODON-7B, LLAMA2-7B and LLAMA2-13B.", "description": "The figure shows the training loss curves for three different language models: MEGALODON-7B, LLAMA2-7B, and LLAMA2-13B.  The x-axis represents the number of training tokens (in billions), and the y-axis represents the negative log-likelihood (train loss), a measure of how well the model predicts the next token in a sequence.  The plot demonstrates that MEGALODON-7B achieves a training loss comparable to LLAMA2-7B while showing slightly better efficiency and stability compared to the Transformer-based LLAMA2 models throughout training.  This suggests that MEGALODON achieves better data efficiency than LLAMA2-7B.", "section": "4.1 LLM Pretraining"}, {"figure_path": "XlAbMZu4Bo/figures/figures_3_1.jpg", "caption": "Figure 2: Normalization methods. The elements in blue or pink are the regions to compute means and variances. We omit the batch dimension for simplicity.", "description": "This figure compares three different normalization methods: Layer Normalization, Group Normalization, and Timestep Normalization.  It visually represents how each method calculates the mean and variance for normalization. Layer Normalization computes these statistics across the feature dimension for each timestep. Group Normalization computes them across a subset of the feature dimension and all timesteps.  Timestep Normalization calculates them cumulatively across the timesteps within each group of the feature dimension. The color coding helps to differentiate the regions over which the statistics are computed.", "section": "3.2 Timestep Normalization"}, {"figure_path": "XlAbMZu4Bo/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of the MEGALODON architecture. Figure (a) shows a sketch of one MEGALODON layer. Figure (b) and (c) display the configurations of pre-norm and pre-norm with two-hop residual, respectively.", "description": "This figure illustrates the architecture of the MEGALODON model, showing a single layer's components and highlighting the differences between three normalization strategies: standard pre-norm, and a novel pre-norm with two-hop residual configuration.  The figure demonstrates the flow of information within a layer, including the complex exponential moving average (CEMA), normalized attention unit, and feed-forward network (FFN).  The subfigures highlight how the placement of Layer Normalization and Timestep Normalization impacts the residual connections.", "section": "3 MEGALODON"}, {"figure_path": "XlAbMZu4Bo/figures/figures_6_1.jpg", "caption": "Figure 4: Average WPS per device.", "description": "This figure compares the training speed (Tokens Per Second) of LLAMA2-7B and MEGALODON-7B models using 4K and 32K context lengths.  It demonstrates that MEGALODON-7B is faster than LLAMA2-7B at 32K context length. At 4K context length, MEGALODON-7B is slightly slower than LLAMA2-7B. The y-axis represents tokens per second, a measure of speed. The x-axis shows the model and context length.", "section": "4.1 LLM Pretraining"}, {"figure_path": "XlAbMZu4Bo/figures/figures_7_1.jpg", "caption": "Figure 5: PPL in various context lengths.", "description": "This figure shows the perplexity (PPL) scores for the MEGALODON-7B model on a validation dataset of long sequences (at least 2M tokens), for various context lengths ranging from 4K to 2M tokens. The graph shows a clear downward trend of the perplexity scores as the context length increases, demonstrating the model's ability to leverage longer context windows for improved prediction accuracy.", "section": "4.3 Long-Context Evaluation"}, {"figure_path": "XlAbMZu4Bo/figures/figures_7_2.jpg", "caption": "Figure 5: PPL in various context lengths.", "description": "The figure shows the perplexity (PPL) scores for MEGALODON and other models across different context lengths, demonstrating the model's ability to handle very long sequences.  The x-axis represents context length, ranging from 4K to 2M tokens, and the y-axis shows the PPL.  Lower PPL indicates better performance. The graph visually demonstrates that as context length increases, the perplexity decreases, indicating that MEGALODON effectively utilizes long-range dependencies.", "section": "4.3 Long-Context Evaluation"}]