{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces the CLIP model, a foundational model for multi-modal learning that is heavily referenced and built upon in the current work."}, {"fullname_first_author": "Shih-Cheng Huang", "paper_title": "GLORIA: A multimodal global-local representation learning framework for label-efficient medical image recognition", "publication_date": "2021-10-01", "reason": "This paper proposes a state-of-the-art medical vision-language pre-training model (GLORIA) which is directly compared against in the current work's experiments."}, {"fullname_first_author": "Jeremy Irvin", "paper_title": "CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison", "publication_date": "2019-01-01", "reason": "This paper introduces the CheXpert dataset, a large-scale public dataset for chest radiograph interpretation that is used extensively as a benchmark in the current work."}, {"fullname_first_author": "Alistair EW Johnson", "paper_title": "MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs", "publication_date": "2019-01-01", "reason": "This paper introduces the MIMIC-CXR dataset, another large-scale dataset used for training and evaluating the model in the current work."}, {"fullname_first_author": "Alexandros Karargyris", "paper_title": "Eye gaze data for chest x-rays", "publication_date": "2020-01-01", "reason": "This paper introduces the MIMIC-EYE dataset, which is unique for its inclusion of eye-gaze data alongside medical images and reports, forming the core dataset for this work."}]}