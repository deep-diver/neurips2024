[{"figure_path": "0bINeW40u4/figures/figures_1_1.jpg", "caption": "Figure 1: The guiding role of radiologists' eye-gaze data. The text provided by radiologists during diagnosis aligns naturally with the attention regions.", "description": "This figure illustrates how radiologists' eye gaze data acts as an auxiliary tool in aligning multimodal medical data.  The diagram shows a radiologist looking at a chest X-ray, with their gaze focused on a specific region of the image.  Simultaneously, the radiologist provides a diagnostic text description (\"normal heart\"). The dashed line connecting the gaze and the text represents the alignment guided by the eye gaze. The process is further explained with visual representation of gaze points, audio transcripts, attention regions, patch features, and sentence features, which are all processed to enhance the alignment of image and text data.", "section": "1 Introduction"}, {"figure_path": "0bINeW40u4/figures/figures_3_1.jpg", "caption": "Figure 2: The framework of EGMA. After images and text are processed by the encoder in Part A, patch feature and sentence feature representations are obtained, resulting in a fine-grained similarity matrix for instances. Subsequently, the two types of eye-gaze-based auxiliary information obtained in Part B are used for fine-grained and cross-mapping alignment in Part C and Part D, respectively.", "description": "This figure illustrates the EGMA framework's four main components. Part A shows image and text feature extraction, creating an instance-level similarity matrix. Part B extracts eye-gaze features, creating attention heatmaps that indicate radiologist focus areas. Part C refines the alignment using eye-gaze data, employing fine-grained contrastive loss. Finally, Part D leverages eye-gaze data to guide cross-modality mapping, improving text-image alignment.", "section": "3 Method"}, {"figure_path": "0bINeW40u4/figures/figures_8_1.jpg", "caption": "Figure 3: Results of cross-modality attention maps visualization. Related text content: (a) \"heart size borderline enlarged\"; (b) \"increased bibasilar opacities are the combination of increased bilateral pleural effusions and bibasilar atelectasis\".", "description": "This figure visualizes the cross-modality attention maps generated by the EGMA model, showcasing its ability to accurately pinpoint disease regions in chest X-ray images.  It compares EGMA's performance with other methods (MGCA and GLORIA) and shows how EGMA better aligns text descriptions with relevant image regions, highlighting the effectiveness of incorporating eye-gaze data in multi-modal alignment. Two examples are provided: one showing the attention map generated for the phrase \"heart size borderline enlarged\", and another for the more complex phrase \"increased bibasilar opacities are the combination of increased bilateral pleural effusions and bibasilar atelectasis\".", "section": "4.3 Visualization"}, {"figure_path": "0bINeW40u4/figures/figures_8_2.jpg", "caption": "Figure 4: t-SNE visualization on CheXpert 5x200 dataset by CLIP and our EGMA. The figures display points of different colors representing various ground truth disease types and their cluster assignments. The color-coded points illustrate the clustering results of each algorithm.", "description": "This figure shows a t-SNE visualization comparing the feature representations learned by CLIP and EGMA on the CheXpert 5x200 dataset.  Each point represents an image, and the color indicates the ground truth disease label. The visualization helps to understand how well each model separates the different disease classes in the feature space. EGMA shows a better clustering and separation of the disease classes than CLIP, indicating improved representation learning.", "section": "4.3 Visualization"}, {"figure_path": "0bINeW40u4/figures/figures_17_1.jpg", "caption": "Figure 5: The generation methods for heatmap at both word-level and sentence-level.", "description": "This figure illustrates the process of generating attention heatmaps from radiologists' eye-gaze data. The top shows the audio recording of the radiologist, its transcription and the timeline. Each word in the transcription is aligned with the corresponding eye-gaze data points marked on the chest X-ray image. The middle panel shows the word-level gaze data, with each image corresponding to one word in the transcription. The bottom panel shows the resulting word-level and sentence-level heatmaps, which are used to align the textual and visual features. The red boxes indicate how word-level heatmaps are aggregated into sentence-level heatmaps.", "section": "3.1 Multi-modal Data Processing"}, {"figure_path": "0bINeW40u4/figures/figures_18_1.jpg", "caption": "Figure 6: Attention heatmap (b) generated from raw gaze data (a) is susceptible to noise. The adaptive filter employed in the preprocessing step of this work removes noisy data (saccades and microsaccades) based on characteristics such as the speed of gaze points (c), resulting in more accurate fixation data (d) and heatmap (e).", "description": "This figure demonstrates the denoising process applied to raw eye-gaze data.  Panel (a) shows the raw gaze data overlaid on an image, highlighting the noisy nature of the raw data, which includes many saccades and microsaccades. Panel (b) displays the heatmap generated from this raw gaze data. Panel (c) shows the speed of gaze points over time, illustrating high speed during saccades and lower speed during fixations.  An adaptive filter is applied to the raw gaze data, resulting in the processed gaze data and heatmap shown in (d) and (e), respectively. This filtering process significantly reduces noise and improves the quality of the gaze data for downstream analysis.", "section": "3.1 Multi-modal Data Processing"}, {"figure_path": "0bINeW40u4/figures/figures_18_2.jpg", "caption": "Figure 7: Inaccurate eye-gaze data of one radiologist (a) in the heart region and several correct eye-gaze data (b) of other radiologists in the same region that compensate for this error, which are included in the dataset used in this work.", "description": "This figure demonstrates how the proposed method handles inaccurate eye gaze data.  In (a), one radiologist's gaze is not accurately focused on the relevant region of the heart. However, (b) shows that data from other radiologists in the dataset correctly identify the heart region, compensating for the inaccurate data point and improving overall accuracy.", "section": "D.1 Eye-gaze Data Denoising"}, {"figure_path": "0bINeW40u4/figures/figures_19_1.jpg", "caption": "Figure 8: Comparison of eye-gaze data in normal and abnormal cases. For the heart region, there are more fixations on disease area (a) compared to normal heart (b). For the lung region, fixations on disease area (c) are more concentrated, whereas fixations on normal lungs are more dispersed.", "description": "This figure visually demonstrates the differences in radiologists' eye-gaze patterns between normal and abnormal chest X-rays.  The heatmaps overlaid on the images show the concentration of gaze points.  In cases of cardiomegaly (a) and pneumonia (c), there's a noticeably higher concentration of gaze in the affected areas (heart and lungs, respectively) compared to normal (b) and clear lungs (d). This highlights how radiologists naturally focus their attention on the regions of interest.", "section": "4.3 Visualization"}, {"figure_path": "0bINeW40u4/figures/figures_19_2.jpg", "caption": "Figure 4: t-SNE visualization on CheXpert 5x200 dataset by CLIP and our EGMA. The figures display points of different colors representing various ground truth disease types and their cluster assignments. The color-coded points illustrate the clustering results of each algorithm.", "description": "This figure visualizes the feature representations of the CLIP and EGMA models on the CheXpert 5x200 dataset using t-SNE.  Different colors represent different disease categories. The visualization shows that EGMA achieves better clustering of the data points than CLIP, indicating its superior ability to differentiate between diseases.", "section": "4.3 Visualization"}]