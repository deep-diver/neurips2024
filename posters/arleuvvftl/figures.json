[{"figure_path": "ARLEUVVfTL/figures/figures_0_1.jpg", "caption": "Figure 1: Test accuracies are depicted over the training epochs on CIFAR-10, utilizing Neural ODEs with different numbers of convolution layers as the backbones for learnable derivatives. Dashed horizontal lines denote instances of training failures.", "description": "This figure shows the test accuracy curves of Neural ODEs with varying numbers of convolution layers (4, 8, 12, 18 layers) trained on the CIFAR-10 dataset.  The curves illustrate the impact of increasing the depth of the Neural ODE on model performance.  It demonstrates the instability of deeper Neural ODEs without batch normalization, as indicated by the dashed horizontal lines representing training failures. The figure also includes results for Neural ODEs with traditional mini-batch Batch Normalization (BN) and the proposed Temporal Adaptive Batch Normalization (TA-BN) to highlight the performance improvement.", "section": "1 Introduction"}, {"figure_path": "ARLEUVVfTL/figures/figures_2_1.jpg", "caption": "Figure 2: Left: The failure of Pop BN in Neural ODEs stems from the misalignment of discretized time grids. Pop-TI BN aggregates all running mini-batch statistics into a single pair of (\u03bc, \u03c3\u00b2), implicitly assuming time-independent population statistics. Right: Our proposed TA-BN automatically conducts temporal interpolation to accumulate statistics and update parameters during training and testing.", "description": "This figure illustrates the limitations of traditional batch normalization (BN) in Neural ODEs and introduces the proposed Temporal Adaptive Batch Normalization (TA-BN).  The left panel shows how the variable time steps used by adaptive ODE solvers in the forward pass lead to misaligned time grids during training and testing, preventing the reliable accumulation of population statistics. This problem affects both Population Batch Normalization (Pop BN) and a time-independent version called Pop-TI BN.  The right panel showcases how TA-BN addresses this issue by employing temporal interpolation to accumulate running statistics and update parameters consistently during both training and testing.", "section": "3 Traditional Batch Normalization in Neural ODES"}, {"figure_path": "ARLEUVVfTL/figures/figures_3_1.jpg", "caption": "Figure 3: Left: We train a Neural ODE with a U-Net backbone as the learnable derivatives on CIFAR-10. Mini-batch BN shows degraded accuracies with a batch size of 2, while TA-BN can maintain high accuracies under varying batch sizes, because it uses the estimated population statistics during testing. Pop-TI BN aggregates running statistics encountered at any time points into a single pair of \u03bc and \u03c3\u00b2. This approach assumes time-independent statistics, leading to erroneous predictions and erratic test loss curves. Middle: When noisy data exist on average in one out of every test batch, Mini-batch BN's performance deteriorates, because the noise affects the mini-batch statistics. The backbone for learnable derivatives in this experiment consists of 6 convolution layers. Right: We plot the output statistics from the first layer of U-Net over time; they are time-dependent.", "description": "This figure compares the performance of three different batch normalization methods (TA-BN, mini-batch BN, and Pop-TI BN) for training Neural ODEs on the CIFAR-10 dataset.  The left panel shows that TA-BN maintains high accuracy across different batch sizes, unlike mini-batch BN, which suffers from accuracy degradation with small batch sizes. Pop-TI BN shows erratic behavior due to its time-independent statistics assumption. The middle panel demonstrates TA-BN's robustness to noisy data compared to mini-batch BN. The right panel visualizes the time-dependent nature of the output statistics generated by the Neural ODE.", "section": "3 Traditional Batch Normalization in Neural ODES"}, {"figure_path": "ARLEUVVfTL/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison between different Neural ODEs on CIFAR-10. The baselines marked by yellow triangles do not adhere to the unmixed structure and are not strictly comparable to ours. It is unknown whether increasing the number of parameters inside their ODEs can lead to better accuracy.", "description": "This figure compares the performance of different Neural ODE architectures on CIFAR-10, focusing on the relationship between the number of parameters and test accuracy.  The primary method being compared against is the proposed TA-BN method, which shows improved performance with increased model size.  Other Neural ODE methods are shown, some of which do not follow the same \"unmixed\" architectural pattern as the TA-BN model, which makes direct comparison difficult. The figure highlights TA-BN's ability to scale effectively without performance saturation, unlike other methods.", "section": "5 Numerical Results"}, {"figure_path": "ARLEUVVfTL/figures/figures_6_2.jpg", "caption": "Figure 3: Left: We train a Neural ODE with a U-Net backbone as the learnable derivatives on CIFAR-10. Mini-batch BN shows degraded accuracies with a batch size of 2, while TA-BN can maintain high accuracies under varying batch sizes, because it uses the estimated population statistics during testing. Pop-TI BN aggregates running statistics encountered at any time points into a single pair of \u03bc and \u03c3\u00b2. This approach assumes time-independent statistics, leading to erroneous predictions and erratic test loss curves. Middle: When noisy data exist on average in one out of every test batch, Mini-batch BN's performance deteriorates, because the noise affects the mini-batch statistics. The backbone for learnable derivatives in this experiment consists of 6 convolution layers. Right: We plot the output statistics from the first layer of U-Net over time; they are time-dependent.", "description": "This figure compares the performance of different batch normalization methods (TA-BN, mini-batch BN, Pop-TI BN) for training Neural ODEs on the CIFAR-10 dataset.  The left panel shows that TA-BN maintains high accuracy across various batch sizes, unlike mini-batch BN which suffers at small batch sizes.  The middle panel demonstrates TA-BN's robustness to noisy data, contrasting with mini-batch BN's vulnerability. The right panel visually confirms the time-dependency of the statistics within the Neural ODE.", "section": "3 Traditional Batch Normalization in Neural ODES"}, {"figure_path": "ARLEUVVfTL/figures/figures_7_1.jpg", "caption": "Figure 6: CIFAR-10 accuracies with increasing sizes of the backbones for learnable derivatives. These figures illustrate the scaling up of Neural ODEs without BN (left) and Neural ODEs with TA-BN (middle). We also compare the accuracies of these two settings in one figure (right).", "description": "This figure compares the performance of Neural ODEs with and without TA-BN on CIFAR-10, demonstrating the scaling capabilities of TA-BN.  The left panel shows that Neural ODEs without BN fail to train effectively when the number of layers increases beyond a certain point, while the middle panel demonstrates that TA-BN successfully allows for the training of deeper Neural ODEs. The right panel directly compares the performance of both settings.", "section": "Ablations on TA-BN"}, {"figure_path": "ARLEUVVfTL/figures/figures_7_2.jpg", "caption": "Figure 3: Left: We train a Neural ODE with a U-Net backbone as the learnable derivatives on CIFAR-10. Mini-batch BN shows degraded accuracies with a batch size of 2, while TA-BN can maintain high accuracies under varying batch sizes, because it uses the estimated population statistics during testing. Pop-TI BN aggregates running statistics encountered at any time points into a single pair of \u03bc and \u03c3\u00b2. This approach assumes time-independent statistics, leading to erroneous predictions and erratic test loss curves. Middle: When noisy data exist on average in one out of every test batch, Mini-batch BN's performance deteriorates, because the noise affects the mini-batch statistics. The backbone for learnable derivatives in this experiment consists of 6 convolution layers. Right: We plot the output statistics from the first layer of U-Net over time; they are time-dependent.", "description": "This figure compares the performance of different batch normalization methods in Neural ODEs on CIFAR-10.  The left panel demonstrates that TA-BN maintains high accuracy even with small batch sizes, unlike Mini-batch BN.  The middle panel shows that TA-BN is more robust to noisy data. The right panel visually confirms that the statistics used in batch normalization are time-dependent in Neural ODEs.", "section": "3 Traditional Batch Normalization in Neural ODES"}, {"figure_path": "ARLEUVVfTL/figures/figures_8_1.jpg", "caption": "Figure 3: Left: We train a Neural ODE with a U-Net backbone as the learnable derivatives on CIFAR-10. Mini-batch BN shows degraded accuracies with a batch size of 2, while TA-BN can maintain high accuracies under varying batch sizes, because it uses the estimated population statistics during testing. Pop-TI BN aggregates running statistics encountered at any time points into a single pair of \u03bc and \u03c3\u00b2. This approach assumes time-independent statistics, leading to erroneous predictions and erratic test loss curves. Middle: When noisy data exist on average in one out of every test batch, Mini-batch BN's performance deteriorates, because the noise affects the mini-batch statistics. The backbone for learnable derivatives in this experiment consists of 6 convolution layers. Right: We plot the output statistics from the first layer of U-Net over time; they are time-dependent.", "description": "This figure compares the performance of three different batch normalization methods (TA-BN, Mini-batch BN, Pop-TI BN) and the case without batch normalization in training a Neural ODE on the CIFAR-10 dataset.  The left panel demonstrates that TA-BN is robust to varying batch sizes, unlike Mini-batch BN which suffers from smaller batch sizes.  Pop-TI BN, which assumes time-independent statistics, performs poorly. The middle panel shows that Mini-batch BN is sensitive to noisy data, whereas TA-BN is more robust. The right panel visualizes the time-dependent nature of the statistics generated by the Neural ODE.", "section": "3 Traditional Batch Normalization in Neural ODES"}, {"figure_path": "ARLEUVVfTL/figures/figures_12_1.jpg", "caption": "Figure 1: Test accuracies are depicted over the training epochs on CIFAR-10, utilizing Neural ODEs with different numbers of convolution layers as the backbones for learnable derivatives. Dashed horizontal lines denote instances of training failures.", "description": "This figure shows the test accuracy curves for Neural ODEs with varying numbers of convolutional layers (4, 8, 12, 18) as the backbone for the learnable temporal derivatives, trained on the CIFAR-10 dataset.  The impact of batch normalization (BN) is also shown.  The curves illustrate the relationship between model depth and training stability.  The dashed lines indicate training failures, demonstrating that deeper Neural ODEs without appropriate techniques often fail to converge during training.", "section": "1 Introduction"}]