[{"figure_path": "CcmHlE6N6u/figures/figures_1_1.jpg", "caption": "Figure 1: Given a hand-held captured low-light scene (a), while (a combination of) existing low-light enhancement/NeRF methods may not produce visually pleasing novel-view images ((b)-(e)), our LuSh-NeRF can produce bright and sharp results (f).", "description": "This figure demonstrates the visual results of different methods on a hand-held captured low-light scene.  (a) shows the original low-light image. (b) shows the result of using the original NeRF method, which suffers from low visibility and blur. (c)-(e) show results from combining existing low-light enhancement and NeRF methods, which still produce unsatisfactory novel views. (f) finally shows the superior results obtained using the proposed LuSh-NeRF, which is able to generate bright and sharp novel views.", "section": "1 Introduction"}, {"figure_path": "CcmHlE6N6u/figures/figures_3_1.jpg", "caption": "Figure 2: The pipeline of our proposed LuSh-NeRF. It contains two novel modules: (a) SND module: Decompose the noise in each view from the origin training image with a Noise NeRF architecture, and utilize the multi-view consistency characteristic in 3D scenario to separate the scene information and noise better; (b) CTP module: To minimize the interference of noise in low-light images on blur kernel predictions, the high frequency domain of the low light regions which are severely affected by noise are abandoned. In the rendering stage, we discard the Noise Estimator and Blur Kernel, and only use the Scenario-NeRF to render the enhanced scene.", "description": "This figure illustrates the pipeline of the LuSh-NeRF model, which consists of two main modules: the Scene-Noise Decomposition (SND) module and the Camera Trajectory Prediction (CTP) module. The SND module separates noise from scene information using multi-view consistency, while the CTP module estimates camera trajectories by focusing on low-frequency information to sharpen the image.  The process starts with preprocessing low-light images and culminates in rendering a clear and sharp enhanced scene using only the Scenario NeRF.", "section": "3 Proposed Method"}, {"figure_path": "CcmHlE6N6u/figures/figures_3_2.jpg", "caption": "Figure 3: Different degradations in the real low light images. (a) Low intensity (b) Noise (c) Blur.", "description": "This figure illustrates the three main degradation types found in real-world low-light images captured using handheld devices.  (a) shows low intensity, meaning the overall brightness is very dim. (b) shows the presence of noise, which appears as random variations in pixel values, making the image grainy. (c) shows motion blur, a result of camera shake during the long exposure needed for low-light capture.  These degradation factors are highly coupled and need to be disentangled for effective NeRF reconstruction.", "section": "3 Problem Statement"}, {"figure_path": "CcmHlE6N6u/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative results of different methods on our real scenes. Our LuSh-NeRF can render cleaner and sharper results for real low-light scenes with camera motions.", "description": "The figure shows a comparison of novel-view image generation results from different methods using real-world low-light images with camera shake. The input images are shown on the left, with subsequent columns showing results from various methods including LEDNet+NeRF, PairLIE+DP-NeRF, Restormer+LLNERF, and finally the authors' LuSh-NeRF method.  LuSh-NeRF demonstrates superior performance in producing clearer and sharper results by effectively handling both low light and motion blur.", "section": "4 Experiments"}, {"figure_path": "CcmHlE6N6u/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative results of different methods on our synthetic scenes. Our method yields the most natural restoration results while sharpening the image.", "description": "This figure showcases a comparison of the results obtained by various methods on synthetic low-light scenes with camera motion blur.  The input images are shown alongside results from LEDNet+NeRF, Restormer+LLNERF, MPRNet+LLNERF, PairLIE+DP-NeRF, URetinex+DP-NeRF, and the proposed LuSh-NeRF method.  The ground truth images are also included for reference.  The figure demonstrates that LuSh-NeRF produces more natural and sharper results compared to the other methods, highlighting its superior performance in restoring low-light images while reducing blurriness.", "section": "4.1 Main Results"}, {"figure_path": "CcmHlE6N6u/figures/figures_8_1.jpg", "caption": "Figure 6: Ablation study of LuSh-NeRF on a real scenario.", "description": "This figure shows the ablation study of LuSh-NeRF on a real-world scenario. It compares the results of different stages of the LuSh-NeRF pipeline, demonstrating the effectiveness of each module (SND and CTP) in improving the quality of the reconstructed NeRF from a hand-held low-light image with camera motion blur. The figure clearly illustrates the individual and combined effects of noise removal and blur kernel estimation on the final output.", "section": "3.4 Training & Rendering"}, {"figure_path": "CcmHlE6N6u/figures/figures_9_1.jpg", "caption": "Figure 7: The RGB intensity mask and CTP mask comparison. Two masks are generated with the same threshold T. White points represent 1 and black points represent 0 in each mask.", "description": "This figure compares two masks generated using different methods for identifying regions of interest in images: RGB intensity thresholding and Camera Trajectory Prediction (CTP) masking. Both masks use the same threshold (T) to classify pixels as either 1 (white) or 0 (black). The RGB intensity mask uses simple intensity values to identify regions, while the CTP mask uses low-frequency image information filtered by a low-pass filter, making it more robust to noise. The comparison helps illustrate the effectiveness of the CTP mask in handling noise in low-light image scenarios.", "section": "3.2 Scenario-Noise Decomposition"}, {"figure_path": "CcmHlE6N6u/figures/figures_15_1.jpg", "caption": "Figure 4: Qualitative results of different methods on our real scenes. Our LuSh-NeRF can render cleaner and sharper results for real low-light scenes with camera motions.", "description": "This figure shows a comparison of novel-view image generation results from different methods on real-world low-light scenes captured with camera motion. The top row displays the input low-light images, followed by the outputs of four different methods (LEDNet+NeRF, PairLIE+DP-NeRF, Restormer+LLNERF, and LuSh-NeRF). The last row shows the ground truth images for comparison.  LuSh-NeRF produces significantly better results in terms of clarity and sharpness, demonstrating its ability to reconstruct cleaner and sharper images from hand-held, low-light photographs with camera motion.", "section": "4 Experiments"}, {"figure_path": "CcmHlE6N6u/figures/figures_16_1.jpg", "caption": "Figure 5: Qualitative results of different methods on our synthetic scenes. Our method yields the most natural restoration results while sharpening the image.", "description": "This figure displays a comparison of various methods for enhancing low-light images, specifically focusing on synthetic scenes.  The input images are shown alongside the results from using LEDNet+NeRF, Restormer+LLNERF, MPRNet+LLNERF, PairLIE+DP-NeRF, URetinex+DP-NeRF, and the proposed LuSh-NeRF method. The ground truth images are also included for reference. The goal is to illustrate LuSh-NeRF's superior performance in restoring natural colors and sharpness compared to existing methods.", "section": "4 Main Results"}, {"figure_path": "CcmHlE6N6u/figures/figures_16_2.jpg", "caption": "Figure 10: A limitation of the proposed LuSh-NeRF. Noise that is relatively similar across views may be difficult to remove due to the reliance on viewpoint consistency.", "description": "This figure shows a limitation of the LuSh-NeRF model.  It demonstrates that if noise in low-light images exhibits similar patterns across multiple viewpoints, the model may struggle to separate this noise from the actual scene information due to its reliance on multi-view consistency for noise removal.  The top row shows the input images.  The second row shows the preprocessed images used for training. The third row displays training views showing the consistent noise patterns. The last row presents the novel view results from different methods, highlighting the challenge posed by this type of noise for accurate reconstruction.  The LuSh-NeRF result indicates some remaining noise.", "section": "4 Experiments"}]