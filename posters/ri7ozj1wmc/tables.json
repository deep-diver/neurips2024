[{"figure_path": "rI7oZj1WMc/tables/tables_15_1.jpg", "caption": "Table 1: 2D Minigrid Environment Specific Parameters", "description": "This table lists the specific parameters used to configure the 2D Minigrid environments for the experiments.  The parameters include details about the grid size, observation type (fully-observable or partially-observable), whether frame stacking was used, the color scheme (RGB or grayscale), the number of training frames per task, the number of exposures, the number of tasks per exposure, the number of frames per epoch per task, the batch size, epsilon decay schedule, whether action repeat was used, the dimensionality of actions and observations, the maximum frames per episode, the task learning rate, and the epsilon decay rate.", "section": "E.1 2D Gridworld Environments"}, {"figure_path": "rI7oZj1WMc/tables/tables_15_2.jpg", "caption": "Table 4: Simple SF Hyperparameters", "description": "This table lists the hyperparameters used for the Simple Successor Features (SFs) model.  It specifies details such as the optimizer used (Adam), discount factor (\u03b3), replay buffer size, whether double Q-learning is employed, target network update frequency, target smoothing coefficient, multi-step return length, minimum replay buffer size before sampling, framestacking parameters, replay buffer reset frequency, exploration method, learning rate, and other relevant parameters specific to the encoder, basis features, and the SF network architecture. These hyperparameters were tuned using grid search and five random seeds to optimize the performance of the model in both 2D Minigrid and 3D Four Rooms environments.", "section": "F Agents"}, {"figure_path": "rI7oZj1WMc/tables/tables_18_1.jpg", "caption": "Table 1: 2D Minigrid Environment Specific Parameters", "description": "This table lists the hyperparameters used in the 2D Minigrid environment experiments.  It specifies details such as grid size, observation type (fully or partially observable), whether frame stacking was used, color space (RGB or Greyscale), the number of training frames, number of exposures and tasks per exposure, frames per epoch, batch size, epsilon decay schedule, action repeat, action dimensionality, observation size, maximum frames per episode, and task learning rate.", "section": "E.1 2D Gridworld Environments"}, {"figure_path": "rI7oZj1WMc/tables/tables_19_1.jpg", "caption": "Table 2: 3D Miniworld Four Rooms Environment Specific Parameters", "description": "This table lists the hyperparameters used in the 3D Miniworld Four Rooms environment experiments.  It specifies details such as observation type (egocentric), whether frame stacking is used, color scheme (RGB), the number of training frames per task, the number of exposures and tasks per exposure, frames per epoch, batch size, epsilon decay schedule, action repeat frequency, action dimensionality, observation size, maximum frames per episode, task learning rate, and the range of slipperiness probabilities tested.", "section": "E.2 3D Miniworld Environments"}, {"figure_path": "rI7oZj1WMc/tables/tables_20_1.jpg", "caption": "Table 3: Mujoco Environment Specific Parameters", "description": "This table lists the hyperparameters used in the Mujoco environment experiments.  It includes settings related to frame stacking, color space, training frames per task, the number of exposures and tasks, action repeat frequency, batch size, feature and hidden dimensions, observation size, maximum frames per episode, successor feature dimension, task learning rate, and the frequency of task updates.", "section": "E.3 Mujoco"}, {"figure_path": "rI7oZj1WMc/tables/tables_22_1.jpg", "caption": "Table 4: Simple SF Hyperparameters", "description": "This table lists the hyperparameters used for training the Simple Successor Features (SFs) model.  It details the optimizer, discount factor, replay buffer size, target network update settings, exploration strategy, and learning rates.  It also specifies the architecture of the encoder (used to extract basis features from observations) and the successor feature network.  Different parameters are provided for the encoder and the SF network.", "section": "F Agents"}, {"figure_path": "rI7oZj1WMc/tables/tables_22_2.jpg", "caption": "Table 5: Task w encoding Hyperparameters", "description": "This table lists the hyperparameters used for training the task encoding vector w.  It includes the dimension of w, the learning rate which is dependent on the environment (see Tables 1 & 2 for details), and the optimizer used (Adam).", "section": "F Agents"}, {"figure_path": "rI7oZj1WMc/tables/tables_38_1.jpg", "caption": "Table 4: Simple SF Hyperparameters", "description": "This table lists the hyperparameters used in the Simple Successor Feature (SF) model proposed by the authors.  It covers optimizer settings, replay buffer details, target network update specifics, exploration strategy, learning rates, and network architecture aspects such as encoder and successor feature network configurations (channels, kernel sizes, non-linearities, etc.).", "section": "F Agents"}, {"figure_path": "rI7oZj1WMc/tables/tables_39_1.jpg", "caption": "Table 4: Simple SF Hyperparameters", "description": "This table lists the hyperparameters used in the Simple Successor Feature model proposed in the paper.  It covers various aspects of the model's training process, including optimizer, learning rate, replay buffer specifics, and network architecture details (e.g., number of layers, activation functions, normalization methods). The hyperparameter values are listed in the right column.", "section": "F Agents"}, {"figure_path": "rI7oZj1WMc/tables/tables_46_1.jpg", "caption": "Table 6: Correlation Analysis against analytically computed Successor Representation in the Center-Wall Environment with mean and standard deviation of the correlations. The data are categorized into three stages: before training, after training, and the observed differences post-training. The left column: Partially-observable scenarios in which our agent shows the highest correlation and greatest improvement post-training. The right column: Fully-observable scenarios where our agent and the agent with orthogonality constraints on basis features exhibit high correlation and significant post-training improvement.", "description": "This table presents the results of a correlation analysis comparing analytically computed Successor Representations (SRs) with learned Successor Features (SFs) from different RL agents in the Center-Wall environment.  The analysis is broken down by whether the environment was partially or fully observable, and considers three phases: before training, after training, and the change in correlation after training.  The table shows the mean and standard deviation of the correlation values for each agent and condition, highlighting which agents show the strongest correlation with the SRs, especially after training.  The table emphasizes the superior performance of the proposed \"Simple SF\" method, particularly in partially-observable settings.", "section": "N.5 Summary Statistics of the Correlation Analysis"}, {"figure_path": "rI7oZj1WMc/tables/tables_46_2.jpg", "caption": "Table 6: Correlation Analysis against analytically computed Successor Representation in the Center-Wall Environment with mean and standard deviation of the correlations. The data are categorized into three stages: before training, after training, and the observed differences post-training. The left column: Partially-observable scenarios in which our agent shows the highest correlation and greatest improvement post-training. The right column: Fully-observable scenarios where our agent and the agent with orthogonality constraints on basis features exhibit high correlation and significant post-training improvement.", "description": "This table presents the results of a correlation analysis comparing analytically computed Successor Representations (SRs) with learned Successor Features (SFs) from different reinforcement learning agents. The analysis is performed across two settings: partially observable and fully observable Center-Wall environments.  The table displays mean and standard deviations of the Spearman's rank correlations for three phases: before training, after training, and the difference between them. This allows for a quantitative assessment of how different SF learning methods and their resulting representations compare to the ground-truth SRs.  The results highlight the performance of the proposed method in different observation settings and its significant improvement after training compared to baselines, notably in the partially-observable case.", "section": "N.5 Summary Statistics of the Correlation Analysis"}]