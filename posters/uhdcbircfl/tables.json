[{"figure_path": "UHDCbIrCFL/tables/tables_7_1.jpg", "caption": "Table 1: Averaged quantitative evaluation on different categories. We color code each cell as best.", "description": "This table presents a quantitative comparison of the proposed Exo2Ego-V model against several state-of-the-art baselines across five different categories of daily-life activities from the Ego-Exo4D dataset.  The metrics used for comparison are PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity).  The table shows the average performance across three different experimental setups: unseen actions, unseen takes, and unseen scenes.  The color coding helps to quickly identify the best-performing method for each metric and category. ", "section": "4.2 Comparisons with SOTA approaches"}, {"figure_path": "UHDCbIrCFL/tables/tables_7_2.jpg", "caption": "Table 3: Averaged quantitative evaluation on different categories against baselines for unseen scenes.", "description": "This table presents a quantitative comparison of the proposed Exo2Ego-V model against three baseline models (PixelNeRF, Stable Diffusion, and Stable Video Diffusion) across five activity categories (Cooking, Basketball, COVID Test, CPR, and Bike) on unseen scenes.  The evaluation metrics used are PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity). Higher PSNR and SSIM values indicate better image quality, while a lower LPIPS score suggests greater perceptual similarity to the ground truth. The table highlights the superior performance of Exo2Ego-V across all metrics and categories.", "section": "4 Experiments"}, {"figure_path": "UHDCbIrCFL/tables/tables_9_1.jpg", "caption": "Table 4: Ablation results of our method.", "description": "This table presents the ablation study results for the proposed Exo2Ego-V model on the Cooking category of the Ego-Exo4D dataset.  It shows the performance of the model with different components removed or modified, allowing for an evaluation of their individual contributions.  Metrics include PSNR, SSIM, and LPIPS, measuring the quality of generated videos compared to ground truth. The variations include removing the exocentric encoder, removing the Exo2Ego prior, using different numbers of exocentric views, replacing the exocentric feature encoder with a CLIP feature extractor, and using a different spatial-temporal attention configuration.", "section": "4.3 Ablation study"}, {"figure_path": "UHDCbIrCFL/tables/tables_14_1.jpg", "caption": "Table 1: Averaged quantitative evaluation on different categories. We color code each cell as best", "description": "This table presents a quantitative comparison of the proposed Exo2Ego-V model against state-of-the-art (SOTA) methods across five categories of daily-life skilled human activities from the Ego-Exo4D dataset.  The metrics used for comparison are PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity). The results are averaged across three different test scenarios: unseen actions, unseen takes, and unseen scenes.  The color-coding helps to quickly identify the best-performing method in each category for each metric.", "section": "4.2 Comparisons with SOTA approaches"}, {"figure_path": "UHDCbIrCFL/tables/tables_15_1.jpg", "caption": "Table 5: Inference time of our method in comparison with baselines.", "description": "This table presents a comparison of the inference time required by the proposed Exo2Ego-V model and three baseline methods (Stable Video Diffusion [5], Stable Diffusion [46], and PixelNeRF [60]) to generate an 8-frame egocentric video.  The inference time is presented in seconds. The purpose is to show that the proposed method's inference time is comparable to those of established baselines.", "section": "4 Experiments"}]