[{"figure_path": "UHDCbIrCFL/figures/figures_0_1.jpg", "caption": "Figure 1: Given sparse 4 exocentric videos configured 360\u00b0 around daily-life skilled human activities such as playing basketball (upper), CPR training (lower), our Exo2Ego-V can generate corresponding egocentric videos with the same activity and environment as the exocentric videos. We encourage readers to click and play the video clips in this figure using Adobe Acrobat.", "description": "This figure shows the input to and output from the Exo2Ego-V model.  The top row shows four exocentric views (Exo-1 through Exo-4) of someone playing basketball, taken from cameras placed 360 degrees around the scene. The bottom row shows a similar example of someone performing CPR. In each case, Exo2Ego-V generates a realistic egocentric view (\"Ours\") that convincingly recreates what the person in the activity would see. The \"Ground Truth\" column provides the actual egocentric recording for comparison.", "section": "Introduction"}, {"figure_path": "UHDCbIrCFL/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of Exo2Ego-V. Given 4 exocentric videos configured 360\u00b0 around daily-life skilled human activities such as cooking (a), our multi-view exocentric encoder (b) extracts the multi-scale exocentric features as the appearance conditions for egocentric video generation, and our Exo2Ego view translation prior (c) predicts the egocentric features as the concatenation guidance for the egocentric noisy latents input. With these information, our egocentric video diffusion pipeline (d) generates the egocentric videos with the same activity and environment as the exocentric videos.", "description": "This figure illustrates the Exo2Ego-V framework, showing how it generates egocentric videos from exocentric inputs.  It breaks down the process into four key stages:\n\n(a) Input: Four exocentric videos (360\u00b0 view) showing a daily-life activity (e.g., cooking).\n(b) Multi-View Exocentric Encoder: This component processes the exocentric videos to extract multi-scale features that capture appearance and context.\n(c) Exo2Ego View Translation Prior: This prior generates spatially aligned egocentric features from the exocentric inputs as guidance for video generation.\n(d) Egocentric Video Diffusion Pipeline: This combines the features from stages (b) and (c) with noisy egocentric latents to generate the final egocentric video.", "section": "3. Method"}, {"figure_path": "UHDCbIrCFL/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative comparisons of our method against SOTA approaches on unseen actions.", "description": "This figure provides a qualitative comparison of the proposed Exo2Ego-V model against state-of-the-art (SOTA) approaches on unseen actions.  It displays example frames (t=0, t=2, t=4, t=6) of generated egocentric videos for two action categories, \"Cooking\" and \"COVID Test.\"  The results from Exo2Ego-V, Stable Video Diffusion (SVD), Stable Diffusion (SD), and PixelNeRF are shown, along with the ground truth egocentric video (Ego GT). This visual comparison allows for an assessment of the visual quality and fidelity of the different approaches in handling unseen action sequences. ", "section": "4.2 Comparisons with SOTA approaches"}, {"figure_path": "UHDCbIrCFL/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparisons of our method against SOTA approaches on unseen actions.", "description": "This figure displays a qualitative comparison of the proposed Exo2Ego-V model's performance against state-of-the-art (SOTA) approaches on unseen actions from the Cooking and COVID Test categories of the Ego-Exo4D dataset. The comparison includes the ground truth (Ego GT), the proposed method (Ours), Stable Video Diffusion (SVD), Stable Diffusion (SD), and PixelNeRF. Each row represents a different method, and the columns show the generated egocentric video frames (t=0, t=2, t=4, t=6) for a given unseen action.  The results visually showcase the superiority of Exo2Ego-V in generating realistic and detailed egocentric videos compared to the other methods.", "section": "4.2 Comparisons with SOTA approaches"}, {"figure_path": "UHDCbIrCFL/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative comparisons of our method against SOTA approaches on unseen actions.", "description": "This figure provides a qualitative comparison of the proposed Exo2Ego-V model against several state-of-the-art (SOTA) approaches on unseen actions from the Ego-Exo4D dataset.  It showcases the generated egocentric videos (t=0, t=2, t=4, t=6) for the 'Cooking' and 'COVID Test' categories, comparing Exo2Ego-V's output to the ground truth (Ego GT) and the results of Stable Video Diffusion (SVD), Stable Diffusion (SD), and PixelNeRF baselines.  The comparison allows for a visual assessment of the quality and realism of the generated egocentric videos produced by each method.", "section": "4.2 Comparisons with SOTA approaches"}, {"figure_path": "UHDCbIrCFL/figures/figures_8_2.jpg", "caption": "Figure 6: Qualitative comparisons of our method against SOTA approaches on H2O dataset.", "description": "This figure shows a qualitative comparison of the proposed Exo2Ego-V method against three state-of-the-art (SOTA) approaches on the H2O dataset.  The H2O dataset contains synchronized egocentric and exocentric videos of desktop activities.  The figure displays generated egocentric video frames for two different activities at various timesteps (t=0, t=2, t=4, t=6) for each method. The ground truth egocentric videos (EgoGT) are shown for comparison.  The goal is to assess the visual quality and accuracy of the generated videos compared to the ground truth.", "section": "4 Experiments"}, {"figure_path": "UHDCbIrCFL/figures/figures_8_3.jpg", "caption": "Figure 7: Qualitative ablation results of our method for cooking category on unseen actions.", "description": "This figure presents a qualitative comparison of the results obtained using the full Exo2Ego-V model against several ablated versions. The ablation studies involve removing key components such as the exocentric encoder, the Exo2Ego prior, and the exo temporal layer.  The results demonstrate the contribution of each component to the overall performance in terms of generating realistic and temporally coherent egocentric videos from exocentric inputs.  Each row shows the ground truth egocentric video and the corresponding results for a different ablation.", "section": "4.3 Ablation study"}, {"figure_path": "UHDCbIrCFL/figures/figures_9_1.jpg", "caption": "Figure 8: More ablation results of our method for cooking category on unseen actions.", "description": "This figure shows ablation study results on the cooking category from the Ego-Exo4D dataset. It shows the impact of different components of the proposed method, such as using different numbers of exocentric views (4, 3, 2, 1), replacing the exocentric feature encoder with CLIP features, and removing the temporal-spatial modules. The results demonstrate the effectiveness of the proposed multi-view exocentric encoder and the temporal-spatial modules in improving the quality of the generated egocentric videos.", "section": "Ablation study"}, {"figure_path": "UHDCbIrCFL/figures/figures_15_1.jpg", "caption": "Figure 1: Given sparse 4 exocentric videos configured 360\u00b0 around daily-life skilled human activities such as playing basketball (upper), CPR training (lower), our Exo2Ego-V can generate corresponding egocentric videos with the same activity and environment as the exocentric videos. We encourage readers to click and play the video clips in this figure using Adobe Acrobat.", "description": "This figure shows a comparison between generated egocentric videos from the Exo2Ego-V model and ground truth egocentric videos for two different activities: basketball and CPR.  The top row shows the basketball example and the bottom row demonstrates CPR. For each activity, there are four input exocentric videos (Exo-1 to Exo-4) arranged at 360 degrees around the activity. Following these are the model's generated egocentric view (Ours) and the ground truth egocentric video (Ground Truth). The figure visually demonstrates the model's ability to generate realistic and coherent egocentric views from limited input exocentric videos.", "section": "Introduction"}, {"figure_path": "UHDCbIrCFL/figures/figures_15_2.jpg", "caption": "Figure 11: Exo2Ego prior feature visualization.", "description": "This figure visualizes the features rendered by the Exo2Ego prior for two different actions: COVID Test and CPR. The top row shows the rendered features, while the bottom row shows the rendered pixels.  The visualization demonstrates the prior's ability to extract and transmit relevant information from egocentric views to the multi-view exocentric encoder, which is crucial for guiding the egocentric video generation process.", "section": "4. Feature Visualization"}]