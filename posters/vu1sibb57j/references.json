{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces diffusion models, the core methodology upon which the current work's policy parameterization is based."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-07-01", "reason": "This paper presents TD3, a key actor-critic algorithm used as a baseline for comparison in the experimental evaluation, highlighting its importance in the field."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-07-01", "reason": "This paper introduces SAC, another critical actor-critic algorithm serving as a baseline, showcasing its relevance to the comparative analysis."}, {"fullname_first_author": "David Silver", "paper_title": "Deterministic policy gradient algorithms", "publication_date": "2014-07-01", "reason": "This paper introduces DPG, a foundational deterministic policy gradient algorithm, which is conceptually related to the proposed diffusion policy gradient method."}, {"fullname_first_author": "Yuri Burda", "paper_title": "Exploration by random network distillation", "publication_date": "2018-10-26", "reason": "This paper introduces RND, a crucial exploration technique adopted in the current work, underscoring its significance in the exploration strategy."}]}