[{"figure_path": "vU1SiBb57j/figures/figures_1_1.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows the eight different tasks used to evaluate the proposed multimodal policy learning algorithm, DDiffPG.  The top row displays four variations of the AntMaze environment, each with a different maze layout, designed to test the agent's ability to find multiple paths to the goal.  The bottom row shows four robotic manipulation tasks: Reach (reaching for an object), Peg-in-hole (inserting a peg into a hole), Drawer-close (closing a drawer), and Cabinet-open (opening a cabinet).  These tasks were selected to present challenges requiring multiple strategies or diverse solutions to complete successfully.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of DDiffPG: (1) the agent interacts with the environment and collects a set of trajectories {Ti}. (2) Given a set of goal-reached trajectories, a DTW distance matrix is computed and used for hierarchical clustering to discover modes. (3) Each mode is associated with a set of trajectories, which is used exclusively to train mode-specific Q-functions and an exploration-specific Qexplore. (4) A multimodal batch is constructed by concatenating (s, atarget) pairs sampled from every mode and used for the diffusion policy update.", "description": "This figure illustrates the Deep Diffusion Policy Gradient (DDiffPG) algorithm's workflow. It starts with data collection where the agent interacts with the environment.  Then, mode discovery is performed using hierarchical clustering on goal-reached trajectories based on dynamic time warping (DTW) distances.  Mode-specific Q-functions, including one for exploration, are trained for each identified mode. Finally, multimodal training is performed using batches constructed from all modes to update the diffusion policy.", "section": "4 Learning Multimodal Behaviors from Scratch"}, {"figure_path": "vU1SiBb57j/figures/figures_4_1.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the proposed multimodal reinforcement learning algorithm.  The top row displays four variations of the AntMaze environment, each presenting a unique navigational challenge with multiple potential solutions. The bottom row illustrates four robotic manipulation tasks: Reach, Peg-in-hole, Drawer-close, and Cabinet-open.  These tasks also involve multiple ways to successfully complete the task, highlighting their multimodality and suitability for evaluating algorithms capable of learning diverse behaviors.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_6_1.jpg", "caption": "Figure 4: Performance of DDiffPG and baseline methods in the four AntMaze and robotic manipulation environments.", "description": "The figure shows the learning curves of DDiffPG and several baseline methods across four AntMaze environments and four robotic manipulation tasks.  Each subplot represents a different environment or task, showing the average return (cumulative reward) over training steps.  The plot visualizes the relative performance of DDiffPG compared to other state-of-the-art reinforcement learning algorithms. The shaded areas indicate the standard error.  This figure helps demonstrate DDiffPG's ability to learn and achieve good performance in complex control tasks compared to other algorithms.", "section": "5 Experiments"}, {"figure_path": "vU1SiBb57j/figures/figures_7_1.jpg", "caption": "Figure 5: Exploration maps of DDiffPG and baselines in AntMaze-v3.", "description": "This figure visualizes the exploration patterns of different reinforcement learning algorithms in the AntMaze-v3 environment.  Each subfigure shows a heatmap representing the frequency of agent visits to different locations within the maze.  The color intensity indicates the number of times each cell was visited, providing insight into the exploration strategy of each algorithm. Comparing the exploration maps helps understand the strengths and weaknesses of each algorithm concerning its ability to explore the environment thoroughly and discover different paths to the goal.", "section": "5.3 Seeking of Multimodality Encourages Exploration and Overcomes Local Minima"}, {"figure_path": "vU1SiBb57j/figures/figures_8_1.jpg", "caption": "Figure 7: Abaltion studies for key parameters of DDiffPG: (a) batch size, (b) diffusion steps, (c) update-to-data (UTD) ratio, and (d) action gradient learning rate.", "description": "This figure shows the ablation study on the key hyperparameters of the DDiffPG algorithm.  Each subfigure shows the effect of varying a single hyperparameter while keeping others constant.  (a) Batch size: impact on cumulative reward. (b) Number of diffusion steps: impact on cumulative reward. (c) UTD ratio (updates-to-data): impact on cumulative reward. (d) Action gradient learning rate: impact on cumulative reward.  The shaded area represents the standard deviation across multiple runs.", "section": "5.4 Ablation Studies"}, {"figure_path": "vU1SiBb57j/figures/figures_15_1.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the proposed method in the paper.  The top row displays four AntMaze environments, each presenting a maze with varying complexities and multiple potential solution paths for an ant agent to reach a goal. The bottom row depicts four robotic manipulation tasks: Reach (reaching a target object), Peg-in-hole (inserting a peg into a hole), Drawer-close (closing a drawer), and Cabinet-open (opening a cabinet).  These tasks all share a characteristic of having multiple ways to successfully complete them, thus demanding multimodal behavior from the learning agent.  The variety in task types (navigation and manipulation) and the complexity within each task create a rigorous benchmark for evaluating multimodal policy learning algorithms.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_15_2.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the proposed multimodal Deep Diffusion Policy Gradient (DDiffPG) algorithm. The top row displays four AntMaze environments with varying levels of complexity, each requiring the agent to navigate a maze to reach a goal.  The bottom row shows four robotic manipulation tasks: Reach (reaching a target object), Peg-in-hole (inserting a peg into a hole), Drawer-close (closing a drawer), and Cabinet-open (opening a cabinet).  These tasks are designed to be multimodal, meaning there are multiple ways to successfully complete each task, which tests the algorithm's ability to learn and utilize diverse strategies.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_16_1.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the proposed multimodal policy learning algorithm, DDiffPG.  The top row presents four variations of the AntMaze environment, each with a different maze layout designed to encourage the discovery of multiple solution paths. The bottom row depicts four robotic manipulation tasks: Reach (reaching a target), Peg-in-hole (inserting a peg into a hole), Drawer-close (closing a drawer), and Cabinet-open (opening a cabinet).  All these tasks are characterized by high multimodality, meaning there are multiple ways to successfully complete each task.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_16_2.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the performance of the proposed multimodal reinforcement learning algorithm. The top row displays four variations of the AntMaze environment, each with a different layout and level of complexity. The bottom row illustrates four robotic manipulation tasks: Reach, Peg-in-hole, Drawer-close, and Cabinet-open. These tasks are designed to be multimodal, meaning that there are multiple ways to successfully complete them.  This multimodality makes them suitable for testing the algorithm's ability to learn and utilize diverse strategies.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_16_3.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the proposed multimodal policy learning algorithm. The top row displays four AntMaze environments with varying maze layouts, while the bottom row shows four robotic manipulation tasks: reaching for an object, inserting a peg into a hole, closing a drawer, and opening a cabinet. These tasks were chosen because they each present multiple ways to solve them, providing a challenging test bed for a multimodal learning algorithm. Each task's high degree of multimodality makes them particularly useful for evaluating the ability of an agent to learn and utilize a variety of strategies for a given task.  The multimodal nature of the tasks is key to demonstrating the strength of the proposed method in contrast to traditional RL methods that typically learn single-mode policies.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_16_4.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the performance of the proposed multimodal reinforcement learning algorithm.  The top row displays four AntMaze environments, each with a unique maze structure and varying difficulty.  The bottom row illustrates four robotic manipulation tasks: Reach (reaching a target), Peg-in-hole (inserting a peg), Drawer-close (closing a drawer), and Cabinet-open (opening a cabinet). These tasks were specifically chosen due to their high degree of multimodality; meaning that each task can be solved through multiple distinct strategies or paths.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_16_5.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure showcases the eight different tasks used in the paper to evaluate the proposed multimodal policy learning algorithm.  The top row shows four variations of the AntMaze environment, which involves navigating a maze with an ant-like robot. Each version presents unique challenges in terms of path complexity and obstacle placement. The bottom row displays four robotic manipulation tasks: Reach (reaching for a target), Peg-in-hole (inserting a peg into a hole), Drawer-close (closing a drawer), and Cabinet-open (opening a cabinet). These tasks were specifically designed to demonstrate the capability of the algorithm to learn diverse strategies for achieving the same goal. They all offer a high degree of multimodality, meaning that there are multiple ways to successfully complete each task.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_16_6.jpg", "caption": "Figure 1: We design (Top) four AntMaze tasks; AntMaze-v1, AntMaze-v2, AntMaze-v3, AntMaze-v4, and (Below) four robotic tasks Reach, Peg-in-hole, Drawer-close, and Cabinet-open that have a high degree of multimodality.", "description": "This figure shows eight different tasks used to evaluate the performance of the proposed multimodal reinforcement learning algorithm.  The top row displays four variations of the AntMaze environment, each presenting a different level of complexity in terms of path finding and obstacle avoidance. The bottom row illustrates four robotic manipulation tasks: Reach (reaching a target point), Peg-in-hole (inserting a peg into a hole), Drawer-close (closing a drawer), and Cabinet-open (opening a cabinet).  All eight tasks are designed to have multiple solutions, making them ideal for testing the algorithm's ability to learn and utilize diverse strategies.", "section": "1 Introduction"}, {"figure_path": "vU1SiBb57j/figures/figures_19_1.jpg", "caption": "Figure 8: Comparison on wall-clock time.", "description": "This figure compares the computation time of DDiffPG and other baseline algorithms (DIPO, TD3, SAC) across four key stages of the algorithm: data collection, policy update, critic update, and target action update. It shows that DDiffPG requires significantly more time for data collection compared to the baselines, while the policy update time is comparable. Notably, DDiffPG has a higher critic update time due to the use of multiple Q-functions, and it also requires more time for target action updates because of the use of multimodal batches.", "section": "5.4 Ablation Studies"}, {"figure_path": "vU1SiBb57j/figures/figures_19_2.jpg", "caption": "Figure 9: Comparison of VQ-VAE clustering and projected embedding space.", "description": "This figure shows the result of using VQ-VAE for trajectory clustering.  Panel (a) displays the clustering results in the AntMaze environment, with different colors representing different clusters. Panel (b) shows the projected embedding space of the trajectories, illustrating how the VQ-VAE captures the distinct modes in the data. The figure highlights the effectiveness of VQ-VAE as an alternative approach to hierarchical clustering for identifying behavioral modes.", "section": "4.1 Unsupervised Mode Discovery"}, {"figure_path": "vU1SiBb57j/figures/figures_19_3.jpg", "caption": "Figure 5: Exploration maps of DDiffPG and baselines in AntMaze-v3.", "description": "This figure compares the exploration maps of DDiffPG and several baseline algorithms (DIPO, Diffusion-QL, SAC, and RPG) in the AntMaze-v3 environment. The maps visually represent the frequency with which the agent visits different areas of the environment during the learning process. A darker color indicates more frequent visits, suggesting a preference for certain paths or strategies. The visual comparison aims to highlight the exploration capabilities of different algorithms and demonstrate whether they tend to focus on a specific region or explore the entire environment more uniformly. DDiffPG is expected to show more diverse exploration due to its multimodal nature.", "section": "5.3 Seeking of Multimodality Encourages Exploration and Overcomes Local Minima"}, {"figure_path": "vU1SiBb57j/figures/figures_20_1.jpg", "caption": "Figure 5: Exploration maps of DDiffPG and baselines in AntMaze-v3.", "description": "This figure compares the exploration maps of DDiffPG with four baseline algorithms (DIPO, Diffusion-QL, SAC, and TD3) in the AntMaze-v3 environment.  The maps visualize the frequency with which the agent visits different areas of the maze during exploration. DDiffPG demonstrates broader exploration compared to other methods which tend to focus on a single, potentially optimal path, highlighting DDiffPG's ability to discover multiple behavioral modes.", "section": "5.3 Seeking of Multimodality Encourages Exploration and Overcomes Local Minima"}, {"figure_path": "vU1SiBb57j/figures/figures_20_2.jpg", "caption": "Figure 5: Exploration maps of DDiffPG and baselines in AntMaze-v3.", "description": "This figure shows the exploration maps for AntMaze-v3 environment, comparing DDiffPG with several baseline algorithms including DIPO, Diffusion-QL, SAC, RPG, TD3, and Consistency-AC. Each subfigure visualizes the density of visits to different states within the environment for each algorithm. DDiffPG demonstrates a wider and more thorough exploration compared to most baselines, suggesting its superior ability to avoid local minima and discover various solutions to the navigation problem. ", "section": "5.3 Seeking of Multimodality Encourages Exploration and Overcomes Local Minima"}, {"figure_path": "vU1SiBb57j/figures/figures_20_3.jpg", "caption": "Figure 5: Exploration maps of DDiffPG and baselines in AntMaze-v3.", "description": "This figure compares the exploration patterns of DDiffPG against four other reinforcement learning algorithms (DIPO, Diffusion-QL, SAC, and TD3) in the AntMaze-v3 environment.  Each subfigure shows a heatmap representing the density of visits to different states (grid cells) in the maze during training.  The heatmaps visualize how extensively the algorithms explore various paths and areas within the maze during learning. Differences in exploration patterns indicate the algorithms' exploration strategies and their effectiveness in covering the state space.", "section": "5.3 Seeking of Multimodality Encourages Exploration and Overcomes Local Minima"}, {"figure_path": "vU1SiBb57j/figures/figures_20_4.jpg", "caption": "Figure 13: (a)-(d) Q-value maps of baselines and DDiffPG in Antmaze-v2.", "description": "This figure displays heatmaps visualizing Q-values for different baselines and the proposed DDiffPG method within the AntMaze-v2 environment.  Each subfigure represents a different algorithm (DDiffPG, DIPO, Diffusion-QL, SAC, TD3, Consistency-AC, and RPG) and shows the Q-values across the state space, providing a visual representation of how each algorithm evaluates different states in terms of expected future rewards. Warmer colors (yellow/red) indicate higher Q-values, suggesting more promising states, while cooler colors (blue) indicate lower Q-values.", "section": "5.2 DDiffPG Masters Multimodal Behaviors"}, {"figure_path": "vU1SiBb57j/figures/figures_21_1.jpg", "caption": "Figure 5: Exploration maps of DDiffPG and baselines in AntMaze-v3.", "description": "The figure shows exploration maps of five different reinforcement learning algorithms on the AntMaze-v3 environment. The algorithms are DDiffPG, DIPO, Diffusion-QL, SAC, and TD3. Each map visualizes the density of visits to each cell in the maze during exploration. DDiffPG explores multiple paths more effectively, demonstrating its superior exploration capability compared to the other baselines. This highlights DDiffPG's ability to discover multiple solutions for solving the same task in a complex environment.", "section": "5.3 Seeking of Multimodality Encourages Exploration and Overcomes Local Minima"}, {"figure_path": "vU1SiBb57j/figures/figures_21_2.jpg", "caption": "Figure 5: Exploration maps of DDiffPG and baselines in AntMaze-v3.", "description": "This figure compares the exploration patterns of different reinforcement learning algorithms in the AntMaze-v3 environment. The maps visually represent the density of visits to different cells in the maze during exploration. DDiffPG is shown to explore multiple paths to reach the goals, indicating multimodal behavior. In contrast, other baselines tend to focus on a single path, demonstrating a lack of multimodality.", "section": "5.3 Seeking of Multimodality Encourages Exploration and Overcomes Local Minima"}, {"figure_path": "vU1SiBb57j/figures/figures_21_3.jpg", "caption": "Figure 4: Performance of DDiffPG and baseline methods in the four AntMaze and robotic manipulation environments.", "description": "This figure shows the learning curves for DDiffPG and several baseline reinforcement learning algorithms across four AntMaze environments and four robotic manipulation tasks.  The y-axis represents the average return (reward) achieved by the agents, and the x-axis represents the number of training steps.  The plot visualizes the performance of each algorithm over time, allowing for a comparison of their learning efficiency and final performance on these challenging control tasks. Each line represents a different algorithm, and shaded regions indicate standard error across multiple runs. This provides a quantitative evaluation of DDiffPG's ability to learn multimodal behaviors compared to established RL methods.", "section": "5 Experiments"}]