[{"figure_path": "7Kz7icCZ6H/figures/figures_0_1.jpg", "caption": "Figure 2: CALVIN: The architecture has 3 main components. (1) A frozen image embedding extractor I, (2) Non-linear projection module Q, and (3) An LLM. We train the model in 2 stages. Stage 1, we train only the projection module Q on image caption data. Stage 2, we use instruction formatted higher quality image-video data and finetune the parameters of Q and LLM. Refer to Sec. 3 for more details. Image and video examples shown here are synthetically generated using Meta Imagine [1].", "description": "The figure shows the architecture of CALVIN, a contextual video captioning model. It consists of three main components: a frozen image embedding extractor, a non-linear projection module, and a large language model (LLM). The model is trained in two stages. In Stage 1, only the projection module is trained on image caption data. In Stage 2, higher-quality image-video data is used, and the parameters of both the projection module and the LLM are fine-tuned.  Synthetic images and videos are used for illustrative purposes.", "section": "3 CALVIN: A contextual video captioner"}, {"figure_path": "7Kz7icCZ6H/figures/figures_3_1.jpg", "caption": "Figure 2: CALVIN: The architecture has 3 main components. (1) A frozen image embedding extractor I, (2) Non-linear projection module Q, and (3) An LLM. We train the model in 2 stages. Stage 1, we train only the projection module Q on image caption data. Stage 2, we use instruction formatted higher quality image-video data and finetune the parameters of Q and LLM. Refer to Sec. 3 for more details. Image and video examples shown here are synthetically generated using Meta Imagine [1].", "description": "This figure illustrates the architecture of CALVIN, a contextual video captioning model.  It's a two-stage training process. Stage 1 trains only the projection module (Q) using image caption data;  Stage 2 fine-tunes both the projection module and the Language Model (LLM) using higher-quality image and video data formatted as instructions.  The model uses three core components:  a frozen image embedding extractor, a non-linear projection module, and an LLM.", "section": "3 CALVIN: A contextual video captioner"}, {"figure_path": "7Kz7icCZ6H/figures/figures_19_1.jpg", "caption": "Figure 2: CALVIN: The architecture has 3 main components. (1) A frozen image embedding extractor I, (2) Non-linear projection module Q, and (3) An LLM. We train the model in 2 stages. Stage 1, we train only the projection module Q on image caption data. Stage 2, we use instruction formatted higher quality image-video data and finetune the parameters of Q and LLM. Refer to Sec. 3 for more details. Image and video examples shown here are synthetically generated using Meta Imagine [1].", "description": "This figure illustrates the architecture of CALVIN, a contextual video captioning model.  It consists of three main components: a frozen image embedding extractor, a non-linear projection module, and a language model (LLM). The model is trained in two stages.  Stage 1 trains only the projection module using image caption data.  Stage 2 fine-tunes both the projection module and the LLM using higher-quality image and video data formatted as instructions.  The example images and videos in the diagram are synthetically generated.", "section": "3 CALVIN: A contextual video captioner"}, {"figure_path": "7Kz7icCZ6H/figures/figures_19_2.jpg", "caption": "Figure 1: A scene from MAD [80]-eval split. We present the captions generated by our model, represented as CALVIN against various off-the-shelf LLMs, with hallucinations highlighted in red. First, our model utilizes the context well, by understanding the name of the character is 'Lenihan' and that there is an alien in the scene, and second, our model has less hallucination and verbosity compared to other models.", "description": "This figure shows a comparison of video captions generated by the proposed model, CALVIN, and several other existing models for a scene from the MAD dataset.  The figure highlights how CALVIN leverages contextual information (the name of a character and the presence of an alien) to produce a more accurate and concise caption, compared to the other models which either hallucinate details or produce overly verbose descriptions.", "section": "Introduction"}, {"figure_path": "7Kz7icCZ6H/figures/figures_20_1.jpg", "caption": "Figure 10: Number of ground-truth samples vs Metrics.", "description": "This figure shows how different metrics (BertScore, CIDEr, ROUGE-L, and SPICE) for video captioning performance change with varying numbers of ground truth samples used in few-shot fine-tuning.  It demonstrates that increasing the number of training samples improves these metrics, but the gains diminish beyond around 50 samples.  This suggests a point of diminishing returns in using more than a certain amount of samples for this specific adaptation task.", "section": "Fewshot finetuning"}]