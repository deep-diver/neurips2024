[{"figure_path": "Uymv9ThB50/figures/figures_2_1.jpg", "caption": "Figure 1: Test accuracy of Pm on different layers of LLMs.", "description": "This figure shows the test accuracy of the safety concept activation vector (SCAV) classifier, Pm, on different layers of four LLMs: Vicuna-v1.5-7B, Llama-2-7B-chat-hf, Llama-2-13B-chat-hf, and Alpaca-7B (unaligned).  The x-axis represents the layer index, and the y-axis represents the test accuracy of Pm (%). The figure demonstrates that for well-aligned LLMs (Vicuna and Llama-2), the test accuracy increases significantly from the 10th or 11th layer onwards, reaching over 98% at the final layers.  This indicates that a linear classifier can accurately interpret the safety mechanism and that these LLMs begin to model safety from these middle layers. In contrast, the unaligned LLM (Alpaca) shows a much lower test accuracy, indicating a lack of explicit safety modeling.", "section": "2.2 SCAV Framework"}, {"figure_path": "Uymv9ThB50/figures/figures_3_1.jpg", "caption": "Figure 2: Comparison of perturbations added by our method (SCAV) and the baselines RepE [16] and JRE [18]. Our method consistently moves embeddings of malicious instructions to the subspace of safe instructions, while the baselines may result in ineffective or even opposite perturbations.", "description": "This figure compares the effectiveness of different methods in perturbing embeddings to evade safety mechanisms. SCAV consistently moves malicious instruction embeddings into the safe subspace, while RepE and JRE show inconsistent and sometimes opposite results, highlighting SCAV's superior performance.", "section": "2.3 Embedding-Level Attack"}, {"figure_path": "Uymv9ThB50/figures/figures_5_1.jpg", "caption": "Figure 3: ASR-keyword vs. training data size on Advbench, LLaMA-2-7B-Chat. Shaded backgrounds denote variations.", "description": "This figure shows the impact of the training dataset size on the ASR-keyword metric for three different attack methods: SCAV, RepE, and JRE.  The x-axis represents the number of malicious and safe instruction pairs used for training. The y-axis represents the ASR-keyword, indicating the success rate of the attack.  The shaded areas around each line represent the variance in ASR-keyword across multiple runs with different subsets of the training data. The figure clearly demonstrates that the SCAV method requires significantly less training data to achieve high ASR-keyword, showcasing its efficiency and stability in comparison to the baseline methods RepE and JRE.", "section": "3.2 Embedding-Level Attack Results"}, {"figure_path": "Uymv9ThB50/figures/figures_8_1.jpg", "caption": "Figure 4: Unveiling the safety mechanisms of LLMs by (a) attacking a single layer; (b) attacking multiple layers, and (c) transferring embedding-level attacks to other white-box LLMs.", "description": "This figure demonstrates three key aspects of the safety mechanisms of LLMs.  (a) shows the effect of attacking individual layers, revealing that attacks on linearly separable layers (where the safety concept is well-defined) consistently increase attack success rates. (b) shows that attacking multiple layers significantly improves attack success, suggesting a cumulative or interconnected safety mechanism across layers.  Finally, (c) illustrates the transferability of attacks across different LLMs, meaning that attacks developed on one model can often succeed on others, highlighting potential vulnerabilities in model design and alignment.", "section": "4.3 How Do Aligned LLMs Differentiate Malicious Instructions from Others?"}, {"figure_path": "Uymv9ThB50/figures/figures_12_1.jpg", "caption": "Figure 5: A Pipeline Demonstration for Conducting Embedding-Level and Prompt-Level Attacks Using SCAVs.", "description": "This figure illustrates the pipeline used for conducting both embedding-level and prompt-level attacks using the Safety Concept Activation Vector (SCAV) framework.  It shows three conceptual LLMs (A, B, and C) representing different stages of the attack process.  For embedding-level attacks, Model A extracts embeddings from a malicious query, while Model B generates a response. Algorithm 1 is applied to perturb the embeddings in between. For prompt-level attacks, Model C generates attack prompts that are combined with the original user input to manipulate the response generated by Model B.", "section": "A Pipeline Overview"}, {"figure_path": "Uymv9ThB50/figures/figures_15_1.jpg", "caption": "Figure 1: Test accuracy of  Pm on different layers of LLMs.", "description": "The figure shows the test accuracy of the safety concept activation vector (SCAV) classifier (Pm) on different layers of various LLMs.  The x-axis represents the layer index, and the y-axis represents the test accuracy.  The results indicate that for well-aligned LLMs (Vicuna and LLaMA-2), the test accuracy is high (above 95%) from the 10th or 11th layer onwards, suggesting that LLMs begin to model the safety concept from around these layers. In contrast, the test accuracy is significantly lower for the unaligned LLM (Alpaca). This demonstrates the linear interpretability assumption of the SCAV framework which helps to accurately interpret LLMs' safety mechanisms.", "section": "2.2 SCAV Framework"}, {"figure_path": "Uymv9ThB50/figures/figures_18_1.jpg", "caption": "Figure 7: Test accuracy of P<sub>m</sub> on different layers of other LLMs.", "description": "This figure shows the test accuracy of the safety concept linear classifier (P<sub>m</sub>) on different layers of various LLMs.  The x-axis represents the layer index, and the y-axis represents the test accuracy.  Multiple lines are shown, each representing a different LLM. The results indicate that for most LLMs, the accuracy of P<sub>m</sub> is relatively low in the early layers, but increases sharply to over 90% and remains high until the final layers.  This suggests that LLMs generally start to model the safety concept from intermediate layers.", "section": "D Linear Interpretability Information"}, {"figure_path": "Uymv9ThB50/figures/figures_18_2.jpg", "caption": "Figure 8: Visualization of embeddings of LLaMA-2-7B-Chat.", "description": "This figure visualizes the embeddings of LLaMA-2-7B-Chat using t-SNE for three different layers (0, 15, and 31). Each point represents an embedding, colored green for safe instructions and red for malicious instructions.  The visualization aims to demonstrate the linear separability of malicious and safe instructions in the hidden space of the LLM.  The increasing separation between the two classes as the layer index increases (from 0 to 31) suggests that the LLM's safety mechanism is better developed in deeper layers.", "section": "D.2 t-SNE Visualization of Embeddings"}, {"figure_path": "Uymv9ThB50/figures/figures_18_3.jpg", "caption": "Figure 9: Visualization of embeddings of Alpaca-7B.", "description": "This figure shows the t-SNE visualization of embeddings from the Alpaca-7B model.  It illustrates the distribution of embeddings for malicious and safe instructions at different layers (0, 15, and 31).  Unlike Figure 8 (LLaMA-2-7B-Chat), which shows clear linear separability between malicious and safe instructions in later layers, Figure 9 demonstrates a lack of separation in Alpaca-7B, indicating a difference in how the models represent and handle the safety concept.", "section": "D.2 t-SNE Visualization of Embeddings"}, {"figure_path": "Uymv9ThB50/figures/figures_24_1.jpg", "caption": "Figure 10: How ASR-keyword changes with the choice of a layer according to our embedding-level attack algorithm. Victim LLM is LLaMA-2 (7B-Chat). The dataset is Advbench. (*) This is because perturbing layer 0 causes the output to be all garbled, thus ASR-keyword is all misjudged. After our manual inspection, the value here should be 0.", "description": "This figure shows the impact of selecting a single layer for perturbation in embedding-level attacks.  It plots the attack success rate (ASR-keyword) against the layer index (0-31) of the LLaMA-2 7B-Chat language model.  The dataset used is Advbench. Notably, perturbing layer 0 results in completely garbled outputs, leading to a misjudged ASR-keyword (indicated by the asterisk).  Correctly, perturbing only layer 0 yields an ASR-keyword of 0%. The figure demonstrates that choosing a single layer for the attack is ineffective and that a multi-layer approach, as proposed by the authors, is necessary to achieve high attack success rates.", "section": "F.3 How the Layer Selection Works for Attacks"}, {"figure_path": "Uymv9ThB50/figures/figures_25_1.jpg", "caption": "Figure 11: How selection probability changes with the layer according to our embedding-level attack algorithm. Victim LLM is LLaMA-2 (7B-Chat). The dataset is Advbench.", "description": "This figure shows the probability of each layer being selected for perturbation in the embedding-level attack.  The x-axis represents the layer number (0-31), and the y-axis shows the probability. The algorithm shows a clear preference for perturbing layers between 13 and 23, indicating these layers are most effective in manipulating the LLM's safety mechanisms. Layers outside of this range have much lower selection probabilities.", "section": "F.3 How the Layer Selection Works for Attacks"}, {"figure_path": "Uymv9ThB50/figures/figures_25_2.jpg", "caption": "Figure 10: How ASR-keyword changes with the choice of a layer according to our embedding-level attack algorithm. Victim LLM is LLaMA-2 (7B-Chat). The dataset is Advbench. (*) This is because perturbing layer 0 causes the output to be all garbled, thus ASR-keyword is all misjudged. After our manual inspection, the value here should be 0.", "description": "This figure shows the impact of selecting a single layer for perturbation in embedding-level attacks.  It reveals that targeting a single layer (except layer 0 which resulted in garbled outputs) is insufficient for achieving high attack success rates (ASR-keyword).  Perturbing multiple layers is necessary to significantly increase ASR.", "section": "F.3 How the Layer Selection Works for Attacks"}, {"figure_path": "Uymv9ThB50/figures/figures_25_3.jpg", "caption": "Figure 1: Test accuracy of P<sub>m</sub> on different layers of LLMs.", "description": "The figure shows the test accuracy of the safety concept activation vector (SCAV) classifier P<sub>m</sub> across different layers of various LLMs.  The x-axis represents the layer index, and the y-axis represents the test accuracy.  It demonstrates that for aligned LLMs (Vicuna and LLaMA-2), the test accuracy surpasses 95% starting from the 10th or 11th layer and increases to over 98% in the final layers. This suggests that these models start to model the safety concept from around the 10th or 11th layer. In contrast, the unaligned LLM (Alpaca) exhibits significantly lower test accuracy, highlighting the difference in safety mechanism modeling between aligned and unaligned models.  The figure is used to verify the assumption of linear interpretability used in the SCAV framework.", "section": "2.2 SCAV Framework"}, {"figure_path": "Uymv9ThB50/figures/figures_26_1.jpg", "caption": "Figure 14: Results of ASR-keyword of three attack methods under different perturbation magnitude.", "description": "This figure shows the relationship between the L2-norm of the perturbation and the ASR-keyword for three different attack methods: SCAV, RepE, and JRE.  The x-axis represents the L2-norm of the perturbation, and the y-axis represents the ASR-keyword (attack success rate based on keyword matching).  The plot shows that as the L2-norm increases, the ASR-keyword also increases for all three methods, but SCAV consistently outperforms RepE and JRE, achieving a higher ASR-keyword for the same perturbation magnitude.  This demonstrates the effectiveness of SCAV's method for generating perturbations that are more likely to lead to successful attacks.", "section": "F.4 How the Perturbation Vector Direction Benefits Attacks"}, {"figure_path": "Uymv9ThB50/figures/figures_32_1.jpg", "caption": "Figure 1: Test accuracy of P<sub>m</sub> on different layers of LLMs.", "description": "This figure shows the test accuracy of the safety concept activation vector (SCAV) classifier (P<sub>m</sub>) on different layers of various LLMs.  The x-axis represents the layer index, and the y-axis represents the test accuracy.  The figure shows that for well-aligned LLMs (Vicuna and LLaMA-2), the test accuracy is high (above 95%) from the 10th or 11th layer onwards, indicating that the LLMs have successfully learned the safety concept. However, for the unaligned LLM (Alpaca), the test accuracy is much lower. This suggests that the linear interpretability assumption of SCAV holds well for aligned LLMs but not for unaligned LLMs.", "section": "2.2 SCAV Framework"}, {"figure_path": "Uymv9ThB50/figures/figures_33_1.jpg", "caption": "Figure 1: Test accuracy of Pm on different layers of LLMs.", "description": "The figure shows the test accuracy of the safety concept activation vector (SCAV) classifier Pm on different layers of various LLMs.  It demonstrates the linear separability of malicious and safe instructions in the hidden space of aligned LLMs (Vicuna and LLaMA-2), with accuracy exceeding 95% from the 10th or 11th layer onwards.  In contrast, the unaligned LLM (Alpaca) shows significantly lower accuracy, highlighting the difference in safety mechanisms between aligned and unaligned models.  The x-axis represents the layer index, and the y-axis represents the test accuracy of the classifier.", "section": "2.2 SCAV Framework"}, {"figure_path": "Uymv9ThB50/figures/figures_33_2.jpg", "caption": "Figure 1: Test accuracy of Pm on different layers of LLMs.", "description": "This figure shows the test accuracy of the safety concept activation vector (SCAV) classifier (Pm) on different layers of various LLMs.  It demonstrates that for well-aligned LLMs (Vicuna and LLaMA-2), the test accuracy is consistently high from layer 10 onwards, reaching over 98% at the final layers, which indicates strong linear separability between safe and malicious instructions in the hidden space. This supports the assumption of linear interpretability used in the SCAV framework. In contrast, the unaligned LLM (Alpaca) shows much lower accuracy, highlighting the difference in how safety concepts are captured and modeled between aligned and unaligned LLMs.", "section": "2.2 SCAV Framework"}]