[{"figure_path": "3HpCVZV9it/tables/tables_4_1.jpg", "caption": "Table 1: Scaling factor \u03c9\u03b8(x, y1, y2, p) in the gradient of loss function (Equation 14). The estimated preference probabilities \u03c1\u03b8 and \u03c1\u03b8\u2032 are defined in Equation 15. Compared to others, geometric averaging has a product of (2p\u02c6\u22121) in a scaling factor, which forces the norm of gradients from the equally preferable responses close to zero.", "description": "This table summarizes the scaling factor w\u03b8 for different preference optimization methods.  The scaling factor influences the gradient of the loss function, adjusting the learning process.  The table highlights how the geometric averaging approach (GDPO, GIPO, GROPO) modifies the scaling factor, specifically by introducing a (2p\u0302\u22121) term. This term effectively dampens the gradient when the soft preference label p\u0302 indicates near-equal preference between response pairs (p\u0302\u22480.5), preventing over-optimization and objective mismatch. In contrast, methods like DPO and cDPO exhibit different scaling behaviors, which can lead to suboptimal performance in scenarios with many modestly confident labels.", "section": "3.2 Geometric Averaging Can Adjust the Scale of Gradients"}, {"figure_path": "3HpCVZV9it/tables/tables_6_1.jpg", "caption": "Table 2: Winning rate on Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan datasets, judged by PaLM 2-L-IT. We evaluate pairs of outputs with binary judge (Binary) and percentage judge (%). The methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (CDPO, CIPO). In particular, geometric averaging has a larger performance gain on Plasma Plan, Skewed, and Stairs datasets, which have richer modestly-confident labels, than others. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with p < 0.01 on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation.", "description": "This table presents the winning rates of different preference optimization methods on four datasets: Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan.  The winning rate is calculated using both binary and percentage judgements by a PaLM 2-L instruction-tuned model.  The results show that methods incorporating weighted geometric averaging consistently outperform other methods, especially on datasets with a higher proportion of moderately confident preference labels. Statistical significance testing confirms the superiority of the geometric averaging methods.", "section": "5 Results"}, {"figure_path": "3HpCVZV9it/tables/tables_24_1.jpg", "caption": "Table 2: Winning rate on Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan datasets, judged by PaLM 2-L-IT. We evaluate pairs of outputs with binary judge (Binary) and percentage judge (%). The methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (CDPO, CIPO). In particular, geometric averaging has a larger performance gain on Plasma Plan, Skewed, and Stairs datasets, which have richer modestly-confident labels, than others. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with p < 0.01 on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation.", "description": "This table presents the results of comparing different preference optimization methods on four datasets: Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan.  The methods are categorized into baseline algorithms (SFT, DPO, CDPO, IPO, CIPO, ROPO) and those using geometric averaging (GDPO, GIPO, GROPO). The table shows the winning rate for each method using two judgment types: binary and percentage. The results highlight that geometric averaging methods consistently outperform baseline methods, especially on datasets with more modestly-confident labels.  Statistical significance testing (p<0.01 using Wilcoxon signed-rank test) confirms the superiority of the geometric averaging methods.", "section": "5.1 Weighted Geometric Averaging Improves the Alignment Performance"}, {"figure_path": "3HpCVZV9it/tables/tables_24_2.jpg", "caption": "Table 2: Winning rate on Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan datasets, judged by PaLM 2-L-IT. We evaluate pairs of outputs with binary judge (Binary) and percentage judge (%). The methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (CDPO, CIPO). In particular, geometric averaging has a larger performance gain on Plasma Plan, Skewed, and Stairs datasets, which have richer modestly-confident labels, than others. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with p < 0.01 on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation.", "description": "This table presents the win rates of different methods on four datasets: Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan.  The win rate is calculated using both binary and percentage judgments. The table highlights that methods incorporating geometric averaging consistently outperform methods without it, especially when the datasets contain more moderately confident labels.  Statistical significance testing supports these findings.", "section": "5.1 Weighted Geometric Averaging Improves the Alignment Performance"}, {"figure_path": "3HpCVZV9it/tables/tables_24_3.jpg", "caption": "Table 5: Agreement accuracy between human and LLM judges.", "description": "This table presents the agreement accuracy between human judges and Large Language Model (LLM) judges (using PaLM 2-L and GPT-3.5) on a preference task.  The numbers represent the counts of agreements between the human judges and each LLM judge, and the overall accuracy reflects the level of concordance between human and LLM assessments of preference.", "section": "J Agreement between Human and LLM-as-a-Judge"}, {"figure_path": "3HpCVZV9it/tables/tables_28_1.jpg", "caption": "Table 2: Winning rate on Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan datasets, judged by PaLM 2-L-IT. We evaluate pairs of outputs with binary judge (Binary) and percentage judge (%). The methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (CDPO, CIPO). In particular, geometric averaging has a larger performance gain on Plasma Plan, Skewed, and Stairs datasets, which have richer modestly-confident labels, than others. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with p < 0.01 on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation.", "description": "This table presents the winning rate results for several different methods on four different datasets.  The methods are compared using both binary and percentage judgments.  The table shows that methods using geometric averaging consistently outperform other methods, especially when the datasets contain many labels with moderate confidence.", "section": "5.1 Weighted Geometric Averaging Improves the Alignment Performance"}, {"figure_path": "3HpCVZV9it/tables/tables_28_2.jpg", "caption": "Table 2: Winning rate on Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan datasets, judged by PaLM 2-L-IT. We evaluate pairs of outputs with binary judge (Binary) and percentage judge (%). The methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (CDPO, CIPO). In particular, geometric averaging has a larger performance gain on Plasma Plan, Skewed, and Stairs datasets, which have richer modestly-confident labels, than others. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with p < 0.01 on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation.", "description": "This table presents the winning rates of different methods (SFT, DPO, CDPO, GDPO, IPO, CIPO, GIPO, ROPO, GROPO) on four datasets: Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan.  The winning rate is calculated using two different judging methods: binary and percentage. The table highlights the consistently superior performance of methods employing geometric averaging (GDPO, GIPO, GROPO) compared to baselines, especially when the datasets contain a higher proportion of moderately confident labels. Statistical significance testing confirms these results.", "section": "5.1 Weighted Geometric Averaging Improves the Alignment Performance"}, {"figure_path": "3HpCVZV9it/tables/tables_29_1.jpg", "caption": "Table 7: Winning rate on Anthropic Helpful and Harmless datasets. We finetune LLMs with both datasets simultaneously, which simulate the preferences from multiple aspects. While DPO suffers from the conflict of preference dropping its performance, soft preference methods, especially GDPO could mitigate such conflict issues best.", "description": "This table presents the results of an experiment designed to evaluate the performance of different methods in aligning LLMs with multiple, potentially conflicting preferences.  The experiment uses the Anthropic Helpfulness and Harmlessness datasets simultaneously during training.  The table shows the winning rates (binary and percentage) achieved by different methods, comparing the output of the trained LLMs to a reference model (PaLM 2-L).  The key finding is that GDPO (the proposed method) outperforms other methods, particularly DPO, which shows performance degradation due to the conflicting preferences.", "section": "M Alignment with Orthogonal Multiple Preference Labels"}, {"figure_path": "3HpCVZV9it/tables/tables_29_2.jpg", "caption": "Table 2: Winning rate on Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan datasets, judged by PaLM 2-L-IT. We evaluate pairs of outputs with binary judge (Binary) and percentage judge (%). The methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (CDPO, CIPO). In particular, geometric averaging has a larger performance gain on Plasma Plan, Skewed, and Stairs datasets, which have richer modestly-confident labels, than others. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with p < 0.01 on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation.", "description": "This table presents the winning rates of different preference optimization methods on four datasets: Reddit TL;DR, Anthropic Helpful, Anthropic Harmless, and Plasma Plan.  The winning rate is calculated using both binary and percentage judgments.  The results show that methods incorporating geometric averaging consistently outperform those without, particularly when datasets contain a higher proportion of moderately confident labels.", "section": "5.1 Weighted Geometric Averaging Improves the Alignment Performance"}]