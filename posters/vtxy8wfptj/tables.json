[{"figure_path": "Vtxy8wFpTj/tables/tables_4_1.jpg", "caption": "Table 1: Worst-case and and average normalized reward on the MovieLens dataset. The best results among algorithms w/o ML predictions and the best results among ML-based algorithms are highlighted in bold font.", "description": "This table presents the worst-case and average normalized rewards achieved by various online algorithms on the MovieLens dataset.  It compares algorithms that do not use machine learning (ML) predictions (Greedy, PrimalDual, MetaAd) against algorithms that do use ML predictions (ML, LOBM-0.8, LOBM-0.5, LOBM-0.3). The best performing algorithm in each category (with and without ML predictions) is highlighted in bold.", "section": "6 Empirical Results"}, {"figure_path": "Vtxy8wFpTj/tables/tables_5_1.jpg", "caption": "Table 1: Worst-case and and average normalized reward on the MovieLens dataset. The best results among algorithms w/o ML predictions and the best results among ML-based algorithms are highlighted in bold font.", "description": "This table presents the worst-case and average normalized rewards achieved by different algorithms on the MovieLens dataset.  It compares algorithms without machine learning (ML) predictions (Greedy, PrimalDual, MetaAd) against ML-based algorithms (ML, LOBM with different lambda values). The best results for both categories (with and without ML) are highlighted, showcasing the relative performance of each approach in terms of both average reward and worst-case performance.", "section": "6 Empirical Results"}, {"figure_path": "Vtxy8wFpTj/tables/tables_9_1.jpg", "caption": "Table 1: Worst-case and and average normalized reward on the MovieLens dataset. The best results among algorithms w/o ML predictions and the best results among ML-based algorithms are highlighted in bold font.", "description": "This table presents the worst-case and average normalized rewards achieved by various algorithms on the MovieLens dataset.  It compares algorithms that do not use machine learning (ML) predictions (Greedy, PrimalDual, MetaAd) against ML-based algorithms (ML, LOBM with different lambda values).  The best results for each category (with and without ML) are highlighted in bold, indicating the superior performance of certain methods under different evaluation metrics.", "section": "6 Empirical Results"}, {"figure_path": "Vtxy8wFpTj/tables/tables_23_1.jpg", "caption": "Table 1: Worst-case and and average normalized reward on the MovieLens dataset. The best results among algorithms w/o ML predictions and the best results among ML-based algorithms are highlighted in bold font.", "description": "This table presents the worst-case and average normalized rewards achieved by different algorithms on the MovieLens dataset.  The algorithms are categorized into those without machine learning (ML) predictions (Greedy, PrimalDual, MetaAd) and those using ML predictions (ML, LOBM-0.8, LOBM-0.5, LOBM-0.3).  The best-performing algorithms in each category (worst-case and average) are highlighted in bold. The results show a comparison of performance between algorithms with and without ML components, illustrating the impact of incorporating ML predictions on both worst-case robustness and average performance.", "section": "6 Empirical Results"}, {"figure_path": "Vtxy8wFpTj/tables/tables_26_1.jpg", "caption": "Table 3: Worst-case and average rewards of different algorithms for VM placement. The worst-case and average rewards are normalized by optimal rewards. We compare MetaAd with the algorithms without using ML (Greedy and PrimalDual introduced in Section D.1.1). Additionally, we compare our learning-augmented algorithm LOBM with the ML algorithm. LOBM-\u03bb means LOBM with a slackness parameter \u03bb in Eqn. (28).", "description": "This table compares the worst-case and average rewards of different algorithms for the VM placement problem.  It shows the performance of algorithms without machine learning (ML) predictions (Greedy, PrimalDual, MetaAd) and algorithms with ML predictions (ML, LOBM with different \u03bb values).  The rewards are normalized by the optimal rewards.  LOBM-\u03bb refers to the learning-augmented algorithm LOBM using a slackness parameter \u03bb.", "section": "D.2.3 Results"}]