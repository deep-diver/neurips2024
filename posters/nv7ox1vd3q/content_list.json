[{"type": "text", "text": "Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chiraag Kaushik ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Justin Romberg ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Electrical and Computer Engineering Georgia Institute of Technology Atlanta, GA 30308 ckaushik7@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Electrical and Computer Engineering Georgia Institute of Technology Atlanta, GA 30308 jrom@ece.gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Vidya Muthukumar Electrical and Computer Engineering, Industrial & Systems Engineering Atlanta, GA 30308 vmuthukumar8@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The classical iteratively reweighted least-squares (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step. Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and $\\ell_{p}$ -norm minimization. Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models. In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks. Our analysis operates in a \u201cbatched\u201d setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations. We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many high-dimensional machine learning and signal processing tasks rely on solving optimization problems with regularizers that explicitly enforce certain structure on the learned parameters. The traditional formulation for such tasks involves a regularized empirical risk minimization (ERM) problem of the form ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{R}^{d}}L(\\pmb{\\theta})+\\lambda R(\\pmb{\\theta}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $L(\\cdot)$ is a loss function that encourages fidelity to the observed training data and $R(\\cdot)$ encodes desirable structural properties. In many important applications, it is desirable to obtain a sparsityseeking solution; in such cases, the regularizer is typically non-smooth, as in the LASSO, group LASSO, and nuclear norm regularizers. As an alternative approach to this non-smooth optimization, several recent works have proposed the \u201cHadamard over-parameterization\u201d of $\\pmb{\\theta}$ into the entrywise product of two factors ${\\textbf{\\em u}}({\\boldsymbol{\\cdot}})~{\\boldsymbol{v}}$ . While the resulting minimization problem is non-convex, this parameterization, coupled with a smooth regularizer, has been shown to achieve competitive empirical performance (in terms of numerical stability, robustness, and convergence rate) when compared to traditional sparse recovery algorithms [13, 25]. For example, rather than solving the convex, but nonsmooth LASSO (where $L$ is the squared loss and $R$ is the $\\ell_{1}$ norm), the Hadamard reparameterization yields the following non-convex and smooth formulation: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{u},\\pmb{v}\\in\\mathbb{R}^{d}}L(\\pmb{u}\\odot\\pmb{v})+\\frac{\\lambda}{2}(\\|\\pmb{u}\\|_{2}^{2}+\\|\\pmb{v}\\|_{2}^{2}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In the case where the regression function is linear in $\\pmb{\\theta}$ , solving (2) is equivalent to learning a function of the form ", "page_idx": 1}, {"type": "equation", "text": "$$\nf_{\\pmb{u},\\pmb{v}}(\\pmb{x})=\\langle\\pmb{x},\\pmb{u}\\odot\\pmb{v}\\rangle=\\langle\\mathrm{diag}(\\pmb{v})\\pmb{x},\\pmb{u}\\rangle,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which can be thought of as a one hidden layer neural network with linear activation function and inner weight matrix $\\mathrm{diag}(v)$ . In this context, this linear diagonal neural network $(L D N N)$ architecture has also been studied as an illustrative case study to improve our understanding of how neural networks perform iterative \u201cfeature learning\u201d to leverage low-dimensional structure in high-dimensional settings [34, 23]. We note here that, in the linear model case, feature learning is equivalent to learning which subset of the input\u2019s coordinates are relevant for the true predictor (i.e., feature selection). ", "page_idx": 1}, {"type": "text", "text": "One way to understand the connection between classical sparse recovery algorithms and the Hadamard product/LDNN form in (2) is to consider the change of variable $v_{i}\\to\\sqrt{\\eta_{i}}$ and $\\begin{array}{r}{u_{i}\\rightarrow\\frac{\\theta_{i}}{\\sqrt{\\eta_{i}}}}\\end{array}$ [25]. This yields the following optimization problem, which is jointly convex in $\\eta$ and $\\pmb{\\theta}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta}\\in\\mathbb{R}^{d}}\\operatorname*{min}_{\\pmb{\\eta}\\in\\mathbb{R}_{+}^{d}}L(\\pmb{\\theta})+\\frac{\\lambda}{2}\\sum_{i=1}^{d}\\mathopen{}\\mathclose\\b{\\left(\\frac{\\theta_{i}^{2}}{\\eta_{i}}+\\eta_{i}\\aftergroup\\egroup\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "After solving the minimum over $\\eta$ explicitly, the second term becomes exactly $\\lambda\\|\\pmb\\theta\\|_{1}$ , and we recover the Lasso objective. This is a special case of the so-called \u201ceta-trick\u201d [1], which can be used to write many common sparsity-inducing penalties as the minimization of a quadratic functional of $\\pmb{\\theta}$ . ", "page_idx": 1}, {"type": "text", "text": "A variety of algorithms for learning Hadamard product parameterizations have recently been studied, including alternating minimization [13], bi-level optimization [25], and joint gradient descent on $(u,v)$ [34]. The connection to the $(\\theta,\\eta)$ optimization in (3) can also be leveraged to construct algorithms based on classical sparse recovery techniques. In particular, alternating minimization over $\\pmb{\\theta}$ and $\\eta$ in (3) yields the popular iteratively reweighted least-squares (IRLS) algorithm [11, 9]. Translating these updates to the equivalent updates on $(u,v)$ , we obtain an iterative least-squares algorithm for LDNNs, which alternately sets $\\pmb{v}^{(t+1)}=\\sqrt{|\\pmb{u}^{(t)}\\odot\\pmb{v}^{(t)}|}$ and performs a weighted least squares update on $\\textbf{\\em u}$ . This particular form of reparameterized IRLS was generalized in [28] to a larger family of iterative least-squares algorithms under the name of linear recursive feature machines (lin-RFM). ", "page_idx": 1}, {"type": "text", "text": "While several methods for learning Hadamard/LDNN parameterizations have been introduced in the literature, there remain many open questions about how they each perform and how they compare. Theoretical analyses of these algorithms typically assume fixed, possibly worst-case training data, and aim to characterize the properties of the fixed-points [28, 34] or give convergence guarantees to second-order stationary points [25]. However, these worst-case analyses do not readily yield guarantees on the estimation error, which is the principal metric of interest. Indeed, many works have shown that studying the average-case, or typical, behavior of non-convex optimization algorithms can allow for estimation guarantees that are more precise and reflective of practice [14, 18, 7]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we provide a precise analysis of a general family of iterative algorithms for learning LDNNs that take the form ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\pmb u}^{(t+1)}=\\arg\\operatorname*{min}_{{\\pmb u}\\in\\mathbb{R}^{d}}\\frac{1}{n}\\bigg\\|{\\pmb y}^{(t)}-\\frac{1}{\\sqrt{d}}{\\pmb X}^{(t)}({\\pmb u}\\odot{\\pmb v}^{(t)})\\bigg\\|_{2}^{2}+\\frac{\\lambda}{d}\\|{\\pmb u}\\|_{2}^{2}}}\\\\ {{\\pmb v}^{(t+1)}=\\psi({\\pmb u}^{(t+1)},{\\pmb v}^{(t)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "for some reweighting function $\\psi$ and batches of training data $(\\boldsymbol{X}^{(t)},\\boldsymbol{y}^{(t)})$ . As we show in Section 2, this formulation encompasses multiple existing algorithms, including reparameterized IRLS, lin-RFM, and alternating minimization over $\\textbf{\\em u}$ and $\\pmb{v}$ . We consider the common scenario where training is performed with batches of data and characterize the exact distribution of the parameters after each iteration in the high-dimensional limit $(n,d)\\to\\infty$ . This allows us to address questions such as ", "page_idx": 1}, {"type": "text", "text": "\u2022 How do different algorithm choices compare (in terms of convergence and signal recovery) in the high-dimensional regime? \u2022 How many iterations does it take common algorithms to find statistically favorable solutions? \u2022 What is the effect of model architecture in LDNNs? Does leveraging group structure provably improve sample complexity when the ground-truth signal is group-sparse? ", "page_idx": 2}, {"type": "text", "text": "Contributions: We define a general class of algorithms which learns LDNNs by alternately performing least-squares and reweighting steps in a sample-split/batched setting, and we show the following. ", "page_idx": 2}, {"type": "text", "text": "(1) Under mild assumptions on the target signal, initialization, and reweighting function, we provide an exact characterization of the distribution of the entries of the parameters at each iteration in the limit as $n,d$ approach infinity (Theorem 1). ", "page_idx": 2}, {"type": "text", "text": "(2) We show that this asymptotic result aligns well with numerical simulations and allows for accurate prediction of the test error at each iteration. This enables rigorous comparison between different algorithms and demonstrates that, with appropriate reweighting schemes, a statistically favorable solution can be obtained in only a handful of iterations. ", "page_idx": 2}, {"type": "text", "text": "(3) Lastly, we extend our asymptotic framework to a setting of structured sparsity, where $\\theta^{*}$ has groupsparse structure (Theorem 2). Our results show that using a grouped Hadamard parameterization (i.e., tying together groups of weights in the LDNN) effectively learns such signals, with performance scaling with the number of non-zero groups, rather than the total sparsity level. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "IRLS and the $\\eta$ -trick: The reformulation of non-smooth regularizers in terms of quadratic variational forms (the \u201c $\\eta$ -trick\u201d) has been studied in various early works in computer vision and robust statistics [12, 4]. Further analysis and examples of sparsity-promoting norms are provided in [20, 2], and [25] provides a characterization of when a regularizer admits a variational form of this type. The resultant optimization algorithm is iteratively-reweighted least-squares (IRLS), a popular technique for compressive sensing and sparse recovery [11, 9]. These works also consider IRLS algorithms corresponding to $\\ell_{p}$ -norm regularization for $0<p<1$ ; in this case, the minimization is no longer convex, but [11] shows that such methods can find sparse solutions with fast local convergence rate. The family of algorithms we consider includes a reparameterized version of each of these IRLS algorithms, but unlike these prior works, we consider a batched setting and the high-dimensional asymptotic regime. Moreover, our results apply to other algorithms which may not be easily expressed as resulting from the $\\eta$ -trick. ", "page_idx": 2}, {"type": "text", "text": "Hadamard parameterization and linear diagonal networks: The reparameterization of $\\pmb{\\theta}$ into the product of factors ${\\textbf{\\em u}}({\\boldsymbol{\\cdot}})~{\\boldsymbol{v}}$ has been considered in a variety of recent works. The authors of [31, 35] show that early-stopped joint gradient descent over the two factors can lead to optimal sample complexity for sparse linear regression. The equivalence of this parameterization to LDNNs has also led to a surge of interest in the implicit bias of gradient descent/flow on this parameterization, i.e., a characterization of which solution gradient descent will reach without explicit regularization (corresponding to $\\lambda=0$ ). These works typically consider gradient flow run until completion and characterize the solution as a minimizer of a certain sparsity-inducing functional that depends on the initialization [34, 10, 23]. ", "page_idx": 2}, {"type": "text", "text": "The connection between the LASSO (as well as some non-convex $\\ell_{q}$ penalties) and the Hadamard parameterization was studied in [13], where alternating minimization over the two factors is used instead of first-order methods. More recently, [25] extends these observations by making explicit the connection to the $\\eta$ -trick and showing that saddle points are strict (escapable). These insights lead to global convergence guarantees and a smooth bi-level optimization scheme [25, 26] for non-smooth structured optimization problems that was shown to perform competitively with state-of-the-art solvers. The non-convex landscape of such formulations is further explored in [15], where it is shown that for a large class of parameterizations (including grouped, deep, and fractional Hadamard products), the non-convex problem has no spurious local minima. Motivated by the type of feature learning observed in neural networks, the authors of [28] propose lin-RFM, which updates one of the parameters via weighted least-squares while iteratively updating the other parameter via a reweighting scheme based on the average gradient outer product of the learned function. The authors characterize properties of the fixed-points and show that, for certain reweighting schemes, lin-RFM is equivalent to a reparameterization of IRLS. The family of algorithms we consider is similar, consisting of a weighted least-squares step and a reweighting step; however, it is more general and doesn\u2019t require the reweighting function to have the particular form required by lin-RFM. Moreover, our asymptotic characterization of the iterates allows for a precise understanding of how the test error evolves. On the other hand, our analysis relies on batching/sample-splitting of training data while all of the above works reuse the entire batch of training data at each iteration. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We make particular note here of the few works which explicitly consider a \u201cgrouped\u201d Hadamard parameterization, which we consider in Section 4. This corresponds to a LDNN with groups of tied weights in the hidden layer. Early stopped gradient flow/descent for this type of architecture was shown in [17] to achieve sample-complexity scaling with the number of non-zero groups (rather than the overall sparsity). The non-convex landscape for this grouped architecture is studied in [36] and [15]. Our results complement these works by studying group-reweighted least-squares algorithms (rather than gradient methods) for learning functions of this form. ", "page_idx": 3}, {"type": "text", "text": "Precise characterization of higher-order non-convex optimization problems: On a technical level, our work provides a precise deterministic characterization of a family of higher-order optimization algorithms. In this sense, our results are of a similar flavor to [7], where Gaussian comparison inequalities are used to obtain a precise characterization of non-convex optimization problems. However, since the Hadamard parameterization is a re-parameterization of the actual estimator of interest $\\mathbf{\\nabla}(\\pmb{\\theta}:=\\pmb{u}\\odot\\pmb{v})$ ), the results of [7] are not directly applicable. While our results are asymptotic and do not provide finite-sample guarantees, we provide a distributional characterization of $\\pmb{v}$ after each reweighting step, which allows us to characterize the behavior of more complicated functions of the iterates. Precise characterizations of alternating minimization and lin-prox methods for rank-1 matrix sensing are studied in the works [6, 19]. While these works obtain non-asymptotic guarantees, the estimation model and resulting optimization objective are quite different, with each unknown parameter interacting with independent sensing vectors (rather than a single sensing vector interacting with the product of the two parameters). ", "page_idx": 3}, {"type": "text", "text": "2 Background and formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation: The ones vector of dimension $d$ is denoted as $\\mathbf{1}_{d}$ . We denote the element-wise multiplication (Hadamard product) of two vectors $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ as $\\mathbf{\\nabla}x\\odot\\mathbf{\\nabla}y$ . Element-wise division of two vectors is denoted as $\\frac{x}{y}$ . We say a function $f\\colon\\ensuremath{\\mathbb{R}}^{p}\\to\\ensuremath{\\mathbb{R}}$ is pseudo-Lipschitz of order 2 if, for all $\\mathbf{\\Delta}x,y\\in\\mathbb{R}^{p}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n|f(\\pmb{x})-f(\\pmb{y})|\\leq C(1+\\|\\pmb{x}\\|_{2}+\\|\\pmb{y}\\|_{2})\\|\\pmb{x}-\\pmb{y}\\|_{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some constant $C>0$ . The set of such functions is denoted by PL(2). ", "page_idx": 3}, {"type": "text", "text": "Convergence in probability of a sequence of random variables $X_{d}$ to a random variable $X$ is denoted by $X_{d}~{\\stackrel{P}{\\to}}~X$ . Convergence in Wasserstein-2 distance of a sequence of probability distributions $\\nu_{d}$ to a limiting distribution $\\nu$ is denoted as $\\nu_{d}\\stackrel{\\mathcal{W}_{2}}{\\rightarrow}\\nu$ , and this fact is equivalent to the statement $\\mathbb{E}_{X\\sim\\nu_{d}}\\,g(X)\\to\\mathbb{E}_{X\\sim\\nu}\\,g(X)$ for all $g\\in\\mathsf{P L}(2)$ [3]. If the $\\nu_{d}$ are random probability measures, we say that $\\nu_{d}\\stackrel{\\displaystyle\\nu_{2}}{\\rightarrow}\\nu$ if the same convergence holds in probability, i.e., $\\mathbb{E}_{X\\sim\\nu_{d}}\\,g(X)\\overset{P}{\\rightarrow}\\mathbb{E}_{X\\sim\\nu}\\,g(X)$ for all $g\\in\\mathsf{P L}(2)$ . The empirical distribution of a vector $z\\in\\mathbb{R}^{d}$ is defined as $\\textstyle{\\frac{1}{d}}\\sum_{i=1}^{d}\\delta(z_{i})$ , where $\\delta(z_{i})$ is the Dirac delta distribution centered at $z_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "Formulation: We consider a batched noisy linear model where, at each time $t=0,1,\\ldots,T$ , a user has access to an independent batch of data $(\\pmb{X}^{(t)},\\pmb{y}^{(t)})\\in\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n}$ satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{y}^{(t)}=\\frac{1}{\\sqrt{d}}\\pmb{X}^{(t)}\\pmb{\\theta}^{\\ast}+\\pmb{\\epsilon}^{(t)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Above, $\\pmb{\\theta}^{*}\\ \\in\\ \\mathbb{R}^{d}$ is an unknown signal, $\\pmb{X}^{(t)}$ has i.i.d. standard Gaussian entries, and $\\mathbf{\\mathcal{\\epsilon}}\\epsilon^{(t)}\\sim$ $\\mathcal{N}(\\mathbf{0},\\sigma^{2}I_{n})$ is i.i.d. noise in the measurements. Given an initial weight vector $\\pmb{v}^{(0)}\\in\\mathbb{R}^{d}$ , we are interested in the behavior of iterative algorithms of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\pmb u}^{(t+1)}=\\arg\\operatorname*{min}_{{\\pmb u}\\in\\mathbb{R}^{d}}\\frac{1}{n}\\bigg\\|{\\pmb y}^{(t)}-\\frac{1}{\\sqrt{d}}{\\pmb X}^{(t)}\\big({\\pmb u}\\odot{\\pmb v}^{(t)}\\big)\\bigg\\|_{2}^{2}+\\frac{\\lambda}{d}\\|{\\pmb u}\\|_{2}^{2}}}\\\\ {{\\pmb v}^{(t+1)}=\\psi({\\pmb u}^{(t+1)},{\\pmb v}^{(t)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "nv7ox1vd3q/tmp/dff59f62772e6932bf1c90342a7bb899d2c446d5d46688f447e95a39e2caaa70.jpg", "table_caption": ["Table 1: Some algorithms taking the form (4) "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\psi\\colon\\mathbb{R}\\times\\mathbb{R}\\rightarrow\\mathbb{R}$ is a \u201creweighting\u201d function that acts entry-wise on $(\\pmb{u}^{(t)},\\pmb{v}^{(t)})$ and $\\lambda>0$ is a hyperparameter governing the strength of the regularization. We we will study the behavior of the iterates $\\pmb{u}^{(t)},\\pmb{v}^{(t)}$ in the high-dimensional limit where $n$ and $d$ both approach infinity with fixed ratio $\\begin{array}{r}{\\frac{d}{n}=\\kappa}\\end{array}$ Siisn ac eh ioguhr- dpirimmenasriyo innatle sriegstn ails  twoi trhe lvoewal- tdhiem efenastiournea ll estarruncitnugr ec, awpae bwiliiltli etsy poifc aslulcy hf oaclguso roitnh tmhes $\\theta^{*}$   \nregime where $\\kappa>T$ , where $T$ is the number of total iterations. This ensures that the total number of observed samples $n T$ is smaller than the ambient dimension $d$ . ", "page_idx": 4}, {"type": "text", "text": "Before proceeding to our main results, we note that this formulation encompasses a wide variety of classical and modern algorithms (summarized in Table 1): ", "page_idx": 4}, {"type": "text", "text": "\u2022 Alternating minimization: One perspective on this algorithm is to consider it as a way to perform alternating minimization on the non-convex loss function ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\pmb{u},\\pmb{v})=\\frac{1}{n}\\bigg\\|\\pmb{y}-\\frac{1}{\\sqrt{d}}\\pmb{X}(\\pmb{u}\\odot\\pmb{v})\\bigg\\|_{2}^{2}+\\frac{\\lambda}{d}\\|\\pmb{u}\\|_{2}^{2}+\\frac{\\lambda}{d}\\|\\pmb{v}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using the fact that the loss function is symmetric in $\\textbf{\\em u}$ and $\\pmb{v}$ , choosing $\\psi(u,v)=u$ recovers the mini-batched alternating minimization algorithm for this loss. In other words, $\\psi$ simply switches the two parameters $\\textbf{\\em u}$ and $\\pmb{v}$ . ", "page_idx": 4}, {"type": "text", "text": "\u2022 IRLS algorithms for sparse recovery: As shown in [28], classical IRLS reweighting schemes used for sparse recovery and compressed sensing [11, 21] can be reparameterized in the form of (4) $\\psi(u,v)\\,=\\,(\\dot{u}^{2}v^{2}+\\epsilon)^{\\dot{\\alpha}}$ , where different choices of $\\alpha$ correspond to different $\\ell_{p}$ penalties. In particular, the choice $\\textstyle p\\,=\\,2\\,-\\,4\\alpha$ corresponds to the IRLS-p algorithm of [21]. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Lin-RFM [28]: Generalizing the reparameterized IRLS update, the authors of [28] propose the choice $\\psi(\\bar{u},v)=\\phi(u^{2}v^{\\mathcal{T}})$ for some continuous function $\\phi\\colon\\mathbb{R}\\to\\mathbb{R}^{+}$ . Here, the quantity $u^{2}v^{2}$ arises from the average outer product of the gradient of the learned regression function, which was shown empirically in [27] to correlate with the features learned in the weight matrices of various common neural network architectures. ", "page_idx": 4}, {"type": "text", "text": "Our goal is to understand statistical properties of the iterates for different choices of $\\psi$ , and in particular how the test error evolves from iteration to iteration. In the following section, we develop an asymptotic characterization of the iterates that can be used to gain insight into these questions for a large class of reweighting functions and problem settings. ", "page_idx": 4}, {"type": "text", "text": "3 A precise characterization of the iterates ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide a precise characterization of the iterates of the algorithm in Equation 4 with i.i.d. Gaussian covariates. First, we introduce and discuss the two main assumptions needed for our main result. The first assumption is concerned with the distribution of the initialization $\\pmb{v}_{0}$ and the target signal $\\theta^{*}$ : ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. The empirical distribution of the entries of $\\pmb{v}^{(0)}$ and $\\theta^{*}$ converges in $\\mathcal{W}_{2}$ distance to some joint distribution $\\Pi_{0}$ , i.e., $\\begin{array}{r}{\\frac{1}{d}\\sum_{i=1}^{d}\\delta\\big(v_{i}^{(0)},\\theta_{i}^{*}\\big)\\stackrel{\\mathcal{W}_{2}}{\\rightarrow}\\Pi_{0}}\\end{array}$ . Additionally, $v_{i}^{(0)}\\neq0$ for all $i$ and $\\theta^{*}$ has bounded entries almost surely. ", "page_idx": 4}, {"type": "text", "text": "Here, the requirement of empirical distribution convergence is easily satisfied by common choices of $\\pmb{v}^{(0)}$ , including the ones vector and i.i.d. Gaussian entries. For a typical sparse regression setup, we might, for example, consider the $\\Pi_{0}$ induced by choosing ${\\pmb v}^{(0)}=\\bar{\\bf1}_{d}$ and letting $\\theta^{*}$ have i.i.d. entries that equal 0 with certain probability. The requirement that $\\theta^{*}$ has bounded entries appears to be an artifact of the proof, and is used only in the proof of one technical lemma. In our simulations, we find that our asymptotic predictions often remain accurate when $\\pmb{\\theta}^{*}$ has entries from distributions which are not bounded almost surely (e.g., Gaussian entries). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Secondly, we define the set of reweighting functions $\\psi$ for which our result will apply. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. The reweighting function $\\psi\\colon\\mathbb{R}\\times\\mathbb{R}\\rightarrow\\mathbb{R}$ satisfies the following: ", "page_idx": 5}, {"type": "text", "text": "1. If $U,V$ are random variables such that $U,V\\neq0$ with probability $^{\\,l}$ , then $\\psi(U,V)\\neq0$ with probability $^{\\,l}$ . 2. $\\psi$ is continuous and bounded or $\\psi^{2}$ is pseudo-Lipschitz of order 2. ", "page_idx": 5}, {"type": "text", "text": "This family allows us to consider many of the choices of $\\psi$ discussed in the previous section, including $\\psi(u,v)=u$ (AM on linear diagonal networks), $\\psi(u,v)=\\sqrt{|u v|}$ (lin-RFM and IRLS), $\\psi(u,v)=\\phi(u^{2}v^{2})$ for bounded $\\phi$ (lin-RFM). We note that this does not include some choices of $\\psi$ which apply more \u201caggressive\u201d weighting, such as $\\psi(u,v)=|u v|$ . Nevertheless, we can apply our theoretical predictions for these choices of $\\psi$ after passing the weights through a bounded activation (such as a sigmoid). In Appendix D, we show that our predictions often still show excellent agreement with empirical simulation even when the boundedness assumption is violated. ", "page_idx": 5}, {"type": "text", "text": "Our results are stated in terms of the following iteration, for $t\\geq0$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{t+1},\\beta_{t+1}=\\arg\\underset{\\tau\\geq0}{\\operatorname*{max}}\\operatorname*{min}\\left\\{\\frac{\\tau\\sigma^{2}}{\\beta}+\\tau\\beta(1-\\kappa)-\\tau^{2}+\\tau\\lambda\\mathbb{E}_{(V,\\Theta)\\sim\\Pi_{t}}\\left[\\frac{\\Theta^{2}+\\beta^{2}\\kappa}{\\tau V^{2}+\\beta\\lambda}\\right]\\right\\}}\\\\ &{Q_{t+1}=\\frac{\\tau_{t+1}V(\\Theta+\\beta_{t+1}\\sqrt{\\kappa}G_{t})}{\\tau_{t+1}V^{2}+\\beta_{t+1}\\lambda},}\\\\ &{\\Pi_{t+1}=\\ L\\mathrm{aw}(\\psi(Q_{t+1},V),\\Theta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $G_{t}^{\\mathrm{~i.i.d.}}\\mathcal{N}(0,1)$ . In words, given a probability distribution $\\Pi_{t}$ over $\\mathbb{R}\\times\\mathbb{R}$ , $\\tau_{t+1}$ and $\\beta_{t+1}$ are scalars computed as the unique1solutions to a deterministic optimization problem (this can be solved easily by studying the optimality conditions, as shown in Appendix $\\mathrm{C}_{.}$ ). Then, $Q_{t+1}$ is defined as a random variable that is a function of $(V,\\Theta)\\sim\\Pi_{t}$ and $G_{t}\\sim\\bar{\\mathcal{N}}(0,1)$ . Lastly, $\\Pi_{t+1}$ is defined as the joint distribution of $\\psi(Q_{t+1},V)$ and $\\Theta$ . ", "page_idx": 5}, {"type": "text", "text": "Given this iteration, we obtain the following result, which is proved in Appendix A: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Suppose Assumptions $^{\\,l}$ and 2 are satisfied. Then, for any $t\\,\\geq\\,0$ and any function $g\\colon\\ensuremath{\\mathbb{R}}^{3}\\to\\ensuremath{\\mathbb{R}}$ such that $g\\in P L(2)$ or $g$ is bounded and continuous, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}g(u_{i}^{(t+1)},v_{i}^{(t)},\\theta_{i}^{*})\\xrightarrow{P}\\mathbb{E}[g(Q_{t+1},V,\\Theta)],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the expectation is over the independent random variables $(V,\\Theta)\\sim\\Pi_{t}$ and $G_{t}\\sim\\mathcal{N}(0,1)$ . ", "page_idx": 5}, {"type": "text", "text": "The limit in this theorem should be interpreted as being the limit as $n,d\\to\\infty$ with their ratio $\\textstyle\\kappa={\\frac{d}{n}}$ held as a constant. Applying the above theorem for each $t\\,\\geq\\,0$ , we can get precise asymptotic predictions for a wide variety of test functions of the iterates. One example of particular interest is the test error, which we measure as the normalized $\\ell_{1}$ distance between $\\pmb{u}^{(t+1)}\\odot\\pmb{v}^{(t)}$ and $\\theta^{*}$ , corresponding to $g(u,v,\\theta)\\,=\\,|u v\\,-\\,\\theta|$ (we provide a proof that this is PL(2) in Proposition 1 in Appendix B). We note that the limiting expectation can be computed via simple Monte Carlo simulation of a scalar random variable. ", "page_idx": 5}, {"type": "text", "text": "From a technical standpoint, our result is obtained by applying the Convex Gaussian Min-Max Theorem (CGMT) [30, 29] to the weighted least-squares objective function in (4). Previous works have obtained a similar distributional characterization for the solution to least-squares with anisotropic covariates (where the \u201cweights\u201d $\\pmb{v}$ are the square root of the eigenvalues of the data covariance) [8]. However, while [8] assume that the eigenvalues are uniformly bounded by constants, this is not a reasonable assumption in our setting, since many common choices of $\\psi$ are not bounded and hence $\\pmb{v}^{(t)}$ is not necessarily bounded uniformly for $t>1$ . A second key difference is that we need to obtain a distributional characterization which can be applied recursively for all $t\\geq0$ . The analysis in [8] assumes convergence of the initialization in $\\mathcal{W}_{4}$ distance and proves convergence of the estimator in $\\mathcal{W}_{3}$ distance. However, to apply the result recursively in our setting, if assume that the empirical distribution of $(\\pmb{v}^{(0)},\\pmb{\\theta}^{\\ast})$ converges in $\\mathcal{W}_{k}$ distance, then we need to show that after one iteration, the iterates also converge in $\\mathcal{W}_{k}$ distance (and not in any weaker sense). ", "page_idx": 5}, {"type": "image", "img_path": "nv7ox1vd3q/tmp/b5f21e7669e8e1945fb689e591b3196de83bc1410587ea0e748fb9e540c29e28.jpg", "img_caption": ["Figure 1: Theoretical predictions and simulations of the test error $\\begin{array}{r}{{\\frac{1}{d}}\\|\\boldsymbol{u}\\odot\\boldsymbol{v}-\\boldsymbol{\\theta}^{*}\\|_{1}}\\end{array}$ (log scale, pluses denote the median over 100 trials and the shaded region indicates the interquartile range) for two different noise levels, where $n=250$ , $d=2000$ , and $\\theta^{*}$ has Bernoulli(0.01) entries. Here, $\\psi=|u v|^{\\frac{1}{2}}$ corresponds to the classical IRLS weighting from [11], $\\psi=\\operatorname{tanh}\\vert u v\\vert$ is a version of lin-RFM, $\\psi=u$ corresponds to AM, and $\\psi=\\operatorname{tan\\bar{h}}u^{2}$ is a new reweighting scheme we introduce. We note that the $\\psi$ which depend only on $\\textbf{\\em u}$ can lead to oscillatory behavior in the test risk. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To overcome these differences, we use a different technique to show distributional convergence of the iterates. Similar to the approach in [5], we apply the CGMT to a perturbed optimization problem, which ultimately allows us to show convergence of test functions of the solutions to the unperturbed problem. While this approach necessitates the additional assumption that $\\theta^{*}$ has bounded entries and we obtain results for a slightly smaller family of test functions $g$ (note, for example, that the squared loss $g(u,v,\\theta)\\,=\\,(u v^{\\dagger}-{\\dot{\\theta}})^{2}$ is not PL(2)), we obtain a distributional convergence result that can be applied to a sequence of recursively defined least-squares problems which define the trajectory of an algorithm, rather than to a single optimization problem. Moreover, our simulations in Appendix D suggest that the predictions of Theorem 1 still often apply without these additional assumptions, including in the case of the squared loss, indicating that these additional assumptions could potentially be weakened with a more complicated analysis. ", "page_idx": 6}, {"type": "text", "text": "3.1 Application to sparse linear regression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we apply Theorem 1 to a sparse recovery setting and compare the asymptotic predictions to numerical simulations on high-dimensional Gaussian data. First, we consider a setting where $n\\,=\\,250,d\\,=\\,2000$ , and $\\pmb{\\theta}^{*}$ has Bernoulli(0.01) entries (so the expected sparsity level is $\\mathbb{E}[s]=20)$ ). We run Algorithm 4 with initialization ${\\pmb v}^{(0)}={\\bf1}_{d}$ for four different choices of reweighting function and display the test error at each iteration (median over 100 trials) in Figure 1. For each choice of reweighting function $\\psi$ , we choose the regularization parameter $\\lambda$ that minimizes the asymptotic test loss achieved within 8 iterations, and we plot the corresponding trajectory. As shown in the figure, the numerical simulations show excellent alignment with the asymptotic predictions even for this moderate choice of $n$ and $d$ . ", "page_idx": 6}, {"type": "text", "text": "The asymptotic predictions show that this family of algorithms can find solutions with low test error within only a few iterations. Our results also reveal fine-grained differences in the convergence behavior of the different algorithms. For instance, more aggressive weightings $\\psi\\,=\\,\\operatorname{tanh}\\left|u v\\right|$ and $\\psi=\\operatorname{tanh}u^{2}$ seem to find better solutions after several iterations. Interestingly, the weighting functions which depend only on $u$ (like alternating minimization) sometimes display a non-monotonic, oscillatory decay of the test loss, particularly in the low-noise regime. However, we do see a steady decrease in test error after every pair of iterations (e.g., in AM, after both parameters have been updated). Finally, we note that our framework allows for analysis of new algorithms for training LDNNs. In particular, to our knowledge, weighting functions of the form $\\bar{\\phi}(u^{2})$ have not been previously considered for this task, but our results indicate that this small modification to AM is competitive with many existing algorithms in this setting. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4 Grouped IRLS and the benefits of structured feature learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In many scenarios, the unknown signal $\\pmb{\\theta}^{*}$ is known to possess additional structure that can be leveraged during training. One commonly studied example of this is structured sparsity, or group sparsity, where $\\theta^{*}$ has many blocks which are zero. In this section, we generalize the results of Theorem 1 to the case where the reweighting function respects this additional structure in the signal, i.e., $\\psi$ acts on blocks of $\\pmb{v}$ , rather than on individual coordinates. ", "page_idx": 7}, {"type": "text", "text": "Concretely, we consider the following modification to our formulation. Let $b\\geq1$ be a constant and write $\\mathbb{R}^{d}$ as a product space over $\\begin{array}{r}{M=\\frac{d}{b}}\\end{array}$ factors: $\\mathbb{R}^{b}\\times\\cdot\\cdot\\times\\mathbb{R}^{b}$ . Then, $\\theta^{*},\\pmb{u}^{(t)},\\pmb{v}^{(t)}\\in\\mathbb{R}^{d}$ can all be represented as $M$ stacked blocks, each in $\\mathbb{R}^{b}$ . Under the same linear measurement model, we now let $\\bar{\\psi^{\\ast}}\\colon\\mathbb{R}^{b}\\times\\mathbb{R}^{b}\\rightarrow\\mathbb{R}^{b}$ act on each of the factors of $(\\boldsymbol{u}^{(t)},\\boldsymbol{\\theta}^{*})$ , and consider the same Algorithm 4. ", "page_idx": 7}, {"type": "text", "text": "Here, the case $b=1$ recovers the results of the previous section, but the case $b>1$ allows us to study the interplay between signal structure and reweighting scheme in a more fine-grained way. For example, suppose $\\theta^{*}$ is known to be group-sparse, meaning that many of the factors $\\{\\pmb{\\theta}_{i}^{*}\\}_{i=1}^{M}$ are zero. In this case, it might make sense for $\\psi$ to return a vector of the form ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\psi({\\pmb u}_{i}^{(t)},{\\pmb v}_{i}^{(t)})=\\alpha_{i}{\\bf1}_{b},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for some $\\alpha_{i}\\,\\in\\,\\mathbb{R}$ that is chosen as a function $\\pmb{u}_{i}^{(t)}$ and ${\\pmb v}_{i}^{(t)}$ . This corresponds to a reweighting scheme which acts on blocks, rather than individual entries. Another way to motivate this \u201cgrouped reweighting\u201d is to leverage the connection to the $\\eta$ -trick, as in (3). In particular, the group Lasso problem can be written in the following variational form [2]: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}\\operatorname*{min}_{\\eta\\in\\mathbb{R}_{+}^{M}}L(\\theta)+\\frac{\\lambda}{2}\\sum_{i=1}^{M}\\biggl(\\frac{\\|\\pmb{\\theta}_{i}\\|_{2}^{2}}{\\eta_{i}}+\\eta_{i}\\biggr),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the closed-form solution to the $\\eta$ minimization yields the classical group norm regularizer $\\sum_{i=1}^{M}\\lVert\\pmb{\\theta}_{i}\\rVert_{2}$ . Here, reparameterizing as $\\alpha_{i}\\rightarrow\\sqrt{\\eta_{i}}$ and $\\begin{array}{r}{u_{i}\\to\\frac{\\theta_{i}}{\\sqrt{\\eta_{i}}}}\\end{array}$ gives rise naturally to the grouped Hadamard parameterization, with ${\\pmb v}_{i}=\\alpha_{i}{\\bf1}_{b}$ . ", "page_idx": 7}, {"type": "text", "text": "From the perspective of linear diagonal networks, such an approach is equivalent to \u201ctying\u201d together the weights of the hidden layer that correspond to each block. Rather than studying gradient descent/flow for this parameterization (as in [16, 17]), we consider an optimization approach that relies on alternate updates of $\\pmb{u}^{(t)}$ and $\\pmb{v}^{(t)}$ . ", "page_idx": 7}, {"type": "text", "text": "We make the same technical assumptions as in Assumptions 1 and 2, with the natural modifications to accommodate $b\\geq1$ : ", "page_idx": 7}, {"type": "text", "text": "1. $\\Pi_{0}$ is the limit of the empirical distribution of factors of $(\\pmb{v}^{(0)},\\pmb{\\theta}^{\\ast})$ and hence is a distribution over $\\mathbb{R}^{b}\\times\\mathbb{R}^{b}$ .   \n2. Each factor $\\pmb{\\theta}_{i}^{*}\\in\\mathbb{R}^{b}$ for $i=1,\\cdot\\cdot\\cdot,M$ has bounded $\\ell_{2}$ -norm almost surely.   \n3. $\\psi$ is bounded and continuous or each of its coordinate projections satisfies $\\psi_{j}^{2}\\in\\operatorname{PL}(2)$ for $j=1,\\dots,b$ . ", "page_idx": 7}, {"type": "image", "img_path": "nv7ox1vd3q/tmp/6a9f63390172f02fe24a7f897854857d03700c06b46260f03dbe4653c03609ac.jpg", "img_caption": ["(a) Comparison of trajectory ", "(b) Effect of group size b "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Group-blind $(\\psi_{g b})$ vs. group-aware $(\\psi_{g a})$ reweighting when $\\pmb{\\theta}^{*}$ has group-sparse structure. We set $n\\,=\\,500,d\\,=\\,4000,\\sigma\\,=\\,0.1$ , and $\\theta_{i}^{*}\\,\\stackrel{\\mathrm{i.i.d.}}{\\sim}$ Bernoulli $(0.01)\\mathbf{1}_{b}$ . For each curve, $\\lambda$ is set to minimize the asymptotic test error achieved. Simulation results are the median/IQR over 100 trials. Left: Comparison of the test error trajectory (log scale) for a fixed block size $b=8$ . Right: $\\ell_{1}$ test error after $T=4$ iterations, for varying group sizes. ", "page_idx": 8}, {"type": "text", "text": "A straightforward extension of Theorem 1 yields the following generalization for the grouped algorithm, where $V,\\Theta,G_{t},Q_{t+1}\\in\\mathbb{R}^{b}$ are now vector-valued random variables: For $t\\geq0$ , let ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{t+1},\\beta_{t+1}=\\arg\\underset{\\tau\\geq0}{\\operatorname*{max}}\\operatorname*{min}\\left\\{\\frac{\\tau\\sigma^{2}}{\\beta}+\\tau\\beta(1-\\kappa)-\\tau^{2}+\\tau\\lambda\\mathbb{E}_{(V,\\Theta)\\sim\\Pi_{t}}\\left[\\cfrac{1}{b}\\,\\cfrac{b}{j=1}\\frac{\\Theta_{j}^{2}+\\beta^{2}\\kappa}{\\tau V_{j}^{2}+\\beta\\lambda}\\right]\\right\\}}\\\\ &{Q_{t+1}=\\frac{\\tau_{t+1}V\\odot(\\Theta+\\beta_{t+1}G_{t}\\sqrt{\\kappa})}{\\tau_{t+1}V^{\\odot2}+\\beta_{t+1}\\lambda\\mathbf{l}_{b}},\\;(\\mathrm{entry-wise~division})}\\\\ &{\\Pi_{t+1}=\\mathbf{Law}(\\psi(Q_{t+1},V),\\Theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here, $G_{t}^{\\mathrm{~i.i.d.}}\\mathcal{N}(\\mathbf{0},I_{b})$ . Then, we have the following result, which is proved in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "Theorem 2. [Generalization of Theorem 1 for $b\\geq1\\jmath$ Under the assumptions above, for any $t\\geq0$ and any function $g\\colon(\\mathbb{R}^{b})^{3}\\to{\\frac{\\bullet}{\\mathbb{R}}}$ such that $g\\in P L(2)$ or $g$ is bounded and continuous, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{i=1}^{M}g(\\pmb{u}_{i}^{(t+1)},\\pmb{v}_{i}^{(t)},\\pmb{\\theta}_{i}^{*})\\xrightarrow{P}\\mathbb{E}[g(\\pmb{Q}_{t+1},\\pmb{V},\\pmb{\\Theta})].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Given a reweighting function $\\psi$ , Theorem 2 characterizes the distribution of the factors (blocks) of the iterates. Hence, by choosing $g(\\pmb{u},\\pmb{v},\\pmb{\\theta})=|\\pmb{u}\\odot\\pmb{v}-\\pmb{\\theta}|$ , we can predict the exact limiting test error for this family of algorithms. ", "page_idx": 8}, {"type": "text", "text": "Computing these theoretical predictions reveals that choosing $\\psi$ in a group-aware way can lead to significant performance improvements compared to coordinate-wise reweighting. In Figure 2, we fix $\\sigma=0.1,n=500,d=4000$ , and set the overall expected sparsity level of $\\theta^{*}$ as in Figure 1. We compare the performance of Algorithm 4 for a \u201cgroup-blind\u201d $(\\psi_{g b})$ and \u201cgroup-aware\u201d $(\\psi_{g a})$ choice of reweighting function: ", "page_idx": 8}, {"type": "text", "text": "\u2022 $\\psi_{g b}(\\mathbf{u},\\mathbf{v})=\\operatorname{tanh}{|\\mathbf{u}\\odot\\mathbf{v}|}$ \u2014 note this is identical to one of the reweightings considered in Section 3.1.   \n\u2022 $\\begin{array}{r}{\\psi_{\\mathrm{ga}}(u,v)=\\left(\\frac{1}{b}\\sum_{j=1}^{b}\\operatorname{tanh}|u_{j}v_{j}|\\right)\\mathbf{1}_{b}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "The theoretical predictions align with simulations and show a notable improvement in performance when using the group-aware scheme with $b\\,>\\,1$ . Moreover, as the group size $b$ increases, the performance of $\\psi_{g b}$ remains approximately the same, indicating that it is not able to take adapt to the group-structure. By contrast, using $\\psi_{g a}$ leads to a consistent improvement in test error as $b$ gets larger. Hence, the test error when using the group-aware scheme scales with the size/number of groups, rather than the overall sparsity level. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we derived a precise asymptotic characterization of the iterates of a family of algorithms for learning high-dimensional linear models with linear diagonal networks. We used these predictions to obtain fine-grained predictions of the test error at each iteration for various existing algorithms for this task, and we showed that our framework can also be used as a test bed for new variations on these algorithms that take a similar form. Lastly, we demonstrated the advantage of embedding more structure into the model by tying together groups of weights when the ground-truth has structured sparsity. Several interesting open questions about these types of algorithms remain. While our simulations align very well with the predicted asymptotic trajectory, it would be interesting to obtain finite-sample guarantees that hold even for batch sizes that are much smaller than $d$ (as in the \u201cminibatch\u201d case studied by [19]). Moreover, seeing as our analysis depends crucially on the independence the covariates at every iteration, developing precise predictions of the trajectory in the non-batched setting remains an interesting direction for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their helpful feedback. This work was supported by an NSF Graduate Research Fellowship (DGE-2039655), the NSF AI Institute AI4OPT, NSF grants IIS-2212182, CCF-223915 and 2112533, and gifts from Amazon and Adobe. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Francis Bach. The \u201c $\\eta$ -trick\u201d or the effectiveness of reweighted least-squares. https://francisbach.com/the-%CE%B7-trick-or-the-effectiveness-ofreweighted-least-squares/, 2019. Accessed: April 2024.   \n[2] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Optimization with sparsity-inducing penalties. Foundations and Trends\u00ae in Machine Learning, 4(1):1\u2013106, 2012.   \n[3] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.   \n[4] Michael J Black and Anand Rangarajan. On the unification of line processes, outlier rejection, and robust statistics with applications in early vision. International Journal of Computer Vision, 19(1):57\u201391, 1996.   \n[5] David Bosch, Ashkan Panahi, Ayca Ozcelikkale, and Devdatt Dubhashi. Random features model with general convex regularization: A fine grained analysis with precise asymptotic learning curves. In International Conference on Artificial Intelligence and Statistics, pages 11371\u201311414. PMLR, 2023.   \n[6] Kabir Aladin Chandrasekher, Mengqi Lou, and Ashwin Pananjady. Alternating minimization for generalized rank one matrix sensing: Sharp predictions from a random initialization. arXiv preprint arXiv:2207.09660, 2022.   \n[7] Kabir Aladin Chandrasekher, Ashwin Pananjady, and Christos Thrampoulidis. Sharp global convergence guarantees for iterative nonconvex optimization with random data. The Annals of Statistics, 51(1):179\u2013210, 2023.   \n[8] Xiangyu Chang, Yingcong Li, Samet Oymak, and Christos Thrampoulidis. Provable beneftis of overparameterization in model compression: From double descent to pruning neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6974\u20136983, 2021.   \n[9] Rick Chartrand and Wotao Yin. Iteratively reweighted algorithms for compressive sensing. In 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 3869\u20133872. IEEE, 2008.   \n[10] Hung-Hsu Chou, Johannes Maly, and Holger Rauhut. More is less: inducing sparsity via overparameterization. Information and Inference: A Journal of the IMA, 12(3):1437\u20131460, 2023.   \n[11] Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan G\u00fcnt\u00fcrk. Iteratively reweighted least squares minimization for sparse recovery. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 63(1):1\u201338, 2010.   \n[12] Donald Geman and George Reynolds. Constrained restoration and the recovery of discontinuities. IEEE Transactions on Pattern Analysis & Machine Intelligence, 14(03):367\u2013383, 1992.   \n[13] Peter D Hoff. Lasso, fractional norm and structured sparse estimation using a Hadamard product parametrization. Computational Statistics & Data Analysis, 115:186\u2013198, 2017.   \n[14] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Proceedings of the 45th Annual ACM Symposium on Theory of Computing, pages 665\u2013674, 2013.   \n[15] Chris Kolb, Christian L M\u00fcller, Bernd Bischl, and David R\u00fcgamer. Smoothing the edges: A general framework for smooth optimization in sparse regularization using Hadamard overparametrization. arXiv preprint arXiv:2307.03571, 2023.   \n[16] Akshay Kumar, Akshay Malhotra, and Shahab Hamidi-Rad. Group sparsity via implicit regularization for MIMO channel estimation. In 2023 IEEE Wireless Communications and Networking Conference (WCNC), pages 1\u20136. IEEE, 2023.   \n[17] Jiangyuan Li, Thanh V Nguyen, Chinmay Hegde, and Raymond KW Wong. Implicit regularization for group sparsity. arXiv preprint arXiv:2301.12540, 2023.   \n[18] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Advances in Neural Information Processing Systems, 24, 2011.   \n[19] Mengqi Lou, Kabir Aladin Verchand, and Ashwin Pananjady. Hyperparameter tuning via trajectory predictions: Stochastic prox-linear methods in matrix sensing. arXiv preprint arXiv:2402.01599, 2024.   \n[20] Charles A Micchelli, Jean M Morales, and Massimiliano Pontil. Regularizers for structured sparsity. Advances in Computational Mathematics, 38:455\u2013489, 2013.   \n[21] Karthik Mohan and Maryam Fazel. Iterative reweighted algorithms for matrix rank minimization. The Journal of Machine Learning Research, 13(1):3441\u20133473, 2012.   \n[22] Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing. Handbook of Econometrics, 4:2111\u20132245, 1994.   \n[23] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefti of stochasticity. Advances in Neural Information Processing Systems, 34:29218\u201329230, 2021.   \n[24] David Pollard. Asymptotics for least absolute deviation regression estimators. Econometric Theory, 7(2):186\u2013199, 1991.   \n[25] Clarice Poon and Gabriel Peyr\u00e9. Smooth bilevel programming for sparse regularization. Advances in Neural Information Processing Systems, 34:1543\u20131555, 2021.   \n[26] Clarice Poon and Gabriel Peyr\u00e9. Smooth over-parameterized solvers for non-smooth structured optimization. Mathematical Programming, 201(1):897\u2013952, 2023.   \n[27] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Mechanism for feature learning in neural networks and backpropagation-free machine learning models. Science, 383(6690):1461\u20131467, Mar 2024.   \n[28] Adityanarayanan Radhakrishnan, Mikhail Belkin, and Dmitriy Drusvyatskiy. Linear recursive feature machines provably recover low-rank matrices. arXiv preprint arXiv:2401.04553, 2024.   \n[29] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized $m$ -estimators in high dimensions. IEEE Transactions on Information Theory, 64(8):5592\u20135628, 2018.   \n[30] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A precise analysis of the estimation error. In Conference on Learning Theory, pages 1683\u20131709. PMLR, 2015.   \n[31] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery. Advances in Neural Information Processing Systems, 32, 2019.   \n[32] Roman Vershynin. High-dimensional Probability: An Introduction with Applications in Data Science, volume 47. Cambridge university press, 2018.   \n[33] C\u00e9dric Villani et al. Optimal Transport: Old and New, volume 338. Springer, 2009.   \n[34] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory, pages 3635\u20133673. PMLR, 2020.   \n[35] Peng Zhao, Yun Yang, and Qiao-Chu He. High-dimensional linear regression via implicit regularization. Biometrika, 109(4):1033\u20131046, 2022.   \n[36] Liu Ziyin and Zihao Wang. spred: Solving $L_{1}$ penalty with SGD. In International Conference on Machine Learning, pages 43407\u201343422. PMLR, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of main results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide the proofs of our main results. ", "page_idx": 12}, {"type": "text", "text": "A.1 Notation and background ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For convenience, we first restate the main notation that is used in our proofs. ", "page_idx": 12}, {"type": "text", "text": "Notation The ones vector of dimension $d$ is denoted $\\mathbf{1}_{d}$ . We write $a\\ \\lesssim\\ b$ when $a\\,\\leq\\,C b$ for some sufficiently large constant $C>0$ which does not depend on $d$ . We denote the element-wise multiplication (Hadamard product) of two vectors $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ as $\\pmb{x}\\odot\\pmb{y}$ . Element-wise division of two vectors is denoted as $\\frac{x}{y}$ . We use the shorthand $(\\cdot)_{+}=\\operatorname*{max}(\\cdot,0)$ . A function $f\\colon\\ensuremath{\\mathbb{R}}^{p}\\to\\ensuremath{\\mathbb{R}}$ is called pseudo-Lipschitz of order 2 if, for all $\\mathbf{\\Delta}x,y\\in\\mathbb{R}^{p}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n|f(\\pmb{x})-f(\\pmb{y})|\\leq C(1+\\|\\pmb{x}\\|_{2}+\\|\\pmb{y}\\|_{2})\\|\\pmb{x}-\\pmb{y}\\|_{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for some constant $C>0$ . The set of such functions is denoted PL(2). ", "page_idx": 12}, {"type": "text", "text": "Convergence in probability of a sequence of random variables $X_{d}$ to a random variable $X$ is denoted $X_{d}~{\\stackrel{P}{\\to}}~X$ . Convergence in Wasserstein-2 distance of a sequence of probability distributions $\\nu_{d}$ to a limiting distribution $\\nu$ is denoted as $\\nu_{d}~\\stackrel{\\triangledown}{\\vec{\\rightarrow}}~\\nu$ , and this fact is equivalent to the statement $\\mathbb{E}_{X\\sim\\nu_{d}}\\,g(X)\\to\\mathbb{E}_{X\\sim\\nu}\\,g(X)$ for all $g\\in\\mathsf{P L}(2)$ [3]. If the $\\nu_{d}$ are random probability measures, we say that $\\nu_{d}\\stackrel{\\displaystyle\\nu_{2}}{\\rightarrow}\\nu$ if the same convergence holds in probability, i.e., $\\mathbb{E}_{X\\sim\\nu_{d}}\\,g(X)\\overset{P}{\\rightarrow}\\mathbb{E}_{X\\sim\\nu}\\,g(X)$ for all $g\\in\\mathsf{P L}(2)$ . The empirical distribution of a vector $z\\in\\mathbb{R}^{d}$ is defined as $\\textstyle{\\frac{1}{d}}\\sum_{i=1}^{d}\\delta(z_{i})$ , where $\\delta(z_{i})$ is the Dirac delta distribution centered at $z_{i}$ . For any random variable (or group of random variables) $X$ , we use the notation $\\operatorname{Law}(X)$ to denote the probability distribution of $X$ . ", "page_idx": 12}, {"type": "text", "text": "We also define two key quantities which appear in the analysis. ", "page_idx": 12}, {"type": "text", "text": "Definition 1. The Moreau envelope function of a lower semi-continuous, proper convex function $\\ell\\colon\\ensuremath{\\mathbb{R}}^{p}\\to\\ensuremath{\\mathbb{R}}$ with step size $\\tau$ is defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\ell}(\\pmb{x};\\tau)=\\operatorname*{min}_{\\pmb{y}\\in\\mathbb{R}^{p}}\\ell(\\pmb{y})+\\frac{1}{2\\tau}\\|\\pmb{y}-\\pmb{x}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The proximal (prox) operator of $\\ell$ with step size $\\tau$ , denoted $p r o x_{\\ell}(\\mathbf{\\boldsymbol{x}},\\tau)$ is defined as the arg min of the above optimization problem. ", "page_idx": 12}, {"type": "text", "text": "Lastly, we restate the version of the Convex Gaussian Min-Max Theorem (CGMT) that we will use in our proofs. ", "page_idx": 12}, {"type": "text", "text": "Theorem 3 (Convex Gaussian Min-Max Theorem [29]). Let $\\pmb{G}\\in\\mathbb{R}^{m\\times n},\\pmb{g}\\in\\mathbb{R}^{m},\\pmb{h}\\in\\mathbb{R}^{n}$ have i.i.d. ${\\mathcal{N}}(0,1)$ entries. Let $\\mathcal{S}_{w}\\subset\\mathbb{R}^{n}$ and $S_{u}\\subset\\mathbb{R}^{m}$ be compact, convex sets, and $f\\colon\\mathbb{R}^{n}\\times\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ be convex-concave on $S_{w}\\times S_{u}$ . Define the following two min-max problems: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Phi(G):=\\displaystyle\\operatorname*{min}_{w\\in S_{w}}\\operatorname*{max}_{u\\in S_{u}}u^{\\top}G w+f(w,u)}\\\\ &{\\phi(\\pmb{g},\\pmb{h}):=\\displaystyle\\operatorname*{min}_{w\\in S_{w}}\\operatorname*{max}_{u\\in S_{u}}\\|\\pmb{w}\\|_{2}\\pmb{u}^{\\top}\\pmb{g}+\\|\\pmb{u}\\|_{2}\\pmb{w}^{\\top}\\pmb{h}+f(\\pmb{w},\\pmb{u})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, for all $c\\in\\mathbb{R}$ and $t>0$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\{|\\Phi(G)-c|>t\\}\\le2\\,\\mathbb{P}\\{|\\phi(\\pmb{g},\\pmb{h})-c|>t\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Theorems 1 and 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . Assume that $\\begin{array}{r}{\\frac{1}{d}\\sum_{i=1}^{d}\\delta(v_{i}^{(t)},\\theta_{i}^{*})\\overset{\\mathcal{W}_{2}}{\\rightarrow}\\Pi_{t}}\\end{array}$ (note that this holds by assumption at $t=0$ ; we will show later that it holds at time $t+1$ ). ", "page_idx": 12}, {"type": "text", "text": "First observe that convergence of the joint empirical distribution of $(\\boldsymbol{\\mathbf{\\mathit{u}}}^{(t+1)},\\boldsymbol{\\mathbf{\\mathit{v}}}^{(t)},\\boldsymbol{\\theta}^{*})$ to the joint distribution of $(Q_{t+1},V,\\Bar{\\Theta})$ in Wasserstein-2 distance implies that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}g(u_{i}^{(t+1)},v_{i}^{(t)},\\theta_{i}^{*})\\xrightarrow{P}\\mathbb{E}[g(Q_{t+1},V,\\Theta)],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for any $g\\in\\mathsf{P L}(2)$ or which is bounded and continuous. This is because $\\mathcal{W}_{2}$ convergence implies convergence in expectation of any pseudo-Lipschitz function of order 2 [3, Lemma 5] and of any bounded continuous function (since $\\mathcal{W}_{2}$ convergence is stronger than weak convergence [33, Theorem 6.9]). Hence, it suffices to show that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}\\delta(u_{i}^{(t+1)},v_{i}^{(t)},\\theta_{i}^{*})\\overset{\\mathcal{W}_{2}}{\\rightarrow}\\mathrm{Law}(Q_{t+1},V,\\Theta),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(V,\\Theta)\\sim\\Pi_{t}$ and $Q_{t+1}$ is defined as in Eq. 5. ", "page_idx": 13}, {"type": "text", "text": "Recall that the objective function for the update on $\\textbf{\\em u}$ is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{u}}^{(t+1)}=\\arg\\operatorname*{min}_{\\mathbf{\\boldsymbol{u}}\\in\\mathbb{R}^{d}}\\frac{1}{n}\\left\\|\\mathbf{\\boldsymbol{y}}^{(t)}-\\frac{1}{\\sqrt{d}}\\mathbf{\\boldsymbol{X}}^{(t)}(\\mathbf{\\boldsymbol{u}}\\odot\\mathbf{\\boldsymbol{v}}^{(t)})\\right\\|_{2}^{2}+\\frac{\\lambda}{d}\\|\\mathbf{\\boldsymbol{u}}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Rather than study this update directly, we first analyze a slightly more general problem (following the approach in [5]). Let $h^{\\frac{2}{\\cdot}}\\mathbb{R}^{3}\\rightarrow\\mathbb{R}$ be a continuous test function with $\\|\\nabla^{2}h\\|_{2}^{\\;^{\\star}}\\leq C$ and that satisfies one of the following: ", "page_idx": 13}, {"type": "text", "text": "1. $h$ is uniformly bounded. ", "page_idx": 13}, {"type": "text", "text": "Then we consider the following problem (the dependence of $\\scriptstyle{X,y,\\epsilon}$ , and $\\pmb{v}$ on $t$ is dropped to simplify the notation): ", "page_idx": 13}, {"type": "equation", "text": "$$\nP_{1}(\\mu)=\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}\\frac{1}{n}\\bigg\\|y-\\frac{1}{\\sqrt{d}}X(u\\odot v)\\bigg\\|_{2}^{2}+\\frac{\\lambda}{d}\\|u\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h(u_{i},v_{i},\\theta_{i}^{*}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mu\\in[-\\mu^{*},\\mu^{*}]$ and $\\begin{array}{r}{\\mu^{*}=\\frac{\\lambda}{C}}\\end{array}$ is chosen sufficiently small so that the objective function (scaled by $d$ ) is $\\lambda$ -strongly convex for all $\\mu$ in this range. The case $\\mu=0$ recovers the original problem of interest. ", "page_idx": 13}, {"type": "text", "text": "Step 1: Convergence of the loss Rewriting this in terms of the error vector $\\begin{array}{r}{\\Delta:=\\frac{1}{\\sqrt{d}}\\big(u\\odot v-\\theta^{*}\\big)}\\end{array}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nP_{1}(\\mu)=\\operatorname*{min}_{\\Delta\\in\\mathbb{R}^{d}}\\frac{1}{n}\\|\\epsilon-X\\Delta\\|_{2}^{2}+\\frac{\\lambda}{d}\\bigg\\|\\frac{\\sqrt{d}\\Delta+\\theta^{*}}{v}\\bigg\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h\\Bigg(\\frac{\\sqrt{d}\\Delta_{i}+\\theta_{i}^{*}}{v_{i}},v_{i},\\theta_{i}^{*}\\Bigg).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In writing this, we use the fact that $v_{i}\\neq0$ for all $i$ with probability 1 (and the notation in the secondto-last term indicates entry-wise division). Now, using the identity $\\|\\cdot\\|_{2}^{2}=\\operatorname*{max}_{q}2\\pmb{q}^{\\top}(\\cdot)-\\|\\pmb{q}\\|_{2}^{2}$ , we can write this as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{\\gamma}_{1}(\\mu)=\\operatorname*{min}_{\\Delta\\in\\mathbb{R}^{d}}\\operatorname*{max}_{q\\in\\mathbb{R}^{n}}\\frac{2}{\\sqrt{n}}q^{\\top}\\epsilon-\\frac{2}{\\sqrt{n}}q^{\\top}X\\Delta-\\|q\\|_{2}^{2}+\\frac{\\lambda}{d}\\left\\|\\frac{\\sqrt{d}\\Delta+\\theta^{*}}{v}\\right\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h\\left(\\frac{\\sqrt{d}\\Delta_{i}+\\theta_{i}^{*}}{v_{i}},v_{i}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, in Lemma 1, we show that there exist Euclidean balls $B_{\\Delta}$ and $B_{q}$ , each of radius $C_{1}\\|\\pmb{v}\\|_{\\infty}$ such that, with probability approaching 1, we can constrain the feasible set to lie with these balls without changing the value of $P_{1}(\\mu)$ , so we can study ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{\\mathrm{\\boldsymbol{s}}}_{1}(\\mu)=\\operatorname*{min}_{\\Delta\\in B_{\\Delta}}\\operatorname*{max}_{q\\in B_{q}}\\frac{2}{\\sqrt{n}}q^{\\top}\\epsilon-\\frac{2}{\\sqrt{n}}q^{\\top}X\\Delta-\\|q\\|_{2}^{2}+\\frac{\\lambda}{d}\\left\\|\\frac{\\sqrt{d}\\Delta+\\theta^{*}}{v}\\right\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h\\Bigg(\\frac{\\sqrt{d}\\Delta_{i}+\\theta_{i}^{*}}{v_{i}},\\epsilon\\Bigg),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $P_{1}(\\mu)=\\tilde{P}_{1}(\\mu)$ with probability tending to 1. We can therefore condition on this event for the remainder of the analysis without changing our asymptotic conclusions. ", "page_idx": 13}, {"type": "text", "text": "Now, noting that this is in the correct form to apply Theorem 3, we define the auxiliary optimization problem ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{\\bar{\\Psi}}_{2}(\\mu)=\\operatorname*{min}_{\\Delta\\in B_{\\Delta}}\\operatorname*{max}_{q\\in B_{q}}\\frac{2}{\\sqrt{n}}q^{\\top}\\epsilon-\\frac{2}{\\sqrt{n}}\\|{q}\\|_{2}g^{\\top}\\Delta-\\frac{2}{\\sqrt{n}}\\|\\Delta\\|_{2}h^{\\top}q-\\|{q}\\|_{2}^{2}+\\frac{\\lambda}{d}\\bigg\\|\\frac{\\sqrt{d}\\Delta+\\theta^{*}}{v}\\bigg\\|_{2}^{2}}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(12\\sqrt{1-\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n+\\;\\frac{\\mu}{d}\\sum_{i=1}^{d}h\\!\\left(\\frac{\\sqrt{d}\\Delta_{i}+\\theta_{i}^{*}}{v_{i}},v_{i},\\theta_{i}^{*}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\pmb{g}\\in\\mathbb{R}^{d}$ and $\\pmb{h}\\in\\mathbb{R}^{n}$ have i.i.d. standard normal entries. By Theorem 3, for all $\\delta>0$ and fixed $\\bar{P}(\\mu)\\in\\mathbb{R}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathbb{P}}\\{|P_{1}(\\mu)-{\\bar{P}}(\\mu)|>\\delta\\}\\leq2\\,{\\mathbb{P}}\\{|P_{2}(\\mu)-{\\bar{P}}(\\mu)|>\\delta\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, if we can find some $\\bar{P}(\\mu)$ such that $P_{2}(\\mu)\\stackrel{P}{\\to}\\bar{P}(\\mu)$ , then we can conclude also that $P_{1}(\\mu)\\stackrel{P}{\\to}\\bar{P}(\\mu)$ . ", "page_idx": 14}, {"type": "text", "text": "To accomplish this, we next perform a series of simplifications to $P_{2}(\\mu)$ which will later help us characterize its asymptotic behavior. First, we can decouple the optimization over $\\pmb q$ into its norm and direction, and the latter can be solved explicitly. Letting $\\tau=\\lVert\\pmb q\\rVert_{2}$ , this yields ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{2}(\\mu)=\\operatorname*{min}_{\\Delta\\in B_{\\Delta}}\\operatorname*{max}_{0\\leq\\tau\\leq R}\\frac{2\\tau}{\\sqrt{n}}\\|\\epsilon-\\|\\Delta\\|_{2}h\\|_{2}-\\frac{2\\tau}{\\sqrt{n}}g^{\\top}\\Delta-\\tau^{2}+\\frac{\\lambda}{d}\\bigg\\|\\frac{\\sqrt{d}\\Delta+\\theta^{*}}{v}\\bigg\\|_{2}^{2}}\\\\ {+\\,\\frac{\\mu}{d}\\sum_{i=1}^{d}h\\bigg(\\frac{\\sqrt{d}\\Delta_{i}+\\theta_{i}^{*}}{v_{i}},v_{i},\\theta_{i}^{*}\\bigg),}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $R:=C_{1}\\|\\pmb{v}\\|_{\\infty}$ . Next, note that $\\epsilon$ and $^h$ are independent Gaussian vectors and hence $\\epsilon-$ $\\|\\pmb{\\Delta}\\|_{2}\\pmb{h}\\stackrel{d}{=}\\sqrt{\\sigma^{2}+\\|\\pmb{\\Delta}\\|_{2}^{2}}\\pmb{h}$ . So, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{2}(\\mu)\\stackrel{d}{=}\\operatorname*{min}_{\\Delta\\in B_{\\Delta}}\\operatorname*{max}_{0\\leq\\tau\\leq R}\\frac{2\\tau\\|h\\|_{2}}{\\sqrt{n}}\\sqrt{\\sigma^{2}+\\|\\Delta\\|_{2}^{2}}-\\frac{2\\tau}{\\sqrt{n}}g^{\\top}\\Delta-\\tau^{2}+\\frac{\\lambda}{d}{\\left\\|\\frac{\\sqrt{d}\\Delta+\\theta^{*}}{v}\\right\\|_{2}^{2}}}\\\\ {+\\,\\frac{\\mu}{d}\\sum_{i=1}^{d}h\\Bigg(\\frac{\\sqrt{d}\\Delta_{i}+\\theta_{i}^{*}}{v_{i}},v_{i},\\theta_{i}^{*}\\Bigg)_{\\mathscr{N}_{i}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Before proceeding further, we rewrite this in terms of a minimization over the variable u = d\u2206v+\u03b8\u2217. ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{2}(\\mu)\\stackrel{d}{=}\\operatorname*{min}_{u\\in B_{u}}\\operatorname*{max}_{0\\le\\tau\\le R}\\frac{2\\tau\\|h\\|_{2}}{\\sqrt{n}}\\sqrt{\\sigma^{2}+\\frac{1}{d}\\|u\\odot v-\\theta^{*}\\|_{2}^{2}}-\\frac{2\\tau}{\\sqrt{n d}}g^{\\top}(u\\odot v-\\theta^{*})}\\\\ {-\\,\\tau^{2}+\\frac{\\lambda}{d}\\|u\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h(u_{i},v_{i},\\theta_{i}^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $\\begin{array}{r}{B_{u}\\;:=\\;\\Big\\{u\\in\\mathbb{R}^{d}\\colon\\left\\|\\frac{1}{\\sqrt{d}}({\\pmb u}\\odot{\\pmb v}-{\\pmb\\theta}^{*})\\right\\|_{2}\\le R\\Big\\}}\\end{array}$ . After this step, observe that the objective function is strongly concave in $\\tau$ and strongly convex in $\\textbf{\\em u}$ (the sum of the last two terms is strongly convex in $\\textbf{\\em u}$ based on the assumption that $h$ has bounded Hessian and $\\mu$ is sufficiently small). Since the objective function is convex-concave over convex and compact sets, we can invoke Sion\u2019s minimax theorem to switch the min and max. Furthermore, we use the fact that $\\begin{array}{r}{\\sqrt{x}=\\operatorname*{min}_{\\beta>0}\\frac{x}{2\\beta}+\\frac{\\beta}{2}}\\end{array}$ to write this as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Lambda}_{2}(\\mu)\\overset{d}{=}\\underset{0\\leq\\tau\\leq R}{\\operatorname*{max}}\\underset{\\sigma\\leq\\beta\\leq\\sigma+R}{\\operatorname*{min}}\\frac{\\tau\\|h\\|_{2}}{\\sqrt{n}}\\Bigg(\\frac{\\sigma^{2}}{\\beta}+\\frac{\\|u\\odot v-\\theta^{*}\\|_{2}^{2}}{\\beta d}+\\beta\\Bigg)-\\frac{2\\tau}{\\sqrt{n d}}g^{\\top}(u\\odot v-\\theta^{*})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,-\\tau^{2}+\\displaystyle\\frac{\\lambda}{d}\\|u\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h(u_{i},v_{i},\\theta_{i}^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, note that we can add the constraint on $\\beta$ without changing the solution since the optimal value of $\\beta$ will be obtained at $\\begin{array}{r}{\\sqrt{\\sigma^{2}+\\frac{1}{d}\\|\\boldsymbol{u}\\odot\\boldsymbol{v}-\\boldsymbol{\\theta}^{*}\\|_{2}^{2}}\\in[\\sigma,\\sigma+R]}\\end{array}$ for all feasible $\\textbf{\\em u}$ . ", "page_idx": 15}, {"type": "text", "text": "Next, we can explicitly solve the inner minimization over $\\textbf{\\em u}$ . To do this, we first show in Lemma 2 that the optimal solution to the unconstrained minimization is strictly feasible for large enough $C_{1}$ , and hence, the unconstrained and constrained minimizations over $\\textbf{\\em u}$ are equivalent. Next, observe that the unconstrained problem is separable over the indices, so we need only to solve the scalar problem ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{u_{i}\\in\\mathbb{R}}\\frac{\\tau\\|h\\|_{2}(u_{i}v_{i}-\\theta_{i}^{*})^{2}}{\\beta d\\sqrt{n}}-\\frac{2\\tau}{\\sqrt{n d}}g_{i}(u_{i}v_{i}-\\theta_{i}^{*})+\\frac{\\lambda}{d}u_{i}^{2}+\\frac{\\mu}{d}h(u_{i},v_{i},\\theta_{i}^{*})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Completing the squares, we obtain that the above problem can be written in terms of the Moreau envelope (Definition 1) of the function $\\ell(u)=\\lambda u^{2}\\,\\dot{+}\\,\\mu h$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\Bigg[\\frac{\\tau\\xi\\theta_{i}^{*2}}{\\beta}-\\frac{\\tau}{\\beta\\xi}\\big(\\beta g_{i}\\sqrt{\\kappa}-\\xi\\theta_{i}^{*}\\big)^{2}+\\mathcal{M}_{\\lambda(\\cdot)^{2}+\\mu h(\\cdot,v_{i},\\theta_{i}^{*})}\\left(\\frac{\\xi\\theta_{i}^{*}-\\beta g_{i}\\sqrt{\\kappa}}{\\xi v_{i}};\\frac{\\beta}{2\\xi\\tau v_{i}^{(t)2}}\\right)\\Bigg],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we have introduced the shorthand notation $\\begin{array}{r}{\\xi=\\frac{\\|h\\|_{2}}{\\sqrt{n}}}\\end{array}$ . Substituting this into the expression for $P_{2}$ above, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{2}(\\mu)\\stackrel{d}{=}\\underset{0\\leq\\tau\\leq R}{\\operatorname*{max}}\\underset{\\sigma\\leq\\beta\\leq\\sigma+R}{\\operatorname*{min}}\\frac{\\tau\\sigma^{2}\\xi}{\\beta}\\quad+\\tau\\beta\\xi-\\tau^{2}+\\displaystyle\\frac{1}{d}\\sum_{i=1}^{d}\\Bigg[\\frac{\\tau\\xi\\theta_{i}^{*2}}{\\beta}-\\frac{\\tau}{\\beta\\xi}\\big(\\beta g_{i}\\sqrt{\\kappa}-\\xi\\theta_{i}^{*}\\big)^{2}\\Bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\cfrac{1}{d}\\sum_{i=1}^{d}\\Bigg[\\mathcal{M}_{\\lambda(\\cdot)^{2}+\\mu h(\\cdot,v_{i},\\theta_{i}^{*})}\\left(\\frac{\\xi\\theta_{i}^{*}-\\beta g_{i}\\sqrt{\\kappa}}{\\xi v_{i}};\\frac{\\beta}{2\\xi\\tau v_{i}^{(t)2}}\\right)\\Bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now that the optimization has been fully \u201cscalarized\u201d, we proceed by considering its asymptotic behavior. First, note that the partial minimization over $\\textbf{\\em u}$ preserves the concavity/convexity in $(\\tau,\\beta)$ . In Lemma 3, we prove that for any fixed $\\tau$ and $\\beta$ , the objective function $f_{d}(\\tau,\\beta)$ converges in probability to ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\tau,\\beta)=\\frac{\\tau\\sigma^{2}}{\\beta}+\\tau\\beta(1-\\kappa)-\\tau^{2}+\\mathbb{E}\\biggl[\\mathcal{M}_{\\lambda(\\cdot)^{2}+\\mu h(\\cdot,V,\\Theta)}\\biggl(\\frac{\\Theta-\\beta G\\sqrt{\\kappa}}{V};\\frac{\\beta}{2\\tau V^{2}}\\biggr)\\biggr],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the expectation is over $(V,\\Theta)\\sim\\Pi_{t}$ and an independent $G\\sim\\mathcal{N}(0,1)$ . We note here that Lemma 3 is the only place in our proof which requires the boundedness of the entries of $\\pmb{\\theta}^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "Since $f_{d}(\\tau,\\beta)$ is strongly concave in $\\tau$ with parameter 1 for all feasible $\\beta$ , we can conclude that $f(\\tau,\\beta)$ is also strongly concave in $\\tau$ with parameter 1. Directly taking a derivative with respect to $\\beta$ , we also find that $f$ has a single non-negative critical point, at the point $\\hat{\\beta}=\\sqrt{\\sigma^{2}+\\mathbb{E}[(\\hat{u}V-\\Theta)^{2}]}$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{u}=\\hat{u}(V,\\Theta)=\\mathrm{prox}_{\\lambda(\\cdot)^{2}+\\mu h(\\cdot,V,\\Theta)}\\bigg(\\frac{\\Theta-\\beta G\\sqrt{\\kappa}}{V};\\frac{\\beta}{2\\tau V^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So, we can conclude that $f$ has unique saddle point $({\\hat{\\tau}},{\\hat{\\beta}})$ . Note $f$ is a deterministic function that does not depend on $d$ and hence $({\\hat{\\tau}},{\\hat{\\beta}})$ are also deterministic and independent of $d$ . ", "page_idx": 15}, {"type": "text", "text": "Now let $C_{2}:=\\operatorname*{max}\\{\\hat{\\tau},\\hat{\\beta}\\}+1$ . By the \u201cconvexity lemma\u201d (as stated in [24]), pointwise convergence (in probability) of a convex function is uniform over compact sets. So, this result implies that the convergence is uniform over $(\\tau,\\beta)\\in[0,C_{2}]\\times[0,C_{2}].$ ,2so ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{0\\leq\\tau\\leq C_{2}}\\operatorname*{min}_{0\\leq\\beta\\leq C_{2}}f_{d}(\\tau,\\beta)\\xrightarrow{P}\\operatorname*{max}_{0\\leq\\tau\\leq C_{2}}\\operatorname*{min}_{0\\leq\\beta\\leq C_{2}}f(\\tau,\\beta)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $(\\hat{\\tau}_{d},\\hat{\\beta}_{d})$ denote the optimnal solution for the problem on the left. We can also conclude that $(\\hat{\\tau}_{d},\\hat{\\beta}_{d})\\overset{P}{\\rightarrow}(\\hat{\\tau},\\hat{\\beta})$ by [22, Theorem 2.1], which states that uniform convergence in probability of ", "page_idx": 15}, {"type": "text", "text": "a convex function over a compact set implies convergence of the optimal minimizer. So, with probability approaching 1, $(\\hat{\\tau}_{d},\\hat{\\beta}_{d})$ are strictly smaller than $C_{2}$ , and the same solution is also optimal for $P_{2}(\\mu)$ . We can therefore conclude ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{2}(\\mu)\\stackrel{P}{\\rightarrow}\\operatorname*{max}_{0\\leq\\tau\\leq C_{2}}\\operatorname*{min}_{0\\leq\\beta\\leq C_{2}}f(\\tau,\\beta)=\\operatorname*{max}_{\\tau\\geq0}\\operatorname*{min}_{\\beta\\geq0}f(\\tau,\\beta)=:\\bar{P}(\\mu)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, by Theorem 3, for any fixed $\\mu\\in[-\\mu^{*},\\mu^{*}]$ , we have the convergence ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{1}(\\mu)\\stackrel{P}{\\to}\\bar{P}(\\mu).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the special case $\\mu=0$ , we can further simplify the Moreau envelope term to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{P}(0):=\\operatorname*{max}_{\\tau\\geq0}\\operatorname*{min}_{\\beta\\geq0}\\frac{\\tau\\sigma^{2}}{\\beta}+\\tau\\beta(1-\\kappa)-\\tau^{2}+\\tau\\lambda\\mathbb{E}\\Bigg[\\frac{\\Theta^{2}+\\beta^{2}\\kappa}{\\tau V^{2}+\\beta\\lambda}\\Bigg].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Step 2: Convergence of the optimal solution We next need to extend this result to the desired Wasserstein-2 convergence result (8). Recall here that $\\pmb{u}^{(t+1)}$ is the solution of $P_{1}(0)$ . ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, let $h\\colon\\mathbb{R}^{3}\\rightarrow\\mathbb{R}$ be any bounded, Lipschitz function, and let $h^{(k)}$ be a sequence of bounded, twice-differentiable functions that converge uniformly to $h$ as $k\\rightarrow\\infty$ (e.g., the convolution of $h$ with a sequence of mollifiers). Let $P_{1}^{(k)}(\\bar{\\mu}),\\bar{P}^{(k)}(\\mu)$ be the optimal cost of $P_{1}$ and $\\bar{P}$ when using test function $h^{(k)}$ and for $\\mu\\in[-\\mu^{*},\\mu^{*}]$ . Note the convergence $P_{1}^{(k)}(\\mu)\\stackrel{P}{\\to}\\bar{P}^{(k)}(\\mu)$ for any $\\mu$ in a sufficiently small neighborhood around zero holds by Step 1. ", "page_idx": 16}, {"type": "text", "text": "By the uniform convergence of the $h^{(k)}$ to $h$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}P_{1}^{(k)}(\\mu)=P_{1}(\\mu)}\\\\ {\\operatorname*{lim}_{k\\to\\infty}\\bar{P}^{(k)}(\\mu)=\\bar{P}(\\mu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, fix $\\delta>0$ and choose $k$ large enough that $\\begin{array}{r}{|P_{1}^{(k)}(\\mu)-P_{1}(\\mu)|<\\frac{\\delta}{3}}\\end{array}$ and $\\begin{array}{r}{|\\bar{P}^{(k)}(\\mu)-\\bar{P}(\\mu)|<\\frac{\\delta}{3}}\\end{array}$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big\\{|P_{1}(\\mu)-\\bar{P}(\\mu)|>\\delta\\big\\}\\leq\\mathbb{P}\\Big\\{|P_{1}^{(k)}(\\mu)-\\bar{P}^{(k)}(\\mu)|>\\delta/3\\Big\\}\\rightarrow0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $P_{1}^{(k)}\\stackrel{P}{\\to}\\bar{P_{2}}^{(k)}$ for all $k$ . Hence, we can also apply the result of Step 1 to any bounded Lipschitz function $h$ . ", "page_idx": 16}, {"type": "text", "text": "Since the convergence result of Step 1 holds for any $\\mu$ in a neighborhood around zero, we can conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}h(u_{i}^{(t+1)},v_{i},\\theta_{i}^{*})\\left.\\stackrel{P}{\\rightarrow}\\frac{d\\bar{P}(\\mu)}{d\\mu}\\right|_{\\mu=0},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the derivative is well-defined since $\\bar{P}$ has a unique solution in a neighborhood around zero. The proof of this fact is identical to that of Lemma 7 of [5], so we omit it here. Moreover, using the Dominated Convergence Theorem to differentiate inside the expectation, we can compute this exactly: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left.{\\frac{d{\\bar{P}}(\\mu)}{d\\mu}}\\right|_{\\mu=0}=\\mathbb{E}\\,h\\left(\\operatorname{prox}_{\\lambda(\\cdot)^{2}}\\left({\\frac{\\Theta-{\\hat{\\beta}}G{\\sqrt{\\kappa}}}{V}};{\\frac{\\hat{\\beta}}{2{\\hat{\\tau}}V^{2}}}\\right),V,\\Theta\\right)=\\mathbb{E}\\,h\\left({\\frac{{\\hat{\\tau}}V(\\Theta-{\\hat{\\beta}}G{\\sqrt{\\kappa}})}{{\\hat{\\tau}}V^{2}+{\\hat{\\beta}}\\lambda}},V,\\Theta\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $({\\hat{\\beta}},{\\hat{\\tau}})$ are found in the optimal solution to $\\bar{P}(0)$ . ", "page_idx": 16}, {"type": "text", "text": "Hence, for all bounded, Lipschitz $h$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}h(u_{i}^{(t+1)},v_{i},\\theta_{i}^{*})\\xrightarrow{P}\\mathbb{E}\\,h\\left(\\frac{\\hat{\\tau}V(\\Theta-\\hat{\\beta}G\\sqrt{\\kappa})}{\\hat{\\tau}V^{2}+\\hat{\\beta}\\lambda},V,\\Theta\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "so the empirical distribution of \u221athe triple $(u_{i}^{(t+1)},v_{i},\\theta_{i}^{*})$ converges weakly to the distribution of the random variable $\\langle\\frac{\\hat{\\tau}V(\\Theta\\!-\\!\\hat{\\beta}G\\sqrt{\\kappa})}{\\hat{\\tau}V^{2}\\!+\\!\\hat{\\beta}\\lambda},V,\\Theta\\rangle$ , where $G\\sim\\mathcal{N}(0,1)$ and $(V,\\Theta)\\,\\sim\\,\\Pi_{t}$ . By choosing $h(u,v,\\theta)\\;=\\;u^{2}$ , we also know that second moments of the empirical distribution converge in probability. Hence, the convergence can be strengthened from weak convergence to convergence in $\\mathcal{W}_{2}$ distance (see, e.g. [33, Theorem 6.9]). ", "page_idx": 16}, {"type": "text", "text": "Step 3: Verifying the inductive hypothesis Lastly, we need to show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}\\delta(v_{i}^{(t+1)},\\theta_{i}^{*})\\overset{\\mathcal{W}_{2}}{\\rightarrow}\\mathrm{Law}(\\psi(Q_{t+1},V),\\Theta):=\\Pi_{t+1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(V,\\Theta)\\sim\\Pi_{t}$ . Here, weak convergence follows from the result of Step 2 since $\\psi$ is a continuous map. To show convergence of second moments, we need to show ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}v_{i}^{(t+1),2}=\\frac{1}{d}\\sum_{i=1}^{d}\\psi(u_{i}^{(t+1)},v_{i}^{(t)})^{2}\\xrightarrow{P}\\mathbb{E}[\\psi(Q_{t+1},V)^{2}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For $\\psi$ that satisfies Assumption 2, this convergence is immediate from the result of Step 2 (since $\\mathcal{W}_{2}$ convergence implies convergence in expectation of bounded continuous and PL(2) functions). Therefore, the initial inductive assumption made at the beginning of this proof holds at time $t+1$ , and we can apply the result inductively to conclude Theorem 1. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 2. The proof is an extension of the proof of Theorem 1 to the case where the test function $h$ acts on blocks rather than individual entries. Much of the proof is identical, so we only sketch the argument and highlight the major differences here. We begin with the inductive hypothesis that $\\begin{array}{r}{\\frac{1}{M}\\sum_{i=1}^{M}\\delta(\\pmb{v}_{i}^{(t)},\\pmb{\\theta}_{i}^{*})\\ \\overset{\\mathcal{W}_{2}}{\\rightarrow}\\Pi_{t}}\\end{array}$ , for a known distribution $\\Pi_{t}$ over $\\mathbb{R}^{b}\\times\\mathbb{R}^{b}$ . Recall here that $M$ denotes the number of blocks/factors of size $b$ (so $\\begin{array}{r}{M=\\frac{d}{b}.}\\end{array}$ ). ", "page_idx": 17}, {"type": "text", "text": "Then, let $h\\colon(\\mathbb{R}^{b})^{3}\\to\\mathbb{R}$ be a test function with $\\|\\nabla^{2}h\\|_{2}\\leq C$ and such that either $h$ is bounded or $h(\\pmb{u}_{i},\\pmb{v}_{i},\\pmb{\\theta}_{i})=\\|\\pmb{u}_{i}\\|_{2}^{2}$ . We consider a similar perturbed optimization problem: ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{1}(\\mu)=\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}\\frac{1}{n}\\bigg\\|y-\\frac{1}{\\sqrt{d}}X(u\\odot v)\\bigg\\|_{2}^{2}+\\frac{\\lambda}{d}\\|u\\|_{2}^{2}+\\frac{\\mu}{M}\\sum_{i=1}^{M}h(u_{i},v_{i},\\theta_{i}^{*}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Again, we consider this for $|\\mu|\\leq{\\frac{\\lambda}{b C}}$ , so that the optimization problem is $\\gtreqless$ strongly convex in $\\textbf{\\em u}$ Noting that the proof of Lemma 1 still holds in this grouped case, we can constrain $P_{1}(\\mu)$ to be over compact sets and apply the CGMT to obtain the auxiliary problem ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{2}(\\mu)=\\operatorname*{min}_{\\Delta\\in B_{\\Delta}}\\operatorname*{max}_{q\\in B_{q}}\\frac{2}{\\sqrt{n}}q^{\\top}\\epsilon-\\frac{2}{\\sqrt{n}}\\|q\\|_{2}g^{\\top}\\Delta-\\frac{2}{\\sqrt{n}}\\|\\Delta\\|_{2}h^{\\top}q-\\|q\\|_{2}^{2}+\\frac{\\lambda}{d}\\left\\|\\frac{\\sqrt{d}\\Delta+\\theta^{*}}{v}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n+\\,\\frac{\\mu}{M}\\sum_{i=1}^{M}h(\\boldsymbol{u}_{i},\\boldsymbol{v}_{i}^{(t)},\\boldsymbol{\\theta}_{i}^{*}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The sequence of \u201cscalarization\u201d steps on $P_{2}$ is identical to in Theorem 1, until we arrive at ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathfrak{z}}_{2}(\\mu)\\overset{d}{=}\\underset{0\\leq\\tau\\leq R}{\\operatorname*{max}}\\underset{\\underset{\\sigma\\leq\\beta\\leq\\sigma+R}{\\operatorname*{min}}}\\frac{\\tau\\|h\\|_{2}}{\\sqrt{n}}\\Bigg(\\frac{\\sigma^{2}}{\\beta}+\\frac{\\|u\\odot v-\\theta^{*}\\|_{2}^{2}}{\\beta d}+\\beta\\Bigg)-\\frac{2\\tau}{\\sqrt{n d}}g^{\\top}(u\\odot v-\\theta^{*})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\tau^{2}+\\displaystyle\\frac{\\lambda}{d}\\|u\\|_{2}^{2}+\\frac{\\mu}{M}\\sum_{i=1}^{M}h(u_{i},v_{i}^{(t)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, since the sum of the last two terms in the objective function is $\\gtreqless$ strongly convex by our choice of $\\mu$ , the proof of Lemma 2 holds without change and we can consider the unconstrained minimization over $\\textbf{\\em u}$ . In this case, the minimization is block-separable over each of the $M$ factors of $\\textbf{\\em u}$ , so it can be expressed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{d}\\sum_{i=1}^{M}\\operatorname*{min}_{u_{i}\\in\\mathbb{R}^{b}}\\left\\{\\frac{\\tau\\xi}{\\beta}\\|u_{i}\\odot v_{i}-\\theta_{i}^{*}\\|_{2}^{2}-2\\tau\\sqrt{\\kappa}g_{i}^{\\top}(u_{i}\\odot v_{i}-\\theta)+\\lambda\\|u\\|_{2}^{2}+\\mu b h(u_{i},v_{i},\\theta_{i}^{*})\\right\\}}\\\\ &{\\displaystyle=\\frac{1}{b M}\\sum_{i=1}^{M}\\operatorname*{min}_{u_{i}\\in\\mathbb{R}^{b}}\\left\\{\\frac{\\tau\\xi}{\\beta}\\|u_{i}\\odot v_{i}-\\theta_{i}^{*}\\|_{2}^{2}-2\\tau\\sqrt{\\kappa}g_{i}^{\\top}(u_{i}\\odot v_{i}-\\theta)+\\lambda\\|u\\|_{2}^{2}+\\mu b h(u_{i},v_{i},\\theta_{i}^{*})\\right\\}}\\\\ &{\\displaystyle:=\\frac{1}{b M}\\sum_{i=1}^{M}q(v_{i},\\theta_{i}^{*},g_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{\\boldsymbol{g}}_{i}\\in\\mathbb{R}^{b}$ denotes the $i$ th block of $\\textbf{\\textit{g}}$ and $q:=(\\mathbb{R}^{b})^{3}\\to\\mathbb{R}$ is defined as a shorthand for the quantity inside the summation. ", "page_idx": 18}, {"type": "text", "text": "Next, we consider the asymptotic behavior of $P_{2}(\\mu)$ . Here, the only term which is different than in Theorem 1 is the termb1M $\\begin{array}{r}{\\frac{1}{b M}\\sum_{i=1}^{M}q(\\pmb{v}_{i},\\pmb{\\theta}_{i}^{\\ast},\\pmb{g}_{i})}\\end{array}$ . By the same argument as in Lemma 3, we can write $q$ as the Moreau envelope of a convex function and show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{b M}\\sum_{i=1}^{M}q(v_{i},\\pmb{\\theta}_{i}^{*},\\pmb{g}_{i})\\stackrel{P}{\\rightarrow}\\mathbb{E}\\,\\frac{1}{b}q(V,\\pmb{\\Theta},\\pmb{G}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the expectation is over $(V,\\Theta)\\sim\\Pi_{t}$ and $G\\sim\\mathcal{N}(\\mathbf{0},I_{b})$ . After the same uniform convergence argument as in the proof of Theorem 1, we can conclude that for all $\\mu\\in[-\\mu^{*},\\mu^{*}]$ , $P_{1}(\\mu)\\stackrel{P}{\\to}\\bar{P}(\\mu)$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{P}(\\mu)=\\operatorname*{max}_{\\tau\\geq0}\\operatorname*{min}_{\\beta\\geq0}\\frac{\\tau\\sigma^{2}}{\\beta}+\\tau\\beta-\\tau^{2}+\\mathbb{E}\\,\\frac{1}{b}q(V,\\Theta,G).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, when $\\mu=0$ , the minimization implicit in the definition of $q$ can be solved exactly; this yields exactly the optimization problem in 7. Step 2 of the proof (convergence of test functions of the optimal minimizer) is identical to that of Theorem 1, and for the final step (showing the inductive hypothesis holds at the next iteration), we need to argue that the second moment of $\\pmb{v}^{(t+1)}=\\psi(\\pmb{u}_{i}^{(t+1)},\\pmb{v}_{i})$ (ui(t+1), vi) converges to its expectation under \u03a0t+1: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{M}\\sum_{i=1}^{M}\\lVert\\psi(\\pmb{u}_{i}^{(t+1)},\\pmb{v}_{i})\\rVert_{2}^{2}\\stackrel{P}{\\rightarrow}\\mathbb{E}\\lVert\\psi(\\pmb{Q}_{t+1},\\pmb{V})\\rVert_{2}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $\\psi$ is bounded and continuous or has PL(2) coordinate projections (as we have assumed), then the above convergence holds based on the Wasserstein-2 convergence of the joint distribution of $(\\boldsymbol{u}^{(t+1)},\\boldsymbol{v})$ . ", "page_idx": 18}, {"type": "text", "text": "B Technical lemmas ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition 1. The function $g(u,v,\\theta)=|u v-\\theta|$ is pseudo-Lipschitz of order 2. ", "page_idx": 18}, {"type": "text", "text": "Proof. The result follows from the following series of inequalities: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|u v-\\theta\\|-|u^{\\prime}v^{\\prime}-\\theta^{\\prime}|\\|\\leq|u v-\\theta-(u^{\\prime}v^{\\prime}-\\theta^{\\prime})|}&{}\\\\ {\\leq|u v-u^{\\prime}v^{\\prime}|+|\\theta-\\theta^{\\prime}|}&{}\\\\ {\\leq|u||v-v^{\\prime}|+|v^{\\prime}||u-u^{\\prime}|+|\\theta-\\theta^{\\prime}|}&{}\\\\ {\\leq(|u|+|v^{\\prime}|+1)(|u-u^{\\prime}|+|v-v^{\\prime}|+|\\theta-\\theta^{\\prime}|)}&{}\\\\ {\\leq(1+\\|x\\|_{1}+\\|x^{\\prime}\\|_{1})\\|x-x^{\\prime}\\|_{1}}&{}\\\\ {\\leq3(1+\\|x\\|_{2}+\\|x^{\\prime}\\|_{2})\\|x-x^{\\prime}\\|_{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{\\Delta}x,x^{\\prime}\\in\\mathbb{R}^{3}$ denote $(u,v,\\theta)$ and $(u^{\\prime},v^{\\prime},\\theta^{\\prime})$ , respectively. ", "page_idx": 18}, {"type": "text", "text": "Lemma 1. Let $\\Delta^{*},q^{*}$ be the optimal solution to $(I O)$ . Then, there exists universal constant $C_{1}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d\\to\\infty}\\mathbb{P}\\{\\|\\Delta^{*}\\|_{2}\\leq C_{1}\\|v\\|_{\\infty}\\}=\\operatorname*{lim}_{d\\to\\infty}\\mathbb{P}\\{\\|q^{*}\\|_{2}\\leq C_{1}\\|v\\|_{\\infty}\\}=1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We proceed via a similar argument to Lemma 2 in [5]. First, consider the original expression for $P_{1}(\\mu)$ from (9) : ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{1}(\\mu)=\\operatorname*{min}_{{\\pmb u}\\in\\mathbb{R}^{d}}F({\\pmb u})+R({\\pmb u}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{F(\\pmb{u}):=\\frac{1}{n}\\Big\\|\\pmb{y}-\\frac{1}{\\sqrt{d}}\\pmb{X}(\\pmb{u}\\odot\\pmb{v})\\Big\\|_{2}^{2}}\\end{array}$ and $\\begin{array}{r}{R(\\pmb{u}):=\\frac{\\lambda}{d}\\|\\pmb{u}\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h(u_{i},v_{i}^{(t)},\\theta_{i}^{\\ast})}\\end{array}$ . Recall here that $R$ is $\\gtreqless$ -strongly convex for all $\\mu\\in[-\\mu^{*},\\mu^{*}]$ , and denote the unique optimal minimizer to this ", "page_idx": 18}, {"type": "text", "text": "problem as $\\pmb{u}^{*}$ . Then, the following chain of inequalities holds, by the optimality of $\\pmb{u}^{*}$ and the non-negativity of $F$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\|\\pmb{y}\\|_{2}^{2}+R(\\mathbf{0})=F(\\mathbf{0})+R(\\mathbf{0})\\geq F(\\pmb{u}^{*})+R(\\pmb{u}^{*})\\geq R(\\pmb{u}^{*}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, by the strong convexity of $R$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nR(\\mathbf{u}^{*})\\geq R(\\mathbf{0})+\\nabla R(\\mathbf{0})^{\\top}\\mathbf{u}^{*}+\\frac{\\lambda}{2d}\\|\\mathbf{u}^{*}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the above two series of inequalities, we obtain (recall $\\textstyle\\kappa={\\frac{d}{n}}$ ) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\pmb{u}^{*}\\|_{2}^{2}+\\frac{2d}{\\lambda}\\nabla R(\\mathbf{0})^{\\top}\\pmb{u}^{*}\\leq\\frac{2\\kappa}{\\lambda}\\|\\pmb{y}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "After completing the square, this yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{\\boldsymbol{u}}^{*}+\\frac{d}{\\lambda}\\nabla R(\\mathbf{\\boldsymbol{0}})\\right\\|_{2}^{2}\\leq\\frac{2\\kappa}{\\lambda}\\|\\mathbf{\\boldsymbol{y}}\\|_{2}^{2}+\\frac{d^{2}}{\\lambda^{2}}\\|\\nabla R(\\mathbf{\\boldsymbol{0}})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "whence, by the triangle inequality, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\|\\pmb{u}^{*}\\|_{2}\\leq\\displaystyle\\frac{d}{\\lambda}\\|\\nabla R(\\mathbf{0})\\|_{2}+\\sqrt{\\frac{2\\kappa}{\\lambda}}\\|\\pmb{y}\\|_{2}^{2}+\\frac{d^{2}}{\\lambda^{2}}\\|\\nabla R(\\mathbf{0})\\|_{2}^{2}}\\\\ {\\leq\\displaystyle\\frac{2d}{\\lambda}\\|\\nabla R(\\mathbf{0})\\|_{2}+\\sqrt{\\frac{2\\kappa}{\\lambda}}\\|\\pmb{y}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, standard concentration inequalities for Gaussian random variables \u221a(e.g., [32, Theore\u221am 5.2.2, Corollary 7.3.3]) imply that, with pr\u221aobability approaching 1, $\\|X\\|_{2}\\lesssim{\\sqrt{d}}$ and $\\|\\epsilon\\|_{2}\\lesssim\\sqrt{d}$ . And Assumption 1 implies that $\\lVert\\theta^{*}\\rVert_{2}\\lesssim\\sqrt{d}$ with probability tending to 1. So, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\pmb{y}\\|_{2}\\leq\\frac{\\mu}{\\sqrt{d}}\\|\\pmb{X}\\|\\|\\pmb{\\theta}^{*}\\|_{2}+\\|\\pmb{\\epsilon}\\|_{2}\\lesssim\\sqrt{d}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability approaching 1. Next, we bound $\\|\\nabla R(\\mathbf{0})\\|_{2}$ . Recalling the definition of $R$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla R(\\mathbf{0})\\|_{2}=\\frac{\\mu}{d}\\sqrt{\\sum_{i=1}^{d}\\biggl(\\frac{\\partial}{\\partial u}h(u,v_{i},\\theta_{i}^{*})\\biggl|_{u=0}\\biggr)^{2}}=\\frac{1}{\\sqrt{d}}\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}\\biggl(\\frac{\\partial}{\\partial u}h(u,v_{i},\\theta_{i}^{*})\\biggl|_{u=0}\\biggr)^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the function $\\begin{array}{r}{g(v,\\theta)\\,=\\,\\frac{\\partial}{\\partial u}h(u,v,\\theta)\\bigg|_{u=0}}\\end{array}$ is Lipschitz (by the fact that $h$ has bounded second derivatives), $g^{2}$ is pseudo-Lipschitz of order 2. So, the quantity under the square root converges in probabil\u221aity to $\\bar{\\mathbb{E}}_{(V,\\Theta)\\sim\\Pi_{t}}\\bar{g}^{2}$ by Assumption 1, and, with probability tending to 1, we have $\\|\\nabla R(\\mathbf{0})\\|_{2}\\lesssim1/\\sqrt{d}$ . ", "page_idx": 19}, {"type": "text", "text": "Combining the above bounds on $\\|\\pmb{y}\\|_{2}$ and $\\|\\nabla R(\\mathbf{0})\\|_{2}$ , we can conclude that $\\|\\pmb{u}^{*}\\|_{2}\\lesssim\\sqrt{d}$ . The first part of the lemma follows by noting that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\pmb{\\Delta}^{*}\\|_{2}=\\frac{1}{\\sqrt{d}}\\|\\pmb{u}^{*}\\odot\\pmb{v}-\\pmb{\\theta}^{*}\\|_{2}\\leq\\frac{1}{\\sqrt{d}}\\|\\pmb{u}^{*}\\odot\\pmb{v}\\|_{2}+\\frac{1}{\\sqrt{d}}\\|\\pmb{\\theta}^{*}\\|_{2}}\\\\ {\\displaystyle\\leq\\frac{1}{\\sqrt{d}}\\|\\pmb{v}\\|_{\\infty}\\|\\pmb{u}^{*}\\|_{2}+\\frac{1}{\\sqrt{d}}\\|\\pmb{\\theta}^{*}\\|_{2}}\\\\ {\\displaystyle\\lesssim\\|\\pmb{v}\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality holds with probability approaching 1. Lastly, the optimal $\\pmb q$ for any $\\Delta$ has closed-form q =\u221a1n\u03f5 \u2212 $\\begin{array}{r}{\\pmb q=\\frac{\\mathbf1}{\\sqrt{n}}\\pmb\\epsilon-\\frac{1}{\\sqrt{n}}\\pmb X\\Delta}\\end{array}$ . By the triangle inequality, we then obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\pmb{q}^{*}\\|_{2}\\leq\\frac{1}{\\sqrt{n}}\\|\\pmb{\\epsilon}\\|_{2}+\\frac{1}{\\sqrt{n}}\\|\\pmb{X}\\|\\|\\pmb{\\Delta}^{*}\\|_{2}\\lesssim\\|\\pmb{v}\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with the last inequality holding with probability approaching 1, by the concentration of norms of $\\epsilon$ and $\\mathbf{\\deltaX}$ as discussed above, and the bound on $\\lVert\\Delta^{*}\\rVert_{2}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 2. Consider the following unconstrained minimization problem over $\\pmb{u}\\in\\mathbb{R}^{d}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}\\operatorname*{max}_{0\\leq\\tau\\leq R}\\frac{2\\tau\\|h\\|_{2}}{\\sqrt{n}}\\sqrt{\\sigma^{2}+\\frac{1}{d}\\|u\\odot v-\\theta^{*}\\|_{2}^{2}}-\\frac{2\\tau}{\\sqrt{n d}}g^{\\top}(u\\odot v-\\theta^{*})}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\tau^{2}+\\frac{\\lambda}{d}\\|u\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h(u_{i},v_{i}^{(t)},\\theta_{i}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With probability approaching $^{\\,I}$ , the solution $\\pmb{u}^{*}$ satisfies $\\begin{array}{r}{\\left\\lVert\\frac{1}{\\sqrt{d}}\\bigl(\\pmb{u}^{*}\\odot\\pmb{v}-\\pmb{\\theta}^{*}\\bigr)\\right\\rVert_{2}\\lesssim\\|\\pmb{v}\\|_{\\infty}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. First note $\\begin{array}{r}{\\left\\|\\frac{1}{\\sqrt{d}}(\\pmb{u}^{*}\\odot\\pmb{v}-\\pmb{\\theta}^{*})\\right\\|_{2}\\leq\\frac{1}{\\sqrt{d}}\\|\\pmb{u}^{*}\\|_{2}\\|\\pmb{v}\\|_{\\infty}+\\frac{1}{\\sqrt{d}}\\|\\pmb{\\theta}^{*}\\|_{2}}\\end{array}$ , and $\\frac{1}{\\sqrt{d}}\\,\\|\\theta^{*}\\|_{2}$ is bounded by a constant with probability approachin\u221ag 1 by the assumed $\\mathcal{W}_{2}$ convergence of $\\theta^{*}$ to a fixed limit. Hence, it suffices to show that $\\|\\pmb{u}^{*}\\|_{2}\\lesssim\\sqrt{d}$ with high probability. We begin by noting that the inner maximization over $\\tau$ admits a closed form solution, so the problem becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\iota\\in\\mathbb{R}^{d}}\\left(\\frac{\\|h\\|_{2}}{2\\sqrt{n}}\\sqrt{\\sigma^{2}+\\frac{1}{d}\\|u\\odot v-\\theta^{*}\\|_{2}^{2}}-\\frac{1}{2\\sqrt{n d}}g^{\\top}(u\\odot v-\\theta^{*})\\right)_{+}^{2}+\\frac{\\lambda}{d}\\|u\\|_{2}^{2}+\\frac{\\mu}{d}\\sum_{i=1}^{d}h(u_{i},v_{i}^{(t)},\\theta_{i}^{*}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, we can proceed similarly to in the proof of Lemma 1. Let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F(\\boldsymbol{u}):=\\left(\\frac{\\|h\\|_{2}}{2\\sqrt{n}}\\sqrt{\\sigma^{2}+\\frac{1}{d}\\|\\boldsymbol{u}\\odot\\boldsymbol{v}-\\boldsymbol{\\theta}^{*}\\|_{2}^{2}}-\\frac{1}{2\\sqrt{n d}}g^{\\top}(\\boldsymbol{u}\\odot\\boldsymbol{v}-\\boldsymbol{\\theta}^{*})\\right)_{+}^{2},}\\\\ {\\displaystyle R(\\boldsymbol{u}):=\\frac{\\lambda}{d}\\|\\boldsymbol{u}\\|_{2}^{2}+\\frac{\\mu}{d}\\displaystyle\\sum_{i=1}^{d}h(\\boldsymbol{u}_{i},\\boldsymbol{v}_{i}^{(t)},\\boldsymbol{\\theta}_{i}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, noting $F$ is always non-negative and $R$ is $\\gtreqless$ strongly-convex, we use the same argument as in Lemma 1 to obtain the inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\pmb{u}^{*}\\|_{2}^{2}+\\frac{2d}{\\lambda}\\nabla R(\\mathbf{0})^{\\top}\\pmb{u}^{*}\\leq\\frac{2d}{\\lambda}F(\\mathbf{0}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "After completing the squares, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{\\boldsymbol{u}}^{*}+\\frac{d}{\\lambda}\\nabla R(\\mathbf{\\boldsymbol{0}})\\right\\|_{2}^{2}\\leq\\frac{2d}{\\lambda}F(\\mathbf{\\boldsymbol{0}})+\\frac{d^{2}}{\\lambda^{2}}\\|\\nabla R(\\mathbf{\\boldsymbol{0}})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so we can conclude ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\pmb{u}^{*}\\|_{2}\\leq\\frac{d}{\\lambda}\\|\\nabla R(\\mathbf{0})\\|_{2}+\\sqrt{\\frac{2d}{\\lambda}}F(\\mathbf{0})+\\frac{d^{2}}{\\lambda^{2}}\\|\\nabla R(\\mathbf{0})\\|_{2}^{2}\\leq\\frac{2d}{\\lambda}\\|\\nabla R(\\mathbf{0})\\|_{2}+\\sqrt{\\frac{2d}{\\lambda}}F(\\mathbf{0}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As shown in Lemma 1, $\\|\\nabla R(\\mathbf{0})\\|_{2}\\lesssim\\frac{1}{\\sqrt{d}}$ with probability approaching 1. It remains to show that $\\sqrt{F(\\mathbf{0})}\\lesssim1$ with probability approaching 1. To see this, observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sqrt{F(\\mathbf{0})}=\\left(\\frac{\\|h\\|_{2}}{2\\sqrt{n}}\\sqrt{\\sigma^{2}+\\frac{1}{d}{\\|\\theta^{*}\\|_{2}^{2}}}-\\frac{1}{2\\sqrt{n d}}g^{\\top}\\theta^{*}\\right)_{+}}\\\\ {\\displaystyle\\leq\\left|\\frac{\\|h\\|_{2}}{2\\sqrt{n}}\\sqrt{\\sigma^{2}+\\frac{1}{d}{\\|\\theta^{*}\\|_{2}^{2}}}-\\frac{1}{2\\sqrt{n d}}g^{\\top}\\theta^{*}\\right|}\\\\ {\\displaystyle\\leq\\frac{\\|h\\|_{2}}{2\\sqrt{n}}\\sqrt{\\sigma^{2}+\\frac{1}{d}{\\|\\theta^{*}\\|_{2}^{2}}}+\\frac{1}{2\\sqrt{n d}}\\|g\\|_{2}\\|\\theta^{*}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last line uses the triangle and Cauchy-Schwarz inequalities. By concentration of the norm for Gaussian vectors, there exists a universal constant $c$ such that $\\begin{array}{r}{\\frac{\\|h\\|_{2}}{\\sqrt{n}}\\,\\leq\\,c}\\end{array}$ \u2264c and \u2225\u221ag\u22252 $\\begin{array}{r}{\\frac{\\|g\\|_{2}}{\\sqrt{n}}\\,\\leq\\,c}\\end{array}$ with probability approaching 1. Moreover, by Assumption 1, \u2225\u03b8\u221ad\u22252 with probability approaching 1. Hence, $\\sqrt{F(\\mathbf{0})}\\lesssim1$ with probability approaching 1. Substituting this into the bound for $\\lVert\\pmb{u}^{*}\\rVert_{2}$ from above completes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma 3. Under Assumption $^{\\,\\,\\,I}$ , the function $f_{d}(\\tau,\\beta)$ (Eq. 19) converges pointwise in probability to $f(\\tau,\\beta)$ (Eq. 20) as $d\\to\\infty$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We consider the limit of each term in $f_{d}$ separately. The limit of the terms $\\frac{\\tau\\sigma^{2}\\xi}{\\beta}$ and $\\tau\\beta\\xi$ is found by noting that $\\xi\\rightarrow1$ in probability by Gaussian Lipschitz concentration. ", "page_idx": 21}, {"type": "text", "text": "The first summation term simplifies as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{d}\\sum_{i=1}^{d}\\Big[\\frac{\\tau\\xi\\theta_{i}^{*2}}{\\beta}-\\frac{\\tau}{\\beta\\xi}\\big(\\beta g_{i}\\sqrt{\\kappa}-\\xi\\theta_{i}^{*}\\big)^{2}\\Big]=\\frac{1}{d}\\sum_{i=1}^{d}\\Big[-\\frac{\\tau}{\\beta\\xi}\\big(\\beta^{2}g_{i}^{2}\\kappa-2\\beta g_{i}\\sqrt{\\kappa}\\xi\\theta_{i}^{*}\\big)\\Big]}}\\\\ &{}&{=-\\frac{\\tau}{\\beta\\xi}\\frac{1}{d}\\sum_{i=1}^{d}\\[\\beta^{2}g_{i}^{2}\\kappa-2\\beta g_{i}\\sqrt{\\kappa}\\xi\\theta_{i}^{*}\\big]\\stackrel{P}{\\rightarrow}-\\tau\\beta\\kappa,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last line follows from the weak law of large numbers since the $g_{i}$ are i.i.d. standard Gaussian variables. ", "page_idx": 21}, {"type": "text", "text": "For the last term, after again using the fact that $\\xi\\xrightarrow[]{P}1$ , we need to consider ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}\\left[\\mathcal{M}_{\\lambda(\\cdot)^{2}+\\mu h(\\cdot,v_{i}^{(t)},\\theta_{i}^{*})}\\left(\\frac{\\theta_{i}^{*}-\\beta g_{i}\\sqrt{\\kappa}}{v_{i}^{(t)}};\\frac{\\beta}{2\\tau v_{i}^{(t)2}}\\right)\\right]=:\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To show convergence in probability of this term, first fix $\\delta>0$ . Then, we want to show ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})-\\mathbb{E}\\,q(V,\\Theta,G)\\right|>\\delta\\right]\\to0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the expectation is over $(V,\\Theta)\\sim\\Pi_{t}$ and $G\\sim\\mathcal{N}(0,1)$ . It suffices to show the following two statements: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}\\left[\\left|\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})-\\mathbb{E}_{g}\\left.\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})\\right|>\\frac{\\delta}{2}\\right]\\rightarrow0,}\\\\ {\\displaystyle\\mathbb{P}\\left[\\left|\\mathbb{E}_{g}\\left.\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})-\\mathbb{E}\\,q(V,\\Theta,G)\\right|>\\frac{\\delta}{2}\\right]\\rightarrow0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To show (26), we rely on a concentration inequality for the Moreau envelope of a Gaussian vector plus a bounded vector from [5]. First note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{i}\\displaystyle\\sum_{i=1}^{d}q(\\boldsymbol{v}_{i},\\boldsymbol{\\theta}_{i}^{*},g_{i})=\\frac{1}{d}\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}\\Biggl\\{\\lambda\\|{\\boldsymbol{u}}\\|_{2}^{2}+\\mu\\displaystyle\\sum_{i=1}^{d}h(\\boldsymbol{u}_{i},\\boldsymbol{v}_{i},\\boldsymbol{\\theta}_{i}^{*})+\\frac{\\tau}{\\beta}\\big\\|{\\boldsymbol{u}}\\odot{\\boldsymbol{v}}-{\\boldsymbol{\\theta}}^{*}+\\beta\\sqrt{\\boldsymbol{\\kappa}}g\\big\\|_{2}^{2}\\Biggr\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{d}\\operatorname*{min}_{u\\in\\mathbb{R}^{d}}\\Biggl\\{\\lambda\\big\\|{\\boldsymbol{u}}\\big\\|_{2}^{2}+\\mu\\displaystyle\\sum_{i=1}^{d}h(\\boldsymbol{u}_{i},\\boldsymbol{v}_{i},\\boldsymbol{\\theta}_{i}^{*})+\\tau\\beta\\kappa\\bigg\\|\\frac{u\\odot{\\boldsymbol{v}}}{\\beta\\sqrt{\\kappa}}-\\frac{\\theta^{*}}{\\beta\\sqrt{\\kappa}}+g\\bigg\\|_{2}^{2}\\Biggr\\}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{d}\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}\\Biggl\\{\\lambda\\bigg\\|\\frac{\\beta\\sqrt{\\kappa}\\theta}{{\\boldsymbol{v}}}\\bigg\\|_{2}^{2}+\\mu\\displaystyle\\sum_{i=1}^{d}h\\bigg(\\frac{\\beta\\sqrt{\\kappa}\\theta_{i}}{v_{i}},\\boldsymbol{v}_{i},\\boldsymbol{\\theta}_{i}^{*}\\bigg)+\\tau\\beta\\kappa\\bigg\\|\\theta-\\frac{\\theta^{*}}{\\beta\\sqrt{\\kappa}}+g\\bigg\\|_{2}^{2}\\Biggr\\}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{d}M_{\\ell}\\bigg(\\frac{\\theta^{*}}{\\beta\\sqrt{\\kappa}}-g;\\frac{1}{2\\tau\\beta\\kappa}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second to last line follows from the change of variable $\\begin{array}{r}{\\pmb{\\theta}=\\frac{\\pmb{u}\\odot\\pmb{v}}{\\beta\\sqrt{\\kappa}}}\\end{array}$ , and $\\ell(\\pmb\\theta):=\\lambda\\Big\\|\\frac{\\beta\\sqrt{\\kappa}\\pmb\\theta}{\\pmb v}\\Big\\|_{2}^{2}+$ $\\begin{array}{r}{\\mu\\sum_{i=1}^{d}h\\Big(\\frac{\\beta\\sqrt{\\kappa}\\theta_{i}}{v_{i}},v_{i},\\theta_{i}^{*}\\Big)}\\end{array}$ . Here, for fixe\u221ad $\\mathbf{\\boldsymbol{v}},\\,\\boldsymbol{\\ell}$ is a proper convex function of $\\pmb{\\theta}$ . Moreover, by Assumption 1, $\\frac{\\pmb{\\theta}^{*}}{\\beta\\sqrt{\\kappa}}$ has norm of order $\\sqrt{d}$ with high probability. Hence, by [5, Lemma 8], this quantity concentrates around its expectation (with respect to $\\textbf{\\textit{g}}$ ), and we can conclude that there is some $c>0$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})-\\mathbb{E}_{g}\\left.\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})\\right|>\\frac{\\delta}{2}\\right]\\leq\\frac{c\\tau^{2}\\beta^{2}\\kappa^{2}}{d\\delta^{2}}\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To show (27), note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{g}\\,\\frac{1}{d}\\sum_{i=1}^{d}q(v_{i},\\theta_{i}^{*},g_{i})=\\frac{1}{d}\\sum_{i=1}^{d}\\mathbb{E}_{G}\\,q(v_{i},\\theta_{i}^{*},G).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Observe that this quantity is an expectation with respect to the joint empirical distribution of $(v,\\theta^{*})$ . By the assumption of $\\mathcal{W}_{2}$ convergence of the empirical distribution of $(v,\\theta^{*})$ (Assumption 1), if we can show that mapping $(v,\\theta)\\rightarrow\\mathbb{E}_{G}\\,q(v,\\theta,G)$ is bounded, then we can conclude that the above quantity converges in probability to $\\mathbb{E}\\,q(V,\\Theta,G)$ , with $(V,\\Theta)\\sim\\Pi_{t}$ . To see this, recall that for all $G\\in\\mathbb R$ , $q$ is bounded below as ", "page_idx": 22}, {"type": "equation", "text": "$$\nq(v,\\theta,G)\\geq\\operatorname*{min}_{u}\\lambda u^{2}+\\mu h(u,v,\\theta),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is always bounded below since $h$ is bounded (or, in the case $h=u^{2}$ , the lower bound is zero). Next, for a given $G$ , we can bound $q$ above as ", "page_idx": 22}, {"type": "equation", "text": "$$\nq(v,\\theta,G)\\leq\\mu h(0,v,\\theta)+\\frac{\\tau}{\\beta}(\\theta-\\beta\\sqrt{\\kappa}G)^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{G}\\,q(v,\\theta,G)\\leq\\mu h(0,v,\\theta)+\\frac{\\tau}{\\beta}\\theta^{2}+\\tau\\beta\\kappa<C\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some universal constant $C>0$ , since $\\theta$ is bounded by assumption and $h(0,v,\\theta)$ is either bounded above or equal to 0 (in the case where $h=u^{2}$ ). Combining (26) and (27) yields the desired result. ", "page_idx": 22}, {"type": "text", "text": "C Solving the min-max problem ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Below, we show that the max-min problems (5) and (7) have easily computable solutions. Note it suffices to consider the grouped case (7), since we can apply it with $b=1$ to recover the ungrouped case. The following derivation closely follows the analysis of the scalar max-min problem in [8], who study a similar scalar problem (albeit in the case of $\\lambda=0$ , which we do not consider). ", "page_idx": 22}, {"type": "text", "text": "First recall that, as shown in the proof of Theorem 1, there exists a unique saddle point, due to the strong convexity in $\\tau$ and strict convexity in $\\beta$ . Now, taking derivatives with respect to $\\tau$ and $\\beta$ , we obtain the following saddle point conditions: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{0=\\displaystyle\\frac{\\sigma^{2}}{\\beta}+\\beta(1-\\kappa)-2\\tau+\\beta\\lambda^{2}\\mathbb{E}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{\\Theta_{i}^{2}+\\beta^{2}\\kappa}{(\\tau V_{i}^{2}+\\beta\\lambda)^{2}}\\right],}}\\\\ {{0=\\displaystyle-\\frac{\\tau\\sigma^{2}}{\\beta^{2}}+\\tau(1-\\kappa)+\\tau\\lambda\\mathbb{E}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{1}{\\tau V_{i}^{2}+\\beta\\lambda}\\right]-\\tau\\lambda^{2}\\mathbb{E}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{\\Theta_{i}^{2}+\\beta^{2}\\kappa}{(\\tau V_{i}^{2}+\\beta\\lambda)^{2}}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Solving each of these equations for the quantity \u03b22\u03bb2 E b1 ib=1(\u03c4\u0398Vi 2i 2 ++\u03b2\u03b22\u03bb\u03ba)2 and then equating the two, we arrive at ", "page_idx": 22}, {"type": "equation", "text": "$$\n2\\tau\\beta-\\beta^{2}(1-\\kappa)-\\sigma^{2}=\\beta^{2}(1-\\kappa)-\\sigma^{2}+2\\lambda\\beta^{3}\\kappa\\mathbb{E}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{1}{\\tau V_{i}^{2}+\\beta\\lambda}\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tau=\\beta(1-\\kappa)+\\lambda\\beta^{2}\\kappa\\mathbb{E}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{1}{\\tau V_{i}^{2}+\\beta\\lambda}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Defining the auxiliary variable $\\gamma=\\tau/\\beta$ , this yields the fixed point equation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma=1-\\kappa+\\lambda\\kappa\\,\\mathbb{E}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{1}{\\gamma V_{i}^{2}+\\lambda}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting this $\\gamma$ back into the first optimality condition in (28), we can express the optimal $\\beta$ in closed-form, in terms of $\\gamma$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta=\\sqrt{\\frac{\\sigma^{2}+\\lambda^{2}\\operatorname{\\mathbb{E}}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{\\Theta_{i}^{2}}{(\\gamma V_{i}^{2}+\\lambda)^{2}}\\right]}{2\\gamma+\\kappa-1-\\lambda^{2}\\kappa\\operatorname{\\mathbb{E}}\\left[\\frac{1}{b}\\sum_{i=1}^{b}\\frac{1}{(\\gamma V_{i}^{2}+\\lambda)^{2}}\\right]}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, the optimal $\\tau$ can be found simply as $\\tau=\\gamma\\beta$ . ", "page_idx": 23}, {"type": "text", "text": "This yields a simple recipe for solving the min-max problem. First, compute the positive solution $\\hat{\\gamma}$ to the fixed point equation (30) (this can be found easily using standard numerical solvers). Then, $({\\hat{\\beta}},{\\hat{\\tau}})$ are both given in closed-form as functions of $\\hat{\\gamma}$ (where the required expectations can all be approximated via Monte Carlo simulation). ", "page_idx": 23}, {"type": "text", "text": "D Further simulations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we demonstrate that our asymptotic predictions can provide accurate estimates of the test error, even when some of our technical assumptions are not satisfied. ", "page_idx": 23}, {"type": "text", "text": "First, we compare the two \u201cheavier\u201d weightings considered in Section 3.1, $\\psi(u,v)=\\operatorname{tanh}{|u v|}$ and $\\psi(u,v)=\\operatorname{tanh}u^{2}$ , to the same weightings without the bounded tanh activation: $\\psi(u,v)=|u v|$ and $\\psi(u,v)=u^{2}$ . We note that the reweighting choice $\\lvert u v\\rvert$ is considered in [21, 28] as a limit as $p\\rightarrow0$ of the classical IRLS update for $\\ell_{p}$ minimization. In Figure 3a, we consider the same sparse regression as in Section 3.1, i.e., with $n=250,d=2000,\\sigma=0.1,\\theta_{i}^{*}\\overset{\\mathrm{i.i.d.}}{\\sim}$ Bernoulli(0.01) and $\\lambda$ chosen to minimize the predicted asymptotic loss. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "For each choice of $\\psi$ , we apply the theoretical predictions of Theorem 1, even if $\\psi$ violates Assumption 2. We find that our predictions remain accurate for all these choices of $\\psi$ . The choice tanh $\\lvert u v\\rvert$ performs almost identically without the tanh activation. Interestingly, the choice $\\psi\\,=\\,\\operatorname{tanh}u^{2}$ outperforms the variant without the tanh and has a more regular decay of the test loss. ", "page_idx": 23}, {"type": "text", "text": "In Figure $^{3\\mathrm{b}}$ , we apply Theorem 1 to predict the asymptotic squared test loss: $\\frac{1}{d}\\|\\boldsymbol{u}\\odot\\pmb{v}-\\pmb{\\theta}^{*}\\|_{2}^{2}$ at each iteration. While this function is not $P L(2)$ , as required by the theorem, the asymptotic predictions still align well with simulations. Extending our technical results to hold formally in such scenarios is an interesting direction for future work. ", "page_idx": 23}, {"type": "image", "img_path": "nv7ox1vd3q/tmp/170a304e02ecf4af1552dd792f07526621684c0ae140c9966930658b192174e0.jpg", "img_caption": ["(a) Predictions for $\\psi$ which are not uniformly bounded "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "nv7ox1vd3q/tmp/d4691a6f5f4cba14f7a35f606494c4e6de4d4d64a8435058cbc5e98633bd26e8.jpg", "img_caption": ["(b) Predictions for the squared test loss "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 3: Here, we fix $n=250,d=2000,\\sigma=0.1,\\theta_{i}^{*^{\\mathrm{i.i.d.}}}$ Bernoulli(0.01) and select $\\lambda$ to minimize the predicted asymptotic loss. Plus marks denote the median over 100 trials, and the shaded region indicates the interquartile range. Left: Predictions and simulations for weighting functions which are not uniformly bounded. Right: Predictions and simulations for the squared error $\\frac{1}{d}\\|\\boldsymbol{u}\\odot\\pmb{v}-\\pmb{\\theta}^{*}\\|_{2}^{2}$ . ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 25}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 25}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 25}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 25}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 25}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Theorems 1 and 2 directly reflect the claims from the abstract/introduction. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide detailed discussion of the strength of our technical assumptions after the statement of Theorem 1 and in Appendix D, and we further elaborate on these areas for improvement in the conclusion. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We formally state and discuss Assumptions 1 and 2, and we provide a complete proof of our results in the Appendix A of the supplemental material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide all details of hyperparameters used to run simulations in the corresponding figure caption. We also detail the exact method used to compute our asymptotic predictions in Appendix C and provide accompanying code for the simulations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide an accompanying file with code for computing the asymptotic predictions and running simulations with high-dimensional Gaussian data. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: For each simulation, hyperparameter choices (and the method for choosing the ridge parameter $\\lambda$ ) are specified either in the caption or in the accompanying discussion in the text. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We report the median and interquartile range for all simulations over 100 independent trials. These metrics are chosen due to the asymmetry of the distribution across trials (e.g., the mean minus the standard deviation might be negative in low-noise settings). Guidelines: ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Due to the small-scale of our included simulations, we do not include this information. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The research presented in this work abides by the Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work provides statistical analysis for a generic family of algorithms in an abstract setup, and we do not believe our work has immediate social impacts or an immediate path toward such impacts. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The provided code includes all necessary information for running simulations. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects or crowdsourcing. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects or crowdsourcing. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]