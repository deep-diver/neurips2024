[{"figure_path": "vjAORqq71s/figures/figures_5_1.jpg", "caption": "Figure 1: Overview over ranking supervision with a differentiable sorting / ranking algorithm. A set of input images is (element-wise) processed by a CNN, producing a scalar for each image. The scalars are sorted / ranked by the differentiable ranking algorithm, which returns the differentiable permutation matrix, which is compared to the ground truth permutation matrix.", "description": "This figure illustrates the ranking supervision task using differentiable sorting algorithms.  A Convolutional Neural Network (CNN) processes a set of four-digit MNIST images, generating a scalar value for each image. These scalars are then fed into a differentiable ranking algorithm (e.g., NeuralSort, SoftSort, or a Differentiable Sorting Network), which outputs a differentiable permutation matrix representing the predicted ranking. This predicted permutation matrix is finally compared against the ground truth permutation matrix using a cross-entropy loss function (LCE). The goal is to train the CNN such that it produces scalars that lead to a predicted ranking that closely matches the ground truth.", "section": "4.1 Ranking Supervision"}, {"figure_path": "vjAORqq71s/figures/figures_6_1.jpg", "caption": "Figure 2: 12 \u00d7 12 Warcraft shortest-path problem. An input terrain map (left), unsupervised ground truth cost embedding (center) and ground truth supervised shortest path (right).", "description": "This figure shows an example of the Warcraft shortest-path benchmark used in the paper.  The left panel displays a 12x12 terrain map (input data). The center panel shows the hidden, unsupervised ground truth cost embedding for the map, which is the underlying cost for calculating shortest paths and is not available during training. The right panel depicts the ground truth shortest path, which is the supervised target for the model to learn to predict.", "section": "4.2 Shortest-Path Supervision"}, {"figure_path": "vjAORqq71s/figures/figures_7_1.jpg", "caption": "Figure 3: Test accuracy (perfect matches) plot for \u2018SS of loss\u2019 with 10 samples on the Warcraft shortest-path benchmark. Lines show the mean and shaded areas show the 95% conf. intervals.", "description": "This figure shows the test accuracy (percentage of perfect matches) over 50 epochs for the stochastic smoothing ('SS of loss') method with 10 samples on the Warcraft shortest-path benchmark.  The lines represent the mean accuracy across multiple runs, and the shaded areas represent the 95% confidence intervals, indicating the variability of the results.  The plot compares the performance of the baseline method with the Hessian-based and Fisher-based Newton Loss variants.", "section": "4.2.2 Stochastic Smoothing"}, {"figure_path": "vjAORqq71s/figures/figures_8_1.jpg", "caption": "Figure 4: Ablation study wrt. the Tikhonov regularization strength hyperparameter \u03bb. Displayed is the element-wise ranking accuracy (individual element ranks correctly identified), averaged over 10 seeds, and additionally each seed with low opacity in the background. Left: NeuralSort. Right: SoftSort. Each for n = 5. Newton Losses, and for both the Hessian and the Fisher variant, significantly improve over the baseline for up to (or beyond) 6 orders of magnitude in variation of its hyperparameter \u03bb. Note the logarithmic horizontal axis.", "description": "This ablation study shows the effect of the Tikhonov regularization strength (\u03bb) on the element-wise ranking accuracy for NeuralSort and SoftSort (n=5).  The plots show that Newton Losses (both Hessian and Fisher variants) consistently outperform the baseline across a wide range of \u03bb values, demonstrating robustness to this hyperparameter.", "section": "4.3 Ablation Study"}, {"figure_path": "vjAORqq71s/figures/figures_17_1.jpg", "caption": "Figure 3: Test accuracy (perfect matches) plot for \u2018SS of loss\u2019 with 10 samples on the Warcraft shortest-path benchmark. Lines show the mean and shaded areas show the 95% conf. intervals.", "description": "This figure shows the test accuracy (percentage of perfect matches) over time (in seconds) for the \"SS of loss\" method with 10 samples on the Warcraft shortest-path benchmark.  Three lines are plotted: one for the baseline method and two for variations using Newton Losses (Hessian and Fisher). Shaded areas represent 95% confidence intervals, showing the variability in the results across multiple runs.", "section": "4.2.2 Stochastic Smoothing"}, {"figure_path": "vjAORqq71s/figures/figures_20_1.jpg", "caption": "Figure 6: We illustrate the gradient of the NeuralSort and logistic DSN losses in dependence of one of the five input dimensions for the n = 5 case. In the illustrated example, one can observe that both algorithms experience exploding gradients when the inputs are too far away from each other (which is also controllable via steepness \u03b2 / temperature \u03c4), see Figures (c) / (d). Further, we can observe that the gradients themselves can be quite chaotic, making the optimization of the loss rather challenging. In Figures (e) / (f), we illustrate how using the empirical Fisher Newton Loss recovers from the exploding gradients experienced in (c) / (d). The input examples are a simplification of actual inputs, as here, x0, x2, x3, x4 are already in their correct order, and having multiple disagreements makes the loss more chaotic.", "description": "The figure shows gradient visualizations for NeuralSort and Logistic DSN, comparing the original gradients and the gradients after applying Newton Losses (using empirical Fisher).  It demonstrates how Newton Losses mitigate issues with exploding gradients and chaotic gradient behavior, particularly when inputs are far apart.", "section": "H Gradient Visualizations"}, {"figure_path": "vjAORqq71s/figures/figures_20_2.jpg", "caption": "Figure 6: We illustrate the gradient of the NeuralSort and logistic DSN losses in dependence of one of the five input dimensions for the n = 5 case. In the illustrated example, one can observe that both algorithms experience exploding gradients when the inputs are too far away from each other (which is also controllable via steepness \u03b2 / temperature \u03c4), see Figures (c) / (d). Further, we can observe that the gradients themselves can be quite chaotic, making the optimization of the loss rather challenging. In Figures (e) / (f), we illustrate how using the empirical Fisher Newton Loss recovers from the exploding gradients experienced in (c) / (d). The input examples are a simplification of actual inputs, as here, x0, x2, x3, x4 are already in their correct order, and having multiple disagreements makes the loss more chaotic.", "description": "This figure visualizes gradients of NeuralSort and Logistic DSN loss functions, highlighting the issue of exploding gradients when inputs are far apart.  It then shows how the empirical Fisher Newton Loss mitigates this problem, leading to smoother, more manageable gradients.", "section": "H Gradient Visualizations"}, {"figure_path": "vjAORqq71s/figures/figures_20_3.jpg", "caption": "Figure 6: We illustrate the gradient of the NeuralSort and logistic DSN losses in dependence of one of the five input dimensions for the n = 5 case. In the illustrated example, one can observe that both algorithms experience exploding gradients when the inputs are too far away from each other (which is also controllable via steepness \u03b2 / temperature \u03c4), see Figures (c) / (d). Further, we can observe that the gradients themselves can be quite chaotic, making the optimization of the loss rather challenging. In Figures (e) / (f), we illustrate how using the empirical Fisher Newton Loss recovers from the exploding gradients experienced in (c) / (d). The input examples are a simplification of actual inputs, as here, x0, x2, x3, x4 are already in their correct order, and having multiple disagreements makes the loss more chaotic.", "description": "Figure 6 shows gradient visualizations for NeuralSort and Logistic DSN, illustrating how Newton Loss addresses exploding gradients and chaotic behavior, especially when inputs are far apart.", "section": "H Gradient Visualizations"}, {"figure_path": "vjAORqq71s/figures/figures_20_4.jpg", "caption": "Figure 6: We illustrate the gradient of the NeuralSort and logistic DSN losses in dependence of one of the five input dimensions for the n = 5 case. In the illustrated example, one can observe that both algorithms experience exploding gradients when the inputs are too far away from each other (which is also controllable via steepness \u03b2 / temperature \u03c4), see Figures (c) / (d). Further, we can observe that the gradients themselves can be quite chaotic, making the optimization of the loss rather challenging. In Figures (e) / (f), we illustrate how using the empirical Fisher Newton Loss recovers from the exploding gradients experienced in (c) / (d). The input examples are a simplification of actual inputs, as here, x\u2080, x\u2082, x\u2083, x\u2084 are already in their correct order, and having multiple disagreements makes the loss more chaotic.", "description": "This figure visualizes the gradients of NeuralSort and Logistic DSN losses, highlighting the issue of exploding gradients when inputs are distant.  It then shows how the empirical Fisher Newton Loss mitigates this problem.", "section": "H Gradient Visualizations"}, {"figure_path": "vjAORqq71s/figures/figures_20_5.jpg", "caption": "Figure 6: We illustrate the gradient of the NeuralSort and logistic DSN losses in dependence of one of the five input dimensions for the n = 5 case. In the illustrated example, one can observe that both algorithms experience exploding gradients when the inputs are too far away from each other (which is also controllable via steepness \u03b2 / temperature \u03c4), see Figures (c) / (d). Further, we can observe that the gradients themselves can be quite chaotic, making the optimization of the loss rather challenging. In Figures (e) / (f), we illustrate how using the empirical Fisher Newton Loss recovers from the exploding gradients experienced in (c) / (d). The input examples are a simplification of actual inputs, as here, x0, x2, x3, x4 are already in their correct order, and having multiple disagreements makes the loss more chaotic.", "description": "The figure shows the gradients of NeuralSort and Logistic DSN losses, demonstrating the issue of exploding gradients when inputs are far apart.  It also compares these gradients to those produced using the empirical Fisher Newton Loss, illustrating its effectiveness in mitigating this problem.", "section": "H Gradient Visualizations"}, {"figure_path": "vjAORqq71s/figures/figures_20_6.jpg", "caption": "Figure 6: We illustrate the gradient of the NeuralSort and logistic DSN losses in dependence of one of the five input dimensions for the n = 5 case. In the illustrated example, one can observe that both algorithms experience exploding gradients when the inputs are too far away from each other (which is also controllable via steepness \u03b2 / temperature \u03c4), see Figures (c) / (d). Further, we can observe that the gradients themselves can be quite chaotic, making the optimization of the loss rather challenging. In Figures (e) / (f), we illustrate how using the empirical Fisher Newton Loss recovers from the exploding gradients experienced in (c) / (d). The input examples are a simplification of actual inputs, as here, x0, x2, x3, x4 are already in their correct order, and having multiple disagreements makes the loss more chaotic.", "description": "This figure shows gradient plots for NeuralSort and Logistic DSN, demonstrating the issues of exploding gradients with these algorithms.  The plots visualize how the gradients behave in relation to input values (x1).  The addition of Newton Losses is shown to mitigate the exploding gradients, demonstrating the improved stability and performance.", "section": "H Gradient Visualizations"}]