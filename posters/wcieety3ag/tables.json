[{"figure_path": "WcIeEtY3AG/tables/tables_6_1.jpg", "caption": "Table 1: Number of parameters and theoretical synaptic operations. R, R, \u0158 and R denote the average spike firing rates (the proportion of non-zero elements in the spike matrix) in various spike matrices. T, N, and D are the time step, sequence length, and channel dimension of the input features, respectively. d is the channel dimension of Am and m is the number of experts. The details are provided in the Appendix. B.", "description": "This table compares the number of parameters and theoretical synaptic operations for different components of the Spiking Transformer model, namely SSA (Spiking Self-Attention), EMSA (Experts Mixture Spiking Attention), MLP (Multi-layer Perceptron), and EMSP (Experts Mixture Spiking Perceptron). It also includes the average spike firing rates for various matrices involved in these components.  The table aims to demonstrate the computational efficiency of SEMM (Spiking Experts Mixture Mechanism) by comparing it to the baselines.", "section": "3.5 Characteristics of SEMM"}, {"figure_path": "WcIeEtY3AG/tables/tables_6_2.jpg", "caption": "Table 2: The average spike rate of EMSA and EMSP router in 8 blocks testing on the ImageNet.", "description": "This table shows the average spiking rate (ASR) of the routers for EMSA and EMSP in each of the eight blocks of the Spiking Transformer model tested on the ImageNet dataset.  The ASR provides insight into the dynamic sparsity of the routers and their role in conditional computation.  Lower ASR values indicate greater sparsity.", "section": "4.1 Sparse Conditional Computation Analysis"}, {"figure_path": "WcIeEtY3AG/tables/tables_8_1.jpg", "caption": "Table 3: Results on ImageNet-1k. Model-L-D represents a model with L encoder blocks and D channels.", "description": "This table presents the results of different Spiking Transformer models on the ImageNet-1k dataset.  It compares various architectures (including SEW ResNet, MS-ResNet, Spikformer, Spike-driven Transformer, and Spikingformer) with and without the proposed SEMM method. The table shows the number of parameters (in millions), the number of time steps, and the Top-1 accuracy achieved by each model.  This allows for a comparison of model complexity against performance.", "section": "4 Experiments"}, {"figure_path": "WcIeEtY3AG/tables/tables_8_2.jpg", "caption": "Table 4: Results on CIFAR10-DVS, DVS128 Gesture, and CIFAR.", "description": "This table presents the experimental results of different Spiking Neural Network (SNN) models on three datasets: CIFAR10-DVS, DVS128 Gesture, and CIFAR.  It shows the accuracy achieved by each model at different time steps (T). The models include several baseline SNN architectures (tdBN, PLIF, DIET-SNN, Dspike, DSR) and three spiking transformer variants (Spikformer, Spike-Driven Transformer, and Spikingformer) both with and without the proposed Spiking Experts Mixture Mechanism (SEMM). The table highlights the improvement in accuracy obtained by integrating SEMM into the different spiking transformer baselines.", "section": "4.2 Results on various Datasets"}, {"figure_path": "WcIeEtY3AG/tables/tables_14_1.jpg", "caption": "Table 5: Ablation study results on the time step.", "description": "This table shows the ablation study results on the time step for three different models: Spikformer + SEMM, Spike Driven Transformer + SEMM, and Spikingformer + SEMM.  It demonstrates the Top-1 accuracy achieved on the CIFAR10/100 dataset at different time steps (1, 2, 4, and 6). This helps to analyze the impact of the time step parameter on the performance of the proposed models.", "section": "4.3 Ablation Study and Hyperparameter Sensitivity"}, {"figure_path": "WcIeEtY3AG/tables/tables_15_1.jpg", "caption": "Table 6: Ablation on Deep Wise Convolution layer.", "description": "This table presents the ablation study results on the Deep Wise Convolution (DWC) layer within the Experts Mixture Spiking Perceptron (EMSP) module. It compares the performance of EMSP with and without the DWC layer across different Spiking Transformer baselines (Spikformer, Spike-driven Transformer, and Spikingformer) on CIFAR100 and CIFAR10-DVS datasets. The results show the impact of the DWC layer on the overall accuracy of the models.", "section": "4.2 Results on various Datasets"}]