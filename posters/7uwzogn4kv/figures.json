[{"figure_path": "7uWzoGn4kv/figures/figures_1_1.jpg", "caption": "Figure 1: Problem Overview. (a) Current VLMs [5] rely on instance-level contrastive learning between video & narration. HelpingHands [4] implicitly induces object occurrence information into video features at final layer of video encoder. (b) Our proposed (HENASY) aims to assemble dynamic entities from video patches via local entity encoder, while entity-aware decoder captures interactions between entities and global context to form comprehensive video. HENASY is trained with suite of multi-grained contrastive alignments to enforce visual representations entity-level upto video-level. (c) By such compositional approach, HENASY is the first VLM that shows strong interpretability via visual grounding with both appearance/motion query types.", "description": "This figure provides a comparison of current video-language models (VLMs) with the proposed HENASY model.  Panel (a) shows that current VLMs utilize instance-level contrastive learning, which limits their understanding of complex interactions within videos.  In contrast, Panel (b) illustrates HENASY's compositional approach, assembling dynamic scene entities and modeling their relationships for improved video representation.  Finally, Panel (c) highlights HENASY's interpretability through visual grounding, enabling queries based on appearance or motion.", "section": "1 Introduction"}, {"figure_path": "7uWzoGn4kv/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of the HENASY framework for video-language modeling. Left: HENASY features a dual-encoder architecture with a compositional video understanding approach. The local entity encoder assembles dynamic scene entities from video patches, while the global encoder provides contextual features. These are combined in the entity-aware decoder to create an interpretable video representation. Right: HENASY is supported by a suite of multi-grained contrastive learning to enforce both entity-level and video-level representations.", "description": "This figure illustrates the architecture of the HENASY model, which uses a dual-encoder architecture with a compositional approach to video understanding. The left side shows how the local entity encoder assembles dynamic scene entities from video patches, and the global encoder provides contextual information. These are combined in the entity-aware decoder to create an interpretable video representation. The right side highlights the model's use of multi-grained contrastive learning to ensure both entity-level and video-level representations are well-learned.", "section": "4 HENASY"}, {"figure_path": "7uWzoGn4kv/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of entity-aware decoder.", "description": "The entity-aware decoder takes entity-level features from the local entity encoder and video patch embeddings as input. It refines the interactions between entities and video patches through a series of cross-attention and self-attention blocks, followed by an MLP layer to produce the final entity-aware video embedding. The decoder aims to enrich the entity-centric video representation.", "section": "4.3 Entity-Aware Decoder"}, {"figure_path": "7uWzoGn4kv/figures/figures_8_1.jpg", "caption": "Figure 4: Vision-Language Grounding. Qualitative comparisons with HelpingHands [4] on EgoCLIP [3]. Left: comparison with a noun query obtained from narration and the pseudo-groundtruth boxes detected by [33] for reference. Right: verb phrase in the narration is used for comparison, as verb phrase cannot be captured by [33], we do not include pseudo boxes.", "description": "This figure presents a qualitative comparison of the proposed HENASY model and the HelpingHands model in terms of their visual grounding capabilities.  The comparison focuses on two examples, each showing the model's ability to generate visual saliency maps that highlight the relevant entities mentioned in a textual narration. The left example uses a noun phrase as the query, showing both methods' ability to localize the object in the scene, but highlighting HENASY's ability to better match the ground truth mask. The right example uses a verb phrase as the query, showcasing HENASY's advantage in handling actions and dynamic events that HelpingHands fails to represent correctly. Overall, the figure demonstrates HENASY's improved visual grounding quality and its capacity for nuanced interpretation of video content.", "section": "5.4 Ablation Studies"}]