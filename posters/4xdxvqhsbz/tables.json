[{"figure_path": "4xDxVQHsbZ/tables/tables_2_1.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents the results of evaluating the performance of LLMs using both the standard Attention mechanism and the proposed NOMAD-Attention method.  It shows the perplexity scores on the WikiText-2 and C4 datasets, as well as accuracy scores across six different benchmark tasks: SciQ, Arc-Easy, Arc-Challenge, HellaSwag, Winogrande, and PIQA. The results are broken down for different LLM sizes (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, LLaMA-2-13B) and varying configurations of NOMAD-Attention (dsub = 1, 2, 4), allowing for a comparison of performance trade-offs between speed and accuracy.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_5_1.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents the results of evaluating the performance of LLMs using standard attention mechanisms and the proposed NOMAD-Attention method.  It shows perplexity scores on the WikiText-2 and C4 datasets, along with accuracy scores on six downstream tasks (SciQ, Arc-E, Arc-C, HellaSwag, Winogrande, and PIQA).  The results are broken down for different LLM sizes (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, and LLaMA-2-13B) and different NOMAD-Attention configurations (dsub = 1, 2, and 4).  This allows for a comparison of the impact of NOMAD-Attention on both the speed and accuracy of LLM inference, showing whether the speed gains come at the cost of reduced accuracy. ", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_6_1.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents a quantitative comparison of the performance of Language Models (LLMs) using standard Attention mechanisms against those using the proposed NOMAD-Attention method.  The evaluation metrics include perplexity scores on the WikiText-2 and C4 datasets, along with accuracy scores across six benchmark tasks (SciQ, Arc-Easy, Arc-Challenge, HellaSwag, Winogrande, and PIQA).  Results are shown for different LLM sizes (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, and LLaMA-2-13B) and variations of the NOMAD-Attention algorithm (dsub=1, 2, and 4). The table allows readers to assess the impact of NOMAD-Attention on both the quantitative performance and the quality of the LLMs.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_8_1.jpg", "caption": "Table 2: Ablative experiments on the effects of FIM-informed centroid learning on the perplexity of LLaMA-7b on WikiText-2.", "description": "This table presents the results of ablation studies on the impact of FIM-informed centroid learning on the perplexity of the LLaMA-7b model when evaluated on the WikiText-2 dataset.  It compares the perplexity scores achieved using NoMAD-Attention with different sub-quantizer dimensions (dsub = 1, 2, 4) under both uninformed and FIM-informed centroid learning methods. The table also includes the perplexity of the original Attention model for comparison, highlighting the effectiveness of FIM-informed learning in preserving model quality while using NoMAD-Attention.", "section": "4.2 Ablation Study"}, {"figure_path": "4xDxVQHsbZ/tables/tables_8_2.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents a comparison of the performance of LLMs using standard attention mechanisms and the proposed NOMAD-Attention method.  The metrics used are perplexity scores on the WikiText-2 and C4 datasets, which measure the model's ability to predict the next word in a sequence, and accuracy scores on six different downstream tasks (SciQ, Arc-E, Arc-C, Hellaswag, WinoGrande, PIQA). Results are shown for various LLM sizes (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, LLaMA-2-13B) and different configurations of NOMAD-Attention, using different sub-quantizer dimensions (dsub = 1, 2, 4).  This allows for an assessment of the trade-off between model accuracy and the efficiency gains achieved through NOMAD-Attention.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_14_1.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents a comparison of the performance of LLMs using standard attention mechanisms versus the proposed NOMAD-Attention, across multiple benchmark datasets.  It shows perplexity scores (a measure of how well the model predicts text) on the WikiText-2 and C4 language modeling datasets and accuracy scores on six downstream tasks (SciQ, Arc-Easy, Arc-Challenge, HellaSwag, Winogrande, and PIQA). The results are broken down by LLM size (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, LLaMA-2-13B) and the number of sub-quantizers (dsub) used in NOMAD-Attention (1, 2, and 4). This allows for an assessment of the impact of NOMAD-Attention on both model quality and efficiency.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_15_1.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents a comparison of the performance of LLMs using standard attention mechanisms versus the proposed NOMAD-Attention method.  It shows perplexity scores (a measure of how well a model predicts text) on two benchmark datasets (WikiText-2 and C4), and accuracy scores on six other benchmark tasks.  The results are broken down for different model sizes (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, LLaMA-2-13B) and different configurations of NOMAD-Attention (varying the value of 'dsub').  The purpose is to demonstrate the effectiveness of NOMAD-Attention in maintaining model accuracy while potentially improving efficiency.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_16_1.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents the results of experiments comparing the performance of LLMs using standard Attention mechanisms against those using the proposed NOMAD-Attention.  It shows perplexity scores on the WikiText-2 and C4 datasets, and accuracy scores across six different benchmark tasks (SciQ, Arc-E, Arc-C, HellaSwag, Winogrande, and PIQA).  Results are broken down for different LLM sizes (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, LLaMA-2-13B) and variations of NOMAD-Attention, parameterized by `dsub` (which represents the number of sub-quantizers used in the product quantization). This allows for assessing the impact of the proposed method on both model accuracy and performance across various model sizes and configurations.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_16_2.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents the results of the experiments conducted to evaluate the performance of LLMs with both standard Attention and the proposed NOMAD-Attention.  It shows the perplexity scores (a measure of how well the model predicts the next word in a sequence) on the WikiText-2 and C4 datasets, as well as accuracy scores across six different downstream tasks (SciQ, Arc-E, Arc-C, Hellaswag, WinoGrande, and PIQA).  The results are shown for four different LLMs (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, and LLaMA-2-13B), and for each LLM, results are provided for NOMAD-Attention with varying numbers of sub-quantizers (dsub = 1, 2, and 4). Comparing the results across the different conditions allows for an assessment of the impact of NOMAD-Attention on model performance.", "section": "4 Results"}, {"figure_path": "4xDxVQHsbZ/tables/tables_17_1.jpg", "caption": "Table 1: Perplexity on WikiText-2 and C4 and accuracy on 6 benchmarks of LLMs with Attention and NOMAD-Attention.", "description": "This table presents a comparison of the performance of LLMs using standard Attention mechanisms and the proposed NOMAD-Attention.  It shows perplexity scores (a measure of how well the model predicts the next word in a sequence) on two datasets, WikiText-2 and C4.  Additionally, it provides accuracy scores on six downstream tasks (SciQ, Arc-Easy, Arc-Challenge, Hellaswag, Winogrande, and PIQA). The results are shown for different LLM sizes (LLaMA-7B, LLaMA-13B, LLaMA-2-7B, LLaMA-2-13B) and different configurations of NOMAD-Attention, denoted by 'dsub', representing the number of sub-quantizers used in the product quantization technique. The table allows for an assessment of the impact of NOMAD-Attention on both model quality (perplexity and accuracy) and performance.", "section": "4 Results"}]