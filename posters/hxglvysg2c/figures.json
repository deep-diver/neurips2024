[{"figure_path": "hXgLvYsG2c/figures/figures_1_1.jpg", "caption": "Figure 1: Training time and total memory cost of COMERA, GaLore [41] and LTE [20] on a six-encoder transformer with varying batch sizes. The experiment is done on Nvidia RTX 3090 GPU.", "description": "This figure compares the training time per epoch and peak memory consumption of three different training methods: COMERA, GaLore, and LTE.  The comparison is made across various batch sizes (1, 32, 64, 128) using a six-encoder transformer model and an Nvidia RTX 3090 GPU.  The results show that COMERA outperforms the other two methods in both training time and memory efficiency.", "section": "5 Training Results"}, {"figure_path": "hXgLvYsG2c/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Tensors. (b) Tensor contractions.", "description": "This figure visualizes tensors and tensor contractions. Part (a) shows the tensor network representations for tensors of orders 1, 2, 3, and 4.  Part (b) illustrates several common tensor contractions: matrix-vector multiplication, matrix-matrix multiplication, tensor-tensor contraction, and multi-tensor contraction.  These operations are fundamental to tensor networks, and visualizing them helps to understand the computational aspects discussed in the paper.", "section": "Background"}, {"figure_path": "hXgLvYsG2c/figures/figures_3_1.jpg", "caption": "Figure 3: Tensor networks for (a) tensor-train and (b) tensor-train-matrix decompositions.", "description": "This figure illustrates the tensor network structures for tensor-train (TT) and tensor-train matrix (TTM) decompositions.  The TT decomposition represents a tensor as a sequence of smaller, interconnected cores (G1, G2,..., Gd) connected in a chain-like fashion, where the dimensions of each core are determined by the ranks (r0, r1,..., rd) and the original tensor's dimensions (n1, n2,..., nd). The TTM decomposition extends this concept, handling unbalanced dimensions more effectively. It represents the tensor as a similar chain, but each core (F1, F2,..., Fd) incorporates both the dimensions from the TT decomposition (ni) and additional dimensions (mi).  The figure visually depicts these connections between the cores and how they combine to form the original tensor.", "section": "3 The CoMERA Training Framework"}, {"figure_path": "hXgLvYsG2c/figures/figures_4_1.jpg", "caption": "Figure 4: Optimized TTM embedding lookup.", "description": "This figure illustrates the optimized process of looking up entries in a tensor-train matrix (TTM) embedding table.  It demonstrates how the process efficiently selects submatrices from the compressed TTM representation to avoid redundant computations. This optimization significantly reduces both memory consumption and computational cost, as detailed in Section 4.1 of the paper. The figure shows the steps involved: reshaping the embedding table into a tensor, selecting indices, performing tensor contractions, and finally, employing batched GEMM (generalized matrix multiplication) to obtain the desired results. ", "section": "Performance Optimization of TTM Embedding Tables"}, {"figure_path": "hXgLvYsG2c/figures/figures_5_1.jpg", "caption": "Figure 5: Performance of optimized TTM embedding table lookup. The labels uncompressed, proposed approach, optimized order, unique indices, without optimization represent standard embedding with sparse gradients, the new method in 4.1, the method that only uses the unique order, the method that only uses the unique indices, and the method without optimization, respectively.", "description": "This figure compares the performance of five different methods for optimizing TTM embedding table lookups. The \"uncompressed\" method represents the standard approach without any optimization. The \"proposed approach\" is the new method introduced in section 4.1 of the paper, which aims to accelerate the lookup process by eliminating redundant computations. The \"optimized order\", \"unique indices\", and \"without optimization\" methods represent variations of the proposed approach with different levels of optimization applied.  The graph displays the speedup (left) and memory reduction (right) achieved by each method for different batch sizes (10000, 20000, 40000).", "section": "Performance Optimization of Tensor-Compressed Training"}, {"figure_path": "hXgLvYsG2c/figures/figures_7_1.jpg", "caption": "Figure 6: Behavior of early-stage CoMERA training on the MNLI dataset.", "description": "This figure shows the validation accuracy during the early-stage training of CoMERA on the MNLI dataset.  It illustrates the training progress and how the model's performance on a validation set changes over epochs. The graph indicates that CoMERA achieves comparable validation accuracy to the uncompressed training approach while significantly reducing model size and training time.", "section": "5.1 A Medium-Size Transformer with Six Encoders"}, {"figure_path": "hXgLvYsG2c/figures/figures_7_2.jpg", "caption": "Figure 7: Training time per epoch for the six-encoder transformer model on the MNLI dataset.", "description": "This figure compares the training time per epoch for different methods on the MNLI dataset using a six-encoder transformer model.  The methods compared are CoMERA with and without CUDA Graph, and uncompressed training with and without CUDA Graph.  The x-axis represents the batch size used (32, 64, and 128), and the y-axis shows the training time in minutes. The figure demonstrates the significant speedup achieved by CoMERA, especially when utilizing CUDA Graph for optimization.", "section": "5.1 A Medium-Size Transformer with Six Encoders"}, {"figure_path": "hXgLvYsG2c/figures/figures_8_1.jpg", "caption": "Figure 9: Performance of optimized COMERA on training DLRM.", "description": "This figure compares the training time per epoch and memory cost of three different methods: COMERA with optimization, COMERA without optimization, and uncompressed training.  The comparison is shown for three different batch sizes (10000, 20000, 40000).  The results demonstrate that COMERA, especially with optimizations, significantly reduces both training time and memory consumption compared to uncompressed training, especially at larger batch sizes.  The lack of a value for 40000 batches in the uncompressed scenario suggests that training at that scale may have been infeasible without optimization.", "section": "5.2 A DLRM Model with 4-GB Model Size"}, {"figure_path": "hXgLvYsG2c/figures/figures_8_2.jpg", "caption": "Figure 8: NCE loss curve of DLRM on the validation dataset.", "description": "This figure shows the training loss curve for both the uncompressed and COMERA methods on the DLRM model during the validation phase.  The x-axis represents the training iteration, and the y-axis represents the normalized cross-entropy (NCE) loss. The plot visually demonstrates the convergence of both methods, with COMERA showing a slightly higher loss at the end of training.", "section": "5.2 A DLRM Model with 4-GB Model Size"}, {"figure_path": "hXgLvYsG2c/figures/figures_9_1.jpg", "caption": "Figure 10: Pre-training loss curves of CodeBERT and CoMERA.", "description": "This figure shows the pre-training loss curves for both the original CodeBERT model and the CoMERA-compressed version.  The x-axis represents the training steps, and the y-axis represents the training loss. The figure is divided into two phases.  In Phase 1, both models show a similar downward trend in loss.  In Phase 2, a further compression step is introduced in CoMERA, leading to a slight increase in loss initially before it continues its downward trend.  The main observation is that CoMERA achieves comparable performance to CodeBERT despite significant model compression.", "section": "5.4 Preliminary LLM Pre-Training Results: Case Study on CodeBERT"}, {"figure_path": "hXgLvYsG2c/figures/figures_15_1.jpg", "caption": "Figure 11: Tensor diagrams for contraction paths of TT forward- and back- propagation.", "description": "This figure shows the tensor diagrams for the contraction paths used in the forward and backward propagation of TT-vector multiplications.  The diagrams illustrate the computational flow for both forward and backward passes, broken down into smaller, manageable steps. This visualization aids in understanding how the algorithm optimizes the order of tensor contractions to minimize computational cost and memory usage. The diagrams show different paths for computing gradients of the TT cores in the forward and backward passes.", "section": "4 Performance Optimization of CoMERA"}, {"figure_path": "hXgLvYsG2c/figures/figures_16_1.jpg", "caption": "Figure 12: Per epoch training time of CoMERA on MNLI for various compression ratios.", "description": "This figure shows the training time per epoch for the six-encoder transformer model on the MNLI dataset with different compression ratios. It demonstrates that CoMERA achieves significant speed-up in training time compared with standard training, especially for high compression ratios. The speedup is more obvious for larger compression ratios, approaching uncompressed training time as the compression ratio approaches 1.", "section": "5 Training Results"}, {"figure_path": "hXgLvYsG2c/figures/figures_17_1.jpg", "caption": "Figure 1: Training time and total memory cost of COMERA, GaLore [41] and LTE [20] on a six-encoder transformer with varying batch sizes. The experiment is done on Nvidia RTX 3090 GPU.", "description": "This figure compares the training time per epoch and peak memory consumption of three different training methods: COMERA, GaLore, and LTE.  The comparison is done on a six-encoder transformer model using different batch sizes (1, 32, 64, 128) and running on an Nvidia RTX 3090 GPU.  The results show that COMERA significantly outperforms GaLore and LTE in terms of both training speed and memory efficiency.", "section": "5 Training Results"}, {"figure_path": "hXgLvYsG2c/figures/figures_17_2.jpg", "caption": "Figure 14: Convergence of mixed-precision COMERA on the six-encoder transformer.", "description": "This figure shows the validation accuracy over training epochs for both mixed-precision and FP32 versions of CoMERA on a six-encoder transformer model.  It demonstrates the comparable convergence speed and accuracy of the mixed-precision approach compared to the standard FP32 training.", "section": "5.1 A Medium-Size Transformer with Six Encoders"}]