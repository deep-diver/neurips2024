[{"figure_path": "FNzpVTpNbN/figures/figures_1_1.jpg", "caption": "Figure 1: Pipeline of the generation process of Deepfake (a) and our proposed DiffusionFake (b).", "description": "This figure illustrates the process of generating a deepfake image and the proposed DiffusionFake framework. (a) shows the Deepfake generation pipeline, which includes two key steps: 1) a feature extractor module that extracts features from both the source and target images, and 2) a feature fusion module that seamlessly blends these features to synthesize a new Deepfake image. (b) shows the DiffusionFake framework, which injects the features extracted by a detection model into a pre-trained Stable Diffusion model to guide the reconstruction of the source and target images. This guided reconstruction process helps the detection model learn rich and disentangled representations that are more resilient to unseen forgeries.", "section": "1 Introduction"}, {"figure_path": "FNzpVTpNbN/figures/figures_3_1.jpg", "caption": "Figure 2: The details of the DiffusionFake method. The blue arrow represents the target branch, the red arrow represents the source branch, the  represents the parameter frozen and does not participate in training, and the  represents the trainable module.", "description": "This figure shows a detailed architecture of the DiffusionFake framework.  It illustrates the flow of features extracted from an input image through various modules.  The features are split into target-related and source-related components using filter modules, and their weights are adjusted by weight modules. These features are then passed through the Guide Module, which injects them into a pre-trained Stable Diffusion model to reconstruct source and target images. This process guides the encoder in learning disentangled representations, improving generalization for forgery detection. The diagram visually explains the interplay of different components and feature transformations, making the methodology more understandable.", "section": "3 Methodology"}, {"figure_path": "FNzpVTpNbN/figures/figures_8_1.jpg", "caption": "Figure 3: Reconstruction results of DiffusionFake for training (A) and unseen (B) samples. For unseen samples, the model is provided with three sets of initial Gaussian noise, differing only in the injected guide information. The numbers below represent the Euclidean distance between the corresponding source and target features.", "description": "This figure shows the reconstruction results of the proposed DiffusionFake method for both training and unseen samples. The top row shows the ground truth target image, followed by the reconstruction results using the RECCE method and the proposed DiffusionFake method. The bottom row shows the same for the source image. The unseen samples show three reconstructions with different random noise but with the same injected features, showing the consistency of the method. Euclidean distances between corresponding source and target features are shown below.", "section": "Analysis and Visualizations"}, {"figure_path": "FNzpVTpNbN/figures/figures_8_2.jpg", "caption": "Figure 2: The details of the DiffusionFake method. The blue arrow represents the target branch, the red arrow represents the source branch, the  represents the parameter frozen and does not participate in training, and the  represents the trainable module.", "description": "This figure shows a detailed diagram of the DiffusionFake framework. It illustrates the process of injecting features extracted by the encoder (from a forgery detection model) into a pre-trained Stable Diffusion model. The process involves two main stages:\n\n1.  **Feature Transformation**: The input features are passed through two filter networks (Fs and Ft) to generate source-related (fs) and target-related (ft) representations. These features are weighted by two weight modules (Ws and Wt) that dynamically adjust the influence of source and target information. \n2.  **Guide Module**: The weighted source and target features are injected into the Stable Diffusion model. A trainable copy of the encoder block (UE) is created and the features are combined with the locked decoder features using zero convolution layers to guide the reconstruction of source and target images. This guides the encoder to learn rich and discriminative features that benefit generalization of the detection model. ", "section": "3 Methodology"}, {"figure_path": "FNzpVTpNbN/figures/figures_8_3.jpg", "caption": "Figure 5: Feature distribution of En-b4 model and the En-b4 model trained with our DiffusionFace on two unseen datasets Celeb-DF and Wild-Deepfake via t-SNE. The red represents the real samples while the blue represents the fake ones.", "description": "This figure visualizes the feature distributions of two models (original EfficientNet-B4 and EfficientNet-B4 trained with DiffusionFake) on two unseen datasets (Celeb-DF and Wild-Deepfake) using t-SNE.  It demonstrates the impact of DiffusionFake on enhancing the separability of real and fake samples in the feature space.  The improved separation in the DiffusionFake-trained model indicates better generalization.", "section": "4.4 Analysis and Visualizations"}, {"figure_path": "FNzpVTpNbN/figures/figures_14_1.jpg", "caption": "Figure 6: CAM maps of the baseline model (EN-b4) and En-b4 trained with DiffusionFake method on three unseen datasets: Celeb-DF, WDF (Wild-Deepfake), and DiffSwap.", "description": "This figure visualizes Class Activation Maps (CAMs) for both the baseline model (EfficientNet-B4) and the model enhanced with DiffusionFake.  It demonstrates the differences in attention focusing on three different datasets: Celeb-DF, Wild Deepfake (WDF), and DiffSwap. The CAMs highlight the regions of the input image that the model considers most important for classification. By comparing the CAMs of the baseline model with those of the DiffusionFake-enhanced model, it is possible to observe whether DiffusionFake improves the focus and precision of the model's attention when classifying fake faces from different sources.", "section": "4.4 Analysis and Visualizations"}, {"figure_path": "FNzpVTpNbN/figures/figures_14_2.jpg", "caption": "Figure 7: Visualization of weights for different attack types. The blue lines connect the target weights, while the red lines connect the source weights.", "description": "This figure shows the weights assigned to source and target features by the Weight Module for four different deepfake generation methods: Deepfakes, Face2Face, FaceSwap, and NeuralTextures.  The blue arrows represent the weights for target features, and the red arrows represent the weights for source features.  The numbers on the arrows indicate the similarity score between the input image and its corresponding source or target image, which is used to calculate the weights.  Higher values indicate a stronger similarity, thus more weight is given to either the source or the target feature. The figure visually demonstrates that different deepfake methods have different weight distributions.", "section": "Analysis and Visualizations"}, {"figure_path": "FNzpVTpNbN/figures/figures_15_1.jpg", "caption": "Figure 3: Reconstruction results of DiffusionFake for training (A) and unseen (B) samples. For unseen samples, the model is provided with three sets of initial Gaussian noise, differing only in the injected guide information. The numbers below represent the Euclidean distance between the corresponding source and target features.", "description": "This figure shows the reconstruction results of the DiffusionFake model for both training and unseen samples.  The top row (A) displays the results on training data. The bottom row (B) shows the results on unseen data where the model was given three different initial Gaussian noise sets, only differing in the injected guide information.  The numbers under each image represent the Euclidean distance between the reconstructed source and target features, giving a visual representation of how well the model reconstructs the features.", "section": "4.4 Analysis and Visualizations"}, {"figure_path": "FNzpVTpNbN/figures/figures_15_2.jpg", "caption": "Figure 9: Visualization of two typical misprediction samples. Represents Profile view Images and Low-quality Images respectively.", "description": "This figure shows two examples of images that the model in the paper misclassified. The first row shows profile view images which are difficult to reconstruct due to information loss, and the second row shows low-quality images which are blurry and difficult to extract useful features from.  These examples illustrate the limitations of the model in handling certain types of images.", "section": "A.9 Limitations and Broader Impacts"}]