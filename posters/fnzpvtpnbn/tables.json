[{"figure_path": "FNzpVTpNbN/tables/tables_6_1.jpg", "caption": "Table 1: Frame-level cross-database evaluation from FF++(HQ) to Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap in terms of AUC and EER. * represents the results reproduced using open-source code or model.", "description": "This table presents the frame-level cross-database evaluation results comparing the performance of different deepfake detection methods.  The evaluation is performed across five different datasets: Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap, with each method's performance measured using the Area Under the Curve (AUC) and Equal Error Rate (EER) metrics. The results show the performance when models trained on the FaceForensics++ (FF++) high-quality dataset are tested on the other unseen datasets, evaluating the generalization capabilities of the models.", "section": "4.2 Experimental Results"}, {"figure_path": "FNzpVTpNbN/tables/tables_7_1.jpg", "caption": "Table 2: Abalation study of different components of DiffusionFake.", "description": "This table presents the ablation study results for different components of the DiffusionFake model.  It shows the impact of removing the pre-trained Stable Diffusion model, the Feature Filter Module, and the Weight Module on the model's performance. The results are evaluated using AUC and EER metrics on two datasets: Celeb-DF and DFDC-P. Each row represents a different configuration of components, with '\u2713' indicating that the component was included, and 'X' indicating that it was excluded. The table demonstrates the importance of each component for the model's overall performance.", "section": "4.3 Ablation Study"}, {"figure_path": "FNzpVTpNbN/tables/tables_7_2.jpg", "caption": "Table 3: Abalation study of backbones.", "description": "This table presents the ablation study results on different backbones (ResNet, EfficientNet-B0, ViT-S) with and without the proposed DiffusionFake method.  It shows the AUC and EER scores on the Celeb-DF and Wild Deepfake datasets for each backbone to demonstrate the effectiveness and generalizability of DiffusionFake across different architectures.", "section": "4.3 Ablation Study"}, {"figure_path": "FNzpVTpNbN/tables/tables_12_1.jpg", "caption": "Table 1: Frame-level cross-database evaluation from FF++(HQ) to Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap in terms of AUC and EER. * represents the results reproduced using open-source code or model.", "description": "This table presents the frame-level cross-database evaluation results comparing different face forgery detection methods.  It evaluates performance on several unseen datasets (Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap) after training on the FF++(HQ) dataset. The metrics used are Area Under the Curve (AUC) and Equal Error Rate (EER). The asterisk (*) indicates that results were reproduced using publicly available code or pre-trained models.", "section": "4.2 Experimental Results"}, {"figure_path": "FNzpVTpNbN/tables/tables_13_1.jpg", "caption": "Table 1: Frame-level cross-database evaluation from FF++(HQ) to Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap in terms of AUC and EER. * represents the results reproduced using open-source code or model.", "description": "This table presents a frame-level cross-database evaluation of several deepfake detection methods.  The models are trained on the high-quality FaceForensics++ (FF++) dataset and then evaluated on five unseen datasets: Celeb-DF, Wild Deepfake, DFDC-Preview, DeepFake Detection, and DiffSwap. The performance is measured using the Area Under the Curve (AUC) and Equal Error Rate (EER) metrics. The table shows AUC and EER scores for each method and dataset, allowing for a comparison of generalization performance across different deepfake datasets and detection models.", "section": "4.2 Experimental Results"}, {"figure_path": "FNzpVTpNbN/tables/tables_13_2.jpg", "caption": "Table 1: Frame-level cross-database evaluation from FF++(HQ) to Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap in terms of AUC and EER. * represents the results reproduced using open-source code or model.", "description": "This table presents a comprehensive evaluation of the DiffusionFake framework's generalization capabilities.  It compares the Area Under the Curve (AUC) and Equal Error Rate (EER) metrics of several state-of-the-art deepfake detection models. The models are trained on the high-quality FaceForensics++ (FF++) dataset and evaluated on five different unseen datasets (Celeb-DF, Wild Deepfake, DFDC-P, DFD, and DiffSwap).  This allows for an assessment of cross-dataset generalization performance. The table shows the AUC and EER for each method across different test datasets and provides an average performance across these datasets.", "section": "4.2 Experimental Results"}]