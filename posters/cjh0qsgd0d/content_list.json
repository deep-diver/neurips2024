[{"type": "text", "text": "Learning Macroscopic Dynamics from Partial Microscopic Observations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mengyi Chen1, Qianxiao Li1, 2 ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics, National University of Singapore1, Institute for Functional Intelligent Materials, National University of Singapore2 chenmengyi@u.nus.edu, qianxiao@nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Macroscopic observables of a system are of keen interest in real applications such as the design of novel materials. Current methods rely on microscopic trajectory simulations, where the forces on all microscopic coordinates need to be computed or measured. However, this can be computationally prohibitive for realistic systems. In this paper, we propose a method to learn macroscopic dynamics requiring only force computations on a subset of the microscopic coordinates. Our method relies on a sparsity assumption: the force on each microscopic coordinate relies only on a small number of other coordinates. The main idea of our approach is to map the training procedure on the macroscopic coordinates back to the microscopic coordinates, on which partial force computations can be used as stochastic estimation to update model parameters. We provide a theoretical justification of this under suitable conditions. We demonstrate the accuracy, force computation efficiency, and robustness of our method on learning macroscopic closure models from a variety of microscopic systems, including those modeled by partial differential equations or molecular dynamics simulations. Our code is available at https://github.com/MLDS-NUS/Learn-Partial.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Macroscopic properties, including thestructural and dynamical properties, provide a way to describe and understand the collective behaviors of complex systems. In a wide range of real applications, researchers focus mainly on the macroscopic properties of a system, e.g., the viscosity and ionic diffusivity of liquid electrolytes for Li-ion batteries (Dajnowicz et al., 2022). Macroscopic observables usually depend on the whole microscopic system, e.g., the calculation of mean squared displacement requires all microscopic coordinates during the simulation. With growing simulation and experimental data, data-driven learning of macroscopic properties from microscopic observations has become an active area of research (Zhang et al., 2018; Wang et al., 2019; Husic et al., 2020; Lee et al., 2020; Fu et al., 2023; Chen et al., 2024). ", "page_idx": 0}, {"type": "text", "text": "Accurate calculation of macroscopic properties requires large-scale microscopic simulation. However, accurate force computations on all microscopic coordinates for large systems are extremely expensive (Jia et al., 2020; Musaelian et al., 2023). For example, in ab initio molecular simulations, accurate forces need to be calculated from density functional theory (DFT). The computational cost of DFT limits its application to relatively small systems, typically ranging from a few hundred atoms to several thousand atoms, depending on the level of accuracy and computation resources (Hafner et al., 2006; Luo et al., 2020). This poses a dilemma: Accurate macroscopic properties are obtained from large-scale microscopic simulation, but the computation of forces on all the microscopic coordinates is extremely challenging. ", "page_idx": 0}, {"type": "image", "img_path": "cjH0Qsgd0D/tmp/c5ca2dc354343cb10d282626dafa7fee14ce85fba92b7ddb278e90f9465b1dc2.jpg", "img_caption": ["Figure 1: Overview of our method. Left. Data generation workflow. For each configuration x, forces on a subset of all the microscopic coordinates are calculated by the microscopic force calculator. Right. Macroscopic dynamics identification. The macroscopic dynamics is mapped to the microscopic space first, then compared with the forces on a subset of the microscopic coordinates. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To solve the dilemma, the corresponding question is: Can we still obtain accurate macroscopic observables even though only access to forces on a subset of the microscopic coordinates? In this work, we develop a method to learn the dynamics of the macroscopic observables directly, while only forces on a subset of the microscopic coordinates are needed. Efficient partial computation of microscopic forces relies on the sparsity assumption, where the computation cost of forces on a subset of microscopic coordinates does not scale with the microscopic system size. To learn the dynamics of the macroscopic observables, we first map the macroscopic dynamics back to the microscopic space, then compare it with the partial microscopic forces. Our key idea is summarized in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a novel method that can learn the macroscopic dynamics from partial computation of the microscopic forces. Our method can significantly reduce the computational cost for force computations.   \n\u2022 We theoretically justify that forces on a subset of the microscopic coordinates can be used as stochastic estimation to update latent model parameters, even if the macroscopic observables depend on all the microscopic coordinates.   \n\u2022 We empirically validate the accuracy, force computation efficiency, and robustness of our method through a variety of microscopic dynamics and latent model structures. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Learning from Partial Observations Several works have sought to learn dynamics from partially observed state $\\hat{\\bf x}$ utilizing machine learning (Ruelle & Takens, 1971; Sauer et al., 1991; Takens, 2006; Ayed et al., 2019; Ouala et al., 2020; Huang et al., 2020; Schlaginhaufen et al., 2021; Lu et al., 2022; Stepaniants et al., 2023). For training, these methods reconstruct the unobserved state x\u02dc first and model the dynamics of $\\mathbf{x}=\\left({\\hat{\\mathbf{x}}},{\\tilde{\\mathbf{x}}}\\right)$ . Our work assumes full state $\\mathbf{x}$ but partial forces f. Furthermore, we do not model the dynamics on state $\\mathbf{x}$ directly, but rather on the latent space because the dimension of $\\mathbf{x}$ would be extremely high for large systems. ", "page_idx": 1}, {"type": "text", "text": "Reduced Order Models By modeling the dynamics in the latent space and then recovering the full states from them, reduced order models (ROMs) substitute expensive full order simulation with cheaper reduced order simulation (Schilders et al., 2008; Fresca et al., 2020; Lee & Carlberg, 2020; Hernandez et al., 2021; Fries et al., 2022). ", "page_idx": 1}, {"type": "text", "text": "Our method can be thought to fall in the range of closure modeling. Unlike ROMs, we aim to model the dynamics of some given macroscopic observables directly and are not interested in recovering the microscopic states from the latent states. ", "page_idx": 2}, {"type": "text", "text": "Equation-free Framework The equation-free framework (EFF) has sought to simulate the macroscopic dynamics efficiently (Kevrekidis et al., 2003; Samaey et al., 2006; Liu et al., 2015). The EFF is usually applied to partial differential equation (PDE) systems, and the macroscopic observables are chosen to be the solution of the PDE at the coarse spatial grid. In EFF, the macroscopic observables depend locally on the microscopic coordinates, allowing the macroscopic dynamics to be directly estimated from the microscopic simulations performed in small spatial domains. In contrast, the macroscopic observables may depend globally on the microscopic coordinates in our method, and the macroscopic dynamics may not be easily estimated from microscopic simulations performed in small spatial domains. ", "page_idx": 2}, {"type": "text", "text": "Another difference is that our method explicitly learns the macroscopic dynamics, while EFF can bypass explicit derivation of macroscopic evolution law by coupling microscale and macroscale dynamics. During simulation, EFF still requires microscopic simulation to be performed in small spatial domains and for short times, but our method can enable fast macroscopic simulation without requiring any microscopic simulation However, for systems where the macroscopic evolution equations conceptually exist but are not available in closed form, EFF can efficiently handle such cases, but the learned dynamics in our method may involve approximation or statistical errors that are often challenging to estimate. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a microscopic system consisting of $n$ particles. Let the state of the microscopic system be $\\mathbf{x}=(\\mathbf{x}_{1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\mathbf{x}_{n})\\in\\mathbb{R}^{\\hat{N}},\\mathbf{x}_{i}^{\\overline{{\\iota}}}\\in\\mathbb{R}^{m},N=\\check{m}n,$ , where $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is some physical quantity associated with the $i$ -th particle, such as the position and velocity. Assume the dynamics of the microscopic system can be characterized by an ordinary differential equation(ODE): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\,\\mathrm{d}\\mathbf{x}(t)}{\\,\\mathrm{d}t}=\\mathbf{f}(\\mathbf{x}(t))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\bf f}({\\bf x})\\,=\\,({\\bf f}_{1}({\\bf x}),\\cdot\\cdot\\cdot\\,,{\\bf f}_{n}({\\bf x}))\\,\\in\\,\\mathbb{R}^{N}$ . We will call $\\mathbf{x}_{i}$ the microscopic coordinate of the $i$ -th particle and $\\mathbf{f}_{i}$ the force acting on the microscopic coordinate $\\mathbf{x}_{i}$ . In many real applications, we are interested in the dynamics of some macroscopic observables $\\mathbf{z}^{\\ast}\\,=\\,\\varphi^{\\ast}(\\mathbf{x})$ . Here $\\varphi^{*}$ is given beforehand and describes the functional dependence of $\\mathbf{z}^{\\ast}$ on $\\mathbf{x}$ . For example, $\\mathbf{z}^{\\ast}$ can be chosen to be the instantaneous temperature or mean squared displacement in a Lennard-Jones system. ", "page_idx": 2}, {"type": "text", "text": "The goal is to learn the dynamics of $\\mathbf{z}^{\\ast}$ from microscopic simulation data. Existing methods that try to learn the macroscopic or latent dynamics require microscopic trajectories or forces on all the microscopic coordinates for training (Champion et al., 2019; Fries et al., 2022; Fu et al., 2023; Chen et al., 2024). The problem is: When the microscopic system size $N$ is very large such that the force computations on all the microscopic coordinates are impossible, these methods are no longer applicable. Instead, our method aims to learn from partial computation of microscopic forces. ", "page_idx": 2}, {"type": "text", "text": "Consider we are given a microscopic force calculator $\\boldsymbol{S}$ for computation of partial microscopic forces. Let the microscopic coordinate $\\mathbf{x}$ be sampled from a distribution $\\mathcal{D}$ . For each $\\mathbf{x}$ , the microscopic force calculator $\\boldsymbol{S}$ will first sample an $n$ dimensional random variable ${\\bf I}({\\bf x})=\\left({\\bf I}_{1}({\\bf x}),\\cdot\\cdot\\cdot{\\bf\\nabla},{\\bf I}_{n}({\\bf x})\\right)\\sim$ $\\mathcal{P}_{\\mathbf{x}}\\in\\{0,1\\}^{n}$ according to a certain strategy. Next $\\boldsymbol{S}$ will calculate the corresponding partial forces $\\mathbf{f}_{\\mathbf{I}(\\mathbf{x})}\\,:=\\,\\left(\\mathbf{f}_{\\mathbf{I}_{1}(\\mathbf{x})},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\mathbf{f}_{\\mathbf{I}_{n}(\\mathbf{x})}\\right)$ . For notation simplicity sometimes we will simply write $\\mathbf{f}_{\\mathbf{I}(\\mathbf{x})}$ as $\\mathbf{f_{I}}$ . $\\mathbf{I}_{i}(\\mathbf{x})$ indicate whether partial $i$ is chosen to calculate the force or not. If particle $i$ is chosen, $\\mathbf{I}_{i}(\\mathbf{x})\\,=\\,1,\\mathbf{f}_{\\mathbf{I}_{i}(\\mathbf{x})}\\,=\\,\\mathbf{f}_{i}$ , otherwise $\\mathbf{I}_{i}(\\mathbf{x})\\,=\\,0,\\mathbf{f}_{\\mathbf{I}_{i}(\\mathbf{x})}\\,=\\,\\mathbf{0}$ . We require the sampling strategy $\\mathcal{P}_{\\mathbf{x}}$ to satisfy: ", "page_idx": 2}, {"type": "text", "text": "1. For each $\\mathbf{I}(\\mathbf{x})\\sim\\mathcal{P}_{\\mathbf{x}}$ , exactly $n\\cdot p$ items are equal to 1 and the rest are 0. ", "page_idx": 2}, {"type": "text", "text": "2. Each particle can be chosen with equal probability p, i.e. $\\mathbb{P}(\\mathbf{I}_{i}(\\mathbf{x})\\,=\\,1)\\,=\\,p,\\mathbb{P}(\\mathbf{I}_{i}(\\mathbf{x})\\,=$ $0)=1-p,i=1,\\cdots\\,,n$ . ", "page_idx": 2}, {"type": "text", "text": "This means that the microscopic force calculator $\\boldsymbol{S}$ can calculate forces on $n\\cdot p$ microscopic coordinates, and $0\\,<\\,p\\,<\\,1$ limits the computation capacity of the microscopic force calculator $\\boldsymbol{S}$ . ", "page_idx": 2}, {"type": "text", "text": "The above requirement is consistent with real applications since it is difficult to calculate all the microscopic forces due to computational cost. Furthermore, for efficient calculation of the partial microscopic forces, we will assume f satisfies the following sparsity assumption: ", "page_idx": 3}, {"type": "text", "text": "Assumption: For a given error tolerance $\\epsilon>0$ , there exists a constant $M\\ll n$ , such that for any $\\mathbf{x}\\sim\\mathcal{D}$ and $i\\in\\{1,\\cdots,n\\}$ , we can always find an index set $J(\\mathbf{x}_{i})\\subset\\{i=1,\\cdots,n\\},|J(\\mathbf{x}_{i})|<\\dot{M}$ which satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n||\\mathbf{f}_{i}(\\mathbf{x}_{1},\\cdot\\cdot\\cdot,\\mathbf{x}_{n})-\\tilde{\\mathbf{f}}_{i}(\\{\\mathbf{x}_{i}\\}_{i\\in J(\\mathbf{x}_{i})})||_{2}<\\epsilon\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, the assumption implies that the computational cost of force $\\mathbf{f}_{i}$ is independent of the microscopic system dimension $N$ . Thus our microscopic force calculator $\\boldsymbol{S}$ can compute partial forces in an efficient way. This assumption is prevalent in real-world applications. To better illustrate this, we give two examples here. The first example is about molecular dynamics. In molecular dynamics, each $\\mathbf{x}_{i}$ represents the position $\\mathbf{r}_{i}$ and velocities $\\mathbf{v}_{i}$ of the $i^{\\th}$ -th atom, $i.e.$ , $\\mathbf{x}_{i}=(\\mathbf{r}_{i},\\mathbf{v}_{i})\\in\\dot{\\mathbb{R}}^{6}$ . Then Eq. (1) becomes the Newton\u2019s law of motion: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\frac{\\mathrm{d}\\mathbf{r}_{i}}{\\mathrm{d}t}}=\\mathbf{v}_{i}}\\ ~}\\\\ {{\\displaystyle{\\frac{\\mathrm{d}\\mathbf{v}_{i}}{\\mathrm{d}t}}={\\frac{1}{m_{i}}}\\mathbf{F}_{i}(\\mathbf{r}_{1},\\cdot\\cdot\\cdot,\\mathbf{r}_{n})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is common to limit the range of pairwise interactions to a cutoff distance (Allen et al., 2004; Zhou & Liu, 2022; Vollmayr-Lee, 2020). To calculate the force on an atom, we only need to consider its interaction with other atoms that are within the cutoff. The second example is about systems modeled by partial differential equation (PDE). We consider a time-dependent PDE and apply finite difference scheme to discretize the spatial derivatives. Then, the resulting semi-discretized equation takes the form of Eq. (1), and each $\\mathbf{x}_{i}$ is the value at the $i$ -th grid. $\\mathbf{f}_{i}$ only depends on those grids that are used for finite difference approximation of the spatial derivatives. ", "page_idx": 3}, {"type": "text", "text": "Let the training data generated by the microscopic force calculator $\\boldsymbol{S}$ be $\\left\\{\\mathbf{x}^{i},\\mathbf{f}_{\\mathbf{I}(\\mathbf{x}^{i})}\\right\\}_{i=1,\\cdots,K}$ . The data generation procedure is provided in Algorithm 1. We will introduce in the next section how we can learn the macroscopic dynamics from the training data with partial forces. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Existing works for macroscopic dynamics identification consist of two parts: dimension reduction and macroscopic dynamics identification (Fu et al., 2023; Chen et al., 2024). We will follow these two parts. We start with most standard parts of closure modeling with an autoencoder, next, we turn to the main difficulty of macroscopic dynamics identification from partial forces. ", "page_idx": 3}, {"type": "text", "text": "4.1 Autoencoder for Closure Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will use an autoencoder to find the closure $\\hat{\\mathbf{z}}=\\hat{\\varphi}(\\mathbf{x})$ to $\\mathbf{z}^{\\ast}=\\varphi^{\\ast}(\\mathbf{x})$ such that $\\mathbf{z}=(\\mathbf{z}^{\\ast},\\hat{\\mathbf{z}})$ forms a closed system. Here we define ${\\bf z}$ as forming a closed system if its dynamics $\\dot{\\bf z}$ depends only on $\\mathbf{z}$ , not any external variables. Note that in $\\mathbf{z}^{\\ast}=\\varphi^{\\ast}(\\mathbf{x}),\\varsigma$ $\\varphi^{*}$ is determined beforehand and contains no trainable parameters. This ensures $\\mathbf{z}^{\\ast}$ represents the desired macroscopic observables and remains unchanged during the training of the autoencoder. ", "page_idx": 3}, {"type": "text", "text": "Denote the encoder by $\\varphi\\;=\\;(\\varphi^{*},\\hat{\\varphi})$ and the decoder by $\\psi$ , we will minimize the following reconstruction loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{rec}}=\\frac{1}{K}\\sum_{i=1}^{K}\\|\\mathbf{x}^{i}-\\boldsymbol{\\psi}\\circ\\boldsymbol{\\varphi}(\\mathbf{x}^{i})\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We also want $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ to be well-conditioned (see Section 4.2), then we impose constraints on the condition number of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cond}}=\\frac{1}{K}\\sum_{i=1}^{K}\\|\\kappa(\\varphi^{\\prime}(\\mathbf{x}^{i})\\varphi^{\\prime}(\\mathbf{x}^{i})^{T})-1\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By enforcing $\\varphi^{\\prime}(\\mathbf{x}^{i})\\varphi^{\\prime}(\\mathbf{x}^{i})^{T}$ to be well-conditioned, we are also enforcing $\\varphi^{\\prime}(\\mathbf{x}^{i})\\in\\mathbb{R}^{d\\times N},d\\ll N$ to have full row rank, which will be used later. The overall loss to train the autoencoder is : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{AE}}=\\mathcal{L}_{\\mathrm{rec}}+\\lambda_{\\mathrm{cond}}\\mathcal{L}_{\\mathrm{cond}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "cjH0Qsgd0D/tmp/8902892005214597ea56d1675cf3efbbec0984106f408cbbb107df12c145c1ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "$\\lambda_{\\mathrm{cond}}$ is a hyperparameter to adjust the ratio of ${\\mathcal{L}}_{\\mathrm{cond}}$ and is chosen to be quite small in our experiments, e.g., $10^{-5}$ or $1\\dot{0}^{-6}$ . The aim of training the decoder $\\psi$ is to help the discovery of the closure variables. The decoder $\\psi$ will not be used for further macroscopic dynamics identification. To facilitate comparison between models tainted with all and partial forces, we will train the autoencoder first and freeze it for macroscopic dynamics identification. ", "page_idx": 4}, {"type": "text", "text": "4.2 Macroscopic Dynamics Identification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now address the difficulty of learning from data with partial microscopic forces. Substitute $\\mathbf{z}=\\varphi(\\mathbf{x})$ into equation Eq. (1) and make use of chain rule, we get the dynamics of $\\mathbf{z}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{z}}{\\mathrm{d}t}=\\varphi^{\\prime}(\\mathbf{x})\\mathbf{f}(\\mathbf{x}),\\quad\\mathbf{z}(0)=\\varphi(\\mathbf{x}_{0})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "here we use $\\varphi^{\\prime}(\\mathbf{x})$ to denote the Jacobian of $\\nabla_{\\mathbf{x}}\\varphi(\\mathbf{x})$ for notation simplicity. If the dynamics of $\\mathbf{z}$ is closed, the right-hand side of Eq. (8) will only depend on ${\\bf z}$ , and we parametrize it using a neural network $\\mathbf{g}_{\\theta}(\\bar{\\mathbf{z}})\\approx\\varphi^{\\prime}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})$ . Since we are only interested in macroscopic dynamics identification, the loss would be naturally defined on the macroscopic coordinates: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})=\\frac{1}{K}\\sum_{i=1}^{K}||\\varphi^{\\prime}(\\mathbf{x}^{i})\\mathbf{f}(\\mathbf{x}^{i})-\\mathbf{g}_{\\theta}(\\mathbf{z}^{i})||^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Eq. (9) is used commonly in existing work (Champion et al., 2019; Fries et al., 2022; Bakarji et al., 2022; Park et al., 2024). ", "page_idx": 4}, {"type": "text", "text": "The main difficulty of ${\\mathcal{L}}_{\\mathbf{z}}$ is that it includes the matrix-vector product $\\varphi^{\\prime}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})$ . Note that the $i$ -th entry of $\\varphi^{\\prime}(\\mathbf{x})\\mathbf{f}(\\mathbf{x})$ can be written as $\\textstyle\\sum_{j=1}^{n}\\varphi_{i j}^{\\prime}(\\mathbf{x})\\mathbf{f}_{j}(\\mathbf{x})$ , and it is difficult to find an unbiased estimation of the $i$ -th entry using a subset  \u0159of $\\{\\mathbf{f}_{j}(\\mathbf{x})\\}_{j=1,\\cdots,N}$ . Thus the accurate calculation of ${\\mathcal{L}}_{\\mathbf{z}}$ requires the forces $\\{\\mathbf{f}_{j}(\\mathbf{x})\\}_{j=1,\\cdots,N}$ on all the microscopic coordinates. ", "page_idx": 4}, {"type": "text", "text": "The main idea of our method is to map the loss on the macroscopic coordinates back to the microscopic coordinates: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathcal L}_{\\mathbf{x}}(\\pmb{\\theta})=\\frac{1}{K}\\sum_{i=1}^{K}||\\mathbf{f}(\\mathbf{x}^{i})-(\\varphi^{\\prime}(\\mathbf{x}^{i}))^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z}^{i})||^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(\\varphi^{\\prime}(\\mathbf{x}^{i}))^{\\dagger}\\in\\mathbb{R}^{N\\times d}$ is the Moore-Penrose inverse. Since $\\varphi^{\\prime}(\\mathbf{x}^{i})$ is of full row rank, $(\\varphi^{\\prime}(\\mathbf{x}^{i}))^{\\dagger}$ is in fact the right inverse of $\\varphi^{\\prime}(\\mathbf{x}^{i})$ , i.e., $\\varphi^{\\prime}(\\mathbf{x}^{i})(\\varphi^{\\prime}(\\mathbf{x}^{i}))^{\\dagger}$ is an identity matrix. Below we will show our main theoretical result: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Assume for any $\\mathbf{x}\\sim\\mathcal{D}$ , the eigenvalues of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ are lower bounded by $b_{1}$ and upper bounded by $b_{2}$ , $0<b_{1}\\leqslant b_{2}$ . Then: ", "page_idx": 4}, {"type": "equation", "text": "$$\nb_{1}(\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})+C)\\leqslant\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})\\leqslant b_{2}(\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})+C)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "here $C$ does not depend on $\\pmb{\\theta}$ hence does not affect the optimization. ", "page_idx": 4}, {"type": "text", "text": "The proof relies on singular value decomposition of $\\varphi^{\\prime}(\\mathbf{x})$ and we provide the full proof in Appendix A.1. Theorem 1 states that by minimizing $\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})$ , we are actually narrowing the range of $\\bar{\\mathcal{L}}_{\\mathbf{z}}(\\pmb{\\theta})$ . Hence we want $b_{1}$ and $b_{2}$ to be as close as possible, this is the reason why we constrain the ", "page_idx": 4}, {"type": "text", "text": "condition number of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ in Eq. (6). In the very special case where $b_{1}=b_{2}$ , minimizing $\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})$ is just equivalent to minimizing $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ . ", "page_idx": 5}, {"type": "text", "text": "Note that in loss ${\\mathcal{L}}_{\\mathbf{x}}$ , the term $||\\mathbf{f}(\\mathbf{x})\\,-\\,(\\varphi^{\\prime}(\\mathbf{x}))^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z})||$ can be rewritten as $\\begin{array}{r}{\\sum_{j=1}^{n}||\\mathbf{f}_{j}(\\mathbf{x})-}\\end{array}$ $(\\varphi^{\\prime}(\\mathbf{x}))_{j}^{\\dag}\\mathbf{g}_{\\theta}(\\mathbf{z})\\vert\\vert$ , and $\\begin{array}{r}{\\frac{1}{p}\\sum_{j\\in{\\bf I}({\\bf x})}||{\\bf f}_{j}({\\bf x})-(\\varphi^{\\prime}({\\bf x}))_{j}^{\\dagger}{\\bf g}_{\\theta}({\\bf z})||}\\end{array}$ can be regarded as its un bi\u0159ased stochastic estimation. Then, we can\u0159 introduce our loss defined for partial forces: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathbf{x},p}(\\pmb{\\theta})=\\frac{1}{p K}\\sum_{i=1}^{K}\\|\\mathbf{f}_{\\mathbf{I}(\\mathbf{x}^{i})}(\\mathbf{x}^{i})-(\\varphi^{\\prime}(\\mathbf{x}^{i}))_{\\mathbf{I}(\\mathbf{x}^{i})}^{\\dagger}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z}^{i})\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here a constant $1/p$ is multiplied to $\\mathcal{L}_{\\mathbf{x},p}$ to guarantee: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}^{1},\\cdots,\\mathbf{x}^{K}}\\mathbb{E}_{\\mathbf{I}(\\mathbf{x}^{1}),\\cdots,\\mathbf{I}(\\mathbf{x}^{K})}\\mathcal{L}_{\\mathbf{x},p}(\\pmb{\\theta})=\\mathbb{E}_{\\mathbf{x}^{1},\\cdots,\\mathbf{x}^{K}}\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We provide the full proof of Eq. (13) in Appendix A.2. By training with the model with $\\mathcal{L}_{\\mathbf{x},p}$ , we can use data with partial forces as stochastic estimation to update model parameters. The full training procedure is provided in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "Note that in Algorithm 1 during the data generation, $\\mathbf{I}(\\boldsymbol{x}^{i})$ is also sampled from its distribution. Thus, $\\mathcal{L}_{\\mathbf{x},p}$ is deterministic once the samples $\\{\\mathbf{x}^{i},\\mathbf{f}_{\\mathbf{I(x^{i})}}(\\mathbf{x}^{i})\\}_{i=1,\\cdots,K}$ are generated. In the limit, the estimation is unbiased: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (informal). Let $\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})\\,=\\,\\mathbb{E}\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta}),\\pmb{\\theta}^{*}\\,\\in\\,\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}),\\,\\pmb{\\theta}_{K,p}\\,\\in\\,\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\mathcal{L}_{\\mathbf{x},p}(\\pmb{\\theta}),$ then under certain conditions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}_{K,p})-\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}^{\\ast})\\xrightarrow{a.s.}0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof utilized Rademacher complexity and a crucial assumption used in the proof is the uniform boundedness of $\\mathcal{L}_{\\mathbf{x},p}$ . The formal version of Theorem 2 and the complete proof is provided in Appendix A.3. Theorem 2 theoretically justifies the expected risk $\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}_{K,p})$ at the optimal parameter found by $\\mathcal{L}_{\\mathbf{x},p}$ , converges to the optimal expected risk $\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}^{\\ast})$ as $K$ goes to infinity. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we experimentally validate the accuracy, force computation efficiency, and robustness through a variety of microscopic dynamics. ", "page_idx": 5}, {"type": "text", "text": "5.1 Force Computation Efficiency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first consider a one-dimensional spatiotemporal Predator-Prey system, mainly to validate the correctness and force computations efficiency of our method. ", "page_idx": 5}, {"type": "text", "text": "Predator-Prey System The simplified form of the Predator-Prey system with diffusion (Murray, 2003) is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial u}{\\partial t}=u(1-u-v)+D\\frac{\\partial^{2}u}{\\partial x^{2}}}\\\\ {\\displaystyle\\frac{\\partial v}{\\partial t}=a v(u-b)+\\frac{\\partial^{2}v}{\\partial x^{2}},\\quad x\\in\\Omega=[0,1],\\quad t\\in[0,\\infty)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $u,v$ denote the dimensionless populations of the prey and predator respectively, $a,b,D$ are three parameters. The complex dynamics of Predator-Prey interaction, including the pursuit of the predator and the evasion of the prey in ecosystems, can be described by Eq. (15). ", "page_idx": 5}, {"type": "text", "text": "We discretize the spatial domain of Eq. (15) into 50 uniform grids with $x_{i}\\,=\\,(i\\mathrm{~-~}\\textstyle{\\frac{1}{2}})\\Delta x,\\Delta x\\,=$ $0.02,1\\,\\leqslant\\,i\\,\\leqslant\\,50.$ . Let $\\mathbf{u}(t)\\ =\\ (u(x_{1},t),\\cdot\\cdot\\cdot\\ ,u(x_{50},t)),\\mathbf{v}(t)\\ =\\ (v(x_{1},t),\\cdot\\cdot\\cdot\\ ,v({\\tilde{x}}_{50},t))$ , then $(\\mathbf{u}(t),\\mathbf{v}(t))\\in\\mathbb{R}^{100}$ are treated as the microscopic states. After approximating the spatial derivatives in Eq. (15) with the finite difference method, we consider the semi-discrete equation which is an $N=100$ dimensional ODE as our microscopic evolution law (see Appendix B.1). ", "page_idx": 5}, {"type": "text", "text": "We choose the macroscopic observable of interest to be $\\mathbf{z}^{\\ast}\\;=\\;\\left(\\bar{u},\\bar{v}\\right)$ , the spatial average of the predator and the prey\u2019s population: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{u}=\\frac{1}{50}\\sum_{i=1}^{50}u(x_{i},t),\\quad\\bar{v}=\\frac{1}{50}\\sum_{i=1}^{50}v(x_{i},t)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We find another 2 closure variables using the autoencoder, then the total dimension of the latent space ${\\bf z}$ is 4. We choose $\\mathcal{D}$ to be the trajectory distribution of the state x. For the data generation with partial forces, given $\\mathbf{x}$ we randomly choose forces on $100\\cdot p$ microscopic coordinate. For example, if $p=1/5$ , then for each configuration, forces on 20 coordinates are calculated for training. ", "page_idx": 6}, {"type": "text", "text": "Results Fig. 2 shows the results on the test dataset which consists of 100 trajectories (for more detials, see Appendix Table 2). For models trained with partial microscopic forces, we report the equivalent number of training data with full forces throughout the paper. For example, for a model trained with $\\mathcal{L}_{\\mathbf{x},p}(p=1/5)$ on $3\\stackrel{\\cdot}{\\times}10^{3}$ data, we will report the number of training data to be $3\\times10^{3}\\times0.2=600$ The test error is defined to be the mean relative error of the macroscopic observables between the ground truth and the predicted trajectories as in Appendix Eq. (43). ", "page_idx": 6}, {"type": "text", "text": "First, we observe that the mean relative error of all the models is around $10^{-4}$ when the number of training data is large enough. This tells us that training with $\\mathcal{L}_{\\mathbf{x},p}$ is correct and accurate, which is consistent with Theorem 2. The predicted trajectories fit quite well with the ground truth trajectories (see Appendix Fig. 6 and Fig. 7). ", "page_idx": 6}, {"type": "image", "img_path": "cjH0Qsgd0D/tmp/d6292184597d5c1e9eaffa26ea81f42bd30a8cb7a46f2c99824f5a879b0807e1.jpg", "img_caption": ["Figure 2: Mean relative error on the test dataset of the Predator-Prey system. The black dashed line represents test error $=$ $3\\times10^{-3}$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We can conclude from Fig. 2 that, under the same number of training data, $\\mathcal{L}_{\\mathbf{x},p}$ with smaller $p$ $\\mathrm{~p~}(1/4,1/5)$ performs better. Similarly, to achieve the same performance, $\\mathcal{L}_{\\mathbf{x},p}$ with smaller $p$ requires less training data. We set the error tolerance to be $e_{\\mathrm{tol}}=3\\times10^{-3}$ and investigate how much training data is required to reach the error tolerance. In Fig. 2 the $x$ -coordinate of the intersection point between the black dashed line and the other curves indicates the minimum data size required If we arrange each model according to their minimal required training data, then $\\mathcal{L}_{\\mathbf{x},p}(p=1/5)\\approx$ $\\mathcal{L}_{\\mathbf{x},p}(p=1/4)<\\mathcal{L}_{\\mathbf{x},p}(p=1/2)\\overset{\\sim}{<}\\mathcal{L}_{\\mathbf{x},p}(p=3/4)<\\hat{\\mathcal{L}}_{\\mathbf{x}}$ . Model trained with $\\mathcal{L}_{\\mathbf{x},p}(p=1/4,1/5)$ requires less data to reach $e_{\\mathrm{tol}}$ , or equivalently, less force computations. This validates the force computation efficiency of our method. One explanation could be that there are many redundant information in the forces acting on all the microscopic coordinates. By using partial microscopic forces, $\\mathcal{L}_{\\mathbf{x},p}$ can explore more configurations $\\mathbf{x}$ given the same size of training data, thus can make use of more useful information. Another observation from Fig. 2 is that as the training data size increases, the gap between models trained with different $p$ narrows down. This is because as more data are provided, these data can contain almost all the information of the Predator-Prey system, thus more information will not lead to significant improvement. ", "page_idx": 6}, {"type": "text", "text": "5.2 Robustness to Different Latent Structures ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Having validated the correctness and force computation efficiency of our method, we are ready to apply our method to a variety of latent structures. We will tackle the Lennard-Jones system in this subsection, and validate the robustness of our method to different latent model structures. Three latent model structures are considered: MLP, OnsagerNet (Yu et al., 2021), GFINNs (Zhang et al., 2022). Both the OnsagerNet and GFINNs endow the latent dynamical model with certain thermodynamic structure to ensure stability and interpretability. The specific implementations of these two models are slightly different. ", "page_idx": 6}, {"type": "text", "text": "Lennard-Jones System The Lennard-Jones system is widely used in molecular simulation to study phase transition, crystallization and macroscopic properties of a system (Hansen & Verlet, 1969; Bengtzelius, 1986; Lin et al., 2003; Luo et al., 2004). The Lennard-Jones potential describes the interaction between two atoms $i$ and $j$ through the potential of the following form: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{i j}(r)=\\left\\{\\begin{array}{l l}{4\\epsilon_{i j}[(\\sigma_{i j}/r)^{12}-(\\sigma_{i j}/r)^{6}]}&{\\mathrm{if}\\;r\\leqslant r_{\\mathrm{cut}},}\\\\ {0}&{\\mathrm{if}\\;r>r_{\\mathrm{cut}}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Table 1: Summary of the results on each system. Results of the Predator-Prey and Lennard-Jones (small) system are taken from Section 5.1, Section 5.2. For each system, ${\\mathcal{L}}_{\\mathbf{z}}$ and $\\mathcal{L}_{\\mathbf{x},p}$ are trained with the same size of data. ", "page_idx": 7}, {"type": "table", "img_path": "cjH0Qsgd0D/tmp/a798e5c5c85c93c548e664cae8f078eb74be4203bf1902b363e3c96fb8774f8b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "All the results in this experiment will be shown in reduced Lennard-Jones units. We consider a threedimensional Lennard-Jones fluid with $N_{\\mathrm{atoms}}=800$ atoms of the same type. The microscopic state consists of the positions and velocities of the 800 atoms, thus the microscopic dimension is $N\\,=\\,4800$ . We simulate the Lennard-Jones system under NVE ensemble using LAMMPS (Thompson et al., 2022). ", "page_idx": 7}, {"type": "text", "text": "We choose the instantaneous temperature $(T)$ as our macroscopic observables: ", "page_idx": 7}, {"type": "image", "img_path": "cjH0Qsgd0D/tmp/19ca62c8b1b67e7261fb71495a33e7bee217384de995323c8641909e498f0f17.jpg", "img_caption": ["Figure 3: Results on the Lennard-Jones system with 800 atoms and $N\\,=\\,4800$ . Forces on 50 atoms are used to train $\\mathcal{L}_{\\mathbf{x},p}$ for all the "], "img_footnote": [], "page_idx": 7}, {"type": "equation", "text": "$$\nT=\\frac{2}{3(N_{\\mathrm{atoms}}-1)}\\times\\sum_{i=1}^{N_{\\mathrm{atoms}}}\\frac{m_{i}v_{i}^{2}}{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "here $v_{i}$ is the velocity of the $i$ -th atom and $\\,m_{i}\\,=$ latent model structures. Each model is trained 1. We find another 31 closure variables using the with ten repeats.   \nautoencoder, then the latent dimension is 32. In our   \nexperiment, we also adopt the trajectory distribution   \nof microscopic statex for $\\mathcal{D}$ . For data generation with partial forces, we choose $p=1/16$ . For each $\\mathbf{x}$ we randomly choose 50 atoms for force computations. ", "page_idx": 7}, {"type": "text", "text": "Results All the models are trained with the same size of data. Fig. 3 shows the test error on 10 test trajectories. The test errors of using $\\mathcal{L}_{\\mathbf{x},p}(p=1/16)$ are relatively small $(\\sim10^{-3})$ , which validates the accuracy of our model on the Lennard-Jones system. It is easy to observe from Fig. 3 that for all the latent model structures, models trained with $\\mathcal{L}_{\\mathbf{x},p}$ can always outperform those trained with ${\\mathcal{L}}_{\\mathbf{z}}$ This validates $\\mathcal{L}_{\\mathbf{x},p}$ is robust over different latent model structures. ", "page_idx": 7}, {"type": "text", "text": "5.3 Robustness to Different Microscopic dynamics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We have already validated the accuracy and force computation efficiency of $\\mathcal{L}_{\\mathbf{x},p}$ on the PredatoryPrey system and the Lennard-Jones system, but their microscopic dimension is still not big enough. In this subsection, we focus on the robustness of our method to different systems including those with much larger microscopic dimension. We will consider two large systems: the Allen-Cahn system and a larger Lennard-Jones system with 51200 atoms. ", "page_idx": 7}, {"type": "text", "text": "Allen-Cahn System The Allen-Cahn equation is widely used to model the phase transition process in binary mixtures (Allen & Cahn, 1979; Del Pino et al., 2008; Shen & Yang, 2010; Kim et al., 2021; Yang et al., 2023). We consider the 2-dimensional Allen-Cahn equation with zero Neumann boundary condition on a bounded domain: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\partial_{t}v=\\nabla^{2}v-\\frac{1}{\\epsilon^{2}}F^{\\prime}(v)\\;\\mathrm{on}\\;\\Omega=[0,1]\\times[0,1]}}\\\\ {{\\displaystyle\\partial_{\\mathbf{n}}v=0\\;\\mathrm{on}\\;\\partial\\Omega,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $v(-1\\leqslant v\\leqslant1)$ denotes the difference of the concentration of the two phases. $F(v)$ is usually chosen to be the double potential taking the form of $\\begin{array}{r}{F(v)=\\frac{1}{4}(v^{2}-1)^{2}}\\end{array}$ . The Allen-Cahn equation is the $L^{2}$ gradient flow of the free energy functional $\\mathcal{E}(v)\\in\\mathbb{R}$ in Eq. (20) (Bartels, 2015). ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}(v)=\\int_{\\mu}\\left(\\frac{1}{\\epsilon^{2}}F(v)+\\frac{1}{2}\\|\\nabla v\\|_{2}^{2}\\right)\\;\\mathrm{d}x\\;\\mathrm{d}y\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The free energy functional $\\mathbf{z}^{\\ast}=\\mathcal{E}(v)$ is a macroscopic observable of wide interest, hence we choose $\\mathcal E(v)$ as our target macroscopic observable. ", "page_idx": 8}, {"type": "text", "text": "The spatial domain is discretized into $200\\times200$ grids, then the dimension of the microscopic system is $N=40000$ . We find another 31 closure variables using the autoencoder hence the total dimension of the macroscopic system is 32, which is much smaller compared to the dimension of the microscopic system. We consider $\\mathcal{D}$ to be the trajectory distribution of $\\mathbf{x}$ . We choose $p=1/25$ , each time the forces on 1600 grids are calculated for training $\\mathcal{L}_{\\mathbf{x},p}$ . ", "page_idx": 8}, {"type": "text", "text": "Lennard-Jones System (large) To further demonstrate the capacity of our method, we scale up the Lennard-Jones system in Section 5.2 to encompass 51200 atoms, then $N=307200$ . The size of the simulation box is increased from $10\\times10\\times10$ to $40\\times40\\times40$ to keep the density unchanged. ", "page_idx": 8}, {"type": "text", "text": "Results For a summary of the experiments and the results, we refer the reader to Table 1. Note that for the Lennard-Jones system (large), we still use the forces on 50 atoms for training, thus $p=1/1024$ . For the training of $\\mathcal{L}_{\\mathbf{x},p}(p=1/1024)$ , only 5000 configurations with partial forces are used due to memory limit, which is equivalent to $5000/1024\\approx5$ training data with forces on all the atoms. Obviously, training data with size 5 is way too small, hence the results of ${\\mathcal{L}}_{\\mathbf{z}}$ are not reported for this system. ", "page_idx": 8}, {"type": "text", "text": "From the results shown in Table 1, one can observe that for a variety of problems, including those modeled by partial differential equations or molecular dynamics simulations, $\\mathcal{L}_{\\mathbf{x},p}$ can always outperform ${\\mathcal{L}}_{\\mathbf{z}}$ . This shows the robustness of our method to a variety of systems. Moreover, the success of our method on the Lennard-Jones system (large) demonstrates the ability and efficiency of our method when scaled to very large systems. ", "page_idx": 8}, {"type": "text", "text": "We also compare the number of force computations that are required for models trained with ${\\mathcal{L}}_{\\mathbf{z}}$ and $\\mathcal{L}_{\\mathbf{x},p}$ to reach test error $\\begin{array}{r l r}{e_{\\mathrm{tol}}}&{{}=}&{3\\ \\times\\ 10^{-3}}\\end{array}$ . Fig. 4 shows the results on the Lennard-Jones system with different sizes. Lennard-Jones system with 800, 2700, 6400, 21600 atoms are considered and the density is 0.8 for all the systems. The number of force computations here refers to the total number of forces on atoms that are used. For example, if $\\mathcal{L}_{\\mathbf{x},p}$ uses 100 configurations to train, and for each configuration, forces on 50 atoms are calculated, then the number of force computations would be $100\\times50\\,=\\,5000$ . From Fig. 4 we can observe that as the system size increases, the number of force computations required by ${\\mathcal{L}}_{\\mathbf{z}}$ continues increasing. In the experiment, we find that the number of training data does not change a lot. The increase in the number of force computations is mainly due to the system size increases, then for each configuration, more force computations are required. However, the number of force computations even decreases a bit. One possible explanation is that, as the system size increases, the dynamics become less fluctuating and are easy to learn. ", "page_idx": 8}, {"type": "image", "img_path": "cjH0Qsgd0D/tmp/1ab87ec151892cfe6028670f7d2645b3194ab02169bf4448f472fe9985bceac6.jpg", "img_caption": ["Figure 4: Number of force computations required to achieve $e_{\\mathrm{tol}}=3\\times10^{-3}$ on LennardJones system with different sizes. Forces on 50 atoms are used to train $\\mathcal{L}_{\\mathbf{x},p}$ for systems of different sizes. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present a framework for modeling the dynamics of macroscopic observables from partial computation of microscopic forces. We theoretically and experimentally demonstrate the accuracy, force computation efficiency, and robustness of our method through different problems. Finally, we apply our method to a very large Lennard-Jones system which contains 51200 atoms. ", "page_idx": 8}, {"type": "text", "text": "While our method can learn the macroscopic dynamics from partial computation of microscopic forces, it relies on the sparsity assumption. For systems that do not satisfy the sparsity assumption, the calculation of partial computation of microscopic forces is not efficient, thus it is not beneficial to learn from partial forces computation. For example, in the McKean-Vlasov system, the force on each microscopic coordinate depends on the collective behavior of all the other coordinates (M\u00e9l\u00e9ard, 1996). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Another limitation is the structure of the autoencoder. For particle systems such as the Lennard-Jones system, an ideal encoder should be permutation-invariant. Currently, we use MLP for the encoder, which can be improved. Additionally, our method assumes the microscopic state to be sampled from a distribution $\\mathcal{D}$ . We choose $\\mathcal{D}$ to be trajectory distribution in the experiments, but in reality trajectory distribution of large systems may be impossible to obtain. Active learning is commonly applied to efficiently select microscopic configurations for training (Ang et al., 2021; Zhang et al., 2019; Farache et al., 2022; Kulichenko et al., 2023; Duschatko et al., 2024). It is of interest to combine active learning and our proposed method to overcome the difficulty of the choice of $\\mathcal{D}$ . Furthermore, our method assumes the microscopic dynamics to be deterministic, another future direction could be generalizing our method to stochastic systems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-RP-2022-028). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Michael P Allen et al. Introduction to molecular dynamics simulation. Computational soft matter: from synthetic polymers to proteins, 23(1):1\u201328, 2004.   \nSamuel M Allen and John W Cahn. A microscopic theory for antiphase boundary motion and its application to antiphase domain coarsening. Acta metallurgica, 27(6):1085\u20131095, 1979.   \nShi Jun Ang, Wujie Wang, Daniel Schwalbe-Koda, Simon Axelrod, and Rafael G\u00f3mez-Bombarelli. Active learning accelerates ab initio molecular dynamics on reactive energy surfaces. Chem, 7(3): 738\u2013751, 2021.   \nIbrahim Ayed, Emmanuel de B\u00e9zenac, Arthur Pajot, Julien Brajard, and Patrick Gallinari. Learning dynamical systems from partial observations. arXiv preprint arXiv:1902.11136, 2019.   \nJoseph Bakarji, Kathleen Champion, J Nathan Kutz, and Steven L Brunton. Discovering governing equations from partial measurements with deep delay autoencoders. arXiv preprint arXiv:2201.05136, 2022.   \nS\u00f6ren Bartels. The Allen\u2013Cahn Equation, pp. 153\u2013182. Springer International Publishing, Cham, 2015.   \nU Bengtzelius. Dynamics of a lennard-jones system close to the glass transition. Physical Review A, 34(6):5059, 1986.   \nKathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences, 116(45): 22445\u201322451, 2019.   \nXiaoli Chen, Beatrice W Soh, Zi-En Ooi, Eleonore Vissol-Gaudin, Haijun Yu, Kostya S Novoselov, Kedar Hippalgaonkar, and Qianxiao Li. Constructing custom thermodynamics using deep learning. Nature Computational Science, 4(1):66\u201385, 2024.   \nSteven Dajnowicz, Garvit Agarwal, James M Stevenson, Leif D Jacobson, Farhad Ramezanghorbani, Karl Leswing, Richard A Friesner, Mathew D Halls, and Robert Abel. High-dimensional neural network potential for liquid electrolyte simulations. The Journal of Physical Chemistry B, 126(33): 6271\u20136280, 2022.   \nManuel Del Pino, Micha\u0142 Kowalczyk, and Juncheng Wei. The toda system and clustering interfaces in the allen\u2013cahn equation. Archive for rational mechanics and analysis, 190(1):141\u2013187, 2008.   \nBlake R Duschatko, Jonathan Vandermause, Nicola Molinari, and Boris Kozinsky. Uncertainty driven active learning of coarse grained free energy models. npj Computational Materials, 10(1):9, 2024.   \nDavid E Farache, Juan C Verduzco, Zachary D McClure, Saaketh Desai, and Alejandro Strachan. Active learning and molecular dynamics simulations to find high melting temperature alloys. Computational Materials Science, 209:111386, 2022.   \nStefania Fresca, Andrea Manzoni, Luca Ded\u00e8, and Alfio Quarteroni. Deep learning-based reduced order models in cardiac electrophysiology. PloS one, 15(10):e0239416, 2020.   \nWilliam D Fries, Xiaolong He, and Youngsoo Choi. Lasdi: Parametric latent space dynamics identification. Computer Methods in Applied Mechanics and Engineering, 399:115436, 2022.   \nXiang Fu, Tian Xie, Nathan J Rebello, Bradley Olsen, and Tommi S Jaakkola. Simulate timeintegrated coarse-grained molecular dynamics with multi-scale graph networks. Transactions on Machine Learning Research, 2023.   \nJ\u00fcrgen Hafner, Christopher Wolverton, and Gerbrand Ceder. Toward computational materials design: the impact of density functional theory on materials research. MRS bulletin, 31(9):659\u2013668, 2006.   \nJean-Pierre Hansen and Loup Verlet. Phase transitions of the lennard-jones system. physical Review, 184(1):151, 1969.   \nQuercus Hernandez, Alberto Badias, David Gonzalez, Francisco Chinesta, and Elias Cueto. Deep learning of thermodynamics-aware reduced-order models from data. Computer Methods in Applied Mechanics and Engineering, 379:113763, 2021.   \nZijie Huang, Yizhou Sun, and Wei Wang. Learning continuous system dynamics from irregularlysampled partial observations. Advances in Neural Information Processing Systems, 33:16177\u2013 16187, 2020.   \nBrooke E Husic, Nicholas E Charron, Dominik Lemm, Jiang Wang, Adri\u00e0 P\u00e9rez, Maciej Majewski, Andreas Kr\u00e4mer, Yaoyi Chen, Simon Olsson, Gianni de Fabritiis, et al. Coarse graining molecular dynamics with graph neural networks. The Journal of chemical physics, 153(19), 2020.   \nWeile Jia, Han Wang, Mohan Chen, Denghui Lu, Lin Lin, Roberto Car, Weinan E, and Linfeng Zhang. Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine learning. In SC20: International conference for high performance computing, networking, storage and analysis, pp. 1\u201314. IEEE, 2020.   \nIoannis G Kevrekidis, C William Gear, James M Hyman, Panagiotis G Kevrekidis, Olof Runborg, Constantinos Theodoropoulos, et al. Equation-free, coarse-grained multiscale computation: enabling microscopic simulators to perform system-level analysis. Commun. Math. Sci, 1(4):715\u2013762, 2003.   \nYongho Kim, Gilnam Ryu, and Yongho Choi. Fast and accurate numerical solution of allen\u2013cahn equation. Mathematical Problems in Engineering, 2021:1\u201312, 2021.   \nMaksim Kulichenko, Kipton Barros, Nicholas Lubbers, Ying Wai Li, Richard Messerly, Sergei Tretiak, Justin S Smith, and Benjamin Nebgen. Uncertainty-driven dynamics for active learning of interatomic potentials. Nature Computational Science, 3(3):230\u2013239, 2023.   \nKookjin Lee and Kevin T Carlberg. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. Journal of Computational Physics, 404:108973, 2020.   \nSeungjoon Lee, Mahdi Kooshkbaghi, Konstantinos Spiliotis, Constantinos I Siettos, and Ioannis G Kevrekidis. Coarse-scale pdes from fine-scale observations via machine learning. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(1), 2020.   \nShiang-Tai Lin, Mario Blanco, and William A Goddard III. The two-phase model for calculating thermodynamic properties of liquids from molecular dynamics: Validation for the phase diagram of lennard-jones fluids. The Journal of chemical physics, 119(22):11792\u201311805, 2003.   \nPing Liu, Giovanni Samaey, C William Gear, and Ioannis G Kevrekidis. On the acceleration of spatially distributed agent-based computations: A patch dynamics scheme. Applied Numerical Mathematics, 92:54\u201369, 2015.   \nPeter Y Lu, Joan Ari\u00f1o Bernad, and Marin Soljac\u02c7ic\u00b4. Discovering sparse interpretable dynamics from partial observations. Communications Physics, 5(1):206, 2022.   \nSheng-Nian Luo, Alejandro Strachan, and Damian C Swift. Nonequilibrium melting and crystallization of a model lennard-jones system. The Journal of chemical physics, 120(24):11640\u201311649, 2004.   \nZhaolong Luo, Xinming Qin, Lingyun Wan, Wei Hu, and Jinlong Yang. Parallel implementation of large-scale linear scaling density functional theory calculations with numerical atomic orbitals in honpas. Frontiers in Chemistry, 8:589910, 2020.   \nSylvie M\u00e9l\u00e9ard. Asymptotic behaviour of some interacting particle systems; McKean-Vlasov and Boltzmann models, pp. 42\u201395. Springer Berlin Heidelberg, Berlin, Heidelberg, 1996. ISBN 978-3-540-68513-5. doi: 10.1007/BFb0093177. URL https://doi.org/10.1007/ BFb0093177.   \nJD Murray. Multi-species waves and practical applications. Mathematical Biology: II: Spatial Models and Biomedical Applications, pp. 1\u201370, 2003.   \nAlbert Musaelian, Simon Batzner, Anders Johansson, and Boris Kozinsky. Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size. In SC23: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1\u201312. IEEE, 2023.   \nSaid Ouala, Duong Nguyen, Lucas Drumetz, Bertrand Chapron, Ananda Pascual, Fabrice Collard, Lucile Gaultier, and Ronan Fablet. Learning latent dynamics for partially observed chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(10), 2020.   \nJun Sur Richard Park, Siu Wun Cheung, Youngsoo Choi, and Yeonjong Shin. tlasdi: Thermodynamicsinformed latent space dynamics identification. arXiv preprint arXiv:2403.05848, 2024.   \nDavid Ruelle and Floris Takens. On the nature of turbulence. Les rencontres physiciensmath\u00e9maticiens de Strasbourg-RCP25, 12:1\u201344, 1971.   \nGiovanni Samaey, Ioannis G Kevrekidis, and Dirk Roose. Patch dynamics with buffers for homogenization problems. Journal of Computational Physics, 213(1):264\u2013287, 2006.   \nTim Sauer, James A Yorke, and Martin Casdagli. Embedology. Journal of statistical Physics, 65: 579\u2013616, 1991.   \nWilhelmus HA Schilders, Henk A Van der Vorst, and Joost Rommes. Model order reduction: theory, research aspects and applications, volume 13. Springer, 2008.   \nAndreas Schlaginhaufen, Philippe Wenk, Andreas Krause, and Florian Dorfler. Learning stable deep dynamics models for partially observed or delayed dynamical systems. Advances in Neural Information Processing Systems, 34:11870\u201311882, 2021.   \nJie Shen and Xiaofeng Yang. Numerical approximations of allen-cahn and cahn-hilliard equations. Discrete Contin. Dyn. Syst, 28(4):1669\u20131691, 2010.   \nGeorge Stepaniants, Alasdair D Hastewell, Dominic J Skinner, Jan F Totz, and J\u00f6rn Dunkel. Discovering dynamics and parameters of nonlinear oscillatory and chaotic systems from partial observations. arXiv preprint arXiv:2304.04818, 2023.   \nFloris Takens. Detecting strange attractors in turbulence. In Dynamical Systems and Turbulence, Warwick 1980: proceedings of a symposium held at the University of Warwick 1979/80, pp. 366\u2013381. Springer, 2006.   \nA. P. Thompson, H. M. Aktulga, R. Berger, D. S. Bolintineanu, W. M. Brown, P. S. Crozier, P. J. in \u2019t Veld, A. Kohlmeyer, S. G. Moore, T. D. Nguyen, R. Shan, M. J. Stevens, J. Tranchida, C. Trott, and S. J. Plimpton. LAMMPS - a flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales. Comp. Phys. Comm., 271:108171, 2022. doi: 10.1016/j.cpc.2021.108171.   \nKatharina Vollmayr-Lee. Introduction to molecular dynamics simulations. American Journal of Physics, 88(5):401\u2013422, 2020.   \nMartin J. Wainwright. Uniform laws of large numbers, pp. 98\u2013120. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.   \nJiang Wang, Simon Olsson, Christoph Wehmeyer, Adri\u00e0 P\u00e9rez, Nicholas E Charron, Gianni De Fabritiis, Frank No\u00e9, and Cecilia Clementi. Machine learning of coarse-grained molecular dynamics force fields. ACS central science, 5(5):755\u2013767, 2019.   \nJunxiang Yang, Yibao Li, Chaeyoung Lee, Yongho Choi, and Junseok Kim. Fast evolution numerical method for the allen\u2013cahn equation. Journal of King Saud University-Science, 35(1):102430, 2023.   \nHaijun Yu, Xinyuan Tian, Weinan E, and Qianxiao Li. Onsagernet: Learning stable and interpretable dynamics using a generalized onsager principle. Physical Review Fluids, 6(11):114402, 2021.   \nLinfeng Zhang, Jiequn Han, Han Wang, Roberto Car, et al. Deepcg: Constructing coarse-grained models via deep neural networks. The Journal of chemical physics, 149(3), 2018.   \nLinfeng Zhang, De-Ye Lin, Han Wang, Roberto Car, and Weinan E. Active learning of uniformly accurate interatomic potentials for materials simulation. Physical Review Materials, 3(2):023804, 2019.   \nZhen Zhang, Yeonjong Shin, and George Em Karniadakis. Gfinns: Generic formalism informed neural networks for deterministic and stochastic dynamical systems. Philosophical Transactions of the Royal Society A, 380(2229):20210207, 2022.   \nKun Zhou and Bo Liu. Chapter 2 - potential energy functions. In Kun Zhou and Bo Liu (eds.), Molecular Dynamics Simulation, pp. 41\u201365. Elsevier, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $\\mathbf{x}\\,=\\,(\\mathbf{x}_{1},\\cdot\\cdot\\cdot\\,,\\mathbf{x}_{n})\\,\\in\\,\\mathcal{X}\\,\\subset\\,\\mathbb{R}^{N}$ , $\\mathbf{y}\\,=\\,\\mathbf{f}(\\mathbf{x})\\,\\in\\,\\mathcal{V}\\,\\subset\\,\\mathbb{R}^{N},\\pmb{\\theta}\\in\\,\\Theta$ , here $\\Theta$ is the set of our model parameters. Write $\\mathcal{D}^{K}=\\{\\mathbf{x}^{i},\\mathbf{y}^{i}\\}_{i=1,\\cdots,K}$ , then $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta}),\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})$ can be written as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{L}_{\\mathbf{z}}({\\pmb\\theta})=\\frac{1}{K}\\sum_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}^{K}}\\|\\varphi^{\\prime}(\\mathbf{x})\\mathbf{y}-\\mathbf{g}_{\\pmb\\theta}(\\mathbf{z})\\|_{2}^{2}}}\\\\ {{\\displaystyle\\mathcal{L}_{\\mathbf{x}}({\\pmb\\theta})=\\frac{1}{K}\\sum_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}^{K}}\\|\\mathbf{y}-(\\varphi^{\\prime}(\\mathbf{x}))^{\\dagger}\\mathbf{g}_{\\pmb\\theta}(\\mathbf{z})\\|_{2}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In our method, we map the training procedure from the macroscopic coordinates to microscopic coordinates and use partial computation of the microscopic forces to train. We treat $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ as baseline and give detailed theoretical analysis of the possible error introduced by using $\\mathcal{L}_{{\\bf x},p}(\\pmb{\\theta})$ . The error can be controlled by two parts: (i) Convert loss from ${\\bf z}$ space to $\\mathbf{x}$ space, i.e., the error introduced by using $\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})$ (ii) Use partial labels, i.e., the error between $\\mathcal{L}_{{\\bf x},p}(\\pmb{\\theta})$ and $\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})$ . ", "page_idx": 13}, {"type": "text", "text": "We will analyze the first part and the second part of the error accordingly (Appendix A). More experimental details are provided in Appendix B. ", "page_idx": 13}, {"type": "text", "text": "A Theoretical Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem. Assume for any $\\mathbf{x}\\sim\\mathcal{D}$ , the eigenvalues of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ are lower bounded by $b_{1}$ and upper bounded by $b_{2}$ , $0<b_{1}\\leqslant b_{2}$ . Then: ", "page_idx": 13}, {"type": "equation", "text": "$$\nb_{1}(\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})+C)\\leqslant\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})\\leqslant b_{2}(\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})+C)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "here $C$ does not depend on $\\pmb{\\theta}$ hence does not affect the optimization. ", "page_idx": 13}, {"type": "text", "text": "Proof. We can write $\\varphi^{\\prime}(\\mathbf{x})$ in the following form by leveraging singular value decomposition: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\boldsymbol{\\varphi}^{\\prime}(\\mathbf{x})=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{T},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\varphi^{\\prime}(\\mathbf{x})\\,\\in\\,\\mathbb{R}^{d\\times N},\\,\\pmb{\\Sigma}\\,\\in\\,\\mathbb{R}^{d\\times N}$ is a rectangular diagonal matrix, $\\mathbf{U}\\,\\in\\,\\mathbb{R}^{d\\times d}$ and ${\\bf V}\\,\\in\\,\\mathbb{R}^{N\\times N}$ are two orthogonal matrices, $d\\,\\ll\\,N.\\,\\,{\\bf U},\\Sigma,\\bar{\\bf V}$ actually depends on $\\mathbf{x}$ but for simplicity we omit the dependence in notation. During the training of the autoencoder, we enforce $\\varphi^{\\prime}(\\mathbf{x})$ to have full row rank, then the diagonal items are all nonzero, i.e., $\\mathbf{\\Sigma}_{\\pmb{i},i}~=~\\lambda_{i}(\\mathbf{x})~\\neq~0,\\dot{i}~=~1,\\cdots,d$ We define $\\pmb{\\Sigma}^{\\dag}\\ \\in\\ \\mathbb{R}^{N\\times d}$ to be the rectangular diagonal matrix such that the diagonal items are $(\\Sigma^{\\dagger})_{i,i}\\;=\\;\\lambda_{i}^{-1}({\\bf x})\\;\\neq\\;0,i\\;=\\;1,\\cdots\\;,d$ and all the remaining items are zero. Actually $\\pmb{\\Sigma}^{\\dagger}$ is the Moore-Penrose inverse of $\\Sigma$ . ", "page_idx": 13}, {"type": "text", "text": "Then $(\\varphi^{\\prime}(\\mathbf{x}))^{\\dagger}$ can be calculated by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\boldsymbol{\\varphi}^{\\prime}({\\bf x}))^{\\dagger}={\\bf V}\\boldsymbol{\\Sigma}^{\\dagger}{\\bf U}^{T}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If we denote the $i$ -th column of $\\mathbf{V}$ by $\\mathbf{v}_{i}$ , the i-th column of $\\mathbf{U}$ by $\\mathbf{u}_{i}$ , then ${\\bf V}=({\\bf v}_{1},\\cdot\\cdot\\cdot,{\\bf v}_{N}),{\\bf U}=$ $\\left(\\mathbf{u}_{1},\\cdot\\cdot\\cdot,\\mathbf{u}_{d}\\right)$ , and we can rewrite $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ and $\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\varphi^{\\prime}(\\mathbf{x})\\mathbf{y}-\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{U}\\Sigma\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{U}\\Sigma\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{U}\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\left(\\Sigma\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\right)^{T}\\mathbf{U}^{T}\\mathbf{U}(\\Sigma\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z}))}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\Sigma\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{i=1}^{d}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\lambda_{i}(\\mathbf{x})\\mathbf{v}_{i}^{T}\\mathbf{y}-\\mathbf{u}_{i}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{y}-(\\varphi^{\\prime}(\\mathbf{x}))^{\\dagger}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{y}-\\mathbf{V}\\mathbf{\\Sigma}\\mathbf{\\Sigma}^{\\dagger}\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{V}\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{V}\\mathbf{\\Sigma}^{\\dagger}\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\mathbf{(V}^{T}\\mathbf{y}-\\mathbf{Z}^{\\dagger}\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z}))^{T}\\mathbf{V}^{T}\\mathbf{V}(\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{Z}^{\\dagger}\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{V}^{T}\\mathbf{y}-\\mathbf{Z}^{\\dagger}\\mathbf{U}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{i=1}^{d}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{v}_{i}^{T}\\mathbf{y}-\\lambda_{i}^{-1}(\\mathbf{x})\\mathbf{u}_{i}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}+\\frac{1}{K}\\sum_{i=d+1}^{N}\\sum_{(\\mathbf{x,y})\\in\\mathcal{D}^{K}}\\|\\mathbf{v}_{i}^{T}\\mathbf{y}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad=\\frac{1}{K}\\sum_{i=1} \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We define: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\hat{\\mathcal{L}}}_{\\mathbf{x}}(\\pmb{\\theta})=\\frac{1}{K}\\sum_{i=1}^{d}\\sum_{(\\mathbf{x},\\mathbf{y})\\in{\\mathcal{D}}^{K}}\\lambda_{i}^{-2}(\\mathbf{x})\\|\\lambda_{i}(\\mathbf{x})\\mathbf{v}_{i}^{T}\\mathbf{y}-\\mathbf{u}_{i}^{T}\\mathbf{g}_{\\pmb{\\theta}}(\\mathbf{z})\\|_{2}^{2}}\\\\ &{~~~~~C=-\\frac{1}{K}\\sum_{i=d+1}^{N}\\sum_{(\\mathbf{x},\\mathbf{y})\\in{\\mathcal{D}}^{K}}\\|\\mathbf{v}_{i}^{T}\\mathbf{y}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then $\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})=\\hat{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})-C,C$ does not depend on $\\pmb{\\theta}$ and: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\,\\mathcal L_{\\mathbf x}(\\theta)\\iff\\operatorname*{min}_{\\theta}\\,\\hat{\\mathcal L}_{\\mathbf x}(\\theta).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Comparing $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ and $\\hat{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})$ , we observe that the only difference between $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ and $\\hat{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})$ is that for every term $\\|\\boldsymbol{\\lambda}_{i}(\\mathbf{x})\\mathbf{v}_{i}^{T}\\mathbf{f}(\\mathbf{x})-\\mathbf{u}_{i}^{T}\\mathbf{g}_{\\theta}(\\mathbf{z})\\|_{2}^{2}$ , there is a constant $\\lambda_{i}^{-2}(\\mathbf{x})$ multiplied to it. Hence $\\hat{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})$ is a weighted version of $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ . Note that if the eigenvalues of $\\varphi^{\\prime}(\\mathbf{x})$ are $\\bar{\\lambda_{i}}({\\bf x}),i=1,\\cdots,d,$ , then the eigenvalues of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{\\dot{x}})^{T}$ are $\\lambda_{i}^{2}({\\bf x}),i=1,\\bar{\\dots},d$ . ", "page_idx": 14}, {"type": "text", "text": "Since the eigenvalues of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ are lower bounded by $b_{1}>0$ and upper bounded by $b_{2}$ , i.e., $\\forall\\mathbf{x}\\in\\mathcal{X},0\\mathring{<}b_{1}\\leqslant\\lambda_{i}^{2}(\\mathbf{x})\\overset{.}{\\leqslant}b_{2}$ , then: ", "page_idx": 14}, {"type": "equation", "text": "$$\nb_{2}^{-1}\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})\\leqslant\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})+C\\leqslant b_{1}^{-1}\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "or equivalently, ", "page_idx": 14}, {"type": "equation", "text": "$$\nb_{1}(\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})+C)\\leqslant\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})\\leqslant b_{2}(\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})+C)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By minimizing $\\hat{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})$ , we are actually narrowing the region of $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ . Hence we want $b_{1}$ and $b_{2}$ to be as close as possible. In the extreme case where $b_{1}=b_{2}$ , minimizing $\\hat{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})$ is just equivalent to minimizing $\\mathcal{L}_{\\mathbf{z}}(\\pmb{\\theta})$ . Another observation is that $\\hat{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})$ is a weighted version of $\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta})$ , and if there exists $i$ such that $\\dot{\\lambda}_{i}({\\bf x})^{2}$ is too small compared to the others, the weighted sum will be dominated by it in Eq. (27). ", "page_idx": 14}, {"type": "text", "text": "The above insights guide us to constrain the condition number of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ during the training of autoencoder, $i.e.$ ., we require $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ to be well-conditioned through Eq. (6). ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Eq. (13) ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{x}^{1},\\cdots,\\mathbf{x}^{K}}\\mathbb{E}_{\\mathbf{I}(\\mathbf{x}^{1}),\\cdots,\\mathbf{I}(\\mathbf{x}^{K})}\\mathcal{L}_{\\mathbf{x},p}(\\theta)=\\mathbb{E}\\big[\\frac{1}{p K}\\sum_{i=1}^{K}\\big|\\big|\\mathbf{f}_{\\mathbf{I}(\\mathbf{x}^{i})}\\big(\\mathbf{x}^{i})-(\\varphi^{\\prime}(\\mathbf{x}^{i}))\\big|_{\\mathbf{I}(\\mathbf{x}^{i})}^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z}^{i})\\big|\\big|_{2}^{2}\\big]}\\\\ {=\\frac{1}{p}\\mathbb{E}_{\\mathbf{x}}\\mathbb{E}_{\\mathbf{I}(\\mathbf{x})}\\big[\\big|\\mathbb{f}_{\\mathbf{I}(\\mathbf{x})}(\\mathbf{x})-(\\varphi^{\\prime}(\\mathbf{x}))\\big|_{\\mathbf{I}(\\mathbf{x})}^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z})\\big|\\big|_{2}^{2}\\big]}\\\\ {=\\frac{1}{p}\\mathbb{E}_{\\mathbf{x}}\\mathbb{E}_{\\mathbf{I}(\\mathbf{x})}\\Big[\\sum_{i=1}^{n}\\mathbf{I}_{i}(\\mathbf{x})\\cdot\\big|\\mathbb{f}_{i}(\\mathbf{x})-(\\varphi^{\\prime}(\\mathbf{x}))_{i}^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z})\\big|\\big|_{2}^{2}\\Big]}\\\\ {=\\frac{1}{p}\\mathbb{E}_{\\mathbf{x}}\\Big[\\sum_{i=1}^{n}\\mathbb{E}_{\\mathbf{I}(\\mathbf{x})}\\mathbf{I}_{i}(\\mathbf{x})\\cdot\\big|\\mathbb{f}_{i}(\\mathbf{x})-(\\varphi^{\\prime}(\\mathbf{x}))_{i}^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z})\\big|\\big|_{2}^{2}\\Big]}\\\\ {=\\mathbb{E}_{\\mathbf{x}}\\Big[\\sum_{i=1}^{n}\\lVert\\mathbb{f}_{i}(\\mathbf{x})-(\\varphi^{\\prime}(\\mathbf{x}))_{i}^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z})\\big|\\big|_{2}^{2}\\Big]}\\\\ {=\\mathbb{E}_{\\mathbf{x}}\\Big[\\big\\lVert\\mathbf{f}(\\mathbf{x})-(\\varphi^{\\prime}(\\mathbf{x \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we will prove the behavior of the minimizer found by $\\mathcal{L}_{\\mathbf{x},p}$ in the limit. Our proof relies on the statistical learning theory and especially Rademacher complexity. We will provide some background information first. ", "page_idx": 15}, {"type": "text", "text": "Let $\\mathcal{H}$ be a family of real-valued functions with domain $\\mathcal{W}$ and integrable w.r.t. $\\mathbb{P}$ , here $\\mathbb{P}$ is a probability over $\\mathcal{W}$ . $\\dot{\\mathcal{W}^{n}}=(\\mathbf{w}^{1},\\cdot\\cdot\\cdot\\mathbf{\\Phi},\\mathbf{w}^{n})$ is a collection of i.i.d. samples from probability distribution $\\mathcal{P}$ defined over $\\mathcal{W}$ . ", "page_idx": 15}, {"type": "text", "text": "We will use the tool of Rademacher complexity: ", "page_idx": 15}, {"type": "text", "text": "Definition 1. Let $\\mathcal{H},\\mathcal{W}^{n},\\mathbb{P}$ be defined as before. The empirical Rademacher complexity of $\\mathcal{H}$ with respect to $\\mathcal{W}^{n}$ is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{W}^{n}}(\\mathcal{H})=\\mathbb{E}_{\\pmb{\\sigma}}\\left[\\operatorname*{sup}_{h\\in\\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}h(\\mathbf{w}^{i})\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\pmb{\\sigma}=(\\sigma_{1},\\cdot\\cdot\\cdot\\,,\\sigma_{n}),\\{\\sigma_{i}\\}_{i=1}^{n}$ are independent random variables uniformly chosen from $\\{-1,1\\}$ , with $P(\\sigma_{i}=1)=P(\\sigma_{i}=-1)=0.5.$ . Taking the expectation with respect to $\\mathcal{W}^{n}$ yields the Rademacher complexity of the functional class $\\mathcal{H}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{n}(\\mathcal{H})=\\mathbb{E}_{\\mathcal{W}^{n}}\\mathbb{E}_{\\pmb{\\sigma}}\\left[\\operatorname*{sup}_{h\\in\\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}h(\\mathbf{w}^{i})\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then one can derive the generalization bound in terms of the Rademacher complexity (Wainwright, 2019): ", "page_idx": 15}, {"type": "text", "text": "Theorem 3. Assume $\\mathcal{H}$ is uniformly bounded by $b$ (i.e., $\\|f\\|_{\\infty}\\leqslant b$ . Then for all $n\\geqslant1$ and $\\delta\\geqslant0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{H}}\\left\\vert\\frac{1}{n}\\sum_{i=1}^{n}h(\\mathbf{w}^{i})-\\mathbb{E}[h]\\right\\vert\\leqslant2\\mathcal{R}_{n}(\\mathcal{H}_{\\mathbf{x}})+\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $\\begin{array}{r}{1-2\\exp\\left(-\\frac{n\\delta^{2}}{8b^{2}}\\right)}\\end{array}$ . Consequently, as long as $\\mathcal{R}_{n}(\\mathcal{H}_{\\mathbf{x}})\\,=\\,o(1)$ , we have $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}f(X_{i})-\\mathbb{E}[f]\\xrightarrow{a.s.}0,\\forall f\\in\\mathcal{H}_{\\mathbf{x}}.}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Now in our problem, let $\\mathcal{H}_{\\mathbf{x},p}$ be the following one function class: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}_{\\mathbf{x},p}=\\{h_{\\theta,p}:\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{Z}\\to\\mathbb{R};h_{\\theta,p}(\\mathbf{x},\\mathbf{y},\\mathbf{I}(\\mathbf{x}))=\\frac{1}{p}||\\mathbf{y}_{\\mathbf{I}(\\mathbf{x})}-(\\varphi^{\\prime}(\\mathbf{x}))_{\\mathbf{I}(\\mathbf{x})}^{\\dagger}\\mathbf{g}_{\\theta}(\\mathbf{z})||_{2}^{2},\\theta\\in\\Theta\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathbf{x},p}(\\pmb{\\theta})=\\frac{1}{K}\\sum_{(\\mathbf{x},\\mathbf{y})\\in\\mathcal{D}^{K}}h_{\\pmb{\\theta},p}(\\mathbf{x},\\mathbf{y},\\operatorname{I}(\\mathbf{x}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we can prove Theorem 2: ", "page_idx": 15}, {"type": "text", "text": "Theorem. Let $\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})\\ =\\ \\mathbb{E}\\mathcal{L}_{\\mathbf{x}}(\\pmb{\\theta}),\\pmb{\\theta}^{*}\\ \\in\\ \\arg\\operatorname*{min}_{\\pmb{\\theta}}\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}),\\ \\pmb{\\theta}_{K,p}\\ \\in\\ \\arg\\operatorname*{min}_{\\pmb{\\theta}}\\mathcal{L}_{\\mathbf{x},p}(\\pmb{\\theta}),$ if $\\mathcal{H}_{\\mathbf{x},p}$ is uniformly bounded by $b_{\\mathbf{x},p}$ and $\\mathcal{R}_{K}(\\mathcal{H}_{\\mathbf{x},p})=o(1),$ , then: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}_{K,p})-\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta}^{\\ast})\\xrightarrow{a.s.}0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We define $\\tilde{\\mathcal{L}}_{\\mathbf{x},p}(\\pmb{\\theta})=\\mathbb{E}\\mathcal{L}_{\\mathbf{x},p}(\\pmb{\\theta})$ , then $\\tilde{\\mathcal{L}}_{\\mathbf{x}}=\\tilde{\\mathcal{L}}_{\\mathbf{x},p}$ by Appendix A.2. Applying Theorem 3, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta}\\Big|\\mathcal{L}_{\\mathbf{x},p}\\big(\\theta\\big)-\\tilde{\\mathcal{L}}_{\\mathbf{x},p}\\big(\\theta\\big)\\Big|\\leqslant2\\mathcal{R}_{K}\\big(\\mathcal{H}_{\\mathbf{x},p}\\big)+\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $\\begin{array}{r}{1-2\\exp(-\\frac{K\\delta^{2}}{8b_{\\mathbf{x},p}})}\\end{array}$ , and $\\begin{array}{r}{\\mathcal{L}_{\\mathbf{x},p}(\\pmb{\\theta})\\xrightarrow{a.s.}\\tilde{\\mathcal{L}}_{\\mathbf{x},p}(\\pmb{\\theta}),\\forall\\pmb{\\theta}\\in\\Theta.}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Note that \u03b8\u02da P ${\\arg\\operatorname*{min}}_{\\pmb{\\theta}}\\,\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\pmb{\\theta})\\in\\arg\\operatorname*{min}_{\\pmb{\\theta}}\\tilde{\\mathcal{L}}_{\\mathbf{x},p}(\\pmb{\\theta})$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\leqslant\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\theta_{K,p})-\\tilde{\\mathcal{L}}_{\\mathbf{x}}(\\theta^{*})=\\underbrace{\\tilde{\\mathcal{L}}_{\\mathbf{x},p}(\\theta_{K,p})-\\mathcal{L}_{\\mathbf{x},p}(\\theta_{K,p})}_{\\stackrel{a\\cdot\\mathcal{s}_{\\rangle0}}{\\longrightarrow}0}+\\underbrace{\\mathcal{L}_{\\mathbf{x},p}(\\theta_{K,p})-\\mathcal{L}_{\\mathbf{x},p}(\\theta^{*})}_{\\leqslant0}}\\\\ {+\\underbrace{\\mathcal{L}_{\\mathbf{x},p}(\\theta^{*})-\\tilde{\\mathcal{L}}_{\\mathbf{x},p}(\\theta^{*})}_{\\stackrel{a\\cdot\\mathcal{s}_{\\rangle0}}{\\longrightarrow}0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "cjH0Qsgd0D/tmp/aa1b0eb84f0b868d0488ca220ab77604fd9d2b22604166eb4c35f86046488058.jpg", "img_caption": ["Figure 5: Visualization of the microscopic state of each system "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.1 Predator-Prey System ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We consider the Neumann boundary condition $\\partial_{n}u=0,\\partial_{n}v=0$ on $\\partial\\Omega$ and the following initial conditions ", "page_idx": 16}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{u(x,0)=\\mu+\\sigma\\cos(5\\pi x)}}\\\\ {{v(x,0)=1-\\mu-\\sigma\\cos(5\\pi x),\\quad(\\mu,\\sigma)\\in[0,0.2]\\times[0.4,0.6]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The Neumann boundary condition is commonly used in mathematical models of ecosystems, restricting any movement of the species out of the boundary. ", "page_idx": 16}, {"type": "text", "text": "We approximate the spatial derivatives in Eq. (15) with finite difference method: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial^{2}v}{\\partial x^{2}}(x_{i},t)\\approx\\frac{v(x_{i+1},t)-2v(x_{i},t)+v(x_{i-1},t)}{\\Delta x^{2}}\\quad2\\leqslant i\\leqslant49}\\\\ &{\\displaystyle\\frac{\\partial^{2}v}{\\partial x^{2}}(x_{1},t)\\approx\\frac{v(x_{2},t)-v(x_{1},t)}{\\Delta x^{2}}}\\\\ &{\\displaystyle\\frac{\\partial^{2}v}{\\partial x^{2}}(x_{50},t)\\approx\\frac{v(x_{49},t)-v(x_{50},t)}{\\Delta x^{2}}}\\end{array}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $h_{u}(u,v)=u(1-u-v),h_{v}(u,v)=a v(u-b)$ , and $h_{u}(\\mathbf{u},\\mathbf{v}),h_{v}(\\mathbf{u},\\mathbf{v})$ denote the element-wise application of $h_{u},h_{v}$ to each $u(x_{i},t),v(x_{i},t),1\\leqslant i\\leqslant50$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}t}=h_{u}(\\mathbf{u},\\mathbf{v})}\\\\ {\\displaystyle\\frac{\\mathrm{d}\\mathbf{v}}{\\mathrm{d}t}=h_{v}(\\mathbf{u},\\mathbf{v})+\\mathbf{A}\\mathbf{v}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "here $\\mathbf{A}\\in\\mathbb{R}^{50\\times50}$ is a matrix defined according to Eq. (42). Hence the predator-prey system after spatial discretization can be written in the form of Eq. (1). ", "page_idx": 16}, {"type": "text", "text": "In our experiment, we choose $a=3,b=0.4,\\lambda=0$ . The training parameter set $\\tau_{\\mathrm{rain}}$ of pairs $(\\mu,\\sigma)$ are sampled uniformly from $[0,0.2]\\times[0.4,0.6]$ . For testing, $\\mathcal{T}_{\\mathrm{test}}$ is also sampled uniformly from $[0,0.2]\\times[0.4,0.6]$ , but with a different random seed from $\\tau_{\\mathrm{rain}}$ . The mean relative error is defined as : ", "page_idx": 16}, {"type": "equation", "text": "$$\ne(\\mathcal{T}_{\\mathrm{test}})=\\frac{1}{|\\mathcal{T}_{\\mathrm{test}}|}\\sum_{(\\mu,\\sigma)\\in\\mathcal{T}_{\\mathrm{test}}}\\left(\\frac{\\sum_{n}\\lVert\\mathbf{z}_{\\mathrm{tue}}^{*}(t_{n};\\mu,\\sigma)-\\mathbf{z}_{\\mathrm{pred}}^{*}(t_{n};\\mu,\\sigma)\\rVert_{2}^{2}}{\\sum_{n}\\lVert\\mathbf{z}_{\\mathrm{tue}}^{*}(t_{n};\\mu,\\sigma)\\rVert_{2}^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "here we use ${\\bf z}(\\cdot;\\mu,\\sigma)$ to denote the dependency of the solution on the initial condition. ", "page_idx": 16}, {"type": "table", "img_path": "cjH0Qsgd0D/tmp/e6a9d90e24da3c25fcf89aece30f0051d60a60141d5380ad4c0204404d45559c.jpg", "table_caption": ["Table 2: Results on the Predator-Prey system. Models are trained with different training metric $\\mathcal{L}_{\\mathbf{x}},\\mathcal{L}_{\\mathbf{x},p}(p=3/4,1/2,1/4,1/5)$ . Mean and standard deviation are reported over three repeats. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Data Generation The microscopic equation is solved with a uniform time step $\\Delta t=0.01$ from $t=0$ to $t=30$ using the Euler method. We subsampled every tenth snapshot for training. During testing, the microscopic evolution equation is solved with the same $\\Delta t=0.01$ using Runge-Kutta 4-order RK4 solver. Then we encode the microscopic trajectories to obtain the ground truth latent trajectories. The predicted latent trajectories are obtained by encoding the initial microscopic state first, then solved using RK4 solver with $\\Delta t=0.1,\\Delta=0.5$ on r0, 30s. Fig. 6 and Fig. 7 show the true and predicted trajectories. ", "page_idx": 17}, {"type": "table", "img_path": "cjH0Qsgd0D/tmp/e5b13e9a498dd753dd380ba7f098badc568d7e1a7f52b90e9a9e1dacee8a21a8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: Latent trajectories with initial condition $\\mu=0.02$ , $\\sigma=0.52$ and $\\Delta t=0.1$ in the PredatorPrey system. ", "page_idx": 17}, {"type": "image", "img_path": "cjH0Qsgd0D/tmp/7a2c9efdebc307d474a590c7a06a3860521482e0b05b23b37ae9ef115936d701.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 7: Latent trajectories with initial condition $\\mu=0.02$ , $\\sigma=0.52$ and $\\Delta t=0.5$ in the PredatorPrey system. ", "page_idx": 17}, {"type": "text", "text": "B.2 Allen-Cahn System ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In our experiment, we consider the initial condition of a torus (Kim et al., 2021): ", "page_idx": 17}, {"type": "equation", "text": "$$\nv(x,y,0)=-1+\\operatorname{tanh}(\\frac{r_{1}-d(x,y)}{\\sqrt{2}\\epsilon})-\\operatorname{tanh}(\\frac{r_{2}-d(x,y)}{\\sqrt{2}\\epsilon})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "here $d(x,y)\\,=\\,\\sqrt{(x-0.5)^{2}+(y-0.5)^{2}}$ , $r_{1}\\,\\in\\,[0.3,0.4]$ is the circumscribed circle radius and $r_{2}\\in[0.1,0.15]$ isa the inscribed circle radius. The initial condition is visualized in Fig. 5 (b). ", "page_idx": 17}, {"type": "text", "text": "The free energy in Eq. (20) also tends to decrease with time, following the energy dissipation law in Eq. (45). Then the minimization of the free energy drives the evolution of the system towards equilibrium. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{E}(v)}{\\partial t}=-\\int_{\\mu}\\lVert\\partial_{t}v\\rVert_{2}^{2}\\,\\mathrm{d}x\\,\\mathrm{d}y\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Data Generation For both training and testing, the microscopic evolution law is solved using RK4 method with $\\Delta t=1/N=2.5\\times1\\bar{0}^{-5}$ from $t=0$ to $t=\\operatorname*{min}(t_{f},1)$ .Here $t_{f}$ is the time when the Allen-Cahn system reaches the equilibrium. We subsample every hundredth snapshots for training. We choose $\\epsilon$ in Eq. (19) and Eq. (44) to be $\\frac{10\\!\\times\\!200}{2\\sqrt{2}\\operatorname{tanh}^{-1}(0.9)}$ as in (Kim et al., 2021). ", "page_idx": 18}, {"type": "text", "text": "For testing, 50 parameter points $(r_{1},r_{2})$ are chosen uniformly from $[0.3,0.4]\\times[0.1,0.15]$ , and we report the mean relative error. The test parameter set $\\mathcal{T}_{\\mathrm{test}}$ contains 50 parameter points $(r_{1},r_{2})$ are chosen uniformly from $[0.3,0.4]\\times[0.\\bar{1},0.15]$ , but are sampled with a different random seed. ", "page_idx": 18}, {"type": "text", "text": "B.3 Lennard-Jones System ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We consider Lennard-Jones systems containing different atoms in this paper: $\\begin{array}{r l}{N_{\\mathrm{atoms}}}&{{}=}\\end{array}$ \u201c 800, 2700, 6400, 21600, 51200. We use periodic boundary conditions and fix the density to be 0.8, then the corresponding box side lengths are 10, 15, 20, 30, 40. We simulate the Lennard-Jones system under the NVE ensemble using the LAMMPS (Thompson et al., 2022). In our experiment, $\\epsilon_{i j}=\\sigma_{i j}=1,\\forall i,j$ , and $r_{\\mathrm{cut}}=2.5$ . The integration step is 0.001 and each trajectory is integrated for 250 steps. We sample the initial temperature randomly from r0.5, 1.5s. The initial velocities are then sampled from the Maxwell\u2013Boltzmann distribution. For each system, the initial configuration has the same atom positions and velocity direction. For testing, the initial temperatures are also randomly sampled from r0.5, 1.5s but with a different random seed to the training data. ", "page_idx": 18}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All the experiments are run on a single NVIDIA GeForce RTX 3090 GPU. For all the experiments, we use the multilayer perceptron (MLP) for both the encoder and the decoder. The autoencoders are trained with $\\mathcal{L}_{\\mathrm{AE}}$ in Eq. (7). The condition number is the maximal eigenvalue $\\lambda_{\\mathrm{max}}$ divided by the minimal eigenvalue $\\lambda_{\\operatorname*{min}}$ of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\kappa\\left(\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}\\right)=\\frac{\\left|\\lambda_{\\operatorname*{max}}(\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T})\\right|}{\\left|\\lambda_{\\operatorname*{min}}(\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T})\\right|}\\geqslant1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\begin{array}{r l r}{\\varphi^{\\prime}(\\mathbf{x})\\underset{\\lefteqn{\\in\\mathrm{~\\mathbb~R}^{d\\times N},\\,\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}}}{\\in\\mathrm{~\\mathbb~R}^{d\\times N}},\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}}&{\\in\\mathrm{~\\mathbb~R}^{d\\times d}}\\end{array}$ , and $d$ is small, the condition number $\\kappa(\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{\\dot{x}})^{T})$ can be calculated efficiently. In our experiments, we calculate $\\lambda_{\\mathrm{max}}$ and $\\lambda_{\\operatorname*{min}}$ with torch.linalg.svd. To better compare ${\\mathcal{L}}_{\\mathbf{x}}$ and $\\mathcal{L}_{\\mathbf{x},p}$ , once finish the training of the autoencoders, we freeze them and use the encoder for macroscopic dynamics identification. For the macroscopic dynamics identification, MLP and GFINNs are used for the latent model in Section 5.2. For the rest experiments, we adopt the structure of OnsagerNet to enhance the stability for latent dynamics prediction for $\\mathbf{g}_{\\theta}$ ( $\\mathrm{Yu}$ et al., 2021). ", "page_idx": 18}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Loss Curve of $\\mathcal{L}_{\\mathbf{z}},\\mathcal{L}_{\\mathbf{x}},\\mathcal{L}_{\\mathbf{x},p}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To give the readers a better idea of the behaviors of the loss $\\mathcal{L}_{\\mathbf{z}},\\mathcal{L}_{\\mathbf{x}},\\mathcal{L}_{\\mathbf{x},p}(p=1/4)$ trained on the same number of training data. Fig. 8 shows the training and test loss curve of different training metrics. Note that the training metrics of these models are different, but they are tested with the same metric Eq. (43). ", "page_idx": 18}, {"type": "text", "text": "D.2 Ablation Analysis of $\\lambda_{\\mathbf{cond}}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To evaluate the influence of the hyperparameter $\\lambda_{\\mathrm{cond}}$ on the performance of the loss $\\mathcal{L}_{\\mathbf{x},p}$ , we conduct experiments with different values of $\\lambda_{\\mathrm{cond}}$ and show the test error in Table 3. ", "page_idx": 18}, {"type": "text", "text": "From Table 3 we can observe when $\\lambda_{\\mathrm{cond}}$ increases from 0 to $10^{-6}$ , the test error gradually decrease. When $\\lambda_{\\mathrm{cond}}$ further increases from $10^{-6}$ to $10^{-2}$ , the test error gradually decrease. Among all the $\\lambda_{\\mathrm{cond}}$ that we tried, the test error has the minimal value when $\\bar{\\lambda_{\\mathrm{cond}}}=10^{\\bar{-6}}$ . ", "page_idx": 18}, {"type": "text", "text": "Theoretically, if $\\lambda_{\\mathrm{cond}}$ is too low, since $\\mathcal{L}_{\\mathrm{AE}}~=~\\mathcal{L}_{\\mathrm{rec}}\\,+\\,\\lambda_{\\mathrm{cond}}\\mathcal{L}_{\\mathrm{cond}}$ , there may not have enough constraint on ${\\mathcal{L}}_{\\mathrm{cond}}$ and the condition number of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ may be very large. By Theorem 1, ", "page_idx": 18}, {"type": "image", "img_path": "cjH0Qsgd0D/tmp/144883a520c1085478732ff6a863c5b952f9cd779d82dff3c82b938114db3c49.jpg", "img_caption": ["Figure 8: Loss curve of the $\\mathcal{L}_{\\mathbf{z}},\\mathcal{L}_{\\mathbf{x}},\\mathcal{L}_{\\mathbf{x},p}(p=1/4)$ on the Predator-Prey system. Models are trained with different loss functions $\\mathcal{L}_{\\mathbf{z}},\\mathcal{L}_{\\mathbf{x}},\\mathcal{L}_{\\mathbf{x},p}(p=1/4)$ on the same number of training data. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 3: Results on the Predator-Prey system. We train the autoencoder with different $\\lambda_{\\mathrm{cond}}$ and then train the macroscopic dynamics model with loss $\\mathcal{L}_{\\mathbf{x},p}(p=1/5)$ . The mean relative error of the macroscopic dynamics model is reported over three repeats. ", "page_idx": 19}, {"type": "table", "img_path": "cjH0Qsgd0D/tmp/a4f26f09fe633caeb8a574b4936e6fb3749ce33989e6c156026b1947e94d5263.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "a small condition number of $\\varphi^{\\prime}(\\mathbf{x})\\varphi^{\\prime}(\\mathbf{x})^{T}$ can guarantee the effectiveness of ${\\mathcal{L}}_{\\mathbf{x}}$ . But when the condition number is large, there is no guarantee that ${\\mathcal{L}}_{\\mathbf{x}}$ thus $\\mathcal{L}_{\\mathbf{x},p}$ can perform well. If instead $\\lambda_{\\mathrm{cond}}$ is too large, $\\mathcal{L}_{\\mathrm{AE}}$ will be dominated by ${\\mathcal{L}}_{\\mathrm{cond}}$ .Then the autoencoder may not reconstruct the microscopic dynamics well and, hence may not capture the closure terms well. If the latent space is not closed enough, we can not learn the macroscopic dynamics well. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claim clearly reflect our contributions and scope both in theory and experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: In Section 6 we discuss the limitations. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Full set of assumptions and complete proof of our theoretical results are provided in Appendix A. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We explain our method in details in Section 4.2, Algorithm 1 and Algorithm 2. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our code is available at https://github.com/MLDS-NUS/ Learn-Partial ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Details of the experiment setting is provided in Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We report the error bar for all the experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We mention in Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our research conform with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: no societal impacts ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: no such risks ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Existing assets are properly cited. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The documentation is provided. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research is not involved in such subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research is not involved in such subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]