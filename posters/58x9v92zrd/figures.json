[{"figure_path": "58X9v92zRd/figures/figures_0_1.jpg", "caption": "Figure 1: Ctrl-G pipeline; both the LLM and the HMM are frozen once trained.", "description": "The figure illustrates the Ctrl-G pipeline, a three-step process. First, it distills a Hidden Markov Model (HMM) from a given Large Language Model (LLM). Second, it specifies the logical constraints using a Deterministic Finite Automaton (DFA).  Finally, during inference, the HMM guides the LLM to generate outputs that satisfy the constraints specified in the DFA.  The LLM and HMM are both considered fixed after training.", "section": "1 Introduction"}, {"figure_path": "58X9v92zRd/figures/figures_1_1.jpg", "caption": "Figure 1: Ctrl-G pipeline; both the LLM and the HMM are frozen once trained.", "description": "This figure illustrates the three main steps of the Ctrl-G pipeline for controlling LLM generation to satisfy logical constraints.  First, the LLM is distilled into a Hidden Markov Model (HMM). Second, the logical constraints are specified using a deterministic finite automaton (DFA). Finally, during inference, the HMM guides the LLM's generation to adhere to the DFA-specified constraints, resulting in outputs that satisfy the constraints.  Both the LLM and the HMM are trained beforehand and are frozen during inference.", "section": "1 Introduction"}, {"figure_path": "58X9v92zRd/figures/figures_3_1.jpg", "caption": "Figure 3: Example of a DFA representing the logical constraint that the phrase \"gets cold\" must appear in the generated text along with pseudo-code for representing this DFA in Ctrl-G.", "description": "This figure shows an example of a Deterministic Finite Automaton (DFA) used to represent a logical constraint in the Ctrl-G framework. The DFA in (a) is a graph representation of the automaton, illustrating its states (A, B, C), transitions (edges with conditions on words like \"gets\", \"cold\", etc.), the initial state (A), and accepting state (C).  (b) demonstrates the DFA's behavior with sample strings. The string \"the weather gets cold in winter\" is accepted because it contains the phrase \"gets cold\", while \"the weather gets warm in winter\" is rejected, as it lacks the specific phrase. (c) provides Python code illustrating how to specify this DFA in the Ctrl-G system, showing the structure as a dictionary including transitions, initial and accepting states.", "section": "3.1 Logical constraints as DFAS"}, {"figure_path": "58X9v92zRd/figures/figures_6_1.jpg", "caption": "Figure 1: Ctrl-G pipeline; both the LLM and the HMM are frozen once trained.", "description": "This figure illustrates the three main steps of the Ctrl-G pipeline for controllable text generation.  First, an LLM (Large Language Model) is combined with a Hidden Markov Model (HMM) which is a white-box approximation of the LLM.  This HMM guides the LLM's output to meet specified constraints. These constraints are defined by a Deterministic Finite Automaton (DFA). The DFA is then used during the inference phase to guide the LLM's generation towards outputs that satisfy the given logical constraints.  Once the LLM and HMM are trained, they remain frozen during inference. ", "section": "1 Introduction"}, {"figure_path": "58X9v92zRd/figures/figures_9_1.jpg", "caption": "Figure 5: Runtime analysis of Ctrl-G; Left: the generation time per token scales linearly w/ respect to DFA size. Right: the generation time per token stays constant w/ respect to sequence length.", "description": "This figure presents the results of a runtime analysis of the Ctrl-G model.  The left panel shows a linear relationship between the generation time per token and the size of the Deterministic Finite Automaton (DFA) used to represent logical constraints. This suggests that the computational cost of constraint enforcement in Ctrl-G increases proportionally with the complexity of the constraints. The right panel demonstrates that the generation time per token remains relatively constant regardless of sequence length. This indicates that the overhead introduced by Ctrl-G does not scale significantly with the length of the text being generated.", "section": "5 Scaling up Ctrl-G for interactive text editing"}, {"figure_path": "58X9v92zRd/figures/figures_15_1.jpg", "caption": "Figure 3: Example of a DFA representing the logical constraint that the phrase \"gets cold\" must appear in the generated text along with pseudo-code for representing this DFA in Ctrl-G.", "description": "This figure illustrates an example of a Deterministic Finite Automaton (DFA) used to represent logical constraints in the Ctrl-G framework.  The DFA shown is designed to accept strings containing the phrase \"gets cold.\" The figure depicts the DFA as a graph with states (A, B, C) and transitions, clearly showing how the DFA processes input strings to determine if the constraint (\"gets cold\") is satisfied.  Pseudo-code is included to demonstrate how such a DFA can be specified within the Ctrl-G system. This is a key component of how Ctrl-G translates logical constraints into a form usable by the HMM and LLM for controlled text generation.", "section": "3.1 Logical constraints as DFAS"}]