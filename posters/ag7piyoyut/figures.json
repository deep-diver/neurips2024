[{"figure_path": "ag7piyoyut/figures/figures_3_1.jpg", "caption": "Figure 1: (a) Illustration of surrogate sharpness; (b) Illustration of surrogate sharpness-based offline optimization: Consider two surrogate parameters \u03c9\u2081 and \u03c9\u2082 where \u03c9\u2081 has a smaller sharpness than \u03c9\u2082. This means the predictions of the models in the perturbation neighborhood of \u03c9\u2081 will vary less than those of the models in the perturbation neighborhood of \u03c9\u2082. As such, if both neighborhoods contain the oracle, the prediction error d\u2081 of \u03c9\u2081 is potentially smaller than the prediction error d\u2082 of \u03c9\u2082. Consequently, the optimal value of g(x; \u03c9\u2081) is closer to the oracle optimal value than g(x; \u03c9\u2082)'s.", "description": "This figure illustrates the concept of surrogate sharpness and how it can improve offline optimization.  Panel (a) shows that a sharper surrogate model (\u03c9\u2082) has a larger difference in prediction values between the model and the model with parameter perturbation, than a less sharp surrogate model (\u03c9\u2081). Panel (b) demonstrates that by using a less sharp surrogate (\u03c9\u2081), the optimizer is guided toward a solution closer to the actual optimum.", "section": "Surrogate Regularization with Sharpness Constraint"}, {"figure_path": "ag7piyoyut/figures/figures_8_1.jpg", "caption": "Figure 2: The percentage improvement in performance achieved by IGNITE across different algorithms (COMS and GA) and tasks (ANT and TF10) in the changes of (a) threshold \u03b5 and (b) step size \u03b7\u03bb.", "description": "This figure shows the effect of hyperparameters epsilon (\u03b5) and eta lambda (\u03b7\u03bb) on the performance improvement achieved by the proposed method IGNITE.  The graphs illustrate how changes in these hyperparameters affect different algorithms (COMS and GA) and tasks (ANT and TF-Bind-10). This visualization helps in determining optimal values for \u03b5 and \u03b7\u03bb, which balance regularization and performance.", "section": "5.3 Ablation Experiments"}, {"figure_path": "ag7piyoyut/figures/figures_22_1.jpg", "caption": "Figure 3: Performance vs. the no. of gradient ascent steps during optimization of IGNITE-2 and Baseline optimized algorithms, e.g, COMs and GA.", "description": "This figure compares the performance of the COMs and GA algorithms with and without the IGNITE-2 regularizer across different gradient ascent steps.  It shows that IGNITE-2 improves the stability and performance of both algorithms, particularly in later optimization stages, on the ANT task.  The results are less clear on the TF-BIND-10 task, but still show that adding the IGNITE-2 regularizer does not degrade performance.", "section": "G.4.1 IGNITE-2 enhances stability of COMs and gradient ascent (GA)"}, {"figure_path": "ag7piyoyut/figures/figures_22_2.jpg", "caption": "Figure 3: Performance vs. the no. of gradient ascent steps during optimization of IGNITE-2 and Baseline optimized algorithms, e.g, COMs and GA.", "description": "This figure shows the performance (normalized objective value or binding affinity) of two optimization algorithms (COMs and GA) with and without the IGNITE-2 regularizer over a certain number of gradient ascent steps.  It illustrates the impact of the IGNITE-2 regularizer on the convergence and stability of the optimization process for two different tasks (ANT and TF-BIND-10). The plots show how the performance changes over time, comparing the baseline algorithms' performance to that of algorithms that incorporate IGNITE-2. In some tasks, IGNITE-2 improves the optimization performance at later steps, while in others it may show performance improvements throughout the optimization process. ", "section": "G.4.1 IGNITE-2 enhances stability of COMs and gradient ascent (GA)"}, {"figure_path": "ag7piyoyut/figures/figures_22_3.jpg", "caption": "Figure 2: The percentage improvement in performance achieved by IGNITE across different algorithms (COMS and GA) and tasks (ANT and TF10) in the changes of (a) threshold \u03b5 and (b) step size \u03b7\u03bb.", "description": "This figure displays the impact of hyperparameter tuning on the performance improvement achieved by IGNITE across different algorithms and tasks.  Specifically, it shows how changing the threshold (\u03b5) and step size (\u03b7\u03bb) in the proposed IGNITE method affects performance gains for both COMS and GA algorithms on the ANT and TF-BIND-10 tasks. The plots illustrate the sensitivity of the performance improvement to variations in these hyperparameters, helping to determine optimal values for effective regularization.", "section": "5.3 Ablation Experiments"}, {"figure_path": "ag7piyoyut/figures/figures_22_4.jpg", "caption": "Figure 2: The percentage improvement in performance achieved by IGNITE across different algorithms (COMS and GA) and tasks (ANT and TF10) in the changes of (a) threshold \u03b5 and (b) step size \u03b7\u03bb.", "description": "This figure demonstrates the impact of hyperparameters epsilon (\u03b5) and eta lambda (\u03b7\u03bb) on the performance improvement achieved by the IGNITE method.  Two baseline algorithms, COMS and GA, are tested across two different tasks, ANT and TF-BIND-10.  The plots show how changes to \u03b5 (threshold) and \u03b7\u03bb (step size) affect the percentage improvement IGNITE provides over the baseline algorithms for each task.  The results suggest that there's an optimal range for these hyperparameters, outside of which the performance gain decreases or even becomes negative.", "section": "5.3 Ablation Experiments"}, {"figure_path": "ag7piyoyut/figures/figures_24_1.jpg", "caption": "Figure 1: (a) Illustration of surrogate sharpness; (b) Illustration of surrogate sharpness-based offline optimization: Consider two surrogate parameters w\u2081 and w\u2082 where w\u2081 has a smaller sharpness than w\u2082. This means the predictions of the models in the perturbation neighborhood of w\u2081 will vary less than those of the models in the perturbation neighborhood of w\u2082. As such, if both neighborhoods contain the oracle, the prediction error d\u2081 of w\u2081 is potentially smaller than the prediction error d\u2082 of w\u2082. Consequently, the optimal value of g(x; w\u2081) is closer to the oracle optimal value than g(x; w\u2082)'s.", "description": "This figure illustrates the concept of surrogate sharpness and how it can improve offline optimization.  Panel (a) shows that a surrogate model with lower sharpness (smaller change in prediction within a parameter neighborhood) will have predictions closer to the true optimal value if the true optimal parameters are within the neighborhood. Panel (b) demonstrates that using a sharpness-based regularizer during training leads to better predictions by selecting models whose predictions are less sensitive to small parameter perturbations, thus reducing the impact of out-of-distribution inputs.", "section": "Surrogate Regularization with Sharpness Constraint"}]