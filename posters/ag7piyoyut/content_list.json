[{"type": "text", "text": "Incorporating Surrogate Gradient Norm to Improve Offline Optimization Techniques ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Manh Cuong Dao, Phi Le Nguyen Hanoi University of Science and Technology Cuong.DM242249M@sis.hust.edu.vn,lenp@soict.hust.edu.vn ", "page_idx": 0}, {"type": "text", "text": "Thao Nguyen Truong National Institute of Advanced Industrial Science and Technology nguyen.truong@aist.go.jp ", "page_idx": 0}, {"type": "text", "text": "Trong Nghia Hoang\u2217 Washington State University trongnghia.hoang@wsu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline optimization has recently emerged as an increasingly popular approach to mitigate the prohibitively expensive cost of online experimentation. The key idea is to learn a surrogate of the black-box function that underlines the target experiment using a static (offilne) dataset of its previous input-output queries. Such an approach is, however, fraught with an out-of-distribution issue where the learned surrogate becomes inaccurate outside the offline data regimes. To mitigate this, existing offline optimizers have proposed numerous conditioning techniques to prevent the learned surrogate from being too erratic. Nonetheless, such conditioning strategies are often specific to particular surrogate or search models, which might not generalize to a different model choice. This motivates us to develop a modelagnostic approach instead, which incorporates a notion of model sharpness into the training loss of the surrogate as a regularizer. Our approach is supported by a new theoretical analysis demonstrating that reducing surrogate sharpness on the offline dataset provably reduces its generalized sharpness on unseen data. Our analysis extends existing theories from bounding generalized prediction loss (on unseen data) with loss sharpness to bounding the worst-case generalized surrogate sharpness with its empirical estimate on training data, providing a new perspective on sharpness regularization. Our extensive experimentation on a diverse range of optimization tasks also shows that reducing surrogate sharpness often leads to significant improvement, marking (up to) a noticeable $9.6\\%$ performance boost. Our code is publicly available at https://github.com/cuong-dm/IGNITE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A central task in numerous scientific disciplines is to optimize for some material configuration that maximizes a certain utility metric. Previously, this would incur an expensive and repetitive experiment process that requires a huge amount of human-labor. To bypass such inefficiencies, a data-driven approach [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] has recently been adopted. Instead of laboring on a new set of expensive on-demand experimentation for each new optimization task, we can leverage past experimentation data to build a parametric model that predicts the outcome of the experiments themselves. Its parameters are tuned to fti past experimental data and then fixed while optimizing for the best input. ", "page_idx": 0}, {"type": "text", "text": "However, in most applications, offline data is rarely representative of the entire input space. As a result, the surrogate model\u2019s prediction is often not accurate outside the offline data regime, potentially overestimating the outputs at sub-optimal input candidates. To mitigate this, existing offline optimizers have introduced numerous regularizing strategies for either the surrogate or the search models. For example, [4, 11, 12, 13] and [3] regularize their surrogate models so that their predictions at inputs outside the offilne data regime are pushed down or pushed towards a constant value. Alternatively, [1] and [2] restrict their search models to regions having certain domain-specific properties under which sampled inputs probably have high performance. ", "page_idx": 1}, {"type": "text", "text": "These strategies therefore depend on the specifics of either the surrogate- or the search procedures to characterize the out-of-distribution regions or the desirable domain-specific properties. Consequently, their regularization might not extend to out-of-distribution regions which are not sufficiently specified. For example, COMs [13] uses an ad-hoc specification that characterizes the out-of-distribution region in terms of inputs that are reached during the first few iterations of gradient ascent on an un-regularized surrogate. This only characterizes a local sub-region of a broader out-of-distribution data regime. ", "page_idx": 1}, {"type": "text", "text": "To mitigate such limitations in the regularizing behaviors of prior work, we instead aim to investigate a more generic approach that is independent of the specifics of the search or surrogate procedures. For instance, instead of regularizing the behavior of the model on certain input regions, we could impose constraints on the geometries of its loss landscape such as requiring it to be in a parameter regime that uniformly produces low loss values [14]. Such regularizing strategies can, therefore, be incorporated into existing offilne optimizers as an additional regularizer to improve their performance. To substantiate this idea, we have made the following technical contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We develop a model-agnostic regularizer (for surrogate training) based on a notion of model sharpness2. This is characterized in terms of the surrogate\u2019s maximum output change under lowenergy parameter perturbation (i.e., norm-bound perturbation) (Section 3.1). Intuitively, suppose a surrogate\u2019s prediction does not change substantially within a perturbation neighborhood (i.e., via adding perturbation to its parameters) that contains the oracle, its predictions are likely to be close to those of the oracle (see Fig. 1). As such, minimizing this sharpness measure can help suppress the erratic behavior of the surrogate model at out-of-distribution input. ", "page_idx": 1}, {"type": "text", "text": "2. We adopt a practical approximation that interestingly reduces the above surrogate sharpness measurement to a function of the surrogate\u2019s gradient norm. The surrogate training can then be augmented into a constrained optimization task whose constraint imposes a user-specified threshold on the surrogate\u2019s gradient norm. We can then solve it to acquire the optimally regularized surrogate using existing constrained optimization solvers. The high-level pseudo-code of our proposed algorithms which incorporate surrogate gradient norms to improve existing offline optimization techniques (IGNITE) is detailed in Algorithm 1 (Section 3.2). ", "page_idx": 1}, {"type": "text", "text": "3. We develop a detailed theoretical analysis to show that reducing surrogate sharpness on the offilne dataset provably reduces its generalized sharpness on unseen data. Our analysis extends existing theories [14] from bounding generalized prediction loss (on unseen data) with loss sharpness to bounding the worst-case generalized surrogate sharpness with its empirical estimate on training data, providing a new perspective on sharpness regularization (Section 4). ", "page_idx": 1}, {"type": "text", "text": "4. We demonstrate empirically that incorporating the proposed model-agnostic regularizer into existing offline optimizers as an additional conditioning component often results in significant improvement over existing offline optimizers in most cases. This sets the first step towards a new, synergistic research direction in offline optimization that can potentially support and complement both existing and future work (Section 5). ", "page_idx": 1}, {"type": "text", "text": "For interested readers, a concise review of existing literature is also provided in Section 2. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Definition and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we will concisely review the preliminaries of offilne optimization. Section 2.1 provides a mathematical formulation of offline optimization, and Section 2.2 summarizes prior works. ", "page_idx": 1}, {"type": "text", "text": "2.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Offline optimization is a computational approach to a variety of material engineering tasks that aim to find a material construction or design that maximizes certain desirable properties. Mathematically, we assume that there is an oracle function $g\\mathbf{(x)}$ that maps from a material design $\\mathbf{x}\\in\\mathcal{X}$ to an overall utility $z=g(\\mathbf{x})$ of its property measurements; and we need to find its maxima: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbf{x}_{*}}&{{}\\triangleq}&{\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\arg\\operatorname*{max}}\\ g(\\mathbf{x})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, the key challenge here is that $g\\mathbf{(x)}$ is inaccessible. Instead, we only have access to an offilne dataset of observations $\\bar{\\boldsymbol{D}}=\\{(\\mathbf{x}_{i},z_{i})\\}_{i=1}^{\\dot{n}}$ where $z_{i}=g(\\mathbf{x}_{i})$ , which denote the past input-output queries extracted from $g\\mathbf{(x)}$ in previous experiments. A direct approach to this problem is to learn a surrogate $g(\\mathbf{x};\\omega_{*})$ of $g\\mathbf{(x)}$ via fitting its parameter $\\omega_{*}$ to the offline dataset, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\omega_{*}\\quad\\triangleq\\quad\\arg\\operatorname*{min}_{\\omega}\\mathcal{L}_{\\mathcal{D}}(\\omega)\\ \\triangleq\\ \\arg\\operatorname*{min}_{\\omega}\\sum_{i=1}^{n}\\ell\\Big(g(\\mathbf{x}_{i};\\omega),\\ z_{i}\\Big)\\ ,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\omega$ denotes a parameter candidate of the surrogate and $\\ell(g(\\mathbf{x};\\omega),z)$ denotes the prediction loss of $g(.;\\omega)$ on $\\mathbf{x}$ if its oracle output is $z$ . The (oracle) maxima of $g\\mathbf{(x)}$ is then approximated via, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{{\\mathbf{x}}_{*}}&{{}\\triangleq}&{\\underset{{\\mathbf{x}}\\in\\mathcal{X}}{\\arg\\operatorname*{max}}\\ \\ g({\\mathbf{x}};\\omega_{*})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Suppose the surrogate $g(\\mathbf{x};\\omega_{*})$ \u2019s prediction is sufficiently accurate over the entire input space, solving Eq. (3) is all we need. However, in most cases, $g(\\bar{\\mathbf{x}};\\omega_{*})$ often predicts erratically outside the offilne data regime, which in turn misleads the optimization towards sub-optimal candidates. To mitigate this, numerous surrogate or search regularizers have been proposed, as summarized next. ", "page_idx": 2}, {"type": "text", "text": "2.2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Most existing offline optimization methods have focused on regularizing either the search or the (surrogate) training procedures. The main focus is to either (1) avoid exploring input regions where the surrogate\u2019s prediction is not reliable or (2) regularize the prediction behavior of the surrogate at out-of-distribution input regimes. For example, to regularize the surrogate\u2019s prediction, [13] uses input candidates found during the first few iterations of gradient updates on un-regularized models to characterize the out-of-distribution regime. The surrogate can then be re-trained with an additional regularizer that penalizes high-value predictions at those sampled input candidates. ", "page_idx": 2}, {"type": "text", "text": "Alternatively, [11] maximizes the normalized data likelihood to reduce prediction uncertainty, which also helps suppress erratic prediction at out-of-distribution regime. Existing techniques in model pre-training and adaptation [3] or transfer learning via co-teaching [15] can also be leveraged to enforce criteria of local smoothness that encodes a preference for conservative prediction to avoid overestimating the oracle output. Otherwise, to regularize the search procedure, [1] and [2] focus instead on learning a generative model of input candidate conditioned their oracle performance. Input candidates that likely achieve high performance can then be synthesized via conditioning the generative procedure on high-value oracle output. [1] characterizes such conditioned distribution via an adversarial zero-sum game. [12] learns a direct inverse mapping from the performance output to the input design using conditional generative adversarial network [16]. ", "page_idx": 2}, {"type": "text", "text": "Despite their reported successes, these approaches are still limited by their ad-hoc characterization of the out-of-distribution regime. As discussed previously in Section 1, the existing characterization of out-of-distribution input is often based on the specifics of either the surrogate or the search procedures, which are not guaranteed to sufficiently characterize the entire out-of-distribution data regime. This motivates us to develop a more generic out-of-distribution characterization (Section 3) that is external to both the search and surrogate models. Such an approach can be readily incorporated into most existing offline optimizers to boost their performance (Section 5). ", "page_idx": 2}, {"type": "text", "text": "3 Surrogate Regularization with Sharpness Constraint ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section introduces an additional constraint that transforms the original surrogate optimization in Eq. (2) (Section 3.1) into a constrained optimization problem (COP). The constraint imposes a user-specified upper-bound on the sharpness of the surrogate. Our proposed formulation draws inspiration from a prior work [14] that aims to minimize the sharpness of the loss function to improve generalization. However, unlike the original work in [14], where the sharpness concept is applied to the loss function, we adapt it for the surrogate\u2019s prediction. We find it more suitable since offline optimization\u2019s search procedure operates on the surrogate landscape instead of the loss landscape. Moreover, while minimizing loss sharpness [14] can ensure low error for single predictions in the OOD regime, errors may accumulate over consecutive predictions in a gradient-based search. Our insight in Section 3.1 (Fig. 1) suggests that such error accumulation can be mitigated by keeping the surrogate sharpness small during training. Furthermore, we show that the sharpness can be practically approximated in terms of the surrogate\u2019s gradient norm, which is more tractable. This allows for direct adoption of existing constrained optimization algorithms [17] to effectively solve for the desired optimally regularized surrogate (Section 3.2). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "ag7piyoyut/tmp/e0c095069ff29f12bb04b2b5a0352af1935645ebf25c1e90a97074777bcee956.jpg", "img_caption": ["Figure 1: (a) Illustration of surrogate sharpness; (b) Illustration of surrogate sharpness-based offilne optimization: Consider two surrogate parameters $\\omega_{1}$ and $\\omega_{2}$ where $\\omega_{1}$ has a smaller sharpness than $\\omega_{2}$ . This means the predictions of the models in the perturbation neighborhood of $\\omega_{1}$ will vary less than those of the models in the perturbation neighborhood of $\\omega_{2}$ . As such, if both neighborhoods contain the oracle, the prediction error $d_{1}$ of $\\omega_{1}$ is potentially smaller than the prediction error $d_{2}$ of $\\omega_{2}$ . Consequently, the optimal value of $g(\\mathbf{x};\\omega_{1})$ is closer to the oracle optimal value than $g(\\mathbf{x};\\omega_{2})^{\\circ}\\mathrm{s}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Surrogate Sharpness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Suppose the oracle lies within the parametric family of the surrogate, there must exist a perturbation neighborhood of the surrogate\u2019s parameters that contains the oracle. That is, the oracle can be obtained by adding to the surrogate\u2019s parameters a noise vector in this neighborhood. Now, suppose the predictions do not change substantially across the models (including both the oracle and surrogate) in the perturbation neighborhood; the surrogate\u2019s predictions (and optimizers) must be close to those of the oracle (see Fig. 1). Motivated by this insight, we consider a potential approach to mitigate the erratic prediction of the surrogate, which is to ensure that its worst-case prediction change across the perturbation neighborhood is sufficiently small. This is formalized below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{R}_{\\mathcal{X}}(\\omega)}&{\\triangleq}&{\\underset{\\|\\delta\\|_{2}}{\\operatorname*{max}}_{\\substack{\\sigma}}\\left|\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathbb{E}}\\left[g(\\mathbf{x};\\omega+\\delta)\\right]-\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathbb{E}}\\left[g(\\mathbf{x};\\omega)\\right]\\right|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Vert\\cdot\\Vert_{2}$ denotes the $\\ell_{2}$ -norm, and $\\rho>0$ bounds the maximum norm of the perturbation, which defines the perturbation neighborhood and can be selected via hyper-parameter tuning (see Section 5.1 and Appendix G.4). To ease the notation, we will omit the subscript and use $\\lVert.\\rVert$ to consistently denote the $\\ell_{2}$ norm in the rest of this manuscript. We will also refer to $\\mathcal{R}_{\\mathcal{X}}(\\omega)$ in Eq. (4) as the generalized surrogate sharpness, which can be used to regularize surrogate training, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\omega_{*}\\quad\\triangleq\\quad\\underset{\\omega}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{\\mathcal{D}}(\\omega)\\quad\\mathrm{s.t.}\\quad\\mathcal{R}_{\\mathcal{X}}(\\omega)\\ \\leq\\ \\epsilon^{\\prime}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon^{\\prime}$ is a user-specified threshold. Solving Eq. (5) is non-trivial since $\\mathcal{R}_{\\mathcal{X}}(\\omega)$ is neither tractable nor differentiable. To sidestep this, we leverage a theoretical result that will be established later in Section 4, which (informally) states that with high confidence, the generalized surrogate sharpness ", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{R}_{\\mathcal{X}}(\\omega)$ can be approximated with its empirical estimate $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ in Eq. (16), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{R}_{\\mathcal{X}}(\\omega)}&{\\leq}&{\\left(\\rho G(\\omega)+\\frac{1}{2}\\rho^{2}\\lambda_{\\mathrm{max}}\\right)\\cdot\\left(\\mathcal{R}_{\\mathcal{D}}(\\omega)\\,+\\,\\mathcal{O}\\left(\\sqrt{\\frac{\\dim(\\omega)\\log{(n\\|\\omega\\|^{2})}}{n}}\\right)\\right)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $G(\\omega)$ and $\\lambda_{\\mathrm{max}}$ denote the norm of the expected (parameter) gradient of the surrogate at $\\omega$ , and the largest eigenvalue of the (parameter) Hessian of the surrogate\u2019s expected prediction, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{G(\\omega)}&{\\triangleq}&{\\left\\|\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathbb{E}}\\left[\\nabla_{\\omega}g(\\mathbf{x};\\omega)\\right]\\right\\|\\quad\\mathrm{and}\\quad\\lambda_{\\operatorname*{max}}\\;\\triangleq\\;\\underset{\\omega}{\\operatorname*{max}}\\,\\lambda_{\\operatorname*{max}}\\left(\\nabla_{\\omega}^{2}\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathbb{E}}\\left[g(\\mathbf{x};\\omega)\\right]\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When $n$ is sufficiently large, $(\\mathrm{dim}(\\omega)\\log(n\\|\\omega\\|^{2})/n)^{\\frac{1}{2}}$ will be negligibly small. Then, suppose within the search region for $\\omega$ (see Assumption 2), $G(\\omega)$ is bounded by a constant $G$ , we can enforce $\\mathcal{R}_{\\mathcal{X}}(\\omega)\\leq\\epsilon^{\\prime}=(\\rho\\bar{G}+(1/2)\\rho^{2}\\lambda_{\\operatorname*{max}})\\epsilon$ via constraining instead $\\mathcal{R}_{\\mathcal{D}}(\\omega)\\leq\\epsilon$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega_{*}\\,\\,\\,\\triangleq\\,\\,\\,\\operatorname{arg\\,min}_{\\omega}\\mathcal{L}_{\\mathcal{D}}(\\omega)\\quad\\mathrm{s.t.}\\quad\\mathcal{R}_{\\mathcal{D}}(\\omega)\\,\\,\\,\\leq\\,\\,\\epsilon\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To mitigate the non-differentiability of $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ , we further propose a practical approximation which reduces $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ to a function of the surrogate\u2019s gradient norm $\\|\\nabla_{\\omega}g(\\mathbf{x};\\omega)\\|$ , which is differentiable, allowing the above COP to be solved effectively with existing constrained optimization algorithms. This is discussed in the next section. ", "page_idx": 4}, {"type": "text", "text": "Remark. Eq. (4) is similar in spirit to the notion of loss sharpness [14] in which the same perturbation model is used to characterize the sharpness or sensitivity of the loss function, $|\\mathcal{L}_{\\mathcal{D}}(\\omega+\\delta\\bar{)}-\\mathcal{L}_{\\mathcal{D}}(\\omega)|$ , to small changes $||\\pmb{\\delta}||\\leq\\rho$ in the model weights $\\omega$ . Our work, however, sets a direct focus on the sensitivity or sharpness of the surrogate\u2019s prediction \u2013 Eq. (4) \u2013 which results in a better surrogate regularizer, leading to better empirical performance (see Section 5.3). It also requires a significant and non-trivial adaptation of the theories presented in [14], as detailed later in Section 4. ", "page_idx": 4}, {"type": "text", "text": "3.2 Practical Algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $h(\\omega+\\delta)\\triangleq\\mathbb{E}_{\\mathbf{x}\\in\\mathcal{D}}[g(\\mathbf{x};\\omega+\\delta)]$ and $h(\\omega)\\triangleq\\mathbb{E}_{\\mathbf{x}\\in\\mathcal{D}}[g(\\mathbf{x};\\omega)]$ . The surrogate sharpness can be approximated via the first-order Taylor expansion of $h(\\omega+\\delta)$ at $\\omega$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{R}_{\\mathcal{D}}(\\omega)}&{=}&{\\underset{\\|\\delta\\|_{2}\\leq\\rho}{\\operatorname*{max}}\\left|\\underset{\\mathbf{x}\\in\\mathcal{D}}{\\mathbb{E}}\\left[g(\\mathbf{x};\\omega+\\delta)\\right]-\\underset{\\mathbf{x}\\in\\mathcal{D}}{\\mathbb{E}}\\big[g(\\mathbf{x};\\omega)\\big]\\right|}\\\\ &{=}&{\\underset{\\|\\delta\\|_{2}\\leq\\rho}{\\operatorname*{max}}\\bigg|h(\\omega+\\delta)-h(\\omega)\\bigg|}&{\\simeq\\underset{\\|\\delta\\|_{2}\\leq\\rho}{\\operatorname*{max}}\\bigg|\\nabla_{\\omega}h(\\omega)^{\\top}\\delta\\bigg|\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using the Cauchy-Schwartz inequality on the right-hand side of the above and noting that $\\delta$ can be selected to make the equality happens, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathcal{D}}(\\omega)~\\simeq~\\operatorname*{max}_{\\|\\delta\\|_{2}\\leq\\rho}\\left|\\nabla_{\\omega}h(\\omega)^{\\top}\\delta\\right|~~=~~\\operatorname*{max}_{\\|\\delta\\|_{2}\\leq\\rho}\\left\\|\\nabla_{\\omega}h(\\omega)\\right\\|\\times\\left\\|\\delta\\right\\|~=~\\rho\\times\\left\\|\\nabla_{\\omega}h(\\omega)\\right\\|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using the above approximation, the COP in Eq. (8) can be rewritten as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\omega_{*}\\quad\\triangleq\\quad\\underset{\\omega}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{\\mathcal{D}}(\\omega)\\quad\\mathrm{s.t.}\\quad\\rho\\cdot\\|\\nabla_{\\omega}h(\\omega)\\|\\ \\leq\\ \\epsilon\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which can be solved via optimizing the corresponding Lagrangian, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\omega_{*}}&{=}&{\\underset{\\omega}{\\mathrm{arg\\,min}}\\,\\mathcal{L}(\\omega,\\lambda)\\;\\mathrm{~where~}\\;\\mathcal{L}(\\omega,\\lambda)\\;\\;\\triangleq\\;\\;\\mathcal{L}_{\\mathcal{D}}(\\omega)\\;\\;+\\;\\;\\lambda\\cdot\\left(\\rho\\cdot\\left\\|\\nabla_{\\omega}h(\\omega)\\right\\|-\\epsilon\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\lambda>0$ denotes the Lagrange multiplier. This can be solved via our approach below. ", "page_idx": 4}, {"type": "text", "text": "IGNITE. We can optimize for both $\\lambda$ and $\\omega$ using the basic differential multiplier method (BDMM) [17], which simultaneously gradient ascent for $\\lambda$ and gradient descent for $\\omega$ , resulting in the following update rules: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\omega^{t+1}}&{=}&{\\omega^{t}\\ -\\ \\eta_{\\omega}\\cdot\\left(\\nabla_{\\omega}\\mathcal{L}_{\\mathcal{D}}\\left(\\omega^{t}\\right)\\ +\\ \\lambda^{t}\\cdot\\rho\\cdot\\nabla_{\\omega}\\Big\\|\\nabla_{\\omega}h\\left(\\omega^{t}\\right)\\Big\\|\\right)\\,,}\\\\ {\\lambda^{t+1}}&{=}&{\\lambda^{t}\\ +\\ \\eta_{\\lambda}\\cdot\\left(\\rho\\cdot\\left\\|\\nabla_{\\omega}h\\left(\\omega^{t}\\right)\\Big\\|-\\epsilon\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Input: offilne data $\\mathbf{\\mathcal{D}}=\\{(\\mathbf{x}_{i},z_{i})\\}_{i=1}^{n}$ ; initial surrogate $g(\\mathbf{x};\\omega^{(0)})$ ; no. of iterations $T$ ; batch size $m$ ; Lagrange multiplier $\\lambda$ ; perturbation radius $\\rho$ and scalar $r$ ; step sizes $\\eta_{\\omega}$ and $\\eta_{\\lambda}$ ; threshold $\\epsilon$ . ", "page_idx": 5}, {"type": "table", "img_path": "ag7piyoyut/tmp/61a9367852da7262be6c4bfd3585a451d146a298828ce3dd4d21f47451615afa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $\\omega^{t}$ represents the surrogate\u2019s parameter estimate at iteration $t$ , $\\lambda^{t}$ is the Lagrange multiplier estimate at iteration $t$ , $\\eta_{\\omega}$ is the step size for updating $\\omega$ , and $\\eta_{\\lambda}$ is the step size for updating $\\lambda$ . We name this method IGNITE. We also conduct grid search to select the optimal value for $\\rho$ , as mentioned in Section 5.1. ", "page_idx": 5}, {"type": "text", "text": "Remark. As an alternative approach, we can also treat $\\lambda$ as a hyper-parameter and optimize for $\\omega$ using gradient descent; we call this method IGNITE-2 . The detailed algorithms, hyper-parameter selection, and experimental results of IGNITE-2 are reported in Appendix G.2. ", "page_idx": 5}, {"type": "text", "text": "Both of the above methods require differentiating $\\|\\nabla_{\\omega}h(\\omega)\\|$ with respect to $\\omega$ , which involves computing the expensive Hessian of $h(\\omega)$ . Fortunately, this expensive computation can be avoided by using the gradient approximation technique detailed in Appendix F. To summarize, we provide a complete pseudo-code of IGNITE in Algorithm 1 whose steps 3-8 implement the approximation in Eq. (84) and Eq. (85) of Appendix F. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we will provide a detailed theoretical analysis to substantiate our earlier (informal) statement in Eq. (6) that with high confidence, reducing the empirical sharpness $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ on the offilne data will also reduce its generalized sharpness $\\mathcal{R}_{\\mathcal{X}}(\\omega)$ . We will show that this is true (see Theorem 1) under certain choices and mild assumptions of the surrogate model (see Assumptions 1 and 2). ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. The output of the above surrogate function $g(\\mathbf{x};\\omega)$ is bounded within $[0,1]$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. $\\lambda_{\\operatorname*{min}}\\Big(\\nabla_{\\omega}^{2}\\mathbb{E}[g(\\mathbf{x};\\omega)]\\Big)>0$ for all $\\lVert\\omega\\rVert\\leq\\tau$ for some $\\tau>0$ . ", "page_idx": 5}, {"type": "text", "text": "We note that the specific bound within $[0,1]$ in Assumption 1 is meant to ease the technical presentation of our theoretical analysis. Otherwise, it can be extended straightforwardly to any bounded functions. Furthermore, we also show below that it is indeed possible to find a non-trivial surrogate function that is bounded and satisfies Assumption 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. There exists $\\tau>0$ and $\\omega_{+}$ , and a non-linear function $r(\\mathbf{x};\\omega)$ such that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g(\\mathbf{x};\\omega)\\ \\triangleq\\ r(\\mathbf{x};\\omega_{+})+\\left(\\omega-\\omega_{+}\\right)^{\\top}\\nabla_{\\omega}r\\big(\\mathbf{x};\\omega_{+}\\big)+\\frac{1}{2}\\big(\\omega-\\omega_{+}\\big)^{\\top}\\nabla_{\\omega}^{2}r\\big(\\mathbf{x};\\omega_{+}\\big)\\big(\\omega-\\omega_{+}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "satisfies Assumption 2 and is bounded on $\\{\\omega\\mid\\|\\omega\\|\\leq\\tau\\}$ . Detailed derivation of this theorem is deferred to Appendix $A$ . ", "page_idx": 5}, {"type": "text", "text": "For such surrogate functions, their generalized sharpness can be upper-bound by a function of their empirical sharpness on the offilne data. As detailed below, the bound depends on both the size of the surrogate $\\mathrm{dim}\\bar{(}\\omega)$ and the number $n$ of offline data points. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. For any $\\rho>0$ , $m=\\dim(\\omega)$ and $2/(m\\lambda_{\\mathrm{min}})\\geq\\sigma^{2}>0$ with $\\lambda_{\\mathrm{min}}$ being defined in Assumption 2, the following holds simultaneously for all $g(.;\\omega)$ for which Assumption 2 is met, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{R}_{\\mathcal{X}}(\\omega)\\le\\frac{1}{\\sigma^{2}m\\lambda_{\\operatorname*{min}}}\\left(2G(\\omega)\\rho+\\lambda_{\\operatorname*{max}}\\rho^{2}\\right)}}\\\\ &{}&{\\times\\left(\\mathcal{R}_{\\mathcal{D}}(\\omega)+\\frac{1}{\\sqrt{n-1}}\\left(m\\log\\left(1+\\frac{\\|\\omega\\|^{2}}{m\\sigma^{2}}\\left(1+\\sqrt{\\frac{\\log n}{m}}\\right)^{2}\\right)+P(n,m,\\alpha)\\right)^{\\frac{1}{2}}\\right)\\left(1-\\rho^{2}\\right)^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability at least $1-\\alpha$ over the random choice of the offilne dataset, and with $P(n,m,\\alpha)=$ $2\\log(n/\\alpha)+4\\log(8n+4m)$ . Detailed derivation of this theorem is deferred to Appendix $E$ . ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch. For clarity, we will provide below a proof sketch of Theorem 2, which highlights the key steps in our derivation. Due to limited space, the specific of each step is deferred to the Appendix E. First, we note that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{R}_{\\mathcal{X}}(\\omega)}&{=}&{\\underset{||\\delta||\\leq\\rho}{\\operatorname*{max}}\\left\\{F_{\\mathcal{X}}(\\omega+\\delta)\\ \\triangleq\\ \\left|\\mathbb{E}_{\\mathcal{X}}[g({\\bf x};\\omega+\\delta)]-\\mathbb{E}_{\\mathcal{X}}[g({\\bf x};\\omega)]\\right|\\right\\}\\ ,}\\\\ {\\mathcal{R}_{\\mathcal{D}}(\\omega)}&{=}&{\\underset{||\\delta||\\leq\\rho}{\\operatorname*{max}}\\left\\{F_{\\mathcal{D}}(\\omega+\\delta)\\ \\triangleq\\ \\left|\\mathbb{E}_{\\mathcal{D}}[g({\\bf x};\\omega+\\delta)]-\\mathbb{E}_{\\mathcal{D}}[g({\\bf x};\\omega)]\\right|\\right\\}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A relation between $\\mathcal{R}_{\\mathcal{X}}(\\omega)$ and $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ can then be derived in three steps: ", "page_idx": 6}, {"type": "text", "text": "1. Upper-bound $\\mathbb{E}_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{X}}(\\omega+\\delta)]$ with a function of $\\mathbb{E}_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{D}}(\\omega+\\delta)]$ . This can be achieved via a direct application of the PAC-Bayes bound [18] which views the perturbed model $\\omega+\\delta$ as a random hypothesis sampled from the posterior $\\mathbb{N}(\\omega,\\sigma^{2}\\mathbf{I})-\\mathrm{se}$ e Appendix B. ", "page_idx": 6}, {"type": "text", "text": "2. Upper-bound $\\mathbb{E}_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{D}}(\\omega+\\delta)]$ with a function of $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ . This can be achieved using a similar proving technique adopted from [14] \u2013 see Appendix C. ", "page_idx": 6}, {"type": "text", "text": "3. Find $\\xi>0$ such that $\\mathcal{R}_{\\mathbf{X}}(\\omega)$ can be upper-bounded with $\\xi\\cdot\\mathbb{E}_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{X}}(\\omega+\\delta)]$ where $\\xi$ is a small scaling factor. To achieve this, we will find the lower-bound for $\\mathbb{E}_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{X}}(\\omega+\\delta)]$ using the Taylor remainder theorem to expand it around $\\omega$ , which can be lower-bounded using the minimum eigenvalue of its Hessian at $\\omega$ . Using the same approach, we can upper-bound $\\mathcal{R}_{\\mathbf{X}}^{\\bar{\\mathbf{\\alpha}}}(\\omega)$ with the maximum eigenvalue of the same Hessian \u2013 see Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Finally, we set $\\xi$ so that the upper-bound of $\\mathcal{R}_{\\mathbf{X}}(\\omega)$ is smaller than the multiplication of $\\xi$ with the lower-bound of $\\mathbb{E}[F_{\\mathcal{X}}(\\pmb{\\omega}+\\pmb{\\delta})]$ . To tighten the bound, we choose the smallest possible value of $\\xi$ such that the bound still holds. Lining up the results of the above steps then shows that $\\mathcal{R}_{\\mathcal{X}}(\\omega)$ can then be bounded with a function of $\\boldsymbol{\\xi}\\cdot\\mathcal{R}_{\\mathcal{D}}(\\omega)$ . See Appendix E for a complete derivation. ", "page_idx": 6}, {"type": "text", "text": "Remark. Note that Theorem 2 is more general than its informal statement in Eq. (6), which can be reproduced by choosing $\\sigma^{2}=2/(m\\bar{\\lambda_{\\mathrm{min}}}\\bar{)}$ in Eq. (16) to make the bound tightest. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section evaluates the efficacy of our proposed method IGNITE in improving state-of-the-art offline optimizers. We describe our experiment settings in Section 5.1 and report detailed empirical results in Section 5.2. We also provide additional ablation studies of our method in Section 5.3. ", "page_idx": 6}, {"type": "text", "text": "5.1 Benchmarks, Baselines, and Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Benchmark Tasks. Our explorations focus on four real-world tasks from Design-Bench3 [4], covering both discrete (TF-Bind-8 and TF-Bind-10) and continuous domains (Ant Morphology [19] and D\u2019Kitty Morphology [20]). ", "page_idx": 6}, {"type": "text", "text": "Baselines. We meticulously curated a diverse set of 11 widely acknowledged offline optimizers for comparative analysis. This ensemble comprises BO-qEI [4], CbAS [1], RoMA [3], ICT [15], ", "page_idx": 6}, {"type": "table", "img_path": "ag7piyoyut/tmp/75aac66bf83a70077270266939def1fc2a9c77668e82e7f57fe51b8ad6ceb3be.jpg", "table_caption": ["Table 1: The percentage improvement in performance achieved by IGNITE across all tasks and baseline algorithms at the 100th percentile level is presented. Gain signifies the percentage gain over the baseline performance (Base). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "CMA-ES [21], COMs [13], MINs [12], REINFORCE [22], and three variations of gradient ascent (GA, ENS-MIN, ENS-MEAN), corresponding to vanilla gradient ascent, the min ensemble of gradient ascent, and the mean ensemble of gradient ascent, respectively. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Protocol. To ensure a comprehensive assessment, we follow the methodology in [4]. Each method generates 128 optimized design candidates evaluated by the oracle function. The candidates\u2019 performances are ranked, and the 50-th, 75-th, and 100-th percentile levels are recorded. All results are averaged over 16 independent runs to ensure reliability. ", "page_idx": 7}, {"type": "text", "text": "Hyper-parameter Configuration. For each baseline algorithm, we carefully configure optimal hyper-parameters as outlined in [4]. Our method IGNITE introduces five additional hyper-parameters: $\\lambda,\\rho,r,\\eta_{\\lambda}$ , and $\\epsilon$ . The hyper-parameter $\\lambda$ , an initial value for the regularizer coefficient, is set to 0.01 through a grid search within $\\{0.0001,0.001,0.01\\}$ . The hyper-parameters $\\rho$ and $r$ are chosen from $\\{0.01,0.05,0.1,0.2\\}$ , with $\\rho$ set to 0.05 and $r$ set to 0.05 for IGNITE. Additionally, IGNITE uses $\\eta_{\\lambda}=1e-3$ and $\\epsilon=0.1$ , which are determined via the experiments in Section 5.3. ", "page_idx": 7}, {"type": "text", "text": "5.2 Results and Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we presented the percentage improvement over baseline performance attained by IGNITE when it is applied to existing baselines. We have evaluated this at the 50-th, 80-th, and 100-th percentile levels. However, due to limited space, we only report results of the 100-th percentile level in the main text. The other results are instead deferred to Appendix G.3. ", "page_idx": 7}, {"type": "text", "text": "Results on Continuous Tasks: The first two columns of Table 1 show that out of 22 cases involving 11 baseline algorithms across 2 tasks, the IGNITE regularizer enhances baseline performance in 18 cases, with improvements reaching up to $9.6\\%$ . In only 1 out of 22 instances IGNITE slightly decreases performance by $0.2\\%$ , which is negligible. Even in cases where performance is not improved, IGNITE reduces the variance in results from $0.2\\%$ to $0.1\\%$ for the CMA-ES baseline on the D\u2019Kitty Morphology task. Additionally, IGNITE helps establish new state-of-the-art (SOTA) ", "page_idx": 7}, {"type": "image", "img_path": "ag7piyoyut/tmp/f82c35c0f38cf404e3dc966a77125bd38666c29d3863c8cf20590f44a3fc0937.jpg", "img_caption": ["Figure 2: The percentage improvement in performance achieved by IGNITE across different algorithms (COMS and GA) and tasks (ANT and TF10) in the changes of (a) threshold $\\epsilon$ and (b) step size $\\eta_{\\lambda}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ag7piyoyut/tmp/0d654454986cc171efd0e085fe3629fd9d1119355c70af31a1ccffd2e2207f25.jpg", "table_caption": ["Table 2: Percentage improvement over the baseline of IGNITE, SAM [14], and L1, L2 regularization across all tasks. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "performances in both tasks. For example, in the Ant Morphology task, it raises the SOTA baseline CMA-ES from $195.5\\%$ to $195.7\\%$ . In the D\u2019Kitty Morphology task, IGNITE achieves a new SOTA of $96.2\\%$ with the ICT baseline. ", "page_idx": 8}, {"type": "text", "text": "Results on Discrete Tasks: The last two columns of Table 1 show the impact of the IGNITE regularizer on the performance of baseline algorithms in two discrete domains (TF-BIND-8 and TFBIND-10). Similar to the continuous tasks, IGNITE significantly enhances baseline performance in most cases (17 out of 22), with improvements of up to $5.6\\%$ . There are only 3 instances where integrating IGNITE results in a minor performance decrease of up to $0.7\\%$ , which is negligible. Additionally, in certain instances, IGNITE not only improves baseline performance but also establishes new state-of-the-art (SOTA) results. For example, on TF-BIND-8 and TF-BIND-10, the original SOTA performances of $98.5\\%$ and $68.1\\%$ achieved by ENS-MEAN and ENS-MIN, respectively, are elevated to $98.7\\%$ and $70.5\\%$ with the addition of IGNITE , setting new SOTA records. ", "page_idx": 8}, {"type": "text", "text": "In summary, IGNITE consistently maintains a high probability of $91\\%$ (40 out of 44) of not degrading baseline performance. There is a high likelihood $(79.55\\%=35$ out of 44 cases) of improving baseline performance, with an average improvement of approximately $1.91\\%$ and a notable peak improvement of $9.6\\%$ . Conversely, IGNITE also exhibits a relatively low probability $(9.09\\%=4$ out of 44 cases) of decreasing performance, with an average degradation of approximately $0.3\\%$ and a minor peak degradation of $0.7\\%$ .Additionally, there is a minor probability $\\mathrm{{11.36\\%=5}}$ out of 44 cases) of maintaining baseline performance. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct additional experiments to assess the sensitivity of two representative baselines, COMs and GA, when regularized with IGNITE, to variations in hyper-parameters $\\epsilon$ and $\\eta_{\\lambda}$ . Additionally, we perform experiments to compare the efficacy of IGNITE with other commonly used regularization methods. ", "page_idx": 8}, {"type": "text", "text": "Changing threshold $\\epsilon$ . We assess the performance enhancement of COMs and GA when regularized with IGNITE using various values of $\\epsilon$ from the set $\\left\\{0.01,0.05,0.1,0.2,0.3,0.4,0.5\\right\\}$ . A high $\\epsilon$ value may result in a surrogate that is overly sharp, potentially hampering the search process and hindering the discovery of optimal designs. Figure 2(a) demonstrates that excessively high $\\epsilon$ values lead to negative improvements. Conversely, excessively low $\\epsilon$ values may cause the regularizer to dominate the original loss, resulting in a surrogate that is not well-ftited to the offilne data. Negative improvements are observed with $\\epsilon=0.01$ and 0.05 in Figure 2(a). As a result, we determine $\\epsilon=0.1$ as the optimal value based on these observations. ", "page_idx": 8}, {"type": "text", "text": "Changing step size $\\eta_{\\lambda}$ . The step size $\\eta_{\\lambda}$ controls the rate at which $\\lambda$ is updated during the optimization process. It\u2019s essential to choose an appropriate step size, avoiding it being either too large or too small. Figure 2(b) demonstrates that an $\\eta_{\\lambda}$ value of $1e-3$ yields optimal results. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Comparison of the surrogate sharpness \u2014 approximately $\\rho\\|\\nabla_{\\omega}h(\\omega)\\|$ in Eq. 10 \u2014 with and without IGNITE. These surrogate sharpness values were computed on unseen data, which are design candidates found by the GA and REINFORCE before and after being equipped with IGNITE. ", "page_idx": 9}, {"type": "table", "img_path": "ag7piyoyut/tmp/aaf745d0c1c30e9460e71ad2395dd92e029cc2b4f60bd6ed9d9c62d70939aa7a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 4: The percentage improvement in performance achieved by IGNITE at the 100th percentile level for Superconductor and Chembl tasks. ", "page_idx": 9}, {"type": "table", "img_path": "ag7piyoyut/tmp/10a15875110ff6744f3ad9d2b9adb66e365d56e156c30f95084f317bc4fba010.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Comparing IGNITE with other regularization methods. We conduct a comparative experiment to assess the performance improvement achieved by using the IGNITE regularizer compared to other regularizers, including L1, L2, and SAM [14] (where SAM is considered as a loss sharpness regularization with a coefficient equal to 1). Table 2 presents the results obtained on two baseline algorithms, REINFORCE and GA, across four tasks. Overall, IGNITE outperforms the other regularizers in most cases, with the largest difference compared to the second best being $3.2\\%$ with REINFORCE on the TF-BIND-10 task. Additionally, IGNITE achieves positive improvements in all cases. Conversely, while the simple regularizers L1 and L2 help boost performance with REINFORCE, they lead to performance degradation with GA. SAM outperforms IGNITE with the GA baseline on the TF-BIND-10 task and shows notable improvement with REINFORCE on the D\u2019Kitty task, though its integration with GA leads to a performance drop. Furthermore, we show that IGNITE also outperforms SAM when integrating with CbAS and BO-qEI in Appendix H. ", "page_idx": 9}, {"type": "text", "text": "Surrogate sharpness on unseen data before and after using IGNITE. We also conduct an experiment measuring the sharpness of the surrogate model, approximated by $\\rho\\|\\nabla_{\\omega}h(\\omega)\\|$ as defined in Eq. (10), with and without IGNITE. This sharpness is computed on unseen data points which are, specifically, the design candidates generated by the GA and REINFORCE baselines. By testing on these unseen candidates, we simulate the out-of-distribution (OOD) conditions that are critical in assessing generalization in optimization tasks. Table 3 reports these surrogate sharpness measurements for some baselines with and without using the IGNITE regularizer. The results demonstrate consistently that the IGNITE regularizer helps reduce the surrogate sharpness on unseen data, as anticipated. This reduction indicates that IGNITE is effective in smoothing the surrogate model\u2019s landscape, leading to better and more stable generalized performance. ", "page_idx": 9}, {"type": "text", "text": "Results on tasks with noisy data. To demonstrate IGNITE\u2019s robust performance in scenarios with noisy oracle, we conducted additional experiments using the GA and REINFORCE baselines on two benchmark tasks with particularly noisy oracles: Superconductor and Chembl. For each baseline, we compared its achieved performance with and without the regularizing effect of IGNITE, as shown in Table 4. Across both tasks, IGNITE helps improve the baseline performance substantially, achieving up to a $2.1\\%$ increase for REINFORCE on the Superconductor task, highlighting IGNITE\u2019s ability to enhance performance robustness even in settings with noisy oracles. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces the concept of generalized surrogate sharpness in offilne optimization, resulting in the development of a new regularization technique, IGNITE. Our theoretical analysis demonstrates that reducing surrogate sharpness on an offilne dataset provably decreases its generalized sharpness on unseen data. Empirically, IGNITE consistently maintains a high probability $(91\\%)$ of not degrading baseline performance and a $79.55\\%$ likelihood of improving it, with a peak improvement of $9.6\\%$ . Additionally, we believe that our novel technique can be adapted to related domains such as robust optimization (RO) and reinforcement learning (RL), suggesting potential future research directions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was funded by Vingroup Joint Stock Company (Vingroup JSC),Vingroup, and supported by Vingroup Innovation Foundation (VINIF) under project code VINIF.2021.DA00128. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for robust design. In International conference on machine learning, pages 773\u2013782. PMLR, 2019.   \n[2] Clara Fannjiang and Jennifer Listgarten. Autofocused oracles for model-based design. Advances in Neural Information Processing Systems, 33:12945\u201312956, 2020.   \n[3] Sihyun Yu, Sungsoo Ahn, Le Song, and Jinwoo Shin. Roma: Robust model adaptation for offilne model-based optimization. Advances in Neural Information Processing Systems, 34:4619\u20134631, 2021.   \n[4] Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Benchmarks for data-driven offilne model-based optimization. In International Conference on Machine Learning, pages 21658\u201321676. PMLR, 2022.   \n[5] Manh Cuong Dao, Phi Le Nguyen, Thao Nguyen Truong, and Trong Nghia Hoang. Boosting offilne optimizers with surrogate sensitivity. In Forty-first International Conference on Machine Learning, 2024.   \n[6] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Diffusion models for black-box optimization. arXiv preprint arXiv:2306.07180, 2023.   \n[7] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Generative pretraining for black-box optimization. arXiv preprint arXiv:2206.10786, 2022.   \n[8] Tung Nguyen, Sudhanshu Agrawal, and Aditya Grover. Expt: Synthetic pretraining for few-shot experimental design. Advances in Neural Information Processing Systems, 36:45856\u201345869, 2023.   \n[9] Yassine Chemingui, Aryan Deshwal, Trong Nghia Hoang, and Janardhan Rao Doppa. Offline model-based optimization via policy-guided gradient search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 11230\u201311239, 2024.   \n[10] Minh Hoang, Azza Fadhel, Aryan Deshwal, Jana Doppa, and Trong Nghia Hoang. Learning surrogates for offilne black-box optimization via gradient matching. In Forty-first International Conference on Machine Learning, 2024.   \n[11] Justin Fu and Sergey Levine. Offline model-based optimization via normalized maximum likelihood estimation. arXiv preprint arXiv:2102.07970, 2021.   \n[12] Aviral Kumar and Sergey Levine. Model inversion networks for model-based optimization. Advances in Neural Information Processing Systems, 33:5126\u20135137, 2020.   \n[13] Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective models for effective offilne model-based optimization. In International Conference on Machine Learning, pages 10358\u201310368. PMLR, 2021.   \n[14] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.   \n[15] Ye Yuan, Can Chen, Zixuan Liu, Willie Neiswanger, and Xue Liu. Importance-aware coteaching for offline model-based optimization. arXiv preprint arXiv:2309.11600, 2023.   \n[16] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.   \n[17] John Platt and Alan Barr. Constrained differential optimization. In Neural Information Processing Systems, 1987.   \n[18] David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference on Computational learning theory, pages 164\u2013170, 1999.   \n[19] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n[20] Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine, and Vikash Kumar. Robel: Robotics benchmarks for learning with low-cost robots. In Conference on robot learning, pages 1300\u20131313. PMLR, 2020.   \n[21] Nikolaus Hansen. The cma evolution strategy: a comparing review. Towards a new evolutionary computation: Advances in the estimation of distribution algorithms, pages 75\u2013102.   \n[22] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229\u2013256, 1992.   \n[23] D. Kingma and M. Welling. Auto-Encoding Variational Bayes. In Proc. ICLR, 2013.   \n[24] John Langford and Rich Caruana. (not) bounding the true error. Advances in Neural Information Processing Systems, 14, 2001.   \n[25] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of statistics, pages 1302\u20131338, 2000.   \n[26] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In International Conference on Machine Learning, pages 26982\u201326992. PMLR, 2022.   \n[27] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147\u2013 160, 1994. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Proofs of Theoretical Results and Additional Experimental Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Part I. Minimum Eigenvalue of Parameter Hessian is Positive. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "First, we re-state the choice of our surrogate in Theorem 1, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{g({\\bf x};\\omega)}&{\\triangleq}&{r({\\bf x};\\omega_{+})+\\big(\\omega-\\omega_{+}\\big)^{\\top}\\nabla_{\\omega}r\\big({\\bf x};\\omega_{+}\\big)+\\frac{1}{2}\\big(\\omega-\\omega_{+}\\big)^{\\top}\\nabla_{\\omega}^{2}r\\big({\\bf x};\\omega_{+}\\big)\\big(\\omega-\\omega_{+}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Regardless of the choice of $r(\\mathbf{x};\\omega)$ , differentiating $g(\\mathbf{x};\\omega)$ with respect to $\\omega$ yields, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{\\omega}^{2}g({\\bf x};\\omega)}&{{}\\!=\\!}&{\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which implies the parameter Hessian of $\\nabla_{\\omega}^{2}g(\\mathbf{x};\\omega)$ is always a constant matrix depending on the specific choice of $r(\\mathbf{x};\\omega)$ and a reference point $\\omega_{+}$ . Hence, it follows trivially that, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lambda_{\\mathrm{min}}\\Big(\\nabla_{\\omega}^{2}g({\\bf x};\\omega)\\Big)}&{=}&{\\lambda_{\\mathrm{min}}\\Big(\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})\\Big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, if we can find $r(\\mathbf{x};\\omega)$ such that its parameter Hessian is strictly positive definite at $\\omega_{+}$ , the parameter Hessian of $g(\\mathbf{x};\\omega)$ will always be strictly positive definite regardless of $\\omega$ . This means $g(\\mathbf{x};\\omega)$ will be $\\lambda_{\\operatorname*{min}}$ -strongly convex in $\\omega$ with $\\lambda_{\\operatorname*{min}}>0$ . As a result, its expectation $\\mathbb{E}[g(\\mathbf{x};\\omega)]$ will also be $\\lambda_{\\operatorname*{min}}$ -strongly convex with $\\lambda_{\\operatorname*{min}}>0$ or equivalently, Assumption 2 holds. ", "page_idx": 12}, {"type": "text", "text": "This can be done by choosing $r(\\mathbf{x};\\omega)$ to be any quantile prediction of a linear regressor with a (pre-trained) random feature map. For example, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r c l}{r(\\mathbf{x};\\omega)}&{\\triangleq}&{\\mathbb{E}_{\\psi}\\left[\\omega^{\\top}\\psi(\\mathbf{x})\\right]+\\frac{\\gamma}{2}\\mathbb{V}_{\\psi}[\\omega^{\\top}\\psi(\\mathbf{x})]^{\\frac{1}{2}}}\\\\ &&{\\quad\\mathrm{~where~}}&{\\psi(\\mathbf{x})\\,\\sim\\,\\mathbb{N}\\!\\left(\\mathbf{m}(\\mathbf{x}),\\mathrm{diag}[\\mathbf{v}(\\mathbf{x})]\\right),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathbb{V}[.]$ denote the variance function. We can interpret the random feature $\\psi(\\mathbf{x})$ is sampled from a pre-trained VAE [23] with two deep neural nets characterizing the mean and variance functions, $\\mathbf{m}(\\mathbf{x})$ and $\\mathbf{v}(\\mathbf{x})$ . These functions can be pre-trained and post-processed in whatever ways we need to control their value ranges. Their parameters are then frozen when we fit $r(\\mathbf{x};\\omega)$ with respect to $\\omega$ . ", "page_idx": 12}, {"type": "text", "text": "To understand the behavior of $r(\\mathbf{x};\\omega)$ , we first note below the closed-form expression of Eq. (23), ", "page_idx": 12}, {"type": "equation", "text": "$$\nr(\\mathbf{x};\\omega)\\;\\;=\\;\\;\\omega^{\\top}\\mathbf{m}(\\mathbf{x})+\\frac{\\gamma}{2}\\left(\\omega^{\\top}\\mathbf{A}\\omega\\right)^{\\frac{1}{2}}\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathbf{A}\\triangleq\\mathrm{diag}[\\mathbf{v}(\\mathbf{x})]$ . Differentiating twice both sides of Eq. (24) at $\\omega_{+}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+})\\quad\\triangleq\\quad\\frac{\\gamma}{2}\\cdot\\left(\\omega_{+}^{\\top}\\mathbf{A}\\omega_{+}\\right)^{-\\frac{3}{2}}\\mathbf{A}\\Bigg(\\left(\\omega_{+}^{\\top}\\mathbf{A}\\omega_{+}\\right)\\mathbf{I}-\\omega_{+}\\omega_{+}^{\\top}\\mathbf{A}\\Bigg)\\ .\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We can now choose $\\begin{array}{r}{\\pmb{\\omega}_{+}=(1/m)\\sum_{i=1}^{m}\\mathbf{e}_{i}}\\end{array}$ where $\\mathbf{e}_{1},\\mathbf{e}_{2},\\ldots,\\mathbf{e}_{m}$ are the eigenvectors of A. Since A is diagonal, these are also the one -hot vectors and their corresponding eigenvalues are the entries on the diagonal of A \u2013 i.e., $\\lambda_{i}(\\mathbf{A})=[\\mathbf{A}]_{i i}$ . With this choice, a closed-form expression of $\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})$ in terms of the entries of $\\mathbf{A}$ can be derived as follow, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})\\;\\;\\;\\triangleq\\;\\;\\;\\frac{\\gamma}{2}\\cdot\\left(\\frac{1}{m^{2}}\\sum_{i=1}^{m}[{\\bf A}]_{i i}\\right)^{-\\frac{3}{2}}{\\bf A}{\\bf B}\\;,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathbf{B}$ is another diagonal matrix with entries $\\begin{array}{r}{[\\mathbf{B}]_{i i}=m^{-2}(\\sum_{\\iota=1}^{m}[\\mathbf{A}]_{\\iota\\iota}-[\\mathbf{A}]_{i i})}\\end{array}$ . As such, it is trivial to see that if we choose the activation unit (i.e., sigmoid or ReLU) for $\\mathbf{v}(\\mathbf{x})$ such that its output is positive, then the entries of both A and $\\mathbf{B}$ are positive. ", "page_idx": 12}, {"type": "text", "text": "Finally, note that Eq. (26) is a scaled matrix product of two diagonal matrices, A and $\\mathbf{B}$ , which will result in a diagonal matrix. Its entries will be positive if the entries of $\\mathbf{A}$ and $\\mathbf{B}$ are positive, as enforced above. This means the minimum eigenvalue of $\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})$ is positive as desired. $\\sqcap$ ", "page_idx": 12}, {"type": "text", "text": "Part II. Boundedness on $\\{\\mathbf{w}\\ :\\ \\|\\mathbf{w}\\|^{2}\\leq\\tau\\}$ . ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Rearranging terms in the expression of $g(\\mathbf{x};\\omega)$ in Eq. (19), ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{g({\\bf x};\\omega)}&{\\triangleq}&{\\omega^{\\top}\\Big(\\nabla_{\\omega}r({\\bf x};\\omega_{+})-\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})\\omega_{+}\\Big)\\;+\\;\\frac{1}{2}\\omega^{\\top}\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})\\omega\\;+\\;\\mathrm{const}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where const absorbs all constant vectors (i.e., not dependent on $\\omega$ ). Hence, taking absolute values for both sides of Eq. (27), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert g(\\mathbf{x};\\omega)\\right\\rvert=\\displaystyle\\left\\lvert\\omega^{\\top}\\left(\\nabla_{\\omega}r(\\mathbf{x};\\omega_{+})-\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+})\\omega_{+}\\right)\\,+\\,\\frac{1}{2}\\omega^{\\top}\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+})\\omega\\,+\\,\\mathrm{const}\\right\\rvert}&{}\\\\ {\\displaystyle\\leq\\,\\left\\lVert\\omega\\right\\rVert\\cdot\\left\\lVert\\nabla_{\\omega}r(\\mathbf{x};\\omega_{+})-\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+})\\omega_{+}\\right\\rVert+\\frac{1}{2}\\displaystyle\\left\\lvert\\omega^{\\top}\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+})\\omega\\right\\rvert+\\left\\lVert\\mathrm{const}\\right\\rVert}&{}\\\\ {\\displaystyle\\leq\\,\\,\\tau\\cdot\\left\\lVert\\mathrm{const}\\right\\rVert+\\left(\\frac{1}{2}\\lambda_{\\operatorname*{max}}\\left(\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+})\\right)\\right)\\left\\lVert\\omega\\right\\rVert^{2}+\\left\\lVert\\mathrm{const}\\right\\rVert}&{}\\\\ {\\displaystyle\\leq\\,\\,\\tau\\cdot\\left\\lVert\\mathrm{const}\\right\\rVert+\\left(\\frac{1}{2}\\lambda_{\\operatorname*{max}}\\left(\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+})\\right)\\right)\\cdot\\tau^{2}+\\left\\lVert\\mathrm{const}\\right\\rVert\\,=\\,\\mathrm{const}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where Eq. (30) follows from the fact that $\\nabla_{\\omega}^{2}r({\\bf x};\\omega_{+})$ is positive definite by construction. Eq. (31) follows because $(1/2)\\lambda_{\\operatorname*{max}}(\\nabla_{\\omega}^{2}r(\\mathbf{x};\\omega_{+}))$ is a fixed value that does not change when $\\omega$ changes. Thus, $g(\\mathbf{x};\\omega)$ is bounded as expected. $\\sqsubset$ ", "page_idx": 13}, {"type": "text", "text": "B Upper-Bound $\\mathbb{E}_{\\delta\\in\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{X}}(\\omega+\\delta)]\\;\\mathbf{with}\\;\\mathbb{E}_{\\delta\\in\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{D}}(\\omega+\\delta)]$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 3. Let us define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{F_{\\mathcal{X}}(\\omega+\\pmb{\\delta})}&{\\triangleq}&{\\left|\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathbb{E}}[g(\\mathbf{x};\\omega+\\pmb{\\delta})]-\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathbb{E}}[g(\\mathbf{x};\\omega)]\\right|,}\\\\ {F_{\\mathcal{D}}(\\omega+\\pmb{\\delta})}&{\\triangleq}&{\\left|\\underset{\\mathbf{x}\\in\\mathcal{D}}{\\mathbb{E}}[g(\\mathbf{x};\\omega+\\pmb{\\delta})]-\\underset{\\mathbf{x}\\in\\mathcal{D}}{\\mathbb{E}}[g(\\mathbf{x};\\omega)]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The following holds simultaneously for all $\\omega$ , $\\sigma^{2}>0$ , $\\alpha\\in(0,1)$ and $m=\\dim(\\omega)$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\underset{\\delta\\sim N(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{X}}(\\omega+\\delta)]}&{\\le}&{\\underset{\\delta\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{D}}(\\omega+\\delta)]}\\\\ &&{+}&{\\sqrt{\\frac{\\frac{1}{4}m\\log\\Big(1+\\frac{\\|\\omega\\|_{2}^{2})}{m\\sigma^{2}}\\Big)+\\frac{1}{4}+\\frac{1}{2}\\log\\frac{n}{\\alpha}+\\log(8n+4m)}{n-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with probability at least $1-\\alpha$ over the (random) choice of the offline dataset $\\mathcal{D}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We can view the random perturbation $\\delta$ as a random hypothesis sampled from a hypothesis distribution (such as the zero-mean Gaussian $\\mathbb{N}(0,\\sigma^{2}\\mathbf{I}))$ and $F_{\\mathcal{X}}(\\omega+\\delta)$ as its incurred loss. In this view, for any hypothesis distribution $\\mathcal{Q}$ , $\\mathbb{E}_{\\delta\\sim\\mathcal{Q}}[F_{\\mathcal{X}}(\\omega+\\delta)]$ and $\\mathbb{E}_{\\delta\\sim\\mathcal{Q}}[F_{\\mathcal{D}}(\\omega+\\delta)]$ denote the corresponding generalized and empirical Gibbs losses whose relationship is characterized via the following classical PAC-Bayesian [18] bound, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{\\delta\\sim Q}{\\mathbb{E}}\\big[F_{X}(\\omega+\\delta)\\big]}&{\\le}&{\\underset{\\delta\\sim Q}{\\mathbb{E}}\\big[F_{{\\mathcal D}}(\\omega+\\delta)\\big]\\;\\;+\\;\\;\\sqrt{\\frac{\\mathrm{KL}(Q||{\\mathcal P})+\\log\\left(\\frac{n}{\\alpha}\\right)}{2(n-1)}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which will hold simultaneously for all choices of $\\omega$ , $\\mathcal{P}$ and $\\mathcal{Q}$ . Historically, in probabilistic learning context, $\\mathcal{P}$ is often referred to as prior or reference distribution which, for example, encodes domainspecific information about certain properties of the solution distribution. Then, $\\mathcal{Q}$ is the posterior, which can be selected to tighten the bound (hence, reducing the generalized loss). ", "page_idx": 13}, {"type": "text", "text": "In our specific context, we will simply use the choices of $\\mathcal{P}$ and $\\mathcal{Q}$ as technical vehicles to tighten the gap between $\\mathbb{E}[F_{\\mathcal{X}}(\\pmb{\\omega}+\\pmb{\\delta})]$ and $\\dot{\\mathbb{E}}[F_{\\mathcal{D}}(\\pmb{\\omega}+\\pmb{\\delta})]$ which will contribute to the gap between our generalized and empirical surrogate sharpness. ", "page_idx": 13}, {"type": "text", "text": "To derive the optimal choices $\\mathcal{P}$ and $\\mathcal{Q}$ , we will leverage part of the proof from [14], which starts with the following fact: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sqrt{\\frac{m\\log\\Big(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\Big)}{4n}}}&{\\leq}&{\\sqrt{\\frac{\\frac{1}{4}m\\log\\Big(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\Big)+\\frac{1}{4}+\\frac{1}{2}\\log\\frac{n}{\\alpha}+\\log(8n+4m)}{n-1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This means if $\\|\\pmb{\\omega}\\|>\\rho^{2}(\\exp(4n/m)-1)$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\,m\\log\\left(1+\\frac{\\|\\pmb{\\omega}\\|_{2}^{2})}{m\\sigma^{2}}\\right)}{4n}}\\quad>\\quad1\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means the bound gap in Eq. (34) is larger than 1 and under Assumption 1 that the surrogate\u2019s output \u2013 hence, $F_{\\mathcal{X}}(\\omega)$ \u2019s \u2013 is bounded in $[0,1]$ , Eq. (34) holds trivially. ", "page_idx": 14}, {"type": "text", "text": "Hence, to choose $\\mathcal{P}$ and $\\mathcal{Q}$ , we can make the assumption that $\\|\\pmb{\\omega}\\|\\leq\\rho^{2}(\\exp(4n/m)-1)$ . Now, let us choose both prior and posterior distribution to be Gaussians, $\\mathcal{\\vec{P}}=\\mathbb{N}(\\mu_{\\mathcal{P}},\\sigma_{\\mathcal{P}}^{2}\\dot{\\mathbf{I}})$ and $\\mathcal{Q}=\\mathbb{N}(\\mu_{\\mathcal{Q}},\\sigma_{\\mathcal{Q}}^{2}\\mathbf{I})$ . Their KL divergence can then be computed in closed-form: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{KL}(\\mathcal{Q}||\\mathcal{P})}&{=}&{\\frac{1}{2}\\left[\\frac{m\\sigma_{Q}^{2}+\\|\\mu_{\\mathcal{P}}-\\mu_{Q}\\|_{2}^{2}}{\\sigma_{\\mathcal{P}}^{2}}\\,-\\,m\\,+\\,m\\log\\left(\\frac{\\sigma_{\\mathcal{P}}}{\\sigma_{Q}}\\right)^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Naively, we can minimize the above KL divergence via setting the derivative of KL with respect to $\\sigma_{\\mathscr P}$ to zero, and solving for the optimal $\\sigma_{\\mathscr P}$ , which gives $\\sigma{p}^{2}\\overset{\\smile}{=}\\sigma_{Q}^{2}+m^{-1}\\|\\pmb{\\mu}_{\\mathcal{P}}-\\pmb{\\mu}_{\\mathcal{Q}}\\|_{2}^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "However, for the PAC-Bayes bound to hold, $\\sigma_{\\mathscr P}$ must be chosen independent of the rest, so the above approach does not work. Instead, we can define in advance a prior set of $\\sigma_{\\mathscr P}$ and then choose the best one in this set, following the proof in [24]. We can choose this set as follow: ", "page_idx": 14}, {"type": "text", "text": "Given fixed $a,b>0$ , let $S=\\{c\\!\\cdot\\!\\exp((1-i)/m)|i\\in\\mathbb{N}\\}$ be predefined set for $\\sigma_{\\mathcal{P}}^{2}$ . If the above bound holds for $\\sigma_{\\mathcal{P}}^{2}=c\\times\\exp((1-i)/m)$ with probability $1-\\alpha_{i}$ with $\\alpha_{i}=6\\alpha\\bar{\\pi}^{-2}i^{-2}$ for any $i\\in\\mathbb N$ , then all above bounds hold simultaneously with probability at least $\\begin{array}{r}{1-\\sum_{i=1}^{\\infty}6\\alpha\\pi^{-2}i^{-2}=\\stackrel{\\cdot}{1}-\\alpha}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Now, let $\\sigma_{\\mathscr{Q}}=\\sigma,\\pmb{\\mu}_{\\mathscr{Q}}=\\pmb{\\omega},\\pmb{\\mu}_{\\mathscr{P}}=0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\sigma_{Q}^{2}\\;+\\;\\left\\|\\mu_{\\mathcal{P}}-\\mu_{Q}\\right\\|_{2}^{2}/m}&{=}&{\\sigma^{2}\\;\\;+\\;\\;\\|\\omega\\|_{2}^{2}/m\\;\\;\\leq\\;\\;\\sigma^{2}(1+\\exp(4n/m))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Choosing $i=\\lfloor1\\!-\\!m\\log((\\sigma^{2}\\!+\\!\\|\\omega\\|_{2}^{2}/m)/c)\\rfloor,c=\\sigma^{2}(1\\!+\\!\\exp(4n/m))$ and $\\sigma_{\\mathcal{P}}^{2}=c{\\cdot}\\mathrm{exp}((1{-}i)/m)$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-m\\log((\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m)/c)\\leq}&{i}\\\\ {(\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m)/c\\leq}&{\\exp((1-i)/m)}&{\\leq\\exp(1/m)\\times(\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m)/c}\\\\ {\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m\\leq}&{c\\times\\exp((1-i)/m)}&{\\leq\\exp(1/m)\\times(\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m)}\\\\ {\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m\\leq}&{\\sigma_{P}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging Eq. (43) and Eq. (39) into Eq. (38), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathrm{KL}(Q||\\mathcal{P})}&{=}&{\\displaystyle\\frac{1}{2}\\left[\\frac{m\\sigma_{Q}^{2}+\\|\\mu_{P}-\\mu_{Q}\\|_{2}^{2}}{\\sigma_{P}^{2}}-m+m\\log\\left(\\frac{\\sigma_{P}}{\\sigma_{Q}}\\right)^{2}\\right]}\\\\ &{\\leq}&{\\displaystyle\\frac{1}{2}\\left[\\frac{m\\left(\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m\\right)}{\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m}-m+m\\log\\left(\\frac{\\exp\\left(\\frac{1}{m}\\right)\\times\\left(\\sigma^{2}+\\frac{\\|\\omega\\|_{2}^{2}}{m}\\right)}{\\sigma^{2}}\\right)\\right]}\\\\ &{=}&{\\displaystyle\\frac{1}{2}\\left[m\\log\\left(\\frac{\\exp\\left(\\frac{1}{m}\\right)\\times\\left(\\sigma^{2}+\\frac{\\|\\omega\\|_{2}^{2}}{m}\\right)}{\\sigma^{2}}\\right)\\right]}\\\\ &{=}&{\\displaystyle\\frac{1}{2}\\left[1+m\\log\\left(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma_{Q}^{2}}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, Eq. (35) holds for each prior (indexed by $i$ ) in the above set with probability $1-\\alpha_{i}$ . Hence, Eq. (35) holds with all prior choices with probability $\\begin{array}{r}{1-\\sum_{i}\\alpha_{i}=1-\\dot{\\alpha_{i}}}\\end{array}$ . This allows us to pick the prior that leads to the tightest bound gap, which is indexed with $i=\\lfloor1-m\\log((\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m)/c)\\rfloor$ . For this choice, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{\\delta\\sim\\mathcal{Q}}{\\mathbb{E}}\\big[F_{X}(\\omega+\\delta)\\big]}&{\\le}&{\\underset{\\delta\\sim\\mathcal{Q}}{\\mathbb{E}}\\big[F_{\\mathcal{D}}(\\omega+\\delta)\\big]\\;\\;+\\;\\;\\sqrt{\\frac{\\mathrm{KL}(Q||\\mathcal{P})+\\log(\\frac{n}{\\alpha_{i}})}{2(n-1)}}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging Eq. (47) into Eq. (48), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\underset{\\delta\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{X}}(\\omega+\\delta)]}&{\\le}&{\\underset{\\delta\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{D}}(\\omega+\\delta)]}\\\\ &&{+}&{\\sqrt{\\frac{\\frac{1}{4}m\\log\\left(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\right)+\\frac{1}{4}+\\frac{1}{2}\\log\\frac{n}{\\alpha_{i}}}{n-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, to remove the dependence on $i$ in the above bound, note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\frac{n}{\\alpha_{i}}}&{=\\;\\log\\frac{n}{\\alpha_{i}}+\\log\\frac{\\pi^{2}i^{2}}{6}}\\\\ {\\le\\;}&{\\log\\frac{n}{\\alpha}+\\log\\frac{\\pi^{2}(1-\\log\\log((\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m)/c))^{2}}{6}}\\\\ &{\\le\\;\\log\\frac{n}{\\alpha}+\\log\\frac{\\pi^{2}m^{2}\\log^{2}(\\sigma^{2}(\\sigma^{2}+\\|\\omega\\|_{2}^{2}/m))}{3}}\\\\ {\\le\\;}&{\\log\\frac{n}{\\alpha}+\\log\\frac{\\pi^{2}m^{2}\\log^{2}(\\sigma^{2}(\\sigma^{2})}{6}}\\\\ {\\le\\;}&{\\log\\frac{n}{\\alpha}+\\log\\frac{\\pi^{2}m^{2}\\log^{2}(1+\\exp(4n/m))}{3}}\\\\ {\\le\\;}&{\\log\\frac{n}{\\alpha}+\\log\\frac{\\pi^{2}m^{2}\\log^{2}(2+4n/m)}{6}}\\\\ {\\le\\;}&{\\log\\frac{n}{\\alpha}+\\log\\frac{\\pi^{2}m^{2}(2+4n/m)^{2}}{5}}\\\\ {\\le\\;}&{\\log\\frac{n}{\\alpha}+2\\log\\frac{\\pi(2m+4n)}{5}}\\\\ {\\le\\;}&{\\log\\frac{n}{\\alpha}+2\\log(8n+4m)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging Eq. (57) into Eq. (49), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\underset{\\delta\\sim N(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{X}}(\\omega+\\delta)]}&{\\le}&{\\underset{\\delta\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{D}}(\\omega+\\delta)]}\\\\ &&{+}&{\\sqrt{\\frac{\\frac{1}{4}m\\log\\Big(1+\\frac{\\|\\omega\\|_{2}^{2})}{m\\sigma^{2}}\\Big)+\\frac{1}{4}+\\frac{1}{2}\\log\\frac{n}{\\alpha}+\\log(8n+4m)}{n-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which completes our proof. ", "page_idx": 15}, {"type": "text", "text": "C Upper-Bound $\\mathbb{E}_{\\delta\\in\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{X}}(\\omega+\\delta)]$ with $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 4. Following the same setup in Lemma 3, the following holds simultaneously for all $\\omega$ , $\\sigma^{2}>0$ , $\\alpha\\in(0,1)$ and $m=\\dim(\\omega)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{\\delta\\sim N(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{X}}(\\omega+\\delta)]}&{\\leq}&{\\underset{\\|\\delta\\|\\leq\\rho}{\\operatorname*{max}}\\left[F_{\\mathcal{D}}(\\omega+\\delta)\\right]}\\\\ &{+}&{\\sqrt{\\frac{m\\log\\left(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\right)+2\\log\\frac{n}{\\alpha}+4\\log(8n+4m)}{n-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1-\\alpha$ over the (random) choice of the offline dataset $\\mathcal{D}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. To derive this result, we need to upper-bound $\\mathbb{E}[F_{\\mathcal{X}}(\\omega)]$ with $F_{D}(\\omega)$ . First, we note that $\\delta\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})$ so $\\lVert\\pmb{\\delta}\\rVert_{2}^{2}$ follows a chi-square distribution. From Lemma 1 in [25], ", "page_idx": 15}, {"type": "equation", "text": "$$\nP\\left(\\|\\delta\\|_{2}^{2}-m\\sigma^{2}\\;\\;\\geq\\;\\;2\\sigma^{2}\\sqrt{m t}+2t\\sigma^{2}\\right)\\;\\;\\;\\leq\\;\\;\\exp(-t)\\;\\;\\;\\forall t>0\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, with probability $1-1/\\sqrt{n}$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|\\delta\\|_{2}^{2}}&{\\leq}&{\\sigma^{2}\\left(2\\log\\left(\\sqrt{n}\\right)+m+2\\sqrt{m\\log\\left(\\sqrt{n}\\right)}\\right)}&{\\leq}&{\\sigma^{2}m\\left(1+\\sqrt{\\frac{\\log(n)}{m}}\\right)^{2}}&{\\leq\\ \\rho^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This means with probab\u221aility $1-1/\\sqrt{n}$ , we have $\\begin{array}{r}{\\mathbb{E}[F_{\\mathcal{D}}(\\omega+\\pmb{\\delta})]\\leq\\operatorname*{max}_{\\|\\pmb{\\delta}\\|\\leq\\rho}F_{\\mathcal{D}}(\\omega+\\pmb{\\delta})}\\end{array}$ . Otherwise, with the remaining $1/\\sqrt{n}$ chance, $\\mathbb{E}[F_{\\mathcal{D}}(\\omega+\\pmb{\\delta})]\\le1$ under Assumption 1. Putting this together, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\underset{\\delta\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{D}}(\\omega+\\delta)]}&{\\le}&{\\displaystyle\\left(1-\\frac{1}{\\sqrt{n}}\\right)\\cdot\\operatorname*{max}_{\\|\\delta\\|\\le\\rho}[F_{\\mathcal{D}}(\\omega+\\delta)]}&{+}&{\\displaystyle\\frac{1}{\\sqrt{n}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, plugging Eq. (62) into Eq. (34), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\sim\\mathcal{N}(0,\\sigma^{2})}{\\mathbb{E}}\\Big[F_{X}(\\omega+\\delta)\\Big]}&{\\le}&{\\Bigg(1-\\frac{1}{\\sqrt{n}}\\Bigg)\\underset{\\|\\delta\\|_{2}\\le\\rho}{\\operatorname*{max}}\\ F_{D}(\\omega+\\delta)\\ +\\ \\frac{1}{\\sqrt{n}}}\\\\ &{+}&{\\sqrt{\\frac{\\frac{1}{4}m\\log\\Big(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\Big(1+\\sqrt{\\frac{\\log(n)}{m}}\\Big)^{2}\\Big)+\\frac{1}{2}\\log\\frac{n}{\\alpha}+\\log(8n+4m)}{n-1}}}\\\\ &{\\le}&{\\operatorname*{max}_{\\|\\delta\\|_{2}\\le\\rho}F_{D}(\\omega+\\delta)}\\\\ &{+}&{\\sqrt{\\frac{m\\log\\Big(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\Big(1+\\sqrt{\\frac{\\log(n)}{m}}\\Big)^{2}\\Big)+2\\log\\frac{n}{\\alpha}+4\\log(8n+4m)}{n-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes our proof. ", "page_idx": 16}, {"type": "text", "text": "D Upper-Bound $\\mathcal{R}_{\\mathcal{X}}(\\omega)$ with a scaled value of $\\mathbb{E}_{\\delta\\in\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{X}}(\\omega+\\delta)]$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 5. Following the same setup in Lemma 3, the below holds for all $\\sigma^{2}\\in(0,2/(m\\lambda_{\\operatorname*{min}}))$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\|\\delta\\|\\le\\rho}F_{X}(\\omega+\\delta)\\,\\le\\,\\frac{1}{m}\\cdot\\frac{1}{\\sigma^{2}}\\cdot\\frac{1}{\\lambda_{\\operatorname*{min}}}\\cdot\\left(2G(\\omega)\\rho\\,+\\,\\lambda_{\\operatorname*{max}}\\rho^{2}\\right)\\cdot\\operatorname*{lim}_{\\delta\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I})}\\left[F_{X}(\\omega+\\delta)\\right]\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $G(\\omega)$ and $\\lambda_{\\mathrm{max}}$ defined previously in Eq. (7), $m=\\dim(\\gamma)$ and $\\lambda_{\\mathrm{min}}$ defined in Assumption 2. ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $h(\\omega)\\triangleq\\mathbb{E}[g(\\mathbf{x};\\omega)]$ where the expectation is over $\\mathbf{x}\\in\\mathcal{X}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\underset{|\\delta|\\leq\\rho}{\\operatorname*{max}}F_{x}(\\omega+\\delta)}&{=}&{\\underset{|\\delta|\\leq\\rho}{\\operatorname*{max}}\\,|h(\\omega+\\delta)-h(\\omega)|\\ }&&{}\\\\ &{=}&{\\underset{||\\delta|\\leq\\rho}{\\operatorname*{max}}\\,\\bigg|\\nabla_{\\omega}h(\\omega)^{\\top}\\delta\\ +\\ \\frac{1}{2}\\delta^{\\top}\\nabla_{\\omega}^{2}h(\\hat{\\omega})\\delta\\bigg|}\\\\ &{\\leq}&{\\underset{||\\delta|\\leq\\rho}{\\operatorname*{max}}\\,\\big|\\nabla_{\\omega}h(\\omega)^{\\top}\\delta\\big|+\\frac{1}{2}\\left|\\delta^{\\top}\\nabla_{\\omega}^{2}h(\\hat{\\omega})\\delta\\right|}\\\\ &{\\leq}&{\\underset{||\\delta|\\leq\\rho}{\\operatorname*{max}}\\,\\|\\nabla_{\\omega}h(\\omega)\\|\\cdot\\|\\delta\\|+\\ \\frac{1}{2}\\|\\delta\\|^{2}\\cdot\\lambda_{\\operatorname*{max}}}\\\\ &{\\leq}&{\\|\\nabla_{\\omega}h(\\omega)\\|\\,\\rho\\ +\\ \\frac{1}{2}\\rho^{2}\\lambda_{\\operatorname*{max}}\\ =\\ G(\\omega)\\rho\\ +\\ \\frac{1}{2}\\rho^{2}\\lambda_{\\operatorname*{max}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\hat{\\pmb{\\omega}}=\\pmb{\\omega}+\\pmb{c}\\cdot\\pmb{\\delta}$ with some constant $c\\in[0,1]$ . In the above, Eq. (65) follows from the Taylor\u2019s remainder theorem for multivariate function. Eq. (67) follows from the Cauchy-Schwartz inequality (for the first term) and the definition of $\\lambda_{\\mathrm{max}}$ as the upper-bound on the largest eigenvalue of the parameter Hessian of $h(\\omega)$ . Note that $\\lambda_{\\operatorname*{max}}>\\lambda_{\\operatorname*{min}}>0$ under Assumption 2 which stipulates that the smallest eigenvalue of the parameter Hessian of $h(\\omega)$ is always positive. ", "page_idx": 16}, {"type": "text", "text": "On the other hand, we also: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathrm{I})}{\\mathbb{E}}\\Big[F_{\\mathcal{X}}(\\omega+\\delta)\\Big]\\,=\\,\\underset{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathrm{I})}{\\mathbb{E}}\\Big|h(\\omega+\\delta)-h(\\omega)\\Big|}\\\\ {\\geq\\,\\bigg|_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathrm{I})}\\,\\bigg[\\nabla_{\\omega}h(\\omega)^{\\top}\\delta+\\frac{1}{2}\\delta^{\\top}\\nabla_{\\omega}^{2}h(\\hat{\\omega})\\delta\\bigg]\\bigg|}\\\\ {=\\,\\frac{1}{2}\\,\\bigg|_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathrm{I})}\\,\\Big[\\delta^{\\top}\\nabla_{\\omega}^{2}h(\\hat{\\omega})\\delta\\Big]\\bigg|}\\\\ {\\geq\\,\\bigg(\\frac{1}{2}\\lambda_{\\operatorname*{min}}\\bigg)_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathrm{I})}\\,\\big[\\|\\delta\\|^{2}\\big]}\\\\ {=\\,\\bigg(\\frac{1}{2}\\lambda_{\\operatorname*{min}}\\bigg)\\,\\mathrm{Tr}\\left[\\sigma^{2}\\mathbf{I}\\right]\\,\\,=\\,\\,\\bigg(\\frac{1}{2}\\lambda_{\\operatorname*{min}}\\bigg)\\cdot m\\cdot\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $m=\\dim(\\omega)$ . In the above, Eq. (70) follows from the Taylor\u2019s remainder theorem (similar to Eq. (65)) and Eq. (72) follows from the definition of $\\lambda_{\\operatorname*{min}}$ in Assumption 2 as the lower-bound on the smallest eigenvalue of the parameter Hessian. Since $\\lambda_{\\operatorname*{min}}>0$ , the quadratic term inside the expectation is always positive which explains why we can remove the absolute operator. Eq. (73) follows from standard moment calculation of Gaussian random vector. ", "page_idx": 17}, {"type": "text", "text": "Finally, we note that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{G(\\omega)\\rho\\;+\\;\\frac{1}{2}\\rho^{2}\\lambda_{\\mathrm{max}}\\:=\\:\\bigg[\\frac{1}{m}\\cdot\\frac{1}{\\sigma^{2}}\\cdot\\frac{1}{\\lambda_{\\mathrm{min}}}\\cdot\\bigg(2G(\\omega)\\rho+\\lambda_{\\mathrm{max}}\\rho^{2}\\bigg)\\bigg]\\bigg(\\frac{1}{2}\\lambda_{\\mathrm{min}}\\bigg)\\cdot m\\cdot\\sigma^{2}}&{{}}\\\\ {\\leq\\:\\bigg[\\frac{1}{m}\\cdot\\frac{1}{\\sigma^{2}}\\cdot\\frac{1}{\\lambda_{\\mathrm{min}}}\\cdot\\bigg(2G(\\omega)\\rho+\\lambda_{\\mathrm{max}}\\rho^{2}\\bigg)\\bigg]\\cdot\\underset{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}\\bigg[F_{X}(\\omega+\\delta)\\bigg]}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which means, intuitively, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\xi}&{=}&{\\left[\\displaystyle\\frac{1}{m}\\cdot\\frac{1}{\\sigma^{2}}\\cdot\\frac{1}{\\lambda_{\\mathrm{min}}}\\cdot\\left(2G(\\omega)\\rho+\\lambda_{\\mathrm{max}}\\rho^{2}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is the smallest scaling factor for the lower-bound of $\\mathbb{E}_{\\delta\\sim\\mathbb{N}(0,\\sigma^{2}\\mathbf{I})}[F_{\\mathcal{X}}(\\omega+\\delta)]$ so that it matches the upper-bound of $\\begin{array}{r}{\\mathcal{R}_{\\mathcal{X}}(\\omega)=\\operatorname*{max}_{\\|\\pmb{\\delta}\\|\\leq\\rho}F_{\\mathcal{X}}(\\omega+\\delta)}\\end{array}$ , as outlined in the 3rd step of our proving plan for Theorem 2. As such, plugging Eq. (68) into the left-hand side of Eq. (75) completes our proof. $\\sqsubset$ ", "page_idx": 17}, {"type": "text", "text": "Remark. In the above, Eq. (73) suggests that $\\mathbb{E}_{\\delta}[F\\chi(\\omega+\\delta)]\\ge(1/2)\\lambda_{\\operatorname*{min}}\\cdot m\\cdot\\sigma^{2}$ . But, under Assumption 1, we know that the surrogate\u2019s output \u2013 and hence $F_{\\mathcal{X}}(\\omega+\\delta)$ \u2019s \u2013 is bounded within [0, 1], which means $1\\geq(1/2)\\lambda_{\\operatorname*{min}}\\cdot m\\stackrel{\\cdot}{\\cdot}\\sigma^{2}$ or equivalently, $\\sigma^{2}\\leq2/(\\dot{m}\\lambda_{\\mathrm{min}})$ as stated in Lemma 5\u2019s statement. That is, under Assumption 1, Lemma 5 is only correct for $\\sigma^{2}\\in\\big(0,2/(m\\lambda_{\\mathrm{min}})\\big)$ . ", "page_idx": 17}, {"type": "text", "text": "E Proof of Theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We are now ready to prove our main result. First, we re-state the result of Lemma 4 below, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\delta\\sim N(0,\\sigma^{2}\\mathbf{I})}{\\mathbb{E}}[F_{\\mathcal{X}}(\\omega+\\delta)]}&{\\leq\\ \\ \\operatorname*{max}_{\\|\\delta\\|\\leq\\rho}[F_{\\mathcal{D}}(\\omega+\\delta)]}\\\\ &{+\\ \\ {\\sqrt{\\frac{m\\log\\left(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\right)+2\\log\\frac{n}{\\alpha}+4\\log(8n+4m)}{n-1}}}}\\\\ &{=\\ \\ R_{\\mathcal{D}}(\\omega)}\\\\ &{+\\ \\ {\\sqrt{\\frac{m\\log\\left(1+\\frac{\\|\\omega\\|_{2}^{2}}{m\\sigma^{2}}\\right)+2\\log\\frac{n}{\\alpha}+4\\log(8n+4m)}{n-1}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last step follows from the definition of $\\mathcal{R}_{\\mathcal{D}}(\\omega)$ . Finally, plugging Eq. (78) into Eq. (63) and replacing $\\begin{array}{r}{\\operatorname*{max}_{\\|\\pmb{\\delta}\\|\\leq\\rho}F_{\\mathcal{X}}(\\pmb{\\omega}+\\pmb{\\delta})}\\end{array}$ with $\\mathcal{R}_{\\mathcal{X}}(\\omega)$ (following its definition) completes our proof. ", "page_idx": 17}, {"type": "text", "text": "F Effective Approximation of $\\nabla_{\\omega}\\|\\nabla_{\\omega}h(\\omega)\\|$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Explicitly, the gradient of the augmented loss in Eq. (12) is given below, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{\\omega}\\mathcal{L}(\\omega,\\lambda)}&{=}&{\\nabla_{\\omega}\\mathcal{L}_{\\mathcal{D}}(\\omega)\\;+\\;\\lambda\\cdot\\rho\\cdot\\nabla_{\\omega}\\left\\|\\nabla_{\\omega}h(\\omega)\\right\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the computation bottleneck lies with the second gradient term which can be further expressed below using the chain rule, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\omega}\\big\\|\\nabla_{\\omega}h(\\omega)\\big\\|\\,=\\,\\nabla_{\\omega}^{2}h(\\omega)\\cdot\\Big(\\nabla_{\\omega}h(\\omega)\\,/\\,\\big\\|\\nabla_{\\omega}h(\\omega)\\big\\|\\Big)\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is evident that $\\nabla_{\\omega}^{2}h(\\omega)$ in Eq. (80) represents a Hessian matrix. Calculating the Hessian matrix for a deep neural network is impractical due to the extensive dimensions of the model\u2019s weights. Nevertheless, since Eq. (80) involves the multiplication of a Hessian matrix with a vector, specific techniques like Hessian-vector products can be employed to approximate this product. In particular, let the Hessian matrix $\\mathbf{M}=\\nabla_{\\omega}^{2}h(\\omega)$ , we have Taylor expansion for function $\\bar{\\nabla}_{\\omega}h(\\omega)$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{\\omega}h(\\omega+\\Delta\\omega)}&{=}&{\\nabla_{\\omega}h(\\omega)\\;+\\;{\\bf M}\\Delta\\omega\\;+\\;\\mathcal{O}\\big(\\|\\Delta\\omega\\|^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This approximation becomes precise as the value of $\\Delta\\omega$ approaches 0. Following [26] and [27], we choose $\\Delta\\omega=r{\\mathbf v}$ where $r$ is a small scalar and $\\mathbf{v}$ is a vector, that transforms Eq. (81) into ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{{\\bf M}{\\bf v}}&{=}&{\\frac{1}{r}\\Big(\\nabla_{\\omega}h(\\omega+\\Delta\\omega)-\\nabla_{\\omega}h(\\omega)\\Big)\\,+\\,\\mathcal{O}(r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, choosing $\\begin{array}{r}{\\mathbf{v}=\\frac{\\nabla_{\\omega}h(\\omega)}{\\|\\nabla_{\\omega}h(\\omega)\\|}}\\end{array}$ leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\omega}\\|\\nabla_{\\omega}h(\\omega)\\|~~=~~\\mathbf{M}\\frac{\\nabla_{\\omega}h(\\omega)}{\\|\\nabla_{\\omega}h(\\omega)\\|}\\,\\simeq~\\frac{1}{r}\\Big(\\nabla_{\\omega}h\\left(\\omega~+~r\\frac{\\nabla_{\\omega}h(\\omega)}{\\|\\nabla_{\\omega}h(\\omega)\\|}\\right)-\\nabla_{\\omega}h(\\omega)\\Big)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is noticed that, as pointed out by [27], an inappropriate choice of $r$ can make Eq. (83) vulnerable to numeric and roundoff issues. The constant $r$ must be sufficiently small so that the $O(r)$ term becomes negligible. However, when $r$ is too small, precision is compromised because the subtraction of the original gradient from the perturbed one, i.e., $\\nabla_{\\omega}h(\\omega+r\\nabla_{\\omega}h(\\omega)/\\|\\nabla_{\\omega}h(\\omega)\\|)-\\nabla_{\\omega}h(\\omega),$ will obtain a small difference between them. Based on Eq. (83), Eq. (79) would be ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\omega}\\mathcal{L}(\\omega)\\;\\;=\\;\\;\\nabla_{\\omega}\\mathcal{L}_{\\mathcal{D}}(\\omega)\\;+\\;\\frac{\\lambda\\rho}{r}\\left(\\nabla_{\\omega}h\\left(\\omega+r\\frac{\\nabla_{\\omega}h(\\omega)}{\\|\\nabla_{\\omega}h(\\omega)\\|}\\right)-\\nabla_{\\omega}h(\\omega)\\right)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In practical applications, we typically employ an additional approximation to compute the second term in Eq. (84), thereby avoiding the need for Hessian computation induced by the chain rule. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{\\omega}h\\left(\\omega\\;+\\;r\\frac{\\nabla_{\\omega}h(\\omega)}{\\left\\|\\nabla_{\\omega}h(\\omega)\\right\\|}\\right)}&{\\simeq}&{\\nabla_{\\omega}h(\\omega)\\Big|_{\\omega=\\omega+\\;r\\frac{\\nabla_{\\omega}h(\\omega)}{\\left\\|\\nabla_{\\omega}h(\\omega)\\right\\|}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "G Hyperparameter Tuning and Additional Experimental Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G.1 Computation Resource ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All our experiments were conducted on a system with the following specifications: Ubuntu 18.04, NVIDIA RTX 3090 GPUs, and CUDA 11.8. ", "page_idx": 18}, {"type": "text", "text": "G.2 IGNITE-2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "IGNITE-2. To solve the Lagrangian in Eq. (12), we can treat $\\lambda$ as a hyper-parameter and optimize for $\\omega$ using stochastic gradient descent (SGD). ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l c c l}{\\omega^{t+1}}&{\\leftarrow}&{\\omega^{t}}&{-}&{\\eta_{\\omega}\\cdot\\left(\\nabla_{\\omega}\\mathcal{L}_{\\mathcal{D}}\\left(\\omega^{t}\\right)\\right.\\ +}&{\\lambda\\cdot\\rho\\cdot\\nabla_{\\omega}\\left\\|\\nabla_{\\omega}h\\left(\\omega^{t}\\right)\\right\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\omega^{t}$ is the estimation of the surrogate\u2019s parameter at the $t^{t h}$ iteration, and $\\eta_{\\omega}$ is step size to update $\\omega$ . We name this method IGNITE-2 and its pseudo-code is in Algorithm 2. ", "page_idx": 18}, {"type": "text", "text": "Hyper-parameter Configuration. Our method IGNITE-2 introduces three additional hyperparameters: $\\lambda,\\rho_{!}$ , and $r$ . The penalty coefficient $\\lambda$ controls the gradient magnitude of our regularizer, set to 0.01 through a grid search within $\\{0.0001,0.001,0.01\\}$ . The hyper-parameters $\\rho$ and $r$ are chosen from $\\left\\lbrace0.01,0.05,0.1,0.2,0.5\\right\\rbrace$ , with $\\rho$ set to 0.2 and $r$ set to 0.2 for IGNITE. These three hyper-parameters are determined through experiments in Section G.4. These hyper-parameters are consistently applied across all experiments, except for the ICT baseline $\\mathit{\\Delta}\\lambda=1e\\mathrm{~-~}4\\$ due to implementation differences. ", "page_idx": 18}, {"type": "text", "text": "Table 5: The percentage improvement in performance achieved by IGNITE-2 and IGNITE across all tasks and baseline algorithms at the 100th percentile level is presented. Gain signifies the percentage gain over the baseline performance (Base). ", "page_idx": 19}, {"type": "table", "img_path": "ag7piyoyut/tmp/4a20bdf1479f9410f630d5f4debd70df12b3a1357e6c61dff91119524b1502c1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Algorithm 2 IGNITE-2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Input: offilne data $\\mathbf{\\mathcal{D}}=\\{(\\mathbf{x}_{i},z_{i})\\}_{i=1}^{n}$ ; initial surrogate $g(\\mathbf{x};\\omega^{(0)})$ ; no. of iterations $T$ ; batch size $m$ ; Lagrange multiplier $\\lambda$ ; perturbation radius $\\rho$ and scalar $r$ ; step sizes $\\eta_{\\omega}$ . ", "page_idx": 19}, {"type": "text", "text": "1: Initialize $\\boldsymbol{\\omega}^{(1)}\\leftarrow\\boldsymbol{\\omega}^{(0)}$   \n2: for $t\\gets1:T$ do   \n3: Sample $\\boldsymbol{\\mathcal{B}}=\\left\\{(\\mathbf{x}_{i},z_{i})\\right\\}_{i=1}^{m}\\sim\\mathcal{D}$   \n45:: CCoommppuuttee $\\hat{z}_{i}=g(\\mathbf{x}_{i};\\pmb{\\omega}^{(t)})$ $\\begin{array}{r}{g_{1}=m^{-1}\\sum_{i=1}^{m}\\nabla_{\\omega}\\ell(\\hat{z}_{i},z_{i})}\\end{array}$ $i\\in[m]$   \n6: Compute $\\begin{array}{r}{g_{2}=m^{-1}\\sum_{i=1}^{m}\\nabla_{\\omega}\\hat{z}_{i}}\\end{array}$   \n7: Compute $\\hat{\\omega}=\\pmb{\\omega}^{(t)}+\\pmb{r}\\cdot\\pmb{g}_{2}/\\|g_{2}\\|$   \n8: Compute $\\begin{array}{r}{g_{3}=m^{-1}\\sum_{i=1}^{m}\\nabla_{\\omega}g(\\mathbf{x}_{i};\\hat{\\omega})}\\end{array}$   \n9: Compute $g^{(t)}=g_{1}+\\lambda\\rho r^{-1}(g_{3}-g_{2})$   \n10: Update $\\boldsymbol{\\omega}^{(t+1)}\\gets\\boldsymbol{\\omega}^{(t)}-\\eta\\boldsymbol{\\omega}g^{(t)}$   \n11: end for   \n12: return learned surrogate \u03c9(T +1) ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "G.3 Performance Evaluation at 100-th, 80-th and 50-th Percentile Level of IGNITE and IGNITE-2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we presented the percentage improvement over baseline performance attained by IGNITE-2 and IGNITE when it is applied to an existing baseline. We have evaluated this at the 100-th, 80-th, and 50-th percentile levels. The results are reported in Table 5, 6, and 7, respectively. ", "page_idx": 19}, {"type": "table", "img_path": "ag7piyoyut/tmp/154c479f4944e51978d08929bafceebed6fc54368363e65fed621ea48199d16e.jpg", "table_caption": ["Table 6: The percentage improvement in performance achieved by IGNITE-2 and IGNITE across all tasks and baseline algorithms at the 80th percentile level is presented. Gain signifies the percentage gain over the baseline performance (Base). "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "G.3.1 Performance Evaluation at 100-th Percentile Level of IGNITE-2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "According to the reported results in Table 5, IGNITE-2 consistently maintains a high probability of $86.36\\%$ (38 out of 44) of not degrading baseline performance. There is a high likelihood $72.27\\%=34$ out of 44 cases) of improving baseline performance, with an average improvement of approximately $1.39\\%$ and a notable peak improvement of $6.8\\%$ . Conversely, IGNITE-2 also exhibits a relatively low probability $\\mathrm{(13.64\\%=6}$ out of 44 cases) of decreasing performance, with an average degradation of approximately $0.45\\%$ and a minor peak degradation of $0.8\\%$ .Additionally, there is a minor probability $(9.09\\%=4$ out of 44 cases) of maintaining baseline performance. Furthermore, while IGNITE-2 demonstrates significant efficiency, the results in Section 5.2 indicate that IGNITE even outperforms IGNITE-2 . ", "page_idx": 20}, {"type": "text", "text": "G.3.2 Performance Evaluation at 80-th Percentile Level of IGNITE and IGNITE-2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As shown in Table 6, IGNITE-2 consistently maintains a high probability high probability of not degrading baseline performance, with a likelihood of $61.36\\%$ (27 out of 44 cases). There is a $\\mathrm{\\dot{4}7.73\\%}$ chance (21 out of 44 cases) of improving baseline performance, with an average improvement of approximately $0.6\\%$ and a peak improvement of $2.4\\bar{\\%}$ . On the other hand, IGNITE-2 also indicates a low likelihood $(38.64\\%=17$ out of 44 cases) of performance decrease, with an average decline of around $0.68\\%$ and a maximum decline of $4.6\\%$ . Furthermore, there is a slight chance ( $13.64\\%=6$ out of 44 cases) of maintaining the baseline performance. ", "page_idx": 20}, {"type": "table", "img_path": "ag7piyoyut/tmp/5f797be99ee8eae4da53bd8732506334e8e300e93e52ecf3c4bfb51fd6d1017c.jpg", "table_caption": ["Table 7: The percentage improvement in performance achieved by IGNITE-2 and IGNITE across all tasks and baseline algorithms at the 50th percentile level is presented. Gain signifies the percentage gain over the baseline performance (Base). "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "In addition, IGNITE also maintains a high probability of $72.73\\%$ (32 out of 44) of not degrading baseline performance. There is a $47.73\\%$ likelihood (21 out of 44 cases) of improving baseline performance, with an average improvement of approximately $1.0\\%$ and a peak improvement of $\\bar{2}.6\\%$ . Besides, IGNITE also exhibits a low probability $(27.27\\%=12$ out of 44 cases) of decreasing performance, with an average degradation of approximately $1.58\\%$ and a peak degradation of $4.6\\%$ . Additionally, there is a small probability $25\\%=11$ out of 44 cases) of maintaining baseline performance. ", "page_idx": 21}, {"type": "text", "text": "G.3.3 Performance Evaluation at 50-th Percentile Level of IGNITE and IGNITE-2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 7 displays the results of IGNITE-2 and IGNITE at the 50th percentile level. The results indicate that both IGNITE-2 and IGNITE do not degrade performance compared to the baseline in $65.91\\%$ of settings (29 out of 44) for each method. In which, IGNITE-2 outperforms the baseline in $50\\%$ likelihood, with an average improvement of approximately $0.85\\%$ and a peak improvement of $2.9\\%$ . For IGNITE , those are $52.27\\%$ likelihood, $1.89\\%$ average improvement and a peak improvement of $14.6\\%$ . Conversely, there are only $34.09\\%$ cases in using our proposed method leads to the performance reduction. The average degradation of IGNITE-2 and IGNITE are $0.69\\%$ and $1.19\\%$ , respectively. Overall, we state that our proposed method helps to boost the performance of almost all algorithms and tasks. ", "page_idx": 21}, {"type": "image", "img_path": "ag7piyoyut/tmp/75136895e60d62866b5715520d9963d62be65c87840d9828fcfd234283010034.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 3: Performance vs. the no. of gradient ascent steps during optimization of IGNITE-2 and Baseline optimized algorithms, e.g, COMs and GA. ", "page_idx": 22}, {"type": "image", "img_path": "ag7piyoyut/tmp/c0939acfd82b7c438aaeb092337e9a331c40491fbaac59039011293ab78c9ad0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 4: Performance variation of COMS and GA (regularized by IGNITE-2) in the change of the regularization coefficient $\\lambda\\ \\in$ [0.0001, 0.001, 0.01]. ", "page_idx": 22}, {"type": "image", "img_path": "ag7piyoyut/tmp/a4c54a9a760888ddb691f33565a7b4790c22cc42ff2d477b49a8179046dc5506.jpg", "img_caption": ["Figure 5: Performance variation of COMS and GA (regularized by IGNITE2) in the change of the hyper-parameter $r\\in[0.01,0.05,0.\\dot{1},0.2,0.5]$ . "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ag7piyoyut/tmp/16378ea564ed8415dc330836567081dbce7946f3cbf1baece8d3d1e32d685dfe.jpg", "img_caption": ["Figure 6: Performance variation of COMS and GA (regularized by IGNITE2) in the change of the hyper-parameter $\\rho\\in[0.01,0.05,0.\\dot{1},0.2,0.5]$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "G.4 Hyper-parameters selection for IGNITE-2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section provides the specific setup of the hyper-parameter selection experiments for IGNITE-2, which is also ran separately to determine the hyperparameters for IGNITE(in the main text). ", "page_idx": 22}, {"type": "text", "text": "G.4.1 IGNITE-2 enhances stability of COMs and gradient ascent (GA). ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figure 3 provides a step-by-step performance comparison between two baseline algorithms, COMs and GA, with and without our regularization method, IGNITE-2. IGNITE-2 consistently outperforms the baseline algorithms at every step in the ANT tasks. However, the trend differs for TF-BIND-10. Initially, the baseline algorithms perform better without IGNITE-2 . However, as the number of gradient ascent steps increases, the performance of the baselines without IGNITE2 begins to decline. In contrast, the algorithms incorporating our method maintain or improve their performance over time. This trend suggests that our method becomes increasingly beneficial in the later stages of the optimization process, ultimately enhancing the overall performance of the baseline algorithms. ", "page_idx": 22}, {"type": "text", "text": "G.4.2 Choosing the regularization coefficient $\\lambda$ . ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figure 4 shows how the performance of baseline models regularized with IGNITE-2 is affected by different values of $\\lambda$ . The results indicate that using an excessively low or high value for $\\lambda$ will have a negative impact on performance. In all cases, the results suggest that a universal value of 0.01 for $\\lambda$ tends to generate consistent and effective performance across all tasks. ", "page_idx": 22}, {"type": "text", "text": "G.4.3 Choosing value for parameter $r$ and the perturbation radius $\\rho$ . ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figure 5 plot the performance of the GA and COMS baselines regularized with IGNITE-2 with respect to the change of hyper-parameter $r$ . That is $r\\in\\{0.01,0.05,0.1,0.2,0.5\\}$ . Figure 6 visualizes how the performance of baselines regularized with IGNITE-2 is influenced by varying the hyperparameter $\\rho$ . Based on those result, we choose to set $r=0.2$ and $\\rho=0.2$ in all of the experiments for IGNITE-2. ", "page_idx": 22}, {"type": "text", "text": "H Percentage improvement over other baselines of IGNITE, SAM across all tasks. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Table 8: Percentage improvement over other baselines of IGNITE, SAM across all tasks. ", "page_idx": 23}, {"type": "table", "img_path": "ag7piyoyut/tmp/4b1203a6e8e925a41f093cbc80821e77927066b7f44206b03526663057c83324.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "I Complexity Overhead of IGNITE. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To analyze the computational complexity of IGNITE, we break down the complexity of each step in Algorithm 1. ", "page_idx": 23}, {"type": "text", "text": "1. Initialization (Line 1): Initializing $\\boldsymbol{\\omega}^{(1)}\\gets\\boldsymbol{\\omega}^{(0)}$ and $\\lambda^{(1)}\\gets\\lambda\\colon O(1)$ each.   \n2. Main Loop (Line 2-12): The loop runs for $T$ iterations. Thus, the complexity of the main loop will be multiplied by $T$ .   \n3. Sampling (Line 3): Sampling a batch $\\mathcal{B}=\\left\\{(\\mathbf{x}_{i},z_{i})\\right\\}_{i=1}^{m}\\sim\\mathcal{D};\\,O(m)$ .   \n4. Computing $\\hat{z}_{i}$ (Line 4): Evaluating the surrogate model $g(\\mathbf{x}_{i};\\omega^{(t)})$ for each $i\\,\\in\\,[m]$ . Assuming the surrogate model evaluation has a computational complexity of $C_{g}=O(d)$ per sample where $d$ is the number of surrogate parameters, the total complexity is $O(m\\cdot d)$ .   \n5. Computing $g_{1}$ and $g_{2}$ (Line 5-6): Computing gradients $\\nabla_{\\omega}\\ell(\\hat{z}_{i},z_{i})$ and $\\nabla_{\\omega}\\hat{z}_{i}$ have complexities $O(C_{\\ell})$ and $O(C_{\\hat{z}})$ respectively per sample where $C_{\\ell}=C_{\\hat{z}}=O(d)$ . Therefore, the total complexities are $O(m\\cdot d)$ .   \n6. Computing $\\hat{\\omega}$ (Line 7): This involves simple vector operations with complexity $O(d)$ , where $d$ is the dimensionality of $\\omega$ .   \n7. Computing $g_{3}$ (Line 8): Similar to lines 4 and 6, involving evaluating the surrogate and gradient computations, with complexity $O(m\\cdot C_{g}+m\\cdot C_{\\hat{z}})=O(m\\cdot d)$ .   \n8. Computing $g^{(t)}$ (Line 9): Vector operations involving addition and scalar multiplication with complexity $O(d)$ .   \n9. Updating $\\omega$ (Line 10): Updating $\\omega$ involves simple subtraction operations with complexity $O(d)$ .   \n10. Updating $\\lambda$ (Line 11): Updating $\\lambda$ is an $O(1)$ operation ", "page_idx": 23}, {"type": "text", "text": "Overall Complexity: Considering the above steps, the most computationally expensive parts are the gradient computations in lines 5, 6, and 8. Thus, the overall complexity per iteration is: ", "page_idx": 23}, {"type": "equation", "text": "$$\nO(2m\\cdot C_{g}+m\\cdot C_{\\ell}+2m\\cdot C_{\\hat{z}})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since this loop runs for $T$ iterations, the total complexity is: ", "page_idx": 23}, {"type": "equation", "text": "$$\nO(T\\cdot(2m\\cdot C_{g}+m\\cdot C_{\\ell}+2m\\cdot C_{\\hat{z}}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, we have the total complexity of the original baseline is: ", "page_idx": 23}, {"type": "equation", "text": "$$\nO(T\\cdot(m\\cdot C_{g}+m\\cdot C_{\\ell}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "ag7piyoyut/tmp/1dd7d8081f8d22852916d7016b1cdf0dd7881c75f82b8e7686ebab4be1afe89b.jpg", "img_caption": ["Figure 7: The convergence of the proposed optimization algorithm. Figure shows the training loss and the sharpness value (plotting $\\|\\nabla_{\\omega}h(\\omega)\\|$ value) during the surrogate fitting process. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "", "table_caption": ["Table 9: The empirical time (seconds) of the participating baselines with and without IGNITE. "], "table_footnote": [], "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\underline{{1}}}\\mathrm{\\underline{{gorithms}}}}\\\\ &{\\mathrm{\\underline{{1}}}\\mathrm{\\underline{{EINFORCE}}}}\\\\ &{\\mathrm{\\underline{{1}}}\\mathrm{\\underline{{EINFORCE}}}+\\mathrm{\\underline{{16\\mathrm{NITE}}}}\\quad\\left|\\begin{array}{l l l}{\\mathrm{\\underline{{1}}}\\mathrm{\\underline{{1}}}\\mathrm{\\underline{{n}}}\\mathrm{\\underline{{t}}}}&{|\\mathrm{\\underline{{~D^{\\vee}K i t y}}}}&{|\\mathrm{\\underline{{~TF\\,Bind}}}\\mathrm{\\underline{{8}}}\\quad\\quad|\\mathrm{\\underline{{~TF\\,Bind}}~}10\\mathrm{\\underline{{\\delta}}}|}\\\\ {|\\mathrm{\\underline{{172.08}}}}&{|\\mathrm{\\underline{{252.33}}}}&{|\\mathrm{\\underline{{477.09}}}}\\end{array}\\right|}\\\\ &{\\mathrm{\\underline{{1}}}\\mathrm{\\underline{{EINFORCE}}}+\\mathrm{\\underline{{1GNITE}}}\\quad\\left|\\begin{array}{l l l}{194.02\\mathrm{\\ensuremath{\\left(+12.75\\varphi_{0}\\right)}}}&{|\\mathrm{\\underline{{275.15\\left(+9.04\\mathcal{G}_{\\ell}\\right)}}}}&{|\\mathrm{\\underline{{582.28\\left(+22.05\\mathcal{G}_{\\ell}\\right)}}}}\\\\ {|\\mathrm{\\underline{{69.99}}}}&{|\\mathrm{\\underline{{168.81}}}}&{|\\mathrm{\\underline{{49.63}}}}\\\\ {\\mathrm{\\underline{{85.15\\left(+21.66\\mathcal{G}_{\\ell}\\right)}}}}&{|\\mathrm{\\underline{{191.83\\times\\left(+13.64\\mathcal{G}_{\\ell}\\right)}}}}&{|\\mathrm{\\underline{{181.71\\left(+21.44\\mathcal{G}_{\\ell}\\right)}}}}\\end{array}\\right|\\begin{array}{l}{364.16\\mathrm{\\qquad\\qquad}}\\\\ {369.29\\mathrm{\\ensuremath{\\left(+1.41\\mathcal{G}_{\\ell}\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thereby, IGNITE will include an additional complexity: ", "page_idx": 24}, {"type": "equation", "text": "$$\nO(T\\cdot(m\\cdot C_{g}+2m\\cdot C_{\\hat{z}}))=O(T m d)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where: ", "page_idx": 24}, {"type": "text", "text": "\u2022 $T$ is the number of iterations.   \n\u2022 $m$ is the batch size.   \n\u2022 $C_{g}=O(d)$ is the complexity of evaluating the surrogate model.   \n\u2022 $C_{\\ell}=O(d)$ is the complexity of computing the loss gradient.   \n\u2022 $C_{\\hat{z}}=O(d)$ is the complexity of computing the gradient of the surrogate output with respect to its parameters.   \n\u2022 $d$ is the no. of surrogate parameters. ", "page_idx": 24}, {"type": "text", "text": "The empirical training time of the participating baselines with and without IGNITEis reported in Table 9. We observe that IGNITE solely consumes an additional negligible GPU memory, while the training time increases by $14.91\\%$ on average. ", "page_idx": 24}, {"type": "text", "text": "J Convergence and effectiveness of IGNITE ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "According to our experiment, despite using a relatively simple BDMM to solve Eq. (12), we have achieved significant improvement in most cases. To demonstrate the convergence of the optimization algorithm, we have plotted the training loss and the sharpness value (plotting $\\|\\nabla_{\\omega}h(\\omega)\\|$ value) during the surrogate fitting process. This is based on an experiment using GA and REINFORCE baselines on Ant and Dkitty tasks. The results are illustrated in Figure 7. These results reveal that BDMM helps decrease both the training loss and sharpness value of the surrogate model during the training phase. This indicates that BDMM is effective and the optimization converges well in practice. Furthermore, our method, IGNITE, can be seamlessly integrated with other, more robust optimization techniques to solve Eq. (12). ", "page_idx": 24}, {"type": "text", "text": "K Limitation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our paper studies the offline optimization task, which has potential applications in material engineering. Similar to the existing literature, our method is extensively tested on a universal benchmark set forward by the pioneering work of [4]. However, it is important to note that the benchmark consists mostly of small- to mid-scale optimization tasks. As such, our method has not considered the challenge of scalability in large-scale domain with extremely high-dimensional input spaces. In addition, as with all machine learning algorithms, while applications of our work to real data could result in ethical considerations, this is an indirect and unpredictable side-effect of our work. Our experimental work uses publicly available datasets to evaluate the performance of our algorithms. No ethical considerations are raised. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our contributions can be found in Section 3 and Section 4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss this in Appendix K ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All our assumptions are detailed in Section 4. All proofs are detailed in the Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes. All information regarding our experiments are disclosed in Section 5 and in the Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our experimental code is also submitted (as extra materials) along with our manuscript. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Such details can be founded in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do report error bars in our experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The information regarding our compute resource is detailed in Appendix G. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and do not find our work in violation of any aspects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We did discuss this in Appendix K. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not create new datasets. We only use existing, publicly available datasets. We also do not create any new pre-trained models. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We cite the source of all datasets used in our experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work does not release any new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]