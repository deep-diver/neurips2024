[{"type": "text", "text": "Revisiting Score Propagation in Graph Out-of-Distribution Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Longfei Ma\u2217 College of Computer Science and Technology Zhejiang University longfeima@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Yiyou Sun\u2217   \nDepartment of Computer Sciences   \nUniversity of Wisconsin-Madison sunyiyou@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Kaize Ding Zemin Liu\u2020 Department of Statistics and Data Science College of Computer Science and Technology Northwestern University Zhejiang University kaize.ding@northwestern.edu liu.zemin@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Fei Wu\u2020 College of Computer Science and Technology Zhejiang University wufei@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The field of graph learning has been substantially advanced by the development of deep learning models, in particular graph neural networks. However, one salient yet largely under-explored challenge is detecting Out-of-Distribution (OOD) nodes on graphs. Prevailing OOD detection techniques developed in other domains like computer vision, do not cater to the interconnected nature of graphs. This work aims to flil this gap by exploring the potential of a simple yet effective method \u2013 OOD score propagation, which propagates OOD scores among neighboring nodes along the graph structure. This post hoc solution can be easily integrated with existing OOD scoring functions, showcasing its excellent flexibility and effectiveness in most scenarios. However, the conditions under which score propagation proves beneficial remain not fully elucidated. Our study meticulously derives these conditions and, inspired by this discovery, introduces an innovative edge augmentation strategy with theoretical guarantee. Empirical evaluations affirm the superiority of our proposed method, outperforming strong OOD detection baselines in various scenarios and settings. Code can be found in https://github.com/longfei-ma/GRASP. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph-like data structures are ubiquitous in many domains, such as social networks [87, 40], molecular chemistry [19, 82], and recommendation systems [84, 45]. As graph neural networks increasingly serve as powerful tools for navigating this complex data landscape, a compelling yet under-explored issue emerges: Out-of-Distribution (OOD) node detection. Imagine a recommender system suggesting irrelevant or even harmful products to users, or a bioinformatics algorithm misusing an unknown protein. This gives rise to the importance of OOD detection in graph data, which determines whether an input is in-distribution (ID) or OOD and enables the model to take precautions. ", "page_idx": 0}, {"type": "text", "text": "While existing OOD detection methods have shown promising results in computer vision [66, 27, 18, 15, 95], natural language procession [11, 57] and tabular data analytics [68], their effectiveness diminishes when applied to graph data [77]. These conventional techniques operate under the assumption that data points are independently sampled, which misaligns with the interconnected nature of graphs. ", "page_idx": 1}, {"type": "text", "text": "To better leverage the structural knowledge from the graph, OOD score propagation [77] has been employed to enhance graph OOD detection performance by directly propagating the computed OOD scores along the graph structure (as shown in Figure 1). Although this strategy has shown promising results on some datasets, the reasons behind its effectiveness and the conditions under which it works are not clear. To dive deep into it, our research embarks on addressing two research questions related to OOD score propagation: ", "page_idx": 1}, {"type": "text", "text": "Question 1: \"Will naive OOD score propagation always help graph OOD Detection?\" The short answer is no. This method can be ineffective on graphs where inter-edges (ID-to-OOD) are predominant. Using the examplse in Figure 2(a), where only inter-edges exist, prior to conducting score propagation, all ID and OOD nodes can be fully distinguished. However, after performing score propagation along these edges, the ID and OOD nodes are completely misclassified. Conversely, after adding intra-edges and making them donimate, as shown in Figure 2(b), score propagation would be beneficial to distinguish ID and OOD nodes. These two examples intuitively illustrate how the ratio of intra-edges and inter-edges can impact the effectiveness of OOD score propagation. We substantiate this intuition in Section 3. This finding naturally paves the way for subsequent questions. ", "page_idx": 1}, {"type": "image", "img_path": "jb5qN3212b/tmp/a490bd435e5286575d23a7ef2baaa711d2525fc7b5377fcf8f3eb331c513c6d2.jpg", "img_caption": ["Figure 1: Illustration of OOD scores propagation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Question 2: \"How to derive a better score propagation strategy for graph OOD detection?\" Building on our prior findings, we propose a graph augmentation strategy as presented in Section 4. Specifically, our strategy selects a subset $G$ of the training set and puts additional edges to the nodes within $G$ . Beyond its practical implications, our solution is theoretically supported: When $G$ predominantly connects to ID data over OOD data, our strategy can provably enhance the post-propagation OOD detection outcomes. ", "page_idx": 1}, {"type": "text", "text": "We summarize our contributions as below: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretical understanding: We delve deeply into the mechanism of score propagation to understand its potential for graph OOD detection and elucidate the conditions under which it thrives, providing an understanding that extends beyond existing knowledge. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Practical solution: To counter the identified challenge of inter-edges\u2019 domination, we propose GRaph-Augmented Score Propagation (GRASP), an innovative edge augmentation strategy with theoretical guarantee. By strategically adding edges to a chosen subset $G$ of the training set, as detailed in Section 4, our method aims to enhance the intra-edge ratio, thereby boosting OOD detection outcomes post-propagation. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Empirical studies: We demonstrate the superior performance of the proposed method on extensive graph OOD detection benchmarks, different pre-trained methodologies [34, 69, 7, 94], and different OOD scoring functions. Under the same condition, our proposed strategy substantially reduces the FPR95 by $17.87\\%$ and $32.21\\%$ compared to the strongest graph OOD detection baselines on common and large-scale benchmarks respectively. Comprehensive analyses are also provided to validate the effectiveness of the proposed approach and the correctness of the theoretical findings. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Problem setup. We consider a traditional semi-supervised node classification setting with the additional unlabeled nodes from the out-of-distribution class. Let $\\mathcal{G}=\\{\\mathcal{V},\\mathcal{E}\\}$ denote the graph with nodes $\\mathcal{V}$ and edges $\\mathcal{E}$ , where the node set $\\mathcal{V}$ with size $N$ are attributed with data matrix $X\\in\\mathbb{R}^{N\\times d}$ . The structure of graph $\\mathcal{G}$ is described by the adjacency matrix $A\\in\\{0,1\\}^{N\\times N}$ . We let the corresponding row-stochastic matrices as $\\bar{A}=\\mathrm{D}^{-1}A$ , where $D$ is the diagonal matrix with $\\begin{array}{r}{D_{i i}=\\sum_{j}\\Bar{A_{i j}}}\\end{array}$ . The $N$ nodes are partially labeled, so we let $\\nu_{l}$ and $\\nu_{u}$ represent the labeled and unlabe led node sets respectively, i.e, $\\mathcal{V}=\\mathcal{V}_{l}\\cup\\mathcal{V}_{u}$ . Given a training set $\\mathcal{D}^{t r}=\\left\\{\\left(\\mathbf{x}_{i},y_{i}\\right)\\right\\}_{i\\in\\mathcal{V}_{l}}$ with $\\mathbf{x}_{i}$ as the $i$ -th row of $X$ and $y_{i}\\in\\mathcal{P}\\triangleq\\{1,\\cdots\\,,C\\}$ , the goal of node classification is to learn a mapping $f:\\mathcal{V}\\to\\mathbb{R}^{C}$ from the nodes to the probability of each class. ", "page_idx": 1}, {"type": "image", "img_path": "jb5qN3212b/tmp/d4540ec41be56a697a1da91689d939eec7a3f9933875868a0e3359325515cdbb.jpg", "img_caption": ["Figure 2: Two illustrative examples when scoring propagation is harmful/helpful. We consider ID nodes in green and OOD nodes in red. Inter-edges are defined to be ID-to-OOD edges and Intra-edges are ID-to-ID or OOD-to-OOD edges. The value represents the respective OOD scores. Consequently, the propagated scores in these cases will be the mean of the scores of adjacent nodes as shown in Figure 1. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Out-of-distribution detection. When deploying a model in the real world, a reliable classifier should not only accurately classify known in-distribution (ID) nodes, but also identify \u201cunknown\u201d nodes or OOD nodes. Formally, we can represent the unlabeled node set by $\\mathcal{V}_{u}=\\mathcal{V}_{u i d}\\cup\\mathcal{V}_{u o o d}$ where $\\nu_{u i d}$ and $\\nu_{u o o d}$ represent the in-distribution (ID) node and OOD node respectively. The goal of the graph OOD detection is to derive an algorithm to decide if a node $i\\in\\mathcal{V}_{u}$ is from $\\nu_{u o o d}$ or $\\nu_{u i d}$ . ", "page_idx": 2}, {"type": "text", "text": "This can be achieved by having an OOD detector, in tandem with the node classification model $f$ . OOD detection can be formulated as a binary classification problem. At test time, the goal of OOD detection is to decide whether an unlabeled node $i\\in\\mathcal{V}_{u}$ is from ID or OOD. The decision can be made via a level set estimation: ", "page_idx": 2}, {"type": "equation", "text": "$$\nF_{O O D D}(i,{\\mathcal G};\\lambda)=\\left\\{\\operatorname{ID}\\quad g(\\mathbf{x}_{i})\\geq\\lambda\\right.,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where nodes with higher scores $g\\mathbf{(x}_{i})$ are classified as $\\mathrm{ID}$ and vice versa, and $\\lambda$ is the threshold commonly chosen so that a high fraction (e.g., $95\\%$ ) of ID data is correctly classified. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we consider post hoc OOD detection methods to produce $g\\mathbf{(x}_{i})$ which does not require expensive re-training. As an example, a classical way to compute $g\\mathbf{(x}_{i})$ is Maximum Softmax Probability (MSP) [23] which is given by the maximum softmax value. We include details of the considered OOD detection methods in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "3 Will propagation always help Graph OOD Detection? ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the introduction, we delineate the limitations of OOD score propagation using a concrete example and elucidate the intuition that it may fail when inter-edges dominate. In this section, we formally delineate the conditions under which OOD score propagation works. We start by showing the formal definition of propagation. ", "page_idx": 2}, {"type": "text", "text": "Define OOD scoring propagation. Given a raw OOD scoring vector $\\hat{\\mathbf{g}}\\in\\mathbb{R}^{N}$ with $\\hat{\\bf g}_{i}=g({\\bf x}_{i})$ , the propagated scoring vector is given by: ", "page_idx": 2}, {"type": "text", "text": "where $k\\in\\mathbb{N}^{+}$ are hyperparameters. ", "page_idx": 2}, {"type": "text", "text": "Is it necessarily the case that g outperforms g\u02c6? The answer is NO. We elucidate with the theoretical insight below. ", "page_idx": 2}, {"type": "text", "text": "Theoretical Insight. As discussed in the Introduction from Figure 2, when the number of ID-to-ID and OOD-to-OOD edges surpasses that of ID-to-OOD edges, the propagation mechanism tends to \u201caggregate\" the scores associated with the ID and OOD nodes respectively, which further amplify the separability between them. Conversely, when the number of ID-to-OOD edges are more than the other types of edges, the scores for both ID and OOD nodes become undistinguishable post-propagation. ", "page_idx": 2}, {"type": "text", "text": "The example above offers the insight that the relative performance of $\\mathbf{g}$ compared to $\\hat{\\bf g}$ is contingent upon the structural dynamics of the network, specifically the distribution of edges. To formally articulate this relationship, we adopt a probabilistic framework for modeling edges. Specifically, we assume that the edge follows a Bernoulli distribution characterized by parameters $\\eta_{i n t r a}$ and $\\eta_{i n t e r}$ for intra-edges (ID-to-ID and OOD-to-OOD) and inter-edges (ID-to-OOD), respectively: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{i j}\\sim\\left\\{\\begin{array}{l l}{B e r(\\eta_{i n t r a}),}&{\\mathrm{~if~}i,j\\in\\mathcal{V}_{u i d}\\mathrm{~or~}i,j\\in\\mathcal{V}_{u o o d}}\\\\ {B e r(\\eta_{i n t e r}),}&{\\mathrm{~if~}i\\in\\mathcal{V}_{u i d},j\\in\\mathcal{V}_{u o o d}\\mathrm{~or~}j\\in\\mathcal{V}_{u i d},i\\in\\mathcal{V}_{u o o d}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the context of probabilistic modeling, the subsequent Theorem 3.1 can be established to formalize the inherent understanding. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. (Informal) (a) When $\\eta_{i n t r a}\\gg\\eta_{i n t e r.}$ , it is highly likely that the propagation algorithm will yield enhanced performance in OOD detection. (b) When $\\eta_{i n t r a}\\approx\\eta_{i n t e r}\\:o$ or even \u03b7intra $<\\eta_{i n t e r}$ , the score propagation is likely to be either ineffective or detrimental to the performance. ", "page_idx": 3}, {"type": "text", "text": "We also provide the formal version below (Theorem 3.2) which provides a mathematical foundation for understanding how varying the Bernoulli parameters influence the efficacy of the propagation in the context of OOD detection. We provide the detailed proof in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2. (Formal) For any two test ID/OOD node set $S_{i d}\\subset\\mathcal{V}_{u i d},S_{o o d}\\subset\\mathcal{V}_{u o o d}$ with equal size $N_{s}$ , let the ID-vs-OOD separability $\\mathcal{M}_{s e p}$ defined on an $o o D$ scoring vector $\\hat{\\mathbf{g}}\\in\\mathbb{R}^{N}$ as   \n$\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})\\triangleq\\mathbb{E}_{i\\in S_{i d}}\\hat{\\mathbf{g}}_{i}-\\mathbb{E}_{j\\in S_{o o d}}\\hat{\\mathbf{g}}_{j}.$   \nIf $\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})\\;>\\;0$ and $\\eta_{i n t r a}\\,-\\,\\eta_{i n t e r}\\,>\\,1/N_{s},$ , for some $\\epsilon\\,>\\,0$ and constant c, we have $\\begin{array}{r}{\\mathbb{P}\\left(\\mathcal{M}_{s e p}(A\\hat{\\mathbf{g}})\\geq\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})-\\boldsymbol{\\epsilon}\\right)\\geq1-e x p(-\\frac{{c\\epsilon}^{2}}{\\|\\hat{\\mathbf{g}}\\|_{2}^{2}}).}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Summary. This section has presented a comprehensive theoretical evidence to substantiate the claim that propagation through the adjacency matrix $A$ does not necessarily enhance out-of-distribution (OOD) detection in graphs. Moreover, Theorem 3.2 reveals that the critical factor in enhancing post-propagation performance lies in improving the ratio of intra-edges within the graph structure. These insights serve as a direct motivation for the augmentation strategy in the next section. ", "page_idx": 3}, {"type": "text", "text": "4 An Augmented Score Propagation Strategy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The findings from the preceding section give rise to a subsequent thought: \"Can we improve the propagation strategy for graph OOD detection performance?\" In an ideal scenario, if an oracle were to indicate that a particular subset in the test set belongs exclusively to the ID or OOD, one could augment the graph by adding intra-edges or removing inter-edges. This would consequently improve the ratio of intra-edges \u03b7intra, leading to enhanced OOD detection performance post-propagation. ", "page_idx": 3}, {"type": "text", "text": "However, such an oracle does not exist in practical settings, and even approximating such a subset proves to be a difficult task. Existing literature has suggested the use of pseudo-labels assigned to nodes [36, 79, 2, 73, 55]. Nonetheless, these studies also caution that this approach is susceptible to \u201cconfirmation bias\", whereby errors in estimation are inadvertently amplified. ", "page_idx": 3}, {"type": "text", "text": "To circumvent it, this paper proposes the solution for adding edges to a subset of the training set $\\protect\\nu_{l}$ , which is assured to be in-distribution data. We start by showing the theoretical underpinnings that adding such a subset can, under specified conditions, contribute to improved OOD detection performance after propagation. ", "page_idx": 3}, {"type": "text", "text": "4.1 Theoretical Insight ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our approach involves adding the edges to a subset $G$ of training data and then propagating the out-of-distribution (OOD) scoring vector using the enhanced adjacency matrix. Specifically, when edges are added to $G$ , this action can be mathematically represented as incorporating a perturbation matrix $E=\\mathbf{e}_{G}\\mathbf{e}_{G}^{\\top}$ into A, as demonstrated in Figure 3. Here, ${\\bf e}_{S}\\in\\mathbb{R}^{N}$ denotes an indicator vector for a set $S\\subset\\mathcal{V}$ , where the vector takes the value of 1 if the index $i\\in S$ and value 0 otherwise. A sufficient condition for the efficacy of this augmentation strategy in enhancing post-propagation OOD detection performance is outlined in Theorem 4.1. ", "page_idx": 3}, {"type": "image", "img_path": "jb5qN3212b/tmp/8964ad146a687f24f2fe96c8967cb385eaed568abb84e3e46cf50a42bc3d7fe2.jpg", "img_caption": ["Figure 3: The augmentation procedure "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. (Informal) For a subset $G$ in the training set, augmenting $G$ by adding edges to all its nodes can lead to improved post-propagation OOD detection performance, provided that the following condition is met: $G$ has more edges to ID data than OOD data. ", "page_idx": 4}, {"type": "text", "text": "We also provide the formal version below (Theorem 4.2) that incorporates a perturbation analysis. This analysis elucidates how edge augmentation in the training set can positively influence the propagation algorithm\u2019s ability to enhance OOD detection. For the sake of the main intuition, we provide the analysis on $A$ instead of $\\bar{A}$ for simplicity. We provide the detailed proof in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2. (Formal) For any two test ID/OOD node set $S_{i d}\\subset\\mathcal{V}_{u i d},S_{o o d}\\subset\\mathcal{V}_{u o o d}$ with   \nsize $N_{s}$ , let the ID-vs-OOD separability $\\mathcal{M}_{s e p}$ defined on a non-negative OOD scoring vector   \n$\\hat{\\mathbf{g}}\\in\\mathbb{R}^{N}$ as $\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})\\triangleq\\mathbb{E}_{i\\in S_{i d}}\\hat{\\mathbf{g}}_{i}-\\mathbb{E}_{j\\in S_{o o d}}\\hat{\\mathbf{g}}_{j}.$   \nLet $\\mathcal{E}_{S\\leftrightarrow S^{\\prime}}\\subset\\mathcal{E}$ to denote the edge set of edges between two node sets $S$ and $S^{\\prime}$ , where   \n$S,S^{\\prime}\\subset\\nu$ . If we can find a node set $G\\subset\\mathcal{V}_{l}$ such that $|{\\mathcal{E}}_{G\\leftrightarrow S_{i d}}|>|{\\mathcal{E}}_{G\\leftrightarrow S_{o o d}}|,$ , we have $\\mathcal{M}_{s e p}((A+\\delta E)^{2}\\hat{\\mathbf{g}})>\\mathcal{M}_{s e p}(A^{2}\\hat{\\mathbf{g}}),$   \nwhere $E=\\mathbf{e}_{G}\\mathbf{e}_{G}^{\\top}$ and $\\delta>0$ . ", "page_idx": 4}, {"type": "text", "text": "The Theorem 4.2 shows a critical principle for enhancing propagation: the optimal strategy entails the addition of edges to the subset $G$ such that there are more edges to ID data than OOD data. For some $S_{i d},S_{o o d}$ in the test set, the goal is to find the set ", "page_idx": 4}, {"type": "equation", "text": "$$\nG_{\\ast}=\\operatorname*{arg\\,max}_{S\\subset\\mathcal{V}_{l},|S|=N_{g}}\\frac{|\\mathcal{E}_{S\\leftrightarrow S_{i d}}|}{|\\mathcal{E}_{S\\leftrightarrow S_{o o d}}|},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N_{g}$ is a hyperparameter to control the size of $G_{*}$ . Inspired by the optimization target, we proceed to present our pragmatic algorithmic approach. ", "page_idx": 4}, {"type": "text", "text": "4.2 Graph-Augmented Score Propagation (GRASP) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our augmentation approach hinges on the selection of a subset, $G$ , from the training set, as exemplified in Equation 2. Two principal challenges arise in implementing this: (1) We cannot directly determine the number of edges linked to ID/OOD data because these reside in the test set and their labels remain unknown. (2) An exhaustive search to find a subset is computationally expensive, as the number of combinatorial possibilities increases in a factorial manner. In this paper, we tackle these challenges by providing the practical approximation method. ", "page_idx": 4}, {"type": "text", "text": "Selection of $\\bf{S_{i d}}/\\bf{S_{o o d}}$ . Our discussion begins by detailing the methodology to select the subset from the test ID/OOD dataset, symbolized by $S_{i d}$ and $S_{o o d}$ in Equation 2. A straightforward approach to obtain the most likely ID is by selecting nodes with the largest confidence and the least for OOD in class predictions. Following [23], we employ the max softmax probability (MSP) as a representation of confidence. The selected sets can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{i d}=\\{i\\in\\mathcal{V}_{u}|\\operatorname*{max}_{c\\in[C]}f_{c}(i)>\\lambda_{\\alpha}\\},S_{o o d}=\\{j\\in\\mathcal{V}_{u}|\\operatorname*{max}_{c\\in[C]}f_{c}(j)<\\lambda_{100-\\alpha},}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{\\alpha}$ denotes the $\\alpha$ -th percentile of the MSP scores corresponding to nodes in $\\nu_{u}$ . To offer a clear view, Figure 4 portrays $S_{i d}$ and $S_{o o d}$ in the marginal regions highlighted in orange. Selecting a subset in the leftmost and rightmost regions reduces the error when identifying the ID/OOD subsets, given that overlapping between ID and OOD predominantly occurs around the central region of the distribution. ", "page_idx": 4}, {"type": "image", "img_path": "jb5qN3212b/tmp/d21e1fd051b40d6241b3acffaf54b8d088c048c20ff527bad3819710c9165adc.jpg", "img_caption": ["Figure 4: Illustration of the rationale in selecting $S_{i d}$ and $S_{o o d}$ . MSP score is reported on Dataset Coauther-CS with the division of ID and OOD classes introduced in Appendix C. ", "Distribution of MSP Score in $\\mathcal{V}_{u}$ "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Selection of $G$ . Upon establishing $S_{i d}$ and $S_{o o d}$ , the next step is to determine $G$ using Equation 2. Directly enumerating every possible $G$ is impractical. Instead, we adopt a greedy approach, prioritizing the node with the highest \"likelihood\" score. To elucidate, for each node $i\\in\\mathcal{V}_{l}$ , the score can be computed as the ratio of the edge count to $S_{i d}$ over $S_{o o d}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(i)=|\\mathcal{E}_{\\{i\\}\\leftrightarrow S_{i d}}|/(|\\mathcal{E}_{\\{i\\}\\leftrightarrow S_{o o d}}|+1),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we incorporate an addition of 1 in the denominator to circumvent division by zero. Subsequently, $G$ can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nG=\\{i\\in\\mathcal{V}_{l}|h(i)>\\tau_{\\beta}\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tau_{\\beta}$ stands for the $\\beta$ -th percentile of $h(i)$ scores for nodes in $\\nu_{l}$ . Once $G$ is defined, edge augmentation can be executed as demonstrated in Section 4.1. The OOD score is then propagated with the new adjacency matrix $\\begin{array}{r}{A_{+}=A+\\mathbf{e}_{G}\\mathbf{e}_{G}^{\\top}}\\end{array}$ in place: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf g}_{G R A S P}=(\\bar{A}_{+})^{k}\\hat{\\bf g},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $k\\in\\mathbb{N}^{+}$ are hyperparameters. ", "page_idx": 5}, {"type": "text", "text": "Complexity analysis. While our algorithm introduces the fully connected matrix $E$ , our method can be effciently implemented by matrix-vector multiplication, leading to computational footprint in terms of runtime and memory usage with $\\mathcal{O}(N+\\dot{2}k|\\mathcal{E}|+n)$ and $\\bar{\\mathcal{O}}(N+|\\dot{\\mathcal{E}}|+n)$ respectively after propagation for $k$ times, where $n$ is node count of subset $G$ . We provide the comprehensive complexity analysis in Appendix D.6. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We conduct extensive experiments using 10 real-world datasets that span diverse domains, scales, and structures (homophily or heterophily). A high-level summary of the dataset statistics is provided in Table 1, with a detailed information of the datasets and the comprehensive description of ID/OOD split in Appendix C. Specifically, Cora [61] serves as a widely recognized citation network. Amazon-Photo [52] represents a co-purchasing network on Amazon. Coauthor-CS [62] portrays a coauthor network within the realm of computer science. Moreover, Chameleon and Squirrel [59] are two notable Wikipedia networks, predominantly utilized as ", "page_idx": 5}, {"type": "table", "img_path": "jb5qN3212b/tmp/017667eb993924498444087d602733e17af460120f2f912915ee6d3edc6467c0.jpg", "table_caption": ["Table 1: Summary statistics of the datasets: size of the training set $|\\nu_{l}|$ , test ID set $|\\nu_{u i d}|$ , test OOD set $|\\mathcal{V}_{u o o d}|$ ; number of ID classes $C$ , scale of the dataset, and whether the graph is homophily. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "heterophilic graph benchmarks. We additionally incorporate 5 large-scale graphs to evaluate our proposed methods: Reddit2 [88] and ogbn-products [24] are large homophily datasets; ArXiv-year, Snap-patents, and Wiki [44] are recently proposed large-scale heterophily benchmarks. ", "page_idx": 5}, {"type": "table", "img_path": "jb5qN3212b/tmp/b13e36ac66af134d15469146cdbc28dba9e20c535f1104cdfa52a0ee3ed5066a.jpg", "table_caption": ["Table 2: Main results on common benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained GCN. We take the average values that are percentages over 5 independently trained backbones. \u2191(\u2193) indicates larger (smaller) values are better. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Remark on homophily/heterophily. In Table 1, datasets are also categorized based on the attribute of homophily, denoting the tendency of nodes with the same class to connect. Conversely, the heterophily graph demonstrates a tendency for nodes of disparate classes to connect. This characteristic not only presents a challenge for node classification but also for graph OOD detection. The underlying reason is that the OOD data is from different classes with ID, and heterophily exacerbates the ratio of inter-edge connections between ID and OOD, which is deemed undesirable for graph OOD detection according to Theorem 3.2. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. Our graph OOD detection technique operates in a post hoc fashion utilizing a pre-trained network and so can be used in various pre-trained network seamlessly. We present results evaluated on Graph Convolutional Network (GCN) [34] in the main paper to save space and put detailed results of other architectures in Appendix D.3. All pre-trained models possess a layer depth of 2. With the pre-trained network, we proceed to execute the graph OOD detection. By default, we report the performance of the augmented propagation (GRASP) on the MSP score [23]. The compatibility with other OOD scoring functions is also shown in Table 6. We set the propagation number $k$ as 8, with percentile values $\\alpha\\,=\\,5$ and $\\beta\\,=\\,50$ . The sensitivity analysis of the hyperparameters is included in Appendix D.4. ", "page_idx": 6}, {"type": "text", "text": "Metrics. Following the convention in literature [23, 47, 65], we use AUROC and FPR95 as evaluation metrics for OOD detection. ", "page_idx": 6}, {"type": "text", "text": "5.1 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "GRASP consistently achieves superior performance. We provide results of 5 common small-scale benchmarks and 5 large-scale datasets in Table 2 and Table 3 respectively, wherein only the averaged results over 5 runs are presented to save space and the detailed results with standard errors of these two scale datasets are shown in Table 12 and Table 13 respectively. From the results we can see that our proposed methodology (GRASP) consistently demonstrates promising performance. The comparative analysis encompasses a broad spectrum of post hoc competitive Out-of-Distribution (OOD) detection techniques in existing literature and training-based methods tailored for graph OOD detection. We categorize the baseline methods into two groups: (a) Traditional OOD detection methods including MSP [23], Energy [47], ODIN [43], Mahalanobis [37], and KNN [67]; (b) Graph OOD detection methods including GKDE [92], GPN [64], OODGAT [63], and GNNSafe [77]. In these tables, we present GRASP results based on the MSP score. Noteworthy findings include: (a) The traditional OOD detection methods exhibit suboptimal performance in the realm of graph OOD detection. For instance, GRASP reduced the average FPR95 by $17.87\\%$ and $32.21\\%$ compared to the strongest traditional OOD detection method GNNSafe and Mahalanobis on common and large-scale benchmarks, respectively. This outcome is anticipated given their lack of specificity in design towards graph data. (b) GRASP outperforms existing baselines by a large margin, surpassing the best baseline GNNSafe by $17.87\\%$ and $40\\%$ concerning average FPR95 on two scale benchmarks respectively. These results further corroborate that the theoretically motivated solution GRASP is also appealing to use in practice. ", "page_idx": 6}, {"type": "text", "text": "GRASP is also competitive on large-scale graph datasets. Contrasted with the small-scale benchmarks in Table 2, the large-scale scenario in Table 3 presents more challenges due to a large number of nodes and edges. From Table 3 we can see that all baseline OOD detection methodologies exhibit suboptimal performance on large-scale benchmarks, while our method GRASP robustly performs the best. ", "page_idx": 6}, {"type": "table", "img_path": "jb5qN3212b/tmp/4d3b3997b1320ed49e94303bb48be7902f28a2c557356c7756bd028fecf19173.jpg", "table_caption": ["Table 3: Main results on large-scale benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained method GCN. We take the average values that are percentages over 5 independently trained backbones. OOM means Out-Of-Memory and OOT denotes that no results have been got after running over 48 hours for each run. \u2191(\u2193) indicates larger (smaller) values are better. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "GRASP exhibits significant advantages over training-based baselines. In addition to contrasting with post hoc methods, we extend our comparison to a parallel line of graph Out-Of-Distribution (OOD) detection research, which focuses on refining the training strategy to improve graph OOD detection performance. The compared methods include GKDE [92], GPN [64] and OODGAT [63]. While these approaches necessitate a costly re-training procedure, they perform mediocrely across all small-scale datasets and even run out-of-memory on almost all large-scale benchmarks, rendering them impractical for real-world deployment. ", "page_idx": 7}, {"type": "text", "text": "GRASP is performant on challenging heterophily datasets. As indicated in Table 2 and 3, GNNSafe, which performs well on homophily datasets, experiences significant degradation on the difficult heterophily benchmarks due to its na\u00efve propagation mechanism. In contrast, GRASP maintains optimal performance on these hard scenarios. ", "page_idx": 7}, {"type": "text", "text": "5.2 A Comprehensive Analysis of GRASP ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation study on augmentation policy. Recall that the key part of our method GRASP is the augmentation policy that consists in adding edges to the training nodes with top $50\\%$ scores of $h(i)$ , which corresponds to the nodes on the right side of Figure 5. We ablate the contributions of $h(i)$ by comparision with alternative augmentation approaches that utilize $h(i)$ differently in Table 4, specifically, (1) selecting $50\\%$ of training nodes with the lowest $h(i)$ values (left side of Figure 5), (2) randomly selecting $50\\%$ of training nodes, corresponding to randomly picking node indices from the $\\mathbf{X}$ -axis of Figure 5, (3) directly adding edges to $S_{i d}$ and $S_{o o d}$ within the test set (i.e., TestAug), and (4) a classic graph augmentation method named GAug [91], which adds or removes edges based on an edge predictor that disregards $h(i)$ completely. We have the following key observations: ", "page_idx": 7}, {"type": "image", "img_path": "jb5qN3212b/tmp/1862e91af40f889ffa31b1838d01ea1a4acfd1b896cec85361c11b0eeaf4852c.jpg", "img_caption": ["Figure 5: Illustration of the number of edges from each training node $i$ to $S_{i d}$ and $S_{o o d}$ on Chameleon dataset. The $\\mathbf{X}$ -axis denotes the training node indices, ordered by $h(i)$ from low to high. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "$(a)$ Selection by $h(i)$ is effective. For example, our strategy using the top $50\\%$ scores of $h(i)$ outperforms that uses random $50\\%$ , which, in turn, outperforms the low $50\\%$ way. This is because a higher score of $h(i)$ implies higher edge count towards ID data than to OOD data, which can increase the ratios of intra-edges and improve OOD detection performance after propagation. In contrast, the alternative augmentation method GAug, which does not consider the score $\\bar{h}(\\bar{i})$ at all, performs even worse than the Low $50\\%$ policy. ", "page_idx": 7}, {"type": "text", "text": "(b) Directly adding edges to $S_{i d}$ and $S_{o o d}$ within the test set is sub-optimal. Specifically, employing this method leads to nearly $20\\%$ lower than that achieved with GRASP, which substantiates the notion that \u201cconfirmation bias\" can adversely affect the graph OOD detection. ", "page_idx": 7}, {"type": "text", "text": "Overall, the ablation study suggests that our proposed augmentation policy is crucial to OOD detection performance. ", "page_idx": 7}, {"type": "table", "img_path": "jb5qN3212b/tmp/6e48d4e28302cdae21d622478c3cb7b9134d679529f0317243ea587b0b38a2af.jpg", "table_caption": ["Table 4: Ablation study on OOD detection performance by different augmentation policy. We report averaged AUROC over 5 independently pre-trained GCN models. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "jb5qN3212b/tmp/e5e2e12d9fd532f7a3e7c45941138d015a769bac8e5d5a34e2abaa0193db0d36.jpg", "table_caption": ["Table 5: GRASP consistently enhances the OOD detection performance of nodes connected by both inter-edges and intra-edges on datasets characterized by a strong degree of heterophily (datasets highlighted in bold in the table). However, naive propagation tends to compromise the performance of these nodes. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "GRASP can effectively boost performance of challenging nodes connected by inter-edges. As stated in the introduction 1, the reason OOD score propagation does not always work is the confusion between ID and OOD nodes resulting from propagation along the inter-edges. For example in heterophily datasets, where connected nodes tend to possess different labels, OOD nodes are more likely to appear on the inter-edges. To assess the capability of our proposed augmentation propagation method to address this challenge, we present in Table 5 the accuracy of detecting OOD nodes connected by intra-edges (intra) and inter-edges (inter) respectively, using the original MSP without any propagation, naive propagation based on MSP $(\\mathtt{M S P+p r o p})$ ), and our proposed augmentation propagation $(\\mathtt{M S P+G R A S P})$ ) respectively. From the results we can see that naive propagation performs well only on strong homophily datasets, while on strong heterophily datasets (datasets highlighted in bold in the table), its performance is even worse than without propagation, as expected. In contrast, employing our augmentation method still results in substantial performance gain after propagation on these challenging datasets. ", "page_idx": 8}, {"type": "text", "text": "GRASP is compatible with a wide range of OOD scoring methods. In Table 6, we demonstrate the compatibility of GRASP with various alternative scoring functions. Each method generates OOD scores to form a scoring vector; GRASP is then applied to facilitate score propagation. The use of GRASP markedly surpasses the performance of its non-augmented counterpart across all datasets. ", "page_idx": 8}, {"type": "table", "img_path": "jb5qN3212b/tmp/5ad94c277868cfb24dfefcd782fbf9df43e5a8d4a2d108f9a85a23b0a24b9910.jpg", "table_caption": ["Table 6: GRASP is compatible with different OOD scoring functions. We compare OOD detection methods and the performance after the simple propagation in Equation 1 (denoted by $^{\\ast}+$ prop\") and with GRASP respectively. We report AUROC results that are averaged over 5 independent pre-trained GCN models. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Remark on other empirical findings. We include other empirical findings in Appendix. Specifically, in Table 14, we prove that GRASP also achieves superior performance on the other GNN architectures in the literature. In Figure 7, we demonstrate a strong positive correlation between ratios of intraedges and the corresponding OOD detection performance, validating the correctness of Theorem 3.2. We also show in Table 15 that GRASP can increase the ratio of intra-edges after augmentation, consequently boosting OOD performance. This substantiates the correctness of Theorem 4.2. What\u2019s more, we conduct a thorough accounting of computation/memory demands compared to baselines on all scale datasets in Table 16 and Table 17 respectively, which underscore the strong practicality of our approach. Lastly, in Appendix D.7, we investigate the other propagation mechanisms in the literature and their impact on graph OOD detection performance emperically. ", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Out-of-distribution Detection. The primary focus within this realm has been on the development of scoring functions for OOD detection. These works can be broadly categorized into two main streams: (1) output-based methods [23, 43, 47, 71, 25, 72, 95, 26, 15, 90, 16, 21], and (2) featurebased methods including the Mahalanobis distance [37, 60, 56] and KNN distance [67]. These methodologies are predominantly applied in domains such as computer vision, where samples are inherently independent of each other. However, these techniques are not designed to adeptly handle data structures like graphs, where samples are inter-connected. ", "page_idx": 9}, {"type": "text", "text": "Out-of-distribution detection for graph data. Graph anomaly detection has a rich history [12, 74, 89, 75, 48, 14, 50, 32, 46, 51]. In recent years, the OOD detection in graph data introduced fresh challenges, particularly with multi-class classification for in-distribution data, escalating the difficulty in discerning outlier data. Some of the works focus on graph-level OOD detection [42, 49, 4, 76]. For node-level OOD detection, GKDE [92] and GPN [64] apply Bayesian Network models to estimate uncertainties to detect OOD nodes. However, Bayesian-based approaches can encounter impediments such as inaccurate predictions and high computational demands, which limit their broader applicability [83]. GNNSafe [77] emerges as the work employing post hoc energy-based score to perform OOD detection. Given the merits of post hoc methods, our study first provides a comprehensive understanding of the OOD score propagation in Graphs, extending beyond existing knowledge. ", "page_idx": 9}, {"type": "text", "text": "Graph Data Augmentation. Graph Data Augmentation is a common technique in graph machine learning [20, 6, 58, 29, 93, 91, 33, 53, 13, 3] to improve the node classification performance. Existing methods operate exclusively on in-distribution (ID) data. Furthermore, their test set data also originates from the in-distribution and shares the same classes as the training set. In contrast, our data augmentation is purposefully crafted for OOD detection, supported by the theoretical explanation. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this research, we delve into an important yet under-explored challenge in the realm of graph data: Out-of-Distribution (OOD) detection. Recognizing the inadequacies of traditional OOD detection techniques in the context of graph data, our exploration centered on the potential of score propagation as a viable and efficient solution. Our findings reveal the specific conditions under which score propagation will be helpful\u2014in situations where the ratio of intra-edges surpasses that of inter-edges. Motivated by this finding, our edge augmentation strategy selectively adds edges to a specific subset $G$ of the training set, which provably improves post-propagation OOD detection outcomes under certain conditions. Extensive empirical evaluations reinforced the merit of our approach. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (62441605), and the Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study (SNZJUSIAS-0010). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing. In international conference on machine learning, pages 21\u201329. PMLR, 2019.   \n[2] Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudolabeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2020.   \n[3] Mehdi Azabou, Venkataramana Ganesh, Shantanu Thakoor, Chi-Heng Lin, Lakshmi Sathidevi, Ran Liu, Michal Valko, Petar Veli\u02c7ckovi\u00b4c, and Eva L Dyer. Half-hop: A graph upsampling approach for slowing down message passing. 2023.   \n[4] Gleb Bazhenov, Sergei Ivanov, Maxim Panov, Alexey Zaytsev, and Evgeny Burnaev. Towards ood detection in graph classification from uncertainty estimation perspective. arXiv preprint arXiv:2206.10691, 2022.   \n[5] Gleb Bazhenov, Denis Kuznedelev, Andrey Malinin, Artem Babenko, and Liudmila Prokhorenkova. Evaluating robustness and uncertainty of graph models under structural distributional shifts. Advances in Neural Information Processing Systems, 36, 2024.   \n[6] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3438\u20133445, 2020.   \n[7] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International conference on machine learning, pages 1725\u20131735. PMLR, 2020.   \n[8] Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, and James Cheng. Does invariant graph learning via environment augmentation learn invariance? Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Zhengyu Chen, Teng Xiao, Kun Kuang, Zheqi Lv, Min Zhang, Jinluan Yang, Chengqiang Lu, Hongxia Yang, and Fei Wu. Learning to reweight for generalizable graph neural network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 8320\u20138328, 2024.   \n[10] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. International Conference on Learning Representations, 2021.   \n[11] Pierre Colombo, Eduardo Dadalto, Guillaume Staerman, Nathan Noiry, and Pablo Piantanida. Beyond mahalanobis distance for textual ood detection. Advances in Neural Information Processing Systems, 35:17744\u201317759, 2022.   \n[12] Kaize Ding, Kai Shu, Xuan Shan, Jundong Li, and Huan Liu. Cross-domain graph anomaly detection. IEEE Transactions on Neural Networks and Learning Systems, 33(6):2406\u20132415, 2021.   \n[13] Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. ACM SIGKDD Explorations Newsletter, 24(2):61\u201377, 2022.   \n[14] Kaize Ding, Qinghai Zhou, Hanghang Tong, and Huan Liu. Few-shot network anomaly detection via cross-network meta-learning. In Proceedings of the Web Conference 2021, pages 2448\u20132456, 2021.   \n[15] Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping for out-of-distribution detection. In The Eleventh International Conference on Learning Representations, 2022.   \n[16] Xuefeng Du, Yiyou Sun, and Yixuan Li. When and how does in-distribution label help out-ofdistribution detection? In Forty-first International Conference on Machine Learning, 2024.   \n[17] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.   \n[18] Ido Galil, Mohammed Dabbah, and Ran El-Yaniv. A framework for benchmarking class-out-ofdistribution detection and its application to imagenet. In The Eleventh International Conference on Learning Representations, 2022.   \n[19] Johannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. In International Conference on Learning Representations, 2019.   \n[20] Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learning. Advances in neural information processing systems, 32, 2019.   \n[21] Soumya Suvra Ghosal, Yiyou Sun, and Yixuan Li. How to overcome curse-of-dimensionality for out-of-distribution detection? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19849\u201319857, 2024.   \n[22] Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, and Shuiwang Ji. Joint learning of label and environment causal independence for graph out-of-distribution generalization. Advances in Neural Information Processing Systems, 36, 2024.   \n[23] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2016.   \n[24] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.   \n[25] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. Advances in Neural Information Processing Systems, 34:677\u2013 689, 2021.   \n[26] Wenjian Huang, Hao Wang, Jiahao Xia, Chengyan Wang, and Jianguo Zhang. Density-driven regularization for out-of-distribution detection. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[27] Paul F Jaeger, Carsten Tim L\u00fcth, Lukas Klein, and Till J Bungert. A call to reflect on evaluation practices for failure detection in image classification. In The Eleventh International Conference on Learning Representations, 2022.   \n[28] Tianrui Jia, Haoyang Li, Cheng Yang, Tao Tao, and Chuan Shi. Graph invariant learning with subgraph co-mixup for out-of-distribution generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 8562\u20138570, 2024.   \n[29] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 66\u201374, 2020.   \n[30] Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, and Neil Shah. Empowering graph representation learning with test-time graph transformation. In The Eleventh International Conference on Learning Representations, 2022.   \n[31] Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, and Yanfang Ye. Graphpatcher: Mitigating degree bias for graph neural networks via test-time augmentation. Advances in Neural Information Processing Systems, 36, 2024.   \n[32] Hwan Kim, Byung Suk Lee, Won-Yong Shin, and Sungsu Lim. Graph anomaly detection with graph neural networks: Current status and challenges. IEEE Access, 2022.   \n[33] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.   \n[34] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations, 2017.   \n[35] Johannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. International Conference on Learning Representations, 2019.   \n[36] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page 896. Atlanta, 2013.   \n[37] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.   \n[38] Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: densification laws, shrinking diameters and possible explanations. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 177\u2013187, 2005.   \n[39] Jure Leskovec and Andrej Krevl. Snap datasets: Stanford large network dataset collection, 2014.   \n[40] Chang Li and Dan Goldwasser. Encoding social information with graph convolutional networks forpolitical perspective detection in news media. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2594\u20132604, 2019.   \n[41] Xiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. Graph structure and feature extrapolation for out-of-distribution generalization. arXiv preprint arXiv:2306.08076, 2023.   \n[42] Zenan Li, Qitian Wu, Fan Nie, and Junchi Yan. Graphde: A generative framework for debiased learning and out-of-distribution detection on graphs. Advances in Neural Information Processing Systems, 35:30277\u201330290, 2022.   \n[43] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In International Conference on Learning Representations, 2018.   \n[44] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems, 34:20887\u201320902, 2021.   \n[45] Changshu Liu, Liangjian Wen, Zhao Kang, Guangchun Luo, and Ling Tian. Self-supervised consensus representation learning for attributed graph. In Proceedings of the 29th ACM international conference on multimedia, pages 2654\u20132662, 2021.   \n[46] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, et al. Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. Advances in Neural Information Processing Systems, 35:27021\u201327035, 2022.   \n[47] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in neural information processing systems, 33:21464\u201321475, 2020.   \n[48] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and choose: a gnn-based imbalanced learning approach for fraud detection. In Proceedings of the web conference 2021, pages 3168\u20133177, 2021.   \n[49] Yixin Liu, Kaize Ding, Huan Liu, and Shirui Pan. Good-d: On unsupervised graph out-ofdistribution detection. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 339\u2013347, 2023.   \n[50] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly detection on attributed networks via contrastive self-supervised learning. IEEE transactions on neural networks and learning systems, 33(6):2378\u20132392, 2021.   \n[51] Xiaoxiao Ma, Ruikun Li, Fanzhen Liu, Kaize Ding, Jian Yang, and Jia Wu. Graph anomaly detection with few labels: A data-centric approach. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2153\u20132164, 2024.   \n[52] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 43\u201352, 2015.   \n[53] Hyeonjin Park, Seunghun Lee, Sihyeon Kim, Jinyoung Park, Jisu Jeong, Kyung-Min Kim, Jung-Woo Ha, and Hyunwoo J Kim. Metropolis-hastings data augmentation for graph neural networks. Advances in Neural Information Processing Systems, 34:19010\u201319020, 2021.   \n[54] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.   \n[55] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11557\u201311568, 2021.   \n[56] Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022, 2021.   \n[57] Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J Liu. Out-of-distribution detection and selective generation for conditional language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[58] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. 2020.   \n[59] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal of Complex Networks, 9(2):cnab014, 2021.   \n[60] Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlier detection. In Proceedings of the International Conference on Learning Representations, 2021.   \n[61] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[62] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web, pages 243\u2013246, 2015.   \n[63] Yu Song and Donglin Wang. Learning on graphs with out-of-distribution nodes. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1635\u20131645, 2022.   \n[64] Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Z\u00fcgner, and Stephan G\u00fcnnemann. Graph posterior network: Bayesian predictive uncertainty for node classification. Advances in Neural Information Processing Systems, 34:18033\u201318048, 2021.   \n[65] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. Advances in Neural Information Processing Systems, 34:144\u2013157, 2021.   \n[66] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, pages 20827\u201320840. PMLR, 2022.   \n[67] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning (ICML). PMLR, 2022.   \n[68] Dennis Ulmer, Lotta Meijerink, and Giovanni Cin\u00e0. Trust issues: Uncertainty estimation does not enable reliable ood detection on medical tabular data. In Machine Learning for Health, pages 341\u2013354. PMLR, 2020.   \n[69] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. International Conference on Learning Representations, 2017.   \n[70] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[71] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4921\u20134930, 2022.   \n[72] Qizhou Wang, Feng Liu, Yonggang Zhang, Jing Zhang, Chen Gong, Tongliang Liu, and Bo Han. Watermarking for out-of-distribution detection. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[73] Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. Be confident! towards trustworthy graph neural networks via confidence calibration. Advances in Neural Information Processing Systems, 34:23768\u201323779, 2021.   \n[74] Xuhong Wang, Baihong Jin, Ying Du, Ping Cui, Yingshui Tan, and Yupu Yang. One-class graph neural networks for anomaly detection in attributed networks. Neural computing and applications, 33:12073\u201312085, 2021.   \n[75] Yanling Wang, Jing Zhang, Shasha Guo, Hongzhi Yin, Cuiping Li, and Hong Chen. Decoupling representation learning and classification for gnn-based anomaly detection. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 1239\u20131248, 2021.   \n[76] Yili Wang, Yixin Liu, Xu Shen, Chenyu Li, Kaize Ding, Rui Miao, Ying Wang, Shirui Pan, and Xin Wang. Unifying unsupervised graph-level anomaly detection and out-of-distribution detection: a benchmark. arXiv preprint arXiv:2406.15523, 2024.   \n[77] Qitian Wu, Yiting Chen, Chenxiao Yang, and Junchi Yan. Energy-based out-of-distribution detection for graph neural networks. In The Eleventh International Conference on Learning Representations, 2022.   \n[78] Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, and Junchi Yan. Graph out-of-distribution generalization via causal intervention. arXiv preprint arXiv:2402.11494, 2024.   \n[79] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[80] Bingbing Xu, Huawei Shen, Qi Cao, Keting Cen, and Xueqi Cheng. Graph convolutional networks using heat kernel for semi-supervised learning.   \n[81] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International conference on machine learning, pages 5453\u20135462. PMLR, 2018.   \n[82] Yujun Yan, Jiong Zhu, Marlena Duda, Eric Solarz, Chandra Sripada, and Danai Koutra. Groupinn: Grouping-based interpretable neural network for classification of limited, noisy brain data. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 772\u2013782, 2019.   \n[83] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334, 2021.   \n[84] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 974\u2013983, 2018.   \n[85] Junchi Yu, Jian Liang, and Ran He. Mind the label shift of augmentation-based graph ood generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11620\u201311630, 2023.   \n[86] Haonan Yuan, Qingyun Sun, Xingcheng Fu, Ziwei Zhang, Cheng Ji, Hao Peng, and Jianxin Li. Environment-aware dynamic graph learning for out-of-distribution generalization. Advances in Neural Information Processing Systems, 36, 2024.   \n[87] Reza Zafarani, Mohammad Ali Abbasi, and Huan Liu. Social media mining: an introduction. Cambridge University Press, 2014.   \n[88] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. In International Conference on Learning Representations, 2019.   \n[89] Ge Zhang, Jia Wu, Jian Yang, Amin Beheshti, Shan Xue, Chuan Zhou, and Quan Z Sheng. Fraudre: Fraud detection dual-resistant to graph inconsistency and imbalance. In 2021 IEEE International Conference on Data Mining (ICDM), pages 867\u2013876. IEEE, 2021.   \n[90] Jinsong Zhang, Qiang Fu, Xu Chen, Lun Du, Zelin Li, Gang Wang, xiaoguang Liu, Shi Han, and Dongmei Zhang. Out-of-distribution detection based on in-distribution data patterns memorization with modern hopfield energy. In The Eleventh International Conference on Learning Representations, 2023.   \n[91] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. In Proceedings of the aaai conference on artificial intelligence, volume 35, pages 11015\u201311023, 2021.   \n[92] Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty aware semi-supervised learning on graph data. Advances in Neural Information Processing Systems, 33:12827\u201312836, 2020.   \n[93] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, pages 11458\u201311468. PMLR, 2020.   \n[94] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in neural information processing systems, 33:7793\u20137804, 2020.   \n[95] Yao Zhu, YueFeng Chen, Chuanlong Xie, Xiaodan Li, Rong Zhang, Hui Xue\u2019, Xiang Tian, bolun zheng, and Yaowu Chen. Boosting out-of-distribution detection with typical features. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem A.1. (Recap of Theorem 3.2) For any two test ID/OOD node set $S_{i d}\\subset\\mathcal{V}_{u i d},S_{o o d}\\subset\\mathcal{V}_{u o o d}$ with equal size $N_{s}$ , let the $I D$ -vs-OOD separability $\\mathcal{M}_{s e p}$ defined on a OOD scoring vector $\\hat{\\mathbf{g}}\\in\\mathbb{R}^{N}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})\\triangleq\\mathbb{E}_{i\\in S_{i d}}\\hat{\\mathbf{g}}_{i}-\\mathbb{E}_{j\\in S_{o o d}}\\hat{\\mathbf{g}}_{j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $\\mathcal{M}_{s e p}(\\hat{\\bf{g}})>0$ and $\\eta_{i n t r a}-\\eta_{i n t e r}>1/N_{s},$ , for some $\\epsilon>0$ and constant $c$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{M}_{s e p}(A\\hat{\\mathbf{g}})\\geq\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})-\\boldsymbol{\\epsilon}\\right)\\geq1-e x p(-\\frac{c\\boldsymbol{\\epsilon}^{2}}{\\|\\hat{\\mathbf{g}}\\|_{2}^{2}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Without losing the generality, we set the $\\hat{\\bf g}_{i}=0$ , if $i\\in S_{o o d}\\cup S_{i d}$ , since we only care about the detection results in the given test node set $S_{o o d}$ and $S_{i d}$ . ", "page_idx": 16}, {"type": "text", "text": "The $\\mathcal{M}_{s e p}(\\hat{\\bf g})$ can be re-written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})=\\hat{\\mathbf{g}}^{\\top}(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}_{s e p}(A\\hat{\\mathbf{g}})=\\hat{\\mathbf{g}}^{\\top}A(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "According to General Hoeffding\u2019s inequality (Theorem 2.6.3) in [70], we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbb{E}[\\hat{\\mathbf{g}}^{\\top}A(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})]-\\hat{\\mathbf{g}}^{\\top}A(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})\\leq\\epsilon\\right)\\geq1-\\exp(-\\frac{c\\epsilon^{2}}{\\|\\hat{\\mathbf{g}}\\|_{2}^{2}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $c$ is some constant value. ", "page_idx": 16}, {"type": "text", "text": "Since $\\hat{\\bf g}_{i}=0$ , if $i\\in\\mathcal{V}_{l}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\hat{\\mathbf{g}}^{\\top}A(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}}])=\\hat{\\mathbf{g}}^{\\top}\\mathbb{E}[A](\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\hat{\\mathbf{g}}^{\\top}N_{s}(\\eta_{i n t r a}-\\eta_{i n t e r})(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\>\\hat{\\mathbf{g}}^{\\top}(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining together, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\hat{\\mathbf{g}}^{\\top}A(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})\\geq\\hat{\\mathbf{g}}^{\\top}(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}})-\\epsilon\\right)\\geq1-\\exp(-\\frac{c\\epsilon^{2}}{\\|\\hat{\\mathbf{g}}\\|_{2}^{2}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Theorem A.2. (Recap of Theorem 4.2) For any two test ID/OOD node set $S_{i d}\\subset\\mathcal{V}_{u i d},S_{o o d}\\subset\\mathcal{V}_{u o o d}$ with equal size $N_{s}$ , let the $I D$ -vs-OOD separability $\\mathcal{M}_{s e p}$ defined on a non-negative OOD scoring vector $\\hat{\\mathbf{g}}\\in\\mathbb{R}^{N}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})\\triangleq\\mathbb{E}_{i\\in S_{i d}}\\hat{\\mathbf{g}}_{i}-\\mathbb{E}_{j\\in S_{o o d}}\\hat{\\mathbf{g}}_{j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $\\mathcal{E}_{S\\leftrightarrow S^{\\prime}}\\subset\\mathcal{E}$ to denote the edge set of edges between two node sets $S$ and $S^{\\prime}$ , where $S,S^{\\prime}\\subset\\mathcal{V}$ . If we can find a node set $G\\subset\\mathcal{V}_{l}$ such that $\\bar{\\varepsilon_{G\\leftrightarrow S_{i d}}}|>|\\mathcal{E}_{G\\leftrightarrow S_{o o d}}|,$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}_{s e p}((A+\\delta E)^{2}\\hat{\\mathbf{g}})>\\mathcal{M}_{s e p}(A^{2}\\hat{\\mathbf{g}}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $E=\\mathbf{e}_{G}\\mathbf{e}_{G}^{\\top}$ and $\\delta>0$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. The $\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})$ can be re-written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}_{s e p}(\\hat{\\mathbf{g}})=\\frac{1}{N_{s}}\\hat{\\mathbf{g}}^{\\top}(\\mathbf{e}_{S_{i d}}-\\mathbf{e}_{S_{o o d}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can then directly derive the proof by expanding ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{M_{s c p}((A+\\delta E)^{2}\\hat{\\mathbf{g}})-M_{s c p}(A^{2}\\hat{\\mathbf{g}})=\\displaystyle\\frac{1}{N_{s}}(\\mathrm{e}_{S i_{d}}-\\mathrm{e}_{S_{c a d}})^{\\top}\\left((A+\\delta E)^{2}\\hat{\\mathbf{g}}-A^{2}\\hat{\\mathbf{g}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{1}{N_{s}}(\\mathrm{e}_{S i_{d}}-\\mathrm{e}_{S_{c a d}})^{\\top}\\left(\\hat{\\delta}(A E+E A)\\hat{\\mathbf{g}}+\\delta^{2}\\mathrm{e}_{G}\\mathrm{e}_{G}^{\\top}\\mathrm{e}_{G}^{\\top}\\hat{\\mathbf{g}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{\\hat{\\delta}}{N_{s}}(\\mathrm{e}_{S i_{d}}-\\mathrm{e}_{S_{c a d}})^{\\top}\\left(A\\mathrm{e}_{G}\\mathrm{e}_{G}^{\\top}\\hat{\\mathbf{g}}+\\mathrm{e}_{G}\\mathrm{e}_{G}^{\\top}A\\hat{\\mathbf{g}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{\\delta}{N_{s}}(\\mathrm{e}_{S i_{d}}-\\mathrm{e}_{S_{c a d}})^{\\top}\\boldsymbol{\\mathrm{Ac}}_{G G}\\mathrm{e}_{G}^{\\top}\\hat{\\mathbf{g}}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{\\delta}{N_{s}}(\\mathrm{e}_{G}^{\\top}\\hat{\\mathbf{g}})(\\mathrm{e}_{S i_{d}}^{\\top}A\\mathrm{e}_{G}-\\mathrm{e}_{S_{c a d}}^{\\top}A\\mathrm{e}_{G})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{\\delta(\\mathrm{e}_{G}^{\\top}\\hat{\\mathbf{g}})}{|G|_{G}}(|\\xi_{G\\neq S i_{d}}|-|\\xi_{G+S_{c a d}}|)}\\\\ &{\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second and the third equation are derived by the fact that $G\\subset\\mathcal{V}_{l}$ and then we have $\\mathbf{e}_{S_{i d}}^{\\top}\\mathbf{e}_{G}=0$ and $\\mathbf{e}_{S_{o o d}}^{\\top}\\mathbf{e}_{G}=0$ . ", "page_idx": 17}, {"type": "text", "text": "B Details of Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the reader\u2019s convenience, we summarize in detail a few common techniques for defining OOD scores that measure the degree of ID-ness on a given input. By convention, a higher (lower) score is indicative of being in-distribution (out-of-distribution). ", "page_idx": 17}, {"type": "text", "text": "MSP [23] This method proposes to use the maximum softmax score as the OOD score. For each node $i$ , we use $F_{O O D D}(\\bar{i})\\stackrel{{}=}{=}\\operatorname*{max}_{c\\in[C]}f_{c}(i)$ as the OOD score. ", "page_idx": 17}, {"type": "text", "text": "ODIN [43] This method improves OOD detection with temperature scaling and input perturbation. In all experiments, we set the temperature scaling parameter $T=1000$ . For graph neural network, we found the input perturbation does not further improve the OOD detection performance and hence we set $\\epsilon=0$ . ", "page_idx": 17}, {"type": "text", "text": "Mahalanobis [37] This method uses multivariate Gaussian distributions to model class-conditional distributions of softmax neural classifiers and uses Mahalanobis distance-based scores for OOD detection. The mean $\\mu_{c}$ of each multivariate Gaussian distribution with class $c$ and a tied covariance $\\Sigma$ are estimated based on training samples. We define the confidence score $M(\\mathbf{x})$ using the Mahalanobis distance between test sample $\\mathbf{x}$ and the closest class-conditional Gaussian distribution. ", "page_idx": 17}, {"type": "text", "text": "Energy [47] This method proposes using energy score for OOD detection. The energy function maps the logit outputs to a scalar $\\bar{E_{}}(\\mathbf{x}_{i};f)\\in\\bar{\\mathbb{R}}$ , which is relatively lower for ID data. Note that [47] used the negative energy score for OOD detection, in order to align with the convention that $S(\\mathbf{x})$ is higher (lower) for ID (OOD) data. ", "page_idx": 17}, {"type": "text", "text": "KNN [67] This method uses the $k$ -th nearest neighbor distance between a test graph node and the training set as the OOD score. We use $k=10$ for all experiments in this paper. ", "page_idx": 17}, {"type": "text", "text": "C Dataset Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We adopt ten publicly available benchmarks used for graph learning, covering diverse domains, scales, and structures (homophily or heterophily). For five homophily datasets Cora, Amazon-Photo, Coauthor-CS, Reddit2 and ogbn-products, we use the data loader provided by the Pytorch Geometric package 3. For the remaining five heterophily datasets, we directly use the pickle file or download from the given hyperlinks proposed by [44]. ", "page_idx": 17}, {"type": "text", "text": "Cora [61] is a 7-class citation network comprising 2,708 nodes, 5,429 edges and 1,433 features. In this network, each node represents a published paper, each edge signifies a citation relationship, and the label class is each paper\u2019s topic, which is the goal to predict. ", "page_idx": 17}, {"type": "text", "text": "Amazon-Photo [52] is an 8-class item co-purchasing network on Amazon, which contains 7,650 nodes, 238,162 edges and 745 features. In this network, each node denotes a product, each edge indicates that two linked products are frequently purchased together, and the node label denotes the category of the product. ", "page_idx": 18}, {"type": "text", "text": "Coauthor-CS [62] is a 15-class coauthor network of computer science, which contains 18,333 nodes, 163,788 edges and 6,805 features. In this network, nodes denote authors and there is an edge between two authors if co-authored a paper. And the label represents the study field for the authors. ", "page_idx": 18}, {"type": "text", "text": "Reddit2 [88] is a large homophily post network with 41 classes, where nodes are posts based on user comments and the task is to predict communities of online posts based on user comments. ", "page_idx": 18}, {"type": "text", "text": "ogbn-products [24] is a super large homophily undirected and unweighted graph with 47 classes, representing an Amazon product co-purchasing network. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels. ", "page_idx": 18}, {"type": "text", "text": "Chameleon and Squirrel [59] are two Wikipedia networks with 5 classes, where nodes represent web pages and edges represent hyperlinks between them. Node features represent several informative nouns in the Wikipedia pages and the task is to predict the average daily traffic of the web page [17]. ", "page_idx": 18}, {"type": "text", "text": "arXiv-year [24] is the ogbn-arXiv network with different labels and is altered to be heterophily, in which the class labels are set to be the year that the paper is posted, instead of subject area in the original paper. The nodes are arXiv papers, and directed edges connect a paper to other papers that it cites. The node features are averaged word2vec token features of both the title and abstract of the paper. The five classes are chosen by partitioning the posting dates so that class ratios are approximately balanced [44]. ", "page_idx": 18}, {"type": "text", "text": "snap-patents [39, 38] is a big dataset of utility patents in the US. Each node represents a patent and edges connect patents that cite each other. Node features are derived from patent metadata [44]. Like arXiv-year, this dataset is changed to set the task to predict the time at which a patent was granted, which is also five classes. ", "page_idx": 18}, {"type": "text", "text": "wiki [44] is a super big dataset of Wikipedia articles, which are crawled and cleaned from the internet. Nodes represent pages and edges represent links between them. Node features are derived from the average GloVe embeddings [54] of the titles and abstracts and labels indicate total page views over a 60-day period, categorized into five classes based on quintiles. ", "page_idx": 18}, {"type": "text", "text": "The complete information and statistics of all these datasets aforementioned are summarized in Table 7. ", "page_idx": 18}, {"type": "table", "img_path": "jb5qN3212b/tmp/1768ce63728de6a38f969cb63b1955771bb9f302cf4db83b6878005d2e08c034.jpg", "table_caption": ["Table 7: Statistics of all the graph datasets. # C is the total number of distinct node classes. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Graph ID Classification Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide a detailed report on the ID ACC results of nine popular GNN pretrained backbones used in our paper, including GCN [34], GAT [69], GCNJK [81], GATJK [81], APPNP [35], MixHop [1], GPR-GNN [10], GCNII [7], and $\\mathrm{H}_{2}\\mathrm{GCN}$ [94]. The results of all the nine architectures on five common benchmarks and architecture GCN on five large-scale benchmarks are presented in Table 8 and Table 9 respectively. In addition, we present the ID ACC results of three training-based methods on the ten benchmarks. It is important to note that these training-based methods only yield results on five small-scale datasets and the moderately sized arxiv-year dataset. They fail to produce results on the remaining four large datasets, as shown in Tables 10 and Table 11. ", "page_idx": 19}, {"type": "table", "img_path": "jb5qN3212b/tmp/f0a5ce2cad795bc682671f3d4e5b1d7e3942402109485c57552dc7cd94ea57c0.jpg", "table_caption": ["Table 8: ID ACCs of six pre-trained methods on common benchmarks. For each pre-trained method, we take the average values that are percentages over 5 independently trained backbones. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "jb5qN3212b/tmp/06c924e9fc302937d8d1a3183e43218e5c9d04fd033061d181c139ea2c5f491a.jpg", "table_caption": ["Table 9: ID ACCs of pre-trained GCN models on five large-scale datasets. We report the average values that are percentages over 5 independently trained backbones. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "jb5qN3212b/tmp/8be75d0c635bf27614af634f7fed745b8181a4efb1ca8a9ed12be062c5803936.jpg", "table_caption": ["Table 10: ID ACCs of three training-based ood detection methods on five small-scale datasets. For each method, we take the average values that are percentages over 5 independently runs. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 11: ID ACCs of three training-based methods on five large-scale datasets, where OOM means Out-Of-Memory and OOT denotes that no results have been got after running over 48 hours for each run. All the training-based methods only have results on the moderately sized arXiv-year dataset. For each method, we take the average values that are percentages over 5 independently runs. ", "page_idx": 19}, {"type": "table", "img_path": "jb5qN3212b/tmp/a056f0f7fed7e876b4d14d1d61defea5f655ca9d7422bcceb32436ad569d0af3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.2 Detailed Main Graph OOD Detection Results ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "jb5qN3212b/tmp/e536c8b671646e3e52d145cb8b92129e156495785130514ede7aa2da41aa2ab9.jpg", "table_caption": ["Table 12: Detailed main results on common benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained GCN. We take the average values with standard errors that are percentages over 5 independently trained backbones. \u2191(\u2193) indicates larger (smaller) values are better. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "jb5qN3212b/tmp/2b64e768b809ca9c97d7e28776c5bccd5534f794979fea1e83acd643b92da456.jpg", "table_caption": ["Table 13: Detailed main results on large-scale benchmarks. Comparison with competitive out-of-distribution detection methods on pre-trained method GCN. We take the average values with standard errors that are percentages over 5 independently trained backbones. OOM means Out-Of-Memory and OOT denotes that no results have been got after running over 48 hours for each run. \u2191(\u2193) indicates larger (smaller) values are better. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.3 Graph OOD Detection Results on Various Backbones ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "GRASP can seamlessly be applied to various GNN backbones. In this section, we report the OOD detection performance of all baselines on various popular architectures in Table 14 ", "page_idx": 21}, {"type": "table", "img_path": "jb5qN3212b/tmp/82126c2aaaa8444386f4b86804df1bce5c15ac1fdae86512b457c35674600985.jpg", "table_caption": ["Table 14: Results of various GNN pretrained backbones on common benchmarks. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.4 Analysis of Hyper-parameters Sensitivity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We show the sensitivity of hyper-parameters $\\alpha$ , $\\beta$ and $k$ in Figure 6. The vertical axis in the figure represents the average AUROC values across common datasets. The performance comparison in the bar plot for each hyper-parameter is reported by fixing other hyper-parameters. We see that within the range of chosen hyperparameter values, our proposed method\u2019s performance does not vary significantly, which constantly outperforms baselines by a large margin. ", "page_idx": 22}, {"type": "image", "img_path": "jb5qN3212b/tmp/8bbeffe2a96e4d41cdaf4456105d8666856f30f8e89ac03d93144a6c6b182e85.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 6: Sensitivity analysis of all the hyper-parameters and the averaged AUROC values of common benchmarks on model GCN with MSP are displayed. The middle bar in each plot corresponds to the hyperparameter value used in our main experiments. ", "page_idx": 22}, {"type": "text", "text": "D.5 Relationship between Ratio of Intra-Edges and OOD Detection performance ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "jb5qN3212b/tmp/7d9fadc398be00d368cff912125bf2bc3021e8a353799d878fba5b4782737c94.jpg", "img_caption": ["Figure 7: Ratio of intra-edges and OOD detection performance have strong positive correlation. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "As shown in Figure 7, the ratio of intra-edges and OOD detection performance have strong positive correlation. This validates the correctness of Theorem 4.2. The detailed the number in Figure 7 are listed in Table 15. From the table we can see that after using our proposed augmentation, the ratio of intra-edges increases and in turn boosts the OOD detection performance. ", "page_idx": 22}, {"type": "text", "text": "Table 15: Ratios of intra-edges $\\eta_{i n t r a}$ and respective OOD detection performance on all benchmarks before and after employing graph augmentation. ", "page_idx": 23}, {"type": "table", "img_path": "jb5qN3212b/tmp/b12e1b4a6179c2bd3379a66ad9653810d6e032ed94d1ce559ce0d3f981f466ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.6 Computational Complexity Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our method\u2019s computational footprint in terms of runtime and memory usage after propagation for $k$ times is: ", "page_idx": 23}, {"type": "text", "text": "While our algorithm introduces the fully connected matrix $E$ , which brings about explosive growth of connections after each propagation and results in challenges in performing related computations, we adeptly transform the complex and time-consuming structural computations into simple matrix-vector multiplication operations, making original expensive operation tractable and leading to efficient linear complexity. Below, we provide the support from both mathmatical and empirical points. ", "page_idx": 23}, {"type": "text", "text": "Complexity analysis. Our method consists of two major computational modules, specifically, $(a)$ calculating connections between training nodes and testing nodes; $(b)$ augmentation propagation. ", "page_idx": 23}, {"type": "text", "text": "$(a)$ Calculating connections between training nodes and testing nodes. We utilize two indicator vectors, $I_{i d}$ and $I_{o o d}$ , both with a length of nodes number $N$ , to represent the number of connections each node has with $S_{i d}$ and $S_{o o d}$ , respectively. The initial values of $I_{i d}$ are set to 1 at the indices corresponding to $S_{i d}$ and 0 at the remaining indices (corresponding to $S_{o o d.}$ ). The initial value of $I_{i d}$ is denoted as $I_{i d}^{(0)}$ . Similarly, $I_{o o d}$ has a initial value of 1 at all indices corresponding to $S_{o o d}$ and 0 at the indices corresponding to $S_{i d}$ . We also represent the initial value of $I_{o o d}$ as $I_{o o d}^{(0)}$ . After propagation for $k$ times, the connections between each node with $S_{i d}$ are: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{i d}^{(k)}=A^{k}I_{i d}^{(0)}}\\\\ &{\\qquad=A^{k-1}\\cdot A I_{i d}^{(0)}}\\\\ &{\\qquad=A^{k-1}\\cdot I_{i d}^{(1)}}\\\\ &{\\qquad=\\left(\\mathrm{run~}k-1\\mathrm{~times~}...\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the above equation, AIi(di) can be computed in $\\mathcal{O}(|\\mathcal{E}_{i d}|)$ with sparse matrix multiplication for each propagation, where $|\\mathcal{E}_{i d}|$ represents number of edges connected to $S_{i d}$ nodes. ", "page_idx": 23}, {"type": "text", "text": "The above abalysis and conclusion also applys to computing $I_{o o d}^{(k)}$ , resulting computational complexity $\\mathcal{O}(|\\mathcal{E}_{o o d}|)$ , where $|\\mathcal{E}_{o o d}|$ is number of edges connected to $S_{o o d}$ nodes. This leads to time and memory costs $\\mathcal{O}(|\\mathcal{E}_{i d}|+|\\mathcal{E}_{o o d}|)^{\\prime}=\\mathcal{O}(|\\mathcal{E}|)$ for calculating needed number of connections in each propagation and $O(k|\\mathcal{E}|)$ and $\\mathcal{O}(\\vert\\vec{\\mathcal{E}}\\vert)$ for time and memory costs after propagation for $k$ times, respectively. ", "page_idx": 23}, {"type": "text", "text": "$(b)$ Augmentation propagation. As described by Equation 6 in the main paper, given a raw OOD scoring vector $\\hat{\\mathbf{g}}\\in\\mathbb{R}^{N}$ , the propagated scoring vector using augmentated adjacency matrix after ", "page_idx": 23}, {"type": "text", "text": "propagation for $k$ times is given by: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{g}_{G R A S P}=(\\bar{A}_{+})^{k}\\hat{\\mathbf{g}}}&{}\\\\ &{=(D_{+}^{-1}A_{+})^{k}\\hat{\\mathbf{g}}}\\\\ &{=(D_{+}^{-1}A_{+})^{k-1}\\cdot(D_{+}^{-1}(A+E))\\hat{\\mathbf{g}}}\\\\ &{=(D_{+}^{-1}A_{+})^{k-1}\\cdot(D_{+}^{-1}\\bigl[A\\hat{\\mathbf{g}}\\bigr]+D_{+}^{-1}\\bigl[E\\hat{\\mathbf{g}}\\bigr])}\\\\ &{=(\\mathsf{m n}\\,k-1\\,\\mathsf{t i m e s}\\,\\ldots)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the above equation, ", "page_idx": 24}, {"type": "text", "text": "\u2022 $E\\hat{\\mathbf{g}}$ can be computed with time/space complexity $\\mathcal{O}(n)$ by a simple summation operation $\\mathbf{g}_{i}$ in $G$ will be replaced by $\\sum_{i\\in G}\\mathbf{g}_{i}\\ )$ to get rid of the matrix multiplication.   \n\u2022 $A{\\hat{\\mathbf{g}}}$ can be computed in $\\mathcal{O}(|\\mathcal{E}|)$ with sparse matrix multiplication.   \n\u2022 $D_{+}^{-1}$ is an $\\mathcal{O}(N)$ operation since it is scaling over all elements in the vector. ", "page_idx": 24}, {"type": "text", "text": "Above all, the time/memory complexity of augmentated propagation is $O(N+|\\mathcal{E}|+n)$ for each propagation and the time and memory costs are $O(N+k|\\bar{\\varepsilon}|\\,\\bar{+}\\,\\bar{n})$ and $O(N+|\\mathcal{E}|+n)$ for $k$ times respectively with memory reutilization. ", "page_idx": 24}, {"type": "text", "text": "Integrating the above two considerations, the time complexity of our algorithm after propagation for $k$ times is $\\mathcal{O}(N+2k|\\mathcal{E}|+n)$ , while the space complexity is $\\dot{\\mathcal{O}}(N+|\\mathcal{E}|^{-}n)$ . Notably, the utilization of space reuse results in a linear space complexity. ", "page_idx": 24}, {"type": "text", "text": "Empirical results. Then we conduct comprehensive experiments comparing the time and space costs of our algorithm with various baselines (both post-hoc and training-based approaches) in Table 16 and Table 17. From the experimental results, it is evident that our algorithm is highly efficient across all datasets. ", "page_idx": 24}, {"type": "text", "text": "Based on the experimental results in Table 16 and Table 17, we have: ", "page_idx": 24}, {"type": "text", "text": "\u2022 In comparison to training-based methods, post-hoc methods exhibit significantly lower runtime and memory consumption.   \n\u2022 Considering that the minimum time and space complexity required to run a graph algorithm is $O(N+|\\mathcal{E}|)$ , as outlined in our algorithm complexity analysis, our algorithm incurs limited additional time and space costs, specifically $O(|\\mathcal{E}|)$ and $\\dot{\\boldsymbol{O}}(\\boldsymbol{n})$ on time and memory costs respectively in each propagation. This can be validated by the small extra overhead incurred by our algorithm compared to MSP or Energy from the table.   \n\u2022 Compared with training-based methods, our algorithm demonstrates substantially lower time and space demands on five large-scale datasets. And when compared to post-hoc baselines on these datasets, the overhead of our method is also reasonable. The performance of our method on these large-scale datasets underscores the strong practicality of our approach. ", "page_idx": 24}, {"type": "table", "img_path": "jb5qN3212b/tmp/fbee1f52e6d2a9854b2e731f9ed63db59eb2f9d16700fdd5ba9c1e024490ff49.jpg", "table_caption": ["Table 16: Time (s) and Memory (M) costs of all algorithms with backbone GCN on common benchmarks. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "jb5qN3212b/tmp/ee975e03beb40d121fcf8e5c2b0ce4c645a06c8b101a34dea2479c44cce05500.jpg", "table_caption": ["Table 17: Time (s) and Memory (M) costs of all algorithms with backbone GCN on large-scale benchmarks. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.7 Explore How Different Propagation Mechanisms Impact the Findings. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we investigate the effect of different propagation mechanisms on graph OOD detection performance, like higher-order diffusion and the other various classical propagation mechanisms in the literature, including Personalized PageRank (PPR) [35], Heat Kernel Diffusion (GraphHeat) [80], Graph Diffusion Convolution (GDC) [20], Mixing Higher-Order Propagation (MixHop) [1], and Generalized PageRank (GPR) [10]. The emperical observations are as follows: ", "page_idx": 25}, {"type": "text", "text": "\u2022 From the analysis of the hyper-parameter (order of propagation) and its impact on the AUROC in Figure 6, we find that OOD detection performance benefits from propagation orders within a reasonable range. However, excessive propagation (greater than 16) may be detrimental. \u2022 The AUROC results of the other various classical propagation mechanisms on graph OOD detection are shown in Table 18 and Table 19. We can see that these propagation mechanisms perform inconsistently among various datasets. In contrast, our propagation mechanism GRASP constantly demonstrates superior performance among all datasets. ", "page_idx": 25}, {"type": "table", "img_path": "jb5qN3212b/tmp/999c1ad44cc1fc8c447396cd9f0c21136b2a3dc8b1ed8b748d95f439976c5b36.jpg", "table_caption": ["Table 18: Various classical propagation mechanisms on each dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "jb5qN3212b/tmp/3a65fcee7985474634418daacbd235ac7c489d526af877dcf315fc9ad4c1af2a.jpg", "table_caption": ["Table 19: Various classical propagation mechanisms on each dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "While the proposed method of OOD score propagation shows promise in improving OOD detection in graph learning, there are limitations worth noting. The effectiveness of the method heavily relies on the connectivity and structure of the graph. In scenarios where the graph exhibits random connectivity patterns, the propagation of OOD scores with the proposed solution may no longer be effective. Addressing these limitations will be crucial for ensuring the robustness and generalizability of the proposed OOD detection approach in graph learning. ", "page_idx": 25}, {"type": "text", "text": "F Broader Impact ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This paper on detecting Out-of-Distribution (OOD) nodes in graph learning holds significant potential for various fields and applications. By addressing a salient yet under-explored challenge in graph neural networks, the proposed method of OOD score propagation offers a promising avenue for enhancing the robustness and reliability of graph-based machine learning systems. The implications extend beyond the realm of graph learning, as OOD detection is a critical component in many realworld applications, including anomaly detection, fraud detection, and network security. Improving OOD detection in graph structures can lead to more accurate and reliable decision-making in domains such as social network analysis, recommendation systems, and biological network analysis. Overall, this research has the potential to drive advancements in graph-based machine learning and contribute to the development of more robust and reliable AI systems with broader societal impact. ", "page_idx": 26}, {"type": "text", "text": "G Other Related Works ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Graph OOD Generalization. Unlike graph OOD detection, which aims to increase the gap between ID and OOD for effective OOD detection, graph OOD generalization aims to learn the invariant aspects behind ID and OOD. By enhancing these invariant points, the gap between ID and OOD data is reduced to improve the robustness of graph models on OOD data. The distinction among different proposed methods lies in identifying different invariant factors. GTRANS [30] leverages contrastive learning to learn the optimal perturbations of features and structures to enhance the GNN model\u2019s robustness against OOD. GRAPHPATCHER [31] improves GNN\u2019s robustness against low-degree scenarios during testing by corrupting a portion of nodes and reconstructing the original structure. LiSA [85] employs variational subgraph generators to generate label-invariant subgraphs for data augmentation, thereby enhancing the robustness of GNN models. [9] leverages the stability of correlation and employs a graph decorrelation method to learn stable associations, raising the graph OOD generalization capability. [78] boosts generalization by eliminating confounding bias. [41] extrapolates structure and feature spaces to generate OOD graph data, which is then used for data augmentation to improve OOD generalization. [28] strengthens graph OOD generalization by extracting invariant subgraphs and learning invariant patterns behind graphs based on these subgraphs. [8] introduces a set of minimal assumptions for feasible invariant graph learning. [5] investigates OOD generalization under different structural shifts. [22] incorporates label and environment causal independence to discover causal subgraphs for intensifying OOD generalization. [86] improves the OOD generalization of dynamic graphs by learning spatio-temporal invariant patterns from an environment learning perspective. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide theoretical contribution in Section 3 and Section 4, and experimental results in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We present the limitation discussion in Appendix E. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We provide the assumptions and complete proof in Appendix A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: See Section 5 and Appendix C. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] Justification: https://github.com/longfei-ma/GRASP ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Section 5. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 29}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix D.6 ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Appendix F ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] Justification: https://github.com/longfei-ma/GRASP ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]