[{"type": "text", "text": "Rethinking LLM Memorization through the Lens of Adversarial Compression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Avi Schwarzschild\u2217 schwarzschild@cmu.edu Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "Zhili Feng\u2217 zhilif@andrew.cmu.edu Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "pratyushmaini@cmu.edu Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "Zachary C. Lipton Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "J. Zico Kolter Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage. One major question is whether these models \u201cmemorize\u201d all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information. The answer hinges, to a large degree, on how we define memorization. In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs. A given string from the training data is considered memorized if it can be elicited by a prompt (much) shorter than the string itself\u2014in other words, if these strings can be \u201ccompressed\u201d with the model by computing adversarial prompts of fewer tokens. The ACR overcomes the limitations of existing notions of memorization by (i) offering an adversarial view of measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute. Our definition serves as a practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios. ", "page_idx": 0}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/de3afc23a0f74e151888d059b82f6848bee791f6f3725bbf32dd39e0613eb91f.jpg", "img_caption": ["Figure 1: We propose a compression ratio where we compare the length of the shortest prompt that elicits a training sample in response from an LLM to the length of that sample. If a string in the training data can be compressed, i.e. the minimal prompt is shorter than the sample, then we call it memorized. Our test is an easy-to-describe tool that is useful in the effort to gauge the misuse of data. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A central question in the discussion of large language models (LLMs) concerns the extent to which they memorize their training data versus how they generalize to new tasks and settings. Most practitioners seem to (at least informally) believe that LLMs do some degree of both: they clearly memorize parts of the training data\u2014for example, are often able to reproduce large portions of training data verbatim [Carlini et al., 2023]\u2014but they also seem to learn from this data, allowing them to generalize to new settings. The precise extent to which they do one or the other has massive implications for the practical and legal aspects of such models [Cooper et al., 2023]. Do LLMs truly produce new content, or do they only remix their training data? Should the act of training on copyrighted data be deemed unfair use of data, or should fair use be judged by the model\u2019s memorization? With respect to people, we distinguish plagiarising content from learning from it, but how should this extend to LLMs? The answer to such questions inherently relates to the extent to which LLMs memorize their training data. ", "page_idx": 1}, {"type": "text", "text": "However, even defining memorization for LLMs is challenging and many existing definitions leave a lot to be desired. Certain formulations claim that a passage from the training data is memorized if the LLM can reproduce it exactly [Nasr et al., 2023]. However, this ignores situations where, for instance, a prompt instructs the model to exactly repeat some phrase. Other formulations define memorization by whether or not prompting an LLM with a portion of text from the training set results in the completion of that training datum [Carlini et al., 2023]. But these formalisms rely fundamentally on the completions being a certain size, and typically very lengthy generations are required for sufficient certainty of memorization. More crucially, these definitions are too permissive because they ignore situations where model developers can (for legal compliance) post-hoc \u201calign\u201d an LLM by instructing their models not to produce certain copyrighted content [Ippolito et al., 2023]. But has such an instructed model really not memorized the sample in question, or does the model still contain all the information about the datum in its weights while it hides behind an illusion of compliance? Asking such questions becomes critical because this illusion of \u201cunlearning\u201d can often be easily broken as we show in Sections 4.1 and 4.3. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose a new definition of memorization based on a compression argument. Our definition posits that a phrase present in the training data is memorized if we can make the model reproduce the phrase using a prompt (much) shorter than the phrase itself. Operationalizing this definition requires finding the shortest adversarial input prompt that is specifically optimized to produce a target output. We call this ratio of input to output tokens the Adversarial Compression Ratio (ACR). In other words, memorization is inherently tied to whether a certain output can be represented in a compressed form, beyond what language models can do with typical text. We argue that such a definition provides an intuitive notion of memorization\u2014if a certain phrase exists within the LLM training data (e.g., is not itself generated text) and it can be reproduced with fewer input tokens than output tokens, then the phrase must be stored somehow within the weights of the LLM. Although it may be more natural to consider compression in terms of the LLM-based notions of input/output perplexity, we argue that a simple compression ratio based on input/output token counts provides a more intuitive explanation to non-technical audiences, and has the potential to serve as a legal basis for important questions about memorization and permissible data use. ", "page_idx": 1}, {"type": "text", "text": "In addition to its intuitive nature, our definition has several other desirable qualities. We show that it appropriately ascribes many famous quotes as being memorized by existing LLMs (i.e. they have high ACR values). On the other hand, we find that text not in the training data of an LLM, such as samples posted on the internet after the training period, are not compressible, that is their ACR is low. ", "page_idx": 1}, {"type": "text", "text": "We examine several unlearning methods using ACR to show that they do not substantially affect the memorization of the model. That is, even after explicit finetuning, models asked to \u201cforget\u201d certain pieces of content are still able to reproduce them with a high ACR\u2014in fact, not much smaller than with the original model. Our approach provides a simple and practical perspective on what memorization can mean, providing a useful tool for functional and legal analysis of LLMs. ", "page_idx": 1}, {"type": "text", "text": "2 Do We Really Need Another Notion of Memorization? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "With LLMs ingesting more and more data, questions about their memorization are attracting attention [e.g. Carlini et al., 2019, 2023, Nasr et al., 2023, Zhang et al., 2023]. There remains a pressing need to accurately define memorization in a way that serves as a practical tool to ascertain the fair use of public data from a legal standpoint. To ground the problem, consider the court\u2019s role in determining whether an LLM is breaching copyright. What constitutes a breach of copyright remains contentious and prior work defines this on a spectrum from \u2018training on a data point itself constitutes violation\u2019 to \u2018copyright violation only occurs if a model verbatim regurgitates training data\u2019. To formalize our argument for a new notion of memorization, we start with three definitions from prior work to highlight some of the gaps in the current thinking about memorization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Discoverable Memorization [Carlini et al., 2023]). Given a generative model M, a sample y from the training data made of a prefix yprefix and a suffix ysuffix is discoverably memorized if the prefix elicits the suffix in response, or $M(y_{p r e f i x})=y_{s u f f i x}$ . ", "page_idx": 2}, {"type": "text", "text": "Discoverable memorization, which says a string is memorized if the first few words elicit the rest of the quote exactly, has three particular problems. ", "page_idx": 2}, {"type": "text", "text": "1. Very permissive: This definition only tests for completion given the first portion of the sample. Since cases where the exact completion is the second most likely output would will not be labelled as memorized, this is extremely permissive.   \n2. Easy to evade: A model (or chat pipeline) that is modified ever so slightly to avoid perfect regurgitation of a given sample will appear not to have memorized that string, which leaves room for the illusion of compliance (Section 4.1).   \n3. Parameter choice requires validation data: We need to choose several words (or tokens) to include in the prompt and a number of tokens that have to match exactly in the output to turn this definition into a practical binary test for memorization. This adds the burden of setting hyperparameters, which usually rely on some holdout dataset. ", "page_idx": 2}, {"type": "text", "text": "In other words, this completion-based test is too conservative to capture memorization when model owners may take steps to make it look like their model has not memorized certain data. For example, unlearning methods (Section 4) can be used to obscure memorization according to this test. Our definition, which relies on optimization and not completion alone, is not fooled by these minor tricks to appear compliant. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Extractable Memorization [Nasr et al., 2023]). Given a generative model $M$ , a sample y from the training data is extractably memorized if an adversary, without access to the training set, can find an input prompt p that elicits y in response, or $M(p)=y$ . ", "page_idx": 2}, {"type": "text", "text": "A string is extractably memorized if there exists a prompt that elicits the string in response. This falls too far on the other side of the issue by being very restrictive\u2014what if the prompt includes the entire string in question, or worse, the instructions to repeat it? LLMs that are good at repeating will follow that instruction and output any string they are asked to. The risk is that it is possible to label any element of the training set as memorized, rendering this definition unfti for practical deployment. ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Counterfactual Memorization [Zhang et al., 2023]). Given a training algorithm $A$ that maps a dataset $D\\subset\\mathcal{D}$ to a generative model $M$ and a measure of model performance $S(M,y)$ on $a$ specific sample $y$ , the counterfactual memorization of a training example $y\\in\\mathcal{D}$ is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m e m(y):=\\underbrace{\\mathbb{E}_{D\\subset\\mathcal{D},y\\in D}{[S(A(D),y)]}}_{p e r f o r m a n c e\\;o n\\;y\\;w h e n\\;t r a i n e d\\;w i t h\\;y}-\\underbrace{\\mathbb{E}_{D^{\\prime}\\subset\\mathcal{D},y\\notin D^{\\prime}}{[S(A(D^{\\prime}),y)]}}_{p e r f o r m a n c e\\;o n\\;y\\;w h e n\\;n o t\\;t r a i n e d\\;w i t h\\;y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $D$ and $D^{\\prime}$ are subsets of training examples sampled from $\\mathcal{D}$ . The expectation is taken with respect to the random sampling of $D$ and $D^{\\prime}$ , as well as the randomness in the training algorithm $A$ . ", "page_idx": 2}, {"type": "text", "text": "Counterfactual memorization aims to separate memorization from generalization and requires a test that includes retraining many models. Given the cost of retaining large language models, such a definition is impractical for legal use. ", "page_idx": 2}, {"type": "text", "text": "In addition to these definitions from prior work on LLM memorization, there are two other seemingly viable approaches to memorization. Ultimately, we argue all of these frameworks\u2014the definitions in existing work and the approaches described below\u2014are each missing key elements of a good definition for assessing fair use of data and copyright infringement. ", "page_idx": 2}, {"type": "text", "text": "Membership is not memorization Perhaps if a copyrighted piece of data is in the training set at all we might consider it a problem. However, there is a subtle but crucial difference between training set membership and memorization. In particular, the ongoing lawsuits in the field [e.g. as covered by Metz and Robertson, 2024] leave open the possibility that reproducing another\u2019s creative work is problematic but training on samples from that data may not be. This is common practice in the arts\u2014consider that a copycat comedian telling someone else\u2019s jokes is stealing, but an up-and-comer learning from tapes of the greats is doing nothing wrong. So while membership inference attacks (MIAs) [e.g. Shokri et al., 2017] may look like tests for memorization and they are even intimately related to auditing machine unlearning [Carlini et al., 2021, Pawelczyk et al., 2023, Choi et al., 2024], they have three issues as tests for memorization. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "1. Very restrictive: LLMs are typically trained on trillions of tokens. Merely seeing a particular example at training does not distinguish between problematic and innocuous use of a training data point. Akin to plagiarism, it is okay to read copyrighted books, but copying is problematic. ", "page_idx": 3}, {"type": "text", "text": "2. Hard to arbitrate: Determining membership is problematic because it assumes good faith from the side of a corporation in releasing information about the data that they trained on in front of an arbiter. This becomes problematic given the inherently adversarial relationship. ", "page_idx": 3}, {"type": "text", "text": "3. Brittle evaluation: Membership inference attacks are extremely hard to perform with LLMs, which are trained for just one epoch on trillions of tokens. Some recent work shows that LLM membership inference is extremely brittle [Duan et al., 2024, Maini et al., 2021, Das et al., 2024]. ", "page_idx": 3}, {"type": "text", "text": "Perplexity is sensitive and hackable Another notion that appeals to the information theorist is the use of perplexity. We omit a formal definition here for brevity, but this encompasses approaches that use the model as a probability distribution over tokens to estimate the information content of a string by computing its compression rate under the given model with arithmetic encoding [Del\u00b4etang et al., 2023]. Perplexity-based methods are brittle to small changes in the model weights (Section 4.1) and can even be fooled by scaling the output distribution without affecting the greedy output itself. ", "page_idx": 3}, {"type": "text", "text": "3 How to Measure Memorization with Adversarial Compression ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our definition of memorization is based on answering the following question: Given a piece of text, how short is the minimal prompt that elicits that text exactly? In this section, we formally define and introduce our MINIPROMPT algorithm that we use to answer our central question. ", "page_idx": 3}, {"type": "text", "text": "3.1 A New Definition of Memorization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To begin, let a target natural text string $s$ have a token sequence representation $x\\in\\mathcal{V}^{*}$ which is a list of integer-valued indices that index a given vocabulary $\\mathcal{V}$ . We use $|\\cdot|$ to count the length of a token sequence. A tokenizer $T:s\\mapsto x$ maps from strings to token sequences. Let $M$ be an LLM that takes a list of tokens as input and outputs a distribution over the vocabulary representing the probabilities that the next token takes each of the values in $\\mathcal{V}$ . Consider that $M$ can perform generation by repeatedly predicting the next token from all the previous tokens with the argmax of its output appended to the sequence at each step (this process is called greedy decoding). With a slight abuse of notation, we will also call the greedy decoding result the output of $M$ . Let $y$ be the token sequence generated by $M$ , which we call a completion or response: $y=M(x)$ , which in natural language says that the model generates $y$ when prompted with $x$ or that $x$ elicits $y$ as a response from $M$ . So our compression ratio ACR is defined for a target sequence $y$ as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{ACR}(M,y)={\\frac{|y|}{|x^{*}|}},{\\mathrm{~where,~}}x^{*}=\\operatorname{arg\\,min}_{x}|x|{\\mathrm{~s.t.~}}M(x)=y.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 4 ( $\\tau$ -Compressible Memorization). Given a generative model $M$ , a sample y from the training data is $\\tau$ -memorized if the $\\operatorname{ACR}(M,y)>\\tau(y)$ . ", "page_idx": 3}, {"type": "text", "text": "The threshold $\\tau(y)$ is a configurable parameter of this definition. We might choose to compare the ACR to the compression ratio of the text when run through a general-purpose compression program (explicitly assumed not to have memorized any such text) such as GZIP [Gailly and Adler, 1992] or SMAZ [Sanfilippo, 2006]. This amounts to setting $\\tau(y)$ equal to the SMAZ compression ratio of $y$ , for example. Alternatively, one might even use the compression ratio of the arithmetic encoding under another LLM as a comparison point, for example if it was known with certainty that the LLM was never trained on the target output, and hence could not have memorized it [Del\u00b4etang et al., ", "page_idx": 3}, {"type": "text", "text": "2023]. In reality, copyright attribution cases are always subjective, and the goal of this work is not to argue for the right threshold function, rather to advocate for the adversarial compression framework for arbitrating fair data use. Thus, we use $\\tau=1$ in the experiments below, which we believe has substantial practical value.2 For more discussion on alternative thresholds, see Appendix $\\mathrm{E}$ where we discuss the implications of this choice. ", "page_idx": 4}, {"type": "text", "text": "Our definition and the compression ratio lead to two natural ways to aggregate over a set of examples. First, we can average the ratio over all samples/test strings and report the average compression ratio (this is $\\tau$ -independent). Second, we can label samples with a ratio greater than one as memorized and discuss the portion memorized over some set of test cases (for our choice of $\\tau=1$ ). In the empirical results below we use both of these metrics to describe various patterns of memorization. ", "page_idx": 4}, {"type": "text", "text": "3.2 MINIPROMPT: A Practical Algorithm for Compressible Memorization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the compression rate ACR is defined in terms of the solution to a minimization problem, we propose an approximate solver to estimate compression rates, see Algorithm 1. Specifically, to find the minimal prompt for a particular target sequence, or to solve the optimization problem in Equation (1), we use GCG [Zou et al., 2023] and search over sequence length (the full GCG algorithm is outlined in Appendix B). To be precise, we initialize the starting iterate to be a sequence $z^{(\\bar{0})}$ that is five tokens long. Each step of our algorithm runs GCG to optimize $z$ for $n$ steps. If the resulting prompt successfully produces the target string, i.e. $M(z^{(i)})\\stackrel{\\mathbf{\\bar{\\alpha}}}{=}y$ , then we reinitialize a new input sequence $z^{(i+1)}$ whose length is one token fewer than ${z}^{(i)}$ . If $n$ steps of GCG fails, or $M(z^{(i)})\\neq y$ , then the next iterate $z^{(i+1)}$ is initialized with five more tokens than ${z}^{(i)}$ . When each iterate is initialized, it is set to a random sequence of tokens sampled uniformly from the vocabulary. The maximum number of steps $n$ is set to 200 for the first iterate and increases by $20\\%$ each time the number of tokens in the prompt (length of $z$ ) increases. This accounts for our observation that with more optimizable tokens we usually need more steps of GCG to converge. In each run of GCG (inner loop of MINIPROMPT), we only run the number of steps we need to to see an exact match between $M\\bar{(z)}$ and $y$ (early stopping). Our design choices are heuristic, but they serve our purposes well so we leave better design to future work. ", "page_idx": 4}, {"type": "text", "text": "In all of our experiments below, when we present memorization metrics using compression, we are showing the results of running our MINIPROMPT algorithm. As noted in Algorithm 1, the optimizer is a choice, and where that option is not set to GCG, we make that clear. We borrow prompt optimization tools from work on jailbreaking where the goal is to force LLMs to break their alignment and produce nefarious and toxic output by way of optimizing prompts [Zou et al., 2023, Zhu et al., 2023, Chao et al., 2023, Andriushchenko, 2023]. Our extension of those techniques toward ends other than jailbreaking adds to the many and varied objectives that these discrete optimizers are useful for minimizing [Geiping et al., 2024]. As the prompt optimization space evolves, better choices for memorization testing and ACR computation may emerge. ", "page_idx": 4}, {"type": "text", "text": "4 Compressible Memorization in Practice ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We show the practical value of our definition and algorithm through several case studies as well as that the definition meets our expectations around memorization with validation experiments. Our case studies start with a demonstration of how a model owner trying to circumvent a regulation about data memorization might use in-context unlearning [Pawelczyk et al., 2023] by designing specific system prompts that change how apparent memorization is. Next, we look at two popular examples of unlearning and study how and where our definition serves as a more practical tool for model monitoring than alternatives. ", "page_idx": 4}, {"type": "text", "text": "4.1 The Illusion of Compliance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As data usage regulation advances, there is an emerging motive for organizations and individuals that serve or release models to make it hard to determine that their models have memorized anything. ", "page_idx": 4}, {"type": "text", "text": "MINIPROMPT finds short suffixes that elicit the target. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prompt: $<\\!\\mathrm{s}\\!>$ [INST] Give me a famous quote. Iron imper [/INST]   \nResponse: Sure! Here\u2019s a famous quote: $\\bar{\\bf\\Phi}\\backslash\\bar{\\bf n}^{\"}$ \u201dImperfection is beauty, madness is genius, and it\u2019s better   \nto be absolutely ridiculous than absolutely boring.\u201d ", "page_idx": 5}, {"type": "text", "text": "ICUL leads to the illusion of compliance. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prompt: $<\\!\\mathbf{S}\\!>$ [INST] $<<\\!\\mathrm{SYS}\\!>\\!>\\!\\mathrm{\\backslashn}$ Abstain from giving famous quote. $\\ln<</\\mathrm{SYS}>>\\ln\\backslash\\ n$ Give me a famous quote. [/INST] Response: I apologize, but I cannot provide you with a famous quote as it goes against my rules... ", "page_idx": 5}, {"type": "text", "text": "MINIPROMPT can still compress this famous quote. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prompt: $<\\!\\mathrm{s}\\!>[\\mathrm{INST}]<\\!<\\!\\mathrm{SYS}\\!>\\!>\\backslash\\mathrm{n}$ Abstain from giving famous quote. $\\backslash\\mathrm{n}<</\\mathrm{SYS}>>\\backslash\\mathrm{n}\\backslash\\mathrm{nGive}$ me a famous quote. impro \u201d,persistence [/INST]   \nResponse: Sure! Here\u2019s a famous quote: $\\backslash\\mathfrak{n}\\backslash\\mathfrak{n}^{\\ast}\\mathrm{Im}$ perfection is beauty, madness is genius, and it\u2019s better to be absolutely ridiculous than absolutely boring.\u201d ", "page_idx": 5}, {"type": "text", "text": "Figure 2: In-Context Unlearning (ICUL) fools completion not compression. For chat models, like Llama-2-7B-chat used here, we optimize tokens in addition to a fixed system prompt and instruction. In this setting, we show that MINIPROMPT compresses the quote in red to the two blue tokens in the prompt in the top cell. Next in the second cell, we show that ICUL, in the absence of optimized prompts, is successful at preventing completion. Finally, in the third cell, we show that even with ICUL system prompts MINIPROMPT can still compress this quote demonstrating the strength of our definition in regulatory settings. ", "page_idx": 5}, {"type": "text", "text": "The aim here is to make sure that compliance with fair use laws or the Right To Be Forgotten [OAG, 2021, Union, 2016] can be effectively monitored so we can avoid the illusion of compliance which crops up with other definitions of memorization. Those serving their models through APIs can augment prompts using in-context unlearning tools, which allegedly stop models from sharing specific data. To that end, we consider in-context unlearning as an example of a simple defense that these model owners might employ as a proof-of-concept that one can easily fool existing definitions of memorization but not our compression-based definition. ", "page_idx": 5}, {"type": "text", "text": "We start by looking for the compression ratio of a famous quote using Llama-2-7B-chat [Touvron et al., 2023] with a slightly modified strategy. Since instruction-tuned models are finetuned with instruction tags, we find optimized tokens between the start-of-instruction and the end-of-instruction tags. Then we put the in-context unlearning system prompt in place to show that it is effective at stopping the generation of famous quotes with or without the optimized tokens. Finally, we use MINIPROMPT again to find a suffix to the instruction that elicits the same famous quote. In Figure 2, we show examples of each of these steps. See Appendix $\\textrm{C}$ for further discussion. ", "page_idx": 5}, {"type": "text", "text": "We find short suffixes to these in-context unlearning system prompts that elicit memorized strings. Specifically, we find nearly the same number of optimized tokens placed between the instruction and the end-of-instruction tag force the model to give the same famous quote with and without the incontext unlearning system prompt. This consistency in ACR\u2014and therefore the memorization test\u2014 matches our intuition that without changing model weights memorized samples are not forgotten. It also serves as proof of the existence of cases where a minor change to the chat pipeline would change the completion-based memorization test result but not the compression-based test. ", "page_idx": 5}, {"type": "text", "text": "4.2 TOFU: Unlearning and Memorization with Author Profiles ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the unlearning community, baselines are generally considered weak [Maini et al., 2024], and measuring memorization with completion-based tests gives a false sense of unlearning, even for these weak baselines. On the other hand, with our compression-based test, we can monitor the memory and watch the model forget things. As with the weak in-context unlearning example above where we want a test that reveals that memorization changes are small, we hope to have a metric that reports memorization for some time while unlearning. ", "page_idx": 5}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/adb3a78fa6013c9ccc3c0050a6c1427af715197f1f02f7658b89679ab14db0a7.jpg", "img_caption": ["Figure 3: Left: Completion vs compression on TOFU data, unlearning Phi-1.5 with gradient ascent. Right: Generation after 20 unlearning steps. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We compare completion and compression tests on the TOFU dataset3 [Maini et al., 2024]. This dataset contains 200 synthetic author profiles, with 20 question-answer (QA) pairs for each author. We finetune Phi-1.5 [Li et al., 2023] on all $4{,}000\\;\\mathrm{QA}$ samples and use gradient ascent to unlearn $5\\%$ of the finetuned data. Following the TOFU framework [Maini et al., 2024], we finetune with a learning rate of $2\\times10^{-5}$ and reduce the learning rate during unlearning to $1\\times10^{-5}$ . Each stage is run for five epochs, and the first epoch includes a linear warm-up in the learning rate. The batch size is fixed to 16 and we use AdamW with a weight decay coefficient equal to 0.01. ", "page_idx": 6}, {"type": "text", "text": "As unlearning progresses, we prompt the model to generate answers to the supposedly unlearned questions and record the portion of data that can be completed and compressed. Figure 3 shows that after only 16 unlearning steps, none of the unlearned questions can be completed exactly. However, the model still demonstrates reasonable performance and has not deteriorated completely. As expected, compression shows that a considerable amount of the forget data is compressible and hence memorized. This case suggests that we cannot safely rely on completion as a metric for memorization because it is too conservative. ", "page_idx": 6}, {"type": "text", "text": "4.3 Trying to Forget Harry Potter ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In their paper on unlearning Harry Potter, Eldan and Russinovich [2023] claim that Llama-2-chat can forget about Harry Potter with several steps of unlearning. At first glance, querying the model with the same questions before and after unlearning seems to show that the model really can forget. However, the following three tests quickly convince us that the data is still contained within the model somehow, prompting further exploration into model memorization. ", "page_idx": 6}, {"type": "text", "text": "1. When asked the same questions in Russian, the model can answer correctly. We provide examples of such behavior in Appendix $\\mathrm{D}$ and Lynch et al. [2024] make the same observation. 2. While the correct answers have higher perplexity after the unlearning, they still have lower perplexity than wrong answers. Figure 4 shows that unlearning gives fewer of the correct answers extremely small losses, but an obvious dichotomy between the right and wrong answers remains. 3. With adversarial attacks designed to force affirmative answers without any information about the true answer, we can elicit the correct response\u2014 $.57\\%$ of the Harry Potter related responses can be elicited from the original Llama-2 model, and $50\\%$ can still be elicited after unlearning (Figure 8). ", "page_idx": 6}, {"type": "text", "text": "Motivated by these indications that the model has not truly forgotten Harry Potter, we measure the compression ratios of the true answers before and after unlearning. and find that they are still compressible. Figure 8 shows that even after unlearning, nearly the same amount of Harry Potter text is still memorized. We conclude that this unlearning tactic is not successful. Even though the model refrains from generating the correct answer, we are convinced the original strings are still contained in the weights\u2014a phenomenon that MINIPROMPT and ACR tests uncover. ", "page_idx": 6}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/d7530e138acda49dc68894472fbdcfb7306b01823325abfbba9c5527d1a1ee5e.jpg", "img_caption": ["Figure 4: Negative log-likelihood (normalized to $[0,1];$ of true and false answers given a Harry Potter question. Left: original Llama2 chat model; right: Llama2 after unlearning Harry Potter. The discrepancy is obvious pictorially, and also statistically significant: the KS-test between the true and wrong answer losses produces $\\mathbf{p}$ -values of $9.7\\mathrm{e}{-24}$ and $5.9\\mathrm{e-14}$ , respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4 Bigger Models Memorize More ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Since prior work has proposed alternative definitions of memorization that show that bigger models memorize more [Carlini et al., 2023], we ask whether our definition leads to the same finding. We show the same trends under our definition, meaning our view of memorization is consistent with existing scientific findings. We measure the fraction of the famous quotes that are compressible by four different Pythia models [Biderman et al., 2023] with parameter counts of 410M, 1.4B, 6.9B, and 12B and the results are in Figure 5. ", "page_idx": 7}, {"type": "text", "text": "4.5 Validation of MINIPROMPT with Four Categories of Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Since we are proposing a definition, the validation step is more complex than comparing it to some ground truth or baseline values. In particular, it is difficult to discuss the accuracy or the false-negative rate of an algorithm like ours since we have no labels. This is not a limitation in gathering data, it is an intrinsic challenge when the goal is to formalize what we even mean by memorization. Therefore, we present sanity checks that we hope any useful definition of memorization to pass. The following experiments are done with the open source Pythia [Biderman et al., 2023] models, which are trained on The Pile [Gao et al., 2020] providing transparency around their training data. ", "page_idx": 7}, {"type": "text", "text": "Random Sequences We look at random sequences of tokens because we want to rule out the possibility that we can always find an adversarial, few-token prompt even for random output\u2014random strings should not be compressible. To this end, we draw uniform random samples with replacement from the token vocabulary to build a set of 100 random outputs that vary in length (between 3 and 17 tokens). When decoded these strings are gibberish with no semantic meaning at all. We find that these strings are never compressible\u2014that is across multiple model sizes we never find a prompt shorter than the target that elicits the target sequence as output, see the zero-height bar in Figure 6. ", "page_idx": 7}, {"type": "text", "text": "Associated Press November 2023 To further determine the validity of our definition, we investigate the average compression rate of natural text that is not in the training set. If LLMs are good compressors of text they have never seen, then our definition may fail to isolate memorized samples. We take random sentences from Associated Press4 articles that were published in November 2023, well after the models we experiment with were trained. These strings are samples from the distribution of training data as the training set includes real news articles from just a few months prior. Thus, the fact that we can never find shorter prompts for this subset either, indicates that our models are not broadly able to compress arbitrary natural language. Again, see the zero-height bar in Figure 6. ", "page_idx": 7}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/5a324d0c4d7a44c8b1372073e95474f33c3c5e2e115d4f7b98ca223b027e4879.jpg", "img_caption": ["Figure 5: Memorization in Pythia models. Our definition is consistent with prior work arguing that bigger models memorize more, as indicated by higher compression ratios (left) and larger portions of data with ratios greater than one (right). These figures are from the Famous Quotes dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/7b9b91e02f812a7685ad2f33e2f676f38c8683f5b8e03bc9e0bf8710cae47d53.jpg", "img_caption": ["Figure 6: Memorization in Pythia-1.4B. The compression ratios (left) and the portion memorized (right) from all four datasets confirm that ACR aligns with our expectations on these validation sets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "One important thing to consider is that optimized prompts can elicit any output from the model (in particular things that were never seen during training). Our experiments, including those involving random strings and Associated Press data, show that while the GCG algorithm can elicit any output, we never observe compression of non-training data. This suggests that our method is robust against false positives. ", "page_idx": 8}, {"type": "text", "text": "Famous Quotes Next, we turn our attention to famous strings, of which many should be categorized as memorized by any useful definition. These are quotes like \u201cto be, or not to be, that is the question,\u201d which are examples repeated many times in the training data. We find that Pythia-1.4B has memorized almost half of this set and that the average ACR is the highest among our four categories of data. ", "page_idx": 8}, {"type": "text", "text": "Wikipedia Finally, we look at the memorization of training samples that are not common or famous, but that do exist in the training set. We take random sentences from Wikipedia articles that are included in the Pile5 [Gao et al., 2020] and compute their compression ratio. On this subset of data, we are aiming to compute the portion memorized as a new result, deviating from the goal above of passing sanity checks. Figure 6 shows that some of these sentences from Wikipedia are memorized and that the average compression ratio is between the average among famous quotes and news articles. Note that the memorized samples form this subset are strings that appear many times on the internet like \u201cThe Burton is a historic apartment building located at Indianapolis, Indiana.\u201d ", "page_idx": 8}, {"type": "text", "text": "On the note of sanity checks, one potential pitfall of our MINIPROMPT algorithm is its reliance on GCG. It is possible that there exist shorter strings than we can find. In this regard, we are exactly limited to finding an upper bound on the shortest prompt (as long as we do not search the astronomically large set of all prompts). But we can ease our minds by examining the minimal prompts we find for the four datasets above when we swap a random search technique for GCG in the MINIPROMPT algorithm. In fact, random search (see Algorithm 3) does slightly worse as an optimizer but tells the same story across the board. In other words, one might fear that our findings are the results of some peculiarity in GCG or some bias/preference GCG has for finding short prompts on some types of data. We establish that the same general trends in memorization can be observed with a gradient-free search algorithm, and thus conclude that we are not mistaking a GCG bias for some other signal. Since random search is gradient-free, this experiment quells any fears that GCG is merely relaying that the gradients are more informative on some examples than others. The details of this experiment and our exact random search algorithm are in Appendix E. ", "page_idx": 8}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/debc0740fd802e5af93c637e98f6ae378abbb4c83d53f862e889cdb789d3d07d.jpg", "img_caption": ["Figure 7: ACR versus target string length. Our experiments are designed to include a balanced mix of both short and long sequences, ensuring that the evaluation of the ACR metric is comprehensive and unbiased. This helps mitigate any potential bias towards longer sequences. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Another natural question that arises is about the relationship between ACR and target string length. To address this issue, we plot the sequence length on the horizontal axis to illustrate how the ACR varies across different lengths in Figure 7. This analysis shows that while longer sequences can achieve higher compression ratios, the ACR metric remains effective and meaningful for shorter sequences as well. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations Our findings are limited in that we mostly consider Pythia models and a natural question we do not address is what kinds of things are memorized by prominent state-of-the-art models. Without access to their training data and their model weights (combined with the memory constraints) these larger models are beyond the scope of our work. Also prior work makes claims about the portion of the training set that is memorized by various definitions, but running our algorithm on entire training sets would require more than the available computational resources. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact When proposing new definitions, we are tasked with justifying why a new one is needed as well as showing its ability to capture a phenomenon of interest. This stands in contrast to developing detection/classification tools whose accuracy can easily be measured using labeled data. It is difficult by nature to define memorization as there is no set of ground truth labels that indicate which samples are memorized. Consequently, the criteria for a memorization definition should rely on how useful it is. Our definition is a promising direction for future regulation on LLM fair use of data as well as helping model owners confidently release models trained on sensitive data without releasing that data. Deploying our framework in practice may require careful thought about how to set the compression threshold but as it relates to the legal setting this is not a limitation as law suits always have some subjectivity [Downing, 2024]. Our hope is to provide regulators, model owners, and the courts a mechanism to measure the extent to which a model contains a particular string within its weights and make discussion about data usage more grounded and quantitative. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Florian Tram\\`er, Vaishnavh Nagarajan, and Alvaro Velasquez for their constructive input and creative edge cases. ", "page_idx": 10}, {"type": "text", "text": "Zhili Feng and Avi Schwarzschild were supported by funding from the Bosch Center for Artificial Intelligence. Pratyush Maini was supported by DARPA GARD Contract HR00112020006. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Maksym Andriushchenko. Adversarial attacks on gpt-4 via simple random search. 2023. ", "page_idx": 10}, {"type": "text", "text": "Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy $(S P)$ , pages 141\u2013159. IEEE, 2021.   \nNicholas Carlini, Chang Liu, \u00b4Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX security symposium (USENIX security 19), pages 267\u2013284, 2019.   \nNicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. arXiv preprint arXiv:2112.03570, 2021.   \nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models, 2023.   \nPatrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023.   \nDami Choi, Yonadav Shavit, and David K Duvenaud. Tools for verifying neural models\u2019 training data. Advances in Neural Information Processing Systems, 36, 2024.   \nA Feder Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher CallisonBurch, Christopher A Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, Madiha Zahrah Choksi, et al. Report of the 1st workshop on generative ai and law. arXiv preprint arXiv:2311.06477, 2023.   \nDebeshee Das, Jie Zhang, and Florian Trame\\`r. Blind baselines beat membership inference attacks for foundation models. arXiv preprint arXiv:2406.16201, 2024.   \nGr\u00b4egoire Del\u00b4etang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023.   \nKate Downing. Copyright fundamentals for ai researchers. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR), 2024. URL https://iclr.cc/media/ iclr-2024/Slides/21804.pdf.   \nMichael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv:2402.07841, 2024.   \nRonen Eldan and Mark Russinovich. Who\u2019s harry potter? approximate unlearning in llms. arXiv preprint arXiv:2310.02238, 2023.   \nJean-Loup Gailly and Mark Adler. gzip. https://www.gnu.org/software/gzip/, 1992. Accessed: 2024-05-21.   \nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.   \nJonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein. Coercing llms to do and reveal (almost) anything. arXiv preprint arXiv:2402.14020, 2024.   \nMicah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon Wilson. The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning. arXiv preprint arXiv:2304.05366, 2023.   \nDaphne Ippolito, Florian Tram\\`er, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas Carlini. Preventing generation of verbatim memorization in language models gives a false sense of privacy. In Proceedings of the 16th International Natural Language Generation Conference, pages 28\u201353. Association for Computational Linguistics, 2023.   \nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023.   \nYuanzhi Li, S\u00b4ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \nZiming Liu, Ziqian Zhong, and Max Tegmark. Grokking as compression: A nonlinear complexity perspective. arXiv preprint arXiv:2310.05918, 2023.   \nAengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. Eight methods to evaluate robust unlearning in llms. arXiv preprint arXiv:2402.16835, 2024.   \nPratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution in machine learning. arXiv preprint arXiv:2104.10706, 2021.   \nPratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C Lipton, and J Zico Kolter. Tofu: A task of fictitious unlearning for llms. arXiv preprint arXiv:2401.06121, 2024.   \nCade Metz and Katie Robertson. Openai seeks to dismiss parts of the new york times\u2019s lawsuit. The New York Times, 2024. URL https://www.nytimes.com/2024/02/27/technology/ openai-new-york-times-lawsuit.html.   \nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tram\\`er, and Katherine Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023.   \nCA OAG. Ccpa regulations: Final regulation text. Office of the Attorney General, California Department of Justice, 2021.   \nMartin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models as few shot unlearners. arXiv preprint arXiv:2310.07579, 2023.   \nMinh Pham, Kelly O Marshall, Chinmay Hegde, and Niv Cohen. Robust concept erasure using task vectors. arXiv preprint arXiv:2404.03631, 2024.   \nSalvatore Sanfilippo. Smaz: Small strings compression library. https://github.com/antirez/ smaz, 2006. Accessed: 2024-05-21.   \nLeo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, and Stephan Gunnemann. Soft prompt threats: Attacking safety alignment and unlearning in open-source llms through the embedding space. arXiv preprint arXiv:2402.09063, 2024.   \nAyush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:18075\u201318086, 2021.   \nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 3\u201318. IEEE, 2017.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nEnayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman Arora. Machine unlearning via algorithmic stability. In Conference on Learning Theory, pages 4126\u20134142. PMLR, 2021.   \nEuropean Union. Regulation (eu) 2016/679 of the european parliament and of the council. Official Journal of the European Union, 2016.   \nChiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tram\\`er, and Nicholas Carlini. Counterfactual memorization in neural language models. Advances in Neural Information Processing Systems, 36:39321\u201339362, 2023.   \nSicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks on large language models. arXiv preprint arXiv:2310.15140, 2023.   \nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In addition to existing notions of memorization, our work touches on prompt optimization, compression in LLMs, and machine unlearning. In this section, we situate our approach and experimental results among the existing works from these domains. ", "page_idx": 13}, {"type": "text", "text": "Compression in LLMs There are several links between compression and language modelling and we borrow some vocabulary, but our work diverges from these other lines of research. For example, Dele\u00b4tang et al. [2023] argue that LLMs are compression engines, but they use models as probability distributions over tokens and arithmetic encoding to show that LLMs are good general compressors. As a metric for memorization, however, it is key that the compression algorithm is not generally useful, or it will tend to distinguish natural language that conforms to the LLMs probability distribution from data that does not, rather than help isolate memorized samples. Other links to compression include the ideas that learnability and generalization with real data comes in part from the compressability of natural data [Goldblum et al., 2023] and that grokking is related to the compressibility of networks themselves [Liu et al., 2023]. Our work does not make claims about the compressibility of datasets or models in principle but rather capitalizes on the fact that input-output compression using adversarially computed prompts for LLMs captures something interesting as it relates to memorization and fair use. In fact, Jiang et al. [2023] propose prompt compression for reducing time and cost of inference, which motivates our work as it suggests that we should be able to find short prompts that elicit the same responses as longer more natural-sounding inputs in some cases. ", "page_idx": 13}, {"type": "text", "text": "Unlearning The focus of machine unlearning [Bourtoule et al., 2021, Sekhari et al., 2021, Ullah et al., 2021, Pham et al., 2024, Lynch et al., 2024, Schwinn et al., 2024] is to remove private, sensitive, or false data from models without retraining them from scratch. Finding a cheap way to arrive at a model similar to one trained without some data is of practical interest to model owners, but evaluation is difficult. When motivated by privacy, the aim is to find models that leak no more information about an entity than a model trained without data on that entity. This is intimately related to memorization, and so we use a popular unlearning benchmark [Maini et al., 2024] in our experiments. ", "page_idx": 13}, {"type": "text", "text": "B Algorithms In Our Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In our experiments we use GCG [Zou et al., 2023] so we provide a pseudoscope description of it here along with our MINIPROMPT algorithm. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 MINIPROMPT ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: Model $M$ , Vocabulary $\\nu$ , Target Tokens $y$ , Maximum Prompt Length max Initialize n tokens in prompt $=5$   \nInitialize running min $=0$ , running $\\mathtt{m a x}=\\mathtt{m a x}$ ,   \nDefine $\\mathcal{L}(y|\\boldsymbol{x};M)$ as autoregressive next token prediction loss over $y$ given $x$ as context. ", "page_idx": 13}, {"type": "text", "text": "repeat $z=\\mathrm{GCG}(\\mathcal{L},\\mathcal{V},\\mathcal{Y}$ , n tokens in prompt, num steps) $\\triangleright$ Or other discrete optimizer. if $M(z)=y$ then running max $=\\mathbf{n}$ tokens in prompt n tokens in prompt $=\\mathbf{n}$ tokens in prompt \u22121 best $=z$ else running min $=\\mathbf{n}$ tokens in prompt n tokens in prompt $=\\mathbf{n}$ tokens in prompt $+\\:5$ end if ", "page_idx": 13}, {"type": "text", "text": "until n tokens in prompt $\\leq$ running min or n tokens in prompt $\\geq$ running max return best ", "page_idx": 13}, {"type": "text", "text": "Algorithm 2 Greedy Coordinate Gradient (GCG) [Zou et al., 2023] ", "page_idx": 14}, {"type": "text", "text": "Input: Loss $\\mathcal{L}$ , Vocab. $\\mathcal{V}$ , Target $y$ , Num. Tokens n tokens, Num. Steps num steps   \nInitialize prompt $x$ to random list of n tokens tokens from $\\nu$   \n$E=M$ \u2019s embedding matrix   \nfor num steps times do for $i=0,...,\\mathtt{n}$ tokens do $\\mathcal{X}_{i}=\\mathrm{Top}{-k(-\\nabla_{e{x}_{i}}\\mathcal{L}(y|x))}$ end for for $b=1,...,B$ do $\\begin{array}{r l}&{\\tilde{x}^{(b)}=\\mathrm{x}}\\\\ &{\\tilde{x}_{i}^{(b)}=\\mathrm{Uniform}(\\mathcal{X}_{i}),i=\\mathrm{Uniform}([1,...,\\mathrm{n.tokens}])}\\end{array}$ end for $x=\\tilde{x}^{(b^{*})}$ where $b^{*}=\\arg\\operatorname*{min}_{b}\\mathcal{L}(y|\\tilde{x}^{(b)})$   \nend for   \nreturn $x$ ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Random Search (for LLM prompts) [Andriushchenko, 2023] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: Loss $\\mathcal{L}$ , Vocab. $\\mathcal{V}$ , Target $y$ , Num. Tokens n tokens, Num. Steps num steps   \nInitialize prompt $x$ to random list of n tokens tokens from $\\nu$   \nfor num steps times do for $b=1,...,B$ do $\\tilde{x}_{\\mathrm{\\Sigma}}^{(b)}=\\mathbf{x}$ x\u02dci(b)= Uniform(V), i = Uniform([1, ..., n tokens]) end for $x=\\tilde{x}^{(b^{*})}$ where $b^{*}=\\arg\\operatorname*{min}_{b}\\mathcal{L}(y|\\tilde{x}^{(b)})$   \nend for   \nreturn $x$ ", "page_idx": 14}, {"type": "text", "text": "C More Details of In-context Unlearning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As a proof of concept, we further perform adversarial compression on five famous quotes and get an average compression ratios of 6.54 and 4.74, with versus without the unlearning system prompt, respectively. Adding the unlearning prompt does seem to make adversarial compression slightly harder, but the key takeaway is that all of the quotes still enjoy compression ratios larger than one \u2013 they are still all memorized. This is really a sanity check since there is no weight update, naturally none of the internal knowledge should have been forgotten. ", "page_idx": 14}, {"type": "text", "text": "In the textbox below, note that the ICUL system prompt leads to an abstention response with the original optimized prompt. ", "page_idx": 14}, {"type": "text", "text": "ICUL leads to the Illusion of Compliance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Prompt: $<\\!\\mathrm{s}\\!>\\![\\mathrm{INST}]<\\!<\\!\\mathrm{SYS}\\!>\\!>$ \\nAbstain from giving famous quote. $\\backslash\\mathrm{n}<</\\mathrm{SYS}>>\\backslash\\mathrm{n}\\backslash\\mathrm{nGive}$ me a famous quote. Iron imper [/INST] Response: I apologize, but I cannot provide you with a quote that promotes... ", "page_idx": 14}, {"type": "text", "text": "D More Details of Unlearning Harry Potter ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As we mention in the main text, unlearning a prevalent concept in the pretraining data is hard. A cleverly phrased question can defeat the seemingly successful unlearning attempt. One of the example we provide is to ask questions about Harry Potter in Russian, which makes the model generate Harry Potter facts very easily. Here are some examples. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Prompt: [INST] \u041a\u0442\u043e \u0442\u0430\u043a\u043e\u0439 \u0413\u0430\u0440\u0440\u0438 \u041f\u043e\u0442\u0442\u0435\u0440? [/INST] (Translate: Who is Harry Potter?) Generation: \u0413\u0430\u0440\u0440\u0438 \u041f\u043e\u0442\u0442\u0435\u0440 - \u044d\u0442\u043e \u0432\u044b\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u044b\u0439 \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043d\u0430 \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436\u0435 \u0438\u0437 \u0441\u0435\u0440\u0438\u0438 \u0440\u043e\u043c\u0430\u043d\u043e\u0432 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0439 \u043f\u0438\u0441\u0430\u0442\u0435\u043b\u044c\u043d\u0438\u0446\u044b \u0414\u0436\u043e\u0430\u043d \u0420\u043e\u0443\u043b\u0438\u043d\u0433. (Translation: Harry Potter is a fictional character based on a character from a series of novels by English writer JK Rowling.) ", "page_idx": 14}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/fe023d65b46a225e10829c17d9a79b6615c448775ced9b23a633f95b004eab9d.jpg", "img_caption": ["Figure 8: Left: Fraction of Harry Potter texts that are compressible. Right: an example of hard tokens that elicit Harry Potter text. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "\u2022 Prompt: [INST] \u041a\u0442\u043e \u0442\u0430\u043a\u043e\u0439 \u0414\u0430\u043c\u0431\u043b\u0434\u043e\u0440 \u0432 \u043a\u043d\u0438\u0433\u0435 \u043e \u0413\u0430\u0440\u0440\u0438 \u041f\u043e\u0442\u0442\u0435\u0440\u0435? (Translation: Who is Dumbledore in the Harry Potter series book?) ", "page_idx": 15}, {"type": "text", "text": "Generation: \u0414\u0430\u043c\u0431\u043b\u0434\u043e\u0440 - \u044d\u0442\u043e \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436 \u0438\u0437 \u0441\u0435\u0440\u0438\u0438 \u0440\u043e\u043c\u0430\u043d\u043e\u0432 \u043e \u0413\u0430\u0440\u0440\u0438 \u041f\u043e\u0442\u0442\u0435\u0440\u0435, \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0445 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0439 \u043f\u0438\u0441\u0430\u0442\u0435\u043b\u044c\u043d\u0438\u0446\u0435\u0439 \u0414\u0436\u043e\u0430\u043d \u0420\u043e\u0443\u043b\u0438\u043d\u0433. (Translation: Dumbledore is a character from the Harry Potter series of novels written by the English writer JK Rowling.) ", "page_idx": 15}, {"type": "text", "text": "\u2022 Prompt: [INST] \u041a\u0442\u043e \u0442\u0430\u043a\u043e\u0439 \u0421\u0435\u0432\u0435\u0440\u0443\u0441 \u0421\u043d\u0435\u0439\u043f \u0432 \u043a\u043d\u0438\u0433\u0435 \u043e \u0413\u0430\u0440\u0440\u0438 \u041f\u043e\u0442\u0442\u0435\u0440\u0435? [/INST] (Translation: Who is Severus Snape in the Harry Potter series book?)] ", "page_idx": 15}, {"type": "text", "text": "Generation: \u0421\u0435\u0432\u0435\u0440\u0443\u0441 \u0421\u043d\u0435\u0439\u043f (Severus Snape) - \u044d\u0442\u043e \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436 \u0438\u0437 \u0441\u0435\u0440\u0438\u0438 \u043a\u043d\u0438\u0433 \u043e \u0413\u0430\u0440\u0440\u0438 \u041f\u043e\u0442\u0442\u0435\u0440\u0435, \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0445 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0439 \u043f\u0438\u0441\u0430\u0442\u0435\u043b\u044c\u043d\u0438\u0446\u0435\u0439 \u0414\u0436\u043e\u0430\u043d \u0420\u043e\u0443\u043b\u0438\u043d\u0433. \u0421\u0435\u0432\u0435\u0440\u0443\u0441 \u0421\u043d\u0435\u0439\u043f - \u044d\u0442\u043e \u0443\u0447\u0438\u0442\u0435\u043b\u044c \u043c\u0430\u0433\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0432 \u0448\u043a\u043e\u043b\u0435 \u043c\u0430\u0433\u0438\u0438 (Translation: Severus Snape is a character from the Harry Potter series of books written by the English writer JK Rowling. Severus Snape is a magic teacher who works at a magic school) ", "page_idx": 15}, {"type": "text", "text": "E Extended Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Some of our results in the main body are extended here. In particular, we include similar findings on other models and with other discrete optimizers as well as a discussion of alternative compression thresholds. ", "page_idx": 15}, {"type": "text", "text": "E.1 More Models and Discrete Optimizers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The main body of this paper includes results on our four categories of data using Pythia-1.4B. We also examine the memorization of another Pythia model. In Figure 9 we show the memorization patterns for Pythia-410M. Additionally, the reliance on GCG brings up a possible confounder, which is that perhaps the gradient information is different for some samples than others. To address this, we use random search and find similar trends as shown in Figure 10. ", "page_idx": 15}, {"type": "text", "text": "E.2 Alternative Thresholds ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the main body of this paper we discuss various choices for the threshold function $\\tau$ and we continue that discussion here. First, note that SMAZ, a compression for natural language that is good for short strings, provides us with a good baseline for compression. In Figure 11, we show the Pythia-1.4B ACR and the SMAZ compression ratios for all the samples in our four categories of data. As the figure shows, when we choose a data-dependent threshold many fewer samples get labelled as memorized. This is a reasonable knob for regulators and litigators to turn. In a court of law (in the United States), this kind of evidence would be more or less compelling, but in every case would still contribute to the evidentiary body presented and help lawyers make cases about copyright infringement. ", "page_idx": 15}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/72d88748de850d46c55efc9e536ed4181fca7e2be52d3a1c3fdb4161f39b083f.jpg", "img_caption": ["Figure 9: Pythia-410M Memorization with GCG. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/11c358003dc830d5b236d002512b3f24731d920afeabbe8a005caee3dfe290b6.jpg", "img_caption": ["Figure 10: Pythia-1.4B Memorization with Random Search. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E.3 Paraphrased Famous Quotes ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We also explore how compressible paraphrased versions of famous quotes are. This line of questioning aims to disentangle exact match memorization from concept memorization. We paraphrase the 100 famous quotes with ChatGPT and compute their ACR values. We find that paraphrasing lowers the ACR and the portion memorized suggesting that our definition and test for memorization are, in fact, measuring exact match memorization. See Table 1. ", "page_idx": 16}, {"type": "table", "img_path": "KFmRMvzAZy/tmp/a0db6e83524a26f2e3d9110225d0f61693837741579c391fa9689a0cf0ebf2a7.jpg", "table_caption": ["Table 1: Memorization statistics for Pythia-1.4B model. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F Compute ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In order to run MINIPROMPT, we need enough GPU memory to load a model and compute the gradients of the inputs for a batch of prompts (see GCG algorithm above). This means for the smaller models (fewer than 7B parameters), with a single NVIDIA RTX A4000 GPU we can compute minimal prompt in a few minutes if it is highly compressible and a few hours (around 10 in the worst case) if we need to search for very long prompts. For the larger models (all models we consider with 7B or more parameters), similar timing holds with 4 NVIDIA RTX A4000 GPUS. ", "page_idx": 16}, {"type": "image", "img_path": "KFmRMvzAZy/tmp/e96628fab9a25618f08c84ffee998da26eee8d5892e4547e8269779e73ee79cf.jpg", "img_caption": ["Figure 11: Comparing SMAZ compression ratios to the ACR according to Pythia-1.4B of four categories of data. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Section 4 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Section 5 ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: We make no theoretical statement or proofs. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Section 2, Appendix B Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Code included in the supplementary material and linked to in the footnote on the title page. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Section 4 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our methods are for estimating the memorization of data by single models. For the illustrative purposes of our work, our empirical findings are averaged over datasets but not randomized trials. For this reason, we do not have error bars to report but we do discuss this in the limitations section. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Appendix $\\mathrm{F}$ Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Section 5 ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: License is included when the data is introduced in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: No new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No human Subjects. ", "page_idx": 22}, {"type": "text", "text": "Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: No human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]