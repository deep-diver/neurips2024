[{"figure_path": "KFmRMvzAZy/figures/figures_0_1.jpg", "caption": "Figure 1: We propose a compression ratio where we compare the length of the shortest prompt that elicits a training sample in response from an LLM to the length of that sample. If a string in the training data can be compressed, i.e. the minimal prompt is shorter than the sample, then we call it memorized. Our test is an easy-to-describe tool that is useful in the effort to gauge the misuse of data.", "description": "This figure illustrates the concept of Adversarial Compression Ratio (ACR) for measuring memorization in LLMs.  It shows two examples: one where a short prompt successfully elicits a long target string (high ACR, indicating memorization), and another where a long prompt is needed for a shorter string (low ACR, no memorization).  The ACR is calculated by comparing the length of the shortest prompt to the length of the target string.  A high ACR suggests that the LLM has memorized the target string because it can be reproduced with a significantly shorter prompt.", "section": "1 Introduction"}, {"figure_path": "KFmRMvzAZy/figures/figures_6_1.jpg", "caption": "Figure 3: Left: Completion vs compression on TOFU data, unlearning Phi-1.5 with gradient ascent. Right: Generation after 20 unlearning steps.", "description": "This figure compares the performance of completion-based and compression-based memorization tests during the unlearning process using the TOFU dataset and Phi-1.5 model.  The left panel shows the portion of memorized data that can be completed and compressed as unlearning progresses. The right panel shows the model's output after 20 steps of unlearning, highlighting the discrepancy between the ground truth and the generated answer. This illustrates that while completion-based tests might indicate that the model forgets the information, compression-based tests still show a considerable amount of memorized information.", "section": "4 Compressible Memorization in Practice"}, {"figure_path": "KFmRMvzAZy/figures/figures_7_1.jpg", "caption": "Figure 4: Negative log-likelihood (normalized to [0, 1]) of true and false answers given a Harry Potter question. Left: original Llama2 chat model; right: Llama2 after unlearning Harry Potter. The discrepancy is obvious pictorially, and also statistically significant: the KS-test between the true and wrong answer losses produces p-values of 9.7e-24 and 5.9e-14, respectively.", "description": "This figure compares the negative log-likelihood (normalized loss) distributions of correct and incorrect answers for Harry Potter related questions, before and after an unlearning process. The left panel shows the distribution for the original Llama2-chat model, while the right panel shows the distribution after an attempt to make the model 'forget' about Harry Potter. The significant difference in the distributions (with p-values from Kolmogorov-Smirnov test showing statistical significance) demonstrates that, despite the unlearning attempt, the model retains information about Harry Potter.", "section": "4.3 Trying to Forget Harry Potter"}, {"figure_path": "KFmRMvzAZy/figures/figures_8_1.jpg", "caption": "Figure 5: Memorization in Pythia models. Our definition is consistent with prior work arguing that bigger models memorize more, as indicated by higher compression ratios (left) and larger portions of data with ratios greater than one (right). These figures are from the Famous Quotes dataset.", "description": "This figure shows the results of applying the Adversarial Compression Ratio (ACR) metric to four different sized Pythia language models. The left panel displays the average compression ratio for each model size, showing a clear trend of increasing compression with increasing model size.  The right panel shows the proportion of famous quotes that have a compression ratio greater than 1 (i.e., they are considered \"memorized\") for each model size.  This also shows an increasing trend with model size, supporting the claim that larger models memorize more.", "section": "4.4 Bigger Models Memorize More"}, {"figure_path": "KFmRMvzAZy/figures/figures_8_2.jpg", "caption": "Figure 6: Memorization in Pythia-1.4B. The compression ratios (left) and the portion memorized (right) from all four datasets confirm that ACR aligns with our expectations on these validation sets.", "description": "This figure displays the average compression ratio and portion memorized for four different datasets using the Pythia-1.4B model.  The four datasets are: Famous Quotes, Wikipedia articles, Associated Press news articles, and randomly generated sequences.  The left bar chart shows the average compression ratio (ACR), which represents the ratio of the shortest prompt length to the target string length.  A higher ratio indicates better compression. The right bar chart shows the portion of samples in each dataset with an ACR greater than 1, indicating that those samples are considered memorized by the model. The results demonstrate that the ACR metric aligns with the expected memorization levels for each dataset type: Famous Quotes have higher memorization, while random sequences have almost no memorization. ", "section": "Validation of MINIPROMPT with Four Categories of Data"}, {"figure_path": "KFmRMvzAZy/figures/figures_9_1.jpg", "caption": "Figure 7: ACR versus target string length. Our experiments are designed to include a balanced mix of both short and long sequences, ensuring that the evaluation of the ACR metric is comprehensive and unbiased. This helps mitigate any potential bias towards longer sequences.", "description": "This figure shows the relationship between the adversarial compression ratio (ACR) and the length of the target string.  The data points are colored by data category. The x-axis represents the length of the target string (in tokens), and the y-axis represents the ACR.  The plot demonstrates that the ACR is not strongly influenced by target string length, with a mix of values observed across the full range of string lengths, supporting the robustness of the ACR as a metric for measuring memorization in LLMs. ", "section": "3 How to Measure Memorization with Adversarial Compression"}, {"figure_path": "KFmRMvzAZy/figures/figures_15_1.jpg", "caption": "Figure 8: Left: Fraction of Harry Potter texts that are compressible. Right: an example of hard tokens that elicit Harry Potter text.", "description": "This figure demonstrates that even after an unlearning process, a significant portion of Harry Potter-related text remains compressible by the model, indicating that the information is still stored in the model's weights and can be retrieved with specific prompts. The left panel shows the portion of Harry Potter texts that are compressible, while the right panel provides an example of a short prompt that elicits a Harry Potter quote.", "section": "4.3 Trying to Forget Harry Potter"}, {"figure_path": "KFmRMvzAZy/figures/figures_16_1.jpg", "caption": "Figure 9: Pythia-410M Memorization with GCG.", "description": "The bar chart displays the average compression ratio and the portion memorized for Pythia-410M model across four datasets: Famous Quotes, Wikipedia articles, Associated Press news, and random sequences.  The results suggest that the model memorizes a significant portion of famous quotes, while memorization is negligible for other datasets.  This figure demonstrates a trend consistent with prior work, which shows that larger models tend to memorize more data.", "section": "4 Compressible Memorization in Practice"}, {"figure_path": "KFmRMvzAZy/figures/figures_16_2.jpg", "caption": "Figure 1: We propose a compression ratio where we compare the length of the shortest prompt that elicits a training sample in response from an LLM to the length of that sample. If a string in the training data can be compressed, i.e. the minimal prompt is shorter than the sample, then we call it memorized. Our test is an easy-to-describe tool that is useful in the effort to gauge the misuse of data.", "description": "This figure illustrates the concept of Adversarial Compression Ratio (ACR).  It shows two examples: one where a short prompt successfully elicits a long target string (high ACR, considered memorized), and another where a long prompt is needed to elicit a short target string (low ACR, not memorized). The figure visually represents the core idea of the proposed memorization metric, highlighting the comparison between the prompt length and the target string length to determine if a string is memorized by the model.", "section": "Introduction"}, {"figure_path": "KFmRMvzAZy/figures/figures_17_1.jpg", "caption": "Figure 11: Comparing SMAZ compression ratios to the ACR according to Pythia-1.4B of four categories of data.", "description": "This figure compares the SMAZ (Small strings compression library) compression ratios with the Adversarial Compression Ratio (ACR) for four different categories of data using the Pythia-1.4B language model.  The x-axis represents the ACR, and the y-axis represents the SMAZ compression ratio. Each point represents a data sample from one of the four categories: Famous Quotes, Wikipedia articles, Associated Press news articles, and random sequences of tokens.  The dashed lines represent thresholds (ACR=1 and ACR=SMAZ ratio). This visualization helps to assess the relationship between the two compression methods and evaluate whether the ACR accurately reflects memorization, especially compared to a general-purpose compression algorithm like SMAZ. ", "section": "E.2 Alternative Thresholds"}]