[{"type": "text", "text": "On provable privacy vulnerabilities of graph representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ruofan Wu\u2217\u00a7, Guanhua Fang\u2217\u2021, Mingyang Zhang\u00a7, Qiying Pan\u00b6, Tengfei Liu\u00a71\u2020, and Weiqiang Wang\u00a7 ", "page_idx": 0}, {"type": "text", "text": "\u00a7Ant Group \u2021Fudan University \u00b6Shanghai Jiao Tong University {ruofan.wrf, zhangmingyang.zmy, aaron.ltf, weiqiang.wwq}@antgroup.com fanggh@fudan.edu.cn, sim10_arity@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of similarity-based edge reconstruction attacks (SERA), furnishing a non-asymptotic analysis of their reconstruction capacities. Moreover, we present empirical corroboration indicating that such attacks can (almost) perfectly reconstruct sparse graphs as graph size increases. Conversely, we establish that sparsity is a critical factor for SERA\u2019s effectiveness, as demonstrated through analysis and experiments on (dense) stochastic block models. Finally, we explore the resilience of private graph representations produced via noisy aggregation (NAG) mechanism against SERA. Through theoretical analysis and empirical assessments, we affirm the mitigation of SERA using NAG. In parallel, we also empirically delineate instances wherein SERA demonstrates both efficacy and deficiency in its capacity to function as an instrument for elucidating the trade-off between privacy and utility. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the surging developments of graph representation learning (GRL) [15], there has been growing apprehensions concerning the security challenges associated with the deployment of graph neural models in real-world scenarios [10]. GRL models harness the topological information of the underlying graph for producing high-quality predictions or graph representations. Meanwhile, these models bear the risk of inadvertently divulging the same topological information through the graph representations they produce. Such kind of security risks have been empirically validated through the examination of the attacking performance of edge reconstruction algorithms [11, 17, 34, 46], among which a simple form of attack based solely on the representation similarity of node pairs is shown to achieve strikingly strong performance, without the requirement of additional knowledge like encoder architecture or auxiliary datasets [17]. ", "page_idx": 0}, {"type": "text", "text": "Despite the empirical evidence of topological vulnerabilities of graph representations, theoretical explanations delineating the effectiveness of such attacks remain largely unexplored: As demonstrated in previous studies [11, 17], similarity-based attacks are remarkably effective against sparse graphs that exhibit a generalized homophily pattern, i.e., there exists a significant correlation between the similarity of node features and edge adjacency information. This phenomenon posits that feature similarity may serve as a confounding factor, potentially impacting the efficacy of similarity-based attacks. It is therefore valuable to understand the influence of graph properties, such as feature similarity and sparsity, on the edge reconstruction process of the attacking procedures. ", "page_idx": 1}, {"type": "text", "text": "Beyond their capability in characterizing the vulnerabilities of representations, attacking algorithms may also function as empirical attestations of privacy-preserving inference protocols that fulflil formal privacy guarantees such as differential privacy [9, Section 4]. As an illustrative case, membership inference attacks can be employed for auditing differential privacy [31]. Since edge reconstruction is equivalent to edge membership inference on graphs [43], it is thus pertinent to explore the performance of similarity-based attacks when confronted with privacy-preserving graph representations [30, 36]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we take initial steps toward a principled understanding of structural vulnerabilities of graph representations under the similarity-based edge reconstruction attack (hereafter abbreviated as SERA) which forms a realistic threat in many practical scenarios such as vertical federated learning [36]. In particular, we establish the following theoretical as well as empirical findings: ", "page_idx": 1}, {"type": "text", "text": "(i) Success modes of SERA Through applying SERA to sparse random graphs equipped with independent random node features, we show that SERA provably reconstructs the input graph via a non-asymptotic analysis. The result indicates that feature similarity is not necessary for SERA to succeed. We conduct both synthetic experiments as well as real-world data evaluations to empirically validate our theory. ", "page_idx": 1}, {"type": "text", "text": "(ii) Failure modes of SERA We show, through theoretical analysis and corroborative synthetic experiments, performance lower bounds when applying SERA to stochastic block models (SBM) with independent random node features: When the underlying SBM has $\\Theta(1)$ intra-group connection probability, edge recovery through graph representations becomes provably hard. ", "page_idx": 1}, {"type": "text", "text": "(iii) Mitigation of SERA We assess the resilience of SERA using noisy aggregation (NAG) as the privacy protection mechanism. Theoretical guarantees of NAG are established which further extends previous results, accompanied by extensive empirical evaluations to corroborate our theoretical assertions. Intriguingly, our findings reveal instances wherein NAG provides significant resistance to SERA, even under some scenarios where it only guarantees very weak privacy. Such discoveries delineate the circumstances that elucidate both the strengths and limitations of SERA as a privacy auditing tool. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Typically, there exist two categories of private information that may potentially be compromised during the training or deployment phases of graph neural network models: The (sensitive) node attributes and the adjacency relation between nodes. In this paper we focus on the later category since edge adjacency relations are less informative, i.e., for each pair of nodes, the existence of an edge constitutes only a single bit of information. ", "page_idx": 1}, {"type": "text", "text": "2.1 Edge reconstruction attacks on graph-structured data ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Contemporary developments on edge reconstruction attacks differ significantly in their conceptualization of adversaries, particularly in terms of their capabilities [45, 44] and the extent of prior knowledge they possess about the GRL model and the underlying graph dataset [17]. The mechanism of SERA was first proposed in [11] and later studies in [17]. Empirical evidences suggest that with only black-box access to node representations, the SERA mechanism obtains a high success rate $(\\mathrm{AUC}>0.9\\$ for the Citeseer dataset). Subsequent developments have explored stronger attacks under more powrful adversaries. In [17] the authors investigated the impact of an adversary\u2019s prior knowledge, including the possession of node features, partial graph structure, and access to a shadow dataset, on the success rate of corresponding attack strategies. Inspire by information bottleneck,[46] improves SERA via carefully exploiting intermediate representations produced by GNNs. Notably, despite the adversaries in [17, 46] being equipped with substantially more information compared to SERA, the resulting enhancement in attack performance exhibited by these adversaries demonstrates only marginal improvements relative to SERA. The GraphMI attack [45] disables the adversary from being able to acquire node representations but instead requires access to node features and labels, as well as white-box access to the GNN model. Recent works explored influence-based attacking schemes, wherein the adversary is allowed to alter the graph information: The LinkTeller attack [34] manipulates node features while [23] infiltrates the underlying graph with malicious nodes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Theoretical explorations in graph recovery from neural representations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In [6], the authors proposed an algorithm that provably recovers graph structure based on representations generated via DeepWalk, which is a factorizaton-based procedure and different from GNN-produced representations. In [43] the authors showed that when block structure exists in the underlying graph, the performance of SERA is uneven across node in different blocks. In [46], the authors use information-theoretic arguments to construct more powerful attacks than SERA. Nevertheless, the aforementioned studies did not provide a theoretical rationale for the practical vulnerabilities manifested as a result of the SERA. In a contemporary work [8], the authors derived generalization bounds of linear GNN under the link prediction context assuming the underlying graph generated by a moderately sparse graphon model. ", "page_idx": 2}, {"type": "text", "text": "2.3 Privacy protection against edge reconstruction attacks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Edge differential privacy (EDP) [26] is the most popular privacy notion that offers a formal protection against edge reconstruction attacks. Standard private training algorithms like DPSGD [1] may produce GNN models that is provably private in the sense that membership information of any individual training sample is limitly disclosed. 2 However, such approaches do not provide privacy during inference time [7]. Protection mechanisms against inference-time adversaries are mostly based on noisy version of GNN encoding such as edge-wise randomized response [34] that provides very strong privacy protection yet being overly destructive to model utility. Noisy aggregation (NAG) mechansims [30, 36, 7] are recently proposed that empirically achieves better privacy-utility trade-offs. Inspired by the information bottleneck principle, [33, 46] proposed to use regularization or saddle-point optimization techniques to control privacy leakage. Yet these proposals are not principled in theory. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Setup and notations Consider an undirected graph $G=(V,E)$ with $n=|V|$ nodes associated with node features $X\\in\\mathbb{R}^{n\\times d}$ . Denote $A$ as the corresponding adjacency matrix and $D$ as the diagonal matrix with the $v$ -th diagonal entry being the degree of node $v$ . In this paper, we will study victim models taking forms of graph neural encoders. Our vulnerability analysis predominantly centers on the linear graph neural network [35] architecture which has been widely adopted in previous theoretical studies on graph neural networks [3, 39, 37, 8]. Specifically, the node representation matrix of an $L$ -layer linear GNN is computed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nH^{(L)}=\\left(\\left(D+I\\right)^{-1}\\left(A+I\\right)\\right)^{L}X W,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the identity matrix is added for ensuring self-loops, and $W\\,\\in\\,\\mathbb{R}^{d\\times d}$ is the weight matrix. Throughout this paper, we will assume the node feature dimension and the hidden dimension to be equal to $d$ and refer to this as the feature dimension, as otherwise we may add an extra input projection to fulfill this requisite. We further denote $\\|W\\|_{\\mathrm{op}}$ and $\\kappa(W)$ as the operator norm (i.e., largest singular value) and condition number (i.e., the ratio of largest and smallest singular value) of matrix $W$ . ", "page_idx": 2}, {"type": "text", "text": "Threat model We assume the adversary knows the node set $V$ and is able to inquire node representations of an arbitrary node subset $V_{\\mathrm{victim}}\\,\\subset\\,V$ . Hereafter we will refer to the subgraph induced via $V_{\\mathrm{victim}}$ as the victim subgraph $G_{\\mathrm{victim}}~=~(V_{\\mathrm{victim}},E_{\\mathrm{victim}})$ . The goal of the adversary is to recover an arbitrary fraction of $E_{\\mathrm{victim}}$ based on the acquired node representations ", "page_idx": 2}, {"type": "text", "text": "$H_{\\mathrm{victim}}^{(L)}\\,=\\,\\{h_{v}^{(L)},v\\,\\in\\,V_{\\mathrm{victim}}\\},L\\,>\\,0$ . We identify two representative scenarios that underscore the potential threat by such adversaries: The first scenario is API-style deployments of graph representations [34], wherein an adversary might query the node representations for a set of nodes using their node identifiers, with this particular subset of nodes constituting the victim nodes. The second scenario pertains to a two-party vertical federated learning (VFL) context [36], wherein the graph topology retained by party A is deemed confidential. Under such a setup, the privacy threat materializes as party B might adhere to the VFL protocol while simultaneously being curious about the topology. Note that the capabilities of the adversaries posited herein are intentionally constrained by denying them access to both the raw node features $X$ and the model parameters. Additionally, the objectives of the adversary are decidedly ambitious, aiming at the potential recovery of the entire suite of edges within the victim subgraph. A more in-depth discussion regarding the threat model and the potent capabilities of the adversary is deferred to appendix B.1. ", "page_idx": 3}, {"type": "text", "text": "The SERA is based on a similarity measure sim, with the adjacence relation between node $u$ and node $v$ inferred as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{A}_{u v}^{\\mathtt{S E R A}}(\\tau)=\\mathbf{1}\\left(\\mathtt{s i m}\\left(h_{u}^{(L)},h_{v}^{(L)}\\right)\\geq\\tau\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we denote $\\mathbf{1}(\\cdot)$ as the indicator function. In this paper we will be primarily interested in two similairty measures: The cosine similarity $\\mathsf{c o s}(x,y)\\,=\\,\\left<x,y\\right>/(\\|x\\|_{2}^{\\!}\\|y\\|_{2})$ and correlation similarity $\\mathsf{c o i r r}(x,y)=\\left<x-\\bar{x},y-\\bar{y}\\right>/(\\|x-\\bar{x}\\|_{2}\\,\\|y-\\bar{y}\\|_{2})$ , which is essentially a centered version of cosine similarity $({\\bar{x}},{\\bar{y}}$ are coordinate-wise averages of $x$ and $y$ ) defined for node representations with dimension greater than 1. The cutoff threshold $\\tau$ is allowed to depend on the embedding set $H_{\\mathrm{victim}}$ but is uniform across all edge decisions. Hereafter without misunderstandings, we will drop the superscript and denote $\\widehat{A}(\\tau)$ as the reconstructed adjacency matrix under threshold $\\tau$ . To measure the performance of the atta ck, we use false positive rate (FPR) and false negative rate (FNR) defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{F P R}_{\\widehat{A}}\\left(\\tau\\right)=\\frac{\\sum_{u,v}\\mathbf{1}\\left(\\widehat{A}_{u v}(\\tau)=1\\right)\\mathbf{1}\\left(A_{u v}=0\\right)}{\\sum_{u,v}\\mathbf{1}\\left(A_{u v}=0\\right)},\\mathsf{F N R}_{\\widehat{A}}\\left(\\tau\\right)=\\frac{\\sum_{u,v}\\mathbf{1}\\left(\\widehat{A}_{u v}(\\tau)=0\\right)\\mathbf{1}\\left(A_{u v}=1\\right)}{\\sum_{u,v}\\mathbf{1}\\left(A_{u v}=1\\right)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We further define the error rate ERR as the summation of FPR and FNR. Employing these metrics facilitates a more nuanced characterization of attack performance, particularly when the underlying graph is sparse. An alternate metric that is often used in practice [17] is the area under the receiver operating characteristic curve (AUROC) metric ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{A U R O C}_{\\widehat{A}}=\\int_{0}^{1}\\left(1-\\mathsf{F N R}_{\\widehat{A}}\\left(\\mathsf{F P R}_{\\widehat{A}}^{-1}\\left(s\\right)\\right)\\right)d s\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which quantifies the aggregate performance ofA by integrating the trade-off between the false positive rate and the false negative rate across different thresholds. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, the success of SERA is determined by the correlation between node representation similarity and edge presence. Previous empirical observations demonstrate the effectiveness of SERA against graphs that exhibit strong correlations between node feature similarity and edge presence [17]. We will refer to such kinds of graphs as being homophilous in a generalized sense [18, 22]. We defer a more formal introduction to homophily measrues to appendix B.2. Due to the message-passing nature of GNN encoders, it is intuitively reasonable that recursive aggregation of node representations strengthens the correlation and results in successful edge reconstructions. However, it is non-trivial whether SERA mechanism may succeed in the absence of the aforementioned generalized homophily pattern, which motivates our first analysis. ", "page_idx": 3}, {"type": "text", "text": "4 SERA against sparse random graphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we study the behavior of SERA with the underlying (victim) graph generated according to a sparse random graph. Here, the adjacency matrix is generated such that each entry is independently distributed (up to symmetric constraints $A_{u v}=A_{v u}\\,$ ) following a Bernoulli distribution $A_{u v}\\sim\\mathrm{Ber}(p_{u v})$ . We focus on the sparse regime and allow $p_{u v}$ to depend on $X_{u}$ and $X_{v}$ . We further assume that the node features $X_{v}$ \u2019s are generated i.i.d. according to an isotropic Gaussian distribution $X_{v}\\sim N(0,I_{d})$ . It follows that the correlation of node feature similarity and edge presence is zero. The following theorem characterizes the effectiveness of SERA under the sparse random graph setup. ", "page_idx": 3}, {"type": "text", "text": "(iii) The condition number of the GNN encoder weight satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\kappa(W))^{2}\\leq\\frac{1}{8(C_{2}\\log n)^{3L}}\\sqrt{\\frac{d}{\\log n}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then there exists a threshold $\\begin{array}{r}{\\tau=\\Theta\\left(\\frac{1}{(C_{2}\\log n)^{2L}}\\right)}\\end{array}$ such that with probability at least $\\textstyle1-{\\frac{2}{n^{2}}}$ , the following holds for SERA with the similarity measure chosen either as cos or corr: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF N R_{\\widehat{A}}\\left(\\tau\\right)=0,\\;F P R_{\\widehat{A}}\\left(\\tau\\right)\\leq\\frac{(C_{2}\\log n)^{2L}}{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequently, on the above set of events we have AUROC A \u22651 \u2212(C2 long n)2L. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 implies that, even when SERA can not borrow strength from the homophily nature of the underlying graph, it is able to produce accurate reconstructions when the graph is sufficiently large and sparse, with the sparsity defined in the sense that each node has at most ${\\bar{O(}}\\log n)$ neighbors on average. An additional intriguing implication from theorem 4.1 pertains to the dependence of reconstruction performance on the GNN encoder depth $L$ : Provided that the node feature dimension is sufficiently large, the reconstruction performance degrades when the depth of the encoder increases, which is related to the renowned phenomenon of oversmoothing in GNN literature [37]. Intuitively, as the depth of GNN encoders increases, the resulting node representations tend to converge [27], becoming less distinct from one another. This convergence diminishes the discriminative capacity of similarity metrics, thereby affecting the attack performance. ", "page_idx": 4}, {"type": "text", "text": "Remark 4.2 (Practicality). Theorem 4.1 requires the node feature dimension $d$ to grow in a polylog $(n)$ rate, a condition which may not consistently align with practical scenarios. At present, this requirement is a byproduct of our proof strategy. In section 7.1 we will further examine the implications of feature dimensionality. The existence of a threshold that theorem 4.1 manifests might not guide the choice of threshold in practice. Instead, we may rely on heuristics or side-information [17] to determine the threshold. Furthermore, Theorem 4.1 posits that the efficacy of SERA is contingent upon a reasonable conditioned weight matrix $W$ . We will empirical validate this claim in section 7, wherein we demonstrate robust reconstruction capabilities of the SERA across diverse scenarios including when the weight matrix $W$ is a fixed entity, when it is subject to random initialization, or when it has undergone extensive training iterations utilizing datasets from real-world supervised learning contexts. ", "page_idx": 4}, {"type": "text", "text": "5 SERA against dense SBMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we reveal the limitation of SERA by constructing a reconstruction problem that is provably hard. We consider the following stochastic block model (SBM) [2], where each node is assigned a community membership from one of $K$ groups $k(v)\\in[K]$ . The $(u,v)$ -th entry of the adjacency matrix is generated as ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{u v}\\sim\\left\\{\\mathbf{Ber}(p),\\quad{\\mathrm{if~}}k(u)=k(v)\\quad\\right..\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For ease of presentation, we further assume that the groups share the same size, i.e., $n$ is a multiple of $K$ . Denote the generation mechanism as $G\\sim\\mathcal{G}_{\\mathrm{sbm}}(n,K,p,q)$ . We have the following result: ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.1. Let $G\\sim\\mathcal{G}_{s b m}(n,K,p,q)$ and $p=\\Theta(1)$ . Assume the GNN encoder to be of depth $L$ and feature dimension $d\\gg\\operatorname*{max}\\{\\log n/p^{2},K^{2}\\log^{3}n\\}$ with the weight matrix being the identity matrix. Then with probability at least $1-1/n^{2}$ , for any fixed $\\tau\\in[0,1],$ , one of the following three statements must hold for SERA with similarity measure chosen either as cos or corr: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F P R_{\\widehat{A}}\\left(\\tau\\right)\\geq\\frac{1-p}{2K}+\\frac{1-q}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{:)}\\ F\\!N\\!R_{\\widehat{A}}\\left(\\tau\\right)\\geq\\frac{p}{2K}+\\frac{q}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "According to theorem 5.1, given any cutoff threshold if the within-group connection probability is of the order $\\Theta(1)$ and the number of groups $K$ does not diverge (Otherwise, we will return to the sparse regime in section 4) , the performance of SERA measured by error rate ERR is lower bounded by non-vanishing constants when the feature dimension is sufficiently large. The theorem characterizes the inherent limitations of SERA when the underlying graph is dense. As $K$ gets large, the lower bound of false positive/negative rate decreases. It indicates that SERA is more successful when the graph is less connected. ", "page_idx": 5}, {"type": "text", "text": "Remark 5.2. Alternatively, we may interpret theorem 5.1 as unveiling instances where SERA is constrained to revealing only population-level relational information\u2014such as the affiliation of two nodes to a common group\u2014rather than identifying the existence of specific edges when the underlying graph is dense and admits certain group structures. ", "page_idx": 5}, {"type": "text", "text": "6 Defense by noisy aggregation: From theory to practice ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having demonstrated the susceptibility of GNN representations to SERA, it becomes an intriguing research question to examine the behavior of SERA within the context of privacy-preserving GRL: In this section, we explore the defensive efficacy of noisy aggregation (NAG), which has been proposed recently as a provably privacy-preserving algorithm [30, 36] under the edge differential privacy model [26]. Concretely, we study an $L$ -layer noisy GNN with the $l$ -th layer computed recursively as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{v}^{(l)}=\\mathsf{A c t}\\left(\\mathsf{A G G}\\left(W_{l}H_{u}^{(l-1)}/\\left\\|H_{u}^{(l-1)}\\right\\|_{2},u\\in\\overline{{N(v)}}\\right)+\\epsilon\\right),\\epsilon\\sim N(0,\\sigma^{2}I_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\overline{{N(v)}}}:=N(v)\\cup\\{v\\}$ denotes node $v$ \u2019s extended neighborhood and $H_{v}^{(l)}$ denotes the representation of node $v$ at the $l$ -th layer. The aggregation mechanism AGG is a permutation invariant function that defines the message-passing process and Act is some (possibly) non-linear transform. Intuitively, the NAG methodology can be understood as a privatization protocol that incorporates both a normalization step and an additive Gaussian perturbation phase into the conventional messagepassing framework, which typically forms the backbone of a GNN. In this paper, we consider 5 representative GNN architectures that allows NAG privatization: GCN [20], GAT [32], SAGE [14] with mean or max pooling, and GIN [38] with their formal definition deferred to appendix B.3. The following theorem characterizes the defensive capability of NAG: ", "page_idx": 5}, {"type": "text", "text": "Theorem 6.1. For any graph $G$ and SERA under any type of similarity measures, the inference error regarding any specific edge is lower bounded by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{u\\in V,v\\in V}\\left[\\mathbb{P}\\left(\\widehat{A}_{u v}=1|A_{u v}=0\\right)+\\mathbb{P}\\left(\\widehat{A}_{u v}=0|A_{u v}=1\\right)\\right]\\geq1-\\sqrt{1-\\exp\\left(-C\\frac{\\sum_{l\\in[L]}\\|W_{l}\\|_{\\sigma p}^{2}}{\\sigma^{2}}\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here the constant $C$ depends on the AGG mechanism of the GNN. In particular, for some standard GNN architectures we have: $C_{G C N}=C_{M E A N-S A G E}=C_{G I N}=1$ and $C_{G A T}=C_{M A X-S A G E}=4$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 6.1 augments existing literature in the sense that it extrapolates upon prior analyses [30, 36] by generalizing to a broader range of aggregation mechanisms, thereby encompassing the vast majority of foundational components integral to modern GNN models. Theorem 6.1 indicates that for any node pairs in any graph, the summation of type-I error and type-II error (in the language of binary hypothesis testing [21]) incurred by any SERA adversary is lower bounded by a constant, which will be significantly above zero when the noise scale is of the same order to the operator norms of the weight matrices of the GNN encoder. In fact, theorem 6.1 holds against a much stronger family of adversaries, which we discuss in appendix C.3. ", "page_idx": 5}, {"type": "text", "text": "Empirical proctection of NAG implementing NAG with a large noise scale according to theorem 6.1 may seriously degrade model efficacy. Contemporary insights [5] suggest that strict adherence to theoretical prescripts may not always be necessary, especially in the face of empirical adversaries whose capabilities may not rise to the level presumed by the defense mechanisms postulated. In this paper we conduct a careful empirical investigation to assess the privacy-utility trade-off of NAG, with privacy evaluated by the SERA adversary. This investigation could also provide empirical evidence of the SERA\u2019s viability as a tool for auditing private GRL algorithms [9]. Furthermore, theorem 6.1 identifies a key determinant of the theoretical privacy bound for NAG \u2014the relative scale of the weight norms regarding the noise intensity. In light of this observation, we propose two distinct noise-infused training paradigms: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Unconstrained scheme We choose a fixed noise scale $\\sigma$ during both training and inference no constraints over the weights. The resulting model might not produce meaningful privacy guarantees in the sense of theorem 6.1 as the operator norms of weights are determined by the training dynamics. ", "page_idx": 6}, {"type": "text", "text": "Constrained scheme We choose a fixed noise scale $\\sigma$ during both training and inference and use normalization techniques [25] to provide a priori control of model weights, thereby providing tighter control of formal privacy level according to theorem 6.1. ", "page_idx": 6}, {"type": "text", "text": "We will empirically inspect the protection of NAG representations trained via both unconstrained and constrained schemes against SERA in section 7.3. ", "page_idx": 6}, {"type": "text", "text": "Remark 6.2 (Alternative defenses). Beyond the scope of NAG, alternative defense mechanisms offer demonstrable protection assurances, one notable example being edge-wise randomized response (EdgeRR). A comparison with such alternatives is reported in appendix D.5. Preliminary experimental comparisons indicate that NAG customarily realizes a more favorable balance between privacy and utility. ", "page_idx": 6}, {"type": "text", "text": "Remark 6.3 (Impact of depth $L$ ). Theorem 6.1 posits that the privacy guarantees furnished by NAG diminishes with an increment in model depth, which is underpinned by the composition theorems of privacy analysis [12, 24]. An extensive discussion concerning the implications of GNN architectural design on the privacy-utility trade-off, particularly as it pertains to the depth of GNN models trained with NAG, will be provided in appendix E. ", "page_idx": 6}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, comprehensive empirical studies are conducted to evaluate the effectiveness of SERA against both non-private and private node representations. By default, cos is employed as the standard measure of similarity across all experiments. The results corresponding to the use of corr as a metric were found to align with those obtained from cos, a concurrence that aligns with observations from [17]. This investigation is oriented around three core research questions: ", "page_idx": 6}, {"type": "text", "text": "RQ1 (Efficacy of SERA on Sparse Graphs): We evaluate SERA on synthetic datasets generated according to theorem 4.1, in addition to 8 real-world datasets to substantiate the effectiveness of SERA. ", "page_idx": 6}, {"type": "text", "text": "RQ2 (Deficiency of SERA on Dense Graphs): We evaluate SERA on synthetic stochastic block models to corroborate the theoretical assertions in theorem 5.1. ", "page_idx": 6}, {"type": "text", "text": "RQ3 (Mitigation of SERA through NAG): We evaluate SERA on privacy-enhanced node representations across three benchmark datasets generated using NAG with varied levels of noise. The outcomes affirm NAG\u2019s capacity for privacy preservation while concurrently delineating the limitations of SERA as a tool for privacy auditing. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics: Predominantly, this section documents the performance of attacks using the AUROC metric. A more expansive presentation of the results, inclusive of both AUROC and ERR, is postponed to appendix D. ", "page_idx": 6}, {"type": "text", "text": "7.1 Efficacy of SERA on sparse graphs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Erd\u02ddos\u2013R\u00e9nyi experiments In our first experiment, we test SERA on graph representations produced by (1) over Erd\u02ddos\u2013R\u00e9nyi graphs with edge probability $\\smash{p_{\\!\\!\\:=\\!\\!\\:}}\\frac{\\log n}{n}$ with graph size $\\bar{n}\\,\\in\\,\\{100,500,1000\\}$ , which is a representative random graph model with controllable sparsity level. We set the weight to be the identity matrix and further present results under random weights in appendix D.1. We vary the feature dimension $d\\in\\{2^{j},2\\leq\\dot{j}\\leq11\\}$ and network depth $1\\leq L\\leq10$ in order to obtain a fine-grained assessment of SERA. We present the evaluations in figure 4. The results corroborate with our theoretical developments: We demonstrate that SERA is able to achieve near-perfect reconstruction of all edges only in the \"large $d$ , small $L\"$ regime. Notably, we find ", "page_idx": 6}, {"type": "table", "img_path": "LSqDcfX3xU/tmp/2070abe65891145cb48dc8af71c67493b8bab8e7b2ad928f8ee1fe873a679005.jpg", "table_caption": ["Table 1: Performances of SERA on eight datasets measured by AUROC metric $(\\%)$ . The feature homophily $\\begin{array}{r}{\\mathcal{H}_{\\mathrm{feature}}(G,X)\\;=\\;\\frac{1}{|E|}\\sum_{(u,v)\\in E}^{\\ \\ v}\\cos\\left(X_{u},X_{v}\\right)}\\end{array}$ is an alternate measure of correlation between feature similarity and edge presence. For each setup, the results (in the form of mean $\\pm\\mathrm{std}.$ ) are obtained via 5 random trials. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "SERA to be less successful under relatively deep network architectures (i.e., $L\\geq5$ ) when the feature dimension is sufficiently large. Yet the behaviors in small $d$ regimes appear to be less predictable. 3 Furthermore, the influence of the feature dimension appears to be more pronounced than that of the network depth. This suggests that a greater number of features, despite their independence from graph topology, lead to potentially more privacy risks as transmitted through GNN representations. Conversely, augmenting the network depth does not necessarily correlate with an elevation in the success rate of SERA. ", "page_idx": 7}, {"type": "text", "text": "Real-world data experiments Given that the Erd\u02ddos\u2013R\u00e9nyi model may not sufficiently capture the complexity of real-world graph structures, we evaluated the SERA algorithm on 8 diverse real-world graph datasets that exhibit contrasting patterns of feature similarity and edge formation. The analysis comprises the well-known Planetoid datasets [41], which are distinguished by their high homophily; the heterophilic datasets Squirrel, Chameleon, and Actor [29], which demonstrate a weak feature-edge correlation; and two larger-scale datasets, namely Amazon-Products [42] and Reddit [14]. Dataset statistics are comprehensively detailed in appendix B.2. Half of the datasets analyzed manifest a strong positive correlation of feature similarity and edge presence, which is measured via the AUROC of the estimator $\\widehat{A}_{u v}^{\\mathrm{FS}}(\\tau)={\\bf1}\\left(\\sf{c o s}(X_{u},X_{v})\\geq\\tau\\right)$ , while the other half show negligible correlations, an observation underscored in the baseline $(\\widehat{A}^{\\mathrm{FS}})$ row of table 1. In all evaluations, we standardize the hidden dimension to $d\\,=\\,128$ , with the number of GNN layers adjusted to $L\\,\\in\\,2,5$ . Our analysis extends beyond the linear aggregation scheme (1) to encompass four additional prominent GNN architectures: GCN [20], GAT [32], GIN [38], and SAGE [14]. To discern the effect of training dynamics on the potency of attacks, we delineate results for both pre-training (i.e., random initialization) and post-training stages. A precise account of training methodologies can be found in appendix D.2. Results pertaining to the linear GNN (LIN) and GCN are presented in table 1, with a comprehensive evaluation reserved for appendix D.2. We have the following observations: ", "page_idx": 7}, {"type": "text", "text": "Homophily is not necessary for SERA to succeed: The efficacy of SERA on the Planetoid datasets aligns with expectations. However, the outcomes from 4 heterophilic datasets illuminate significant privacy risks, despite a vacuous association between feature resemblance and edge formation. Notably, the Squirrel and Actor datasets, which demonstrate a mild negative feature-edge correlation, are still subject to substantial privacy breach, particularly with nonlinear models. These empirical findings support our theoretical assertion that a graph\u2019s sparsity plays a more pivotal role in its susceptibility to edge reconstruction attacks than the degree of homophily it exhibits. Moreover, in instances of comparatively denser networks, such as the Reddit dataset, the homophily of features can be exploited to mount more sophisticated attacks. ", "page_idx": 7}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/fe253d4409a0d59005624e58be6debc5b450fbfc143125100d60b880b5ca6290.jpg", "img_caption": ["Figure 1: Attacking efficacy of SERA over sparse Erdo\u02dds\u2013R\u00e9nyi graphs and dense SBM graphs, with performance measured in AUROC metric averaged over 5 random trials for each configuration. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Efficacy of Linear GNNs as Proxies for Nonlinear Counterparts: Evidence presented in table 1 suggests that the trends exhibited by linear GNN models are broadly reflective of those displayed by their nonlinear, GCN equivalents. It is typically observed that the attack efficacy is modestly reduced in the linear GNN setting, with further details deferred to Appendix D.2. ", "page_idx": 8}, {"type": "text", "text": "Influence of Network Depth and Training Dynamics: Table 1 indicates that the post-training performance of SERA is frequently less effective compared to the scenarios with randomly initialized weights. This observation may be attributed to the notion that supervised training tends to adversely affect the conditioning of weight matrices relative to their initialized state. Additionally, augmenting model depth does not correspond with enhanced attack efficacy, an outcome that is in alignment with our theoretical predictions. ", "page_idx": 8}, {"type": "text", "text": "7.2 Deficiency of SERA on dense graphs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SBM experiments In this section, we test SERA graph representations over SBM graphs with $K\\;=\\;3,p\\;=\\;0.3,q\\;=\\;0.05$ , with the rest of the experimental setups analogous to that in the Erdo\u02dds\u2013R\u00e9nyi experiments. The evaluations are presented in figure 7. The results reveal the presence of a pronounced barrier that hinders the success of the attack across a wide range of configurations corresponding to different network depths and feature dimensions. Furthermore, we observe that the results tend to stabilize as the size of the graph increases. We provide a further study on the impact of SBM structure in appendix D.1. ", "page_idx": 8}, {"type": "text", "text": "7.3 Mitigation of SERA through NAG ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we empirically study the defensive performance of noisy aggregation (8) against SERA We will use the Planetoid datasets [41] for evaluation. We consider a transductive node classification setting and use the standard train-test splits. The GNN models are trained using the training labels and evaluated on the test nodes. The performances of SERA are evaluated on the subgraphs induced by the test nodes. We report the configuration of GNN encoding, as well as the attacking pipeline and training hyperparameters in appendix D.3.1. Due to space limits, we report results on the Cora dataset under GCN and GAT in the main text and postpone the complete report in appendix D.3. We use the following two types of training configurations as proposed in section 6: ", "page_idx": 8}, {"type": "text", "text": "Under the unconstrained scheme, we use aggressive perturbation plans by applying noise with scale range $\\sigma\\in\\{0,1,2,4\\}$ , with $\\sigma=0$ indicating no protection, and $\\bar{d}\\in\\{2^{i},5\\overset{<}{\\leq}i\\leq\\bar{1}3\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Under the constrained scheme, we adopt the spectral normalization technique [25] to control the spectral norm of each layer at approximately 1 (with relative error $<10\\%$ ). We use conservative perturbation plans by applying noise with scale range $\\sigma\\,\\in\\,\\{0,0.01,0.05,0.1,0.5,1\\}$ , and $d\\ \\in$ $\\left\\{2^{i},5\\leq i\\leq13\\right\\}$ . Note that with $\\sigma=1$ , we obtain a non-vacuous lower bound according to (9). We present the evaluations in figure 2 and summarize our observations and findings as follows: ", "page_idx": 8}, {"type": "text", "text": "SERA empirically elicits privacy-utility trade-off under the constrained scheme When the noise level is moderate, i.e., $\\sigma\\,\\in\\,\\{0.01,0.05\\}$ . The result demonstrates that privacy and utility are, at least to some extent, at odds: Under lower noise level, SERA is able to achieve non-trivial success especially when $d$ is small. Furthermore, raising the feature dimension $d$ results in both a decrease ", "page_idx": 8}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/36d4952a6476ef34be4a3c8548962d073893e427de88887bacfc9e3f37d76a5e.jpg", "img_caption": ["(e) model performance (f) model performance (g) model performance (h) model performance "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2: Privacy and utility assessments on the Cora dataset with underlying model of NAG being GCN and GAT. The first row contains attack performances of SERA measured using AUROC metric under both constrained and unconstrained training scheme. The second row presents corresponding model performances. ", "page_idx": 9}, {"type": "text", "text": "in utility as well as an increase in privacy. This is actually predictable: Since we explicitly control the operator norm to be around 1, a larger $d$ implies a smaller \"signal-to-noise ratio\" with the signal being (loosely) defined as the magnitude of the aggregated node representations. ", "page_idx": 9}, {"type": "text", "text": "SERA losses power against NAG using larger ds in the unconstrained scheme A surprising evidence according to figure 2 is that when the feature dimension $d$ is sufficiently raised, i.e., $d~>~1024$ , the attacking performances degrade. Consequently, we are able to achieve decent protection against SERA $(\\mathrm{AUROC}<0.6)$ ) while at the same time incurring slight degradation in model utility $(>0.7$ Accuracy in Cora) Moreover, the phenomenon is more evident for higher noise levels. While the outcome seems favorable insofar as we have identified GNN solutions that manifest both high performance and a degree of privacy since the training procedure is unrelated to the attacking mechanism, these solutions may exhibit diminished robustness, as the corresponding Lipschitz constants are likely to be inadequately regulated [40]. Due to space limits, we postpone a more detailed discussion to appendix D.4. ", "page_idx": 9}, {"type": "text", "text": "8 Discussion and conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have studied the behavior of the SERA adversary by characterizing its performance against different kinds of underlying graph structures as well as encoding mechanisms. Theoretically, we first identify sparse random graphs where SERA provably reconstructs the input graph, which ascertains the empirical findings of previous works. We then reveal limitations of SERA by showing its performance lower bounds when the input graph follows a dense SBM. Additionally, we discuss protection mechanisms to SERA by exploiting both theoretically and empirically the defensive capability of NAG. Empirical investigations corroborate with our theoretical findings, while suggesting intriguing phenomenons that questions the viability of SERA as a formal privacy auditing procedure for private graph representations. Notwithstanding, several research problems warrant further study, which we discuss in appendix E alongside with the limitations of this paper. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors from Ant Group are supported by the Leading Innovative and Entrepreneur Team Introduction Program of Hangzhou (Grant No.TD2022005). Guanhua Fang is partly supported by the National Natural Science Foundation of China (nos. 12301376). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \n[2] E. Abbe. Community detection and stochastic block models: recent developments. Journal of Machine Learning Research, 18(177):1\u201386, 2018.   \n[3] P. Awasthi, A. Das, and S. Gollapudi. A convergence analysis of gradient descent on graph neural networks. Advances in Neural Information Processing Systems, 34:20385\u201320397, 2021.   \n[4] C. L. Canonne. A short note on an inequality between kl and tv. arXiv preprint arXiv:2202.07198, 2022.   \n[5] N. Carlini, C. Liu, \u00da. Erlingsson, J. Kos, and D. Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19), pages 267\u2013284, 2019.   \n[6] S. Chanpuriya, C. Musco, K. Sotiropoulos, and C. Tsourakakis. Deepwalking backwards: from embeddings back to graphs. In International Conference on Machine Learning, pages 1473\u20131483. PMLR, 2021.   \n[7] E. Chien, W.-N. Chen, C. Pan, P. Li, A. Ozgur, and O. Milenkovic. Differentially private decoupled graph convolutions for multigranular topology protection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[8] A. Chung, A. Saberi, and M. Austern. Statistical guarantees for link prediction using graph neural networks. arXiv preprint arXiv:2402.02692, 2024.   \n[9] R. Cummings, D. Desfontaines, D. Evans, R. Geambasu, M. Jagielski, Y. Huang, P. Kairouz, G. Kamath, S. Oh, O. Ohrimenko, et al. Challenges towards the next frontier in privacy. arXiv preprint arXiv:2304.06929, 2023.   \n[10] E. Dai, T. Zhao, H. Zhu, J. Xu, Z. Guo, H. Liu, J. Tang, and S. Wang. A comprehensive survey on trustworthy graph neural networks: Privacy, robustness, fairness, and explainability. arXiv preprint arXiv:2204.08570, 2022.   \n[11] V. Duddu, A. Boutet, and V. Shejwalkar. Quantifying privacy leakage in graph embedding. In MobiQuitous 2020-17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, pages 76\u201385, 2020.   \n[12] C. Dwork, A. Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407, 2014.   \n[13] M. Fey and J. E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.   \n[14] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[15] W. L. Hamilton, R. Ying, and J. Leskovec. Representation learning on graphs: Methods and applications. arXiv preprint arXiv:1709.05584, 2017.   \n[16] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026\u20131034, 2015.   \n[17] X. He, J. Jia, M. Backes, N. Z. Gong, and Y. Zhang. Stealing links from graph neural networks. In 30th USENIX Security Symposium (USENIX Security 21), pages 2669\u20132686, 2021.   \n[18] D. Jin, R. Wang, M. Ge, D. He, X. Li, W. Lin, and W. Zhang. Raw-gnn: Random walk aggregation based graph neural network. arXiv preprint arXiv:2206.13953, 2022.   \n[19] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793\u2013826, 2011.   \n[20] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[21] E. L. Lehmann, J. P. Romano, and G. Casella. Testing statistical hypotheses, volume 3. Springer, 1986.   \n[22] S. Luan, C. Hua, M. Xu, Q. Lu, J. Zhu, X.-W. Chang, J. Fu, J. Leskovec, and D. Precup. When do graph neural networks help with node classification? investigating the homophily principle on node distinguishability. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[23] L. Meng, Y. Bai, Y. Chen, Y. Hu, W. Xu, and H. Weng. Devil in disguise: Breaching graph neural networks privacy through inflitration. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pages 1153\u20131167, 2023.   \n[24] I. Mironov. R\u00e9nyi differential privacy. In 2017 IEEE 30th computer security foundations symposium (CSF), pages 263\u2013275. IEEE, 2017.   \n[25] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.   \n[26] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 75\u201384, 2007.   \n[27] K. Oono and T. Suzuki. Graph neural networks exponentially lose expressive power for node classification. arXiv preprint arXiv:1905.10947, 2019.   \n[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K\u00f6pf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024\u20138035, 2019.   \n[29] H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang. Geom-gcn: Geometric graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.   \n[30] S. Sajadmanesh, A. S. Shamsabadi, A. Bellet, and D. Gatica-Perez. Gap: Differentially private graph neural networks with aggregation perturbation. In USENIX Security 2023-32nd USENIX Security Symposium, 2023.   \n[31] F. Tramer, A. Terzis, T. Steinke, S. Song, M. Jagielski, and N. Carlini. Debugging differential privacy: A case study for privacy auditing, 2022.   \n[32] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.   \n[33] B. Wang, J. Guo, A. Li, Y. Chen, and H. Li. Privacy-preserving representation learning on graphs: A mutual information perspective. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1667\u20131676, 2021.   \n[34] F. Wu, Y. Long, C. Zhang, and B. Li. Linkteller: Recovering private edges from graph neural networks via influence analysis. In 2022 IEEE Symposium on Security and Privacy $(S P)$ , pages 2005\u20132024. IEEE, 2022.   \n[35] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861\u20136871. PMLR, 2019.   \n[36] R. Wu, M. Zhang, L. Lyu, X. Xu, X. Hao, X. Fu, T. Liu, T. Zhang, and W. Wang. Privacypreserving design of graph neural networks with applications to vertical federated learning. arXiv preprint arXiv:2310.20552, 2023.   \n[37] X. Wu, Z. Chen, W. Wang, and A. Jadbabaie. A non-asymptotic analysis of oversmoothing in graph neural networks. arXiv preprint arXiv:2212.10701, 2022.   \n[38] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.   \n[39] K. Xu, M. Zhang, S. Jegelka, and K. Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, pages 11592\u201311602. PMLR, 2021.   \n[40] Y.-Y. Yang, C. Rashtchian, H. Zhang, R. R. Salakhutdinov, and K. Chaudhuri. A closer look at accuracy vs. robustness. Advances in neural information processing systems, 33:8588\u20138601, 2020.   \n[41] Z. Yang, W. Cohen, and R. Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016.   \n[42] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019.   \n[43] H. Zhang, B. Wu, S. Wang, X. Yang, M. Xue, S. Pan, and X. Yuan. Demystifying uneven vulnerability of link stealing attacks against graph neural networks. In International Conference on Machine Learning, pages 41737\u201341752. PMLR, 2023.   \n[44] Z. Zhang, Q. Liu, Z. Huang, H. Wang, C.-K. Lee, and E. Chen. Model inversion attacks against graph neural networks. IEEE Transactions on Knowledge and Data Engineering, 2022.   \n[45] Z. Zhang, Q. Liu, Z. Huang, H. Wang, C. Lu, C. Liu, and E. Chen. Graphmi: Extracting private graph data from graph neural networks. arXiv preprint arXiv:2106.02820, 2021.   \n[46] Z. Zhou, C. Zhou, X. Li, J. Yao, Q. Yao, and B. Han. On strengthening and defending graph reconstruction attack with markov chain approximation. arXiv preprint arXiv:2306.09104, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Broader impacts 14 ", "page_idx": 13}, {"type": "text", "text": "B Additional backgrounds 15 ", "page_idx": 13}, {"type": "text", "text": "B.1 VFL and the SERA adversary . 15   \nB.2 Measures of homophily 15   \nB.3 Representative GNNs and their corresponding aggregations rules . . 16 ", "page_idx": 13}, {"type": "text", "text": "C Proofs of Theorems 16 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Proof of theorem 4.1 16   \nC.2 Proof of theorem 5.1 20   \nC.3 Proof of theorem 6.1 . 25 ", "page_idx": 13}, {"type": "text", "text": "D Further experiments 26 ", "page_idx": 13}, {"type": "text", "text": "D.1 Synthetic experiments . 26   \nD.2 A complete report of attack performance on real-world datasets 28   \nD.3 A complete report of privacy-utility assessments on Planetoid datasets 28   \nD.4 Spectrum study of GNN solutions obtained under the unconstrained scheme 33   \nD.5 Privacy-utility trade-off comparisons: NAG vs EdgeRR 33 ", "page_idx": 13}, {"type": "text", "text": "E Discussions and Limitations 35 ", "page_idx": 13}, {"type": "text", "text": "E.1 On the impact of depth $L$ for NAG 35   \nE.2 Stronger adversary for dense graphs or deep encoders 36   \nE.3 Extension to more complicated victim GNN models 36   \nE.4 Quantifying the advantage of adversaries with more knowledge 36 ", "page_idx": 13}, {"type": "text", "text": "A Broader impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The pervasive integration of graph representation learning (GRL) into various sectors, from social networks to bioinformatics, underscores the necessity of addressing the security and privacy risks inherent in these technologies. This paper contributes to the understanding of such risks by dissecting the structural vulnerabilities of graph representations under cosine-similarity-based edge reconstruction attacks (SERA). Our work has significant ethical implications and societal consequences, as we aim to balance the need for advanced data analytics with the imperative of safeguarding individual and community privacy. ", "page_idx": 13}, {"type": "text", "text": "Theoretically articulating the success and failure modes of SERA, our research offers a framework for evaluating GRL models against potential privacy breaches. The insights gained can guide the development of more secure algorithms that resist inadvertent information disclosure. By highlighting the efficacy of SERA in various settings, this paper also underscores the potential for such attacks to serve as auditing tools for privacy-preserving mechanisms, thereby fostering the creation of more trustworthy GRL systems. ", "page_idx": 13}, {"type": "text", "text": "As GRL technologies continue to evolve, our work calls attention to the importance of proactive privacy research in the field. It encourages the industry to adopt privacy-by-design principles and serves as a reminder to policymakers to consider the implications of GRL in legislation around data protection. Future societal consequences hinge on our ability to reconcile the benefits of GRL with the privacy rights of individuals, necessitating ongoing research, transparent practices, and informed governance to navigate this complex landscape. ", "page_idx": 13}, {"type": "text", "text": "B Additional backgrounds ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/3b45f01f026c6c0881ecfc804fb407375cf16a43859f9a4a18c86e2dcc41520e.jpg", "img_caption": ["B.1 VFL and the SERA adversary ", "Figure 3: Illustration of a typical vertically federated graph representation learning scenario, the figure is adapted from [36]. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We describe the scenario of vertically federated graph representation learning mentioned in section 1 in more detail, with the system architecture illustrated in figure 3. ", "page_idx": 14}, {"type": "text", "text": "VFL setup Under this scenario, we assume that party A (on the left side of figure 3 holds the graph as well as the node features, and party B (on the right side of figure 3 holds the node labels. Such kind of scenarios are encountered in applications like financial risk management [36]. Under VFL protocols, party A and party B iteratively exchange intermediate outputs to facilitate collaborative learning. The most representative method is split learning [36], where in each step, party A sends the node representations of a possibly sampled subgraph encoded using a graph neural network whose parameters are stored at party A. We call this operation the forward transmission step and highlighted in figure 3 as the red arrow. ", "page_idx": 14}, {"type": "text", "text": "SERA adversary We assume that party B is honest-but-curious in the sense that party B strictly follows the VFL protocol but tries to infer the graph structure belonged to party A, both during training and during inference, as the forward step is required for both stages. The attack requires nothing more than the VFL protocol: In each step, the two parties agree on a list of node indices that participates in this step (typically carried out using some cryptographically secure primitives), which constitutes the potential victim nodes $V_{\\mathrm{victim}}$ . Upon receiving the node embeddings from party A, party B is then free to conduct SERA that targets the topological structure of $G_{\\mathrm{victim}}$ . Furthermore, party B can even target a larger subgraph via storing multiple batches of embeddings and conduct attacks based on the unioned collections. Note that the adversary does not have access to GNN model parameters as they are kept locally at party A, which manifests the practicality of the proposed SERA adversary. ", "page_idx": 14}, {"type": "text", "text": "B.2 Measures of homophily ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The homophily metric is a way to describe the relationship between node properties and graph structure. Depending on the intrinsic nature of the property, we use two types of homophily measures: The label homophily (or edge homophily) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\mathrm{label}}(G,Y)=\\frac{1}{\\left|E\\right|}\\sum_{(u,v)\\in E}\\mathbf{1}\\left(Y_{u}=Y_{v}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which measures the averaged agreement of adjacent nodes\u2019 labels and the feature homophily (or generalized homophily [18, 22]) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\mathrm{feature}}(G,X)=\\frac{1}{\\left|E\\right|}\\sum_{(u,v)\\in E}\\cos{(X_{u},X_{v})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which replaces the agreement measure in the definition of label homophily with the cosine similarity between adjacent nodes\u2019 features. Another important metric we use in the experiments is the AUROC of guessing edge presence using feature similarities, i.e., $\\widehat{A}_{u v}^{\\mathsf{F S}}(\\tau)={\\bf1}\\left(\\cos(x_{u},x_{v})\\geq\\tau\\right)$ . Note that $\\mathcal{H}_{\\mathrm{feature}}$ might be related to but not always correlate well with the AUROC of $\\widehat{A}^{\\sf F S}$ , as $\\mathcal{H}_{\\mathrm{feature}}$ ignores the feature similarity of non-edges. ", "page_idx": 14}, {"type": "text", "text": "The feature homophily metric is sometimes related to, but not always correlated with the ", "page_idx": 14}, {"type": "text", "text": "B.3 Representative GNNs and their corresponding aggregations rules ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we briefly review GNN architectures that are involved in theorem 6.1 in the message-passing form as in (8): ", "page_idx": 15}, {"type": "text", "text": "Mean pooling [14] This is the most standard form of message passing GNN. With the un-normalized and un-perturbed version analyzed in section 4 and 5: ", "page_idx": 15}, {"type": "equation", "text": "$$\nH_{v}^{(l)}={\\sf R e L U}\\left(\\frac{1}{d_{v}+1}\\sum_{u\\in N(v)\\cup\\{v\\}}\\frac{W_{l}H_{u}^{(l-1)}}{\\left\\|H_{u}^{(l-1)}\\right\\|_{2}}+\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Summation pooling [38] This is a simplified version of the GIN model which is also analyzed in [36]: ", "page_idx": 15}, {"type": "equation", "text": "$$\nH_{v}^{(l)}=\\mathsf{R e L U}\\left(\\sum_{u\\in N(v)\\cup\\{v\\}}\\frac{W_{l}H_{u}^{(l-1)}}{\\left\\|H_{u}^{(l-1)}\\right\\|_{2}}+\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Max pooling [14] In its un-normalized and un-perturbed version, this corresponds to the mostly used SAGE model: ", "page_idx": 15}, {"type": "equation", "text": "$$\nH_{v}^{(l)}=\\mathsf{R e L U}\\left(\\operatorname*{max}_{u\\in N(v)\\cup\\{v\\}}\\frac{W_{l}H_{u}^{(l-1)}}{\\left\\|H_{u}^{(l-1)}\\right\\|_{2}}+\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "GCN pooling [20] The GCN pooling takes the form ", "page_idx": 15}, {"type": "equation", "text": "$$\nH_{v}^{(l)}=\\mathsf{R e L U}\\left(\\frac{1}{\\sqrt{d_{v}+1}}\\sum_{u\\in N(v)\\cup\\{v\\}}\\frac{W_{l}H_{u}^{(l-1)}}{\\sqrt{d_{u}+1}\\left\\|H_{u}^{(l-1)}\\right\\|_{2}}+\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Attentive pooling [32] This is also know as the GAT model. To simplify notations, let $\\tilde{H}_{v}^{(l)}=$ $H_{v}^{(l)}/\\left|\\left|H_{v}^{(l)}\\right|\\right|_{2}$ , then the GAT model is recursively defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{v}^{(l)}=\\mathsf{R e L U}\\left(\\displaystyle\\sum_{u\\in N(v)\\cup\\{v\\}}\\alpha_{u v}W_{l}\\tilde{H}_{u}^{(l-1)}+\\epsilon\\right)}\\\\ &{\\alpha_{u v}=\\frac{\\exp\\left(\\mathsf{L e a k y R e L U}\\left(\\langle\\beta_{\\mathrm{sre}},W_{l}\\tilde{H}_{u}^{(l-1)}\\rangle+\\langle\\beta_{\\mathrm{dst}},W_{l}\\tilde{H}_{v}^{(l-1)}\\rangle\\right)\\right)}{\\displaystyle\\sum_{u\\in N(v)\\cup\\{v\\}}\\exp\\left(\\mathsf{L e a k y R e L U}\\left(\\langle\\beta_{\\mathrm{sre}},W_{l}\\tilde{H}_{v}^{(l-1)}\\rangle+\\langle\\beta_{\\mathrm{dst}},W_{l}\\tilde{H}_{v}^{(l-1)}\\rangle\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\beta_{\\mathrm{src}},\\beta_{\\mathrm{dst}}\\in\\mathbb{R}^{d}$ are learnable vector parameters. ", "page_idx": 15}, {"type": "text", "text": "C Proofs of Theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Proof of theorem 4.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the proof, for notational simplicity, we abuse notation by treating $A=A+I$ and $D=D+I$ (i.e., self-edge is included in the edge graph). We then define $A^{(L)}:=A\\cdot\\underbrace{\\dots}_{I,\\ \\mathrm{times}}\\cdot A$ and $p_{i j}^{(L)}:=$ L  t i m es $((D^{-1}A)^{L})_{i j}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of the theorem. For any pair of two nodes $i$ and $j$ , we next recall the formula of cosine similarity, $\\cos\\theta(H_{i}^{(L)},H_{j}^{(L)})$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\cos\\theta(H_{i}^{(L)},H_{j}^{(L)}):=\\frac{\\langle H_{i}^{(L)},H_{j}^{(L)}\\rangle}{\\sqrt{\\langle H_{i}^{(L)},H_{i}^{(L)}\\rangle\\cdot\\langle H_{j}^{(L)},H_{j}^{(L)}\\rangle}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which will be used recurrently in the following main proof. ", "page_idx": 16}, {"type": "text", "text": "According to the generation mechanism of node features (i.e., isotropic Gaussian assumption), we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\frac{1}{d}\\|X_{j}\\|^{2}-1|\\leq3{\\sqrt{\\frac{\\log n}{d}}}{\\mathrm{~and~}}|{\\frac{1}{d}}\\langle X_{j},X_{j^{\\prime}}\\rangle|\\leq3{\\sqrt{\\frac{\\log n}{d}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $j,j^{\\prime}$ with probability at least $1-1/n^{2}$ . ", "page_idx": 16}, {"type": "text", "text": "Case $^{\\,l}$ : without considering the learnable weight matrix $W$ . For the numerator in $\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})$ , when $i_{1}$ and $i_{2}$ are truly connected, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle}&{=}&{\\displaystyle\\sum_{j=1}^{n}p_{i_{1}j}^{(L)}p_{i_{2}j}^{(L)}\\|X_{j}\\|^{2}+\\displaystyle\\sum_{j\\neq j}p_{i_{1}j}^{(L)}p_{i_{2}j^{\\prime}}^{(L)}\\langle X_{j},X_{j^{\\prime}}\\rangle}\\\\ &{\\ge}&{p_{i_{1}i_{1}}^{(L)}p_{i_{2}i_{1}}^{(L)}\\|X_{i_{1}}\\|^{2}+\\displaystyle\\sum_{j\\neq j^{\\prime}}p_{i_{1}j}^{(L)}p_{i_{2}j^{\\prime}}^{(L)}\\langle X_{j},X_{j^{\\prime}}\\rangle}\\\\ &{\\ge}&{\\displaystyle\\frac{1}{|N_{i}^{(L)}||N_{i-2}^{(L)}||}\\|X_{i_{1}}\\|^{2}+\\displaystyle\\sum_{j\\neq j^{\\prime}}p_{i_{1}j}^{(L)}p_{i_{2}j^{\\prime}}^{(L)}\\langle X_{j},X_{j^{\\prime}}\\rangle}\\\\ &{\\ge}&{\\displaystyle\\frac{1}{(C_{2}\\log{n})^{2L}}-3\\sqrt{\\frac{\\log{n}}{d}}\\quad(\\mathrm{usc~the~fact~that~\\displaystyle\\sum_{j\\neqj^{\\prime}}p_{i_{1}j}^{(L)}p_{i_{2}j^{\\prime}}^{(L)}}\\le1)}\\\\ &{\\ge}&{\\displaystyle\\frac{2}{3}\\cdot\\displaystyle\\frac{1}{(C_{2}\\log{n})^{2L}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "when $d>9(C_{2}\\log n)^{4L+2}\\cdot\\log n$ . On the other hand, when $i_{1}$ and $i_{2}$ are not connected, by Lemma C.2, we know that, with high probability, there are at most (C2 long n)2L $\\frac{(C_{2}\\log n)^{2L}}{n}\\cdot n(n-1)/2$ pairs of $i_{1},i_{2}$ such that $\\begin{array}{r}{\\sum_{j}A_{i_{1}j}^{(L)}A_{i_{2}j}^{(L)}\\geq1}\\end{array}$ which is equivalent to $\\begin{array}{r}{\\sum_{j}p_{i_{1}j}^{(L)}p_{i_{2}j}^{(L)}>0}\\end{array}$ . For the rest of pairs, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle}}&{{=}}&{{\\displaystyle\\sum_{j\\neq j^{\\prime}}p_{i_{1}j}^{(L)}p_{i_{2}j^{\\prime}}^{(L)}\\langle X_{j},X_{j^{\\prime}}\\rangle}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{\\leq}}&{{\\displaystyle3\\sqrt{\\frac{\\log n}{d}}}}\\\\ {{}}&{{<}}&{{\\displaystyle\\frac{1}{3}\\cdot\\frac{1}{(C_{2}\\log n)^{3L+1}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "when $d>9(C_{2}\\log n)^{6L+2}\\cdot\\log n$ . ", "page_idx": 16}, {"type": "text", "text": "For the denominator $(\\|H_{i_{1}}^{(L)}\\|\\cdot\\|H_{i_{2}}^{(L)}\\|)^{1/2}$ , we give the upper and lower bounds of $\\|H_{i}^{(L)}\\|$ . We can compute ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\langle H_{i}^{(L)},H_{i}^{(L)}\\rangle}}&{{=}}&{{\\displaystyle\\sum_{j=1}^{n}p_{i j}^{(L)}p_{i j}^{(L)}\\|X_{j}\\|^{2}+\\displaystyle\\sum_{j\\neq j^{\\prime}}p_{i j}^{(L)}p_{i j^{\\prime}}^{(L)}\\langle X_{j},X_{j^{\\prime}}\\rangle}}\\\\ {{}}&{{\\le}}&{{\\displaystyle1+3\\sqrt{\\frac{\\log n}{d}}}}\\\\ {{}}&{{<}}&{{1+\\displaystyle\\frac{1}{3}\\cdot\\frac{1}{(C_{2}\\log n)^{2L+1}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we use the fact that j pi(jL )pi(jL)\u22641. Conversely, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\langle H_{i}^{(L)},H_{i}^{(L)}\\rangle}&{=}&{\\displaystyle\\sum_{j=1}^{n}p_{i j}^{(L)}p_{i j}^{(L)}\\|X_{j}\\|^{2}+\\displaystyle\\sum_{j\\neq j^{\\prime}}p_{i j}^{(L)}p_{i j^{\\prime}}^{(L)}\\langle X_{j},X_{j^{\\prime}}\\rangle}\\\\ &{\\ge}&{\\displaystyle\\frac{1}{(C_{2}\\log n)^{L}}-3\\sqrt{\\frac{\\log n}{d}}}\\\\ &{\\ge}&{\\displaystyle\\frac{1}{(C_{2}\\log n)^{L}}-\\frac{1}{3}\\cdot\\frac{1}{(C_{2}\\log n)^{2L+1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we use the fact that j pi(jL )pi(jL) $\\begin{array}{r}{\\sum_{j}p_{i j}^{(L)}p_{i j}^{(L)}\\ge1/(C_{2}\\log n)^{L}}\\end{array}$ when $|N_{i}^{(L)}|\\le(C_{2}\\log n)^{L}$ . ", "page_idx": 17}, {"type": "text", "text": "To sum up, $\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})$ is at least ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{2}{3}\\cdot\\frac{1}{(C_{2}\\log n)^{2L}}/(1+\\frac{1}{3(C_{2}\\log n)^{2L+1}})\\geq\\frac{1}{2}\\cdot\\frac{1}{(C_{2}\\log n)^{2L}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "when node $i_{1}$ and $i_{2}$ are connected. On the other hand, $\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})$ is at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\cdot\\frac{1}{(C_{2}\\log n)^{3L+1}}/(\\frac{1}{(C_{2}\\log n)^{L}}-\\frac{1}{3}\\cdot\\frac{1}{(C_{2}\\log n)^{2L+1}})<\\frac{1}{2}\\cdot\\frac{1}{(C_{2}\\log n)^{2L}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all pairs (except at most $\\frac{(C_{2}\\log n)^{2L}}{n}\\cdot n(n-1)/2$ pairs) of disconnected nodes $i_{1}$ and $i_{2}$ ", "page_idx": 17}, {"type": "text", "text": "By choosing the cutoff $\\begin{array}{r}{\\tau\\,=\\,\\frac{1}{2}\\,\\cdot\\,\\frac{1}{(C_{2}\\log n)^{2L}}}\\end{array}$ , with probability at least $1-2/n^{2}$ , we have the false negative is zero and the false positive is (C2 long n)2L. ", "page_idx": 17}, {"type": "text", "text": "Case 2: with considering the learnable weight matrix $W$ . Additionally, if the learnable weight $W$ is taken into account, we can derive the following results. We define $\\kappa_{1}$ and $\\kappa_{2}$ to be the largest and smallest positive constants such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\kappa_{1}\\langle X,X^{\\prime}\\rangle\\leq\\langle W X,W X^{\\prime}\\rangle\\leq\\kappa_{2}\\langle X,X^{\\prime}\\rangle\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "holds. It is easy to see that $\\kappa_{2}/\\kappa_{1}=(\\kappa(W))^{2}$ . Then the parallel version of (14) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle\\geq\\kappa_{1}\\frac{2}{3}\\frac{1}{(C_{2}\\log n)^{2L}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The parallel version of (15) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle\\leq3\\kappa_{2}\\sqrt{\\frac{\\log n}{d}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The parallel version of (16) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle H_{i}^{(L)},H_{i}^{(L)}\\rangle\\leq\\kappa_{2}(1+3\\sqrt{\\frac{\\log n}{d}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The parallel version of (17) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle\\geq\\kappa_{1}(\\frac{1}{(C_{2}\\log n)^{L}}-3\\sqrt{\\frac{\\log n}{d}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining above results, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})}&{\\ge}&{\\displaystyle\\frac{\\kappa_{1}\\frac{2}{3}\\frac{1}{(C_{2}\\log n)^{2L}}}{\\kappa_{2}(1+3\\sqrt{\\frac{\\log n}{d}})}}\\\\ &{\\ge}&{\\displaystyle\\frac{\\kappa_{1}}{2\\kappa_{2}}\\frac{1}{(C_{2}\\log n)^{2L}}=:\\mathrm{cut}_{1}(L)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "when $i_{1},i_{2}$ are connected and $d\\gg\\log^{2}n$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})}&{\\leq}&{\\frac{3\\kappa_{2}\\sqrt{\\frac{\\log n}{d}}}{\\kappa_{1}\\left(\\frac{1}{(C_{2}\\log n)^{L}}-3\\sqrt{\\frac{\\log n}{d}}\\right)}}\\\\ &{\\leq}&{\\frac{4\\kappa_{2}}{\\kappa_{1}}\\frac{\\sqrt{\\frac{\\log n}{d}}}{\\frac{1}{(C_{2}\\log n)^{L}}}=:\\mathrm{cut}_{2}(L)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "when $i_{1},i_{2}$ are not connected and $d\\gg(C_{2}\\log n)^{6L+2}\\log n$ . ", "page_idx": 17}, {"type": "text", "text": "Therefore as long as $d\\gg(C_{2}\\log n)^{6L+2}\\log n$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\frac{\\kappa_{1}}{\\kappa_{2}})^{2}\\geq8(C_{2}\\log n)^{3L}\\cdot\\sqrt{\\frac{\\log n}{d}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "holds, we can choose any cutoff $\\tau$ between $\\mathrm{cut_{1}}(L)$ and $\\mathrm{cut_{2}}(L)$ so that false negative rate is zero and false positive rate is no larger than $(C_{2}\\log n)^{2L}/n$ . This completes the proof regarding FPR and FNR. For the implications in AUROC, the result follows immediately by noting the discoveries are montone in $\\tau$ . ", "page_idx": 17}, {"type": "text", "text": "Supporting Lemma of Theorem 4.1 The following Lemmas are used for controlling the number of pairs of nodes $(u,v)$ \u2019s which satisfy $\\begin{array}{r}{\\sum_{j}A_{u j}^{(L)}A_{v j}^{(L)}\\geq1}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1. Let $B(n,p)$ denote the binomial distribution with probability $p$ and size $n$ . ", "page_idx": 18}, {"type": "text", "text": "1. Suppose $X$ dominates $B(n,p)$ . For any $a>0$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}(X<n p-a)\\leq\\exp\\{-a^{2}/2n p\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "2. Suppose $X$ is dominated by $B(n,p)$ . For any $a>0$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}(X>n p+a)\\leq\\operatorname*{min}\\{\\exp\\{-a^{2}/2n p+a^{3}/(n p)^{3}\\},\\exp\\{-\\frac{a^{2}}{2n p+2a/3}\\}\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma C.1 is standard and we omit it here. The consequences of this Lemma is that $\\begin{array}{r}{\\frac{C_{1}}{2}\\log n\\leq\\sum_{j}A_{i j}\\leq\\frac{3C_{1}}{2}\\log n}\\end{array}$ with high probability at least $1-1/n^{2}$ for all node $i\\in[n]$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. Given a graph with edge probability $p$ $\\begin{array}{r}{\\gamma\\left(p\\leq C_{1}\\frac{\\log n}{n}\\right)}\\end{array}$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\sum_{j}A_{i_{1}j}^{(L)}A_{i_{2}j}^{(L)}\\geq1)\\leq\\frac{(C_{2}\\log n)^{2L}}{n-1},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $i_{1},i_{2}$ are two nodes uniformly randomly sampled from the graph. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma C.2. By recalling the definition of Ai(jL)that Ai(jL)equals one only when node i and node $j$ can be connected within a path of length $L$ . Therefore, with probability at least $1-1/n^{2}$ , it holds $\\begin{array}{r}{|\\mathcal{N}_{j}^{(L)}|\\le(\\frac{3C_{1}\\log n}{2})^{L}}\\end{array}$ , where $\\mathcal{N}_{j}^{(L)}:=\\{i:A_{i j}^{(L)}=1\\}$ ", "page_idx": 18}, {"type": "text", "text": "Note that, given fixed $j$ , $,A_{i_{1}j}A_{i_{2}j}$ is greater than 0 only if $i_{1},i_{2}\\in\\mathcal{N}_{j}^{(L)}$ . By the symmetry, we know that this happens with probability at most |N j(L)n|((|nN\u2212 j(1L))|\u22121)when i1, i2 are uniformly randomly sampled. Therefore, by union bound, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{P}(\\sum_{j}A_{i_{1}j}^{(L)}A_{i_{2}j}^{(L)}\\ge1)}&{\\le}&{\\displaystyle\\sum_{j}\\frac{|{\\mathcal{N}}_{j}^{(L)}|(|{\\mathcal{N}}_{j}^{(L)}|-1)}{n(n-1)}}\\\\ &{\\le}&{\\displaystyle\\frac{(1.5C_{1}\\log n)^{2L}}{n-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "The implication of this lemma is that there are at most $n\\cdot(C_{2}\\log n)^{2L}$ pairs of $(u,v)$ such that $\\begin{array}{r}{\\sum_{j}A_{u j}^{(\\bar{L})}A_{v j}^{(L)}\\geq1.}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Extension to different similarities The cosine similarity can be replaced by other similarity metrics. For example, we can use correlation similarity, i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{corr}(H_{i}^{(L)},H_{j}^{(L)}):=:=\\frac{\\langle H_{i}^{(L)}-\\bar{H}_{i}^{(L)},H_{j}^{(L)}-\\bar{H}_{j}^{(L)}\\rangle}{\\sqrt{\\langle H_{i}^{(L)}-\\bar{H}_{i}^{(L)},H_{i}^{(L)}-\\bar{H}_{i}^{(L)}\\rangle\\cdot\\langle H_{j}^{(L)}-\\bar{H}_{j}^{(L)},H_{j}^{(L)}-\\bar{H}_{j}^{(L)}\\rangle}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\bar{H}_{i}^{(L)}$ is a $d$ -dimensional vector whose elements are equal to the mean of $H_{i}^{(L)}$ . By treating $X_{j}-{\\bar{X}}_{j}$ as $X_{j}\\;(j\\in[n])$ , where $\\bar{X}_{j}$ is a $d_{\\cdot}$ -dimensional vector whose elements are equal to the mean of $X_{j}$ . (13) changes to ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\frac{1}{d}\\|X_{j}\\|^{2}-1|\\leq4{\\sqrt{\\frac{\\log n}{d}}}\\;{\\mathrm{and}}\\;|{\\frac{1}{d}}\\langle X_{j},X_{j^{\\prime}}\\rangle|\\leq4{\\sqrt{\\frac{\\log n}{d}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the above proof still holds by adjusting the constant accordingly. ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof of theorem 5.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To prove the desired result, we first need the following lemmas. In the rest of proof, we abuse the notation by treating $p$ as $p_{0}$ and $q$ as $q_{0}$ . ", "page_idx": 19}, {"type": "text", "text": "By applying the Hoeffding\u2019s inequality, we can obtain the following two lemmas. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.3. It holds $|\\sum_{j:i,j}$ in the same group $\\begin{array}{r}{A_{i j}-\\frac{n}{K}\\cdot p_{0}|\\le3\\log n=:\\epsilon_{1}}\\end{array}$ for all i with probability at least $1-1/n^{2}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma C.4. Suppose $i$ is in group $k$ , it holds $|\\sum_{j:i,j}$ in the group $\\begin{array}{r}{k^{\\prime}(\\neq k)\\;A_{i j}-\\frac{n}{K}\\cdot q_{0}\\big|\\leq\\operatorname*{min}\\{\\frac{1}{2}\\frac{n}{K}}\\end{array}$ $q_{0},3\\log n\\}=:\\epsilon_{2}$ for all $i$ with probability at least $1-1/n^{2}$ . ", "page_idx": 19}, {"type": "text", "text": "Combining Lemma C.3 and Lemma C.4, we have the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.5. It holds $\\begin{array}{r}{|\\sum_{j}A_{i j}-(\\frac{n}{K}\\cdot p_{0}+(n-\\frac{n}{K})\\cdot q_{0})|\\le\\epsilon_{1}+(K-1)\\epsilon_{2}}\\end{array}$ with probability at least $1-2/n^{2}$ . ", "page_idx": 19}, {"type": "text", "text": "In summary, with high probability confidence, Lemma C.5 gives the characterization of degree (i.e.   \nnumber of neighbours) of every node $i$ . ", "page_idx": 19}, {"type": "text", "text": "We then make a step forward and characterize the normalized degree pi(jL)for L \u22652 in the following lemmas. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.6. With probability at least $1-1/n^{2}$ , it holds that $\\begin{array}{r}{|A_{i j}^{(2)}-(\\frac{n}{K}p_{0}^{2}+(n-n/K)q_{0}p_{0})|\\leq}\\end{array}$ $\\begin{array}{r}{6\\log n\\!+\\!\\frac{1}{2}\\frac{n}{K}\\!\\cdot\\!q_{0}\\,f\\!o r\\,i,j}\\end{array}$ from the same group and $\\begin{array}{r}{|A_{i j}^{(2)}-(\\frac{n}{K}p_{0}q_{0}+(n-n/K)q_{0}^{2})|\\leq\\operatorname*{min}\\{\\frac{2}{3}(\\frac{n}{K}p_{0}q_{0}+}\\end{array}$ $(n-n/K)q_{0}^{2}),3(K-1)\\log n\\}.$ for $i,j$ from different groups. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.7. For $L\\ge2$ , suppose there exist constants $a_{1}^{(L)}$ and $a_{2}^{(L)}$ such that $|A_{i j}^{(L)}-a_{1}^{(L)}|\\le\\epsilon_{1}^{(L)}$ when $i,j$ are in the same group and $|A_{i j}^{(L)}-a_{2}^{(L)}|\\le\\epsilon_{2}^{(L)}$ when $i,j$ are not in the same group. It holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A_{i j}^{(L+1)}-a_{1}^{(L+1)}|\\le\\epsilon_{1}^{(L)}\\;\\;i,j\\;i n\\;t h e\\;s a m e\\;g r o u p}\\\\ &{|A_{i j}^{(L+1)}-a_{2}^{(L+1)}|\\le\\epsilon_{2}^{(L)}\\;\\;i,j\\;n o t\\;i n\\;t h e\\;s a m e\\;g r o u p,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{1}^{(L+1)}:=(a_{1}^{(L)}\\frac{n}{K}p_{0}+a_{2}^{(L)}(n-n/K)q_{0}),}\\\\ &{a_{2}^{(L+1)}:=a_{1}^{(L)}\\frac{n}{K}q_{0}+a_{2}^{(L)}\\frac{n}{K}p_{0}+a_{2}^{(L)}(n-2n/K)q_{0}}\\\\ &{\\epsilon_{1}^{(L+1)}:=\\epsilon_{1}^{(L)}\\frac{n}{K}p_{0}+\\epsilon_{1}a_{1}^{(L)}+\\epsilon_{1}\\epsilon_{1}^{(L)}+\\epsilon_{2}^{(L)}(n-n/K)q_{0}+(K-1)\\epsilon_{2}a_{2}^{(L)}+(K-1)\\epsilon_{2}\\epsilon_{2}^{(L)},}\\\\ &{\\epsilon_{2}^{(L+1)}:=\\epsilon_{1}^{(L)}\\frac{n}{K}q_{0}+\\epsilon_{2}a_{1}^{(L)}+\\epsilon_{2}\\epsilon_{1}^{(L)}+\\epsilon_{2}^{(L)}\\frac{n}{K}p_{0}+\\epsilon_{1}a_{2}^{(L)}+\\epsilon_{1}\\epsilon_{2}^{(L)}+\\epsilon_{2}^{(L)}(n-2n/K)q_{0}}\\\\ &{\\hphantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}}\\\\ &{\\hphantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}}+(K-2)\\epsilon_{2}a_{2}^{(L)}+(K-2)\\epsilon_{2}\\epsilon_{2}^{(L)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma C.6 is a special case of that of Lemma C.7. In the following, we prove Lemma C.7. ", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r}{A_{i j}^{(L+1)}=\\sum_{j^{\\prime}}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "When $i,j$ are from the same class (w.l.o.g, we denote it as class 1), then it holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{|A_{i j}^{(L+1)}-(a_{1}^{(L)}\\frac{n}{K}p_{0}+a_{2}^{(L)}(n-n/K)q_{0})|}\\\\ {=}&{|\\displaystyle\\sum_{j^{\\prime}}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}-(a_{1}^{(L)}\\frac{n}{K}p_{0}+a_{2}^{(L)}(n-n/K)q_{0})|}\\\\ {\\leq}&{|\\displaystyle\\sum_{j^{\\prime}\\mathrm{~in~class}\\,1}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}-a_{1}^{(L)}\\frac{n}{K}p_{0}|+|\\displaystyle\\sum_{j^{\\prime}\\mathrm{~not\\,in\\,class}\\,1}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}-a_{2}^{(L)}(n-n/K)q_{0}|}\\\\ {=}&{\\epsilon_{1}^{(L)}\\displaystyle\\frac{n}{K}p_{0}+\\epsilon_{1}a_{1}^{(L)}+\\epsilon_{1}\\epsilon_{1}^{(L)}+\\epsilon_{2}^{(L)}(n-n/K)q_{0}+(K-1)\\epsilon_{2}a_{2}^{(L)}+(K-1)\\epsilon_{2}\\epsilon_{2}^{(L)}(2-n/K)q_{0}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we can let $\\begin{array}{r}{a_{1}^{(L+1)}:=(a_{1}^{(L)}\\frac{n}{k}p_{0}+a_{2}^{(L)}(n-n/K)q_{0})}\\end{array}$ and $\\epsilon_{1}^{(L+1)}:=\\epsilon_{1}^{(L)}\\frac{n}{K}p_{0}+\\epsilon_{1}a_{1}^{(L)}+$ $\\epsilon_{1}\\epsilon_{1}^{(L)}+\\epsilon_{2}^{(L)}(n-n/K)q_{0}+(K-1)\\epsilon_{2}a_{2}^{(L)}+(K-1)\\epsilon_{2}\\epsilon_{2}^{(L)}.$ ", "page_idx": 20}, {"type": "text", "text": "When $i,j$ are not from the same class (w.l.o.g. we assume $i$ is from class 1 and $j$ is from class 2), then it holds ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A_{i j}^{(L+1)}-(a_{1}^{(L)}\\frac{n}{K}q_{0}+a_{2}^{(L)}\\frac{n}{K}p_{0}+a_{2}^{(L)}(n-2n/K)q_{0})|}\\\\ {=}&{|\\displaystyle\\sum_{j^{\\prime}}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}-(a_{1}^{(L)}\\frac{n}{K}q_{0}+a_{2}^{(L)}\\frac{n}{K}p_{0}+a_{2}^{(L)}(n-2n/K)q_{0})|}\\\\ {\\le}&{|\\displaystyle\\sum_{j^{\\prime}\\operatorname*{ins}\\{1\\leq i\\}}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}-a_{1}^{(L)}\\frac{n}{K}q_{0}|+|\\displaystyle\\sum_{j^{\\prime}\\operatorname*{insts}2}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}-a_{2}^{(L)}\\frac{n}{K}p_{0}|}\\\\ &{+|\\displaystyle\\sum_{j^{\\prime}\\operatorname*{minstic}1\\leq i}A_{i j^{\\prime}}^{(L)}A_{j^{\\prime}j}-a_{2}^{(L)}(n-2n/K)q_{0}|}\\\\ {\\le}&{\\epsilon_{1}^{(L)}\\displaystyle\\frac{n}{K}q_{0}+\\epsilon_{2}a_{1}^{(L)}+\\epsilon_{2}\\epsilon_{1}^{(L)}+\\epsilon_{2}^{(L)}\\displaystyle\\frac{n}{K}p_{0}+\\epsilon_{1}a_{2}^{(L)}+\\epsilon_{1}\\epsilon_{2}^{(L)}}\\\\ &{+\\epsilon_{2}^{(L)}(n-2n/K)q_{0}+(K-2)\\epsilon_{2}a_{2}^{(L)}+(K-2)\\epsilon_{2}\\epsilon_{2}^{(L)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we can let a(2L $\\begin{array}{r}{a_{2}^{(L+1)}:=a_{1}^{(L)}\\frac{n}{K}q_{0}\\!+\\!a_{2}^{(L)}\\frac{n}{K}p_{0}\\!+\\!a_{2}^{(L)}(n\\!-\\!2n/K)q_{0}}\\end{array}$ and $\\begin{array}{r}{\\epsilon_{2}^{(L+1)}:=\\epsilon_{1}^{(L)}\\frac{n}{K}q_{0}+}\\end{array}$ $\\epsilon_{2}a_{1}^{(L)}+\\epsilon_{2}\\epsilon_{1}^{(L)}+\\epsilon_{2}^{(L)}\\frac{\\pi}{K}p_{0}+\\epsilon_{1}a_{2}^{(L)}+\\epsilon_{1}\\epsilon_{2}^{(L)}+\\epsilon_{2}^{(L)}(n-2n/K)q_{0}+(K-2)\\epsilon_{2}a_{2}^{(L)}+(K-2)\\epsilon_{2}\\epsilon_{2}^{(L)}+(K-2)\\epsilon_{1}^{(L)}\\epsilon_{2}^{(L)};$ ). \u53e3 ", "page_idx": 20}, {"type": "text", "text": "By above induction, it can be seen that, for any fixed $L$ , $\\epsilon_{1}^{(L)}/a_{1}^{(L)}\\;=\\;{\\cal O}_{p}(\\frac{\\log n}{n})$ , $\\epsilon_{2}^{(L)}/a_{1}^{(L)}\\;=\\;$ $O_{p}(\\frac{\\log n}{n})$ . It also holds $a_{2}^{(L)}/a_{1}^{(L)}=O_{p}(\\frac{\\log n}{n})$ when true edge probability satisfies $q_{0}=O_{p}(\\frac{\\log n}{n})$ , and \u03f52 $\\epsilon_{2}^{(\\stackrel{\\cdot}{L})}/a_{2}^{(L)}=O_{p}(\\frac{\\log n}{n})$ when $q_{0}\\gg{\\frac{\\log n}{n}}$ ", "page_idx": 20}, {"type": "text", "text": "Recall the definition that pi(jL) $p_{i j}^{(L)}=((D^{-1}A)^{L})_{i j}$ , therefore $p_{i j}^{(L)}\\propto A_{i j}^{(L)}$ Ai(jL)for any fixed i. In other words, for fixed $L\\ge2$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Pi}_{i j}^{(L)}:=\\bar{p}_{i j}^{(L)}+O_{p}(\\frac{k\\log n}{n^{2}})=\\frac{a_{1}^{(L)}}{\\underbrace{\\frac{n}{K}\\cdot a_{1}^{(L)}+(n-\\frac{n}{K})\\cdot a_{2}^{(L)}}_{=:p_{1}^{(L)}}}+O_{p}(\\frac{k\\log n}{n^{2}}),\\;\\;i,j\\;\\mathrm{in~the~same~group},}\\\\ &{\\mathsf{\\Pi}_{i j}^{(L)}=\\bar{p}_{i j}^{(L)}+O_{p}(\\frac{k\\log n}{n^{2}})=\\underbrace{\\frac{a_{2}^{(L)}}{\\underbrace{\\frac{n}{K}\\cdot a_{1}^{(L)}+(n-\\frac{n}{K})\\cdot a_{2}^{(L)}}_{=:p_{2}^{(L)}}}}_{=:p_{2}^{(L)}}+O_{p}(\\frac{k\\log n}{n^{2}}),\\;\\;i,j\\;\\mathrm{not~in~the~same~group}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, on a very high level, we can treat $\\bar{p}_{i j}^{(L)}$ as the population version of $((D^{-1}A)^{L})_{i j}$ . When $i,j$ in the same group, then $\\bar{p}_{i j\\_}^{(L)}\\equiv p_{1}^{(L)}$ . Otherwise, p\u00afi(jL) $\\bar{p}_{i j}^{(L)}\\equiv p_{2}^{(L)}$ p(2L). With above preparations, we are ready to prove the theorem as follows. ", "page_idx": 20}, {"type": "text", "text": "Proof of the theorem. We need to consider the case $L\\ge2$ and $L=1$ separately. ", "page_idx": 20}, {"type": "text", "text": "Case 1: $L\\,\\geq\\,2$ . We define $\\begin{array}{r}{\\bar{X}_{k}\\,:=\\,\\sum_{i\\in\\mathrm{~group~k~}}X_{i}}\\end{array}$ and $r^{(L)}\\,:=\\,a_{2}^{(L)}/a_{1}^{(L)}$ . For the numerator in $\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})$ , when $i_{1}$ and $i_{2}$ are in the same group (w.l.o.g, suppose it is group 1), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle}&{=\\phantom{\\frac{u}{\\int_{\\left\\vert\\eta_{1}\\right\\vert}^{L}}}\\gamma_{i_{1}}^{(L)}\\psi_{i_{2}}^{(L)}\\vert\\left\\vert X_{2}\\right\\vert^{2}+\\phantom{\\frac{u}{\\int_{\\left\\vert\\eta_{1}\\right\\vert}^{L}}}\\gamma_{i_{1}}^{(L)}\\vert\\mathcal{S}_{i_{2}}^{(L)};\\langle X_{j},X_{j}\\rangle}\\\\ &{=\\phantom{\\frac{u}{\\int_{\\left\\vert\\eta_{1}\\right\\vert}^{L}}}\\gamma_{i_{1}}^{(L)}\\beta_{i_{2}}^{(L)}\\left\\vert\\left\\vert X_{2}\\right\\vert^{2}+\\sum_{j\\neq j^{\\prime}}^{D}\\beta_{i_{1}j}^{(L)}\\beta_{i_{2}}^{(L)};\\langle X_{j},X_{j^{\\prime}}\\rangle+\\mathrm{error}\\phantom{\\frac{u}{\\int_{\\left(\\left(\\left(\\left(\\left(\\begin{array}{l\\begin{array}{l}{l}{l}\\\\ {l}\\end{array}\\right)\\right)\\right)\\right)}}}\\right.}\\\\ &{=\\phantom{\\frac{u}{\\int_{\\left(\\left(\\begin{array}{l}{l}\\\\ {l}\\end{array}{l}\\right)\\right)}}}\\rho_{1}^{(L)}\\langle X_{1},\\bar{X}_{1}\\rangle+2\\sum_{j\\neq j}^{D}\\beta_{i_{1}}^{(L)}\\bar{\\gamma}_{i_{2}}^{(L)}\\langle\\bar{X}_{1},\\bar{X}_{k}\\rangle+\\sum_{k\\neq1}^{D}\\beta_{j_{2}}^{(L)}\\bar{\\gamma}_{i_{3}}^{(L)}\\langle\\bar{X}_{k},\\bar{X}_{k}\\rangle}\\\\ &{\\left.+\\sum_{k\\neq k^{\\prime}\\neq k^{\\prime}}\\frac{\\eta_{2}^{(L)}p_{i}^{(L)}}{D_{k}^{2}}\\langle\\bar{X}_{k},\\bar{X}_{k^{\\prime}}\\rangle+\\mathrm{error}\\phantom{\\frac{u}{\\int_{\\left(\\left(\\begin{array}{l}{l}\\\\ {k}\\end{array}\\right)\\right)}}}\\end{array}\\right.}\\\\ &{=\\phantom{\\frac{u}{\\int_{\\left(\\left(\\begin{array}{l}{l}\\\\ {l}\\end{array}{l}\\right)}}\\gamma_{\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (36) uses the property of node feature generation mechanism that $\\begin{array}{r}{\\langle\\bar{X}_{k},\\bar{X}_{k}\\rangle=\\frac{n}{K}(1+\\sqrt{1/d})}\\end{array}$ for any $k$ and $\\langle{\\bar{X}}_{k},{\\bar{X}}_{k^{\\prime}}\\rangle\\ =\\ O_{p}(\\frac{n}{K\\sqrt{d}})$ for $k\\ \\ne\\ k^{\\prime}$ . Here the error term in (36) is error $:=$ $\\begin{array}{r}{\\sum_{j=1}^{n}(p_{i_{1}j}^{(L)}p_{i_{2}j}^{(L)}-\\bar{p}_{i_{1}j}^{(L)}\\bar{p}_{i_{2}j}^{(L)})\\|X_{j}\\|^{2}+\\sum_{j\\neq j^{\\prime}}(p_{i_{1}j}^{(L)}p_{i_{2}j^{\\prime}}^{(L)}-\\bar{p}_{i_{1}j}^{(L)}\\bar{p}_{i_{2}j^{\\prime}}^{(L)})\\langle X_{j},X_{j^{\\prime}}\\rangle}\\end{array}$ , which can be controlled as follows. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{error}|=|\\displaystyle\\sum_{j=1}^{n}(\\boldsymbol{p}_{i,j}^{(L)}\\boldsymbol{p}_{i,j}^{(L)}-\\bar{\\boldsymbol{p}}_{i,j}^{(L)}\\bar{\\boldsymbol{p}}_{i,j}^{(L)})||X_{j}||^{2}+\\displaystyle\\sum_{j\\neq j^{\\prime}}(p_{i,j}^{(L)}p_{i,j^{\\prime}}^{(L)}-\\bar{p}_{i,j}^{(L)}\\bar{p}_{i,j^{\\prime}}^{(L)})\\langle X_{j},X_{j^{\\prime}}\\rangle|}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x}\\leq|\\displaystyle\\sum_{j=1}^{n}(\\boldsymbol{p}_{i,j}^{(L)}\\boldsymbol{p}_{i,j}^{(L)}-\\bar{p}_{i,j}^{(L)}\\bar{p}_{i,j}^{(L)})||X_{j}||^{2}|+|\\displaystyle\\sum_{j\\neq j^{\\prime}}(p_{i,j}^{(L)}p_{i,j^{\\prime}}^{(L)}-\\bar{p}_{i,j}^{(L)}\\bar{p}_{i,j^{\\prime}}^{(L)})\\langle X_{j},X_{j^{\\prime}}\\rangle|}\\\\ &{\\phantom{x x x x x x}\\leq C(\\displaystyle\\frac{k\\log n}{n^{2}}+\\sum_{j}\\displaystyle\\frac{k\\log n}{n^{2}}\\sqrt{\\frac{\\log n}{d}})}\\\\ &{\\phantom{x x x x x}=O_{p}(\\displaystyle\\frac{k\\log n}{n^{2}}+\\frac{k\\log n}{n}\\sqrt{\\frac{\\log n}{d}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (37) utilizes the fact that $\\textstyle\\sum_{j}{\\bar{p}}_{i j}\\equiv1$ for any $i$ and (34) by adjusting the constant. ", "page_idx": 21}, {"type": "text", "text": "When $i_{1},i_{2}$ are not in the same group (w.l.o.g, suppose $i_{1}$ in group 1 and $i_{2}$ in group 2), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})}&{=}&{\\displaystyle\\sum_{j=1}^{N}p_{i_{1}j}^{(L)}p_{i_{2}j}^{(L)}\\|X_{j}\\|^{2}+\\displaystyle\\sum_{j\\neq j}p_{i_{1}j}^{(L)}p_{i_{2}j}^{(L)}\\langle X_{j},X_{j}\\rangle}\\\\ &{=}&{\\displaystyle\\sum_{j=1}^{N}p_{i_{1}j}^{(L)}p_{j_{2}j}^{(L)}\\|X_{j}\\|^{2}+\\displaystyle\\sum_{j\\neq j}p_{i_{1}j}^{(L)}p_{i_{2}j}^{(L)}\\langle X_{j},X_{j}\\rangle+\\mathrm{eror}\\ }\\\\ &{=}&{p_{1}^{(L)}p_{2}^{(L)}(\\langle X_{1},X_{1}\\rangle+\\langle X_{2},X_{2}\\rangle)+(p_{1}^{(L)}p_{1}^{(L)}+p_{2}^{(L)}p_{2}^{(L)})\\langle X_{1},X_{2}\\rangle}\\\\ &&{\\displaystyle+(p_{1}^{(L)}p_{2}^{(L)}+p_{2}^{(L)}p_{2}^{(L)})\\sum_{j=1}^{N}(\\langle X_{1}+X_{2},X_{k}\\rangle+\\sum_{k\\in\\mathcal{N}(\\mathcal{N})}p_{k}^{(L)}p_{j}^{(L)}\\langle X_{k},X_{k}\\rangle}\\\\ &&{+\\displaystyle\\sum_{j\\neq j}p_{2}^{(L)}\\langle X_{j},X_{j}\\rangle+\\mathrm{eror}\\ }\\\\ &&{\\displaystyle k_{j}k_{j}^{\\mu\\nu_{j+1}}\\Sigma_{j}^{0}}\\\\ &{=}&{2p_{i}^{(L)}p_{2}^{(L)}\\,\\underline{{N}}+(K-2)p_{2}^{(L)}p_{2}^{(L)}\\,\\underline{{N}}}\\\\ &&{+o_{p}(p_{1}^{(L)}p_{1}^{(L)}+K p_{1}^{(L)}p_{2}^{(L)}+K p_{2}^{(L)}p_{2}^{(L)})\\underline{{N}}\\,\\binom{\\lambda}{d}+\\mathrm{eror}\\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To sum up, if $i_{1},i_{2}$ are in the same group, $\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})}\\\\ &{=\\!\\frac{\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle}{\\sqrt{\\langle H_{i_{1}}^{(L)},H_{i_{1}}^{(L)}\\rangle\\cdot\\langle H_{i_{2}}^{(L)},H_{i_{2}}^{(L)}\\rangle}}}\\\\ &{=\\!\\frac{p_{1}^{(L)}p_{1}^{(L)}\\frac{n}{k}+(K-1)p_{2}^{(L)}p_{2}^{(L)}\\frac{n}{K}+(K-1)p_{2}^{(L)}p_{2}^{(L)}\\frac{n}{K}}{p_{1}^{(L)}p_{1}^{(L)}\\frac{n}{K}+(K-1)p_{2}^{(L)}p_{2}^{(L)}\\frac{n}{K}+C\\big((K p_{1}^{(L)}p_{2}^{(L)}+K^{2}p_{2}^{(L)2})\\frac{n}{K\\sqrt{d}}+O_{p}(\\frac{K\\log n}{n}(\\frac{1}{n}+\\sqrt{\\frac{\\log n}{d}})}}\\\\ &{=\\!\\underbrace{1}_{\\mathrm{cut}(L)}-o_{p}(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "as long as $d\\gg K^{2}\\log^{3}n/b^{2}$ . ", "page_idx": 22}, {"type": "text", "text": "$i_{1},i_{2}$ are not in the same group, $\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cos\\theta(H_{i_{1}}^{(L)},H_{i_{2}}^{(L)})}\\\\ &{=\\frac{\\langle H_{i_{1}}^{(L)},H_{i_{2}}^{(L)}\\rangle}{\\sqrt{\\langle H_{i_{1}}^{(L)},H_{i_{1}}^{(L)}\\rangle\\cdot\\langle H_{i_{2}}^{(L)},H_{i_{2}}^{(L)}\\rangle}}}\\\\ &{=\\frac{2p_{1}^{L}p_{2}^{L}\\frac{n}{R}+(K-2)p_{2}^{(L)}p_{2}^{(L)}\\frac{n}{R}+C\\left((K p_{1}^{(L)}p_{2}^{(L)}+K^{2}p_{2}^{(L)2})\\frac{n}{K\\sqrt{d}}+O_{p}\\big(\\frac{K\\log n}{n}(\\frac{1}{n}+\\sqrt{\\frac{\\log n}{d}}\\|\\mathbf{x}\\|)\\|_{2}^{2}\\right)}{p_{1}^{(L)}p_{1}^{(L)}\\frac{n}{K}+(K-1)p_{2}^{(L)}p_{2}^{(L)}\\frac{n}{K}}}\\\\ &{=\\underbrace{\\frac{2r^{(L)}+(K-2)r^{(L)2}}{1+(K-1)r^{(L)2}}}_{\\mathrm{cug}_{(L)}}+o_{p}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark. As $L\\rightarrow\\infty$ , $r^{(L)}$ will converge to 1. Therefore, $\\mathrm{cut_{2}}(L)$ will eventually equal $1\\equiv\\operatorname{cut}_{1}(L)$ . Case 2: $L=1$ . For notational convenience, we define $\\tilde{X}_{k,1}^{(i)}:=\\sum_{i\\in}$ group k $b_{i,1}^{(i)}X_{i}$ where $b_{i,1}^{(i)}$ \u2019 s are i.i.d. Bernoulli random variables with success probability $p_{0}$ and $\\tilde{X}_{k,2}^{(i)}:=\\sum_{i\\in}$ group $\\boldsymbol{\\mathbf{\\ell}}_{k}\\,b_{i,2}^{(i)}X_{i}$ where $b_{i,2}^{(i)}$ \u2019s are i.i.d. Bernoulli random variables with success probability $q_{0}$ . Then it is straightforward to calculate that, if $i_{1},i_{2}$ are in the same group 1, it holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\langle H_{i_{1}}^{(1)},H_{i_{2}}^{(1)}\\rangle}&{=\\displaystyle}&{\\displaystyle\\frac{1}{D_{i_{1}}D_{i_{2}}}\\big(\\langle\\tilde{X}_{1,1}^{(i_{1})},\\tilde{X}_{1,1}^{(i_{2})}\\rangle+\\displaystyle\\sum_{k\\neq1}\\langle\\tilde{X}_{1,1}^{(i_{1})},\\tilde{X}_{k,2}^{(i_{2})}\\rangle+\\displaystyle\\sum_{k\\neq1}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{1,1}^{(i_{2})}\\rangle}\\\\ &&{+\\displaystyle\\sum_{k\\neq1}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{k,2}^{(i_{2})}\\rangle+\\displaystyle\\sum_{k,k^{\\prime}\\neq1}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{k^{\\prime},2}^{(i_{2})}\\rangle\\big)}\\\\ &{=\\displaystyle}&{\\frac{1}{D_{i_{1}}D_{i_{2}}}\\big(\\frac{n}{k}p_{0}^{2}+(K-1)\\frac{n}{K}q_{0}^{2}+O_{p}(p_{0}\\frac{n\\sqrt{\\log n}}{K\\sqrt{d}}+\\sqrt{p_{0}q_{0}}\\frac{n\\sqrt{\\log n}}{\\sqrt{d}}}\\\\ &&{+K q_{0}\\frac{n\\sqrt{\\log n}}{\\sqrt{d}})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, if $i_{1},i_{2}$ are not in the same group (w.l.o.g, they are in group 1 and 2 respectively), it holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle H_{i_{1}}^{(1)},H_{i_{2}}^{(1)}\\rangle}&{=d\\quad\\cfrac{1}{D_{i_{1}}D_{i_{2}}}\\big(\\langle\\tilde{X}_{1,1}^{(i_{1})},\\tilde{X}_{1,2}^{(i_{2})}\\rangle+\\langle\\tilde{X}_{2,2}^{(i_{1})},\\tilde{X}_{1,1}^{(i_{2})}\\rangle+\\langle\\tilde{X}_{1,1}^{(i_{1})},\\tilde{X}_{2,1}^{(i_{2})}\\rangle+\\langle\\tilde{X}_{2,2}^{(i_{1})},\\tilde{X}_{1,2}^{(i_{2})}\\rangle\\big.}\\\\ &{\\quad+\\displaystyle\\sum_{k\\neq1,2}\\langle\\tilde{X}_{1,1}^{(i_{1})}+\\tilde{X}_{2,2}^{(i_{1})},\\tilde{X}_{k,2}^{(i_{2})}\\rangle+\\displaystyle\\sum_{k\\neq1,2}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{1,2}^{(i_{2})}+\\tilde{X}_{2,1}^{(i_{2})}\\rangle}\\\\ &{\\quad+\\displaystyle\\sum_{k\\neq1,2}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{k,2}^{(i_{2})}\\rangle+\\displaystyle\\sum_{k,k^{\\prime}\\neq1,2}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{k^{\\prime},2}^{(i_{2})}\\rangle}\\\\ {=}&{\\displaystyle\\frac{1}{D_{i_{1}}D_{i_{2}}}\\big(2\\frac{n}{k}p_{0}q_{0}+(K-2)\\frac{n}{K}q_{0}^{2}+O_{p}(p_{0}\\frac{n\\sqrt{\\log n}}{K\\sqrt{d}}+\\sqrt{p_{0}q_{0}}\\frac{n\\sqrt{\\log n}}{\\sqrt{d}}}\\\\ &{\\quad+K q_{0}\\frac{n\\sqrt{\\log n}}{\\sqrt{d}})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, if $i_{1}=i_{2}$ , it holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\langle H_{i_{1}}^{(1)},H_{i_{1}}^{(1)}\\rangle}&{=d}&{\\displaystyle\\frac{1}{D_{i_{1}}D_{i_{1}}}\\big(\\langle\\tilde{X}_{1,1}^{(i_{1})},\\tilde{X}_{1,1}^{(i_{1})}\\rangle+2\\displaystyle\\sum_{k\\neq1}\\langle\\tilde{X}_{1,1}^{(i_{1})},\\tilde{X}_{k,2}^{(i_{1})}\\rangle\\big)}\\\\ &&{+\\displaystyle\\sum_{k\\neq1}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{k,2}^{(i_{1})}\\rangle+\\displaystyle\\sum_{k,k^{\\prime}\\neq1}\\langle\\tilde{X}_{k,2}^{(i_{1})},\\tilde{X}_{k^{\\prime},2}^{(i_{1})}\\rangle\\big)}\\\\ &{=}&{\\displaystyle\\frac{1}{D_{i_{1}}D_{i_{2}}}\\big(\\frac{n}{k}p_{0}+(K-1)\\frac{n}{K}q_{0}+O_{p}(p_{0}\\frac{n\\sqrt{\\log n}}{K\\sqrt{d}}+\\sqrt{p_{0}q_{0}}\\frac{n\\sqrt{\\log n}}{\\sqrt{d}}}\\\\ &&{+K q_{0}\\frac{n\\sqrt{\\log n}}{\\sqrt{d}})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To sum up, we arrive at ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\cos\\theta(H_{i_{1}}^{(1)},H_{i_{2}}^{(1)})}\\\\ {=}&{{\\frac{\\langle H_{i_{1}}^{(1)},H_{i_{2}}^{(1)}\\rangle}{\\sqrt{\\langle H_{i_{1}}^{(1)},H_{i_{1}}^{(1)}\\rangle\\cdot\\langle H_{i_{2}}^{(1)},H_{i_{2}}^{(1)}\\rangle}}}}\\\\ {=}&{{\\frac{\\frac{n}{k}p_{0}^{2}+(K-1){\\frac{n}{K}}q_{0}+O_{p}(p_{0}{\\frac{n\\sqrt{10\\cos\\theta}}{K\\sqrt{d}}}+\\sqrt{p_{0}q_{0}{\\frac{n\\sqrt{10\\cos\\theta}}{\\sqrt{d}}}+K q_{0}{\\frac{n\\sqrt{\\log n}}{\\sqrt{d}}}})}{{\\frac{n}{k}}p_{0}+(K-1){\\frac{n}{K}}q_{0}}}}\\\\ {=}&{{\\frac{{\\frac{n}{k}}p_{0}^{2}+(K-1){\\frac{n}{K}}q_{0}^{2}}{{\\frac{n}{k}}p_{0}+(K-1){\\frac{n}{K}}{\\frac{q_{0}}{K}}}}+o_{p}(1)}\\\\ &{{\\frac{\\sin p_{0}+(K-1){\\frac{n}{K}}q_{0}}{\\operatorname*{cut}(1)}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $i_{1},i_{2}$ from the same group, when $d\\gg\\log n$ . Similarly, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\cos\\theta(H_{i_{1}}^{(1)},H_{i_{2}}^{(2)})}\\\\ {=}&{{\\frac{\\langle H_{i_{1}}^{(1)},H_{i_{2}}^{(1)}\\rangle}{\\sqrt{\\langle H_{i_{1}}^{(1)},H_{i_{1}}^{(1)}\\rangle\\cdot\\langle H_{i_{2}}^{(1)},H_{i_{2}}^{(1)}\\rangle}}}}\\\\ {=}&{{\\frac{{\\frac{n}{k}}p_{0}^{2}+(K-1){\\frac{n}{K}}q_{0}^{2}+O_{p}(p_{0}{\\frac{n\\sqrt{\\log n}}{K\\sqrt{d}}}+{\\sqrt{p_{0}q_{0}}}{\\frac{n\\sqrt{\\log n}}{\\sqrt{d}}}+K q_{0}{\\frac{n\\sqrt{\\log n}}{\\sqrt{d}}})}{{\\frac{n}{k}}p_{0}+(K-1){\\frac{n}{K}}q_{0}}}}\\\\ {=}&{{\\frac{2{\\frac{n}{k}}p_{0}q_{0}+(K-2){\\frac{n}{K}}q_{0}^{2}}{{\\frac{n}{k}}p_{0}+(K-1){\\frac{n}{K}}q_{0}}}+o_{p}(1)}\\\\ &{{\\frac{\\qquad n}{\\sqrt{\\frac{n}{k}}p_{0}+(K-1){\\frac{n}{K}}q_{0}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $i_{1},i_{2}$ from different groups, when $d\\gg\\log n/p_{0}^{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Therefore, for any fixed $L\\ge$ and any fixed cutoff $\\tau\\geq\\cot_{1}(L)$ , then SERA will predict at least $\\begin{array}{r}{p K\\frac{n}{K}\\cdot(\\frac{n}{K}-1)/2+q K(K-1)/2+\\frac{n}{K}\\cdot\\frac{n}{K}}\\end{array}$ truly connected pairs as dis-connected. In other words, we have the false negative rate is at least $p/(2k)\\!+\\!q/2$ . If the cutoff $\\tau$ is between $\\mathrm{cut_{2}}(L)$ and $\\operatorname{cut}_{1}(L)$ , then SERA will predict at least $(1-p)K\\frac{n}{K}\\cdot(\\frac{n}{K}-1)/2$ truly dis-connected pairs as connected and predict at least $\\begin{array}{r}{q K(K-1)/2+\\frac{n}{K}\\cdot\\frac{n}{K}}\\end{array}$ truly connected pairs as dis-connected. That is, false positive rate is at least $(1-p)/(2k)$ and false negative rate is at least $(1-q)/2$ . If the cutoff $\\tau$ is less than $\\mathrm{cut_{2}}(L)$ , then SERA will predict at least $\\begin{array}{r}{(1-p)K\\frac{n}{K}\\cdot(\\frac{n}{K}-1)/2+\\stackrel{\\cdot}{(1-q)}\\!\\!K(K-1)/2+\\frac{n}{K}\\cdot\\frac{n}{K}}\\end{array}$ truly connected pairs as dis-connected. That is, false positive rate is at least $(1-p)/(2k)+(1\\stackrel{..}{-}q)/2$ . This completes the proof. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "C.3 Proof of theorem 6.1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "As mentioned in section 6, NAG actually protects against a much stronger class of adversaries. Specifically, let $\\mathbf{H}\\,=\\,\\{H^{(l)}\\}_{0\\leq l\\leq L}$ denote all the intermediate representations produced by the underlying GNN with weights $\\mathbf{\\bar{W}}=\\{W_{l}\\}_{l\\in[L]}$ . The following theorem is a stronger version of theorem 6.1: ", "page_idx": 24}, {"type": "text", "text": "Theorem C.8. Assume the adversary $\\boldsymbol{\\mathcal{A}}$ has access to $\\mathbf{H}$ and W, and outputs an estimate of graph adjacencies $\\widehat{A}=\\mathcal{A}(\\mathbf{H},\\mathbf{W})$ . Then for any graph $G$ and any such adversary $\\mathcal{A}$ , we have the lower bound: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{u\\in V,v\\in V}\\left[\\mathbb{P}\\left(\\widehat{A}_{u v}=1|A_{u v}=0\\right)+\\mathbb{P}\\left(\\widehat{A}_{u v}=0|A_{u v}=1\\right)\\right]\\geq1-\\sqrt{1-\\exp\\left(-C\\frac{\\sum_{l\\in[L]}\\|W_{l}\\|_{\\sigma p}^{2}}{\\sigma^{2}}\\right)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here the constant $C$ depends on the AGG mechanism of the GNN. In particular, for some standard GNN architectures we have: $C_{G C N}=C_{M E A N-S A G E}=C_{G I N}=1$ and $C_{G A T}=C_{M A X-S A G E}=4$ . ", "page_idx": 24}, {"type": "text", "text": "The theorem is a consequence of the following lemma: ", "page_idx": 24}, {"type": "text", "text": "Lemma C.9. Fix an arbitrary node pair $(u,v)$ . Let $\\mathbf{H}_{1}$ and $\\mathbf{H}_{0}$ be the collection of node representations generated under $A_{u v}=1$ and $A_{u v}=0,$ , respectively. It follows that the Kullback-Leibler divergence between $\\mathbf{H}_{1}$ and $\\mathbf{H}_{0}$ is bounded: ", "page_idx": 24}, {"type": "equation", "text": "$$\nD_{K L}\\left(\\mathbf{H}_{1}\\parallel\\mathbf{H}_{0}\\right)\\leq C\\frac{\\sum_{l\\in[L]}\\|W_{l}\\|_{o p}^{2}}{\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here the constant $C=1$ for (SAGE-meanpool), (GIN) and (GCN); and $C=4$ for (SAGE-maxpool) and (GAT). ", "page_idx": 24}, {"type": "text", "text": "Proof of lemma C.9. The proof is essentially a proof of R\u00e9nyi differential privacy similar to that in [36]. First we fix a single $l_{\\cdot}$ -th layer of GNN defined in (8). We rewrite (8) as: ", "page_idx": 24}, {"type": "equation", "text": "$$\nH_{v}^{(l)}=\\mathsf{A c t}\\left(\\mathsf{A G G}\\left(\\frac{W_{l}H_{u}^{(l-1)}}{\\left\\|H_{u}^{(l-1)}\\right\\|_{2}},u\\in N(v)\\cup\\{v\\}\\right)+\\epsilon\\right):=\\mathsf{A c t}\\left(\\tilde{H}_{v}^{(l-1)}+\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let the corresponding representation matrix be $H_{1}^{(l)}$ for $A_{u v}\\,=\\,1$ and $H_{0}^{(l)}$ for $A_{u v}\\,=\\,0$ for any $l\\in[L]$ . Further denote $\\widetilde{H}_{a}^{l}=\\{\\tilde{H}_{v,a}^{(l)}\\}_{v\\in V}$ as the intermediate representation defined as in (48) with $A_{u v}\\dot{=}a,a\\in\\{0,1\\}$ . T hen by standard results on R\u00e9nyi divergence [24], we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{KL}}\\left(H_{1}^{l}\\parallel H_{0}^{l}\\right)=\\frac{\\left\\|\\widetilde{H}_{1}^{(l)}-\\widetilde{H}_{0}^{(l)}\\right\\|_{2}^{2}}{2\\sigma^{2}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For some input $H^{l-1}$ . It follows that given all the other edges, the only terms that contributes to $\\left\\|\\widetilde{H}_{1}^{(l)}-\\widetilde{H}_{0}^{(l)}\\right\\|_{2}^{2}$ are $\\left\\|\\tilde{H}_{v,1}^{(l)}-\\tilde{H}_{v,0}^{(l)}\\right\\|_{2}^{2}$ $\\left\\|\\tilde{H}_{u,1}^{(l)}-\\tilde{H}_{u,0}^{(l)}\\right\\|_{2}^{2}$ . Next we give the derivation of various GNN architectures: ", "page_idx": 24}, {"type": "text", "text": "The case of (SAGE-meanpool) We let $d_{v}$ to be the degree of $v$ assuming $A_{u v}\\,=\\,1$ . Further let $g_{v}^{(l)}=\\frac{W_{l}h_{u}^{(l-1)}}{\\left\\|h_{u}^{(l-1)}\\right\\|_{2}}$ We have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\left|\\bar{H}_{u,1}^{(l)}-\\tilde{H}_{u,0}^{(l)}\\right|\\right|_{2}=\\left\\|\\frac{1}{d_{v}+1}\\left(g_{v}^{(l-1)}-\\frac{1}{d_{v}}\\displaystyle\\sum_{u\\in\\overline{{N}}(v)\\backslash\\{v\\}}g_{u}^{(l-1)}\\right)\\right\\|_{2}}&{}\\\\ {\\qquad\\le\\frac{1}{2}\\left(\\left\\|g_{v}^{(l-1)}\\right\\|_{2}+\\frac{1}{d_{v}}\\displaystyle\\sum_{u\\in\\overline{{N}}(v)\\backslash\\{v\\}}\\left\\|g_{u}^{(l-1)}\\right\\|_{2}\\right)}&{}\\\\ {\\qquad\\le\\frac{1}{2}\\left(\\|W_{l}\\|_{\\infty}+\\frac{1}{d_{v}}\\displaystyle\\sum_{u\\in\\overline{{N}}(v)\\backslash\\{v\\}}\\|W_{l}\\|_{\\infty}\\right)}&{}\\\\ {\\qquad=\\|W_{l}\\|_{\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Analogously we have   H\u02dcu(l,)1 \u2212H\u02dcu(l,)0   22 and thus $\\begin{array}{r}{\\mathrm{D}_{\\mathrm{KL}}\\left(H_{1}^{(l)}\\parallel H_{0}^{(l)}\\right)\\le\\frac{\\parallel W_{l}\\parallel_{\\mathrm{op}}^{2}}{\\sigma^{2}}}\\end{array}$ . The result follows from adaptive composition as in [24, Proposition 1]. ", "page_idx": 25}, {"type": "text", "text": "The case of (GIN) This follows by combining the preceding argument with [36, Proposition 1]. ", "page_idx": 25}, {"type": "text", "text": "The case of (GCN) This follows by combining the preceding argument with [36, Proposition 2]. ", "page_idx": 25}, {"type": "text", "text": "The case of (SAGE-maxpool) The result follows from the following fact that $\\begin{array}{r}{\\left\\|\\operatorname*{max}_{u\\in\\overline{{N}}(v)}g_{u}-\\operatorname*{max}_{u\\in\\overline{{N}}\\setminus\\{v\\}}g_{u}\\right\\|_{2}}\\end{array}$ attains its maximum when $g_{v}\\;\\;=\\;\\;-g_{u},\\forall u\\;\\;\\in\\;\\;\\overline{{N}}\\backslash\\{v\\}$ since all the $g_{u}\\mathbf{s}$ are unit vectors. ", "page_idx": 25}, {"type": "text", "text": "Proof of theorem 6.1. We view the reconstruction problem regarding $A_{u v}$ as a binary hypothesis testing problem ", "page_idx": 25}, {"type": "equation", "text": "$$\nH_{0}:A_{u v}=0\\qquad\\mathrm{v.s.}\\qquad H_{1}:A_{u v}=1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then according to hypothesis testing theory [21], we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{A}}\\left[\\mathbb{P}\\left(\\widehat{A}_{u v}=1|A_{u v}=0\\right)+\\mathbb{P}\\left(\\widehat{A}_{u v}=0|A_{u v}=1\\right)\\right]\\geq1-d_{\\mathrm{TV}}\\left(\\mathbf{H}_{1},\\mathbf{H}_{0}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we use $d_{\\mathrm{TV}}\\left(\\mathbf{H}_{1},\\mathbf{H}_{0}\\right)$ to denote the total variation distance of distributions induced by $\\mathbf{H}_{1}$ and $\\mathbf{H}_{0}$ respectively. By the Bretagnolle\u2013Huber bound [4, Theorem 1], we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nd_{\\mathrm{TV}}\\left(\\mathbf{H}_{1},\\mathbf{H}_{0}\\right)\\leq{\\sqrt{1-\\exp\\left(-\\mathrm{D}_{\\mathrm{KL}}\\left(\\mathbf{H}_{1}\\parallel\\mathbf{H}_{0}\\right)\\right)}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The result then follows by combining (55), (56) and lemma C.9. ", "page_idx": 25}, {"type": "text", "text": "D Further experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Synthetic experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Erdo\u02dds\u2013R\u00e9nyi experiments with random GNN weights The experimental setup in this section is basically the same as that in section 7.1, except that the model weights are generated by the following process: For an $L$ -layer Linear GNN, we generate the weight matrix as: ", "page_idx": 25}, {"type": "equation", "text": "$$\nW=W_{1}\\times\\cdot\\cdot\\cdot\\times W_{L}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here each $W_{l},1\\leq l\\leq10$ is a random matrix generated using the initialization method proposed in [16]. The evaluations are shown in figure 5. ", "page_idx": 25}, {"type": "text", "text": "The results exhibit a similar pattern to figure 4 where the weight matrix is set to identity. However, the attacking performance differs between the two scenarios: When the matrix $W$ is poorly conditioned (a consequence of the construction (57)), the attacking performance degrades especially when the feature dimension $d$ is not sufficiently large. ", "page_idx": 25}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/8ddc587b0184853c1dcd70a9a524c160843cc2dda6aae81329bfbf0e8e87cae0.jpg", "img_caption": ["(a) Measured in AUROC metric, darker color implies higher attacking performance "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/1d60ae4f38d6bbc6be20bf66067d8e5237d985629fba3509bde271e20db94483.jpg", "img_caption": ["(b) Measured in ERR metric, lighter color implies higher attacking performance "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 4: Attacking efficacy of SERA over sparse Erd\u02ddos\u2013R\u00e9nyi graphs, with each grid\u2019s value indicating SERA\u2019s performance measured in either AUROC (first row) or ERR (second row) metric. ", "page_idx": 26}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/ff5d3c26c7028bd8a7151ad3c86261b9530f1ffba9e54441f965a2cd4e71a634.jpg", "img_caption": ["(a) Measured in AUROC metric, darker color implies higher attacking performance "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/b10147f4bc6e850ba9b8e5f8f2378514373e477268abb9d07664b09396b4e2c5.jpg", "img_caption": ["(b) Measured in ERR metric, lighter color implies higher attacking performance "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 5: Attacking efficacy of SERA over sparse Erd\u02ddos\u2013R\u00e9nyi graphs, with each grid\u2019s value indicating SERA\u2019s performance measured in either AUROC (first row) or ERR (second row) metric. ", "page_idx": 26}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/f3fe776c04cef3ac1bfb44c63803d67f3464702b74e263be35ce04c5c1345ef6.jpg", "img_caption": ["Figure 6: Performance of SERA on SBM with varying $K$ and $p$ . All plots are based on 5 independent trials with shades indicating one standard deviation. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "LSqDcfX3xU/tmp/1e8a6ea80ed8cca8bcd9dfab0bf805804eaa4278b51d709927607e24c7136113.jpg", "table_caption": ["Table 2: Summary of dataset characteristics "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Impact of SBM structure To investigate the impact of SBM structure on the performance of SERA, we fix the GNN architecture at $L=1$ and evaluate on a graph with 100 nodes and node feature dimension $d=2048$ . Note that we choose a relatively large node feature dimension to ensure that the assumption listed in theorem 5.1 is approximately met. We vary the SBM within-group probability according to $p\\in\\{0.1,0.3,0.5,0.7\\}$ and the number of groups according to $1\\leq K\\leq20$ . The results, plotted in figure 6, suggest that in general, the attacking performance is positively correlated with the number of groups $K$ since more groups yield stronger sparsity according to the SBM generation law. This phenomenon is also in accordance with theorem 5.1. ", "page_idx": 27}, {"type": "text", "text": "D.2 A complete report of attack performance on real-world datasets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Dataset characteristics We report a brief summary of datasets used in table 2. ", "page_idx": 27}, {"type": "text", "text": "Training configurations Across all experiments we use a hidden dimension of $d=128$ with number of GNN layers adjusted to $L\\in\\{2,5\\}$ . We use Adam optimizer with learning rate 0.001 and train for 1000 epochs on the 6 relatively small datasets, and train for 5 epochs on Amazon-Products and Reddit datasets, using a stochastic training strategy that samples up to 20 neighbors per node in each layer. ", "page_idx": 27}, {"type": "text", "text": "Results We present a full list of results in table 3. A comprehensive tabulation of outcomes is furnished in Table 3. Consistent with the findings explicated in section 7, SERA demonstrates proficient reconstruction capabilities irrespective of graph properties such as homophily. Additionally, the reconstruction potency is resilient across a spectrum of GNN architectures. These results imply that prevalent GNN architectures are likely to engender models that are vulnerable to exploitation by SERA adversaries. ", "page_idx": 27}, {"type": "text", "text": "D.3 A complete report of privacy-utility assessments on Planetoid datasets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "D.3.1 Training configurations and attacking pipeline ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Network design For node $v$ with label $y_{v}$ , the prediction is defined as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{y}_{v}=\\arg\\operatorname*{max}_{c\\in\\left[C\\right]}{\\sf d e c}\\left({\\sf e n c}\\left(G,{\\bf W}\\right)\\left[v\\right]\\right)\\left[c\\right]},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use $[\\cdot]$ to denote the operation of vector index. Here the encoder enc is designed via stacking $L$ noisy GNN layers (in the sense of NAG) with aggregation mechanism $\\mathsf{A G G}\\in$ {MEAN, SUM, GCN, ATTENTION, $\\mathsf{M A X}\\}$ as defined above. Note that the encoder maps input node features into node representations of dimension $d$ , which might be larger than the number of classes $C$ . The decoder dec is a linear map that maps node representations to predictions. ", "page_idx": 27}, {"type": "table", "img_path": "LSqDcfX3xU/tmp/e9340c7090e2995fe92b2eed05c3388596cbf6cef35faa676169d0c05de1c4a3.jpg", "table_caption": ["Table 3: Performances of SERA on eight datasets measured by AUROC metric $(\\%)$ . For each setup, the results (in the form of mean $\\pm\\mathrm{std}.$ ) are obtained via 5 random trials. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Attacking paradigm The attacking procedure of SERA will be based on the node representations produced by the GNN encoder enc under a dimension of $d$ . The attack is conducted over the node representations corresponding to the test subset, i.e., the victim subgraph is the subgraph induced by the test nodes. ", "page_idx": 28}, {"type": "text", "text": "Training configurations Across all the experiments, we fix the GNN model to be of depth 2 and use full-batch training for 1000 steps(epochs) using the Adam optimizer with a learning rate of 0.001. ", "page_idx": 28}, {"type": "text", "text": "D.3.2 Unconstrained scheme ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We plot the full experimental results under the unconstrained scheme for the Cora, Citeseer and Pubmed datasets in figure 8, figure 9 and figure 10, respectively, where we evaluate the performance of SERA under both ERR and AUROC metrics. The result is consistent with those findings listed in section 7.3. ", "page_idx": 28}, {"type": "text", "text": "D.3.3 Constrained scheme ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We plot the full experimental results under the constrained scheme for the Cora, Citeseer and Pubmed datasets in figure 11, figure 12 and figure 13, respectively, where we evaluate the performance of SERA under both ERR and AUROC metrics. The result is consistent with those findings listed in section 7.3. ", "page_idx": 28}, {"type": "text", "text": "Impact of different AGG mechanisms According to figure 11, 8, 12, 9, 13, 10, the previously discovered phenomenons are present for all the 5 aggregation types. Nevertheless, the degree to which these phenomena exhibit varies with the specific type of aggregation employed. Notably, the ", "page_idx": 28}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/79e2d0c154989ae0d7946c70cbc077668d12b53d7f975e40193332260461489f.jpg", "img_caption": ["(a) Measured in AUROC metric, darker color implies higher attacking performance "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/b19be95e1da3d76d1258dd68f99dfd6e005e4a0f8f0776000a011e316281a9f7.jpg", "img_caption": ["(b) Measured in ERR metric, lighter color implies higher attacking performance "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 7: Attacking efficacy of SERA over dense SBM graphs, with each grid\u2019s value indicating SERA\u2019s performance measured in either AUROC (first row) or ERR (second row) metric. ", "page_idx": 29}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/3f892ef2a9bc666b4735914988ec3349c712696d6a033c51b774675b5c906da4.jpg", "img_caption": ["(a) GNN model performance over Cora dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/689fbd8ec99dc330f6cde0c27b5687f1a17b36bc4ef3998714d08d955f58a480.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "(b) Attacking performance of SERA over Cora dataset (measured by ERR) under 5 different aggregation types. ", "page_idx": 29}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/4fbd04b626f0ccf178368ed23bcf627bab4694bef32cb137203c2498ff109a2c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "(c) Attacking performance of SERA over Cora dataset (measured by AUROC) under 5 different aggregation types. ", "page_idx": 29}, {"type": "text", "text": "Figure 8: Privacy-utility trade-off on Cora dataset using the unconstrained training scheme. The horizontal axes measure feature dimension $d$ in $\\mathrm{log_{2}}$ scale and the vertical axes stands for performance measures All plots are based on 5 independent trials with shades indicating one standard deviation. ", "page_idx": 29}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/71671ce4521eb671c42fa7f9f2bf5d67c6170d5ddb1fd6864a94968723f19076.jpg", "img_caption": ["(a) GNN model performance over Citeseer dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/900bc4965b5e6b4e9ca819c732315459786ce61ad6d9a35ee6b2fa338f2bfc48.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "(b) Attacking performance of SERA over Citeseer dataset (measured by ERR) under 5 different aggregation types. ", "page_idx": 30}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/dc10aca4d9d4747f4296fe64766f7a80844c395336446df4c0302e80f180107a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "(c) Attacking performance of SERA over Citeseer dataset (measured by AUROC) under 5 different aggregation types. ", "page_idx": 30}, {"type": "text", "text": "Figure 9: Privacy-utility trade-off on Citeseer dataset using the unconstrained training scheme. The horizontal axes measure feature dimension $d$ in $\\mathrm{log_{2}}$ scale and the vertical axes stands for performance measures All plots are based on 5 independent trials with shades indicating one standard deviation. ", "page_idx": 30}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/ed7bf545c9c9c1a479a815b87874fd105fe802959614e678bf6e3810c8ccf13c.jpg", "img_caption": ["(a) GNN model performance over Pubmed dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/8d0707618cad1a9ef4f1aa800999350bd3b224602435fe3a92f4babeba3fdfce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "(b) Attacking performance of SERA over Pubmed dataset (measured by ERR) under 5 different aggregation types. ", "page_idx": 30}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/6428ca46511c20d5165aa3412873498c32b552964c6508c0d12d657fccf0756a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "(c) Attacking performance of SERA over Pubmed dataset (measured by AUROC) under 5 different aggregation types. ", "page_idx": 30}, {"type": "text", "text": "Figure 10: Privacy-utility trade-off on Pubmed dataset using the unconstrained training scheme. The horizontal axes measure feature dimension $d$ in $\\mathrm{log_{2}}$ scale and the vertical axes stands for performance measures All plots are based on 5 independent trials with shades indicating one standard deviation. ", "page_idx": 30}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/53858bc7d02d86e32f3e37539126f7b3227dc70640493141b16967b5274493b9.jpg", "img_caption": ["(a) GNN model performance over Cora dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/790684f8c0305c66749b21516f6273e8efd0f85becebbec1680b1f451ec63a58.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "(b) Attacking performance of SERA over Cora dataset (measured by ERR) under 5 different aggregation types. ", "page_idx": 31}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/c88d2a1ecd65e08afa7426618b6f597586df7ea2489d3c90aed02199840841fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "(c) Attacking performance of SERA over Cora dataset (measured by AUROC) under 5 different aggregation types. ", "page_idx": 31}, {"type": "text", "text": "Figure 11: Privacy-utility trade-off on Cora dataset using the constrained training scheme. The horizontal axes measure feature dimension $d$ in $\\mathrm{log_{2}}$ scale and the vertical axes stands for performance measures All plots are based on 5 independent trials with shades indicating one standard deviation. ", "page_idx": 31}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/16dd544de3e9cb5ff9e60b7ff60d2a1f558f49d2036891b76ced0438ed23e765.jpg", "img_caption": ["(a) GNN model performance over Citeseer dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/6db6c271a41d9f7a72c3a773a48eb76a52963c185c73f9870d8e3b8c7d4c5de7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "(b) Attacking performance of SERA over Citeseer dataset (measured by ERR) under 5 different aggregation types. ", "page_idx": 31}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/27cc33aa1250b0cc0bddf66f855eb0ef7960d8817353979917c978ea293abc93.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "(c) Attacking performance of SERA over Citeseer dataset (measured by AUROC) under 5 different aggregation types. ", "page_idx": 31}, {"type": "text", "text": "Figure 12: Privacy-utility trade-off on Citeseer dataset using the constrained training scheme. The horizontal axes measure feature dimension $d$ in $\\mathrm{log_{2}}$ scale and the vertical axes stands for performance measures All plots are based on 5 independent trials with shades indicating one standard deviation. ", "page_idx": 31}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/9521263c61732ca6e73652527f3c4abc5c757ddef90b827d6291f0f5dac7ac37.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "(b) Attacking performance of SERA over Pubmed dataset (measured by ERR) under 5 different aggregation types. ", "page_idx": 32}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/ce7b599f8652bf7543bf91dbfb570d1bf39427d2308572368bd1fa7d7a703a41.jpg", "img_caption": ["(c) Attacking performance of SERA over Pubmed dataset (measured by AUROC) under 5 different aggregation types. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 13: Privacy-utility trade-off on Pubmed dataset using the constrained training scheme. The horizontal axes measure feature dimension $d$ in $\\mathrm{log_{2}}$ scale and the vertical axes stands for performance measures All plots are based on 5 independent trials with shades indicating one standard deviation. ", "page_idx": 32}, {"type": "text", "text": "behaviors of ATTENTION, MEAN, and GCN pooling display similarities attributable to their shared mechanism in (weighted) average aggregation. Conversely, the efficacy of the SERA against Noisy Aggregation (NAG) when SUM and MAX pooling are utilized appears less susceptible to changes in $d$ . ", "page_idx": 32}, {"type": "text", "text": "D.4 Spectrum study of GNN solutions obtained under the unconstrained scheme ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "A closer look at GNN solutions obtained via NAG in the unconstrained scheme As SERA is just one form of attack mechanism under a weak adversary, protecting against SERA does not necessarily imply strict notions of privacy. Motivated by theorem 6.1, we conduct a spectrum study regarding the GNN solutions obtained via NAG in the unconstrained scheme. Specifically, we plot the operator norm of the weight matrices corresponding to the GNN layers across all scenarios and report them in the last column in figure 14 in appendix D.4. The results exhibit a rapidly growing trend of weights\u2019 operator norms regarding the increase of both feature dimension $d$ and noise level $\\sigma$ . For GNN models trained using noisy aggregation under large $d\\mathbf{s}$ , the corresponding bounds according to (8) become vacuous, i.e., practically zero. Additionally, these solutions may exhibit diminished robustness, as the corresponding Lipschitz constants are likely to be inadequately regulated [40]. To conclude, we have found successful empirical defenses against SERA without satisfying strict notions of privacy, suggesting that SERA has limitations as a tool for auditing private GRL training procedures. ", "page_idx": 32}, {"type": "text", "text": "D.5 Privacy-utility trade-off comparisons: NAG vs EdgeRR ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section we provide preliminary comparisons between NAG and EdgeRR regarding their privacy-utility trade-offs. In particular, for a graph with $n$ nodes, the EdgeRR with budget $\\varepsilon$ is implemented as a graph-level transform that perturbs the adjacency matrix $\\mathsf{E d g e R R}(A)\\in\\mathbb{R}^{n\\times n}$ ", "page_idx": 32}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/e669b793e9bf2bff625b19cad8fe38f1040d6aa0cbfc97d79ead285542dfcaef.jpg", "img_caption": ["(a) Spectrum study on the Cora dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/a2ec2450db8e750ef533f20624eec6fa32ea4cd0c0c924cddee8300d43ff21e1.jpg", "img_caption": ["(b) Spectrum study on the Citeseer dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/00c5a399ee49cd996e219d23d310bc01fc89ffce91b0962a9b182e9987b16753.jpg", "img_caption": ["(c) Spectrum study on the Pubmed dataset under 5 different aggregation types. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 14: Spectrum study on the Planetoid datasets under the unconstrained training scheme. The horizontal axes measure feature dimension $d$ in $\\mathrm{log_{2}}$ scale and the vertical axes measures the operator norm of the projection weights of the GNN. All plots are based on 5 independent trials with shades indicating one standard deviation. ", "page_idx": 33}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/df2e208228ed90a6d0eaf4bb9c78e641c0107a8febc95c2f417549762b0458d2.jpg", "img_caption": ["Figure 15: Comparison of privacy-utility trade-off between NAG and EdgeRR "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "with each of its entries defined as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\mathsf{E d g e R R}}(A)_{u,v}={\\left\\{\\!\\!\\begin{array}{l l}{A_{u,v}}&{{\\mathrm{With~probability~}}{\\frac{e^{\\varepsilon}}{1+e^{\\varepsilon}}}}\\\\ {1-A_{u,v}}&{{\\mathrm{Otherwise}}}\\end{array}\\!\\!\\right.}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It then follows from the theory of local differential privacy [19] that for any $(u,v)$ the perturbed entry is a $\\varepsilon$ -LDP view of the underlying true adjacency. Combining the property of max-divergence along with the proof techique in section C.3 we have the following performance lower bound of any adversary $\\boldsymbol{\\mathcal{A}}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathcal{A}}\\operatorname*{min}_{u\\in V,v\\in V}\\left[\\mathbb{P}\\left(\\widehat{A}_{u v}=1|A_{u v}=0\\right)+\\mathbb{P}\\left(\\widehat{A}_{u v}=0|A_{u v}=1\\right)\\right]\\geq1-\\sqrt{1-e^{-\\varepsilon}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "While EdgeRR has a very strong privacy protection guarantee, it is also criticized for low utility. We provide a comparison using a two-layer GCN as the backbone under the embedding dimension $d=128$ on the Planetoid datasets. The results, which can be viewed at figure 15, suggest that when considering SERA as the basis for evaluating privacy, NAG achieves a Pareto-dominant privacy-utility trade-off curve relative to EdgeRR. ", "page_idx": 34}, {"type": "text", "text": "Software and hardware infrastructures. Our framework is built upon PyTorch [28] and PyTorch Geometric [13], which are open-source software released under BSD-style 4 and MIT 5 license, respectively. All datasets used throughout experiments are publicly available. All experiments are done on a single NVIDIA A100 GPU (with 80GB memory). ", "page_idx": 34}, {"type": "text", "text": "E Discussions and Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "E.1 On the impact of depth $L$ for NAG ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "As elucidated in theorem 6.1, the privacy assurances provided by NAG are inclined to diminish as the depth parameter $L$ increases, a phenomenon attributable to the compositional nature of (differential) privacy mechanisms [12, 24]. However, this same compositional principle enables NAG to disseminate all intermediate node representations $H^{(1)},\\dots,H^{(\\bar{L})}$ while preserving the identical level of privacy as would be the case if only $H^{(L)}$ were released. Consequently, this framework permits the design of superior privacy-preserving GNN architectures by leveraging a blend of $H^{l}{}_{l\\in[L]}$ through inter-layer aggregation techniques, sometimes termed as residual connections in GRL [39]. Probing the resilience of SERA against such intricate GNN configurations poses considerable challenges and falls outside the purview of this paper. Nonetheless, preliminary evaluations have been executed to discern the ramifications of the depth parameter $L$ on the privacy-utility compromises of NAG, absent residual connections, with privacy quantified via SERA. The inquiries, undertaken using the Planetoid datasets with a fixed noise magnitude of $\\sigma=0.05$ and hidden dimensionality $d=128$ , are visually synthesized in Figure 16. Findings reveal that optimal defensive utility typically transpires at $L=1$ , while greater GNN depths, notably those with $L>5$ , tend to undermine the model\u2019s utility. Moreover, the apex of attack performance generally materializes at relatively incipient layers, a confirmation of our theoretical insights set forth in theorem 4.1. Finally, we postulate that incorporating residual connections might proffer an enhanced Pareto frontier for the model. Exploration of this hypothesis is deferred to subsequent research endeavors. ", "page_idx": 34}, {"type": "image", "img_path": "LSqDcfX3xU/tmp/b71cddd99af670f98b1472b2e5fb3653b5ca98993e2bf9aa25a5db8d528cb2fa.jpg", "img_caption": ["Figure 16: Privacy-utility trade-off on Planetoid regarding different model depths "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "E.2 Stronger adversary for dense graphs or deep encoders ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We have shown the limitations of SERA over dense SBM graphs as well as deep GNN encoders. As our analysis applies to the specific SERA adversary, it is thus of interest to ask whether there exists stronger attacking paradigms that is provably effective against dense graphs or deep GNN encoders. On the flipside, it is also valuable to understand whether the phenomenon of oversmoothing may fundamentally affect the performance of any black-box adversary. ", "page_idx": 35}, {"type": "text", "text": "E.3 Extension to more complicated victim GNN models ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The theoreical analysis presented in section 4 and section 5 is dedicated to graph neural networks employing mean aggregation without nonlinear activation functions. Prospects exist for augmenting our theoretical framework to encompass alternative aggregation schemes, such as summation [38] and attention-based aggregation [32], conditional upon the satisfaction of specific prerequisites\u2014namely, some lower bound of attention coefficients. A more challenging task lies in the extension of our analysis to graph neural networks (GNNs) that incorporate nonlinear activations between their layers. This inclusion significantly complicates the straightforward application of our non-asymptotic analysis in a cohesive end-to-end manner. Acknowledging the complexity of this endeavor, we left a thorough investigation for future research initiatives. ", "page_idx": 35}, {"type": "text", "text": "E.4 Quantifying the advantage of adversaries with more knowledge ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Despite its effectiveness, the knowledge available to SERA is rather limited. Although previous study [17] has shown empirical evidences that equipping the adversary with more capability may results in stronger attacking algorithms, theoretical explication of these enhancements has yet to be articulated. In particular, it is of interest to quantify the amplification of adversarial capacity afforded by scenarios in which the adversary is granted white-box access to the model weights or node features. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All our contributions are concisely represented in the abstract. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The limitations are discussed in section E ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The assumptions are listed in the theory statements in the main text, and proofs are presented in section C. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The experimental details are presented in the main text and in section D in the appendix. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Codes for reproducing experiments are included in the supplementary material.   \nAll the data used in this paper are open-sourced. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The details are contained in the main text and section D in the appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All plots in this paper are presented with error bars indicating one standard deviation based on 5 random trials. All numbers reported in each tables are also accompanied by $\\pm$ one standard deviation. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide hardware configurations used in our experiments in section D in the appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We conform in every aspect to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We discuss the broader impacts of this paper in section A. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 39}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All the lisence information of the codes released in this paper are detailed in section D. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}]