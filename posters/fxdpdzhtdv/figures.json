[{"figure_path": "fXDpDzHTDV/figures/figures_0_1.jpg", "caption": "Figure 1: Left: Conventional large multimodal models (LMMs) string all visual tokens into a sequence for high- and low-resolution images. Middle: Our DeepStack LMMs stack the tokens into a grid and infuse them into the first and middle transformer layers from bottom to top (\u2191\u2191\u2191) simply using a residual connection. With no architecture modification and context length increasing, our model can handle multiple times more visual tokens as inputs. Right: We apply DeepStack separately to Vicuna-7B (DeepStack-L) and CLIP ViT-L (DeepStack-V). Our models can take 4\u00d7 more visual tokens, and significantly outperforms the sequence LMM with same context length and rival the one using a much longer context, over a wide range of benchmarks.", "description": "This figure compares three different approaches for handling visual tokens in large multimodal models (LMMs).  The left panel shows the conventional method, where visual tokens are linearly concatenated into a sequence and fed into the LLM. The middle panel illustrates the DeepStack approach, which stacks visual tokens in a grid-like structure and feeds them into different transformer layers of the LLM using residual connections. The right panel presents the performance comparison of DeepStack with the conventional method, showing significant improvements. DeepStack uses fewer visual tokens and context length to surpass the sequential LMM and match the one using much longer contexts.", "section": "Abstract"}, {"figure_path": "fXDpDzHTDV/figures/figures_3_1.jpg", "caption": "Figure 2: Architecture of DeepStack. The main innovation lies in the DeepStack strategy that infuses visual tokens into different layers. Left: DeepStack for LLMs. Given an input image, we feed the tokens extracted from the low-resolution version to the input layer of LLM. Considering the 2D nature of images, we extra the neighbors from the high-resolution version and reorganize them into DeepStack, which are then fed to the consequent layers in LLMs. Right: DeepStack for ViTs. We apply similar sampling strategy but feed the visual tokens into the ViT layers of vision encoder.", "description": "This figure illustrates the DeepStack architecture applied to both LLMs and Vision Transformers (ViTs).  For LLMs, a low-resolution image is processed, and its tokens are fed to the input layer. High-resolution tokens are extracted from the same image, organized into a stack, and infused into subsequent LLM layers via residual connections. The ViT version uses a similar strategy, feeding the visual tokens into various ViT layers instead of LLM layers.  This demonstrates how DeepStack integrates multi-resolution visual information across multiple layers for improved multimodal understanding.", "section": "3 DeepStack"}, {"figure_path": "fXDpDzHTDV/figures/figures_4_1.jpg", "caption": "Figure 2: Architecture of DeepStack. The main innovation lies in the DeepStack strategy that infuses visual tokens into different layers. Left: DeepStack for LLMs. Given an input image, we feed the tokens extracted from the low-resolution version to the input layer of LLM. Considering the 2D nature of images, we extra the neighbors from the high-resolution version and reorganize them into DeepStack, which are then fed to the consequent layers in LLMs. Right: DeepStack for ViTs. We apply similar sampling strategy but feed the visual tokens into the ViT layers of vision encoder.", "description": "This figure illustrates the DeepStack architecture. It shows how visual tokens are processed in two different ways: one for Large Language Models (LLMs) and one for Vision Transformers (ViTs). In both cases, DeepStack enhances the input visual tokens by dividing image feature extraction into two streams: a global-view stream and a high-resolution stream.  The global-view stream provides an overview of the image, while the high-resolution stream adds fine-grained details by stacking dilated high-resolution image features across different layers. This dual-stream approach increases the efficiency and improves understanding of fine-grained details.", "section": "3 DeepStack"}, {"figure_path": "fXDpDzHTDV/figures/figures_5_1.jpg", "caption": "Figure 2: Architecture of DeepStack. The main innovation lies in the DeepStack strategy that infuses visual tokens into different layers. Left: DeepStack for LLMs. Given an input image, we feed the tokens extracted from the low-resolution version to the input layer of LLM. Considering the 2D nature of images, we extract the neighbors from the high-resolution version and reorganize them into DeepStack, which are then fed to the consequent layers in LLMs. Right: DeepStack for ViTs. We apply similar sampling strategy but feed the visual tokens into the ViT layers of vision encoder.", "description": "This figure illustrates the DeepStack architecture for both LLMs and Vision Transformers (ViTs).  For LLMs, it shows how low-resolution image tokens are fed into the initial layer, while high-resolution tokens, organized in a layered structure (DeepStack), are infused into subsequent layers via residual connections.  The ViT version shows a similar approach, but with visual tokens fed into various ViT encoder layers.  The figure highlights the core concept of DeepStack: integrating high-resolution visual details throughout the model without increasing the context length.", "section": "3 DeepStack"}, {"figure_path": "fXDpDzHTDV/figures/figures_7_1.jpg", "caption": "Figure 3: Analysis on using LLM layers to process visual tokens. (a) We insert the visual tokens into different starting layers and initialize the correspondence input embeddings as zero; (b) We fix the first layer to insert global visual tokens and ablation on the interval s for stacking high-resolution tokens; (c) We ablation number of layers for token stacking.", "description": "The figure shows the ablation studies on different aspects of DeepStack. Specifically, it investigates the impact of (a) inserting visual tokens into different starting layers of LLMs, (b) varying the interval between layers for stacking high-resolution tokens, and (c) changing the number of layers used for stacking.  The results demonstrate the effect of these variations on the model's overall performance.", "section": "4 Experiments"}, {"figure_path": "fXDpDzHTDV/figures/figures_8_1.jpg", "caption": "Figure 5: Visualization. Both LLaVA-1.5 and DeepStack use 576 visual context length for a fair comparison. Top: We mark the area corresponding to each question with a red circle. DeepStack can well answer the questions which need high-resolution and fine-grained understanding. Bottom: DeepStack demonstrates a more accurate visual understanding in detailed visual captioning.", "description": "This figure shows a comparison between LLaVA-1.5 and DeepStack on several visual question answering tasks.  The top part highlights specific examples where DeepStack outperforms LLaVA-1.5 by correctly identifying details requiring high resolution and fine-grained understanding. The bottom section presents a radar chart summarizing the performance of both models across multiple benchmarks, demonstrating DeepStack's superior accuracy in detailed visual captioning.", "section": "4 Experiments"}, {"figure_path": "fXDpDzHTDV/figures/figures_8_2.jpg", "caption": "Figure 2: Architecture of DeepStack. The main innovation lies in the DeepStack strategy that infuses visual tokens into different layers. Left: DeepStack for LLMs. Given an input image, we feed the tokens extracted from the low-resolution version to the input layer of LLM. Considering the 2D nature of images, we extract the neighbors from the high-resolution version and reorganize them into DeepStack, which are then fed to the consequent layers in LLMs. Right: DeepStack for ViTs. We apply similar sampling strategy but feed the visual tokens into the ViT layers of vision encoder.", "description": "This figure illustrates the DeepStack architecture for both LLMs and Vision Transformers (ViTs).  The left side shows how DeepStack integrates low and high-resolution visual tokens into different layers of an LLM.  High-resolution tokens are extracted from the image and arranged in a grid structure that is sequentially fed to subsequent transformer layers. The right side demonstrates a similar approach for ViTs, but with visual tokens fed into the ViT layers.", "section": "3 DeepStack"}, {"figure_path": "fXDpDzHTDV/figures/figures_16_1.jpg", "caption": "Figure 1: Left: Conventional large multimodal models (LMMs) string all visual tokens into a sequence for high- and low-resolution images. Middle: Our DeepStack LMMs stack the tokens into a grid and infuse them into the first and middle transformer layers from bottom to top (\u2191\u2191\u2191) simply using a residual connection. With no architecture modification and context length increasing, our model can handle multiple times more visual tokens as inputs. Right: We apply DeepStack separately to Vicuna-7B (DeepStack-L) and CLIP ViT-L (DeepStack-V). Our models can take 4\u00d7 more visual tokens, and significantly outperforms the sequence LMM with same context length and rival the one using a much longer context, over a wide range of benchmarks.", "description": "This figure illustrates the core concept of DeepStack. The left panel shows traditional LMMs processing visual tokens as a sequence, while the middle panel introduces DeepStack, which stacks tokens into a grid and feeds them into different transformer layers. The right panel presents a comparison of DeepStack's performance against conventional LMMs and highlights its ability to handle significantly more visual tokens and achieve superior results.", "section": "DeepStack"}, {"figure_path": "fXDpDzHTDV/figures/figures_16_2.jpg", "caption": "Figure 1: Left: Conventional large multimodal models (LMMs) string all visual tokens into a sequence for high- and low-resolution images. Middle: Our DeepStack LMMs stack the tokens into a grid and infuse them into the first and middle transformer layers from bottom to top (\u2191\u2191\u2191) simply using a residual connection. With no architecture modification and context length increasing, our model can handle multiple times more visual tokens as inputs. Right: We apply DeepStack separately to Vicuna-7B (DeepStack-L) and CLIP ViT-L (DeepStack-V). Our models can take 4\u00d7 more visual tokens, and significantly outperforms the sequence LMM with same context length and rival the one using a much longer context, over a wide range of benchmarks.", "description": "This figure illustrates the core concept of DeepStack, a novel architecture for Large Multimodal Models (LMMs).  The left panel shows the traditional approach of linearly processing visual tokens. The middle panel demonstrates DeepStack's method of stacking visual tokens into a grid and integrating them into multiple transformer layers. The right panel presents a comparison of DeepStack's performance against traditional methods, highlighting its ability to handle significantly more visual tokens while maintaining comparable or superior accuracy.", "section": "Introduction"}, {"figure_path": "fXDpDzHTDV/figures/figures_16_3.jpg", "caption": "Figure 1: Left: Conventional large multimodal models (LMMs) string all visual tokens into a sequence for high- and low-resolution images. Middle: Our DeepStack LMMs stack the tokens into a grid and infuse them into the first and middle transformer layers from bottom to top (\u2191\u2191\u2191) simply using a residual connection. With no architecture modification and context length increasing, our model can handle multiple times more visual tokens as inputs. Right: We apply DeepStack separately to Vicuna-7B (DeepStack-L) and CLIP ViT-L (DeepStack-V). Our models can take 4\u00d7 more visual tokens, and significantly outperforms the sequence LMM with same context length and rival the one using a much longer context, over a wide range of benchmarks.", "description": "This figure illustrates the core idea of DeepStack, a novel architecture for Large Multimodal Models (LMMs).  The left panel shows the traditional approach of processing visual tokens as a sequence, limiting the number of tokens that can be handled. The middle panel demonstrates DeepStack's method of stacking visual tokens into a grid and injecting them into multiple transformer layers, significantly increasing capacity without modifying the context length.  The right panel provides a comparison of DeepStack's performance against traditional methods on various benchmarks, showing significant improvements.", "section": "Abstract"}]