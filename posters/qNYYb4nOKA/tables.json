[{"figure_path": "qNYYb4nOKA/tables/tables_4_1.jpg", "caption": "Table 1: Are we lost in the middle? After finishing the extraction of a whole document (consisting of fifteen pieces), we re-extract the information from each of its pieces. Columns 1-15 then compare the re-extracted information with the information that was extracted from the same piece of the text before. The pieces in the middle of the document contain more duplicated entities then those at the beginning and the end.", "description": "This table presents the results of an experiment designed to test the 'Lost in the Middle' phenomenon in LLMs.  A long document is split into 15 pieces and processed. Then, each piece is re-extracted and compared to the initial extraction for the same part. The redundancy score, representing the proportion of identical entities (by name), is shown for each of the 15 pieces, revealing how significantly the redundancy increases for the middle sections of the document, indicating the LLM's decreased attention to this portion compared to the beginning and end.", "section": "4.2 Lost in the middle"}, {"figure_path": "qNYYb4nOKA/tables/tables_5_1.jpg", "caption": "Table 2: Quality of extraction depends on a number of calls to LLM. The first iterated call is the most beneficial one. From some point (bold) the scores stagnate or even deteriorate. All scores have values between 0 and 1, the arrows indicate whether lower (\u2193) or higher (\u2191) values are desired.", "description": "This table shows how the quality of information extraction, measured by several metrics, changes with the number of iterations of LLM calls.  It demonstrates that while additional iterations initially improve quality, there's a point of diminishing returns where further iterations lead to stagnation or even a decrease in performance.  The metrics used assess semantic similarity, relevance, redundancy, bias avoidance, and completeness of the extracted information.", "section": "5.1 Iterated LLM calls"}, {"figure_path": "qNYYb4nOKA/tables/tables_6_1.jpg", "caption": "Table 3: Quality of extraction \u2013 MINEA score \u2013 total and per schema type. Entity types are grouped into five classes - 1. three most frequent schema.org types in the documents; 2. med-bio-chem entities, somewhat interchangeable types; 3. best distinguishable types; 4. custom (non Schema.org) types; 5. Schema.org types related to documents, but not stated in the chosen schema. Note: an entity is assumed to be extracted if it is contained within the extracted information - often its type can be misclassified (Project-Product-OpportunityArea, Substance-Thing-BioChemEntity) or sometimes it can be mentioned indirectly (Organization is related to a Person by property 'works for').", "description": "This table presents the MINEA scores for different entity types, categorized into five classes based on frequency and inter-changeability.  It shows the extraction accuracy for each type and the number of entities used for evaluation. The note clarifies that an entity is considered extracted if present in the extracted information, even if the type might be misclassified or indirectly mentioned.", "section": "5.2.2 Multiple infused needle extraction accuracy"}, {"figure_path": "qNYYb4nOKA/tables/tables_7_1.jpg", "caption": "Table 4: Toy example: fulfillment of the conditions. The text enriched by two needles from Figure 2 was extracted into the form shown in Figure 3.", "description": "This table shows whether the conditions for needle identification are met for each needle in the toy example. The conditions include whether the needle name perfectly matches the extracted entity name, whether the needle name is found in the extracted information, whether at least half of the needle keywords match the extracted entity keywords, and whether an LLM identifies the needle in the extracted information. The table demonstrates the various ways that needles can be identified and how these different criteria affect the overall success of needle extraction.", "section": "5.2.2 Multiple infused needle extraction accuracy"}, {"figure_path": "qNYYb4nOKA/tables/tables_8_1.jpg", "caption": "Table 3: Quality of extraction \u2013 MINEA score \u2013 total and per schema type. Entity types are grouped into five classes - 1. three most frequent schema.org types in the documents; 2. med-bio-chem entities, somewhat interchangeable types; 3. best distinguishable types; 4. custom (non Schema.org) types; 5. Schema.org types related to documents, but not stated in the chosen schema. Note: an entity is assumed to be extracted if it is contained within the extracted information - often its type can be misclassified (Project-Product-OpportunityArea, Substance-Thing-BioChemEntity) or sometimes it can be mentioned indirectly (Organization is related to a Person by property 'works for').", "description": "This table presents the MINEA scores obtained for different entity types categorized into five classes.  It shows the extraction accuracy for each class and the number of entities used for evaluation.  The classes are based on frequency in the documents and the level of distinction between the types.  The note explains how entities are considered extracted, handling potential misclassifications.", "section": "5.2.2 Multiple infused needle extraction accuracy"}, {"figure_path": "qNYYb4nOKA/tables/tables_8_2.jpg", "caption": "Table 6: LLMs comparison using MINEA score.", "description": "This table compares the performance of three different LLMs (gpt-3.5-turbo, gpt-4-turbo, and gpt-4o) on the MINEA score.  The MINEA score is a metric used to evaluate the quality of information extraction. Lower scores indicate lower quality extraction, whereas higher scores indicate better quality extraction. The results show that gpt-4o outperforms the other two LLMs, followed by gpt-4-turbo, and then gpt-3.5-turbo.", "section": "5.2.4 Model comparison"}]