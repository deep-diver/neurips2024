[{"type": "text", "text": "Assessing the quality of information extraction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Advances in large language models have notably enhanced the efficiency of infor  \n2 mation extraction from unstructured and semi-structured data sources. As these   \n3 technologies become integral to various applications, establishing an objective   \n4 measure for the quality of information extraction becomes imperative. However,   \n5 the scarcity of labeled data presents significant challenges to this endeavor. In   \n6 this paper, we introduce an automatic framework to assess the quality of the in  \n7 formation extraction/retrieval and its completeness. The framework focuses on   \n8 information extraction in the form of entity and its properties. We discuss how to   \n9 handle the input/output size limitations of the large language models and analyze   \n10 their performance when extracting the information. In particular, we introduce   \n11 scores to evaluate the quality of the extraction and provide an extensive discussion   \n12 on how to interpret them. ", "page_idx": 0}, {"type": "text", "text": "13 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "14 In the domain of natural language processing (NLP), information extraction (IE) stands as a critical   \n15 task, transforming unstructured or semi-structured data into a structured format conducive to indexing,   \n16 exploration, and further analysis. The increasing amount of data across digital platforms underscores   \n17 the urgency for sophisticated IE techniques that can parse through volumes of information with   \n18 precision. An extensive survey about IE is provided by [1], where the authors highlight the complexity   \n19 of processing and analyzing text to derive meaningful information, given the heterogeneity and volume   \n20 of such data.   \n21 Large language models (LLMs) have revolutionized IE by introducing generative methods for   \n22 structuring knowledge from text. LLMs excel across diverse domains without extensive task-specific   \n23 training. A survey by [9] details the progress of LLMs on IE tasks. Here, the authors address specific   \n24 aspects of information extraction, including entity recognition, relation extraction, event detection,   \n25 and universal IE. They review the existing models and their efficiency on a comprehensive collection   \n26 of annotated benchmarks. Nonetheless, the challenge of quantitatively assessing the quality and   \n2 completeness of extracted information persists, particularly in the absence of labeled datasets for   \n28 benchmarking. Before conducting the experiments introduced in this paper, we perform IE on a vast   \n29 corpus of business documents utilizing LLMs. While the extraction process is beyond the scope of   \n30 this paper, some details about the extraction are given in Section 3.   \n31 To measure the quality of extraction, we propose an evaluation framework that relies on artificially   \n32 generated complex information which is infused into the document to test the efficiency of LLMs in   \n33 IE tasks. This paper introduces an iterative extraction process and a novel score, MINEA (Multiple   \n34 Infused Needle Extraction Accuracy), to address the critical need for objective quality assessment   \n35 measures. By inserting artificial information (\"needles\") into the data, the proposed method creates   \n36 a synthetic ground truth for evaluation, enabling the measurement of extraction quality in various   \n37 specific domains even without manually labeled data. The empirical analysis demonstrates the   \n38 utility of MINEA for evaluating LLM-based IE in scenarios where ground truth is unavailable. By   \n39 automating the quality assessment of information extraction, the framework could reduce the need   \n40 for manual review by experts, saving time and resources and thus enhance the efficiency and accuracy   \n41 of information extraction from large volumes of unstructured data.   \n42 The paper is organized as follows: Section 2 presents a related work that inspired us when developing   \n43 our IE quality assessment method; Section 3 sketch a way in which structured information is obtained   \n44 using LLMs; Section 4 deals with shortcomings arising when treating long contexts by LLMs; finally   \n45 Section 5 introduces the novel method to access the quality of IE and provide the reader with practical   \n46 tips; Sections 4 and 5 are supplemented by numerical studies. The data used in these studies are an   \n47 internal set of documents related to a business case in the healthcare industry. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "48 2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "49 A common practice in many specialized IE tasks is that well-trained experts review what was extracted   \n50 and provide ground truth as done in [5]. Such an approach is relatively reliable, however, it is manual   \n51 and very time-consuming.   \n52 In [4] they suggest summary score without reference (SUSWIR), a score to evaluate the quality of   \n53 text summaries without the need for human annotations. The SUSWIR score can be used for IE tasks   \n54 where the extracted information is viewed as a compression of original data. The score compares   \n55 the original text with its summary. From its nature, it is very useful when comparing the outputs   \n56 of extraction tasks among themselves, i.e., the best extraction/summary has the highest score value.   \n57 On the other hand, its ability to provide an objective absolute evaluation of a single extraction is   \n58 disadvantaged because the desirable output is not known.   \n59 Recently, an effort to eliminate the requirement for human involvement relies on LLMs. These prove   \n60 themselves as highly cost-effective data creators, either by labeling unlabeled data or generating data   \n61 given the labels, see [7]. Therefore they may substitute human experts providing the ground truth by   \n62 doing their work in an automatic way.   \n63 Needle In A Haystack (NIAH)1 evaluation is a tool designed to evaluate the performance of LLMs in   \n64 retrieval across different sizes of context. Short targeted information, the \u2018needle\u2019, is inserted into a   \n65 large, more complex text body, the \u2018haystack\u2019. The goal is to test an LLM\u2019s ability to find and make   \n66 use of this piece of information.   \n67 Our method builds on LLMs acting as data creators, but instead of annotating the complete data, it   \n68 only automatizes the process of creating the needle. I.e., given an original text, an LLM generates the   \n69 needle. The needle then substitutes the ground truth. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "70 3 Capturing the structure ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 The form of needles depends on a form of data, on structure capturing the information and on the   \n72 task being solved. The needles can be short paragraphs of text, account records, graph nodes as you   \n73 extract information from continuous text, table, graph, respectively. The structured arrangement of   \n74 information is beneficial for consecutive processing and analysis. It helps to highlight relationships   \n75 among distinct information pieces. There are countless ways to impose a structure on unstructured   \n76 data in order to capture the relevant information. To demonstrate our methodology for measuring the   \n77 quality of information extraction, we specify a particular structure and tailor the needles to it. ", "page_idx": 1}, {"type": "text", "text": "78 3.1 Schema ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "79 To impose a structure on the data, we adopt the idea of schema markup [3] which is used to   \n80 communicate the content of a web page to the search tool. The schema markup is in the form of   \n81 structured data and can be viewed as a compression of the essential information. The structure   \n82 is defined by Schema.org2 vocabulary which is a set of entity types, each associated with a set of   \n83 properties and hierarchically arranged. Figure 1 shows an example of structured information inspired   \n84 by Schema.org. It describes three entities of types \u2018Insight\u2019, \u2018Person\u2019 and \u2018Organization\u2019. Each   \n85 type has its own set of properties, e.g., an entity of type \u2018Person\u2019 is described by \u2018type\u2019, \u2018name\u2019,   \n86 \u2018birthDate\u2019, \u2018worksFor\u2019, and \u2018jobTitle\u2019. In other words, each entity is a set of key-value pairs, e.g.,   \n87 \u2018name\u2019 is the key and \u2018AI Enthusiast\u2019 is the value. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\"@type\": \"Insight\"   \n\"name\": \"Information exctraction tested by Needle in a Haystack test\",   \n\"description\": \"A short targeted information pieces, the needles', are inserted to a large, more complex text body, the \u2018haystack'. The quality of information extraction task is measured by ratio of succesfully extracted needles.\",   \n\"keywords\": \"information extraction (9), large language models (8), quality evaluation (10), needle in a hayastack (8), named entity recogniction (7), schema.org (6)\",   \n\"author\": { \"@type\": \"Person\", \"name\": \"AI Enthusiast\"\" \"birthDate\": \" \"worksFor\": { \"@type\": \"organization\", \"name\": \"Creative Dock\" \"description\": \"Creative Dock builds and scales disruptive tech companies, as a startup and corporate venture builder. The company provides end-to-end venture-building, from idea to building and scaling.\", \"keywords\": \"tech company (8), venture builder (9), AI (7), startup growth (8)\" } \"jobTitle\": \"Data Scientist\" ", "page_idx": 2}, {"type": "text", "text": "88 Similarly, we extract and compress the relevant information contained in data using an LLM.   \n89 Schema.org presents a clear basis for the categorization of various entities contained in data. In the   \n90 rest of the paper, by schema we mean a predetermined set of types, such as $\\{$ {\u2018Person\u2019, \u2018Project\u2019,   \n91 \u2018Product\u2019, \u2018Legislation\u2019, \u2018Event\u2019, \u2018OpportunityArea\u2019, \u2018Insight\u2019, \u2018Substance\u2019, \u2018Thing\u2019, \u2018BioChemEn  \n92 tity\u2019, \u2018MedicalCondition\u2019}, together with their properties. The schema is set at the beginning and   \n93 the information to be extracted depends on it. Therefore the schema has to be tailored to a particular   \n94 scope of the (proprietary) knowledge and application. If a more complex or uncommon entity needs   \n95 to be captured, it is natural and very easy to extend the set of core types by more detailed descriptive   \n96 and custom vocabulary. E.g., \u2018Insight\u2019 and \u2018OpportunityArea\u2019 are not native Schema.org types, but   \n97 we will use them in our study. The usage of suitably tailored schema is beneficial for specialized   \n98 applications since it narrows the information to the relevant core and hence potentially improves the   \n99 overall performance. On the other hand, the usage of schemata is not restrictive as the scope can be   \n00 always extended by using a broader set of types. ", "page_idx": 2}, {"type": "text", "text": "101 3.2 The role of LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "102 LLMs are rather effective in the creation of structured data, cf. [9]. Using dedicated prompts, we get   \n103 a structured text file describing entities found in the documents and matching types of predefined   \n104 schema. The predefined schema (types and properties) is given to an LLM within the prompt. The   \n105 LLM is asked to analyse the document, identify an information relevant to the mentioned types of   \n106 entities and populate the schema with this information. It is asked to be attentive to nested entities,   \n107 maintain consistency and uniqueness of extracted entities. Indeed, LLM is not prohibited from   \n108 extracting entities whose types do not appear in the predefined schema. It is worthy to note, that   \n109 LLMs are known to inherit biases present in their training data. If not carefully managed, these biases   \n110 could lead to unfair or inaccurate information extraction, impacting decision-making processes.   \n111 Besides the information extraction task, LLMs can be used to suggest suitable Schema.org types for   \n112 a particular document. An example together with a prompt is shown in Appendix B1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "113 4 Length aspects ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "114 When focusing on the quality of IE performed by an LLM, several limitations that LLM presents   \n115 in terms of the length of data to be extracted from must be considered. Each LLM has a maximal   \n116 content limit it can process, both on the input and the output. The limit on the output is typically   \n117 much more strict. When trying to use the maximal possible input another issue may appear \u2013 the   \n118 Lost in the middle phenomenon [8] says that the ability of LLMs to retrieve information from a long   \n119 context declines and that the attention focuses on the beginning and the end of the context while it   \n120 tends to attenuate information in the middle.   \n121 To demonstrate shortcomings arising from these limitations numerically we use gpt-4-1106-preview   \n122 model.3 The model is limited by 4095 tokens on the output and by 128000 tokens on the input   \n123 (context window limit). The following sections present two major LLM limitations we have to   \n124 consider before performing IE, namely length restrictions in Section 4.1 and Lost in the middle   \n125 problem in Section 4.2. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "126 4.1 Length restrictions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 Long data are difficult to process because of the restrictions posed by the maximum amount of: ", "page_idx": 3}, {"type": "text", "text": "128 (O) output tokens: The restriction on output tokens means that there is some maximal length of   \n129 data from which most entities can be efficiently extracted. If the length of the text exceeds   \n130 this maximum, there would be no tokens for extra entities.   \n131 (I) input tokens: Maximal size of context window (input) prohibits the extraction of data   \n132 exceeding the specific token limit.   \n133 Another difficulty regarding the output is the tendency of LLMs to generate rather brief responses   \n134 which do not use the allowed maximal number of tokens. This unwillingness of models can be   \n135 circumvented by prompting. Even so, the limited number of output tokens is typically too low and   \n136 prevents effective extraction from long texts.   \n137 With a more sophisticated approach, the restriction (O) becomes irrelevant and only the restriction (I)   \n138 will apply. The issue imposed by (O) is overcome by splitting the source document into smaller pieces   \n139 which are extracted independently. A significant drawback is that the extracted information can be   \n140 easily duplicated \u2013 extracted independently from multiple text pieces. Iterating the calls to the LLM   \n141 with instruction to continue with already started extraction, i.e., continuing with the extraction in a   \n142 single thread, helps to extract more information and avoid duplication. As we insist on continuation,   \n143 more and more information is added and the extraction is more thorough, at least to some point \u2013 this   \n144 will be addressed in detail in Section 5.1. Further, a lower number of duplicates is found due to the   \n145 extraction history, i.e., all information extracted until present, which is kept within the thread.   \n146 The combination of both improvements \u2013 text splitting and iterated calls, has proven itself to perform   \n147 the best. We split the document into distinct text pieces which we extract sequentially. Extraction   \n148 from each text piece is carried out by several iterated LLM calls while taking into account the   \n149 extraction history from previously extracted text pieces. Once the sum of the lengths of the text   \n150 pieces and the extraction history exceeds the context window limit, i.e., restriction (I) applies, a new   \n151 independent extraction starts. A single structured output, per document or once (I) is applied, is   \n152 created by appending all entities identified from each text piece. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "153 4.2 Lost in the middle ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "154 In the case of long documents, whose extraction consumes almost the whole context window,   \n155 LLMs are giving more inconsistent results and we can observe a presence of the Lost in the middle   \n156 phenomenon, see [8]. We extract information from several long documents from our business case   \n157 which are each split into 15 pieces and its processing consumes almost the whole context window.   \n158 We add the sixteenth piece identical to one of the fifteen that are already extracted and measure a   \n159 redundancy score, for details see Appendix A. Each column of Table 1 then states the redundancy of   \n160 the newly extracted information with the information that was already extracted from the same piece   \n161 of the text before. The table presents mean values per four distinct documents. We can notice that   \n162 for the parts \u2019in the middle\u2019 the proportion of redundantly extracted entities (entities with the same   \n163 \u2019name\u2019 attribute) is higher than for those at the beginning and the end. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "qNYYb4nOKA/tmp/1f1592021dacb8ffcdf440a1393f4cd3f2d46e6f8a86bb5acde633dd36fb9d27.jpg", "table_caption": ["Table 1: Are we lost in the middle? After finishing the extraction of a whole document (consisting of fifteen pieces), we re-extract the information from each of its pieces. Columns 1-15 then compare the re-extracted information with the information that was extracted from the same piece of the text before. The pieces in the middle of the document contain more duplicated entities then those at the beginning and the end. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "164 5 Quality of extraction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "165 Once the information is extracted from data into a structured form defined by the chosen schema,   \n166 e.g., Figure 1, the quality of such extraction is important to evaluate. In practice, it is very rare to be   \n167 equipped with ground truth and its human generation requires vast expertise in the scope of data and a   \n168 ridiculous amount of time. Therefore we adopt methods from [4]. They examine semantic similarity,   \n169 relevance, redundancy, and bias and compound these into a single score called SUSWIR, for details   \n170 see Appendix A. The score and its subparts are very useful when comparing distinct extractions   \n171 among themselves, e.g., we can use it to find an optimal number of iterated LLM calls. Unfortunately,   \n72 the score does not represent an absolute way of evaluation. It does not provide a complete insight into   \n173 the task \u2013 some information ( $\\mathbf{\\bar{\\rho}}=\\mathbf{\\bar{\\rho}}$ entities) can be missing, misclassified or their properties not filled   \n174 in correctly. To come up with a robust and general solution we generalize the NIAH test, which is   \n175 commonly used to measure the ability of LLMs to process long documents, cf. [6]. ", "page_idx": 4}, {"type": "text", "text": "176 5.1 Iterated LLM calls ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "177 Since the first LLM extraction is typically not exhaustive, iterating the extraction process helps with   \n178 the completeness of extraction. To improve the quality of extraction, we ask LLM to process the   \n179 document again and search for other entities which were not extracted yet. A question arises: What is   \n180 the optimal number of iterations? It is desirable to stop when additional LLM call will return no or   \n181 only a few new entities. The answer however depends heavily on the text being extracted and on the   \n182 chosen schema. Below, we present a small comparative study regarding the contribution of iterated   \n183 extraction to its quality. We interpret the extracted structured data, e.g., Figure 3, as a summary of   \n184 the original text document. To measure the quality of the summary we adopt the scores from [4] (a   \n185 convex combination of these scores creates the overall SUSWIR metric), namely semantic similarity,   \n186 relevance, and redundancy avoidance. We use a modified bias avoidance score from [4] and add two   \n187 new scores, relevance spread, and incompleteness score. See Appendix A for more details.   \n188 Consider document which length is approximately 12k chars. Table 2 compares the content of the   \n189 document with extracted information created iteratively by succeeding LLM calls. Each iteration   \n190 enriches the extracted information, but the benefit decreases. From the third iteration, i.e., after   \n191 four LLM calls, the majority of scores in Table 2 are either getting worse or stagnating (the arrows   \n192 following the score name indicate the direction in which the score improves). It is obvious that shorter   \n193 and longer text will require less or more iterations to extract majority of information without reducing   \n194 its semantic and factually relevant meaning, respectively. Further, the risk that the LLM will suffer   \n195 from hallucinations increases as we observe a growth of bias. In the rest of the paper we use three   \n196 iterations to extract documents of approximate length 12k chars within all extractions (if not stated   \n197 otherwise). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "198 5.2 Test the quality ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "199 This section introduces a robust and versatile score to objectively measure the quality of IE. Assuming   \n200 the structure is imposed by some schema, see Section 3.1, we would like to measure the IE quality as   \n201 a portion of successfully extracted entities, i.e., the accuracy of name entity recognition (NER) task   \n202 taking into account even the context captured by entity properties. Unfortunately, such an experiment   \n203 is unfeasible without labeled data. As a consequence, it is unfeasible in many specialized tasks   \n204 because of the absence of suitable labeled data unseen by LLM models. This can be the case with   \n205 very recent datasets as well as proprietary datasets. To overcome this issue we use inspiration by   \n206 NIAH test to build up an automatic and general procedure to access the quality of IE tasks. ", "page_idx": 4}, {"type": "table", "img_path": "qNYYb4nOKA/tmp/17c4bb117067a11a89d8c44a3be326cf382bd17d1507a40fe9d1295719ee1b8b.jpg", "table_caption": ["Table 2: Quality of extraction depends on a number of calls to LLM. The first iterated call is the most beneficial one. From some point (bold) the scores stagnate or even deteriorate. All scores have values between 0 and 1, the arrows indicate whether lower $\\left(\\downarrow\\right)$ or higher (\u2191) values are desired. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "207 5.2.1 Needles ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "208 A \u2018needle\u2019 in our context represents an entity. It is created according to the chosen schema, i.e.,   \n209 a list of types we want to extract from the document. We use an LLM to generate a short paragraph   \n210 introducing a new original (not appearing in the document) entity, but still relevant to the scope of the   \n211 document, for an example see Figure 2, and for more details on generation process see Appendix B2.   \n212 This artificial paragraph, the needle, is then placed into the document body at random (taking into   \n213 the account natural units within the text as sentences, paragraphs, etc. if applicable). Moreover,   \n214 the needle is accompanied with several properties, namely we assign to the needle a name, short   \n215 description and keywords, see Figure 2. This additional properties are assigned to the needle by the   \n216 LLM. ", "page_idx": 5}, {"type": "text", "text": "217 5.2.2 Multiple infused needle extraction accuracy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "218 To measure the quality of extraction we propose a multiple infused needle extraction accuracy   \n219 (MINEA) score. Its computation combines the approach of NIAH evaluation and NER task. We   \n220 scatter several needles at random over the text document body (such that the inserted needles fill 10   \n221 to $30\\%$ of the enriched text) and measure how many of them were successfully extracted. Since we   \n222 know what exactly was inserted, we know what should be extracted. Then we can objectively measure   \n223 the quality of extraction on these new entities and moreover, we can compare extracted information   \n224 from the document with and without needles. Table 3 shows extraction accuracy \u2013 MINEA score   \n225 \u2013 total and per schema type \u2013 measured on a vast corpus of business documents with predefined   \n226 schema consisting of types \u2018BioChemEntity\u2019, \u2018Event\u2019, \u2018Insight\u2019, \u2018Legislation\u2019, \u2018MedicalCondition\u2019,   \n227 \u2018OpportunityArea\u2019, \u2018Person\u2019, \u2018Product\u2019, \u2018Project\u2019, \u2018Substance\u2019 and \u2018Thing\u2019. ", "page_idx": 5}, {"type": "text", "text": "228 5.2.3 Identification of needles ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "229 Matching the generated needles with extracted entities imposes a challenge and mostly depends   \n230 on the formulation of needles. If the needles are too complex or too vague, the straightforward   \n231 identification changes into a serious problem. For this reason, we equip the needles with additional   \n232 properties which are then used to compare the needles with extracted entities and to decide whether   \n233 the needles were extracted successfully or not. ", "page_idx": 5}, {"type": "text", "text": "234 We present several alternative ways how to measure whether the extraction of a needle is successful: ", "page_idx": 5}, {"type": "text", "text": "235 n an entity with a name perfectly matching the needle name is found;   \n236 ns the needle name is found among the extracted information; ", "page_idx": 5}, {"type": "text", "text": "\"@type\": \"Event\", ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "\"needie\": \"The AI Clan Meeting on Thursday aims to bring together a diverse team for collaboration and knowledge sharing. It is a hybrid event, with team members gathering in person at the office while also connecting online via video conferencing. The meeting will feature discussions on recent AI projects, updates on upcoming initiatives, and collaborative brainstorming sessions.\"   \n\"name\": \"AI clan Meeting\",   \n\"description\": \"The aim of hybrid event AI Clan Meeting happening on Thursday is to foster collaboration and engagement among the team. The agenda includes project discussions, updates on upcoming initiatives, and brainstorming sessions.   \n\"keywords\": \"A1 (9), AI projects (9), project updates (7), team collaboration (6), knowledge sharing (7), hybrid event (4)\",   \n\"@type\": \"Product\",   \n\"needle\": \"Graph Index (GRIX) is a cutting-edge retrieval-augmented generation model that is based on a knowledge graph. A graph representation of the knowledge base enhances effectiveness and ability to answer complex user queries. It is end-toend solution for question-answering task dealing with the knowledge graph construction from and the retrievai of a relevant information from it.\",   \n\"name\": \"Graph Index\"   \n\"description\": \"GRIx is an innovative retrieval-augmented generation model based on a knowledge graph. A great focus is laid on proper extraction of information from data, its composition into the graph and retrieval of a relevant subgraph.\",   \n\"keywords\": \"retrieval-augmented generation (9), knowledge graph (8), information extraction (6), product innovation (7), graph index (8), question-answering (8)\" ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Figure 2: Toy example: two needles, highlighted by blue color, accompanied by additional information described by \u2018name\u2019, \u2018description\u2019, and \u2018keywords\u2019. ", "page_idx": 6}, {"type": "text", "text": "Table 3: Quality of extraction \u2013 MINEA score \u2013 total and per schema type. Entity types are grouped into five classes - 1. three most frequent schema.org types in the documents; 2. med-bio-chem entities, somewhat interchangeable types; 3. best distinguishable types; 4. custom (non Schema.org) types; 5. Schema.org types related to documents, but not stated in the chosen schema. Note: an entity is assumed to be extracted if it is contained within the extracted information - often its type can be misclassified (Project-Product-OpportunityArea, Substance-Thing-BioChemEntity) or sometimes it can be mentioned indirectly (Organization is related to a Person by property \u2019works for\u2019). ", "page_idx": 6}, {"type": "table", "img_path": "qNYYb4nOKA/tmp/d54a6fb5cad4ee2c54c1ec2fb9439414f61aaebc3077476fa6cb58a9745bac98.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "k an entity with some number of keywords perfectly matching the needle keywords is found, the number is determined by the threshold parameter determining the percentage of keywords to be matched; ", "page_idx": 6}, {"type": "text", "text": "\"@type\": \"Insight\", \"name\": \"Information exctraction tested by Needle in a Haystack test\", }, \"@type\": \"Event\" \"name\": \"AI Meeting\", \"description\": \"A hybrid event bringing together a diverse team for collaboration and knowledge sharing.\", \"keywords\": \"AI Clan Meeting (9), collaboration (8), knowledge sharing (8), hybrid event (7), team gathering (7), video conferencing (6)\" }, { \"@type\": \"Product\"\" \"name\": \"GRIX\", \"description\": \"Cutting-edge retrieval-augmented generation model based on a knowledge graph\", \"keywords\": \"GRIx (10), retrieval-augmented generation (9), knowledge graph (10), question-answering (8), graph construction (6), information extraction (7)\" Figure 3: Toy example: extracted information from the data infused by needles from Figure 2. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Toy example: fulflilment of the conditions. The text enriched by two needles from Figure 2 was extracted into the form shown in Figure 3. ", "page_idx": 7}, {"type": "table", "img_path": "qNYYb4nOKA/tmp/1950b3e562f3e2ba0259259b5c162ef28f122f3dfc1afcb2ed636783ff46cd12.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "241 Note that other conditions can be constructed, e.g., based on the short description instead of keywords,   \n242 etc. Table 4 shows whether the conditions are fulfilled in the example illustrated by Figures 2 and   \n243 3. Namely, the condition $\\mathbf{n}$ is not satisfied (\u2018AI Clan Meeting\u2019 $\\neq\\mathbf{\\langleAI}$ Meeting\u2019, \u2018Graph Index\u2019 $\\ne$   \n244 \u2018GRIX\u2019). Condition ns is satisfied only for needle representing an entity of type \u2018Event\u2019 (\u2018AI Clan   \n245 Meeting\u2019 can be found in the extracted information). There are three keywords out of the six assigned   \n246 to the needle representing the entity of type \u2018Event\u2019 which match the keywords of an extracted entity,   \n247 hence $\\mathbf{k0.5}$ is, and k0.6, k0.7 are not satisfied (there is an entity within the extracted information   \n248 with $50\\%$ of keywords being the same as the keywords of the needle). In the case of the second   \n249 needle, there are four such keywords, therefore $\\mathbf{k0.5}$ and $\\mathbf{k0.6}$ are satisfied. Finally, both needles are   \n250 identified within the extracted information by an LLM.   \n251 Table 5 shows scores (ratios of successfully extracted entities) based on the above criteria in the case   \n252 of our business documents. The types of inserted needles are \u2019BioChemEntity\u2019, \u2019Country\u2019, \u2019Event\u2019,   \n253 \u2019Insight\u2019, \u2019Legislation\u2019, \u2019Person\u2019, \u2019Product\u2019, \u2019Project\u2019 and \u2019Substance\u2019. Matching the needle and   \n254 entity name usually does not perform well if the name is prone to modification (e.g., person name   \n255 with and without title), or if the entity is easy to be misclassified (an entity of type \u2018Country\u2019 was   \n256 often extracted as \u2018Place\u2019 whose name did not match the country name). Searching for a needle name   \n257 in all extracted information gives very accurate results if the entities are well characterized by their   \n258 name (compare for example types \u2018Person\u2019 and \u2019Legislation\u2019 with type \u2019Insight\u2019 where the name is   \n259 not a natural attribute). Matching the needle and entity keywords depends on the threshold parameter   \n260 \u2013 with a lower proportion of keywords that have to match the score value increases and the reliability   \n261 of the entity identification decreases. An LLM performs well the entity identification and it is an   \n262 important criterion in the case of more creative types such as \u2018Insight\u2019. Finally, the MINEA score for   \n263 each type is taken as the maximum of the scores (the values are highlighted). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "qNYYb4nOKA/tmp/2237a42d8cd68ceb9c092635fca3d76b033e8d93e42fd1a5cafcf056336d1291.jpg", "table_caption": ["Table 5: The decision about the success of needle extraction can be made based on several criteria: comparing the corresponding needle and entity properties (columns n and $\\mathbf{k0.5-k0.7}$ compare name and keywords, respectively), full-text search (column ns search for the needle name in extracted information), comparison of needles and entities using LLM (column llm). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "264 5.2.4 Model comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "265 MINEA score can be used to compare the performance of distinct LLMs, see Table 6. A corpus   \n266 of documents is infused by needles representing entities whose types match the schema introduced   \n267 in Section 5.2.2. Three OpenAI LLMs4are used to extract a relevant information under the same   \n268 setting (the same model parameters such as temperature, the same number of iterations, the same   \n269 prompting, etc.). Model gpt-3.5-turbo is outperformed by gpt-4-turbo by almost $15\\%$ and gpt-4-turbo   \n270 is outperformed by $g p t{-}4\\sigma$ model by another $12\\%$ . Note that the achieved accuracy is lower than   \n271 presented in Table 3, since only one iteration instead of three was performed in order to reduce the   \n272 computational time. ", "page_idx": 8}, {"type": "table", "img_path": "qNYYb4nOKA/tmp/f8f42e5615fd00d72e7f8514f3ff19a7601d30d9dd5a324d4cd169f464c7cfca.jpg", "table_caption": ["Table 6: LLMs comparison using MINEA score. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "273 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "274 In this paper, we focused on quality evaluation of information extraction (IE) performed by large   \n275 language models (LLMs). First, we delved into the technical limitations of LLMs complicating the   \n276 extraction of information from a long context. To extract reasonable information from data it is   \n277 needed to take into the account features such as context window limits, iterated extractions, extraction   \n278 history recording and Lost in the middle phenomenon. Once the extraction is performed, assessing its   \n279 quality is essential. However in many customized tasks, a truly objective method is missing, because   \n280 of the lack of labeled data ftiting the scope of the application. The versatile method presented in this   \n281 paper overcomes the issue by adjustment of the data by insertion of an artificial information, a needle,   \n282 into it. The artificial information created to this purpose is application and data-specific, but the   \n283 method itself is applicable generally across the field of IE. By controlling the generation process of   \n284 the needles, we created a synthetic ground truth that enables us to absolutely measure the extraction   \n285 quality even when no labeled data is available. We introduced a MINEA score to measure the quality   \n286 of extraction. The key part is a decision rule on whether a needle was successfully extracted or not.   \n287 MINEA possibly combines several decision rules into one final score. Our empirical analysis of the   \n288 MINEA score on a specialized dataset demonstrated its utility for evaluation of LLM-based IE tasks   \n289 when ground truth is unavailable. ", "page_idx": 8}, {"type": "text", "text": "290 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "291 [1] Kiran Adnan and Rehan Akbar. Limitations of information extraction methods and techniques for   \n292 heterogeneous unstructured big data. International Journal of Engineering Business Management,   \n293 11:1847979019890771, 2019.   \n294 [2] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with   \n295 improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic   \n296 and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372,   \n297 2005.   \n298 [3] Matthew Edgar. Schema and structured data markup. In Tech SEO Guide: A Reference Guide for   \n299 Developers and Marketers Involved in Technical SEO, pages 67\u201378. Springer, 2023.   \n300 [4] Abdullah Al Foysal and Ronald B\u00f6ck. Who Needs External References?\u2014Text Summarization   \n301 Evaluation Using Original Documents. AI, 4(4):970\u2013995, 2023.   \n302 [5] Neil Jethani, Simon Jones, Nicholas Genes, Vincent J Major, Ian S Jaffe, Anthony B Cardillo,   \n303 Noah Heilenbach, Nadia Fazal Ali, Luke J Bonanni, Andrew J Clayburn, et al. Evaluating   \n304 ChatGPT in Information Extraction: A Case Study of Extracting Cognitive Exam Dates and   \n305 Scores. 2023.   \n306 [6] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail   \n307 Burtsev. In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss.   \n308 arXiv preprint arXiv:2402.10790v2, 2024.   \n309 [7] Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W White, and Sujay Kumar Jauhar. Making large   \n310 language models better data creators. arXiv preprint arXiv:2310.20111, 2023.   \n311 [8] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,   \n312 and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint   \n313 arXiv:2307.03172, 2023.   \n314 [9] Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng   \n315 Zheng, and Enhong Chen. Large language models for generative information extraction: A   \n316 survey. arXiv preprint arXiv:2312.17617, 2023. ", "page_idx": 9}, {"type": "text", "text": "317 Appendix A ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "318 To measure the quality of the summary we adopt the methods from [4]: semantic similarity combines   \n319 latent semantic similarity and cosine similarity; relevance is measured using METEOR score, see   \n320 [2], without chunk penalty; redundancy avoidance compares extracted entities among themselves   \n321 using a threshold parameter \u2013 entities with a higher cosine similarity are assumed to be redundant;   \n322 redundancy avoidance can be focused on a single particular property of entities (we use \u2019name\u2019 as   \n323 this pivotal property).   \n324 We modify the bias avoidance score from [4] to be J\u2217(A, B) = |A|\u2229B|B |, where A represents the   \n325 entities in the original text document and we normalize by a number of entities that were extracted,   \n326 $|B|$ . The score controls how much information in the structured flie is not present in the original text,   \n327 i.e., a potential hallucination of an LLM.   \n328 We add two new scores: the relevance spread is the standard deviation of relevance over the text   \n329 pieces to which the document is split and normalized by the mean value, its higher values indicate   \n330 that the extraction from distinct text pieces is unbalanced; the incompleteness score just measures the   \n331 proportion of entities with incomplete information (at least one property value missing or unfilled),   \n332 e.g., the entity \u2018AI Enthusiast\u2019 in Figure 1 has an unknown \u2018birthDate\u2019. ", "page_idx": 9}, {"type": "text", "text": "333 Appendix B ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "334 Except for the IE task, LLMs are used in several subtasks within the paper, namely to determine   \n335 schema types appearing in the document, to create a suitable needles fitting contextually to the   \n336 document and to identify whether a needle was extracted or not. In the following, we provide the   \n337 reader with prompts and examples of these subtasks. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "338 B1 Discovering a schema ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "339 Figure 4 shows a prompt to obtain the Schema.org types from the attached text \u2013 Wikipedia article   \n340 about IE.5 An LLM is asked to assign relevance to the types to distinguish the most important ones.   \n341 Figure 5 shows the entity types that were deduced from the text, together with their relevance and   \n342 reasoning for why they were chosen. The most relevant types are those directly mentioned \u2013 \u2018Article\u2019,   \n343 as the webpage content itself is represented as an article, \u2018SoftwareApplication\u2019, and \u2018WebSite\u2019 (all   \n344 with maximal relevance). The least relevant identified types are generic \u2013 \u2018Thing\u2019, as a parent type of   \n345 many directly mentioned types, and \u2018LearningResource\u2019, as a categorization of the article style. ", "page_idx": 10}, {"type": "text", "text": "SYSTEM   \nYou are skilled proof reader. Analyze the attached text and entities it contains carefully and determine all schema.org types being present. Output just a list of all TYPES found. Provide rating of relevance (1-10) for each identified type and url to website where the type is defined. \\*Attached text:   \nhttps://en.wikipedia.org/wiki/Information_extraction ", "page_idx": 10}, {"type": "text", "text": "Figure 4: Prompt to determine a possible suitable schema from a given text \u2013 Wikipedia article about IE. ", "page_idx": 10}, {"type": "image", "img_path": "qNYYb4nOKA/tmp/5073d1d1a3b1f43527f0c06a2ac75c68bcfb9c03137379dc8da5cfb1c96e7156.jpg", "img_caption": [], "img_footnote": [], "page_idx": 10}, {"type": "text", "text": "Figure 5: Schema.org types found by an LLM within Wikipedia article about IE. ", "page_idx": 10}, {"type": "text", "text": "346 B2 Creating needles ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "347 A needle, i.e., a text paragraph ftiting thematically to the document, but being new and unique to it, is   \n348 generated by an LLM using the prompt in Figure 6. The prompt specifies the type of entity that the   \n349 needle should represent. Multiple needles of the same type can be obtained easily within a single   \n350 LLM call.   \n351 Figure 7 shows ten needles representing the entities of type \u2018Person\u2019 generated based on a Wikipedia   \n352 article about IE. In the next step properties such as a name, description and keywords can be generated   \n353 by an LLM. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "qNYYb4nOKA/tmp/0f788632f2b19fd8f9324df0427ead645f9bec7cc958e0215b65267bfffd2f4e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "Figure 6: Prompt to generate needles. Given a Wikipedia article about IE, the LLM is asked to think out 10 relevant persons. ", "page_idx": 11}, {"type": "text", "text": "1. Arnold Jennings is arenowned expert in the field of Information Extraction and has published several groundbreaking research papers on the topic in prestigious journals. ", "page_idx": 12}, {"type": "text", "text": "2.In 1995,Professor ElenaMartinezpioneered anewapproachtoNamed EntityRecognition,revolutionizing the way entities are identified in natural language processing.   \n3.Dr.JamesCarter,aleading figure inthefield of Information Extraction,willbe deliveringakeynotespeech at the upcoming International Conference on NLP and Machine Learning.   \n4.VanessaRodriguez,arising star inthe world of InformationExtraction,recently developedacutting-edge algorithm that significantly enhances the accuracy of event extraction from texts.   \n5.The collaborationbetween Dr.Michael Nguyen and Dr.SophiaLee resulted in the creation of a powerful tool for Relationship Extraction,which hasbeen widely acclaimed in theNLP community.   \n6.ProfessorSamuel Brownreceived the prestigious Excellence inInformation ExtractionAward for his   \ngroundbreaking research on CoreferenceResolution incomplextextual data.   \n7. In her latest research project,Dr.Lily Martinez successfully applied Information Extraction techniques to analyze and extract valuable insights froma large dataset of social mediaposts.   \n8.The innovative work of Dr. OliverWalker inTemplate Filling haspaved the wayformore eficient extractionof structured information from unstructured documents.   \n9.Christina Lee,aleading researcher in the field of Named Entity Recognition,has developed a state-of-the-art system that can accurately detect and classify various types of entities in textual data.   \n10.ProfessorNathanTurner's contributions toSemi-structured Information Extraction have been instrumental in advancing the capabilities of automated systems inprocessing and interpreting diverse types of data sources. ", "page_idx": 12}, {"type": "text", "text": "Figure 7: Needles generated by an LLM and representing ten entities of type \u2018Person\u2019. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "355 B3 Identifying needles ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "356 The quality of extraction is evaluated based on the proportion of successfully extracted needles. An   \n357 LLM can be used to decide whether the needle was extracted or not using the prompt presented in   \n358 Figure 8. ", "page_idx": 13}, {"type": "image", "img_path": "qNYYb4nOKA/tmp/47435c35e2cf2103e01440d29d9e8e11fc26a7dabbb8d74565ea799b6136674a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 8: Prompt to identify whether the needles were extracted or not. ", "page_idx": 13}, {"type": "text", "text": "359 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "64 Justification: The abstract and introduction clearly state the development of an automatic   \n65 framework to assess the quality of information extraction (IE), which is the main contribution   \n66 of the paper. This is supported by the introduction of the MINEA score and the discussion   \n67 on handling input/output size limitations of large language models (LLMs). ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The paper discusses the limitations related to the complexity or vagueness of the needles, dependence on the chosen schema and criteria for needle identification (Section 5). Further the paper focuses on limitations of LMMs in IE tasks such as input/output size constraints, lost in the middle phenomenon, bias and hallucinations (Section 4). ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper does not include theoretical results that require formal proofs. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "428 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The paper provides detailed descriptions of the experimental setup, including the use of LLMs for IE and the creation of synthetic ground truth data. This is detailed in Sections 3 and 5. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 15}, {"type": "text", "text": "465 In the case of closed-source models, it may be that access to the model is limited in   \n466 some way (e.g., to registered users), but it should be possible for other researchers   \n467 to have some path to reproducing or verifying the results.   \n468 5. Open access to data and code   \n469 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n470 tions to faithfully reproduce the main experimental results, as described in supplemental   \n471 material?   \n472 Answer: [No]   \n473 Justification: The paper does not provide open access to the data and code due to the   \n474 proprietary nature of the business documents used in the experiments. However, it provides   \n475 detailed instructions on how to replicate the methodology.   \n476 Guidelines:   \n477 \u2022 The answer NA means that paper does not include experiments requiring code.   \n478 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n479 public/guides/CodeSubmissionPolicy) for more details.   \n480 \u2022 While we encourage the release of code and data, we understand that this might not be   \n8 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n482 including code, unless this is central to the contribution (e.g., for a new open-source   \n483 benchmark).   \n484 \u2022 The instructions should contain the exact command and environment needed to run to   \n485 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n486 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n487 \u2022 The authors should provide instructions on data access and preparation, including how   \n488 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n489 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n490 proposed method and baselines. If only a subset of experiments are reproducible, they   \n491 should state which ones are omitted from the script and why.   \n492 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n493 versions (if applicable).   \n494 \u2022 Providing as much information as possible in supplemental material (appended to the   \n495 paper) is recommended, but including URLs to data and code is permitted.   \n496 6. Experimental Setting/Details   \n497 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n498 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n499 results?   \n500 Answer: [Yes]   \n501 Justification: The paper specifies the use of LLMs, the schema used for structuring data, and   \n502 the process of generating needles for evaluation. These details are provided in Sections 3, 4   \n503 and 5.   \n504 Guidelines:   \n505 \u2022 The answer NA means that the paper does not include experiments.   \n506 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n507 that is necessary to appreciate the results and make sense of them.   \n508 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n509 material.   \n510 7. Experiment Statistical Significance   \n511 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n512 information about the statistical significance of the experiments?   \n513 Answer: [No]   \n514 Justification: The paper does not include experiments that require statistical significance   \n515 testing or error bars. The experiments in Sections 4 and 5 present mean values of reasonably   \n516 large samples. The experiments are not repeated, each of them is carried once on a set of   \n517 distinct documents containing a large amount of entities. In Section 5, a vast set of unique   \n518 needles (with repeating types) is used to infuse the documents.   \n519 Guidelines:   \n520 \u2022 The answer NA means that the paper does not include experiments.   \n521 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n522 dence intervals, or statistical significance tests, at least for the experiments that support   \n523 the main claims of the paper.   \n524 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n525 example, train/test split, initialization, random drawing of some parameter, or overall   \n526 run with given experimental conditions).   \n527 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n528 call to a library function, bootstrap, etc.)   \n529 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n530 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n531 of the mean.   \n532 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n533 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n534 of Normality of errors is not verified.   \n535 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n536 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n537 error rates).   \n538 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n539 they were calculated and reference the corresponding figures or tables in the text.   \n540 8. Experiments Compute Resources   \n541 Question: For each experiment, does the paper provide sufficient information on the com  \n542 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n543 the experiments?   \n544 Answer: [No]   \n545 Justification: The paper does not provide detailed information on the compute resources used   \n546 for the experiments. The requirements such as time of execution are determined especially   \n547 by used LLMs.   \n548 Guidelines:   \n549 \u2022 The answer NA means that the paper does not include experiments.   \n550 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n551 or cloud provider, including relevant memory and storage.   \n552 \u2022 The paper should provide the amount of compute required for each of the individual   \n553 experimental runs as well as estimate the total compute.   \n554 \u2022 The paper should disclose whether the full research project required more compute   \n555 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n556 didn\u2019t make it into the paper).   \n557 9. Code Of Ethics   \n558 Question: Does the research conducted in the paper conform, in every respect, with the   \n559 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n560 Answer: [Yes]   \n561 Justification: The research adheres to the NeurIPS Code of Ethics, ensuring that the methods   \n562 and data used do not violate ethical guidelines. The proprietary data used is handled with   \n563 confidentiality and integrity.   \n564 Guidelines:   \n565 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n566 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n567 deviation from the Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper is primarily concerned with the technical methodology, the introduction of the MINEA score, and the empirical analysis of the framework\u2019s performance. The potential positive impacts are mentioned in Introduction: by automating the quality assessment of information extraction, the framework could reduce the need for manual review by experts, saving time and resources and thus enhance the efficiency and accuracy of information extraction from large volumes of unstructured data. The negative aspects of using LLMs for IE tasks such as inherited bias and potential hallucinations are mentioned especially in Sections 4.2 (Lost in the middle problem) and 5.1 (bias avoidance score). Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "605 11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "06 Question: Does the paper describe safeguards that have been put in place for responsible   \n07 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n08 image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not release any data or models that pose a high risk for misuse. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "622 12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "623 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n624 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n625 properly respected?   \n626 Answer: [Yes]   \n627 Justification: All existing models are properly referenced and credit to their creators is given.   \n628 These are either LLMs or metrics such as SUSWIR and METEOR (Section 5 and Appendix   \n629 A).   \n630 Guidelines:   \n631 \u2022 The answer NA means that the paper does not use existing assets.   \n632 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n633 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n634 URL.   \n635 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n636 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n637 service of that source should be provided.   \n638 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n639 package should be provided. For popular datasets, paperswithcode.com/datasets   \n640 has curated licenses for some datasets. Their licensing guide can help determine the   \n641 license of a dataset.   \n642 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n643 the derived asset (if it has changed) should be provided.   \n644 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n645 the asset\u2019s creators.   \n646 13. New Assets   \n647 Question: Are new assets introduced in the paper well documented and is the documentation   \n648 provided alongside the assets?   \n649 Answer: [NA]   \n650 Justification: The paper does not introduce new assets that require documentation.   \n651 Guidelines:   \n652 \u2022 The answer NA means that the paper does not release new assets.   \n653 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n654 submissions via structured templates. This includes details about training, license,   \n655 limitations, etc.   \n656 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n657 asset is used.   \n658 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n659 create an anonymized URL or include an anonymized zip file.   \n660 14. Crowdsourcing and Research with Human Subjects   \n661 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n662 include the full text of instructions given to participants and screenshots, if applicable, as   \n663 well as details about compensation (if any)?   \n664 Answer: [NA]   \n665 Justification: The paper does not involve crowdsourcing or research with human subjects.   \n666 Guidelines:   \n667 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n668 human subjects.   \n669 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n670 tion of the paper involves human subjects, then as much detail as possible should be   \n671 included in the main paper.   \n672 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n673 or other labor should be paid at least the minimum wage in the country of the data   \n674 collector.   \n675 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n676 Subjects   \n677 Question: Does the paper describe potential risks incurred by study participants, whether   \n678 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n679 approvals (or an equivalent approval/review based on the requirements of your country or   \n680 institution) were obtained?   \n681 Answer: [NA]   \n682 Justification: The paper does not involve research with human subjects that would require   \n683 IRB approval.   \n684 Guidelines:   \n685 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n686 human subjects.   \n687 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n688 may be required for any human subjects research. If you obtained IRB approval, you   \n689 should clearly state this in the paper.   \n690 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n691 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n692 guidelines for their institution.   \n693 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n694 applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}]