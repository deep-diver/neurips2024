{"references": [{"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-02", "reason": "This paper introduced the foundational concept of knowledge distillation using Kullback-Leibler divergence, which the current paper builds upon and improves."}, {"fullname_first_author": "Borui Zhao", "paper_title": "Decoupled knowledge distillation", "publication_date": "2022-00-00", "reason": "This paper proposed decoupled knowledge distillation, addressing limitations of traditional methods that the current work further refines."}, {"fullname_first_author": "Zhendong Yang", "paper_title": "From knowledge distillation to self-knowledge distillation: A unified approach with normalized loss and customized soft labels", "publication_date": "2023-00-00", "reason": "This work provides a strong baseline for comparison, addressing limitations of KL-divergence based knowledge distillation."}, {"fullname_first_author": "Martin Arjovsky", "paper_title": "Wasserstein generative adversarial networks", "publication_date": "2017-00-00", "reason": "This paper introduced the Wasserstein distance, a key concept used in the current paper's proposed method."}, {"fullname_first_author": "Gabriel Peyr\u00e9", "paper_title": "Computational optimal transport: With applications to data science", "publication_date": "2019-00-00", "reason": "This work provides theoretical background and context for using Wasserstein distance in the proposed knowledge distillation method."}]}