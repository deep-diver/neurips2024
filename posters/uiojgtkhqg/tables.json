[{"figure_path": "UIOjGTKHQG/tables/tables_6_1.jpg", "caption": "Table 1: Performance Comparison on different tasks under Few-shot Settings based on LLaMA2-7B. Sh. Lla. PPL refers to Shortened-LLaMA applying PPL metric. Sh. Lla. Tay. refers to Shortened-LLaMA applying Taylor metric. Ada-Inf. refers to Ada-Infer. For convenience, we mark the best performance in red and the lowest computational cost in blue.", "description": "This table compares the performance of D-LLM against other state-of-the-art methods on various tasks using the LLaMA2-7B model.  It shows the perplexity (PPL) or accuracy (Acc.) achieved by each method, along with the fraction of FLOPs (floating-point operations) used relative to a baseline. Lower PPL and higher Acc. indicate better performance, while lower FLOPs indicate better efficiency. The best performance and lowest computational cost are highlighted in red and blue, respectively.  Abbreviations used are explained in the caption.", "section": "4.2 Performance Comparison"}, {"figure_path": "UIOjGTKHQG/tables/tables_7_1.jpg", "caption": "Table 1: Performance Comparison on different tasks under Few-shot Settings based on LLaMA2-7B. Sh. Lla. PPL refers to Shortened-LLaMA applying PPL metric. Sh. Lla. Tay. refers to Shortened-LLaMA applying Taylor metric. Ada-Inf. refers to Ada-Infer. For convenience, we mark the best performance in red and the lowest computational cost in blue.", "description": "This table compares the performance of D-LLM against other methods (MoD, Shortened-LLaMA with PPL and Taylor metrics, Ada-Infer) on various tasks (Q&A, Math, Common Sense Reasoning) using the LLaMA2-7B model.  It shows the perplexity (PPL) or accuracy (Acc.) and FLOPs (floating-point operations) for each method across multiple datasets. The best performance and lowest computational cost are highlighted for each dataset.", "section": "4.2 Performance Comparison"}, {"figure_path": "UIOjGTKHQG/tables/tables_7_2.jpg", "caption": "Table 3: The parameter analysis on numbers of reserved tokens not participate in dynamic inference.", "description": "This table presents the results of ablation studies conducted to determine the optimal number of reserved tokens (m) in the D-LLMs' KV-cache eviction strategy.  The experiments measure the impact of varying the number of reserved tokens (m = 0, 1, 2, 4, 8) on perplexity (PPL) for the SAMSum task and accuracy (Acc.) for the SIQA task, while maintaining a consistent computational cost (FLOPs). The optimal value of m is selected based on the best performance (lowest PPL and highest Acc.) observed for each task.", "section": "4.3 Quantitative Analysis"}, {"figure_path": "UIOjGTKHQG/tables/tables_14_1.jpg", "caption": "Table 4: The accuracy against computational cost on MaWPS, OBQA, and SAMSum datasets.", "description": "This table presents the accuracy and perplexity results for three different benchmarks (MaWPS, OBQA, and SAMSum) under various levels of computational cost (expressed as a fraction of FLOPs). It shows the trade-off between model accuracy/perplexity and computational resource usage, demonstrating the effectiveness of the D-LLMs approach in achieving high accuracy/low perplexity with reduced computational cost.  The results illustrate that D-LLMs can significantly reduce computational cost while maintaining competitive performance compared to a baseline model.", "section": "4.2 Performance Comparison"}, {"figure_path": "UIOjGTKHQG/tables/tables_15_1.jpg", "caption": "Table 5: The accuracy and computational cost against hyper-parameter \u03b1 on MaWPS dataset.", "description": "This table shows the accuracy and computational cost (represented by FLOPs) obtained by D-LLMs on the MaWPS dataset under different values of hyperparameter \u03b1 (0.1, 1, and 10).  The target acceleration ratio (\u03a9) is varied from 0.5 to 0.9.  It demonstrates the effect of this hyperparameter on the model's performance and resource consumption.", "section": "4.3 Quantitative Analysis"}, {"figure_path": "UIOjGTKHQG/tables/tables_15_2.jpg", "caption": "Table 6: Overhead information of decision modules in D-LLMs.", "description": "This table presents a comparison of the overhead introduced by the decision modules in D-LLMs against the base LLM. It shows that the decision modules are parameter-efficient, adding only a small percentage to the total parameters and FLOPs of the LLM.  The table also details the increases in training and inference memory usage, as well as the additional latency introduced per block.", "section": "4.2 Performance Comparison"}]