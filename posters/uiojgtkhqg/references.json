{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces LoRA, a parameter-efficient finetuning method used extensively in D-LLMs, crucial for its compatibility with pre-trained LLMs without retraining."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "LLaMA serves as the foundational large language model upon which D-LLMs are based and whose architecture is dynamically adapted."}, {"fullname_first_author": "Coleman Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper provides a benchmark dataset used to evaluate D-LLM's performance on mathematical reasoning tasks, demonstrating its effectiveness across various tasks."}, {"fullname_first_author": "Sotiris Anagnostidis", "paper_title": "Dynamic context pruning for efficient and interpretable autoregressive transformers", "publication_date": "2024-00-00", "reason": "This paper presents a similar approach of dynamically adjusting the network during inference, providing a relevant comparison and context for evaluating D-LLM's novelty."}, {"fullname_first_author": "David Raposo", "paper_title": "Mixture-of-depths: Dynamically allocating compute in transformer-based language models", "publication_date": "2024-04-02", "reason": "This paper explores a related concept of dynamic inference in transformers, offering insights and comparative analysis for the D-LLM approach."}]}