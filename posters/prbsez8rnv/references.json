{"references": [{"fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-10-27", "reason": "This paper introduces DINO, a self-supervised learning method that is used as a backbone in the experiments and is directly compared to the proposed method."}, {"fullname_first_author": "Ting Chen", "paper_title": "Exploring simple siamese representation learning", "publication_date": "2021-06-14", "reason": "This paper introduces SimCLR, another self-supervised learning method used in the experiments and compared against the proposed method."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-05-01", "reason": "This paper introduces Vision Transformers (ViT), the backbone architecture used in the experiments."}, {"fullname_first_author": "Ivana Balazevic", "paper_title": "Towards in-context scene understanding", "publication_date": "2024-01-01", "reason": "This paper introduces a method for retrieval-based in-context scene understanding, which is directly compared to the proposed method in the experiments."}, {"fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "publication_date": "2020-06-14", "reason": "This paper introduces MoCo, a self-supervised learning method used as a backbone in the experiments and is compared to the proposed method."}]}