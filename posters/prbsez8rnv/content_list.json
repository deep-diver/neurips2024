[{"type": "text", "text": "No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Walter Simoncini1,\u2217 Spyros Gidaris2 Andrei Bursuc2 Yuki M. Asano1 ", "page_idx": 0}, {"type": "text", "text": "1QUVA Lab, University of Amsterdam; 2valeo.ai, Paris, France ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces FUNGI, Features from UNsupervised GradIents, a method to enhance the features of transformer encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These gradients are projected to a lower dimension and then concatenated with the model\u2019s output embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, FUNGI features provide consistent performance improvements over the embeddings. We also show that using FUNGI features can benefit linear classification, clustering and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by $+17\\%$ for semantic segmentation \u2013 without any training. Code is available at https://github.com/WalterSimoncini/fungivision. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The $\\mathbf{k}$ -nearest neighbor algorithm (kNN) (Fix, 1985) is a fundamental non-parametric machine learning tool, and can be scaled to datasets with billion of examples thanks to advances in quantization (Jegou et al., 2010; Guo et al., 2020) and efficient GPU implementations (Johnson et al., 2019). This simple and versatile algorithm has shown potential in multiple applications well before deep neural networks became relevant (Efros & Leung, 1999; Hays & Efros, 2008; Torralba et al., 2008). Its recent applications include fast and robust image classification with Vision Transformers (Caron et al., 2021; Chen & He, 2021), unlabeled data selection (Yalniz et al., 2019), relevant text-retrieval (Lewis et al., 2020), and visual in-context learning (Balazevic et al., 2024), where a context of data samples with their annotations (e.g., a semantic segmentation map) are used to make dense predictions. ", "page_idx": 0}, {"type": "text", "text": "Devising powerful and expressive features for recognition and image understanding has a long history in computer vision. Feature engineering strategies range from simple local features (Lowe, 2004; Dalal & Triggs, 2005; Van De Sande et al., 2009) extracting gradient, boundary or color information, to various mid-level (Boureau et al., 2010) or global pooling (Oliva & Torralba, 2001; Sivic & Zisserman, 2003; J\u00e9gou et al., 2010) techniques. It is also possible to couple off-the-shelf pretrained backbones as feature extractors with such pooling strategies (Gong et al., 2014; Kulkarni et al., 2015; Gidaris et al., 2020) to improve performances. While these approaches demonstrate the utility of using a neural network\u2019s learned embedding space, they still require specific expertise and tuning for each backbone and task with only limited guidance from the data itself. ", "page_idx": 0}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/76f2e8f11aac24785a32db8313f6f90e472342e117e173426788a90bfce50e91.jpg", "img_caption": ["Figure 1: Gradient-augmented features: given a pretrained backbone $f_{\\theta^{*}}$ and its embeddings, we apply a family of SSL losses, extract their gradients, and project and concatenate them. These new features are used to build a $k$ -nearest neighbor index, which can be used for classification or retrieval. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We depart from this line of work and aim to attain strong representations without training and feature engineering, yet still exploiting information cues from data. In particular, we suggest to enhance the neural network\u2019s embeddings by incorporating FUNGI (Features from Unsupervised GradIents). FUNGI are obtained from self-supervised loss functions, as these do not require any human annotations and allow for a simple enhancement to embedding-only kNN. The losses are computed on top of pretrained backbones (with randomly initialized linear layers if needed), which permits our method to be \u201cplug-and-play\u201d and benefit from the diverse set of pretraining objectives put forth by the community. ", "page_idx": 1}, {"type": "text", "text": "We explore gradients from various learning objectives, such as contrastive learning (Chen et al., 2020a) and self-distillation (Caron et al., 2021) thereby integrating complementary information that mitigates the weaknesses of individual losses. The gradients are obtained from late hidden layers, making them computationally cheap. Finally, these are projected to smaller dimensions, and concatenated with the neural network embeddings, to yield new inputs to the classic kNN algorithm. ", "page_idx": 1}, {"type": "text", "text": "Using kNN with FUNGI can be regarded as a non-parametric transfer learning approach: the gradient information encodes first-order learning signals that are specific to the downstream data, yet no parameters need to be updated. Despite this simplicity, we achieve consistent performance improvements across multiple models and benchmarks. ", "page_idx": 1}, {"type": "text", "text": "Overall, our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce FUNGI, a novel method that combines neural network features and gradients to enhance representations.   \n\u2022 We demonstrate that the gradients from self-supervised losses have predictive abilities and offer complementary information to model embeddings.   \n\u2022 We validate the generality and utility of our method by achieving consistent gains across 11 image, 5 text, and 2 audio classification benchmarks, plus 2 in-context image segmentation and 2 image retrieval tasks, utilizing a total of 20 backbones. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Fast Adaptation There is a broad range of approaches to quickly adapt models to newly specified tasks and data. Inspired by early learning-to-learn work (Hochreiter et al., 2001), meta-learning methods (Finn et al., 2017; Nichol et al., 2018) learn to initialize the parameters of a learner such that it becomes faster to fine-tune with a small number of gradient steps and data. Alternative approaches leverage external memory modules to store relevant training samples to learn to match query examples (Santoro et al., 2016; Vinyals et al., 2016), learn to produce and compare class-based prototypes (Snell et al., 2017) or learn to generate the weights of a classifier (Gidaris & Komodakis, 2018) or even of an entire neural network (Bertinetto et al., 2016) from only a few labeled examples. The advent of Vision Transformers (Dosovitskiy et al., 2021) enable new parameter- and dataefficient strategies to adapt pretrained models through visual prompts (Jia et al., 2022) and in-context learning (Zhang et al., 2023). In contrast to this line of works, our method does not require specialized training and can be applied to any frozen pretrained backbone. FUNGI can also be related to test-time training (Sun et al., 2020; Hardt & Sun, 2024), where the parameters of a predictive model are updated over test samples with a self-supervised objective to reduce the gap between the training and test distributions. While we also use self-supervised objectives and gradients, our approach does not update model parameters and is not limited to predictive models, as it can be applied to any task that can be solved with retrieval. ", "page_idx": 1}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/5bcfac729eb6cdb242078a87745bf4f327385eb6d8eca638cd38f900f56c2477.jpg", "img_caption": ["Figure 2: Combining diverse features (embeddings, gradients) leads to large improvements. Pairwise CKA similarity of features (top) and the kNN accuracy of their combination (bottom). "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/8486dba5e4f41a4997bff5980fddb01b459bcceeb543c39f6a0b55a9bf89c3d0.jpg", "img_caption": ["Figure 3: Gradients encode different information. Delta in per-class kNN accuracy of gradients from different objectives compared to the embeddings, indicated as \u201cEmb.\u201d in the plot. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Self-Supervised Learning Objectives In recent years, self-supervised learning (SSL) has made tremendous progress in computer vision. SSL aims to learn good representations from unlabeled data by leveraging supervision from different signals in the data itself via pretext objectives, thus foregoing human supervision. Models pretrained with self-supervision are subsequently finetuned to downstream tasks of interest with few labeled samples. The crux of SSL is in the pretext learning objective. A wide and diverse collection of pretext objectives have been proposed in the community relying on contrastive learning (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b), clustering (Caron et al., 2018; Asano et al., 2020; Caron et al., 2020), self-distillation (Caron et al., 2021; Grill et al., 2020; Chen & He, 2021; Gidaris et al., 2021), feature (Zhou et al., 2022; Assran et al., 2023) or input reconstruction (He et al., 2022). We hypothesize that the gradients induced by these objectives encapsulate different information from the input data, and that this information can be combined to produce more information-rich representations. Here, we do not use self-supervision in the usual way, i.e., to pretrain an encoder, but rather focus on pretext objectives and data augmentation strategies to compute representations from a frozen pretrained model. ", "page_idx": 2}, {"type": "text", "text": "Feature Engineering A long-standing research area for pattern recognition and image understanding before the advent of deep neural networks that brought the paradigm of end-to-end representation learning. In contrast, classic feature extraction methods are devised without labeled data and often from only a few data samples. They range from local features, such as SIFT (Lowe, 2004), HOG (Dalal & Triggs, 2005), to global pooling, such as GIST (Oliva & Torralba, 2001), Bag-ofVisual-Words (Sivic & Zisserman, 2003), Fisher vectors (Perronnin et al., 2010), VLAD (J\u00e9gou et al., 2010), selective match kernels (Tolias et al., 2013), etc. These pooling strategies can be easily plugged to intermediate or output neural network activations (Gong et al., 2014; Kulkarni et al., 2015; Gidaris et al., 2020), harnessing data-driven learned representations. Other modern examples of feature engineering include Head2Toe (Evci et al., 2022), which augments the model embeddings using intermediate activations, kNN-prompting (Xu et al., 2023), which uses the next token probabilities of a language model to perform few shot nearest neighbor classification and LOST (Sim\u00e9oni et al., 2021) which uses patch features from self-supervised vision transformers for object localization. Closer to our line of work, Wei et al. (2022) shows that kernel regression using the empirical neural tangent kernel (eNTK), which corresponds to the model Jacobian, can achieve a performance similar to fine-tuning in vision, and Mu et al. (2020) shows that features obtained from the Jacobian of the top layers of a convolutional neural network can be used to enhance a linear classifier. In contrast, our method does not require any annotations and is computationally cheaper, as we only compute the gradient for a single layer rather than the full Jacobian. ", "page_idx": 2}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/cf9a040cd2bcc544744adaeeaa22157d2046fcca5118fae53ce5e2dbc48a1033.jpg", "img_caption": ["Figure 4: Gradients extraction using a SimCLR loss. Given a pretrained backbone $f$ and a randomly initialized projection head $h$ , we first patchify an image, obtain the latent representations of patches (1), calculate the SimCLR loss by maximizing the pairwise cosine similarity of patches, and minimizing their similarity to a fixed negatives batch and backpropagate (2), extract the per-sample gradients (3) and finally project the gradients to the same dimensionality as the embeddings (4). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Gradients as Features ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Gradients encode information on how the weights of a neural network should change to solve a given task. Thus, they contain information about the current state of the network and its relation to the data point(s) used to compute it. Therefore, we hypothesize that features from self-supervised gradients should perform better than the embeddings, as they are adapted to the dataset at hand. Empirically, the second row in Figure 2 shows this to be accurate, e.g., gradients from a SimCLR loss improve the accuracy of $\\mathbf{k}$ -nearest neighbor classification by $10.1\\%$ on Flowers102. ", "page_idx": 3}, {"type": "text", "text": "If we plot the per-class accuracy distribution as in Figure 3, we notice that gradient features encode different information depending on the loss and that they can perform significantly worse on some classes, possibly because they are estimated using a single data point, and are thus dependent on the local loss curvature. These findings suggest that the information in embeddings and gradient features could be complementary. We show that this holds in the second row of Figure 2, as feature pairs perform better. Moreover, the first row of the figure suggests that more diverse feature pairs, as measured via their centered kernel alignment (CKA) (Kornblith et al., 2019) score, lead to better overall performance. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our method, FUNGI, enhances k-nearest neighbor search by incorporating features from unsupervised gradients. We extract gradients from self-supervised loss functions, project them to smaller dimensions, and concatenate them with neural network embeddings. The extraction of self-supervised gradients is illustrated in Figure 4, while Figure 1 shows how FUNGI features are constructed. ", "page_idx": 3}, {"type": "text", "text": "Definitions Throughout this section, we define $L_{2}$ normalization as $z^{\\prime}=z/||z||_{2}$ , a vision backbone as $f$ , a linear projection head as $h$ and vectorization as $\\mathtt{v e c}(\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "4.1 FUNGI: Features from Unsupervised Gradients ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Gradients Extraction. Given an arbitrary vision backbone $f$ , in our case a vision transformer (ViT) (Dosovitskiy et al., 2021), we attach a randomly initialized linear projection head $h$ and obtain a latent representation $z=h(f^{\\prime}(x))$ of the input images, which we use to compute the loss for one of our self-supervised objectives. We then run backpropagation and extract the gradients with respect to the weights and biases of an arbitrary hidden linear layer within $f$ . Unless specified otherwise, we use the attention output projection of the last transformer block as our gradient\u2019s source. ", "page_idx": 3}, {"type": "text", "text": "From Gradients to Retrieval-Friendly Features. Gradients are high dimensional and thus impractical for nearest-neighbor retrieval due to speed and storage considerations and the curse of dimensionality. To tackle these issues, we downsample the gradients to the dimensionality of original model embeddings $d$ using the binary random projections method introduced by Achlioptas (2003). For this, we first vectorize the gradients by flattening them to a $m$ -dimensional vector and then multiply them by a matrix $R\\in\\{-1,1\\}^{d,m}$ whose entries are the realizations of a Bernoulli random variable with $p=0.5$ . The gradient $g_{\\beta}$ with respect to a loss $\\mathcal{L}_{\\beta}$ is then defined as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{\\beta}(x)=R\\,{\\mathsf{v e c}}\\left(\\nabla{\\mathcal{L}}_{\\beta}(x)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Combining with Embeddings. To produce our FUNGI features $\\phi$ , we concatenate one or more gradients to the model embeddings. As gradient magnitudes can vary widely across losses, and we want gradients to be equally considered as the embeddings, we $L_{2}$ -normalize each gradient, as well the embeddings and compute ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi(x)=\\mathsf{c a t}\\left[g_{\\beta_{1}}^{\\prime}(x),g_{\\beta_{2}}^{\\prime}(x),...,f^{\\prime}(x)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where cat denotes concatenation. Finally, we reduce the dimensionality of these combined features via PCA to a $d$ -dimensional vector. This allows the combination of multiple losses at iso-storage cost. Our final FUNGI features for a sample $x$ are thus obtained as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{\\mathrm{PCA}}(x)=\\mathsf{P C A}_{d}\\left(\\phi(x)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Self-Supervised Objectives ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider losses representing three families of self-supervised objectives: DINO (Caron et al., 2021), SimCLR (Chen et al., 2020a) and a KL-divergence based loss inspired by the out-ofdistribution detection literature (Huang et al., 2021). In this section we briefly describe the objectives and our adjustments to them, and in Appendix B.6, we also briefly discuss clustering and masked image modeling-based losses. ", "page_idx": 4}, {"type": "text", "text": "DINO. DINO is a distillation and implicit clustering-based learning method. We use the standard DINO loss, which, given an image, enforces global and local crop correspondence between teacher and student models using a cross-entropy loss. In our case, both models share the same parameters, but have independent heads $h_{s}$ and $h_{t}$ for student and teacher respectively, thus we have $z_{i}=$ $h_{i}\\left(f^{\\prime}(x)\\right),i\\in\\{s,t\\}$ . The DINO objective can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DINO}}=\\mathrm{Cross-Entropy}\\left(z_{s},z_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "SimCLR. SimCLR is a noise-contrastive method. Given a batch of images, SimCLR generates two views for each image and aims to minimize the distance between views belonging to the same image and maximize their distance to all other views. Instead, we generate a set of 49 overlapping patches for each image, which we call the positive set. This set is then contrasted against a fixed comparison batch of $49\\times256$ negative examples. Our objective is the expectation of the pair-wise InfoNCE (Oord et al., 2018) loss for each pair of positive views. If we define the positive set of latent view representations as $Z$ , where $z_{i}\\in{\\dot{Z}}=h^{\\prime}(f(x_{i}))$ for a view $x_{i}$ , the comparison batch size as $N$ and the temperature as $\\tau$ , the $\\mathcal{L}_{\\mathrm{SimCLR}}$ objective is then defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{L}_{\\mathrm{SimCLR}}=\\mathbb{E}_{(z_{i},z_{j})\\sim Z,z_{i}\\ne z_{j}}[\\ell_{z_{i},z_{j}}]}&{}&{\\ell_{z_{i},z_{j}}=-\\log\\frac{\\exp(\\sin(z_{i},z_{j})/\\tau)}{\\sum_{k=1}^{49(N+1)}\\mathbb{1}_{[k\\ne i]}\\exp(\\sin(z_{i},z_{k})/\\tau)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "KL Divergence. The KL objective is calculated as the KL divergence between the softmaxed logits of the latents and a uniform distribution $\\boldsymbol{\\mathcal{U}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{KL}}=\\mathrm{KL}\\big(\\mathrm{softmax}(z)||\\mathcal{U}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We hypothesize two reasons as for why this loss produces predictive gradients: first, it receives a non-augmented image, with a higher signal-to-noise ratio compared to other objectives, and second, if we assume that similar images (e.g., the ones that belong to the same class) produce similar activations, then maximizing their entropy by forcing the output distribution to match an uniform should produce similar intra-class gradients and help separability. This hypothesis is supported by the fact that while the KL gradients are discriminative, they have chance performance in other tasks, such as in-context scene understanding. ", "page_idx": 4}, {"type": "text", "text": "4.3 In-Context Scene Understanding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Balazevic et al. (2024) introduced a method for retrieval-based in-context scene understanding, where, for semantic segmentation, they first build a memory bank containing training image patches and their labels, and at test time, for each image patch, retrieve its nearest neighbors and use them to predict its ", "page_idx": 4}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/b4a264d5f4c37184c911fdfbcc72c2e3c2b5f1e96faa567a8255c54c61622a91.jpg", "img_caption": ["Figure 6: FUNGI works across backbones. Accuracy in $k$ -nearest neighbor classification using embeddings and FUNGI features from various ViT backbones, both for full dataset and few shot setups, averaged over 11 datasets. For the FUNGI features we chose the best performing combination across datasets. \u201cAR\u201d indicates backbones trained with the AugReg strategy (Steiner et al., 2022). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 1: FUNGI features are better on several datasets. Accuracy of embeddings and FUNGI features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022) ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K. ", "page_idx": 5}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/7ddb85340dc447c36e48076680c2590fdecf0ea2deb88d7984e64d3f1da2f43a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "label using an attention mechanism. Images are first resized to $512\\times512$ , and then encoded as a set of $32^{2}={\\overline{{10}}}24$ patch features using a ViT with patch size 16. ", "page_idx": 5}, {"type": "text", "text": "We enhance the patch features using SimCLR gradients, obtained by contrasting the input patch tokens against their nearest neighbors from a support index built with ScaNN (Guo et al., 2020). We use the reproduction of this evaluation protocol by Pariza et al. (2024) to run our experiments. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate the performance of FUNGI in k-nearest neighbor image, text and audio classification and retrieval-based in-context scene understanding. Further experiments, including image retrieval, clustering and linear probing, are provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "5.1 Image Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following Caron et al. (2021), we evaluate our FUNGI features using the task of kNN classification. To show the generalizability of our method, we evaluate our features across ViT backbones (Dosovitskiy et al., 2021) with varying model sizes and pretraining strategies, including both supervised and self-supervised methods. ", "page_idx": 5}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/518e9e5e53988b65a49d79b87c4d705eb0161514e7c8aca74e6d45c4be9b59b0.jpg", "img_caption": ["Figure 5: Better data-efficiency. kNN accuracy of embeddings and FUNGI (using only KL and SimCLR gradients) on ImageNet-100 using a DeIT-B/16 backbone when only $k$ shots are used. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We conduct our experiments on 11 diverse downstream datasets, described in Appendix D. Unless otherwise specified, we report the average accuracy across these datasets. We evaluate our features using the kNN implementation of scikit-learn (Pedregosa et al., 2011) with majority voting over 20 neighbors, for full dataset and few shot scenarios, the latter using five examples per class, to analyze the efficacy of our approach in low-data scenarios. ", "page_idx": 5}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/f27757f61eb05601d89e20cf0b101c11e2df72e22ca5dd1fca95efe409d72bf5.jpg", "table_caption": ["Table 2: Performance improves as more gradients are used. Accuracy in image classification using kNN with embeddings and FUNGI features, averaged across 11 datasets for 7 backbones, for standard and few shot setups. Results for additional backbones are shown in Table 8. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/8627115454e728d30dc5808d106b870bd9fa410abd1ba1a336aba24a6a791f6b.jpg", "table_caption": ["Table 3: FUNGI features improve in-context semantic segmentation. mIoU for retrieval-based semantic segmentation on Pascal VOC 2012, comparing a DINO baseline against FUNGI features and the self-supervised HummingBird model. Results from Balazevic et al. (2024) are marked with $\\ddagger$ We resize each image to $512\\times512$ and extract $32^{2}=1024$ patch features. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/e4fb9b05c730ee03c1ea6d970e5690c0a5fb7348527a67892570c1708a49f55c.jpg", "img_caption": ["Figure 7: FUNGI produces sharper and more complete segmentation masks. Segmentation masks produced via nearest neighbor retrieval using DINO features (left), FUNGI (center) and the ground truth (right). Both methods use a memory bank of $1024\\times10^{4}$ patches. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Our results, presented in Figure 6, show that FUNGI consistently improves the kNN performance of all ViT models, regardless of model size or pretraining strategy, both for the full dataset and in few shot scenarios. We further investigate data-efficient settings in Figure 5, where FUNGI shows a significant improvement when 3 to 6 shots are used, highlighting the potential of FUNGI in low-data regimes. ", "page_idx": 6}, {"type": "text", "text": "In Table 1, we show that, with some exceptions, FUNGI provides consistent improvements across datasets for two AugReg (Steiner et al., 2022) ViT-B/16 backbones, pretrained on IN1K and IN21K, with FUNGI providing better results on the former. We further discuss these results in Section 6. ", "page_idx": 6}, {"type": "text", "text": "Lastly, in Table 2 we show that performance improves as more gradients from different self-supervised objectives are used. ", "page_idx": 6}, {"type": "text", "text": "5.2 In-Context Scene Understanding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we assess the effectiveness of our approach in the task of retrieval-based semantic segmentation on Pascal VOC 2012 (Everingham et al., 2010) and ADE20K (Zhou et al., 2019, ", "page_idx": 6}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/d9ae7057bc501e6a2a99494db340463f10a39ae8966823d43d55142f1c5fa4b6.jpg", "table_caption": ["Table 4: Data-efficient semantic segmentation. mIoU scores for data-efficient retrieval-based semantic segmentation on Pascal VOC 2012 and ADE20K, using DINO backbones and their FUNGI features and embeddings. We also compare FUNGI to end-to-end fine-tuning and find our method to perform best for VOC. Results from Balazevic et al. (2024) are marked with $^{\\ddag}$ . "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/959c088ed7cc6dd88abdf27cdb7a5666e3be4b5e2d4baae5c1b2bee87de9af54.jpg", "table_caption": ["Table 5: FUNGI features are useful for the text modality. Top-1 accuracy in kNN text classification for the full dataset and few shot setups. \u201cK\u201d and \u201cS\u201d stand for KL and SimCLR, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "2017). We use the trainaug and train splits to build the memory banks for Pascal VOC and ADE20K, respectively, and report the mean intersection over union (mIoU) on the validation set. ", "page_idx": 7}, {"type": "text", "text": "We apply FUNGI to DINO ViT-S/16 and ViT-B/16 models. Our results, presented in Table 3 and Table 7, demonstrate that FUNGI significantly enhances DINO\u2019s performance across all memory bank sizes, with the most substantial improvements observed in smaller memory banks for Pascal VOC. Qualitatively, FUNGI produces sharper and more complete segmentation masks, as shown in Figure 7. Notably, the DINO ViT-B/16 model, when enhanced with our FUNGI approach, achieves competitive results against the current state-of-the-art HummingBird model (Balazevic et al., 2024), with a difference of only $3.5\\%$ on Pascal VOC and $3.1\\%$ on ADE20K, without any training. This is a particularly promising result, as HummingBird employs a self-supervised pretraining strategy that is specialized for retrieval-based dense prediction tasks, which are the focus of our evaluation in this study. ", "page_idx": 7}, {"type": "text", "text": "In addition, we evaluate the efficacy of FUNGI in a data-efficient setup, and report the results in Table 4. Our findings indicate that our method outperforms DINO in this scenario, even when compared to end-to-end fine-tuning of DINO on the downstream task for Pascal VOC. ", "page_idx": 7}, {"type": "text", "text": "5.3 Other Modalities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Natural language. We evaluate FUNGI in the text domain using five datasets, described in Appendix D, and two transformer language models: BERT base uncased (Devlin et al., 2019) and T5-small (Raffel et al., 2020). We use the $\\mathcal{L}_{\\mathrm{KL}}$ and $\\mathcal{L}_{\\mathrm{SimCLR}}$ losses and obtain the SimCLR views by randomly deleting words with a $10\\%$ probability. The results are presented in Table 5 and show that FUNGI achieves improvements in the text domain. However, SimCLR gradients struggle with some datasets. Different data augmentation strategies, such as back-translation (Sennrich et al., 2016), or language-specific self-supervised losses, e.g., masked language modeling (Devlin et al., 2019), may yield more discriminative gradients. We leave this investigation for future work. Furthermore, in Appendix B.5, we investigate the potential of FUNGI in language in-context learning. ", "page_idx": 7}, {"type": "text", "text": "Audio. We demonstrate gains for the audio modality in Table 12, where we improve the ESC-50 kNN classification accuracy from $42.8\\%$ to $47.0\\%$ and SpeechCommands from $27.4\\%$ to $29.9\\%$ with an SSAST backbone (Gong et al., 2022). Further details are provided in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "Table 6: Impact of the projection head configuration. Top-1 accuracy of gradients on ImageNet100 in $\\boldsymbol{\\mathrm{k}}$ -nearest neighbor classification versus the projection head configuration for KL, DINO and SimCLR gradients. \u201cnorm\u201d indicates whether the features are $L_{2}$ -normalized before being projected. As features are always $L_{2}$ -normalized for the SimCLR objective, the \u201cempty\u201d head configuration is not applicable. The default setup is marked in cyan . ", "page_idx": 8}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/d83b52f15943470b489e7311e8dd12b1e34f5f0cc5f73ccde78ffe9743efe889.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Projection head. To compute our self-supervised losses, we first $L_{2}$ -normalize the model embeddings (except for SimCLR) and then project them using a randomly initialized linear head. We motivate this choice empirically by ablating these components, and the results in Table 6 show that this configuration produces the most predictive gradients for ImageNet-100. ", "page_idx": 8}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/81f8f4aadffb72402b948d4a4facab342406662ff0c77a5261fac71b3cc2813d.jpg", "img_caption": ["Figure 8: Gradients from deeper layers are more predictive. Top-1 accuracy of gradients obtained from every layer of a supervised DeIT ViT-B/16 in k-nearest neighbor classification on ImageNet-100 for the KL, DINO, and SimCLR objectives. The default setup (last layers) is marked in cyan . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Gradients source layer. Throughout the paper, we extract gradients from the self-attention output projection of the last transformer block. Intuitively, deeper layers provide more predictive features, and thus, their gradient should display the same behavior. This assumption is confirmed by our results in Figure 8, where, for all losses, deeper layers consistently produce more predictive gradients. Regarding the choice of layer within a transformer block, for shallower blocks, the second MLP layer is significantly more predictive, but the performance gap becomes insignificant as we move towards deeper blocks, favoring (by a small margin) the attention output projection, which is also more memory efficient, as it has fewer parameters compared to other layers. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Broader impact. Our method improves the features used for the kNN algorithm. As such, it is a fundamental contribution to Machine Learning. Given the ubiquitous use of kNN, our method could have positive consequences, such as improving reliability and factuality in Retrieval Augmented Generation (RAG) systems, where Language Models are grounded in retrieved pieces of text before generating an answer. We do not foresee any direct negative consequence caused by our method. ", "page_idx": 8}, {"type": "text", "text": "Impact of pretraining dataset. Our method works with various backbones, model sizes, and pretraining strategies. However, we have observed that the benefits diminish as the size of the pretraining dataset increases: in Table 1, FUNGI provides a smaller relative improvements on a backbone pretrained with IN21K compared to one pretrained on IN1K, and similarly, in Table 16 the relative improvement over EVA-CLIP (Sun et al., 2023) is smaller compared to the improvement over CLIP (Radford et al., 2021), as they are pretrained on 2B and 400M text-image pairs respectively. ", "page_idx": 8}, {"type": "text", "text": "Computational efficiency. Computing FUNGI features introduces an overhead, which we measure in Table 27 by comparing the throughput of a DeIT ViT-B/16 when extracting gradients and embeddings. The DINO and SimCLR losses have the largest overhead, as they forward 12 and 49 views per image, respectively. As shown in Appendix C.1, this number can be reduced, at a performance cost. However, thanks to our dimensionality reduction, the speed of kNN retrieval is not impacted. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have shown that gradients from self-supervised objectives have predictive abilities and encode complementary information to the model embeddings. Building on those findings, we introduced FUNGI , which effectively combines embeddings and gradients into powerful features for retrievalbased tasks. Specifically, we have shown that FUNGI enhance the performance of kNN-based image and text classification across models, pretraining strategies, and downstream datasets, both for full dataset and few shot setups. Moreover, we have shown that FUNGI significantly boost the performance of DINO features for retrieval-based semantic segmentation tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. We acknowledge the use of the Dutch national supercomputer Snellius to run the experiments presented in this paper. YMA thanks Tengda Han for the initial discussions on using self-supervised gradients for tasks other than learning. WS thanks the Amsterdam ELLIS Unit for their generous funding, which allowed him to visit the Valeo laboratory in Paris. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 19   \nDimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary coins. JCSS, 2003. 4, 22   \nYuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. 3   \nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In ICCV, 2023. 3   \nIvana Balazevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovic\u00b4, and Olivier Henaff. Towards in-context scene understanding. NeurIPS, 2024. 1, 5, 7, 8, 17, 24   \nFrancesco Barbieri, Jose Camacho-Collados, Francesco Ronzano, Luis Espinosa-Anke, Miguel Ballesteros, Valerio Basile, Viviana Patti, and Horacio Saggion. Semeval 2018 task 2: Multilingual emoji prediction. In IWSE, 2018. 25, 26   \nFrancesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In EMNLP, 2020. 25, 26   \nLuca Bertinetto, Jo\u00e3o F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learning feed-forward one-shot learners. In NeurIPS, 2016. 2   \nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In ECCV, 2014. 25, 26   \nY-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-level features for recognition. In CVPR, 2010. 1   \nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018. 3, 20, 21   \nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020. 3   \nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 1, 2, 3, 5, 6, 26   \nI\u00f1igo Casanueva, Tadas Tem\u02c7cinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli\u00b4c. Efficient intent detection with dual sentence encoders. In NLP4ConvAI, 2020. 25, 26   \nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020a. 2, 3, 5   \nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. 1, 3   \nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b. 3   \nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021. 26   \nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014. 25, 26   \nNavneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1, 3   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NACL, 2019. 8, 26   \nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 2, 4, 6   \nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar\u00e9, Maria Lomeli, Lucas Hosseini, and Herv\u00e9 J\u00e9gou. The faiss library. 2024. 19   \nAlexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In ICCV, 1999. 1   \nUtku Evci, Vincent Dumoulin, Hugo Larochelle, and Michael C Mozer. Head2toe: Utilizing intermediate representations for better transfer learning. In ICML, 2022. 3   \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 7   \nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2   \nEvelyn Fix. Discriminatory analysis: nonparametric discrimination, consistency properties. USAF school of Aviation Medicine, 1985. 1   \nSpyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In CVPR, 2018. 2   \nSpyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, and Matthieu Cord. Learning representations by predicting bags of visual words. In CVPR, 2020. 1, 3   \nSpyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick P\u00e9rez. Obow: Online bag-of-visual-words generation for self-supervised learning. In CVPR, 2021. 3   \nYuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Interspeech, 2021. 18, 19, 26   \nYuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. Ssast: Self-supervised audio spectrogram transformer. In AAAI, 2022. 8, 18, 19, 26   \nYunchao Gong, Liwei Wang, Ruiqi Guo, and Svetlana Lazebnik. Multi-scale orderless pooling of deep convolutional activation features. In ECCV, 2014. 1, 3   \nJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 3   \nAntonio Gulli. Ag\u2019s corpus of news articles. http://groups.di.unipi.it/\\~gulli/AG_corpus_ of_news_articles.html, 2005. Accessed: 2020-05-21. 25, 26   \nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In ICML, 2020. 1, 6, 24   \nMoritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models. In ICLR, 2024. 3   \nJames Hays and Alexei A Efros. Im2gps: estimating geographic information from a single image. In CVPR, 2008. 1   \nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 3   \nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 3, 17, 26   \nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. JST-AEORS, 2019. 25, 26   \nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In ICANN, 2001. 2   \nRui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. NeurIPS, 2021. 5   \nHerve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. TPAMI, 2010. 1   \nHerv\u00e9 J\u00e9gou, Matthijs Douze, Cordelia Schmid, and Patrick P\u00e9rez. Aggregating local descriptors into a compact image representation. In CVPR, 2010. 1, 3   \nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022. 2   \nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. T-BD, 2019. 1, 19   \nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In ICML, 2019. 4   \nJonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of fine-grained cars. 2013. 25, 26   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 25, 26   \nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 1955. 19   \nPraveen Kulkarni, Joaquin Zepeda, Frederic Jurie, Patrick Perez, and Louis Chevallier. Hybrid multi-layer deep cnn/aggregator feature for image classification. In ICASSP, 2015. 1, 3   \nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS, 2020. 1 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Ping Li, Trevor J Hastie, and Kenneth W Church. Very sparse random projections. In KDD, 2006. 22 ", "page_idx": 12}, {"type": "text", "text": "Xin Li and Dan Roth. Learning question classifiers. In COLING, 2002. 25, 26 ", "page_idx": 12}, {"type": "text", "text": "W Johnson J Lindenstrauss and J Johnson. Extensions of lipschitz maps into a hilbert space. Contemporary Mathematics, 1984. 21 ", "page_idx": 12}, {"type": "text", "text": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In DeeLIO, 2022. 19 ", "page_idx": 12}, {"type": "text", "text": "Stuart Lloyd. Least squares quantization in pcm. T-IT, 1982. 19 ", "page_idx": 12}, {"type": "text", "text": "David G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004. 1, 3 ", "page_idx": 12}, {"type": "text", "text": "Julien Mairal. Cyanure: An open-source toolbox for empirical risk minimization for python, $\\mathrm{c}++$ , and soon more. arXiv preprint arXiv:1912.08165, 2019. 18 ", "page_idx": 12}, {"type": "text", "text": "S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. 25, 26 ", "page_idx": 12}, {"type": "text", "text": "Fangzhou Mu, Yingyu Liang, and Yin Li. Gradients as features for deep representation learning. In ICLR, 2020. 3 ", "page_idx": 12}, {"type": "text", "text": "Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. 2 ", "page_idx": 12}, {"type": "text", "text": "Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008. 25, 26 ", "page_idx": 12}, {"type": "text", "text": "Aude Oliva and Antonio Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV, 2001. 1, 3 ", "page_idx": 12}, {"type": "text", "text": "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 5 ", "page_idx": 12}, {"type": "text", "text": "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TMLR, 2023. 26 ", "page_idx": 12}, {"type": "text", "text": "Valentinos Pariza, Mohammadreza Salehi, and Yuki Asano. Hummingbird evaluation for vision encoders, 4 2024. URL https://github.com/vpariza/open-hummingbird-eval. 6 ", "page_idx": 12}, {"type": "text", "text": "Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012. 25, 26 ", "page_idx": 12}, {"type": "text", "text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 2011. 6 ", "page_idx": 12}, {"type": "text", "text": "Florent Perronnin, Jorge S\u00e1nchez, and Thomas Mensink. Improving the fisher kernel for large-scale image classification. In ECCV, 2010. 3 ", "page_idx": 12}, {"type": "text", "text": "James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In CVPR, 2007. 17, 19 ", "page_idx": 12}, {"type": "text", "text": "James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Lost in quantization: Improving particular object retrieval in large scale image databases. In CVPR, 2008. 17, 19 ", "page_idx": 12}, {"type": "text", "text": "Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In ACMMM, 2015. 18, 25, 26 ", "page_idx": 12}, {"type": "text", "text": "Filip Radenovic\u00b4, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ond\u02c7rej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. In CVPR, 2018. 17 ", "page_idx": 12}, {"type": "text", "text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 9, 17, 26 ", "page_idx": 12}, {"type": "text", "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020. 8, 26   \nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 25, 26   \nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In ICML, 2016. 2   \nRico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In ACL, 2016. 8   \nOriane Sim\u00e9oni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick P\u00e9rez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. In BMVC, 2021. 3   \nSivic and Zisserman. Video google: A text retrieval approach to object matching in videos. In ICCV, 2003. 1, 3   \nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. NeurIPS, 30, 2017. 2   \nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013. 25, 26   \nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. In TMLR, 2022. 6, 7, 17, 20, 26   \nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 9, 17, 26   \nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. 3   \nGiorgos Tolias, Yannis Avrithis, and Herv\u00e9 J\u00e9gou. To aggregate or not to aggregate: Selective match kernels for image search. In ICCV, 2013. 3   \nAntonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. TPAMI, 2008. 1   \nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In ICML, 2021. 26   \nHugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. In ECCV, 2022. 17   \nKoen Van De Sande, Theo Gevers, and Cees Snoek. Evaluating color descriptors for object and scene recognition. TPAMI, 2009. 1   \nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. NeurIPS, 2016. 2   \nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 25, 26   \nPete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018. 18, 25, 26   \nAlexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-world neural representations generalize. In ICML, 2022. 3 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In EMNLP, 2020. 26   \nBenfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, and Yongdong Zhang. $k$ nn prompting: Beyond-context learning with calibration-free nearest neighbor inference. In ICLR, 2023. 3   \nI Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semisupervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019. 1   \nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NeurIPS, 2015. 25, 26   \nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? In NeurIPS, 2023. 2   \nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. In CVPR, 2017. 8   \nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. 7   \nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. In ICLR, 2022. 3, 20 ", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Algorithm 16 ", "page_idx": 15}, {"type": "text", "text": "B Additional Experimental Results 16 ", "page_idx": 15}, {"type": "text", "text": "B.1 Image Retrieval 17   \nB.2 Audio Classification . . 18   \nB.3 Linear Classification of Image Features . 18   \nB. 4 Clustering . . . . 19   \nB.5 Language In-Context Learning 19   \nB.6 Additional Self-Supervised Objectives 20   \nB.7 Additional Ablations . . 21   \nC Experimental Details 23   \nC.1 Vision Nearest Neighbor Classification Experimental Details . . 23   \nC.2 In-Context Scene Understanding Experimental Details . . 24   \nC.3 Text Classification Experimental Details 24 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "D Data and Models 25 ", "page_idx": 15}, {"type": "text", "text": "E Compute Resources 26 ", "page_idx": 15}, {"type": "text", "text": "A Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Algorithm 1 provides pytorch-style pseudocode for the computation of $\\mathcal{L}_{\\mathrm{KL}}$ , the gradient extraction, and the computation of FUNGI features (without PCA). ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 PyTorch pseudocode for the KL FUNGI features. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "# model, head, proj: the vision backbone, head and the random projection used to downsample gradients   \n$\\p r{0}\\,\\dot{]}\\;\\;=$ (torch.rand(feat_dim, grad_dim) - 0.5) > 0   \nuniform $=$ torch.ones(feat_dim) / feat_dim ", "page_idx": 15}, {"type": "text", "text": "for ${\\tt x}$ in dataset: # Extract the feature and its projection y = model(x) $_z=$ head(y) ", "page_idx": 15}, {"type": "text", "text": "kl_div(log_softmax $\\left(\\mathbf{z}\\right)$ , softmax(uniform)).backward() # Calculate the loss and backpropagate layer $=$ model.blocks.11.attn.proj # Select the target layer ", "page_idx": 15}, {"type": "text", "text": "# Extract and project the gradients   \ngradients $=$ torch.cat([layer.weight.grad, layer.bias.grad.unsqueeze $\\scriptstyle{\\mathrm{dim}}=-1;$ )], dim=-1).view(-1)   \ngradients $=$ proj @ gradients.view(-1) ", "page_idx": 15}, {"type": "text", "text": "feature $=$ torch.cat([normalize(y), normalize(gradients)], dim=-1) # Build the final feature ", "page_idx": 15}, {"type": "text", "text": "B Additional Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we first illustrate additional results that solidify the findings discussed in the main paper, including the evaluation of FUNGI in image retrieval, $\\mathbf{k}$ -nearest neighbor audio classification, linear classification and clustering and a brief investigation of the performance of gradients from clustering and masked image modeling self-supervised objectives. ", "page_idx": 15}, {"type": "text", "text": "In Table 8, we report the performance of embeddings and FUNGI features for some additional backbones, including CLIP and EVA-CLIP models, for which, as explained in the main text, we experience diminishing returns as the pretraining dataset size grows. Moreover, for both models, the SimCLR loss does not produce predictive gradients, which we hypothesize is due to models being saturated to a contrastive loss, as they use a similar objective for pretraining. ", "page_idx": 16}, {"type": "text", "text": "In Table 10, we report the mean accuracy and one standard deviation (computed via numpy.std) across three seeds for a subset of our datasets, using a DeIT ViT-B/16 backbone, showing that the performance improvements of FUNGI are consistent across seeds. ", "page_idx": 16}, {"type": "text", "text": "In Figure 11 we evaluate the effectiveness of FUNGI across different ViT model sizes. The findings show that FUNGI improves the results for all three ViT models (ViT-S, ViT-B, and ViT-L), with the most significant improvements observed in the ViT-B model. In Table 9, we provide further evidence for the scalability of FUNGI by evaluating it on several ViT-L and ViT-H backbones. ", "page_idx": 16}, {"type": "text", "text": "In Table 7, we report the performance of FUNGI for in-context retrieval-based semantic segmentation on ADE20K, and show that our method outperforms DINO across all memory bank sizes and is competitive against HummingBird. ", "page_idx": 16}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/9ed09641993c5e466b2e808ab5a70f6f2ad5ae3b2fc37f77528d88a305e3c3cf.jpg", "table_caption": ["Table 7: FUNGI features improve in-context semantic segmentation on ADE20K. We report the mIoU for retrieval-based semantic segmentation on ADE20K, comparing a DINO baseline against FUNGI features and the self-supervised HummingBird model. Results from Balazevic et al. (2024) are marked with $\\ddagger$ . We resize each image to $512\\times512$ and extract $32^{2}=1024$ patch features. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/02be974f77eda5911df8a861fb71d3ccffc0e6e53fd16600a3e967be73ae88a8.jpg", "table_caption": ["Table 8: Additional backbones. Average accuracy of embeddings and FUNGI features in $\\mathbf{k}\\cdot$ -nearest neighbor classification across 11 datasets for CLIP (Radford et al., 2021; Sun et al., 2023), AugReg (Steiner et al., 2022), DeIT III (Touvron et al., 2022) and masked autoencoder (He et al., 2022) models. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.1 Image Retrieval ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We evaluate the performance of FUNGI features in image retrieval using the revisited (Radenovi\u00b4c et al., 2018) Oxford (Philbin et al., 2007) and Paris (Philbin et al., 2008) landmarks datasets. We report the mean average precision (mAP) for both the medium (M) and hard (H) splits. ", "page_idx": 16}, {"type": "text", "text": "For this task, we use FUNGI features built with DINO and KL gradients, as the SimCLR gradients did not result in good retrieval performance. The results displayed in Table 11 show that FUNGI features improve the retrieval abilities of all backbones, except for DINOv2. Our method is particularly effective when applied on CLIP backbones: on the Paris hard split, we improve by $12.4\\%$ and $7.2\\%$ for CLIP and EVA-CLIP, respectively. ", "page_idx": 16}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/47787bccd87e727995bccdc8a03ed97c0d6b0110877da383cff08e0d03c1ab6c.jpg", "table_caption": ["Table 9: Scalability experiments. Average accuracy of embeddings and FUNGI features in k-nearest neighbor classification across 11 datasets (7 for ViT-H) for ViT-L and H backbones. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 10: FUNGI is consistent across seeds. Average accuracy in kNN classification and one standard deviation for FUNGI features on 8 datasets, measured across three seeds using a DeIT ViT-B/16 backbone. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively. ", "page_idx": 17}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/7ed9d1eefd6f8754470b7d99ef12ddb631ccf47280476decef7b4241af86cba6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Audio Classification ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We evaluate FUNGI on the audio modality using SSAST (Gong et al., 2021, 2022), a self-supervised audio spectrogram transformer trained for audio and speech classification, as the backbone. We construct FUNGI features using KL and SimCLR gradients, and test their performance in $\\mathbf{k}$ -nearest neighbor classification on ESC 50 (Piczak, 2015) and SpeechCommands V2 (Warden, 2018). ", "page_idx": 17}, {"type": "text", "text": "We use the same formulation as in the vision experiments for the $\\mathcal{L}_{\\mathrm{KL}}$ and $\\mathcal{L}_{\\mathrm{SimCLR}}$ objectives. However, for the latter, we obtain 16 views of the same clip by adding uniform noise following Gong et al. (2022). If we define the filter bank of an audio clip as $\\bar{c}\\in\\mathbb{R}^{h,\\bar{w}}$ , the noise-augmented clip $\\hat{c}$ is computed as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{c}=c+\\frac{x_{1}\\cdot x_{2}}{10}\\qquad x_{1}\\sim U(0,1)^{h,w}\\qquad x_{2}\\sim U(0,1).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, $\\hat{c}$ is shifted by a factor sampled from a discrete uniform distribution $U(-10,10)$ . The complete list of hyperparameters used for the audio classification experiments is reported in Table 13. ", "page_idx": 17}, {"type": "text", "text": "The results are reported in Table 12, and show that FUNGI features built using $\\mathrm{KL}$ gradients yield promising results, improving by up to $4.2\\%$ on the baseline. On the other hand, using SimCLR gradients does not consistently yield improvements. It rather often causes a performance drop when combined with KL gradients. As with text classification, further research is needed to determine the optimal self-supervised objectives and data augmentation to extract predictive gradients. ", "page_idx": 17}, {"type": "text", "text": "B.3 Linear Classification of Image Features ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We evaluate FUNGI features in logistic regression, using the implementation from the cyanure library (Mairal, 2019). We train each classifier for a maximum of 300 epochs (30 in the case of ImageNet-1K) using $L_{2}$ regularization. For each dataset and feature combination (i.e., embeddings, embeddings $^+$ $\\nabla_{\\mathrm{KL}}$ , etc.), we pick the best regularization parameter between 5 linearly spaced values in the interval $[5\\times10^{-6},5\\times\\bar{1}0^{-4}]$ using the validation set. For datasets without a validation set, we generate one using an 80/20 stratified split. The final model is trained using the entire training dataset. ", "page_idx": 17}, {"type": "text", "text": "We report the results in Table 15 and Table 16 and find that, in linear classification, FUNGI features are most effective for backbones pretrained using a supervised objective. In contrast, self-supervised backbones do not benefit as much. This is especially evident for DINO and DINOv2, where ", "page_idx": 17}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/c14f6cd5864e0c7f818d165c4de81ff6ead9eb5f8609791df32f91c9e5713013.jpg", "table_caption": ["Table 11: FUNGI improves image retrieval. Mean average precision (mAP) of embeddings and FUNGI for retrieval on the Paris (Philbin et al., 2008) and Oxford (Philbin et al., 2007) landmarks datasets, for both medium (M) and hard (H) splits. \u201cK\u201d and \u201cD\u201d stand for KL and DINO, respectively. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/d6c061bc4858850c3d15f3708981fe528159632ef8067a0483d8323594451970.jpg", "table_caption": ["Table 12: FUNGI works for audio. Top-1 accuracies in k-nearest neighbor audio classification of embeddings and FUNGI features obtained from a SSAST backbone (Gong et al., 2022, 2021). \u201cK\u201d and \u201cS\u201d stand for KL and SimCLR, respectively. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/da82a5d7974391ae2612398c05727774bb9a4e80d104685ba6798d97abf83954.jpg", "table_caption": ["Table 13: Audio classification experimental details. Parameters used to extract audio encoder gradients for the $\\mathcal{L}_{\\mathrm{KL}}$ (left) and $\\mathcal{L}_{\\mathrm{SimCLR}}$ (right) objectives. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "FUNGI often yields worse results, especially in a few shot scenarios. Contrary to the $\\mathbf{k}$ -nearest neighbor classification results, the best feature combination is backbone specific, and in Figure 9, we show that significant performance improvements can be achieved by picking the best feature combination for each backbone. ", "page_idx": 18}, {"type": "text", "text": "B.4 Clustering ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We evaluate the performance of FUNGI features built with KL and DINO gradients in $\\mathbf{k}$ -means clustering (Lloyd, 1982) of image features using faiss (Johnson et al., 2019; Douze et al., 2024). Given a dataset with $C$ classes, we compute $C$ clusters via $\\mathbf{k}$ -means and match clusters to classes using the Hungarian algorithm (Kuhn, 1955). We then measure the average overlap between clusters and classes. The results in Table 14 show that, on average, FUNGI features built with KL and DINO gradients improve the overlap between clusters and classes. In particular, FUNGI can improve the clustering performance by up to $15.8\\%$ on Oxford-IIT Pets for a CLIP ViT-L/14 backbone. ", "page_idx": 18}, {"type": "text", "text": "B.5 Language In-Context Learning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Liu et al. (2022) has shown that selecting examples for in-context learning using retrieval outperforms a random baseline, and that using encoders fine-tuned on a task similar to the target one further improves performance. Thus, we hypothesize that using FUNGI features to retrieve in-context examples can improve performance, as they contain an adaptation signal to the task at hand. We test this hypothesis by measuring the in-context classification performance of GPT 4o mini (Achiam et al., 2023) using examples retrieved either using embeddings or FUNGI features built with KL and ", "page_idx": 18}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/f432fb33ab608f3fb69bd3c2617aafe166f67afda88b297b3c734d797924bf6a.jpg", "table_caption": ["Table 14: FUNGI improves clustering. Overlap between classes and clusters found via $\\mathbf{k}$ -means clustering of embeddings and FUNGI features. \"K\" and \"D\" stand for KL and DINO respectively. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/3ee421954a93b86fd480f13e09612d60e8a4783a6e64b979022561476443bf86.jpg", "img_caption": ["Figure 9: FUNGI works across backbones for linear probing. Accuracy in logistic regression-based image classification of embeddings and FUNGI features on various ViT backbones, both for full dataset and few shot setups, averaged over 11 datasets. For the FUNGI features, we chose the best performing combination across datasets. \u201cAR\u201d indicates AugReg backbones (Steiner et al., 2022). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "SimCLR gradients using a BERT backbone. We retrieve the top 20 most similar training examples for each test sample and ask the model to predict its label using a prompt template similar to the one shown in Listing 1. For a fair evaluation, we set the model temperature to 0. The results listed in Table 17 show that examples retrieved using FUNGI features improve the in-context learning accuracy by up to $2.5\\%$ on banking-77. ", "page_idx": 19}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/c885d96c873e6f79be5e61c474179977398915f44a1658345494248cd5da3693.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Listing 1: The prompt used for the in-context learning evaluation of embeddings and FUNGI features on banking-77 using a GPT 4o mini backbone. The labels are given as strings, e.g. exchange_rate. ", "page_idx": 19}, {"type": "text", "text": "B.6 Additional Self-Supervised Objectives ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we study the performance of gradients obtained by two additional self-supervised objectives, DeepCluster (Caron et al., 2018) and iBOT (Zhou et al., 2022) in $\\mathbf{k}\\cdot$ -nearest neighbor classification on ImageNet-100 using a DeIT ViT-B/16 backbone. DeepCluster is a self-distillation and explicit clustering-based self-supervised method that alternates between clustering image features and training a model to predict cluster assignments. iBOT is an extension of DINO that combines image and patch-level objectives, the latter implemented as a latent-space masked image modeling (MIM) objective, where a learnable patch token replaces a subset of patches, and the model must reconstruct them. ", "page_idx": 19}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/6d9ec3e9878e07f0b63f80cdae704226a0311a16c5c5517c90156a17c87477b2.jpg", "table_caption": ["Table 15: The best gradients for linear probing are backbone-specific for the main backbones. Average accuracy across 11 datasets for logistic regression-based image classification of embeddings and FUNGI features. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/1cb92a842f55e7b9d83b8c88959bc72b68d9d31316337a5223ea7001d3d2b119.jpg", "table_caption": ["Table 16: The best gradients for linear probing are backbone-specific for the additional backbones. Average accuracy across 11 datasets for logistic regression-based image classification of embeddings and FUNGI features. \u201cK\u201d, \u201cD\u201d and \u201cS\u201d stand for KL, DINO and SimCLR, respectively. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "The results are displayed in Figure 10, and show that the objectives used in this work achieve similar performances to the model embeddings, even surpassing them in the case of DINO. At the same time, iBOT and DeepCluster instead produce gradients with relatively poor predictive performance. For the former, a possible reason is that it incorporates a dense loss, whose gradients may not help to discriminate examples on the image level. Regarding DeepCluster, models pretrained using this strategy had worse performance in retrieval tasks compared to supervised pretraining (Caron et al., 2018), which may explain the poor retrieval abilities of its gradients. ", "page_idx": 20}, {"type": "text", "text": "B.7 Additional Ablations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "DINO data augmentation and head. To maximize the signal-to-noise ratio, we only use local and global crops for the DINO data augmentation. We validate this choice empirically, and the results in Table 18 show that random crops produce more discriminative gradients compared to the standard data augmentation policy. Moreover, we also empirically validate the choice of using two independent heads for the DINO loss in Table 18, showing that this choice is beneficial for kNN classification. ", "page_idx": 20}, {"type": "text", "text": "Random Projections. In order to reduce the dimensionality of the gradients we use random projections, an unsupervised dimensionality reduction technique based on the lemma by Lindenstrauss & Johnson (1984), which states that for a set of $N\\,d\\cdot$ -dimensional points and a projection subspace of dimension $k\\geq k_{0}=O(\\epsilon^{-2}\\mathrm{log}(N))$ there exist a mapping $f:\\dot{\\mathbb{R}}^{d}\\rightarrow\\mathbb{R}^{k}$ that preserves euclidean distances with a distortion of at most $\\epsilon\\pm1$ . This mapping can be either implemented as a Gaussian, binary (Achlioptas, 2003) or sparse (Li et al., 2006) projection. Our method uses a binary projection, as it\u2019s more memory-efficient than a Gaussian matrix, and in Table 19, we compare its performance to the other initializations. The results show that the initialization has little impact on downstream performance, favoring binary projections by a small margin. ", "page_idx": 20}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/6e4ffa8104ac0084338681a985f806b9ee63dde920ec2765889eb18372490512.jpg", "table_caption": ["Table 17: FUNGI improves language in-context learning. Classification accuracy of GPT 4o mini in language in-context learning with examples retrieved using embeddings or FUNGI features, both extracted from a BERT backbone. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/430b248e5618ef7e9299a2abc612a040b7e0ec887de61cc33e87814eebe16a6d.jpg", "img_caption": ["Figure 10: Not all objectives produce good predictive gradients. Top-1 accuracy in $\\mathbf{k}\\cdot$ - nearest neighbor classification of gradients obtained from different self-supervised objectives using a DeIT ViT-B/16 backbone. \u201cMIM\u201d stands for masked image modeling. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/7e07958ea24f215b764440f8b24026aa28036a482296bfbc000fa2dae62c4808.jpg", "img_caption": ["Improvement vs Model Size ", "Figure 11: Gains across backbone sizes. Accuracy in $\\mathbf{k}\\cdot$ -nearest neighbor image classification averaged across 11 datasets using the model embeddings and FUNGI features extracted from AugReg backbones of increasing size. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/1e535f9d7301d268d6cbffd5b6d5a395c46fdb8963389ebb4b3af965429dd0f0.jpg", "table_caption": ["Table 18: DINO head configuration and data augmentation. Top-1 accuracy on ImageNet-100 in $\\mathbf{k}\\cdot$ -nearest neighbor classification for the DINO gradients using shared or independent teacher and student heads (left) and with respect to the data augmentation policy (right). "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/cfb12c9cacb7d341bbab9e02f41af2f8c2296211ff4f48adb5d472edbb2f6957.jpg", "table_caption": ["Table 19: The random projection initialization has little impact on performance. Comparison of the downstream accuracy of FUNGI features built with gradients projected using matrices with different initializations. We report the mean and one standard deviation measured across three runs using the Flowers102 dataset and a DeIT ViT-B/16 backbone. No further dimensionality reduction was applied to the concatenated features. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "PCA. We use Principal Component Analysis (PCA) to combine data from multiple losses at an iso-storage and retrieval speed cost. Given a dataset of FUNGI features, we fti the PCA on the training split and use it to transform training and test splits. Table 20 lists the PCA dimensionalities used for each model architecture and shows that they do not cause a decrease in performance but rather provide a minor improvement on average. Moreover, we compare PCA against binary, Gaussian, and sparse random projections in Table 21 and find that all random projection-based methods result in a drop in accuracy compared to the original features, while PCA consistently improves performance. ", "page_idx": 21}, {"type": "text", "text": "Table 20: PCA does not degrade performance. PCA output dimensionalities with respect to the backbone architecture (left) and its impact on $\\mathbf{k}$ -nearest neighbor image classification accuracy averaged across 11 datasets using a DeIT ViT-B/16 backbone (right). ", "page_idx": 22}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/37b08eb88e8375dfb392c173477ec7581dd7ef4a5c56078e8afa1b589a3a6322.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/4ccf57a5f4e30f0ca3dbe1b1d969b11262fce5426892904f62d92682f2038ae2.jpg", "table_caption": ["Table 21: PCA is the best dimensionality reduction method. The mean-per-class accuracy of embeddings and FUNGI features from a DeIT ViT-16/B backbone on Flowers102, transformed with PCA and random projections. We report the mean and one standard deviation across three seeds. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Vision Nearest Neighbor Classification Experimental Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Hyperparameters. We use three losses to extract gradients from vision backbones: $\\mathcal{L}_{\\mathrm{KL}},\\mathcal{L}_{\\mathrm{DINO}}$ and $\\mathcal{L}_{\\mathrm{SimCLR}}$ . The parameters used for each loss are shown in Table 22. This set of parameters is used consistently across backbones and datasets. While $\\mathcal{L}_{\\mathrm{KL}}$ and $\\mathcal{L}_{\\mathrm{DINO}}$ are robust to the choice of hyperparameters, $\\mathcal{L}_{\\mathrm{SimCLR}}$ is particularly sensitive to the number of positive views, as shown in Figure 12, where performance increases in a logarithmic fashion as more positive views are used, at the cost of gradient extraction speed. While this behavior is consistent across datasets, it has the most significant impact in datasets with many classes, e.g., Flowers102. ", "page_idx": 22}, {"type": "text", "text": "SimCLR data augmentation and loss details. Given an image, we patchify it in 49 overlapping views as follows: we first resize the input image to (224, 224), and then extract 49 patches of size $112\\times112$ , using a stride corresponding to $1/6$ of the patch size. No other style or color augmentation is used. As the number of patches increases, so does the memory required to compute the loss and the gradients. This problem can be partially tackled by precomputing the negative batch, which in our experiments is picked randomly from the training dataset and kept constant for every input. Moreover, we can observe that the SimCLR loss is only defined for positive pairs, so we only need to compute the similarity of positive pairs to all other pairs, significantly reducing the size of the similarities matrix and memory usage. ", "page_idx": 22}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/20a15e077fbe3da650d0c54c5bed1ce307c607363a454e0980c7cc3738590c84.jpg", "table_caption": ["Table 22: Image gradients setup. Data augmentation and loss parameters used to extract gradients from vision encoders for $\\mathcal{L}_{\\mathrm{KL}}$ , $\\mathcal{L}_{\\mathrm{SimCLR}}$ and $\\mathcal{L}_{\\mathrm{DINO}}$ (left to right). "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "PRBsEz8rnV/tmp/3f53427e953d6dde2da728e0e419b89877a77fda81e1d3a00c2ab141e6c445d0.jpg", "img_caption": ["Figure 12: SimCLR is sensitive to the number of views. The SimCLR gradients mean-per-class accuracy on Flowers102 with respect to the number of patches (left) and the images/s versus the number of patches (right) using a supervised DeIT ViT-B/16 backbone. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.2 In-Context Scene Understanding Experimental Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the evaluation of the in-context scene understanding abilities of FUNGI features we closely replicate the setup described by Balazevic et al. (2024) for both the full and few shot setups, with two minor exceptions: (1) we use a single augmentation epoch for the full dataset evaluation and (2) we use an anisotropic quantization threshold of 0.2 for the nearest neighbor index, as this parameter was not specified in the paper. The full set of parameters for the evaluation, loss computation and data augmentation is reported in Table 23. As for data augmentation, we use the same policy of Balazevic et al. (2024), and apply each augmentation independently. ", "page_idx": 23}, {"type": "text", "text": "In order to construct FUNGI features for this task, we implement a SimCLR loss that contrasts patch tokens from an input image to their nearest neighbors retrieved from a supporting memory bank. In practice, we: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Construct a memory bank of image patches of the same size as the one used for evaluation and its nearest neighbor index with ScaNN (Guo et al., 2020) following the procedure by Balazevic et al. (2024). We call this our support index.   \n\u2022 Then, for each image, we: 1. Resize it to $512\\times512$ , compute its [CLS] and patch tokens and project them with a linear head. Excluding the [CLS] token, each image is mapped to $\\dot{3}2^{2}\\stackrel{\\bullet}{=}1024$ features, as all our backbones use patches of size 16. 2. For each token, retrieve its two nearest neighbors from the support index and randomly drop $50\\%$ of them. 3. Compute the SimCLR loss, where the patch tokens constitute the positive set and the neighbors the negative batch. This allows us to compute a per-patch gradient. 4. Drop the [CLS] token, as it does not correspond to a real image patch. 5. Construct FUNGI features as in Equation 8, where $f(x)$ maps an image to patches of dimension $d$ , $L_{2}$ normalization is defined as $z^{\\prime}=z/||\\boldsymbol{z}||_{2}$ and cat indicates concatenation. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F=\\mathsf{c a t}^{\\prime}\\left(\\nabla^{\\prime}\\mathcal{L}_{\\mathrm{SimCLR}},f^{\\prime}(x)\\right)\\qquad f(x):\\mathbb{R}^{3\\times512\\times512}\\to\\mathbb{R}^{1024\\times d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.3 Text Classification Experimental Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The parameters used to extract gradients from text encoders for $\\mathcal{L}_{\\mathrm{KL}}$ and $\\mathcal{L}_{\\mathrm{SimCLR}}$ are shown in Table 24. The gradient source layer is always the attention output projection of the last transformer encoder block. We use the same parameters across backbones. No data augmentation is used for the $\\mathcal{L}_{\\mathrm{KL}}$ , while for $\\mathcal{L}_{\\mathrm{SimCLR}}$ the views are obtained by randomly deleting words independently, where each word has a $10\\%$ probability of being deleted. ", "page_idx": 23}, {"type": "table", "img_path": "", "table_caption": ["Table 23: In-context scene understanding setup. Parameters (left) and data augmentation (right) used for the in-context scene understanding task for both full dataset and few shot setups. For the computation of $\\mathcal{L}_{\\mathrm{SimCLR}}$ we use 1025 views as we include the [CLS] token, which is discarded afterwards. The retrieved negatives indicate the number of neighbors retrieved from the support index, while the loss negatives the number of neighbors used for the loss computation. The color data augmentations are applied independently, in the order shown here. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/f7fd005b0fa757fffb62f363a34e914ce1c6ceaf320a146d32ae2999e84782cc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/cc2ad9a5ba6b552022ce9afd4707ce8dc0a3bdcf92835c93a3cdcc268ac3466b.jpg", "table_caption": ["Table 24: Text classification experimental details. Parameters used to extract text encoder gradients for the $\\mathcal{L}_{\\mathrm{KL}}$ (left) and $\\mathcal{L}_{\\mathrm{SimCLR}}$ (right) objectives. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D Data and Models ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We investigate the performance of our gradient-enhanced features on 11 image classification datasets, namely CIFAR 10 and CIFAR 100 (Krizhevsky et al., 2009), Oxford Flowers 102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), ImageNet-1K (Russakovsky et al., 2015), FGVC Aircraft (Maji et al., 2013), CUB 200-2011 (Wah et al., 2011), Oxford-IIT Pets (Parkhi et al., 2012), Stanford Cars (Krause et al., 2013), DTD Textures (Cimpoi et al., 2014) and EuroSAT (Helber et al., 2019), 5 text classification datasets: TREC (Li & Roth, 2002) in its coarse version, banking-77 (Casanueva et al., 2020), Stanford Sentiment Treebank (SST) (Socher et al., 2013) in its fine-grained version, AG news (Zhang et al., 2015; Gulli, 2005) and tweet eval (emoji) (Barbieri et al., 2018, 2020) and 2 audio classification datasets: ESC 50 (Piczak, 2015), an environmental sound classification dataset, and SpeechCommands V2 (Warden, 2018), a keyword spotting task, where the goal is to classify utterances into a predefined set of words. The datasets, their license and citations are also listed in Table 25. ", "page_idx": 24}, {"type": "text", "text": "We follow the evaluation protocol for each individual dataset and report the top-1 accuracy for CIFAR 10 and 100, Food101, ImageNet-1K, Stanford Cars, EuroSAT, DTD Textures, CUB 200-2011, TREC, banking-77, SST, AG news, tweet eval (emoji), ESC 50 and SpeechCommands V2, and the mean-per-class accuracy for Flowers102, FGVC Aircraft and Oxford-IIT Pets. ", "page_idx": 24}, {"type": "text", "text": "We use the default splits defined by torchvision or the dataset authors where possible. As EuroSAT does not explicitly define a test split, we use an 80/20 stratified split as indicated by the dataset paper. We always report metrics on the test splits, with the exception of ImageNet, for which we use the validation split. ", "page_idx": 25}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/cd1427d134edf4ee11d39b2dbaa61f8d66f00da1b9720c5e59c7719750cf031a.jpg", "table_caption": ["Table 25: Datasets. Summary table of all datasets used in this paper, their license and citation. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "We evaluate FUNGI features across several architectures, pretraining strategies and model sizes. These are listed in Table 26, alongside their license, data type and citation. ", "page_idx": 25}, {"type": "text", "text": "E Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The gradient features were extracted using a machine with a single NVIDIA A100 GPU with 40GB of VRAM. Considering the inference times listed in Table 27, replicating the k-nearest neighbor image classification results would require approximately 27 GPU hours per backbone using float16, which we use throughout all our experiments. As we evaluate our method across 17 vision backbones, reproducing these results would require 459 GPU hours. As for the text and audio classification experiments, they require around 3 GPU hours per backbone, for a total of 9 hours. The extracted gradient features were reused for the linear probing and clustering experiments, the former requiring 168 hours on a machine with a single AMD EPYC 7H12 CPU and the latter requiring 18 hours on a machine with a single NVIDIA A100 GPU with 40GB of VRAM. ", "page_idx": 25}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/c9aff74601e1c8689596e68be3b88540776c1539fa687f49081f329082ca18bf.jpg", "table_caption": ["Table 26: Models used in the paper. Summary table of all architectures/pretraining strategies evaluated in the paper, along with their license, citation, and implementation, if applicable. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 27: FUNGI introduces a speed overhead. Embeddings and gradients extraction speed measured in images/second on an NVIDIA A100 GPU for a DeIT ViT-B/16 backbone. The gradients speed include the random projection step. The performance column reports the accuracy averaged across 11 datasets for the combination of a single gradient with the model embeddings. $^{\\dagger}$ indicates $\\boldsymbol{\\mathrm{k}}$ -nearest neighbor inference on CPU. ", "page_idx": 26}, {"type": "table", "img_path": "PRBsEz8rnV/tmp/7a59af075ba33a97e5b7903c67832f67b88cbde2403f3679fbdf9e2bcd04a0b8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Finally, additional experiments such as image retrieval, in-context learning, and ablation studies required approximately 84 hours, while the preliminary experiments for this paper required a negligible amount of compute. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: we analyze the predictive abilities of gradients and show that they are complementary the model embeddings in Section 3. We show that our method improves $\\mathbf{k}$ -nearest neighbor classification for vision, text and audio in Sections 5.1 and 5.3. We evaluate FUNGI in linear probing in Appendix B.3, image retrieval in Appendix B.1 and visual in-context learning in Section 4.3. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: we discuss limitations concerning the backbone pretraining datasets and the computational efficiency in Section 6. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: we do not provide any theoretical result in the paper. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: we describe all our self-supervised objectives and data augmentation in Section 4. Additional details regarding the self-supervised objectives are described in Appendices C.1, C.2 and C.3. The complete set of hyperparameters used for all our experiments are listed in Table 22, Table 23 and Table 24. Furthermore, Appendix A contains pytorch-like pseudocode that illustrates how to perform gradient extraction for one of our objectives. Finally, we open sourced the code used to run our experiments at https://github.com/WalterSimoncini/no-train-all-gain. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: we evaluate our method using only publicly available datasets and backbones, and we release the code to reproduce our experimental results at this URL https:// github.com/WalterSimoncini/no-train-all-gain. The code documentation and the supplementary materials contain all the information needed to replicate our experiments. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: to the best of our knowledge, we have provided all the necessary hyperparameters, dataset splits, and experimental details to fully reproduce our results. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In Table 10, we show that our method is consistent across three seeds by reporting the average performance and one standard deviation. The results support our claim that FUNGI features provide consistent improvements over the model embeddings. We calculate the standard deviation using numpy.std. We only report the standard deviation for this experiment, as computing it for all backbone and dataset combinations would require a significant amount of compute. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We fully disclose the hardware and the run time of all our experiments in appendix E. ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: we have read the code of ethics and made sure that the paper conforms to it in every aspect. ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: in the main paper, we discuss potential broader impacts, but as we make a fundamental contribution to machine learning, we do not foresee any direct negative consequences from our method. ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: we properly cite all models, datasets, and code implementations used to run our experiments. A summary of all models and datasets used in the paper are available in Table 26 and Table 25, along with their license and citation. We made sure to adhere to the usage guidelines and license of each individual asset. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: we release two new assets: (a) the code to replicate our experimental results, available at https://github.com/WalterSimoncini/no-train-all-gain and (b) a library to extract FUNGI features from ViT backbones, available at https://github.com/ WalterSimoncini/fungivision. Both repositories include proper documentation and are released under the MIT license. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 29}]