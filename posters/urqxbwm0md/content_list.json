[{"type": "text", "text": "Cryptographic Hardness of Score Estimation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Min Jae Song Paul G. Allen School of Computer Science and Engineering University of Washington mjsong32@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We show that $L^{2}$ -accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard even when sample complexity is polynomial in the relevant problem parameters. Our reduction builds on the result of Chen et al. (ICLR 2023), who showed that the problem of generating samples from an unknown data distribution reduces to $L^{2}$ -accurate score estimation. Our hard-toestimate distributions are the \u201cGaussian pancakes\u201d distributions, originally due to Diakonikolas et al. (FOCS 2017), which have been shown to be computationally indistinguishable from the standard Gaussian under widely believed hardness assumptions from lattice-based cryptography (Bruna et al., STOC 2021; Gupte et al., FOCS 2022). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models [70, 72, 42, 73] have firmly established themselves as a powerful approach to generative modeling, serving as the foundation for leading image generation models such as DALL-E 2 [62], Imagen [66], and Stable Diffusion [65]. A diffusion model consists of a pair of forward and reverse processes. In the forward process, noise drawn from a standard distribution, such as the standard Gaussian, is sequentially applied to data samples, leading its distribution to a pure noise distribution in the limit. The reverse process, as the name suggests, reverses the noising process and takes the pure noise distribution \u201cbackward in time\u201d to the original data distribution, thereby allowing us to generate new samples from the data distribution. A key element in implementing the reverse process is the score function of the data distribution, which is the gradient of its log density. Since the data distribution is typically unknown, the score function must be learned from samples [44, 78, 72]. ", "page_idx": 0}, {"type": "text", "text": "Recent advances in the theory of diffusion models have revealed that the task of sampling, in fact, reduces to score estimation under minimal assumptions on the data distribution [9, 23, 53, 61, 22, 17, 51, 6]. In particular, Chen et al. [17] have shown that $L^{2}$ -accurate score estimates along the forward process are sufficient for efficient sampling. Thus, assuming access to an oracle for $L^{2}$ -accurate score estimation, one can efficiently sample from essentially any data distribution. However, this leaves open the question of whether score estimation oracles themselves can be implemented efficiently, in terms of both required sample size and computation, for interesting classes of distributions. ", "page_idx": 0}, {"type": "text", "text": "We show that $L^{2}$ -accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard, even when sample complexity is polynomial in the relevant problem parameters. This establishes a statistical-to-computational gap for $L^{2}$ -accurate score estimation, which refers to an intrinsic gap between what is statistically achievable and computationally feasible. Our hard-to-estimate distributions are the \u201cGaussian pancakes\u201d distributions, which previous works [31, 13, 39] have shown are computationally indistinguishable from the standard Gaussian under plausible and widely believed hardness assumptions. In fact, \u201cbreaking\u201d the hardness of Gaussian pancakes, by means of an efficient detection or estimation algorithm, has profound implications for lattice-based cryptography, which is central to the post-quantum cryptography standardization led by the National Institute of Standards and Technology (NIST) [58]. Building on the sampling-toscore estimation reduction by Chen et al. [17], we show that computationally efficient $L^{2}$ -accurate score estimation for Gaussian pancakes implies an efficient algorithm for distinguishing Gaussian pancakes from the standard Gaussian. Thus, while sampling may ultimately reduce to ${\\bar{L^{2}}}$ -accurate score estimation under minimal assumptions on the data distribution, score estimation itself requires stronger assumptions on the data distribution for computational feasibility. It is worth noting that the presence of statistical-to-computational gaps in $L^{2}$ -accurate score estimation was anticipated by Chen et al. [17, Section 1.1], who mentioned it without formal statement or proof. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Main contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our main result is a simple reduction from the Gaussian pancakes problem (i.e., the problem of distinguishing Gaussian pancakes from the standard Gaussian) to $L^{2}$ -accurate score estimation. We show that given oracle access to $L^{2}$ -accurate score estimates along the forward process (Assumption A3), one can compute a test statistic that distinguishes, with non-trivial success probability, whether the given score estimates belong to a Gaussian pancakes distribution or the standard Gaussian. ", "page_idx": 1}, {"type": "text", "text": "A Gaussian pancakes distribution $P_{u}$ with secret direction $\\pmb{u}\\in\\mathbb{S}^{d-1}$ can be viewed as a \u201cbackdoored\u201d Gaussian. It is distributed as a (noisy) discrete Gaussian along the direction $\\textbf{\\em u}$ and as a standard Gaussian in the remaining $d-1$ directions (see Figure 1).1 A class of Gaussian pancakes $(P_{u})_{u\\in\\mathbb{S}^{d-1}}$ is parameterized by two parameters, $\\gamma$ and $\\sigma$ , which govern the spacing and thickness of pancakes, respectively. For instance, a Gaussian pancakes distribution $P_{u}$ with spacing $\\gamma$ and thickness $\\sigma\\approx0$ is essentially supported on the one-dimensional lattice $(1/\\gamma)\\mathbb{Z}$ along the secret direction $\\textbf{\\em u}$ . The Gaussian pancakes problem, then, is a sequence of hypothesis testing problems indexed by the data dimension $d\\in\\mathbb{N}$ in which the goal is to distinguish between samples from a Gaussian pancakes distribution (with unknown $\\textbf{\\em u}$ ) and the standard Gaussian distribution $\\bar{\\mathcal{N}}(0,I_{d})$ with success probability slightly better than random guessing (see Section 2.3 for formal definitions). Thus, our result can be summarized informally as follows. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Informal, see Theorem 3.1). Let $\\gamma(d)>1,\\sigma(d)>0$ be sequences such that $\\sigma\\geq$ $1/\\mathrm{poly}(d)$ and the corresponding $(\\gamma,\\sigma)$ -Gaussian pancakes distributions $\\overline{{(P_{u})}}_{u\\in\\mathbb{S}^{d-1}}$ all satisfy $\\dot{\\mathrm{TV}}(\\dot{P_{u}},\\dot{N}(0,I_{d}))>1/2$ . Then, there exists a\u221a polynomial-time randomized algorithm with access to a score estimation oracle of $L^{2}$ -error $O(1/{\\sqrt{\\log d}})$ that solves the Gaussian pancakes problem. ", "page_idx": 1}, {"type": "text", "text": "We emphasize that the hardness of estimating score functions of Gaussian pancakes distributions arises solely from hardness of learning. The score function of $P_{u}$ is efficiently approximable by function classes commonly used in practice for generative modeling, such as residual networks [40]. In addition, under the scaling $\\sigma\\gamma=O(1)$ , which includes the cryptographically hard regime, the secret parameter $\\textbf{\\em u}$ can be estimated upto $L^{2}$ -error $\\eta$ via brute-force search over $\\mathbb{S}^{d-1}$ with p $\\mathrm{)}\\mathrm{ly}(d,\\gamma,1/\\eta)$ samples (Theorem 4.2). The estimated parameter $\\hat{\\pmb u}$ in turn enables $L^{2}$ -accurate score estimation (see Section 4). Our estimator, based on projection pursuit [35, 43], may be of independent interest. ", "page_idx": 1}, {"type": "text", "text": "We also analyze properties of Gaussian pancakes using Banaszczyk\u2019s theorems on the Gaussian mass of lattices [3, 74]. This serves two purposes. Firstly, it allows us to verify that Gaussian pancakes distributions readily satisfy the assumptions for the sampling-to-score estimation reduction of Chen et al. [17], namely Lipschitzness of the score functions along the forward process (Assumption A1). This is necessary as the proof of our main theorem crucially relies on the reduction. Secondly, Banaszczyk\u2019s theorems provide simple means of analyzing properties of Gaussian pancakes, which are interesting mathematical objects in their own right. While these theorems are standard tools in lattice-based cryptography (see e.g., [74, 1]), they are likely less known outside the community. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Theory of diffusion models. Recent advances in the theoretical study of diffusion models have focused on convergence rates of discretized reverse processes [9, 23, 53, 61, 22, 17, 51, 6]. Of particular relevance to our work is the result of Chen et al. [17] who showed that $L^{2}$ -accurate score estimates are sufficient to guarantee convergence rates that are polynomial in all the relevant problem parameters under minimal assumptions on the data distribution, namely Lipschitz scores throughout the forward process and finite second moment. Prior studies fell short by requiring strong structural assumptions on the data distribution, such as a log-Sobolev inequality [50, 82], assuming ", "page_idx": 1}, {"type": "image", "img_path": "URQXbwM0Md/tmp/d8e3eb5b7948570c3e62801d6ea62dc1f2f2a37e0ba7da7cbaec35c1653c2de2.jpg", "img_caption": ["Figure 1: Top: Scatter plot of 2D Gaussian pancakes $P_{u}$ with secret direction $\\pmb{u}=(-1/\\sqrt{2},1/\\sqrt{2})$ , spacing $\\gamma=6$ , and thickness $\\sigma\\in\\{0.01,\\bar{0}.05,0.25\\}$ . Bottom: Re-scaled probability densities of Gaussian pancakes (blue) for each $\\sigma\\in\\{0.01,0.05,0.25\\}$ and the standard Gaussian (black) along $\\textbf{\\em u}$ . For fixed $\\gamma$ , the pancakes \u201cblur into each other\u201d as $\\sigma$ increases. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "$L^{\\infty}$ -accurate score estimates [23], or providing convergence rates that are exponential in the problem parameters [9, 23, 22]. We note that recent works have made the \u201cminimal\u201d assumptions of Chen et al. [17] even more minimal by considering early stopped reverse processes and dropping the Lipschitz score assumption [15, 6]. We refer to the book draft of Chewi [19] for more background. ", "page_idx": 2}, {"type": "text", "text": "Gaussian pancakes. The Gaussian pancakes problem stands out among problems exhibiting statistical-to-computational gaps due to its versatility and strong hardness guarantees. Initially introduced as hard-to-learn Gaussian mixtures in the work of Diakonikolas et al. [31], which established their SQ hardness, Gaussian pancakes have been extensively utilized in establishing SQ lower bounds for various statistical inference problems such as robust Gaussian mean estimation [31] and agnostically learning halfspaces and ReLUs over Gaussian inputs [28]. For further details, we refer to the textbook by Diakonikolas and Kane [29, Chapter 8]. Gaussian pancakes distributions themselves serve as instances of fundamental high-dimensional inference problems such as non-Gaussian component analysis [8] and Wasserstein distance estimation in the spiked transport model [57]. ", "page_idx": 2}, {"type": "text", "text": "Bruna et al. [13] initiated the exploration of the cryptographic hardness of Gaussian pancakes. They showed that assuming hardness of worst-case lattice problems, fundamental to lattice-based cryptography [56, 60], both the Gaussian pancakes and the closely related continuous learning with errors (CLWE) problems are hard. Follow-up work by Gupte et al. [39] showed that the learning with errors (LWE) problem [63], a versatile problem which lies at the heart of numerous lattice-based cryptographic constructions, reduces to the Gaussian pancakes as well. These cryptographic hardness results have sparked a wave of recent works showcasing various applications of this newly discovered property of Gaussian pancakes. Notable examples include planting undetectable backdoors in machine learning models [38], novel public-key encryption schemes based on Gaussian pancakes [10], and cryptographic hardness of agnostically learning halfspaces [26, 75]. ", "page_idx": 2}, {"type": "text", "text": "For additional related work on score estimation and statistical-to-computational gaps, see Section A. ", "page_idx": 2}, {"type": "text", "text": "1.3 Future directions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our work brings together the latest advances from the theory of diffusion models and computational complexity of statistical inference. This intersection provides fertile ground for future research, some avenues of which we outline below. ", "page_idx": 2}, {"type": "text", "text": "Stronger data assumptions for efficient score estimation. Our result shows that for any class of distributions that encompasses hard Gaussian pancakes, computationally efficient $L^{2}$ -accurate score estimation is impossible. This implies that stronger assumptions on the data, which exclude Gaussian pancakes, are necessary for efficient score estimation. Finding assumptions that exclude hard instances while being able to capture realistic models of data is an interesting open problem. ", "page_idx": 3}, {"type": "text", "text": "Weaker criteria for evaluating sample quality. In the context of learning Gaussian pancakes, if the goal of the sampling algorithm were to merely fool computationally bounded testers that lack knowledge of the secret $\\pmb{u}\\in\\mathbb{S}^{d-1}$ , then it could simply generate standard Gaussian samples and fool any polynomial-time test. Thus, sampling is strictly easier than $L^{2}$ -accurate score estimation if the criteria for evaluating sample quality is less stringent. This suggests exploring sampling under different evaluation criteria, such as \u201cdiscriminators\u201d with bounded compute or memory. For example, Christ et al. [20] have used weaker notions of sample quality in the context of watermarking large language models (LLMs). More precisely, they used the notion of computational indistinguishability to guarantee quality of the watermarked model relative to the original model. Exploring potential connections to the literature on leakage simulation [45, 18, 76] and outcome indistinguishability [41, 32, 33] is also an interesting future direction, as these areas have addressed related questions for distributions on finite sets. ", "page_idx": 3}, {"type": "text", "text": "Extracting \u201cknowledge\u201d from sampling algorithms. A key difficulty in directly reducing the Gaussian pancakes problem to sampling is that the Gaussian pancakes problem is hard for polynomialtime distinguishers, even with access to exact sampling oracles (see Section 2.3 for more details).2 Thus, any procedure utilizing the learned sampler in a black box manner cannot solve the Gaussian pancakes problem. This is puzzling since for the algorithm to have \u201clearned\u201d to generate samples from the given distribution, it ought to possess some non-trivial information about it (e.g., leak information about the secret parameter $\\pmb{u}$ )! ", "page_idx": 3}, {"type": "text", "text": "This raises the question: How much \u201cknowledge\u201d can we extract with white box access to the sampling algorithm? Under reasonable structural assumptions on the sampling algorithm, can we extract privileged information about the data distribution it simulates, beyond what is obtainable solely via sample access? Our work provides one such example. White box access to a diffusion model gives access to its score estimates. These score estimates, which enable efficient solutions to the Gaussian pancakes problem, constitute privileged knowledge that cannot be learned efficiently even with unlimited access to bona fide samples. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation. We denote by $\\left(a_{t}\\right)$ the sequence $a_{1},a_{2},a_{3},\\ldots$ indexed by $t\\,\\in\\,\\mathbb{N}$ . When there is no natural ordering on the index set (e.g., $(a_{\\mathbf{\\boldsymbol{u}}})_{\\mathbf{\\boldsymbol{u}}\\in\\mathbb{S}^{d-1}})$ , we interpret it as a set. We write $a\\lesssim b$ or $a=O(b)$ to mean that $a\\leq C b$ for some universal constant $C>0$ . The notation $a\\gtrsim b$ and $a=\\Omega(b)$ are defined analogously. We write $a\\,\\asymp\\,b$ or $a\\,=\\,\\Theta(b)$ to mean that $a\\lesssim b$ and $a\\gtrsim$ both hold. We write $\\wedge,\\vee$ to mean logical AND and OR, respectively. We also write $a\\wedge b$ and $a\\vee b$ to mean $\\operatorname*{min}(a,b)$ and $\\operatorname*{max}(a,b)$ , respectively. We denote by $Q_{d}$ the \u201cstandard\u201d Gaussian $\\mathcal{N}(0,1/(2\\pi)I_{d})$ (see Remark 2.3). We omit the subscript $d$ when it is clear from context. ", "page_idx": 3}, {"type": "text", "text": "2.1 Background on denoising diffusion probabilistic modeling (DDPM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We give a brief exposition on denoising diffusion probabilistic models (DDPM) [42], a specific type of diffusion model, since the main reduction of Chen et al. [17] pertains to DDPMs. This section closely follows [17, Section 2.1] and [80, Section 3] ", "page_idx": 3}, {"type": "text", "text": "Let $D$ be the target distribution defined on $\\mathbb{R}^{d}$ . In DDPMs, we begin with the forward process, an Ornstein-Uhlenbeck (OU) process that converges towards $Q=\\mathcal{N}(0,(1/2\\pi)I_{d})$ , whic\u221ah is described by the following stochastic differential equation (SDE). Note that the constant $1/\\sqrt{\\pi}$ in front of $d W_{t}$ in Eq.(1) is non-standard.3 See Remark 2.3 for an explanation of our unconventional choice of variance for the resulting stationary distribution $Q$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nd X_{t}=-X_{t}d t+(1/\\sqrt{\\pi})d W_{t}\\ ,\\qquad X_{0}\\sim D_{0}=D\\ ,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W_{t}$ is the standard Brownian motion in $\\mathbb{R}^{d}$ . ", "page_idx": 4}, {"type": "text", "text": "Let $D_{t}$ be the distribution of $X_{t}$ along the OU process. It is well-known that for any distribution $D$ , $D_{t}\\to Q$ exponentially fast in various divergences and metrics such as the KL divergence [2]. In this work, we only consider Gaussian pancakes $\\left(P_{u}\\right)$ and the standard Gaussian $Q$ . If $D=P_{u}$ and $t>0$ , then the distribution $D_{t}$ is simply another Gaussian pancakes distribution with a larger thickness parameter (see Definition 2.5). Meanwhile, if $D=Q$ , then $D_{t}=Q$ for any $t\\geq0$ . We run the OU process until time $T>0$ , and then simulate the reverse process, described by the following SDE. ", "page_idx": 4}, {"type": "equation", "text": "$$\nd Y_{t}=(Y_{t}+(1/\\pi)\\nabla\\log D_{T-t}(Y_{t}))d t+(1/\\sqrt{\\pi})d W_{t}\\;,\\qquad Y_{0}\\sim F_{0}=D_{T}\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\nabla\\log{D_{t}}$ is called the score function of $D_{t}$ . Since the target $D$ is not known, in order to implement the reverse process the score function must be estimated from data samples. Assuming for the moment that we have exact scores $(\\nabla\\log D_{t})_{t\\in[0,T]}$ , if we start the reverse process from $F_{0}=D_{T}$ , we have $Y_{t}\\sim F_{t}=D_{T-t}$ for any $0\\le t\\le T$ , and ultimately $Y_{T}\\sim F_{T}=D$ . Thus, starting from (approximately) pure noise $D_{T}\\approx Q$ , the reverse process generates fresh samples from the target distribution $D$ . We need to make several approximations to algorithmically implement this reverse process. In particular, we need to approximate $D_{T}$ by $Q$ , discretize the continuous-time SDE, and approximate scores along the (discretized) forward process. Let $h>0$ be the step size for the SDE discretization and denote $N:=T/h$ . Given score estimates $(s_{k h})_{k\\in[N]}$ , the DDPM algorithm performs the following update (see e.g., [67, Chapter 4.3]). ", "page_idx": 4}, {"type": "equation", "text": "$$\ny_{k+1}=e^{h}y_{k}+(1/\\pi)(e^{h}-1)s_{(N-k)h}(y_{k})+\\sqrt{e^{2h}-1}z_{k}\\ ,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $z_{k}\\sim Q$ is an independent Gaussian vector. Note that the only dependence of the reverse process on the target distribution $D$ arises through the score estimates $(s_{k h})_{k\\in[N]}$ . ", "page_idx": 4}, {"type": "text", "text": "The result of Chen et al. [17] demonstrates polynomial convergence rates of this process to the target distribution $D$ , assuming access to $L^{2}$ -accurate score estimates $(s_{k h})_{k\\in[N]}$ and minimal conditions on the target $D$ . We refer to Section 3.1 for a formal statement of their assumptions and theorem. ", "page_idx": 4}, {"type": "text", "text": "2.2 Lattices and discrete Gaussians ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Lattices. A lattice $\\mathcal{L}\\subset\\mathbb{R}^{d}$ is a discrete additive subgroup of $\\mathbb{R}^{d}$ . In this work, we assume all lattices are full rank, i.e., their linear span is $\\mathbb{R}^{d}$ . For a $d$ -dimensional lattice $\\mathcal{L}$ , a set of linearly independent vectors $\\{b_{1},\\ldots,b_{d}\\}$ is called a basis of $\\mathcal{L}$ if $\\mathcal{L}$ is generated by the set, i.e., $\\mathcal{L}\\,=\\,B\\mathbb{Z}^{\\dot{d}}$ where $B=[b_{1},\\ldots,b_{d}]$ . The determinant of a lattice $\\mathcal{L}$ with basis $B$ is defined as $\\operatorname*{det}(\\mathcal{L})=|\\operatorname*{det}(B)|$ . ", "page_idx": 4}, {"type": "text", "text": "The dual lattice of a lattice $\\mathcal{L}$ , denoted by $\\mathcal{L}^{\\ast}$ , is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}=\\left\\{y\\in\\mathbb{R}^{d}\\ \\vert\\ \\langle\\pmb{x},y\\rangle\\in\\mathbb{Z}\\;\\mathrm{for}\\;\\mathrm{all}\\;x\\in\\mathcal{L}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If $B$ is a basis of $\\mathcal{L}$ then $(B^{T})^{-1}$ is a basis of $\\mathcal{L}^{\\ast}$ ; in particular, $\\operatorname*{det}(\\mathcal{L}^{*})=\\operatorname*{det}(\\mathcal{L})^{-1}$ . ", "page_idx": 4}, {"type": "text", "text": "Fourier analysis. We define the Fourier transform of a function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{C}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{f}({\\pmb y})=\\int_{{\\mathbb R}^{d}}f({\\pmb x})\\exp(-2\\pi i\\langle{\\pmb x},{\\pmb y}\\rangle)d{\\pmb x}\\;.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The Poisson summation formula offers a valuable tool for analyzing functions defined on lattices. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.1 (Poisson summation formula). For any lattice $\\mathcal{L}\\subset\\mathbb{R}^{d}$ and any function $f:\\mathbb{R}^{d}\\to\\mathbb{C},^{4}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\mathcal{L})=\\operatorname*{det}(\\mathcal{L}^{*})\\cdot\\widehat{f}(\\mathcal{L}^{*})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we denote $\\textstyle f(S)=\\sum_{x\\in S}f(x)$ for any set $S$ . ", "page_idx": 4}, {"type": "text", "text": "4To be precise, $f$ must satisfy some niceness conditions; this will always hold in our applications. ", "page_idx": 4}, {"type": "text", "text": "Discrete Gaussians. A discrete Gaussian is a discrete distribution whose probability mass function is given by the Gaussian function. These distributions are closely related to Gaussian pancakes distributions and their properties will be crucial for our analysis. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.2 (Gaussian function). We define the Gaussian function $\\rho_{s}:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ of width $s>0$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{s}({\\pmb x}):=\\exp(-\\pi\\|{\\pmb x}\\|^{2}/s^{2})\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When $s=1$ , we omit the subscript and simply write $\\rho$ . In addition, for any lattice $\\mathcal{L}\\subset\\mathbb{R}^{d}$ , we denote by $\\begin{array}{r}{\\rho_{s}(\\mathcal{L})=\\sum_{\\pmb{x}\\in\\mathcal{L}}\\rho_{s}(\\pmb{x})}\\end{array}$ the corresponding Gaussian mass of $\\mathcal{L}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 2.3 (Non-standard choice of \u201cstandard\u201d variance). We refer to $Q_{d}=\\mathcal{N}(0,1/(2\\pi)I_{d})$ as the \u201cstandard\u201d Gaussian. This is indeed the standard choice in lattice-based cryptography because it simplifies normalization factors that arise from taking Fourier transforms. For instance, it allows us to simply write ${\\widehat{\\rho}}_{s}=s^{n}\\rho_{1/s}$ and ${\\widehat{\\rho}}=\\rho$ for $s=1$ . To translate these results for the \u201cusual\u201d standard Gaussian, we can simply replace s with $s/\\sqrt{2\\pi}$ for each occurrence. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.4 (Discrete Gaussian). For any lattice $\\mathcal{L}\\subset\\mathbb{R}^{d}$ , parameter $s>0$ , and shift $t\\in\\mathbb{R}^{d}$ , the discrete Gaussian $D_{\\mathcal{L}-t,s}$ is a distribution supported on the coset $\\mathcal{L}-t$ with probability mass ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{\\mathcal{L}-t,s}(\\pmb{x})=\\frac{\\rho_{s}(\\pmb{x}-t)}{\\rho_{s}(\\mathcal{L}-t)}\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We denote by $A_{\\gamma}$ the discrete Gaussian of width $s=1$ supported on the one-dimensional lattice $(1/\\gamma)\\mathbb{Z}$ . The distribution of a Gaussian pancakes distribution $P_{u}$ along the hidden direction is a smoothed discrete Gaussian, which we formalize in the following. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.5 (Smoothed discrete Gaussian). For any $\\gamma>0$ , let $A_{\\gamma}$ be the discrete Gaussian of width 1 on the lattice $(1/\\gamma)\\mathbb{Z}$ . We define the $\\sigma$ -smoothed discrete Gaussian $A_{\\gamma}^{\\sigma}$ as the distribution of the random variable $y$ induced by the following process. ", "page_idx": 5}, {"type": "equation", "text": "$$\ny=\\frac{1}{\\sqrt{1+\\sigma^{2}}}(x+\\sigma z)\\ ,\\ \\,w h e r e\\ x\\sim A_{\\gamma}\\ a n d\\,z\\sim Q\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore, the density of $A_{\\gamma}^{\\sigma}$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{\\gamma}^{\\sigma}(z)=\\frac{\\sqrt{1+\\sigma^{2}}}{\\sigma\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{k\\in\\mathbb{Z}}\\rho(k/\\gamma)\\rho_{\\sigma/\\sqrt{1+\\sigma^{2}}}(z-k/\\gamma\\sqrt{1+\\sigma^{2}})\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Likelihood ratio of smoothed discrete Gaussians. Let $A_{\\gamma}^{\\sigma}$ be the $\\sigma$ -smoothed discrete Gaussian on $(1/\\gamma)\\mathbb{Z}$ . Its likelihood ratio $T_{\\gamma}^{\\sigma}$ with respect to the standard Gaussian is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nT_{\\gamma}^{\\sigma}(z)=\\frac{\\sqrt{1+\\sigma^{2}}}{\\sigma\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{k\\in\\mathbb{Z}}\\rho_{\\sigma}(z-\\sqrt{1+\\sigma^{2}}k/\\gamma)~.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When $\\gamma$ and $\\sigma$ are clear from context, we omit them and simply denote the likelihood ratio by $T$ . ", "page_idx": 5}, {"type": "text", "text": "2.3 Gaussian pancakes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We define Gaussian pancakes distributions using the likelihood ratio $T_{\\gamma}^{\\sigma}=A_{\\gamma}^{\\sigma}/Q$ . It is important to note that our parametrization differs from the one used in previous works [13, 39]. We believe our parametrization is more convenient as it elucidates a natural partial ordering on the space of parameters $(\\gamma,\\sigma)$ . In addition, there is an explicit mapping between the two different parametrizations, so computationally hard parameter regimes identified by previous works [13, 39] can readily be translated into setting. See Remark 2.7 for more details. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.6 (Gaussian pancakes). For any $d\\in\\mathbb{N}$ , spacing and thickness parameters $\\gamma,\\sigma>0$ , we define the $(\\gamma,\\sigma)$ -Gaussian pancakes distribution $P_{\\gamma,u}^{\\sigma}$ with secret direction $\\pmb{u}\\in\\mathbb{S}^{d-1}$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{\\gamma,u}^{\\sigma}(x):=Q(\\pmb{x})\\cdot T_{\\gamma}^{\\sigma}(\\langle\\pmb{x},\\pmb{u}\\rangle)\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Q=\\mathcal{N}(0,(1/2\\pi)I_{d})$ and $T_{\\gamma}^{\\sigma}$ is the likelihood ratio of $A_{\\gamma}^{\\sigma}$ with respect to $Q$ . When parameters $\\gamma,\\sigma$ are clear from context, we omit them in the notation and simply denote the distribution by $P_{u}$ to avoid clutter. ", "page_idx": 5}, {"type": "text", "text": "Remark 2.7 (Partial ordering on Gaussian pancakes). The smoothed discrete Gaussian $A_{\\gamma}^{\\sigma}$ arises in the OU process for the discrete Gaussian $A_{\\gamma}$ at time $t=\\log\\left(\\sqrt{1+\\sigma^{2}}\\right)$ . Consequently, for any fixed $\\gamma>0$ , there exists a natural partial ordering on the family of Gaussian pancakes parametrized by $(\\gamma,\\sigma)$ , given by $(\\gamma,\\sigma_{1})\\leq(\\gamma,\\bar{\\sigma}_{2})$ whenever $\\sigma_{1}\\leq\\sigma_{2}$ . This ordering arises from the fact that $A_{\\gamma}^{\\sigma_{1}}$ reduces to $A_{\\gamma}^{\\sigma_{2}}$ whenever $\\sigma_{1}\\leq\\sigma_{2}$ via the OU process starting at $t_{1}=\\log{\\left(\\sqrt{1+\\sigma_{1}^{2}}\\right)}$ and run until $t_{2}=\\log{\\left(\\sqrt{1+\\sigma_{2}^{2}}\\right)}$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 2.8 (Advantage). Let $A:\\mathcal{X}\\to\\{0,1\\}$ be any decision rule (i.e., distinguisher). For any pair of distributions $(\\mathcal P,\\mathcal Q)$ on $\\mathcal{X}$ , we define the advantage of $\\boldsymbol{\\mathcal{A}}$ by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha(A):=\\Big|\\mathcal{P}[A(X)=1]-\\mathcal{Q}[A(X)=1]\\Big|\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For a sequence of decision rules $(\\mathcal{A}_{d})_{d\\in\\mathbb{N}}$ and distribution pairs $(\\mathcal{P}_{d},\\mathcal{Q}_{d})_{d\\in\\mathbb{N}}$ , we say $(\\mathcal{A}_{d})$ has non-negligible advantage with respect to $(\\mathcal{P}_{d},\\mathcal{Q}_{d})$ if its advantage sequence ${\\alpha_{d}}\\mathrm{~=~}\\alpha({\\cal A}_{d})$ is $a$ non-negligible function in $d,$ , i.e., a function in $\\Omega(d^{-c})$ for some constant $c>0$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 2.9 (Computational indistinguishability). $A$ sequence of distribution pairs $(\\mathcal{P}_{d},\\mathcal{Q}_{d})$ is computationally indistinguishable if no poly $(d)$ -time computable decision rule achieves nonnegligible advantage. ", "page_idx": 6}, {"type": "text", "text": "Definition 2.10 (Gaussian pancakes problem). For any sequences $\\gamma(d),\\sigma(d)>0$ and $n(d)\\in\\mathbb{N},$ , the $(\\gamma,\\sigma,n)$ -Gaussian pancakes problem is to distinguish $(\\mathcal{P}_{d},\\mathcal{Q}_{d})$ with non-negligible advantage, where $\\mathcal{P}_{d}$ is the $n$ -sample distribution induced by the following two-stage process: $^{\\,l}$ ) draw $\\textbf{\\em u}$ uniformly from $\\mathbb{S}^{d-1}$ , 2) draw n i.i.d. Gaussian pancakes samples $\\pmb{x}_{1},\\pmb{\\ldots},\\pmb{x}_{n}\\sim P_{\\pmb{u}}^{\\otimes n}$ , and $\\mathcal{Q}_{d}=Q_{d}^{\\otimes n}$ , i.e., the distribution of n i.i.d. standard Gaussian vectors. ", "page_idx": 6}, {"type": "text", "text": "As will be explained next, the exact number of samples $n$ is irrelevant for most applications due to the cryptographic hardness of the Gaussian pancakes problem. For certain parameter regimes of $(\\gamma,\\sigma)$ , the problem maintains its computational intractability regardless of the sample size $n$ . ", "page_idx": 6}, {"type": "text", "text": "Hardness of Gaussian pancakes. There is an abundance of evidence demonstrating the hardness of the Gaussian pancakes problem. This makes it compelling to directly assume that Gaussian pancakes and the standard Gaussian are computationally indistinguishable (see Definition 2.9) for certain parameter regimes of $(\\gamma,\\sigma)$ . Initial results by Bruna et al. [13, Corallary 4.2] showed that the Gaussian pancakes p\u221aroblem is as hard as worst-case lattice problems for any parameter sequence $(\\gamma,\\sigma)$ satisfying $\\gamma\\geq2{\\sqrt{d}}$ and $\\sigma\\geq1/\\mathrm{poly}(d)$ . SQ hardness of the problem has been demonstrated as well [31, 13, 27]. Perhaps surprisingly, the reduction of Bruna et al. shows that even with unlimited access to an exact sampling oracle, no polynomial-time algorithm $\\boldsymbol{\\mathcal{A}}$ can achieve non-negligible advantage on the $(\\gamma,\\sigma)$ -Gaussian pancakes problem. This stems from the fact that the running time of $\\boldsymbol{\\mathcal{A}}$ naturally restricts the number of samples it can \u201csee\u201d, resolving the apparent mystery. ", "page_idx": 6}, {"type": "text", "text": "An important follow-up work by Gupte et al. [39] reduced the well-known LWE problem to the Gaussian pancakes problem. Assuming sub-exponential hardness of LWE [52], a standard assumption underlying post-quantum cryptosystems expected to be standardized by NIST, the Gaussian pancakes problem is hard for any $\\gamma\\stackrel{\\cdot}{\\geq}(\\log d)^{1+\\varepsilon}$ , where $\\varepsilon\\,>\\,0$ is any constant, and $\\sigma\\,\\geq\\,1/\\mathrm{poly}(d)$ [39, Section 1.2]. Taken together, these findings strongly support the hardness of Gaussian pancakes for the specified regimes of $(\\gamma,\\sigma)$ . Note, however, that the condition $\\sigma\\geq1/\\mathrm{poly}(d)$ is necessary for hardness as there exist polynomial-time algorithms, based on lattice basis reduction, for exponentially small $\\sigma$ [83, 25]. ", "page_idx": 6}, {"type": "text", "text": "3 Hardness of Score Estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our main result is Theorem 3.1, which presents a reduction from the Gaussian pancakes problem to $L^{2}$ -accurate score estimation. Since the Gaussi\u221aan pancakes problem exhibits both cryptographic and SQ hardness in the parameter regime $\\gamma\\geq2{\\sqrt{d}}$ and $\\sigma\\geq1/\\mathrm{poly}(d)$ [13, 39], these notions of hardness extend to the task of estimating scores of Gaussian pancakes. Further details on the hardness of the Gaussian pancakes problem can be found in Section 2.3. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 (Main result). Let $\\gamma(d)\\,>\\,1,\\sigma(d)\\,>\\,0$ be any pair of sequences such that $\\sigma\\mathrm{~}\\geq$ $1/\\mathrm{poly}(d)$ and the corresponding (sequence of) $(\\gamma,\\sigma)$ -Gaussian pancakes distributions $(P_{u})_{u\\in\\mathbb{S}^{d-1}}$ satisfies $\\mathrm{TV}(P_{\\boldsymbol{u}},Q_{d})>1/2,$ for any $d\\in\\mathbb{N}$ . Then, for any $\\delta\\in(0,1)$ , there exi\u221asts a $\\mathrm{poly}(d)\\cdot\\log(1/\\delta).$ - time algorithm with access to a score estimation oracle of $L^{2}$ -error $O(1/{\\sqrt{\\log d}})$ that solves the $(\\gamma,\\sigma)$ -Gaussian pancakes problem with probability at least $1-\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The requirement $\\mathrm{TV}(P_{\\mathbf{\\boldsymbol{u}}},Q_{d})>1/2$ is mild and entirely captures interesting parameter regimes of $(\\gamma,\\sigma)$ for which cryptographic and SQ hardness of Gaussian pancakes are known. We provide a sufficient condition in Lemma B.9, which shows that $\\gamma\\sigma<C$ for some constant $C>0$ ensures separation in TV distance. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.1 implies that even a score estimation oracle running in time $\\mathrm{poly}(d,2^{1/\\varepsilon^{2}})$ , where $\\varepsilon>0$ is the $L^{2}$ estimation error bound, implies a $\\mathrm{poly}(d)$ time algorithm for the Gaussian pancakes problem. This means that estimating the score functions of Gaussian pancakes to $L^{2}$ -accuracy $\\varepsilon$ even in poly $\\cdot(d,2^{1/\\varepsilon^{2}})$ time is impossible under standard cryptographic assumptions. ", "page_idx": 7}, {"type": "text", "text": "3.1 Proof outline of Theorem 3.1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we sketch the proof of Theorem 3.1 and defer full details to Section B.1. We first recall the sampling-to-score estimation reduction of Chen et al. [17] and its required assumptions to illustrate the main idea behind our reduction. The precise formulation of our idea is given in Lemma 3.3. ", "page_idx": 7}, {"type": "text", "text": "Assumptions on data distribution. The reduction of Chen et al. [17] requires the following assumptions on the data distribution $D$ over $\\mathbb{R}^{d}$ . ", "page_idx": 7}, {"type": "text", "text": "A1 (Lipschitz score). For all $t\\geq0$ , the score $\\nabla\\log D_{t}$ is $L$ -Lipschitz. A2 (Finite second moment). $D$ has finite second moment, i.e., $\\mathfrak{m}_{2}^{2}:=\\mathbb{E}_{{\\pmb{x}}\\sim D}[||{\\pmb{x}}||^{2}]<\\infty$ . A3 (Score estimation error). For step size $h:=T/N$ and all $k=1,\\ldots,N$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D_{k h}}[\\|s_{k h}-\\nabla\\log D_{k h}\\|^{2}]\\leq\\varepsilon_{\\mathrm{score}}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 3.2 ([17, Theorem 2]). Suppose assumptions A1-A3 hold. Let $Q_{d}$ be the standard Gaussian on $\\mathbb{R}^{d}$ and let $F_{T}$ be the output of the DDPM algorithm (Section 2.1) at time $T$ with step size $h:=T/N$ such that $h\\lesssim1/L,$ , where $L\\geq1$ . Then, it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{TV}(F_{T},D)\\lesssim\\underset{c o n v e r g e n c e\\;o f\\underline{{f}}\\rho_{d})\\cdot\\mathrm{exp}(-T)}{\\underbrace{\\sqrt{\\mathrm{KL}(D\\parallel Q_{d})}\\cdot\\exp(-T)}}+}&{\\underbrace{\\left(L\\sqrt{d h}+L\\mathfrak{m}_{2}h\\right)\\sqrt{T}}_{d i s c r e t i z a t i o n\\;e r r o r}\\quad+\\underset{s c o r e\\;e s t i m a t i o n\\;e r r o r}{\\underbrace{\\xi_{s c o r e}\\sqrt{T}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In particular, $i f\\mathfrak{m}_{2}\\leq d,$ , then $T\\asymp\\operatorname*{max}(\\log(\\mathrm{KL}(D\\parallel Q_{d})/\\varepsilon),1)$ and $h\\asymp\\varepsilon^{2}/(L^{2}d)$ gives ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{TV}(F_{T},D)\\lesssim\\varepsilon+\\varepsilon_{\\mathrm{score}}\\cdot\\operatorname*{max}(\\sqrt{\\log(\\mathrm{KL}(D\\parallel Q_{d})/\\varepsilon)},1)\\ ,\\qquad f o r\\ N=\\tilde{\\Theta}\\Bigl(\\frac{L^{2}d}{\\varepsilon^{2}}\\Bigr)\\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 3.2 shows that if the unknown data distribution $D$ has Lipschitz scores and satisfies ${\\mathfrak{m}}_{2}\\leq d$ , then its $\\varepsilon$ -accurate score estimates along the discretized forward process $(s_{k h})_{k\\in[N]}$ can be used to compute a certificate of Gaussianity defined as follows. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Delta:=\\operatorname*{max}_{k\\in[N]}\\mathbb{E}_{Q_{d}}\\Vert s_{k h}({\\pmb x})+2\\pi{\\pmb x}\\Vert^{2}\\;.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using $\\Delta$ as a test statistic, we decide $D\\in(P_{\\mathbf{\\mathit{u}}})_{\\mathbf{\\mathit{u}}\\in\\mathbb{S}^{d-1}}$ if $\\Delta\\geq\\tau$ for some carefully chosen threshold $\\tau>0$ and $D=Q_{d}$ otherwise. This test statistic is motivated by the observation that the discretized reverse process depends on the data distribution $D$ solely through its score estimates (see Eq.(3)). For any sequence of score estimates $(s_{t})_{t\\in[0,T]}$ the reverse process outputs $F_{T}$ that is TV-close to $D$ provided its $L^{2}$ error along the forward process $(D_{t})_{t\\in[0,T]}$ is small. ", "page_idx": 7}, {"type": "text", "text": "We claim that if $\\Delta\\leq\\eta^{2}$ and the score estimates $\\left(s_{k h}\\right)$ are $\\varepsilon$ -accurate for $(D_{k h})$ , then the output of the reverse process $F_{T}$ is roughly $(\\varepsilon+\\eta)$ -close in TV distance to the standard Gaussian. This is because the standard Gaussian is invariant throughout the OU process, so $\\Delta$ is, in fact, the $L^{2}$ score estimation error bound for the case where the data distribution $D$ is equal to $Q_{d}$ . In other words, $\\Delta\\leq\\eta^{2}$ means that for all $k\\,\\in\\,[N]$ , the score estimates $\\left(s_{k h}\\right)$ are $\\eta$ -close to $-2\\pi x$ which is the score function of $Q_{d}$ . Thus, $\\Delta$ is small only if $D$ is close in TV distance to $Q_{d}$ , which shows that $\\Delta$ distinguishes between $D=Q_{d}$ and $D\\in(P_{u})_{u\\in\\mathbb{S}^{d-1}}$ provided $\\mathrm{TV}(P_{\\boldsymbol{u}},Q_{d})>1/2$ . The following lemma formalizes this idea. Theorem 3.1 follows from Lemma 3.3 and Lipschitzness of the score functions (Lemma B.5). ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.3 (Gaussianity testing with scores). For any $\\varepsilon\\in\\left(0,1\\right)$ and $K\\ge2,$ , let $D$ be any distribution on $\\mathbb{R}^{d}$ such that $\\mathfrak{m}_{2}\\ \\leq\\ d$ and ${\\mathrm{KL}}(D\\parallel Q_{d})\\,\\leq\\,K$ with $L$ -Lipschitz score $\\nabla\\log{D_{t}}$ for any $t\\geq0$ , and let $\\tilde{\\varepsilon}\\asymp\\varepsilon/\\sqrt{\\log(K/\\varepsilon)}$ be the $L^{2}$ score estimation error bound with discretization parameters $T\\,\\asymp\\,\\log(K/\\varepsilon),h\\,\\asymp\\,\\varepsilon^{2}/(L^{2}d),$ , and $N:=T/h$ so that $\\mathrm{TV}(F_{T}\\parallel D)\\,\\le\\,\\varepsilon$ (via Theorem 3.2). If $(s_{k h})_{k\\in[N]}$ are $\\tilde{\\varepsilon}$ -accurate score estimates for the forward process $(D_{k h})_{k\\in[N]}$ and $\\begin{array}{r}{\\Delta=\\operatorname*{max}_{k\\in[N]}\\mathbb{E}_{Q_{d}}\\|s_{k h}({\\pmb x})+2\\pi{\\pmb x}\\|^{2}}\\end{array}$ , then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}(D,Q_{d})\\lesssim\\varepsilon+\\sqrt{\\Delta\\log(K/\\varepsilon)}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In particular, if $\\mathrm{TV}(D,Q_{d})>1/2$ then there exist constants $C_{1},C_{2}>0$ such that for any score estimates $(s_{k h})_{k\\in[N]}$ of the forward process satisfying $\\begin{array}{r}{\\operatorname*{max}_{k\\in[N]}\\mathbb{E}_{D_{k h}}\\|s_{k h}(\\pmb{x})-\\nabla\\log D_{k h}(\\pmb{x})\\|^{2}\\leq}\\end{array}$ $C_{1}/\\log K$ , it holds ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Delta\\geq C_{2}/\\log K\\ .\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof. By Theorem 3.2 and our choice of $L^{2}$ score esti\u221amation error bound $\\tilde{\\varepsilon}$ , we have $\\mathrm{TV}(F_{T},D)\\leq$ $\\varepsilon$ . In addition, the score estimates $\\left(s_{t}\\right)$ also satisfy a $\\sqrt{\\Delta}$ -error bound with respect to the forward process of $Q_{d}$ , which is invariant with respect to time $t$ . Thus, Theorem 3.2 applied with $D=Q_{d}$ as the data distribution, discretization parameters $T,h$ , and score estimates $\\left(s_{k h}\\right)$ gives $\\mathrm{TV}(F_{T},Q_{d})\\lesssim$ $\\sqrt{\\Delta\\log(K/\\varepsilon)}$ . By the triangle inequality, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname{TV}(D,Q)\\leq\\operatorname{TV}(F_{T},D)+\\operatorname{TV}(F_{T},Q)\\lesssim\\varepsilon+{\\sqrt{\\Delta\\log(K/\\varepsilon)}}~.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The second part of the theorem follows from using the assumptions $\\mathrm{TV}(D,Q)>1/2$ , $K\\ge2$ , and fixing $\\varepsilon>0$ to a sufficiently small constant, which gives us ", "page_idx": 8}, {"type": "equation", "text": "$$\n1\\lesssim\\sqrt{\\Delta\\log K}\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "4 Sample Complexity of Gaussian Pancakes ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To establish that there is indeed a gap between statistical and computational feasibility, we demonstrate a polynomial upper bound on the sample complexity of $L^{2}$ -accurate score estimation for Gaussian pancakes. In particular, we show that a sufficiently good estimate $\\hat{\\pmb u}$ of the hidden direction $\\textbf{\\em u}$ is enough (Lemma 4.1). The polynomial sample complexity of score estimation then follows from Theorem 4.2, which shows that if $\\gamma(d)$ and $\\sigma(d)$ satisfy $\\dot{\\gamma\\sigma}=O(1)$ , then $1-\\langle\\hat{\\mathbf{u}},\\pmb{u}\\rangle^{2}\\leq\\eta^{2}$ is statistically achievable with poly $(d,\\gamma,1/\\eta)$ samples, albeit through brute-force search over $\\mathbb{S}^{d-1}$ . ", "page_idx": 8}, {"type": "text", "text": "Our estimator $\\hat{\\pmb u}$ is based on projection pursuit [35, 43]. We design a functional $E:\\mathbb{S}^{d-1}\\rightarrow\\mathbb{R}$ of the form $E(\\pmb{v}):=\\mathbb{E}_{\\pmb{x}\\sim P_{\\pmb{u}}}g(\\pmb{\\langle x,\\tilde{v}\\rangle})$ with $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ carefully chosen to ensure that $E(\\pmb{v}_{1})\\geq E(\\pmb{v}_{2})$ if and only if $|\\langle\\pmb{v}_{1},\\pmb{u}\\rangle|\\geq|\\langle\\pmb{v}_{2},\\pmb{u}\\rangle|$ . Given such a \u201cmonotonic\u201d functional (its empirical counterpart $\\hat{E}$ , to be precise), our estimator for the secret direction $\\textbf{\\em u}$ is essentially ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\pmb u}=\\underset{\\pmb v\\in\\mathbb{S}^{d-1}}{\\arg\\operatorname*{max}}\\,\\hat{E}(\\pmb v)\\ .\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We remark that parameter regime $\\gamma\\sigma=O(1)$ in Theorem 4.2 encompasses the crypt\u221aographically hard regime of Gaussian pancakes. It is also worth noting that if $\\operatorname*{min}(\\gamma,\\gamma\\sigma)=\\dot{\\omega}\\dot{(}\\sqrt{\\log{d}})$ , then Gaussian pancakes are statistically indistinguishable from $Q_{d}$ by Lemma B.10. ", "page_idx": 8}, {"type": "text", "text": "We defer the full proofs of Lemma 4.1 and Theorem 4.2 to Section C. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.1 (Score-to-parameter estimation reduction). For any $\\gamma>1,\\sigma>0,$ , let $P_{u}$ be the $(\\gamma,\\sigma)$ - Gaussian pancakes distribution with secret direction $\\pmb{u}\\in\\mathbb{S}^{d-1}$ . Given any $\\eta\\in(0,1)$ and $\\hat{\\pmb u}\\in\\mathbb S^{d-1}$ such that $\\dot{\\mathbf{1}}-\\langle\\hat{\\mathbf{u}},\\pmb{u}\\rangle^{2}\\le\\eta^{2}$ , the score estimate $\\hat{s}(\\pmb{x})=-2\\pi\\pmb{x}+\\nabla\\log T_{\\gamma}^{\\sigma}(\\langle\\pmb{x},\\hat{\\pmb{u}}\\rangle)$ satisfies ", "page_idx": 8}, {"type": "equation", "text": "$\\mathbb{E}_{\\pmb{x}\\sim P_{u}}\\|\\hat{s}(\\pmb{x})-s(\\pmb{x})\\|^{2}\\lesssim\\operatorname*{max}(1,1/\\sigma^{8})\\cdot\\eta^{2}d$ ", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $s(\\pmb{x})=-2\\pi\\pmb{x}+\\nabla\\log T_{\\gamma}^{\\sigma}(\\langle\\pmb{x},\\pmb{u}\\rangle)$ is the true score function of $P_{u}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2 (Sample complexity of parameter estimation). For any constant $C>0,$ , given $\\gamma(d)>$ $1,\\sigma(d)>0$ such that $\\gamma\\sigma<C_{!}$ , estimation error parameter $\\eta>0$ , and $\\delta\\in(0,1)$ , there exists a bruteforce search estimator $\\hat{\\pmb u}:\\mathbb{R}^{d\\times n}\\rightarrow\\mathbb{S}^{d-1}$ that uses $n=\\mathrm{poly}(d,\\gamma,1/\\eta,1/\\delta)$ samples and achieves $||{\\hat{\\boldsymbol{u}}}({\\boldsymbol{\\mathbf{\\mathit{x}}}}_{1},\\ldots,{\\boldsymbol{\\mathbf{\\mathit{x}}}}_{n})-{\\boldsymbol{\\mathbf{\\mathit{u}}}}||^{2}\\leq\\eta^{2}$ with probability at least $1-\\delta$ over i.i.d. samples $x_{1},\\ldots,x_{n}\\sim P_{u}$ , where $P_{u}$ is the $(\\gamma,\\sigma)$ -Gaussian pancakes distribution with secret direction $\\pmb{u}\\in\\mathbb{S}^{d-1}$ . ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] D. Aggarwal and N. Stephens-Davidowitz. An improved constant in banaszczyk\u2019s transference theorem. arXiv preprint arXiv:1907.09020, 2019. [2] D. Bakry, I. Gentil, M. Ledoux, et al. Analysis and geometry of Markov diffusion operators, volume 103. Springer, 2014. [3] W. Banaszczyk. New bounds in some transference theorems in the geometry of numbers. Mathematische Annalen, 296:625\u2013635, 1993. [4] A. S. Bandeira, A. Perry, and A. S. Wein. Notes on computational-to-statistical gaps: predictions using statistical physics. Portugaliae Mathematica, 75(2):159\u2013186, 2018. [5] B. Barak and D. Steurer. Sum-of-squares proofs and the quest toward optimal algorithms. arXiv preprint arXiv:1404.5236, 2014. [6] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Nearly d-linear convergence bounds for diffusion models via stochastic localization. In The Twelfth International Conference on Learning Representations, 2024. [7] Q. Berthet and P. Rigollet. Computational lower bounds for sparse pca. arXiv preprint arXiv:1304.0828, 2013. [8] G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, K.-R. M\u00fcller, and S. Roweis. In search of non-gaussian components of a high-dimensional distribution. Journal of Machine Learning Research, 7(2), 2006. [9] A. Block, Y. Mroueh, and A. Rakhlin. Generative modeling with denoising auto-encoders and langevin sampling. arXiv preprint arXiv:2002.00107, 2020. [10] A. Bogdanov, M. C. Noval, C. Hoffmann, and A. Rosen. Public-key encryption from continuous lwe. Cryptology ePrint Archive, 2022. [11] M. Brennan and G. Bresler. Reducibility and statistical-computational gaps from secret leakage. In Conference on Learning Theory, pages 648\u2013847. PMLR, 2020. [12] B. Brubaker. In neural networks, unbreakable locks can hide invisible doors. Quanta magazine, 2023. https://www.quantamagazine.org/ cryptographers-show-how-to-hide-invisible-backdoors-in-ai-20230302/ [Accessed: 2024-03-12]. [13] J. Bruna, O. Regev, M. J. Song, and Y. Tang. Continuous lwe. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, 2021. [14] C. L. Canonne. A short note on an inequality between kl and tv. arXiv preprint arXiv:2202.07198, 2022. [15] H. Chen, H. Lee, and J. Lu. Improved analysis of score-based generative modeling: Userfriendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735\u20134763. PMLR, 2023. [16] M. Chen, K. Huang, T. Zhao, and M. Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning, pages 4672\u20134712. PMLR, 2023. [17] S. Chen, S. Chewi, J. Li, Y. Li, A. Salim, and A. R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In International Conference on Learning Representations (ICLR), 2023. [18] Y.-H. Chen, K.-M. Chung, and J.-J. Liao. On the complexity of simulating auxiliary input. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pages 371\u2013390. Springer, 2018. [19] S. Chewi. Log-concave sampling, 2024. ", "page_idx": 9}, {"type": "text", "text": "[20] M. Christ, S. Gunn, and O. Zamir. Undetectable watermarks for language models. arXiv preprint arXiv:2306.09194, 2023.   \n[21] A. Daniely, N. Linial, and S. Shalev-Shwartz. From average case complexity to improper learning complexity. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 441\u2013448, 2014.   \n[22] V. De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint arXiv:2208.05314, 2022.   \n[23] V. De Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[24] S. Decatur, O. Goldreich, and D. Ron. Computational sample complexity. In Proceedings of the tenth annual conference on Computational learning theory, pages 130\u2013142, 1997.   \n[25] I. Diakonikolas and D. Kane. Non-gaussian component analysis via lattice basis reduction. In Conference on Learning Theory, pages 4535\u20134547. PMLR, 2022.   \n[26] I. Diakonikolas, D. Kane, and L. Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. In International Conference on Machine Learning, pages 7922\u20137938. PMLR, 2023.   \n[27] I. Diakonikolas, D. Kane, L. Ren, and Y. Sun. Sq lower bounds for non-gaussian component analysis with weaker assumptions. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] I. Diakonikolas, D. Kane, and N. Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.   \n[29] I. Diakonikolas and D. M. Kane. Algorithmic High-Dimensional Robust Statistics. Cambridge university press Cambridge, 2023.   \n[30] I. Diakonikolas, D. M. Kane, V. Kontonis, and N. Zarifis. Algorithms and sq lower bounds for pac learning one-hidden-layer relu networks. In Conference on Learning Theory, pages 1514\u20131539. PMLR, 2020.   \n[31] I. Diakonikolas, D. M. Kane, and A. Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 73\u201384. IEEE, 2017.   \n[32] C. Dwork, M. P. Kim, O. Reingold, G. N. Rothblum, and G. Yona. Outcome indistinguishability. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 1095\u20131108, 2021.   \n[33] C. Dwork, M. P. Kim, O. Reingold, G. N. Rothblum, and G. Yona. Beyond bernoulli: Generating random outcomes that cannot be distinguished from nature. In International Conference on Algorithmic Learning Theory, pages 342\u2013380. PMLR, 2022.   \n[34] V. Feldman, E. Grigorescu, L. Reyzin, S. S. Vempala, and Y. Xiao. Statistical algorithms and a lower bound for detecting planted cliques. J. ACM, 64(2), 2017.   \n[35] J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on computers, 100(9):881\u2013890, 1974.   \n[36] D. Gamarnik. The overlap gap property: A topological barrier to optimizing over random structures. Proceedings of the National Academy of Sciences, 118(41):e2108492118, 2021.   \n[37] S. Goel, A. Gollakota, Z. Jin, S. Karmalkar, and A. Klivans. Superpolynomial lower bounds for learning one-layer neural networks using gradient descent. In International Conference on Machine Learning, pages 3587\u20133596. PMLR, 2020.   \n[38] S. Goldwasser, M. P. Kim, V. Vaikuntanathan, and O. Zamir. Planting undetectable backdoors in machine learning models. arXiv preprint arXiv:2204.06974, 2022.   \n[39] A. Gupte, N. Vafa, and V. Vaikuntanathan. Continuous lwe is as hard as lwe & applications to learning gaussian mixtures. arXiv preprint arXiv:2204.02550, 2022.   \n[40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016.   \n[41] U. H\u00e9bert-Johnson, M. Kim, O. Reingold, and G. Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In International Conference on Machine Learning, pages 1939\u20131948. PMLR, 2018.   \n[42] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[43] P. J. Huber. Projection pursuit. The annals of Statistics, pages 435\u2013475, 1985.   \n[44] A. Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \n[45] D. Jetchev and K. Pietrzak. How to fake auxiliary input. In Theory of Cryptography Conference, pages 566\u2013590. Springer, 2014.   \n[46] M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983\u20131006, 1998.   \n[47] D. Kunisky. Spectral Barriers in Certification Problems. PhD thesis, New York University, 2022.   \n[48] D. Kunisky, A. S. Wein, and A. S. Bandeira. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio. In Mathematical Analysis, its Applications and Computation: ISAAC 2019, Aveiro, Portugal, July 29\u2013August 2, pages 1\u201350. Springer, 2022.   \n[49] J. B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on optimization, 11(3):796\u2013817, 2001.   \n[50] H. Lee, J. Lu, and Y. Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870\u201322882, 2022.   \n[51] H. Lee, J. Lu, and Y. Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023.   \n[52] R. Lindner and C. Peikert. Better key sizes (and attacks) for lwe-based encryption. In Topics in Cryptology\u2013CT-RSA 2011: The Cryptographers\u2019 Track at the RSA Conference 2011, San Francisco, CA, USA, February 14-18, 2011. Proceedings, pages 319\u2013339. Springer, 2011.   \n[53] X. Liu, L. Wu, M. Ye, and Q. Liu. Let us build bridges: Understanding and extending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022.   \n[54] Z. Ma and Y. Wu. Computational barriers in minimax submatrix detection. The Annals of Statistics, 43(3), 2015.   \n[55] S. Mei and Y. Wu. Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models. arXiv preprint arXiv:2309.11420, 2023.   \n[56] D. Micciancio and O. Regev. Lattice-based cryptography. In Post-quantum cryptography, pages 147\u2013191. Springer, 2009.   \n[57] J. Niles-Weed and P. Rigollet. Estimation of Wasserstein distances in the Spiked Transport Model. Bernoulli, 28(4):2663 \u2013 2688, 2022.   \n[58] NIST. Post-quantum cryptography. https://csrc.nist.gov/Projects/ Post-Quantum-Cryptography.   \n[59] P. A. Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization. California Institute of Technology, 2000.   \n[60] C. Peikert. A decade of lattice cryptography. Foundations and trends\u00ae in theoretical computer science, 10(4):283\u2013424, 2016.   \n[61] J. Pidstrigach. Score-based generative models detect manifolds. Advances in Neural Information Processing Systems, 35:35852\u201335865, 2022.   \n[62] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[63] O. Regev. On lattices, learning with errors, random linear codes, and cryptography. Journal of the ACM (JACM), 56(6):1\u201340, 2009.   \n[64] S. Roman. The umbral calculus. Pure and Applied Mathematics, 1984.   \n[65] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[66] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \n[67] S. S\u00e4rkk\u00e4 and A. Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019.   \n[68] K. Shah, S. Chen, and A. Klivans. Learning mixtures of gaussians using the ddpm objective. Advances in Neural Information Processing Systems, 36, 2024.   \n[69] S. Shalev-Shwartz, O. Shamir, and E. Tromer. Using more data to speed-up training time. In Artificial Intelligence and Statistics, pages 1019\u20131027. PMLR, 2012.   \n[70] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[71] M. J. Song, I. Zadik, and J. Bruna. On the cryptographic hardness of learning single periodic neurons. Advances in Neural Processing Systems (NeurIPS), 2021.   \n[72] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[73] Y. Song and D. P. Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288, 2021.   \n[74] N. Stephens-Davidowitz. On the Gaussian Measure Over Lattices. PhD thesis, New York University, USA, 2017.   \n[75] S. Tiegel. Hardness of agnostically learning halfspaces from worst-case lattice problems. In The Thirty Sixth Annual Conference on Learning Theory, pages 3029\u20133064. PMLR, 2023.   \n[76] L. Trevisan, M. Tulsiani, and S. Vadhan. Regularity, boosting, and efficiently simulating every high-entropy distribution. In 2009 24th Annual IEEE Conference on Computational Complexity, pages 126\u2013136. IEEE, 2009.   \n[77] R. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[78] P. Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[79] M. J. Wainwright. Constrained forms of statistical minimax: Computation, communication and privacy. In Proceedings of the International Congress of Mathematicians, pages 13\u201321, 2014.   \n[80] A. Wibisono, Y. Wu, and K. Y. Yang. Optimal score estimation via empirical bayes smoothing. arXiv preprint arXiv:2402.07747, 2024.   \n[81] Y. Wu and J. Xu. Statistical problems with planted structures: Information theoretical and computational limits. Information-Theoretic Methods in Data Science, 383:13, 2021.   \n[82] K. Y. Yang and A. Wibisono. Convergence of the inexact langevin algorithm and score-based generative models in kl divergence. arXiv preprint arXiv:2211.01512, 2022.   \n[83] I. Zadik, M. J. Song, A. S. Wein, and J. Bruna. Lattice-based methods surpass sum-of-squares in clustering. In Conference on Learning Theory, pages 1247\u20131248. PMLR, 2022.   \n[84] L. Zdeborov\u00e1 and F. Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances in Physics, 65(5):453\u2013552, 2016.   \n[85] Y. Zhang, M. J. Wainwright, and M. I. Jordan. Lower bounds on the performance of polynomialtime algorithms for sparse linear regression. In Conference on Learning Theory, pages 921\u2013948. PMLR, 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Additional Related Work 16 ", "page_idx": 14}, {"type": "text", "text": "B Proofs for Section 3 16   \nB.1 Proof of Theorem 3.1 16   \nB.2 Score functions of Gaussian pancakes 19   \nB.3 Distance from the standard Gaussian 21   \nC Proofs for Section 4 24   \nC.1 Sample complexity of score estimation 24   \nC.2 Sample complexity of parameter estimation 25 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Score estimation. Several works have addressed the statistical question of sample complexity for various data distributions and function classes used to estimate the score [16, 55, 80]. Wibisono et al. [80] study score estimation for the class of subgaussian distributions with Lipschitz scores. By establishing a minimax lower bound exhibiting a curse of dimensionality, they show that the exponential dependence on dimension is fundamental to this nonparametric distribution class, underscoring the need for stronger assumptions on the data distribution for polynomial sample complexity. Chen et al. [16] study neural network-based score estimation and derive finite-sample bounds for data distributions that lie on a low-dimensional linear subspace, which circumvents the curse of dimensionality. Mei and Wu [55] study neural network-based score estimation for graphical models which are intrinsically high dimensional. Assuming the efficiency of variational inference approximation to the data-generating graphical model, they show that the score function can be efficiently approximated by residual networks [40] and learned with polynomially many samples. Efficient score estimation both in sample and computational complexity has been achieved by Shah et al. [68] for mixtures of two spherical Gaussians using a shallow neural network architecture that matches the closed-form expression of the score function. ", "page_idx": 15}, {"type": "text", "text": "Statistical-to-computational gaps. Statistical-to-computational gaps have a rich history in statistics, machine learning, and computer science [7, 85, 54, 69, 24]. Indeed, many high-dimensional inference problems exhibit gaps between what is achievable statistically (with infinite computational resources) and what is achievable under limited computation. Notable examples include sparse PCA [7], sparse linear regression [85, 79], and learning one-hidden-layer neural networks over Gaussian inputs [37, 30]. ", "page_idx": 15}, {"type": "text", "text": "Since there are no known reductions from NP-hard problems, the gold standard for computational hardness, to any average-case problem arising in statistical inference, alternative techniques have been developed to provide rigorous evidence of hardness. These include proving lower bounds for restricted classes of algorithms like sum of squares (SoS) [49, 59, 5] and statistical query (SQ) algorithms [46, 34], which capture spectral, moment, and tensor methods, and reducing from \u201ccanonical\u201d problems believed to be hard on average, such as the planted clique [11] or random $k$ -SAT [21] problem. We refer the reader to surveys and monographs [84, 4, 81, 36, 48, 47, 29] for a thorough overview of recent literature and diverse perspectives ranging from computer science and information theory to statistical physics. ", "page_idx": 15}, {"type": "text", "text": "B Proofs for Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem B.1 (Theorem 3.1 restated). Let $\\gamma(d)\\,>\\,1,\\sigma(d)\\,>\\,0$ be any pair of sequences such that $\\sigma\\geq1/\\mathrm{poly}(d)$ and the corresponding (sequence of) $(\\gamma,\\sigma)$ -Gaussian pancakes distributions $(P_{u})_{u\\in\\mathbb{S}^{d-1}}$ satisfies $\\mathrm{TV}(P_{\\mathbf{\\boldsymbol{u}}},Q_{d})>1/2$ for any $d\\in\\mathbb N$ . Then, for any $\\delta\\,\\in\\,(0,1)$ , there e\u221axists $a$ $\\mathrm{poly}(d)\\cdot\\log(1/\\delta)$ -time algorithm with access to a score estimation oracle of $L^{2}$ -error $O(1/{\\sqrt{\\log d}})$ that solves the $(\\gamma,\\sigma)$ -Gaussian pancakes problem with probability at least $1-\\delta$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $D\\in(P_{\\mathbf{\\mathit{u}}})\\cup Q$ be the given data distribution. We first verify that assumptions A1-A3 hold, allowing us to apply the score-based Gaussianity test from Lemma 3.3. For any $\\bar{\\sigma}(\\boldsymbol{d}),\\gamma(\\boldsymbol{d})>0$ such that $\\sigma\\geq1/\\mathrm{poly}(d)$ , the $(\\gamma,\\sigma)$ -Gaussian pancakes satisfy the Lipschitz score condition $L\\leq$ $\\mathrm{poly}(d)$ (Assumption A1) by Lemma B.5 and the second moment bound ${\\mathfrak{m}}_{2}\\leq d$ (Assumption A2) by Lemma B.2. In addition, $\\mathrm{KL}(P_{\\pmb{u}}\\parallel Q)\\,\\le\\,\\mathrm{poly}(d)$ (Lemma B.11), so Lemma 3.3 applies to $D$ with $K\\;=\\;\\mathrm{poly}(d)$ . Moreover, since $\\mathrm{TV}(P_{\\mathbf{\\boldsymbol{u}}},Q)\\;>\\;1/2$ , Lemma 3.3 implies that if $D\\ \\in$ $(P_{u})_{u\\in\\mathbb{S}^{d-1}}$ , there exist universal constants $C_{1},C_{2}>0$ such that if the score estimates $(s_{k h})_{k\\in[N]}$ satisfy $\\mathbb{E}_{D_{k h}}\\|s_{k h}(\\pmb{x})-\\nabla\\log D_{k h}(\\pmb{x})\\|^{2}\\leq C_{1}/\\log d$ , then $\\Delta\\geq C_{2}/\\log d$ . ", "page_idx": 15}, {"type": "text", "text": "Let $\\tau\\,=\\,C_{2}/\\log d$ and $\\eta^{2}\\;=\\;\\operatorname*{min}(\\tau/4,C_{1}/\\log d)$ . Then, we have that with $\\eta$ -accurate\u221a score estimates $\\left(s_{k h}\\right)$ , we have $\\Delta\\le\\eta^{2}\\le\\bar{\\tau}/4$ if $D=Q$ and $\\Delta\\geq\\tau$ otherwise. Note that $\\eta\\asymp1/\\sqrt{\\log d}$ and $N=\\dot{T}/h\\stackrel{\\cdot}{\\sim}L^{2}d\\log K\\stackrel{\\cdot}{\\leq}\\mathrm{poly}(d)$ . Our proposed distinguisher $\\boldsymbol{\\mathcal{A}}$ uses a finite-sample estimate of $\\Delta$ as a test statistic (Eq.(7)) using $\\eta$ -accurate score estimates $(s_{k h})_{k\\in[N]}$ and $N\\ell$ i.i.d. standard Gaussian samples $(z_{i}^{(k)})_{(k,i)\\in[N]\\times[\\ell]}$ as follows. Later, it will be shown that setting the batch size $\\ell$ to $\\ell=\\mathrm{poly}(d)$ is sufficient for our distinguisher $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\Delta}=\\operatorname*{max}_{k\\in[N]}\\hat{\\Delta}^{(k)}\\mathrm{~,~}\\quad\\mathrm{where~}\\hat{\\Delta}^{(k)}=\\frac{1}{\\ell}\\sum_{i=1}^{\\ell}\\left\\|s_{k h}(z_{i}^{(k)})+2\\pi z_{i}^{(k)}\\right\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The distinguisher $\\boldsymbol{\\mathcal{A}}$ decides $D=Q$ if $\\hat{\\Delta}\\le\\tau/2$ and $D\\in(P_{u})_{u\\in\\mathbb{S}^{d-1}}$ otherwise. This procedure runs in\u221a time $O(N\\ell)$ and makes $N=\\mathrm{poly}(d)$ queries to the score estimation oracle of $L^{2}$ -accuracy $O(1/{\\sqrt{\\log d}})$ . ", "page_idx": 16}, {"type": "text", "text": "One issue in estimating $\\Delta$ is that the score estimates $\\left(s_{k h}\\right)$ only satisfy the mean guarantee with respect to the forward process $(D_{k h})$ , i.e., $\\mathbb{E}_{D_{k h}}\\|s_{k h}(\\pmb{x})-\\dot{\\nabla}\\log^{\\overleftarrow{}}D_{k h}(\\pmb{x})\\|^{2}\\leq\\eta^{2}$ . These guarantees do not necessarily provide control over the concentration of random variables $\\|s_{t}(z)+2\\pi z\\|^{2}$ induced by $z\\sim Q_{d}$ . Moreover, if $D=P_{u}$ , then $s_{t}(z)$ may behave erratically for $z\\sim Q_{d}$ , taking on large norms in low density areas between the pancakes, which may deter the estimation of $\\Delta$ . We handle this by truncating the score estimates. ", "page_idx": 16}, {"type": "text", "text": "Let $M>0$ be some large number to be determined later. Define the truncated score $\\bar{s}$ by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{s}_{t}(\\pmb{x})=\\left\\{\\mathfrak{s}_{t}(\\pmb{x})\\quad\\mathrm{~if~}\\lVert s_{t}(\\pmb{x})\\rVert\\leq M\\ ,\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We claim that using the truncated score estimates $\\left(\\Bar{s}_{t}\\right)$ in place of $\\left(s_{t}\\right)$ introduces negligible (in data dimension $d$ ) additional $L^{2}$ score estimation error with respect to the forward process $\\left(D_{t}\\right)$ compared to the original score estimates $\\left(s_{t}\\right)$ . Hence, Lemma 3.3 applies with the uniformly bounded vector fields $(\\tilde{s}_{k h})_{k\\in[N]}$ as the $L^{2}$ -accurate score estimates for $(D_{k h})_{k\\in[N]}$ . For any (discretized) time $0\\leq t\\leq T$ and distribution $D_{t}$ from the forward process, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{D_{t}}\\|\\bar{s}_{t}({\\pmb x})-\\nabla\\log D_{t}({\\pmb x})\\|^{2}=\\mathbb{E}_{D_{t}}\\big[\\|s_{t}({\\pmb x})-\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot\\mathbb{1}[\\|s_{t}({\\pmb x})\\|\\le M]\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{D_{t}}\\big[\\|\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot\\mathbb{1}[\\|s_{t}({\\pmb x})\\|>M]\\big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second term on the RHS can be upper bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{D_{t}}\\big[\\|\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot{\\pmb1}[\\|s_{t}({\\pmb x})\\|>M]\\big]}\\\\ &{\\qquad=\\mathbb{E}_{D_{t}}\\big[\\|\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot{\\pmb1}[(\\|s_{t}({\\pmb x})\\|>M)\\wedge(\\|\\nabla\\log D_{t}({\\pmb x})\\|>M/2)]\\big]}\\\\ &{\\qquad+\\,\\mathbb{E}_{D_{t}}\\big[\\|\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot{\\pmb1}[(\\|s_{t}({\\pmb x})\\|>M)\\wedge(\\|\\nabla\\log D_{t}({\\pmb x})\\|\\leq M/2)]\\big]}\\\\ &{\\qquad\\leq\\mathbb{E}_{D_{t}}\\big[\\|\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot{\\pmb1}[\\|\\nabla\\log D_{t}({\\pmb x})\\|>M/2]\\big]}\\\\ &{\\qquad+\\,\\mathbb{E}_{D_{t}}\\big[\\|s_{t}({\\pmb x})-\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot{\\pmb1}[\\|s_{t}({\\pmb x})\\|>M]\\big]\\enspace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where in Eq.(8) we used the fact that if $\\|s_{t}(\\pmb{x})\\|\\ >\\ M$ and $\\|\\nabla\\log D_{t}(\\mathbf{x})\\|\\ \\leq\\ M/2$ , then $\\|\\nabla\\log D_{t}(\\pmb{x})\\|\\leq M/2\\leq\\|s_{t}(\\pmb{x})-\\nabla\\log D_{t}^{*}(\\pmb{x})\\|$ . ", "page_idx": 16}, {"type": "text", "text": "Putting things together, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{D_{t}}\\|\\bar{s}_{t}({\\pmb x})-\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\leq\\mathbb{E}_{D_{t}}\\|s_{t}({\\pmb x})-\\nabla\\log D_{t}({\\pmb x})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{D_{t}}\\big[\\|\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot\\mathbb{1}[\\|\\nabla\\log D_{t}({\\pmb x})\\|>M/2]\\big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It remains to bound $\\mathbb{E}[\\|\\nabla\\log D_{t}({\\pmb x})\\|^{2}\\cdot\\mathbb{1}[\\|\\nabla\\log D_{t}({\\pmb x})\\|>M/2]]$ which depends only on the distribution $D_{t}$ . We choose $M\\asymp(\\sqrt{d}+1/\\sigma^{2})$ . If $D=Q_{d}$ , then $D_{t}=Q_{d}$ for any $t>0$ , and by Cauchy-Schwarz and norm concentration (see e.g., [77, Theorem 3.1.1]), there exists a constant $C>0$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Q_{d}}\\big[\\|x\\|^{2}\\cdot\\mathbb{1}[\\|x\\|>M/2]\\big]\\leq\\mathbb{E}_{Q_{d}}\\|x\\|^{4}\\cdot\\operatorname*{Pr}_{Q_{d}}[\\|x\\|>M/2]\\leq\\exp(-C M^{2})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, if $D=P_{u}$ , then by Lemma B.4, Eq.(13) and the triangle inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\nabla\\log P_{u}(\\pmb{x})\\|=\\left\\|\\pmb{x}+\\frac{(T_{\\gamma}^{\\sigma})^{\\prime}(\\langle\\pmb{x},\\pmb{u}\\rangle)}{T_{\\gamma}^{\\sigma}(\\langle\\pmb{x},\\pmb{u}\\rangle)}\\pmb{u}\\right\\|\\leq\\|\\pmb{x}\\|+8\\pi(1+1/\\sigma^{2})\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using a similar concentration argument, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{u}}\\big[\\|\\nabla\\log P_{u}({\\boldsymbol x})\\|^{2}\\cdot\\mathbb{1}[\\|\\nabla\\log P_{u}({\\boldsymbol x})\\|>M/2]\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\mathbb{E}_{P_{u}}\\big[(\\|{\\boldsymbol x}\\|^{2}+1/\\sigma^{4}+1)\\cdot\\mathbb{1}[\\|{\\boldsymbol x}\\|>M/2-8\\pi(1+1/\\sigma^{2})]\\big]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\exp(-O(M^{2}))~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The OU process applied to $P_{u}$ only increases the $\\sigma$ parameter, so the upper bound i\u221an Eq.(9) holds uniformly over the forward process $(D_{k h})_{k\\in[N]}$ if $D=P_{u}$ . Thus, choosing $M\\asymp(\\sqrt{d}+1/\\sigma^{2})=$ $\\mathrm{poly}(d)$ as the truncation threshold suffices to ensure that for all discretized time steps $t=k h$ , the $L^{2}$ score estimation error with respect to the forward process $\\left(D_{t}\\right)$ introduced by truncating the score estimate $s_{t}$ to $\\bar{s}_{t}$ is negligible in $d$ . Therefore, we apply Lemma 3.3 with $(\\tilde{s}_{k h})_{k\\in[N]}$ as the score estimates for the forward process $(D_{k h})_{k\\in[N]}$ . ", "page_idx": 17}, {"type": "text", "text": "Since $\\lVert\\bar{s}_{t}(z)+2\\pi z\\rVert^{2}$ , where $z\\sim Q_{d}$ , is a random variable with subexponential norm $O(M^{2})$ , we can apply Bernstein\u2019s inequality [77, Corollary 2.8.3] to the i.i.d. sum $\\hat{\\Delta}^{(k)}$ . Thus, for any $k\\in[N]$ , $\\ell\\asymp(\\dot{M}^{\\bar{4}}/\\varepsilon^{2})\\cdot\\log(N/\\delta)$ Gaussian samples suffice to guarantee accurate estimation of the population mean $\\Delta^{(k)}$ with additive error less than $\\varepsilon$ with probability at least $1-\\delta/N$ . By a union bound, with probability at least $1-\\delta$ , this holds for all $k\\,\\in\\,[N]$ . Setting $\\varepsilon\\,=\\,\\^{\\cdot}\\!\\left/8\\,=\\,\\Theta(1/\\log d)\\right.$ and recalling that $N=\\mathrm{poly}(d)$ , we have a distinguisher $\\boldsymbol{\\mathcal{A}}$ for the Gaussian pancakes problem that makes $N=\\mathrm{poly}(d)$ queries to the score estimation oracle, runs in time $N\\ell\\stackrel{}{=}\\mathrm{poly}(\\bar{d})\\cdot\\log(1/\\delta)$ , and is correct with probability at least $1-\\delta$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2 (Second moment of Gaussian pancakes). For any $\\gamma,\\sigma~>~0,$ , and $\\textbf{\\em u}\\in\\ \\mathbb{S}^{d-1}$ , the $(\\gamma,\\sigma)$ -Gaussian pancakes $P_{u}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{{\\pmb x}\\sim P_{\\pmb u}}[\\|{\\pmb x}\\|^{2}]\\leq\\frac{d}{2\\pi}~.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Without loss of generality, we assume $\\pmb{u}=e_{1}$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}\\sim P_{u}}\\|\\mathbf{x}\\|^{2}=\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[x^{2}]+(d-1)\\mathbb{E}_{z\\sim Q^{\\mathcal{z}}}{\\boldsymbol{z}}^{2}=\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[x^{2}]+\\frac{d-1}{2\\pi}\\ .\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In addition, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[x^{2}]=\\mathbb{E}_{x\\sim A_{\\gamma}}\\mathbb{E}_{z\\sim\\mathcal{N}(0,1/(2\\pi))}\\big(x/\\sqrt{1+\\sigma^{2}}+\\sigma z/\\sqrt{1+\\sigma^{2}}\\big)^{2}}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{1}{1+\\sigma^{2}}\\cdot\\mathbb{E}_{x\\sim A_{\\gamma}}[x^{2}]+\\displaystyle\\frac{\\sigma^{2}}{1+\\sigma^{2}}\\cdot\\frac{1}{2\\pi}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, it suffices to establish an upper bound on $\\mathbb{E}_{x\\sim A_{\\gamma}}x^{2}$ , i.e., the second moment of the discrete Gaussian on $(1/\\gamma)\\mathbb{Z}$ . Using the Poisson summation formula (Lemma 2.1) and the fact that the Fourier transform of $x^{2}\\rho(x)$ is $(1/(2\\pi)-y^{2})\\rho(y)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{x\\sim A_{\\gamma}}[x^{2}]=\\frac{1}{\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{x\\in(1/\\gamma)\\mathbb{Z}}x^{2}\\rho(x)}\\\\ {\\displaystyle=\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{y\\in\\gamma\\mathbb{Z}}\\left(\\frac{1}{2\\pi}-y^{2}\\right)\\rho(y)}\\\\ {\\displaystyle=\\frac{1}{2\\pi}\\cdot\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\cdot\\rho(\\gamma\\mathbb{Z})-\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{y\\in\\gamma\\mathbb{Z}}y^{2}\\rho(y)}\\\\ {\\displaystyle<\\frac{1}{2\\pi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in Eq.(10), we used the fact that $\\rho(\\gamma\\mathbb{Z})=\\rho((1/\\gamma)\\mathbb{Z})/\\gamma$ and that the second term is positive.   \nSince $\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[x^{2}]$ is a convex combination of $\\mathbb{E}_{x\\sim A_{\\gamma}}[x^{2}]$ and $1/(2\\pi)$ , the conclusion follows. ", "page_idx": 17}, {"type": "text", "text": "B.2 Score functions of Gaussian pancakes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we show that score functions of Gaussian pancakes distributions are Lipschitz with respect to $\\pmb{x}\\in\\mathbb{R}^{d}$ . The score function of $P_{u}$ , the $(\\gamma,\\sigma)$ -Gaussian pancakes distribution with secret direction $\\pmb{u}\\in\\mathbb{S}^{d-1}$ , admits the following analytical expression. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla\\log P_{u}({\\pmb x})=-2\\pi{\\pmb x}+\\nabla\\log T(\\langle{\\pmb x},{\\pmb u}\\rangle)=-2\\pi{\\pmb x}+\\frac{(T_{\\gamma}^{\\sigma})^{\\prime}(\\langle{\\pmb x},{\\pmb u}\\rangle)}{T_{\\gamma}^{\\sigma}(\\langle{\\pmb x},{\\pmb u}\\rangle)}{\\pmb u}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use Banaszczyk\u2019s theorems on the Gaussian mass of lattices [3] to upper bound the Lipschitz constant of $(T_{\\gamma}^{\\sigma})^{\\prime}/T_{\\gamma}^{\\sigma}$ in terms of $\\sigma$ . ", "page_idx": 18}, {"type": "text", "text": "Theorem B.3 ([74, Corollary 1.3.5]). For any lattice $\\mathcal{L}\\subset\\mathbb{R}^{d}$ , parameter $s>0$ , shift $t\\in\\mathbb{R}^{d}$ , and radius $r>\\sqrt{d/(2\\pi)}\\cdot s,$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho_{s}((\\mathcal{L}-t)\\setminus r\\mathcal{B}_{2}^{d})<\\exp(-\\pi x^{2})\\rho_{s}(\\mathcal{L})\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $x:=r/s-\\sqrt{d/(2\\pi)}$ and $B_{2}^{d}$ denotes the Euclidean ball in $\\mathbb{R}^{d}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma B.4 ([74, Lemma 1.3.10]). For any lattice $\\mathcal{L}\\subset\\mathbb{R}^{d}$ , parameter $s>0$ , and shift $t\\in\\mathbb{R}^{d}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\exp(-\\pi\\cdot\\mathrm{dist}(t,\\mathcal{L})^{2}/s^{2})\\cdot\\rho_{s}(\\mathcal{L})\\leq\\rho_{s}(\\mathcal{L}-t)\\leq\\rho_{s}(\\mathcal{L})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma B.5 (Lipschitzness of $\\nabla\\log{P_{u}}$ ). For any $\\gamma>1,\\sigma>0,$ , $s=\\sigma/\\sqrt{1+\\sigma^{2}}$ , and $\\pmb{u}\\in\\mathbb{S}^{d-1}$ , the score function of the $(\\gamma,\\sigma)$ -Gaussian pancakes distribution $P_{u}$ satisfies the Lipschitz condition: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\log P_{u}(\\pmb{y})-\\nabla\\log P_{u}(\\pmb{x})\\|\\lesssim(1/s^{4})\\|\\pmb{y}-\\pmb{x}\\|\\quad\\,f o r\\,a n y\\,\\pmb{x},\\pmb{y}\\in\\mathbb{R}^{d}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the likelihood ratio $T_{\\gamma}^{\\sigma}$ of the $\\sigma$ -smoothed discrete Gaussian $A_{\\gamma}^{\\sigma}$ relative to $\\mathcal{N}(0,1/(2\\pi))$ satisfies the uniform bound: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\frac{(T_{\\gamma}^{\\sigma})^{\\prime}(z)}{T_{\\gamma}^{\\sigma}(z)}\\right|\\leq\\frac{8\\pi}{s^{2}}\\ \\ \\,f o r\\,a n y\\,\\,z\\in\\mathbb{R}\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. It suffices to analyze the Lipschitz constant of the univariate function $(T_{\\gamma}^{\\sigma})^{\\prime}/T_{\\gamma}^{\\sigma}:\\mathbb R\\rightarrow\\mathbb R$ , which we denote by $f=(T_{\\gamma}^{\\sigma})^{\\prime}/T_{\\gamma}^{\\sigma}$ since for any $\\pmb{x}\\neq\\pmb{y}\\in\\mathbb{R}^{d}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\log P_{u}(y)-\\nabla\\log P_{u}(x)\\|\\leq2\\pi\\|y-x\\|+\\left\\|\\left(f(\\langle y,u\\rangle)-f(\\langle x,u\\rangle)\\right)u\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\pi\\|y-x\\|+\\frac{\\left|f(\\langle y,u\\rangle)-f(\\langle x,u\\rangle)\\right|}{\\left|\\langle y,u\\rangle-\\langle x,u\\rangle\\right|}\\cdot\\left|\\langle y-x,u\\rangle\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(2\\pi+\\displaystyle\\operatorname*{sup}_{a,b\\in\\mathbb{R}}\\left|\\frac{f(b)-f(a)}{b-a}\\right|\\right)\\|y-x\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We bound the Lipschitz constant of the function $f(z)$ by demonstrating a uniform upper bound on the absolute value of its derivative $f^{\\prime}(z):=(d/\\dot{d}z)\\dot{f}(z)$ . For notational convenience, we omit the parameters $\\gamma,\\sigma$ when denoting the likelihood ratio $T$ . The derivative $f^{\\prime}$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\nf^{\\prime}=\\left({\\frac{T^{\\prime}}{T}}\\right)^{\\prime}={\\frac{(T^{\\prime\\prime})T-(T^{\\prime})^{2}}{T^{2}}}={\\frac{T^{\\prime\\prime}}{T}}-\\left({\\frac{T^{\\prime}}{T}}\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, $|f^{\\prime}(z)|\\leq|(T^{\\prime\\prime}/T)(z)|+|(T^{\\prime}/T)(z)|^{2}$ for any $z\\in\\mathbb{R}$ . We prove uniform upper bounds on the two RHS terms, starting with $|T^{\\prime}/T|$ . Using the definition of the likelihood ratio $T$ (Eq.(5)), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\frac{T^{\\prime}(z)}{T(z)}}={\\frac{2\\pi}{\\sigma^{2}}}\\cdot{\\frac{\\sum_{k\\in\\mathbb{Z}}-(z-\\sqrt{1+\\sigma^{2}}k/\\gamma)\\rho_{\\sigma}(z-\\sqrt{1+\\sigma^{2}}k/\\gamma)}{\\sum_{k\\in\\mathbb{Z}}\\rho_{\\sigma}(z-\\sqrt{1+\\sigma^{2}}k/\\gamma)}}}}\\\\ {{\\mathrm{~}}}\\\\ {{\\mathrm{~}={\\frac{2\\pi}{\\sigma s}}\\cdot{\\frac{\\mathbb{E}}{x\\sim D_{\\mathcal{L}-\\bar{z},s}}}[x]\\;,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\tilde{z}\\;=\\;z/\\sqrt{1+\\sigma^{2}}$ , $s\\;=\\;\\sigma/\\sqrt{1+\\sigma^{2}}$ , $\\mathcal{L}\\;=\\;(1/\\gamma)\\mathbb{Z}$ , and $D_{\\mathcal{L}-\\tilde{z},s}$ is the discrete Gaussian distribution of width $s$ on the lattice coset $\\mathcal{L}-\\tilde{z}$ . ", "page_idx": 19}, {"type": "text", "text": "We upper bound $\\mathbb{E}[|x|]$ via a tail bound on $D_{\\mathcal{L}-\\tilde{z},s}$ . Let $r\\,>\\,0$ be any number and denote $t:=$ $r/s-\\sqrt{1/(2\\pi)}$ . By Theorem B.3 and Lemma B.4, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{s}((\\mathcal{L}-\\tilde{z})\\setminus r B_{2})<\\exp(-\\pi t^{2})\\rho_{s}(\\mathcal{L})}\\\\ &{\\qquad\\qquad\\qquad<\\exp(-\\pi t^{2})\\exp(\\pi\\cdot\\mathrm{dist}(\\tilde{z},\\mathcal{L})^{2}/s^{2})\\rho_{s}(\\mathcal{L}-\\tilde{z})}\\\\ &{\\qquad\\qquad\\qquad=\\exp(-\\pi((r/s-\\sqrt{1/(2\\pi)})^{2}-1/(2s\\gamma)^{2}))\\rho_{s}(\\mathcal{L}-z)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the fact that $\\mathrm{dist}(\\tilde{z},\\mathcal{L})\\leq1/(2\\gamma)$ for any $\\tilde{z}\\in\\mathbb R$ in the last line. ", "page_idx": 19}, {"type": "text", "text": "Thus, for $r/(2s)\\geq\\sqrt{1/(2\\pi)}+1/(2s\\gamma).$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{x\\sim D_{\\mathcal{L}-\\tilde{z},s}}[|x|\\geq r]=\\frac{\\rho_{s}((\\mathcal{L}-\\tilde{z})\\setminus r\\mathcal{B}_{2})}{\\rho_{s}(\\mathcal{L}-\\tilde{z})}\\leq\\exp(-\\pi r^{2}/(2s)^{2})\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denote $r_{0}:=s\\sqrt{2/\\pi}+1/\\gamma$ . Note that $r_{0}<\\sqrt{2/\\pi}+1/\\gamma$ since $s\\in[0,1)$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{x\\sim D_{\\mathcal{L}-\\bar{z},s}}|x|=\\int_{r}\\mathrm{Pr}[|x|\\geq r]d r\\leq r_{0}+\\int_{r>r_{0}}\\mathrm{Pr}[|x|\\geq r]d r}}\\\\ &{\\leq r_{0}+\\int_{r>r_{0}}\\exp(-\\pi r^{2}/(2s)^{2})d r}\\\\ &{\\leq r_{0}+2s}\\\\ &{\\leq(2+\\sqrt{2/\\pi})s+1/\\gamma\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore by Eq.(15) and (17), for any $z\\in\\mathbb{R}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\frac{T^{\\prime}(z)}{T(z)}\\right|\\le\\frac{2\\pi}{\\sigma s}((2+\\sqrt{2/\\pi})s+1/\\gamma)\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Eq.(13) in the statement of Lemma B.5 follows immediately from the fact that $s<\\operatorname*{min}(1,\\sigma)$ and $\\gamma>1$ . Next, we demonstrate a uniform upper bound on $T^{\\prime\\prime}/T$ . The analytical expression of ${\\dot{T}}^{\\prime\\prime}/T$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{T^{\\prime\\prime}(z)}{T(z)}=\\biggl(\\frac{2\\pi}{\\sigma^{2}}\\biggr)^{2}\\cdot\\biggl(\\frac{\\sum_{k\\in\\mathbb{Z}}(z-\\sqrt{1+\\sigma^{2}}k/\\gamma)^{2}\\rho_{\\sigma}(z-\\sqrt{1+\\sigma^{2}}k/\\gamma)}{\\sum_{k\\in\\mathbb{Z}}\\rho_{\\sigma}(z-\\sqrt{1+\\sigma^{2}}k/\\gamma)}-\\frac{\\sigma^{2}}{2\\pi}\\biggr)}}\\\\ {{\\displaystyle\\qquad=\\biggl(\\frac{2\\pi}{\\sigma s}\\biggr)^{2}\\biggl(\\underset{x\\sim D_{\\mathcal{L}-\\bar{z},s}}{\\mathbb{E}}x^{2}-\\frac{s^{2}}{2\\pi}\\biggr)~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\tilde{z}=z/\\sqrt{1+\\sigma^{2}},s=\\sigma/\\sqrt{1+\\sigma^{2}}$ , and $\\mathcal{L}=(1/\\gamma)\\mathbb{Z}$ . ", "page_idx": 19}, {"type": "text", "text": "To uniformly bound $|T^{\\prime\\prime}/T|$ , we upper bound the second moment of $D_{\\mathcal{L}-\\tilde{z},s}$ . Again, let $r_{0}~=$ $s\\sqrt{2/\\pi}+1/\\gamma$ . Using the tail bound from Eq.(16) and the fact that $(a+b)^{2}\\leq2a^{2}+2b^{2}$ for any $a,b\\in\\mathbb{R}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\sim D_{\\mathcal{L}-z,\\sigma}}{\\mathbb{E}}x^{2}=\\displaystyle\\int_{0}^{\\infty}r\\,\\mathrm{Pr}[|x|\\ge r]d r\\le r_{0}^{2}/2+\\displaystyle\\int_{r\\ge r_{0}}r\\,\\mathrm{Pr}[|x|\\ge r]d r}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le r_{0}^{2}/2+\\displaystyle\\int_{r\\ge r_{0}}r\\exp(-\\pi r^{2}/(2s^{2}))d r}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le r_{0}^{2}/2+s^{2}/\\pi}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le s^{2}+1/\\gamma^{2}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying Eq.(17) \u221aand Eq.(19) to the expression for $f^{\\prime}=T^{\\prime}/T$ (Eq.(14)) and using the fact that $\\gamma>1$ and $s=\\sigma/\\sqrt{1+\\sigma^{2}}\\leq\\operatorname*{min}(1,\\sigma)$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f^{\\prime}\\|_{\\infty}\\leq\\|T^{\\prime\\prime}/T\\|_{\\infty}+\\|(T^{\\prime}/T)^{2}\\|_{\\infty}}\\\\ &{\\qquad\\leq\\Big(\\frac{2\\pi}{\\sigma s}\\Big)^{2}\\big((1+1/2\\pi)s^{2}+1/\\gamma^{2}+((2+\\sqrt{2/\\pi})s+1/\\gamma)^{2}\\big)}\\\\ &{\\qquad\\leq\\frac{100\\pi^{2}}{s^{4}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, $\\nabla\\log P_{\\mathbf{u}}(\\mathbf{x})$ is $O(1/s^{4})$ -Lipschitz since $2\\pi+\\|f^{\\prime}\\|_{\\infty}\\lesssim1/s^{4}$ . ", "page_idx": 20}, {"type": "text", "text": "B.3 Distance from the standard Gaussian ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now prove lower (Lemma B.9) and upper (Lemma B.10) bounds on the TV distance between Gaussian pancakes distributions $\\left(P_{u}\\right)$ and the standard Gaussian $Q$ . We also show that the KL divergence is upper bounded by $\\mathrm{poly}(d)$ for Gaussian pancakes with $\\sigma\\geq1/\\mathrm{poly}(d)$ (Lemma B.11). ", "page_idx": 20}, {"type": "text", "text": "The following fact reduces the $d$ -dimensional problem of bounding $\\mathrm{TV}(P_{u},Q_{d})$ to a one-dimensional problem of bounding $\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)$ , where $A_{\\gamma}^{\\sigma}$ is the $\\sigma$ -smoothed discrete Gaussian on $(1/\\gamma)\\mathbb{Z}$ and $Q=Q_{1}$ . Without loss of generality, assume $\\pmb{u}=e_{1}$ . Then, by the $L^{1}$ -characterization of the TV distance, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{TV}(P_{u},Q_{d})=\\frac{1}{2}\\int|P_{u}({\\pmb x})-Q_{d}({\\pmb x})|d x=\\frac{1}{2}\\int Q_{d}({\\pmb x})|T_{\\gamma}^{\\sigma}(x_{1})-1|d x}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2}\\int|A_{\\gamma}^{\\sigma}(x_{1})-Q(x_{1})|d x_{1}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\quad=\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, it suffices to demonstrate bounds on $\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)$ . The same applies to the KL divergence since ${\\mathrm{KL}}(P_{\\pmb{u}}\\parallel Q_{d})\\,=\\,{\\mathrm{KL}}(A_{\\gamma}^{\\sigma}\\parallel Q)$ . We first demonstrate a lower bound on the total variation distance. To this end, we introduce the periodic Gaussian distribution and its useful properties. The key lemma is Lemma B.9. ", "page_idx": 20}, {"type": "text", "text": "Definition B.6 (Periodic Gaussian distribution). For any one-dimensional lattice ${\\mathcal{L}}\\subset\\mathbb{R},$ , we define the periodic Gaussian distribution $\\Psi_{\\mathcal{L},s}:\\mathbb{R}\\rightarrow\\mathbb{R}_{\\geq0}$ as follows. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Psi_{\\mathcal{L},s}(z):=\\frac{1}{s}\\sum_{x\\in\\mathcal{L}}\\rho_{s}(x-z)=\\rho_{s}(\\mathcal{L}-z)/s\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can regard the function $\\Psi_{\\mathcal{L},s}$ as a distribution for the following reason: Let $\\lambda_{1}(\\mathcal{L})$ denote the spacing of $\\mathcal{L}$ . Then, $\\Psi_{\\mathcal{L},s}$ restricted to $[0,\\lambda_{1}(\\mathcal{L})]$ is a probability density since ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{0}^{\\lambda_{1}(\\mathcal{L})}\\Psi_{\\mathcal{L},s}(z)d z=\\frac{1}{s}\\int_{0}^{\\lambda_{1}(\\mathcal{L})}\\sum_{x\\in\\mathcal{L}}\\rho_{s}(x-z)=\\frac{1}{s}\\sum_{x\\in\\mathcal{L}}\\int_{0}^{\\lambda_{1}(\\mathcal{L})}\\rho_{s}(x-z)d z=\\frac{1}{s}\\int_{-\\infty}^{\\infty}\\rho_{s}(z)d z=1\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma B.7 (Mill\u2019s inequality [77, Proposition 2.1.2]). Let $z\\sim\\mathcal{N}(0,1)$ . Then for all $t>0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}(|z|\\geq t)=\\sqrt{\\frac{2}{\\pi}}\\int_{t}^{\\infty}e^{-x^{2}/2}d x\\leq\\frac{1}{t}\\cdot\\sqrt{\\frac{2}{\\pi}}e^{-t^{2}/2}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma B.8 (Periodic Gaussian density bound [71, Claim I.6]). For any $s>0$ and any $z\\in[0,1)$ the periodic Gaussian density $\\Psi_{\\mathbb{Z},s}:[0,1)\\to\\mathbb{R}_{\\geq0}$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\Psi_{\\mathbb{Z},s}(z)-1|\\leq2(1+1/(\\pi s))e^{-\\pi s^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By the Poisson summation formula (Lemma 2.1), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{\\mathbb{Z},s}(z)=\\displaystyle\\frac{1}{s}\\sum_{x\\in\\mathbb{Z}}\\rho_{s}(x-z)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{y\\in\\mathbb{Z}}e^{-2\\pi i z y}\\cdot\\rho_{1/s}(y)}\\\\ &{\\qquad\\qquad=1+\\displaystyle\\sum_{y\\in\\mathbb{Z}\\backslash\\{0\\}}e^{-2\\pi i z y}\\cdot\\rho_{1/s}(y)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $|e^{i a}|\\leq1$ for any $a\\in\\mathbb{R}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\Psi_{\\mathbb{Z},s}(z)-1|\\leq\\displaystyle\\sum_{y\\in\\mathbb{Z}\\setminus\\{0\\}}\\vert e^{-2\\pi i z y}\\vert\\cdot\\rho_{1/s}(y)\\le\\displaystyle\\sum_{y\\in\\mathbb{Z}\\backslash\\{0\\}}\\rho_{1/s}(y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le2\\bigg(e^{-\\pi s^{2}}+\\displaystyle\\int_{1}^{\\infty}e^{-\\pi s^{2}t^{2}}d t\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le2(1+1/(\\pi s))e^{-\\pi s^{2}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma B.9 (TV lower bound). There exists a constant $C>0$ such that for any $\\gamma>1$ and $\\sigma>0$ , $i f\\gamma\\sigma<C$ , then $\\mathrm{TV}\\big(A_{\\gamma}^{\\sigma},\\!\\mathcal{N}(0,1/(2\\pi))\\big)>1/2$ , where $A_{\\gamma}^{\\sigma}$ is the $\\sigma$ -smoothed discrete Gaussian on $(1/\\gamma)\\mathbb{Z}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. For ease of notation, we denote $\\mathcal{N}(0,1/(2\\pi))$ by $Q$ . Since $\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)=\\mathrm{sup}_{S\\in\\mathcal{F}}\\,|A_{\\gamma}^{\\sigma}(S)-$ $Q(S)|$ , where $\\mathcal{F}$ is the Borel $\\sigma$ -algebra on $\\mathbb{R}$ , it suffices to find a measurable set $S\\subset\\mathbb{R}$ such that $A_{\\gamma}^{\\dot{\\sigma}}(\\ddot{S})-Q(S)>1/2$ . ", "page_idx": 21}, {"type": "text", "text": "Let $C=1/(12{\\sqrt{2\\log2}})$ be the constant in the statement of Lemma B.9. Let $\\delta\\,\\in\\,(0,1/4)$ be the smallest number satisfying the condition $\\gamma\\sigma\\leq\\delta/(3\\sqrt{\\log(1/\\delta)})$ . Such $\\delta>0$ always exists under the given assumptions since $\\delta/{\\sqrt{\\log(1/\\delta)}}$ is increasing in $\\delta$ and $\\delta=1/4$ satisfies the condition (thanks to our choice of $C$ ). We claim that the set $S$ defined below witnesses the TV lower bound. ", "page_idx": 21}, {"type": "equation", "text": "$$\nS:=\\left\\{z\\in\\mathbb{R}\\mid{\\mathrm{dist}}(z,{\\mathcal{L}})\\leq{\\frac{\\sigma}{\\sqrt{1+\\sigma^{2}}}}\\cdot{\\sqrt{\\log(1/\\delta)}}\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathcal{L}=(1/\\gamma\\sqrt{1+\\sigma^{2}})\\mathbb{Z}$ . ", "page_idx": 21}, {"type": "text", "text": "We show a lower bound for $A_{\\gamma}^{\\sigma}(S)$ and an upper bound for $Q(S)$ . Using the mixture form of the density of $A_{\\gamma}^{\\sigma}$ (Eq.(4)) and Mill\u2019s tail bounds for the univariate Gaussian (Lemma B.7), we have that for each Gaussian component in the mixture $A_{\\gamma}^{\\sigma}$ , at least $1-\\delta$ fraction of its probability mass is contained in $S$ . This is because the component means precisely form the one-dimensional lattice $\\mathcal{L}$ and $S$ contains all significant neighborhoods of $\\mathcal{L}$ . Thus, $A_{\\gamma}^{\\sigma}(\\dot{S})\\geq1-\\delta$ . ", "page_idx": 21}, {"type": "text", "text": "Now we show an upper bound for $Q(S)$ . Recall from Definition B.6 the density of the periodic Gaussi\u221aan distribution $\\Psi_{\\mathbb{Z},s}$ . Since $S$ is a periodic set, its mass $Q(S)$ is equal to $\\Psi_{\\mathbb{Z},s}(\\tilde{S}\\cap\\mathbb{Z})$ , where $s=\\gamma\\sqrt{1+\\sigma^{2}}$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{S}=\\left\\{z\\in\\mathbb{R}\\mid\\mathrm{dist}(z,\\mathbb{Z})\\leq\\gamma\\sigma\\sqrt{\\log(1/\\delta)}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $s=\\gamma\\sqrt{1+\\sigma^{2}}>1$ , by Lemma B.8 for any $z\\in[0,1)$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\Psi_{\\mathbb{Z},s}(z)-1|\\leq4e^{-\\pi s^{2}}<1/2\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\gamma\\sigma\\leq\\delta/(3\\sqrt{\\log(1/\\delta)})$ , it follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\nQ(S)=\\Psi_{\\mathbb{Z},s}(\\tilde{S}\\cap[0,1])\\leq\\left(1+4e^{-\\pi s^{2}}\\right)\\cdot2\\gamma\\sigma\\sqrt{\\log(1/\\delta)}<3\\gamma\\sigma\\sqrt{\\log(1/\\delta)}\\leq\\delta\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)\\geq A_{\\gamma}^{\\sigma}(S)-Q(S)>1-2\\delta>1/2\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now establish upper bounds on $\\mathrm{TV}(P_{u},Q_{d})$ via upper bounds on $\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)$ . Lemma B.10 provides a tighter upper bound when $\\operatorname*{min}(\\gamma,\\gamma\\sigma)\\;=\\;\\omega(\\sqrt{\\log d})$ . In this regime, the sequence $\\bar{(\\mathrm{TV}}(P_{\\boldsymbol{u}},Q_{d})\\bar{)}_{d\\in\\mathbb{N}}$ is negligible in $d$ . It is worth noting that Lemma B.10 is not tight since $\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)\\rightarrow0$ as $\\sigma\\rightarrow\\infty$ , whereas the upper bound only converges to $e^{-\\pi\\gamma^{2}}$ ", "page_idx": 22}, {"type": "text", "text": "On the other hand, Lemma B.11 provides an upper bound on $\\mathrm{KL}(P_{\\mathbf{\\sigma}}\\parallel Q_{d})$ which is useful when $\\sigma$ is large. Note that the KL divergence upper bounds the TV distance through Pinsker\u2019s or the Bretagnolle\u2013Huber inequality (see e.g., [14]). ", "page_idx": 22}, {"type": "text", "text": "Lemma B.10 (TV upper bound). For any $\\gamma>1$ and $\\sigma>0$ such that $\\sigma\\geq2/\\gamma,$ the following holds for the $\\sigma$ -smoothed discrete Gaussian $A_{\\gamma}^{\\sigma}$ and $Q=\\mathcal{N}(0,1/(2\\pi))$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)\\lesssim e^{-\\pi s^{2}}\\ ,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $s=\\gamma\\sigma/\\sqrt{1+\\sigma^{2}}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. By the $L^{1}$ -characterization of the TV distance, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)=\\frac{1}{2}\\int|A_{\\gamma}^{\\sigma}(x)-Q(x)|d x=\\frac{1}{2}\\int Q(x)|T_{\\gamma}^{\\sigma}(x)-1|d x\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the likelihood ratio $T_{\\gamma}^{\\sigma}$ is given by (see Eq.(5)) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{T_{\\gamma}^{\\sigma}(x)=\\displaystyle\\frac{\\sqrt{1+\\sigma^{2}}}{\\sigma\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{k\\in\\mathbb{Z}}\\rho_{\\sigma}(x-\\sqrt{1+\\sigma^{2}}k/\\gamma)}}\\\\ {{\\displaystyle\\qquad=\\frac{\\sqrt{1+\\sigma^{2}}}{\\sigma\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{k\\in\\mathbb{Z}}\\rho_{\\gamma\\sigma/\\sqrt{1+\\sigma^{2}}}(\\gamma x/\\sqrt{1+\\sigma^{2}}-k)~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The likelihood ratio is also a re-scaled version of the periodic Gaussian density since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Psi_{\\mathbb{Z},s}(t)=\\frac{\\rho_{s}(\\mathbb{Z}-t)}{s}=\\frac{1}{s}\\sum_{k\\in\\mathbb{Z}}\\rho_{s}(k-t)~.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Plugging in $s=\\gamma\\sigma/\\sqrt{1+\\sigma^{2}}=\\gamma/\\sqrt{(1/\\sigma^{2})+1}$ to Eq.(20), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nT_{\\gamma}^{\\sigma}(x)=\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\cdot\\Psi_{\\mathbb{Z},s}(\\gamma x/\\sqrt{1+\\sigma^{2}})\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The assumption $\\sigma\\geq2/\\gamma$ implies that $s\\geq1$ . Thus, by Lemma B.8 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|T_{\\gamma}^{\\sigma}(x)-1|\\leq\\displaystyle\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\bigg\\vert\\Psi_{\\mathbb{Z},s}(\\gamma x/\\sqrt{1+\\sigma^{2}})-1\\bigg\\vert+\\bigg\\vert\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}-1\\bigg\\vert}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\cdot2(1+1/(\\pi s))e^{-\\pi s^{2}}+\\bigg\\vert\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}-1\\bigg\\vert}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\cdot3e^{-\\pi s^{2}}+\\bigg\\vert\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}-1\\bigg\\vert\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\rho((1/\\gamma)\\mathbb{Z})=\\rho_{\\gamma}(\\mathbb{Z})=\\gamma\\Psi_{\\mathbb{Z},\\gamma}(0)$ and $\\gamma>1$ , by Lemma B.8 applied to $\\Psi_{\\mathbb{Z},\\gamma}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\frac{\\rho((1/\\gamma)\\mathbb{Z})}{\\gamma}-1\\right|\\leq2(1+1/(\\pi\\gamma))e^{-\\pi\\gamma^{2}}<3e^{-\\pi\\gamma^{2}}<1/4\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We may thus write $\\rho((1/\\gamma)\\mathbb{Z})/\\gamma=1+\\varepsilon$ for some $\\varepsilon\\in\\mathbb R$ such that $|\\varepsilon|\\leq3e^{-\\pi\\gamma^{2}}<1/4$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}}-1\\right|=\\left|{\\frac{1}{1+\\varepsilon}}-1\\right|=\\left|{\\frac{\\varepsilon}{1+\\varepsilon}}\\right|<(4/3)\\varepsilon~.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Applying the above inequalities to Eq.(21), we have that for any $x\\in\\mathbb R$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|T_{\\gamma}^{\\sigma}(x)-1|\\leq\\cfrac{\\gamma}{\\rho\\left((1/\\gamma)\\mathbb{Z}\\right)}\\cdot3e^{-\\pi s^{2}}+\\bigg|\\cfrac{\\gamma}{\\rho\\left((1/\\gamma)\\mathbb{Z}\\right)}-1\\bigg|}\\\\ &{\\qquad\\qquad<(1+3e^{-\\pi\\gamma^{2}})\\cdot3e^{-\\pi s^{2}}+4e^{-\\pi\\gamma^{2}}}\\\\ &{\\qquad\\qquad\\leq4(e^{-\\pi s^{2}}+e^{-\\pi\\gamma^{2}})\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $0<s<\\gamma$ , it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{TV}(A_{\\gamma}^{\\sigma},Q)\\leq4(e^{-\\pi s^{2}}+e^{-\\pi\\gamma^{2}})\\leq8e^{-\\pi s^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma B.11 (KL upper bound). For any $\\gamma>0$ and $\\sigma>0$ , the following holds for the $\\sigma$ -smoothed discrete Gaussian $A_{\\gamma}^{\\sigma}$ and $Q=\\mathcal{N}(0,1/\\dot{(2\\pi)})$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{KL}(A_{\\gamma}^{\\sigma}||Q)\\leq\\log(\\sqrt{1+\\sigma^{2}}/\\sigma)\\leq\\frac{1}{2\\sigma^{2}}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Expressed in terms of the time with respect to the OU process $e^{-t}=1/\\sqrt{1+\\sigma^{2}}$ , for any $t>0$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{KL}(A_{\\gamma}^{\\sigma}\\parallel Q)\\leq\\frac{e^{-2t}}{2(1-e^{-2t})}\\ .\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By Jensen\u2019s inequality and the fact that for any $x\\in\\mathbb R$ , $T_{\\gamma}^{\\sigma}(x)\\leq T_{\\gamma}^{\\sigma}(0)$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(A_{\\gamma}^{\\sigma}||Q)=\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[\\log T_{\\gamma}^{\\sigma}(x)]\\le\\log\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}T_{\\gamma}^{\\sigma}(x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\log T_{\\gamma}^{\\sigma}(0)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\log\\frac{\\sqrt{1+\\sigma^{2}}}{\\sigma\\rho_{\\gamma}(\\mathbb Z)}\\cdot\\rho_{s}(\\mathbb Z)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $s=\\gamma\\sigma/\\sqrt{1+\\sigma^{2}}<\\gamma.$ . ", "page_idx": 23}, {"type": "text", "text": "If $0\\leq s_{1}\\leq s_{2}$ , then $\\rho_{s_{1}}(\\mathbb{Z})\\leq\\rho_{s_{2}}(\\mathbb{Z})$ . Hence, $\\rho_{s}(\\mathbb{Z})/\\rho_{\\gamma}(\\mathbb{Z})\\leq1$ . Using the fact that $\\log(1\\!+\\!a)\\leq a$ for any $a>-1$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{KL}(A_{\\gamma}^{\\sigma}\\parallel Q)\\leq\\log({\\sqrt{1+\\sigma^{2}}}/\\sigma)=(1/2)\\log(1+1/\\sigma^{2})\\leq1/(2\\sigma^{2})\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The second part of the lemma follows straightforwardly from the relation $\\sigma/\\sqrt{1+\\sigma^{2}}=\\sqrt{1-e^{-2t}}$ . Using the fact that $\\log(1-a)\\geq-a/(1-a)$ for any $a<1$ , for any $t>0$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{KL}(A_{\\gamma}^{\\sigma}\\parallel Q)\\leq\\log(1/\\sqrt{1-e^{-2t}})=-(1/2)\\log(1-e^{-2t})\\leq\\frac{e^{-2t}}{2(1-e^{-2t})}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C Proofs for Section 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Sample complexity of score estimation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We show that score estimation reduces to parameter estimation for Gaussian pancakes. Given that the sample complexity of parameter estimation for Gaussian pancakes is polynomial in the relevant problem parameters (Theorem 4.2), our reduction implies that the sample complexity of score estimation is polynomial as well. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.1 (Lemma 4.1 restated). For any $\\gamma>1,\\sigma>0$ , let $P_{u}$ be the $(\\gamma,\\sigma)$ -Gaussian pancakes distribution with secret direction $\\textbf{\\em u}\\in\\ \\mathbb{S}^{d-1}$ . Given any $\\eta~\\in~(0,1)$ and $\\hat{\\textbf{\\em u}}\\in\\ \\mathbb{S}^{d-1}$ such that $1-\\langle\\hat{\\mathbf{u}},\\pmb{u}\\rangle^{2}\\leq\\eta^{2}$ , the score estimate $\\hat{s}(\\pmb{x})=-2\\pi\\pmb{x}+\\nabla\\log T_{\\gamma}^{\\sigma}(\\langle\\pmb{x},\\hat{\\pmb{u}}\\rangle)$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{x}\\sim P_{u}}\\|\\hat{\\pmb{s}}(\\pmb{x})-\\pmb{s}(\\pmb{x})\\|^{2}\\lesssim\\operatorname*{max}(1,1/\\sigma^{8})\\cdot\\eta^{2}d\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $s(\\pmb{x})=-2\\pi\\pmb{x}+\\nabla\\log T_{\\gamma}^{\\sigma}(\\langle\\pmb{x},\\pmb{u}\\rangle)$ is the score function of $P_{u}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. For simplicity, we omit super and subscripts of the likelihood ratio $T_{\\gamma}^{\\sigma}$ and simply denote it by $T$ . Let $\\hat{\\pmb u}$ be an estimate satisfying $1-\\langle\\hat{\\mathbf{u}},\\pmb{u}\\rangle^{2}\\leq\\eta^{2}$ and denote $\\hat{\\pmb u}=\\langle\\hat{\\pmb u},\\pmb u\\rangle\\pmb u+\\pmb w$ . Note that $\\pmb{w}\\in\\mathbb{R}^{d}$ is orthogonal to $\\textbf{\\em u}$ and $\\|\\pmb{w}\\|^{2}=1-\\langle\\hat{\\pmb{u}},\\pmb{u}\\rangle^{2}\\leq\\eta^{2}$ . Then, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s(\\mathbf{x})-\\hat{s}(\\mathbf{x})=\\frac{T^{\\prime}\\left(\\langle\\mathbf{x},\\mathbf{u}\\rangle\\right)}{T\\left(\\langle\\mathbf{x},\\mathbf{u}\\rangle\\right)}\\pmb{u}-\\frac{T^{\\prime}\\left(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle\\right)}{T\\left(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle\\right)}\\hat{\\pmb{u}}}\\\\ &{\\qquad\\qquad=\\left(\\frac{T^{\\prime}\\left(\\langle\\mathbf{x},\\mathbf{u}\\rangle\\right)}{T\\left(\\langle\\mathbf{x},\\mathbf{u}\\rangle\\right)}-\\frac{T^{\\prime}\\left(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle\\right)}{T\\left(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle\\right)}\\langle\\mathbf{u},\\hat{\\pmb{u}}\\rangle\\right)\\pmb{u}-\\frac{T^{\\prime}\\left(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle\\right)}{T\\left(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle\\right)}\\pmb{w}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the triangle inequality and the fact that $(a+b)^{2}\\leq2a^{2}+2b^{2}$ for any $a,b\\in\\mathbb{R}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s(\\boldsymbol{x})-\\hat{s}(\\boldsymbol{x})\\|^{2}\\le2\\bigg(\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\boldsymbol{u}\\rangle)}{T(\\langle\\boldsymbol{x},\\boldsymbol{u}\\rangle)}-\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}{T(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}\\langle\\boldsymbol{u},\\hat{\\boldsymbol{u}}\\rangle\\bigg)^{2}+2\\bigg(\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}{T(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}\\bigg)^{2}\\eta^{2}}\\\\ &{\\qquad\\qquad\\le4\\bigg(\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\boldsymbol{u}\\rangle)}{T(\\langle\\boldsymbol{x},\\boldsymbol{u}\\rangle)}-\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}{T(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}\\bigg)^{2}+4\\bigg(\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}{T(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}\\bigg)^{2}(1-|\\langle\\boldsymbol{u},\\hat{\\boldsymbol{u}}\\rangle|)^{2}+2\\bigg(\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}{T(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}\\bigg)^{2}}\\\\ &{\\qquad\\qquad\\lesssim\\bigg(\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\boldsymbol{u}\\rangle)}{T(\\langle\\boldsymbol{x},\\boldsymbol{u}\\rangle)}-\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}{T(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}\\bigg)^{2}+\\bigg(\\displaystyle\\frac{T^{\\prime}(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}{T(\\langle\\boldsymbol{x},\\hat{\\boldsymbol{u}}\\rangle)}\\bigg)^{2}\\eta^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma B.5 and the fact that $(1+\\sigma^{2})/\\sigma^{2}\\,\\leq\\,2\\operatorname*{max}(1,1/\\sigma^{2})$ , we know that the Lipschitz constant $L$ of $T^{\\prime}/T$ satisfies $L\\lesssim\\operatorname*{max}(1,1/\\sigma^{4})$ . Furthermore, by Eq. (13) in Lemma B.5, we have $\\|T^{\\prime}/T\\|_{\\infty}\\lesssim\\operatorname*{max}(1,1/\\sigma^{2})$ . Applying these upper bounds to Eq.(22), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|s(\\mathbf{x})-\\hat{s}(\\mathbf{x})\\|^{2}\\lesssim\\bigg(\\frac{T^{\\prime}(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle)}{T(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle)}-\\frac{T^{\\prime}(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle)}{T(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle)}\\bigg)^{2}+\\bigg(\\frac{T^{\\prime}(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle)}{T(\\langle\\mathbf{x},\\hat{\\mathbf{u}}\\rangle)}\\bigg)^{2}\\eta^{2}}\\\\ {\\lesssim\\operatorname*{max}(1,1/\\sigma^{8})(\\langle\\mathbf{x},u-\\hat{\\mathbf{u}}\\rangle^{2}+\\eta^{2})}\\\\ {\\leq\\operatorname*{max}(1,1/\\sigma^{8})(\\|\\mathbf{x}\\|^{2}\\|u-\\hat{u}\\|^{2}+\\eta^{2})}\\\\ {\\leq\\operatorname*{max}(1,1/\\sigma^{8})\\cdot\\eta^{2}(\\|\\mathbf{x}\\|^{2}+1)~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\mathbb{E}_{\\pmb{x}\\sim P_{\\pmb{u}}}\\|\\pmb{x}\\|^{2}\\leq\\,d$ by Lemma B.2, it follows that $\\mathbb{E}_{\\pmb{x}\\sim P_{\\pmb{u}}}||\\pmb{s}(\\pmb{x})-\\hat{\\pmb{s}}(\\pmb{x})||^{2}\\lesssim\\operatorname*{max}(1,1/\\sigma^{8})$ \u00b7 $\\eta^{2}d$ . ", "page_idx": 24}, {"type": "text", "text": "C.2 Sample complexity of parameter estimation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For parameter estimation, we design a contrast function $g\\,:\\,\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ such that the (population) functionals $G$ and $E$ , defined below, are monotonic. For any $\\gamma>0$ , $\\pmb{u}\\in\\mathbb{S}^{d-1}$ , and $(\\gamma,0)$ -Gaussian pancakes $P_{u}$ , we define ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\sigma):=\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[g(x)]}\\\\ &{E(\\pmb{v}):=\\mathbb{E}_{\\pmb{x}\\sim P_{\\pmb{u}}}[g(\\langle\\pmb{x},\\pmb{v}\\rangle)]\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $E(\\pmb{v})\\,=\\,G(\\sigma)$ , where $\\sigma^{2}\\,=\\,(1\\,-\\,\\langle{\\pmb u},{\\pmb v}\\rangle^{2})/\\langle{\\pmb u},{\\pmb v}\\rangle^{2}$ . We choose $g$ so that $G(\\sigma)$ is decreasing in $\\sigma$ and $E(v)$ is increasing in $\\langle\\boldsymbol{u},\\boldsymbol{v}\\rangle^{2}$ . We use $g=T_{\\gamma}^{\\beta}$ for some appropriately chosen $\\beta\\,>\\,0$ . The monotonicity property of $T_{\\gamma}^{\\beta}$ is shown in Lemma C.3. Thus, given two candidate directions $\\pmb{v}_{1},\\pmb{v}_{2}$ , if $E(\\pmb{v}_{1})\\geq E(\\pmb{v}_{2})$ , then $\\langle\\mathbf{u},\\mathbf{v}_{1}\\rangle^{2}\\geq\\langle\\mathbf{u},\\mathbf{v}_{2}\\rangle^{2}$ . This suggests the projection pursuitbased estimator $\\hat{\\pmb u}\\,=\\,\\arg\\operatorname*{max}_{\\pmb v\\in\\mathcal C}\\hat{E}(\\pmb v)$ , where $\\mathcal{C}$ is an $\\eta$ -net of the parameter space $\\mathbb{S}^{d-1}$ and $\\begin{array}{r}{\\hat{E}(\\pmb{v})=(1/n)\\sum_{i=1}^{n}T_{\\gamma}^{\\beta}(\\langle\\pmb{x}_{i},\\pmb{v}\\rangle)}\\end{array}$ is the empirical version of $E$ . The main theorem of this section (Theorem 4.2) shows that $n=\\mathrm{poly}(d,\\gamma,1/\\eta)$ samples is sufficient for achieving $L^{2}$ -error $\\eta$ using the estimator $\\hat{\\pmb u}$ . We start with a useful fact about $\\sigma$ -smoothed likelihood ratios $T_{\\gamma}^{\\sigma}$ . ", "page_idx": 24}, {"type": "text", "text": "Claim C.2. For any $\\beta>0,\\sigma\\geq0$ , and $x\\in\\mathbb R$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z\\sim Q}\\left[T_{\\gamma}^{\\beta}\\left(\\frac{1}{\\sqrt{1+\\sigma^{2}}}x+\\frac{\\sigma}{\\sqrt{1+\\sigma^{2}}}z\\right)\\right]=T_{\\gamma}^{s}(x)\\ ,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $s=\\sqrt{(1+\\beta^{2})(1+\\sigma^{2})-1}$ and $T_{\\gamma}^{\\sigma}$ denotes the likelihood ratio of the $\\sigma$ -smoothed discrete Gaussian $A_{\\gamma}^{\\sigma}$ on $(1/\\gamma)\\mathbb{Z}$ with respect to $Q^{'}{=}\\mathcal{N}(0,1/(2\\pi))$ (see Eq. (5)). ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{Q}\\left[T_{\\gamma}^{\\beta}\\big(\\tilde{x}+\\tilde{\\sigma}z\\big)\\right]=\\frac{\\sqrt{1+\\beta^{2}}}{\\beta\\rho((1/\\gamma)\\mathbb{Z})}\\int\\rho(z)\\sum_{k\\in\\mathbb{Z}}\\rho_{\\beta}\\bigg(\\tilde{x}+\\tilde{\\sigma}z-\\sqrt{1+\\beta^{2}}k/\\gamma\\bigg)d z}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\sqrt{1+\\beta^{2}}}{\\beta\\rho((1/\\gamma)\\mathbb{Z})}\\displaystyle\\sum_{k\\in\\mathbb{Z}}\\int\\rho_{\\beta/\\sqrt{\\beta^{2}+\\tilde{\\sigma}^{2}}}(z-c_{k}(\\tilde{x}))d z\\cdot\\rho_{\\sqrt{\\beta^{2}+\\tilde{\\sigma}^{2}}}(c_{k}(\\tilde{x}))}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\sqrt{1+\\beta^{2}}}{\\sqrt{\\beta^{2}+\\tilde{\\sigma}^{2}}\\cdot\\rho((1/\\gamma)\\mathbb{Z})}\\displaystyle\\sum_{k\\in\\mathbb{Z}}\\rho_{\\sqrt{\\beta^{2}+\\tilde{\\sigma}^{2}}}(\\tilde{x}-\\sqrt{1+\\beta^{2}}k/\\gamma)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging in $\\tilde{x}=x/\\sqrt{1+\\sigma^{2}}$ and $\\tilde{\\sigma}=\\sigma/\\sqrt{1+\\sigma^{2}}$ gives us ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Q}\\left[T_{\\gamma}^{\\beta}\\Big(\\frac{1}{\\sqrt{1+\\sigma^{2}}}x+\\frac{\\sigma}{\\sqrt{1+\\sigma^{2}}}z\\Big)\\right]=\\frac{\\sqrt{1+s^{2}}}{s\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{k\\in\\mathbb{Z}}\\rho_{s}(x-\\sqrt{1+s^{2}}k/\\gamma)=T_{\\gamma}^{s}(x)\\;,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $s=\\sqrt{(1+\\beta^{2})(1+\\sigma^{2})-1}$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma C.3 (Monotonicity). Given any $\\gamma>0$ and $\\beta>0$ , define $G(\\sigma):=\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[T_{\\gamma}^{\\beta}(x)]$ , where $T_{\\gamma}^{\\sigma}$ denotes the likelihood ratio of the $\\sigma$ -smoothed discrete Gaussian $A_{\\gamma}^{\\sigma}$ on $(1/\\gamma)\\mathbb{Z}$ with respect to $\\tilde{\\mathcal{N}}(0,1/(2\\pi))$ . Then, for any $\\sigma>0$ , we have $G^{\\prime}(\\sigma)<0$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{T_{\\gamma}^{0}(x)=\\sum_{k=0}^{\\infty}\\alpha_{2k}h_{2k}(x)}\\end{array}$ be the (formal) Hermite expansion of $T_{\\gamma}^{0}(x)$ , where $(h_{k})_{k\\in\\mathbb{N}}$ form an orthonormal sequence with respect to $\\langle\\cdot,\\cdot\\rangle_{Q}$ . By Claim C.2, for any $\\sigma\\geq0$ ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{\\gamma}^{\\sigma}(x)=\\sum_{k=0}^{\\infty}\\alpha_{2k}{\\left(\\frac{1}{1+\\sigma^{2}}\\right)}^{k}h_{2k}(x)\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the orthonormality of $\\left(h_{k}\\right)$ , for any $\\sigma>0$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{G(\\sigma)=\\mathbb{E}_{x\\sim A_{\\gamma}^{\\sigma}}[T_{\\gamma}^{\\beta}(x)]=\\langle T_{\\gamma}^{\\beta},T_{\\gamma}^{\\sigma}\\rangle_{Q}=\\displaystyle\\sum_{k=0}^{\\infty}\\alpha_{2k}^{2}\\cdot\\frac{1}{(1+\\beta^{2})^{k}(1+\\sigma^{2})^{k}}}}\\\\ {{G^{\\prime}(\\sigma)=-\\displaystyle\\sum_{k\\geq1}^{\\infty}\\alpha_{2k}^{2}\\cdot\\frac{1}{(1+\\beta^{2})^{k}}\\cdot\\frac{2k\\sigma}{(1+\\sigma^{2})^{k+1}}<0\\;.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma C.4 (Non-trivial Hermite coefficient). Let $\\left(h_{k}\\right)$ be the normalized Hermite polynomials with respect to $\\langle\\cdot,\\cdot\\rangle_{Q}$ . For any $\\gamma>1$ such that $\\pi\\gamma^{2}\\in\\dot{\\mathbb{N}}$ and $\\ell\\in\\mathbb{N}$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{x\\sim A_{\\gamma}}h_{2\\pi\\ell^{2}\\gamma^{2}}(x)\\right|\\geq\\frac{1}{\\sqrt{2\\pi\\ell\\gamma}}\\ ,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $A_{\\gamma}$ is the discrete Gaussian on $(1/\\gamma)\\mathbb{Z}$ ", "page_idx": 25}, {"type": "text", "text": "Proof. Let $\\left(H_{k}\\right)$ be the unnormalized Hermite polynomials defined by ", "page_idx": 25}, {"type": "equation", "text": "$$\nH_{k}(x)\\rho(x)=(-1)^{k}\\frac{d^{k}}{d x^{k}}\\rho(x)\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the relation between the Fourier transform and differentiation, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{F}\\{H_{k}(x)\\rho(x)\\}=\\mathcal{F}\\bigg\\{{(-1)}^{k}\\frac{d^{k}}{d x^{k}}\\rho(x)\\bigg\\}=(-2\\pi i y)^{k}\\rho(y)\\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $h_{k}\\,=\\,c_{k}H_{k}$ be the normalized Hermite polynomials, where $c_{k}\\,=\\,\\sqrt{(2\\pi)^{k}k!}$ ! (see e.g., [64, Chapter 4.2.1]). By the Poisson summation formula (Lemma 2.1), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x\\sim A_{\\gamma}}h_{k}(x)=\\frac{1}{\\rho((1/\\gamma)\\mathbb{Z})}\\displaystyle\\sum_{x\\in(1/\\gamma)\\mathbb{Z}}c_{k}H_{k}(x)\\rho(x)}\\\\ &{\\qquad\\qquad=\\frac{\\gamma c_{k}}{\\rho((1/\\gamma)\\mathbb{Z})}\\displaystyle\\sum_{y\\in\\gamma\\mathbb{Z}}(-2\\pi i y)^{k}\\rho(y)~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now analyze the maximum among the terms inside the sum. Let $f(y)=y^{k}\\rho(y)$ . Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(y)=y^{k}\\rho(y)=(2\\pi)^{k}\\exp(-\\pi y^{2}+k\\log y)\\;.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The exponent in the above expression is maximized at $2\\pi(y^{\\ast})^{2}\\,=\\,k$ . This maximum is indeed achieved in the sum since we can choose $y^{\\ast}\\,=\\,\\ell\\gamma$ , which is permissible given the assumption $\\pi\\gamma^{2}\\in\\mathbb{N}$ . Hence, the maximum value of $f(y)$ is ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(y^{*})=\\exp(-k/2+(k/2)\\log(k/2\\pi))=(k/2e\\pi)^{k/2}\\;.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging in the value $c_{k}=1/\\sqrt{(2\\pi)^{k}k!}$ and using the Stirling lower bound $k!\\geq{\\sqrt{2\\pi k}}(k/e)^{k}$ for all $k\\in\\mathbb{N}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{x\\sim A_{\\gamma}}h_{k}(x)\\right|\\geq\\frac{2\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}c_{k}(2\\pi)^{k}(k/2e\\pi)^{k/2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{2\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}c_{k}(2\\pi k/e)^{k/2}}\\\\ &{\\qquad\\qquad=\\frac{2\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\cdot\\frac{(k/e)^{k/2}}{\\sqrt{k!}}}\\\\ &{\\qquad\\qquad\\geq\\frac{2\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\cdot\\frac{1}{(2\\pi k)^{1/4}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In addition, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\rho((1/\\gamma)\\mathbb{Z})=\\gamma\\rho(\\gamma\\mathbb{Z})\\leq\\gamma\\rho(\\mathbb{Z})\\leq2\\gamma\\;.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging in $k=2\\pi\\ell^{2}\\gamma^{2}$ , we therefore have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{x\\sim A_{\\gamma}}h_{k}(x)\\right|\\geq{\\frac{1}{\\sqrt{2\\pi\\ell\\gamma}}}~.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Corollary C.5 (Non-trivial Hermite coefficient, rounded degree). Let $\\left(h_{k}\\right)$ be the normalized Hermite polynomials with respect to $\\langle\\cdot,\\cdot\\rangle_{Q}$ . For any $\\gamma>1,\\,\\ell\\in\\mathbb{N}$ , and $k=2\\dot{\\lfloor\\ell^{2}\\pi\\gamma^{2}\\rfloor}$ , it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{x\\sim A_{\\gamma}}h_{k}(x)\\right|\\geq{\\frac{1}{e{\\sqrt{2\\pi\\ell\\gamma}}}}~,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $A_{\\gamma}$ is the discrete Gaussian on $(1/\\gamma)\\mathbb{Z}$ ", "page_idx": 26}, {"type": "text", "text": "Proof. Let $\\ell^{2}\\pi\\gamma^{2}-\\lfloor\\ell^{2}\\pi\\gamma^{2}\\rfloor=\\alpha$ . Then, $\\alpha\\in[0,1)$ and $k=\\ell^{2}(2\\pi\\gamma^{2})-2\\alpha$ . Similar to the proof of Lemma C.4, we apply the Poisson summation formula (Lemma 2.1) as follows. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{x\\sim A_{\\gamma}}h_{k}(x)=\\frac{1}{\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{x\\in(1/\\gamma)\\mathbb{Z}}h_{k}(x)\\rho(x)}}\\\\ &{=\\frac{\\gamma}{\\rho((1/\\gamma)\\mathbb{Z})}\\sum_{y\\in\\gamma\\mathbb{Z}}c_{k}(-2\\pi i y)^{k}\\rho(y)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As previously shown in Lemma C.4, the function $f(y)=y^{k}\\rho(y)$ achieves its maximum at $(y^{*})^{2}=$ $k/2\\bar{\\pi}=\\ell^{2}\\gamma^{\\frac{5}{2}}-\\alpha/\\pi$ . The issue now is that $y^{\\ast}$ is not necessarily contained in $\\gamma\\mathbb{Z}$ . However, we show that \u201crounding up\u201d $y^{\\ast}$ to $s\\gamma$ is sufficient to establish a non-trivial lower bound. Taking the ratio of $f(y^{\\ast})$ and $f(s\\gamma)$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log f(s\\gamma)/f(y^{*})=k\\log\\left(\\frac{\\ell\\gamma}{y^{*}}\\right)-\\pi(\\ell^{2}\\gamma^{2}-(y^{*})^{2})\\geq-\\alpha>-1\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, $f(s\\gamma)\\,>\\,f(y^{*})/e$ . Combining this observation and the proof of Lemma C.4 leads to the conclusion. ", "page_idx": 27}, {"type": "text", "text": "Theorem C.6 (Theorem 4.2 restated). For any constant $C>0$ , given $\\gamma(d)>1,\\sigma(d)>0$ such that $\\gamma\\sigma<C$ , estimation error parameter $\\eta>0$ , and $\\delta\\in(0,1)$ , there exists a brute-force search estimator $\\hat{\\pmb u}:\\mathbb R^{d\\times n}\\,\\rightarrow\\,\\mathbb S^{d-1}$ that uses $n\\,=\\,\\mathrm{poly}(d,\\gamma,1/\\eta,1/\\delta)$ samples and achieves $\\lVert{\\hat{\\mathbf{u}}}({\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{n})\\rVert_{\\infty}$ ${\\pmb u}\\|^{2}\\,\\leq\\,\\eta^{2}$ with probability at least $1-\\delta$ over i.i.d. samples $x_{1},\\ldots,x_{n}\\sim P_{u}$ , where $P_{u}$ is the $(\\gamma,\\sigma)$ -Gaussian pancakes distribution with secret direction $\\pmb{u}\\in\\mathbb{S}^{d-1}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Without loss of generality, we assume $\\eta\\le1/\\gamma$ . If the given error parameter $\\eta$ is larger than $1/\\gamma$ , we set $\\eta=1/\\gamma$ . Let $\\mathcal{C}$ be any $\\eta$ -net of $\\mathbb{S}^{d-1}$ . Our brute-force search estimator is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\pmb u}=\\operatorname*{arg\\,max}_{\\pmb v\\in\\mathcal{C}}\\hat{E}(\\pmb v)\\,,\\qquad\\mathrm{where~}\\hat{E}(\\pmb v)=\\frac{1}{n}\\sum_{i=1}^{n}T_{\\gamma}^{\\beta}(\\langle\\pmb x_{i},\\pmb v\\rangle)\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We choose $\\beta=1/\\sqrt{\\pi}\\gamma$ as the contrast function parameter for reasons explained later. The population limit of $\\hat{E}$ is $E$ (Eq.(24)), and the monotonicity of $E$ with respect to $\\langle\\boldsymbol{u},\\boldsymbol{v}\\rangle^{2}$ (Lemma C.3) implies that $\\pmb{u}=\\arg\\operatorname*{max}_{\\mathbb{S}^{d-1}}\\pmb{E}(\\pmb{v})$ in the infinite-sample limit. For $x\\sim P_{u}$ , the distribution of $\\langle\\boldsymbol{x},\\boldsymbol{v}\\rangle$ is $A_{\\gamma}^{\\xi}$ , where $\\xi^{2}=(1+\\sigma^{2})/\\langle{\\pmb u},{\\pmb v}\\rangle^{2}-1$ . Let $\\pmb{v}_{1},\\pmb{v}_{2}\\in\\mathbb{S}^{d-1}$ be such that $\\langle{\\pmb u},{\\pmb v}_{1}\\rangle^{2}-\\langle{\\pmb u},{\\pmb v}_{2}\\rangle^{2}=\\varepsilon^{2}$ Let $\\xi_{1}^{2}=(1+\\sigma^{2})/\\langle{\\pmb u},{\\pmb v}_{1}\\rangle^{2}-1$ and $\\xi_{2}^{2}=(1+\\sigma^{2})/\\langle{\\pmb u},{\\pmb v}_{2}\\rangle^{2}-1$ . Notice that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\xi_{1}^{2}}-\\frac{1}{1+\\xi_{2}^{2}}=\\frac{\\varepsilon^{2}}{1+\\sigma^{2}}\\ .\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $\\varepsilon$ -far pairs $\\pmb{v}_{1},\\pmb{v}_{2}$ such that $\\xi_{1}$ is sufficiently small, we establish a lower bound on $E(\\pmb{v}_{1})-E(\\pmb{v}_{2})$ in terms of $\\varepsilon$ . This implies that if $E(v_{1})$ and $E(v_{2})$ are close, then $\\langle\\pmb{u},\\pmb{v}_{1}\\rangle^{2}$ and $\\langle u,v_{2}\\rangle^{2}$ are also close. ", "page_idx": 27}, {"type": "text", "text": "By Claim C.2, for any $\\pmb{v}\\in\\mathbb{S}^{d-1}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nE(v)=\\langle T_{\\gamma}^{\\xi},T_{\\gamma}^{\\beta}\\rangle_{Q}=\\sum_{k=0}^{\\infty}\\alpha_{2k}^{2}\\cdot\\frac1{(1+\\beta^{2})^{k}(1+\\xi^{2})^{k}}\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since all the terms in the series are non-negative and monotonically decreasing in $\\xi$ , for any $k\\in\\mathbb{N}$ and $\\pmb{v}_{1},\\pmb{v}_{2}\\in\\mathbb{S}^{d-1}$ such that $\\xi_{1}\\leq\\xi_{2}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi(v_{1})-E(v_{2})\\geq\\cfrac{\\alpha_{2k}^{2}}{(1+\\beta^{2})^{k}}\\bigg(\\bigg(\\cfrac{1}{1+\\xi_{1}^{2}}\\bigg)^{k}-\\bigg(\\cfrac{1}{1+\\xi_{2}^{2}}\\bigg)^{k}\\bigg)}\\\\ &{\\qquad\\qquad=\\cfrac{\\alpha_{2k}^{2}}{(1+\\beta^{2})^{k}(1+\\xi_{1}^{2})^{k}}\\bigg(1-\\cfrac{1+\\xi_{1}^{2}}{1+\\xi_{2}^{2}}\\bigg)\\bigg(\\bigg(\\cfrac{1+\\xi_{1}^{2}}{1+\\xi_{2}^{2}}\\bigg)^{k-1}+\\bigg(\\cfrac{1+\\xi_{1}^{2}}{1+\\xi_{2}^{2}}\\bigg)^{k-2}+\\cdots+1\\bigg)}\\\\ &{\\qquad\\qquad\\geq\\cfrac{\\alpha_{2k}^{2}}{(1+\\beta^{2})^{k}(1+\\xi_{1}^{2})^{k}}\\bigg(\\cfrac{\\varepsilon^{2}(1+\\xi_{1}^{2})}{1+\\sigma^{2}}\\bigg)}\\\\ &{\\qquad\\qquad\\geq\\cfrac{\\alpha_{2k}^{2}}{(1+\\beta^{2})^{k}(1+\\xi_{1}^{2})^{k-1}}\\bigg(\\cfrac{\\varepsilon^{2}}{1+\\sigma^{2}}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We choose $k=\\lfloor\\pi\\gamma^{2}\\rfloor,\\beta^{2}=1/k$ . If $\\xi_{1}^{2}\\leq C^{\\prime}/k$ for some constant $C^{\\prime}>0$ (this will be justified later), then by Corollary C.5, we have that $\\alpha_{2k}^{2}\\stackrel{.}{\\geq}1/(2\\pi e^{2}\\gamma)$ . Thus, using the fact that $1+t\\le e^{t}$ for any $t\\in\\mathbb R$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\alpha_{2k}^{2}}{(1+\\beta^{2})^{k}(1+\\xi_{1}^{2})^{k-1}}\\bigg(\\frac{\\varepsilon^{2}}{1+\\sigma^{2}}\\bigg)\\geq\\frac{\\alpha_{2k}^{2}}{e^{C^{\\prime}+1}}\\bigg(\\frac{\\varepsilon^{2}}{1+\\sigma^{2}}\\bigg)\\geq\\frac{1}{2\\pi e^{C^{\\prime}+3}\\gamma}\\bigg(\\frac{\\varepsilon^{2}}{1+\\sigma^{2}}\\bigg)\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\sigma^{2}\\leq C^{2}/\\gamma^{2}<C^{2}$ , it follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\nE(v_{1})-E(v_{2})>\\frac{1}{2\\pi(1+C^{2})e^{C^{\\prime}+3}}\\cdot\\frac{\\varepsilon^{2}}{\\gamma}\\ .\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Taking the contrapositive, if $\\pmb{v}_{1},\\pmb{v}_{2}\\in\\mathbb{S}^{d-1}$ are such that $\\xi_{1}\\le\\xi_{2},\\xi_{1}^{2}\\le C^{\\prime}/k$ and $E(\\pmb{v}_{1})-E(\\pmb{v}_{2})\\leq$ $\\varepsilon^{2}/(2\\pi(1+C^{2})e^{C^{\\prime}+3}\\gamma)$ , then $\\langle\\boldsymbol{u},\\boldsymbol{v}_{1}\\rangle^{2}-\\langle\\boldsymbol{u},\\boldsymbol{v}_{2}\\rangle^{2}\\leq\\varepsilon^{2}$ . ", "page_idx": 28}, {"type": "text", "text": "Equipped with this result, we revisit our $\\eta$ -net $\\mathcal{C}$ of $\\mathbb{S}^{d-1}$ . Let $\\pmb{v}^{*}\\;=\\;\\arg\\operatorname*{max}_{\\pmb{v}\\in\\mathcal{C}}E(\\pmb{v})$ be the population maximizer of $E$ within $\\mathcal{C}$ . By the monotonicity of $E(v)$ with respect to $\\langle\\boldsymbol{u},\\boldsymbol{v}\\rangle^{2}$ , we have that $1-\\langle\\boldsymbol{u},\\boldsymbol{v}^{*}\\rangle^{2}\\leq\\eta^{2}/2$ . Moreover, since by assumption $\\sigma^{2}\\le C^{2}/\\gamma^{2}$ and $\\eta^{2}\\leq1/\\dot{\\gamma}^{2}\\leq1$ , the corresponding noise level $(\\xi^{*})^{2}:=(1+\\sigma^{2})/\\bar{\\langle{\\boldsymbol u},{\\boldsymbol v}^{*}\\rangle^{2}}^{\\star}-1$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n(\\xi^{*})^{2}\\leq\\frac{1+\\sigma^{2}}{1-\\eta^{2}/2}-1=\\frac{\\sigma^{2}+\\eta^{2}/2}{1-\\eta^{2}/2}\\leq2\\sigma^{2}+\\eta^{2}\\leq(2C^{2}+1)/\\gamma^{2}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now consider $\\hat{\\pmb u}=\\arg\\operatorname*{max}_{\\pmb v\\in\\mathcal C}\\hat{E}(\\pmb v)$ , the maximizer of the empirical objective. By the triangle inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E(v^{*})-E(\\hat{u})=(E(v^{*})-\\hat{E}(v^{*}))+(\\hat{E}(v^{*})-\\hat{E}(\\hat{u}))+(\\hat{E}(\\hat{u})-E(\\hat{u}))}\\\\ &{\\qquad\\qquad\\qquad\\leq|E(v^{*})-\\hat{E}(v^{*})|+|\\hat{E}(\\hat{u})-E(\\hat{u})|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We show that both terms in Eq.(28) concentrate. Recall that $\\begin{array}{r}{\\hat{E}(\\pmb{v})=(1/n)\\sum_{i=1}^{n}T_{\\gamma}^{\\beta}(\\langle\\pmb{v},\\pmb{x}_{i}\\rangle)}\\end{array}$ and that the function $T_{\\gamma}^{\\beta}$ satisfies (see proof of Lemma B.11) ", "page_idx": 28}, {"type": "equation", "text": "$$\nT_{\\gamma}^{\\beta}(z)\\leq T_{\\gamma}^{\\beta}(0)=\\frac{\\sqrt{1+\\beta^{2}}}{\\beta\\rho((1/\\gamma)\\mathbb{Z})}\\cdot\\rho((\\sqrt{1+\\beta^{2}}/\\beta\\gamma)\\mathbb{Z})\\leq\\sqrt{1+\\beta^{2}}/\\beta\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\beta^{2}=1/(\\pi\\gamma^{2})$ , we have $T_{\\gamma}^{\\beta}(z)\\leq\\gamma\\sqrt{2\\pi}$ for any $z\\in\\mathbb{R}$ . Thus, for any fixed $\\pmb{v}\\in\\mathbb{S}^{d-1}$ , $\\hat{E}(v)$ is a sum of i.i.d. bounded random variables $T_{\\gamma}^{\\beta}$ . By Hoeffding\u2019s inequality, $n\\lesssim(\\gamma^{3}/\\eta^{4})\\log(|\\mathcal{C}|/\\delta)$ samples are sufficient to guarantee that for all $\\pmb{v}\\in\\mathcal{C}$ , $|E(\\pmb{v})-\\hat{E}(\\pmb{v})|\\,\\lesssim\\,\\eta^{2}/\\gamma$ with probability at least $1\\,-\\,\\delta$ . From standard covering number bounds (e.g., [77, Corollary 4.2.13]), we have $|{\\mathcal{C}}|\\leq(3/\\eta)^{d}$ . Hence, $n\\lesssim(d\\gamma^{3}/\\eta^{4})(\\breve{\\log}(1/\\eta)+\\log(1/\\delta))$ samples are sufficient for the desired level of concentration. ", "page_idx": 28}, {"type": "text", "text": "It follows that $E(\\pmb{v}^{*})_{\\diamond}-E(\\hat{\\pmb{u}})\\lesssim\\eta^{2}/\\gamma$ with probability $1-\\delta$ over the randomness of $\\hat{\\pmb u}$ . Since $\\pmb{v}^{*}$ satisfies $(\\xi^{*})^{2}\\lesssim1/\\dot{\\gamma}^{2}$ (see Eq.(27)) and $\\xi^{*}\\leq\\xi$ , where we denote $\\xi^{2}=(1+\\sigma^{2})/\\langle{\\pmb u},{\\hat{\\pmb u}}\\rangle^{2}-1$ , the closeness of $E(v^{*})$ and $E(\\hat{\\boldsymbol{u}})$ implies $\\langle u,v^{*}\\rangle^{2}-\\langle u,\\hat{u}\\rangle^{2}\\leq\\eta^{2}$ due to Eq.(26). Hence, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lVert\\boldsymbol{u}-\\hat{\\boldsymbol{u}}\\rVert^{2}=2(1-\\langle\\boldsymbol{u},\\hat{\\boldsymbol{u}}\\rangle^{2})\\leq2\\eta^{2}+2(1-\\langle\\boldsymbol{u},\\boldsymbol{v}^{*}\\rangle^{2})\\leq3\\eta^{2}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See our main result presented in Section 3. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: In Section 1.3, we mention that our hardness result can be circumvented if we aim for learning not in statistical distance, but against bounded discriminators. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Full theorem statements and proofs are provided in Sections B and C. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This is a purely theoretical paper with no experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a purely theoretical paper with no experiments. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a purely theoretical paper with no experiments. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a purely theoretical paper with no experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This is a purely theoretical paper with no experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We present purely theoretical results. No human subjects or data were involved in this work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We present purely theoretical results on fundamental limitations of machine learning, so we do not see any potential for having societal impacts. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work does not involve any release of data or models. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not use any assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper does not release any new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]