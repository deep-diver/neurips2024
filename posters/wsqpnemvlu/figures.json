[{"figure_path": "wSqpNeMVLU/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Standard Auto-Regressive Decoding (Algorithm 3) v.s. Right: Speculative Decoding (Algorithm 1), where a large model is used to validate the responses of the small model.", "description": "This figure compares the standard autoregressive decoding method with the proposed speculative decoding method. In the standard method (left), the large language model (LLM) generates tokens one by one in an autoregressive manner.  In the speculative decoding method (right), a smaller, faster draft model generates a sequence of tokens, which are then validated by the LLM.  If a token is rejected by the LLM, the draft model generates a new sequence from the last accepted token. This process speeds up inference, as the smaller model is much faster than the LLM.", "section": "1 Introduction"}, {"figure_path": "wSqpNeMVLU/figures/figures_5_1.jpg", "caption": "Figure 2: The numeric instance in this figure chooses p, q to be nonstationary Markov Chains with horizon T = 50. Left (a): A simulation of Speculative Decoding. The green line is the empirical average rejections among 100N runs and the orange line the theoretical value computed via Theorem 1. Middle (b): Batch Speculative Decoding simulations with batch M = 4,5. The green/purple lines are the empirical average rejections among 100N runs and the orange/pink lines are the theoretical values computed via Theorem 3. Right (c): The scaling law of expected rejections for Batch SD as a function of M. It converges to a limit as M \u2192 \u221e.", "description": "This figure displays simulation results that compare empirical and theoretical results of speculative decoding and batch speculative decoding.  The left panel shows a simulation of standard speculative decoding, demonstrating the convergence of empirical average rejections to the theoretical value predicted by Theorem 1. The middle panel presents simulations of batch speculative decoding with different batch sizes (M=4 and M=5), again comparing empirical and theoretical results (this time using Theorem 3). Finally, the right panel illustrates the scaling law of expected rejections for batch speculative decoding as a function of batch size (M), showing the convergence to a limit as M increases.", "section": "Analysis on Efficiency and Optimality for Speculative Decoding"}, {"figure_path": "wSqpNeMVLU/figures/figures_6_1.jpg", "caption": "Figure 3: Left: Batch Speculative Decoding. Right: Batch Improvement vs. Batch size M. Upper: Bernoulli distributions with q = Ber(0.5). Lower: p ~ Unif(V), q ~ Unif(V') with r = V/V'.", "description": "The left panel shows the process of Batch Speculative Decoding.  Multiple draft sequences are generated in parallel, and the large model verifies them. The right panel displays two graphs showing the relationship between batch improvement and batch size (M) for two different scenarios. The upper graph uses Bernoulli distributions for p and q, while the lower graph uses uniform distributions.  Both graphs illustrate that the benefit of increased batch size diminishes as M increases.", "section": "3.2 Analysis for Batch Speculative Decoding"}, {"figure_path": "wSqpNeMVLU/figures/figures_7_1.jpg", "caption": "Figure 4: Left (a): The Pareto Front between Rejection Probability PA(reject) vs. Distribution bias TV[PA, q]. For a given rejection probability, the black line denotes the optimal deviation Lossy. Middle (b) and Right (c): A numeric example. In the plot, the over acceptance e's are set as positive constants that define b(x) = min{1, q(x)+e}.", "description": "This figure shows the Pareto front between rejection probability and distribution bias. The left panel shows the Pareto optimal tradeoff between these two metrics. The middle and right panels show a numerical example illustrating this tradeoff, where the over-acceptance threshold is varied. The black line in the left panel represents the optimal deviation given by Theorem 4, showing the minimum distribution bias for a given rejection probability. The unattainable region is the area below the Pareto front, indicating that no algorithm can achieve better performance than what is indicated by the Pareto front.", "section": "Analysis on the Optimal Rejection-Distribution Bias Tradeoff"}, {"figure_path": "wSqpNeMVLU/figures/figures_27_1.jpg", "caption": "Figure 2: The numeric instance in this figure chooses p, q to be nonstationary Markov Chains with horizon T = 50. Left (a): A simulation of Speculative Decoding. The green line is the empirical average rejections among 100N runs and the orange line the theoretical value computed via Theorem 1. Middle (b): Batch Speculative Decoding simulations with batch M = 4,5. The green/purple lines are the empirical average rejections among 100N runs and the orange/pink lines are the theoretical values computed via Theorem 3. Right (c): The scaling law of expected rejections for Batch SD as a function of M. It converges to a limit as M \u2192 \u221e.", "description": "This figure presents simulation results for Speculative Decoding and its batch version.  The left panel shows a comparison of empirical and theoretical expected rejections for standard speculative decoding. The middle panel shows similar comparisons for the batch version with different batch sizes (M=4 and M=5). The right panel illustrates how the expected number of rejections scales with increasing batch size (M), demonstrating convergence to a limit.", "section": "3 Analysis on Efficiency and Optimality for Speculative Decoding"}]