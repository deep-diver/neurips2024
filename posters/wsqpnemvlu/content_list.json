[{"type": "text", "text": "A Theoretical Perspective for Speculative Decoding Algorithm ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ming Yin\u02da Princeton University my0049@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Minshuo Chen Northwestern University minshuo.chen@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Kaixuan Huang Princeton University kaixuanh@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Mengdi Wang Princeton University mengdiw@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer-based autoregressive sampling has been the major bottleneck for slowing down large language model inferences. One effective way to accelerate inference is Speculative Decoding, which employs a small model to sample a sequence of draft tokens and a large model to validate. Given its empirical effectiveness, the theoretical understanding of Speculative Decoding is falling behind. This paper tackles this gap by conceptualizing the decoding problem via markov chain abstraction and studying the key properties, output quality and inference acceleration, from a theoretical perspective. Our analysis covers the theoretical limits of speculative decoding, batch algorithms, and output quality-inference acceleration tradeoffs. Our results reveal the fundamental connections between different components of LLMs via total variation distances and show how they jointly affect the efficiency of decoding algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent surge of scaling Transformer models has led to the flourishing of AI, where success has been witnessed in wide areas such as natural language [39, 1], computer vision [12, 17], video generations [3, 19], and robotics [8, 31]. In the meantime, the decoding process also becomes more and more time-consuming as the model size scales up. This is mainly due to the autoregressive nature of Transformers, where each generated token also serves as the input for future generations. As a result, decoding $T$ tokens would take $T$ forward passes of the full model. ", "page_idx": 0}, {"type": "text", "text": "A recent effort to tackle this challenge is speculative decoding (SD) [10, 24], where the autoregressive sampling is performed on a small draft model and the large language model verifies tokens generated by draft model to decide whether it should be accepted/rejected. Once a token is rejected, the generation process will start from the most recently accepted token, until a full response is completed. Speculative decoding achieves $2{-}2.5\\times$ LLM inference speedup empirically, while preserving the quality of generation. ", "page_idx": 0}, {"type": "text", "text": "Subsequently, numerous studies [26, 25, 23, 46] have expanded this methodology, enabling further inference acceleration. Intuitively, for speculative decoding, when the generation distribution of small model $p$ and large model $q$ are close to each other, decoding is faster (since less rejection occurs), and when the distribution overlap between $p$ and $q$ is small, the opposite happens. However, a precise understanding of inference accelerating given the small model $p$ and large model $q$ remains elusive. This motivates us to ask the following question: ", "page_idx": 0}, {"type": "text", "text": "What is the fundamental limit for inference acceleration via speculative decoding? In addition, what is the best trade-off between inference acceleration and output quality for speculative decoding? ", "page_idx": 1}, {"type": "image", "img_path": "wSqpNeMVLU/tmp/bfd8ce42af1da473064dbf2a39cb580954281fe82a39aa5a1dfc869d5d2faf85.jpg", "img_caption": ["Figure 1: Left: Standard Auto-Regressive Decoding (Algorithm 3) v.s. Right: Speculative Decoding (Algorithm 1), where a large model is used to validate the responses of the small model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In this paper, we answer these questions from the theoretical lens. Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "We formalize the decoding problem through the Markov Chain abstraction that establishes the theoretical setup. We draw the connection runtime $=\\#$ rejections and use it to measure efficiency. We derive the exact formula, fully characterized by distribution $p$ and $q$ , for the expected rejections $\\mathbb{E}[N_{\\mathrm{rej}}]$ for Speculative Decoding (Theorem 1). This renders a theoretical reference for understanding the acceleration rate $T/\\mathbb{E}[N_{\\mathrm{rej}}]$ . ", "page_idx": 1}, {"type": "text", "text": "Next, to understand whether Speculative Decoding can be further improved, we generalize it to a class of rejection-based algorithms 2 where probability $b_{t}$ and distribution $\\mathcal{P}_{t}$ can be customized. We prove in Theorem 2 that any unbiased algorithm cannot have fewer rejections than Speculative Decoding. This indicates its optimality among the class, and having fewer rejections needs to suffer quality loss or requires extra information. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we consider a batch version of Speculative Decoding (Algorithm 4) that utilizes multiple draft sequences. We show our batch algorithm is unbiased. We derive the expected rejections that is fully expressed by $p$ and $q$ and exhibit the improvement over non-batch version in Theorem 3. We provide examples and detailed discussion to explain how our theory characterize the improvement. ", "page_idx": 1}, {"type": "text", "text": "In section 4, we shift from unbiased algorithms and study the tradeoff between inference cost and quality degradation. We formulate this into an optimization model (1). Theorem 5 established a linear Pareto front that characterizes the tradeoff between inference cost and quality degradation (Figure 4). A simple experiment in Section 4.2 is also consistent with our theoretical finding. ", "page_idx": 1}, {"type": "text", "text": "Last but not least, our technical results involve novel analysis, for instance, the design of $\\mathcal{V}_{+},\\mathcal{V}_{-}$ in the lower bound proof C and the iterative computation for $f$ in D.1. They are the first of its kind and consist of our technical contributions. We provide a proof sketch section in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Speculative Decoding and its applications. Speculative execution, where performing speculative work can expedite computation on parallel machines by allowing certain tasks to commence before their necessity is confirmed, can date back to [9, 15]. Recently, [10, 24] formalize this idea with rejection sampling based design for LLM Decoding and achieve multiple-time inference acceleration compared to vanilla auto-regressive decoding. There are fruitful studies [41, 26, 25, 37, 23, 46, 32, 27, 18, 4, 34, 2, 30, 43, 11, 42, 45, 29, 35, 5, 44, 40, 20] since then, and they improve speculative decoding from different angles such as online updating [25], multiple candidates [45], retrieval technique [18], Multimodality [16] or even decoding without draft models [14, 6]. ", "page_idx": 1}, {"type": "text", "text": "Theoretical endeavor for Speculative Decoding. There are also works that study the theoretical properties for speculative decoding (SD). In particular, [37] considers speculative decoding from the optimal transport perspective and show it is optimal in the single token regime. It further extends to the $k$ multiple draft token setting and formulates the optimal transport solution via linear programming with exponential in $k$ computation time. An approximate sequential selection algorithm is also proposed with linear time. [2] further proposes the improved plan, and [36] extends [37] to the block-level optimal transport for SD. [34] investigates the synergy between draft length and batch size for Speculative Decoding and formulate the optimal speculation length as the root of a polynomial equation. [26] proposes the SpecInfer, a batch algorithm that uses small speculative models to form the token tree, and proves its output matches the distribution of the large model. [45] considers batch speculative decoding without replacement to avoid repeatedly sampling rejected tokens and proves it keeps the decoding quality. Nevertheless, the findings for inference acceleration of these works are mostly empirical, lacking theoretical guarantees. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "table", "img_path": "wSqpNeMVLU/tmp/6ad47ab0a708331fde902230230c4e319621417218f94d115f2c4c7deb53d991.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Background for decoding problems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we provide the mathematical formulation for decoding problems using Markov Chains, and we explain auto-regressive models and speculative decoding algorithm based upon that. ", "page_idx": 2}, {"type": "text", "text": "A Markov Chain Model for Decoding. We denote $x_{0}$ as the prompt.2 $x_{n}$ is the $n$ -th token output and $x_{1:T}$ is the trajectory output by the algorithm with $T$ to be the fixed decoding horizon.3. Then any decoding algorithm can be characterized by a Markov Chain: state at $t$ is described by history $x_{0:t}$ , the initial state is $x_{0}$ ; the state transition $P_{t}$ maps state $x_{0:t}$ to state $x_{0:t+1}$ . In the context of decoding problem, the transition matrix is defined as $P(x_{0:t+1}|x_{0:t}):=p(x_{t+1}|x_{1:t})$ . In particular, we use $p$ to denote the (conditional) distribution for the draft model that resembles small speculative model, and $q$ for the target model that represents large language model. We use $[q-p]_{+}$ to denote the normalized distribution for $\\operatorname*{max}\\{0,q(x)-p(x)\\}$ , $\\forall x\\in\\mathcal{V}$ with $\\mathcal{V}$ being the token vocabulary. We also denote $\\begin{array}{r}{\\bar{\\mathbb{E}}_{x\\sim f}[g]:=\\sum_{x}f(x)g(\\dot{x})}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Auto-Regressive Deco ding. Consider sampling a trajectory $x_{1:T}$ from an auto-regressive model, where for the given $x_{1:n-1}$ , the next token $x_{n}$ follows the conditional distribution $q$ . This decoding mechanism (Algorithm 3) is the prototype for Transformer-based LLMs (e.g. GPT-4). As mentioned in [29], the accuracy performance of Transformer-based LLMs has been shown to scale with model size, with larger models demonstrating improved capabilities [22]. However, this improvement comes at the cost of higher latency during inference and increased computational requirements. ", "page_idx": 2}, {"type": "text", "text": "Speculative Decoding. Different from large models, small models are usually much faster at the inference stage. Consequently, we can use a small model to perform auto-regressive sampling and assign large model as a verifier, where the goal of the large model is to check whether the token sampled by the small model should be accepted/rejected. Concretely, this procedure can be summarized into the following three steps: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "\u2022 Draft sampling: given the verified tokens $x_{1:n-1}$ , the draft model obtains $K$ sequential candidate tokens $\\tilde{x}_{n:n+K-1}$ via sampling from $p(\\cdot|x_{1:n-1},\\tilde{x}_{n:n+i})$ , $i\\in[K-1]$ ;   \n\u2022 Conditional score computation: given $\\tilde{x}_{n:n+K-1}$ , computing the logits of the $K$ tokens $q(\\cdot|x_{1:n-1},\\tilde{x}_{n:n+i})$ , $i\\in[K-1]$ in parallel;   \n\u2022 Token validation: Accept the candidate token $\\tilde{x}_{t}$ (as $x_{n}$ ) with some probability $0\\leqslant b_{t}\\leqslant1$ . If accepted, continue to validate the next candidate $\\tilde{x}_{t+1}$ , otherwise reject and sample $x_{n}$ from some designed distribution $\\mathcal{P}_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "The above process repeats until $x_{1:T}$ are generated, and the whole algorithm is summarized as the general rejection-based decoding 2. Specifically, Speculative Decoding [10, 24] designs $b_{t}(\\tilde{x}_{t}):=$ $\\begin{array}{r}{\\operatorname*{min}\\{1,\\frac{q_{t}\\left(\\tilde{x}_{t}\\right)}{p_{t}\\left(\\tilde{x}_{t}\\right)}\\}}\\end{array}$ 1, pqttppx\u02dcx\u02dcttqqu and distribution Ptp\u00a8q :\u201c rqt \u00b4 pts\\`p\u00a8q (see Algorithm 1 and Figure 1). ", "page_idx": 3}, {"type": "text", "text": "2.2 Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Speculative Decoding has two key merits. First, it maintains output quality, meaning the output distribution by the algorithm is identical to the output distribution of the large model $q$ , and we term it distribution unbiasedness. Second, it has fast inference property since the auto-regressive sampling is performed on the small model and the target model only verifies. With (possibly) multiple draft tokens being accepted very round, Speculative Decoding can be much faster than direct decoding on large models, and its inference is bottlenecked by the parallel score computation $q(\\cdot|x_{1:n-1},\\tilde{x}_{n:n+i})$ , $i\\in[T-n]$ . To highlight this, we defined it as the oracle call and make an assumption based on that. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. We define one trigger of obtaining logits for $\\tilde{x}_{n:T}$ in Step 7 of 2 as one Oracle call. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. We assume that: $(I)$ compared to the large model, the computational cost of the draft/small model is negligible. (2) each oracle call has runtime $O(1)$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. We assume the above only for the theoretical cleanliness. With negligible draft model, the lookhead $K=T$ in Algorithm 2. In other words, given $x_{1:n-1}$ , instead of sampling the next $K$ draft tokens $\\tilde{x}_{n:n+K-1}$ , we are allowed to sample until the end, i.e. $\\tilde{x}_{n:T}$ . In practice, Assumption $I(I)$ also holds true in many cases. One example of negligible-cost model $c\\approx0$ is n-gram models, and the empirical evidence in [24, 28] shows n-gram draft model speeds up inference pretty well. In addition, for summarization tasks where long sequences are likely to repeat, any draft model that reuses tokens from the context with a matching prefix, is also cost negligible. Assumption 1(2) is also a standard abstraction, since parallelization consumes (roughly) equal computation as for a single logit. It will consume more memory, but such aspect is beyond the scope of our theoretical study. ", "page_idx": 3}, {"type": "text", "text": "Metric for measuring efficiency. Desired algorithms should preserve the output quality and be efficient for decoding. To formalize, our theory aims to study the following two quantities: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Inference acceleration: the ratio between the inference time of decoding large model $q$ and the inference time of decoding the algorithm; \u2022 Output quality: the distribution bias between the algorithm and the large model distribution $q$ . An algorithm maintains the output quality if it is distribution unbiased. ", "page_idx": 3}, {"type": "text", "text": "Rejections, and why consider it? A third metric for decoding is the number of draft tokens being rejected. With more draft tokens being accepted, the fewer rejections would occur. Therefore, algorithms with large number of rejections are slower than those with fewer rejections. This means Rejections serves as an alternative metric for the inference speedups. Throughout the paper, we use number of rejections for measuring inference acceleration. ", "page_idx": 3}, {"type": "text", "text": "To further motivate why choosing Rejections is appropriate, we draw its connection to inference runtime. For Speculative Decoding, the runtime is dominated by the number of oracle calls (Definition 1). After each rejection (Step 10 of 2 or Step 7 of 1), there is one oracle call, thus we obtain ", "page_idx": 3}, {"type": "text", "text": "Notice the runtime for auto-regressive decoding (3) is $T$ , therefore we have the relation: inference acceleration $=\\;T/$ # rejections. Based on this setup, we present our main results in next sections. ", "page_idx": 3}, {"type": "text", "text": "3 Analysis on Efficiency and Optimality for Speculative Decoding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start with the following theorem at the beginning of the section. It covers output quality, which is measured by distribution bias, and expected number of rejections for speculative decoding. Its proof is deferred to Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. We have the following two results for Speculative Decoding. ", "page_idx": 4}, {"type": "text", "text": "$(I)$ We define random variables $R_{n}\\in\\{0,1\\}$ that indicates whether the n-th token is rejected (with 1 being rejected). Here rejection means Line 7 of Algorithm 1 is executed. Then, the total number of rejections $\\begin{array}{r}{N_{r e j}=\\sum_{n=1}^{T}R_{n}}\\end{array}$ . For Speculative Decoding (here TV denote the TV distance): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[N_{r e j}]=\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q}[\\mathsf{T V}(p_{n}(\\cdot|x_{1:n-1}),q_{n}(\\cdot|x_{1:n-1}))].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(2) The output distributions of Algorithm 1 and the target model q are identical, i.e. for any output sequence $\\dot{x}_{1:T}\\in\\mathcal{V}^{T}$ , the joint the distributions over $x_{1:T}$ satisfies: $\\mathbb{P}^{S D}(x_{1:T})=q(x_{1:T})$ . ", "page_idx": 4}, {"type": "text", "text": "The first part of Theorem 1, to our knowledge, is the first result that characterizes the expected rejection for speculative decoding a sequence of length $T$ using $p$ and $q$ . The second part of Theorem 1 shows the distribution unbiasedness for SD, which has been presented in [10, 24]. There are three interesting indications: ", "page_idx": 4}, {"type": "text", "text": "\u2022 If $\\mathbb{E}[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})]\\,=\\,0$ for all $n$ , all tokens are accepted and the accelerate rate $=T$ ;   \n\u2022 If $\\mathbb{E}[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})]=1$ for all $n$ , all tokens are rejected and the accelerate rate $=1$ , i.e. all $T$ tokens are sampled from the large model $q$ ;   \n\u2022 In general, the accelerate rate for SD is $T/\\sum_{n=1}^{T}\\mathbb{E}[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})].$ ", "page_idx": 4}, {"type": "text", "text": "Remark 2. $I24J$ derive the expected number of  token generated per run of Speculative Decoding as $1/\\mathbb{E}[\\mathsf{T V}(p,q)]$ for $K\\,=\\,\\infty$ .4 Their result equals $T/\\sum_{n}^{T}\\mathbb{\\bar{E}}[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})]$ when $\\mathbb{E}[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})]$ is identical for all $n$ , and this is due to th ei\u0159r assumption that the acceptance rates $\\beta$ are i.i.d.. In contrast, our guarantee holds for the case that allows the sequential dependence between different decoding steps. ", "page_idx": 4}, {"type": "text", "text": "Simulation. We also provide a simulation of Speculative Decoding and compare it with our Theorem 1 in the left panel of Figure 2(a) with horizon $T=50$ , $p_{n},q_{n},n=1,\\ldots,50$ are nonstationary Markov Chains. The green line is the empirical average rejections among $100\\,\\times\\,N$ runs of Algorithm 1 and the orange line the theoretical value computed via Theorem 1. From the simulation, after 5000 runs the empirical average rejections converge to our theoretical value 16.41. In this example, the acceleration rate is $50/16.41=3.05$ . The specifications of the simulation is included in Appendix F. ", "page_idx": 4}, {"type": "text", "text": "3.1 Optimality of Speculative Decoding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we have analyzed the efficiency of speculated decoding and provided mathematical characterization of the number of expected rejections, which is depicted by the TV distance and scales linearly with the inference time. A natural next-step question to ask is: Is there any other rejection-based algorithm 2 that can do better? We answer this question in the next theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Instance-dependent Rejection Lower Bound). For an instance $\\mathcal{P}:=(p,q)$ , where $p,q$ stand for the distributions of the draft model and the target model respectively, defining the family of algorithms as $\\mathcal{F}:=\\;\\{\\mathcal{A}:\\mathcal{A}$ is a specification of Algorithm 2 that satisfies $\\bar{\\mathbb{P}}_{t}^{A}\\,=$ $q_{t}\\not\\in{\\sf{H}}$ pi.e., unbiasedqu. For an algorithm $\\boldsymbol{\\mathcal{A}}$ , denote $N_{r e j}$ as the number of rejections. Then we have the lower bound ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in\\mathcal{F}}\\mathbb{E}_{\\mathcal{P}}^{A}\\left[N_{r e j}\\right]\\geqslant\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q}\\left[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4This is from their equation (1) that has $1/(1-\\alpha)$ with $\\alpha\\,=\\,\\mathbb{E}(\\operatorname*{min}(p,q))$ and $1\\,-\\,\\mathbb{E}(\\operatorname*{min}(p,q))\\,=$ $\\mathbb{E}[\\mathsf{T V}(p,q)]$ . ", "page_idx": 4}, {"type": "image", "img_path": "wSqpNeMVLU/tmp/fcc6ed59b484f82005e71098bed9629da72849e0ba20f837ff666dd56c60e418.jpg", "img_caption": ["Figure 2: The numeric instance in this figure chooses $p,q$ to be nonstationary Markov Chains with horizon $T\\,=\\,50$ . Left $(a)$ : A simulation of Speculative Decoding. The green line is the empirical average rejections among $100N$ runs and the orange line the theoretical value computed via Theorem 1. Middle $(b)$ : Batch Speculative Decoding simulations with batch $M\\,=\\,4,5$ . The green/purple lines are the empirical average rejections among $100N$ runs and the orange/pink lines are the theoretical values computed via Theorem 3. Right $(c)$ : The scaling law of expected rejections for Batch SD as a function of $M$ . It converges to a limit as $M\\to\\infty$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Takeaways. Via Theorem 2, the answer to the key question is: No rejection improvement can be made in Algorithm 2 by changing the acceptance probability $b_{t}$ and the distribution $\\mathcal{P}_{t}$ if we want to keep the distribution unbiasedness. We point out that lower bound of Theorem 2 matches the complexity upper bound of Speculative Decoding given by Theorem 1. This result confirms that speculative decoding is optimal in the class of all rejection-based methods. ", "page_idx": 5}, {"type": "text", "text": "The practical implication is that there is no need to tweak acceptance probability, as it will not make performance better. In the next Section 3.2, we will see that Speculative Decoding can be provably improved, as long as one can decode and verify multiple sequence of tokens in parallel. ", "page_idx": 5}, {"type": "text", "text": "Connection to optimal transport. We mention [37] nicely consider maximizing the acceptance probability from the optimal transport perspective. For a single token, the optimal transport cost is $\\textstyle\\sum_{x}\\operatorname*{min}(p(x),q(x))$ , which corresponds to the optimal expected rejection $\\mathsf{T V}(p,q)$ . However, fo r \u0159the sequential $T$ tokens, their Section 5,6 does not provide a explicit formulation for optimal acceptance/rejections. In this sense, their optimality result can be cast as a special case of ours. However, we do emphasis there are differences in the settings, where [37] consider the optimal transport and we study the class $\\mathcal{F}$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Analysis for Batch Speculative Decoding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To further improve the provable efficiency, we consider batch algorithms that extend the speculative decoding with multiple candidate draft sequences. Most of the existing works [32, 34, 26, 45, 21] formulates batch sequences via a speculation tree structure. In particular, the representative work [26] propose the merged token tree structure that combines sequences with the same prefix. A depth-first search is designed for speculation to ensure the unbiasedness of the algorithm. In addition, [33] devise the parallel decoding structure that speculate the token of each responses given its previous tokens are accepted. Motivated by these works, we consider a batch version of speculative decoding algorithm using a simpler parallel structure (Left of Figure 3). Our Algorithm 4 can be viewed as a simplified approximation to those batch algorithms. There are several differences that distinguish batch algorithm from the non-batch version. We highlighting them as follows. ", "page_idx": 5}, {"type": "text", "text": "Difference1: Oracle call. At the beginning of batch speculation, $M$ draft sequences are generated in parallel as well as the corresponding logits $q$ . This corresponds to Step 4-9 of $\\mathrm{Alg\\4}$ and is defined as one oracle call which we assume to have unit computation $O(1)$ ;5 ", "page_idx": 5}, {"type": "text", "text": "Difference2: Speculation procedure. It follows the $D F S$ principle: If the first token of a response is accepted, only that response will be speculated until rejection. For instance in the Left panel of Figure 3, if the token \u2018deep\u2019 is accepted, the algorithm will keep verifying token \u2018learn\u2019, \u2018ing\u2019 until rejection and the rest of sequences won\u2019t be examined; if \u2018deep\u2019 is not verified, then the algorithm will keep examing \u2018rein\u2019. In this case, rejection happens only if \u2018deep\u2019, \u2018rein\u2019 and \u2018atten\u2019 are all rejected. Once rejection happens, the process will restart. By this design, it still holds true that: inference time $=\\#$ oracle calls $=\\#$ rejections. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Again, we measure the output quality and rejections for the batch algorithm. The following main result (Theorem 3) provides the explicit characterization for rejections and batch improvement. Detailed proofs and discussions can be found in D. ", "page_idx": 6}, {"type": "image", "img_path": "wSqpNeMVLU/tmp/229fc6e10668b1778b1cd7a44f740f4a4928a1f51dedebc17d28fbd8d5ce9256.jpg", "img_caption": ["Figure 3: Left: Batch Speculative Decoding. Right: Batch Improvement vs. Batch size $M$ . Upper: Bernoulli distributions with $q=B e r(0.5)$ . Lower: $p\\sim\\mathrm{Unif}(\\bar{V})$ , $q\\sim\\operatorname{Unif}(V^{\\prime})$ with $r=V/V^{\\prime}$ . ", "where $f(x_{1:n}):=\\mathbb{P}(x_{1:n}\\cap\\{n$ -th draft token rejecteduq. $f$ can be iteratively computed via $p,q$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Unbiasedness and efficiency of batch SD). Recall the definition of $R_{n}$ and $N_{r e j}$ in Thm 1, and iteratively define: $q^{m+1}=[q^{m}\\!-\\!p]_{+}$ , $\\forall m\\in[M]$ with $q^{1}=q$ being the target distribution. Then, for Batch Speculative Decoding 4, $\\mathbb{P}^{B a t c h}(x_{1:T})=q(x_{1:T}).\\;\\forall x_{1:T}\\in$ $\\forall x_{1:T}\\in\\nu^{T}$ . Moreover, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[N_{r\\beta}]=\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q}[\\mathrm{TV}[q,p](\\cdot|x_{1:n-1})]-\\sum_{n=1}^{T}\\bar{\\mathbb{E}}_{x_{1:n-1}\\sim f}[\\mathrm{TV}(q,p)(x_{1:n-1})-[\\prod_{m=1}^{M}\\mathrm{TV}(q^{m},p)(x_{1:n-1})]]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Takeaways. The expected rejection of Batch Decoding composes of two parts: $\\begin{array}{r}{\\dot{\\i})\\sum_{n}^{T}\\mathbb{E}_{q}[\\mathsf{T V}[q,p]]}\\end{array}$ which is the rejection that matches the non-batch speculative decoding; $(i i)$ the b at\u0159ch improvement (BI) which comes from our design and is always non-negative. To better understand how the batch improvement scale with $M$ , we instantiate the single token $\\begin{array}{r}{\\mathbf{B}\\mathrm{I}(\\boldsymbol{q},\\boldsymbol{p}):=\\mathsf{T V}[\\boldsymbol{q},\\boldsymbol{p}]-\\prod_{m=1}^{M}\\mathsf{T V}[\\boldsymbol{q}^{m},\\boldsymbol{p}]}\\end{array}$ with two simple examples. ", "page_idx": 6}, {"type": "text", "text": "Uniform Distribution. For the whole space $\\nu$ , let $p$ be a uniform distribution with support $\\nu$ and $q$ be a uniform distribution over a subset of $\\mathcal{V}$ with size $V^{\\prime}\\,<\\,V$ . Let $V/V^{\\prime}\\,=\\,r$ , in this case $\\begin{array}{r}{\\hat{\\mathrm{BI}}(\\mathrm{Unif}(V^{\\prime}),\\mathrm{Unif}(V))=\\big(1-\\frac{1}{r}\\big)-\\big(1-\\frac{1}{r}\\big)^{M}.}\\end{array}$ , $r\\,=\\,V/V^{\\prime}$ . Its pattern is visualized in the lower right panel of Figure 3. The improvement converges to $\\mathrm{1-1}/r$ and is always positive as long as batch size is at least 2. Also, when the vocabulary size $V$ scales up, i.e. $r$ goes up, the improvement is going to zero. This is not surprising, since the probability of draft sample to fall within in the support of target distribution is very small. ", "page_idx": 6}, {"type": "text", "text": "Bernoulli Distribution. Suppose $u\\ \\ \\geqslant\\ \\ v$ and let $\\begin{array}{r l r}{p}&{{}\\sim}&{\\mathbf{Ber}(u)}\\end{array}$ , $\\begin{array}{r l r}{q}&{{}\\sim}&{\\mathsf{B e r}(v)}\\end{array}$ . Then $\\mathrm{BI}(\\mathrm{Ber}(v),\\mathrm{Ber}(u))\\;=\\;|u\\,-\\,v|\\stackrel{\\cdot\\,\\cdot\\,}{\\cdot\\,}(1\\,-\\,u^{M-1})$ . Its pattern is exhibited in the upper right panel of Figure 3. Notice for both cases, the limiting batch improvement is $\\mathsf{T V}(p,q)$ , which is only significant when $p$ deviates from $q$ . This provides the practical design heuristic: batch design can significantly reduce the number of rejections when the draft distribution p and target distribution q are different from each other and won\u2019t make too much improvement when p and q are close. ", "page_idx": 6}, {"type": "text", "text": "Batch Simulation. The Middle panel of Figure 2(b) shows Batch Speculative Decoding simulations with batch $M\\,=\\,4,5$ . The green/purple lines are the empirical average rejections simulated via Algorithm 4 and the orange/pink lines are the theoretical values computed via Theorem 3. The right panel of Figure 2(c) plots the rejection vs. batch size. In particular, the the black dot with coordinate $(1,16.41)$ represents the Speculative Decoding 1. The numeric example shows when $M\\to\\infty$ , $\\mathbb{E}[N_{\\mathrm{rej}}]\\nrightarrow0$ . Intuitively, this is partial due to, if the distribution mismatch between $p$ and $q$ is large, rejection is unavoidable no matter how many draft responses are sampled from $p$ . We also formally prove this in Proposition 1. This theoretical discovery indicates that having a very large draft batches does not necessarily result in significant inference speedups compared to small batches. ", "page_idx": 6}, {"type": "image", "img_path": "wSqpNeMVLU/tmp/77ad90053837ea1161094cdb2511a9924ec0162946a44aa828c77bfcbd116f60.jpg", "img_caption": ["Figure 4: Left $(a)$ : The Pareto Front between Rejection Probability $\\mathbb{P}^{\\mathcal{A}}(\\mathrm{reject})$ vs. Distribution bias $\\mathsf{T}\\bar{\\mathsf{V}}[\\mathbb{P}^{A},q]$ . For a given rejection probability, the black line denotes the optimal deviation $\\mathrm{Loss}_{\\mathrm{TV}}^{*}$ . Middle $(b)$ and Right $(c)$ : A numeric example. In the plot, the over acceptance $\\epsilon\\mathbf{\\dot{s}}$ are set as positive constants that define $\\begin{array}{r}{b(x)=\\operatorname*{min}\\{1,\\frac{q(x)+\\epsilon}{p(x)}\\}}\\end{array}$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4 Analysis on the Optimal Rejection-Distribution Bias Tradeoff ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In earlier sections, we focus on analyzing SD or Batch SD which are distribution unbiased. To further reduce rejections, the algorithm may have to compromise on output quality. Nevertheless, it is usually acceptable for many real-world applications. For instance, for the family of Gemini models [38], Google may deploy the small model Gemini Nano as the draft model $p$ and Gemini Ultra as the target model $q$ and tune the acceptance probability $b$ (in Algorithm 2) higher such that Gemini Nano is applied more often for the purpose of less expenditure. Therefore, an intriguing question to ask is: For biased algorithms, what is the optimal tradeoff between rejection and distribution bias? ", "page_idx": 7}, {"type": "text", "text": "4.1 An Optimization Formulation and Pareto-optimal Characterization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We measure the distribution bias between $\\mathbb{P}^{A}$ and $q$ via TV distance.6 Concretely, for any algorithm ${\\mathcal{A}}:=(b,{\\mathcal{P}})$ in 2 with acceptance probability $b$ and rejection distribution $\\mathcal{P}$ , we fix $b$ and aim to optimize the following objective ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{Loss}_{\\mathsf{T V}}^{*}(b):=\\operatorname*{min}_{\\mathcal{P}}\\mathsf{T V}[\\mathbb{P}^{A},q],\\quad w h e r e\\,\\,\\mathcal{A}:=(b,\\mathcal{P}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We wish to solve the above since it characterizes the minimal distribution bias for any design $b$ . We present our solution in the next. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Optimal solution to optimization (1)). We can show the objective (1) is equivalent to $L o s s_{\\Gamma\\backslash\\mathbf{V}}^{*}(b):=\\frac{1}{2}\\operatorname*{min}_{\\mathcal{P}}\\,\\sum_{x}\\left|q(x)-b(x)p(x)-\\mathcal{P}(x)\\sum_{\\tilde{x}}[1-b(\\tilde{x})]p(\\tilde{x})\\right|\\,s.t.\\,\\sum_{x}\\mathcal{P}(x)=1,\\;\\mathcal{P}(x)\\geqslant0,\\;\\forall x.$ Suppose $\\begin{array}{r}{\\sum_{x}[1\\,-\\,b(x)]p(x)\\,>\\,0}\\end{array}$ . Define $\\begin{array}{r}{A(x)\\;:=\\;\\frac{q(x)-b(x)p(x)}{\\sum_{\\tilde{x}}[1-b(\\tilde{x})]p(\\tilde{x})}}\\end{array}$ qx\u02dcprx1q\u00b4\u00b4bbppxx\u02dcqqsppppxx\u02dcqq, and the sets A\\` \u201c tx P V : $A(x)\\geq0\\}$ u, $A_{-}\\,=\\,\\{x\\in\\mathcal{V}:A(x)\\,<\\,0\\}$ . Then the s\u0159et of optimal distributions of objective (1) is characterized as $\\{\\mathcal{P}^{*}:\\mathcal{P}^{*}|_{A_{-}}(\\cdot)=0;0\\leqslant\\mathcal{P}^{*}|_{A_{+}}(\\cdot)\\leqslant A(\\cdot)\\}$ , and the optimal value is $L o s s_{\\mathsf{T V}}^{*}(b)=$ ${\\begin{array}{r}{{\\frac{1}{2}}\\sum_{x}|q(x)-b(x)p(x)|-{\\frac{1}{2}}\\sum_{x}(1-b(x))p(x)\\geqslant0}\\end{array}}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 is a universal characterization that contains both biased and unbiased situations. 1. If $b$ is less than Speculative Decoding threshold, i.e. $b\\leqslant\\operatorname*{min}\\{1,q/p\\}$ then $A$ becomes a probability distribution and the optimal distribution $\\mathcal{P}^{*}$ equals $A$ which is also unbiased $(\\mathrm{Loss}_{\\mathsf{T V}}^{*}(b)\\,\\bar{=}\\,0)$ ); 2. If $b$ exceeds the Speculative Decoding threshold, then $A$ is no longer a distribution and there are multiple optimal distributions $\\mathcal{P}^{*}$ . In this case, the optimal distribution bias $\\mathrm{Loss}_{\\mathrm{TV}}^{*}(b)>0$ for Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "Main Takeaways. Via Theorem 4, we derive the pareto front (the optimal tradeoff) between rejection probability7 $\\mathbb{P}$ prejectq vs. distribution distance $\\mathsf{T}^{\\mathsf{\\Sigma}}\\mathsf{V}[\\mathbb{P}^{A},q]$ (Left panel of Figure 4). The blue region can be realized by some algorithm 2, and the red region cannot. $(0,0)$ is the \u201cperfect algorithm\u201d (no rejection, no bias) which does not exists, and, in particular, $(0,\\mathsf{T V}[p,q])$ stands for Speculative Decoding. Surprisingly, the pareto front is a straight line connecting $(0,\\bar{\\mathsf{T V}}[p,q])$ and $(\\mathsf{T}\\bar{\\mathsf{V}}[p,q],0)$ , which represents a linear relationship between the rejection probability and the optimal TV deviation. This is guaranteed by the following theorem. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5 (Pareto front). For any Algorithm $\\boldsymbol{\\mathcal{A}}$ in the class 2 that satisfies $\\begin{array}{r}{\\operatorname*{min}\\{1,\\frac{q(x)}{p(x)}\\}\\leqslant b(x)\\leqslant}\\end{array}$ 1, $\\forall x\\in\\mathcal{V}$ . Then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\textnormal{\\texttt{\\textbf{P}}}^{\\mathcal{A}}(r e j e c t)+L o s s_{\\mathsf{T V}}^{*}(b)=\\mathsf{T V}[p,q].}&\\\\ &{\\mathbb{P}^{{\\cal A}}(r e j e c t)=1-\\sum_{x}b(x)p(x)\\;a n d\\;L o s s_{\\mathsf{T V}}^{*}(b):=\\operatorname*{min}_{\\mathcal{P}}\\mathsf{T V}[\\mathbb{P}^{\\cal A},q].}&\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We plot the numerical example using two Markov Chains in the middle and right panel of Figure 4 that coincides with our theoretical finding. In the figure, the acceptance probability is set to be $\\begin{array}{r}{b(x)\\,=\\,\\operatorname*{min}\\{1,\\frac{q(x)+\\epsilon}{p(x)}\\}}\\end{array}$ . The orange line in $(c)$ and the green boundary in $(b)$ are computed via $\\mathrm{Loss}_{\\mathrm{TV}}^{*}(b)$ from Theorem 4. The complete proofs for Theorem 5 and Theorem 4 are deferred to Appendix E. For the clearness of illustration, we focus on the pareto front between rejection vs. the minimal TV deviation for a single token. ", "page_idx": 8}, {"type": "text", "text": "4.2 Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide an additional simple experiment to show the effectiveness of our Pareto-optimal solution in Theorem 4. Consider objective (1). For the given acceptance probability $b>\\operatorname*{min}\\{1,q/p\\}$ , we have two options for $\\mathcal{P}$ : 1. Baseline: select $\\mathcal{P}$ to be suboptimal to (1), i.e. simply set $\\mathcal{P}:=q$ , which is the target distribution; 2. Set $\\mathcal{P}:=\\mathcal{P}^{\\ast}$ be the optimal distribution of Theorem 4. We call the first method Decoding-UNO and the latter one Decoding-OPT. Instead of TV distance, we measure the quality via WinRate in our experiment. Concretely, for each prompt, we let Decoding-UNO and Decoding-OPT to generate responses independently, and use score models to compare whose generation has higher quality. We specify draft model $p$ as pythia-70m and target model $q$ as pythia-2.8b from EleutherAI [7]. We apply the score model to be RM-Mistral-7B or GPT-4. We test 200 prompts from Alpaca-Farm-Eval Dataset [13] with 500 responses/comparisons per prompt. Table 1 shows that Decoding-OPT achieves better performance than Decoding-UNO across different choice of $\\epsilon$ \u2019s. Due to space constraint, the missing details are deffered to Appendix G. ", "page_idx": 8}, {"type": "table", "img_path": "wSqpNeMVLU/tmp/7f423ca830d8d121ea873a2bf9501d43fdad7e50b76b26b4a4e7ff0875413282.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1: WinRate for Decoding-OPT vs Decoding-UNO with different over-acceptance threshold $\\epsilon$ . The acceptance probability $\\begin{array}{r}{b(x)=\\operatorname*{min}\\{1,\\frac{q(x)+\\epsilon}{p(x)}\\}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "5 Discussions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "On the optimality for batch algorithms. Unlike their non-batch counterparts, our batch algorithm studies do not come with optimality guarantees. This is largely due to the diverse and arbitrary nature of batch algorithm designs, making it challenging to define a comprehensive class that encompasses a wide range of batch algorithms. While [37] investigate optimal batch algorithms through an optimal transport lens, their work does not extend to calculating optimal rejection rates or developing an efficient algorithm to achieve this (they only propose an approximate solution). Consequently, the pursuit of batch optimality remains an open field. Identifying the optimal batch algorithm could yield valuable insights for enhancing practical applications in real-world scenarios. ", "page_idx": 8}, {"type": "text", "text": "Extending Speculative Decoding to other studies. Speculative Decoding is a generic sampling approach that extends beyond mere decoding tasks. It holds potential for wider applications such as search engines and recommendation systems, where it can be employed to quickly generate and refine search outcomes or content suggestions, enhancing the overall efficiency and user experience of these systems. We leave these as future works. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank anonymous reviewers for their valuable feedback. Mengdi Wang acknowledges the support by NSF IIS-2107304, NSF CPS-2312093, and ONR 1006977. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Kwangjun Ahn, Ahmad Beirami, Ziteng Sun, and Ananda Theertha Suresh. Spectr $^{++}$ : Improved transport plans for speculative decoding of large language models. In NeurIPS 2023 Workshop Optimal Transport and Machine Learning, 2023. [3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6836\u20136846, 2021. [4] Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. arXiv preprint arXiv:2310.05424, 2023.   \n[5] Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and Babak Ehteshami Bejnordi. Think big, generate quick: Llm-to-slm for fast autoregressive decoding. arXiv preprint arXiv:2402.16844, 2024.   \n[6] Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, and Mahyar Najibi. Speculative streaming: Fast llm inference without auxiliary models. arXiv preprint arXiv:2402.11131, 2024. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023. [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [9] F Warren Burton. Speculative computation, parallelism, and functional programming. IEEE Transactions on Computers, 100(12):1190\u20131193, 1985.   \n[10] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.   \n[11] Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin ChenChuan Chang. Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462, 2023.   \n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[13] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2023.   \n[14] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024.   \n[15] Freddy Gabbay and Avi Mendelson. Speculative execution based on value prediction. TechnionIIT, Department of Electrical Engineering, 1996.   \n[16] Mukul Gagrani, Raghavv Goel, Wonseok Jeon, Junyoung Park, Mingu Lee, and Christopher Lott. On speculative decoding for multimodal large language models. arXiv preprint arXiv:2404.08856, 2024.   \n[17] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence, 45(1):87\u2013110, 2022.   \n[18] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252, 2023.   \n[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \n[20] Kaixuan Huang, Xudong Guo, and Mengdi Wang. Specdec++: Boosting speculative decoding via adaptive candidate lengths. arXiv preprint arXiv:2405.19715, 2024.   \n[21] Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott. Recursive speculative decoding: Accelerating llm inference via sampling without replacement. arXiv preprint arXiv:2402.14160, 2024.   \n[22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[23] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[24] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274\u201319286. PMLR, 2023.   \n[25] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023.   \n[26] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.   \n[27] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. An emulator for fine-tuning large language models using small language models. arXiv preprint arXiv:2310.12962, 2023.   \n[28] Jie Ou, Yueming Chen, and Wenhong Tian. Lossless acceleration of large language model via adaptive n-gram parallel decoding. arXiv preprint arXiv:2404.08698, 2024.   \n[29] Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, and Anoop Deoras. Bass: Batched attentionoptimized speculative sampling. arXiv preprint arXiv:2404.15778, 2024.   \n[30] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola\\`. Accelerating transformer inference for translation via parallel decoding. arXiv preprint arXiv:2305.10427, 2023.   \n[31] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785\u2013799. PMLR, 2023.   \n[32] Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. arXiv preprint arXiv:2308.04623, 2023.   \n[33] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018.   \n[34] Qidong Su, Christina Giannoula, and Gennady Pekhimenko. The synergy of speculative decoding and batching in serving large language models. arXiv preprint arXiv:2310.18813, 2023.   \n[35] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024.   \n[36] Ziteng Sun, Jae Hun Ro, Ahmad Beirami, and Ananda Theertha Suresh. Optimal block-level draft verification for accelerating speculative decoding. arXiv preprint arXiv:2403.10444, 2024.   \n[37] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. arXiv preprint arXiv:2310.15141, 2023.   \n[38] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe\u00b4e Lacroix, Baptiste Rozie\\`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[40] Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, et al. Minions: Accelerating large language model inference with adaptive and collective speculative decoding. arXiv preprint arXiv:2402.15678, 2024.   \n[41] Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909\u20133925, 2023.   \n[42] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.   \n[43] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. Llmcad: Fast and scalable on-device large language model inference. arXiv preprint arXiv:2309.04255, 2023.   \n[44] Minghao Yan, Saurabh Agarwal, and Shivaram Venkataraman. Decoding speculative decoding. arXiv preprint arXiv:2402.01528, 2024.   \n[45] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-candidate speculative decoding. arXiv preprint arXiv:2401.06706, 2024.   \n[46] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Franc\u00b8ois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Proof Sketch ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof sketch of Theorem 2. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For any algorithm $A\\in{\\mathcal{F}}$ , its design $b_{n}$ can be written as a function of any sequence $x_{1:n-1},\\tilde{x}_{n}$ with $0\\leqslant b_{n}(\\tilde{x}_{n},x_{1:n-1})\\leqslant1$ .8 Based on this, we can further define the new function $\\epsilon_{n}:\\mathcal{V}\\times\\mathcal{V}^{n-1}\\mapsto\\mathbb{R}$ according to the following equation: ", "page_idx": 12}, {"type": "equation", "text": "$$\nb_{n}(\\tilde{x}_{n},x_{1:n-1})=\\operatorname*{min}\\left\\{1,\\frac{q_{n}(\\tilde{x}_{n}|x_{1:n-1})+\\epsilon_{n}(\\tilde{x}_{n},x_{1:n-1})}{p_{n}(\\tilde{x}_{n}|x_{1:n-1})}\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Indeed, we can choose $\\epsilon_{n}:=b_{n}\\cdot p_{n}-q_{n}$ , and the validity of the definition is guaranteed by the Lemma 1. Then, the acceptance probability $\\begin{array}{r}{b_{n}>\\operatorname*{min}\\{1,\\frac{q_{n}}{p_{n}}\\}}\\end{array}$ implies $\\epsilon_{n}>0$ . ", "page_idx": 12}, {"type": "text", "text": "Next, we show for any $A\\in{\\mathcal{F}}$ , it must satisfy $\\begin{array}{r}{b_{n}\\leqslant\\operatorname*{min}\\{1,\\frac{q_{n}}{p_{n}}\\}}\\end{array}$ for all $n$ . To this end, we design the two token sets $\\mathcal{V}_{+}=\\{x:\\exists n\\;s.t$ . $\\epsilon_{n}(x)>0\\}$ and $\\mathcal{V}_{-}=\\{x\\stackrel{\\cdot}{:}\\exists n\\;s$ .t. $\\epsilon_{n}(x)\\leqslant0\\}$ and prove $\\upnu_{+}=\\emptyset$ , ${\\mathcal{V}}_{-}={\\mathcal{V}}$ . ", "page_idx": 12}, {"type": "text", "text": "Finally, by Lemma 2 in Appendix, any algorithm satisfies $\\begin{array}{r}{b_{n}\\leqslant\\operatorname*{min}\\{1,\\frac{q_{n}}{p_{n}}\\},\\forall n\\in[T]}\\end{array}$ must have $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{P}}^{\\mathcal{A}}\\left[N_{\\mathrm{rej}}\\right]\\geqslant\\sum_{n=1}^{T}\\mathbb{E}_{q}[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})]}\\end{array}$ . Since $A\\in{\\mathcal{F}}$ is arbitrary, this concludes the proof. The full proo f \u0159is included in C. ", "page_idx": 12}, {"type": "text", "text": "A.2 High Level Proof Sketch for the second part of Theorem 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The derivation for the number of expected rejections using the batch speculative decoding is more involved than the Algorithm 4 due to the parallel response structure. The key step is to compute the intermediate quantity $\\mathbb{P}^{A}({\\tilde{x}}_{n}\\operatorname{acc},{\\tilde{x}}_{n}={\\^{\\cdot}}x_{n}|x_{1:n-1})$ . Let $\\tilde{x}_{n-1}\\sim p(\\cdot|x_{1:n-\\overset{.}{2}})$ , then there are two cases: $\\tilde{x}_{n-1}$ accepted or rejected. We have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\mathsf{A}}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n}|x_{1:n-1})=\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n},\\tilde{x}_{n-1}\\mathsf{a c c}|x_{1:n-1})+\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n},\\tilde{x}_{n-1})}\\\\ &{=\\underbrace{\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n}|\\tilde{x}_{n-1}\\mathsf{a c c},x_{1:n-1})}_{p_{a}}\\mathbb{P}^{\\cal A}(\\tilde{x}_{n-1}\\mathsf{a c c}|x_{1:n-1})}\\\\ &{+\\underbrace{\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n}|\\tilde{x}_{n-1}\\mathsf{r e j},x_{1:n-1})}_{p_{b}}\\mathbb{P}^{\\cal A}(\\tilde{x}_{n-1}\\mathsf{r e j}|x_{1:n-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In the process of finding $p_{b}$ , we need to compute the the quantity $f(x_{1:n})\\;\\;:=\\;\\;\\mathbb{P}(x_{1:n}\\;\\cap$ $\\{n$ -th draft token rejecteduq and it can be recursively computed via (22) using $p,q$ . ", "page_idx": 12}, {"type": "text", "text": "A.3 Proof sketch for Theorem 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Due to space constraint, we only summarize the high-level proof ideas for Theorem 4. Since $\\textstyle\\sum_{x}A(x)^{\\star}=1$ , the original objective (1) can be equivalently rewritten as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathcal{P}}~\\frac{1}{2}\\sum_{x}\\left|A(x)-\\mathcal{P}(x)\\right|,\\quad s.t.~~\\sum_{x}\\mathcal{P}(x)=1,\\quad\\mathcal{P}(x)\\geqslant0,~\\forall x\\in\\mathcal{V}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We now find the solution of objective (4) in two steps. ", "page_idx": 12}, {"type": "text", "text": "Step1: Recall $A_{+}=\\{x\\in\\mathcal{V}:A(x)\\geqslant0\\}$ and $A_{-}=\\{x\\in\\mathcal{V}:A(x)<0\\}$ , then any optimal $\\mathcal{P}^{*}$ must satisfy $\\mathcal{P}^{*}(x)=0$ for all $x\\in A_{-}$ . This can be shown by contradiction via an alternative construction $\\mathcal{P}^{\\prime}$ to reason $\\mathcal{P}^{*}$ is suboptimal to $\\mathcal{P}^{\\prime}$ . ", "page_idx": 12}, {"type": "text", "text": "Step2: We characterize the optimal solutions of the objective. By Step1, we can show any optimal solution $\\mathcal{P}^{*}$ satisfies $\\begin{array}{r}{\\sum_{x}|A(\\bar{x})-\\mathcal P^{*}(x)|\\geqslant\\|A\\|_{1}-1}\\end{array}$ and the equal sign can be achieved. Then we can convert $\\left\\Vert A\\right\\Vert_{1}-1$ \u0159back to $\\mathrm{Loss}_{\\mathrm{TV}}^{*}$ . This also helps identify the optimal set of $\\mathcal{P}^{*}$ . ", "page_idx": 12}, {"type": "table", "img_path": "wSqpNeMVLU/tmp/6fe1e6b1b7a0a669b0a91b1b27c06f7c1d18c12dded42745407c6e53e5e19469.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 6 (Restatement of the first part of Theorem 1). We define random variables $R_{n}\\in\\{0,1\\}$ indicating whether the $n$ -th token is rejected with 1 being rejected (here rejection means Line $^{\\sc6}$ of Algorithm $^{\\,l}$ is executed). Then, the total number of rejections $\\begin{array}{r}{N_{r e j}=\\sum_{n=1}^{T}R_{n}}\\end{array}$ . For Speculative Decoding, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[N_{r e j}]=\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q}[\\mathsf{T V}(p_{n}(\\cdot|x_{1:n-1}),q_{n}(\\cdot|x_{1:n-1}))].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here TV denote the TV distance between two distributions. ", "page_idx": 13}, {"type": "text", "text": "Proof. Given the verified the tokens $x_{1:n-1}$ , we first compute PpReject at $n|x_{1:n-1}\\rangle$ q. Denote the candidate draft token $\\tilde{x}\\sim p_{n}(\\cdot|x_{1:n-1})$ , then by law of total probability ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}(\\mathrm{Reject~at~}n|x_{1:n-1})=\\sum_{\\vec{x}}\\mathbb{P}(\\mathrm{Reject~at~}n,\\vec{x}|x_{1:n-1})}}\\\\ &{=\\sum_{\\vec{x}}\\mathbb{P}(\\mathrm{Reject~}\\tilde{x}|\\tilde{x},x_{1:n-1})\\mathbb{P}(\\tilde{x}|x_{1:n-1})}\\\\ &{=\\sum_{\\vec{x}}\\mathbb{P}(\\mathrm{Reject~}\\tilde{x}|\\tilde{x},x_{1:n-1})p_{n}(\\tilde{x}|x_{1:n-1})}\\\\ &{=\\sum_{\\vec{x}}\\mathbb{P}(\\mathrm{Reject~}\\tilde{x}|\\tilde{x},x_{1:n-1})\\log(\\tilde{x}|x_{1:n-1})}\\\\ &{=\\sum_{\\vec{x}}(1-\\operatorname*{min}\\{1,\\frac{q_{n}(\\tilde{x}|x_{1:n-1})}{p_{n}(\\tilde{x}|x_{1:n-1})}\\})p_{n}(\\tilde{x}|x_{1:n-1})}\\\\ &{=\\sum_{\\vec{x}}\\operatorname*{max}\\{0,p_{n}-q_{n}\\}(\\tilde{x}|x_{1:n-1})=\\mathbb{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the third equal sign uses draft token is sampled from $p_{n}$ and the fourth equality is by design of Speculative Decoding (Algorithm 1, Line 4). ", "page_idx": 13}, {"type": "text", "text": "Lastly, by law of total expectation and above ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\big[N_{\\mathrm{e}}\\big]=\\sum_{t=1}^{T}\\mathbb{E}[R_{t}]=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathbb{E}[R_{t}|x_{1:t-1}]\\big]}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{x_{1}:\\ldots=1}\\mathbb{E}[R_{t}|x_{1:t-1}]\\mathbb{P}(x_{1:t-1})}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{x_{1}:\\ldots=1}\\mathbb{E}[R_{t}|x_{1:t-1}]q(x_{1:t-1})}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{x_{1}:\\ldots=1}\\mathbb{P}(\\mathbf{R}\\mathbf{e}|\\mathrm{exta}\\ \\mathbf{n}|x_{1:n-1})q(x_{1:n-1})}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{x_{1}:\\ldots=1}\\mathbb{E}[\\mathbf{p}(p_{n}(\\cdot|x_{1:n-1}),q_{n}(\\cdot|x_{1:n-1}))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here the fourth equal sign comes from Speculative Decoding keep the distribution $q$ (Proposition ??), the fifth equal sign comes from the event $\\{R_{n}=1\\}=\\{{\\mathrm{Reject}}\\,{\\mathrm{at}}\\,n\\}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Theorem 7 (Restatement of the second part of Theorem 1). The output distributions of Speculative Decoding Algorithm 1 and the large model $q$ are identical, i.e. for any output sequence $x_{1:T}\\in\\mathcal{V}^{T}$ , the joint the distributions over $x_{1:T}$ satisfies: $\\mathbb{P}^{S p e c D e c o d i n g}(x_{1:T})=q(\\boldsymbol{x}_{1:T})^{\\intercal}$ . ", "page_idx": 13}, {"type": "text", "text": "Remark 3. The proof of Theorem 7 is very similar to [10] except we have $K=\\infty$ . In addition, [10] proves the distribution match for a single token, we complement the proof to show the distribution match holds for a sequence of tokens $x_{1:T}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. In particular, we use induction to show the stronger result that $\\forall\\;t\\in[T],\\;\\forall x_{1},x_{2},\\ldots,x_{t}\\in\\mathcal{V},$ , it holds $\\mathbb{P}_{t}^{\\hat{\\boldsymbol{A}}}(x_{1},x_{2},\\ldots,x_{t})=q_{t}(x_{1},x_{2},\\ldots,x_{t})$ . ", "page_idx": 14}, {"type": "text", "text": "Step1: Since $x_{0}$ is the prompt, its distribution is independent to $p,q$ . Then for $t\\,=\\,1$ , applying Theorem 1 of [10] (with $K\\,=\\,\\infty$ ) directly with $p$ and $q$ to be conditional distributions $p_{1}(\\cdot|x_{0})$ and $q_{1}(\\cdot|x_{0})$ gives $\\mathbb{P}_{1}^{A}(x_{1}|x_{0})\\;=\\;q_{1}(x_{1}|x_{0})$ , which further implies $\\mathbb{P}_{1}^{A}(x_{1})\\;=\\;q_{1}(x_{1})$ (since the distribution of $x_{0}$ is independent of $p,q)$ . ", "page_idx": 14}, {"type": "text", "text": "Step2: assume $\\begin{array}{r l r}{\\mathbb{P}_{t}^{\\mathcal{A}}(x_{1},x_{2},\\ldots,x_{t})}&{=}&{q_{t}(x_{1},x_{2},\\ldots,x_{t})}\\end{array}$ , we first show $\\begin{array}{r l}{\\mathbb{P}^{A}(x_{t+1}|x_{1:t})}&{{}=}\\end{array}$ \u201c $q(x_{t+1}|x_{1:t})$ . Indeed, let $\\tilde{x}_{t+1}\\sim p(\\cdot|x_{1:t})$ , then by law of total probability ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\mathcal{A}}(x_{t+1}\\vert x_{1:t})=\\mathbb{P}^{\\mathcal{A}}(\\tilde{x}_{t+1}=x_{t+1}\\vert x_{1:t})\\mathbb{P}(\\tilde{x}_{t+1}\\mathrm{~acc}|\\tilde{x}_{t+1}=x_{t+1},x_{1:t})}\\\\ &{\\qquad\\qquad\\qquad+\\mathbb{P}^{\\mathcal{A}}(\\tilde{x}_{t+1}\\mathrm{~rej}|x_{1:t})\\mathbb{P}^{\\mathcal{A}}(x_{t+1}\\vert\\tilde{x}_{t+1}\\mathrm{~rej},x_{1:t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Algorithm 1, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}^{\\cal A}(\\tilde{x}_{t+1}=x_{t+1}|x_{1:t})\\mathbb{P}^{\\cal A}(\\tilde{x}_{t+1}\\mathrm{~acc}|\\tilde{x}_{t+1}=x_{t+1},x_{1:t})}\\\\ &{=\\!p(x_{t+1}|x_{1:t})\\operatorname*{min}\\bigg(1,\\!\\frac{q\\big(x_{t+1}|x_{1:t}\\big)}{p\\big(x_{t+1}|x_{1:t}\\big)}\\!\\bigg)=\\!\\operatorname*{min}\\{p(x_{t+1}|x_{1:t}),q(x_{t+1}|x_{1:t})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, the probability of rejection is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\tilde{x}_{t+1}\\mathrm{~rej}|x_{1:t})=1-\\mathbb{P}(\\tilde{x}_{t+1}\\mathrm{~acc}|x_{1:t})=1-\\displaystyle\\sum_{x^{\\prime}}\\mathbb{P}(\\tilde{x}_{t+1}=x^{\\prime},\\ \\tilde{x}_{t+1}\\mathrm{~acc}|x_{1:t})}\\\\ &{=1-\\displaystyle\\sum_{x^{\\prime}}\\operatorname*{min}\\{p(x^{\\prime}|x_{1:t}),q(x^{\\prime}|x_{1:t})\\}=\\displaystyle\\sum_{x^{\\prime}}\\operatorname*{max}\\{0,q(x^{\\prime}|x_{1:t})-p(x^{\\prime}|x_{1:t})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second equal sign comes from (6). Lastly, by the construction of the algorithm, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\cal A}(x_{t+1}|{\\tilde{x}}_{t+1}\\mathrm{~rej},x_{1:t})=\\frac{\\operatorname*{max}\\{0,q(x_{t+1}|x_{1:t})-p(x_{t+1}|x_{1:t})\\}}{\\sum_{x^{\\prime}}\\operatorname*{max}\\{0,q(x^{\\prime}|x_{1:t})-p(x^{\\prime}|x_{1:t})\\}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining (8) and (7) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\mathcal{A}}(\\tilde{x}_{t+1}\\operatorname{rej}\\lvert x_{1:t})\\mathbb{P}^{\\mathcal{A}}(x_{t+1}\\lvert\\tilde{x}_{t+1}\\operatorname{rej},x_{1:t})=\\operatorname*{max}\\{0,q(x_{t+1}\\lvert x_{1:t})-p(x_{t+1}\\lvert x_{1:t})\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Plugging (6) and the above equation into (5) to obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n^{\\mathcal{A}}(x_{t+1}|x_{1:t})=\\operatorname*{min}\\{p(x_{t+1}|x_{1:t}),q(x_{t+1}|x_{1:t})\\}+\\operatorname*{max}\\{0,q(x_{t+1}|x_{1:t})-p(x_{t+1}|x_{1:t})\\}=q(x_{t+1}|x_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, applying the above we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t+1}^{A}(x_{1:t+1})=\\mathbb{P}_{t}^{A}(x_{1:t})\\cdot\\mathbb{P}^{A}(x_{t+1}|x_{1:t})=\\mathbb{P}_{t}^{A}(x_{1:t})\\cdot q(x_{t+1}|x_{1:t})=q_{t+1}(x_{1:t+1}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last equal sign uses the induction hypothesis. ", "page_idx": 14}, {"type": "text", "text": "C Lower Bound ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Theorem 8 (Restatement of Theorem 2). Define the arbitrary instance $\\mathcal{P}:=(p,q)$ , and define the family of algorithms as ", "page_idx": 15}, {"type": "text", "text": "$\\mathcal{F}:=\\{\\mathcal{A}:\\mathcal{A}$ is a specification of Algorithm 2 that satisfies $\\mathbb{P}_{t}^{A}=q_{t}\\;\\forall t$ pi.e., distribution unbiasedqu. For an algorithm $\\boldsymbol{\\mathcal{A}}$ , denote $N_{r e j}$ as the number of rejections. Then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{A\\in\\mathcal{F}}\\mathbb{E}_{\\mathcal{P}}^{A}\\left[N_{r e j}\\right]\\geqslant\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q}\\left[\\mathsf{T V}(p_{n},q_{n})(\\cdot|x_{1:n-1})\\right]:=\\mathfrak{C}(\\mathcal{P}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Remark 4. Theorem $8$ shows the rejections of Algorithm $^{\\,l}$ is tight at the instance level (over the family of algorithms ${\\mathcal F}_{.}$ ). Therefore, it attains the instance-optimality over sequential decoding algorithms family $\\mathcal{F}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. For any algorithm $A\\in{\\mathcal{F}}$ , its $b_{n}$ can be written as a function of the sequence $x_{1:n-1},\\tilde{x}_{n}$ with $0\\leqslant b_{n}({\\tilde{x}}_{n},x_{1:n-1})\\leqslant1.$ .9 Based on this, we define the new function $\\epsilon_{n}:\\mathcal{V}\\times\\mathcal{V}^{n-1}\\mapsto\\mathbb{R}$ according to the following equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\nb_{n}(\\tilde{x}_{n},x_{1:n-1})=\\operatorname*{min}\\left\\{1,\\frac{q_{n}(\\tilde{x}_{n}|x_{1:n-1})+\\epsilon_{n}(\\tilde{x}_{n},x_{1:n-1})}{p_{n}(\\tilde{x}_{n}|x_{1:n-1})}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Indeed, we can choose $\\epsilon_{n}:=b_{n}\\cdot p_{n}-q_{n}$ , and the validity of the definition is guaranteed by the Lemma 1. Recall $A\\in{\\mathcal{F}}$ is a distribution unbiased algorithm w.r.t. $q$ . Let $x_{1},\\ldots,x_{n}$ be the validated sequence of $\\boldsymbol{\\mathcal{A}}$ , then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}_{n}^{A}(x_{n}|x_{1:n-1})=\\frac{\\mathbb{P}_{n}^{A}(x_{1:n})}{\\mathbb{P}_{n}^{A}(x_{1:n-1})}=\\frac{q_{n}(x_{1:n})}{q_{n}(x_{1:n-1})}=q_{n}(x_{n}|x_{1:n-1}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, let $\\tilde{x}_{n}\\sim p_{n}(\\cdot|x_{1:n-1})$ , the decomposition holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{n}^{A}(x_{n}|x_{1:n-1})=\\mathbb{P}_{n}^{A}(x_{n},\\tilde{x}_{n}\\mathrm{~accept}|x_{1:n-1})+\\mathbb{P}_{n}^{A}(x_{n},\\tilde{x}_{n}\\mathrm{~reject}|x_{1:n-1})}\\\\ &{=\\!\\!\\mathbb{P}_{n}^{A}(\\tilde{x}_{n}\\mathrm{~accept}|\\tilde{x}_{n}=x_{n},x_{1:n-1})\\mathbb{P}_{n}^{A}(\\tilde{x}_{n}=x_{n}|x_{1:n-1})}\\\\ &{+\\!\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})\\!\\mathbb{P}_{n}^{A}(\\tilde{x}_{n}\\mathrm{~reject}|x_{1:n-1})}\\\\ &{=\\!b_{n}(x_{n},x_{1:n-1})\\cdot p_{n}(x_{n}|x_{1:n-1})+\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})\\cdot\\mathbb{P}_{n}^{A}(\\tilde{x}_{n}\\mathrm{~reject}|x_{1:n-1})}\\\\ &{\\!\\!=\\!b_{n}(x_{n},x_{1:n-1})\\cdot p_{n}(x_{n}|x_{1:n-1})+\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})\\cdot(1-\\sum_{n}^{\\infty}\\!b_{n}(x,x_{1:n-1})p_{n}(x|x_{1:n-1})-\\mathbb{P}_{n}^{B}(x_{n}|x_{1:n-1}))}\\\\ &{\\!\\!=\\!\\!b_{n}(x_{n},x_{1:n-1})\\cdot p_{n}(x_{n}|x_{1:n-1})+\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})\\cdot(1-\\sum_{n}^{\\infty}\\!b_{n}(x,x_{1:n-1})p_{n}(x|x_{1:n-1})-\\mathbb{P}_{n}^{B}(x_{1:n-1}))}\\\\ &{\\!\\!=\\!\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the third equal sign uses $\\mathbb{P}_{n}^{A}(\\tilde{x}_{n}=x_{n}|x_{1:n-1})=p_{n}(x_{n}|x_{1:n-1})$ and the last equal sign is due to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{n}^{\\cal A}(\\tilde{x}_{n}\\mathrm{~reject}|x_{1:n-1})=1-\\mathbb{P}_{n}^{\\cal A}(\\tilde{x}_{n}\\mathrm{~accept}|x_{1:n-1})=1-\\displaystyle\\sum_{x}\\mathbb{P}_{n}^{\\cal A}(\\tilde{x}_{n}\\mathrm{~accept},\\tilde{x}_{n}=x|x_{1:n-1})}\\\\ &{=1-\\displaystyle\\sum_{x}\\mathbb{P}_{n}^{\\cal A}(\\tilde{x}_{n}\\mathrm{~accept}|\\tilde{x}_{n}=x,x_{1:n-1})\\mathbb{P}_{n}^{\\cal A}(\\tilde{x}_{n}=x|x_{1:n-1})=1-\\displaystyle\\sum_{x}b_{n}(x,x_{1:n-1})p_{n}(x|x_{1:n-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let\u2019s do some further simplifications. First off, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{b_{n}(x_{n},x_{1:n-1})\\cdot p_{n}(x_{n}|x_{1:n-1})=\\!p_{n}(x_{n}|x_{1:n-1})\\operatorname*{min}\\left\\{1,\\!\\frac{q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x_{n},x_{1:n-1})}{p_{n}(x_{n}|x_{1:n-1})}\\right\\}}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\operatorname*{min}\\left\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x_{n},x_{1:n-1})\\right\\},\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{1-\\displaystyle\\sum_{x}b_{n}(x,x_{1:n-1})p_{n}(x|x_{1:n-1})=1-\\displaystyle\\sum_{x}\\operatorname*{min}\\left\\{p_{n}(x|x_{1:n-1}),q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x,x_{1:n-1})\\right\\}}\\\\ &{}&{\\qquad=\\displaystyle\\sum_{x}\\operatorname*{max}\\left\\{0,p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x,x_{1:n-1})\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "9For Speculative Decoding, its $\\begin{array}{r}{b_{n}\\big(\\tilde{x}_{n},x_{1:n-1}\\big)=\\operatorname*{min}\\left\\{1,\\frac{q_{n}(\\tilde{x}_{n}|x_{1:n-1})}{p_{n}(\\tilde{x}_{n}|x_{1:n-1})}\\right\\}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Plug (12) and (13) into (11), and then plug (11) into (10) to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad q_{n}(x_{n}|x_{1:n-1})=\\operatorname*{min}\\left\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x_{n},x_{1:n-1})\\right\\}}\\\\ &{+\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})\\cdot\\displaystyle\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x,x_{1:n-1}))_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, we define the token set $\\mathcal{V}_{+}=\\{x:\\exists\\;n,x_{1:n-1}\\;s.\\}$ t. $\\epsilon_{n}(x,x_{1:n-1})>0\\}$ and similarly the token set $\\mathcal{V}_{-}=\\{x:\\exists\\;n,x_{1:n-1}\\;s.\\i$ t. $\\epsilon_{n}(x,x_{1:n-1})\\leqslant0\\}$ . Next, we show $\\mathcal{V}_{+}=\\mathcal{O}$ , and ${\\mathcal{V}}_{-}={\\mathcal{V}}$ . ", "page_idx": 16}, {"type": "text", "text": "Case1. If $p_{n}(x_{n}|x_{1:n-1})\\geqslant q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x_{n},x_{1:n-1})$ , then by (14) we have ", "page_idx": 16}, {"type": "text", "text": "$q_{n}(x_{n}|x_{1:n-1})=q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x_{n},x_{1:n-1}$ q $+\\mathbb{P}_{n}^{A}\\!\\left(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},\\tilde{x}_{n}=x_{n},x_{1:n-1}\\right)\\cdot\\sum_{x}\\!(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x,x_{1:n-1}))+$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\geqslant q_{n}{\\left(x_{n}|x_{1:n-1}\\right)}+\\epsilon_{n}{\\left(x_{n},x_{1:n-1}\\right)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and this implies $0\\geqslant\\epsilon_{n}{\\left(x_{n},x_{1:n-1}\\right)}$ , which means $x_{n}\\in\\mathcal{V}_{-}$ ; ", "page_idx": 16}, {"type": "text", "text": "Case2. If $p_{n}(x_{n}|x_{1:n-1})<q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x_{n},x_{1:n-1})$ , then by (14) we have ", "page_idx": 16}, {"type": "text", "text": "$q_{n}(x_{n}|x_{1:n-1})=p_{n}(x_{n}|x_{1:n-1})$ $+\\mathbb{P}_{n}^{A}\\!\\left(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},\\tilde{x}_{n}=x_{n},x_{1:n-1}\\right)\\cdot\\sum_{x}\\!(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x,x_{1:n-1}))+$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\geqslant p_{n}{\\bigl(}x_{n}|x_{1:n-1}{\\bigr)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and this implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{n}(x_{n},x_{1:n-1})=\\!b_{n}(x_{n},x_{1:n-1})p_{n}(x_{n}|x_{1:n-1})-q_{n}(x_{n}|x_{1:n-1})}\\\\ &{\\qquad\\qquad\\qquad\\leqslant p_{n}(x_{n}|x_{1:n-1})-q_{n}(x_{n}|x_{1:n-1})\\leqslant0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means $x_{n}\\in\\mathcal{V}_{-}$ . ", "page_idx": 16}, {"type": "text", "text": "Therefore, combining the two cases we always have $x\\in\\mathcal{V}_{-}$ , which indicates $\\mathcal{V}_{+}=\\mathcal{O}$ . By (9), this implies for all $n$ , $\\begin{array}{r}{b_{n}\\overset{}{\\leqslant}\\operatorname*{min}\\{1,\\frac{q_{n}}{p_{n}}\\}}\\end{array}$ . Finally, by Lemma 2, this implies $\\mathbb{E}_{\\mathcal{P}}^{A}\\left[N_{\\mathrm{rej}}\\right]\\geqslant\\mathfrak{C}(\\mathcal{P})$ . Since $A\\in{\\mathcal{F}}$ is arbitrary, this concludes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Corollary 1. For any algorithm $A\\in{\\mathcal{F}}$ , it follows @ $n\\in[T],x\\in\\mathcal{V}$ and $x_{1:n-1},\\,b_{n}(x,x_{1:n-1})\\leqslant$ $\\begin{array}{r}{\\operatorname*{min}\\left\\{1,\\frac{q_{n}(x|x_{1:n-1})}{p_{n}(x|x_{1:n-1})}\\right\\}}\\end{array}$ . In this case, the distribution ${\\mathcal{P}}_{n}$ is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{P}_{n}(x|x_{1:n-1})=\\frac{q_{n}(x|x_{1:n-1})-\\operatorname*{min}\\left\\{p_{n}(x|x_{1:n-1}),q_{n}(x|x_{1:n-1})+\\epsilon_{n}(x,x_{1:n-1})\\right\\}}{\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1}))_{+}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying $\\epsilon_{n}=b_{n}p_{n}-q_{n},$ , this is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{P}_{n}(x|x_{1:n-1})=\\frac{q_{n}(x|x_{1:n-1})-b_{n}(x,x_{1:n-1})p_{n}(x|x_{1:n-1})}{\\sum_{x}(1-b_{n}(x,x_{1:n-1}))p_{n}(x|x_{1:n-1})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Corollary $^{\\,l}$ . We reutilize (14) here and call it (15). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad q_{n}(x_{n}|x_{1:n-1})=\\operatorname*{min}\\left\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}(x_{n}|x_{1:n-1})+\\epsilon_{n}(x_{n},x_{1:n-1})\\right\\}}\\\\ &{+\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})\\cdot\\underset{x}{\\sum}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x,x_{1:n-1}))_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By two cases discussion as in the proof of Theorem 8, we have $\\forall\\,A\\in{\\mathcal{F}}$ , it follows $\\forall\\;n\\in[T],x\\in\\mathcal{V}$ and x1:n\u00b41, bnpx, x1:n\u00b41q \u010f min!1, pqnnppxx||xx11::nn\u00b4\u00b411qq). Then we can directly solve (15) to obtain $\\mathbb{P}_{n}^{\\cal A}(x|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})=\\frac{q_{n}(x|x_{1:n-1})-\\operatorname*{min}\\left\\{p_{n}(x|x_{1:n-1}),q_{n}(x|x_{1:n-1})+\\epsilon_{n}(x,x_{1:n-1})\\right\\}}{\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1}))_{+}}.$ Lastly, we verify such a $\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})$ is a valid distribution. First of all, since $\\epsilon_{n}=$ $b_{n}\\cdot p_{n}-q_{n}$ , then $\\begin{array}{r}{b_{n}(x,x_{1:n-1})\\leqslant\\operatorname*{min}\\left\\{1,\\frac{q_{n}(x|x_{1:n-1})}{p_{n}(x|x_{1:n-1})}\\right\\}}\\end{array}$ implies $\\epsilon_{n}(x,x_{1:n-1})\\leqslant0$ , and this further implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad q_{n}(x|x_{1:n-1})-\\operatorname*{min}\\left\\{p_{n}(x|x_{1:n-1}),q_{n}(x|x_{1:n-1})+\\epsilon_{n}(x,x_{1:n-1})\\right\\}}\\\\ &{\\quad\\geqslant q_{n}(x|x_{1:n-1})-\\operatorname*{min}\\left\\{p_{n}(x|x_{1:n-1}),q_{n}(x|x_{1:n-1})\\right\\}\\geqslant0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies $\\mathbb{P}_{n}^{A}(x_{n}|\\tilde{x}_{n}\\mathrm{~reject},x_{1:n-1})\\geqslant0$ . Second, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{x}\\mathbb{P}_{n}^{A}(x|\\bar{x}_{n}\\mathrm{~reject},x_{1:n-1})=\\sum_{x}\\frac{q_{n}(x|x_{1:n-1})-\\operatorname*{min}\\{p_{n}(x|x_{1:n-1}),q_{n}(x|x_{1:n-1})+\\epsilon_{n}(x,x_{1:n-1})\\}}{\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1}))+}}}\\\\ &{=\\frac{1-\\sum_{x}\\operatorname*{min}\\{p_{n}(x|x_{1:n-1}),q_{n}(x|x_{1:n-1})+\\epsilon_{n}(x,x_{1:n-1})\\}}{\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1}))+}}\\\\ &{\\frac{1+\\sum_{x}\\operatorname*{max}\\{-p_{n}(x|x_{1:n-1}),-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1})\\}}{\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1}))+}}\\\\ &{=\\frac{\\sum_{x}p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1})-\\epsilon_{n}(x,x_{1:n-1})\\}{\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1}))+}}\\\\ &{=\\frac{\\sum_{x}p_{n}(x|x_{1:n-1})+\\sum_{x}\\operatorname*{max}\\{-p_{n}(x|x_{1:n-1}),-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x_{1:n-1})-\\epsilon_{n}(x,x_{1:n-1})\\}}{\\sum_{x}(p_{n}(x|x_{1:n-1})-q_{n}(x|x_{1:n-1})-\\epsilon_{n}(x|x \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma 1. For any $0\\leqslant b_{n}(\\tilde{x}_{n},x_{1:n-1})\\leqslant1$ , there exists $\\epsilon_{n}(\\tilde{x}_{n},x_{1:n-1})\\in\\mathbb{R}$ such that (9) holds true. ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Indeed, set $\\epsilon_{n}=\\boldsymbol{b}_{n}\\cdot\\boldsymbol{p}_{n}-q_{n}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{1,\\frac{q_{n}(\\tilde{x}_{n}|x_{1:n-1})+\\epsilon_{n}(\\tilde{x}_{n},x_{1:n-1})}{p_{n}(\\tilde{x}_{n}|x_{1:n-1})}\\right\\}=\\operatorname*{min}\\{1,b_{n}(\\tilde{x}_{n},x_{1:n-1})\\}=b_{n}(\\tilde{x}_{n},x_{1:n-1})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first equal sign uses that $\\tilde{x}_{n}$ is sampled from $p_{n}(\\cdot|x_{1:n-1})$ so $p_{n}(\\tilde{x}_{n}|x_{1:n-1})>0$ , and the second equal sign uses $0\\leqslant b_{n}\\leqslant1$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 2. For any instance $\\boldsymbol{\\mathcal{P}}=(p,q)$ , let $\\mathcal{F}:=\\{\\mathcal{A}:\\mathcal{A}$ is a realization of Algorithm 2 s.t. $\\mathbb{P}_{t}^{A}=$ $q_{t}\\not\\in{\\sf{H}}t$ pi.e., unbiasedqu. Suppose there is an $A\\in{\\mathcal{F}}$ such that $\\mathbb{E}_{\\mathcal{P}}^{\\mathcal{A}}\\left[N_{r e j}\\right]<\\bar{\\mathfrak{C}}(\\mathcal{P}),$ , then there exists $a$ $b_{n}$ in Line 8 of Template 2 such that $\\exists x,x_{1:n-1}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nb_{n}(x|x_{1:n-1})>\\operatorname*{min}\\left\\{1,\\frac{q_{n}(x|x_{1:n-1})}{p_{n}(x|x_{1:n-1})}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "rParnodoof mof  vLaerimabmlea s f ionr dailcl $n,x,x_{1:n-1}$ r, $\\begin{array}{r}{b_{n}(x|x_{1:n-1})\\leqslant\\operatorname*{min}\\left\\{1,\\frac{q_{n}(x|x_{1:n-1})}{p_{n}(x|x_{1:n-1})}\\right\\}}\\end{array}$ ..  TWhee rdeeffoirnee, $R_{n}\\in\\{0,1\\}$ $n$   \nthe expected number of rejection is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}^{A}\\left[\\sum_{n=1}^{T}R_{n}\\right]=\\sum_{n=1}^{T}\\mathbb{E}^{A}[R_{n}]=\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim\\mathbb{P}^{A}}\\left[\\mathbb{E}^{A}[R_{n}|x_{1:n-1}]\\right]=\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q}\\left[\\mathbb{E}^{A}[R_{n}|x_{1:n-1}]\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here the second to last equality comes from the tower property and the last equal signs is due to $\\boldsymbol{\\mathcal{A}}$ is unbiased. Next, denote $\\tilde{x}_{n}\\sim p_{n}(\\cdot|x_{1:n-1})$ to be the candidate token, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}^{A}[R_{n}|x_{1:n-1}]=\\mathbb{P}^{A}[R_{n}=1|x_{1:n-1}]=\\sum_{\\Tilde{x}_{n}}\\mathbb{P}^{A}[R_{n}=1,\\Tilde{x}_{n}|x_{1:n-1}]}}\\\\ &{=\\!\\sum_{\\Tilde{x}_{n}}\\!\\mathbb{P}^{A}[R_{n}=1|\\Tilde{x}_{n},x_{1:n-1}]\\mathbb{P}^{A}[\\Tilde{x}_{n}|x_{1:n-1}]=\\sum_{\\Tilde{x}_{n}}\\!\\mathbb{P}^{A}[R_{n}=1|\\Tilde{x}_{n},x_{1:n-1}]p_{n}[\\Tilde{x}_{n}|x_{1:n-1}]}\\\\ &{=\\!\\sum_{\\Tilde{x}_{n}}\\!(1-b_{n})\\cdot p_{n}[\\Tilde{x}_{n}|x_{1:n-1}]\\geqslant\\!\\sum_{\\Tilde{x}_{n}}\\!(1-\\operatorname*{min}\\left\\{1,\\frac{q_{n}\\left(\\Tilde{x}_{n}|x_{1:n-1}\\right)}{p_{n}\\left(\\Tilde{x}_{n}|x_{1:n-1}\\right)}\\right\\})\\cdot p_{n}[\\Tilde{x}_{n}|x_{1:n-1}]}\\\\ &{=\\!\\sum_{\\Tilde{x}_{n}}\\![p_{n}(\\Tilde{x}_{n}|x_{1:n-1})-q_{n}(\\Tilde{x}_{n}|x_{1:n-1})]_{+}=\\mathrm{TV}[(p_{n}(\\cdot|x_{1:n-1})-q_{n}(\\cdot|x_{1:n-1})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and this implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal P}^{A}\\left[N_{\\mathrm{rej}}\\right]=\\mathbb{E}^{A}\\left[\\sum_{n=1}^{T}r_{n}\\right]\\geqslant\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q}\\left[\\mathsf{T V}\\big[(p_{n}(\\cdot|x_{1:n-1})-q_{n}(\\cdot|x_{1:n-1})\\big]\\big]=\\mathfrak{C}(\\mathcal P)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "contradicts $\\mathbb{E}_{\\mathcal{P}}^{\\mathcal{A}}\\left[N_{\\mathrm{rej}}\\right]<\\mathfrak{C}(\\mathcal{P});$ ! This concludes the proof. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Init: Horizon $T$ , Distributions $q_{t}$ and $p_{t}$ , with $q=\\mathbb{P}^{\\mathrm{LLM}}$ , $p=\\mathbb{P}^{\\mathrm{Draft}}$ . Lookahead $K=\\infty$ .   \n2: while $n<T$ do   \n3: Reset $q_{t}=\\mathbb{P}_{t}^{\\mathrm{LLM}}\\;\\forall t$ . $n_{0}=n$ .   \n4: for $m=1:M\\diamondsuit$ Sample $M$ draft responses in parallel\u02db do   \n5: for $t=n:T$ do   \n6: Sample $\\tilde{x}_{t}^{m}\\sim p_{t}(\\cdot|x_{1:n-1},\\tilde{x}_{n:t-1}^{m})$ .   \n7: end for   \n8: end for   \n9: Obtain logits $q_{n}(\\cdot|x_{1:n-1})$ , . . . , $q_{T}(\\cdot|x_{1:n-1},\\tilde{x}_{n:T}^{m}),\\;\\forall m\\in[M]$ in parallel for $\\tilde{x}_{n:T}^{m}$ .   \n10: Verification Begins   \n11: Set Sample $=$ False.   \n12: for $m=1,\\dots,M$ do   \n13: for $t=n:T$ do   \n14: Sample $r\\sim\\mathsf{U n i f o r m}[0,1]$ .   \n15: if $\\begin{array}{r}{r\\leqslant\\operatorname*{min}\\left\\{1,\\frac{q_{t}(\\tilde{x}_{t}^{m}|x_{1:n-1},\\tilde{x}_{n:t-1}^{m})}{p_{t}(\\tilde{x}_{t}^{m}|x_{1:n-1},\\tilde{x}_{n:t-1}^{m})}\\right\\}}\\end{array}$ then   \n16: Accept with $x_{n}=\\tilde{x}_{t}^{m}$ . $n\\gets n+1$ . Sample $\\c=$ True.   \n17: else   \n18: if $t=n_{0}$ then   \n19: Update $q_{n}\\leftarrow\\left[q_{n}(\\cdot|x_{1:n-1})-p_{n}(\\cdot|x_{1:n-1})\\right]_{+}$ . Break. //Here $n_{0}$ equals $n$ .   \n20: else   \n21: $\\diamondsuit$ Rejection Sample $x_{n}\\sim\\left[q_{n}(\\cdot|x_{1:n-1})-p_{n}(\\cdot|x_{1:n-1})\\right]_{+}$ . $n\\gets n+1$ . Break.   \n22: end if   \n23: end if   \n24: end for   \n25: if Sample $=$ TRUE then   \n26: Break.   \n27: else   \n28: $\\diamondsuit$ Rejection Sample $x_{n}\\sim\\left[q_{n}(\\cdot|x_{1:n-1})-p_{n}(\\cdot|x_{1:n-1})\\right]_{+}$ . $n\\gets n+1$ . Break.   \n29: end if   \n30: end for   \n31: end while ", "page_idx": 19}, {"type": "text", "text": "D Batch Speculative Decoding ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We split the proofs for unbiasedness and rejections in two parts. ", "page_idx": 20}, {"type": "text", "text": "Theorem 9 (Unbiasedness of Theorem 3). Denote the Algorithm $^{4}$ as $A_{B}$ . For any sequence $x_{1:T}$ (where $x_{i}\\in\\mathcal{V},$ ), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}_{T}^{A_{B}}(x_{1},\\ldots,x_{T})=\\mathbb{P}_{T}^{L L M}(x_{1},\\ldots,x_{T}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Step1: Let $x_{1:n-1}$ be the accepted tokens up to $n-1$ . We first show $\\mathbb{P}_{n}^{A_{B}}(x_{n}|x_{1:n-1})=$ $\\mathbb{P}_{n}^{\\mathrm{LLM}}(x_{n}|\\mathcal{\\bar{x_{1:n-1}}})\\;\\forall x_{n}\\in\\mathcal{V}$ . ", "page_idx": 20}, {"type": "text", "text": "We partition the generation of $x_{n}$ into two cases: (i). accepted as the first token of the $m$ -th responses $(m\\:=\\:1,\\ldots,M)$ , or rejected by all $M$ responses and sampled by Line 28 of Algorithm 4; (ii). accepted/rejected as the $t$ -th token of the $m$ -th responses $(m=1,\\ldots,M)$ for $t\\geqslant2$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}_{n}^{A_{B}}(x_{n}|x_{1:n-1})=\\mathbb{P}_{n}^{A_{B}}(x_{n},\\tilde{x}_{n}^{m}\\mathrm{~acc}|x_{1:n-1})+\\mathbb{P}_{n}^{A_{B}}(x_{n},\\tilde{x}_{n}^{m}\\mathrm{~rej}|x_{1:n-1})}}\\\\ &{=\\mathbb{P}_{n}^{A_{B}}(\\tilde{x}_{n}^{m}=x_{n}|x_{1:n-1})\\mathbb{P}_{n}^{A_{B}}(\\tilde{x}_{n}^{m}\\mathrm{~acc}|\\tilde{x}_{n}^{m}=x_{n},x_{1:n-1})}\\\\ &{+\\mathbb{P}_{n}^{A_{B}}(\\tilde{x}_{n}^{m}\\mathrm{~rej}|x_{1:n-1})\\mathbb{P}_{n}^{A_{B}}(x_{n}|\\tilde{x}_{n}^{m}\\mathrm{~rej},x_{1:n-1})}\\\\ &{=p_{n}(x_{n}|x_{1:n-1})\\operatorname*{min}\\{1,\\frac{q_{n}(x_{n}|x_{1:n-1})}{p_{n}(x_{n}|x_{1:n-1})}\\}}\\\\ &{+\\underset{x^{\\prime}}{\\sum}\\mathrm{max}\\{0,q_{n}(x^{\\prime}|x_{1:n-1})-p_{n}(x^{\\prime}|x_{1:n-1})\\}\\underset{x^{\\prime}}{\\frac{\\operatorname*{max}\\{0,q_{n}(x_{n}|x_{1:n-1})-p_{n}(x_{n}|x_{1:n-1})\\}}{\\sum_{x^{\\prime}}\\mathrm{max}\\{0,q_{n}(x^{\\prime}|x_{1:n-1})-p_{n}(x^{\\prime}|x_{1:n-1})\\}}=q_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the construction of Algorithm 4 (Line 3), when $x_{n}$ is accepted/rejected as the $t(\\geqslant2)$ -th draft token s.e, we have $q_{n}(x_{n}|x_{1:n-1})\\,=\\,\\mathbb{P}_{n}^{\\mathrm{LLM}}(x_{n}|\\bar{x_{1:n-1}})$ . This gives $\\mathbb{P}_{n}^{A_{B}}(x_{n}|x_{1:n-1})\\,=$ $\\mathbb{P}_{n}^{\\mathrm{LLM}}(x_{n}|x_{1:n-1})$ ", "page_idx": 20}, {"type": "text", "text": "For the first case. This part of the proof largely follows Theorem 4.2 of [26]. In this case, the $n$ -th generated token $x_{n}$ has $M+1$ possibilities: accepted at the $m$ -th response or rejected by all $M$ responses and sample at Line 28. Since the algorithm will iterate all $M$ responses if not accepted, we denote $q_{n}^{m}$ as the $q_{n}$ for the $m$ -th response. Then we have the recursion ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{n}^{m+1}=\\frac{\\operatorname*{max}\\{0,q_{n}^{m}-p_{n}\\}}{r_{m}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $q_{n}^{1}=\\mathbb{P}_{n}^{\\mathrm{LLM}}$ and $r_{m}$ is the rejection probability satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{m}=1-\\mathbb{P}_{n}^{A_{B}}(\\tilde{x}_{n}^{m}\\mathrm{~acc}|x_{1:n-1})=1-\\displaystyle\\sum_{x}\\mathbb{P}_{n}^{A_{B}}(\\tilde{x}_{n}^{m}=x|x_{1:n-1})\\mathbb{P}_{n}^{A_{B}}(\\tilde{x}_{n}^{m}\\mathrm{~acc}|\\tilde{x}_{n}^{m}=x,x_{1:n-1})}\\\\ &{\\quad=1-\\displaystyle\\sum_{x}p_{n}(x|x_{1:n-1})\\operatorname*{min}\\{1,\\displaystyle{\\frac{q_{n}^{m}(x|x_{1:n-1})}{p_{n}(x|x_{1:n-1})}}\\}=\\displaystyle\\sum_{x}\\operatorname*{max}\\{0,q_{n}^{m}(x|x_{1:n-1})-p_{n}(x|x_{1:n-1})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denote $E_{m}=\\{m$ -th response rejectedu and $E_{1:m}=\\left\\{1:m\\right.$ -th responses all rejectedu, then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}_{n}^{A_{B}}(x_{n}|x_{1:n-1})=\\mathbb{P}_{n}^{A_{B}}(x_{n},E_{1}^{c}|x_{1:n-1})+\\mathbb{P}_{n}^{A_{B}}(x_{n},E_{1}|x_{1:n-1})}\\\\ &{=\\operatorname*{min}\\{p_{n}(x_{n}|x_{1:n-1}),q_{n|}^{1}(x_{n}|x_{1:n-1})\\}+\\mathbb{P}_{n}^{A_{B}}(x_{n},E_{1}|x_{1:n-1})}\\\\ &{=\\operatorname*{min}\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}^{1}(x_{n}|x_{1:n-1})\\}+r_{1}\\cdot\\mathbb{P}_{n}^{A_{B}}(x_{n}|E_{1},x_{1:n-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denote $\\mathbb{P}_{n}^{A_{B}}(x_{n}|x_{1:n-1}):=\\mathbb{P}_{n}^{A_{B}}(x_{n}|E_{0},x_{1:n-1})$ , by similar calculation we have in general ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{\\times4s}{n}(x_{n}|E_{0:m-1},x_{1:n-1})=\\operatorname*{min}\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}^{m}(x_{n}|x_{1:n-1})\\}+r_{m}\\cdot\\mathbb{P}_{n}^{A_{B}}(x_{n}|E_{0:m},x_{1:n-1}).}\\end{array}$ Next we prove $\\begin{array}{r l r}{\\mathbb{P}_{n}^{\\mathcal{A}_{\\mathcal{B}}}(\\cdot|E_{0:m},x_{1:n-1})}&{{}=}&{q_{n}^{m+1}(\\cdot|x_{1:n-1})}\\end{array}$ @m P t0, 1, . . . , Mu by backward induction. First of all, $\\begin{array}{r l r}{\\mathbb{P}_{n}^{A_{B}}\\left(\\cdot|E_{0:M},x_{1:n-1}\\right)}&{=}&{\\left[q_{n+1}^{M}(\\cdot|x_{1:n-1})-p_{n+1}(\\cdot|x_{1:n-1})\\right]_{+}\\;=}\\end{array}$ $\\begin{array}{r}{\\frac{\\operatorname*{max}\\{0,q_{n}^{M}-p_{n}\\}\\left(\\cdot|x_{1:n-1}\\right)}{r_{M}}=q_{n}^{M+1}}\\end{array}$ . Suppose $\\mathbb{P}_{n}^{\\mathcal{A}_{B}}\\!\\left(\\cdot|E_{0:m+1},x_{1:n-1}\\right)=q_{n}^{m+2}(\\cdot|x_{1:n-1})$ , then ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad\\mathbb{P}_{n}^{A_{B}}\\big(\\cdot|E_{0:m},x_{1:n-1}\\big)=\\mathbb{P}_{n}^{A_{B}}\\big(\\cdot,E_{m+1}^{c}|E_{0:m},x_{1:n-1}\\big)+\\mathbb{P}_{n}^{A_{B}}\\big(\\cdot,E_{m+1}|E_{0:m},x_{1:n-1}\\big)}\\\\ &{=\\operatorname*{min}\\{p_{n}\\big(\\cdot|x_{1:n-1}\\big),q_{n}^{m+1}\\big(\\cdot|x_{1:n-1}\\big)\\}+\\mathbb{P}_{n}^{A_{B}}\\big(\\cdot,E_{m+1}|E_{0:m},x_{1:n-1}\\big)}\\\\ &{=\\operatorname*{min}\\{p_{n}\\big(\\cdot|x_{1:n-1}\\big),q_{n}^{m+1}\\big(\\cdot|x_{1:n-1}\\big)\\}+r_{m+1}\\cdot\\mathbb{P}_{n}^{A_{B}}\\big(\\cdot|E_{0:m+1},x_{1:n-1}\\big)}\\\\ &{=\\operatorname*{min}\\{p_{n}\\big(\\cdot|x_{1:n-1}\\big),q_{n}^{m+1}\\big(\\cdot|x_{1:n-1}\\big)\\}+\\operatorname*{max}\\{0,q_{n}^{m+1}-p_{n}\\}\\equiv q_{n}^{m+1}.}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "In particular, we take $m=0$ to obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}_{n}^{\\mathcal{A}_{B}}\\!\\left(\\cdot\\big|x_{1:n-1}\\right)=q_{n}^{1}\\!\\left(\\cdot\\big|x_{1:n-1}\\right)=\\mathbb{P}_{n}^{\\mathrm{LLM}}\\!\\left(\\cdot\\big|x_{1:n-1}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining both cases we finish the proof of Step1. ", "page_idx": 21}, {"type": "text", "text": "Step2: For any $n$ , first of all we have $\\mathbb{P}^{A_{B}}(x_{0})\\;=\\;\\mathbb{P}^{\\mathrm{LLM}}(x_{0})$ since $x_{0}$ is the prompt. Suppose $\\mathbb{P}_{n-1}^{A_{B}}(x_{1:n-1})=\\mathbb{P}_{n-1}^{\\mathrm{LLM}}(x_{1:n-1})$ , $\\forall x_{1:n-1}$ , then by Step1 $\\mathbb{P}_{n}^{A_{B}}(x_{1:n})=\\mathbb{P}_{n}^{A_{B}}(x_{n}|x_{1:n-1})\\mathbb{P}_{n-1}^{A_{B}}(x_{1:n-1})=\\mathbb{P}_{n}^{A_{B}}(x_{n}|x_{1:n-1})\\mathbb{P}_{n-1}^{\\mathrm{LM}}(x_{1:n-1})=\\mathbb{P}_{n}^{\\mathrm{LM}}(x_{1:n})$ ", "page_idx": 21}, {"type": "text", "text": "where the second equal sign is by induction and the third equal sign is by Step1. This finish the proof. ", "page_idx": 21}, {"type": "text", "text": "D.1 Expected Rejections for Batch Speculative Decoding (Proof for the second part of Theorem 3) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we derive the number of expected rejections using the batch speculative decoding. However, the analysis is more involved than the Algorithm 4 due to the parallel response structure, since, given the verified token $x_{1:n-1}$ , the probability of $n$ -th token being rejected does not possess a unified expression and depends on the location of $x_{n-1}$ . We detail this below. ", "page_idx": 21}, {"type": "text", "text": "We recall the notion in Theorem 1 that random variables $R_{n}\\in\\{0,1\\}$ indicates whether the $n$ -th token is rejected (1 is rejected). Therefore, the expected number of rejection is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{n=1}^{T}R_{n}\\right]=\\sum_{n=1}^{T}\\mathbb{E}[R_{n}]=\\sum_{n=1}^{T}\\mathbb{E}\\left[\\mathbb{E}[R_{n}|x_{1:n-1}]\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last equality comes from the tower property of expectation and we assume $x_{0}$ is a given initial token and $\\dot{x}_{1:0}=\\left\\{x_{0}\\right\\}$ . Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[R_{n}|x_{1:n-1}]=\\mathbb{P}^{\\mathcal{A}}(n{\\mathrm{-th~token~rej}}|x_{1:n-1})=1-\\mathbb{P}^{\\mathcal{A}}(n{\\mathrm{-th~token~acc}}|x_{1:n-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In this sc $\\mathrm{nario,\\,we\\;cannot\\;obtain\\;\\mathbb{P}^{\\mathcal{A}}(\\boldsymbol{n}\\mathrm{\\cdotth\\;token\\;rej}|\\boldsymbol{x}_{1:n-1})}=\\mathsf{T V}(p_{n}(\\cdot|\\boldsymbol{x}_{1:n-1}),\\mathbb{P}_{n}^{\\mathrm{LLM}}(\\cdot|\\boldsymbol{x}_{1:n-1}))$ since the conditional rejection probability implicitly encodes the location of $(n\\!-\\!1)$ -th token: whether $\\tilde{x}_{n-1}\\sim p(\\cdot|x_{1:n-1})$ is rejected (at the root of the tree) or $\\tilde{x}_{n-1}\\sim p(\\cdot|x_{1:n-1})$ is accepted (at the branch of the tree). To formalize this, given validated token $x_{1:n-1}$ , we denote $q_{n}^{m}(\\cdot|x_{1:n-1}^{\\bar{}})$ to be the $m$ -th rejection distribution, then by the construction of Algorithm 4 (Line 19), ", "page_idx": 21}, {"type": "equation", "text": "$$\nq_{n}^{m+1}=\\frac{\\operatorname*{max}\\{0,q_{n}^{m}-p_{n}\\}}{r_{m}},\\quad\\forall m\\in[M].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here $\\begin{array}{r}{r_{m}=\\sum_{x^{\\prime}}\\operatorname*{max}\\{0,q_{n}^{m}(x^{\\prime}|x_{1:n-1})-p_{n}(x^{\\prime}|x_{1:n-1})\\}}\\end{array}$ is normalizing factor and $q_{n}^{1}=\\mathbb{P}^{\\mathrm{LLM}}$ . Let $\\tilde{x}_{n}\\,\\sim\\,p(\\cdot|x_{1:n-1})$ , then $\\mathbb{P}^{\\mathcal{A}}(n$ -th token $\\mathrm{acc}|x_{1:n-1})\\,=\\,\\mathbb{P}^{\\mathcal{A}}(\\tilde{x}_{n}\\;\\mathrm{acc}|x_{1:n-1})$ . Next, we compute the quantity $\\mathbb{P}^{A}(\\tilde{x}_{n}\\;\\mathbf{acc}|x_{1:n-1})$ . ", "page_idx": 21}, {"type": "text", "text": "We begin by first considering $\\mathbb{P}^{A}(\\tilde{x}_{n}\\operatorname{acc},\\tilde{x}_{n}=x_{n}|x_{1:n-1})$ . Let $\\tilde{x}_{n-1}\\sim p(\\cdot|x_{1:n-2})$ , then there are two cases: $\\tilde{x}_{n-1}$ accepted or rejected. We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\mathsf{A}}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n}|x_{1:n-1})=\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n},\\tilde{x}_{n-1}\\mathsf{a c c}|x_{1:n-1})+\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n},\\tilde{x}_{n-1})}\\\\ &{=\\underbrace{\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n}|\\tilde{x}_{n-1}\\mathsf{a c c},x_{1:n-1})}_{p_{a}}\\mathbb{P}^{\\cal A}(\\tilde{x}_{n-1}\\mathsf{a c c}|x_{1:n-1})}\\\\ &{+\\underbrace{\\mathbb{P}^{\\cal A}(\\tilde{x}_{n}\\,\\mathsf{a c c},\\tilde{x}_{n}=x_{n}|\\tilde{x}_{n-1}\\mathsf{r e j},x_{1:n-1})}_{p_{b}}\\mathbb{P}^{\\cal A}(\\tilde{x}_{n-1}\\mathsf{r e j}|x_{1:n-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $p_{a}$ . Given that $\\tilde{x}_{n-1}$ is accepted, $\\tilde{x}_{n}\\,=\\,x_{n}$ can only be accepted as the $t$ -th token within the certain response for $t\\geqslant2$ . This is because $\\tilde{x}_{n-1}$ is accepted and has to be at least the first token in the response. In this case, $q_{n}\\bigl(\\cdot\\vert x_{1:n-1}\\bigr)=\\mathbb{P}_{n}^{\\mathrm{LLM}}\\bigl(\\cdot\\vert x_{1:n-1}\\bigr)=q_{n}^{1}\\bigl(\\cdot\\vert x_{1:n-1}\\bigr)$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{a}=\\mathbb{P}^{A}(\\tilde{x}_{n}\\mathrm{~acc}|\\tilde{x}_{n}=x_{n},\\tilde{x}_{n-1}\\mathrm{acc},x_{1:n-1})\\mathbb{P}^{A}(\\tilde{x}_{n}=x_{n}|\\tilde{x}_{n-1}\\mathrm{acc},x_{1:n-1})}\\\\ &{\\quad=\\operatorname*{min}\\{1,\\frac{q_{n}\\left(x_{n}|x_{1:n-1}\\right)}{p_{n}\\left(x_{n}|x_{1:n-1}\\right)}\\}\\cdot p_{n}(x_{n}|x_{1:n-1})=\\operatorname*{min}\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}(x_{n}|x_{1:n-1})\\}}\\\\ &{\\quad=\\operatorname*{min}\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}^{1}(x_{n}|x_{1:n-1})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $p_{b}$ . Given that $\\tilde{x}_{n-1}$ is rejected, $\\tilde{x}_{n}\\,=\\,x_{n}$ can only be accepted as the first token within the certain response. This is because $\\tilde{x}_{n-1}$ is rejected will restart the parallel tree. Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{b}=\\mathbb{P}^{A}(x_{n}|\\tilde{x}_{n-1}\\mathrm{rej},x_{1:n-1})-\\mathbb{P}^{A}(\\tilde{x}_{n}\\mathrm{~rej},x_{n}|\\tilde{x}_{n-1}\\mathrm{rej},x_{1:n-1})}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{=\\mathbb{P}_{n}^{\\mathrm{LLM}}(x_{n}|x_{1:n-1})-\\mathbb{P}^{A}(\\tilde{x}_{n}\\mathrm{~rej},x_{n}|\\tilde{x}_{n-1}\\mathrm{rej},x_{1:n-1})}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{=\\mathbb{P}_{n}^{\\mathrm{LLM}}(x_{n}|x_{1:n-1})-\\left(\\prod_{m=1}^{M}r_{m}\\right)q_{n}^{M+1}(x_{n}|x_{1:n-1})},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first equal sign comes from: since $\\tilde{x}_{n-1}$ rej implies $x_{n}$ represents the first token of the parallel tree, then it is identical to the proof of the first case of Step1 in Theorem 9. The second equal sign is from: $\\tilde{x}_{n}$ is rejected means all $M$ responses since $x_{n}$ is the first token of the tree. The conditional rejection probability ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\tilde{x}_{n}^{m}\\mathbf{\\Phi}\\mathbf{r}\\mathbf{\\mathrm{j}}|\\tilde{x}_{n}^{1:m-1}\\mathbf{\\Phi}\\mathbf{r}\\mathbf{\\mathrm{j}},x_{1:n-1})=1-\\mathbb{P}(\\tilde{x}_{n}^{m}\\mathbf{\\Phi}\\mathbf{acc}|\\tilde{x}_{n}^{1:m-1}\\mathbf{\\Phi}\\mathbf{r}\\mathbf{\\mathrm{j}},x_{1:n-1})}\\\\ &{{=}{\\displaystyle{\\mathrm{\\normalfont~1}-\\sum_{x}\\mathbb{P}_{n}^{\\cal A_{B}}}(\\tilde{x}_{n}^{m}=x|\\tilde{x}_{n}^{1:m-1}\\mathbf{\\Phi}\\mathbf{r}\\mathbf{\\mathrm{j}},x_{1:n-1})\\mathbb{P}_{n}^{\\cal A_{B}}(\\tilde{x}_{n}^{m}\\mathbf{\\Phi}\\mathbf{acc}|\\tilde{x}_{n}^{1:m-1}\\mathbf{\\Phi}\\mathbf{r}\\mathbf{\\mathrm{j}},\\tilde{x}_{n}^{m}=x,x_{1:n-1})}}\\\\ &{{=}{\\displaystyle{\\mathrm{\\normalfont~1}-\\sum_{x}p_{n}}(x|x_{1:n-1})\\operatorname*{min}\\{1,\\frac{q_{n}^{m}}{p_{n}(x|x_{1:n-1})}\\}=\\sum_{x}\\operatorname*{max}\\{0,q_{n}^{m}(x|x_{1:n-1})-p_{n}(x|x_{1:n-1})\\}=r_{m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so by chain rule, the total rejection probability is $\\prod_{m=1}^{M}r_{m}$ . ", "page_idx": 22}, {"type": "text", "text": "Plug (18), (19) into (17) to obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}^{{\\cal A}}(\\tilde{x}_{n}\\mathrm{~acc},x_{n}|x_{1:n-1})=\\operatorname*{min}\\{p_{n}(x_{n}|x_{1:n-1}),q_{n}^{1}(x_{n}|x_{1:n-1})\\}\\mathbb{P}^{{\\cal A}}(\\tilde{x}_{n-1}\\mathrm{acc}|x_{1:n-1})}\\quad}&{}\\\\ &{\\qquad\\qquad\\qquad+[q_{n}^{1}(x_{n}|x_{1:n-1})-(\\displaystyle\\prod_{m=1}^{M}r_{m})\\cdot q_{n}^{M+1}(x_{n}|x_{1:n-1})]\\mathbb{P}^{{\\cal A}}(\\tilde{x}_{n-1}\\mathrm{rej}|x_{1:n-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{1}({x_{n}}|{x_{n-1}})-1\\sqrt{\\lambda}\\left({{\\bar{x}}_{n}}\\right)\\exp\\left({{x_{n}}|{x_{[n-1]}}}\\right)=\\operatorname*{min}{\\left({{p_{n}}}({x_{n}}|{x_{[n-1]}}),{q_{[n]}^{n}}({x_{n-1}})}\\right)[1-\\mathbb{P}^{A}({{\\bar{x}}_{n-1}}|{x_{[n]}})]}\\\\ {\\displaystyle+[\\eta_{n}^{1}({x_{n}}|{x_{[n-1]}})-(\\prod_{n=1}^{M}{\\gamma_{n}})\\cdot\\eta_{n}^{M+1}({x_{n}}|{x_{[n-1]}})]\\mathbb{P}^{A}({\\bar{x}}_{n-1}|{\\times})[{\\varepsilon_{[n-1]}})}\\\\ {\\displaystyle+\\operatorname{P}^{A}({\\bar{x}}_{n}|{x_{n}})\\exp\\left({{x_{n}}|{x_{[n-1]}}}\\right)=\\operatorname*{max}{\\left({{\\eta_{n}}^{1}({x_{n}}|{x_{[n-1]}})}-{p_{{\\eta_{n}}}}({x_{n}}|{x_{[n-1]}})}\\right)}\\\\ {\\displaystyle-\\left(\\operatorname*{max}{\\{0,{\\eta_{n}^{1}({x_{n}}|{x_{[n-1]}})}-p_{{\\eta_{n}}}({x_{n}}|{x_{[n-1]}})\\}}-(\\prod_{n=1}^{M}{\\gamma_{n}})\\eta_{n}^{M+1}({x_{n}}|{x_{[n-1]}})\\right)\\mathbb{P}^{A}({\\bar{x}}_{n-1}|{\\times})[{x_{[n-1]}})}\\\\ {\\displaystyle+\\operatorname{P}^{A}({\\bar{x}}_{n}|{x_{n}})\\exp\\left({{x_{n}}|{x_{[n-1]}}}\\right)\\mathbb{P}^{A}({x_{[n-1]}})=\\operatorname*{max}{\\left({{\\eta_{n}}^{1}({x_{n}}|{x_{[n-1]}})}-{p_{{\\eta_{n}}}}({x_{n}}|{x_{[n]}})}\\right)\\mathbb{P}^{+}(\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first line uses $\\mathbb{P}^{A}(x_{n}|x_{1:n-1})\\;=\\;q_{n}^{1}(x_{n}|x_{1:n-1})$ , the second equivalence uses Bayes rule, the third equivalence uses $\\mathbb{P}^{A}(x_{1:n})\\,=\\,q^{1}(x_{1:n})\\,=\\,q_{n}^{1}(x_{1:n})$ again. Now denote $f(x_{1:n}):=$ $\\mathbb{P}^{A}(\\tilde{x}_{n}\\,\\mathrm{rej},x_{n}|x_{1:n-1}^{\\cdot})q_{n}^{1}(x_{1:n-1})$ , and sum over $x_{1:n}$ for the above to obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Leftrightarrow\\;\\mathbb{E}_{x_{1:n-1}\\sim q^{1}}[\\mathbb{P}^{\\mathcal{A}}(\\widetilde{x}_{n}\\;\\mathrm{rej}|x_{1:n-1})]=\\mathbb{E}_{x_{1:n-1}\\sim q^{1}}[\\mathsf{T V}(q^{1}(\\cdot|x_{1:n-1},p(\\cdot|x_{1:n-1})))]}\\\\ {-\\bar{\\mathbb{E}}_{x_{1:n-1}\\sim f}\\left[\\mathsf{T V}(q^{1},p)(x_{1:n-1})-[\\displaystyle\\prod_{m=1}^{M}\\mathsf{T V}(q^{m},p)(x_{1:n-1})]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where by (21) pseudo-measure $f$ satisfies $\\forall x_{1:n}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f(x_{1:n})=\\operatorname*{max}\\{0,q_{n}^{1}(x_{n}|x_{1:n-1})-p_{n}(x_{n}|x_{1:n-1}))\\}q_{n}^{1}(x_{1:n-1})}}\\\\ {{-\\left(\\operatorname*{max}\\{0,q_{n}^{1}(x_{n}|x_{1:n-1})-p_{n}(x_{n}|x_{1:n-1}))\\}-(\\displaystyle\\prod_{m=1}^{M}r_{m})q_{n}^{M+1}(x_{n}|x_{1:n-1})\\right)f(x_{1:n-1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here we used $\\begin{array}{r}{r_{m}=\\sum_{x}\\operatorname*{max}\\{0,q_{n}^{m}(x|x_{1:n-1})-p_{n}(x|x_{1:n-1})\\}=\\mathsf{T V}(q^{m},p)(x_{1:n-1}).}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Plug the above back  to (16), we finally have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}^{A_{B}}\\big[\\displaystyle\\sum_{n=1}^{T}R_{n}\\big]}}\\\\ &{=\\displaystyle\\sum_{n=1}^{T}\\mathbb{E}_{x_{1:n-1}\\sim q^{1}}\\big[\\mathsf{T V}(q^{1}(\\cdot|x_{1:n-1},p(\\cdot|x_{1:n-1})))\\big]}\\\\ &{-\\displaystyle\\sum_{n=1}^{T}\\bar{\\mathbb{E}}_{x_{1:n-1}\\sim f}\\left[\\mathsf{T V}(q^{1},p)(x_{1:n-1})-\\big[\\displaystyle\\prod_{m=1}^{M}\\mathsf{T V}(q^{m},p)(x_{1:n-1})\\big]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The formulation for $f$ is iteratively obtained in (22). ", "page_idx": 23}, {"type": "text", "text": "D.2 Increasing batch size to inf doesn\u2019t help ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proposition 1. Let $f^{M}$ be the $f$ in Theorem 3 with batch $M$ , and let $f^{\\infty}=\\operatorname*{lim}_{M\\to\\infty}f^{M}$ . Then we have: ", "page_idx": 23}, {"type": "text", "text": "$\\begin{array}{r l}&{\\bullet\\ f^{M}(\\cdot)\\leqslant q^{1}(\\cdot),\\forall M\\in\\mathbb{N};\\ f^{\\infty}(\\cdot)\\leqslant q^{1}(\\cdot).}\\\\ &{\\bullet\\ f^{\\infty}(x_{1:n})=h(x_{n}|x_{<n})[q^{1}(x_{1:n-1})-f^{\\infty}(x_{1:n-1})].}\\\\ &{\\bullet\\ \\operatorname*{lim}_{M\\rightarrow\\infty}\\mathbb{E}[N_{r e j}]=\\sum_{n=1}^{T}(\\mathbb{E}_{q^{1}}[\\mathsf{T V}[q^{1},p]]-\\bar{\\mathbb{E}}_{f^{\\infty}}[\\mathsf{T V}(q^{1},p)]).}\\end{array}$ \u2022 $M\\to\\infty$ , $\\mathbb{E}\\big[N_{r e j}\\big]\\nrightarrow0$ . This indicates increasing batch size to $\\infty$ doesn\u2019t help. ", "page_idx": 23}, {"type": "text", "text": "Proof. First item: Recall in the proof of Theorem 3, $f$ is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x_{1:n}):=\\!\\mathbb{P}^{\\scriptscriptstyle A}\\!\\left(\\tilde{x}_{n}\\mathrm{~rej},x_{n}|x_{1:n-1}\\right)\\!q_{n}^{1}(x_{1:n-1})}\\\\ &{\\qquad\\quad\\;\\,=\\!\\mathbb{P}^{\\scriptscriptstyle A}\\!\\left(\\tilde{x}_{n}\\mathrm{~rej},x_{n}|x_{1:n-1}\\right)\\!\\mathbb{P}^{\\scriptscriptstyle A}\\!\\left(x_{1:n-1}\\right)}\\\\ &{\\qquad\\quad\\;\\,=\\!\\mathbb{P}^{\\scriptscriptstyle A}\\!\\left(\\tilde{x}_{n}\\mathrm{~rej},x_{1:n}\\right)\\leqslant\\mathbb{P}^{\\scriptscriptstyle A}\\!\\left(x_{1:n}\\right)=q^{1}(x_{1:n}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second equality is due to Batch algorithm is unbiased (Theorem 3). ", "page_idx": 23}, {"type": "text", "text": "Second item: by ", "page_idx": 23}, {"type": "equation", "text": "$$\n{}^{\\mathrm{\\tiny~\\sharp}}(x_{1:n})=h(x_{n}|x_{<n})q_{n}^{1}(x_{1:n-1})-[h(x_{n}|x_{<n})-(\\prod_{m=1}^{M}\\mathsf{T V}(q^{m},p)(x_{<n}))q^{M+1}(x_{n}|x_{<n})]f(x_{1:n-1}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since $\\begin{array}{r}{\\prod_{m=1}^{M}{\\sf T V}(q^{m},p)(x_{<n})\\rightarrow0}\\end{array}$ as $M\\rightarrow0$ (note $\\mathsf{T V}(q^{m},p)(x_{<n})\\,=\\,1$ iff there is no overlap betwe e\u015bn $q^{m}$ and $p\\breve{}$ ), it implies $f^{\\infty}(x_{1:n})=h(x_{n}|x_{<n})[q^{1}(x_{1:n-1})-f^{\\infty}(x_{1:n-1})]$ . Third item: Similar to the second item, it holds true via taking $M\\to\\infty$ . For proving $\\mathbb{E}[N_{\\mathrm{rej}}^{\\infty}]>0$ , suppose $\\mathbb{E}[N_{\\mathrm{rej}}^{\\infty}]=0$ . Then it holds $q^{1}\\equiv f^{\\infty}$ . By the second item, this further implies $q^{1}\\equiv f^{\\infty}=0$ , which is impossible. ", "page_idx": 23}, {"type": "text", "text": "Fourth item: We prove by contradiction. Suppose $\\mathbb{E}[N_{\\mathrm{rej}}]\\nrightarrow0$ , then by the first item and the third item this implies $q^{1}\\,\\equiv\\,f^{\\infty}$ . Plug this back to the second item, this further implies $f^{\\infty}\\,\\equiv\\,0$ , so $q^{1}\\equiv f^{\\infty}\\equiv\\dot{0}$ is a contradiction $\\bar{g}^{1}$ is a probability distribution)! ", "page_idx": 23}, {"type": "text", "text": "E Proofs of Section 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Proof of Theorem 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For the sampling scheme where the acceptance probability $b_{n}$ goes beyond $\\begin{array}{r}{\\operatorname*{min}\\{1,\\frac{q_{n}(x|x_{1:n-1})}{p_{n}(x|x_{1:n-1})}\\}}\\end{array}$ , there is a quality degradation for the output sequence as the algorithm is leaning towards the draft model (smaller model). In this case, the objective is to minimize the quality degradation via considering the TV distance ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathcal{P}_{n}}\\mathsf{T V}[\\mathbb{P}_{n}^{\\mathcal{A}}(\\cdot|x_{1:n-1}),q_{n}(\\cdot|x_{1:n-1})],\\;\\;\\forall x_{1:n-1}\\in\\mathcal{V}^{n-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Under the above, via equation (14) the objective is equivalent to the following (note that according to Algorithm 2 $\\mathbb{P}_{n}^{\\mathcal{A}}(x_{n}|$ draft token rejected, $x_{1:n-1})=\\mathcal{P}_{n}(x_{n}|x_{1:n-1})$ is an algorithmic choice) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\mathcal{P}_{n}}~\\frac{1}{2}\\sum_{x}\\bigg|q_{n}(x)-\\operatorname*{min}\\{p_{n}(x),q_{n}(x)+\\epsilon_{n}(x)\\}-\\mathcal{P}_{n}(x)\\sum_{\\bar{x}}[p_{n}(\\tilde{x})-q_{n}(\\tilde{x})-\\epsilon_{n}(\\tilde{x})]_{+}\\bigg|}\\\\ {\\mathit{s.t.}~}&{\\displaystyle\\sum_{x}\\mathcal{P}_{n}(x)=1,\\quad\\mathcal{P}_{n}(x)\\geqslant0,~\\forall x\\in\\mathcal{V}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we removed $x_{1:n-1}$ for notation simplicity, and recall again $\\epsilon_{n}\\;:=\\;b_{n}p_{n}\\,-\\,q_{n}$ . When $\\Sigma_{\\tilde{x}}[p_{n}(\\tilde{x})-q_{n}(\\tilde{x})-\\epsilon_{n}(\\tilde{x})]_{+}\\,=\\,0\\$ , the objective degenerates to the constant (in ${\\mathcal{P}}_{n}$ ) ${\\sf T V}(q_{n},p_{n})$ . T\u0159herefore, for the rest of the section, we focus on the case where $\\begin{array}{r}{\\sum_{\\tilde{x}}[p_{n}(\\tilde{x})-q_{n}(\\tilde{x})-\\epsilon_{n}(\\tilde{x})]_{+}>0}\\end{array}$ We have the following Theorem that characterizes the solution o f\u0159 (23). ", "page_idx": 24}, {"type": "text", "text": "Theorem 10 (Restatement of Theorem 4). Suppose $\\begin{array}{r}{\\sum_{x}[p_{n}-q_{n}-\\epsilon_{n}]_{+}(x)>0.}\\end{array}$ . Define ", "page_idx": 24}, {"type": "equation", "text": "$$\nA_{n}(x):=\\frac{q_{n}(x)-\\operatorname*{min}\\{p_{n}(x),q_{n}(x)+\\epsilon_{n}(x)\\}}{\\sum_{\\tilde{x}}[p_{n}(\\tilde{x})-q_{n}(\\tilde{x})-\\epsilon_{n}(\\tilde{x})]_{+}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and define the positive token set $A_{+}=\\{x\\in\\mathcal{V}:A_{n}(x)\\geqslant0\\}$ and the negative token set $A_{-}=\\{x\\in$ $\\mathcal{V}:A_{n}(x)<0\\}$ . Then the set of optimal distributions of objective (23) is characterized as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\{\\mathcal{P}_{n}^{\\ast}:\\mathcal{P}_{n}^{\\ast}(x)=0,\\forall x\\in A_{-};0\\leqslant\\mathcal{P}_{n}^{\\ast}(x)\\leqslant A_{n}(x),\\forall x\\in A_{+};\\sum_{x}\\mathcal{P}_{n}^{\\ast}(x)=1\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and the optimal value is ", "page_idx": 24}, {"type": "equation", "text": "$$\nL o s s_{\\mathrm{TV}}^{\\ast}(b)=\\frac{1}{2}\\sum_{x}|q_{n}(x)-\\mathrm{min}\\{p_{n}(x),q_{n}(x)+\\epsilon_{n}(x)\\}|-\\frac{1}{2}\\sum_{x}[p_{n}(x)-q_{n}(x)-\\epsilon_{n}(x)]_{+}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Remark 5. In the main context (Theorem 4) we define $\\begin{array}{r}{A_{n}(x):=\\frac{q_{n}(x)-b_{n}(x)p_{n}(x)}{\\sum_{\\tilde{x}}[1-b_{n}(\\tilde{x})]p_{n}(\\tilde{x})}}\\end{array}$ and $L o s s_{\\mathsf{T V}}^{*}(b)=$ $\\begin{array}{r}{\\frac{1}{t}\\sum_{x}|q_{n}(x)-b_{n}(x)p_{n}(x)|-\\frac{1}{2}\\sum_{x}(1-b_{n}(x))p_{n}(x)}\\end{array}$ . This is eq\u0159uivalent to the above since $\\epsilon_{n}:=$ bnp\u0159n \u00b4 qn. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem $I O$ . In this case, note $\\operatorname*{min}\\{p_{n}(x),q_{n}(x)+\\epsilon_{n}(x)\\}+[p_{n}(x)-q_{n}(x)-\\epsilon_{n}(x)]_{+}\\equiv$ $p_{n}(x)$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{x}q_{n}(x)-\\operatorname*{min}\\{p_{n}(x),q_{n}(x)+\\epsilon_{n}(x)\\}=\\sum_{\\tilde{x}}[p_{n}({\\tilde{x}})-q_{n}({\\tilde{x}})-\\epsilon_{n}({\\tilde{x}})]_{+}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By definition of $A_{n}$ then $\\textstyle\\sum_{x}A_{n}(x)=1$ , and the original objective (23) can be equivalently written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathcal{P}_{n}}~~\\frac{1}{2}\\sum_{x}\\left|A_{n}(x)-\\mathcal{P}_{n}(x)\\right|,\\quad s.t.~\\sum_{x}\\mathcal{P}_{n}(x)=1,\\quad\\mathcal{P}_{n}(x)\\geqslant0,~\\forall x\\in\\mathcal{P}_{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now find the solution of objective (25) in two steps. ", "page_idx": 24}, {"type": "text", "text": "Step1: Let the positive token set $A_{+}=\\{x\\in\\mathcal{V}:A_{n}(x)\\geqslant0\\}$ and the negative token set $A_{-}=\\{x\\in$ $\\bar{\\nu}:\\bar{A}_{n}(x)<0\\bar{\\}$ , then any optimal $\\mathcal{P}_{n}^{*}$ must satisfy $\\mathcal{P}_{n}^{*}(x)=0$ for all $x\\in A_{-}$ . ", "page_idx": 24}, {"type": "text", "text": "First, since $\\textstyle\\sum_{x}A_{n}(x)=1$ , it implies $A_{+}\\neq\\varnothing$ . Suppose for some optimal $\\mathcal{P}_{n}^{*}$ , there exists ${\\bar{x}}\\in A_{-}$ such that $\\mathcal{P}_{n}^{*}(\\tilde{x})>0$ , then we show there exists $\\check{x}\\in A_{+}$ such that $A_{n}(\\check{x})>\\mathcal{\\ddot{P}}_{n}^{*}(\\check{x})$ . Suppose this is ", "page_idx": 24}, {"type": "text", "text": "not the case, i.e. $A_{n}(x)\\leqslant\\mathcal{P}_{n}^{*}(x)\\;\\forall x\\in A_{+}$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1=\\displaystyle\\sum_{x}A_{n}(x)=\\sum_{x\\in A_{-}}A_{n}(x)+\\displaystyle\\sum_{x\\in A_{+}}A_{n}(x)\\leqslant A_{n}(\\bar{x})+\\displaystyle\\sum_{x\\in A_{+}}A_{n}(x)}\\\\ &{\\quad<\\displaystyle\\sum_{x\\in A_{+}}A_{n}(x)\\leqslant\\displaystyle\\sum_{x\\in A_{+}}\\mathcal{P}_{n}^{*}(x)\\leqslant\\displaystyle\\sum_{x\\in\\mathcal{V}}\\mathcal{P}_{n}^{*}(x)=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Contradiction! Hence, there exists $\\check{x}\\in A_{+}$ such that $A_{n}(\\check{x})>\\mathcal{P}_{n}^{*}(\\check{x})$ . ", "page_idx": 25}, {"type": "text", "text": "Second, by $A_{n}(\\check{x})>\\mathcal{P}_{n}^{*}(\\check{x}),\\,-\\mathcal{P}_{n}^{*}(\\bar{x})<0$ , and triangular inequality, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n|-\\mathcal{P}_{n}^{*}(\\bar{x})|+|A_{n}(\\check{x})-\\mathcal{P}_{n}^{*}(\\check{x})|>|A_{n}(\\check{x})-\\mathcal{P}_{n}^{*}(\\check{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note $A_{n}({\\bar{x}})<0$ , the above is equivalent to ", "page_idx": 25}, {"type": "equation", "text": "$$\n|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\check{x})|>|A_{n}(\\bar{x})|+|A_{n}(\\check{x})-\\mathcal{P}_{n}^{*}(\\check{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now set ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathscr{P}_{n}^{\\prime}(x)=\\left\\{\\begin{array}{l l}{\\mathscr{P}_{n}^{*}(x),~\\;x\\notin\\{\\bar{x},\\bar{x}\\},}\\\\ {0,~~~x=\\bar{x},}\\\\ {\\mathscr{P}_{n}^{*}(\\check{x})+\\mathscr{P}_{n}^{*}(\\bar{x}),~~x=\\check{x},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then apply (26) we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x\\in\\{\\bar{x},\\bar{x}\\}}|A_{n}(x)-\\mathcal{P}_{n}^{*}(x)|=\\displaystyle\\sum_{x\\notin\\{\\bar{x},\\bar{x}\\}}|A_{n}(x)-\\mathcal{P}_{n}^{*}(x)|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|}\\\\ &{=\\displaystyle\\sum_{x\\notin\\{\\bar{x},\\bar{x}\\}}|A_{n}(x)-\\mathcal{P}_{n}^{\\prime}(x)|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|}\\\\ &{\\displaystyle>\\displaystyle\\sum_{x\\notin\\{\\bar{x},\\bar{x}\\}}|A_{n}(x)-\\mathcal{P}_{n}^{\\prime}(x)|+|A_{n}(\\bar{x})|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\bar{x})-\\mathcal{P}_{n}^{*}(\\bar{x})|}\\\\ &{=\\displaystyle\\sum_{x\\in\\{\\bar{x},\\bar{x}\\}}|A_{n}(x)-\\mathcal{P}_{n}^{\\prime}(x)|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{\\prime}(\\bar{x})|+|A_{n}(\\bar{x})-\\mathcal{P}_{n}^{\\prime}(\\bar{x})|=\\displaystyle\\sum_{x}|A_{n}(x)-\\mathcal{P}_{n}^{\\prime}(x)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This contradicts $\\mathcal{P}_{n}^{*}$ is the optimal solution! Therefore, for any optimal $\\mathcal{P}_{n}^{*}$ , it holds $\\mathcal{P}_{n}^{*}(x)\\,=\\,0$ $\\forall x\\in A_{-}$ . ", "page_idx": 25}, {"type": "text", "text": "Step2: We characterize the optimal solutions of the objective. Indeed, by Step1, any optimal solution $\\mathcal{P}_{n}^{*}$ satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x}|A_{n}(x)-\\mathcal{P}_{n}^{*}(x)|=\\displaystyle\\sum_{x\\in A_{-}}|A_{n}(x)-\\mathcal{P}_{n}^{*}(x)|+\\displaystyle\\sum_{x\\in A_{+}}|A_{n}(x)-\\mathcal{P}_{n}^{*}(x)|}\\\\ &{=\\displaystyle\\sum_{x\\in A_{-}}|A_{n}(x)|+\\sum_{x\\in A_{+}}|A_{n}(x)-\\mathcal{P}_{n}^{*}(x)|\\geqslant\\displaystyle\\sum_{x\\in A_{-}}|A_{n}(x)|+|\\displaystyle\\sum_{x\\in A_{+}}A_{n}(x)-\\sum_{x\\in A_{+}}\\mathcal{P}_{n}^{*}(x)|}\\\\ &{=\\displaystyle\\sum_{x\\in A_{-}}|A_{n}(x)|+|\\displaystyle\\sum_{x\\in A_{+}}A_{n}(x)-1|=\\sum_{x\\in A_{-}}|A_{n}(x)|+\\displaystyle\\sum_{x\\in A_{+}}A_{n}(x)-1=\\|A_{n}\\|_{1}-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The inequality becomes equality if and only if $A_{n}(x)-{\\mathcal{P}}_{n}^{*}(x)\\geqslant0$ . Finally, recall the definition of $A_{n}$ we receive the optimal value for the original objective is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{x}|q_{n}(x)-\\operatorname*{min}\\{p_{n}(x),q_{n}(x)+\\epsilon_{n}(x)\\}|-\\frac{1}{2}\\sum_{x}[p_{n}(x)-q_{n}(x)-\\epsilon_{n}(x)]_{+}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Replace $\\epsilon_{n}$ by $b_{n}$ gives the desired result. ", "page_idx": 25}, {"type": "text", "text": "Remark 6. The general optimization should follow ", "page_idx": 25}, {"type": "equation", "text": "$$\nL o s s_{\\mathsf{T V}}^{*}(b_{1:T}):=\\operatorname*{min}_{\\mathcal{P}}\\mathsf{T V}[\\mathbb{P}^{A}(x_{1:T}),q(x_{1:T})],\\quad w h e r e\\;A:=(b_{1:T},\\mathcal{P}_{1:T}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Meanwhile, our current analysis only considers the single token setting. We mention that solving the (27) is challenging as it corresponds to a high-dimensional discrete optimization with dimension $T$ and it might not have closed-form solutions in the general cases. ", "page_idx": 25}, {"type": "text", "text": "E.2 Proof of Theorem 5 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. For an algorithm $\\boldsymbol{\\mathcal{A}}$ with the rejection probability $b(\\cdot)$ . Let $\\tilde{x}\\;\\sim\\;p(\\cdot)$ , then the rejection probability is computed as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}({\\mathrm{reject}})=\\sum_{\\tilde{x}}\\mathbb{P}({\\mathrm{reject}},{\\tilde{x}})=\\sum_{{\\tilde{x}}}\\mathbb{P}({\\mathrm{reject}}|{\\tilde{x}})\\mathbb{P}({\\tilde{x}})=\\sum_{{\\tilde{x}}}(1-b({\\tilde{x}}))p({\\tilde{x}}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also, from Theorem 4 ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname{Loss}_{\\mathsf{T V}}^{*}(b)=\\sum_{x}|q(x)-b(x)p(x)|-\\sum_{x}(1-b(x))p(x).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, we show ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{x}|q(x)-b(x)p(x)|+\\sum_{x}(1-b(x))p(x)=\\sum_{x}|p(x)-q(x)|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Indeed, since $\\begin{array}{r}{\\operatorname*{min}\\{1,\\frac{q(x)}{p(x)}\\}\\leqslant b(x)\\leqslant1,\\;\\forall x\\in\\mathcal{V},}\\end{array}$ , then $b(x)p(x)\\geqslant\\operatorname*{min}\\{p(x),q(x)\\}$ . Then we prove the following stronger claim ", "page_idx": 26}, {"type": "equation", "text": "$$\n|q(x)-b(x)p(x)|+(1-b(x))p(x)=|p(x)-q(x)|.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "\u2022 If $q(x)\\,\\geqslant\\,p(x)$ , then $\\begin{array}{r}{1\\,=\\,\\operatorname*{min}\\{1,\\frac{q(x)}{p(x)}\\}\\,\\leqslant\\,b(x)\\,\\leqslant\\,1}\\end{array}$ implies $b(x)\\,=\\,1$ , so the above is equivalent to $|q(x)-p(x)|=|p(x)\\dot{-q}(x)|$ is always true; ", "page_idx": 26}, {"type": "text", "text": "\u2022 If $q(x)<p(x)$ , then $b(x)p(x)\\geqslant\\operatorname*{min}\\{p(x),q(x)\\}=q(x)$ . In this case $|q(x)-b(x)p(x)|+(1-b(x))p(x)=b(x)p(x)-q(x)+(1-b(x))p(x)=p(x)-q(x)=|p(x)-q(x)|.$ ", "page_idx": 26}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "F Numerical Simulation Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To validate the correctness of our theory, we provide the numeric simulations that are displayed in Figure 4,2. We model the distribution $p_{1:T}$ and $q_{1:T}$ to be two non-stationary Markov Chains with 7 states/tokens. Instead of being $p(x_{n}|x_{1:n-1})$ , for Markov Chain, the one step transition is Markovian with $p(x_{n}|x_{1:n-1})=p(x_{n}|x_{n-1})$ . We set the random seed to be 10. The prompt distribution $p_{0}$ is set to be Uniform distribution. For Figure 2, the true value is computed via Theorem 1,3 respectively, and solid line is computed by ", "page_idx": 27}, {"type": "equation", "text": "$$\nN_{r e j}:=\\frac{1}{N}\\sum_{i=1}^{N}N_{r e j}^{i}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the shaded regions are error bars. ", "page_idx": 27}, {"type": "text", "text": "Below presents the simulation for horizon $T=100$ . Left panel of Figure 5 is the standard speculative decoding, the middle panel is batch speculative decoding with batch size $M=2$ , and Right panel shows the expected rejections with varying batch sizes $M$ computed from Theorem 1. ", "page_idx": 27}, {"type": "image", "img_path": "wSqpNeMVLU/tmp/f577a4c4967a7fef20f2d0c37f29001963449130ab621185f87862098c30b59c.jpg", "img_caption": ["Figure 5: A simulation of (Batch) Speculative Decoding with horizon $T=100$ . "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "G Details for Experiment Section 4.2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Consider objective (1) ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname{Loss}_{\\mathsf{T V}}^{*}(b):=\\operatorname*{min}_{\\mathcal{P}}\\mathsf{T V}[\\mathbb{P}^{A},q],\\quad w h e r e\\,\\,\\mathcal{A}:=(b,\\mathcal{P}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any biased algorithm with the given acceptance probability $b>\\operatorname*{min}\\{1,q/p\\}$ , we can rewrite: ", "page_idx": 28}, {"type": "equation", "text": "$$\nb=\\operatorname*{min}\\{1,\\frac{q+\\epsilon}{p}\\},\\quad\\mathrm{for\\;some\\;}\\epsilon>0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We consider the following two decoding options for $\\mathcal{P}$ : ", "page_idx": 28}, {"type": "text", "text": "\u2022 Baseline: select $\\mathcal{P}$ to be suboptimal to (28), i.e. simply set $\\mathcal{P}:=\\;q$ , which is the target distribution;10 We call this method Decoding-UNO.   \n\u2022 Set $\\mathcal{P}:=\\mathcal{P}^{*}$ to be the optimal solution of (28), whose solution is presented in Theorem 4. We call this method Decoding-OPT. ", "page_idx": 28}, {"type": "text", "text": "Measuring performance. Instead of TV distance, we measure the quality via WinRate in our experiment. Concretely, for each prompt, we let Decoding-UNO and Decoding-OPT to generate responses independently, and use score models to compare whose generation has higher quality. We specify draft model $p$ as pythia-70m and target model $q$ as pythia-2.8b from EleutherAI [7]. We apply the score model to be RM-Mistral-7B or GPT-4. We test 200 prompts from Alpaca-Farm-Eval Dataset [13] with 500 responses/comparisons per prompt. For a given prompt, Decoding-OPT wins if for more than 250 comparisons, score model prefer its response11 over Decoding-UNO. Then the WinRate for each method is computed as #wins{200. ", "page_idx": 28}, {"type": "text", "text": "Sanity Check for the experiment. To validate having smaller distance w.r.t. the large model $q$ (pythia-2.8b) indicates higher performance, we preform the WinRate test for decoding via pythia-70m only against decoding via pythia-2.8b only. Table 2 shows that there is a significant performance gap between large model and small model, therefore validate the legitimacy of the experiment in Table 1. ", "page_idx": 28}, {"type": "table", "img_path": "wSqpNeMVLU/tmp/2b2d5151a7fea29673b20460b9f75f65824ae2f4c443fe148d24aef249a5470f.jpg", "table_caption": ["Table 2: WinRate for pythia-2.8b vs pythia-2.8b. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Implementation detail for Table 1. Concretely, we leverage EleutherAI/pythia-2.8b and EleutherAI/pythia-70m from HuggingFace. To perform speculative decoding, we specify assistant model $\\cdot{}$ EleutherAI/pythia-70m in the generation function for target model EleutherAI/pythia-2.8b. ", "page_idx": 28}, {"type": "text", "text": "Notice that for the biased speculative decoding with $\\begin{array}{r}{b(x)=\\operatorname*{min}\\{1,\\frac{q(x)+\\epsilon}{p(x)}\\}}\\end{array}$ , $A(x)$ in Theorem 4 can equivalently expressed as $\\begin{array}{r}{A(x):=\\frac{\\operatorname*{max}\\{q(x)-p(x),-\\epsilon\\}}{\\sum_{\\tilde{x}}\\operatorname*{max}\\{q(\\tilde{x})-p(\\tilde{x}),-\\epsilon\\}}}\\end{array}$ , and we can select $\\mathcal{P}^{*}:=[A]_{+}$ (recall $[\\cdot]_{+}$ in Section 2.1), which satisfies $\\mathcal{P}^{*}\\vert\\overset{\\mathcal{-}}{A_{-}}(\\cdot)=0;0\\leqslant\\mathcal{P}^{*}\\vert_{A_{+}}(\\cdot)\\leqslant A(\\cdot)$ . ", "page_idx": 28}, {"type": "text", "text": "To implement Decoding-UNO, we modify the speculative sampling function in HuggingFace transformers/generation/utils.py flie as follows12 (where variable eps is $\\epsilon$ in Table 1). This is conducted in a single A100 GPU. ", "page_idx": 28}, {"type": "text", "text": "def _speculative_sampling( candidate_input_ids,   \n): #####---- ## The following modification happens at Line 4727 ## of the original file mode_ $\\mathit{\\Theta}=\\mathit{\\Theta}1$ // mode_ $_{\\cdot}{=}1$ denotes Decoding-OPT, else denotes Decoding-UNO ep $\\mathbf{s}_{-}\\ =\\ 0\\,.\\,1$ // This is the epsilon in Table 1. _eps_ $=$ eps_ \\* torch.ones(p_i.shape,dtype $=$ torch.float32,\\ device $\\mathrel{\\mathop:}$ torch.device('cuda:0')) probability_ratio $=$ (p_i $^+$ _eps_) / q_i #####--- if last_assistant_token_is_eos and n_matches $==$ candidate_length: n_matches $\\c-1$ valid_tokens $=$ new_candidate_input_ids[:, : n_matches $^+$ 1] else: n_matches $=$ min(n_matches, max_matches) gamma $=$ min(candidate_logits.shape[1], max_matches) p_n_plus_1 $=$ p[:, n_matches, :] if n_matches $<$ gamma: q_n_plus_ $\\ 1\\ =\\ {\\mathfrak{q}}\\,[\\$ :, n_matches, :] ## The following modification happens at Line 4760 ## of the original file if mode_ $\\circleddash\\ 1$ : ## The following two lines compute A(x) p_prime $=$ torch.clamp((p_n_plus_1 - q_n_plus_1), min= -eps_ p_prime.div_(p_prime.sum()) ## The following two lines compute $\\mathrm{P*}~=~\\left[\\mathrm{A}\\right]_{-}+$ p_prime $=$ torch.clamp(p_prime, min $\\mathit{\\Theta}=\\;0$ ) p_prime.div_(p_prime.sum()) else: ## Baseline Decoding-UNO p_prime $=$ q_n_plus_1 ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We study the theoretical properties for Speculative decoding, which is identical what is stated in abstract. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: In the discussion section, we mentioned that we don\u2019t have a lower bound the batch SD algorithms. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provide the full proof in appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The code is written down in the appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The code is in the appendix, the Alpaca data is also public. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have detailed section in Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have error bars in Figure 2. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Yes, we followed that. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work is most theoretical. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper has no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the materials are open-sourced. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: NO new assets intorduced. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No crowdsourcing involved. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]