[{"figure_path": "wSqpNeMVLU/tables/tables_2_1.jpg", "caption": "Table 1: WinRate for Decoding-OPT vs Decoding-UNO with different over-acceptance threshold  \nThe acceptance probability b(x) = min{1, q(x)+\u20ac}.", "description": "This table presents the results of a simple experiment comparing two decoding methods: Decoding-OPT (using the optimal distribution from Theorem 4) and Decoding-UNO (using the target distribution as a suboptimal solution). The experiment uses pythia-70m as the draft model and pythia-2.8b as the target model.  The WinRate is used as the metric for quality, indicating the percentage of prompts where Decoding-OPT outperforms Decoding-UNO.  Different over-acceptance thresholds (epsilon values) are tested using two different score models: RM-Mistral-7B and GPT-4. The results show that Decoding-OPT consistently outperforms Decoding-UNO across various epsilon values and score models.", "section": "4.2 Experiment"}, {"figure_path": "wSqpNeMVLU/tables/tables_8_1.jpg", "caption": "Table 1: WinRate for Decoding-OPT vs Decoding-UNO with different over-acceptance threshold \n\u20ac. The acceptance probability b(x) = min{1, q(x)+\u20ac}.", "description": "This table compares the performance of two decoding methods, Decoding-OPT and Decoding-UNO, using different over-acceptance thresholds (epsilon values).  The WinRate metric shows the percentage of times each method produced a higher-quality response as judged by a score model (either RM-Mistral-7B or GPT-4) across 200 prompts, with 500 comparisons per prompt.  Decoding-OPT consistently outperforms Decoding-UNO across various epsilon values and scoring models, demonstrating the effectiveness of the Pareto-optimal solution for output quality and rejection trade-offs.", "section": "4.2 Experiment"}, {"figure_path": "wSqpNeMVLU/tables/tables_13_1.jpg", "caption": "Table 1: WinRate for Decoding-OPT vs Decoding-UNO with different over-acceptance threshold \n\u20ac. The acceptance probability b(x) = min{1, q(x)+\n\u20ac}.", "description": "This table presents the results of an experiment comparing two decoding methods: Decoding-OPT and Decoding-UNO.  Decoding-OPT uses the optimal distribution derived in Theorem 4, while Decoding-UNO uses a suboptimal distribution. The experiment measures the \"WinRate\", which is the percentage of times Decoding-OPT's responses are preferred by a score model over Decoding-UNO's responses, for a set of prompts and different values of a hyperparameter (epsilon). The table shows the WinRate for two different score models (RM-Mistral-7B and GPT-4) and different epsilon values.", "section": "4.2 Experiment"}, {"figure_path": "wSqpNeMVLU/tables/tables_28_1.jpg", "caption": "Table 2: WinRate for Decoding-OPT vs Decoding-UNO with different over-acceptance threshold  \n\u20ac. The acceptance probability b(x) = min{1, q(x)+\u20ac}.", "description": "This table presents the results of a simple experiment comparing two decoding methods, Decoding-OPT and Decoding-UNO, using different over-acceptance thresholds (epsilon).  The WinRate metric is used to measure the quality of responses, indicating the percentage of times each method produced a higher-quality response (as determined by a score model) for a set of prompts.  The methods use pythia-70m as a draft model and pythia-2.8b as the target model.  Two scoring models, RM-Mistral-7B and GPT-4, are used to evaluate the quality of generated responses. The table shows how the choice of epsilon and the scoring model affect the relative performance of the two decoding methods.", "section": "4.2 Experiment"}]