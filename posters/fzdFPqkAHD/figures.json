[{"figure_path": "fzdFPqkAHD/figures/figures_0_1.jpg", "caption": "Figure 1: Learning agent behavior from longitudinal casual video recordings. We answer the following question: can we simulate the behavior of an agent, by learning from casually-captured videos of the same agent recorded across a long period of time (e.g., a month)? A) We first reconstruct videos in 4D (3D & time), which includes the scene, the trajectory of the agent, and the trajectory of the observer (i.e., camera held by observer). Such individual 4D reconstruction are registered across time, resulting in a complete 4D reconstructions. B) Then we learn a representation of the agent that allows for interactive behavior simulation. The behavior model explicitly reasons about goals, paths, and full body movements conditioned on the agent's ego-perception and past trajectory. Such agent representation allows us to simulate novel scenarios through conditioning. For example, conditioned different observer trajectories, the cat agent choose to walk to the carpet, stays still while quivering his tail, or hide under the tray stand. Please see videos and results of other agents in the supplement.", "description": "This figure shows the overall process of the Agent-to-Sim (ATS) framework.  Panel A illustrates the 4D reconstruction of casually recorded videos, integrating agent and scene information across multiple video shots to create a spatiotemporal representation. Panel B depicts the interactive behavior simulator, which uses this 4D reconstruction to learn an agent behavior model capable of generating plausible and interactive actions conditioned on the agent's perception of the scene and the observer's movement.  The example given shows how the cat's behavior changes based on different observer trajectories.", "section": "1 Introduction"}, {"figure_path": "fzdFPqkAHD/figures/figures_1_1.jpg", "caption": "Figure 1: Learning agent behavior from longitudinal casual video recordings. We answer the following question: can we simulate the behavior of an agent, by learning from casually-captured videos of the same agent recorded across a long period of time (e.g., a month)? A) We first reconstruct videos in 4D (3D & time), which includes the scene, the trajectory of the agent, and the trajectory of the observer (i.e., camera held by observer). Such individual 4D reconstruction are registered across time, resulting in a complete 4D reconstructions. B) Then we learn a representation of the agent that allows for interactive behavior simulation. The behavior model explicitly reasons about goals, paths, and full body movements conditioned on the agent's ego-perception and past trajectory. Such agent representation allows us to simulate novel scenarios through conditioning. For example, conditioned different observer trajectories, the cat agent choose to walk to the carpet, stays still while quivering his tail, or hide under the tray stand. Please see videos and results of other agents in the supplement.", "description": "This figure shows the overall process of learning agent behavior from casual videos.  Panel A illustrates the 4D reconstruction of the scene, agent, and observer from multiple video recordings. Panel B demonstrates the interactive behavior simulator which simulates agent behavior based on its perception and past trajectory, allowing for interactive simulations. An example simulation of a cat responding to various observer movements is given. ", "section": "1 Introduction"}, {"figure_path": "fzdFPqkAHD/figures/figures_1_2.jpg", "caption": "Figure 7: Interactive behavior simulation with user conditioning. By changing the trajectory of the user, one could influence the behavior of the agent. Given different control inputs, the agent may follow the user or run away from the user.", "description": "This figure demonstrates the interactive nature of the agent's behavior in response to user actions.  Three scenarios are shown, each depicting a different user trajectory ('idle', 'move sideway', 'approach quickly').  The agent's response to each trajectory is depicted visually using a colored line showing its path, indicating how the agent modifies its behavior (walking towards the user, moving away from the user, or staying in place) depending on the user's actions. The figure highlights the model's capacity to generate plausible and interactive behaviors that dynamically adapt to environmental changes (here, changes in user trajectory).", "section": "3.3 Interactive Behavior Generation"}, {"figure_path": "fzdFPqkAHD/figures/figures_4_1.jpg", "caption": "Figure 2: Results of 4D reconstruction. Top: reference images and renderings of the reconstructions. The color on the background represents correspondence. The colored blobs on the agent body represent B = 25 body parts of the agent (e.g., head is represented by the yellow blob). Bottom: Bird's eye view of the reconstructed scene and agent trajectories, registered to the same scene coordinate. Each colored line represents a unique video sequence where boxes and spheres indicate the starting and the end location. Please see videos and results on other agents in the supplement.", "description": "This figure shows the results of the 4D reconstruction process. The top part displays reference images and renderings of the reconstructions, highlighting the correspondence between different shots through color-coding.  The colored blobs on the cat's body represent 25 body parts, facilitating motion tracking. The bottom part provides a bird's-eye view of the reconstructed scene and agent trajectories, aligning all video sequences to a common coordinate system.  Each colored line represents a separate video sequence, with boxes and spheres indicating trajectory start and end points.", "section": "3.2 Optimization: Multi-Video Registration"}, {"figure_path": "fzdFPqkAHD/figures/figures_6_1.jpg", "caption": "Figure 3: Pipeline for behavior generation. We first encode egocentric information into a perception code \u03c9 and then generate full body motion in a hierarchical fashion. We start by generating goals Z with low latency, and then generate a path P and body motion G conditioned on the previous node. Each node is represented by the gradient of its log distribution, trained with the denoising objectives (Eq. 9). Given G, the dense deformation of an agent can be computed via blend skinning (Eq. 3).", "description": "This figure illustrates the pipeline for behavior generation in the Agent-to-Sim (ATS) framework.  It shows a hierarchical process: First, egocentric information (scene, observer, past trajectory) is encoded into a perception code. This code is then used to generate goals, paths, and finally, the full body motion of the agent. Each step is conditioned on the previous one, and the model uses diffusion models for generating multi-modal outputs.  The final body motion is calculated using blend skinning.", "section": "3.3 Interactive Behavior Generation"}, {"figure_path": "fzdFPqkAHD/figures/figures_7_1.jpg", "caption": "Figure 4: Comparison on multi-video scene reconstruction. We show a top-down visualization of the reconstructed scene using the bunny dataset. Compared to TotalRecon that does not register multiple videos, ATS produces higher-quality scene reconstruction. Neural localizer and featuremetric losses are shown important for camera registration. Scene annealing is important for reconstructing high-quality scenes from limited views in a video.", "description": "This figure compares the results of multi-video scene reconstruction using the proposed Agent-to-Sim (ATS) framework and the TotalRecon method.  The top-down visualization shows that ATS, which leverages neural localization, featuremetric losses, and scene annealing, produces a significantly higher-quality reconstruction than TotalRecon, especially when dealing with limited views in the video data.  The improved registration of multiple videos contributes to a more complete and coherent representation of the 3D environment.", "section": "3.2 Optimization: Multi-Video Registration"}, {"figure_path": "fzdFPqkAHD/figures/figures_8_1.jpg", "caption": "Figure 5: Analysis of conditioning signals. We show results of removing one conditioning signal at a time. Removing observer conditioning and past trajectory conditioning makes the sampled goals more spread out (e.g., regions both in front of the agent and behind the agent); removing the environment conditioning introduces infeasible goals that penetrate the ground and the walls.", "description": "This figure demonstrates the effects of removing conditioning signals (user, past trajectory, and environment) from the interactive behavior generation model.  By systematically removing each signal, the figure shows how the model's predictions of agent goals change. The removal of observer and past trajectory conditioning leads to a wider distribution of predicted goals. The lack of environmental conditioning results in infeasible goals, suggesting the model relies on environmental constraints for realistic predictions.", "section": "3.3 Interactive Behavior Generation"}, {"figure_path": "fzdFPqkAHD/figures/figures_14_1.jpg", "caption": "Figure 6: Given the 3D trajectories of the agent and the user accumulated over time (top), one could compute their preference represented by 3D heatmaps (bottom). Note the high agent preference over table and sofa.", "description": "This figure visualizes the agent's and user's movement trajectories in 3D space over time, represented by colored lines.  The top row displays the trajectories themselves, while the bottom row shows heatmaps indicating the frequency of visits to specific locations, revealing the agent's and user's preferences in the environment. Notably, the agent demonstrates a strong preference for areas such as the table and sofa.", "section": "B Additional Results"}, {"figure_path": "fzdFPqkAHD/figures/figures_15_1.jpg", "caption": "Figure 7: Interactive behavior simulation with user conditioning. By changing the trajectory of the user, one could influence the behavior of the agent. Given different control inputs, the agent may follow the user or run away from the user.", "description": "This figure demonstrates the interactive nature of the generated agent behavior.  The agent's actions are conditioned on the user's trajectory. In different scenarios, the agent's response to the user's movement varies, showing that it can either follow the user or avoid the user depending on the input conditions. The figure visually represents this by showing several simulated scenarios with different user trajectories and the corresponding agent responses.", "section": "3.3 Interactive Behavior Generation"}, {"figure_path": "fzdFPqkAHD/figures/figures_15_2.jpg", "caption": "Figure 8: Qualitative comparison with TotalRecon [52] on 4D reconstruction. Top: reconstruction of the agent at at specific frame. Total-recon produces shapes with missing limbs and bone transformations that are misaligned with the shape, while our method produces complete shapes and good alignment. Bottom: reconstruction of the environment. TotalRecon produces distorted and incomplete geometry (due to lack of observations from a single video), while our method produces an accurate and complete environment reconstruction.", "description": "This figure compares the results of 4D reconstruction using the proposed method (ATS) and TotalRecon.  The top row shows the reconstructed agent model from different perspectives, highlighting that ATS produces a complete and accurately aligned model, unlike TotalRecon which results in missing limbs and misalignment. The bottom row shows the reconstructed environment, with ATS achieving a complete and undistorted model in contrast to TotalRecon's distorted and incomplete reconstruction.", "section": "4.2 Interactive Behavior Prediction"}]